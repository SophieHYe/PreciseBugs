{"buggy_code": ["/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#ifndef TENSORFLOW_CORE_KERNELS_RESHAPE_OP_H_\n#define TENSORFLOW_CORE_KERNELS_RESHAPE_OP_H_\n\n#include <memory>\n\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/register_types.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/tensor_shape.h\"\n#include \"tensorflow/core/framework/types.h\"\n#include \"tensorflow/core/lib/core/status.h\"\n#include \"tensorflow/core/platform/logging.h\"\n#include \"tensorflow/core/util/overflow.h\"\n\nnamespace tensorflow {\n\n// Note that this op is subclassed for QuantizedReshapeOp.\nclass ReshapeOp : public OpKernel {\n public:\n  explicit ReshapeOp(OpKernelConstruction* context) : OpKernel(context) {}\n\n  void Compute(OpKernelContext* context) override {\n    const Tensor& input = context->input(0);\n    const Tensor& sizes = context->input(1);\n    // Preliminary validation of sizes.\n    OP_REQUIRES(\n        context,\n        (TensorShapeUtils::IsVector(sizes.shape()) ||\n         // TODO(rmlarsen): Disallow legacy use of scalars to represent shape.\n         TensorShapeUtils::IsScalar(sizes.shape())),\n        errors::InvalidArgument(\"sizes input must be 1-D, not \",\n                                sizes.shape().DebugString()));\n\n    // Compute the output shape.  Determine product of specified\n    // dimensions, and find the index of the unspecified one.\n    TensorShape shape;\n    int64_t product = 1;\n    int unknown_index = -1;\n    bool sizes_has_zero_dim;\n    switch (sizes.dtype()) {\n      case DT_INT32:\n        OP_REQUIRES_OK(context,\n                       ValidateSizes<int32>(sizes, &product, &unknown_index,\n                                            &shape, &sizes_has_zero_dim));\n        break;\n      case DT_INT64:\n        OP_REQUIRES_OK(context,\n                       ValidateSizes<int64_t>(sizes, &product, &unknown_index,\n                                              &shape, &sizes_has_zero_dim));\n        break;\n      default:\n        context->CtxFailure(errors::InvalidArgument(\n            \"desired shape must be a DT_INT32 or DT_INT64 vector, not a \",\n            DataTypeString(sizes.dtype())));\n        return;\n    }\n    if (unknown_index != -1) {\n      int64_t input_num_elements = 1;\n      bool input_has_zero_dim = false;\n      for (int dim = 0; dim < input.dims(); dim++) {\n        // For zero dimension, we don't count it into `input_num_elements`\n        // unless `sizes` has no zero dimension, so we are still able to\n        // infer shapes for other dimensions.\n        if (input.dim_size(dim) > 0 || !sizes_has_zero_dim) {\n          input_num_elements *= input.dim_size(dim);\n        } else {\n          input_has_zero_dim = true;\n        }\n      }\n\n      const int64_t missing = input_num_elements / product;\n      if (!input_has_zero_dim) {\n        OP_REQUIRES(\n            context, product * missing == input_num_elements,\n            errors::InvalidArgument(\n                \"Input to reshape is a tensor with \", input_num_elements,\n                \" values, but the requested shape requires a multiple of \",\n                product));\n      }\n      shape.set_dim(unknown_index, missing);\n    }\n    OP_REQUIRES(context, shape.num_elements() == input.NumElements(),\n                errors::InvalidArgument(\"Input to reshape is a tensor with \",\n                                        input.NumElements(),\n                                        \" values, but the requested shape has \",\n                                        shape.num_elements()));\n\n    // Actually produce the reshaped output.\n    Tensor output(input.dtype());\n    CHECK(output.CopyFrom(input, shape));\n    context->set_output(0, output);\n  }\n\n  bool IsExpensive() override { return false; }\n\n private:\n  template <typename Tshape>\n  Status ValidateSizes(const Tensor& sizes, int64_t* product,\n                       int* unknown_index, TensorShape* shape,\n                       bool* has_zero_dim) {\n    *product = 1;\n    *unknown_index = -1;\n    *has_zero_dim = false;\n    const int64_t num_dims = sizes.NumElements();\n    auto Svec = sizes.flat<Tshape>();\n    for (int d = 0; d < num_dims; ++d) {\n      const Tshape size = Svec(d);\n      if (size == -1) {\n        if (*unknown_index != -1) {\n          return errors::InvalidArgument(\n              \"Only one input size may be -1, not both \", *unknown_index,\n              \" and \", d);\n        }\n        *unknown_index = d;\n        shape->AddDim(1);\n      } else if (size < 0) {\n        return errors::InvalidArgument(\"Size \", d,\n                                       \" must be non-negative, not \", size);\n      } else if (size == 0) {\n        // We don't include zero-sized dimension in product, so that we can\n        // still calculate number of elements for non-zero-sized dimensions and\n        // therefore infer their shapes.\n        shape->AddDim(size);\n        *has_zero_dim = true;\n      } else {\n        if (MultiplyWithoutOverflow(shape->num_elements(), size) < 0) {\n          string msg;\n          for (int ii = 0; ii < num_dims; ++ii) {\n            if (ii != 0) {\n              strings::StrAppend(&msg, \", \");\n            }\n            strings::StrAppend(&msg, Svec(ii));\n          }\n          return errors::InvalidArgument(\"Shape [\", msg,\n                                         \"] has too many elements\");\n        }\n        shape->AddDim(size);\n        (*product) *= size;\n      }\n    }\n    return Status::OK();\n  }\n};\n\n}  // namespace tensorflow\n\n#endif  // TENSORFLOW_CORE_KERNELS_RESHAPE_OP_H_\n", "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for array_ops.\"\"\"\nimport re\nimport time\nimport unittest\n\nfrom absl.testing import parameterized\nimport numpy as np\n\nfrom tensorflow.python.client import session\nfrom tensorflow.python.eager import backprop\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.eager import def_function\nfrom tensorflow.python.framework import config\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors\nfrom tensorflow.python.framework import errors_impl\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import sparse_tensor\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.framework import tensor_spec\nfrom tensorflow.python.framework import test_ops\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import gen_array_ops\nfrom tensorflow.python.ops import gradient_checker_v2\nfrom tensorflow.python.ops import init_ops\nfrom tensorflow.python.ops import list_ops\nfrom tensorflow.python.ops import map_fn\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import random_ops\nfrom tensorflow.python.ops import resource_variable_ops\nfrom tensorflow.python.ops import state_ops\nfrom tensorflow.python.ops import variable_scope\nfrom tensorflow.python.ops import variables\nfrom tensorflow.python.ops.ragged.ragged_tensor import RaggedTensor\nfrom tensorflow.python.platform import test as test_lib\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass BatchMatrixTransposeTest(test_util.TensorFlowTestCase):\n\n  def testNonBatchMatrix(self):\n    matrix = [[1, 2, 3], [4, 5, 6]]  # Shape (2, 3)\n    expected_transposed = [[1, 4], [2, 5], [3, 6]]  # Shape (3, 2)\n    transposed = array_ops.matrix_transpose(matrix)\n    self.assertEqual((3, 2), transposed.get_shape())\n    self.assertAllEqual(expected_transposed, transposed)\n\n  def testConjugate(self):\n    m = [[1 + 1j, 2 + 2j, 3 + 3j], [4 + 4j, 5 + 5j, 6 + 6j]]\n    expected_transposed = [[1 - 1j, 4 - 4j], [2 - 2j, 5 - 5j], [3 - 3j, 6 - 6j]]\n    matrix = ops.convert_to_tensor(m)\n    transposed = array_ops.matrix_transpose(matrix, conjugate=True)\n    self.assertEqual((3, 2), transposed.get_shape())\n    self.assertAllEqual(expected_transposed, transposed)\n\n  def testBatchMatrix(self):\n    matrix_0 = [[1, 2, 3], [4, 5, 6]]\n    matrix_0_t = [[1, 4], [2, 5], [3, 6]]\n    matrix_1 = [[11, 22, 33], [44, 55, 66]]\n    matrix_1_t = [[11, 44], [22, 55], [33, 66]]\n    batch_matrix = [matrix_0, matrix_1]  # Shape (2, 2, 3)\n    expected_transposed = [matrix_0_t, matrix_1_t]  # Shape (2, 3, 2)\n    transposed = array_ops.matrix_transpose(batch_matrix)\n    self.assertEqual((2, 3, 2), transposed.get_shape())\n    self.assertAllEqual(expected_transposed, transposed)\n\n  def testNonBatchMatrixDynamicallyDefined(self):\n    # needs explicit `constant` because lists are not automatically\n    # converted to sensors when applying `transpose` below\n    matrix = constant_op.constant([[1, 2, 3], [4, 5, 6]])  # Shape (2, 3)\n    expected_transposed = [[1, 4], [2, 5], [3, 6]]  # Shape (3, 2)\n\n    @def_function.function(input_signature=[\n        tensor_spec.TensorSpec(shape=None, dtype=dtypes.int32)\n    ])\n    def transpose(matrix):\n      self.assertIs(matrix.shape.ndims, None)\n      return array_ops.matrix_transpose(matrix)\n\n    self.assertAllEqual(expected_transposed, transpose(matrix))\n\n  def testBatchMatrixDynamicallyDefined(self):\n    matrix_0 = [[1, 2, 3], [4, 5, 6]]\n    matrix_0_t = [[1, 4], [2, 5], [3, 6]]\n    matrix_1 = [[11, 22, 33], [44, 55, 66]]\n    matrix_1_t = [[11, 44], [22, 55], [33, 66]]\n    # needs explicit `constant` because lists are not automatically\n    # converted to sensors when applying `transpose` below\n    batch_matrix = constant_op.constant([matrix_0, matrix_1])  # Shape (2, 2, 3)\n    expected_transposed = [matrix_0_t, matrix_1_t]  # Shape (2, 3, 2)\n\n    @def_function.function(input_signature=[\n        tensor_spec.TensorSpec(shape=None, dtype=dtypes.int32)\n    ])\n    def transpose(matrix):\n      self.assertIs(matrix.shape.ndims, None)\n      return array_ops.matrix_transpose(matrix)\n\n    self.assertAllEqual(expected_transposed, transpose(batch_matrix))\n\n  def testTensorWithStaticRankLessThanTwoRaisesBecauseNotAMatrix(self):\n    vector = [1, 2, 3]\n    with self.assertRaisesRegex(ValueError, \"should be a \"):\n      array_ops.matrix_transpose(vector)\n\n  def testNarrowMatrixConjugateTranspose(self):\n    for dtype in (dtypes.float32, dtypes.float64):\n      for conjugate in (True, False):\n        with self.subTest(complex_type=dtype, conjugate=conjugate):\n          vector = math_ops.complex(\n              constant_op.constant(0, dtype=dtype),\n              math_ops.range(96, dtype=dtype))\n          column_vector = array_ops.expand_dims(vector, axis=-1)\n          row_vector = array_ops.expand_dims(vector, axis=0)\n          narrow_matrix = array_ops.tile(column_vector, [1, 2])  # [96, 2]\n          expected_transposed = array_ops.tile(row_vector, [2, 1])  # [2, 96]\n          if conjugate:\n            expected_transposed = -expected_transposed\n\n          transposed = array_ops.matrix_transpose(\n              narrow_matrix, conjugate=conjugate)\n\n          self.assertEqual((2, 96), transposed.get_shape())\n          self.assertAllEqual(expected_transposed, transposed)\n\n\nclass BooleanMaskTest(test_util.TensorFlowTestCase):\n\n  def setUp(self):\n    self.rng = np.random.RandomState(42)\n\n  def CheckVersusNumpy(self, ndims_mask, arr_shape, make_mask=None, axis=None):\n    \"\"\"Check equivalence between boolean_mask and numpy masking.\"\"\"\n    if make_mask is None:\n      make_mask = lambda shape: self.rng.randint(0, 2, size=shape).astype(bool)\n    arr = np.random.rand(*arr_shape)\n    mask = make_mask(arr_shape[:ndims_mask])\n    if axis is not None:\n      mask = make_mask(arr_shape[axis:ndims_mask + axis])\n    if axis is None or axis == 0:\n      masked_arr = arr[mask]\n    elif axis == 1:\n      masked_arr = arr[:, mask]\n    elif axis == 2:\n      masked_arr = arr[:, :, mask]\n    masked_tensor = array_ops.boolean_mask(arr, mask, axis=axis)\n\n    # Leading dimension size of masked_tensor is always unknown until runtime\n    # since we don't how many elements will be kept.\n    leading = 1 if axis is None else axis + 1\n    self.assertAllEqual(masked_tensor.get_shape()[leading:],\n                        masked_arr.shape[leading:])\n\n    self.assertAllClose(masked_arr, masked_tensor)\n\n  def testMaskDim1ArrDim2Axis1(self):\n    ndims_mask = 1\n    for arr_shape in [(1, 1), (2, 2), (2, 5)]:\n      with self.subTest(arr_shape=arr_shape):\n        self.CheckVersusNumpy(ndims_mask, arr_shape, axis=1)\n\n  def testMaskDim2ArrDim2Axis1(self):\n    ndims_mask = 2\n    for arr_shape in [(1, 1), (2, 2), (2, 5)]:\n      with self.subTest(arr_shape=arr_shape):\n        self.CheckVersusNumpy(ndims_mask, arr_shape, axis=1)\n\n  def testMaskDim1ArrDim1(self):\n    ndims_mask = 1\n    for arr_shape in [(1,), (2,), (3,), (10,)]:\n      with self.subTest(arr_shape=arr_shape):\n        self.CheckVersusNumpy(ndims_mask, arr_shape)\n\n  def testMaskDim1ArrDim2(self):\n    ndims_mask = 1\n    for arr_shape in [(1, 1), (2, 2), (2, 5)]:\n      with self.subTest(arr_shape=arr_shape):\n        self.CheckVersusNumpy(ndims_mask, arr_shape)\n\n  def testMaskDim2ArrDim2(self):\n    ndims_mask = 2\n    for arr_shape in [(1, 1), (2, 2), (2, 5)]:\n      with self.subTest(arr_shape=arr_shape):\n        self.CheckVersusNumpy(ndims_mask, arr_shape)\n\n  def testMaskDim2ArrDim3(self):\n    ndims_mask = 2\n    for arr_shape in [(1, 1, 1), (1, 2, 2), (2, 2, 1)]:\n      with self.subTest(arr_shape=arr_shape):\n        self.CheckVersusNumpy(ndims_mask, arr_shape)\n\n  def testEmptyInput2D(self):\n    mask = np.array([True, False])\n    arr = np.array([[], []]).astype(np.float32)\n    numpy_result = arr[mask]\n    tf_result = array_ops.boolean_mask(arr, mask)\n    self.assertAllEqual(numpy_result.shape[1:], tf_result.get_shape()[1:])\n    with self.cached_session():\n      self.assertAllClose(numpy_result, tf_result)\n\n  def testEmptyInput1D(self):\n    mask = np.array([]).astype(bool)\n    arr = np.array([]).astype(np.float32)\n    numpy_result = arr[mask]\n    tf_result = array_ops.boolean_mask(arr, mask)\n    self.assertAllEqual(numpy_result.shape[1:], tf_result.get_shape()[1:])\n    with self.cached_session():\n      self.assertAllClose(numpy_result, tf_result)\n\n  def testEmptyOutput(self):\n    make_mask = lambda shape: np.zeros(shape, dtype=bool)\n    for ndims_mask in range(1, 4):\n      for ndims_arr in range(ndims_mask, ndims_mask + 3):\n        for _ in range(3):\n          with self.subTest(ndims_mask=ndims_mask, ndims_arr=ndims_arr, _=_):\n            arr_shape = np.random.randint(1, 5, size=ndims_arr)\n            self.CheckVersusNumpy(ndims_mask, arr_shape, make_mask=make_mask)\n\n  def testWorksWithDimensionsEqualToNoneDuringGraphBuild(self):\n    # The rank of the mask tensor must be specified. This is explained\n    # in the docstring as well.\n    @def_function.function\n    def func(ph_tensor, ph_mask):\n      return array_ops.boolean_mask(ph_tensor, ph_mask)\n\n    f = func.get_concrete_function(\n        tensor_spec.TensorSpec(None, dtypes.int32),\n        tensor_spec.TensorSpec([None], dtypes.bool))\n    arr = np.array([[1, 2], [3, 4]], np.int32)\n    mask = np.array([False, True])\n    masked_tensor = f(arr, mask)\n    self.assertAllEqual(masked_tensor, arr[mask])\n\n  def testMaskDimensionsSetToNoneRaises(self):\n    # The rank of the mask tensor must be specified. This is explained\n    # in the docstring as well.\n    @def_function.function\n    def func(tensor, mask):\n      return array_ops.boolean_mask(tensor, mask)\n\n    with self.assertRaisesRegex(ValueError, \"dimensions must be specified\"):\n      _ = func.get_concrete_function(\n          tensor_spec.TensorSpec([None, 2], dtypes.int32),\n          tensor_spec.TensorSpec(None, dtypes.bool))\n\n  def testMaskHasMoreDimsThanTensorRaises(self):\n    mask = [[True, True], [False, False]]\n    tensor = [1, 2, 3, 4]\n    with self.cached_session():\n      with self.assertRaisesRegex(ValueError, \"incompatible\"):\n        self.evaluate(array_ops.boolean_mask(tensor, mask))\n\n  def testMaskIsScalarRaises(self):\n    mask = True\n    tensor = 1\n    with self.cached_session():\n      with self.assertRaisesRegex(ValueError, \"mask.*scalar\"):\n        self.evaluate(array_ops.boolean_mask(tensor, mask))\n\n  def testMaskShapeDifferentThanFirstPartOfTensorShapeRaises(self):\n    mask = [True, True, True]\n    tensor = [[1, 2], [3, 4]]\n    with self.cached_session():\n      with self.assertRaisesRegex(ValueError, \"incompatible\"):\n        self.evaluate(array_ops.boolean_mask(tensor, mask))\n\n  def testStringMask(self):\n    # Reproduces b/111171330, where the optimized boolean_mask graph would\n    # be incorrectly placed on GPU.\n    config.set_optimizer_experimental_options({\"shape_optimization\": True})\n\n    @def_function.function\n    def func(tile_input):\n      string_tensor = array_ops.tile([[\"hello\"]], tile_input)\n      bool_tensor = array_ops.tile([[True]], tile_input)\n      masked_tensor = array_ops.boolean_mask(string_tensor, bool_tensor)\n      return masked_tensor\n\n    result = func([2, 2])\n    self.assertAllEqual([b\"hello\", b\"hello\", b\"hello\", b\"hello\"], result)\n\n  def testMaskWithAxisTensor(self):\n\n    @def_function.function(autograph=False)\n    def f():\n      return array_ops.boolean_mask([1, 2, 3], [True, False, True],\n                                    axis=constant_op.constant(\n                                        0, dtype=dtypes.int32))\n\n    self.assertAllEqual(self.evaluate(f()), [1, 3])\n\n  def testMaskWithAxisNonConstTensor(self):\n\n    @def_function.function(\n        autograph=False,\n        input_signature=[\n            tensor_spec.TensorSpec(shape=None, dtype=dtypes.int32)\n        ])\n    def f(axis):\n      return array_ops.boolean_mask([1, 2, 3], [True, False, True], axis=axis)\n\n    self.assertAllEqual(\n        self.evaluate(f(constant_op.constant(0, dtype=dtypes.int32))), [1, 3])\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass OperatorShapeTest(test_util.TensorFlowTestCase):\n\n  def testExpandScalar(self):\n    scalar = \"hello\"\n    scalar_expanded = array_ops.expand_dims(scalar, [0])\n    self.assertEqual(scalar_expanded.get_shape(), (1,))\n\n  def testSqueezeScalar(self):\n    scalar = \"hello\"\n    scalar_squeezed = array_ops.squeeze(scalar, ())\n    self.assertEqual(scalar_squeezed.get_shape(), ())\n\n  def testSqueezeMatrix(self):\n    matrix = [[1, 2, 3]]\n    matrix_squeezed = array_ops.squeeze(matrix, [0])\n    self.assertEqual(matrix_squeezed.get_shape(), (3))\n\n    with self.assertRaisesRegex(\n        Exception, \"Can not squeeze dim.1., expected a dimension of 1, got 3\"):\n      matrix_squeezed = array_ops.squeeze(matrix, [1])\n\n  def testSqueezeScalarDim(self):\n    matrix = [[1, 2, 3]]\n    matrix_squeezed = array_ops.squeeze(matrix, 0)\n    self.assertEqual(matrix_squeezed.get_shape(), (3))\n\n  def testExpandDimsWithNonScalarDim(self):\n    with self.assertRaisesRegex(Exception,\n                                \"must be a tensor with a single value\"):\n      array_ops.expand_dims(1, axis=[0, 1])\n\n\n@test_util.with_eager_op_as_function\nclass ReverseV2Test(test_util.TensorFlowTestCase):\n\n  def testReverse0DimAuto(self):\n    x_np = 4\n    for use_gpu in [False, True]:\n      with self.subTest(use_gpu=use_gpu):\n        with self.cached_session(use_gpu=use_gpu):\n          x_tf = self.evaluate(array_ops.reverse_v2(x_np, []))\n          self.assertAllEqual(x_tf, x_np)\n\n  def _reverse1DimAuto(self, np_dtype):\n    x_np = np.array([1, 200, 3, 40, 5], dtype=np_dtype)\n\n    for use_gpu in [False, True]:\n      for axis_dtype in [dtypes.int32, dtypes.int64]:\n        with self.subTest(use_gpu=use_gpu, axis_dtype=axis_dtype):\n          x_tf = self.evaluate(\n              array_ops.reverse_v2(x_np,\n                                   constant_op.constant([0], dtype=axis_dtype)))\n          self.assertAllEqual(x_tf, np.asarray(x_np)[::-1])\n\n  def _reverse2DimAuto(self, np_dtype):\n    x_np = np.array([[1, 200, 3], [4, 5, 60]], dtype=np_dtype)\n\n    for reverse_f in [array_ops.reverse_v2, array_ops.reverse]:\n      for use_gpu in [False, True]:\n        for axis_dtype in [dtypes.int32, dtypes.int64]:\n          with self.subTest(\n              reverse_f=reverse_f, use_gpu=use_gpu, axis_dtype=axis_dtype):\n            x_tf_1 = self.evaluate(\n                reverse_f(x_np, constant_op.constant([0], dtype=axis_dtype)))\n            x_tf_2 = self.evaluate(\n                reverse_f(x_np, constant_op.constant([-2], dtype=axis_dtype)))\n            x_tf_3 = self.evaluate(\n                reverse_f(x_np, constant_op.constant([1], dtype=axis_dtype)))\n            x_tf_4 = self.evaluate(\n                reverse_f(x_np, constant_op.constant([-1], dtype=axis_dtype)))\n            x_tf_5 = self.evaluate(\n                reverse_f(x_np, constant_op.constant([1, 0], dtype=axis_dtype)))\n            self.assertAllEqual(x_tf_1, np.asarray(x_np)[::-1, :])\n            self.assertAllEqual(x_tf_2, np.asarray(x_np)[::-1, :])\n            self.assertAllEqual(x_tf_3, np.asarray(x_np)[:, ::-1])\n            self.assertAllEqual(x_tf_4, np.asarray(x_np)[:, ::-1])\n            self.assertAllEqual(x_tf_5, np.asarray(x_np)[::-1, ::-1])\n\n  # This test covers the axis validation in the shape function\n  # (no eval())\n  def testInvalidAxis(self):\n    x_np = np.array([[1, 2, 3], [4, 5, 6]], dtype=np.float32)\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"is out of.* range\"):\n      array_ops.reverse_v2(x_np, [-30])\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"is out of.* range\"):\n      array_ops.reverse_v2(x_np, [2])\n    with self.assertRaisesRegex(\n        (ValueError, errors.InvalidArgumentError),\n        r\"axis 0 specified more than once|axis 0 was repeated\"):\n      array_ops.reverse_v2(x_np, [0, -2])\n\n  # This is the version of reverse that uses axis indices rather than\n  # bool tensors\n  # TODO(b/32254538): Change this test to use array_ops.reverse\n  #\n  # Note: this test passes placeholder as constant axis is validated\n  # in shape function (see testInvalidAxis)\n  def testInvalid(self):\n    x_np = np.array([[1, 2, 3], [4, 5, 6]], dtype=np.float32)\n\n    @def_function.function\n    def func(ax):\n      return array_ops.reverse_v2(x_np, ax)\n\n    with self.assertRaisesRegex((ValueError, errors_impl.InvalidArgumentError),\n                                \"is out of.*range\"):\n      func([-30])\n    with self.assertRaisesRegex((ValueError, errors_impl.InvalidArgumentError),\n                                \"is out of.*range\"):\n      func([2])\n    with self.assertRaisesRegex(\n        (ValueError, errors_impl.InvalidArgumentError),\n        \"(axis 0 specified more than once|canonicalized axis 0 was repeated.)\"):\n      func([0, -2])\n\n  def testReverse1DimAuto(self):\n    for dtype in [\n        np.uint8, np.int8, np.uint16, np.int16, np.uint32, np.int32, np.uint64,\n        np.int64, np.bool_, np.float16, np.float32, np.float64, np.complex64,\n        np.complex128,\n        np.array(b\"\").dtype.type\n    ]:\n      self._reverse1DimAuto(dtype)\n\n  def testReverse2DimAuto(self):\n    for dtype in [\n        np.uint8, np.int8, np.uint16, np.int16, np.uint32, np.int32, np.uint64,\n        np.int64, np.bool_, np.float16, np.float32, np.float64, np.complex64,\n        np.complex128,\n        np.array(b\"\").dtype.type\n    ]:\n      self._reverse2DimAuto(dtype)\n\n  def testReverseRowsOf3Channels(self):\n    \"\"\"Tests optimized code for reversing rows with last dim size = 3.\"\"\"\n    for reverse_f in [array_ops.reverse_v2, array_ops.reverse]:\n      for outer_size in (1, 2):\n        for middle_size in list(range(50)) + [100000]:\n          with self.subTest(\n              reverse_f=reverse_f,\n              outer_size=outer_size,\n              middle_size=middle_size,\n              use_gpu=True):\n            x_np = np.reshape(\n                np.arange(outer_size * middle_size * 3, dtype=np.float32),\n                newshape=(outer_size, middle_size, 3))\n            x_tf = self.evaluate(reverse_f(x_np, [1]))\n            np_answer = x_np[:, ::-1, :]\n            self.assertAllEqual(x_tf, np_answer)\n\n  def testReverseRowsOf4Channels(self):\n    for reverse_f in [array_ops.reverse_v2, array_ops.reverse]:\n      for outer_size in (1, 2):\n        for middle_size in list(range(50)) + [100000]:\n          with self.subTest(\n              reverse_f=reverse_f,\n              outer_size=outer_size,\n              middle_size=middle_size,\n              use_gpu=True):\n            x_np = np.reshape(\n                np.arange(outer_size * middle_size * 4, dtype=np.float32),\n                newshape=(outer_size, middle_size, 4))\n            x_tf = self.evaluate(reverse_f(x_np, [1]))\n            np_answer = x_np[:, ::-1, :]\n            self.assertAllEqual(x_tf, np_answer)\n\n  def testReverseColumnsOf3Channels(self):\n    for reverse_f in [array_ops.reverse_v2, array_ops.reverse]:\n      for outer_size in list(range(50)) + [100000]:\n        for middle_size in (1, 2):\n          with self.subTest(\n              reverse_f=reverse_f,\n              outer_size=outer_size,\n              middle_size=middle_size,\n              use_gpu=True):\n            x_np = np.reshape(\n                np.arange(outer_size * middle_size * 3, dtype=np.float32),\n                newshape=(outer_size, middle_size, 3))\n            x_tf = self.evaluate(reverse_f(x_np, [0]))\n            np_answer = x_np[::-1, :, :]\n            self.assertAllEqual(x_tf, np_answer)\n\n  def testReverseInvalidShape(self):\n    x = np.ndarray(shape=[0, 1, 1])\n    v = array_ops.reverse_v2(x, axis=[1])\n    self.assertAllEqual(self.evaluate(v), v)\n\n\nclass MeshgridTest(test_util.TensorFlowTestCase):\n\n  def _compareDiff(self, x, y, use_gpu):\n    for index in (\"ij\", \"xy\"):\n      numpy_out = np.meshgrid(x, y, indexing=index)\n      tf_out = array_ops.meshgrid(x, y, indexing=index)\n      with self.cached_session(use_gpu=use_gpu):\n        for xx, yy in zip(numpy_out, tf_out):\n          self.assertAllEqual(xx, yy)\n\n  def _compareDiffType(self, n, np_dtype, use_gpu):\n    inputs = []\n    for index in (\"ij\", \"xy\"):\n      for _ in range(n):\n        x = np.linspace(-10, 10, 5).astype(np_dtype)\n        if np_dtype in (np.complex64, np.complex128):\n          x += 1j\n        inputs.append(x)\n      numpy_out = np.meshgrid(*inputs, indexing=index)\n      with test_util.device(use_gpu=use_gpu):\n        tf_out = array_ops.meshgrid(*inputs, indexing=index)\n        for x_np, x_tf in zip(numpy_out, tf_out):\n          self.assertAllEqual(x_np, x_tf)\n\n  def testCompare(self):\n    for t in (np.float16, np.float32, np.float64, np.int32, np.int64,\n              np.complex64, np.complex128):\n      with self.subTest(t=t):\n        self._compareDiffType(2, t, False)\n        self._compareDiffType(3, t, False)\n\n        x = [1, 2, 3]\n        y = [4, 5]\n\n        a = [[1, 1], [1, 1]]\n\n        self._compareDiff(x, y, False)\n        self._compareDiff(x, a, False)\n\n\nclass StridedSliceChecker(object):\n  \"\"\"Check a given tensor against the numpy result.\"\"\"\n\n  REF_TENSOR = np.arange(1, 19, dtype=np.float32).reshape(3, 2, 3)\n  REF_TENSOR_ALIGNED = np.arange(1, 97, dtype=np.float32).reshape(3, 4, 8)\n\n  def __init__(self, test, x, tensor_type=dtypes.int32, check_type_infer=True):\n    self.x_np = np.array(x).astype(tensor_type.as_numpy_dtype)\n    if tensor_type.is_bool:\n      self.x_np = np.array(x % 3).astype(np.bool_)\n    # Give the value a non-zero imaginary component for complex types.\n    if tensor_type.is_complex:\n      self.x_np -= 1j * self.x_np\n    self.test = test\n    self.x = constant_op.constant(self.x_np, dtype=tensor_type)\n    self.check_type_infer = check_type_infer\n\n  def __getitem__(self, spec):\n    op = self.x.__getitem__(spec)\n\n    def eval_if_tensor(x):\n      try:\n        return self.test.evaluate(x)\n      except (AttributeError, TypeError, ValueError):\n        return x\n\n    def casts_to_bool_nparray(x):\n      try:\n        return np.asarray(x).dtype == bool\n      except NotImplementedError:\n        return False\n\n    if isinstance(spec, bool) or \\\n      (isinstance(spec, ops.Tensor) and spec.dtype == dtypes.bool) or \\\n      (isinstance(spec, np.ndarray) and spec.dtype == bool) or \\\n      (isinstance(spec, (list, tuple)) and casts_to_bool_nparray(spec)):\n      tensor = self.test.evaluate(op)\n      np_spec = eval_if_tensor(spec)\n      self.test.assertAllEqual(self.x_np[np_spec], tensor)\n      return tensor\n\n    if not isinstance(spec, (list, tuple)):\n      spec = [spec]\n\n    tensor = self.test.evaluate(op)\n\n    # Make a numpy spec that pre-evals the tensors\n    np_specs = []\n\n    for s in spec:\n      if isinstance(s, slice):\n        start = eval_if_tensor(s.start)\n        stop = eval_if_tensor(s.stop)\n        step = eval_if_tensor(s.step)\n        np_specs.append(slice(start, stop, step))\n      else:\n        np_specs.append(eval_if_tensor(s))\n\n    self.test.assertAllEqual(self.x_np[tuple(np_specs)], tensor)\n    if self.check_type_infer:\n      self.test.assertAllEqual(tensor.shape, op.get_shape())\n    return tensor\n\n\nSTRIDED_SLICE_TYPES = [\n    dtypes.int32, dtypes.int64, dtypes.int16, dtypes.int8, dtypes.uint8,\n    dtypes.float32, dtypes.float64, dtypes.complex64, dtypes.complex128,\n    dtypes.bool\n]\n\n\nclass StridedSliceTest(test_util.TensorFlowTestCase):\n  \"\"\"Test the strided slice operation with variants of slices.\"\"\"\n\n  def test_basic_slice(self):\n    for tensor_type in STRIDED_SLICE_TYPES:\n      with self.subTest(tensor_type=tensor_type, use_gpu=True):\n        checker = StridedSliceChecker(\n            self, StridedSliceChecker.REF_TENSOR, tensor_type=tensor_type)\n        _ = checker[:, :, :]\n        # Various ways of representing identity slice\n        _ = checker[:, :, :]\n        _ = checker[::, ::, ::]\n        _ = checker[::1, ::1, ::1]\n        # Not zero slice\n        _ = checker[::1, ::5, ::2]\n        # Reverse in each dimension independently\n        _ = checker[::-1, :, :]\n        _ = checker[:, ::-1, :]\n        _ = checker[:, :, ::-1]\n        ## negative index tests i.e. n-2 in first component\n        _ = checker[-2::-1, :, ::1]\n        # negative index tests i.e. n-2 in first component, non-unit stride\n        _ = checker[-2::-1, :, ::2]\n\n        # Check rank-0 examples\n        checker2 = StridedSliceChecker(self, 5, tensor_type=tensor_type)\n        _ = checker2[None]\n        _ = checker2[...]\n        _ = checker2[tuple()]\n\n  def testInt64GPU(self):\n    if not test_util.is_gpu_available():\n      self.skipTest(\"No GPU available\")\n\n    with test_util.force_gpu():\n      x = constant_op.constant([1., 2., 3.])\n      begin = constant_op.constant([2], dtype=dtypes.int64)\n      end = constant_op.constant([3], dtype=dtypes.int64)\n      strides = constant_op.constant([1], dtype=dtypes.int64)\n      s = array_ops.strided_slice(x, begin, end, strides)\n      self.assertAllEqual([3.], self.evaluate(s))\n\n  @test_util.assert_no_new_pyobjects_executing_eagerly\n  @test_util.assert_no_garbage_created\n  def testTensorSliceEagerMemory(self):\n    with context.eager_mode():\n      inputs = constant_op.constant([[[1], [2], [3], [4]]],\n                                    dtype=dtypes.float32)\n      # Tests that slicing an EagerTensor doesn't leak memory\n      inputs[0]  # pylint: disable=pointless-statement\n\n  @test_util.assert_no_new_pyobjects_executing_eagerly\n  @test_util.assert_no_garbage_created\n  def testVariableSliceEagerMemory(self):\n    with context.eager_mode():\n      v = variables.Variable([1., 2.])\n      v[0]  # pylint: disable=pointless-statement\n\n  def testDegenerateSlices(self):\n    with test_util.device(use_gpu=True):\n      checker = StridedSliceChecker(self, StridedSliceChecker.REF_TENSOR)\n      # degenerate by offering a forward interval with a negative stride\n      _ = checker[0:-1:-1, :, :]\n      # degenerate with a reverse interval with a positive stride\n      _ = checker[-1:0, :, :]\n      # empty interval in every dimension\n      _ = checker[-1:0, 2:2, 2:3:-1]\n      # empty first dimension only (used to break for aligned tensors).\n      checker = StridedSliceChecker(self,\n                                    StridedSliceChecker.REF_TENSOR_ALIGNED)\n      _ = checker[1:0]\n\n  def testSliceWithUndefinedDimension(self):\n    t = constant_op.constant([1, 2, 3])\n    d = tensor_shape.Dimension(None)\n    self.assertAllEqual(t[d:d:d], t)\n\n  def testEllipsis(self):\n    with test_util.device(use_gpu=True):\n      raw = [[[[[1, 2], [3, 4], [5, 6]]], [[[7, 8], [9, 10], [11, 12]]]]]\n      checker = StridedSliceChecker(self, raw)\n\n      _ = checker[0:]\n      # implicit ellipsis\n      _ = checker[0:, ...]\n      # ellipsis alone\n      _ = checker[...]\n      # ellipsis at end\n      _ = checker[0:1, ...]\n      # ellipsis at begin\n      _ = checker[..., 0:1]\n      # ellipsis at middle\n      _ = checker[0:1, ..., 0:1]\n      # multiple ellipses not allowed\n      with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                  \"Multiple ellipses\"):\n        _ = checker[..., :, ...].eval()\n\n  def testShrink(self):\n    with test_util.device(use_gpu=True):\n      raw = [[[[[1, 2, 4, 5], [5, 6, 7, 8], [9, 10, 11, 12]]],\n              [[[13, 14, 15, 16], [17, 18, 19, 20], [21, 22, 23, 24]]]]]\n      checker = StridedSliceChecker(self, raw)\n      _ = checker[:, :, :, :, 3]\n      _ = checker[..., 3]\n      _ = checker[:, 0]\n      _ = checker[:, :, 0]\n\n  def testBothNewAxisAndShrink(self):\n    with test_util.device(use_gpu=True):\n\n      @def_function.function\n      def func(inp):\n        return inp[array_ops.newaxis, :, 0]\n\n      f = func.get_concrete_function(\n          tensor_spec.TensorSpec([2, 2], dtypes.int16))\n\n      # TODO(b/190416665): Allow the constant to be eagerly copied/created on\n      # the GPU.\n      with ops.device(\"CPU\"):\n        ones = constant_op.constant([[1, 1], [1, 1]], dtypes.int16)\n      self.assertAllEqual([[1, 1]], self.evaluate(f(ones)))\n\n  def testTensorIndexing(self):\n    with test_util.device(use_gpu=True):\n      raw = [[[[[1, 2, 4, 5], [5, 6, 7, 8], [9, 10, 11, 12]]],\n              [[[13, 14, 15, 16], [17, 18, 19, 20], [21, 22, 23, 24]]]]]\n      checker = StridedSliceChecker(self, raw, check_type_infer=False)\n      bar = constant_op.constant(2)\n      bar2 = constant_op.constant(3)\n      _ = checker[..., bar:bar2]\n      _ = checker[..., bar]\n      _ = checker[..., 3]\n      _ = checker[..., 2**64 // 2**63]  # Test longs in Python 2\n\n  def testTensorIndexingTypeError(self):\n    with self.session():\n      checker = StridedSliceChecker(self, StridedSliceChecker.REF_TENSOR)\n      expected = re.escape(array_ops._SLICE_TYPE_ERROR)\n      with self.assertRaisesRegex(TypeError, expected):\n        _ = checker[\"foo\"]\n      with self.assertRaisesRegex(TypeError, expected):\n        _ = checker[constant_op.constant(\"foo\")]\n      with self.assertRaisesRegex(TypeError, expected):\n        _ = checker[0.0]\n      with self.assertRaisesRegex(TypeError, expected):\n        _ = checker[constant_op.constant(0.0)]\n      with self.assertRaisesRegex(TypeError, expected):\n        _ = checker[constant_op.constant([1, 2, 3])]\n      with self.assertRaisesRegex(TypeError, expected):\n        _ = checker[[2.1, -0.7, 1.5]]\n\n  def testExpand(self):\n    with test_util.device(use_gpu=True):\n      raw = [[[[[1, 2, 4, 5], [5, 6, 7, 8], [9, 10, 11, 12]]],\n              [[[13, 14, 15, 16], [17, 18, 19, 20], [21, 22, 23, 24]]]]]\n      checker = StridedSliceChecker(self, raw)\n      # new axis (followed by implicit ellipsis)\n      _ = checker[np.newaxis]\n      # newaxis after ellipsis\n      _ = checker[..., np.newaxis]\n      # newaxis in between ellipsis and explicit range\n      _ = checker[..., np.newaxis, :]\n      _ = checker[:, ..., np.newaxis, :, :]\n      # Reverse final dimension with new axis\n      _ = checker[:, :, np.newaxis, :, 2::-1]\n      # Ellipsis in middle of two newaxis\n      _ = checker[np.newaxis, ..., np.newaxis]\n\n  def testExpandVariable(self):\n    with test_util.device(use_gpu=True):\n      x = variables.Variable(7, dtype=dtypes.int32)\n      self.evaluate(x.initializer)\n      y = self.evaluate(x[None])\n      self.assertEqual(y.shape, (1,))\n      self.assertAllEqual(y, (7,))\n\n  def testOptimizedCases(self):\n    with test_util.device(use_gpu=True):\n      checker = StridedSliceChecker(self,\n                                    StridedSliceChecker.REF_TENSOR_ALIGNED)\n      # Identity\n      _ = checker[:]\n      # Identity\n      _ = checker[...]\n      # Identity\n      _ = checker[np.newaxis, ..., np.newaxis]\n      # First axis slice\n      _ = checker[1:]\n      # First axis slice\n      _ = checker[np.newaxis, 1:]\n\n  def testMasks(self):\n    with test_util.device(use_gpu=True):\n      scalar = np.array(0)\n      # Test tensor type mask\n      checker = StridedSliceChecker(self, StridedSliceChecker.REF_TENSOR)\n      _ = checker[checker.x > 2]\n      _ = checker[checker.x <= 5]\n      _ = checker[ops.convert_to_tensor(scalar)]\n\n      # Test numpy array type mask\n      raw = np.array([[[[[1, 2, 4, 5], [5, 6, 7, 8], [9, 10, 11, 12]]],\n                       [[[13, 14, 15, 16], [17, 18, 19, 20], [21, 22, 23,\n                                                              24]]]]])\n      checker1 = StridedSliceChecker(self, raw)\n      _ = checker1[raw >= 4]\n      _ = checker1[raw < 19]\n      _ = checker1[scalar]\n\n      # Test boolean and non boolean cases\n      mask = np.array([True, False, True])\n      raw1 = np.array([[1, 2, 4, 5], [5, 6, 7, 8], [9, 10, 11, 12]])\n      checker2 = StridedSliceChecker(self, raw1)\n      _ = checker2[mask]\n      _ = checker2[ops.convert_to_tensor(mask)]\n\n  def test_int16_indices(self):\n\n    def _int16(i):\n      return constant_op.constant(i, dtype=dtypes.int16)\n\n    def _int64(i):\n      return constant_op.constant(i, dtype=dtypes.int64)\n\n    for tensor_type in STRIDED_SLICE_TYPES:\n      with self.subTest(tensor_type=tensor_type, use_gpu=True):\n        checker = StridedSliceChecker(\n            self, StridedSliceChecker.REF_TENSOR, tensor_type=tensor_type)\n\n        _ = checker[_int16(1)]\n\n        with self.assertRaises(Exception):\n          _ = checker[_int16(1)::1, :, 1:_int64(3):2]\n        with self.assertRaises(Exception):\n          _ = checker[:, _int16(1):_int16(5):-1, :]\n        with self.assertRaises(Exception):\n          _ = checker[::_int64(1), _int64(1):10:_int16(3), ::_int64(2)]\n\n        _ = checker[::_int16(1), _int16(1)::_int16(5), ::2]\n        _ = checker[_int16(1):_int16(5):_int16(2), 1:2, :]\n\n\nclass StridedSliceShapeTest(test_util.TensorFlowTestCase):\n  \"\"\"Test the shape inference of StridedSliceShapes.\"\"\"\n\n  def testUnknown(self):\n    with test_util.device(use_gpu=True):\n\n      @def_function.function\n      def f(x):\n        y = x[...]\n        self.assertAllEqual(y.get_shape().ndims, None)\n\n      _ = f.get_concrete_function(tensor_spec.TensorSpec(None, dtypes.float32))\n\n  def tensorShapeEqual(self, x, y):\n    self.assertTrue(x is not None and y is not None or x is None and y is None)\n    self.assertEqual(x.as_list(), y.as_list())\n\n  def testTensorShapeUncertain(self):\n    with test_util.device(use_gpu=True):\n\n      @def_function.function\n      def f1(x):\n        y = x[3:5]\n        self.tensorShapeEqual(y.get_shape(),\n                              tensor_shape.TensorShape([2, None, 7]))\n\n      _ = f1.get_concrete_function(\n          tensor_spec.TensorSpec((5, None, 7), dtypes.float32))\n\n      @def_function.function\n      def f2(x):\n        y = x[3:5, :, 4]\n        self.tensorShapeEqual(y.get_shape(), tensor_shape.TensorShape([2,\n                                                                       None]))\n\n      _ = f2.get_concrete_function(\n          tensor_spec.TensorSpec((5, None, 7), dtypes.float32))\n\n      @def_function.function\n      def f3(x):\n        y = x[3:5, 3:4, 4]\n        self.tensorShapeEqual(y.get_shape(), tensor_shape.TensorShape([2,\n                                                                       None]))\n\n      _ = f3.get_concrete_function(\n          tensor_spec.TensorSpec((5, None, 7), dtypes.float32))\n\n      @def_function.function\n      def f4(x):\n        y = x[3:5, :, 5:10]\n        self.tensorShapeEqual(y.get_shape(),\n                              tensor_shape.TensorShape([2, None, 2]))\n\n      _ = f4.get_concrete_function(\n          tensor_spec.TensorSpec((5, None, 7), dtypes.float32))\n\n      @def_function.function\n      def f5(x):\n        y = x[3:5, :, 50:3]\n        self.tensorShapeEqual(y.get_shape(),\n                              tensor_shape.TensorShape([2, None, 0]))\n\n      _ = f5.get_concrete_function(\n          tensor_spec.TensorSpec((5, None, 7), dtypes.float32))\n\n      @def_function.function\n      def f6(x):\n        y = x[3:5, :, array_ops.newaxis, 50:3,]\n        self.tensorShapeEqual(y.get_shape(),\n                              tensor_shape.TensorShape([2, None, 1, 0]))\n\n      _ = f6.get_concrete_function(\n          tensor_spec.TensorSpec((5, None, 7), dtypes.float32))\n\n      @def_function.function\n      def f7(x):\n        y = x[1:5:2, :, array_ops.newaxis, 50:3,]\n        self.tensorShapeEqual(y.get_shape(),\n                              tensor_shape.TensorShape([2, None, 1, 0]))\n\n      _ = f7.get_concrete_function(\n          tensor_spec.TensorSpec((5, None, 7), dtypes.float32))\n\n      @def_function.function\n      def f8(x):\n        y = x[:5:3, :, array_ops.newaxis, 50:3,]\n        self.tensorShapeEqual(y.get_shape(),\n                              tensor_shape.TensorShape([2, None, 1, 0]))\n\n      _ = f8.get_concrete_function(\n          tensor_spec.TensorSpec((5, None, 7), dtypes.float32))\n\n      @def_function.function\n      def f9(x):\n        y = x[:2:3, :, array_ops.newaxis, 50:3,]\n        self.tensorShapeEqual(y.get_shape(),\n                              tensor_shape.TensorShape([1, None, 1, 0]))\n\n      _ = f9.get_concrete_function(\n          tensor_spec.TensorSpec((5, None, 7), dtypes.float32))\n\n      @def_function.function\n      def f10(x):\n        y = x[::-1, :, array_ops.newaxis, ::-2]\n        self.tensorShapeEqual(y.get_shape(),\n                              tensor_shape.TensorShape([5, None, 1, 4]))\n\n      _ = f10.get_concrete_function(\n          tensor_spec.TensorSpec((5, None, 7), dtypes.float32))\n\n  def testTensorValuedIndexShape(self):\n    with self.session():\n\n      @def_function.function\n      def f1(x, y):\n        z = x[y]\n        self.tensorShapeEqual(z.get_shape(), tensor_shape.TensorShape([3, 7]))\n\n      _ = f1.get_concrete_function(\n          tensor_spec.TensorSpec((5, 3, 7)),\n          tensor_spec.TensorSpec((), dtypes.int32))\n\n      @def_function.function\n      def f2(x, y):\n        z = x[y, ::-1]\n        self.tensorShapeEqual(z.get_shape(), tensor_shape.TensorShape([3, 7]))\n\n      _ = f2.get_concrete_function(\n          tensor_spec.TensorSpec((5, 3, 7)),\n          tensor_spec.TensorSpec((), dtypes.int32))\n\n      @def_function.function\n      def f3(x, y):\n        z = x[y, ::-2]\n        self.tensorShapeEqual(z.get_shape(), tensor_shape.TensorShape([2, 7]))\n\n      _ = f3.get_concrete_function(\n          tensor_spec.TensorSpec((5, 3, 7)),\n          tensor_spec.TensorSpec((), dtypes.int32))\n\n      @def_function.function\n      def f4(x, y, s):\n        z = x[y, s:2]\n        self.tensorShapeEqual(z.get_shape(), tensor_shape.TensorShape([None,\n                                                                       7]))\n\n      _ = f4.get_concrete_function(\n          tensor_spec.TensorSpec((5, 3, 7)),\n          tensor_spec.TensorSpec((), dtypes.int32),\n          tensor_spec.TensorSpec((), dtypes.int32))\n\n\nclass GradSliceChecker(object):\n  \"\"\"Tests that we can compute a gradient for var^2.\"\"\"\n\n  def __init__(self, test, var, varnp, use_tape):\n    self.test = test\n    self.var = var\n    self.varnp = varnp\n    self.use_tape = use_tape\n\n  def __getitem__(self, spec):\n    with test_util.AbstractGradientTape(\n        use_tape=self.use_tape, persistent=True) as tape:\n      tape.watch(self.var)\n      val = self.var * self.var\n      slice_var = self.var[spec]\n      slice_val = val[spec]\n\n      # compute analytic 2nd derivative\n      analytic_grad2 = 2 * slice_val\n\n      dy = variables.Variable(\n          array_ops.ones_like(slice_var, dtype=dtypes.float32))\n      assign = dy.assign(slice_var)\n\n      slice_val_grad = tape.gradient(slice_val, self.var, [dy])\n      slice_val_grad2 = tape.gradient(slice_val_grad, dy, [self.var])\n    self.test.evaluate(assign)\n    slice_val_grad_evaled, slice_val_grad2_evaled = (\n        self.test.evaluate([slice_val_grad, slice_val_grad2]))\n    analytic_grad2_evaled = self.test.evaluate(analytic_grad2)\n    self.test.assertAllEqual(slice_val_grad2_evaled, analytic_grad2_evaled)\n\n    # compute analytic gradient for slice\n    np_val_grad = (2 * self.varnp * self.varnp)\n    np_sliceval_grad = np.zeros(self.var.get_shape())\n    if isinstance(spec, ops.Tensor):\n      spec = self.test.evaluate([spec])\n    np_sliceval_grad[spec] = np_val_grad[spec]\n    # verify gradient\n    self.test.assertAllEqual(slice_val_grad_evaled, np_sliceval_grad)\n\n\nclass StridedSliceGradTest(test_util.TensorFlowTestCase,\n                           parameterized.TestCase):\n  \"\"\"Test that strided slice's custom gradient produces correct gradients.\"\"\"\n\n  @parameterized.parameters(set((True, context.executing_eagerly())))\n  @test_util.disable_xla(\n      \"b/210077724: Auto-clustering with where op isn't supported. Has loose \"\n      \"output shape bounds\")\n  def testGradient(self, use_tape):\n    with test_util.device(use_gpu=True):\n      var = variables.Variable(\n          array_ops.reshape(\n              math_ops.range(1, 97, 1, dtype=dtypes.float32), shape=(6, 4, 4)))\n      self.evaluate(var.initializer)\n\n      raw = np.array(range(1, 97, 1)).reshape((6, 4, 4))\n      grad = GradSliceChecker(self, var, raw, use_tape)\n      _ = grad[2:6:2, 1:3, 1:3]\n      _ = grad[3:0:-2, 1:3, 1:3]\n      _ = grad[3:0:-2, array_ops.newaxis, 1:3, 2, array_ops.newaxis]\n      _ = grad[3:0:-2, 1:3, 2]\n      _ = grad[:, -1, :]\n      _ = grad[:, -2, :]\n      with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                  \"out of bounds\"):\n        _ = grad[:, -200, :]\n      with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                  \"out of bounds\"):\n        _ = grad[:, 200, :]\n\n      # Test numpy array type mask\n      _ = grad[raw > 51]\n      # Test tensor type mask\n      _ = grad[ops.convert_to_tensor(raw) <= 76]\n\n  @parameterized.parameters(set((True, context.executing_eagerly())))\n  def testGradientZero(self, use_tape):\n    with test_util.device(use_gpu=True):\n      var = variables.Variable(8.)\n      self.evaluate(var.initializer)\n      grad = GradSliceChecker(self, var, np.array(8), use_tape)\n      _ = grad[tuple()]\n\n  @parameterized.parameters(set((True, context.executing_eagerly())))\n  def testInt64Indices(self, use_tape):\n    with test_util.AbstractGradientTape(use_tape=use_tape) as tape:\n      a = math_ops.range(3, dtype=dtypes.float32)\n      tape.watch(a)\n      index = constant_op.constant(1, dtype=dtypes.int64)\n      b = 2. * a[index]\n    grad = tape.gradient(b, a)\n    self.assertAllEqual(self.evaluate(grad), [0., 2., 0.])\n\n\nclass StridedSliceGradTypeTest(test_util.TensorFlowTestCase):\n  \"\"\"Test varied index types and host located memory.\"\"\"\n\n  def testHostVsDevice(self):\n    var2 = variables.Variable(\n        array_ops.reshape(\n            math_ops.cast(math_ops.range(1, 5, 1), dtypes.float32),\n            shape=(4, 1, 1)))\n    varshape = variables.Variable([6, 4, 4], dtype=dtypes.int32)\n    begin = constant_op.constant([0, 0, 0])\n    end = constant_op.constant([4, 1, 1])\n    strides = constant_op.constant([1, 1, 1])\n    foo = array_ops.strided_slice_grad(varshape, begin, end, strides, var2)\n    self.evaluate(var2.initializer)\n    self.evaluate(varshape.initializer)\n    self.evaluate(foo)\n\n  def testInt64Shape(self):\n    original_dy = array_ops.reshape(\n        math_ops.cast(math_ops.range(1, 5, 1), dtypes.float32), shape=(4, 1, 1))\n    original_shape = constant_op.constant([6, 4, 4], dtype=dtypes.int64)\n    begin = constant_op.constant([0, 0, 0], dtype=dtypes.int64)\n    end = constant_op.constant([4, 1, 1], dtype=dtypes.int64)\n    strides = constant_op.constant([1, 1, 1], dtype=dtypes.int64)\n    dx = array_ops.strided_slice_grad(original_shape, begin, end, strides,\n                                      original_dy)\n    self.evaluate(dx)\n\n  def testMixedIndexTypes(self):\n    original_dy = array_ops.reshape(\n        math_ops.cast(math_ops.range(1, 5, 1), dtypes.float32), shape=(4, 1, 1))\n    original_shape = constant_op.constant([6, 4, 4], dtype=dtypes.int64)\n    begin = constant_op.constant([0, 0, 0], dtype=dtypes.int32)\n    end = constant_op.constant([4, 1, 1], dtype=dtypes.int64)\n    strides = constant_op.constant([1, 1, 1], dtype=dtypes.int64)\n    with self.assertRaises((TypeError, errors_impl.InvalidArgumentError)):\n      dx = array_ops.strided_slice_grad(original_shape, begin, end, strides,\n                                        original_dy)\n      self.evaluate(dx)\n\n\nclass BenchmarkSlice(object):\n\n  def __init__(self, tensor):\n    self.tensor = tensor\n\n  def __getitem__(self, x):\n    return self.tensor[x]\n\n\nclass StridedSliceBenchmark(test_lib.Benchmark):\n  \"\"\"Benchmark new strided slice operation on non-trivial case.\"\"\"\n\n  def run_and_time(self, slice_op):\n    self.evaluate(variables.global_variables_initializer())\n    for _ in range(10):\n      _ = self.evaluate(slice_op)\n    iters = 1000\n    t0 = time.time()\n    for _ in range(iters):\n      self.evaluate(slice_op)\n    t1 = time.time()\n    self.report_benchmark(iters=iters, wall_time=(t1 - t0) / 1000.0)\n\n  def make_variable(self):\n    n = 256\n    shape = (n, n, n)\n    items = n**3\n    var = variables.Variable(\n        array_ops.reshape(math_ops.linspace(1., float(items), items), shape),\n        dtype=dtypes.float32)\n    return var\n\n  def benchmark_strided_slice_skip(self):\n    with session.Session():\n      var = self.make_variable()\n      helper = BenchmarkSlice(var)\n      slice_op = helper[::2, ::1, ::2]\n      self.run_and_time(slice_op)\n\n  def benchmark_strided_slice_easy(self):\n    with session.Session():\n      var = self.make_variable()\n      helper = BenchmarkSlice(var)\n      slice_op = helper[3::1, 3::1, 3::1]\n      self.run_and_time(slice_op)\n\n  def benchmark_slice_easy(self):\n    with session.Session():\n      var = self.make_variable()\n      slice_op = var[3::1, 3::1, 3::1]\n      self.run_and_time(slice_op)\n\n\nclass StridedSliceAssignChecker(object):\n\n  def __init__(self, test, x, tensor_type=dtypes.float32, use_resource=False):\n    self.tensor_type = tensor_type\n    self.test = test\n    self._use_resource = use_resource\n\n    self.x_np = np.array(x).astype(tensor_type.as_numpy_dtype)\n    # Give the value a non-zero imaginary component for complex types.\n    if tensor_type.is_complex:\n      self.x_np -= 1j * self.x_np\n    self.x = constant_op.constant(self.x_np, dtype=tensor_type)\n\n  def __setitem__(self, index, value):\n    value = np.array(value).astype(self.tensor_type.as_numpy_dtype)\n    # Give the value a non-zero imaginary component for complex types.\n    if self.tensor_type.is_complex:\n      value -= 1j * value\n\n    with test_util.device(use_gpu=True):\n      if self._use_resource:\n        var = resource_variable_ops.ResourceVariable(self.x)\n      else:\n        var = variables.Variable(self.x)\n      self.test.evaluate(var.initializer)\n      val = self.test.evaluate(var[index].assign(value))\n      # val_copy is used to check that tf.compat.v1.assign works equivalently\n      # to the assign method above.\n      val_copy = self.test.evaluate(state_ops.assign(var[index], value))\n      valnp = np.copy(self.x_np)\n      valnp[index] = np.array(value)\n      self.test.assertAllEqual(val, valnp)\n      self.test.assertAllEqual(val_copy, valnp)\n\n\nclass SliceAssignTest(test_util.TensorFlowTestCase, parameterized.TestCase):\n\n  def testInvalidSlice(self):\n    foo = constant_op.constant([1, 2, 3])\n    with self.assertRaisesRegex(AttributeError, \"no attribute 'assign'\"):\n      bar = foo[:2].assign(constant_op.constant([1, 2]))\n      self.evaluate(bar)\n\n  def doTestSliceAssign(self, use_resource):\n    for dtype in STRIDED_SLICE_TYPES:\n      with self.subTest(dtype=dtype):\n        checker = StridedSliceAssignChecker(\n            self, [[1, 2, 3], [4, 5, 6]],\n            use_resource=use_resource,\n            tensor_type=dtype)\n        # Check if equal\n        checker[:] = [[10, 20, 30], [40, 50, 60]]\n        # Check trivial (1,1) shape tensor\n        checker[1:2, 1:2] = [[66]]\n        # shrinks shape changes\n        checker[1:2, 1] = [66]\n        checker[1, 1:2] = [66]\n        checker[1, 1] = 66\n        # newaxis shape changes\n        checker[:, None, :] = [[[10, 20, 30]], [[40, 50, 50]]]\n        # shrink and newaxis\n        checker[None, None, 0, 0:1] = [[[99]]]\n        # Non unit strides\n        checker[::1, ::-2] = [[3, 33], [4, 44]]\n        # degenerate interval\n        checker[8:10, 0] = []\n        checker[8:10, 8:10] = [[]]\n    # Assign vector to scalar (rank-0) using newaxis\n    checker2 = StridedSliceAssignChecker(self, 222)\n    checker2[()] = 6  # no indices\n    checker2[...] = 6  # ellipsis\n    checker2[None] = [6]  # new axis\n\n  @test_util.disable_xla(\"b/123559667\")\n  def testSliceAssign(self):\n    self.doTestSliceAssign(use_resource=False)\n\n  @test_util.disable_xla(\"b/123559667\")\n  def testSliceAssignResource(self):\n    self.doTestSliceAssign(use_resource=True)\n\n  def testTypeError(self):\n    init_val = constant_op.constant([1, 2], dtype=dtypes.int32)\n    too_small_val = constant_op.constant([3, 4], dtype=dtypes.int8)\n    too_large_val = constant_op.constant([3, 4], dtype=dtypes.int64)\n    v = variables.VariableV1(init_val)\n    with self.assertRaises((ValueError, TypeError)):\n      self.evaluate(v[:].assign(too_small_val))\n    with self.assertRaises((ValueError, TypeError)):\n      self.evaluate(v[:].assign(too_large_val))\n\n  def testTypeErrorResource(self):\n    init_val = constant_op.constant([1, 2], dtype=dtypes.int32)\n    too_small_val = constant_op.constant([3, 4], dtype=dtypes.int8)\n    too_large_val = constant_op.constant([3, 4], dtype=dtypes.int64)\n    v = resource_variable_ops.ResourceVariable(init_val)\n    self.evaluate(v.initializer)\n    with self.assertRaises(ValueError):\n      self.evaluate(v[:].assign(too_large_val))\n    with self.assertRaises(ValueError):\n      self.evaluate(v[:].assign(too_small_val))\n\n  @test_util.disable_xla(\"b/123559667\")\n  @test_util.run_in_graph_and_eager_modes\n  def testTensorStridedSliceUpdateWithInputForward(self):\n    \"\"\"Tests tensor_strided_slice_update with input-forwarding taking effect.\"\"\"\n    @def_function.function\n    def assign(x):\n      y = x + 1\n      return gen_array_ops.tensor_strided_slice_update(y, [0], [1], [1], [0])\n    self.assertAllEqual([0, 1], self.evaluate(assign(array_ops.zeros([2]))))\n\n  @test_util.disable_xla(\"b/123559667\")\n  @test_util.run_in_graph_and_eager_modes\n  def testTensorStridedSliceUpdateNoInputForward(self):\n    \"\"\"Tests tensor_strided_slice_update with no input-forwarding.\"\"\"\n    x = constant_op.constant([0.2, 0.3])\n    y = x + 1\n    # y's buffer won't be forwarded to z because y and z will be alive at the\n    # same time later.\n    z = gen_array_ops.tensor_strided_slice_update(y, [0], [1], [1], [0.4])\n    ans = y + z\n    self.assertAllClose([1.6, 2.6], self.evaluate(ans))\n\n  @test_util.disable_xla(\"b/123559667\")\n  def testTensorStridedSliceUpdateGradSimple(self):\n    original = constant_op.constant([0.2, 0.3])\n    updates = constant_op.constant([0.4])\n    with backprop.GradientTape() as tape:\n      tape.watch([original, updates])\n      updated = gen_array_ops.tensor_strided_slice_update(\n          original, [0], [1], [1], updates)\n    d1, d2 = tape.gradient(updated, [original, updates],\n                           output_gradients=constant_op.constant([2.0, 3.0]))\n    self.assertAllClose([0.0, 3.0], d1)\n    self.assertAllClose([2.0], d2)\n\n  @parameterized.named_parameters(\n      (\"_%s\" % i, *args) for i, args in enumerate([  # pylint:disable=g-complex-comprehension\n          ([2, 5], [0, 1], [1, 0], [1, 2], [2], 0, 2, 0, 0, 1),\n          ([4], [5], [3], [1], [3], 1, 0, 0, 0, 0),\n          ([2, 2, 3, 2], [0, 0, 1], [1, 0, 2], [1, 0, 1], [2, 3], 0, 0, 2, 0, 5)\n      ]))\n  @test_util.disable_xla(\"b/123559667\")\n  def testTensorStridedSliceUpdateGrad(\n      self, shape, begin, end, strides, updates_shape, *args):\n    with self.cached_session():\n      def f(a, b):\n        return gen_array_ops.tensor_strided_slice_update(\n            a, begin, end, strides, b, *args)\n      theoretical, numerical = gradient_checker_v2.compute_gradient(\n          f, [array_ops.zeros(shape), array_ops.ones(updates_shape)], delta=1.0)\n      self.assertAllClose(theoretical, numerical)\n\n\nclass ShapeSizeRankTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def testDenseShape(self):\n    t_value = [[0, 42], [24, 0]]\n    self.assertAllEqual((2, 2), self.evaluate(array_ops.shape(t_value)))\n    self.assertEqual(4, self.evaluate(array_ops.size(t_value)))\n    self.assertEqual(2, self.evaluate(array_ops.rank(t_value)))\n\n    t = constant_op.constant(t_value)\n    self.assertAllEqual((2, 2), self.evaluate(array_ops.shape(t)))\n    self.assertEqual(4, self.evaluate(array_ops.size(t)))\n    self.assertEqual(2, self.evaluate(array_ops.rank(t)))\n\n  @test_util.run_in_graph_and_eager_modes\n  def testSparseShape(self):\n    sp_value = sparse_tensor.SparseTensorValue(\n        indices=((0, 1), (1, 0)), values=(42, 24), dense_shape=(2, 2))\n    self.assertAllEqual((2, 2), self.evaluate(array_ops.shape(sp_value)))\n    self.assertEqual(4, self.evaluate(array_ops.size(sp_value)))\n    self.assertEqual(2, self.evaluate(array_ops.rank(sp_value)))\n\n    sp = sparse_tensor.SparseTensor.from_value(sp_value)\n    self.assertAllEqual((2, 2), self.evaluate(array_ops.shape(sp)))\n    self.assertEqual(4, self.evaluate(array_ops.size(sp)))\n    self.assertEqual(2, self.evaluate(array_ops.rank(sp)))\n\n  @test_util.run_in_graph_and_eager_modes\n  def testSizeDtype(self):\n    tensor = [1]\n    self.assertEqual(dtypes.int32, self.evaluate(array_ops.size(tensor)).dtype)\n    self.assertEqual(\n        dtypes.int64,\n        self.evaluate(array_ops.size(tensor, out_type=dtypes.int64)).dtype)\n\n\nclass SequenceMaskTest(test_util.TensorFlowTestCase):\n\n  def testExceptions(self):\n    with self.cached_session():\n      with self.assertRaisesRegex(ValueError, \"`maxlen` must be scalar\"):\n        array_ops.sequence_mask([10, 20], [10, 20])\n\n  def testOneDimensionalWithMaxlen(self):\n    res = array_ops.sequence_mask(constant_op.constant([1, 3, 2]), 5)\n    self.assertAllEqual(res.get_shape(), [3, 5])\n    self.assertAllEqual(\n        res,\n        [[True, False, False, False, False], [True, True, True, False, False],\n         [True, True, False, False, False]])\n\n  def testOneDimensionalDtypeWithoutMaxlen(self):\n    # test dtype and default maxlen:\n    res = array_ops.sequence_mask(\n        constant_op.constant([0, 1, 4]), dtype=dtypes.float32)\n    self.assertAllEqual(res.get_shape().as_list(), [3, 4])\n    self.assertAllEqual(\n        res, [[0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 1.0, 1.0, 1.0]])\n\n  def testOneDimensionalWithoutMaxlen(self):\n    res = array_ops.sequence_mask(constant_op.constant([0, 1, 4]))\n    self.assertAllEqual(res.get_shape().as_list(), [3, 4])\n    self.assertAllEqual(res,\n                        [[False, False, False, False],\n                         [True, False, False, False], [True, True, True, True]])\n\n  def testTwoDimensional(self):\n    res = array_ops.sequence_mask(constant_op.constant([[1, 3, 2]]), 5)\n    self.assertAllEqual(res.get_shape(), [1, 3, 5])\n    self.assertAllEqual(\n        res,\n        [[[True, False, False, False, False], [True, True, True, False, False],\n          [True, True, False, False, False]]])\n\n    # test dtype and default maxlen:\n    res = array_ops.sequence_mask(\n        constant_op.constant([[0, 1, 4], [1, 2, 3]]), dtype=dtypes.float32)\n    self.assertAllEqual(res.get_shape().as_list(), [2, 3, 4])\n    self.assertAllEqual(\n        res,\n        [[[0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 1.0, 1.0, 1.0]],\n         [[1.0, 0.0, 0.0, 0.0], [1.0, 1.0, 0.0, 0.0], [1.0, 1.0, 1.0, 0.0]]])\n\n  def testDtypes(self):\n\n    def check_dtypes(lengths_dtype, maxlen_dtype):\n      res = array_ops.sequence_mask(\n          constant_op.constant([1, 3, 2], dtype=lengths_dtype),\n          constant_op.constant(5, dtype=maxlen_dtype))\n      self.assertAllEqual(res.get_shape(), [3, 5])\n      self.assertAllEqual(\n          res,\n          [[True, False, False, False, False], [True, True, True, False, False],\n           [True, True, False, False, False]])\n\n    check_dtypes(dtypes.int32, dtypes.int32)\n    check_dtypes(dtypes.int32, dtypes.int64)\n    check_dtypes(dtypes.int64, dtypes.int32)\n    check_dtypes(dtypes.int64, dtypes.int64)\n\n  def testOutputDtype(self):\n\n    def check_output_dtype(output_dtype):\n      res = self.evaluate(\n          array_ops.sequence_mask(\n              constant_op.constant([1, 3, 2], dtype=dtypes.int32),\n              constant_op.constant(5, dtype=dtypes.int32),\n              dtype=output_dtype))\n      self.assertAllEqual(\n          res,\n          self.evaluate(\n              math_ops.cast([[True, False, False, False, False],\n                             [True, True, True, False, False],\n                             [True, True, False, False, False]], output_dtype)))\n\n    check_output_dtype(dtypes.bool)\n    check_output_dtype(\"bool\")\n    check_output_dtype(np.bool_)\n    check_output_dtype(dtypes.int32)\n    check_output_dtype(\"int32\")\n    check_output_dtype(np.int32)\n    check_output_dtype(dtypes.float32)\n    check_output_dtype(\"float32\")\n    check_output_dtype(np.float32)\n    check_output_dtype(dtypes.int64)\n    check_output_dtype(\"float64\")\n    check_output_dtype(np.float64)\n\n\nclass ConcatSliceResourceTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConcatSlice(self):\n    r1 = test_ops.stub_resource_handle_op(container=\"a\", shared_name=\"b\")\n    r2 = test_ops.stub_resource_handle_op(container=\"a\", shared_name=\"c\")\n    c = array_ops.stack([r1, r2])\n    s = array_ops.strided_slice(c, [1], [2])\n    self.evaluate(test_ops.resource_create_op(s))\n    with self.assertRaises(errors.AlreadyExistsError):\n      self.evaluate(test_ops.resource_create_op(r2))\n\n\nclass IdentityTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_gpu_only\n  def testEagerIdentity(self):\n    with context.eager_mode():\n\n      def _test(x, y, device):\n        self.assertAllEqual(x.numpy(), y.numpy())\n        self.assertTrue(device in y.device.lower())\n\n      with test_util.force_gpu():\n        a = constant_op.constant([[2], [3]], dtype=dtypes.float32)\n      with test_util.force_gpu():\n        b = array_ops.identity(a)\n        _test(a, b, \"gpu\")\n      with test_util.force_cpu():\n        c = array_ops.identity(b)\n        _test(b, c, \"cpu\")\n      with test_util.force_cpu():\n        d = array_ops.identity(c)\n        _test(c, d, \"cpu\")\n      with test_util.force_gpu():\n        e = array_ops.identity(d)\n        _test(d, e, \"gpu\")\n\n\nclass PadTest(test_util.TensorFlowTestCase):\n\n  def testEager(self):\n    with context.eager_mode():\n      t = constant_op.constant([[1, 2, 3], [4, 5, 6]])\n      paddings = constant_op.constant([[\n          1,\n          1,\n      ], [2, 2]])\n      padded = array_ops.pad(t, paddings, \"CONSTANT\")\n      self.assertAllEqual(padded.numpy(),\n                          [[0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 2, 3, 0, 0],\n                           [0, 0, 4, 5, 6, 0, 0], [0, 0, 0, 0, 0, 0, 0]])\n\n  def testSymmetricMirrorPadGrad(self):\n    t = np.broadcast_to(np.arange(0, 7), (3, 2, 1, 7))\n    paddings = constant_op.constant([\n        [1, 1],\n        [0, 0],\n        [0, 0],\n        [2, 2],\n    ])\n    expected = np.broadcast_to(np.array([9, 27, 27]), (1, 2, 1, 3))\n    result = gen_array_ops.mirror_pad_grad(t, paddings, \"SYMMETRIC\")\n    self.assertAllEqual(result, expected)\n\n  def testReflectMirrorPadGrad(self):\n    t = np.broadcast_to(np.reshape(np.arange(0, 7), (7, 1)), (1, 4, 7, 1))\n    paddings = constant_op.constant([\n        [0, 0],\n        [1, 1],\n        [2, 2],\n        [0, 0],\n    ])\n    expected = np.broadcast_to(\n        np.reshape(np.array([16, 18, 8]), (3, 1)), (1, 2, 3, 1))\n    result = gen_array_ops.mirror_pad_grad(t, paddings, \"REFLECT\")\n    self.assertAllEqual(result, expected)\n\n\nclass InvertPermutationTest(test_util.TensorFlowTestCase):\n\n  def testInvertPermutation(self):\n    for dtype in [dtypes.int32, dtypes.int64]:\n      with self.subTest(dtype=dtype, use_gpu=True):\n        x = constant_op.constant([3, 4, 0, 2, 1], dtype=dtype)\n        y = array_ops.invert_permutation(x)\n        self.assertAllEqual(y.get_shape(), [5])\n        self.assertAllEqual(y, [2, 4, 3, 0, 1])\n\n\nclass UnravelIndexTest(test_util.TensorFlowTestCase):\n\n  # TODO(b/73086570): Reenable test.\n  @unittest.skip(\"Test does not pass internally.\")\n  def testUnravelIndex(self):\n    with self.cached_session():\n      for dtype in [dtypes.int32, dtypes.int64]:\n        with self.subTest(dtype=dtype):\n          indices_1 = constant_op.constant(1621, dtype=dtype)\n          dims_1 = constant_op.constant([6, 7, 8, 9], dtype=dtype)\n          out_1 = array_ops.unravel_index(indices_1, dims_1)\n          self.assertAllEqual(out_1, [3, 1, 4, 1])\n\n          indices_2 = constant_op.constant([1621], dtype=dtype)\n          dims_2 = constant_op.constant([6, 7, 8, 9], dtype=dtype)\n          out_2 = array_ops.unravel_index(indices_2, dims_2)\n          self.assertAllEqual(out_2, [[3], [1], [4], [1]])\n\n          indices_3 = constant_op.constant([22, 41, 37], dtype=dtype)\n          dims_3 = constant_op.constant([7, 6], dtype=dtype)\n          out_3 = array_ops.unravel_index(indices_3, dims_3)\n          self.assertAllEqual(out_3, [[3, 6, 6], [4, 5, 1]])\n\n  # Test case for GitHub issue 40204.\n  def testUnravelIndexZeroDim(self):\n    with self.cached_session():\n      for dtype in [dtypes.int32, dtypes.int64]:\n        with self.assertRaisesRegex(errors.InvalidArgumentError,\n                                    \"dims cannot contain a dim of zero\"):\n          indices = constant_op.constant([2, 5, 7], dtype=dtype)\n          dims = constant_op.constant([3, 0], dtype=dtype)\n          self.evaluate(array_ops.unravel_index(indices=indices, dims=dims))\n\n  def testUnravelIndexIntegerOverflow(self):\n    with self.cached_session():\n      for dtype in [dtypes.int32, dtypes.int64]:\n        with self.assertRaisesRegex(\n            errors.InvalidArgumentError,\n            r\"Input dims product is causing integer overflow\"):\n          indices = constant_op.constant(-0x100000, dtype=dtype)\n          if dtype == dtypes.int32:\n            value = 0x10000000\n          else:\n            value = 0x7FFFFFFFFFFFFFFF\n          dims = constant_op.constant([value, value], dtype=dtype)\n          self.evaluate(array_ops.unravel_index(indices=indices, dims=dims))\n\n\nclass GuaranteeConstOpTest(test_util.TensorFlowTestCase):\n\n  def testSimple(self):\n    a = array_ops.constant(10)\n    guarantee_a = array_ops.guarantee_const(a)\n    self.assertEqual(10, self.evaluate(guarantee_a))\n\n  def testVariables(self):\n    for use_resource in [False, True]:\n      with self.subTest(use_resource=use_resource):\n        a = variable_scope.get_variable(\n            \"var_{}\".format(use_resource), [],\n            initializer=init_ops.constant_initializer(10.0),\n            use_resource=use_resource)\n        guarantee_a = array_ops.guarantee_const(a)\n        self.evaluate(a.initializer)\n        self.assertEqual(10.0, self.evaluate(guarantee_a))\n\n  def testResourceRejection(self):\n    with ops.device(\"/cpu:0\"):\n      a = variable_scope.get_variable(\n          \"resource_var\", [],\n          initializer=init_ops.constant_initializer(10.0),\n          use_resource=True)\n    with self.assertRaisesWithPredicateMatch(errors.InvalidArgumentError,\n                                             \"cannot be a resource variable\"):\n      guarantee_a = array_ops.guarantee_const(a.handle)\n      self.evaluate(a.initializer)\n      self.evaluate(guarantee_a)\n\n\nclass SnapshotOpTest(test_util.TensorFlowTestCase):\n\n  def testInvertPermutation(self):\n    for dtype in [dtypes.int32, dtypes.int64, dtypes.float32, dtypes.float64]:\n      with self.subTest(dtype=dtype, use_gpu=True):\n        x = constant_op.constant([0, 1, 2, 3], dtype=dtype)\n        y = gen_array_ops.snapshot(x)\n        self.assertAllEqual(y, [0, 1, 2, 3])\n\n\n@test_util.with_eager_op_as_function\n@test_util.run_all_in_graph_and_eager_modes\nclass QuantizeAndDequantizeTest(test_util.TensorFlowTestCase):\n\n  # Generates a tensor of the specified `shape` using values from `values`\n  # scaled by (slice_idx + 1) along `axis` dimension.\n  def _scale_per_slice(self, shape, axis, values):\n    # Note: repeats the values if the shape is larger than values.\n    out = np.take(values, np.remainder(np.arange(np.prod(shape)),\n                                       len(values))).reshape(shape)\n    if axis is not None:\n      scale_shape = [1] * len(shape)\n      scale_shape[axis] = shape[axis]\n      out *= np.arange(1, shape[axis] + 1).reshape(scale_shape)\n    return out\n\n  def testAxis(self):\n    shape = np.array([2, 3, 4, 5])\n    values = np.array([-1, -0.5, 0, 0.3, 0.8, 0.555, 0.5], dtype=np.float32)\n    quant_values = np.array(\n        [-1, -0.5, 0, 38.0 / 128, 102.0 / 128, 71.0 / 128, 0.5],\n        dtype=np.float32)\n    for axis in [None, 0, 1, 2, 3]:\n      with self.subTest(axis=axis):\n        inputs = constant_op.constant(\n            self._scale_per_slice(shape, axis, values))\n        expected = self._scale_per_slice(shape, axis, quant_values)\n        unused_minmax_value = 0 if axis is None else [0] * shape[axis]\n        fake_quantized = self.evaluate(\n            array_ops.quantize_and_dequantize_v2(\n                inputs,\n                unused_minmax_value,\n                unused_minmax_value,\n                range_given=False,\n                round_mode=\"HALF_UP\",\n                axis=axis))\n        self.assertAllEqual(fake_quantized, expected)\n        if axis is not None:\n          fake_quantized = self.evaluate(\n              array_ops.quantize_and_dequantize_v2(\n                  inputs,\n                  unused_minmax_value,\n                  unused_minmax_value,\n                  range_given=False,\n                  axis=(axis - 4)))\n          self.assertAllClose(fake_quantized, expected)\n\n  def testBadAxis(self):\n    input_tensor = [2.5, 2.5]\n    input_min = [0, 0]\n    input_max = [1, 1]\n    # When eager_op_as_function mode is enabled XLA auto-clustering kicks in.\n    # XLA raises an UnimplementedError on invalid axis.\n    error_message_pattern = (r\"Shape must be at least rank 11 but is rank \"\n                             r\"1|invalid axis\")\n    # TODO(b/171260356): Eager mode and graph mode throw different error types\n    error = (errors.InvalidArgumentError, ValueError, errors.UnimplementedError)\n    with self.assertRaisesRegex(error, error_message_pattern):\n      self.evaluate(\n          array_ops.quantize_and_dequantize_v2(\n              input=input_tensor,\n              input_min=input_min,\n              input_max=input_max,\n              axis=10))\n\n  def testQuantizeDequantizeGrad(self):\n    shape = (2, 2)\n    max_threshold = 0\n    min_threshold = -10\n    input_value = np.random.rand(2, 2) * 40.0 - 20.0\n    input_tensor = constant_op.constant(input_value, shape=shape,\n                                        name=\"input_tensor\")\n    with self.cached_session():\n      def f(a):\n        return array_ops.quantize_and_dequantize_v2(\n            a,\n            input_min=min_threshold,\n            input_max=max_threshold,\n            range_given=True)\n      output_grad = gradient_checker_v2.compute_gradient(f, [input_tensor])\n      self.assertAllClose(output_grad[0], np.zeros([1, 4, 4]))\n\n  def testOutOfBoundAxis(self):\n    input_tensor = constant_op.constant([1., 1.])\n    input_min = [0]\n    input_max = [1]\n    q_input, _, _ = array_ops.quantize(input_tensor, 0, 1, dtypes.qint32)\n    error = (errors.InvalidArgumentError, ValueError)\n    with self.assertRaisesRegex(error,\n                                r\".*Axis must be less than input dimension.*\"):\n      self.evaluate(\n          gen_array_ops.dequantize(\n              input=q_input,\n              min_range=input_min,\n              max_range=input_max,\n              axis=2**31 - 1))\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass SortedSearchTest(test_util.TensorFlowTestCase):\n\n  def testUpperBoundFloatHandCoded(self):\n    cdf = np.array([0, .2, .5, .6, .8, 1.], dtype=np.float32)\n    arr = np.array([.04, .99, .53, .58, .31, .01, .79, .8, .21],\n                   dtype=np.float32)\n    result = np.searchsorted(cdf, arr, side=\"right\")\n    tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"right\"))\n    self.assertAllEqual(result, tf_result)\n\n  def testUpperBoundFloatRandomNd(self):\n    dim_size = 7\n    for d in range(1, 5):\n      shape = [dim_size] * d\n      cdf = np.cumsum(\n          np.random.uniform(size=shape).astype(np.float32), axis=(d - 1))\n      arr = np.random.uniform(size=shape).astype(np.float32) * dim_size\n\n      tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"right\"))\n\n      cdf = cdf.reshape([-1, dim_size])\n      arr = arr.reshape([-1, dim_size])\n      result = np.zeros(arr.shape, dtype=np.int32)\n      for i in range(dim_size**(d - 1)):\n        result[i, :] = np.searchsorted(cdf[i, :], arr[i, :], side=\"right\")\n\n      result = result.reshape(shape)\n\n      self.assertAllEqual(result, tf_result)\n\n  def testUpperBoundFloatUneven(self):\n    batch_size = 7\n    size_search_array = 1000\n    size_values = 47\n    cdf = np.cumsum(\n        np.random.uniform(size=[batch_size, size_search_array]).astype(\n            np.float32),\n        axis=1)\n    arr = np.random.uniform(size=[batch_size, size_values]).astype(\n        np.float32) * size_search_array\n\n    tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"right\"))\n\n    result = np.zeros(arr.shape, dtype=np.int32)\n    for i in range(batch_size):\n      result[i, :] = np.searchsorted(cdf[i, :], arr[i, :], side=\"right\")\n\n    self.assertAllEqual(result, tf_result)\n\n  def testLowerBoundFloatHandCoded(self):\n    cdf = np.array([0, .2, .5, .6, .8, 1.], dtype=np.float32)\n    arr = np.array([.04, .99, .53, .58, .31, .01, .79, .8, .21],\n                   dtype=np.float32)\n    result = np.searchsorted(cdf, arr, side=\"left\")\n    tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"left\"))\n    self.assertAllEqual(result, tf_result)\n\n  def testLowerBoundFloatRandomNd(self):\n    dim_size = 7\n    for d in range(1, 5):\n      shape = [dim_size] * d\n      cdf = np.cumsum(\n          np.random.uniform(size=shape).astype(np.float32), axis=(d - 1))\n      arr = np.random.uniform(size=shape).astype(np.float32) * dim_size\n\n      tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"left\"))\n\n      cdf = cdf.reshape([-1, dim_size])\n      arr = arr.reshape([-1, dim_size])\n      result = np.zeros(arr.shape, dtype=np.int32)\n      for i in range(dim_size**(d - 1)):\n        result[i, :] = np.searchsorted(cdf[i, :], arr[i, :], side=\"left\")\n\n      result = result.reshape(shape)\n\n      self.assertAllEqual(result, tf_result)\n\n  def testLowerBoundFloatUneven(self):\n    batch_size = 7\n    size_search_array = 1000\n    size_values = 47\n    cdf = np.cumsum(\n        np.random.uniform(size=[batch_size, size_search_array]).astype(\n            np.float32),\n        axis=1)\n    arr = np.random.uniform(size=[batch_size, size_values]).astype(\n        np.float32) * size_search_array\n\n    tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"left\"))\n\n    result = np.zeros(arr.shape, dtype=np.int32)\n    for i in range(batch_size):\n      result[i, :] = np.searchsorted(cdf[i, :], arr[i, :], side=\"left\")\n\n    self.assertAllEqual(result, tf_result)\n\n  def testUpperBoundIntHandCoded(self):\n    cdf = np.array([0, 20, 50, 60, 80, 100], dtype=np.int64)\n    arr = np.array([4, 99, 53, 58, 31, 1, 79, 8, 21], dtype=np.int64)\n    result = np.searchsorted(cdf, arr, side=\"right\")\n    tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"right\"))\n    self.assertAllEqual(result, tf_result)\n\n  def testUpperBoundIntRandomNd(self):\n    dim_size = 7\n    for d in range(1, 5):\n      shape = [dim_size] * d\n      cdf = np.cumsum(\n          np.random.randint(low=0, high=10, size=shape).astype(np.int64),\n          axis=(d - 1))\n      arr = np.random.randint(\n          low=0, high=10 * dim_size, size=shape).astype(np.int64)\n\n      tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"right\"))\n\n      cdf = cdf.reshape([-1, dim_size])\n      arr = arr.reshape([-1, dim_size])\n      result = np.zeros(arr.shape, dtype=np.int32)\n      for i in range(dim_size**(d - 1)):\n        result[i, :] = np.searchsorted(cdf[i, :], arr[i, :], side=\"right\")\n\n      result = result.reshape(shape)\n\n      self.assertAllEqual(result, tf_result)\n\n  def testUpperBoundIntUneven(self):\n    batch_size = 7\n    size_search_array = 1000\n    size_values = 47\n    cdf = np.cumsum(\n        np.random.randint(low=0, high=10,\n                          size=[batch_size,\n                                size_search_array]).astype(np.int64),\n        axis=1)\n    arr = np.random.randint(\n        low=0, high=10 * size_search_array, size=[batch_size,\n                                                  size_values]).astype(np.int64)\n\n    tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"right\"))\n\n    result = np.zeros(arr.shape, dtype=np.int32)\n    for i in range(batch_size):\n      result[i, :] = np.searchsorted(cdf[i, :], arr[i, :], side=\"right\")\n\n    self.assertAllEqual(result, tf_result)\n\n  def testLowerBoundIntHandCoded(self):\n    cdf = np.array([0, 20, 50, 60, 80, 100], dtype=np.int64)\n    arr = np.array([4, 99, 53, 58, 31, 1, 79, 8, 21], dtype=np.int64)\n    result = np.searchsorted(cdf, arr, side=\"left\")\n    tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"left\"))\n    self.assertAllEqual(result, tf_result)\n\n  def testLowerBoundIntRandomNd(self):\n    dim_size = 7\n    for d in range(1, 5):\n      shape = [dim_size] * d\n      cdf = np.cumsum(\n          np.random.randint(low=0, high=10, size=shape).astype(np.int64),\n          axis=(d - 1))\n      arr = np.random.randint(\n          low=0, high=10 * dim_size, size=shape).astype(np.int64)\n\n      tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"left\"))\n\n      cdf = cdf.reshape([-1, dim_size])\n      arr = arr.reshape([-1, dim_size])\n      result = np.zeros(arr.shape, dtype=np.int32)\n      for i in range(dim_size**(d - 1)):\n        result[i, :] = np.searchsorted(cdf[i, :], arr[i, :], side=\"left\")\n\n      result = result.reshape(shape)\n\n      self.assertAllEqual(result, tf_result)\n\n  def testLowerBoundIntUneven(self):\n    batch_size = 7\n    size_search_array = 1000\n    size_values = 47\n    cdf = np.cumsum(\n        np.random.randint(low=0, high=10,\n                          size=[batch_size,\n                                size_search_array]).astype(np.int64),\n        axis=1)\n    arr = np.random.randint(\n        low=0, high=10 * size_search_array, size=[batch_size,\n                                                  size_values]).astype(np.int64)\n\n    tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"left\"))\n\n    result = np.zeros(arr.shape, dtype=np.int32)\n    for i in range(batch_size):\n      result[i, :] = np.searchsorted(cdf[i, :], arr[i, :], side=\"left\")\n\n    self.assertAllEqual(result, tf_result)\n\n  def testZeroSequenceSize(self):\n    dtype = dtypes.int32\n    for side in (\"left\", \"right\"):\n      with self.subTest(side=side):\n        self.assertAllEqual(\n            array_ops.searchsorted(\n                array_ops.ones([2, 0]),\n                array_ops.ones([2, 3]),\n                side=side,\n                out_type=dtype), array_ops.zeros([2, 3], dtype))\n\n  def testZeroValueSize(self):\n    dtype = dtypes.int32\n    for side in (\"left\", \"right\"):\n      with self.subTest(side=side):\n        self.assertAllEqual(\n            array_ops.searchsorted(\n                array_ops.ones([2, 3]),\n                array_ops.ones([2, 0]),\n                side=side,\n                out_type=dtype), array_ops.zeros([2, 0], dtype))\n\n  def testInt64(self):\n\n    @def_function.function\n    def g():\n      x = random_ops.random_normal(shape=[int(1e10)])\n      y = array_ops.ones(shape=[int(1e10)])\n      return array_ops.searchsorted(x, y, out_type=dtypes.int64)\n\n    _ = g.get_concrete_function()\n\n  def testInt64UnspecifiedOutType(self):\n\n    @def_function.function\n    def g():\n      x = random_ops.random_normal(shape=[int(1e10)])\n      y = array_ops.ones(shape=[int(1e10)])\n      return array_ops.searchsorted(x, y)\n\n    _ = g.get_concrete_function()\n\n\nclass BatchGatherNdTest(test_util.TensorFlowTestCase):\n\n  def testShapesMatch(self):\n    \"\"\"Tests for various different shape combinations.\"\"\"\n    shapes = []\n    # params_shape, indices_shape, batch_dims\n    shapes.append(((2, 2, 2), (2, 1), 1),)\n    shapes.append(((2, 2, 2), (2, 2), 1),)\n    shapes.append(((2, 2, 2), (2, 3), 0),)\n    shapes.append(((2, 2, 2), (3,), 0),)\n    shapes.append(((2, 2, 2), (1,), 0),)\n    shapes.append(((2, 2, 3, 2), (2, 3), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 2), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 1), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 1, 3), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 2, 2), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 3, 1), 1),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 3), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 2), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 1), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 1, 3), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 2, 2), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 3, 1), 2),)\n\n    for params_shape, indices_shape, batch_dims in shapes:\n      with self.subTest(\n          params_shape=params_shape,\n          indices_shape=indices_shape,\n          batch_dims=batch_dims):\n        params = constant_op.constant(1.0, shape=(params_shape))\n        indices = constant_op.constant(\n            1, shape=(indices_shape), dtype=dtypes.int32)\n        out = array_ops.batch_gather_nd(\n            params=params, indices=indices, batch_dims=batch_dims)\n        ndims_params = len(params_shape) - batch_dims\n        ndims_rows = ndims_params - indices_shape[-1]\n        expected_out_shape = indices_shape[:-1]\n        if ndims_rows > 0:\n          expected_out_shape += params_shape[-ndims_rows:]\n        self.assertSequenceEqual(out.shape, expected_out_shape)\n\n  def testReducesToGatherNDWhenBatchDimIsZero(self):\n    \"\"\"Confirms setting batch_dims to zero reduces to tf.gather_nd.\"\"\"\n    params = constant_op.constant(np.random.uniform(0.0, 1.0, size=(7, 8, 9)))\n    indices_shapes = []\n    indices_shapes.append((1,))\n    indices_shapes.append((3, 1))\n    indices_shapes.append((3, 3, 1))\n    indices_shapes.append((2,))\n    indices_shapes.append((3, 2))\n    indices_shapes.append((3, 3, 2))\n    indices_shapes.append((3,))\n    indices_shapes.append((3, 3))\n    indices_shapes.append((3, 3, 3))\n\n    for indices_shape in indices_shapes:\n      with self.subTest(indices_shape=indices_shape):\n        indices = np.random.randint(0, 7, size=indices_shape)\n        gather_nd_result = gen_array_ops.gather_nd(params, indices)\n        batch_gather_nd_result = array_ops.batch_gather_nd(\n            params=params, indices=indices, batch_dims=0)\n        self.assertAllEqual(gather_nd_result, batch_gather_nd_result)\n\n  def testSameResultAsMapFn(self):\n    \"\"\"Compares results with gather_nd called on every element with map_fn.\"\"\"\n    shapes = []\n    # params_shape, indices_shape, batch_dims\n    shapes.append(((2, 2, 2), (2, 1), 1),)\n    shapes.append(((2, 2, 2), (2, 2), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 3), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 2), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 1), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 1, 3), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 2, 2), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 3, 1), 1),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 3), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 2), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 1), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 1, 3), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 2, 2), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 3, 1), 2),)\n\n    for params_shape, indices_shape, batch_dims in shapes:\n      with self.subTest(\n          params_shape=params_shape,\n          indices_shape=indices_shape,\n          batch_dims=batch_dims):\n        params = constant_op.constant(\n            np.random.uniform(0.0, 1.0, size=(params_shape)))\n        indices = np.random.randint(0, 2, size=indices_shape)\n        batch_gather_nd_result = array_ops.batch_gather_nd(\n            params=params, indices=indices, batch_dims=batch_dims)\n\n        if batch_dims > 1:\n          params = array_ops.reshape(\n              params, shape=[-1] + list(params_shape[batch_dims:]))\n          indices = array_ops.reshape(\n              indices, shape=[-1] + list(indices_shape[batch_dims:]))\n\n        map_fn_gather_nd_result = map_fn.map_fn(\n            fn=self._map_fn_body, elems=(params, indices), dtype=dtypes.float64)\n\n        if batch_dims > 1:\n          out_shape = map_fn_gather_nd_result.shape.as_list()\n          out_shape = list(params_shape[:batch_dims]) + out_shape[1:]\n          map_fn_gather_nd_result = array_ops.reshape(\n              map_fn_gather_nd_result, shape=out_shape)\n\n        self.assertAllEqual(map_fn_gather_nd_result, batch_gather_nd_result)\n\n  def _map_fn_body(self, elems):\n    return gen_array_ops.gather_nd(elems[0], elems[1])\n\n  def testBatchDimsAsTensor(self):\n    \"\"\"Tests Tensor batch_dims as input works as intended.\"\"\"\n    shapes = []\n    # params_shape, indices_shape, batch_dims\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 3, 1), 0),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 3, 1), 1),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 3, 1), 2),)\n\n    for params_shape, indices_shape, batch_dims in shapes:\n      with self.subTest(\n          params_shape=params_shape,\n          indices_shape=indices_shape,\n          batch_dims=batch_dims):\n        params = constant_op.constant(\n            np.random.uniform(0.0, 1.0, size=(params_shape)))\n        indices = np.random.randint(0, 2, size=indices_shape)\n        batch_gather_nd_result = array_ops.gather_nd(\n            params=params, indices=indices, batch_dims=batch_dims)\n        batch_dims_tensor = constant_op.constant([batch_dims])\n        batch_gather_nd_tensor_batch_dims_result = array_ops.gather_nd(\n            params=params, indices=indices, batch_dims=batch_dims_tensor)\n\n        self.assertAllEqual(batch_gather_nd_tensor_batch_dims_result,\n                            batch_gather_nd_result)\n\n  def testInvalidBatchDimsRaisesException(self):\n    \"\"\"Tests whether invalid batch_dims raise expected exceptions.\"\"\"\n    params = constant_op.constant(\n        np.random.uniform(0.0, 1.0, size=(3, 2, 2, 3, 4)))\n    indices = np.random.randint(0, 2, size=(3, 2, 3))\n\n    with self.assertRaises(TypeError):\n      array_ops.batch_gather_nd(\n          params=params,\n          indices=indices,\n          batch_dims=constant_op.constant((0, 1)))\n\n    with self.assertRaises(ValueError):\n      array_ops.batch_gather_nd(params=params, indices=indices, batch_dims=-1)\n\n    with self.assertRaises(ValueError):\n      array_ops.batch_gather_nd(params=params, indices=indices, batch_dims=4)\n\n  def testNoneBatchDimensions(self):\n    \"\"\"Tests gather_nd works with None dimensions.\"\"\"\n    shapes = []\n    # params_shape, indices_shape, batch_dims\n    shapes.append(((2, 2, 2), (2, 1), 1),)\n    shapes.append(((2, 2, 2), (2, 2), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 3), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 2), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 1), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 1, 3), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 2, 2), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 3, 1), 1),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 3), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 2), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 1), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 1, 3), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 2, 2), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 3, 1), 2),)\n\n    for params_shape, indices_shape, batch_dims in shapes:\n      params_ph_shape = list(params_shape)\n      indices_ph_shape = list(indices_shape)\n      for i in range(batch_dims):\n        params_ph_shape[i] = None\n        indices_ph_shape[i] = None\n\n      @def_function.function\n      def func(params, indices):\n        return array_ops.batch_gather_nd(\n            params=params, indices=indices, batch_dims=batch_dims)  # pylint: disable=cell-var-from-loop\n\n      f = func.get_concrete_function(\n          tensor_spec.TensorSpec(params_ph_shape, dtypes.float32),\n          tensor_spec.TensorSpec(indices_ph_shape, dtypes.int32))\n\n      params_val = np.ones(dtype=np.float32, shape=params_shape)\n      indices_val = np.ones(dtype=np.int32, shape=indices_shape)\n      res = f(params_val, indices_val)\n      row_ndims = len(params_shape) - batch_dims - indices_shape[-1]\n      expected_out_shape = indices_shape[:-1]\n      if row_ndims > 0:\n        expected_out_shape += params_shape[-row_ndims:]\n\n      self.assertSequenceEqual(res.shape, expected_out_shape)\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass RepeatTest(test_util.TensorFlowTestCase, parameterized.TestCase):\n\n  @parameterized.parameters(\n      (3, 4, None),\n      ([[1, 2], [3, 4]], 2, None),\n      ([[1, 2], [3, 4]], [1, 2], 0),\n      ([[1, 2], [3, 4]], [1, 2], 1),\n      ([[1, 2], [3, 4]], 3, 1),\n      ([[1, 2], [3, 4]], [1, 2, 3, 4], None),\n      (np.ones([0, 4]), 0, 1),\n      (np.ones([1, 2]), [2], None),\n  )\n  @test_util.with_forward_compatibility_horizons(None, [2052, 2, 7])\n  def testRepeat(self, array, repeats, axis):\n    array = np.array(array)\n\n    @def_function.function(\n        input_signature=[tensor_spec.TensorSpec(None, dtypes.int32)] * 2)\n    def repeat_fn(array, repeats):\n      return array_ops.repeat(array, repeats, axis)\n\n    v_tf = array_ops.repeat(constant_op.constant(array), repeats, axis)\n    v_tf_fn = repeat_fn(\n        constant_op.constant(array, dtype=dtypes.int32), repeats)\n    v_np = np.repeat(array, repeats, axis)\n    self.assertAllEqual(v_tf, v_np)\n    self.assertAllEqual(v_tf_fn, v_np)\n\n\nclass RepeatBenchmark(test_lib.Benchmark):\n  \"\"\"Benchmark the repeat implementation.\"\"\"\n\n  def run_and_time(self, op, iters=100, warmup_iters=10):\n    self.evaluate(variables.global_variables_initializer())\n    for _ in range(warmup_iters):\n      _ = self.evaluate(op)\n    t0 = time.time()\n    for _ in range(iters):\n      self.evaluate(op)\n    t1 = time.time()\n    self.report_benchmark(iters=iters, wall_time=(t1 - t0) / float(iters))\n\n  def make_variable(self, shape, dtype=dtypes.float32):\n    items = 1\n    for dim in shape:\n      items *= dim\n    var = variables.Variable(\n        array_ops.reshape(math_ops.linspace(1., float(items), items), shape),\n        dtype=dtype)\n    return var\n\n  def run_benchmark(self, shape, max_repeats, axis=None):\n    with session.Session():\n      var = self.make_variable(shape)\n      if axis is None:\n        axis_size = 1\n        for dim in shape:\n          axis_size *= dim\n      else:\n        axis_size = shape[axis]\n      repeats = constant_op.constant(\n          np.random.randint(max_repeats, size=[axis_size]), dtype=dtypes.int64)\n      repeat_op = array_ops.repeat(var, repeats, axis=axis)\n      # Return a scalar to reduce the device-to-host memcopy overhead.\n      repeat_op = repeat_op[(0,) * len(shape)]\n      self.run_and_time(repeat_op)\n\n  def benchmark_repeat_few_1d(self):\n    self.run_benchmark(shape=[1024 * 1024], max_repeats=8, axis=0)\n\n  def benchmark_repeat_many_1d(self):\n    self.run_benchmark(shape=[8 * 1024], max_repeats=1024, axis=0)\n\n  def benchmark_repeat_few_2d_axis0(self):\n    self.run_benchmark(shape=[8, 128 * 1024], max_repeats=8, axis=0)\n\n  def benchmark_repeat_many_2d_axis0(self):\n    self.run_benchmark(shape=[8, 1024], max_repeats=1024, axis=0)\n\n  def benchmark_repeat_many_2d_axis0_big(self):\n    self.run_benchmark(shape=[1024, 32], max_repeats=1024, axis=0)\n\n  def benchmark_repeat_few_2d_axis1(self):\n    self.run_benchmark(shape=[8, 128 * 1024], max_repeats=8, axis=1)\n\n  def benchmark_repeat_many_2d_axis1(self):\n    self.run_benchmark(shape=[8, 1024], max_repeats=1024, axis=1)\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass TileVariantTest(test_util.TensorFlowTestCase):\n\n  def test_tile_tensor_list(self):\n    t = constant_op.constant(np.random.uniform(size=[2, 3, 4]))\n    handle = list_ops.tensor_list_from_tensor(t, element_shape=None)\n    with ops.device(\"CPU:0\"):\n      tiled_handles = array_ops.tile(array_ops.reshape(handle, [1]), [2])\n    tiled_tensor_0 = list_ops.tensor_list_stack(tiled_handles[0], t.dtype, 2,\n                                                [3, 4])\n    tiled_tensor_1 = list_ops.tensor_list_stack(tiled_handles[1], t.dtype, 2,\n                                                [3, 4])\n    self.assertAllEqual(t, tiled_tensor_0)\n    self.assertAllEqual(t, tiled_tensor_1)\n    # Now mutate some of the lists and make sure the changes are not reflected\n    # in the tiled handles.\n    with ops.control_dependencies([\n        list_ops.tensor_list_scatter([t[0] + 1], [0], input_handle=handle),\n        list_ops.tensor_list_set_item(tiled_handles[0], 0, t[0] + 2)]):\n      tiled_tensor_0 = list_ops.tensor_list_stack(tiled_handles[0], t.dtype, 2,\n                                                  [3, 4])\n      tiled_tensor_1 = list_ops.tensor_list_stack(tiled_handles[1], t.dtype, 2,\n                                                  [3, 4])\n    self.assertAllEqual(t, tiled_tensor_0)\n    self.assertAllEqual(t, tiled_tensor_1)\n\n\nclass StopGradientTest(test_util.TensorFlowTestCase):\n\n  def testStopGradient(self):\n    x = array_ops.zeros(3)\n    y = array_ops.stop_gradient(x)\n    self.assertAllEqual(x, y)\n\n  def testStopGradientRaggedTensor(self):\n    x = RaggedTensor.from_row_splits(values=[1, 2, 3], row_splits=[0, 1, 1, 3])\n    y = array_ops.stop_gradient(x)\n    self.assertAllEqual(x, y)\n\n  def testStopGradientGradientTape(self):\n    x = array_ops.zeros(3)\n    with backprop.GradientTape() as tape:\n      y = array_ops.stop_gradient(x)\n\n    self.assertIsNone(tape.gradient(y, x))\n\n  def testStopGradientGradientTapeRaggedTensor(self):\n    x = RaggedTensor.from_row_splits(values=[1, 2, 3], row_splits=[0, 1, 1, 3])\n    with backprop.GradientTape() as tape:\n      y = array_ops.stop_gradient(x)\n\n    self.assertIsNone(tape.gradient(y, x))\n\n\nif __name__ == \"__main__\":\n  test_lib.main()\n"], "fixing_code": ["/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#ifndef TENSORFLOW_CORE_KERNELS_RESHAPE_OP_H_\n#define TENSORFLOW_CORE_KERNELS_RESHAPE_OP_H_\n\n#include <memory>\n\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/register_types.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/tensor_shape.h\"\n#include \"tensorflow/core/framework/types.h\"\n#include \"tensorflow/core/lib/core/status.h\"\n#include \"tensorflow/core/platform/logging.h\"\n#include \"tensorflow/core/util/overflow.h\"\n\nnamespace tensorflow {\n\n// Note that this op is subclassed for QuantizedReshapeOp.\nclass ReshapeOp : public OpKernel {\n public:\n  explicit ReshapeOp(OpKernelConstruction* context) : OpKernel(context) {}\n\n  void Compute(OpKernelContext* context) override {\n    const Tensor& input = context->input(0);\n    const Tensor& sizes = context->input(1);\n    // Preliminary validation of sizes.\n    OP_REQUIRES(\n        context,\n        (TensorShapeUtils::IsVector(sizes.shape()) ||\n         // TODO(rmlarsen): Disallow legacy use of scalars to represent shape.\n         TensorShapeUtils::IsScalar(sizes.shape())),\n        errors::InvalidArgument(\"sizes input must be 1-D, not \",\n                                sizes.shape().DebugString()));\n    OP_REQUIRES(\n        context, sizes.NumElements() < TensorShape::MaxDimensions(),\n        errors::InvalidArgument(\"too many dimensions: must be < \",\n                                TensorShape::MaxDimensions(), \", but received \",\n                                sizes.NumElements()));\n\n    // Compute the output shape.  Determine product of specified\n    // dimensions, and find the index of the unspecified one.\n    TensorShape shape;\n    int64_t product = 1;\n    int unknown_index = -1;\n    bool sizes_has_zero_dim;\n    switch (sizes.dtype()) {\n      case DT_INT32:\n        OP_REQUIRES_OK(context,\n                       ValidateSizes<int32>(sizes, &product, &unknown_index,\n                                            &shape, &sizes_has_zero_dim));\n        break;\n      case DT_INT64:\n        OP_REQUIRES_OK(context,\n                       ValidateSizes<int64_t>(sizes, &product, &unknown_index,\n                                              &shape, &sizes_has_zero_dim));\n        break;\n      default:\n        context->CtxFailure(errors::InvalidArgument(\n            \"desired shape must be a DT_INT32 or DT_INT64 vector, not a \",\n            DataTypeString(sizes.dtype())));\n        return;\n    }\n    if (unknown_index != -1) {\n      int64_t input_num_elements = 1;\n      bool input_has_zero_dim = false;\n      for (int dim = 0; dim < input.dims(); dim++) {\n        // For zero dimension, we don't count it into `input_num_elements`\n        // unless `sizes` has no zero dimension, so we are still able to\n        // infer shapes for other dimensions.\n        if (input.dim_size(dim) > 0 || !sizes_has_zero_dim) {\n          input_num_elements *= input.dim_size(dim);\n        } else {\n          input_has_zero_dim = true;\n        }\n      }\n\n      const int64_t missing = input_num_elements / product;\n      if (!input_has_zero_dim) {\n        OP_REQUIRES(\n            context, product * missing == input_num_elements,\n            errors::InvalidArgument(\n                \"Input to reshape is a tensor with \", input_num_elements,\n                \" values, but the requested shape requires a multiple of \",\n                product));\n      }\n      shape.set_dim(unknown_index, missing);\n    }\n    OP_REQUIRES(context, shape.num_elements() == input.NumElements(),\n                errors::InvalidArgument(\"Input to reshape is a tensor with \",\n                                        input.NumElements(),\n                                        \" values, but the requested shape has \",\n                                        shape.num_elements()));\n\n    // Actually produce the reshaped output.\n    Tensor output(input.dtype());\n    CHECK(output.CopyFrom(input, shape));\n    context->set_output(0, output);\n  }\n\n  bool IsExpensive() override { return false; }\n\n private:\n  template <typename Tshape>\n  Status ValidateSizes(const Tensor& sizes, int64_t* product,\n                       int* unknown_index, TensorShape* shape,\n                       bool* has_zero_dim) {\n    *product = 1;\n    *unknown_index = -1;\n    *has_zero_dim = false;\n    const int64_t num_dims = sizes.NumElements();\n    auto Svec = sizes.flat<Tshape>();\n    for (int d = 0; d < num_dims; ++d) {\n      const Tshape size = Svec(d);\n      if (size == -1) {\n        if (*unknown_index != -1) {\n          return errors::InvalidArgument(\n              \"Only one input size may be -1, not both \", *unknown_index,\n              \" and \", d);\n        }\n        *unknown_index = d;\n        shape->AddDim(1);\n      } else if (size < 0) {\n        return errors::InvalidArgument(\"Size \", d,\n                                       \" must be non-negative, not \", size);\n      } else if (size == 0) {\n        // We don't include zero-sized dimension in product, so that we can\n        // still calculate number of elements for non-zero-sized dimensions and\n        // therefore infer their shapes.\n        shape->AddDim(size);\n        *has_zero_dim = true;\n      } else {\n        if (MultiplyWithoutOverflow(shape->num_elements(), size) < 0) {\n          string msg;\n          for (int ii = 0; ii < num_dims; ++ii) {\n            if (ii != 0) {\n              strings::StrAppend(&msg, \", \");\n            }\n            strings::StrAppend(&msg, Svec(ii));\n          }\n          return errors::InvalidArgument(\"Shape [\", msg,\n                                         \"] has too many elements\");\n        }\n        shape->AddDim(size);\n        (*product) *= size;\n      }\n    }\n    return Status::OK();\n  }\n};\n\n}  // namespace tensorflow\n\n#endif  // TENSORFLOW_CORE_KERNELS_RESHAPE_OP_H_\n", "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for array_ops.\"\"\"\nimport re\nimport time\nimport unittest\n\nfrom absl.testing import parameterized\nimport numpy as np\n\nfrom tensorflow.python.client import session\nfrom tensorflow.python.eager import backprop\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.eager import def_function\nfrom tensorflow.python.framework import config\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors\nfrom tensorflow.python.framework import errors_impl\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import sparse_tensor\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.framework import tensor_spec\nfrom tensorflow.python.framework import test_ops\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import gen_array_ops\nfrom tensorflow.python.ops import gradient_checker_v2\nfrom tensorflow.python.ops import init_ops\nfrom tensorflow.python.ops import list_ops\nfrom tensorflow.python.ops import map_fn\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import random_ops\nfrom tensorflow.python.ops import resource_variable_ops\nfrom tensorflow.python.ops import state_ops\nfrom tensorflow.python.ops import variable_scope\nfrom tensorflow.python.ops import variables\nfrom tensorflow.python.ops.ragged.ragged_tensor import RaggedTensor\nfrom tensorflow.python.platform import test as test_lib\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass BatchMatrixTransposeTest(test_util.TensorFlowTestCase):\n\n  def testNonBatchMatrix(self):\n    matrix = [[1, 2, 3], [4, 5, 6]]  # Shape (2, 3)\n    expected_transposed = [[1, 4], [2, 5], [3, 6]]  # Shape (3, 2)\n    transposed = array_ops.matrix_transpose(matrix)\n    self.assertEqual((3, 2), transposed.get_shape())\n    self.assertAllEqual(expected_transposed, transposed)\n\n  def testConjugate(self):\n    m = [[1 + 1j, 2 + 2j, 3 + 3j], [4 + 4j, 5 + 5j, 6 + 6j]]\n    expected_transposed = [[1 - 1j, 4 - 4j], [2 - 2j, 5 - 5j], [3 - 3j, 6 - 6j]]\n    matrix = ops.convert_to_tensor(m)\n    transposed = array_ops.matrix_transpose(matrix, conjugate=True)\n    self.assertEqual((3, 2), transposed.get_shape())\n    self.assertAllEqual(expected_transposed, transposed)\n\n  def testBatchMatrix(self):\n    matrix_0 = [[1, 2, 3], [4, 5, 6]]\n    matrix_0_t = [[1, 4], [2, 5], [3, 6]]\n    matrix_1 = [[11, 22, 33], [44, 55, 66]]\n    matrix_1_t = [[11, 44], [22, 55], [33, 66]]\n    batch_matrix = [matrix_0, matrix_1]  # Shape (2, 2, 3)\n    expected_transposed = [matrix_0_t, matrix_1_t]  # Shape (2, 3, 2)\n    transposed = array_ops.matrix_transpose(batch_matrix)\n    self.assertEqual((2, 3, 2), transposed.get_shape())\n    self.assertAllEqual(expected_transposed, transposed)\n\n  def testNonBatchMatrixDynamicallyDefined(self):\n    # needs explicit `constant` because lists are not automatically\n    # converted to sensors when applying `transpose` below\n    matrix = constant_op.constant([[1, 2, 3], [4, 5, 6]])  # Shape (2, 3)\n    expected_transposed = [[1, 4], [2, 5], [3, 6]]  # Shape (3, 2)\n\n    @def_function.function(input_signature=[\n        tensor_spec.TensorSpec(shape=None, dtype=dtypes.int32)\n    ])\n    def transpose(matrix):\n      self.assertIs(matrix.shape.ndims, None)\n      return array_ops.matrix_transpose(matrix)\n\n    self.assertAllEqual(expected_transposed, transpose(matrix))\n\n  def testBatchMatrixDynamicallyDefined(self):\n    matrix_0 = [[1, 2, 3], [4, 5, 6]]\n    matrix_0_t = [[1, 4], [2, 5], [3, 6]]\n    matrix_1 = [[11, 22, 33], [44, 55, 66]]\n    matrix_1_t = [[11, 44], [22, 55], [33, 66]]\n    # needs explicit `constant` because lists are not automatically\n    # converted to sensors when applying `transpose` below\n    batch_matrix = constant_op.constant([matrix_0, matrix_1])  # Shape (2, 2, 3)\n    expected_transposed = [matrix_0_t, matrix_1_t]  # Shape (2, 3, 2)\n\n    @def_function.function(input_signature=[\n        tensor_spec.TensorSpec(shape=None, dtype=dtypes.int32)\n    ])\n    def transpose(matrix):\n      self.assertIs(matrix.shape.ndims, None)\n      return array_ops.matrix_transpose(matrix)\n\n    self.assertAllEqual(expected_transposed, transpose(batch_matrix))\n\n  def testTensorWithStaticRankLessThanTwoRaisesBecauseNotAMatrix(self):\n    vector = [1, 2, 3]\n    with self.assertRaisesRegex(ValueError, \"should be a \"):\n      array_ops.matrix_transpose(vector)\n\n  def testNarrowMatrixConjugateTranspose(self):\n    for dtype in (dtypes.float32, dtypes.float64):\n      for conjugate in (True, False):\n        with self.subTest(complex_type=dtype, conjugate=conjugate):\n          vector = math_ops.complex(\n              constant_op.constant(0, dtype=dtype),\n              math_ops.range(96, dtype=dtype))\n          column_vector = array_ops.expand_dims(vector, axis=-1)\n          row_vector = array_ops.expand_dims(vector, axis=0)\n          narrow_matrix = array_ops.tile(column_vector, [1, 2])  # [96, 2]\n          expected_transposed = array_ops.tile(row_vector, [2, 1])  # [2, 96]\n          if conjugate:\n            expected_transposed = -expected_transposed\n\n          transposed = array_ops.matrix_transpose(\n              narrow_matrix, conjugate=conjugate)\n\n          self.assertEqual((2, 96), transposed.get_shape())\n          self.assertAllEqual(expected_transposed, transposed)\n\n\nclass BooleanMaskTest(test_util.TensorFlowTestCase):\n\n  def setUp(self):\n    self.rng = np.random.RandomState(42)\n\n  def CheckVersusNumpy(self, ndims_mask, arr_shape, make_mask=None, axis=None):\n    \"\"\"Check equivalence between boolean_mask and numpy masking.\"\"\"\n    if make_mask is None:\n      make_mask = lambda shape: self.rng.randint(0, 2, size=shape).astype(bool)\n    arr = np.random.rand(*arr_shape)\n    mask = make_mask(arr_shape[:ndims_mask])\n    if axis is not None:\n      mask = make_mask(arr_shape[axis:ndims_mask + axis])\n    if axis is None or axis == 0:\n      masked_arr = arr[mask]\n    elif axis == 1:\n      masked_arr = arr[:, mask]\n    elif axis == 2:\n      masked_arr = arr[:, :, mask]\n    masked_tensor = array_ops.boolean_mask(arr, mask, axis=axis)\n\n    # Leading dimension size of masked_tensor is always unknown until runtime\n    # since we don't how many elements will be kept.\n    leading = 1 if axis is None else axis + 1\n    self.assertAllEqual(masked_tensor.get_shape()[leading:],\n                        masked_arr.shape[leading:])\n\n    self.assertAllClose(masked_arr, masked_tensor)\n\n  def testMaskDim1ArrDim2Axis1(self):\n    ndims_mask = 1\n    for arr_shape in [(1, 1), (2, 2), (2, 5)]:\n      with self.subTest(arr_shape=arr_shape):\n        self.CheckVersusNumpy(ndims_mask, arr_shape, axis=1)\n\n  def testMaskDim2ArrDim2Axis1(self):\n    ndims_mask = 2\n    for arr_shape in [(1, 1), (2, 2), (2, 5)]:\n      with self.subTest(arr_shape=arr_shape):\n        self.CheckVersusNumpy(ndims_mask, arr_shape, axis=1)\n\n  def testMaskDim1ArrDim1(self):\n    ndims_mask = 1\n    for arr_shape in [(1,), (2,), (3,), (10,)]:\n      with self.subTest(arr_shape=arr_shape):\n        self.CheckVersusNumpy(ndims_mask, arr_shape)\n\n  def testMaskDim1ArrDim2(self):\n    ndims_mask = 1\n    for arr_shape in [(1, 1), (2, 2), (2, 5)]:\n      with self.subTest(arr_shape=arr_shape):\n        self.CheckVersusNumpy(ndims_mask, arr_shape)\n\n  def testMaskDim2ArrDim2(self):\n    ndims_mask = 2\n    for arr_shape in [(1, 1), (2, 2), (2, 5)]:\n      with self.subTest(arr_shape=arr_shape):\n        self.CheckVersusNumpy(ndims_mask, arr_shape)\n\n  def testMaskDim2ArrDim3(self):\n    ndims_mask = 2\n    for arr_shape in [(1, 1, 1), (1, 2, 2), (2, 2, 1)]:\n      with self.subTest(arr_shape=arr_shape):\n        self.CheckVersusNumpy(ndims_mask, arr_shape)\n\n  def testEmptyInput2D(self):\n    mask = np.array([True, False])\n    arr = np.array([[], []]).astype(np.float32)\n    numpy_result = arr[mask]\n    tf_result = array_ops.boolean_mask(arr, mask)\n    self.assertAllEqual(numpy_result.shape[1:], tf_result.get_shape()[1:])\n    with self.cached_session():\n      self.assertAllClose(numpy_result, tf_result)\n\n  def testEmptyInput1D(self):\n    mask = np.array([]).astype(bool)\n    arr = np.array([]).astype(np.float32)\n    numpy_result = arr[mask]\n    tf_result = array_ops.boolean_mask(arr, mask)\n    self.assertAllEqual(numpy_result.shape[1:], tf_result.get_shape()[1:])\n    with self.cached_session():\n      self.assertAllClose(numpy_result, tf_result)\n\n  def testEmptyOutput(self):\n    make_mask = lambda shape: np.zeros(shape, dtype=bool)\n    for ndims_mask in range(1, 4):\n      for ndims_arr in range(ndims_mask, ndims_mask + 3):\n        for _ in range(3):\n          with self.subTest(ndims_mask=ndims_mask, ndims_arr=ndims_arr, _=_):\n            arr_shape = np.random.randint(1, 5, size=ndims_arr)\n            self.CheckVersusNumpy(ndims_mask, arr_shape, make_mask=make_mask)\n\n  def testWorksWithDimensionsEqualToNoneDuringGraphBuild(self):\n    # The rank of the mask tensor must be specified. This is explained\n    # in the docstring as well.\n    @def_function.function\n    def func(ph_tensor, ph_mask):\n      return array_ops.boolean_mask(ph_tensor, ph_mask)\n\n    f = func.get_concrete_function(\n        tensor_spec.TensorSpec(None, dtypes.int32),\n        tensor_spec.TensorSpec([None], dtypes.bool))\n    arr = np.array([[1, 2], [3, 4]], np.int32)\n    mask = np.array([False, True])\n    masked_tensor = f(arr, mask)\n    self.assertAllEqual(masked_tensor, arr[mask])\n\n  def testMaskDimensionsSetToNoneRaises(self):\n    # The rank of the mask tensor must be specified. This is explained\n    # in the docstring as well.\n    @def_function.function\n    def func(tensor, mask):\n      return array_ops.boolean_mask(tensor, mask)\n\n    with self.assertRaisesRegex(ValueError, \"dimensions must be specified\"):\n      _ = func.get_concrete_function(\n          tensor_spec.TensorSpec([None, 2], dtypes.int32),\n          tensor_spec.TensorSpec(None, dtypes.bool))\n\n  def testMaskHasMoreDimsThanTensorRaises(self):\n    mask = [[True, True], [False, False]]\n    tensor = [1, 2, 3, 4]\n    with self.cached_session():\n      with self.assertRaisesRegex(ValueError, \"incompatible\"):\n        self.evaluate(array_ops.boolean_mask(tensor, mask))\n\n  def testMaskIsScalarRaises(self):\n    mask = True\n    tensor = 1\n    with self.cached_session():\n      with self.assertRaisesRegex(ValueError, \"mask.*scalar\"):\n        self.evaluate(array_ops.boolean_mask(tensor, mask))\n\n  def testMaskShapeDifferentThanFirstPartOfTensorShapeRaises(self):\n    mask = [True, True, True]\n    tensor = [[1, 2], [3, 4]]\n    with self.cached_session():\n      with self.assertRaisesRegex(ValueError, \"incompatible\"):\n        self.evaluate(array_ops.boolean_mask(tensor, mask))\n\n  def testStringMask(self):\n    # Reproduces b/111171330, where the optimized boolean_mask graph would\n    # be incorrectly placed on GPU.\n    config.set_optimizer_experimental_options({\"shape_optimization\": True})\n\n    @def_function.function\n    def func(tile_input):\n      string_tensor = array_ops.tile([[\"hello\"]], tile_input)\n      bool_tensor = array_ops.tile([[True]], tile_input)\n      masked_tensor = array_ops.boolean_mask(string_tensor, bool_tensor)\n      return masked_tensor\n\n    result = func([2, 2])\n    self.assertAllEqual([b\"hello\", b\"hello\", b\"hello\", b\"hello\"], result)\n\n  def testMaskWithAxisTensor(self):\n\n    @def_function.function(autograph=False)\n    def f():\n      return array_ops.boolean_mask([1, 2, 3], [True, False, True],\n                                    axis=constant_op.constant(\n                                        0, dtype=dtypes.int32))\n\n    self.assertAllEqual(self.evaluate(f()), [1, 3])\n\n  def testMaskWithAxisNonConstTensor(self):\n\n    @def_function.function(\n        autograph=False,\n        input_signature=[\n            tensor_spec.TensorSpec(shape=None, dtype=dtypes.int32)\n        ])\n    def f(axis):\n      return array_ops.boolean_mask([1, 2, 3], [True, False, True], axis=axis)\n\n    self.assertAllEqual(\n        self.evaluate(f(constant_op.constant(0, dtype=dtypes.int32))), [1, 3])\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass OperatorShapeTest(test_util.TensorFlowTestCase):\n\n  def testExpandScalar(self):\n    scalar = \"hello\"\n    scalar_expanded = array_ops.expand_dims(scalar, [0])\n    self.assertEqual(scalar_expanded.get_shape(), (1,))\n\n  def testSqueezeScalar(self):\n    scalar = \"hello\"\n    scalar_squeezed = array_ops.squeeze(scalar, ())\n    self.assertEqual(scalar_squeezed.get_shape(), ())\n\n  def testSqueezeMatrix(self):\n    matrix = [[1, 2, 3]]\n    matrix_squeezed = array_ops.squeeze(matrix, [0])\n    self.assertEqual(matrix_squeezed.get_shape(), (3))\n\n    with self.assertRaisesRegex(\n        Exception, \"Can not squeeze dim.1., expected a dimension of 1, got 3\"):\n      matrix_squeezed = array_ops.squeeze(matrix, [1])\n\n  def testSqueezeScalarDim(self):\n    matrix = [[1, 2, 3]]\n    matrix_squeezed = array_ops.squeeze(matrix, 0)\n    self.assertEqual(matrix_squeezed.get_shape(), (3))\n\n  def testExpandDimsWithNonScalarDim(self):\n    with self.assertRaisesRegex(Exception,\n                                \"must be a tensor with a single value\"):\n      array_ops.expand_dims(1, axis=[0, 1])\n\n  def testReshapeWithManyDims(self):\n    with self.assertRaisesRegex(errors.InvalidArgumentError,\n                                \"too many dimensions\"):\n      self.evaluate(\n          array_ops.reshape(\n              tensor=[[1]],\n              shape=constant_op.constant([1 for i in range(254)],\n                                         dtype=dtypes.int64)))\n\n\n@test_util.with_eager_op_as_function\nclass ReverseV2Test(test_util.TensorFlowTestCase):\n\n  def testReverse0DimAuto(self):\n    x_np = 4\n    for use_gpu in [False, True]:\n      with self.subTest(use_gpu=use_gpu):\n        with self.cached_session(use_gpu=use_gpu):\n          x_tf = self.evaluate(array_ops.reverse_v2(x_np, []))\n          self.assertAllEqual(x_tf, x_np)\n\n  def _reverse1DimAuto(self, np_dtype):\n    x_np = np.array([1, 200, 3, 40, 5], dtype=np_dtype)\n\n    for use_gpu in [False, True]:\n      for axis_dtype in [dtypes.int32, dtypes.int64]:\n        with self.subTest(use_gpu=use_gpu, axis_dtype=axis_dtype):\n          x_tf = self.evaluate(\n              array_ops.reverse_v2(x_np,\n                                   constant_op.constant([0], dtype=axis_dtype)))\n          self.assertAllEqual(x_tf, np.asarray(x_np)[::-1])\n\n  def _reverse2DimAuto(self, np_dtype):\n    x_np = np.array([[1, 200, 3], [4, 5, 60]], dtype=np_dtype)\n\n    for reverse_f in [array_ops.reverse_v2, array_ops.reverse]:\n      for use_gpu in [False, True]:\n        for axis_dtype in [dtypes.int32, dtypes.int64]:\n          with self.subTest(\n              reverse_f=reverse_f, use_gpu=use_gpu, axis_dtype=axis_dtype):\n            x_tf_1 = self.evaluate(\n                reverse_f(x_np, constant_op.constant([0], dtype=axis_dtype)))\n            x_tf_2 = self.evaluate(\n                reverse_f(x_np, constant_op.constant([-2], dtype=axis_dtype)))\n            x_tf_3 = self.evaluate(\n                reverse_f(x_np, constant_op.constant([1], dtype=axis_dtype)))\n            x_tf_4 = self.evaluate(\n                reverse_f(x_np, constant_op.constant([-1], dtype=axis_dtype)))\n            x_tf_5 = self.evaluate(\n                reverse_f(x_np, constant_op.constant([1, 0], dtype=axis_dtype)))\n            self.assertAllEqual(x_tf_1, np.asarray(x_np)[::-1, :])\n            self.assertAllEqual(x_tf_2, np.asarray(x_np)[::-1, :])\n            self.assertAllEqual(x_tf_3, np.asarray(x_np)[:, ::-1])\n            self.assertAllEqual(x_tf_4, np.asarray(x_np)[:, ::-1])\n            self.assertAllEqual(x_tf_5, np.asarray(x_np)[::-1, ::-1])\n\n  # This test covers the axis validation in the shape function\n  # (no eval())\n  def testInvalidAxis(self):\n    x_np = np.array([[1, 2, 3], [4, 5, 6]], dtype=np.float32)\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"is out of.* range\"):\n      array_ops.reverse_v2(x_np, [-30])\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"is out of.* range\"):\n      array_ops.reverse_v2(x_np, [2])\n    with self.assertRaisesRegex(\n        (ValueError, errors.InvalidArgumentError),\n        r\"axis 0 specified more than once|axis 0 was repeated\"):\n      array_ops.reverse_v2(x_np, [0, -2])\n\n  # This is the version of reverse that uses axis indices rather than\n  # bool tensors\n  # TODO(b/32254538): Change this test to use array_ops.reverse\n  #\n  # Note: this test passes placeholder as constant axis is validated\n  # in shape function (see testInvalidAxis)\n  def testInvalid(self):\n    x_np = np.array([[1, 2, 3], [4, 5, 6]], dtype=np.float32)\n\n    @def_function.function\n    def func(ax):\n      return array_ops.reverse_v2(x_np, ax)\n\n    with self.assertRaisesRegex((ValueError, errors_impl.InvalidArgumentError),\n                                \"is out of.*range\"):\n      func([-30])\n    with self.assertRaisesRegex((ValueError, errors_impl.InvalidArgumentError),\n                                \"is out of.*range\"):\n      func([2])\n    with self.assertRaisesRegex(\n        (ValueError, errors_impl.InvalidArgumentError),\n        \"(axis 0 specified more than once|canonicalized axis 0 was repeated.)\"):\n      func([0, -2])\n\n  def testReverse1DimAuto(self):\n    for dtype in [\n        np.uint8, np.int8, np.uint16, np.int16, np.uint32, np.int32, np.uint64,\n        np.int64, np.bool_, np.float16, np.float32, np.float64, np.complex64,\n        np.complex128,\n        np.array(b\"\").dtype.type\n    ]:\n      self._reverse1DimAuto(dtype)\n\n  def testReverse2DimAuto(self):\n    for dtype in [\n        np.uint8, np.int8, np.uint16, np.int16, np.uint32, np.int32, np.uint64,\n        np.int64, np.bool_, np.float16, np.float32, np.float64, np.complex64,\n        np.complex128,\n        np.array(b\"\").dtype.type\n    ]:\n      self._reverse2DimAuto(dtype)\n\n  def testReverseRowsOf3Channels(self):\n    \"\"\"Tests optimized code for reversing rows with last dim size = 3.\"\"\"\n    for reverse_f in [array_ops.reverse_v2, array_ops.reverse]:\n      for outer_size in (1, 2):\n        for middle_size in list(range(50)) + [100000]:\n          with self.subTest(\n              reverse_f=reverse_f,\n              outer_size=outer_size,\n              middle_size=middle_size,\n              use_gpu=True):\n            x_np = np.reshape(\n                np.arange(outer_size * middle_size * 3, dtype=np.float32),\n                newshape=(outer_size, middle_size, 3))\n            x_tf = self.evaluate(reverse_f(x_np, [1]))\n            np_answer = x_np[:, ::-1, :]\n            self.assertAllEqual(x_tf, np_answer)\n\n  def testReverseRowsOf4Channels(self):\n    for reverse_f in [array_ops.reverse_v2, array_ops.reverse]:\n      for outer_size in (1, 2):\n        for middle_size in list(range(50)) + [100000]:\n          with self.subTest(\n              reverse_f=reverse_f,\n              outer_size=outer_size,\n              middle_size=middle_size,\n              use_gpu=True):\n            x_np = np.reshape(\n                np.arange(outer_size * middle_size * 4, dtype=np.float32),\n                newshape=(outer_size, middle_size, 4))\n            x_tf = self.evaluate(reverse_f(x_np, [1]))\n            np_answer = x_np[:, ::-1, :]\n            self.assertAllEqual(x_tf, np_answer)\n\n  def testReverseColumnsOf3Channels(self):\n    for reverse_f in [array_ops.reverse_v2, array_ops.reverse]:\n      for outer_size in list(range(50)) + [100000]:\n        for middle_size in (1, 2):\n          with self.subTest(\n              reverse_f=reverse_f,\n              outer_size=outer_size,\n              middle_size=middle_size,\n              use_gpu=True):\n            x_np = np.reshape(\n                np.arange(outer_size * middle_size * 3, dtype=np.float32),\n                newshape=(outer_size, middle_size, 3))\n            x_tf = self.evaluate(reverse_f(x_np, [0]))\n            np_answer = x_np[::-1, :, :]\n            self.assertAllEqual(x_tf, np_answer)\n\n  def testReverseInvalidShape(self):\n    x = np.ndarray(shape=[0, 1, 1])\n    v = array_ops.reverse_v2(x, axis=[1])\n    self.assertAllEqual(self.evaluate(v), v)\n\n\nclass MeshgridTest(test_util.TensorFlowTestCase):\n\n  def _compareDiff(self, x, y, use_gpu):\n    for index in (\"ij\", \"xy\"):\n      numpy_out = np.meshgrid(x, y, indexing=index)\n      tf_out = array_ops.meshgrid(x, y, indexing=index)\n      with self.cached_session(use_gpu=use_gpu):\n        for xx, yy in zip(numpy_out, tf_out):\n          self.assertAllEqual(xx, yy)\n\n  def _compareDiffType(self, n, np_dtype, use_gpu):\n    inputs = []\n    for index in (\"ij\", \"xy\"):\n      for _ in range(n):\n        x = np.linspace(-10, 10, 5).astype(np_dtype)\n        if np_dtype in (np.complex64, np.complex128):\n          x += 1j\n        inputs.append(x)\n      numpy_out = np.meshgrid(*inputs, indexing=index)\n      with test_util.device(use_gpu=use_gpu):\n        tf_out = array_ops.meshgrid(*inputs, indexing=index)\n        for x_np, x_tf in zip(numpy_out, tf_out):\n          self.assertAllEqual(x_np, x_tf)\n\n  def testCompare(self):\n    for t in (np.float16, np.float32, np.float64, np.int32, np.int64,\n              np.complex64, np.complex128):\n      with self.subTest(t=t):\n        self._compareDiffType(2, t, False)\n        self._compareDiffType(3, t, False)\n\n        x = [1, 2, 3]\n        y = [4, 5]\n\n        a = [[1, 1], [1, 1]]\n\n        self._compareDiff(x, y, False)\n        self._compareDiff(x, a, False)\n\n\nclass StridedSliceChecker(object):\n  \"\"\"Check a given tensor against the numpy result.\"\"\"\n\n  REF_TENSOR = np.arange(1, 19, dtype=np.float32).reshape(3, 2, 3)\n  REF_TENSOR_ALIGNED = np.arange(1, 97, dtype=np.float32).reshape(3, 4, 8)\n\n  def __init__(self, test, x, tensor_type=dtypes.int32, check_type_infer=True):\n    self.x_np = np.array(x).astype(tensor_type.as_numpy_dtype)\n    if tensor_type.is_bool:\n      self.x_np = np.array(x % 3).astype(np.bool_)\n    # Give the value a non-zero imaginary component for complex types.\n    if tensor_type.is_complex:\n      self.x_np -= 1j * self.x_np\n    self.test = test\n    self.x = constant_op.constant(self.x_np, dtype=tensor_type)\n    self.check_type_infer = check_type_infer\n\n  def __getitem__(self, spec):\n    op = self.x.__getitem__(spec)\n\n    def eval_if_tensor(x):\n      try:\n        return self.test.evaluate(x)\n      except (AttributeError, TypeError, ValueError):\n        return x\n\n    def casts_to_bool_nparray(x):\n      try:\n        return np.asarray(x).dtype == bool\n      except NotImplementedError:\n        return False\n\n    if isinstance(spec, bool) or \\\n      (isinstance(spec, ops.Tensor) and spec.dtype == dtypes.bool) or \\\n      (isinstance(spec, np.ndarray) and spec.dtype == bool) or \\\n      (isinstance(spec, (list, tuple)) and casts_to_bool_nparray(spec)):\n      tensor = self.test.evaluate(op)\n      np_spec = eval_if_tensor(spec)\n      self.test.assertAllEqual(self.x_np[np_spec], tensor)\n      return tensor\n\n    if not isinstance(spec, (list, tuple)):\n      spec = [spec]\n\n    tensor = self.test.evaluate(op)\n\n    # Make a numpy spec that pre-evals the tensors\n    np_specs = []\n\n    for s in spec:\n      if isinstance(s, slice):\n        start = eval_if_tensor(s.start)\n        stop = eval_if_tensor(s.stop)\n        step = eval_if_tensor(s.step)\n        np_specs.append(slice(start, stop, step))\n      else:\n        np_specs.append(eval_if_tensor(s))\n\n    self.test.assertAllEqual(self.x_np[tuple(np_specs)], tensor)\n    if self.check_type_infer:\n      self.test.assertAllEqual(tensor.shape, op.get_shape())\n    return tensor\n\n\nSTRIDED_SLICE_TYPES = [\n    dtypes.int32, dtypes.int64, dtypes.int16, dtypes.int8, dtypes.uint8,\n    dtypes.float32, dtypes.float64, dtypes.complex64, dtypes.complex128,\n    dtypes.bool\n]\n\n\nclass StridedSliceTest(test_util.TensorFlowTestCase):\n  \"\"\"Test the strided slice operation with variants of slices.\"\"\"\n\n  def test_basic_slice(self):\n    for tensor_type in STRIDED_SLICE_TYPES:\n      with self.subTest(tensor_type=tensor_type, use_gpu=True):\n        checker = StridedSliceChecker(\n            self, StridedSliceChecker.REF_TENSOR, tensor_type=tensor_type)\n        _ = checker[:, :, :]\n        # Various ways of representing identity slice\n        _ = checker[:, :, :]\n        _ = checker[::, ::, ::]\n        _ = checker[::1, ::1, ::1]\n        # Not zero slice\n        _ = checker[::1, ::5, ::2]\n        # Reverse in each dimension independently\n        _ = checker[::-1, :, :]\n        _ = checker[:, ::-1, :]\n        _ = checker[:, :, ::-1]\n        ## negative index tests i.e. n-2 in first component\n        _ = checker[-2::-1, :, ::1]\n        # negative index tests i.e. n-2 in first component, non-unit stride\n        _ = checker[-2::-1, :, ::2]\n\n        # Check rank-0 examples\n        checker2 = StridedSliceChecker(self, 5, tensor_type=tensor_type)\n        _ = checker2[None]\n        _ = checker2[...]\n        _ = checker2[tuple()]\n\n  def testInt64GPU(self):\n    if not test_util.is_gpu_available():\n      self.skipTest(\"No GPU available\")\n\n    with test_util.force_gpu():\n      x = constant_op.constant([1., 2., 3.])\n      begin = constant_op.constant([2], dtype=dtypes.int64)\n      end = constant_op.constant([3], dtype=dtypes.int64)\n      strides = constant_op.constant([1], dtype=dtypes.int64)\n      s = array_ops.strided_slice(x, begin, end, strides)\n      self.assertAllEqual([3.], self.evaluate(s))\n\n  @test_util.assert_no_new_pyobjects_executing_eagerly\n  @test_util.assert_no_garbage_created\n  def testTensorSliceEagerMemory(self):\n    with context.eager_mode():\n      inputs = constant_op.constant([[[1], [2], [3], [4]]],\n                                    dtype=dtypes.float32)\n      # Tests that slicing an EagerTensor doesn't leak memory\n      inputs[0]  # pylint: disable=pointless-statement\n\n  @test_util.assert_no_new_pyobjects_executing_eagerly\n  @test_util.assert_no_garbage_created\n  def testVariableSliceEagerMemory(self):\n    with context.eager_mode():\n      v = variables.Variable([1., 2.])\n      v[0]  # pylint: disable=pointless-statement\n\n  def testDegenerateSlices(self):\n    with test_util.device(use_gpu=True):\n      checker = StridedSliceChecker(self, StridedSliceChecker.REF_TENSOR)\n      # degenerate by offering a forward interval with a negative stride\n      _ = checker[0:-1:-1, :, :]\n      # degenerate with a reverse interval with a positive stride\n      _ = checker[-1:0, :, :]\n      # empty interval in every dimension\n      _ = checker[-1:0, 2:2, 2:3:-1]\n      # empty first dimension only (used to break for aligned tensors).\n      checker = StridedSliceChecker(self,\n                                    StridedSliceChecker.REF_TENSOR_ALIGNED)\n      _ = checker[1:0]\n\n  def testSliceWithUndefinedDimension(self):\n    t = constant_op.constant([1, 2, 3])\n    d = tensor_shape.Dimension(None)\n    self.assertAllEqual(t[d:d:d], t)\n\n  def testEllipsis(self):\n    with test_util.device(use_gpu=True):\n      raw = [[[[[1, 2], [3, 4], [5, 6]]], [[[7, 8], [9, 10], [11, 12]]]]]\n      checker = StridedSliceChecker(self, raw)\n\n      _ = checker[0:]\n      # implicit ellipsis\n      _ = checker[0:, ...]\n      # ellipsis alone\n      _ = checker[...]\n      # ellipsis at end\n      _ = checker[0:1, ...]\n      # ellipsis at begin\n      _ = checker[..., 0:1]\n      # ellipsis at middle\n      _ = checker[0:1, ..., 0:1]\n      # multiple ellipses not allowed\n      with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                  \"Multiple ellipses\"):\n        _ = checker[..., :, ...].eval()\n\n  def testShrink(self):\n    with test_util.device(use_gpu=True):\n      raw = [[[[[1, 2, 4, 5], [5, 6, 7, 8], [9, 10, 11, 12]]],\n              [[[13, 14, 15, 16], [17, 18, 19, 20], [21, 22, 23, 24]]]]]\n      checker = StridedSliceChecker(self, raw)\n      _ = checker[:, :, :, :, 3]\n      _ = checker[..., 3]\n      _ = checker[:, 0]\n      _ = checker[:, :, 0]\n\n  def testBothNewAxisAndShrink(self):\n    with test_util.device(use_gpu=True):\n\n      @def_function.function\n      def func(inp):\n        return inp[array_ops.newaxis, :, 0]\n\n      f = func.get_concrete_function(\n          tensor_spec.TensorSpec([2, 2], dtypes.int16))\n\n      # TODO(b/190416665): Allow the constant to be eagerly copied/created on\n      # the GPU.\n      with ops.device(\"CPU\"):\n        ones = constant_op.constant([[1, 1], [1, 1]], dtypes.int16)\n      self.assertAllEqual([[1, 1]], self.evaluate(f(ones)))\n\n  def testTensorIndexing(self):\n    with test_util.device(use_gpu=True):\n      raw = [[[[[1, 2, 4, 5], [5, 6, 7, 8], [9, 10, 11, 12]]],\n              [[[13, 14, 15, 16], [17, 18, 19, 20], [21, 22, 23, 24]]]]]\n      checker = StridedSliceChecker(self, raw, check_type_infer=False)\n      bar = constant_op.constant(2)\n      bar2 = constant_op.constant(3)\n      _ = checker[..., bar:bar2]\n      _ = checker[..., bar]\n      _ = checker[..., 3]\n      _ = checker[..., 2**64 // 2**63]  # Test longs in Python 2\n\n  def testTensorIndexingTypeError(self):\n    with self.session():\n      checker = StridedSliceChecker(self, StridedSliceChecker.REF_TENSOR)\n      expected = re.escape(array_ops._SLICE_TYPE_ERROR)\n      with self.assertRaisesRegex(TypeError, expected):\n        _ = checker[\"foo\"]\n      with self.assertRaisesRegex(TypeError, expected):\n        _ = checker[constant_op.constant(\"foo\")]\n      with self.assertRaisesRegex(TypeError, expected):\n        _ = checker[0.0]\n      with self.assertRaisesRegex(TypeError, expected):\n        _ = checker[constant_op.constant(0.0)]\n      with self.assertRaisesRegex(TypeError, expected):\n        _ = checker[constant_op.constant([1, 2, 3])]\n      with self.assertRaisesRegex(TypeError, expected):\n        _ = checker[[2.1, -0.7, 1.5]]\n\n  def testExpand(self):\n    with test_util.device(use_gpu=True):\n      raw = [[[[[1, 2, 4, 5], [5, 6, 7, 8], [9, 10, 11, 12]]],\n              [[[13, 14, 15, 16], [17, 18, 19, 20], [21, 22, 23, 24]]]]]\n      checker = StridedSliceChecker(self, raw)\n      # new axis (followed by implicit ellipsis)\n      _ = checker[np.newaxis]\n      # newaxis after ellipsis\n      _ = checker[..., np.newaxis]\n      # newaxis in between ellipsis and explicit range\n      _ = checker[..., np.newaxis, :]\n      _ = checker[:, ..., np.newaxis, :, :]\n      # Reverse final dimension with new axis\n      _ = checker[:, :, np.newaxis, :, 2::-1]\n      # Ellipsis in middle of two newaxis\n      _ = checker[np.newaxis, ..., np.newaxis]\n\n  def testExpandVariable(self):\n    with test_util.device(use_gpu=True):\n      x = variables.Variable(7, dtype=dtypes.int32)\n      self.evaluate(x.initializer)\n      y = self.evaluate(x[None])\n      self.assertEqual(y.shape, (1,))\n      self.assertAllEqual(y, (7,))\n\n  def testOptimizedCases(self):\n    with test_util.device(use_gpu=True):\n      checker = StridedSliceChecker(self,\n                                    StridedSliceChecker.REF_TENSOR_ALIGNED)\n      # Identity\n      _ = checker[:]\n      # Identity\n      _ = checker[...]\n      # Identity\n      _ = checker[np.newaxis, ..., np.newaxis]\n      # First axis slice\n      _ = checker[1:]\n      # First axis slice\n      _ = checker[np.newaxis, 1:]\n\n  def testMasks(self):\n    with test_util.device(use_gpu=True):\n      scalar = np.array(0)\n      # Test tensor type mask\n      checker = StridedSliceChecker(self, StridedSliceChecker.REF_TENSOR)\n      _ = checker[checker.x > 2]\n      _ = checker[checker.x <= 5]\n      _ = checker[ops.convert_to_tensor(scalar)]\n\n      # Test numpy array type mask\n      raw = np.array([[[[[1, 2, 4, 5], [5, 6, 7, 8], [9, 10, 11, 12]]],\n                       [[[13, 14, 15, 16], [17, 18, 19, 20], [21, 22, 23,\n                                                              24]]]]])\n      checker1 = StridedSliceChecker(self, raw)\n      _ = checker1[raw >= 4]\n      _ = checker1[raw < 19]\n      _ = checker1[scalar]\n\n      # Test boolean and non boolean cases\n      mask = np.array([True, False, True])\n      raw1 = np.array([[1, 2, 4, 5], [5, 6, 7, 8], [9, 10, 11, 12]])\n      checker2 = StridedSliceChecker(self, raw1)\n      _ = checker2[mask]\n      _ = checker2[ops.convert_to_tensor(mask)]\n\n  def test_int16_indices(self):\n\n    def _int16(i):\n      return constant_op.constant(i, dtype=dtypes.int16)\n\n    def _int64(i):\n      return constant_op.constant(i, dtype=dtypes.int64)\n\n    for tensor_type in STRIDED_SLICE_TYPES:\n      with self.subTest(tensor_type=tensor_type, use_gpu=True):\n        checker = StridedSliceChecker(\n            self, StridedSliceChecker.REF_TENSOR, tensor_type=tensor_type)\n\n        _ = checker[_int16(1)]\n\n        with self.assertRaises(Exception):\n          _ = checker[_int16(1)::1, :, 1:_int64(3):2]\n        with self.assertRaises(Exception):\n          _ = checker[:, _int16(1):_int16(5):-1, :]\n        with self.assertRaises(Exception):\n          _ = checker[::_int64(1), _int64(1):10:_int16(3), ::_int64(2)]\n\n        _ = checker[::_int16(1), _int16(1)::_int16(5), ::2]\n        _ = checker[_int16(1):_int16(5):_int16(2), 1:2, :]\n\n\nclass StridedSliceShapeTest(test_util.TensorFlowTestCase):\n  \"\"\"Test the shape inference of StridedSliceShapes.\"\"\"\n\n  def testUnknown(self):\n    with test_util.device(use_gpu=True):\n\n      @def_function.function\n      def f(x):\n        y = x[...]\n        self.assertAllEqual(y.get_shape().ndims, None)\n\n      _ = f.get_concrete_function(tensor_spec.TensorSpec(None, dtypes.float32))\n\n  def tensorShapeEqual(self, x, y):\n    self.assertTrue(x is not None and y is not None or x is None and y is None)\n    self.assertEqual(x.as_list(), y.as_list())\n\n  def testTensorShapeUncertain(self):\n    with test_util.device(use_gpu=True):\n\n      @def_function.function\n      def f1(x):\n        y = x[3:5]\n        self.tensorShapeEqual(y.get_shape(),\n                              tensor_shape.TensorShape([2, None, 7]))\n\n      _ = f1.get_concrete_function(\n          tensor_spec.TensorSpec((5, None, 7), dtypes.float32))\n\n      @def_function.function\n      def f2(x):\n        y = x[3:5, :, 4]\n        self.tensorShapeEqual(y.get_shape(), tensor_shape.TensorShape([2,\n                                                                       None]))\n\n      _ = f2.get_concrete_function(\n          tensor_spec.TensorSpec((5, None, 7), dtypes.float32))\n\n      @def_function.function\n      def f3(x):\n        y = x[3:5, 3:4, 4]\n        self.tensorShapeEqual(y.get_shape(), tensor_shape.TensorShape([2,\n                                                                       None]))\n\n      _ = f3.get_concrete_function(\n          tensor_spec.TensorSpec((5, None, 7), dtypes.float32))\n\n      @def_function.function\n      def f4(x):\n        y = x[3:5, :, 5:10]\n        self.tensorShapeEqual(y.get_shape(),\n                              tensor_shape.TensorShape([2, None, 2]))\n\n      _ = f4.get_concrete_function(\n          tensor_spec.TensorSpec((5, None, 7), dtypes.float32))\n\n      @def_function.function\n      def f5(x):\n        y = x[3:5, :, 50:3]\n        self.tensorShapeEqual(y.get_shape(),\n                              tensor_shape.TensorShape([2, None, 0]))\n\n      _ = f5.get_concrete_function(\n          tensor_spec.TensorSpec((5, None, 7), dtypes.float32))\n\n      @def_function.function\n      def f6(x):\n        y = x[3:5, :, array_ops.newaxis, 50:3,]\n        self.tensorShapeEqual(y.get_shape(),\n                              tensor_shape.TensorShape([2, None, 1, 0]))\n\n      _ = f6.get_concrete_function(\n          tensor_spec.TensorSpec((5, None, 7), dtypes.float32))\n\n      @def_function.function\n      def f7(x):\n        y = x[1:5:2, :, array_ops.newaxis, 50:3,]\n        self.tensorShapeEqual(y.get_shape(),\n                              tensor_shape.TensorShape([2, None, 1, 0]))\n\n      _ = f7.get_concrete_function(\n          tensor_spec.TensorSpec((5, None, 7), dtypes.float32))\n\n      @def_function.function\n      def f8(x):\n        y = x[:5:3, :, array_ops.newaxis, 50:3,]\n        self.tensorShapeEqual(y.get_shape(),\n                              tensor_shape.TensorShape([2, None, 1, 0]))\n\n      _ = f8.get_concrete_function(\n          tensor_spec.TensorSpec((5, None, 7), dtypes.float32))\n\n      @def_function.function\n      def f9(x):\n        y = x[:2:3, :, array_ops.newaxis, 50:3,]\n        self.tensorShapeEqual(y.get_shape(),\n                              tensor_shape.TensorShape([1, None, 1, 0]))\n\n      _ = f9.get_concrete_function(\n          tensor_spec.TensorSpec((5, None, 7), dtypes.float32))\n\n      @def_function.function\n      def f10(x):\n        y = x[::-1, :, array_ops.newaxis, ::-2]\n        self.tensorShapeEqual(y.get_shape(),\n                              tensor_shape.TensorShape([5, None, 1, 4]))\n\n      _ = f10.get_concrete_function(\n          tensor_spec.TensorSpec((5, None, 7), dtypes.float32))\n\n  def testTensorValuedIndexShape(self):\n    with self.session():\n\n      @def_function.function\n      def f1(x, y):\n        z = x[y]\n        self.tensorShapeEqual(z.get_shape(), tensor_shape.TensorShape([3, 7]))\n\n      _ = f1.get_concrete_function(\n          tensor_spec.TensorSpec((5, 3, 7)),\n          tensor_spec.TensorSpec((), dtypes.int32))\n\n      @def_function.function\n      def f2(x, y):\n        z = x[y, ::-1]\n        self.tensorShapeEqual(z.get_shape(), tensor_shape.TensorShape([3, 7]))\n\n      _ = f2.get_concrete_function(\n          tensor_spec.TensorSpec((5, 3, 7)),\n          tensor_spec.TensorSpec((), dtypes.int32))\n\n      @def_function.function\n      def f3(x, y):\n        z = x[y, ::-2]\n        self.tensorShapeEqual(z.get_shape(), tensor_shape.TensorShape([2, 7]))\n\n      _ = f3.get_concrete_function(\n          tensor_spec.TensorSpec((5, 3, 7)),\n          tensor_spec.TensorSpec((), dtypes.int32))\n\n      @def_function.function\n      def f4(x, y, s):\n        z = x[y, s:2]\n        self.tensorShapeEqual(z.get_shape(), tensor_shape.TensorShape([None,\n                                                                       7]))\n\n      _ = f4.get_concrete_function(\n          tensor_spec.TensorSpec((5, 3, 7)),\n          tensor_spec.TensorSpec((), dtypes.int32),\n          tensor_spec.TensorSpec((), dtypes.int32))\n\n\nclass GradSliceChecker(object):\n  \"\"\"Tests that we can compute a gradient for var^2.\"\"\"\n\n  def __init__(self, test, var, varnp, use_tape):\n    self.test = test\n    self.var = var\n    self.varnp = varnp\n    self.use_tape = use_tape\n\n  def __getitem__(self, spec):\n    with test_util.AbstractGradientTape(\n        use_tape=self.use_tape, persistent=True) as tape:\n      tape.watch(self.var)\n      val = self.var * self.var\n      slice_var = self.var[spec]\n      slice_val = val[spec]\n\n      # compute analytic 2nd derivative\n      analytic_grad2 = 2 * slice_val\n\n      dy = variables.Variable(\n          array_ops.ones_like(slice_var, dtype=dtypes.float32))\n      assign = dy.assign(slice_var)\n\n      slice_val_grad = tape.gradient(slice_val, self.var, [dy])\n      slice_val_grad2 = tape.gradient(slice_val_grad, dy, [self.var])\n    self.test.evaluate(assign)\n    slice_val_grad_evaled, slice_val_grad2_evaled = (\n        self.test.evaluate([slice_val_grad, slice_val_grad2]))\n    analytic_grad2_evaled = self.test.evaluate(analytic_grad2)\n    self.test.assertAllEqual(slice_val_grad2_evaled, analytic_grad2_evaled)\n\n    # compute analytic gradient for slice\n    np_val_grad = (2 * self.varnp * self.varnp)\n    np_sliceval_grad = np.zeros(self.var.get_shape())\n    if isinstance(spec, ops.Tensor):\n      spec = self.test.evaluate([spec])\n    np_sliceval_grad[spec] = np_val_grad[spec]\n    # verify gradient\n    self.test.assertAllEqual(slice_val_grad_evaled, np_sliceval_grad)\n\n\nclass StridedSliceGradTest(test_util.TensorFlowTestCase,\n                           parameterized.TestCase):\n  \"\"\"Test that strided slice's custom gradient produces correct gradients.\"\"\"\n\n  @parameterized.parameters(set((True, context.executing_eagerly())))\n  @test_util.disable_xla(\n      \"b/210077724: Auto-clustering with where op isn't supported. Has loose \"\n      \"output shape bounds\")\n  def testGradient(self, use_tape):\n    with test_util.device(use_gpu=True):\n      var = variables.Variable(\n          array_ops.reshape(\n              math_ops.range(1, 97, 1, dtype=dtypes.float32), shape=(6, 4, 4)))\n      self.evaluate(var.initializer)\n\n      raw = np.array(range(1, 97, 1)).reshape((6, 4, 4))\n      grad = GradSliceChecker(self, var, raw, use_tape)\n      _ = grad[2:6:2, 1:3, 1:3]\n      _ = grad[3:0:-2, 1:3, 1:3]\n      _ = grad[3:0:-2, array_ops.newaxis, 1:3, 2, array_ops.newaxis]\n      _ = grad[3:0:-2, 1:3, 2]\n      _ = grad[:, -1, :]\n      _ = grad[:, -2, :]\n      with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                  \"out of bounds\"):\n        _ = grad[:, -200, :]\n      with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                  \"out of bounds\"):\n        _ = grad[:, 200, :]\n\n      # Test numpy array type mask\n      _ = grad[raw > 51]\n      # Test tensor type mask\n      _ = grad[ops.convert_to_tensor(raw) <= 76]\n\n  @parameterized.parameters(set((True, context.executing_eagerly())))\n  def testGradientZero(self, use_tape):\n    with test_util.device(use_gpu=True):\n      var = variables.Variable(8.)\n      self.evaluate(var.initializer)\n      grad = GradSliceChecker(self, var, np.array(8), use_tape)\n      _ = grad[tuple()]\n\n  @parameterized.parameters(set((True, context.executing_eagerly())))\n  def testInt64Indices(self, use_tape):\n    with test_util.AbstractGradientTape(use_tape=use_tape) as tape:\n      a = math_ops.range(3, dtype=dtypes.float32)\n      tape.watch(a)\n      index = constant_op.constant(1, dtype=dtypes.int64)\n      b = 2. * a[index]\n    grad = tape.gradient(b, a)\n    self.assertAllEqual(self.evaluate(grad), [0., 2., 0.])\n\n\nclass StridedSliceGradTypeTest(test_util.TensorFlowTestCase):\n  \"\"\"Test varied index types and host located memory.\"\"\"\n\n  def testHostVsDevice(self):\n    var2 = variables.Variable(\n        array_ops.reshape(\n            math_ops.cast(math_ops.range(1, 5, 1), dtypes.float32),\n            shape=(4, 1, 1)))\n    varshape = variables.Variable([6, 4, 4], dtype=dtypes.int32)\n    begin = constant_op.constant([0, 0, 0])\n    end = constant_op.constant([4, 1, 1])\n    strides = constant_op.constant([1, 1, 1])\n    foo = array_ops.strided_slice_grad(varshape, begin, end, strides, var2)\n    self.evaluate(var2.initializer)\n    self.evaluate(varshape.initializer)\n    self.evaluate(foo)\n\n  def testInt64Shape(self):\n    original_dy = array_ops.reshape(\n        math_ops.cast(math_ops.range(1, 5, 1), dtypes.float32), shape=(4, 1, 1))\n    original_shape = constant_op.constant([6, 4, 4], dtype=dtypes.int64)\n    begin = constant_op.constant([0, 0, 0], dtype=dtypes.int64)\n    end = constant_op.constant([4, 1, 1], dtype=dtypes.int64)\n    strides = constant_op.constant([1, 1, 1], dtype=dtypes.int64)\n    dx = array_ops.strided_slice_grad(original_shape, begin, end, strides,\n                                      original_dy)\n    self.evaluate(dx)\n\n  def testMixedIndexTypes(self):\n    original_dy = array_ops.reshape(\n        math_ops.cast(math_ops.range(1, 5, 1), dtypes.float32), shape=(4, 1, 1))\n    original_shape = constant_op.constant([6, 4, 4], dtype=dtypes.int64)\n    begin = constant_op.constant([0, 0, 0], dtype=dtypes.int32)\n    end = constant_op.constant([4, 1, 1], dtype=dtypes.int64)\n    strides = constant_op.constant([1, 1, 1], dtype=dtypes.int64)\n    with self.assertRaises((TypeError, errors_impl.InvalidArgumentError)):\n      dx = array_ops.strided_slice_grad(original_shape, begin, end, strides,\n                                        original_dy)\n      self.evaluate(dx)\n\n\nclass BenchmarkSlice(object):\n\n  def __init__(self, tensor):\n    self.tensor = tensor\n\n  def __getitem__(self, x):\n    return self.tensor[x]\n\n\nclass StridedSliceBenchmark(test_lib.Benchmark):\n  \"\"\"Benchmark new strided slice operation on non-trivial case.\"\"\"\n\n  def run_and_time(self, slice_op):\n    self.evaluate(variables.global_variables_initializer())\n    for _ in range(10):\n      _ = self.evaluate(slice_op)\n    iters = 1000\n    t0 = time.time()\n    for _ in range(iters):\n      self.evaluate(slice_op)\n    t1 = time.time()\n    self.report_benchmark(iters=iters, wall_time=(t1 - t0) / 1000.0)\n\n  def make_variable(self):\n    n = 256\n    shape = (n, n, n)\n    items = n**3\n    var = variables.Variable(\n        array_ops.reshape(math_ops.linspace(1., float(items), items), shape),\n        dtype=dtypes.float32)\n    return var\n\n  def benchmark_strided_slice_skip(self):\n    with session.Session():\n      var = self.make_variable()\n      helper = BenchmarkSlice(var)\n      slice_op = helper[::2, ::1, ::2]\n      self.run_and_time(slice_op)\n\n  def benchmark_strided_slice_easy(self):\n    with session.Session():\n      var = self.make_variable()\n      helper = BenchmarkSlice(var)\n      slice_op = helper[3::1, 3::1, 3::1]\n      self.run_and_time(slice_op)\n\n  def benchmark_slice_easy(self):\n    with session.Session():\n      var = self.make_variable()\n      slice_op = var[3::1, 3::1, 3::1]\n      self.run_and_time(slice_op)\n\n\nclass StridedSliceAssignChecker(object):\n\n  def __init__(self, test, x, tensor_type=dtypes.float32, use_resource=False):\n    self.tensor_type = tensor_type\n    self.test = test\n    self._use_resource = use_resource\n\n    self.x_np = np.array(x).astype(tensor_type.as_numpy_dtype)\n    # Give the value a non-zero imaginary component for complex types.\n    if tensor_type.is_complex:\n      self.x_np -= 1j * self.x_np\n    self.x = constant_op.constant(self.x_np, dtype=tensor_type)\n\n  def __setitem__(self, index, value):\n    value = np.array(value).astype(self.tensor_type.as_numpy_dtype)\n    # Give the value a non-zero imaginary component for complex types.\n    if self.tensor_type.is_complex:\n      value -= 1j * value\n\n    with test_util.device(use_gpu=True):\n      if self._use_resource:\n        var = resource_variable_ops.ResourceVariable(self.x)\n      else:\n        var = variables.Variable(self.x)\n      self.test.evaluate(var.initializer)\n      val = self.test.evaluate(var[index].assign(value))\n      # val_copy is used to check that tf.compat.v1.assign works equivalently\n      # to the assign method above.\n      val_copy = self.test.evaluate(state_ops.assign(var[index], value))\n      valnp = np.copy(self.x_np)\n      valnp[index] = np.array(value)\n      self.test.assertAllEqual(val, valnp)\n      self.test.assertAllEqual(val_copy, valnp)\n\n\nclass SliceAssignTest(test_util.TensorFlowTestCase, parameterized.TestCase):\n\n  def testInvalidSlice(self):\n    foo = constant_op.constant([1, 2, 3])\n    with self.assertRaisesRegex(AttributeError, \"no attribute 'assign'\"):\n      bar = foo[:2].assign(constant_op.constant([1, 2]))\n      self.evaluate(bar)\n\n  def doTestSliceAssign(self, use_resource):\n    for dtype in STRIDED_SLICE_TYPES:\n      with self.subTest(dtype=dtype):\n        checker = StridedSliceAssignChecker(\n            self, [[1, 2, 3], [4, 5, 6]],\n            use_resource=use_resource,\n            tensor_type=dtype)\n        # Check if equal\n        checker[:] = [[10, 20, 30], [40, 50, 60]]\n        # Check trivial (1,1) shape tensor\n        checker[1:2, 1:2] = [[66]]\n        # shrinks shape changes\n        checker[1:2, 1] = [66]\n        checker[1, 1:2] = [66]\n        checker[1, 1] = 66\n        # newaxis shape changes\n        checker[:, None, :] = [[[10, 20, 30]], [[40, 50, 50]]]\n        # shrink and newaxis\n        checker[None, None, 0, 0:1] = [[[99]]]\n        # Non unit strides\n        checker[::1, ::-2] = [[3, 33], [4, 44]]\n        # degenerate interval\n        checker[8:10, 0] = []\n        checker[8:10, 8:10] = [[]]\n    # Assign vector to scalar (rank-0) using newaxis\n    checker2 = StridedSliceAssignChecker(self, 222)\n    checker2[()] = 6  # no indices\n    checker2[...] = 6  # ellipsis\n    checker2[None] = [6]  # new axis\n\n  @test_util.disable_xla(\"b/123559667\")\n  def testSliceAssign(self):\n    self.doTestSliceAssign(use_resource=False)\n\n  @test_util.disable_xla(\"b/123559667\")\n  def testSliceAssignResource(self):\n    self.doTestSliceAssign(use_resource=True)\n\n  def testTypeError(self):\n    init_val = constant_op.constant([1, 2], dtype=dtypes.int32)\n    too_small_val = constant_op.constant([3, 4], dtype=dtypes.int8)\n    too_large_val = constant_op.constant([3, 4], dtype=dtypes.int64)\n    v = variables.VariableV1(init_val)\n    with self.assertRaises((ValueError, TypeError)):\n      self.evaluate(v[:].assign(too_small_val))\n    with self.assertRaises((ValueError, TypeError)):\n      self.evaluate(v[:].assign(too_large_val))\n\n  def testTypeErrorResource(self):\n    init_val = constant_op.constant([1, 2], dtype=dtypes.int32)\n    too_small_val = constant_op.constant([3, 4], dtype=dtypes.int8)\n    too_large_val = constant_op.constant([3, 4], dtype=dtypes.int64)\n    v = resource_variable_ops.ResourceVariable(init_val)\n    self.evaluate(v.initializer)\n    with self.assertRaises(ValueError):\n      self.evaluate(v[:].assign(too_large_val))\n    with self.assertRaises(ValueError):\n      self.evaluate(v[:].assign(too_small_val))\n\n  @test_util.disable_xla(\"b/123559667\")\n  @test_util.run_in_graph_and_eager_modes\n  def testTensorStridedSliceUpdateWithInputForward(self):\n    \"\"\"Tests tensor_strided_slice_update with input-forwarding taking effect.\"\"\"\n    @def_function.function\n    def assign(x):\n      y = x + 1\n      return gen_array_ops.tensor_strided_slice_update(y, [0], [1], [1], [0])\n    self.assertAllEqual([0, 1], self.evaluate(assign(array_ops.zeros([2]))))\n\n  @test_util.disable_xla(\"b/123559667\")\n  @test_util.run_in_graph_and_eager_modes\n  def testTensorStridedSliceUpdateNoInputForward(self):\n    \"\"\"Tests tensor_strided_slice_update with no input-forwarding.\"\"\"\n    x = constant_op.constant([0.2, 0.3])\n    y = x + 1\n    # y's buffer won't be forwarded to z because y and z will be alive at the\n    # same time later.\n    z = gen_array_ops.tensor_strided_slice_update(y, [0], [1], [1], [0.4])\n    ans = y + z\n    self.assertAllClose([1.6, 2.6], self.evaluate(ans))\n\n  @test_util.disable_xla(\"b/123559667\")\n  def testTensorStridedSliceUpdateGradSimple(self):\n    original = constant_op.constant([0.2, 0.3])\n    updates = constant_op.constant([0.4])\n    with backprop.GradientTape() as tape:\n      tape.watch([original, updates])\n      updated = gen_array_ops.tensor_strided_slice_update(\n          original, [0], [1], [1], updates)\n    d1, d2 = tape.gradient(updated, [original, updates],\n                           output_gradients=constant_op.constant([2.0, 3.0]))\n    self.assertAllClose([0.0, 3.0], d1)\n    self.assertAllClose([2.0], d2)\n\n  @parameterized.named_parameters(\n      (\"_%s\" % i, *args) for i, args in enumerate([  # pylint:disable=g-complex-comprehension\n          ([2, 5], [0, 1], [1, 0], [1, 2], [2], 0, 2, 0, 0, 1),\n          ([4], [5], [3], [1], [3], 1, 0, 0, 0, 0),\n          ([2, 2, 3, 2], [0, 0, 1], [1, 0, 2], [1, 0, 1], [2, 3], 0, 0, 2, 0, 5)\n      ]))\n  @test_util.disable_xla(\"b/123559667\")\n  def testTensorStridedSliceUpdateGrad(\n      self, shape, begin, end, strides, updates_shape, *args):\n    with self.cached_session():\n      def f(a, b):\n        return gen_array_ops.tensor_strided_slice_update(\n            a, begin, end, strides, b, *args)\n      theoretical, numerical = gradient_checker_v2.compute_gradient(\n          f, [array_ops.zeros(shape), array_ops.ones(updates_shape)], delta=1.0)\n      self.assertAllClose(theoretical, numerical)\n\n\nclass ShapeSizeRankTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def testDenseShape(self):\n    t_value = [[0, 42], [24, 0]]\n    self.assertAllEqual((2, 2), self.evaluate(array_ops.shape(t_value)))\n    self.assertEqual(4, self.evaluate(array_ops.size(t_value)))\n    self.assertEqual(2, self.evaluate(array_ops.rank(t_value)))\n\n    t = constant_op.constant(t_value)\n    self.assertAllEqual((2, 2), self.evaluate(array_ops.shape(t)))\n    self.assertEqual(4, self.evaluate(array_ops.size(t)))\n    self.assertEqual(2, self.evaluate(array_ops.rank(t)))\n\n  @test_util.run_in_graph_and_eager_modes\n  def testSparseShape(self):\n    sp_value = sparse_tensor.SparseTensorValue(\n        indices=((0, 1), (1, 0)), values=(42, 24), dense_shape=(2, 2))\n    self.assertAllEqual((2, 2), self.evaluate(array_ops.shape(sp_value)))\n    self.assertEqual(4, self.evaluate(array_ops.size(sp_value)))\n    self.assertEqual(2, self.evaluate(array_ops.rank(sp_value)))\n\n    sp = sparse_tensor.SparseTensor.from_value(sp_value)\n    self.assertAllEqual((2, 2), self.evaluate(array_ops.shape(sp)))\n    self.assertEqual(4, self.evaluate(array_ops.size(sp)))\n    self.assertEqual(2, self.evaluate(array_ops.rank(sp)))\n\n  @test_util.run_in_graph_and_eager_modes\n  def testSizeDtype(self):\n    tensor = [1]\n    self.assertEqual(dtypes.int32, self.evaluate(array_ops.size(tensor)).dtype)\n    self.assertEqual(\n        dtypes.int64,\n        self.evaluate(array_ops.size(tensor, out_type=dtypes.int64)).dtype)\n\n\nclass SequenceMaskTest(test_util.TensorFlowTestCase):\n\n  def testExceptions(self):\n    with self.cached_session():\n      with self.assertRaisesRegex(ValueError, \"`maxlen` must be scalar\"):\n        array_ops.sequence_mask([10, 20], [10, 20])\n\n  def testOneDimensionalWithMaxlen(self):\n    res = array_ops.sequence_mask(constant_op.constant([1, 3, 2]), 5)\n    self.assertAllEqual(res.get_shape(), [3, 5])\n    self.assertAllEqual(\n        res,\n        [[True, False, False, False, False], [True, True, True, False, False],\n         [True, True, False, False, False]])\n\n  def testOneDimensionalDtypeWithoutMaxlen(self):\n    # test dtype and default maxlen:\n    res = array_ops.sequence_mask(\n        constant_op.constant([0, 1, 4]), dtype=dtypes.float32)\n    self.assertAllEqual(res.get_shape().as_list(), [3, 4])\n    self.assertAllEqual(\n        res, [[0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 1.0, 1.0, 1.0]])\n\n  def testOneDimensionalWithoutMaxlen(self):\n    res = array_ops.sequence_mask(constant_op.constant([0, 1, 4]))\n    self.assertAllEqual(res.get_shape().as_list(), [3, 4])\n    self.assertAllEqual(res,\n                        [[False, False, False, False],\n                         [True, False, False, False], [True, True, True, True]])\n\n  def testTwoDimensional(self):\n    res = array_ops.sequence_mask(constant_op.constant([[1, 3, 2]]), 5)\n    self.assertAllEqual(res.get_shape(), [1, 3, 5])\n    self.assertAllEqual(\n        res,\n        [[[True, False, False, False, False], [True, True, True, False, False],\n          [True, True, False, False, False]]])\n\n    # test dtype and default maxlen:\n    res = array_ops.sequence_mask(\n        constant_op.constant([[0, 1, 4], [1, 2, 3]]), dtype=dtypes.float32)\n    self.assertAllEqual(res.get_shape().as_list(), [2, 3, 4])\n    self.assertAllEqual(\n        res,\n        [[[0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 1.0, 1.0, 1.0]],\n         [[1.0, 0.0, 0.0, 0.0], [1.0, 1.0, 0.0, 0.0], [1.0, 1.0, 1.0, 0.0]]])\n\n  def testDtypes(self):\n\n    def check_dtypes(lengths_dtype, maxlen_dtype):\n      res = array_ops.sequence_mask(\n          constant_op.constant([1, 3, 2], dtype=lengths_dtype),\n          constant_op.constant(5, dtype=maxlen_dtype))\n      self.assertAllEqual(res.get_shape(), [3, 5])\n      self.assertAllEqual(\n          res,\n          [[True, False, False, False, False], [True, True, True, False, False],\n           [True, True, False, False, False]])\n\n    check_dtypes(dtypes.int32, dtypes.int32)\n    check_dtypes(dtypes.int32, dtypes.int64)\n    check_dtypes(dtypes.int64, dtypes.int32)\n    check_dtypes(dtypes.int64, dtypes.int64)\n\n  def testOutputDtype(self):\n\n    def check_output_dtype(output_dtype):\n      res = self.evaluate(\n          array_ops.sequence_mask(\n              constant_op.constant([1, 3, 2], dtype=dtypes.int32),\n              constant_op.constant(5, dtype=dtypes.int32),\n              dtype=output_dtype))\n      self.assertAllEqual(\n          res,\n          self.evaluate(\n              math_ops.cast([[True, False, False, False, False],\n                             [True, True, True, False, False],\n                             [True, True, False, False, False]], output_dtype)))\n\n    check_output_dtype(dtypes.bool)\n    check_output_dtype(\"bool\")\n    check_output_dtype(np.bool_)\n    check_output_dtype(dtypes.int32)\n    check_output_dtype(\"int32\")\n    check_output_dtype(np.int32)\n    check_output_dtype(dtypes.float32)\n    check_output_dtype(\"float32\")\n    check_output_dtype(np.float32)\n    check_output_dtype(dtypes.int64)\n    check_output_dtype(\"float64\")\n    check_output_dtype(np.float64)\n\n\nclass ConcatSliceResourceTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConcatSlice(self):\n    r1 = test_ops.stub_resource_handle_op(container=\"a\", shared_name=\"b\")\n    r2 = test_ops.stub_resource_handle_op(container=\"a\", shared_name=\"c\")\n    c = array_ops.stack([r1, r2])\n    s = array_ops.strided_slice(c, [1], [2])\n    self.evaluate(test_ops.resource_create_op(s))\n    with self.assertRaises(errors.AlreadyExistsError):\n      self.evaluate(test_ops.resource_create_op(r2))\n\n\nclass IdentityTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_gpu_only\n  def testEagerIdentity(self):\n    with context.eager_mode():\n\n      def _test(x, y, device):\n        self.assertAllEqual(x.numpy(), y.numpy())\n        self.assertTrue(device in y.device.lower())\n\n      with test_util.force_gpu():\n        a = constant_op.constant([[2], [3]], dtype=dtypes.float32)\n      with test_util.force_gpu():\n        b = array_ops.identity(a)\n        _test(a, b, \"gpu\")\n      with test_util.force_cpu():\n        c = array_ops.identity(b)\n        _test(b, c, \"cpu\")\n      with test_util.force_cpu():\n        d = array_ops.identity(c)\n        _test(c, d, \"cpu\")\n      with test_util.force_gpu():\n        e = array_ops.identity(d)\n        _test(d, e, \"gpu\")\n\n\nclass PadTest(test_util.TensorFlowTestCase):\n\n  def testEager(self):\n    with context.eager_mode():\n      t = constant_op.constant([[1, 2, 3], [4, 5, 6]])\n      paddings = constant_op.constant([[\n          1,\n          1,\n      ], [2, 2]])\n      padded = array_ops.pad(t, paddings, \"CONSTANT\")\n      self.assertAllEqual(padded.numpy(),\n                          [[0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 2, 3, 0, 0],\n                           [0, 0, 4, 5, 6, 0, 0], [0, 0, 0, 0, 0, 0, 0]])\n\n  def testSymmetricMirrorPadGrad(self):\n    t = np.broadcast_to(np.arange(0, 7), (3, 2, 1, 7))\n    paddings = constant_op.constant([\n        [1, 1],\n        [0, 0],\n        [0, 0],\n        [2, 2],\n    ])\n    expected = np.broadcast_to(np.array([9, 27, 27]), (1, 2, 1, 3))\n    result = gen_array_ops.mirror_pad_grad(t, paddings, \"SYMMETRIC\")\n    self.assertAllEqual(result, expected)\n\n  def testReflectMirrorPadGrad(self):\n    t = np.broadcast_to(np.reshape(np.arange(0, 7), (7, 1)), (1, 4, 7, 1))\n    paddings = constant_op.constant([\n        [0, 0],\n        [1, 1],\n        [2, 2],\n        [0, 0],\n    ])\n    expected = np.broadcast_to(\n        np.reshape(np.array([16, 18, 8]), (3, 1)), (1, 2, 3, 1))\n    result = gen_array_ops.mirror_pad_grad(t, paddings, \"REFLECT\")\n    self.assertAllEqual(result, expected)\n\n\nclass InvertPermutationTest(test_util.TensorFlowTestCase):\n\n  def testInvertPermutation(self):\n    for dtype in [dtypes.int32, dtypes.int64]:\n      with self.subTest(dtype=dtype, use_gpu=True):\n        x = constant_op.constant([3, 4, 0, 2, 1], dtype=dtype)\n        y = array_ops.invert_permutation(x)\n        self.assertAllEqual(y.get_shape(), [5])\n        self.assertAllEqual(y, [2, 4, 3, 0, 1])\n\n\nclass UnravelIndexTest(test_util.TensorFlowTestCase):\n\n  # TODO(b/73086570): Reenable test.\n  @unittest.skip(\"Test does not pass internally.\")\n  def testUnravelIndex(self):\n    with self.cached_session():\n      for dtype in [dtypes.int32, dtypes.int64]:\n        with self.subTest(dtype=dtype):\n          indices_1 = constant_op.constant(1621, dtype=dtype)\n          dims_1 = constant_op.constant([6, 7, 8, 9], dtype=dtype)\n          out_1 = array_ops.unravel_index(indices_1, dims_1)\n          self.assertAllEqual(out_1, [3, 1, 4, 1])\n\n          indices_2 = constant_op.constant([1621], dtype=dtype)\n          dims_2 = constant_op.constant([6, 7, 8, 9], dtype=dtype)\n          out_2 = array_ops.unravel_index(indices_2, dims_2)\n          self.assertAllEqual(out_2, [[3], [1], [4], [1]])\n\n          indices_3 = constant_op.constant([22, 41, 37], dtype=dtype)\n          dims_3 = constant_op.constant([7, 6], dtype=dtype)\n          out_3 = array_ops.unravel_index(indices_3, dims_3)\n          self.assertAllEqual(out_3, [[3, 6, 6], [4, 5, 1]])\n\n  # Test case for GitHub issue 40204.\n  def testUnravelIndexZeroDim(self):\n    with self.cached_session():\n      for dtype in [dtypes.int32, dtypes.int64]:\n        with self.assertRaisesRegex(errors.InvalidArgumentError,\n                                    \"dims cannot contain a dim of zero\"):\n          indices = constant_op.constant([2, 5, 7], dtype=dtype)\n          dims = constant_op.constant([3, 0], dtype=dtype)\n          self.evaluate(array_ops.unravel_index(indices=indices, dims=dims))\n\n  def testUnravelIndexIntegerOverflow(self):\n    with self.cached_session():\n      for dtype in [dtypes.int32, dtypes.int64]:\n        with self.assertRaisesRegex(\n            errors.InvalidArgumentError,\n            r\"Input dims product is causing integer overflow\"):\n          indices = constant_op.constant(-0x100000, dtype=dtype)\n          if dtype == dtypes.int32:\n            value = 0x10000000\n          else:\n            value = 0x7FFFFFFFFFFFFFFF\n          dims = constant_op.constant([value, value], dtype=dtype)\n          self.evaluate(array_ops.unravel_index(indices=indices, dims=dims))\n\n\nclass GuaranteeConstOpTest(test_util.TensorFlowTestCase):\n\n  def testSimple(self):\n    a = array_ops.constant(10)\n    guarantee_a = array_ops.guarantee_const(a)\n    self.assertEqual(10, self.evaluate(guarantee_a))\n\n  def testVariables(self):\n    for use_resource in [False, True]:\n      with self.subTest(use_resource=use_resource):\n        a = variable_scope.get_variable(\n            \"var_{}\".format(use_resource), [],\n            initializer=init_ops.constant_initializer(10.0),\n            use_resource=use_resource)\n        guarantee_a = array_ops.guarantee_const(a)\n        self.evaluate(a.initializer)\n        self.assertEqual(10.0, self.evaluate(guarantee_a))\n\n  def testResourceRejection(self):\n    with ops.device(\"/cpu:0\"):\n      a = variable_scope.get_variable(\n          \"resource_var\", [],\n          initializer=init_ops.constant_initializer(10.0),\n          use_resource=True)\n    with self.assertRaisesWithPredicateMatch(errors.InvalidArgumentError,\n                                             \"cannot be a resource variable\"):\n      guarantee_a = array_ops.guarantee_const(a.handle)\n      self.evaluate(a.initializer)\n      self.evaluate(guarantee_a)\n\n\nclass SnapshotOpTest(test_util.TensorFlowTestCase):\n\n  def testInvertPermutation(self):\n    for dtype in [dtypes.int32, dtypes.int64, dtypes.float32, dtypes.float64]:\n      with self.subTest(dtype=dtype, use_gpu=True):\n        x = constant_op.constant([0, 1, 2, 3], dtype=dtype)\n        y = gen_array_ops.snapshot(x)\n        self.assertAllEqual(y, [0, 1, 2, 3])\n\n\n@test_util.with_eager_op_as_function\n@test_util.run_all_in_graph_and_eager_modes\nclass QuantizeAndDequantizeTest(test_util.TensorFlowTestCase):\n\n  # Generates a tensor of the specified `shape` using values from `values`\n  # scaled by (slice_idx + 1) along `axis` dimension.\n  def _scale_per_slice(self, shape, axis, values):\n    # Note: repeats the values if the shape is larger than values.\n    out = np.take(values, np.remainder(np.arange(np.prod(shape)),\n                                       len(values))).reshape(shape)\n    if axis is not None:\n      scale_shape = [1] * len(shape)\n      scale_shape[axis] = shape[axis]\n      out *= np.arange(1, shape[axis] + 1).reshape(scale_shape)\n    return out\n\n  def testAxis(self):\n    shape = np.array([2, 3, 4, 5])\n    values = np.array([-1, -0.5, 0, 0.3, 0.8, 0.555, 0.5], dtype=np.float32)\n    quant_values = np.array(\n        [-1, -0.5, 0, 38.0 / 128, 102.0 / 128, 71.0 / 128, 0.5],\n        dtype=np.float32)\n    for axis in [None, 0, 1, 2, 3]:\n      with self.subTest(axis=axis):\n        inputs = constant_op.constant(\n            self._scale_per_slice(shape, axis, values))\n        expected = self._scale_per_slice(shape, axis, quant_values)\n        unused_minmax_value = 0 if axis is None else [0] * shape[axis]\n        fake_quantized = self.evaluate(\n            array_ops.quantize_and_dequantize_v2(\n                inputs,\n                unused_minmax_value,\n                unused_minmax_value,\n                range_given=False,\n                round_mode=\"HALF_UP\",\n                axis=axis))\n        self.assertAllEqual(fake_quantized, expected)\n        if axis is not None:\n          fake_quantized = self.evaluate(\n              array_ops.quantize_and_dequantize_v2(\n                  inputs,\n                  unused_minmax_value,\n                  unused_minmax_value,\n                  range_given=False,\n                  axis=(axis - 4)))\n          self.assertAllClose(fake_quantized, expected)\n\n  def testBadAxis(self):\n    input_tensor = [2.5, 2.5]\n    input_min = [0, 0]\n    input_max = [1, 1]\n    # When eager_op_as_function mode is enabled XLA auto-clustering kicks in.\n    # XLA raises an UnimplementedError on invalid axis.\n    error_message_pattern = (r\"Shape must be at least rank 11 but is rank \"\n                             r\"1|invalid axis\")\n    # TODO(b/171260356): Eager mode and graph mode throw different error types\n    error = (errors.InvalidArgumentError, ValueError, errors.UnimplementedError)\n    with self.assertRaisesRegex(error, error_message_pattern):\n      self.evaluate(\n          array_ops.quantize_and_dequantize_v2(\n              input=input_tensor,\n              input_min=input_min,\n              input_max=input_max,\n              axis=10))\n\n  def testQuantizeDequantizeGrad(self):\n    shape = (2, 2)\n    max_threshold = 0\n    min_threshold = -10\n    input_value = np.random.rand(2, 2) * 40.0 - 20.0\n    input_tensor = constant_op.constant(input_value, shape=shape,\n                                        name=\"input_tensor\")\n    with self.cached_session():\n      def f(a):\n        return array_ops.quantize_and_dequantize_v2(\n            a,\n            input_min=min_threshold,\n            input_max=max_threshold,\n            range_given=True)\n      output_grad = gradient_checker_v2.compute_gradient(f, [input_tensor])\n      self.assertAllClose(output_grad[0], np.zeros([1, 4, 4]))\n\n  def testOutOfBoundAxis(self):\n    input_tensor = constant_op.constant([1., 1.])\n    input_min = [0]\n    input_max = [1]\n    q_input, _, _ = array_ops.quantize(input_tensor, 0, 1, dtypes.qint32)\n    error = (errors.InvalidArgumentError, ValueError)\n    with self.assertRaisesRegex(error,\n                                r\".*Axis must be less than input dimension.*\"):\n      self.evaluate(\n          gen_array_ops.dequantize(\n              input=q_input,\n              min_range=input_min,\n              max_range=input_max,\n              axis=2**31 - 1))\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass SortedSearchTest(test_util.TensorFlowTestCase):\n\n  def testUpperBoundFloatHandCoded(self):\n    cdf = np.array([0, .2, .5, .6, .8, 1.], dtype=np.float32)\n    arr = np.array([.04, .99, .53, .58, .31, .01, .79, .8, .21],\n                   dtype=np.float32)\n    result = np.searchsorted(cdf, arr, side=\"right\")\n    tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"right\"))\n    self.assertAllEqual(result, tf_result)\n\n  def testUpperBoundFloatRandomNd(self):\n    dim_size = 7\n    for d in range(1, 5):\n      shape = [dim_size] * d\n      cdf = np.cumsum(\n          np.random.uniform(size=shape).astype(np.float32), axis=(d - 1))\n      arr = np.random.uniform(size=shape).astype(np.float32) * dim_size\n\n      tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"right\"))\n\n      cdf = cdf.reshape([-1, dim_size])\n      arr = arr.reshape([-1, dim_size])\n      result = np.zeros(arr.shape, dtype=np.int32)\n      for i in range(dim_size**(d - 1)):\n        result[i, :] = np.searchsorted(cdf[i, :], arr[i, :], side=\"right\")\n\n      result = result.reshape(shape)\n\n      self.assertAllEqual(result, tf_result)\n\n  def testUpperBoundFloatUneven(self):\n    batch_size = 7\n    size_search_array = 1000\n    size_values = 47\n    cdf = np.cumsum(\n        np.random.uniform(size=[batch_size, size_search_array]).astype(\n            np.float32),\n        axis=1)\n    arr = np.random.uniform(size=[batch_size, size_values]).astype(\n        np.float32) * size_search_array\n\n    tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"right\"))\n\n    result = np.zeros(arr.shape, dtype=np.int32)\n    for i in range(batch_size):\n      result[i, :] = np.searchsorted(cdf[i, :], arr[i, :], side=\"right\")\n\n    self.assertAllEqual(result, tf_result)\n\n  def testLowerBoundFloatHandCoded(self):\n    cdf = np.array([0, .2, .5, .6, .8, 1.], dtype=np.float32)\n    arr = np.array([.04, .99, .53, .58, .31, .01, .79, .8, .21],\n                   dtype=np.float32)\n    result = np.searchsorted(cdf, arr, side=\"left\")\n    tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"left\"))\n    self.assertAllEqual(result, tf_result)\n\n  def testLowerBoundFloatRandomNd(self):\n    dim_size = 7\n    for d in range(1, 5):\n      shape = [dim_size] * d\n      cdf = np.cumsum(\n          np.random.uniform(size=shape).astype(np.float32), axis=(d - 1))\n      arr = np.random.uniform(size=shape).astype(np.float32) * dim_size\n\n      tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"left\"))\n\n      cdf = cdf.reshape([-1, dim_size])\n      arr = arr.reshape([-1, dim_size])\n      result = np.zeros(arr.shape, dtype=np.int32)\n      for i in range(dim_size**(d - 1)):\n        result[i, :] = np.searchsorted(cdf[i, :], arr[i, :], side=\"left\")\n\n      result = result.reshape(shape)\n\n      self.assertAllEqual(result, tf_result)\n\n  def testLowerBoundFloatUneven(self):\n    batch_size = 7\n    size_search_array = 1000\n    size_values = 47\n    cdf = np.cumsum(\n        np.random.uniform(size=[batch_size, size_search_array]).astype(\n            np.float32),\n        axis=1)\n    arr = np.random.uniform(size=[batch_size, size_values]).astype(\n        np.float32) * size_search_array\n\n    tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"left\"))\n\n    result = np.zeros(arr.shape, dtype=np.int32)\n    for i in range(batch_size):\n      result[i, :] = np.searchsorted(cdf[i, :], arr[i, :], side=\"left\")\n\n    self.assertAllEqual(result, tf_result)\n\n  def testUpperBoundIntHandCoded(self):\n    cdf = np.array([0, 20, 50, 60, 80, 100], dtype=np.int64)\n    arr = np.array([4, 99, 53, 58, 31, 1, 79, 8, 21], dtype=np.int64)\n    result = np.searchsorted(cdf, arr, side=\"right\")\n    tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"right\"))\n    self.assertAllEqual(result, tf_result)\n\n  def testUpperBoundIntRandomNd(self):\n    dim_size = 7\n    for d in range(1, 5):\n      shape = [dim_size] * d\n      cdf = np.cumsum(\n          np.random.randint(low=0, high=10, size=shape).astype(np.int64),\n          axis=(d - 1))\n      arr = np.random.randint(\n          low=0, high=10 * dim_size, size=shape).astype(np.int64)\n\n      tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"right\"))\n\n      cdf = cdf.reshape([-1, dim_size])\n      arr = arr.reshape([-1, dim_size])\n      result = np.zeros(arr.shape, dtype=np.int32)\n      for i in range(dim_size**(d - 1)):\n        result[i, :] = np.searchsorted(cdf[i, :], arr[i, :], side=\"right\")\n\n      result = result.reshape(shape)\n\n      self.assertAllEqual(result, tf_result)\n\n  def testUpperBoundIntUneven(self):\n    batch_size = 7\n    size_search_array = 1000\n    size_values = 47\n    cdf = np.cumsum(\n        np.random.randint(low=0, high=10,\n                          size=[batch_size,\n                                size_search_array]).astype(np.int64),\n        axis=1)\n    arr = np.random.randint(\n        low=0, high=10 * size_search_array, size=[batch_size,\n                                                  size_values]).astype(np.int64)\n\n    tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"right\"))\n\n    result = np.zeros(arr.shape, dtype=np.int32)\n    for i in range(batch_size):\n      result[i, :] = np.searchsorted(cdf[i, :], arr[i, :], side=\"right\")\n\n    self.assertAllEqual(result, tf_result)\n\n  def testLowerBoundIntHandCoded(self):\n    cdf = np.array([0, 20, 50, 60, 80, 100], dtype=np.int64)\n    arr = np.array([4, 99, 53, 58, 31, 1, 79, 8, 21], dtype=np.int64)\n    result = np.searchsorted(cdf, arr, side=\"left\")\n    tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"left\"))\n    self.assertAllEqual(result, tf_result)\n\n  def testLowerBoundIntRandomNd(self):\n    dim_size = 7\n    for d in range(1, 5):\n      shape = [dim_size] * d\n      cdf = np.cumsum(\n          np.random.randint(low=0, high=10, size=shape).astype(np.int64),\n          axis=(d - 1))\n      arr = np.random.randint(\n          low=0, high=10 * dim_size, size=shape).astype(np.int64)\n\n      tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"left\"))\n\n      cdf = cdf.reshape([-1, dim_size])\n      arr = arr.reshape([-1, dim_size])\n      result = np.zeros(arr.shape, dtype=np.int32)\n      for i in range(dim_size**(d - 1)):\n        result[i, :] = np.searchsorted(cdf[i, :], arr[i, :], side=\"left\")\n\n      result = result.reshape(shape)\n\n      self.assertAllEqual(result, tf_result)\n\n  def testLowerBoundIntUneven(self):\n    batch_size = 7\n    size_search_array = 1000\n    size_values = 47\n    cdf = np.cumsum(\n        np.random.randint(low=0, high=10,\n                          size=[batch_size,\n                                size_search_array]).astype(np.int64),\n        axis=1)\n    arr = np.random.randint(\n        low=0, high=10 * size_search_array, size=[batch_size,\n                                                  size_values]).astype(np.int64)\n\n    tf_result = self.evaluate(array_ops.searchsorted(cdf, arr, side=\"left\"))\n\n    result = np.zeros(arr.shape, dtype=np.int32)\n    for i in range(batch_size):\n      result[i, :] = np.searchsorted(cdf[i, :], arr[i, :], side=\"left\")\n\n    self.assertAllEqual(result, tf_result)\n\n  def testZeroSequenceSize(self):\n    dtype = dtypes.int32\n    for side in (\"left\", \"right\"):\n      with self.subTest(side=side):\n        self.assertAllEqual(\n            array_ops.searchsorted(\n                array_ops.ones([2, 0]),\n                array_ops.ones([2, 3]),\n                side=side,\n                out_type=dtype), array_ops.zeros([2, 3], dtype))\n\n  def testZeroValueSize(self):\n    dtype = dtypes.int32\n    for side in (\"left\", \"right\"):\n      with self.subTest(side=side):\n        self.assertAllEqual(\n            array_ops.searchsorted(\n                array_ops.ones([2, 3]),\n                array_ops.ones([2, 0]),\n                side=side,\n                out_type=dtype), array_ops.zeros([2, 0], dtype))\n\n  def testInt64(self):\n\n    @def_function.function\n    def g():\n      x = random_ops.random_normal(shape=[int(1e10)])\n      y = array_ops.ones(shape=[int(1e10)])\n      return array_ops.searchsorted(x, y, out_type=dtypes.int64)\n\n    _ = g.get_concrete_function()\n\n  def testInt64UnspecifiedOutType(self):\n\n    @def_function.function\n    def g():\n      x = random_ops.random_normal(shape=[int(1e10)])\n      y = array_ops.ones(shape=[int(1e10)])\n      return array_ops.searchsorted(x, y)\n\n    _ = g.get_concrete_function()\n\n\nclass BatchGatherNdTest(test_util.TensorFlowTestCase):\n\n  def testShapesMatch(self):\n    \"\"\"Tests for various different shape combinations.\"\"\"\n    shapes = []\n    # params_shape, indices_shape, batch_dims\n    shapes.append(((2, 2, 2), (2, 1), 1),)\n    shapes.append(((2, 2, 2), (2, 2), 1),)\n    shapes.append(((2, 2, 2), (2, 3), 0),)\n    shapes.append(((2, 2, 2), (3,), 0),)\n    shapes.append(((2, 2, 2), (1,), 0),)\n    shapes.append(((2, 2, 3, 2), (2, 3), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 2), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 1), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 1, 3), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 2, 2), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 3, 1), 1),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 3), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 2), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 1), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 1, 3), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 2, 2), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 3, 1), 2),)\n\n    for params_shape, indices_shape, batch_dims in shapes:\n      with self.subTest(\n          params_shape=params_shape,\n          indices_shape=indices_shape,\n          batch_dims=batch_dims):\n        params = constant_op.constant(1.0, shape=(params_shape))\n        indices = constant_op.constant(\n            1, shape=(indices_shape), dtype=dtypes.int32)\n        out = array_ops.batch_gather_nd(\n            params=params, indices=indices, batch_dims=batch_dims)\n        ndims_params = len(params_shape) - batch_dims\n        ndims_rows = ndims_params - indices_shape[-1]\n        expected_out_shape = indices_shape[:-1]\n        if ndims_rows > 0:\n          expected_out_shape += params_shape[-ndims_rows:]\n        self.assertSequenceEqual(out.shape, expected_out_shape)\n\n  def testReducesToGatherNDWhenBatchDimIsZero(self):\n    \"\"\"Confirms setting batch_dims to zero reduces to tf.gather_nd.\"\"\"\n    params = constant_op.constant(np.random.uniform(0.0, 1.0, size=(7, 8, 9)))\n    indices_shapes = []\n    indices_shapes.append((1,))\n    indices_shapes.append((3, 1))\n    indices_shapes.append((3, 3, 1))\n    indices_shapes.append((2,))\n    indices_shapes.append((3, 2))\n    indices_shapes.append((3, 3, 2))\n    indices_shapes.append((3,))\n    indices_shapes.append((3, 3))\n    indices_shapes.append((3, 3, 3))\n\n    for indices_shape in indices_shapes:\n      with self.subTest(indices_shape=indices_shape):\n        indices = np.random.randint(0, 7, size=indices_shape)\n        gather_nd_result = gen_array_ops.gather_nd(params, indices)\n        batch_gather_nd_result = array_ops.batch_gather_nd(\n            params=params, indices=indices, batch_dims=0)\n        self.assertAllEqual(gather_nd_result, batch_gather_nd_result)\n\n  def testSameResultAsMapFn(self):\n    \"\"\"Compares results with gather_nd called on every element with map_fn.\"\"\"\n    shapes = []\n    # params_shape, indices_shape, batch_dims\n    shapes.append(((2, 2, 2), (2, 1), 1),)\n    shapes.append(((2, 2, 2), (2, 2), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 3), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 2), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 1), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 1, 3), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 2, 2), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 3, 1), 1),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 3), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 2), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 1), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 1, 3), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 2, 2), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 3, 1), 2),)\n\n    for params_shape, indices_shape, batch_dims in shapes:\n      with self.subTest(\n          params_shape=params_shape,\n          indices_shape=indices_shape,\n          batch_dims=batch_dims):\n        params = constant_op.constant(\n            np.random.uniform(0.0, 1.0, size=(params_shape)))\n        indices = np.random.randint(0, 2, size=indices_shape)\n        batch_gather_nd_result = array_ops.batch_gather_nd(\n            params=params, indices=indices, batch_dims=batch_dims)\n\n        if batch_dims > 1:\n          params = array_ops.reshape(\n              params, shape=[-1] + list(params_shape[batch_dims:]))\n          indices = array_ops.reshape(\n              indices, shape=[-1] + list(indices_shape[batch_dims:]))\n\n        map_fn_gather_nd_result = map_fn.map_fn(\n            fn=self._map_fn_body, elems=(params, indices), dtype=dtypes.float64)\n\n        if batch_dims > 1:\n          out_shape = map_fn_gather_nd_result.shape.as_list()\n          out_shape = list(params_shape[:batch_dims]) + out_shape[1:]\n          map_fn_gather_nd_result = array_ops.reshape(\n              map_fn_gather_nd_result, shape=out_shape)\n\n        self.assertAllEqual(map_fn_gather_nd_result, batch_gather_nd_result)\n\n  def _map_fn_body(self, elems):\n    return gen_array_ops.gather_nd(elems[0], elems[1])\n\n  def testBatchDimsAsTensor(self):\n    \"\"\"Tests Tensor batch_dims as input works as intended.\"\"\"\n    shapes = []\n    # params_shape, indices_shape, batch_dims\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 3, 1), 0),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 3, 1), 1),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 3, 1), 2),)\n\n    for params_shape, indices_shape, batch_dims in shapes:\n      with self.subTest(\n          params_shape=params_shape,\n          indices_shape=indices_shape,\n          batch_dims=batch_dims):\n        params = constant_op.constant(\n            np.random.uniform(0.0, 1.0, size=(params_shape)))\n        indices = np.random.randint(0, 2, size=indices_shape)\n        batch_gather_nd_result = array_ops.gather_nd(\n            params=params, indices=indices, batch_dims=batch_dims)\n        batch_dims_tensor = constant_op.constant([batch_dims])\n        batch_gather_nd_tensor_batch_dims_result = array_ops.gather_nd(\n            params=params, indices=indices, batch_dims=batch_dims_tensor)\n\n        self.assertAllEqual(batch_gather_nd_tensor_batch_dims_result,\n                            batch_gather_nd_result)\n\n  def testInvalidBatchDimsRaisesException(self):\n    \"\"\"Tests whether invalid batch_dims raise expected exceptions.\"\"\"\n    params = constant_op.constant(\n        np.random.uniform(0.0, 1.0, size=(3, 2, 2, 3, 4)))\n    indices = np.random.randint(0, 2, size=(3, 2, 3))\n\n    with self.assertRaises(TypeError):\n      array_ops.batch_gather_nd(\n          params=params,\n          indices=indices,\n          batch_dims=constant_op.constant((0, 1)))\n\n    with self.assertRaises(ValueError):\n      array_ops.batch_gather_nd(params=params, indices=indices, batch_dims=-1)\n\n    with self.assertRaises(ValueError):\n      array_ops.batch_gather_nd(params=params, indices=indices, batch_dims=4)\n\n  def testNoneBatchDimensions(self):\n    \"\"\"Tests gather_nd works with None dimensions.\"\"\"\n    shapes = []\n    # params_shape, indices_shape, batch_dims\n    shapes.append(((2, 2, 2), (2, 1), 1),)\n    shapes.append(((2, 2, 2), (2, 2), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 3), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 2), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 1), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 1, 3), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 2, 2), 1),)\n    shapes.append(((2, 2, 3, 2), (2, 3, 1), 1),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 3), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 2), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 1), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 1, 3), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 2, 2), 2),)\n    shapes.append(((3, 2, 2, 3, 4), (3, 2, 3, 1), 2),)\n\n    for params_shape, indices_shape, batch_dims in shapes:\n      params_ph_shape = list(params_shape)\n      indices_ph_shape = list(indices_shape)\n      for i in range(batch_dims):\n        params_ph_shape[i] = None\n        indices_ph_shape[i] = None\n\n      @def_function.function\n      def func(params, indices):\n        return array_ops.batch_gather_nd(\n            params=params, indices=indices, batch_dims=batch_dims)  # pylint: disable=cell-var-from-loop\n\n      f = func.get_concrete_function(\n          tensor_spec.TensorSpec(params_ph_shape, dtypes.float32),\n          tensor_spec.TensorSpec(indices_ph_shape, dtypes.int32))\n\n      params_val = np.ones(dtype=np.float32, shape=params_shape)\n      indices_val = np.ones(dtype=np.int32, shape=indices_shape)\n      res = f(params_val, indices_val)\n      row_ndims = len(params_shape) - batch_dims - indices_shape[-1]\n      expected_out_shape = indices_shape[:-1]\n      if row_ndims > 0:\n        expected_out_shape += params_shape[-row_ndims:]\n\n      self.assertSequenceEqual(res.shape, expected_out_shape)\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass RepeatTest(test_util.TensorFlowTestCase, parameterized.TestCase):\n\n  @parameterized.parameters(\n      (3, 4, None),\n      ([[1, 2], [3, 4]], 2, None),\n      ([[1, 2], [3, 4]], [1, 2], 0),\n      ([[1, 2], [3, 4]], [1, 2], 1),\n      ([[1, 2], [3, 4]], 3, 1),\n      ([[1, 2], [3, 4]], [1, 2, 3, 4], None),\n      (np.ones([0, 4]), 0, 1),\n      (np.ones([1, 2]), [2], None),\n  )\n  @test_util.with_forward_compatibility_horizons(None, [2052, 2, 7])\n  def testRepeat(self, array, repeats, axis):\n    array = np.array(array)\n\n    @def_function.function(\n        input_signature=[tensor_spec.TensorSpec(None, dtypes.int32)] * 2)\n    def repeat_fn(array, repeats):\n      return array_ops.repeat(array, repeats, axis)\n\n    v_tf = array_ops.repeat(constant_op.constant(array), repeats, axis)\n    v_tf_fn = repeat_fn(\n        constant_op.constant(array, dtype=dtypes.int32), repeats)\n    v_np = np.repeat(array, repeats, axis)\n    self.assertAllEqual(v_tf, v_np)\n    self.assertAllEqual(v_tf_fn, v_np)\n\n\nclass RepeatBenchmark(test_lib.Benchmark):\n  \"\"\"Benchmark the repeat implementation.\"\"\"\n\n  def run_and_time(self, op, iters=100, warmup_iters=10):\n    self.evaluate(variables.global_variables_initializer())\n    for _ in range(warmup_iters):\n      _ = self.evaluate(op)\n    t0 = time.time()\n    for _ in range(iters):\n      self.evaluate(op)\n    t1 = time.time()\n    self.report_benchmark(iters=iters, wall_time=(t1 - t0) / float(iters))\n\n  def make_variable(self, shape, dtype=dtypes.float32):\n    items = 1\n    for dim in shape:\n      items *= dim\n    var = variables.Variable(\n        array_ops.reshape(math_ops.linspace(1., float(items), items), shape),\n        dtype=dtype)\n    return var\n\n  def run_benchmark(self, shape, max_repeats, axis=None):\n    with session.Session():\n      var = self.make_variable(shape)\n      if axis is None:\n        axis_size = 1\n        for dim in shape:\n          axis_size *= dim\n      else:\n        axis_size = shape[axis]\n      repeats = constant_op.constant(\n          np.random.randint(max_repeats, size=[axis_size]), dtype=dtypes.int64)\n      repeat_op = array_ops.repeat(var, repeats, axis=axis)\n      # Return a scalar to reduce the device-to-host memcopy overhead.\n      repeat_op = repeat_op[(0,) * len(shape)]\n      self.run_and_time(repeat_op)\n\n  def benchmark_repeat_few_1d(self):\n    self.run_benchmark(shape=[1024 * 1024], max_repeats=8, axis=0)\n\n  def benchmark_repeat_many_1d(self):\n    self.run_benchmark(shape=[8 * 1024], max_repeats=1024, axis=0)\n\n  def benchmark_repeat_few_2d_axis0(self):\n    self.run_benchmark(shape=[8, 128 * 1024], max_repeats=8, axis=0)\n\n  def benchmark_repeat_many_2d_axis0(self):\n    self.run_benchmark(shape=[8, 1024], max_repeats=1024, axis=0)\n\n  def benchmark_repeat_many_2d_axis0_big(self):\n    self.run_benchmark(shape=[1024, 32], max_repeats=1024, axis=0)\n\n  def benchmark_repeat_few_2d_axis1(self):\n    self.run_benchmark(shape=[8, 128 * 1024], max_repeats=8, axis=1)\n\n  def benchmark_repeat_many_2d_axis1(self):\n    self.run_benchmark(shape=[8, 1024], max_repeats=1024, axis=1)\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass TileVariantTest(test_util.TensorFlowTestCase):\n\n  def test_tile_tensor_list(self):\n    t = constant_op.constant(np.random.uniform(size=[2, 3, 4]))\n    handle = list_ops.tensor_list_from_tensor(t, element_shape=None)\n    with ops.device(\"CPU:0\"):\n      tiled_handles = array_ops.tile(array_ops.reshape(handle, [1]), [2])\n    tiled_tensor_0 = list_ops.tensor_list_stack(tiled_handles[0], t.dtype, 2,\n                                                [3, 4])\n    tiled_tensor_1 = list_ops.tensor_list_stack(tiled_handles[1], t.dtype, 2,\n                                                [3, 4])\n    self.assertAllEqual(t, tiled_tensor_0)\n    self.assertAllEqual(t, tiled_tensor_1)\n    # Now mutate some of the lists and make sure the changes are not reflected\n    # in the tiled handles.\n    with ops.control_dependencies([\n        list_ops.tensor_list_scatter([t[0] + 1], [0], input_handle=handle),\n        list_ops.tensor_list_set_item(tiled_handles[0], 0, t[0] + 2)]):\n      tiled_tensor_0 = list_ops.tensor_list_stack(tiled_handles[0], t.dtype, 2,\n                                                  [3, 4])\n      tiled_tensor_1 = list_ops.tensor_list_stack(tiled_handles[1], t.dtype, 2,\n                                                  [3, 4])\n    self.assertAllEqual(t, tiled_tensor_0)\n    self.assertAllEqual(t, tiled_tensor_1)\n\n\nclass StopGradientTest(test_util.TensorFlowTestCase):\n\n  def testStopGradient(self):\n    x = array_ops.zeros(3)\n    y = array_ops.stop_gradient(x)\n    self.assertAllEqual(x, y)\n\n  def testStopGradientRaggedTensor(self):\n    x = RaggedTensor.from_row_splits(values=[1, 2, 3], row_splits=[0, 1, 1, 3])\n    y = array_ops.stop_gradient(x)\n    self.assertAllEqual(x, y)\n\n  def testStopGradientGradientTape(self):\n    x = array_ops.zeros(3)\n    with backprop.GradientTape() as tape:\n      y = array_ops.stop_gradient(x)\n\n    self.assertIsNone(tape.gradient(y, x))\n\n  def testStopGradientGradientTapeRaggedTensor(self):\n    x = RaggedTensor.from_row_splits(values=[1, 2, 3], row_splits=[0, 1, 1, 3])\n    with backprop.GradientTape() as tape:\n      y = array_ops.stop_gradient(x)\n\n    self.assertIsNone(tape.gradient(y, x))\n\n\nif __name__ == \"__main__\":\n  test_lib.main()\n"], "filenames": ["tensorflow/core/kernels/reshape_op.h", "tensorflow/python/kernel_tests/array_ops/array_ops_test.py"], "buggy_code_start_loc": [47, 352], "buggy_code_end_loc": [47, 352], "fixing_code_start_loc": [48, 353], "fixing_code_end_loc": [53, 362], "type": "CWE-617", "message": "TensorFlow is an open source platform for machine learning. The implementation of tf.reshape op in TensorFlow is vulnerable to a denial of service via CHECK-failure (assertion failure) caused by overflowing the number of elements in a tensor. This issue has been patched in GitHub commit 61f0f9b94df8c0411f0ad0ecc2fec2d3f3c33555. The fix will be included in TensorFlow 2.10.0. We will also cherrypick this commit on TensorFlow 2.9.1, TensorFlow 2.8.1, and TensorFlow 2.7.2, as these are also affected and still in supported range. There are no known workarounds for this issue.", "other": {"cve": {"id": "CVE-2022-35934", "sourceIdentifier": "security-advisories@github.com", "published": "2022-09-16T20:15:09.980", "lastModified": "2022-09-20T16:42:18.377", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "TensorFlow is an open source platform for machine learning. The implementation of tf.reshape op in TensorFlow is vulnerable to a denial of service via CHECK-failure (assertion failure) caused by overflowing the number of elements in a tensor. This issue has been patched in GitHub commit 61f0f9b94df8c0411f0ad0ecc2fec2d3f3c33555. The fix will be included in TensorFlow 2.10.0. We will also cherrypick this commit on TensorFlow 2.9.1, TensorFlow 2.8.1, and TensorFlow 2.7.2, as these are also affected and still in supported range. There are no known workarounds for this issue."}, {"lang": "es", "value": "TensorFlow es una plataforma de c\u00f3digo abierto para el aprendizaje autom\u00e1tico. La implementaci\u00f3n de tf.reshape op en TensorFlow es vulnerable a una denegaci\u00f3n de servicio por medio de CHECK-failure (fallo de aserci\u00f3n) causado por el desbordamiento del n\u00famero de elementos en un tensor. Este problema ha sido parcheado en el commit 61f0f9b94df8c0411f0ad0ecc2fec2d3f3c33555 de GitHub. La correcci\u00f3n ser\u00e1 incluida en TensorFlow versi\u00f3n 2.10.0. Tambi\u00e9n seleccionaremos este compromiso en TensorFlow versi\u00f3n 2.9.1, TensorFlow versi\u00f3n 2.8.1, y TensorFlow versi\u00f3n 2.7.2, ya que estos tambi\u00e9n est\u00e1n afectados y todav\u00eda est\u00e1n en el rango admitido. No se presentan mitigaciones conocidas para este problema"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 7.5, "baseSeverity": "HIGH"}, "exploitabilityScore": 3.9, "impactScore": 3.6}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:H/PR:N/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "HIGH", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.9, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 2.2, "impactScore": 3.6}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-617"}]}, {"source": "security-advisories@github.com", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-617"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.7.0", "versionEndExcluding": "2.7.2", "matchCriteriaId": "C4DFBF2D-5283-42F6-8800-D653BFA5CE82"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.8.0", "versionEndExcluding": "2.8.1", "matchCriteriaId": "0F9D273D-02DC-441E-AA91-EAC8DEAA4B44"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.9.0", "versionEndExcluding": "2.9.1", "matchCriteriaId": "FE4F8A81-6CC2-4F7F-9602-C170FDD926E7"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.10:rc0:*:*:*:*:*:*", "matchCriteriaId": "1DBFBCE2-0A01-4575-BE45-6775ABFB8B28"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.10:rc1:*:*:*:*:*:*", "matchCriteriaId": "89806CF9-E423-4CA6-A01A-8175C260CB24"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.10:rc2:*:*:*:*:*:*", "matchCriteriaId": "F2B80690-A257-4E16-BD27-9AE045BC56ED"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.10:rc3:*:*:*:*:*:*", "matchCriteriaId": "F335F9A4-5AB8-4E53-BC18-E01F7C653E5E"}]}]}], "references": [{"url": "https://github.com/tensorflow/tensorflow/commit/61f0f9b94df8c0411f0ad0ecc2fec2d3f3c33555", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/security/advisories/GHSA-f4w6-h4f5-wx45", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/tensorflow/tensorflow/commit/61f0f9b94df8c0411f0ad0ecc2fec2d3f3c33555"}}