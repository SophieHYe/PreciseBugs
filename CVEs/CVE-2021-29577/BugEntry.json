{"buggy_code": ["/* Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n#define EIGEN_USE_THREADS\n\n#include \"tensorflow/core/kernels/pooling_ops_3d.h\"\n\n#include <array>\n\n#include \"third_party/eigen3/Eigen/Core\"\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n#include \"tensorflow/core/framework/kernel_shape_util.h\"\n#include \"tensorflow/core/framework/numeric_op.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/register_types.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/tensor_shape.h\"\n#include \"tensorflow/core/framework/tensor_slice.h\"\n#include \"tensorflow/core/kernels/eigen_pooling.h\"\n#include \"tensorflow/core/kernels/ops_util.h\"\n#include \"tensorflow/core/lib/core/errors.h\"\n#include \"tensorflow/core/util/padding.h\"\n#include \"tensorflow/core/util/tensor_format.h\"\n#include \"tensorflow/core/util/work_sharder.h\"\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n#include \"tensorflow/core/kernels/cudnn_pooling_gpu.h\"\n#include \"tensorflow/core/kernels/pooling_ops_3d_gpu.h\"\n#endif\n\n\nnamespace tensorflow {\n\ntypedef Eigen::ThreadPoolDevice CPUDevice;\ntypedef Eigen::GpuDevice GPUDevice;\n\nPool3dParameters::Pool3dParameters(OpKernelContext* context,\n                                   const std::vector<int32>& ksize,\n                                   const std::vector<int32>& stride,\n                                   Padding padding, TensorFormat data_format,\n                                   const TensorShape& tensor_in_shape) {\n  // For maxpooling, tensor_in should have 4 dimensions.\n  OP_REQUIRES(context, tensor_in_shape.dims() == 5,\n              errors::InvalidArgument(\"tensor_in must be 4-dimensional\"));\n\n  this->data_format = data_format;\n  depth = GetTensorDim(tensor_in_shape, data_format, 'C');\n  tensor_in_planes = GetTensorDim(tensor_in_shape, data_format, '0');\n  tensor_in_rows = GetTensorDim(tensor_in_shape, data_format, '1');\n  tensor_in_cols = GetTensorDim(tensor_in_shape, data_format, '2');\n  tensor_in_batch = GetTensorDim(tensor_in_shape, data_format, 'N');\n  window_planes = GetTensorDim(ksize, data_format, '0');\n  window_rows = GetTensorDim(ksize, data_format, '1');\n  window_cols = GetTensorDim(ksize, data_format, '2');\n  depth_window = GetTensorDim(ksize, data_format, 'C');\n  plane_stride = GetTensorDim(stride, data_format, '0');\n  row_stride = GetTensorDim(stride, data_format, '1');\n  col_stride = GetTensorDim(stride, data_format, '2');\n  depth_stride = GetTensorDim(stride, data_format, 'C');\n\n  // We only support 3D pooling across plane/width/height. Depthwise\n  // pooling is not supported.\n  OP_REQUIRES(\n      context, depth_window == 1 && depth_stride == 1,\n      errors::Unimplemented(\n          \"Pooling3d only supports pooling across plane/width/height.\"));\n\n  OP_REQUIRES_OK(context, GetWindowedOutputSize(tensor_in_planes, window_planes,\n                                                plane_stride, padding,\n                                                &out_plane, &pad_planes));\n  OP_REQUIRES_OK(context,\n                 GetWindowedOutputSize(tensor_in_rows, window_rows, row_stride,\n                                       padding, &out_height, &pad_rows));\n  OP_REQUIRES_OK(context,\n                 GetWindowedOutputSize(tensor_in_cols, window_cols, col_stride,\n                                       padding, &out_width, &pad_cols));\n}\n\nTensorShape Pool3dParameters::forward_output_shape() {\n  return ShapeFromFormat(data_format, tensor_in_batch,\n                         {{out_plane, out_height, out_width}}, depth);\n}\n\ntemplate <typename T>\nstruct LaunchPoolingOp<CPUDevice, T, AVG> {\n  static void launch(OpKernelContext* context, const Tensor& tensor_in,\n                     const std::array<int64, 3>& window,\n                     const std::array<int64, 3>& stride,\n                     const std::array<int64, 3>& padding,\n                     TensorFormat data_format, Padding padding_type,\n                     Tensor* output) {\n    output->tensor<T, 5>().device(context->eigen_device<CPUDevice>()) =\n        Eigen::CuboidAvgPooling(tensor_in.tensor<T, 5>(), window[0], window[1],\n                                window[2], stride[0], stride[1], stride[2],\n                                BrainPadding2EigenPadding(padding_type));\n  }\n};\n\ntemplate <typename T>\nstruct LaunchPoolingOp<CPUDevice, T, MAX> {\n  static void launch(OpKernelContext* context, const Tensor& tensor_in,\n                     const std::array<int64, 3>& window,\n                     const std::array<int64, 3>& stride,\n                     const std::array<int64, 3>& padding,\n                     TensorFormat data_format, Padding padding_type,\n                     Tensor* output) {\n    output->tensor<T, 5>().device(context->eigen_device<CPUDevice>()) =\n        Eigen::CuboidMaxPooling(tensor_in.tensor<T, 5>(), window[0], window[1],\n                                window[2], stride[0], stride[1], stride[2],\n                                BrainPadding2EigenPadding(padding_type));\n  }\n};\n\ntemplate <typename Device, typename T, PoolingType Type>\nclass Pooling3DOp : public UnaryOp<T> {\n public:\n  explicit Pooling3DOp(OpKernelConstruction* context) : UnaryOp<T>(context) {\n    string data_format;\n    OP_REQUIRES_OK(context, context->GetAttr(\"data_format\", &data_format));\n    OP_REQUIRES(context, FormatFromString(data_format, &data_format_),\n                errors::InvalidArgument(\"Invalid data format\"));\n    if (context->device_type() == DEVICE_CPU) {\n      OP_REQUIRES(\n          context, data_format_ == FORMAT_NHWC,\n          errors::InvalidArgument(\"Default Pooling3DOp only supports NDHWC \",\n                                  \"on device type \",\n                                  DeviceTypeString(context->device_type())));\n    }\n    OP_REQUIRES_OK(context, context->GetAttr(\"ksize\", &ksize_));\n    OP_REQUIRES(context, ksize_.size() == 5,\n                errors::InvalidArgument(\"Sliding window ksize field must \"\n                                        \"specify 5 dimensions\"));\n    OP_REQUIRES_OK(context, context->GetAttr(\"strides\", &stride_));\n    OP_REQUIRES(context, stride_.size() == 5,\n                errors::InvalidArgument(\"Sliding window stride field must \"\n                                        \"specify 5 dimensions\"));\n    OP_REQUIRES_OK(context, context->GetAttr(\"padding\", &padding_));\n    OP_REQUIRES(context,\n                (GetTensorDim(ksize_, data_format_, 'N') == 1 &&\n                 GetTensorDim(stride_, data_format_, 'N') == 1),\n                errors::Unimplemented(\n                    \"Pooling is not yet supported on the batch dimension.\"));\n    OP_REQUIRES(context,\n                (GetTensorDim(ksize_, data_format_, 'C') == 1 &&\n                 GetTensorDim(stride_, data_format_, 'C') == 1),\n                errors::Unimplemented(\n                    \"Pooling is not yet supported on the depth dimension.\"));\n  }\n\n  void Compute(OpKernelContext* context) override {\n    const Tensor& tensor_in = context->input(0);\n\n    OP_REQUIRES(context, tensor_in.dims() == 5,\n                errors::InvalidArgument(\"tensor_in must be 5-dimensional\"));\n    const int64 depth = GetTensorDim(tensor_in, data_format_, 'C');\n    const int64 in_batch = GetTensorDim(tensor_in, data_format_, 'N');\n\n    // Dimension order for these arrays is: x, y, z.\n    std::array<int64, 3> input_size{\n        {GetTensorDim(tensor_in, data_format_, '2'),\n         GetTensorDim(tensor_in, data_format_, '1'),\n         GetTensorDim(tensor_in, data_format_, '0')}};\n    std::array<int64, 3> window{{GetTensorDim(ksize_, data_format_, '2'),\n                                 GetTensorDim(ksize_, data_format_, '1'),\n                                 GetTensorDim(ksize_, data_format_, '0')}};\n    std::array<int64, 3> stride{{GetTensorDim(stride_, data_format_, '2'),\n                                 GetTensorDim(stride_, data_format_, '1'),\n                                 GetTensorDim(stride_, data_format_, '0')}};\n    std::array<int64, 3> padding, out;\n\n    OP_REQUIRES_OK(context, Get3dOutputSize(input_size, window, stride,\n                                            padding_, &out, &padding));\n\n    TensorShape out_shape = ShapeFromFormat(data_format_, in_batch,\n                                            {{out[2], out[1], out[0]}}, depth);\n    Tensor* output;\n    OP_REQUIRES_OK(context, context->allocate_output(0, out_shape, &output));\n    if (out_shape.num_elements() == 0) return;\n    LaunchPoolingOp<Device, T, Type>::launch(context, tensor_in, window, stride,\n                                             padding, data_format_, padding_,\n                                             output);\n  }\n\n private:\n  std::vector<int32> ksize_;\n  std::vector<int32> stride_;\n  Padding padding_;\n  TensorFormat data_format_;\n};\n\ntemplate <typename T>\nstruct LaunchMaxPooling3dGradOp<CPUDevice, T> {\n  static void launch(OpKernelContext* context, const Tensor& tensor_in,\n                     const Tensor& tensor_out, const Tensor& out_backprop,\n                     const std::array<int64, 3>& window,\n                     const std::array<int64, 3>& stride,\n                     const std::array<int64, 3>& out,\n                     const std::array<int64, 3>& padding,\n                     TensorFormat data_format, Tensor* output) {\n    output->flat<T>().setZero();\n    for (int64 p = 0; p < out_backprop.dim_size(3); ++p) {\n      // Calculate broadcast size for planes/rows/cols. For SAME padding,\n      // current index could be in the padding area, and\n      //   p * stride_planes + window_planes\n      // could be beyond the input tensor's boundary. In such cases, change\n      // the starting index and reduce the broadcast size.\n      //\n      // The same procedure is repeated for every spatial dimension in the\n      // nested loops below.\n      int pindex, psize;\n      std::array<int64, 3> input_size{{tensor_in.dim_size(3),\n                                       tensor_in.dim_size(2),\n                                       tensor_in.dim_size(1)}};\n      OP_REQUIRES_OK(context,\n                     GetBroadcastSize(p, input_size[0], window[0], stride[0],\n                                      padding[0], &pindex, &psize));\n      for (int64 r = 0; r < out_backprop.dim_size(2); ++r) {\n        int rindex, rsize;\n        OP_REQUIRES_OK(context,\n                       GetBroadcastSize(r, input_size[1], window[1], stride[1],\n                                        padding[1], &rindex, &rsize));\n        for (int64 c = 0; c < out_backprop.dim_size(1); ++c) {\n          int cindex, csize;\n          OP_REQUIRES_OK(\n              context, GetBroadcastSize(c, input_size[2], window[2], stride[2],\n                                        padding[2], &cindex, &csize));\n          TensorSlice src{{0, -1}, {c, 1}, {r, 1}, {p, 1}, {0, -1}};\n          TensorSlice dst{{0, -1},\n                          {cindex, csize},\n                          {rindex, rsize},\n                          {pindex, psize},\n                          {0, -1}};\n          Eigen::DSizes<Eigen::DenseIndex, 5> src_indices;\n          Eigen::DSizes<Eigen::DenseIndex, 5> src_sizes;\n          Eigen::DSizes<Eigen::DenseIndex, 5> dst_indices;\n          Eigen::DSizes<Eigen::DenseIndex, 5> dst_sizes;\n          src.FillIndicesAndSizes<5>(out_backprop.shape(), &src_indices,\n                                     &src_sizes);\n          dst.FillIndicesAndSizes<5>(tensor_in.shape(), &dst_indices,\n                                     &dst_sizes);\n\n#if !defined(EIGEN_HAS_INDEX_LIST)\n          Eigen::array<int, 5> bcast = {1, csize, rsize, psize, 1};\n#else\n          Eigen::IndexList<Eigen::type2index<1>, int, int, int,\n                           Eigen::type2index<1>>\n              bcast;\n          bcast.set(1, csize);\n          bcast.set(2, rsize);\n          bcast.set(3, psize);\n#endif\n\n          // Slice from tensor_in.\n          Eigen::Tensor<T, 5, Eigen::RowMajor> tensor_in_slice(dst_sizes);\n          tensor_in_slice.device(context->eigen_cpu_device()) =\n              tensor_in.tensor<T, 5>().slice(dst_indices, dst_sizes);\n\n          // Slice from tensor_out.\n          Eigen::Tensor<T, 5, Eigen::RowMajor> tensor_out_slice(src_sizes);\n          tensor_out_slice.device(context->eigen_cpu_device()) =\n              tensor_out.tensor<T, 5>().slice(src_indices, src_sizes);\n\n          // Backprop slice.\n          Eigen::Tensor<T, 5, Eigen::RowMajor> out_backprop_slice(src_sizes);\n          out_backprop_slice.device(context->eigen_cpu_device()) =\n              out_backprop.tensor<T, 5>().slice(src_indices, src_sizes);\n\n          // The true backprop slice: if an element is the max, choose\n          // the backprop slice; otherwise set to 0.\n          Eigen::Tensor<T, 5, Eigen::RowMajor> select_slice(dst_sizes);\n          Eigen::Tensor<T, 5, Eigen::RowMajor> mat0(dst_sizes);\n          mat0.setZero();\n          select_slice =\n              ((tensor_in_slice - tensor_out_slice.broadcast(bcast)).abs() <\n               tensor_in_slice.constant(1e-5))\n                  .select(out_backprop_slice.broadcast(bcast), mat0);\n\n          output->tensor<T, 5>()\n              .slice(dst_indices, dst_sizes)\n              .device(context->eigen_cpu_device()) += select_slice;\n        }\n      }\n    }\n  }\n};\n\ntemplate <class Device, class T>\nclass MaxPooling3dGradOp : public OpKernel {\n public:\n  explicit MaxPooling3dGradOp(OpKernelConstruction* context)\n      : OpKernel(context) {\n    string data_format;\n    OP_REQUIRES_OK(context, context->GetAttr(\"data_format\", &data_format));\n    OP_REQUIRES(context, FormatFromString(data_format, &data_format_),\n                errors::InvalidArgument(\"Invalid data format\"));\n    if (context->device_type() == DEVICE_CPU) {\n      OP_REQUIRES(\n          context, data_format_ == FORMAT_NHWC,\n          errors::InvalidArgument(\n              \"Default MaxPooling3dGradOp only supports NDHWC \",\n              \"on device type \", DeviceTypeString(context->device_type())));\n    }\n    OP_REQUIRES_OK(context, context->GetAttr(\"ksize\", &ksize_));\n    OP_REQUIRES(context, ksize_.size() == 5,\n                errors::InvalidArgument(\"Sliding window ksize field must \"\n                                        \"specify 5 dimensions\"));\n    OP_REQUIRES_OK(context, context->GetAttr(\"strides\", &stride_));\n    OP_REQUIRES(context, stride_.size() == 5,\n                errors::InvalidArgument(\"Sliding window stride field must \"\n                                        \"specify 5 dimensions\"));\n    OP_REQUIRES_OK(context, context->GetAttr(\"padding\", &padding_));\n    OP_REQUIRES(context,\n                (GetTensorDim(ksize_, data_format_, 'N') == 1 &&\n                 GetTensorDim(stride_, data_format_, 'N') == 1),\n                errors::Unimplemented(\n                    \"Pooling is not yet supported on the batch dimension.\"));\n    OP_REQUIRES(context,\n                (GetTensorDim(ksize_, data_format_, 'C') == 1 &&\n                 GetTensorDim(stride_, data_format_, 'C') == 1),\n                errors::Unimplemented(\n                    \"Pooling is not yet supported on the depth dimension.\"));\n  }\n\n  void Compute(OpKernelContext* context) override {\n    const Tensor& tensor_in = context->input(0);\n    const Tensor& tensor_out = context->input(1);\n    const Tensor& out_backprop = context->input(2);\n    OP_REQUIRES(context, tensor_in.dims() == 5,\n                errors::InvalidArgument(\"tensor_in must be 5-dimensional\"));\n    OP_REQUIRES(context, tensor_out.dims() == 5,\n                errors::InvalidArgument(\"tensor_out must be 5-dimensional\"));\n    OP_REQUIRES(context, out_backprop.dims() == 5,\n                errors::InvalidArgument(\"out_backprop must be 5-dimensional\"));\n\n    const TensorShape& output_shape = tensor_in.shape();\n    Tensor* input_backprop;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(0, output_shape, &input_backprop));\n    std::array<int64, 3> input_size{\n        {GetTensorDim(output_shape, data_format_, '2'),\n         GetTensorDim(output_shape, data_format_, '1'),\n         GetTensorDim(output_shape, data_format_, '0')}};\n    std::array<int64, 3> window{{GetTensorDim(ksize_, data_format_, '2'),\n                                 GetTensorDim(ksize_, data_format_, '1'),\n                                 GetTensorDim(ksize_, data_format_, '0')}};\n    std::array<int64, 3> stride{{GetTensorDim(stride_, data_format_, '2'),\n                                 GetTensorDim(stride_, data_format_, '1'),\n                                 GetTensorDim(stride_, data_format_, '0')}};\n    std::array<int64, 3> out, padding;\n\n    OP_REQUIRES_OK(context, Get3dOutputSize(input_size, window, stride,\n                                            padding_, &out, &padding));\n    LaunchMaxPooling3dGradOp<Device, T>::launch(\n        context, tensor_in, tensor_out, out_backprop, window, stride, out,\n        padding, data_format_, input_backprop);\n  }\n\n private:\n  std::vector<int32> ksize_;\n  std::vector<int32> stride_;\n  Padding padding_;\n  TensorFormat data_format_;\n};\n\ntemplate <typename T>\nstruct LaunchAvgPooling3dGradOp<CPUDevice, T> {\n  static void launch(OpKernelContext* context,\n                     const TensorShape& tensor_in_shape,\n                     const Tensor& out_backprop,\n                     const std::array<int64, 3>& window,\n                     const std::array<int64, 3>& stride,\n                     const std::array<int64, 3>& output_shape,\n                     const std::array<int64, 3>& padding,\n                     TensorFormat data_format, Tensor* output) {\n    output->flat<T>().setZero();\n    std::array<int64, 3> input_size = {{tensor_in_shape.dim_size(3),\n                                        tensor_in_shape.dim_size(2),\n                                        tensor_in_shape.dim_size(1)}};\n    for (int64 p = 0; p < out_backprop.dim_size(3); ++p) {\n      // Calculate broadcast size for planes/rows/cols. For SAME padding,\n      // current index could be in the padding area, and\n      //   p * stride_planes + window_planes\n      // could be beyond the input tensor's boundary. In such cases, change\n      // the starting index and reduce the broadcast size.\n      //\n      // The same procedure is repeated for every spatial dimension in the\n      // nested loops below.\n      int pindex, psize;\n      OP_REQUIRES_OK(context,\n                     GetBroadcastSize(p, input_size[0], window[0], stride[0],\n                                      padding[0], &pindex, &psize));\n      for (int64 r = 0; r < out_backprop.dim_size(2); ++r) {\n        int rindex, rsize;\n        OP_REQUIRES_OK(context,\n                       GetBroadcastSize(r, input_size[1], window[1], stride[1],\n                                        padding[1], &rindex, &rsize));\n        for (int64 c = 0; c < out_backprop.dim_size(1); ++c) {\n          int cindex, csize;\n          OP_REQUIRES_OK(\n              context, GetBroadcastSize(c, input_size[2], window[2], stride[2],\n                                        padding[2], &cindex, &csize));\n          TensorSlice src{{0, -1}, {c, 1}, {r, 1}, {p, 1}, {0, -1}};\n          TensorSlice dst{{0, -1},\n                          {cindex, csize},\n                          {rindex, rsize},\n                          {pindex, psize},\n                          {0, -1}};\n          Eigen::DSizes<Eigen::DenseIndex, 5> src_indices;\n          Eigen::DSizes<Eigen::DenseIndex, 5> src_sizes;\n          Eigen::DSizes<Eigen::DenseIndex, 5> dst_indices;\n          Eigen::DSizes<Eigen::DenseIndex, 5> dst_sizes;\n          src.FillIndicesAndSizes<5>(out_backprop.shape(), &src_indices,\n                                     &src_sizes);\n          dst.FillIndicesAndSizes<5>(tensor_in_shape, &dst_indices, &dst_sizes);\n#if !defined(EIGEN_HAS_INDEX_LIST)\n          Eigen::array<int, 5> bcast = {1, csize, rsize, psize, 1};\n#else\n          Eigen::IndexList<Eigen::type2index<1>, int, int, int,\n                           Eigen::type2index<1>>\n              bcast;\n          bcast.set(1, csize);\n          bcast.set(2, rsize);\n          bcast.set(3, psize);\n#endif\n          Eigen::Tensor<T, 5, Eigen::RowMajor> slices(src_sizes);\n          slices.device(context->eigen_cpu_device()) =\n              out_backprop.tensor<T, 5>().slice(src_indices, src_sizes);\n          // Divide by the size of the actual patch (psize * rsize * csize).\n          float divide_size = rsize * csize * psize * 1.0f;\n          slices *= slices.constant(1.0f / divide_size);\n\n          output->tensor<T, 5>()\n              .slice(dst_indices, dst_sizes)\n              .device(context->eigen_cpu_device()) += slices.broadcast(bcast);\n        }\n      }\n    }\n  }\n};\n\ntemplate <class Device, class T>\nclass AvgPooling3dGradOp : public OpKernel {\n public:\n  explicit AvgPooling3dGradOp(OpKernelConstruction* context)\n      : OpKernel(context) {\n    string data_format;\n    OP_REQUIRES_OK(context, context->GetAttr(\"data_format\", &data_format));\n    OP_REQUIRES(context, FormatFromString(data_format, &data_format_),\n                errors::InvalidArgument(\"Invalid data format\"));\n    if (context->device_type() == DEVICE_CPU) {\n      OP_REQUIRES(\n          context, data_format_ == FORMAT_NHWC,\n          errors::InvalidArgument(\n              \"Default AvgPooling3dGradOp only supports NDHWC \",\n              \"on device type \", DeviceTypeString(context->device_type())));\n    }\n    OP_REQUIRES_OK(context, context->GetAttr(\"ksize\", &ksize_));\n    OP_REQUIRES(context, ksize_.size() == 5,\n                errors::InvalidArgument(\"Sliding window ksize field must \"\n                                        \"specify 5 dimensions\"));\n    OP_REQUIRES_OK(context, context->GetAttr(\"strides\", &stride_));\n    OP_REQUIRES(context, stride_.size() == 5,\n                errors::InvalidArgument(\"Sliding window stride field must \"\n                                        \"specify 5 dimensions\"));\n    OP_REQUIRES_OK(context, context->GetAttr(\"padding\", &padding_));\n    OP_REQUIRES(context,\n                (GetTensorDim(ksize_, data_format_, 'N') == 1 &&\n                 GetTensorDim(stride_, data_format_, 'N') == 1),\n                errors::Unimplemented(\n                    \"Pooling is not yet supported on the batch dimension.\"));\n    OP_REQUIRES(context,\n                (GetTensorDim(ksize_, data_format_, 'C') == 1 &&\n                 GetTensorDim(stride_, data_format_, 'C') == 1),\n                errors::Unimplemented(\n                    \"Pooling is not yet supported on the depth dimension.\"));\n  }\n\n  void Compute(OpKernelContext* context) override {\n    const Tensor& tensor_in_shape = context->input(0);\n    const Tensor& out_backprop = context->input(1);\n    OP_REQUIRES(\n        context,\n        tensor_in_shape.dims() == 1 && tensor_in_shape.NumElements() == 5,\n        errors::InvalidArgument(\"tensor_in must be 1-dimensional and 5 \"\n                                \"elements\"));\n    OP_REQUIRES(context, out_backprop.dims() == 5,\n                errors::InvalidArgument(\"out_backprop must be 5-dimensional\"));\n\n    TensorShape output_shape;\n    auto shape_vec = tensor_in_shape.vec<int32>();\n    for (int64 i = 0; i < tensor_in_shape.NumElements(); ++i) {\n      output_shape.AddDim(shape_vec(i));\n    }\n\n    Tensor* output;\n    OP_REQUIRES_OK(context, context->allocate_output(0, output_shape, &output));\n\n    // Dimension order for these arrays is x, y, z.\n    std::array<int64, 3> input_size{\n        {GetTensorDim(output_shape, data_format_, '2'),\n         GetTensorDim(output_shape, data_format_, '1'),\n         GetTensorDim(output_shape, data_format_, '0')}};\n    std::array<int64, 3> window{{GetTensorDim(ksize_, data_format_, '2'),\n                                 GetTensorDim(ksize_, data_format_, '1'),\n                                 GetTensorDim(ksize_, data_format_, '0')}};\n    std::array<int64, 3> stride{{GetTensorDim(stride_, data_format_, '2'),\n                                 GetTensorDim(stride_, data_format_, '1'),\n                                 GetTensorDim(stride_, data_format_, '0')}};\n    std::array<int64, 3> padding, out;\n\n    OP_REQUIRES_OK(context, Get3dOutputSize(input_size, window, stride,\n                                            padding_, &out, &padding));\n\n    LaunchAvgPooling3dGradOp<Device, T>::launch(\n        context, output_shape, out_backprop, window, stride, out, padding,\n        data_format_, output);\n  }\n\n private:\n  std::vector<int32> ksize_;\n  std::vector<int32> stride_;\n  Padding padding_;\n  TensorFormat data_format_;\n};\n\ntemplate <typename T>\nstruct LaunchMaxPooling3dGradGradOp<CPUDevice, T> {\n  static void launch(OpKernelContext* context, const Pool3dParameters& params,\n                     const Tensor& tensor_in, const Tensor& tensor_out,\n                     const Tensor& tensor_top_diff,\n                     Tensor* tensor_bottom_diff) {\n    OP_REQUIRES(\n        context, params.data_format == FORMAT_NHWC,\n        errors::InvalidArgument(\"Default MaxPooling3dGradGradOp only supports\",\n                                \"NDHWC on CPU device type\"));\n\n    typedef Eigen::Map<const Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>\n        ConstEigenMatrixMap;\n    typedef Eigen::Map<Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>\n        EigenMatrixMap;\n\n    ConstEigenMatrixMap in_mat(tensor_in.flat<T>().data(), params.depth,\n                               params.tensor_in_planes * params.tensor_in_cols *\n                                   params.tensor_in_rows *\n                                   params.tensor_in_batch);\n    ConstEigenMatrixMap out_mat(tensor_out.flat<T>().data(), params.depth,\n                                params.out_plane * params.out_width *\n                                    params.out_height * params.tensor_in_batch);\n    ConstEigenMatrixMap top_diff_mat(\n        tensor_top_diff.flat<T>().data(), params.depth,\n        params.tensor_in_planes * params.tensor_in_cols *\n            params.tensor_in_rows * params.tensor_in_batch);\n    EigenMatrixMap bottom_diff_mat(\n        tensor_bottom_diff->flat<T>().data(), params.depth,\n        params.out_plane * params.out_width * params.out_height *\n            params.tensor_in_batch);\n\n    const DeviceBase::CpuWorkerThreads& worker_threads =\n        *(context->device()->tensorflow_cpu_worker_threads());\n\n    auto shard = [&params, &in_mat, &out_mat, &top_diff_mat, &bottom_diff_mat](\n                     int64 start, int64 limit) {\n      const int32 depth = params.depth;\n      const int32 in_planes = params.tensor_in_planes;\n      const int32 in_rows = params.tensor_in_rows;\n      const int32 in_cols = params.tensor_in_cols;\n      const int32 pad_planes = params.pad_planes;\n      const int32 pad_rows = params.pad_rows;\n      const int32 pad_cols = params.pad_cols;\n      const int32 window_planes = params.window_planes;\n      const int32 window_rows = params.window_rows;\n      const int32 window_cols = params.window_cols;\n      const int32 plane_stride = params.plane_stride;\n      const int32 row_stride = params.row_stride;\n      const int32 col_stride = params.col_stride;\n      const int32 out_plane = params.out_plane;\n      const int32 out_height = params.out_height;\n      const int32 out_width = params.out_width;\n\n      {\n        // Initializes the output grad backprop tensor with 0.\n        const int32 output_image_size =\n            out_plane * out_height * out_width * params.depth;\n        EigenMatrixMap bottom_diff_shard(\n            bottom_diff_mat.data() + start * output_image_size, 1,\n            (limit - start) * output_image_size);\n        bottom_diff_shard.setZero();\n      }\n\n      for (int b = start; b < limit; ++b) {\n        for (int pp = 0; pp < out_plane; ++pp) {\n          for (int ph = 0; ph < out_height; ++ph) {\n            for (int pw = 0; pw < out_width; ++pw) {\n              // (p_start, p_end) * (h_start, h_end) * (w_start, w_end) is the\n              // range that the input vector projects to.\n              int p_start = pp * plane_stride - pad_planes;\n              const int p_end = std::min(p_start + window_planes, in_planes);\n              int h_start = ph * row_stride - pad_rows;\n              const int h_end = std::min(h_start + window_rows, in_rows);\n              int w_start = pw * col_stride - pad_cols;\n              const int w_end = std::min(w_start + window_cols, in_cols);\n              p_start = std::max(p_start, 0);\n              h_start = std::max(h_start, 0);\n              w_start = std::max(w_start, 0);\n              const int out_index =\n                  ((b * out_plane + pp) * out_height + ph) * out_width + pw;\n              // Find value corresponding to the input maximum in top_diff.\n              for (int d = 0; d < depth; ++d) {\n                const T& output_ref = out_mat.coeffRef(d, out_index);\n                bool should_stop = false;\n                for (int p = p_start; p < p_end && !should_stop; ++p) {\n                  for (int h = h_start; h < h_end && !should_stop; ++h) {\n                    for (int w = w_start; w < w_end && !should_stop; ++w) {\n                      const int in_index =\n                          ((b * in_planes + p) * in_rows + h) * in_cols + w;\n                      const T& input_ref = in_mat.coeffRef(d, in_index);\n                      if (output_ref == input_ref) {\n                        T& bottom_diff_ref =\n                            bottom_diff_mat.coeffRef(d, out_index);\n                        bottom_diff_ref = top_diff_mat.coeffRef(d, in_index);\n                        should_stop = true;\n                      }\n                    }\n                  }\n                }\n              }\n            }\n          }\n        }\n      }\n    };\n    const int64 shard_cost =\n        params.out_plane * params.out_height * params.out_width * params.depth *\n        params.window_planes * params.window_rows * params.window_cols;\n    Shard(worker_threads.num_threads, worker_threads.workers,\n          params.tensor_in_batch, shard_cost, shard);\n  }\n};\n\ntemplate <class Device, class T>\nclass MaxPooling3dGradGradOp : public OpKernel {\n public:\n  explicit MaxPooling3dGradGradOp(OpKernelConstruction* context)\n      : OpKernel(context) {\n    string data_format;\n    OP_REQUIRES_OK(context, context->GetAttr(\"data_format\", &data_format));\n    OP_REQUIRES(context, FormatFromString(data_format, &data_format_),\n                errors::InvalidArgument(\"Invalid data format\"));\n    OP_REQUIRES_OK(context, context->GetAttr(\"ksize\", &ksize_));\n    OP_REQUIRES(context, ksize_.size() == 5,\n                errors::InvalidArgument(\"Sliding window ksize field must \"\n                                        \"specify 5 dimensions\"));\n    OP_REQUIRES_OK(context, context->GetAttr(\"strides\", &stride_));\n    OP_REQUIRES(context, stride_.size() == 5,\n                errors::InvalidArgument(\"Sliding window strides field must \"\n                                        \"specify 5 dimensions\"));\n    OP_REQUIRES_OK(context, context->GetAttr(\"padding\", &padding_));\n    OP_REQUIRES(context, ksize_[0] == 1 && stride_[0] == 1,\n                errors::Unimplemented(\n                    \"Pooling is not yet supported on the batch dimension.\"));\n    const int32 ksize_c = GetTensorDim(ksize_, data_format_, 'C');\n    const int32 stride_c = GetTensorDim(stride_, data_format_, 'C');\n    OP_REQUIRES(context, ksize_c == 1 && stride_c == 1,\n                errors::Unimplemented(\"MaxPooling3dGradGrad is not yet \"\n                                      \"supported on the depth dimension.\"));\n  }\n\n  void Compute(OpKernelContext* context) override {\n    const Tensor& tensor_in = context->input(0);\n    const Tensor& tensor_out = context->input(1);\n    const Tensor& out_grad_backprop = context->input(2);\n\n    // For maxpooling3d, tensor_in should have 5 dimensions.\n    OP_REQUIRES(context, tensor_in.dims() == 5,\n                errors::InvalidArgument(\"tensor_in must be 5-dimensional\"));\n    OP_REQUIRES(context, tensor_out.dims() == 5,\n                errors::InvalidArgument(\"tensor_out must be 5-dimensional\"));\n    // For maxpooling3d, out_grad_backprop should have 5 dimensions.\n    OP_REQUIRES(\n        context, out_grad_backprop.dims() == 5,\n        errors::InvalidArgument(\"out_grad_backprop must be 5-dimensional\"));\n\n    Pool3dParameters params{context,  ksize_,       stride_,\n                            padding_, data_format_, tensor_in.shape()};\n    if (!context->status().ok()) return;  // params is invalid\n\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                {2}, 0, tensor_out.shape(), &output));\n\n    // Given access patterns in LaunchMaxPooling3dGradGradOp, these tensors must\n    // have elements.\n    OP_REQUIRES(context, tensor_in.NumElements() > 0,\n                errors::InvalidArgument(\"received empty tensor tensor_in: \",\n                                        tensor_in.DebugString()));\n    OP_REQUIRES(context, tensor_out.NumElements() > 0,\n                errors::InvalidArgument(\"received empty tensor tensor_out: \",\n                                        tensor_out.DebugString()));\n    OP_REQUIRES(\n        context, out_grad_backprop.NumElements() > 0,\n        errors::InvalidArgument(\"received empty tensor out_grad_backprop: \",\n                                out_grad_backprop.DebugString()));\n    OP_REQUIRES(context,\n                tensor_in.NumElements() == out_grad_backprop.NumElements(),\n                errors::InvalidArgument(\"tensor_in and out_grad_backprop must \"\n                                        \"have same number of elements, got <\",\n                                        tensor_in.DebugString(), \"> and <\",\n                                        out_grad_backprop.DebugString(), \">\"));\n    OP_REQUIRES(\n        context, tensor_out.NumElements() == output->NumElements(),\n        errors::InvalidArgument(\n            \"tensor_out and output must have same number of elements, got <\",\n            tensor_out.DebugString(), \"> and <\", output->DebugString(), \">\"));\n\n    LaunchMaxPooling3dGradGradOp<Device, T>::launch(\n        context, params, tensor_in, tensor_out, out_grad_backprop, output);\n  }\n\n private:\n  std::vector<int32> ksize_;\n  std::vector<int32> stride_;\n  Padding padding_;\n  TensorFormat data_format_;\n};\n\n#define REGISTER_KERNELS(D, T)                                             \\\n  REGISTER_KERNEL_BUILDER(                                                 \\\n      Name(\"MaxPool3D\").Device(DEVICE_##D).TypeConstraint<T>(\"T\"),         \\\n      Pooling3DOp<D##Device, T, MAX>);                                     \\\n  REGISTER_KERNEL_BUILDER(Name(\"MaxPool3DGrad\")                            \\\n                              .Device(DEVICE_##D)                          \\\n                              .TypeConstraint<T>(\"T\")                      \\\n                              .TypeConstraint<T>(\"TInput\"),                \\\n                          MaxPooling3dGradOp<D##Device, T>);               \\\n  REGISTER_KERNEL_BUILDER(                                                 \\\n      Name(\"MaxPool3DGradGrad\").Device(DEVICE_##D).TypeConstraint<T>(\"T\"), \\\n      MaxPooling3dGradGradOp<D##Device, T>);                               \\\n  REGISTER_KERNEL_BUILDER(                                                 \\\n      Name(\"AvgPool3D\").Device(DEVICE_##D).TypeConstraint<T>(\"T\"),         \\\n      Pooling3DOp<D##Device, T, AVG>);                                     \\\n  REGISTER_KERNEL_BUILDER(Name(\"AvgPool3DGrad\")                            \\\n                              .Device(DEVICE_##D)                          \\\n                              .TypeConstraint<T>(\"T\")                      \\\n                              .HostMemory(\"orig_input_shape\"),             \\\n                          AvgPooling3dGradOp<D##Device, T>);\n\n#define REGISTER_CPU_KERNELS(T) REGISTER_KERNELS(CPU, T)\nTF_CALL_float(REGISTER_CPU_KERNELS);\n#undef REGISTER_CPU_KERNELS\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\ntemplate <typename T>\nstruct LaunchPoolingOp<GPUDevice, T, AVG> {\n  static void launch(OpKernelContext* context, const Tensor& tensor_in,\n                     const std::array<int64, 3>& window,\n                     const std::array<int64, 3>& stride,\n                     const std::array<int64, 3>& padding,\n                     TensorFormat data_format, Padding padding_type,\n                     Tensor* output) {\n    DnnPooling3dOp<T>::Compute(context, se::dnn::PoolingMode::kAverage, window,\n                               stride, padding, data_format, tensor_in, output);\n  }\n};\n\ntemplate <typename T>\nstruct LaunchPoolingOp<GPUDevice, T, MAX> {\n  static void launch(OpKernelContext* context, const Tensor& tensor_in,\n                     const std::array<int64, 3>& window,\n                     const std::array<int64, 3>& stride,\n                     const std::array<int64, 3>& padding,\n                     TensorFormat data_format, Padding padding_type,\n                     Tensor* output) {\n    DnnPooling3dOp<T>::Compute(context, se::dnn::PoolingMode::kMaximum, window,\n                               stride, padding, data_format, tensor_in, output);\n  }\n};\n\ntemplate <typename T>\nstruct LaunchMaxPooling3dGradOp<GPUDevice, T> {\n  static void launch(OpKernelContext* context, const Tensor& tensor_in,\n                     const Tensor& tensor_out, const Tensor& out_backprop,\n                     const std::array<int64, 3>& window,\n                     const std::array<int64, 3>& stride,\n                     const std::array<int64, 3>& out,\n                     const std::array<int64, 3>& padding,\n                     TensorFormat data_format, Tensor* input_backprop) {\n    const TensorShape output_shape = tensor_in.shape();\n    DnnPooling3dGradOp<T>::Compute(context, se::dnn::PoolingMode::kMaximum,\n                                   window, stride, padding, out, data_format,\n                                   out_backprop, output_shape, &tensor_in,\n                                   &tensor_out, input_backprop);\n  }\n};\n\ntemplate <typename T>\nstruct LaunchAvgPooling3dGradOp<GPUDevice, T> {\n  static void launch(OpKernelContext* context,\n                     const TensorShape& tensor_in_shape,\n                     const Tensor& out_backprop,\n                     const std::array<int64, 3>& window,\n                     const std::array<int64, 3>& stride,\n                     const std::array<int64, 3>& out,\n                     const std::array<int64, 3>& padding,\n                     TensorFormat data_format, Tensor* output) {\n    DnnPooling3dGradOp<T>::Compute(\n        context, se::dnn::PoolingMode::kAverage, window, stride, padding, out,\n        data_format, out_backprop, tensor_in_shape, nullptr, nullptr, output);\n  }\n};\n\ntemplate <typename T>\nstruct LaunchMaxPooling3dGradGradOp<GPUDevice, T> {\n  static void launch(OpKernelContext* context, const Pool3dParameters& params,\n                     const Tensor& tensor_in, const Tensor& tensor_out,\n                     const Tensor& tensor_top_diff,\n                     Tensor* tensor_bottom_diff) {\n    bool status = functor::MaxPool3dGradBackward<T>()(\n        params.data_format, tensor_in.flat<T>().data(),\n        tensor_out.flat<T>().data(), params.tensor_in_batch, params.out_plane,\n        params.out_height, params.out_width, params.depth,\n        params.tensor_in_planes, params.tensor_in_rows, params.tensor_in_cols,\n        params.window_planes, params.window_rows, params.window_cols,\n        params.plane_stride, params.row_stride, params.col_stride,\n        params.pad_planes, params.pad_rows, params.pad_cols,\n        tensor_top_diff.flat<T>().data(), tensor_bottom_diff->flat<T>().data(),\n        context->eigen_gpu_device());\n    if (!status) {\n      context->SetStatus(\n          errors::Internal(\"Failed launching MaxPool3dGradBackward\"));\n    }\n  }\n};\n\n#define REGISTER_GPU_KERNELS(T) REGISTER_KERNELS(GPU, T)\nTF_CALL_float(REGISTER_GPU_KERNELS) TF_CALL_half(REGISTER_GPU_KERNELS)\n#undef REGISTER_GPU_KERNELS\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n\n#undef REGISTER_KERNELS\n\n}  // namespace tensorflow\n"], "fixing_code": ["/* Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n#define EIGEN_USE_THREADS\n\n#include \"tensorflow/core/kernels/pooling_ops_3d.h\"\n\n#include <array>\n\n#include \"third_party/eigen3/Eigen/Core\"\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n#include \"tensorflow/core/framework/kernel_shape_util.h\"\n#include \"tensorflow/core/framework/numeric_op.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/register_types.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/tensor_shape.h\"\n#include \"tensorflow/core/framework/tensor_slice.h\"\n#include \"tensorflow/core/kernels/eigen_pooling.h\"\n#include \"tensorflow/core/kernels/ops_util.h\"\n#include \"tensorflow/core/lib/core/errors.h\"\n#include \"tensorflow/core/util/padding.h\"\n#include \"tensorflow/core/util/tensor_format.h\"\n#include \"tensorflow/core/util/work_sharder.h\"\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n#include \"tensorflow/core/kernels/cudnn_pooling_gpu.h\"\n#include \"tensorflow/core/kernels/pooling_ops_3d_gpu.h\"\n#endif\n\n\nnamespace tensorflow {\n\ntypedef Eigen::ThreadPoolDevice CPUDevice;\ntypedef Eigen::GpuDevice GPUDevice;\n\nPool3dParameters::Pool3dParameters(OpKernelContext* context,\n                                   const std::vector<int32>& ksize,\n                                   const std::vector<int32>& stride,\n                                   Padding padding, TensorFormat data_format,\n                                   const TensorShape& tensor_in_shape) {\n  // For maxpooling, tensor_in should have 4 dimensions.\n  OP_REQUIRES(context, tensor_in_shape.dims() == 5,\n              errors::InvalidArgument(\"tensor_in must be 4-dimensional\"));\n\n  this->data_format = data_format;\n  depth = GetTensorDim(tensor_in_shape, data_format, 'C');\n  tensor_in_planes = GetTensorDim(tensor_in_shape, data_format, '0');\n  tensor_in_rows = GetTensorDim(tensor_in_shape, data_format, '1');\n  tensor_in_cols = GetTensorDim(tensor_in_shape, data_format, '2');\n  tensor_in_batch = GetTensorDim(tensor_in_shape, data_format, 'N');\n  window_planes = GetTensorDim(ksize, data_format, '0');\n  window_rows = GetTensorDim(ksize, data_format, '1');\n  window_cols = GetTensorDim(ksize, data_format, '2');\n  depth_window = GetTensorDim(ksize, data_format, 'C');\n  plane_stride = GetTensorDim(stride, data_format, '0');\n  row_stride = GetTensorDim(stride, data_format, '1');\n  col_stride = GetTensorDim(stride, data_format, '2');\n  depth_stride = GetTensorDim(stride, data_format, 'C');\n\n  // We only support 3D pooling across plane/width/height. Depthwise\n  // pooling is not supported.\n  OP_REQUIRES(\n      context, depth_window == 1 && depth_stride == 1,\n      errors::Unimplemented(\n          \"Pooling3d only supports pooling across plane/width/height.\"));\n\n  OP_REQUIRES_OK(context, GetWindowedOutputSize(tensor_in_planes, window_planes,\n                                                plane_stride, padding,\n                                                &out_plane, &pad_planes));\n  OP_REQUIRES_OK(context,\n                 GetWindowedOutputSize(tensor_in_rows, window_rows, row_stride,\n                                       padding, &out_height, &pad_rows));\n  OP_REQUIRES_OK(context,\n                 GetWindowedOutputSize(tensor_in_cols, window_cols, col_stride,\n                                       padding, &out_width, &pad_cols));\n}\n\nTensorShape Pool3dParameters::forward_output_shape() {\n  return ShapeFromFormat(data_format, tensor_in_batch,\n                         {{out_plane, out_height, out_width}}, depth);\n}\n\ntemplate <typename T>\nstruct LaunchPoolingOp<CPUDevice, T, AVG> {\n  static void launch(OpKernelContext* context, const Tensor& tensor_in,\n                     const std::array<int64, 3>& window,\n                     const std::array<int64, 3>& stride,\n                     const std::array<int64, 3>& padding,\n                     TensorFormat data_format, Padding padding_type,\n                     Tensor* output) {\n    output->tensor<T, 5>().device(context->eigen_device<CPUDevice>()) =\n        Eigen::CuboidAvgPooling(tensor_in.tensor<T, 5>(), window[0], window[1],\n                                window[2], stride[0], stride[1], stride[2],\n                                BrainPadding2EigenPadding(padding_type));\n  }\n};\n\ntemplate <typename T>\nstruct LaunchPoolingOp<CPUDevice, T, MAX> {\n  static void launch(OpKernelContext* context, const Tensor& tensor_in,\n                     const std::array<int64, 3>& window,\n                     const std::array<int64, 3>& stride,\n                     const std::array<int64, 3>& padding,\n                     TensorFormat data_format, Padding padding_type,\n                     Tensor* output) {\n    output->tensor<T, 5>().device(context->eigen_device<CPUDevice>()) =\n        Eigen::CuboidMaxPooling(tensor_in.tensor<T, 5>(), window[0], window[1],\n                                window[2], stride[0], stride[1], stride[2],\n                                BrainPadding2EigenPadding(padding_type));\n  }\n};\n\ntemplate <typename Device, typename T, PoolingType Type>\nclass Pooling3DOp : public UnaryOp<T> {\n public:\n  explicit Pooling3DOp(OpKernelConstruction* context) : UnaryOp<T>(context) {\n    string data_format;\n    OP_REQUIRES_OK(context, context->GetAttr(\"data_format\", &data_format));\n    OP_REQUIRES(context, FormatFromString(data_format, &data_format_),\n                errors::InvalidArgument(\"Invalid data format\"));\n    if (context->device_type() == DEVICE_CPU) {\n      OP_REQUIRES(\n          context, data_format_ == FORMAT_NHWC,\n          errors::InvalidArgument(\"Default Pooling3DOp only supports NDHWC \",\n                                  \"on device type \",\n                                  DeviceTypeString(context->device_type())));\n    }\n    OP_REQUIRES_OK(context, context->GetAttr(\"ksize\", &ksize_));\n    OP_REQUIRES(context, ksize_.size() == 5,\n                errors::InvalidArgument(\"Sliding window ksize field must \"\n                                        \"specify 5 dimensions\"));\n    OP_REQUIRES_OK(context, context->GetAttr(\"strides\", &stride_));\n    OP_REQUIRES(context, stride_.size() == 5,\n                errors::InvalidArgument(\"Sliding window stride field must \"\n                                        \"specify 5 dimensions\"));\n    OP_REQUIRES_OK(context, context->GetAttr(\"padding\", &padding_));\n    OP_REQUIRES(context,\n                (GetTensorDim(ksize_, data_format_, 'N') == 1 &&\n                 GetTensorDim(stride_, data_format_, 'N') == 1),\n                errors::Unimplemented(\n                    \"Pooling is not yet supported on the batch dimension.\"));\n    OP_REQUIRES(context,\n                (GetTensorDim(ksize_, data_format_, 'C') == 1 &&\n                 GetTensorDim(stride_, data_format_, 'C') == 1),\n                errors::Unimplemented(\n                    \"Pooling is not yet supported on the depth dimension.\"));\n  }\n\n  void Compute(OpKernelContext* context) override {\n    const Tensor& tensor_in = context->input(0);\n\n    OP_REQUIRES(context, tensor_in.dims() == 5,\n                errors::InvalidArgument(\"tensor_in must be 5-dimensional\"));\n    const int64 depth = GetTensorDim(tensor_in, data_format_, 'C');\n    const int64 in_batch = GetTensorDim(tensor_in, data_format_, 'N');\n\n    // Dimension order for these arrays is: x, y, z.\n    std::array<int64, 3> input_size{\n        {GetTensorDim(tensor_in, data_format_, '2'),\n         GetTensorDim(tensor_in, data_format_, '1'),\n         GetTensorDim(tensor_in, data_format_, '0')}};\n    std::array<int64, 3> window{{GetTensorDim(ksize_, data_format_, '2'),\n                                 GetTensorDim(ksize_, data_format_, '1'),\n                                 GetTensorDim(ksize_, data_format_, '0')}};\n    std::array<int64, 3> stride{{GetTensorDim(stride_, data_format_, '2'),\n                                 GetTensorDim(stride_, data_format_, '1'),\n                                 GetTensorDim(stride_, data_format_, '0')}};\n    std::array<int64, 3> padding, out;\n\n    OP_REQUIRES_OK(context, Get3dOutputSize(input_size, window, stride,\n                                            padding_, &out, &padding));\n\n    TensorShape out_shape = ShapeFromFormat(data_format_, in_batch,\n                                            {{out[2], out[1], out[0]}}, depth);\n    Tensor* output;\n    OP_REQUIRES_OK(context, context->allocate_output(0, out_shape, &output));\n    if (out_shape.num_elements() == 0) return;\n    LaunchPoolingOp<Device, T, Type>::launch(context, tensor_in, window, stride,\n                                             padding, data_format_, padding_,\n                                             output);\n  }\n\n private:\n  std::vector<int32> ksize_;\n  std::vector<int32> stride_;\n  Padding padding_;\n  TensorFormat data_format_;\n};\n\ntemplate <typename T>\nstruct LaunchMaxPooling3dGradOp<CPUDevice, T> {\n  static void launch(OpKernelContext* context, const Tensor& tensor_in,\n                     const Tensor& tensor_out, const Tensor& out_backprop,\n                     const std::array<int64, 3>& window,\n                     const std::array<int64, 3>& stride,\n                     const std::array<int64, 3>& out,\n                     const std::array<int64, 3>& padding,\n                     TensorFormat data_format, Tensor* output) {\n    output->flat<T>().setZero();\n    for (int64 p = 0; p < out_backprop.dim_size(3); ++p) {\n      // Calculate broadcast size for planes/rows/cols. For SAME padding,\n      // current index could be in the padding area, and\n      //   p * stride_planes + window_planes\n      // could be beyond the input tensor's boundary. In such cases, change\n      // the starting index and reduce the broadcast size.\n      //\n      // The same procedure is repeated for every spatial dimension in the\n      // nested loops below.\n      int pindex, psize;\n      std::array<int64, 3> input_size{{tensor_in.dim_size(3),\n                                       tensor_in.dim_size(2),\n                                       tensor_in.dim_size(1)}};\n      OP_REQUIRES_OK(context,\n                     GetBroadcastSize(p, input_size[0], window[0], stride[0],\n                                      padding[0], &pindex, &psize));\n      for (int64 r = 0; r < out_backprop.dim_size(2); ++r) {\n        int rindex, rsize;\n        OP_REQUIRES_OK(context,\n                       GetBroadcastSize(r, input_size[1], window[1], stride[1],\n                                        padding[1], &rindex, &rsize));\n        for (int64 c = 0; c < out_backprop.dim_size(1); ++c) {\n          int cindex, csize;\n          OP_REQUIRES_OK(\n              context, GetBroadcastSize(c, input_size[2], window[2], stride[2],\n                                        padding[2], &cindex, &csize));\n          TensorSlice src{{0, -1}, {c, 1}, {r, 1}, {p, 1}, {0, -1}};\n          TensorSlice dst{{0, -1},\n                          {cindex, csize},\n                          {rindex, rsize},\n                          {pindex, psize},\n                          {0, -1}};\n          Eigen::DSizes<Eigen::DenseIndex, 5> src_indices;\n          Eigen::DSizes<Eigen::DenseIndex, 5> src_sizes;\n          Eigen::DSizes<Eigen::DenseIndex, 5> dst_indices;\n          Eigen::DSizes<Eigen::DenseIndex, 5> dst_sizes;\n          src.FillIndicesAndSizes<5>(out_backprop.shape(), &src_indices,\n                                     &src_sizes);\n          dst.FillIndicesAndSizes<5>(tensor_in.shape(), &dst_indices,\n                                     &dst_sizes);\n\n#if !defined(EIGEN_HAS_INDEX_LIST)\n          Eigen::array<int, 5> bcast = {1, csize, rsize, psize, 1};\n#else\n          Eigen::IndexList<Eigen::type2index<1>, int, int, int,\n                           Eigen::type2index<1>>\n              bcast;\n          bcast.set(1, csize);\n          bcast.set(2, rsize);\n          bcast.set(3, psize);\n#endif\n\n          // Slice from tensor_in.\n          Eigen::Tensor<T, 5, Eigen::RowMajor> tensor_in_slice(dst_sizes);\n          tensor_in_slice.device(context->eigen_cpu_device()) =\n              tensor_in.tensor<T, 5>().slice(dst_indices, dst_sizes);\n\n          // Slice from tensor_out.\n          Eigen::Tensor<T, 5, Eigen::RowMajor> tensor_out_slice(src_sizes);\n          tensor_out_slice.device(context->eigen_cpu_device()) =\n              tensor_out.tensor<T, 5>().slice(src_indices, src_sizes);\n\n          // Backprop slice.\n          Eigen::Tensor<T, 5, Eigen::RowMajor> out_backprop_slice(src_sizes);\n          out_backprop_slice.device(context->eigen_cpu_device()) =\n              out_backprop.tensor<T, 5>().slice(src_indices, src_sizes);\n\n          // The true backprop slice: if an element is the max, choose\n          // the backprop slice; otherwise set to 0.\n          Eigen::Tensor<T, 5, Eigen::RowMajor> select_slice(dst_sizes);\n          Eigen::Tensor<T, 5, Eigen::RowMajor> mat0(dst_sizes);\n          mat0.setZero();\n          select_slice =\n              ((tensor_in_slice - tensor_out_slice.broadcast(bcast)).abs() <\n               tensor_in_slice.constant(1e-5))\n                  .select(out_backprop_slice.broadcast(bcast), mat0);\n\n          output->tensor<T, 5>()\n              .slice(dst_indices, dst_sizes)\n              .device(context->eigen_cpu_device()) += select_slice;\n        }\n      }\n    }\n  }\n};\n\ntemplate <class Device, class T>\nclass MaxPooling3dGradOp : public OpKernel {\n public:\n  explicit MaxPooling3dGradOp(OpKernelConstruction* context)\n      : OpKernel(context) {\n    string data_format;\n    OP_REQUIRES_OK(context, context->GetAttr(\"data_format\", &data_format));\n    OP_REQUIRES(context, FormatFromString(data_format, &data_format_),\n                errors::InvalidArgument(\"Invalid data format\"));\n    if (context->device_type() == DEVICE_CPU) {\n      OP_REQUIRES(\n          context, data_format_ == FORMAT_NHWC,\n          errors::InvalidArgument(\n              \"Default MaxPooling3dGradOp only supports NDHWC \",\n              \"on device type \", DeviceTypeString(context->device_type())));\n    }\n    OP_REQUIRES_OK(context, context->GetAttr(\"ksize\", &ksize_));\n    OP_REQUIRES(context, ksize_.size() == 5,\n                errors::InvalidArgument(\"Sliding window ksize field must \"\n                                        \"specify 5 dimensions\"));\n    OP_REQUIRES_OK(context, context->GetAttr(\"strides\", &stride_));\n    OP_REQUIRES(context, stride_.size() == 5,\n                errors::InvalidArgument(\"Sliding window stride field must \"\n                                        \"specify 5 dimensions\"));\n    OP_REQUIRES_OK(context, context->GetAttr(\"padding\", &padding_));\n    OP_REQUIRES(context,\n                (GetTensorDim(ksize_, data_format_, 'N') == 1 &&\n                 GetTensorDim(stride_, data_format_, 'N') == 1),\n                errors::Unimplemented(\n                    \"Pooling is not yet supported on the batch dimension.\"));\n    OP_REQUIRES(context,\n                (GetTensorDim(ksize_, data_format_, 'C') == 1 &&\n                 GetTensorDim(stride_, data_format_, 'C') == 1),\n                errors::Unimplemented(\n                    \"Pooling is not yet supported on the depth dimension.\"));\n  }\n\n  void Compute(OpKernelContext* context) override {\n    const Tensor& tensor_in = context->input(0);\n    const Tensor& tensor_out = context->input(1);\n    const Tensor& out_backprop = context->input(2);\n    OP_REQUIRES(context, tensor_in.dims() == 5,\n                errors::InvalidArgument(\"tensor_in must be 5-dimensional\"));\n    OP_REQUIRES(context, tensor_out.dims() == 5,\n                errors::InvalidArgument(\"tensor_out must be 5-dimensional\"));\n    OP_REQUIRES(context, out_backprop.dims() == 5,\n                errors::InvalidArgument(\"out_backprop must be 5-dimensional\"));\n\n    const TensorShape& output_shape = tensor_in.shape();\n    Tensor* input_backprop;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(0, output_shape, &input_backprop));\n    std::array<int64, 3> input_size{\n        {GetTensorDim(output_shape, data_format_, '2'),\n         GetTensorDim(output_shape, data_format_, '1'),\n         GetTensorDim(output_shape, data_format_, '0')}};\n    std::array<int64, 3> window{{GetTensorDim(ksize_, data_format_, '2'),\n                                 GetTensorDim(ksize_, data_format_, '1'),\n                                 GetTensorDim(ksize_, data_format_, '0')}};\n    std::array<int64, 3> stride{{GetTensorDim(stride_, data_format_, '2'),\n                                 GetTensorDim(stride_, data_format_, '1'),\n                                 GetTensorDim(stride_, data_format_, '0')}};\n    std::array<int64, 3> out, padding;\n\n    OP_REQUIRES_OK(context, Get3dOutputSize(input_size, window, stride,\n                                            padding_, &out, &padding));\n    LaunchMaxPooling3dGradOp<Device, T>::launch(\n        context, tensor_in, tensor_out, out_backprop, window, stride, out,\n        padding, data_format_, input_backprop);\n  }\n\n private:\n  std::vector<int32> ksize_;\n  std::vector<int32> stride_;\n  Padding padding_;\n  TensorFormat data_format_;\n};\n\ntemplate <typename T>\nstruct LaunchAvgPooling3dGradOp<CPUDevice, T> {\n  static void launch(OpKernelContext* context,\n                     const TensorShape& tensor_in_shape,\n                     const Tensor& out_backprop,\n                     const std::array<int64, 3>& window,\n                     const std::array<int64, 3>& stride,\n                     const std::array<int64, 3>& output_shape,\n                     const std::array<int64, 3>& padding,\n                     TensorFormat data_format, Tensor* output) {\n    OP_REQUIRES(\n        context, tensor_in_shape.dim_size(0) == out_backprop.dim_size(0),\n        errors::InvalidArgument(\n            \"Expected first dimension of tensor_in_shape and \"\n            \"out_backprop to match, got \",\n            tensor_in_shape.dim_size(0), \" and \", out_backprop.dim_size(0)));\n    OP_REQUIRES(\n        context, tensor_in_shape.dim_size(4) == out_backprop.dim_size(4),\n        errors::InvalidArgument(\n            \"Expected last dimension of tensor_in_shape and \"\n            \"out_backprop to match, got \",\n            tensor_in_shape.dim_size(4), \" and \", out_backprop.dim_size(4)));\n\n    output->flat<T>().setZero();\n    std::array<int64, 3> input_size = {{tensor_in_shape.dim_size(3),\n                                        tensor_in_shape.dim_size(2),\n                                        tensor_in_shape.dim_size(1)}};\n    for (int64 p = 0; p < out_backprop.dim_size(3); ++p) {\n      // Calculate broadcast size for planes/rows/cols. For SAME padding,\n      // current index could be in the padding area, and\n      //   p * stride_planes + window_planes\n      // could be beyond the input tensor's boundary. In such cases, change\n      // the starting index and reduce the broadcast size.\n      //\n      // The same procedure is repeated for every spatial dimension in the\n      // nested loops below.\n      int pindex, psize;\n      OP_REQUIRES_OK(context,\n                     GetBroadcastSize(p, input_size[0], window[0], stride[0],\n                                      padding[0], &pindex, &psize));\n      for (int64 r = 0; r < out_backprop.dim_size(2); ++r) {\n        int rindex, rsize;\n        OP_REQUIRES_OK(context,\n                       GetBroadcastSize(r, input_size[1], window[1], stride[1],\n                                        padding[1], &rindex, &rsize));\n        for (int64 c = 0; c < out_backprop.dim_size(1); ++c) {\n          int cindex, csize;\n          OP_REQUIRES_OK(\n              context, GetBroadcastSize(c, input_size[2], window[2], stride[2],\n                                        padding[2], &cindex, &csize));\n          TensorSlice src{{0, -1}, {c, 1}, {r, 1}, {p, 1}, {0, -1}};\n          TensorSlice dst{{0, -1},\n                          {cindex, csize},\n                          {rindex, rsize},\n                          {pindex, psize},\n                          {0, -1}};\n          Eigen::DSizes<Eigen::DenseIndex, 5> src_indices;\n          Eigen::DSizes<Eigen::DenseIndex, 5> src_sizes;\n          Eigen::DSizes<Eigen::DenseIndex, 5> dst_indices;\n          Eigen::DSizes<Eigen::DenseIndex, 5> dst_sizes;\n          src.FillIndicesAndSizes<5>(out_backprop.shape(), &src_indices,\n                                     &src_sizes);\n          dst.FillIndicesAndSizes<5>(tensor_in_shape, &dst_indices, &dst_sizes);\n#if !defined(EIGEN_HAS_INDEX_LIST)\n          Eigen::array<int, 5> bcast = {1, csize, rsize, psize, 1};\n#else\n          Eigen::IndexList<Eigen::type2index<1>, int, int, int,\n                           Eigen::type2index<1>>\n              bcast;\n          bcast.set(1, csize);\n          bcast.set(2, rsize);\n          bcast.set(3, psize);\n#endif\n          Eigen::Tensor<T, 5, Eigen::RowMajor> slices(src_sizes);\n          slices.device(context->eigen_cpu_device()) =\n              out_backprop.tensor<T, 5>().slice(src_indices, src_sizes);\n          // Divide by the size of the actual patch (psize * rsize * csize).\n          float divide_size = rsize * csize * psize * 1.0f;\n          slices *= slices.constant(1.0f / divide_size);\n\n          output->tensor<T, 5>()\n              .slice(dst_indices, dst_sizes)\n              .device(context->eigen_cpu_device()) += slices.broadcast(bcast);\n        }\n      }\n    }\n  }\n};\n\ntemplate <class Device, class T>\nclass AvgPooling3dGradOp : public OpKernel {\n public:\n  explicit AvgPooling3dGradOp(OpKernelConstruction* context)\n      : OpKernel(context) {\n    string data_format;\n    OP_REQUIRES_OK(context, context->GetAttr(\"data_format\", &data_format));\n    OP_REQUIRES(context, FormatFromString(data_format, &data_format_),\n                errors::InvalidArgument(\"Invalid data format\"));\n    if (context->device_type() == DEVICE_CPU) {\n      OP_REQUIRES(\n          context, data_format_ == FORMAT_NHWC,\n          errors::InvalidArgument(\n              \"Default AvgPooling3dGradOp only supports NDHWC \",\n              \"on device type \", DeviceTypeString(context->device_type())));\n    }\n    OP_REQUIRES_OK(context, context->GetAttr(\"ksize\", &ksize_));\n    OP_REQUIRES(context, ksize_.size() == 5,\n                errors::InvalidArgument(\"Sliding window ksize field must \"\n                                        \"specify 5 dimensions\"));\n    OP_REQUIRES_OK(context, context->GetAttr(\"strides\", &stride_));\n    OP_REQUIRES(context, stride_.size() == 5,\n                errors::InvalidArgument(\"Sliding window stride field must \"\n                                        \"specify 5 dimensions\"));\n    OP_REQUIRES_OK(context, context->GetAttr(\"padding\", &padding_));\n    OP_REQUIRES(context,\n                (GetTensorDim(ksize_, data_format_, 'N') == 1 &&\n                 GetTensorDim(stride_, data_format_, 'N') == 1),\n                errors::Unimplemented(\n                    \"Pooling is not yet supported on the batch dimension.\"));\n    OP_REQUIRES(context,\n                (GetTensorDim(ksize_, data_format_, 'C') == 1 &&\n                 GetTensorDim(stride_, data_format_, 'C') == 1),\n                errors::Unimplemented(\n                    \"Pooling is not yet supported on the depth dimension.\"));\n  }\n\n  void Compute(OpKernelContext* context) override {\n    const Tensor& tensor_in_shape = context->input(0);\n    const Tensor& out_backprop = context->input(1);\n    OP_REQUIRES(\n        context,\n        tensor_in_shape.dims() == 1 && tensor_in_shape.NumElements() == 5,\n        errors::InvalidArgument(\"tensor_in must be 1-dimensional and 5 \"\n                                \"elements\"));\n    OP_REQUIRES(context, out_backprop.dims() == 5,\n                errors::InvalidArgument(\"out_backprop must be 5-dimensional\"));\n\n    TensorShape output_shape;\n    auto shape_vec = tensor_in_shape.vec<int32>();\n    for (int64 i = 0; i < tensor_in_shape.NumElements(); ++i) {\n      output_shape.AddDim(shape_vec(i));\n    }\n\n    Tensor* output;\n    OP_REQUIRES_OK(context, context->allocate_output(0, output_shape, &output));\n\n    // Dimension order for these arrays is x, y, z.\n    std::array<int64, 3> input_size{\n        {GetTensorDim(output_shape, data_format_, '2'),\n         GetTensorDim(output_shape, data_format_, '1'),\n         GetTensorDim(output_shape, data_format_, '0')}};\n    std::array<int64, 3> window{{GetTensorDim(ksize_, data_format_, '2'),\n                                 GetTensorDim(ksize_, data_format_, '1'),\n                                 GetTensorDim(ksize_, data_format_, '0')}};\n    std::array<int64, 3> stride{{GetTensorDim(stride_, data_format_, '2'),\n                                 GetTensorDim(stride_, data_format_, '1'),\n                                 GetTensorDim(stride_, data_format_, '0')}};\n    std::array<int64, 3> padding, out;\n\n    OP_REQUIRES_OK(context, Get3dOutputSize(input_size, window, stride,\n                                            padding_, &out, &padding));\n\n    LaunchAvgPooling3dGradOp<Device, T>::launch(\n        context, output_shape, out_backprop, window, stride, out, padding,\n        data_format_, output);\n  }\n\n private:\n  std::vector<int32> ksize_;\n  std::vector<int32> stride_;\n  Padding padding_;\n  TensorFormat data_format_;\n};\n\ntemplate <typename T>\nstruct LaunchMaxPooling3dGradGradOp<CPUDevice, T> {\n  static void launch(OpKernelContext* context, const Pool3dParameters& params,\n                     const Tensor& tensor_in, const Tensor& tensor_out,\n                     const Tensor& tensor_top_diff,\n                     Tensor* tensor_bottom_diff) {\n    OP_REQUIRES(\n        context, params.data_format == FORMAT_NHWC,\n        errors::InvalidArgument(\"Default MaxPooling3dGradGradOp only supports\",\n                                \"NDHWC on CPU device type\"));\n\n    typedef Eigen::Map<const Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>\n        ConstEigenMatrixMap;\n    typedef Eigen::Map<Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>\n        EigenMatrixMap;\n\n    ConstEigenMatrixMap in_mat(tensor_in.flat<T>().data(), params.depth,\n                               params.tensor_in_planes * params.tensor_in_cols *\n                                   params.tensor_in_rows *\n                                   params.tensor_in_batch);\n    ConstEigenMatrixMap out_mat(tensor_out.flat<T>().data(), params.depth,\n                                params.out_plane * params.out_width *\n                                    params.out_height * params.tensor_in_batch);\n    ConstEigenMatrixMap top_diff_mat(\n        tensor_top_diff.flat<T>().data(), params.depth,\n        params.tensor_in_planes * params.tensor_in_cols *\n            params.tensor_in_rows * params.tensor_in_batch);\n    EigenMatrixMap bottom_diff_mat(\n        tensor_bottom_diff->flat<T>().data(), params.depth,\n        params.out_plane * params.out_width * params.out_height *\n            params.tensor_in_batch);\n\n    const DeviceBase::CpuWorkerThreads& worker_threads =\n        *(context->device()->tensorflow_cpu_worker_threads());\n\n    auto shard = [&params, &in_mat, &out_mat, &top_diff_mat, &bottom_diff_mat](\n                     int64 start, int64 limit) {\n      const int32 depth = params.depth;\n      const int32 in_planes = params.tensor_in_planes;\n      const int32 in_rows = params.tensor_in_rows;\n      const int32 in_cols = params.tensor_in_cols;\n      const int32 pad_planes = params.pad_planes;\n      const int32 pad_rows = params.pad_rows;\n      const int32 pad_cols = params.pad_cols;\n      const int32 window_planes = params.window_planes;\n      const int32 window_rows = params.window_rows;\n      const int32 window_cols = params.window_cols;\n      const int32 plane_stride = params.plane_stride;\n      const int32 row_stride = params.row_stride;\n      const int32 col_stride = params.col_stride;\n      const int32 out_plane = params.out_plane;\n      const int32 out_height = params.out_height;\n      const int32 out_width = params.out_width;\n\n      {\n        // Initializes the output grad backprop tensor with 0.\n        const int32 output_image_size =\n            out_plane * out_height * out_width * params.depth;\n        EigenMatrixMap bottom_diff_shard(\n            bottom_diff_mat.data() + start * output_image_size, 1,\n            (limit - start) * output_image_size);\n        bottom_diff_shard.setZero();\n      }\n\n      for (int b = start; b < limit; ++b) {\n        for (int pp = 0; pp < out_plane; ++pp) {\n          for (int ph = 0; ph < out_height; ++ph) {\n            for (int pw = 0; pw < out_width; ++pw) {\n              // (p_start, p_end) * (h_start, h_end) * (w_start, w_end) is the\n              // range that the input vector projects to.\n              int p_start = pp * plane_stride - pad_planes;\n              const int p_end = std::min(p_start + window_planes, in_planes);\n              int h_start = ph * row_stride - pad_rows;\n              const int h_end = std::min(h_start + window_rows, in_rows);\n              int w_start = pw * col_stride - pad_cols;\n              const int w_end = std::min(w_start + window_cols, in_cols);\n              p_start = std::max(p_start, 0);\n              h_start = std::max(h_start, 0);\n              w_start = std::max(w_start, 0);\n              const int out_index =\n                  ((b * out_plane + pp) * out_height + ph) * out_width + pw;\n              // Find value corresponding to the input maximum in top_diff.\n              for (int d = 0; d < depth; ++d) {\n                const T& output_ref = out_mat.coeffRef(d, out_index);\n                bool should_stop = false;\n                for (int p = p_start; p < p_end && !should_stop; ++p) {\n                  for (int h = h_start; h < h_end && !should_stop; ++h) {\n                    for (int w = w_start; w < w_end && !should_stop; ++w) {\n                      const int in_index =\n                          ((b * in_planes + p) * in_rows + h) * in_cols + w;\n                      const T& input_ref = in_mat.coeffRef(d, in_index);\n                      if (output_ref == input_ref) {\n                        T& bottom_diff_ref =\n                            bottom_diff_mat.coeffRef(d, out_index);\n                        bottom_diff_ref = top_diff_mat.coeffRef(d, in_index);\n                        should_stop = true;\n                      }\n                    }\n                  }\n                }\n              }\n            }\n          }\n        }\n      }\n    };\n    const int64 shard_cost =\n        params.out_plane * params.out_height * params.out_width * params.depth *\n        params.window_planes * params.window_rows * params.window_cols;\n    Shard(worker_threads.num_threads, worker_threads.workers,\n          params.tensor_in_batch, shard_cost, shard);\n  }\n};\n\ntemplate <class Device, class T>\nclass MaxPooling3dGradGradOp : public OpKernel {\n public:\n  explicit MaxPooling3dGradGradOp(OpKernelConstruction* context)\n      : OpKernel(context) {\n    string data_format;\n    OP_REQUIRES_OK(context, context->GetAttr(\"data_format\", &data_format));\n    OP_REQUIRES(context, FormatFromString(data_format, &data_format_),\n                errors::InvalidArgument(\"Invalid data format\"));\n    OP_REQUIRES_OK(context, context->GetAttr(\"ksize\", &ksize_));\n    OP_REQUIRES(context, ksize_.size() == 5,\n                errors::InvalidArgument(\"Sliding window ksize field must \"\n                                        \"specify 5 dimensions\"));\n    OP_REQUIRES_OK(context, context->GetAttr(\"strides\", &stride_));\n    OP_REQUIRES(context, stride_.size() == 5,\n                errors::InvalidArgument(\"Sliding window strides field must \"\n                                        \"specify 5 dimensions\"));\n    OP_REQUIRES_OK(context, context->GetAttr(\"padding\", &padding_));\n    OP_REQUIRES(context, ksize_[0] == 1 && stride_[0] == 1,\n                errors::Unimplemented(\n                    \"Pooling is not yet supported on the batch dimension.\"));\n    const int32 ksize_c = GetTensorDim(ksize_, data_format_, 'C');\n    const int32 stride_c = GetTensorDim(stride_, data_format_, 'C');\n    OP_REQUIRES(context, ksize_c == 1 && stride_c == 1,\n                errors::Unimplemented(\"MaxPooling3dGradGrad is not yet \"\n                                      \"supported on the depth dimension.\"));\n  }\n\n  void Compute(OpKernelContext* context) override {\n    const Tensor& tensor_in = context->input(0);\n    const Tensor& tensor_out = context->input(1);\n    const Tensor& out_grad_backprop = context->input(2);\n\n    // For maxpooling3d, tensor_in should have 5 dimensions.\n    OP_REQUIRES(context, tensor_in.dims() == 5,\n                errors::InvalidArgument(\"tensor_in must be 5-dimensional\"));\n    OP_REQUIRES(context, tensor_out.dims() == 5,\n                errors::InvalidArgument(\"tensor_out must be 5-dimensional\"));\n    // For maxpooling3d, out_grad_backprop should have 5 dimensions.\n    OP_REQUIRES(\n        context, out_grad_backprop.dims() == 5,\n        errors::InvalidArgument(\"out_grad_backprop must be 5-dimensional\"));\n\n    Pool3dParameters params{context,  ksize_,       stride_,\n                            padding_, data_format_, tensor_in.shape()};\n    if (!context->status().ok()) return;  // params is invalid\n\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                {2}, 0, tensor_out.shape(), &output));\n\n    // Given access patterns in LaunchMaxPooling3dGradGradOp, these tensors must\n    // have elements.\n    OP_REQUIRES(context, tensor_in.NumElements() > 0,\n                errors::InvalidArgument(\"received empty tensor tensor_in: \",\n                                        tensor_in.DebugString()));\n    OP_REQUIRES(context, tensor_out.NumElements() > 0,\n                errors::InvalidArgument(\"received empty tensor tensor_out: \",\n                                        tensor_out.DebugString()));\n    OP_REQUIRES(\n        context, out_grad_backprop.NumElements() > 0,\n        errors::InvalidArgument(\"received empty tensor out_grad_backprop: \",\n                                out_grad_backprop.DebugString()));\n    OP_REQUIRES(context,\n                tensor_in.NumElements() == out_grad_backprop.NumElements(),\n                errors::InvalidArgument(\"tensor_in and out_grad_backprop must \"\n                                        \"have same number of elements, got <\",\n                                        tensor_in.DebugString(), \"> and <\",\n                                        out_grad_backprop.DebugString(), \">\"));\n    OP_REQUIRES(\n        context, tensor_out.NumElements() == output->NumElements(),\n        errors::InvalidArgument(\n            \"tensor_out and output must have same number of elements, got <\",\n            tensor_out.DebugString(), \"> and <\", output->DebugString(), \">\"));\n\n    LaunchMaxPooling3dGradGradOp<Device, T>::launch(\n        context, params, tensor_in, tensor_out, out_grad_backprop, output);\n  }\n\n private:\n  std::vector<int32> ksize_;\n  std::vector<int32> stride_;\n  Padding padding_;\n  TensorFormat data_format_;\n};\n\n#define REGISTER_KERNELS(D, T)                                             \\\n  REGISTER_KERNEL_BUILDER(                                                 \\\n      Name(\"MaxPool3D\").Device(DEVICE_##D).TypeConstraint<T>(\"T\"),         \\\n      Pooling3DOp<D##Device, T, MAX>);                                     \\\n  REGISTER_KERNEL_BUILDER(Name(\"MaxPool3DGrad\")                            \\\n                              .Device(DEVICE_##D)                          \\\n                              .TypeConstraint<T>(\"T\")                      \\\n                              .TypeConstraint<T>(\"TInput\"),                \\\n                          MaxPooling3dGradOp<D##Device, T>);               \\\n  REGISTER_KERNEL_BUILDER(                                                 \\\n      Name(\"MaxPool3DGradGrad\").Device(DEVICE_##D).TypeConstraint<T>(\"T\"), \\\n      MaxPooling3dGradGradOp<D##Device, T>);                               \\\n  REGISTER_KERNEL_BUILDER(                                                 \\\n      Name(\"AvgPool3D\").Device(DEVICE_##D).TypeConstraint<T>(\"T\"),         \\\n      Pooling3DOp<D##Device, T, AVG>);                                     \\\n  REGISTER_KERNEL_BUILDER(Name(\"AvgPool3DGrad\")                            \\\n                              .Device(DEVICE_##D)                          \\\n                              .TypeConstraint<T>(\"T\")                      \\\n                              .HostMemory(\"orig_input_shape\"),             \\\n                          AvgPooling3dGradOp<D##Device, T>);\n\n#define REGISTER_CPU_KERNELS(T) REGISTER_KERNELS(CPU, T)\nTF_CALL_float(REGISTER_CPU_KERNELS);\n#undef REGISTER_CPU_KERNELS\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\ntemplate <typename T>\nstruct LaunchPoolingOp<GPUDevice, T, AVG> {\n  static void launch(OpKernelContext* context, const Tensor& tensor_in,\n                     const std::array<int64, 3>& window,\n                     const std::array<int64, 3>& stride,\n                     const std::array<int64, 3>& padding,\n                     TensorFormat data_format, Padding padding_type,\n                     Tensor* output) {\n    DnnPooling3dOp<T>::Compute(context, se::dnn::PoolingMode::kAverage, window,\n                               stride, padding, data_format, tensor_in, output);\n  }\n};\n\ntemplate <typename T>\nstruct LaunchPoolingOp<GPUDevice, T, MAX> {\n  static void launch(OpKernelContext* context, const Tensor& tensor_in,\n                     const std::array<int64, 3>& window,\n                     const std::array<int64, 3>& stride,\n                     const std::array<int64, 3>& padding,\n                     TensorFormat data_format, Padding padding_type,\n                     Tensor* output) {\n    DnnPooling3dOp<T>::Compute(context, se::dnn::PoolingMode::kMaximum, window,\n                               stride, padding, data_format, tensor_in, output);\n  }\n};\n\ntemplate <typename T>\nstruct LaunchMaxPooling3dGradOp<GPUDevice, T> {\n  static void launch(OpKernelContext* context, const Tensor& tensor_in,\n                     const Tensor& tensor_out, const Tensor& out_backprop,\n                     const std::array<int64, 3>& window,\n                     const std::array<int64, 3>& stride,\n                     const std::array<int64, 3>& out,\n                     const std::array<int64, 3>& padding,\n                     TensorFormat data_format, Tensor* input_backprop) {\n    const TensorShape output_shape = tensor_in.shape();\n    DnnPooling3dGradOp<T>::Compute(context, se::dnn::PoolingMode::kMaximum,\n                                   window, stride, padding, out, data_format,\n                                   out_backprop, output_shape, &tensor_in,\n                                   &tensor_out, input_backprop);\n  }\n};\n\ntemplate <typename T>\nstruct LaunchAvgPooling3dGradOp<GPUDevice, T> {\n  static void launch(OpKernelContext* context,\n                     const TensorShape& tensor_in_shape,\n                     const Tensor& out_backprop,\n                     const std::array<int64, 3>& window,\n                     const std::array<int64, 3>& stride,\n                     const std::array<int64, 3>& out,\n                     const std::array<int64, 3>& padding,\n                     TensorFormat data_format, Tensor* output) {\n    DnnPooling3dGradOp<T>::Compute(\n        context, se::dnn::PoolingMode::kAverage, window, stride, padding, out,\n        data_format, out_backprop, tensor_in_shape, nullptr, nullptr, output);\n  }\n};\n\ntemplate <typename T>\nstruct LaunchMaxPooling3dGradGradOp<GPUDevice, T> {\n  static void launch(OpKernelContext* context, const Pool3dParameters& params,\n                     const Tensor& tensor_in, const Tensor& tensor_out,\n                     const Tensor& tensor_top_diff,\n                     Tensor* tensor_bottom_diff) {\n    bool status = functor::MaxPool3dGradBackward<T>()(\n        params.data_format, tensor_in.flat<T>().data(),\n        tensor_out.flat<T>().data(), params.tensor_in_batch, params.out_plane,\n        params.out_height, params.out_width, params.depth,\n        params.tensor_in_planes, params.tensor_in_rows, params.tensor_in_cols,\n        params.window_planes, params.window_rows, params.window_cols,\n        params.plane_stride, params.row_stride, params.col_stride,\n        params.pad_planes, params.pad_rows, params.pad_cols,\n        tensor_top_diff.flat<T>().data(), tensor_bottom_diff->flat<T>().data(),\n        context->eigen_gpu_device());\n    if (!status) {\n      context->SetStatus(\n          errors::Internal(\"Failed launching MaxPool3dGradBackward\"));\n    }\n  }\n};\n\n#define REGISTER_GPU_KERNELS(T) REGISTER_KERNELS(GPU, T)\nTF_CALL_float(REGISTER_GPU_KERNELS) TF_CALL_half(REGISTER_GPU_KERNELS)\n#undef REGISTER_GPU_KERNELS\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n\n#undef REGISTER_KERNELS\n\n}  // namespace tensorflow\n"], "filenames": ["tensorflow/core/kernels/pooling_ops_3d.cc"], "buggy_code_start_loc": [385], "buggy_code_end_loc": [385], "fixing_code_start_loc": [386], "fixing_code_end_loc": [399], "type": "CWE-787", "message": "TensorFlow is an end-to-end open source platform for machine learning. The implementation of `tf.raw_ops.AvgPool3DGrad` is vulnerable to a heap buffer overflow. The implementation(https://github.com/tensorflow/tensorflow/blob/d80ffba9702dc19d1fac74fc4b766b3fa1ee976b/tensorflow/core/kernels/pooling_ops_3d.cc#L376-L450) assumes that the `orig_input_shape` and `grad` tensors have similar first and last dimensions but does not check that this assumption is validated. The fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range.", "other": {"cve": {"id": "CVE-2021-29577", "sourceIdentifier": "security-advisories@github.com", "published": "2021-05-14T20:15:14.153", "lastModified": "2021-05-20T15:36:52.857", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "TensorFlow is an end-to-end open source platform for machine learning. The implementation of `tf.raw_ops.AvgPool3DGrad` is vulnerable to a heap buffer overflow. The implementation(https://github.com/tensorflow/tensorflow/blob/d80ffba9702dc19d1fac74fc4b766b3fa1ee976b/tensorflow/core/kernels/pooling_ops_3d.cc#L376-L450) assumes that the `orig_input_shape` and `grad` tensors have similar first and last dimensions but does not check that this assumption is validated. The fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range."}, {"lang": "es", "value": "TensorFlow es una plataforma de c\u00f3digo abierto de extremo a extremo para el aprendizaje autom\u00e1tico.&#xa0;La implementaci\u00f3n de \"tf.raw_ops.AvgPool3DGrad\" es vulnerable a un desbordamiento del b\u00fafer de la pila.&#xa0;La implementaci\u00f3n (https://github.com/tensorflow/tensorflow/blob/d80ffba9702dc19d1fac74fc4b766b3fa1ee976b/tensorflow/core/kernels/pooling_ops_3d.cc#L376-L450) asume que las dimensiones al inicio y al final de los tensores \"orig_input_shape\" y \"grad\" son similares pero no comprueba que esta suposici\u00f3n est\u00e9 validada.&#xa0;La correcci\u00f3n ser\u00e1 incluida en TensorFlow versi\u00f3n 2.5.0.&#xa0;Tambi\u00e9n seleccionaremos este compromiso en TensorFlow versi\u00f3n 2.4.2, TensorFlow versi\u00f3n 2.3.3, TensorFlow versi\u00f3n 2.2.3 y TensorFlow versi\u00f3n 2.1.4, ya que estos tambi\u00e9n est\u00e1n afectados y a\u00fan est\u00e1n en el rango admitido"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.9}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:H/PR:L/UI:N/S:U/C:N/I:N/A:L", "attackVector": "LOCAL", "attackComplexity": "HIGH", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "LOW", "baseScore": 2.5, "baseSeverity": "LOW"}, "exploitabilityScore": 1.0, "impactScore": 1.4}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:P/I:P/A:P", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "PARTIAL", "availabilityImpact": "PARTIAL", "baseScore": 4.6}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.9, "impactScore": 6.4, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-787"}]}, {"source": "security-advisories@github.com", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-119"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionEndExcluding": "2.1.4", "matchCriteriaId": "323ABCCE-24EB-47CC-87F6-48C101477587"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.2.0", "versionEndExcluding": "2.2.3", "matchCriteriaId": "64ABA90C-0649-4BB0-89C9-83C14BBDCC0F"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.3.0", "versionEndExcluding": "2.3.3", "matchCriteriaId": "0F83E0CF-CBF6-4C24-8683-3E7A5DC95BA9"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.4.0", "versionEndExcluding": "2.4.2", "matchCriteriaId": "8259531B-A8AC-4F8B-B60F-B69DE4767C03"}]}]}], "references": [{"url": "https://github.com/tensorflow/tensorflow/commit/6fc9141f42f6a72180ecd24021c3e6b36165fe0d", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/security/advisories/GHSA-v6r6-84gr-92rm", "source": "security-advisories@github.com", "tags": ["Exploit", "Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/tensorflow/tensorflow/commit/6fc9141f42f6a72180ecd24021c3e6b36165fe0d"}}