{"buggy_code": ["/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include <math.h>\n\n#include <cstddef>\n\n#include \"tensorflow/lite/c/builtin_op_data.h\"\n#include \"tensorflow/lite/c/common.h\"\n#include \"tensorflow/lite/kernels/cpu_backend_context.h\"\n#include \"tensorflow/lite/kernels/internal/compatibility.h\"\n#include \"tensorflow/lite/kernels/internal/kernel_utils.h\"\n#include \"tensorflow/lite/kernels/internal/quantization_util.h\"\n#include \"tensorflow/lite/kernels/internal/tensor_ctypes.h\"\n#include \"tensorflow/lite/kernels/internal/tensor_utils.h\"\n#include \"tensorflow/lite/kernels/kernel_util.h\"\n#include \"tensorflow/lite/kernels/lstm_eval.h\"\n#include \"tensorflow/lite/kernels/lstm_shared.h\"\n\nnamespace tflite {\nnamespace ops {\nnamespace builtin {\nnamespace unidirectional_sequence_lstm {\nnamespace {\n\nstruct OpData {\n  // If the lstm is layer norm.\n  bool use_layer_norm;\n  // The scratch tensor index.\n  int scratch_tensor_index;\n  bool compute_row_sums = false;\n\n  lstm_eval::IntegerLstmParameter integer_lstm_param;\n};\n\nTfLiteStatus PopulateQuantizedLstmParams8x8_16(\n    TfLiteContext* context, TfLiteNode* node,\n    lstm_eval::IntegerLstmParameter* integer_lstm_param) {\n  // Calculate quantized clip for projection and cell.\n  const auto* params =\n      static_cast<TfLiteUnidirectionalSequenceLSTMParams*>(node->builtin_data);\n  const float cell_clip = params->cell_clip;\n  const float proj_clip = params->proj_clip;\n\n  const TfLiteTensor* cell_state =\n      GetVariableInput(context, node, lstm::full::kCellStateTensor);\n  TF_LITE_ENSURE(context, cell_state != nullptr);\n  TfLiteTensor* output_tensor;\n  TF_LITE_ENSURE_OK(\n      context,\n      GetOutputSafe(context, node, lstm::full::kOutputTensor, &output_tensor));\n\n  auto* cell_state_params =\n      static_cast<TfLiteAffineQuantization*>(cell_state->quantization.params);\n  auto* proj_params = static_cast<TfLiteAffineQuantization*>(\n      output_tensor->quantization.params);\n  if (cell_clip > 0.0) {\n    integer_lstm_param->quantized_cell_clip = static_cast<int16_t>(std::min(\n        std::max(cell_clip / cell_state_params->scale->data[0], -32768.0f),\n        32767.0f));\n  } else {\n    integer_lstm_param->quantized_cell_clip = 0;\n  }\n  if (proj_clip > 0.0) {\n    integer_lstm_param->quantized_proj_clip = static_cast<int8_t>(std::min(\n        std::max(proj_clip / proj_params->scale->data[0], -128.0f), 127.0f));\n  } else {\n    integer_lstm_param->quantized_proj_clip = 0;\n  }\n\n  // Calculate effective scales.\n  OpData* op_data = static_cast<OpData*>(node->user_data);\n  const bool use_layer_norm = op_data->use_layer_norm;\n\n  const TfLiteTensor* input;\n  TF_LITE_ENSURE_OK(\n      context, GetInputSafe(context, node, lstm::full::kInputTensor, &input));\n\n  const TfLiteTensor* input_to_input_weights = GetOptionalInputTensor(\n      context, node, lstm::full::kInputToInputWeightsTensor);\n  const TfLiteTensor* input_to_forget_weights;\n  TF_LITE_ENSURE_OK(\n      context,\n      GetInputSafe(context, node, lstm::full::kInputToForgetWeightsTensor,\n                   &input_to_forget_weights));\n  const TfLiteTensor* input_to_cell_weights;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node,\n                                          lstm::full::kInputToCellWeightsTensor,\n                                          &input_to_cell_weights));\n  const TfLiteTensor* input_to_output_weights;\n  TF_LITE_ENSURE_OK(\n      context,\n      GetInputSafe(context, node, lstm::full::kInputToOutputWeightsTensor,\n                   &input_to_output_weights));\n\n  const TfLiteTensor* recurrent_to_input_weights = GetOptionalInputTensor(\n      context, node, lstm::full::kRecurrentToInputWeightsTensor);\n  const TfLiteTensor* recurrent_to_forget_weights;\n  TF_LITE_ENSURE_OK(\n      context,\n      GetInputSafe(context, node, lstm::full::kRecurrentToForgetWeightsTensor,\n                   &recurrent_to_forget_weights));\n  const TfLiteTensor* recurrent_to_cell_weights;\n  TF_LITE_ENSURE_OK(\n      context,\n      GetInputSafe(context, node, lstm::full::kRecurrentToCellWeightsTensor,\n                   &recurrent_to_cell_weights));\n  const TfLiteTensor* recurrent_to_output_weights;\n  TF_LITE_ENSURE_OK(\n      context,\n      GetInputSafe(context, node, lstm::full::kRecurrentToOutputWeightsTensor,\n                   &recurrent_to_output_weights));\n\n  const TfLiteTensor* cell_to_input_weights = GetOptionalInputTensor(\n      context, node, lstm::full::kCellToInputWeightsTensor);\n  const TfLiteTensor* cell_to_forget_weights = GetOptionalInputTensor(\n      context, node, lstm::full::kCellToForgetWeightsTensor);\n  const TfLiteTensor* cell_to_output_weights = GetOptionalInputTensor(\n      context, node, lstm::full::kCellToOutputWeightsTensor);\n\n  const TfLiteTensor* input_layer_norm_coefficients = GetOptionalInputTensor(\n      context, node, lstm::full::kInputLayerNormCoefficientsTensor);\n  const TfLiteTensor* forget_layer_norm_coefficients = GetOptionalInputTensor(\n      context, node, lstm::full::kForgetLayerNormCoefficientsTensor);\n  const TfLiteTensor* cell_layer_norm_coefficients = GetOptionalInputTensor(\n      context, node, lstm::full::kCellLayerNormCoefficientsTensor);\n  const TfLiteTensor* output_layer_norm_coefficients = GetOptionalInputTensor(\n      context, node, lstm::full::kOutputLayerNormCoefficientsTensor);\n\n  const TfLiteTensor* projection_weights = GetOptionalInputTensor(\n      context, node, lstm::full::kProjectionWeightsTensor);\n\n  TfLiteTensor* output_state =\n      GetVariableInput(context, node, lstm::full::kOutputStateTensor);\n  TF_LITE_ENSURE(context, output_state != nullptr);\n\n  // Since we have already checked that weights are all there or none, we can\n  // check the existence of only one to get the condition.\n  const bool use_cifg = (input_to_input_weights == nullptr);\n  const bool use_peephole = (cell_to_output_weights != nullptr);\n  const bool use_projection = (projection_weights != nullptr);\n\n  // Get intermediate scales and zero points.\n  std::vector<float> intermediate_scale;\n  std::vector<int32> intermediate_zp;\n  for (int i = 0; i < 4; ++i) {\n    if (use_layer_norm) {\n      TfLiteTensor* intermediate;\n      TF_LITE_ENSURE_OK(context,\n                        GetIntermediatesSafe(context, node, i, &intermediate));\n      auto* params = static_cast<TfLiteAffineQuantization*>(\n          intermediate->quantization.params);\n      intermediate_scale.push_back(params->scale->data[0]);\n      intermediate_zp.push_back(params->zero_point->data[0]);\n    } else {\n      // Q3.12 for activation functions.\n      intermediate_scale.push_back(std::pow(2, -12));\n      intermediate_zp.push_back(0);\n    }\n  }\n  // In the absence of projection, hidden becomes otuput and this intermediate\n  // is ignored.\n  TfLiteTensor* hidden;\n  TF_LITE_ENSURE_OK(context, GetIntermediatesSafe(context, node, 4, &hidden));\n  auto* hidden_params =\n      static_cast<TfLiteAffineQuantization*>(hidden->quantization.params);\n  intermediate_scale.push_back(hidden_params->scale->data[0]);\n  intermediate_zp.push_back(hidden_params->zero_point->data[0]);\n\n  // Scales.\n  const float default_scale = 1.0;\n  float input_scale = default_scale;\n  float input_to_input_weight_scale = default_scale;\n  float recurrent_to_input_weight_scale = default_scale;\n  float cell_to_input_weight_scale = default_scale;\n  float input_to_forget_weight_scale = default_scale;\n  float recurrent_to_forget_weight_scale = default_scale;\n  float cell_to_forget_weight_scale = default_scale;\n  float input_to_cell_weight_scale = default_scale;\n  float recurrent_to_cell_weight_scale = default_scale;\n  float input_to_output_weight_scale = default_scale;\n  float recurrent_to_output_weight_scale = default_scale;\n  float cell_to_output_weight_scale = default_scale;\n  float projection_weight_scale = default_scale;\n  float layer_norm_input_scale = default_scale;\n  float layer_norm_forget_scale = default_scale;\n  float layer_norm_cell_scale = default_scale;\n  float layer_norm_output_scale = default_scale;\n  float output_state_scale = default_scale;\n  int cell_scale = 1;\n\n  // Effective scales.\n  float effective_input_to_input_scale = default_scale;\n  float effective_recurrent_to_input_scale = default_scale;\n  float effective_cell_to_input_scale = default_scale;\n  float effective_input_to_forget_scale = default_scale;\n  float effective_recurrent_to_forget_scale = default_scale;\n  float effective_cell_to_forget_scale = default_scale;\n  float effective_input_to_cell_scale = default_scale;\n  float effective_recurrent_to_cell_scale = default_scale;\n  float effective_input_to_output_scale = default_scale;\n  float effective_recurrent_to_output_scale = default_scale;\n  float effective_cell_to_output_scale = default_scale;\n  float effective_proj_scale = default_scale;\n  float effective_hidden_scale = default_scale;\n\n  // Populate scales.\n  if (!use_cifg) {\n    input_to_input_weight_scale = input_to_input_weights->params.scale;\n    recurrent_to_input_weight_scale = recurrent_to_input_weights->params.scale;\n  }\n\n  if (use_peephole) {\n    if (!use_cifg) {\n      cell_to_input_weight_scale = cell_to_input_weights->params.scale;\n    }\n    cell_to_forget_weight_scale = cell_to_forget_weights->params.scale;\n    cell_to_output_weight_scale = cell_to_output_weights->params.scale;\n  }\n\n  if (use_layer_norm) {\n    if (!use_cifg) {\n      layer_norm_input_scale = input_layer_norm_coefficients->params.scale;\n    }\n    layer_norm_forget_scale = forget_layer_norm_coefficients->params.scale;\n    layer_norm_cell_scale = cell_layer_norm_coefficients->params.scale;\n    layer_norm_output_scale = output_layer_norm_coefficients->params.scale;\n  }\n\n  if (use_projection) {\n    projection_weight_scale = projection_weights->params.scale;\n  }\n  output_state_scale = output_state->params.scale;\n\n  input_to_forget_weight_scale = input_to_forget_weights->params.scale;\n  input_to_cell_weight_scale = input_to_cell_weights->params.scale;\n  input_to_output_weight_scale = input_to_output_weights->params.scale;\n  recurrent_to_forget_weight_scale = recurrent_to_forget_weights->params.scale;\n  recurrent_to_cell_weight_scale = recurrent_to_cell_weights->params.scale;\n  recurrent_to_output_weight_scale = recurrent_to_output_weights->params.scale;\n\n  // Check cell state (already used above)\n  TF_LITE_ENSURE(context, CheckedLog2(cell_state->params.scale, &cell_scale));\n  // TF_LITE_ENSURE(context, cell_scale <= -9);\n  integer_lstm_param->cell_scale = cell_scale;\n  input_scale = input->params.scale;\n\n  // Calculate effective scales.\n  if (!use_cifg) {\n    effective_input_to_input_scale =\n        input_to_input_weight_scale * input_scale / intermediate_scale[0];\n    effective_recurrent_to_input_scale = recurrent_to_input_weight_scale *\n                                         output_state_scale /\n                                         intermediate_scale[0];\n  }\n  effective_input_to_forget_scale =\n      input_to_forget_weight_scale * input_scale / intermediate_scale[1];\n  effective_recurrent_to_forget_scale = recurrent_to_forget_weight_scale *\n                                        output_state_scale /\n                                        intermediate_scale[1];\n\n  effective_input_to_cell_scale =\n      input_to_cell_weight_scale * input_scale / intermediate_scale[2];\n  effective_recurrent_to_cell_scale = recurrent_to_cell_weight_scale *\n                                      output_state_scale /\n                                      intermediate_scale[2];\n\n  effective_input_to_output_scale =\n      input_to_output_weight_scale * input_scale / intermediate_scale[3];\n  effective_recurrent_to_output_scale = recurrent_to_output_weight_scale *\n                                        output_state_scale /\n                                        intermediate_scale[3];\n\n  effective_hidden_scale =\n      std::pow(2, -15) / intermediate_scale[4] * std::pow(2, -15);\n\n  effective_proj_scale =\n      projection_weight_scale * intermediate_scale[4] / output_state_scale;\n\n  if (use_peephole) {\n    if (!use_cifg) {\n      effective_cell_to_input_scale = std::pow(2, cell_scale) *  // NOLINT\n                                      cell_to_input_weight_scale /\n                                      intermediate_scale[0];\n    }\n    effective_cell_to_forget_scale = std::pow(2, cell_scale) *  // NOLINT\n                                     cell_to_forget_weight_scale /\n                                     intermediate_scale[1];\n    effective_cell_to_output_scale = std::pow(2, cell_scale) *  // NOLINT\n                                     cell_to_output_weight_scale /\n                                     intermediate_scale[3];\n  }\n\n  // Decompose scales.\n  QuantizeMultiplier(effective_input_to_input_scale,\n                     &integer_lstm_param->effective_input_to_input_scale_a,\n                     &integer_lstm_param->effective_input_to_input_scale_b);\n  QuantizeMultiplier(effective_recurrent_to_input_scale,\n                     &integer_lstm_param->effective_recurrent_to_input_scale_a,\n                     &integer_lstm_param->effective_recurrent_to_input_scale_b);\n  QuantizeMultiplier(effective_cell_to_input_scale,\n                     &integer_lstm_param->effective_cell_to_input_scale_a,\n                     &integer_lstm_param->effective_cell_to_input_scale_b);\n  QuantizeMultiplier(effective_input_to_forget_scale,\n                     &integer_lstm_param->effective_input_to_forget_scale_a,\n                     &integer_lstm_param->effective_input_to_forget_scale_b);\n  QuantizeMultiplier(\n      effective_recurrent_to_forget_scale,\n      &integer_lstm_param->effective_recurrent_to_forget_scale_a,\n      &integer_lstm_param->effective_recurrent_to_forget_scale_b);\n  QuantizeMultiplier(effective_cell_to_forget_scale,\n                     &integer_lstm_param->effective_cell_to_forget_scale_a,\n                     &integer_lstm_param->effective_cell_to_forget_scale_b);\n  QuantizeMultiplier(effective_input_to_cell_scale,\n                     &integer_lstm_param->effective_input_to_cell_scale_a,\n                     &integer_lstm_param->effective_input_to_cell_scale_b);\n  QuantizeMultiplier(effective_recurrent_to_cell_scale,\n                     &integer_lstm_param->effective_recurrent_to_cell_scale_a,\n                     &integer_lstm_param->effective_recurrent_to_cell_scale_b);\n  QuantizeMultiplier(effective_input_to_output_scale,\n                     &integer_lstm_param->effective_input_to_output_scale_a,\n                     &integer_lstm_param->effective_input_to_output_scale_b);\n  QuantizeMultiplier(\n      effective_recurrent_to_output_scale,\n      &integer_lstm_param->effective_recurrent_to_output_scale_a,\n      &integer_lstm_param->effective_recurrent_to_output_scale_b);\n  QuantizeMultiplier(effective_cell_to_output_scale,\n                     &integer_lstm_param->effective_cell_to_output_scale_a,\n                     &integer_lstm_param->effective_cell_to_output_scale_b);\n  QuantizeMultiplier(effective_proj_scale,\n                     &integer_lstm_param->effective_proj_scale_a,\n                     &integer_lstm_param->effective_proj_scale_b);\n  QuantizeMultiplier(effective_hidden_scale,\n                     &integer_lstm_param->effective_hidden_scale_a,\n                     &integer_lstm_param->effective_hidden_scale_b);\n  QuantizeMultiplier(layer_norm_input_scale,\n                     &integer_lstm_param->layer_norm_input_scale_a,\n                     &integer_lstm_param->layer_norm_input_scale_b);\n  QuantizeMultiplier(layer_norm_forget_scale,\n                     &integer_lstm_param->layer_norm_forget_scale_a,\n                     &integer_lstm_param->layer_norm_forget_scale_b);\n  QuantizeMultiplier(layer_norm_cell_scale,\n                     &integer_lstm_param->layer_norm_cell_scale_a,\n                     &integer_lstm_param->layer_norm_cell_scale_b);\n  QuantizeMultiplier(layer_norm_output_scale,\n                     &integer_lstm_param->layer_norm_output_scale_a,\n                     &integer_lstm_param->layer_norm_output_scale_b);\n\n  integer_lstm_param->hidden_zp = intermediate_zp[4];\n\n  // 10000 is used to make sure the kernel logic does not overflow.\n  if (!use_cifg) {\n    integer_lstm_param->input_variance_guard =\n        std::max(1, static_cast<int32_t>(10000 * layer_norm_input_scale));\n  }\n  integer_lstm_param->forget_variance_guard =\n      std::max(1, static_cast<int32_t>(10000 * layer_norm_forget_scale));\n  integer_lstm_param->cell_variance_guard =\n      std::max(1, static_cast<int32_t>(10000 * layer_norm_cell_scale));\n  integer_lstm_param->output_variance_guard =\n      std::max(1, static_cast<int32_t>(10000 * layer_norm_output_scale));\n\n  return kTfLiteOk;\n}\n\n}  // namespace\n\n// Temporary tensors\nenum TemporaryTensor {\n  kScratchBuffer = 0,\n  kInputQuantized = 1,\n  kOutputStateQuantized = 2,\n  kCellStateQuantized = 3,\n  kInputScalingFactors = 4,\n  kOutputStateScalingFactors = 5,\n  kProductScalingFactors = 6,\n  kRecoveredCellWeights = 7,\n  kAccumScratch = 8,\n  kInputZeroPoints = 9,\n  kOutputStateZeroPoints = 10,\n  kRowSums = 11,\n  kNumTemporaryTensors = 12,\n};\n\nvoid* Init(TfLiteContext* context, const char* buffer, size_t length) {\n  auto* op_data = new OpData();\n  context->AddTensors(context, kNumTemporaryTensors,\n                      &op_data->scratch_tensor_index);\n  return op_data;\n}\n\nvoid Free(TfLiteContext* context, void* buffer) {\n  delete reinterpret_cast<OpData*>(buffer);\n}\n\n// Check that input tensor dimensions matches with each other.\nTfLiteStatus CheckInputTensorDimensions(TfLiteContext* context,\n                                        TfLiteNode* node, int n_input,\n                                        int n_output, int n_cell,\n                                        bool use_layer_norm, bool is_integer) {\n  const auto* params = reinterpret_cast<TfLiteLSTMParams*>(node->builtin_data);\n\n  // Making sure clipping parameters have valid values.\n  // == 0 means no clipping\n  //  > 0 means clipping\n  TF_LITE_ENSURE(context, params->cell_clip >= 0);\n  TF_LITE_ENSURE(context, params->proj_clip >= 0);\n\n  const TfLiteTensor* input_to_input_weights = GetOptionalInputTensor(\n      context, node, lstm::full::kInputToInputWeightsTensor);\n  if (input_to_input_weights != nullptr) {\n    TF_LITE_ENSURE_EQ(context, input_to_input_weights->dims->size, 2);\n    TF_LITE_ENSURE_EQ(context, input_to_input_weights->dims->data[0], n_cell);\n    TF_LITE_ENSURE_EQ(context, input_to_input_weights->dims->data[1], n_input);\n  }\n\n  const TfLiteTensor* input_to_forget_weights;\n  TF_LITE_ENSURE_OK(\n      context,\n      GetInputSafe(context, node, lstm::full::kInputToForgetWeightsTensor,\n                   &input_to_forget_weights));\n  TF_LITE_ENSURE_EQ(context, input_to_forget_weights->dims->size, 2);\n  TF_LITE_ENSURE_EQ(context, input_to_forget_weights->dims->data[0], n_cell);\n  TF_LITE_ENSURE_EQ(context, input_to_forget_weights->dims->data[1], n_input);\n\n  const TfLiteTensor* input_to_cell_weights;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node,\n                                          lstm::full::kInputToCellWeightsTensor,\n                                          &input_to_cell_weights));\n  TF_LITE_ENSURE_EQ(context, input_to_cell_weights->dims->size, 2);\n  TF_LITE_ENSURE_EQ(context, input_to_cell_weights->dims->data[0], n_cell);\n  TF_LITE_ENSURE_EQ(context, input_to_cell_weights->dims->data[1], n_input);\n\n  const TfLiteTensor* recurrent_to_input_weights = GetOptionalInputTensor(\n      context, node, lstm::full::kRecurrentToInputWeightsTensor);\n  if (recurrent_to_input_weights != nullptr) {\n    TF_LITE_ENSURE_EQ(context, recurrent_to_input_weights->dims->size, 2);\n    TF_LITE_ENSURE_EQ(context, recurrent_to_input_weights->dims->data[0],\n                      n_cell);\n    TF_LITE_ENSURE_EQ(context, recurrent_to_input_weights->dims->data[1],\n                      n_output);\n  }\n\n  const TfLiteTensor* recurrent_to_forget_weights;\n  TF_LITE_ENSURE_OK(\n      context,\n      GetInputSafe(context, node, lstm::full::kRecurrentToForgetWeightsTensor,\n                   &recurrent_to_forget_weights));\n  TF_LITE_ENSURE_EQ(context, recurrent_to_forget_weights->dims->size, 2);\n  TF_LITE_ENSURE_EQ(context, recurrent_to_forget_weights->dims->data[0],\n                    n_cell);\n  TF_LITE_ENSURE_EQ(context, recurrent_to_forget_weights->dims->data[1],\n                    n_output);\n\n  const TfLiteTensor* recurrent_to_cell_weights;\n  TF_LITE_ENSURE_OK(\n      context,\n      GetInputSafe(context, node, lstm::full::kRecurrentToCellWeightsTensor,\n                   &recurrent_to_cell_weights));\n  TF_LITE_ENSURE_EQ(context, recurrent_to_cell_weights->dims->size, 2);\n  TF_LITE_ENSURE_EQ(context, recurrent_to_cell_weights->dims->data[0], n_cell);\n  TF_LITE_ENSURE_EQ(context, recurrent_to_cell_weights->dims->data[1],\n                    n_output);\n\n  // We make sure the input-gate's parameters are either both present (regular\n  // LSTM) or not at all (CIFG-LSTM).\n  const bool cifg_weights_all_or_none =\n      ((input_to_input_weights != nullptr) &&\n       (recurrent_to_input_weights != nullptr)) ||\n      ((input_to_input_weights == nullptr) &&\n       (recurrent_to_input_weights == nullptr));\n  TF_LITE_ENSURE(context, cifg_weights_all_or_none == true);\n\n  const TfLiteTensor* cell_to_input_weights = GetOptionalInputTensor(\n      context, node, lstm::full::kCellToInputWeightsTensor);\n  if (cell_to_input_weights != nullptr) {\n    TF_LITE_ENSURE_EQ(context, cell_to_input_weights->dims->size, 1);\n    TF_LITE_ENSURE_EQ(context, cell_to_input_weights->dims->data[0], n_cell);\n    TF_LITE_ENSURE_TYPES_EQ(\n        context, cell_to_input_weights->type,\n        is_integer ? kTfLiteInt16 : input_to_forget_weights->type);\n  }\n\n  const TfLiteTensor* cell_to_forget_weights = GetOptionalInputTensor(\n      context, node, lstm::full::kCellToForgetWeightsTensor);\n  if (cell_to_forget_weights != nullptr) {\n    TF_LITE_ENSURE_EQ(context, cell_to_forget_weights->dims->size, 1);\n    TF_LITE_ENSURE_EQ(context, cell_to_forget_weights->dims->data[0], n_cell);\n    TF_LITE_ENSURE_TYPES_EQ(\n        context, cell_to_forget_weights->type,\n        is_integer ? kTfLiteInt16 : input_to_forget_weights->type);\n  }\n\n  const TfLiteTensor* cell_to_output_weights = GetOptionalInputTensor(\n      context, node, lstm::full::kCellToOutputWeightsTensor);\n  if (cell_to_output_weights != nullptr) {\n    TF_LITE_ENSURE_EQ(context, cell_to_output_weights->dims->size, 1);\n    TF_LITE_ENSURE_EQ(context, cell_to_output_weights->dims->data[0], n_cell);\n    TF_LITE_ENSURE_TYPES_EQ(\n        context, cell_to_output_weights->type,\n        is_integer ? kTfLiteInt16 : input_to_forget_weights->type);\n  }\n\n  // Making sure the peephole weights are there all or none.\n  const bool use_cifg = (input_to_input_weights == nullptr);\n  const bool peephole_weights_all_or_none =\n      ((cell_to_input_weights != nullptr || use_cifg) &&\n       (cell_to_forget_weights != nullptr) &&\n       (cell_to_output_weights != nullptr)) ||\n      ((cell_to_input_weights == nullptr) &&\n       (cell_to_forget_weights == nullptr) &&\n       (cell_to_output_weights == nullptr));\n  TF_LITE_ENSURE(context, peephole_weights_all_or_none == true);\n\n  // Make sure the input gate bias is present only when not a CIFG-LSTM.\n  const TfLiteTensor* input_gate_bias =\n      GetOptionalInputTensor(context, node, lstm::full::kInputGateBiasTensor);\n  if (use_cifg) {\n    TF_LITE_ENSURE_EQ(context, input_gate_bias, nullptr);\n  } else {\n    TF_LITE_ENSURE_EQ(context, input_gate_bias->dims->size, 1);\n    TF_LITE_ENSURE_EQ(context, input_gate_bias->dims->data[0], n_cell);\n    if (is_integer) {\n      TF_LITE_ENSURE_TYPES_EQ(context, input_gate_bias->type, kTfLiteInt32);\n    } else {\n      TF_LITE_ENSURE_TYPES_EQ(context, input_gate_bias->type, kTfLiteFloat32);\n    }\n  }\n\n  const TfLiteTensor* forget_gate_bias;\n  TF_LITE_ENSURE_OK(\n      context, GetInputSafe(context, node, lstm::full::kForgetGateBiasTensor,\n                            &forget_gate_bias));\n  TF_LITE_ENSURE_EQ(context, forget_gate_bias->dims->size, 1);\n  TF_LITE_ENSURE_EQ(context, forget_gate_bias->dims->data[0], n_cell);\n  if (is_integer) {\n    TF_LITE_ENSURE_TYPES_EQ(context, forget_gate_bias->type, kTfLiteInt32);\n  } else {\n    TF_LITE_ENSURE_TYPES_EQ(context, forget_gate_bias->type, kTfLiteFloat32);\n  }\n\n  const TfLiteTensor* cell_gate_bias;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, lstm::full::kCellGateBiasTensor,\n                                 &cell_gate_bias));\n  TF_LITE_ENSURE_EQ(context, cell_gate_bias->dims->size, 1);\n  TF_LITE_ENSURE_EQ(context, cell_gate_bias->dims->data[0], n_cell);\n  if (is_integer) {\n    TF_LITE_ENSURE_TYPES_EQ(context, cell_gate_bias->type, kTfLiteInt32);\n  } else {\n    TF_LITE_ENSURE_TYPES_EQ(context, cell_gate_bias->type, kTfLiteFloat32);\n  }\n\n  const TfLiteTensor* output_gate_bias;\n  TF_LITE_ENSURE_OK(\n      context, GetInputSafe(context, node, lstm::full::kOutputGateBiasTensor,\n                            &output_gate_bias));\n  TF_LITE_ENSURE_EQ(context, output_gate_bias->dims->size, 1);\n  TF_LITE_ENSURE_EQ(context, output_gate_bias->dims->data[0], n_cell);\n  if (is_integer) {\n    TF_LITE_ENSURE_TYPES_EQ(context, output_gate_bias->type, kTfLiteInt32);\n  } else {\n    TF_LITE_ENSURE_TYPES_EQ(context, output_gate_bias->type, kTfLiteFloat32);\n  }\n\n  const TfLiteTensor* projection_weights = GetOptionalInputTensor(\n      context, node, lstm::full::kProjectionWeightsTensor);\n  if (projection_weights != nullptr) {\n    TF_LITE_ENSURE_EQ(context, projection_weights->dims->size, 2);\n    TF_LITE_ENSURE_EQ(context, projection_weights->dims->data[0], n_output);\n    TF_LITE_ENSURE_EQ(context, projection_weights->dims->data[1], n_cell);\n  }\n\n  const TfLiteTensor* projection_bias =\n      GetOptionalInputTensor(context, node, lstm::full::kProjectionBiasTensor);\n  if (projection_bias != nullptr) {\n    TF_LITE_ENSURE_EQ(context, projection_bias->dims->size, 1);\n    TF_LITE_ENSURE_EQ(context, projection_bias->dims->data[0], n_output);\n    if (is_integer) {\n      TF_LITE_ENSURE_TYPES_EQ(context, projection_bias->type, kTfLiteInt32);\n    } else {\n      TF_LITE_ENSURE_TYPES_EQ(context, projection_bias->type, kTfLiteFloat32);\n    }\n  }\n\n  // Making sure the projection tensors are consistent:\n  // 1) If projection weight is not present, then projection bias should not be\n  // present.\n  // 2) If projection weight is present, then projection bias is optional.\n  // TODO(ghodrat): make sure this is correct.\n  const bool projecton_tensors_consistent =\n      ((projection_weights != nullptr) || (projection_bias == nullptr));\n  TF_LITE_ENSURE(context, projecton_tensors_consistent == true);\n\n  if (use_layer_norm) {\n    const TfLiteTensor* input_layer_norm_coefficients = GetOptionalInputTensor(\n        context, node, lstm::full::kInputLayerNormCoefficientsTensor);\n    if (use_cifg) {\n      TF_LITE_ENSURE_EQ(context, input_layer_norm_coefficients, nullptr);\n    } else {\n      TF_LITE_ENSURE(context, input_layer_norm_coefficients != nullptr);\n      TF_LITE_ENSURE_EQ(context, input_layer_norm_coefficients->dims->size, 1);\n      TF_LITE_ENSURE_EQ(context, input_layer_norm_coefficients->dims->data[0],\n                        n_cell);\n      if (is_integer) {\n        TF_LITE_ENSURE_TYPES_EQ(context, input_layer_norm_coefficients->type,\n                                kTfLiteInt16);\n      } else {\n        TF_LITE_ENSURE_TYPES_EQ(context, input_layer_norm_coefficients->type,\n                                kTfLiteFloat32);\n      }\n    }\n\n    const TfLiteTensor* forget_layer_norm_coefficients;\n    TF_LITE_ENSURE_OK(\n        context, GetInputSafe(context, node,\n                              lstm::full::kForgetLayerNormCoefficientsTensor,\n                              &forget_layer_norm_coefficients));\n    TF_LITE_ENSURE_EQ(context, forget_layer_norm_coefficients->dims->size, 1);\n    TF_LITE_ENSURE_EQ(context, forget_layer_norm_coefficients->dims->data[0],\n                      n_cell);\n    if (is_integer) {\n      TF_LITE_ENSURE_TYPES_EQ(context, forget_layer_norm_coefficients->type,\n                              kTfLiteInt16);\n    } else {\n      TF_LITE_ENSURE_TYPES_EQ(context, forget_layer_norm_coefficients->type,\n                              kTfLiteFloat32);\n    }\n\n    const TfLiteTensor* cell_layer_norm_coefficients;\n    TF_LITE_ENSURE_OK(context,\n                      GetInputSafe(context, node,\n                                   lstm::full::kCellLayerNormCoefficientsTensor,\n                                   &cell_layer_norm_coefficients));\n    TF_LITE_ENSURE_EQ(context, cell_layer_norm_coefficients->dims->size, 1);\n    TF_LITE_ENSURE_EQ(context, cell_layer_norm_coefficients->dims->data[0],\n                      n_cell);\n    if (is_integer) {\n      TF_LITE_ENSURE_TYPES_EQ(context, cell_layer_norm_coefficients->type,\n                              kTfLiteInt16);\n    } else {\n      TF_LITE_ENSURE_TYPES_EQ(context, cell_layer_norm_coefficients->type,\n                              kTfLiteFloat32);\n    }\n\n    const TfLiteTensor* output_layer_norm_coefficients;\n    TF_LITE_ENSURE_OK(\n        context, GetInputSafe(context, node,\n                              lstm::full::kOutputLayerNormCoefficientsTensor,\n                              &output_layer_norm_coefficients));\n    TF_LITE_ENSURE_EQ(context, output_layer_norm_coefficients->dims->size, 1);\n    TF_LITE_ENSURE_EQ(context, output_layer_norm_coefficients->dims->data[0],\n                      n_cell);\n    if (is_integer) {\n      TF_LITE_ENSURE_TYPES_EQ(context, output_layer_norm_coefficients->type,\n                              kTfLiteInt16);\n    } else {\n      TF_LITE_ENSURE_TYPES_EQ(context, output_layer_norm_coefficients->type,\n                              kTfLiteFloat32);\n    }\n  }\n\n  return kTfLiteOk;\n}\n\nTfLiteStatus PrecomputeZeroPointTimesWeightWithBias(\n    TfLiteContext* context, int32_t zero_point,\n    const TfLiteTensor* weight_tensor, const TfLiteTensor* bias_tensor,\n    std::unique_ptr<int32_t[]>* output) {\n  if (weight_tensor == nullptr) {\n    return kTfLiteOk;\n  }\n\n  const RuntimeShape& weight_shape = GetTensorShape(weight_tensor);\n  TF_LITE_ENSURE_EQ(context, weight_shape.DimensionsCount(), 2);\n  const int row = weight_shape.Dims(0);\n  const int col = weight_shape.Dims(1);\n  output->reset(new int32_t[row]);\n  if (bias_tensor == nullptr) {\n    memset(output->get(), 0, row * sizeof(int32_t));\n  } else {\n    const int32_t* bias = GetTensorData<int32_t>(bias_tensor);\n    memcpy(output->get(), bias, row * sizeof(int32_t));\n  }\n  if (zero_point != 0) {\n    const int8_t* weight = GetTensorData<int8_t>(weight_tensor);\n    tensor_utils::MatrixScalarMultiplyAccumulate(weight, zero_point, row, col,\n                                                 output->get());\n  }\n  return kTfLiteOk;\n}\n\nTfLiteStatus PopulatePrecomputedZPTimesWeightsWithBias(TfLiteContext* context,\n                                                       OpData* op_data,\n                                                       TfLiteNode* node) {\n  const TfLiteTensor* input;\n  TF_LITE_ENSURE_OK(\n      context, GetInputSafe(context, node, lstm::full::kInputTensor, &input));\n  const TfLiteTensor* output_state =\n      GetVariableInput(context, node, lstm::full::kOutputStateTensor);\n  TF_LITE_ENSURE(context, output_state != nullptr);\n\n  const int32_t input_zero_point = -input->params.zero_point;\n  const int32_t output_state_zero_point = -output_state->params.zero_point;\n\n  const TfLiteTensor* input_to_input_weights = GetOptionalInputTensor(\n      context, node, lstm::full::kInputToInputWeightsTensor);\n  const TfLiteTensor* input_to_forget_weights;\n  TF_LITE_ENSURE_OK(\n      context,\n      GetInputSafe(context, node, lstm::full::kInputToForgetWeightsTensor,\n                   &input_to_forget_weights));\n  const TfLiteTensor* input_to_cell_weights;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node,\n                                          lstm::full::kInputToCellWeightsTensor,\n                                          &input_to_cell_weights));\n  const TfLiteTensor* input_to_output_weights;\n  TF_LITE_ENSURE_OK(\n      context,\n      GetInputSafe(context, node, lstm::full::kInputToOutputWeightsTensor,\n                   &input_to_output_weights));\n\n  const TfLiteTensor* recurrent_to_input_weights = GetOptionalInputTensor(\n      context, node, lstm::full::kRecurrentToInputWeightsTensor);\n  const TfLiteTensor* recurrent_to_forget_weights;\n  TF_LITE_ENSURE_OK(\n      context,\n      GetInputSafe(context, node, lstm::full::kRecurrentToForgetWeightsTensor,\n                   &recurrent_to_forget_weights));\n  const TfLiteTensor* recurrent_to_cell_weights;\n  TF_LITE_ENSURE_OK(\n      context,\n      GetInputSafe(context, node, lstm::full::kRecurrentToCellWeightsTensor,\n                   &recurrent_to_cell_weights));\n  const TfLiteTensor* recurrent_to_output_weights;\n  TF_LITE_ENSURE_OK(\n      context,\n      GetInputSafe(context, node, lstm::full::kRecurrentToOutputWeightsTensor,\n                   &recurrent_to_output_weights));\n\n  const TfLiteTensor* projection_weights = GetOptionalInputTensor(\n      context, node, lstm::full::kProjectionWeightsTensor);\n  const TfLiteTensor* projection_bias =\n      GetOptionalInputTensor(context, node, lstm::full::kProjectionBiasTensor);\n\n  lstm_eval::IntegerLstmParameter* integer_lstm_params =\n      &op_data->integer_lstm_param;\n\n  const TfLiteTensor* intermediate =\n      &context->tensors[node->intermediates->data[4]];\n  const auto* params =\n      static_cast<TfLiteAffineQuantization*>(intermediate->quantization.params);\n  const int32_t hidden_zp = params->zero_point->data[0];\n\n  // Get bias and perform zero point calculation.\n  // When there is layer normalization, the gate bias does not apply to matmul\n  // directly:\n  //      y = ln(w * x + w * r + w * c) + b.\n  const bool is_layer_norm = op_data->use_layer_norm;\n\n  // Forget gate.\n  const TfLiteTensor* forget_gate_bias =\n      is_layer_norm\n          ? nullptr\n          : GetInput(context, node, lstm::full::kForgetGateBiasTensor);\n  TF_LITE_ENSURE_OK(\n      context,\n      PrecomputeZeroPointTimesWeightWithBias(\n          context, input_zero_point, input_to_forget_weights, forget_gate_bias,\n          &(integer_lstm_params->input_to_forget_effective_bias)));\n\n  TF_LITE_ENSURE_OK(\n      context,\n      PrecomputeZeroPointTimesWeightWithBias(\n          context, output_state_zero_point, recurrent_to_forget_weights,\n          nullptr, &(integer_lstm_params->recurrent_to_forget_effective_bias)));\n\n  // Modulation gate.\n  const TfLiteTensor* cell_gate_bias =\n      is_layer_norm ? nullptr\n                    : GetInput(context, node, lstm::full::kCellGateBiasTensor);\n  TF_LITE_ENSURE_OK(\n      context,\n      PrecomputeZeroPointTimesWeightWithBias(\n          context, input_zero_point, input_to_cell_weights, cell_gate_bias,\n          &(integer_lstm_params->input_to_cell_effective_bias)));\n  TF_LITE_ENSURE_OK(\n      context,\n      PrecomputeZeroPointTimesWeightWithBias(\n          context, output_state_zero_point, recurrent_to_cell_weights, nullptr,\n          &(integer_lstm_params->recurrent_to_cell_effective_bias)));\n\n  // Output gate.\n  const TfLiteTensor* output_gate_bias =\n      is_layer_norm\n          ? nullptr\n          : GetInput(context, node, lstm::full::kOutputGateBiasTensor);\n  TF_LITE_ENSURE_OK(\n      context,\n      PrecomputeZeroPointTimesWeightWithBias(\n          context, input_zero_point, input_to_output_weights, output_gate_bias,\n          &(integer_lstm_params->input_to_output_effective_bias)));\n\n  TF_LITE_ENSURE_OK(\n      context,\n      PrecomputeZeroPointTimesWeightWithBias(\n          context, output_state_zero_point, recurrent_to_output_weights,\n          nullptr, &(integer_lstm_params->recurrent_to_output_effective_bias)));\n\n  // Input gate. The calculation is only meaningful for non-cifg case.\n  const TfLiteTensor* input_gate_bias =\n      is_layer_norm ? nullptr\n                    : GetInput(context, node, lstm::full::kInputGateBiasTensor);\n  TF_LITE_ENSURE_OK(\n      context,\n      PrecomputeZeroPointTimesWeightWithBias(\n          context, input_zero_point, input_to_input_weights, input_gate_bias,\n          &(integer_lstm_params->input_to_input_effective_bias)));\n  TF_LITE_ENSURE_OK(\n      context,\n      PrecomputeZeroPointTimesWeightWithBias(\n          context, output_state_zero_point, recurrent_to_input_weights, nullptr,\n          &(integer_lstm_params->recurrent_to_input_effective_bias)));\n\n  // Projection bias. The calculation is only meaningful for with projection.\n  TF_LITE_ENSURE_OK(context,\n                    PrecomputeZeroPointTimesWeightWithBias(\n                        context, hidden_zp, projection_weights, projection_bias,\n                        &(integer_lstm_params->projection_effective_bias)));\n  return kTfLiteOk;\n}\n\n// Resize the output and  state tensors based on the sizes of the input tensors.\n// Allocate a temporary scratch tensor. Also check that the sizes of the input\n// tensors match each other.\nTfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n  OpData* op_data = reinterpret_cast<OpData*>(node->user_data);\n  const int scratch_tensor_index = op_data->scratch_tensor_index;\n\n  // Check we have all the inputs and outputs we need.\n  bool use_layer_norm = false;\n  if (node->inputs->size == 24) {\n    const TfLiteTensor* forget_layer_norm_coefficients = GetOptionalInputTensor(\n        context, node, lstm::full::kForgetLayerNormCoefficientsTensor);\n    if (forget_layer_norm_coefficients == nullptr) {\n      use_layer_norm = false;\n    } else {\n      use_layer_norm = true;\n    }\n  } else if (node->inputs->size == 20) {\n    // This is deprecated and is only kept here for backward compatibility.\n    use_layer_norm = false;\n  } else {\n    context->ReportError(\n        context, \"The LSTM Full kernel expects 20 or 24 inputs. Got %d inputs\",\n        node->inputs->size);\n    return kTfLiteError;\n  }\n  TF_LITE_ENSURE_EQ(context, node->outputs->size, 1);\n  op_data->use_layer_norm = use_layer_norm;\n\n  // Inferring batch size, number of outputs and sequence length and\n  // number of cells from the input tensors.\n  const TfLiteTensor* input;\n  TF_LITE_ENSURE_OK(\n      context, GetInputSafe(context, node, lstm::full::kInputTensor, &input));\n  const bool is_integer = input->type == kTfLiteInt8;\n  TF_LITE_ENSURE(context, input->dims->size > 1);\n  const auto* params =\n      reinterpret_cast<TfLiteUnidirectionalSequenceLSTMParams*>(\n          node->builtin_data);\n  const bool time_major = params->time_major;\n  const int n_batch = time_major ? input->dims->data[1] : input->dims->data[0];\n  const int n_input = input->dims->data[2];\n\n  const TfLiteTensor* input_to_output_weights;\n  TF_LITE_ENSURE_OK(\n      context,\n      GetInputSafe(context, node, lstm::full::kInputToOutputWeightsTensor,\n                   &input_to_output_weights));\n  const int n_cell = input_to_output_weights->dims->data[0];\n  TF_LITE_ENSURE_EQ(context, input_to_output_weights->dims->size, 2);\n  TF_LITE_ENSURE_EQ(context, input_to_output_weights->dims->data[1], n_input);\n\n  const TfLiteTensor* recurrent_to_output_weights;\n  TF_LITE_ENSURE_OK(\n      context,\n      GetInputSafe(context, node, lstm::full::kRecurrentToOutputWeightsTensor,\n                   &recurrent_to_output_weights));\n  TF_LITE_ENSURE_EQ(context, recurrent_to_output_weights->dims->size, 2);\n  TF_LITE_ENSURE_EQ(context, recurrent_to_output_weights->dims->data[0],\n                    n_cell);\n  const int n_output = recurrent_to_output_weights->dims->data[1];\n\n  // Check that input tensor dimensions matches with each other.\n  TF_LITE_ENSURE_OK(\n      context, CheckInputTensorDimensions(context, node, n_input, n_output,\n                                          n_cell, use_layer_norm, is_integer));\n\n  // Get the pointer to output, output_state and cell_state buffer tensors.\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node,\n                                           lstm::full::kOutputTensor, &output));\n\n  TfLiteTensor* output_state =\n      GetVariableInput(context, node, lstm::full::kOutputStateTensor);\n  TF_LITE_ENSURE(context, output_state != nullptr);\n  TfLiteTensor* cell_state =\n      GetVariableInput(context, node, lstm::full::kCellStateTensor);\n  TF_LITE_ENSURE(context, cell_state != nullptr);\n\n  // Check the shape of input state tensors.\n  // These tensor may be 1D or 2D. It's fine as long as the total size is\n  // correct.\n  TF_LITE_ENSURE_EQ(context, NumElements(output_state), n_batch * n_output);\n  TF_LITE_ENSURE_EQ(context, NumElements(cell_state), n_batch * n_cell);\n\n  // Resize the output tensors.\n  TfLiteIntArray* output_size = TfLiteIntArrayCopy(input->dims);\n  output_size->data[input->dims->size - 1] = n_output;\n  TF_LITE_ENSURE_OK(context,\n                    context->ResizeTensor(context, output, output_size));\n\n  if (is_integer) {\n    const int num_intermediate_tensors = node->intermediates->size;\n    TF_LITE_ENSURE(context, num_intermediate_tensors == 5);\n  }\n\n  TfLiteIntArrayFree(node->temporaries);\n  if (IsHybridOp(input, input_to_output_weights)) {\n    node->temporaries = TfLiteIntArrayCreate(kNumTemporaryTensors);\n  } else if (is_integer) {\n    node->temporaries = TfLiteIntArrayCreate(6);\n  } else {\n    node->temporaries = TfLiteIntArrayCreate(1);\n  }\n  node->temporaries->data[kScratchBuffer] =\n      scratch_tensor_index + kScratchBuffer;\n\n  // Create a scratch buffer tensor.\n  TfLiteTensor* scratch_buffer;\n  TF_LITE_ENSURE_OK(context, GetTemporarySafe(context, node, kScratchBuffer,\n                                              &scratch_buffer));\n  scratch_buffer->type = input->type;\n  scratch_buffer->allocation_type = kTfLiteArenaRw;\n\n  const TfLiteTensor* input_to_input_weights = GetOptionalInputTensor(\n      context, node, lstm::full::kInputToInputWeightsTensor);\n  const bool use_cifg = (input_to_input_weights == nullptr);\n  TfLiteIntArray* scratch_buffer_size = TfLiteIntArrayCreate(2);\n  scratch_buffer_size->data[0] = n_batch;\n  if (use_cifg) {\n    // Reserving space for Cell, Forget, Output gates\n    scratch_buffer_size->data[1] = n_cell * 3;\n  } else {\n    // Reserving space for Input, Cell, Forget, Output gates\n    scratch_buffer_size->data[1] = n_cell * 4;\n  }\n  TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, scratch_buffer,\n                                                   scratch_buffer_size));\n\n  if (IsHybridOp(input, input_to_output_weights)) {\n    op_data->compute_row_sums = true;\n    // Allocate temporary tensors to store quantized values of input,\n    // output_state and cell_state tensors.\n    node->temporaries->data[kInputQuantized] =\n        scratch_tensor_index + kInputQuantized;\n    TfLiteTensor* input_quantized;\n    TF_LITE_ENSURE_OK(context, GetTemporarySafe(context, node, kInputQuantized,\n                                                &input_quantized));\n    input_quantized->type = input_to_output_weights->type;\n    input_quantized->allocation_type = kTfLiteArenaRw;\n    if (!TfLiteIntArrayEqual(input_quantized->dims, input->dims)) {\n      TfLiteIntArray* input_quantized_size = TfLiteIntArrayCopy(input->dims);\n      TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, input_quantized,\n                                                       input_quantized_size));\n    }\n    node->temporaries->data[kOutputStateQuantized] =\n        scratch_tensor_index + kOutputStateQuantized;\n    TfLiteTensor* output_state_quantized;\n    TF_LITE_ENSURE_OK(context,\n                      GetTemporarySafe(context, node, kOutputStateQuantized,\n                                       &output_state_quantized));\n    output_state_quantized->type = input_to_output_weights->type;\n    output_state_quantized->allocation_type = kTfLiteArenaRw;\n    if (!TfLiteIntArrayEqual(output_state_quantized->dims,\n                             output_state->dims)) {\n      TfLiteIntArray* output_state_quantized_size =\n          TfLiteIntArrayCopy(output_state->dims);\n      TF_LITE_ENSURE_OK(context,\n                        context->ResizeTensor(context, output_state_quantized,\n                                              output_state_quantized_size));\n    }\n    node->temporaries->data[kCellStateQuantized] =\n        scratch_tensor_index + kCellStateQuantized;\n    TfLiteTensor* cell_state_quantized;\n    TF_LITE_ENSURE_OK(context,\n                      GetTemporarySafe(context, node, kCellStateQuantized,\n                                       &cell_state_quantized));\n    cell_state_quantized->type = input_to_output_weights->type;\n    cell_state_quantized->allocation_type = kTfLiteArenaRw;\n    if (!TfLiteIntArrayEqual(cell_state_quantized->dims, cell_state->dims)) {\n      TfLiteIntArray* cell_state_quantized_size =\n          TfLiteIntArrayCopy(cell_state->dims);\n      TF_LITE_ENSURE_OK(context,\n                        context->ResizeTensor(context, cell_state_quantized,\n                                              cell_state_quantized_size));\n    }\n\n    // Allocate temporary tensors to store scaling factors and product scaling\n    // factors. The latter is a convenience storage which allows to quantize\n    // a vector once (which produces the scaling factors) and multiply it with\n    // different matrices (which requires multiplying the scaling factors with\n    // the scaling factor of the matrix).\n    node->temporaries->data[kInputScalingFactors] =\n        op_data->scratch_tensor_index + kInputScalingFactors;\n    TfLiteTensor* input_sf;\n    TF_LITE_ENSURE_OK(\n        context,\n        GetTemporarySafe(context, node, kInputScalingFactors, &input_sf));\n    input_sf->type = kTfLiteFloat32;\n    input_sf->allocation_type = kTfLiteArenaRw;\n    int scaling_dims[1] = {n_batch};\n    if (!TfLiteIntArrayEqualsArray(input_sf->dims, 1, scaling_dims)) {\n      TfLiteIntArray* input_sf_size = TfLiteIntArrayCreate(1);\n      input_sf_size->data[0] = n_batch;\n      TF_LITE_ENSURE_OK(\n          context, context->ResizeTensor(context, input_sf, input_sf_size));\n    }\n    node->temporaries->data[kOutputStateScalingFactors] =\n        op_data->scratch_tensor_index + kOutputStateScalingFactors;\n    TfLiteTensor* output_state_sf;\n    TF_LITE_ENSURE_OK(\n        context, GetTemporarySafe(context, node, kOutputStateScalingFactors,\n                                  &output_state_sf));\n    output_state_sf->type = kTfLiteFloat32;\n    output_state_sf->allocation_type = kTfLiteArenaRw;\n    if (!TfLiteIntArrayEqualsArray(output_state_sf->dims, 1, scaling_dims)) {\n      TfLiteIntArray* output_state_sf_size = TfLiteIntArrayCreate(1);\n      output_state_sf_size->data[0] = n_batch;\n      TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, output_state_sf,\n                                                       output_state_sf_size));\n    }\n    node->temporaries->data[kProductScalingFactors] =\n        scratch_tensor_index + kProductScalingFactors;\n    TfLiteTensor* prod_scaling_factors;\n    TF_LITE_ENSURE_OK(context,\n                      GetTemporarySafe(context, node, kProductScalingFactors,\n                                       &prod_scaling_factors));\n    prod_scaling_factors->type = kTfLiteFloat32;\n    prod_scaling_factors->allocation_type = kTfLiteArenaRw;\n    if (!TfLiteIntArrayEqualsArray(prod_scaling_factors->dims, 1,\n                                   scaling_dims)) {\n      TfLiteIntArray* prod_scaling_factors_size = TfLiteIntArrayCreate(1);\n      prod_scaling_factors_size->data[0] = n_batch;\n      TF_LITE_ENSURE_OK(context,\n                        context->ResizeTensor(context, prod_scaling_factors,\n                                              prod_scaling_factors_size));\n    }\n\n    // Allocate a temporary tensor to store the recovered cell weights. Since\n    // this is used for diagonal matrices, only need to store n_cell values.\n    node->temporaries->data[kRecoveredCellWeights] =\n        scratch_tensor_index + kRecoveredCellWeights;\n    TfLiteTensor* recovered_cell_weights;\n    TF_LITE_ENSURE_OK(context,\n                      GetTemporarySafe(context, node, kRecoveredCellWeights,\n                                       &recovered_cell_weights));\n    recovered_cell_weights->type = kTfLiteFloat32;\n    recovered_cell_weights->allocation_type = kTfLiteArenaRw;\n    int recovered_cell_dims[1] = {n_cell};\n    if (!TfLiteIntArrayEqualsArray(recovered_cell_weights->dims, 1,\n                                   recovered_cell_dims)) {\n      TfLiteIntArray* recovered_cell_weights_size = TfLiteIntArrayCreate(1);\n      recovered_cell_weights_size->data[0] = n_cell;\n      TF_LITE_ENSURE_OK(context,\n                        context->ResizeTensor(context, recovered_cell_weights,\n                                              recovered_cell_weights_size));\n    }\n\n    // Allocate a temporary tensor to store the accumulated int32 values.\n    node->temporaries->data[kAccumScratch] =\n        scratch_tensor_index + kAccumScratch;\n    TfLiteTensor* accum_scratch;\n    TF_LITE_ENSURE_OK(context, GetTemporarySafe(context, node, kAccumScratch,\n                                                &accum_scratch));\n    accum_scratch->type = kTfLiteInt32;\n    accum_scratch->allocation_type = kTfLiteArenaRw;\n    int accum_scratch_dims[2] = {n_cell, n_batch};\n    if (!TfLiteIntArrayEqualsArray(accum_scratch->dims, 2,\n                                   accum_scratch_dims)) {\n      TfLiteIntArray* accum_size = TfLiteIntArrayCreate(2);\n      accum_size->data[0] = n_cell;\n      accum_size->data[1] = n_batch;\n      TF_LITE_ENSURE_OK(\n          context, context->ResizeTensor(context, accum_scratch, accum_size));\n    }\n    node->temporaries->data[kInputZeroPoints] =\n        op_data->scratch_tensor_index + kInputZeroPoints;\n    TfLiteTensor* input_zp;\n    TF_LITE_ENSURE_OK(\n        context, GetTemporarySafe(context, node, kInputZeroPoints, &input_zp));\n    input_zp->type = kTfLiteFloat32;\n    input_zp->allocation_type = kTfLiteArenaRw;\n    if (!TfLiteIntArrayEqualsArray(input_zp->dims, 1, scaling_dims)) {\n      TfLiteIntArray* input_zp_size = TfLiteIntArrayCreate(1);\n      input_zp_size->data[0] = n_batch;\n      TF_LITE_ENSURE_OK(\n          context, context->ResizeTensor(context, input_zp, input_zp_size));\n    }\n    node->temporaries->data[kOutputStateZeroPoints] =\n        op_data->scratch_tensor_index + kOutputStateZeroPoints;\n    TfLiteTensor* output_state_zp;\n    TF_LITE_ENSURE_OK(context,\n                      GetTemporarySafe(context, node, kOutputStateZeroPoints,\n                                       &output_state_zp));\n    output_state_zp->type = kTfLiteFloat32;\n    output_state_zp->allocation_type = kTfLiteArenaRw;\n    if (!TfLiteIntArrayEqualsArray(output_state_zp->dims, 1, scaling_dims)) {\n      TfLiteIntArray* output_state_zp_size = TfLiteIntArrayCreate(1);\n      output_state_zp_size->data[0] = n_batch;\n      TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, output_state_zp,\n                                                       output_state_zp_size));\n    }\n    node->temporaries->data[kRowSums] = scratch_tensor_index + kRowSums;\n    TfLiteTensor* row_sums;\n    TF_LITE_ENSURE_OK(context,\n                      GetTemporarySafe(context, node, kRowSums, &row_sums));\n    row_sums->type = kTfLiteInt32;\n    row_sums->allocation_type = kTfLiteArenaRwPersistent;\n    int row_sums_rows = use_cifg ? 6 : 8;\n    const TfLiteTensor* projection_weights = GetOptionalInputTensor(\n        context, node, lstm::full::kProjectionWeightsTensor);\n    if (projection_weights != nullptr) {\n      row_sums_rows += ceil(static_cast<float>(n_output) / n_cell);\n    }\n    int row_sums_dims[2] = {row_sums_rows, n_cell};\n    if (!TfLiteIntArrayEqualsArray(row_sums->dims, 2, row_sums_dims)) {\n      TfLiteIntArray* row_sums_size = TfLiteIntArrayCreate(2);\n      row_sums_size->data[0] = row_sums_dims[0];\n      row_sums_size->data[1] = row_sums_dims[1];\n      TF_LITE_ENSURE_OK(\n          context, context->ResizeTensor(context, row_sums, row_sums_size));\n    }\n  }\n\n  if (is_integer) {\n    // Integer UnidirectionalSequenceLSTM prepare function for 8x8->16.\n    // This code path needs 5 intermediate tensors per Op.\n    // Populate quantization parameters.\n    PopulateQuantizedLstmParams8x8_16(context, node,\n                                      &op_data->integer_lstm_param);\n    // Allocate scratch buffer. Need 6 16bit buffer with size n_batch * n_cell\n    // and 1 8bit buffer with size n_batch * n_cell. We also need 1 32 bit\n    // buffer with size n_batch * n_cell.\n    //\n    // Handle cifg case as well, which might save one buffer.\n    for (int scratch_index = 0; scratch_index < 6; ++scratch_index) {\n      node->temporaries->data[scratch_index] =\n          op_data->scratch_tensor_index + scratch_index;\n      TfLiteTensor* scratch_tensor;\n      TF_LITE_ENSURE_OK(context, GetTemporarySafe(context, node, scratch_index,\n                                                  &scratch_tensor));\n\n      scratch_tensor->type = kTfLiteInt16;\n      if (scratch_index == 4) {\n        scratch_tensor->type = kTfLiteInt8;\n      } else if (scratch_index == 5) {\n        scratch_tensor->type = kTfLiteInt32;\n      }\n\n      scratch_tensor->allocation_type = kTfLiteArenaRw;\n      const int scratch_dimension[2] = {n_batch, n_cell};\n      if (!TfLiteIntArrayEqualsArray(scratch_tensor->dims, 2,\n                                     scratch_dimension)) {\n        TfLiteIntArray* scratch_buffer_size = TfLiteIntArrayCreate(2);\n        scratch_buffer_size->data[0] = n_batch;\n        scratch_buffer_size->data[1] = n_cell;\n        TF_LITE_ENSURE_OK(context,\n                          context->ResizeTensor(context, scratch_tensor,\n                                                scratch_buffer_size));\n      }\n    }\n\n    // Populate precomputed zp * weight.\n    TF_LITE_ENSURE_OK(context, PopulatePrecomputedZPTimesWeightsWithBias(\n                                   context, op_data, node));\n  }\n\n  return kTfLiteOk;\n}\n\nTfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n  const auto* params =\n      reinterpret_cast<TfLiteUnidirectionalSequenceLSTMParams*>(\n          node->builtin_data);\n  const OpData* op_data = reinterpret_cast<OpData*>(node->user_data);\n  const bool use_layer_norm = op_data->use_layer_norm;\n  const bool time_major = params->time_major;\n  const TfLiteTensor* input;\n  TF_LITE_ENSURE_OK(\n      context, GetInputSafe(context, node, lstm::full::kInputTensor, &input));\n\n  const TfLiteTensor* input_to_input_weights = GetOptionalInputTensor(\n      context, node, lstm::full::kInputToInputWeightsTensor);\n  const TfLiteTensor* input_to_forget_weights;\n  TF_LITE_ENSURE_OK(\n      context,\n      GetInputSafe(context, node, lstm::full::kInputToForgetWeightsTensor,\n                   &input_to_forget_weights));\n  const TfLiteTensor* input_to_cell_weights;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node,\n                                          lstm::full::kInputToCellWeightsTensor,\n                                          &input_to_cell_weights));\n  const TfLiteTensor* input_to_output_weights;\n  TF_LITE_ENSURE_OK(\n      context,\n      GetInputSafe(context, node, lstm::full::kInputToOutputWeightsTensor,\n                   &input_to_output_weights));\n\n  const TfLiteTensor* recurrent_to_input_weights = GetOptionalInputTensor(\n      context, node, lstm::full::kRecurrentToInputWeightsTensor);\n  const TfLiteTensor* recurrent_to_forget_weights;\n  TF_LITE_ENSURE_OK(\n      context,\n      GetInputSafe(context, node, lstm::full::kRecurrentToForgetWeightsTensor,\n                   &recurrent_to_forget_weights));\n  const TfLiteTensor* recurrent_to_cell_weights;\n  TF_LITE_ENSURE_OK(\n      context,\n      GetInputSafe(context, node, lstm::full::kRecurrentToCellWeightsTensor,\n                   &recurrent_to_cell_weights));\n  const TfLiteTensor* recurrent_to_output_weights;\n  TF_LITE_ENSURE_OK(\n      context,\n      GetInputSafe(context, node, lstm::full::kRecurrentToOutputWeightsTensor,\n                   &recurrent_to_output_weights));\n\n  const TfLiteTensor* cell_to_input_weights = GetOptionalInputTensor(\n      context, node, lstm::full::kCellToInputWeightsTensor);\n  const TfLiteTensor* cell_to_forget_weights = GetOptionalInputTensor(\n      context, node, lstm::full::kCellToForgetWeightsTensor);\n  const TfLiteTensor* cell_to_output_weights = GetOptionalInputTensor(\n      context, node, lstm::full::kCellToOutputWeightsTensor);\n\n  const TfLiteTensor* input_gate_bias =\n      GetOptionalInputTensor(context, node, lstm::full::kInputGateBiasTensor);\n  const TfLiteTensor* forget_gate_bias;\n  TF_LITE_ENSURE_OK(\n      context, GetInputSafe(context, node, lstm::full::kForgetGateBiasTensor,\n                            &forget_gate_bias));\n  const TfLiteTensor* cell_gate_bias;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, lstm::full::kCellGateBiasTensor,\n                                 &cell_gate_bias));\n  const TfLiteTensor* output_gate_bias;\n  TF_LITE_ENSURE_OK(\n      context, GetInputSafe(context, node, lstm::full::kOutputGateBiasTensor,\n                            &output_gate_bias));\n\n  const TfLiteTensor* projection_weights = GetOptionalInputTensor(\n      context, node, lstm::full::kProjectionWeightsTensor);\n  const TfLiteTensor* projection_bias =\n      GetOptionalInputTensor(context, node, lstm::full::kProjectionBiasTensor);\n\n  TfLiteTensor* output_state =\n      GetVariableInput(context, node, lstm::full::kOutputStateTensor);\n  TFLITE_DCHECK(output_state != nullptr);\n  TfLiteTensor* cell_state =\n      GetVariableInput(context, node, lstm::full::kCellStateTensor);\n  TFLITE_DCHECK(cell_state != nullptr);\n\n  const TfLiteTensor* input_layer_norm_coefficients =\n      use_layer_norm\n          ? GetOptionalInputTensor(\n                context, node, lstm::full::kInputLayerNormCoefficientsTensor)\n          : nullptr;\n  const TfLiteTensor* forget_layer_norm_coefficients =\n      use_layer_norm ? GetInput(context, node,\n                                lstm::full::kForgetLayerNormCoefficientsTensor)\n                     : nullptr;\n  const TfLiteTensor* cell_layer_norm_coefficients =\n      use_layer_norm ? GetInput(context, node,\n                                lstm::full::kCellLayerNormCoefficientsTensor)\n                     : nullptr;\n  const TfLiteTensor* output_layer_norm_coefficients =\n      use_layer_norm ? GetInput(context, node,\n                                lstm::full::kOutputLayerNormCoefficientsTensor)\n                     : nullptr;\n\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node,\n                                           lstm::full::kOutputTensor, &output));\n\n  // Copy out the LSTM specific params so they can be passed in the function.\n  TfLiteLSTMParams lstm_params;\n  lstm_params.activation = params->activation;\n  lstm_params.cell_clip = params->cell_clip;\n  lstm_params.proj_clip = params->proj_clip;\n  lstm_params.asymmetric_quantize_inputs = params->asymmetric_quantize_inputs;\n\n  switch (input_to_output_weights->type) {\n    case kTfLiteFloat32: {\n      // Index the scratch buffers pointers to the global scratch buffer.\n      TfLiteTensor* scratch_buffer;\n      TF_LITE_ENSURE_OK(context, GetTemporarySafe(context, node, kScratchBuffer,\n                                                  &scratch_buffer));\n      return lstm_eval::EvalFloat(\n          input, input_to_input_weights, input_to_forget_weights,\n          input_to_cell_weights, input_to_output_weights,\n          recurrent_to_input_weights, recurrent_to_forget_weights,\n          recurrent_to_cell_weights, recurrent_to_output_weights,\n          cell_to_input_weights, cell_to_forget_weights, cell_to_output_weights,\n          input_layer_norm_coefficients, forget_layer_norm_coefficients,\n          cell_layer_norm_coefficients, output_layer_norm_coefficients,\n          /*aux_input=*/nullptr,\n          /*aux_input_to_input_weights=*/nullptr,\n          /*aux_input_to_forget_weights=*/nullptr,\n          /*aux_input_to_cell_weights=*/nullptr,\n          /*aux_input_to_output_weights=*/nullptr, input_gate_bias,\n          forget_gate_bias, cell_gate_bias, output_gate_bias,\n          projection_weights, projection_bias, &lstm_params,\n          /*forward_sequence=*/true, time_major,\n          /*output_offset=*/0, scratch_buffer, output_state, cell_state,\n          output);\n    }\n    case kTfLiteUInt8:\n    case kTfLiteInt8: {\n      const bool is_hybrid = input->type == kTfLiteFloat32;\n      if (is_hybrid) {\n        // Index the scratch buffers pointers to the global scratch buffer.\n        TfLiteTensor* scratch_buffer;\n        TF_LITE_ENSURE_OK(\n            context,\n            GetTemporarySafe(context, node, kScratchBuffer, &scratch_buffer));\n\n        OpData* op_data = reinterpret_cast<OpData*>(node->user_data);\n        TfLiteTensor* row_sums;\n        TF_LITE_ENSURE_OK(context,\n                          GetTemporarySafe(context, node, kRowSums, &row_sums));\n        const int row_sums_size = row_sums->dims->data[0];\n        return lstm_eval::EvalHybrid(\n            input, input_to_input_weights,\n            /*input_to_input_weights_ledger*/ nullptr, input_to_forget_weights,\n            /*input_to_forget_weights_ledger*/ nullptr, input_to_cell_weights,\n            /*input_to_cell_weights_ledger*/ nullptr, input_to_output_weights,\n            /*input_to_output_weights_ledger*/ nullptr,\n            recurrent_to_input_weights,\n            /*recurrent_to_input_weights_ledger*/ nullptr,\n            recurrent_to_forget_weights,\n            /*recurrent_to_forget_weights_ledger*/ nullptr,\n            recurrent_to_cell_weights,\n            /*recurrent_to_cell_weights_ledger*/ nullptr,\n            recurrent_to_output_weights,\n            /*recurrent_to_output_weights_ledger*/ nullptr,\n            cell_to_input_weights, cell_to_forget_weights,\n            cell_to_output_weights, input_layer_norm_coefficients,\n            forget_layer_norm_coefficients, cell_layer_norm_coefficients,\n            output_layer_norm_coefficients,\n            /*aux_input=*/nullptr,\n            /*aux_input_to_input_weights=*/nullptr,\n            /*aux_input_to_forget_weights=*/nullptr,\n            /*aux_input_to_cell_weights=*/nullptr,\n            /*aux_input_to_output_weights=*/nullptr, input_gate_bias,\n            forget_gate_bias, cell_gate_bias, output_gate_bias,\n            projection_weights, /*projection_weights_ledger*/ nullptr,\n            projection_bias, &lstm_params,\n            /*forward_sequence=*/true, time_major,\n            /*output_offset=*/0, scratch_buffer,\n            GetTemporary(context, node, kInputScalingFactors),\n            /*aux_input_sf=*/nullptr,\n            GetTemporary(context, node, kOutputStateScalingFactors),\n            GetTemporary(context, node, kProductScalingFactors),\n            GetTemporary(context, node, kRecoveredCellWeights),\n            GetTemporary(context, node, kInputQuantized),\n            /*aux_input_quantized=*/nullptr,\n            GetTemporary(context, node, kOutputStateQuantized),\n            GetTemporary(context, node, kCellStateQuantized), output_state,\n            cell_state, GetTemporary(context, node, kAccumScratch), output,\n            GetTemporary(context, node, kInputZeroPoints),\n            /*aux_input_zp=*/nullptr,\n            GetTemporary(context, node, kOutputStateZeroPoints), row_sums,\n            row_sums_size, &op_data->compute_row_sums,\n            CpuBackendContext::GetFromContext(context));\n      } else {\n        TfLiteTensor* scratch0;\n        TF_LITE_ENSURE_OK(context,\n                          GetTemporarySafe(context, node, 0, &scratch0));\n        TfLiteTensor* scratch1;\n        TF_LITE_ENSURE_OK(context,\n                          GetTemporarySafe(context, node, 1, &scratch1));\n        TfLiteTensor* scratch2;\n        TF_LITE_ENSURE_OK(context,\n                          GetTemporarySafe(context, node, 2, &scratch2));\n        TfLiteTensor* scratch3;\n        TF_LITE_ENSURE_OK(context,\n                          GetTemporarySafe(context, node, 3, &scratch3));\n        TfLiteTensor* scratch4;\n        TF_LITE_ENSURE_OK(context,\n                          GetTemporarySafe(context, node, 4, &scratch4));\n        TfLiteTensor* scratch5;\n        TF_LITE_ENSURE_OK(context,\n                          GetTemporarySafe(context, node, 5, &scratch5));\n        return lstm_eval::EvalInteger8x8_16(\n            input, input_to_input_weights, input_to_forget_weights,\n            input_to_cell_weights, input_to_output_weights,\n            recurrent_to_input_weights, recurrent_to_forget_weights,\n            recurrent_to_cell_weights, recurrent_to_output_weights,\n            cell_to_input_weights, cell_to_forget_weights,\n            cell_to_output_weights, input_layer_norm_coefficients,\n            forget_layer_norm_coefficients, cell_layer_norm_coefficients,\n            output_layer_norm_coefficients, input_gate_bias, forget_gate_bias,\n            cell_gate_bias, output_gate_bias, projection_weights,\n            projection_bias, &lstm_params, /*forward_sequence=*/true,\n            time_major, &op_data->integer_lstm_param, output_state, cell_state,\n            output, scratch0, scratch1, scratch2, scratch3, scratch4, scratch5,\n            CpuBackendContext::GetFromContext(context));\n      }\n    }\n    default:\n      TF_LITE_KERNEL_LOG(context, \"Type %s is not currently supported.\",\n                         TfLiteTypeGetName(input_to_output_weights->type));\n      return kTfLiteError;\n  }\n}\n}  // namespace unidirectional_sequence_lstm\n\nTfLiteRegistration* Register_UNIDIRECTIONAL_SEQUENCE_LSTM() {\n  static TfLiteRegistration r = {unidirectional_sequence_lstm::Init,\n                                 unidirectional_sequence_lstm::Free,\n                                 unidirectional_sequence_lstm::Prepare,\n                                 unidirectional_sequence_lstm::Eval};\n  return &r;\n}\n\n}  // namespace builtin\n}  // namespace ops\n}  // namespace tflite\n"], "fixing_code": ["/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include <math.h>\n\n#include <cstddef>\n\n#include \"tensorflow/lite/c/builtin_op_data.h\"\n#include \"tensorflow/lite/c/common.h\"\n#include \"tensorflow/lite/kernels/cpu_backend_context.h\"\n#include \"tensorflow/lite/kernels/internal/compatibility.h\"\n#include \"tensorflow/lite/kernels/internal/kernel_utils.h\"\n#include \"tensorflow/lite/kernels/internal/quantization_util.h\"\n#include \"tensorflow/lite/kernels/internal/tensor_ctypes.h\"\n#include \"tensorflow/lite/kernels/internal/tensor_utils.h\"\n#include \"tensorflow/lite/kernels/kernel_util.h\"\n#include \"tensorflow/lite/kernels/lstm_eval.h\"\n#include \"tensorflow/lite/kernels/lstm_shared.h\"\n\nnamespace tflite {\nnamespace ops {\nnamespace builtin {\nnamespace unidirectional_sequence_lstm {\nnamespace {\n\nstruct OpData {\n  // If the lstm is layer norm.\n  bool use_layer_norm;\n  // The scratch tensor index.\n  int scratch_tensor_index;\n  bool compute_row_sums = false;\n\n  lstm_eval::IntegerLstmParameter integer_lstm_param;\n};\n\nTfLiteStatus PopulateQuantizedLstmParams8x8_16(\n    TfLiteContext* context, TfLiteNode* node,\n    lstm_eval::IntegerLstmParameter* integer_lstm_param) {\n  // Calculate quantized clip for projection and cell.\n  const auto* params =\n      static_cast<TfLiteUnidirectionalSequenceLSTMParams*>(node->builtin_data);\n  const float cell_clip = params->cell_clip;\n  const float proj_clip = params->proj_clip;\n\n  const TfLiteTensor* cell_state =\n      GetVariableInput(context, node, lstm::full::kCellStateTensor);\n  TF_LITE_ENSURE(context, cell_state != nullptr);\n  TfLiteTensor* output_tensor;\n  TF_LITE_ENSURE_OK(\n      context,\n      GetOutputSafe(context, node, lstm::full::kOutputTensor, &output_tensor));\n\n  TF_LITE_ENSURE(context,\n                 cell_state->quantization.type != kTfLiteNoQuantization);\n  auto* cell_state_params =\n      static_cast<TfLiteAffineQuantization*>(cell_state->quantization.params);\n  TF_LITE_ENSURE(context,\n                 output_tensor->quantization.type != kTfLiteNoQuantization);\n  auto* proj_params = static_cast<TfLiteAffineQuantization*>(\n      output_tensor->quantization.params);\n  if (cell_clip > 0.0) {\n    integer_lstm_param->quantized_cell_clip = static_cast<int16_t>(std::min(\n        std::max(cell_clip / cell_state_params->scale->data[0], -32768.0f),\n        32767.0f));\n  } else {\n    integer_lstm_param->quantized_cell_clip = 0;\n  }\n  if (proj_clip > 0.0) {\n    integer_lstm_param->quantized_proj_clip = static_cast<int8_t>(std::min(\n        std::max(proj_clip / proj_params->scale->data[0], -128.0f), 127.0f));\n  } else {\n    integer_lstm_param->quantized_proj_clip = 0;\n  }\n\n  // Calculate effective scales.\n  OpData* op_data = static_cast<OpData*>(node->user_data);\n  const bool use_layer_norm = op_data->use_layer_norm;\n\n  const TfLiteTensor* input;\n  TF_LITE_ENSURE_OK(\n      context, GetInputSafe(context, node, lstm::full::kInputTensor, &input));\n\n  const TfLiteTensor* input_to_input_weights = GetOptionalInputTensor(\n      context, node, lstm::full::kInputToInputWeightsTensor);\n  const TfLiteTensor* input_to_forget_weights;\n  TF_LITE_ENSURE_OK(\n      context,\n      GetInputSafe(context, node, lstm::full::kInputToForgetWeightsTensor,\n                   &input_to_forget_weights));\n  const TfLiteTensor* input_to_cell_weights;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node,\n                                          lstm::full::kInputToCellWeightsTensor,\n                                          &input_to_cell_weights));\n  const TfLiteTensor* input_to_output_weights;\n  TF_LITE_ENSURE_OK(\n      context,\n      GetInputSafe(context, node, lstm::full::kInputToOutputWeightsTensor,\n                   &input_to_output_weights));\n\n  const TfLiteTensor* recurrent_to_input_weights = GetOptionalInputTensor(\n      context, node, lstm::full::kRecurrentToInputWeightsTensor);\n  const TfLiteTensor* recurrent_to_forget_weights;\n  TF_LITE_ENSURE_OK(\n      context,\n      GetInputSafe(context, node, lstm::full::kRecurrentToForgetWeightsTensor,\n                   &recurrent_to_forget_weights));\n  const TfLiteTensor* recurrent_to_cell_weights;\n  TF_LITE_ENSURE_OK(\n      context,\n      GetInputSafe(context, node, lstm::full::kRecurrentToCellWeightsTensor,\n                   &recurrent_to_cell_weights));\n  const TfLiteTensor* recurrent_to_output_weights;\n  TF_LITE_ENSURE_OK(\n      context,\n      GetInputSafe(context, node, lstm::full::kRecurrentToOutputWeightsTensor,\n                   &recurrent_to_output_weights));\n\n  const TfLiteTensor* cell_to_input_weights = GetOptionalInputTensor(\n      context, node, lstm::full::kCellToInputWeightsTensor);\n  const TfLiteTensor* cell_to_forget_weights = GetOptionalInputTensor(\n      context, node, lstm::full::kCellToForgetWeightsTensor);\n  const TfLiteTensor* cell_to_output_weights = GetOptionalInputTensor(\n      context, node, lstm::full::kCellToOutputWeightsTensor);\n\n  const TfLiteTensor* input_layer_norm_coefficients = GetOptionalInputTensor(\n      context, node, lstm::full::kInputLayerNormCoefficientsTensor);\n  const TfLiteTensor* forget_layer_norm_coefficients = GetOptionalInputTensor(\n      context, node, lstm::full::kForgetLayerNormCoefficientsTensor);\n  const TfLiteTensor* cell_layer_norm_coefficients = GetOptionalInputTensor(\n      context, node, lstm::full::kCellLayerNormCoefficientsTensor);\n  const TfLiteTensor* output_layer_norm_coefficients = GetOptionalInputTensor(\n      context, node, lstm::full::kOutputLayerNormCoefficientsTensor);\n\n  const TfLiteTensor* projection_weights = GetOptionalInputTensor(\n      context, node, lstm::full::kProjectionWeightsTensor);\n\n  TfLiteTensor* output_state =\n      GetVariableInput(context, node, lstm::full::kOutputStateTensor);\n  TF_LITE_ENSURE(context, output_state != nullptr);\n\n  // Since we have already checked that weights are all there or none, we can\n  // check the existence of only one to get the condition.\n  const bool use_cifg = (input_to_input_weights == nullptr);\n  const bool use_peephole = (cell_to_output_weights != nullptr);\n  const bool use_projection = (projection_weights != nullptr);\n\n  // Get intermediate scales and zero points.\n  std::vector<float> intermediate_scale;\n  std::vector<int32> intermediate_zp;\n  for (int i = 0; i < 4; ++i) {\n    if (use_layer_norm) {\n      TfLiteTensor* intermediate;\n      TF_LITE_ENSURE_OK(context,\n                        GetIntermediatesSafe(context, node, i, &intermediate));\n      TF_LITE_ENSURE(context,\n                     intermediate->quantization.type != kTfLiteNoQuantization);\n      auto* params = static_cast<TfLiteAffineQuantization*>(\n          intermediate->quantization.params);\n      intermediate_scale.push_back(params->scale->data[0]);\n      intermediate_zp.push_back(params->zero_point->data[0]);\n    } else {\n      // Q3.12 for activation functions.\n      intermediate_scale.push_back(std::pow(2, -12));\n      intermediate_zp.push_back(0);\n    }\n  }\n  // In the absence of projection, hidden becomes otuput and this intermediate\n  // is ignored.\n  TfLiteTensor* hidden;\n  TF_LITE_ENSURE_OK(context, GetIntermediatesSafe(context, node, 4, &hidden));\n  TF_LITE_ENSURE(context, hidden->quantization.type != kTfLiteNoQuantization);\n  auto* hidden_params =\n      static_cast<TfLiteAffineQuantization*>(hidden->quantization.params);\n  intermediate_scale.push_back(hidden_params->scale->data[0]);\n  intermediate_zp.push_back(hidden_params->zero_point->data[0]);\n\n  // Scales.\n  const float default_scale = 1.0;\n  float input_scale = default_scale;\n  float input_to_input_weight_scale = default_scale;\n  float recurrent_to_input_weight_scale = default_scale;\n  float cell_to_input_weight_scale = default_scale;\n  float input_to_forget_weight_scale = default_scale;\n  float recurrent_to_forget_weight_scale = default_scale;\n  float cell_to_forget_weight_scale = default_scale;\n  float input_to_cell_weight_scale = default_scale;\n  float recurrent_to_cell_weight_scale = default_scale;\n  float input_to_output_weight_scale = default_scale;\n  float recurrent_to_output_weight_scale = default_scale;\n  float cell_to_output_weight_scale = default_scale;\n  float projection_weight_scale = default_scale;\n  float layer_norm_input_scale = default_scale;\n  float layer_norm_forget_scale = default_scale;\n  float layer_norm_cell_scale = default_scale;\n  float layer_norm_output_scale = default_scale;\n  float output_state_scale = default_scale;\n  int cell_scale = 1;\n\n  // Effective scales.\n  float effective_input_to_input_scale = default_scale;\n  float effective_recurrent_to_input_scale = default_scale;\n  float effective_cell_to_input_scale = default_scale;\n  float effective_input_to_forget_scale = default_scale;\n  float effective_recurrent_to_forget_scale = default_scale;\n  float effective_cell_to_forget_scale = default_scale;\n  float effective_input_to_cell_scale = default_scale;\n  float effective_recurrent_to_cell_scale = default_scale;\n  float effective_input_to_output_scale = default_scale;\n  float effective_recurrent_to_output_scale = default_scale;\n  float effective_cell_to_output_scale = default_scale;\n  float effective_proj_scale = default_scale;\n  float effective_hidden_scale = default_scale;\n\n  // Populate scales.\n  if (!use_cifg) {\n    input_to_input_weight_scale = input_to_input_weights->params.scale;\n    recurrent_to_input_weight_scale = recurrent_to_input_weights->params.scale;\n  }\n\n  if (use_peephole) {\n    if (!use_cifg) {\n      cell_to_input_weight_scale = cell_to_input_weights->params.scale;\n    }\n    cell_to_forget_weight_scale = cell_to_forget_weights->params.scale;\n    cell_to_output_weight_scale = cell_to_output_weights->params.scale;\n  }\n\n  if (use_layer_norm) {\n    if (!use_cifg) {\n      layer_norm_input_scale = input_layer_norm_coefficients->params.scale;\n    }\n    layer_norm_forget_scale = forget_layer_norm_coefficients->params.scale;\n    layer_norm_cell_scale = cell_layer_norm_coefficients->params.scale;\n    layer_norm_output_scale = output_layer_norm_coefficients->params.scale;\n  }\n\n  if (use_projection) {\n    projection_weight_scale = projection_weights->params.scale;\n  }\n  output_state_scale = output_state->params.scale;\n\n  input_to_forget_weight_scale = input_to_forget_weights->params.scale;\n  input_to_cell_weight_scale = input_to_cell_weights->params.scale;\n  input_to_output_weight_scale = input_to_output_weights->params.scale;\n  recurrent_to_forget_weight_scale = recurrent_to_forget_weights->params.scale;\n  recurrent_to_cell_weight_scale = recurrent_to_cell_weights->params.scale;\n  recurrent_to_output_weight_scale = recurrent_to_output_weights->params.scale;\n\n  // Check cell state (already used above)\n  TF_LITE_ENSURE(context, CheckedLog2(cell_state->params.scale, &cell_scale));\n  // TF_LITE_ENSURE(context, cell_scale <= -9);\n  integer_lstm_param->cell_scale = cell_scale;\n  input_scale = input->params.scale;\n\n  // Calculate effective scales.\n  if (!use_cifg) {\n    effective_input_to_input_scale =\n        input_to_input_weight_scale * input_scale / intermediate_scale[0];\n    effective_recurrent_to_input_scale = recurrent_to_input_weight_scale *\n                                         output_state_scale /\n                                         intermediate_scale[0];\n  }\n  effective_input_to_forget_scale =\n      input_to_forget_weight_scale * input_scale / intermediate_scale[1];\n  effective_recurrent_to_forget_scale = recurrent_to_forget_weight_scale *\n                                        output_state_scale /\n                                        intermediate_scale[1];\n\n  effective_input_to_cell_scale =\n      input_to_cell_weight_scale * input_scale / intermediate_scale[2];\n  effective_recurrent_to_cell_scale = recurrent_to_cell_weight_scale *\n                                      output_state_scale /\n                                      intermediate_scale[2];\n\n  effective_input_to_output_scale =\n      input_to_output_weight_scale * input_scale / intermediate_scale[3];\n  effective_recurrent_to_output_scale = recurrent_to_output_weight_scale *\n                                        output_state_scale /\n                                        intermediate_scale[3];\n\n  effective_hidden_scale =\n      std::pow(2, -15) / intermediate_scale[4] * std::pow(2, -15);\n\n  effective_proj_scale =\n      projection_weight_scale * intermediate_scale[4] / output_state_scale;\n\n  if (use_peephole) {\n    if (!use_cifg) {\n      effective_cell_to_input_scale = std::pow(2, cell_scale) *  // NOLINT\n                                      cell_to_input_weight_scale /\n                                      intermediate_scale[0];\n    }\n    effective_cell_to_forget_scale = std::pow(2, cell_scale) *  // NOLINT\n                                     cell_to_forget_weight_scale /\n                                     intermediate_scale[1];\n    effective_cell_to_output_scale = std::pow(2, cell_scale) *  // NOLINT\n                                     cell_to_output_weight_scale /\n                                     intermediate_scale[3];\n  }\n\n  // Decompose scales.\n  QuantizeMultiplier(effective_input_to_input_scale,\n                     &integer_lstm_param->effective_input_to_input_scale_a,\n                     &integer_lstm_param->effective_input_to_input_scale_b);\n  QuantizeMultiplier(effective_recurrent_to_input_scale,\n                     &integer_lstm_param->effective_recurrent_to_input_scale_a,\n                     &integer_lstm_param->effective_recurrent_to_input_scale_b);\n  QuantizeMultiplier(effective_cell_to_input_scale,\n                     &integer_lstm_param->effective_cell_to_input_scale_a,\n                     &integer_lstm_param->effective_cell_to_input_scale_b);\n  QuantizeMultiplier(effective_input_to_forget_scale,\n                     &integer_lstm_param->effective_input_to_forget_scale_a,\n                     &integer_lstm_param->effective_input_to_forget_scale_b);\n  QuantizeMultiplier(\n      effective_recurrent_to_forget_scale,\n      &integer_lstm_param->effective_recurrent_to_forget_scale_a,\n      &integer_lstm_param->effective_recurrent_to_forget_scale_b);\n  QuantizeMultiplier(effective_cell_to_forget_scale,\n                     &integer_lstm_param->effective_cell_to_forget_scale_a,\n                     &integer_lstm_param->effective_cell_to_forget_scale_b);\n  QuantizeMultiplier(effective_input_to_cell_scale,\n                     &integer_lstm_param->effective_input_to_cell_scale_a,\n                     &integer_lstm_param->effective_input_to_cell_scale_b);\n  QuantizeMultiplier(effective_recurrent_to_cell_scale,\n                     &integer_lstm_param->effective_recurrent_to_cell_scale_a,\n                     &integer_lstm_param->effective_recurrent_to_cell_scale_b);\n  QuantizeMultiplier(effective_input_to_output_scale,\n                     &integer_lstm_param->effective_input_to_output_scale_a,\n                     &integer_lstm_param->effective_input_to_output_scale_b);\n  QuantizeMultiplier(\n      effective_recurrent_to_output_scale,\n      &integer_lstm_param->effective_recurrent_to_output_scale_a,\n      &integer_lstm_param->effective_recurrent_to_output_scale_b);\n  QuantizeMultiplier(effective_cell_to_output_scale,\n                     &integer_lstm_param->effective_cell_to_output_scale_a,\n                     &integer_lstm_param->effective_cell_to_output_scale_b);\n  QuantizeMultiplier(effective_proj_scale,\n                     &integer_lstm_param->effective_proj_scale_a,\n                     &integer_lstm_param->effective_proj_scale_b);\n  QuantizeMultiplier(effective_hidden_scale,\n                     &integer_lstm_param->effective_hidden_scale_a,\n                     &integer_lstm_param->effective_hidden_scale_b);\n  QuantizeMultiplier(layer_norm_input_scale,\n                     &integer_lstm_param->layer_norm_input_scale_a,\n                     &integer_lstm_param->layer_norm_input_scale_b);\n  QuantizeMultiplier(layer_norm_forget_scale,\n                     &integer_lstm_param->layer_norm_forget_scale_a,\n                     &integer_lstm_param->layer_norm_forget_scale_b);\n  QuantizeMultiplier(layer_norm_cell_scale,\n                     &integer_lstm_param->layer_norm_cell_scale_a,\n                     &integer_lstm_param->layer_norm_cell_scale_b);\n  QuantizeMultiplier(layer_norm_output_scale,\n                     &integer_lstm_param->layer_norm_output_scale_a,\n                     &integer_lstm_param->layer_norm_output_scale_b);\n\n  integer_lstm_param->hidden_zp = intermediate_zp[4];\n\n  // 10000 is used to make sure the kernel logic does not overflow.\n  if (!use_cifg) {\n    integer_lstm_param->input_variance_guard =\n        std::max(1, static_cast<int32_t>(10000 * layer_norm_input_scale));\n  }\n  integer_lstm_param->forget_variance_guard =\n      std::max(1, static_cast<int32_t>(10000 * layer_norm_forget_scale));\n  integer_lstm_param->cell_variance_guard =\n      std::max(1, static_cast<int32_t>(10000 * layer_norm_cell_scale));\n  integer_lstm_param->output_variance_guard =\n      std::max(1, static_cast<int32_t>(10000 * layer_norm_output_scale));\n\n  return kTfLiteOk;\n}\n\n}  // namespace\n\n// Temporary tensors\nenum TemporaryTensor {\n  kScratchBuffer = 0,\n  kInputQuantized = 1,\n  kOutputStateQuantized = 2,\n  kCellStateQuantized = 3,\n  kInputScalingFactors = 4,\n  kOutputStateScalingFactors = 5,\n  kProductScalingFactors = 6,\n  kRecoveredCellWeights = 7,\n  kAccumScratch = 8,\n  kInputZeroPoints = 9,\n  kOutputStateZeroPoints = 10,\n  kRowSums = 11,\n  kNumTemporaryTensors = 12,\n};\n\nvoid* Init(TfLiteContext* context, const char* buffer, size_t length) {\n  auto* op_data = new OpData();\n  context->AddTensors(context, kNumTemporaryTensors,\n                      &op_data->scratch_tensor_index);\n  return op_data;\n}\n\nvoid Free(TfLiteContext* context, void* buffer) {\n  delete reinterpret_cast<OpData*>(buffer);\n}\n\n// Check that input tensor dimensions matches with each other.\nTfLiteStatus CheckInputTensorDimensions(TfLiteContext* context,\n                                        TfLiteNode* node, int n_input,\n                                        int n_output, int n_cell,\n                                        bool use_layer_norm, bool is_integer) {\n  const auto* params = reinterpret_cast<TfLiteLSTMParams*>(node->builtin_data);\n\n  // Making sure clipping parameters have valid values.\n  // == 0 means no clipping\n  //  > 0 means clipping\n  TF_LITE_ENSURE(context, params->cell_clip >= 0);\n  TF_LITE_ENSURE(context, params->proj_clip >= 0);\n\n  const TfLiteTensor* input_to_input_weights = GetOptionalInputTensor(\n      context, node, lstm::full::kInputToInputWeightsTensor);\n  if (input_to_input_weights != nullptr) {\n    TF_LITE_ENSURE_EQ(context, input_to_input_weights->dims->size, 2);\n    TF_LITE_ENSURE_EQ(context, input_to_input_weights->dims->data[0], n_cell);\n    TF_LITE_ENSURE_EQ(context, input_to_input_weights->dims->data[1], n_input);\n  }\n\n  const TfLiteTensor* input_to_forget_weights;\n  TF_LITE_ENSURE_OK(\n      context,\n      GetInputSafe(context, node, lstm::full::kInputToForgetWeightsTensor,\n                   &input_to_forget_weights));\n  TF_LITE_ENSURE_EQ(context, input_to_forget_weights->dims->size, 2);\n  TF_LITE_ENSURE_EQ(context, input_to_forget_weights->dims->data[0], n_cell);\n  TF_LITE_ENSURE_EQ(context, input_to_forget_weights->dims->data[1], n_input);\n\n  const TfLiteTensor* input_to_cell_weights;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node,\n                                          lstm::full::kInputToCellWeightsTensor,\n                                          &input_to_cell_weights));\n  TF_LITE_ENSURE_EQ(context, input_to_cell_weights->dims->size, 2);\n  TF_LITE_ENSURE_EQ(context, input_to_cell_weights->dims->data[0], n_cell);\n  TF_LITE_ENSURE_EQ(context, input_to_cell_weights->dims->data[1], n_input);\n\n  const TfLiteTensor* recurrent_to_input_weights = GetOptionalInputTensor(\n      context, node, lstm::full::kRecurrentToInputWeightsTensor);\n  if (recurrent_to_input_weights != nullptr) {\n    TF_LITE_ENSURE_EQ(context, recurrent_to_input_weights->dims->size, 2);\n    TF_LITE_ENSURE_EQ(context, recurrent_to_input_weights->dims->data[0],\n                      n_cell);\n    TF_LITE_ENSURE_EQ(context, recurrent_to_input_weights->dims->data[1],\n                      n_output);\n  }\n\n  const TfLiteTensor* recurrent_to_forget_weights;\n  TF_LITE_ENSURE_OK(\n      context,\n      GetInputSafe(context, node, lstm::full::kRecurrentToForgetWeightsTensor,\n                   &recurrent_to_forget_weights));\n  TF_LITE_ENSURE_EQ(context, recurrent_to_forget_weights->dims->size, 2);\n  TF_LITE_ENSURE_EQ(context, recurrent_to_forget_weights->dims->data[0],\n                    n_cell);\n  TF_LITE_ENSURE_EQ(context, recurrent_to_forget_weights->dims->data[1],\n                    n_output);\n\n  const TfLiteTensor* recurrent_to_cell_weights;\n  TF_LITE_ENSURE_OK(\n      context,\n      GetInputSafe(context, node, lstm::full::kRecurrentToCellWeightsTensor,\n                   &recurrent_to_cell_weights));\n  TF_LITE_ENSURE_EQ(context, recurrent_to_cell_weights->dims->size, 2);\n  TF_LITE_ENSURE_EQ(context, recurrent_to_cell_weights->dims->data[0], n_cell);\n  TF_LITE_ENSURE_EQ(context, recurrent_to_cell_weights->dims->data[1],\n                    n_output);\n\n  // We make sure the input-gate's parameters are either both present (regular\n  // LSTM) or not at all (CIFG-LSTM).\n  const bool cifg_weights_all_or_none =\n      ((input_to_input_weights != nullptr) &&\n       (recurrent_to_input_weights != nullptr)) ||\n      ((input_to_input_weights == nullptr) &&\n       (recurrent_to_input_weights == nullptr));\n  TF_LITE_ENSURE(context, cifg_weights_all_or_none == true);\n\n  const TfLiteTensor* cell_to_input_weights = GetOptionalInputTensor(\n      context, node, lstm::full::kCellToInputWeightsTensor);\n  if (cell_to_input_weights != nullptr) {\n    TF_LITE_ENSURE_EQ(context, cell_to_input_weights->dims->size, 1);\n    TF_LITE_ENSURE_EQ(context, cell_to_input_weights->dims->data[0], n_cell);\n    TF_LITE_ENSURE_TYPES_EQ(\n        context, cell_to_input_weights->type,\n        is_integer ? kTfLiteInt16 : input_to_forget_weights->type);\n  }\n\n  const TfLiteTensor* cell_to_forget_weights = GetOptionalInputTensor(\n      context, node, lstm::full::kCellToForgetWeightsTensor);\n  if (cell_to_forget_weights != nullptr) {\n    TF_LITE_ENSURE_EQ(context, cell_to_forget_weights->dims->size, 1);\n    TF_LITE_ENSURE_EQ(context, cell_to_forget_weights->dims->data[0], n_cell);\n    TF_LITE_ENSURE_TYPES_EQ(\n        context, cell_to_forget_weights->type,\n        is_integer ? kTfLiteInt16 : input_to_forget_weights->type);\n  }\n\n  const TfLiteTensor* cell_to_output_weights = GetOptionalInputTensor(\n      context, node, lstm::full::kCellToOutputWeightsTensor);\n  if (cell_to_output_weights != nullptr) {\n    TF_LITE_ENSURE_EQ(context, cell_to_output_weights->dims->size, 1);\n    TF_LITE_ENSURE_EQ(context, cell_to_output_weights->dims->data[0], n_cell);\n    TF_LITE_ENSURE_TYPES_EQ(\n        context, cell_to_output_weights->type,\n        is_integer ? kTfLiteInt16 : input_to_forget_weights->type);\n  }\n\n  // Making sure the peephole weights are there all or none.\n  const bool use_cifg = (input_to_input_weights == nullptr);\n  const bool peephole_weights_all_or_none =\n      ((cell_to_input_weights != nullptr || use_cifg) &&\n       (cell_to_forget_weights != nullptr) &&\n       (cell_to_output_weights != nullptr)) ||\n      ((cell_to_input_weights == nullptr) &&\n       (cell_to_forget_weights == nullptr) &&\n       (cell_to_output_weights == nullptr));\n  TF_LITE_ENSURE(context, peephole_weights_all_or_none == true);\n\n  // Make sure the input gate bias is present only when not a CIFG-LSTM.\n  const TfLiteTensor* input_gate_bias =\n      GetOptionalInputTensor(context, node, lstm::full::kInputGateBiasTensor);\n  if (use_cifg) {\n    TF_LITE_ENSURE_EQ(context, input_gate_bias, nullptr);\n  } else {\n    TF_LITE_ENSURE_EQ(context, input_gate_bias->dims->size, 1);\n    TF_LITE_ENSURE_EQ(context, input_gate_bias->dims->data[0], n_cell);\n    if (is_integer) {\n      TF_LITE_ENSURE_TYPES_EQ(context, input_gate_bias->type, kTfLiteInt32);\n    } else {\n      TF_LITE_ENSURE_TYPES_EQ(context, input_gate_bias->type, kTfLiteFloat32);\n    }\n  }\n\n  const TfLiteTensor* forget_gate_bias;\n  TF_LITE_ENSURE_OK(\n      context, GetInputSafe(context, node, lstm::full::kForgetGateBiasTensor,\n                            &forget_gate_bias));\n  TF_LITE_ENSURE_EQ(context, forget_gate_bias->dims->size, 1);\n  TF_LITE_ENSURE_EQ(context, forget_gate_bias->dims->data[0], n_cell);\n  if (is_integer) {\n    TF_LITE_ENSURE_TYPES_EQ(context, forget_gate_bias->type, kTfLiteInt32);\n  } else {\n    TF_LITE_ENSURE_TYPES_EQ(context, forget_gate_bias->type, kTfLiteFloat32);\n  }\n\n  const TfLiteTensor* cell_gate_bias;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, lstm::full::kCellGateBiasTensor,\n                                 &cell_gate_bias));\n  TF_LITE_ENSURE_EQ(context, cell_gate_bias->dims->size, 1);\n  TF_LITE_ENSURE_EQ(context, cell_gate_bias->dims->data[0], n_cell);\n  if (is_integer) {\n    TF_LITE_ENSURE_TYPES_EQ(context, cell_gate_bias->type, kTfLiteInt32);\n  } else {\n    TF_LITE_ENSURE_TYPES_EQ(context, cell_gate_bias->type, kTfLiteFloat32);\n  }\n\n  const TfLiteTensor* output_gate_bias;\n  TF_LITE_ENSURE_OK(\n      context, GetInputSafe(context, node, lstm::full::kOutputGateBiasTensor,\n                            &output_gate_bias));\n  TF_LITE_ENSURE_EQ(context, output_gate_bias->dims->size, 1);\n  TF_LITE_ENSURE_EQ(context, output_gate_bias->dims->data[0], n_cell);\n  if (is_integer) {\n    TF_LITE_ENSURE_TYPES_EQ(context, output_gate_bias->type, kTfLiteInt32);\n  } else {\n    TF_LITE_ENSURE_TYPES_EQ(context, output_gate_bias->type, kTfLiteFloat32);\n  }\n\n  const TfLiteTensor* projection_weights = GetOptionalInputTensor(\n      context, node, lstm::full::kProjectionWeightsTensor);\n  if (projection_weights != nullptr) {\n    TF_LITE_ENSURE_EQ(context, projection_weights->dims->size, 2);\n    TF_LITE_ENSURE_EQ(context, projection_weights->dims->data[0], n_output);\n    TF_LITE_ENSURE_EQ(context, projection_weights->dims->data[1], n_cell);\n  }\n\n  const TfLiteTensor* projection_bias =\n      GetOptionalInputTensor(context, node, lstm::full::kProjectionBiasTensor);\n  if (projection_bias != nullptr) {\n    TF_LITE_ENSURE_EQ(context, projection_bias->dims->size, 1);\n    TF_LITE_ENSURE_EQ(context, projection_bias->dims->data[0], n_output);\n    if (is_integer) {\n      TF_LITE_ENSURE_TYPES_EQ(context, projection_bias->type, kTfLiteInt32);\n    } else {\n      TF_LITE_ENSURE_TYPES_EQ(context, projection_bias->type, kTfLiteFloat32);\n    }\n  }\n\n  // Making sure the projection tensors are consistent:\n  // 1) If projection weight is not present, then projection bias should not be\n  // present.\n  // 2) If projection weight is present, then projection bias is optional.\n  // TODO(ghodrat): make sure this is correct.\n  const bool projecton_tensors_consistent =\n      ((projection_weights != nullptr) || (projection_bias == nullptr));\n  TF_LITE_ENSURE(context, projecton_tensors_consistent == true);\n\n  if (use_layer_norm) {\n    const TfLiteTensor* input_layer_norm_coefficients = GetOptionalInputTensor(\n        context, node, lstm::full::kInputLayerNormCoefficientsTensor);\n    if (use_cifg) {\n      TF_LITE_ENSURE_EQ(context, input_layer_norm_coefficients, nullptr);\n    } else {\n      TF_LITE_ENSURE(context, input_layer_norm_coefficients != nullptr);\n      TF_LITE_ENSURE_EQ(context, input_layer_norm_coefficients->dims->size, 1);\n      TF_LITE_ENSURE_EQ(context, input_layer_norm_coefficients->dims->data[0],\n                        n_cell);\n      if (is_integer) {\n        TF_LITE_ENSURE_TYPES_EQ(context, input_layer_norm_coefficients->type,\n                                kTfLiteInt16);\n      } else {\n        TF_LITE_ENSURE_TYPES_EQ(context, input_layer_norm_coefficients->type,\n                                kTfLiteFloat32);\n      }\n    }\n\n    const TfLiteTensor* forget_layer_norm_coefficients;\n    TF_LITE_ENSURE_OK(\n        context, GetInputSafe(context, node,\n                              lstm::full::kForgetLayerNormCoefficientsTensor,\n                              &forget_layer_norm_coefficients));\n    TF_LITE_ENSURE_EQ(context, forget_layer_norm_coefficients->dims->size, 1);\n    TF_LITE_ENSURE_EQ(context, forget_layer_norm_coefficients->dims->data[0],\n                      n_cell);\n    if (is_integer) {\n      TF_LITE_ENSURE_TYPES_EQ(context, forget_layer_norm_coefficients->type,\n                              kTfLiteInt16);\n    } else {\n      TF_LITE_ENSURE_TYPES_EQ(context, forget_layer_norm_coefficients->type,\n                              kTfLiteFloat32);\n    }\n\n    const TfLiteTensor* cell_layer_norm_coefficients;\n    TF_LITE_ENSURE_OK(context,\n                      GetInputSafe(context, node,\n                                   lstm::full::kCellLayerNormCoefficientsTensor,\n                                   &cell_layer_norm_coefficients));\n    TF_LITE_ENSURE_EQ(context, cell_layer_norm_coefficients->dims->size, 1);\n    TF_LITE_ENSURE_EQ(context, cell_layer_norm_coefficients->dims->data[0],\n                      n_cell);\n    if (is_integer) {\n      TF_LITE_ENSURE_TYPES_EQ(context, cell_layer_norm_coefficients->type,\n                              kTfLiteInt16);\n    } else {\n      TF_LITE_ENSURE_TYPES_EQ(context, cell_layer_norm_coefficients->type,\n                              kTfLiteFloat32);\n    }\n\n    const TfLiteTensor* output_layer_norm_coefficients;\n    TF_LITE_ENSURE_OK(\n        context, GetInputSafe(context, node,\n                              lstm::full::kOutputLayerNormCoefficientsTensor,\n                              &output_layer_norm_coefficients));\n    TF_LITE_ENSURE_EQ(context, output_layer_norm_coefficients->dims->size, 1);\n    TF_LITE_ENSURE_EQ(context, output_layer_norm_coefficients->dims->data[0],\n                      n_cell);\n    if (is_integer) {\n      TF_LITE_ENSURE_TYPES_EQ(context, output_layer_norm_coefficients->type,\n                              kTfLiteInt16);\n    } else {\n      TF_LITE_ENSURE_TYPES_EQ(context, output_layer_norm_coefficients->type,\n                              kTfLiteFloat32);\n    }\n  }\n\n  return kTfLiteOk;\n}\n\nTfLiteStatus PrecomputeZeroPointTimesWeightWithBias(\n    TfLiteContext* context, int32_t zero_point,\n    const TfLiteTensor* weight_tensor, const TfLiteTensor* bias_tensor,\n    std::unique_ptr<int32_t[]>* output) {\n  if (weight_tensor == nullptr) {\n    return kTfLiteOk;\n  }\n\n  const RuntimeShape& weight_shape = GetTensorShape(weight_tensor);\n  TF_LITE_ENSURE_EQ(context, weight_shape.DimensionsCount(), 2);\n  const int row = weight_shape.Dims(0);\n  const int col = weight_shape.Dims(1);\n  output->reset(new int32_t[row]);\n  if (bias_tensor == nullptr) {\n    memset(output->get(), 0, row * sizeof(int32_t));\n  } else {\n    const int32_t* bias = GetTensorData<int32_t>(bias_tensor);\n    memcpy(output->get(), bias, row * sizeof(int32_t));\n  }\n  if (zero_point != 0) {\n    const int8_t* weight = GetTensorData<int8_t>(weight_tensor);\n    tensor_utils::MatrixScalarMultiplyAccumulate(weight, zero_point, row, col,\n                                                 output->get());\n  }\n  return kTfLiteOk;\n}\n\nTfLiteStatus PopulatePrecomputedZPTimesWeightsWithBias(TfLiteContext* context,\n                                                       OpData* op_data,\n                                                       TfLiteNode* node) {\n  const TfLiteTensor* input;\n  TF_LITE_ENSURE_OK(\n      context, GetInputSafe(context, node, lstm::full::kInputTensor, &input));\n  const TfLiteTensor* output_state =\n      GetVariableInput(context, node, lstm::full::kOutputStateTensor);\n  TF_LITE_ENSURE(context, output_state != nullptr);\n\n  const int32_t input_zero_point = -input->params.zero_point;\n  const int32_t output_state_zero_point = -output_state->params.zero_point;\n\n  const TfLiteTensor* input_to_input_weights = GetOptionalInputTensor(\n      context, node, lstm::full::kInputToInputWeightsTensor);\n  const TfLiteTensor* input_to_forget_weights;\n  TF_LITE_ENSURE_OK(\n      context,\n      GetInputSafe(context, node, lstm::full::kInputToForgetWeightsTensor,\n                   &input_to_forget_weights));\n  const TfLiteTensor* input_to_cell_weights;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node,\n                                          lstm::full::kInputToCellWeightsTensor,\n                                          &input_to_cell_weights));\n  const TfLiteTensor* input_to_output_weights;\n  TF_LITE_ENSURE_OK(\n      context,\n      GetInputSafe(context, node, lstm::full::kInputToOutputWeightsTensor,\n                   &input_to_output_weights));\n\n  const TfLiteTensor* recurrent_to_input_weights = GetOptionalInputTensor(\n      context, node, lstm::full::kRecurrentToInputWeightsTensor);\n  const TfLiteTensor* recurrent_to_forget_weights;\n  TF_LITE_ENSURE_OK(\n      context,\n      GetInputSafe(context, node, lstm::full::kRecurrentToForgetWeightsTensor,\n                   &recurrent_to_forget_weights));\n  const TfLiteTensor* recurrent_to_cell_weights;\n  TF_LITE_ENSURE_OK(\n      context,\n      GetInputSafe(context, node, lstm::full::kRecurrentToCellWeightsTensor,\n                   &recurrent_to_cell_weights));\n  const TfLiteTensor* recurrent_to_output_weights;\n  TF_LITE_ENSURE_OK(\n      context,\n      GetInputSafe(context, node, lstm::full::kRecurrentToOutputWeightsTensor,\n                   &recurrent_to_output_weights));\n\n  const TfLiteTensor* projection_weights = GetOptionalInputTensor(\n      context, node, lstm::full::kProjectionWeightsTensor);\n  const TfLiteTensor* projection_bias =\n      GetOptionalInputTensor(context, node, lstm::full::kProjectionBiasTensor);\n\n  lstm_eval::IntegerLstmParameter* integer_lstm_params =\n      &op_data->integer_lstm_param;\n\n  const TfLiteTensor* intermediate =\n      &context->tensors[node->intermediates->data[4]];\n  TF_LITE_ENSURE(context,\n                 intermediate->quantization.type != kTfLiteNoQuantization);\n  const auto* params =\n      static_cast<TfLiteAffineQuantization*>(intermediate->quantization.params);\n  const int32_t hidden_zp = params->zero_point->data[0];\n\n  // Get bias and perform zero point calculation.\n  // When there is layer normalization, the gate bias does not apply to matmul\n  // directly:\n  //      y = ln(w * x + w * r + w * c) + b.\n  const bool is_layer_norm = op_data->use_layer_norm;\n\n  // Forget gate.\n  const TfLiteTensor* forget_gate_bias =\n      is_layer_norm\n          ? nullptr\n          : GetInput(context, node, lstm::full::kForgetGateBiasTensor);\n  TF_LITE_ENSURE_OK(\n      context,\n      PrecomputeZeroPointTimesWeightWithBias(\n          context, input_zero_point, input_to_forget_weights, forget_gate_bias,\n          &(integer_lstm_params->input_to_forget_effective_bias)));\n\n  TF_LITE_ENSURE_OK(\n      context,\n      PrecomputeZeroPointTimesWeightWithBias(\n          context, output_state_zero_point, recurrent_to_forget_weights,\n          nullptr, &(integer_lstm_params->recurrent_to_forget_effective_bias)));\n\n  // Modulation gate.\n  const TfLiteTensor* cell_gate_bias =\n      is_layer_norm ? nullptr\n                    : GetInput(context, node, lstm::full::kCellGateBiasTensor);\n  TF_LITE_ENSURE_OK(\n      context,\n      PrecomputeZeroPointTimesWeightWithBias(\n          context, input_zero_point, input_to_cell_weights, cell_gate_bias,\n          &(integer_lstm_params->input_to_cell_effective_bias)));\n  TF_LITE_ENSURE_OK(\n      context,\n      PrecomputeZeroPointTimesWeightWithBias(\n          context, output_state_zero_point, recurrent_to_cell_weights, nullptr,\n          &(integer_lstm_params->recurrent_to_cell_effective_bias)));\n\n  // Output gate.\n  const TfLiteTensor* output_gate_bias =\n      is_layer_norm\n          ? nullptr\n          : GetInput(context, node, lstm::full::kOutputGateBiasTensor);\n  TF_LITE_ENSURE_OK(\n      context,\n      PrecomputeZeroPointTimesWeightWithBias(\n          context, input_zero_point, input_to_output_weights, output_gate_bias,\n          &(integer_lstm_params->input_to_output_effective_bias)));\n\n  TF_LITE_ENSURE_OK(\n      context,\n      PrecomputeZeroPointTimesWeightWithBias(\n          context, output_state_zero_point, recurrent_to_output_weights,\n          nullptr, &(integer_lstm_params->recurrent_to_output_effective_bias)));\n\n  // Input gate. The calculation is only meaningful for non-cifg case.\n  const TfLiteTensor* input_gate_bias =\n      is_layer_norm ? nullptr\n                    : GetInput(context, node, lstm::full::kInputGateBiasTensor);\n  TF_LITE_ENSURE_OK(\n      context,\n      PrecomputeZeroPointTimesWeightWithBias(\n          context, input_zero_point, input_to_input_weights, input_gate_bias,\n          &(integer_lstm_params->input_to_input_effective_bias)));\n  TF_LITE_ENSURE_OK(\n      context,\n      PrecomputeZeroPointTimesWeightWithBias(\n          context, output_state_zero_point, recurrent_to_input_weights, nullptr,\n          &(integer_lstm_params->recurrent_to_input_effective_bias)));\n\n  // Projection bias. The calculation is only meaningful for with projection.\n  TF_LITE_ENSURE_OK(context,\n                    PrecomputeZeroPointTimesWeightWithBias(\n                        context, hidden_zp, projection_weights, projection_bias,\n                        &(integer_lstm_params->projection_effective_bias)));\n  return kTfLiteOk;\n}\n\n// Resize the output and  state tensors based on the sizes of the input tensors.\n// Allocate a temporary scratch tensor. Also check that the sizes of the input\n// tensors match each other.\nTfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n  OpData* op_data = reinterpret_cast<OpData*>(node->user_data);\n  const int scratch_tensor_index = op_data->scratch_tensor_index;\n\n  // Check we have all the inputs and outputs we need.\n  bool use_layer_norm = false;\n  if (node->inputs->size == 24) {\n    const TfLiteTensor* forget_layer_norm_coefficients = GetOptionalInputTensor(\n        context, node, lstm::full::kForgetLayerNormCoefficientsTensor);\n    if (forget_layer_norm_coefficients == nullptr) {\n      use_layer_norm = false;\n    } else {\n      use_layer_norm = true;\n    }\n  } else if (node->inputs->size == 20) {\n    // This is deprecated and is only kept here for backward compatibility.\n    use_layer_norm = false;\n  } else {\n    context->ReportError(\n        context, \"The LSTM Full kernel expects 20 or 24 inputs. Got %d inputs\",\n        node->inputs->size);\n    return kTfLiteError;\n  }\n  TF_LITE_ENSURE_EQ(context, node->outputs->size, 1);\n  op_data->use_layer_norm = use_layer_norm;\n\n  // Inferring batch size, number of outputs and sequence length and\n  // number of cells from the input tensors.\n  const TfLiteTensor* input;\n  TF_LITE_ENSURE_OK(\n      context, GetInputSafe(context, node, lstm::full::kInputTensor, &input));\n  const bool is_integer = input->type == kTfLiteInt8;\n  TF_LITE_ENSURE(context, input->dims->size > 1);\n  const auto* params =\n      reinterpret_cast<TfLiteUnidirectionalSequenceLSTMParams*>(\n          node->builtin_data);\n  const bool time_major = params->time_major;\n  const int n_batch = time_major ? input->dims->data[1] : input->dims->data[0];\n  const int n_input = input->dims->data[2];\n\n  const TfLiteTensor* input_to_output_weights;\n  TF_LITE_ENSURE_OK(\n      context,\n      GetInputSafe(context, node, lstm::full::kInputToOutputWeightsTensor,\n                   &input_to_output_weights));\n  const int n_cell = input_to_output_weights->dims->data[0];\n  TF_LITE_ENSURE_EQ(context, input_to_output_weights->dims->size, 2);\n  TF_LITE_ENSURE_EQ(context, input_to_output_weights->dims->data[1], n_input);\n\n  const TfLiteTensor* recurrent_to_output_weights;\n  TF_LITE_ENSURE_OK(\n      context,\n      GetInputSafe(context, node, lstm::full::kRecurrentToOutputWeightsTensor,\n                   &recurrent_to_output_weights));\n  TF_LITE_ENSURE_EQ(context, recurrent_to_output_weights->dims->size, 2);\n  TF_LITE_ENSURE_EQ(context, recurrent_to_output_weights->dims->data[0],\n                    n_cell);\n  const int n_output = recurrent_to_output_weights->dims->data[1];\n\n  // Check that input tensor dimensions matches with each other.\n  TF_LITE_ENSURE_OK(\n      context, CheckInputTensorDimensions(context, node, n_input, n_output,\n                                          n_cell, use_layer_norm, is_integer));\n\n  // Get the pointer to output, output_state and cell_state buffer tensors.\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node,\n                                           lstm::full::kOutputTensor, &output));\n\n  TfLiteTensor* output_state =\n      GetVariableInput(context, node, lstm::full::kOutputStateTensor);\n  TF_LITE_ENSURE(context, output_state != nullptr);\n  TfLiteTensor* cell_state =\n      GetVariableInput(context, node, lstm::full::kCellStateTensor);\n  TF_LITE_ENSURE(context, cell_state != nullptr);\n\n  // Check the shape of input state tensors.\n  // These tensor may be 1D or 2D. It's fine as long as the total size is\n  // correct.\n  TF_LITE_ENSURE_EQ(context, NumElements(output_state), n_batch * n_output);\n  TF_LITE_ENSURE_EQ(context, NumElements(cell_state), n_batch * n_cell);\n\n  // Resize the output tensors.\n  TfLiteIntArray* output_size = TfLiteIntArrayCopy(input->dims);\n  output_size->data[input->dims->size - 1] = n_output;\n  TF_LITE_ENSURE_OK(context,\n                    context->ResizeTensor(context, output, output_size));\n\n  if (is_integer) {\n    const int num_intermediate_tensors = node->intermediates->size;\n    TF_LITE_ENSURE(context, num_intermediate_tensors == 5);\n  }\n\n  TfLiteIntArrayFree(node->temporaries);\n  if (IsHybridOp(input, input_to_output_weights)) {\n    node->temporaries = TfLiteIntArrayCreate(kNumTemporaryTensors);\n  } else if (is_integer) {\n    node->temporaries = TfLiteIntArrayCreate(6);\n  } else {\n    node->temporaries = TfLiteIntArrayCreate(1);\n  }\n  node->temporaries->data[kScratchBuffer] =\n      scratch_tensor_index + kScratchBuffer;\n\n  // Create a scratch buffer tensor.\n  TfLiteTensor* scratch_buffer;\n  TF_LITE_ENSURE_OK(context, GetTemporarySafe(context, node, kScratchBuffer,\n                                              &scratch_buffer));\n  scratch_buffer->type = input->type;\n  scratch_buffer->allocation_type = kTfLiteArenaRw;\n\n  const TfLiteTensor* input_to_input_weights = GetOptionalInputTensor(\n      context, node, lstm::full::kInputToInputWeightsTensor);\n  const bool use_cifg = (input_to_input_weights == nullptr);\n  TfLiteIntArray* scratch_buffer_size = TfLiteIntArrayCreate(2);\n  scratch_buffer_size->data[0] = n_batch;\n  if (use_cifg) {\n    // Reserving space for Cell, Forget, Output gates\n    scratch_buffer_size->data[1] = n_cell * 3;\n  } else {\n    // Reserving space for Input, Cell, Forget, Output gates\n    scratch_buffer_size->data[1] = n_cell * 4;\n  }\n  TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, scratch_buffer,\n                                                   scratch_buffer_size));\n\n  if (IsHybridOp(input, input_to_output_weights)) {\n    op_data->compute_row_sums = true;\n    // Allocate temporary tensors to store quantized values of input,\n    // output_state and cell_state tensors.\n    node->temporaries->data[kInputQuantized] =\n        scratch_tensor_index + kInputQuantized;\n    TfLiteTensor* input_quantized;\n    TF_LITE_ENSURE_OK(context, GetTemporarySafe(context, node, kInputQuantized,\n                                                &input_quantized));\n    input_quantized->type = input_to_output_weights->type;\n    input_quantized->allocation_type = kTfLiteArenaRw;\n    if (!TfLiteIntArrayEqual(input_quantized->dims, input->dims)) {\n      TfLiteIntArray* input_quantized_size = TfLiteIntArrayCopy(input->dims);\n      TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, input_quantized,\n                                                       input_quantized_size));\n    }\n    node->temporaries->data[kOutputStateQuantized] =\n        scratch_tensor_index + kOutputStateQuantized;\n    TfLiteTensor* output_state_quantized;\n    TF_LITE_ENSURE_OK(context,\n                      GetTemporarySafe(context, node, kOutputStateQuantized,\n                                       &output_state_quantized));\n    output_state_quantized->type = input_to_output_weights->type;\n    output_state_quantized->allocation_type = kTfLiteArenaRw;\n    if (!TfLiteIntArrayEqual(output_state_quantized->dims,\n                             output_state->dims)) {\n      TfLiteIntArray* output_state_quantized_size =\n          TfLiteIntArrayCopy(output_state->dims);\n      TF_LITE_ENSURE_OK(context,\n                        context->ResizeTensor(context, output_state_quantized,\n                                              output_state_quantized_size));\n    }\n    node->temporaries->data[kCellStateQuantized] =\n        scratch_tensor_index + kCellStateQuantized;\n    TfLiteTensor* cell_state_quantized;\n    TF_LITE_ENSURE_OK(context,\n                      GetTemporarySafe(context, node, kCellStateQuantized,\n                                       &cell_state_quantized));\n    cell_state_quantized->type = input_to_output_weights->type;\n    cell_state_quantized->allocation_type = kTfLiteArenaRw;\n    if (!TfLiteIntArrayEqual(cell_state_quantized->dims, cell_state->dims)) {\n      TfLiteIntArray* cell_state_quantized_size =\n          TfLiteIntArrayCopy(cell_state->dims);\n      TF_LITE_ENSURE_OK(context,\n                        context->ResizeTensor(context, cell_state_quantized,\n                                              cell_state_quantized_size));\n    }\n\n    // Allocate temporary tensors to store scaling factors and product scaling\n    // factors. The latter is a convenience storage which allows to quantize\n    // a vector once (which produces the scaling factors) and multiply it with\n    // different matrices (which requires multiplying the scaling factors with\n    // the scaling factor of the matrix).\n    node->temporaries->data[kInputScalingFactors] =\n        op_data->scratch_tensor_index + kInputScalingFactors;\n    TfLiteTensor* input_sf;\n    TF_LITE_ENSURE_OK(\n        context,\n        GetTemporarySafe(context, node, kInputScalingFactors, &input_sf));\n    input_sf->type = kTfLiteFloat32;\n    input_sf->allocation_type = kTfLiteArenaRw;\n    int scaling_dims[1] = {n_batch};\n    if (!TfLiteIntArrayEqualsArray(input_sf->dims, 1, scaling_dims)) {\n      TfLiteIntArray* input_sf_size = TfLiteIntArrayCreate(1);\n      input_sf_size->data[0] = n_batch;\n      TF_LITE_ENSURE_OK(\n          context, context->ResizeTensor(context, input_sf, input_sf_size));\n    }\n    node->temporaries->data[kOutputStateScalingFactors] =\n        op_data->scratch_tensor_index + kOutputStateScalingFactors;\n    TfLiteTensor* output_state_sf;\n    TF_LITE_ENSURE_OK(\n        context, GetTemporarySafe(context, node, kOutputStateScalingFactors,\n                                  &output_state_sf));\n    output_state_sf->type = kTfLiteFloat32;\n    output_state_sf->allocation_type = kTfLiteArenaRw;\n    if (!TfLiteIntArrayEqualsArray(output_state_sf->dims, 1, scaling_dims)) {\n      TfLiteIntArray* output_state_sf_size = TfLiteIntArrayCreate(1);\n      output_state_sf_size->data[0] = n_batch;\n      TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, output_state_sf,\n                                                       output_state_sf_size));\n    }\n    node->temporaries->data[kProductScalingFactors] =\n        scratch_tensor_index + kProductScalingFactors;\n    TfLiteTensor* prod_scaling_factors;\n    TF_LITE_ENSURE_OK(context,\n                      GetTemporarySafe(context, node, kProductScalingFactors,\n                                       &prod_scaling_factors));\n    prod_scaling_factors->type = kTfLiteFloat32;\n    prod_scaling_factors->allocation_type = kTfLiteArenaRw;\n    if (!TfLiteIntArrayEqualsArray(prod_scaling_factors->dims, 1,\n                                   scaling_dims)) {\n      TfLiteIntArray* prod_scaling_factors_size = TfLiteIntArrayCreate(1);\n      prod_scaling_factors_size->data[0] = n_batch;\n      TF_LITE_ENSURE_OK(context,\n                        context->ResizeTensor(context, prod_scaling_factors,\n                                              prod_scaling_factors_size));\n    }\n\n    // Allocate a temporary tensor to store the recovered cell weights. Since\n    // this is used for diagonal matrices, only need to store n_cell values.\n    node->temporaries->data[kRecoveredCellWeights] =\n        scratch_tensor_index + kRecoveredCellWeights;\n    TfLiteTensor* recovered_cell_weights;\n    TF_LITE_ENSURE_OK(context,\n                      GetTemporarySafe(context, node, kRecoveredCellWeights,\n                                       &recovered_cell_weights));\n    recovered_cell_weights->type = kTfLiteFloat32;\n    recovered_cell_weights->allocation_type = kTfLiteArenaRw;\n    int recovered_cell_dims[1] = {n_cell};\n    if (!TfLiteIntArrayEqualsArray(recovered_cell_weights->dims, 1,\n                                   recovered_cell_dims)) {\n      TfLiteIntArray* recovered_cell_weights_size = TfLiteIntArrayCreate(1);\n      recovered_cell_weights_size->data[0] = n_cell;\n      TF_LITE_ENSURE_OK(context,\n                        context->ResizeTensor(context, recovered_cell_weights,\n                                              recovered_cell_weights_size));\n    }\n\n    // Allocate a temporary tensor to store the accumulated int32 values.\n    node->temporaries->data[kAccumScratch] =\n        scratch_tensor_index + kAccumScratch;\n    TfLiteTensor* accum_scratch;\n    TF_LITE_ENSURE_OK(context, GetTemporarySafe(context, node, kAccumScratch,\n                                                &accum_scratch));\n    accum_scratch->type = kTfLiteInt32;\n    accum_scratch->allocation_type = kTfLiteArenaRw;\n    int accum_scratch_dims[2] = {n_cell, n_batch};\n    if (!TfLiteIntArrayEqualsArray(accum_scratch->dims, 2,\n                                   accum_scratch_dims)) {\n      TfLiteIntArray* accum_size = TfLiteIntArrayCreate(2);\n      accum_size->data[0] = n_cell;\n      accum_size->data[1] = n_batch;\n      TF_LITE_ENSURE_OK(\n          context, context->ResizeTensor(context, accum_scratch, accum_size));\n    }\n    node->temporaries->data[kInputZeroPoints] =\n        op_data->scratch_tensor_index + kInputZeroPoints;\n    TfLiteTensor* input_zp;\n    TF_LITE_ENSURE_OK(\n        context, GetTemporarySafe(context, node, kInputZeroPoints, &input_zp));\n    input_zp->type = kTfLiteFloat32;\n    input_zp->allocation_type = kTfLiteArenaRw;\n    if (!TfLiteIntArrayEqualsArray(input_zp->dims, 1, scaling_dims)) {\n      TfLiteIntArray* input_zp_size = TfLiteIntArrayCreate(1);\n      input_zp_size->data[0] = n_batch;\n      TF_LITE_ENSURE_OK(\n          context, context->ResizeTensor(context, input_zp, input_zp_size));\n    }\n    node->temporaries->data[kOutputStateZeroPoints] =\n        op_data->scratch_tensor_index + kOutputStateZeroPoints;\n    TfLiteTensor* output_state_zp;\n    TF_LITE_ENSURE_OK(context,\n                      GetTemporarySafe(context, node, kOutputStateZeroPoints,\n                                       &output_state_zp));\n    output_state_zp->type = kTfLiteFloat32;\n    output_state_zp->allocation_type = kTfLiteArenaRw;\n    if (!TfLiteIntArrayEqualsArray(output_state_zp->dims, 1, scaling_dims)) {\n      TfLiteIntArray* output_state_zp_size = TfLiteIntArrayCreate(1);\n      output_state_zp_size->data[0] = n_batch;\n      TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, output_state_zp,\n                                                       output_state_zp_size));\n    }\n    node->temporaries->data[kRowSums] = scratch_tensor_index + kRowSums;\n    TfLiteTensor* row_sums;\n    TF_LITE_ENSURE_OK(context,\n                      GetTemporarySafe(context, node, kRowSums, &row_sums));\n    row_sums->type = kTfLiteInt32;\n    row_sums->allocation_type = kTfLiteArenaRwPersistent;\n    int row_sums_rows = use_cifg ? 6 : 8;\n    const TfLiteTensor* projection_weights = GetOptionalInputTensor(\n        context, node, lstm::full::kProjectionWeightsTensor);\n    if (projection_weights != nullptr) {\n      row_sums_rows += ceil(static_cast<float>(n_output) / n_cell);\n    }\n    int row_sums_dims[2] = {row_sums_rows, n_cell};\n    if (!TfLiteIntArrayEqualsArray(row_sums->dims, 2, row_sums_dims)) {\n      TfLiteIntArray* row_sums_size = TfLiteIntArrayCreate(2);\n      row_sums_size->data[0] = row_sums_dims[0];\n      row_sums_size->data[1] = row_sums_dims[1];\n      TF_LITE_ENSURE_OK(\n          context, context->ResizeTensor(context, row_sums, row_sums_size));\n    }\n  }\n\n  if (is_integer) {\n    // Integer UnidirectionalSequenceLSTM prepare function for 8x8->16.\n    // This code path needs 5 intermediate tensors per Op.\n    // Populate quantization parameters.\n    PopulateQuantizedLstmParams8x8_16(context, node,\n                                      &op_data->integer_lstm_param);\n    // Allocate scratch buffer. Need 6 16bit buffer with size n_batch * n_cell\n    // and 1 8bit buffer with size n_batch * n_cell. We also need 1 32 bit\n    // buffer with size n_batch * n_cell.\n    //\n    // Handle cifg case as well, which might save one buffer.\n    for (int scratch_index = 0; scratch_index < 6; ++scratch_index) {\n      node->temporaries->data[scratch_index] =\n          op_data->scratch_tensor_index + scratch_index;\n      TfLiteTensor* scratch_tensor;\n      TF_LITE_ENSURE_OK(context, GetTemporarySafe(context, node, scratch_index,\n                                                  &scratch_tensor));\n\n      scratch_tensor->type = kTfLiteInt16;\n      if (scratch_index == 4) {\n        scratch_tensor->type = kTfLiteInt8;\n      } else if (scratch_index == 5) {\n        scratch_tensor->type = kTfLiteInt32;\n      }\n\n      scratch_tensor->allocation_type = kTfLiteArenaRw;\n      const int scratch_dimension[2] = {n_batch, n_cell};\n      if (!TfLiteIntArrayEqualsArray(scratch_tensor->dims, 2,\n                                     scratch_dimension)) {\n        TfLiteIntArray* scratch_buffer_size = TfLiteIntArrayCreate(2);\n        scratch_buffer_size->data[0] = n_batch;\n        scratch_buffer_size->data[1] = n_cell;\n        TF_LITE_ENSURE_OK(context,\n                          context->ResizeTensor(context, scratch_tensor,\n                                                scratch_buffer_size));\n      }\n    }\n\n    // Populate precomputed zp * weight.\n    TF_LITE_ENSURE_OK(context, PopulatePrecomputedZPTimesWeightsWithBias(\n                                   context, op_data, node));\n  }\n\n  return kTfLiteOk;\n}\n\nTfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n  const auto* params =\n      reinterpret_cast<TfLiteUnidirectionalSequenceLSTMParams*>(\n          node->builtin_data);\n  const OpData* op_data = reinterpret_cast<OpData*>(node->user_data);\n  const bool use_layer_norm = op_data->use_layer_norm;\n  const bool time_major = params->time_major;\n  const TfLiteTensor* input;\n  TF_LITE_ENSURE_OK(\n      context, GetInputSafe(context, node, lstm::full::kInputTensor, &input));\n\n  const TfLiteTensor* input_to_input_weights = GetOptionalInputTensor(\n      context, node, lstm::full::kInputToInputWeightsTensor);\n  const TfLiteTensor* input_to_forget_weights;\n  TF_LITE_ENSURE_OK(\n      context,\n      GetInputSafe(context, node, lstm::full::kInputToForgetWeightsTensor,\n                   &input_to_forget_weights));\n  const TfLiteTensor* input_to_cell_weights;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node,\n                                          lstm::full::kInputToCellWeightsTensor,\n                                          &input_to_cell_weights));\n  const TfLiteTensor* input_to_output_weights;\n  TF_LITE_ENSURE_OK(\n      context,\n      GetInputSafe(context, node, lstm::full::kInputToOutputWeightsTensor,\n                   &input_to_output_weights));\n\n  const TfLiteTensor* recurrent_to_input_weights = GetOptionalInputTensor(\n      context, node, lstm::full::kRecurrentToInputWeightsTensor);\n  const TfLiteTensor* recurrent_to_forget_weights;\n  TF_LITE_ENSURE_OK(\n      context,\n      GetInputSafe(context, node, lstm::full::kRecurrentToForgetWeightsTensor,\n                   &recurrent_to_forget_weights));\n  const TfLiteTensor* recurrent_to_cell_weights;\n  TF_LITE_ENSURE_OK(\n      context,\n      GetInputSafe(context, node, lstm::full::kRecurrentToCellWeightsTensor,\n                   &recurrent_to_cell_weights));\n  const TfLiteTensor* recurrent_to_output_weights;\n  TF_LITE_ENSURE_OK(\n      context,\n      GetInputSafe(context, node, lstm::full::kRecurrentToOutputWeightsTensor,\n                   &recurrent_to_output_weights));\n\n  const TfLiteTensor* cell_to_input_weights = GetOptionalInputTensor(\n      context, node, lstm::full::kCellToInputWeightsTensor);\n  const TfLiteTensor* cell_to_forget_weights = GetOptionalInputTensor(\n      context, node, lstm::full::kCellToForgetWeightsTensor);\n  const TfLiteTensor* cell_to_output_weights = GetOptionalInputTensor(\n      context, node, lstm::full::kCellToOutputWeightsTensor);\n\n  const TfLiteTensor* input_gate_bias =\n      GetOptionalInputTensor(context, node, lstm::full::kInputGateBiasTensor);\n  const TfLiteTensor* forget_gate_bias;\n  TF_LITE_ENSURE_OK(\n      context, GetInputSafe(context, node, lstm::full::kForgetGateBiasTensor,\n                            &forget_gate_bias));\n  const TfLiteTensor* cell_gate_bias;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, lstm::full::kCellGateBiasTensor,\n                                 &cell_gate_bias));\n  const TfLiteTensor* output_gate_bias;\n  TF_LITE_ENSURE_OK(\n      context, GetInputSafe(context, node, lstm::full::kOutputGateBiasTensor,\n                            &output_gate_bias));\n\n  const TfLiteTensor* projection_weights = GetOptionalInputTensor(\n      context, node, lstm::full::kProjectionWeightsTensor);\n  const TfLiteTensor* projection_bias =\n      GetOptionalInputTensor(context, node, lstm::full::kProjectionBiasTensor);\n\n  TfLiteTensor* output_state =\n      GetVariableInput(context, node, lstm::full::kOutputStateTensor);\n  TFLITE_DCHECK(output_state != nullptr);\n  TfLiteTensor* cell_state =\n      GetVariableInput(context, node, lstm::full::kCellStateTensor);\n  TFLITE_DCHECK(cell_state != nullptr);\n\n  const TfLiteTensor* input_layer_norm_coefficients =\n      use_layer_norm\n          ? GetOptionalInputTensor(\n                context, node, lstm::full::kInputLayerNormCoefficientsTensor)\n          : nullptr;\n  const TfLiteTensor* forget_layer_norm_coefficients =\n      use_layer_norm ? GetInput(context, node,\n                                lstm::full::kForgetLayerNormCoefficientsTensor)\n                     : nullptr;\n  const TfLiteTensor* cell_layer_norm_coefficients =\n      use_layer_norm ? GetInput(context, node,\n                                lstm::full::kCellLayerNormCoefficientsTensor)\n                     : nullptr;\n  const TfLiteTensor* output_layer_norm_coefficients =\n      use_layer_norm ? GetInput(context, node,\n                                lstm::full::kOutputLayerNormCoefficientsTensor)\n                     : nullptr;\n\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node,\n                                           lstm::full::kOutputTensor, &output));\n\n  // Copy out the LSTM specific params so they can be passed in the function.\n  TfLiteLSTMParams lstm_params;\n  lstm_params.activation = params->activation;\n  lstm_params.cell_clip = params->cell_clip;\n  lstm_params.proj_clip = params->proj_clip;\n  lstm_params.asymmetric_quantize_inputs = params->asymmetric_quantize_inputs;\n\n  switch (input_to_output_weights->type) {\n    case kTfLiteFloat32: {\n      // Index the scratch buffers pointers to the global scratch buffer.\n      TfLiteTensor* scratch_buffer;\n      TF_LITE_ENSURE_OK(context, GetTemporarySafe(context, node, kScratchBuffer,\n                                                  &scratch_buffer));\n      return lstm_eval::EvalFloat(\n          input, input_to_input_weights, input_to_forget_weights,\n          input_to_cell_weights, input_to_output_weights,\n          recurrent_to_input_weights, recurrent_to_forget_weights,\n          recurrent_to_cell_weights, recurrent_to_output_weights,\n          cell_to_input_weights, cell_to_forget_weights, cell_to_output_weights,\n          input_layer_norm_coefficients, forget_layer_norm_coefficients,\n          cell_layer_norm_coefficients, output_layer_norm_coefficients,\n          /*aux_input=*/nullptr,\n          /*aux_input_to_input_weights=*/nullptr,\n          /*aux_input_to_forget_weights=*/nullptr,\n          /*aux_input_to_cell_weights=*/nullptr,\n          /*aux_input_to_output_weights=*/nullptr, input_gate_bias,\n          forget_gate_bias, cell_gate_bias, output_gate_bias,\n          projection_weights, projection_bias, &lstm_params,\n          /*forward_sequence=*/true, time_major,\n          /*output_offset=*/0, scratch_buffer, output_state, cell_state,\n          output);\n    }\n    case kTfLiteUInt8:\n    case kTfLiteInt8: {\n      const bool is_hybrid = input->type == kTfLiteFloat32;\n      if (is_hybrid) {\n        // Index the scratch buffers pointers to the global scratch buffer.\n        TfLiteTensor* scratch_buffer;\n        TF_LITE_ENSURE_OK(\n            context,\n            GetTemporarySafe(context, node, kScratchBuffer, &scratch_buffer));\n\n        OpData* op_data = reinterpret_cast<OpData*>(node->user_data);\n        TfLiteTensor* row_sums;\n        TF_LITE_ENSURE_OK(context,\n                          GetTemporarySafe(context, node, kRowSums, &row_sums));\n        const int row_sums_size = row_sums->dims->data[0];\n        return lstm_eval::EvalHybrid(\n            input, input_to_input_weights,\n            /*input_to_input_weights_ledger*/ nullptr, input_to_forget_weights,\n            /*input_to_forget_weights_ledger*/ nullptr, input_to_cell_weights,\n            /*input_to_cell_weights_ledger*/ nullptr, input_to_output_weights,\n            /*input_to_output_weights_ledger*/ nullptr,\n            recurrent_to_input_weights,\n            /*recurrent_to_input_weights_ledger*/ nullptr,\n            recurrent_to_forget_weights,\n            /*recurrent_to_forget_weights_ledger*/ nullptr,\n            recurrent_to_cell_weights,\n            /*recurrent_to_cell_weights_ledger*/ nullptr,\n            recurrent_to_output_weights,\n            /*recurrent_to_output_weights_ledger*/ nullptr,\n            cell_to_input_weights, cell_to_forget_weights,\n            cell_to_output_weights, input_layer_norm_coefficients,\n            forget_layer_norm_coefficients, cell_layer_norm_coefficients,\n            output_layer_norm_coefficients,\n            /*aux_input=*/nullptr,\n            /*aux_input_to_input_weights=*/nullptr,\n            /*aux_input_to_forget_weights=*/nullptr,\n            /*aux_input_to_cell_weights=*/nullptr,\n            /*aux_input_to_output_weights=*/nullptr, input_gate_bias,\n            forget_gate_bias, cell_gate_bias, output_gate_bias,\n            projection_weights, /*projection_weights_ledger*/ nullptr,\n            projection_bias, &lstm_params,\n            /*forward_sequence=*/true, time_major,\n            /*output_offset=*/0, scratch_buffer,\n            GetTemporary(context, node, kInputScalingFactors),\n            /*aux_input_sf=*/nullptr,\n            GetTemporary(context, node, kOutputStateScalingFactors),\n            GetTemporary(context, node, kProductScalingFactors),\n            GetTemporary(context, node, kRecoveredCellWeights),\n            GetTemporary(context, node, kInputQuantized),\n            /*aux_input_quantized=*/nullptr,\n            GetTemporary(context, node, kOutputStateQuantized),\n            GetTemporary(context, node, kCellStateQuantized), output_state,\n            cell_state, GetTemporary(context, node, kAccumScratch), output,\n            GetTemporary(context, node, kInputZeroPoints),\n            /*aux_input_zp=*/nullptr,\n            GetTemporary(context, node, kOutputStateZeroPoints), row_sums,\n            row_sums_size, &op_data->compute_row_sums,\n            CpuBackendContext::GetFromContext(context));\n      } else {\n        TfLiteTensor* scratch0;\n        TF_LITE_ENSURE_OK(context,\n                          GetTemporarySafe(context, node, 0, &scratch0));\n        TfLiteTensor* scratch1;\n        TF_LITE_ENSURE_OK(context,\n                          GetTemporarySafe(context, node, 1, &scratch1));\n        TfLiteTensor* scratch2;\n        TF_LITE_ENSURE_OK(context,\n                          GetTemporarySafe(context, node, 2, &scratch2));\n        TfLiteTensor* scratch3;\n        TF_LITE_ENSURE_OK(context,\n                          GetTemporarySafe(context, node, 3, &scratch3));\n        TfLiteTensor* scratch4;\n        TF_LITE_ENSURE_OK(context,\n                          GetTemporarySafe(context, node, 4, &scratch4));\n        TfLiteTensor* scratch5;\n        TF_LITE_ENSURE_OK(context,\n                          GetTemporarySafe(context, node, 5, &scratch5));\n        return lstm_eval::EvalInteger8x8_16(\n            input, input_to_input_weights, input_to_forget_weights,\n            input_to_cell_weights, input_to_output_weights,\n            recurrent_to_input_weights, recurrent_to_forget_weights,\n            recurrent_to_cell_weights, recurrent_to_output_weights,\n            cell_to_input_weights, cell_to_forget_weights,\n            cell_to_output_weights, input_layer_norm_coefficients,\n            forget_layer_norm_coefficients, cell_layer_norm_coefficients,\n            output_layer_norm_coefficients, input_gate_bias, forget_gate_bias,\n            cell_gate_bias, output_gate_bias, projection_weights,\n            projection_bias, &lstm_params, /*forward_sequence=*/true,\n            time_major, &op_data->integer_lstm_param, output_state, cell_state,\n            output, scratch0, scratch1, scratch2, scratch3, scratch4, scratch5,\n            CpuBackendContext::GetFromContext(context));\n      }\n    }\n    default:\n      TF_LITE_KERNEL_LOG(context, \"Type %s is not currently supported.\",\n                         TfLiteTypeGetName(input_to_output_weights->type));\n      return kTfLiteError;\n  }\n}\n}  // namespace unidirectional_sequence_lstm\n\nTfLiteRegistration* Register_UNIDIRECTIONAL_SEQUENCE_LSTM() {\n  static TfLiteRegistration r = {unidirectional_sequence_lstm::Init,\n                                 unidirectional_sequence_lstm::Free,\n                                 unidirectional_sequence_lstm::Prepare,\n                                 unidirectional_sequence_lstm::Eval};\n  return &r;\n}\n\n}  // namespace builtin\n}  // namespace ops\n}  // namespace tflite\n"], "filenames": ["tensorflow/lite/kernels/unidirectional_sequence_lstm.cc"], "buggy_code_start_loc": [64], "buggy_code_end_loc": [762], "fixing_code_start_loc": [65], "fixing_code_end_loc": [772], "type": "CWE-908", "message": "TensorFlow is an end-to-end open source platform for machine learning. In affected versions all TFLite operations that use quantization can be made to use unitialized values. [For example](https://github.com/tensorflow/tensorflow/blob/460e000de3a83278fb00b61a16d161b1964f15f4/tensorflow/lite/kernels/depthwise_conv.cc#L198-L200). The issue stems from the fact that `quantization.params` is only valid if `quantization.type` is different that `kTfLiteNoQuantization`. However, these checks are missing in large parts of the code. We have patched the issue in GitHub commits 537bc7c723439b9194a358f64d871dd326c18887, 4a91f2069f7145aab6ba2d8cfe41be8a110c18a5 and 8933b8a21280696ab119b63263babdb54c298538. The fix will be included in TensorFlow 2.6.0. We will also cherrypick this commit on TensorFlow 2.5.1, TensorFlow 2.4.3, and TensorFlow 2.3.4, as these are also affected and still in supported range.", "other": {"cve": {"id": "CVE-2021-37682", "sourceIdentifier": "security-advisories@github.com", "published": "2021-08-12T23:15:08.390", "lastModified": "2021-08-19T13:43:43.123", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "TensorFlow is an end-to-end open source platform for machine learning. In affected versions all TFLite operations that use quantization can be made to use unitialized values. [For example](https://github.com/tensorflow/tensorflow/blob/460e000de3a83278fb00b61a16d161b1964f15f4/tensorflow/lite/kernels/depthwise_conv.cc#L198-L200). The issue stems from the fact that `quantization.params` is only valid if `quantization.type` is different that `kTfLiteNoQuantization`. However, these checks are missing in large parts of the code. We have patched the issue in GitHub commits 537bc7c723439b9194a358f64d871dd326c18887, 4a91f2069f7145aab6ba2d8cfe41be8a110c18a5 and 8933b8a21280696ab119b63263babdb54c298538. The fix will be included in TensorFlow 2.6.0. We will also cherrypick this commit on TensorFlow 2.5.1, TensorFlow 2.4.3, and TensorFlow 2.3.4, as these are also affected and still in supported range."}, {"lang": "es", "value": "TensorFlow es una plataforma de c\u00f3digo abierto de extremo a extremo para el aprendizaje autom\u00e1tico.&#xa0;En las versiones afectadas, todas las operaciones de TFLite que utilizan la cuantificaci\u00f3n se pueden hacer para utilizar valores unitarios.&#xa0;[Por ejemplo] (https://github.com/tensorflow/tensorflow/blob/460e000de3a83278fb00b61a16d161b1964f15f4/tensorflow/lite/kernels/depthwise_conv.cc#L198-L200).&#xa0;El problema surge del hecho que \"quantization.params\" solo es v\u00e1lido si \"quantization.type\" es diferente de \"kTfLiteNoQuantization\".&#xa0;Sin embargo, estas comprobaciones faltan en gran parte del c\u00f3digo.&#xa0;Hemos solucionado el problema en las commits de GitHub 537bc7c723439b9194a358f64d871dd326c18887, 4a91f2069f7145aab6ba2d8cfe41be8a110c18a5 y 8933b8a21280696ab119b63263babdb54c298538.&#xa0;La correcci\u00f3n ser\u00e1 incluida en TensorFlow versi\u00f3n 2.6.0.&#xa0;Tambi\u00e9n seleccionaremos este commit en TensorFlow versi\u00f3n 2.5.1, TensorFlow versi\u00f3n 2.4.3 y TensorFlow versi\u00f3n 2.3.4,"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.1, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.2}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:L/A:L", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "LOW", "availabilityImpact": "LOW", "baseScore": 4.4, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 2.5}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:N/I:P/A:P", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "PARTIAL", "availabilityImpact": "PARTIAL", "baseScore": 3.6}, "baseSeverity": "LOW", "exploitabilityScore": 3.9, "impactScore": 4.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "security-advisories@github.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-908"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.3.0", "versionEndExcluding": "2.3.4", "matchCriteriaId": "0F83C081-51CC-415F-A8C0-0A44C75E2CD6"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.4.0", "versionEndExcluding": "2.4.3", "matchCriteriaId": "BD3F2BF8-EBA9-42BF-8F9B-D918B880B15A"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.5.0:*:*:*:*:*:*:*", "matchCriteriaId": "D03E99A7-4E3D-427D-A156-C0713E9FB02A"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.6.0:rc0:*:*:*:*:*:*", "matchCriteriaId": "70FA6E48-6C57-40CA-809F-4E3D07CBF348"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.6.0:rc1:*:*:*:*:*:*", "matchCriteriaId": "42187561-E491-434D-828C-F36701446634"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.6.0:rc2:*:*:*:*:*:*", "matchCriteriaId": "C66B61C8-450A-4C5E-9174-F970D6DEE778"}]}]}], "references": [{"url": "https://github.com/tensorflow/tensorflow/commit/4a91f2069f7145aab6ba2d8cfe41be8a110c18a5", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/commit/537bc7c723439b9194a358f64d871dd326c18887", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/commit/8933b8a21280696ab119b63263babdb54c298538", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/security/advisories/GHSA-4c4g-crqm-xrxw", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/tensorflow/tensorflow/commit/4a91f2069f7145aab6ba2d8cfe41be8a110c18a5"}}