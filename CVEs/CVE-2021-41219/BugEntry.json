{"buggy_code": ["/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n// See docs in ../ops/math_ops.cc.\n\n#define EIGEN_USE_THREADS\n\n#include \"tensorflow/core/kernels/sparse_matmul_op.h\"\n\n#include <map>\n#include <memory>\n#include <vector>\n\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n#include \"tensorflow/core/common_runtime/device.h\"\n#include \"tensorflow/core/framework/bfloat16.h\"\n#include \"tensorflow/core/framework/op.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/types.h\"\n#include \"tensorflow/core/kernels/fill_functor.h\"\n#include \"tensorflow/core/lib/core/blocking_counter.h\"\n#include \"tensorflow/core/lib/core/threadpool.h\"\n#include \"tensorflow/core/platform/logging.h\"\n#include \"tensorflow/core/platform/macros.h\"\n#include \"tensorflow/core/platform/mutex.h\"\n#include \"tensorflow/core/platform/thread_annotations.h\"\n#include \"tensorflow/core/platform/types.h\"\n#ifdef TENSORFLOW_USE_LIBXSMM\n#include \"include/libxsmm_intrinsics_x86.h\"\n#include \"include/libxsmm_malloc.h\"\n#include \"include/libxsmm_spmdm.h\"\n#endif\n\n#if defined(TENSORFLOW_USE_CUSTOM_CONTRACTION_KERNEL)\n#include \"tensorflow/core/kernels/eigen_contraction_kernel.h\"\n#endif\n\n#define ALWAYS_INLINE EIGEN_ALWAYS_INLINE\n\nnamespace tensorflow {\nnamespace {\n\ntemplate <typename T>\nusing BasicMatrix = Eigen::Tensor<T, 2, Eigen::RowMajor>;\n\ntemplate <typename T>\nusing BasicMatrixMap =\n    Eigen::TensorMap<Eigen::Tensor<T, 2, Eigen::RowMajor>, Eigen::Aligned>;\n\nusing Matrix = BasicMatrix<float>;\nusing MatrixMap = BasicMatrixMap<float>;\nusing CPUDevice = Eigen::ThreadPoolDevice;\nusing DSizes = Eigen::DSizes<Eigen::DenseIndex, 2>;\n\n// Two commonly used static dsizes. We use Eigen::type2index to allow as much\n// compile time optimization as possible.\n#ifdef EIGEN_HAS_INDEX_LIST\ninline Eigen::IndexList<Eigen::type2index<0>, Eigen::type2index<0>>\ndsizes_00() {\n  return Eigen::IndexList<Eigen::type2index<0>, Eigen::type2index<0>>();\n}\ninline Eigen::IndexList<Eigen::type2index<1>, Eigen::type2index<0>>\ndsizes_10() {\n  return Eigen::IndexList<Eigen::type2index<1>, Eigen::type2index<0>>();\n}\n#else\ninline DSizes dsizes_00() { return DSizes(0, 0); }\ninline DSizes dsizes_10() { return DSizes(1, 0); }\n#endif\n\n// Blocksizes\n// TODO(agarwal): compute these sizes based on cache sizes.\nconst int K = 64;\nconst int M = 64;\nconst int N = 128;\n\n// This stores a sparse representation of a slice of a matrix with size\n// (num_rows, num_cols). The slice is represented as a series of blocks of size\n// (num_rows, b), where b = block_size for all but the last block, which may\n// have fewer columns.\n//\n// num_rows and block_size are assumed to be <= 256. This allows storing\n// different indices as uint8.\n//\n// For each block, we store all the non zero entries in data/data3 vector and\n// the corresponding coordinates of the element in index/index3 vectors. index3\n// vector stores index of 3 elements in the same row so that these elements can\n// share the same row coordinate. Each entry in Index3 corresponds to 3 entries\n// in data3.\n//\n// Note that all the data/indices of all the blocks are stored in the same\n// vectors respectively. To identify block boundaries, we store the block\n// offsets using index3_offset/index_offset. If there are n blocks in the slice,\n// index3_offset and index_offset have n entries. The indices for the ith block\n// are the values in the following range:\n// [index3[index3_offset[i-1]], index3[index3_offset[i]]). Similarly for\n// index_offset.\ntemplate <typename T>\nstruct SparseSlice {\n  using ConstMatrixMap = BasicMatrixMap<const T>;\n\n public:\n  // Indices of three elements on the same row.\n  struct Index3 {\n    uint8 m;  // row\n    // columns\n    uint8 k1;\n    uint8 k2;\n    uint8 k3;\n  };\n\n  // Index of one element.\n  struct Index {\n    uint8 m;\n    uint8 k;\n  };\n\n  SparseSlice(int nrows, int ncols, int bsize)\n      : num_rows(nrows), num_cols(ncols), block_size(bsize) {\n    DCHECK_LE(nrows, 256);\n    DCHECK_LE(block_size, 256);\n  }\n\n  // Initializes the slice with data starting at mat(0, col_offset) and with\n  // size (num_rows, num_cols).\n  // If Transpose is true, implicitly transposes mat.\n  template <bool Transpose = false>\n  void Initialize(const ConstMatrixMap& mat, int col_offset);\n\n  void Clear();\n\n  // See comments above.\n  std::vector<int> index3_offset;\n  std::vector<Index3> index3;\n  std::vector<T> data3;\n\n  // See comments above. Similar to \"index3\" except that each element in \"index\"\n  // corresponds to one element in data.\n  std::vector<int> index_offset;\n  std::vector<Index> index;\n  std::vector<T> data;\n\n  // Number of rows and columns for the slice.\n  const int num_rows;\n  const int num_cols;\n\n  // Block size used to initialize from a matrix.\n  const int block_size;\n};\n\ntemplate <typename T>\nbool IsZero(T v);\n\ntemplate <>\nALWAYS_INLINE bool IsZero(bfloat16 v) {\n  return !static_cast<bool>(v);\n}\n\ntemplate <>\nALWAYS_INLINE bool IsZero(float v) {\n  return v == 0.0f;\n}\n\ntemplate <typename T>\ntemplate <bool Transpose>\nvoid SparseSlice<T>::Initialize(\n    const typename SparseSlice<T>::ConstMatrixMap& mat, int col_offset) {\n  const int mat_rows = Transpose ? mat.dimension(1) : mat.dimension(0);\n  const int mat_cols = Transpose ? mat.dimension(0) : mat.dimension(1);\n  DCHECK_LE(num_rows, mat_rows);\n  DCHECK_LE(num_cols + col_offset, mat_cols);\n\n  int num_blocks = (num_cols + block_size - 1) / block_size;\n  int mat_size = num_rows * num_cols;\n\n  index3_offset.reserve(num_blocks);\n  data3.reserve(mat_size);\n  index3.reserve(mat_size / 3);\n\n  index_offset.reserve(num_blocks);\n  data.reserve(num_blocks * num_rows * 2);\n  index.reserve(num_blocks * num_rows * 2);\n\n  Index3 idx3;\n  const int stride = Transpose ? mat.dimension(1) : 1;\n\n  for (int i = 0; i < num_blocks; ++i) {\n    int num_block_cols = std::min(block_size, num_cols - block_size * i);\n    for (int row = 0; row < num_rows; ++row) {\n      idx3.m = static_cast<uint8>(row);\n      // Safety note: The following code has a race, since it checks whether\n      // *curr is nonzero and then reads it again on use.  However, the result\n      // of the race is only that some of the \"nonzeros\" in the resulting sparse\n      // representation may actually be zero, which is harmless.\n      const auto* start =\n          Transpose ? &mat(col_offset, row) : &mat(row, col_offset);\n      const auto* curr = start;\n      const auto* end = start + stride * num_block_cols;\n      uint8 k = 0;\n#define NEXT_ELEM \\\n  curr += stride; \\\n  ++k;\n#define EAT_ZEROS                          \\\n  while (curr < end && IsZero<T>(*curr)) { \\\n    NEXT_ELEM;                             \\\n  }\n      while (true) {\n        EAT_ZEROS\n        if (curr >= end) break;\n        idx3.k1 = k;\n        const T value1 = *curr;\n        NEXT_ELEM;\n\n        EAT_ZEROS\n        if (curr >= end) {\n          data.push_back(value1);\n          index.push_back({idx3.m, idx3.k1});\n          break;\n        }\n        idx3.k2 = k;\n        const T value2 = *curr;\n        NEXT_ELEM;\n\n        EAT_ZEROS\n        if (curr >= end) {\n          data.push_back(value2);\n          index.push_back({idx3.m, idx3.k2});\n          data.push_back(value1);\n          index.push_back({idx3.m, idx3.k1});\n          break;\n        }\n        idx3.k3 = k;\n        data3.push_back(value1);\n        data3.push_back(value2);\n        data3.push_back(*curr);\n        NEXT_ELEM;\n        index3.push_back(idx3);\n#undef NEXT_ELEM\n#undef EAT_ZEROS\n      }\n    }\n    col_offset += block_size;\n    index3_offset.push_back(index3.size());\n    index_offset.push_back(index.size());\n  }\n  DCHECK_EQ(index3_offset.size(), num_blocks);\n  DCHECK_EQ(index_offset.size(), num_blocks);\n  DCHECK_EQ(3 * index3.size(), data3.size());\n  DCHECK_EQ(index.size(), data.size());\n}\n\ntemplate <typename T>\nvoid SparseSlice<T>::Clear() {\n  index3_offset.clear();\n  index3.clear();\n  data3.clear();\n  index_offset.clear();\n  index.clear();\n  data.clear();\n}\n\nusing Packet = Eigen::internal::packet_traits<float>::type;\nconst int kNumOperands = (sizeof(Packet) / sizeof(float));\n#define LOAD(x) Eigen::internal::pload<Packet>(x);\n#define EXPAND_BFLOAT_L(x, y) \\\n  const auto y = Eigen::internal::pexpand_bf16_l<Packet>(x);\n#define EXPAND_BFLOAT_U(x, y) \\\n  const auto y = Eigen::internal::pexpand_bf16_u<Packet>(x);\n#define STORE(x, y) Eigen::internal::pstore<float>(x, y);\n#define FMA(a, b, c, d) d = Eigen::internal::pmadd<Packet>(a, b, c);\n\nALWAYS_INLINE float ConvertBfloat16ToFloat(const bfloat16* src) {\n  float out = 0;\n  auto tmp = reinterpret_cast<bfloat16*>(&out);\n#if __BYTE_ORDER__ == __ORDER_BIG_ENDIAN__\n  tmp[0] = *src;\n#else\n  tmp[1] = *src;\n#endif\n  return out;\n}\n\nALWAYS_INLINE Packet ConvertFourBfloat16ToFloat(const bfloat16* src) {\n  return Eigen::internal::pload4bf16<Packet>(\n      reinterpret_cast<const float*>(src));\n}\n\nALWAYS_INLINE Packet ConvertTwoBfloat16ToFloat(const bfloat16* src) {\n  return Eigen::internal::pload2bf16<Packet>(\n      reinterpret_cast<const float*>(src));\n}\n\nALWAYS_INLINE void ScalarMulAdd(const float a, const float** inp, float** out) {\n  **out += a * **inp;\n  ++*inp;\n  ++*out;\n}\n\nALWAYS_INLINE void ScalarMulAdd(const float a, const bfloat16** inp,\n                                float** out) {\n  float inp_f = ConvertBfloat16ToFloat(*inp);\n  **out += a * inp_f;\n  ++*inp;\n  ++*out;\n}\nALWAYS_INLINE void ScalarMulAdd3Way(const float a1, const float a2,\n                                    const float a3, const bfloat16** inp1,\n                                    const bfloat16** inp2,\n                                    const bfloat16** inp3, float** out) {\n  float inp1_f = ConvertBfloat16ToFloat(*inp1);\n  float inp2_f = ConvertBfloat16ToFloat(*inp2);\n  float inp3_f = ConvertBfloat16ToFloat(*inp3);\n  **out += a1 * inp1_f + a2 * inp2_f + a3 * inp3_f;\n  ++*out;\n  ++*inp1;\n  ++*inp2;\n  ++*inp3;\n}\n\nALWAYS_INLINE void ScalarMulAdd3Way(const float a1, const float a2,\n                                    const float a3, const float** inp1,\n                                    const float** inp2, const float** inp3,\n                                    float** out) {\n  **out += a1 * **inp1 + a2 * **inp2 + a3 * **inp3;\n  ++*out;\n  ++*inp1;\n  ++*inp2;\n  ++*inp3;\n}\n\nALWAYS_INLINE void LoadSingleScalar(const bfloat16** data, Packet* l) {\n  auto tmp = ConvertBfloat16ToFloat(*data);\n  *l = Eigen::internal::pset1<Packet>(tmp);\n  ++*data;\n}\n\nALWAYS_INLINE void LoadTwoScalars(const bfloat16** data, Packet* l1,\n                                  Packet* l2) {\n  if (kNumOperands >= 2) {\n    auto tmp = ConvertTwoBfloat16ToFloat(*data);\n    *l1 = Eigen::internal::pbroadcast_first<Packet>(tmp);\n    *l2 = Eigen::internal::pbroadcast_second<Packet>(tmp);\n    *data += 2;\n  } else {\n    LoadSingleScalar(data, l1);\n    LoadSingleScalar(data, l2);\n  }\n}\n\nALWAYS_INLINE void LoadFourScalars(const bfloat16** data, Packet* l1,\n                                   Packet* l2, Packet* l3, Packet* l4) {\n  if (kNumOperands >= 4) {\n    auto tmp = ConvertFourBfloat16ToFloat(*data);\n    *l1 = Eigen::internal::pbroadcast_first<Packet>(tmp);\n    *l2 = Eigen::internal::pbroadcast_second<Packet>(tmp);\n    *l3 = Eigen::internal::pbroadcast_third<Packet>(tmp);\n    *l4 = Eigen::internal::pbroadcast_fourth<Packet>(tmp);\n    *data += 4;\n  } else {\n    LoadTwoScalars(data, l1, l2);\n    LoadTwoScalars(data, l3, l4);\n  }\n}\n\nALWAYS_INLINE void LoadSingleScalar(const float** data, Packet* l) {\n  *l = Eigen::internal::pload1<Packet>(*data);\n  ++(*data);\n}\n\nALWAYS_INLINE void LoadTwoScalars(const float** data, Packet* l1, Packet* l2) {\n  LoadSingleScalar(data, l1);\n  LoadSingleScalar(data, l2);\n}\n\nALWAYS_INLINE void LoadFourScalars(const float** data, Packet* l1, Packet* l2,\n                                   Packet* l3, Packet* l4) {\n  LoadTwoScalars(data, l1, l2);\n  LoadTwoScalars(data, l3, l4);\n}\n\ntemplate <typename T>\nALWAYS_INLINE void LoadThreeScalars(const T** data, Packet* l1, Packet* l2,\n                                    Packet* l3) {\n  LoadTwoScalars(data, l1, l2);\n  LoadSingleScalar(data, l3);\n}\n\ntemplate <typename T>\nALWAYS_INLINE void LoadSixScalars(const T** data, Packet* l1, Packet* l2,\n                                  Packet* l3, Packet* l4, Packet* l5,\n                                  Packet* l6) {\n  LoadFourScalars(data, l1, l2, l3, l4);\n  LoadTwoScalars(data, l5, l6);\n}\n\n// Vectorized version of ScalarMulAdd.\nALWAYS_INLINE void MulAdd(const Packet a, const bfloat16** binp, float** out) {\n  auto inp = reinterpret_cast<const float*>(*binp);\n  const auto b = LOAD(inp);\n  EXPAND_BFLOAT_L(b, b_0);\n  EXPAND_BFLOAT_U(b, b_1);\n  *binp += 2 * kNumOperands;\n  auto c1 = LOAD(*out);\n  auto c2 = LOAD(*out + kNumOperands);\n  FMA(a, b_0, c1, c1);\n  FMA(a, b_1, c2, c2);\n  STORE(*out, c1);\n  STORE(*out + kNumOperands, c2);\n  *out += 2 * kNumOperands;\n}\n\n// Vectorized version of ScalarMulAdd3Way.\nALWAYS_INLINE void MulAdd3Way(const Packet a1, const Packet a2, const Packet a3,\n                              const bfloat16** binp1, const bfloat16** binp2,\n                              const bfloat16** binp3, float** out) {\n  auto inp1 = reinterpret_cast<const float*>(*binp1);\n  auto inp2 = reinterpret_cast<const float*>(*binp2);\n  auto inp3 = reinterpret_cast<const float*>(*binp3);\n  auto c1 = LOAD(*out);\n  auto c2 = LOAD(*out + kNumOperands);\n  const auto b1 = LOAD(inp1);\n  EXPAND_BFLOAT_L(b1, b1_0);\n  EXPAND_BFLOAT_U(b1, b1_1);\n  *binp1 += 2 * kNumOperands;\n  const auto b2 = LOAD(inp2);\n  EXPAND_BFLOAT_L(b2, b2_0);\n  EXPAND_BFLOAT_U(b2, b2_1);\n  *binp2 += 2 * kNumOperands;\n  const auto b3 = LOAD(inp3);\n  EXPAND_BFLOAT_L(b3, b3_0);\n  EXPAND_BFLOAT_U(b3, b3_1);\n  *binp3 += 2 * kNumOperands;\n  FMA(a1, b1_0, c1, c1);\n  FMA(a1, b1_1, c2, c2);\n  FMA(a2, b2_0, c1, c1);\n  FMA(a2, b2_1, c2, c2);\n  FMA(a3, b3_0, c1, c1);\n  FMA(a3, b3_1, c2, c2);\n  STORE(*out, c1);\n  STORE(*out + kNumOperands, c2);\n  *out += 2 * kNumOperands;\n}\n\n// Unroll MulAdd3Way for two iterations\nALWAYS_INLINE void TwoMulAdd3Way(const Packet a1, const Packet a2,\n                                 const Packet a3, const bfloat16** binp1,\n                                 const bfloat16** binp2, const bfloat16** binp3,\n                                 float** out) {\n  auto inp1 = reinterpret_cast<const float*>(*binp1);\n  auto inp2 = reinterpret_cast<const float*>(*binp2);\n  auto inp3 = reinterpret_cast<const float*>(*binp3);\n  auto c1 = LOAD(*out);\n  auto c2 = LOAD(*out + kNumOperands);\n  const auto b1 = LOAD(inp1);\n  const auto b2 = LOAD(inp2);\n  const auto b3 = LOAD(inp3);\n\n  EXPAND_BFLOAT_L(b1, b1_0);\n  EXPAND_BFLOAT_U(b1, b1_1);\n  EXPAND_BFLOAT_L(b2, b2_0);\n  EXPAND_BFLOAT_U(b2, b2_1);\n  EXPAND_BFLOAT_L(b3, b3_0);\n  EXPAND_BFLOAT_U(b3, b3_1);\n  auto c3 = LOAD(*out + 2 * kNumOperands);\n  auto c4 = LOAD(*out + 3 * kNumOperands);\n  const auto b4 = LOAD(inp1 + kNumOperands);\n  const auto b5 = LOAD(inp2 + kNumOperands);\n  const auto b6 = LOAD(inp3 + kNumOperands);\n\n  EXPAND_BFLOAT_L(b4, b4_0);\n  EXPAND_BFLOAT_U(b4, b4_1);\n  EXPAND_BFLOAT_L(b5, b5_0);\n  EXPAND_BFLOAT_U(b5, b5_1);\n  EXPAND_BFLOAT_L(b6, b6_0);\n  EXPAND_BFLOAT_U(b6, b6_1);\n\n  FMA(a1, b1_0, c1, c1);\n  FMA(a1, b1_1, c2, c2);\n  FMA(a1, b4_0, c3, c3);\n  FMA(a1, b4_1, c4, c4);\n  FMA(a2, b2_0, c1, c1);\n  FMA(a2, b2_1, c2, c2);\n  FMA(a2, b5_0, c3, c3);\n  FMA(a2, b5_1, c4, c4);\n  FMA(a3, b3_0, c1, c1);\n  FMA(a3, b3_1, c2, c2);\n  FMA(a3, b6_0, c3, c3);\n  FMA(a3, b6_1, c4, c4);\n  STORE(*out, c1);\n  STORE(*out + kNumOperands, c2);\n  STORE(*out + 2 * kNumOperands, c3);\n  STORE(*out + 3 * kNumOperands, c4);\n  *out += 4 * kNumOperands;\n  *binp1 += 4 * kNumOperands;\n  *binp2 += 4 * kNumOperands;\n  *binp3 += 4 * kNumOperands;\n}\n\n// Apply MulAdd3Way on 128 operands.\nALWAYS_INLINE void MulAdd3Way128(const Packet a1, const Packet a2,\n                                 const Packet a3, const bfloat16** inp1,\n                                 const bfloat16** inp2, const bfloat16** inp3,\n                                 float** out) {\n  for (int k = 0; k < 128 / (8 * kNumOperands); ++k) {\n    TwoMulAdd3Way(a1, a2, a3, inp1, inp2, inp3, out);\n    TwoMulAdd3Way(a1, a2, a3, inp1, inp2, inp3, out);\n  }\n}\n\n// Vectorized version of ScalarMulAdd\nALWAYS_INLINE void MulAdd(const Packet a, const float** inp, float** out) {\n  const auto b = LOAD(*inp);\n  *inp += kNumOperands;\n  auto c = LOAD(*out);\n  FMA(a, b, c, c);\n  STORE(*out, c);\n  *out += kNumOperands;\n}\n\n// Vectorized version of ScalarMulAdd3Way\nALWAYS_INLINE void MulAdd3Way(const Packet a1, const Packet a2, const Packet a3,\n                              const float** inp1, const float** inp2,\n                              const float** inp3, float** out) {\n  auto c = LOAD(*out);\n  const auto b1 = LOAD(*inp1);\n  *inp1 += kNumOperands;\n  const auto b2 = LOAD(*inp2);\n  *inp2 += kNumOperands;\n  const auto b3 = LOAD(*inp3);\n  *inp3 += kNumOperands;\n  FMA(a1, b1, c, c);\n  FMA(a2, b2, c, c);\n  FMA(a3, b3, c, c);\n  STORE(*out, c);\n  *out += kNumOperands;\n}\n\n// Unroll MulAdd3Way for two iterations\nALWAYS_INLINE void TwoMulAdd3Way(const Packet a1, const Packet a2,\n                                 const Packet a3, const float** inp1,\n                                 const float** inp2, const float** inp3,\n                                 float** out) {\n  auto c1 = LOAD(*out);\n  const auto b1 = LOAD(*inp1);\n  const auto b2 = LOAD(*inp2);\n  const auto b3 = LOAD(*inp3);\n\n  auto c2 = LOAD(*out + kNumOperands);\n  const auto b4 = LOAD(*inp1 + kNumOperands);\n  const auto b5 = LOAD(*inp2 + kNumOperands);\n  const auto b6 = LOAD(*inp3 + kNumOperands);\n\n  FMA(a1, b1, c1, c1);\n  FMA(a1, b4, c2, c2);\n  FMA(a2, b2, c1, c1);\n  FMA(a2, b5, c2, c2);\n  FMA(a3, b3, c1, c1);\n  FMA(a3, b6, c2, c2);\n  STORE(*out, c1);\n  STORE(*out + kNumOperands, c2);\n  *out += 2 * kNumOperands;\n  *inp1 += 2 * kNumOperands;\n  *inp2 += 2 * kNumOperands;\n  *inp3 += 2 * kNumOperands;\n}\n\n// Unroll MulAdd3Way for four iterations\nALWAYS_INLINE void FourMulAdd3Way(const Packet a1, const Packet a2,\n                                  const Packet a3, const float** inp1,\n                                  const float** inp2, const float** inp3,\n                                  float** out) {\n  TwoMulAdd3Way(a1, a2, a3, inp1, inp2, inp3, out);\n  TwoMulAdd3Way(a1, a2, a3, inp1, inp2, inp3, out);\n}\n\n// Apply MulAdd3Way on 128 operands.\nALWAYS_INLINE void MulAdd3Way128(const Packet a1, const Packet a2,\n                                 const Packet a3, const float** inp1,\n                                 const float** inp2, const float** inp3,\n                                 float** out) {\n  if (kNumOperands == 8) {\n    FourMulAdd3Way(a1, a2, a3, inp1, inp2, inp3, out);\n    FourMulAdd3Way(a1, a2, a3, inp1, inp2, inp3, out);\n    FourMulAdd3Way(a1, a2, a3, inp1, inp2, inp3, out);\n    FourMulAdd3Way(a1, a2, a3, inp1, inp2, inp3, out);\n  } else {\n    DCHECK_LE(4 * kNumOperands, 128);\n    for (int i = 0; i < 128 / (4 * kNumOperands); ++i) {\n      MulAdd3Way(a1, a2, a3, inp1, inp2, inp3, out);\n      MulAdd3Way(a1, a2, a3, inp1, inp2, inp3, out);\n      MulAdd3Way(a1, a2, a3, inp1, inp2, inp3, out);\n      MulAdd3Way(a1, a2, a3, inp1, inp2, inp3, out);\n    }\n  }\n}\n// Computes product of \"left_slices\" with \"num_cols\" columns of \"right\", and\n// stores the output in *\"output\".\n// Note that left_slices is a list of SparseSlices, which are conceptually\n// assumed to be concatenated along the column dimension. Also each SparseSlice\n// is encoded as a list of blocks with upto N columns. See SparseSlice for more\n// details.\ntemplate <typename TL, typename TR, int Cols>\ninline void GEPP(\n    const std::vector<SparseSlice<TL>*>& left_slices,\n    const Eigen::TensorMap<Eigen::Tensor<const TR, 2, Eigen::RowMajor>,\n                           Eigen::Aligned>& right,\n    const int num_cols, Matrix* output) {\n  const int cols = (Cols == -1) ? num_cols : Cols;\n  DCHECK_EQ(num_cols, cols);\n  const int right_num_cols = right.dimension(1);\n  const int output_num_cols = output->dimension(1);\n  static const int kNumOperandsR = kNumOperands * sizeof(float) / sizeof(TR);\n  const int cols_mod = cols % kNumOperandsR;\n  int k_offset = 0;\n  // Pre-compute pointers for output matrix.\n  float* out_ptrs[M];\n  float* const out_start = &(*output)(0, 0);\n  for (int j = 0; j < M; ++j) {\n    out_ptrs[j] = out_start + output_num_cols * j;\n  }\n  for (const auto* left_slice : left_slices) {\n    const auto& left = *left_slice;\n    const auto* data3 = (!left.data3.empty()) ? &left.data3[0] : nullptr;\n    const auto* data = (!left.data.empty()) ? &left.data[0] : nullptr;\n    const int num_blocks = left.index3_offset.size();\n    int begin3 = 0;\n    int begin = 0;\n    for (int i = 0; i < num_blocks; ++i) {\n      // Pre-compute pointers for right matrix\n      const TR* right_ptrs[K];\n      const auto* const right_start = &right(k_offset, 0);\n      DCHECK_LT(k_offset, right.dimension(0));\n      for (int j = 0; j < K; ++j) {\n        right_ptrs[j] = right_start + right_num_cols * j;\n      }\n\n      const int end3 = left.index3_offset[i];\n      int j = begin3;\n      // Loop unrolled for 2 iterations.\n      for (; j + 1 < end3; j += 2) {\n        Packet l1, l2, l3, nl1, nl2, nl3;\n        LoadSixScalars(&data3, &l1, &l2, &l3, &nl1, &nl2, &nl3);\n        const auto& index = left.index3[j];\n        const auto& nindex = left.index3[j + 1];\n        float* out = out_ptrs[index.m];\n        float* nout = out_ptrs[nindex.m];\n        const auto* r1 = right_ptrs[index.k1];\n        const auto* r2 = right_ptrs[index.k2];\n        const auto* r3 = right_ptrs[index.k3];\n\n        const auto* nr1 = right_ptrs[nindex.k1];\n        const auto* nr2 = right_ptrs[nindex.k2];\n        const auto* nr3 = right_ptrs[nindex.k3];\n        if (cols == 128) {\n          MulAdd3Way128(l1, l2, l3, &r1, &r2, &r3, &out);\n          MulAdd3Way128(nl1, nl2, nl3, &nr1, &nr2, &nr3, &nout);\n        } else {\n          for (int n = 0; n < cols / kNumOperandsR; ++n) {\n            MulAdd3Way(l1, l2, l3, &r1, &r2, &r3, &out);\n            MulAdd3Way(nl1, nl2, nl3, &nr1, &nr2, &nr3, &nout);\n          }\n\n          const float sl1 = Eigen::internal::pfirst<Packet>(l1);\n          const float sl2 = Eigen::internal::pfirst<Packet>(l2);\n          const float sl3 = Eigen::internal::pfirst<Packet>(l3);\n          const float nsl1 = Eigen::internal::pfirst<Packet>(nl1);\n          const float nsl2 = Eigen::internal::pfirst<Packet>(nl2);\n          const float nsl3 = Eigen::internal::pfirst<Packet>(nl3);\n          for (int k = 0; k < cols_mod; ++k) {\n            ScalarMulAdd3Way(sl1, sl2, sl3, &r1, &r2, &r3, &out);\n            ScalarMulAdd3Way(nsl1, nsl2, nsl3, &nr1, &nr2, &nr3, &nout);\n          }\n        }\n      }\n      if (j < end3) {\n        Packet l1, l2, l3;\n        LoadThreeScalars(&data3, &l1, &l2, &l3);\n\n        const auto& index = left.index3[j];\n        float* out = out_ptrs[index.m];\n        const auto* r1 = right_ptrs[index.k1];\n        const auto* r2 = right_ptrs[index.k2];\n        const auto* r3 = right_ptrs[index.k3];\n        if (cols == 128) {\n          MulAdd3Way128(l1, l2, l3, &r1, &r2, &r3, &out);\n        } else {\n          for (int n = 0; n < cols / kNumOperandsR; ++n) {\n            MulAdd3Way(l1, l2, l3, &r1, &r2, &r3, &out);\n          }\n          const float sl1 = Eigen::internal::pfirst<Packet>(l1);\n          const float sl2 = Eigen::internal::pfirst<Packet>(l2);\n          const float sl3 = Eigen::internal::pfirst<Packet>(l3);\n          for (int k = 0; k < cols_mod; ++k) {\n            ScalarMulAdd3Way(sl1, sl2, sl3, &r1, &r2, &r3, &out);\n          }\n        }\n      }\n      begin3 = end3;\n      int end = left.index_offset[i];\n      // Loop unrolled for 4 iterations.\n      j = begin;\n      for (; j + 3 < end; j += 4) {\n        Packet l, nl, n2l, n3l;\n        LoadFourScalars(&data, &l, &nl, &n2l, &n3l);\n\n        const auto& index = left.index[j];\n        const auto& nindex = left.index[j + 1];\n        const auto& n2index = left.index[j + 2];\n        const auto& n3index = left.index[j + 3];\n        const auto* r = right_ptrs[index.k];\n        const auto* nr = right_ptrs[nindex.k];\n        const auto* n2r = right_ptrs[n2index.k];\n        const auto* n3r = right_ptrs[n3index.k];\n        float* out = out_ptrs[index.m];\n        float* nout = out_ptrs[nindex.m];\n        float* n2out = out_ptrs[n2index.m];\n        float* n3out = out_ptrs[n3index.m];\n\n        for (int n = 0; n < cols / kNumOperandsR; ++n) {\n          MulAdd(l, &r, &out);\n          MulAdd(nl, &nr, &nout);\n          MulAdd(n2l, &n2r, &n2out);\n          MulAdd(n3l, &n3r, &n3out);\n        }\n\n        const float sl1 = Eigen::internal::pfirst<Packet>(l);\n        const float sl2 = Eigen::internal::pfirst<Packet>(nl);\n        const float sl3 = Eigen::internal::pfirst<Packet>(n2l);\n        const float sl4 = Eigen::internal::pfirst<Packet>(n3l);\n        for (int k = 0; k < cols_mod; ++k) {\n          ScalarMulAdd(sl1, &r, &out);\n          ScalarMulAdd(sl2, &nr, &nout);\n          ScalarMulAdd(sl3, &n2r, &n2out);\n          ScalarMulAdd(sl4, &n3r, &n3out);\n        }\n      }\n      while (j < end) {\n        Packet l;\n        LoadSingleScalar(&data, &l);\n        const auto& index = left.index[j];\n        const auto* r = right_ptrs[index.k];\n        float* out = out_ptrs[index.m];\n        for (int n = 0; n < cols / kNumOperandsR; ++n) {\n          MulAdd(l, &r, &out);\n        }\n        const float sl = Eigen::internal::pfirst<Packet>(l);\n        for (int k = 0; k < cols_mod; ++k) {\n          ScalarMulAdd(sl, &r, &out);\n        }\n        j++;\n      }\n      k_offset += left.block_size;\n      begin = end;\n    }\n  }\n}\n\n#undef LOAD\n#undef EXPAND_BFLOAT_L\n#undef EXPAND_BFLOAT_U\n#undef STORE\n#undef FMA\n\n}  // namespace\n\ntemplate <typename TL, typename TR>\nclass SparseMatMul {\n  using MatrixL = BasicMatrix<TL>;\n  using MatrixR = BasicMatrix<TR>;\n  using ConstMatrixMapL = BasicMatrixMap<const TL>;\n  using ConstMatrixMapR = BasicMatrixMap<const TR>;\n  using MatrixMapR = BasicMatrixMap<TR>;\n\n public:\n  // Not used; added to match interface of LibxsmmSparseMatMul\n  struct TensorInfoCache {};\n\n  // Perform matrix multiplication of \"left\" and \"right\", and store the result\n  // in *\"output\".\n public:\n  static inline void Compute(TensorInfoCache* cache,\n                             const ConstMatrixMapL& left,\n                             const ConstMatrixMapR& right, bool transpose_left,\n                             const DeviceBase::CpuWorkerThreads* thread_pool,\n                             bool transpose_output, MatrixMap* output);\n\n private:\n  // Computes multiplication of left and num_cols columns of right, and stores\n  // the output block in *\"output\" at offsets \"output_row_offset\" and\n  // \"output_col_offset\". If assign is true, assigns the value to that block,\n  // else adds the values to the existing values.\n  static inline void ComputeOutputBlock(\n      const std::vector<SparseSlice<TL>*>& left, const ConstMatrixMapR& right,\n      int num_cols, int output_row_offset, int output_col_offset, bool assign,\n      bool transpose_output, MatrixMap* output);\n\n  // Encodes \"mat\" using a sparse representation and stores that in\n  // \"mat_slices\". \"mat\" is broken into a grid with sizes \"slice_num_rows\" and\n  // \"slice_num_cols\", each grid element is converted into a SparseSlice and\n  // stored in mat_slices. \"slice_block_size\" is used to perform further column\n  // blocking of each slice.\n  static inline std::unique_ptr<BlockingCounter> CreateSparseSlices(\n      const ConstMatrixMapL& mat, bool transpose, int slice_num_rows,\n      int slice_block_size, int slice_num_cols,\n      std::vector<std::vector<SparseSlice<TL>*>>* mat_slices,\n      const DeviceBase::CpuWorkerThreads* thread_pool);\n\n  // This function chops \"mat\" along column dimension into pieces with at most N\n  // columns, and concatenates the pieces one after the other in \"buffer\". It\n  // returns the list of the pieces in \"slices\". It returns a BlockingCounter\n  // which should be used to wait for the shuffle operations to complete.\n  static inline std::unique_ptr<BlockingCounter> CreateDenseSlices(\n      const ConstMatrixMapR& mat, int row_start, int num_rows, int col_start,\n      int num_cols, const DeviceBase::CpuWorkerThreads* thread_pool,\n      MatrixR* buffer, std::vector<ConstMatrixMapR*>* slices);\n\n  // Helper function for CreateDenseSlices to move the data around. It returns a\n  // BlockingCounter which should be used to wait for the shuffle operations to\n  // complete.\n  static inline BlockingCounter* ShuffleMatrix(\n      const ConstMatrixMapR& mat, int slice_row_start, int slice_num_rows,\n      int slice_col_start, int slice_num_cols, const int N,\n      const DeviceBase::CpuWorkerThreads* thread_pool, MatrixR* buffer);\n\n  // Helper function for CreateDenseSlices to create slices.\n  static inline void SliceMatrix(const MatrixR& mat, const int num_rows,\n                                 const int num_slices,\n                                 std::vector<ConstMatrixMapR*>* slices);\n\n  // Heuristics to compute various block sizes.\n  // KR, NR: block sizes for \"right\". We run blocking iterations that operate on\n  // matrices with at most this size.\n  // KL: grid size along the column dimension used while encoding left.\n  // IB, JB: number of left and right slices to multiply together. This is used\n  // for ordering different ComputeBlockOutput operations inside each blocking\n  // iteration so as to potentially reduce the working set size.\n  static inline void ComputeBlockSizes(const ConstMatrixMapL& left,\n                                       const ConstMatrixMapR& right,\n                                       bool transpose_left, int num_threads,\n                                       int* KR, int* NR, int* KL, int* JB,\n                                       int* IB);\n\n  TF_DISALLOW_COPY_AND_ASSIGN(SparseMatMul);\n};\n\n#ifdef TENSORFLOW_USE_LIBXSMM\ntemplate <typename TL, typename TR>\nclass LibxsmmSparseMatMul {\n  using MatrixL = BasicMatrix<TL>;\n  using MatrixR = BasicMatrix<TR>;\n  using ConstMatrixMapL = BasicMatrixMap<const TL>;\n  using ConstMatrixMapR = BasicMatrixMap<const TR>;\n  using MatrixMapR = BasicMatrixMap<TR>;\n\n public:\n  // This structure contains a set of libxsmm kernels for sizes that have been\n  // encountered previously by this operator so that libxsmm does not need to\n  // reallocate its scratchpad memory each time (which hurts performance\n  // substantially).\n  struct TensorInfoCache {\n    struct TensorInfoCacheEntry {\n      // Parameters for kernel\n      int M;\n      int K;\n      int N;\n      int max_threads;\n      // libxsmm handle and matrix data\n      libxsmm_spmdm_handle handle;\n      libxsmm_CSR_sparseslice* output_csr;\n      // Chain to non-libxsmm implementation's cache in case that ever becomes\n      // useful (it is an empty struct right now)\n      typename SparseMatMul<TL, TR>::TensorInfoCache\n          non_libxsmm_cache;  // Currently not used\n    };\n    // protects entries; invariant: entries is a valid std::multimap\n    tensorflow::mutex lock;\n    // Because there could be multiple matrix multiplies with the same sizes\n    // going on at the same time, we need to allow multiple cache entries for a\n    // given set of parameters. Taking and returning entries is used to make\n    // sure the same cache entry is not used from two threads at a time.\n    std::multimap<std::tuple<int, int, int, int>,\n                  std::unique_ptr<TensorInfoCacheEntry>>\n        entries TF_GUARDED_BY(lock);\n\n    TensorInfoCache() : lock(), entries() {}\n    // Look up and remove first entry with these parameters, creating one if\n    // there isn't one\n    std::unique_ptr<TensorInfoCacheEntry> take_cache_entry(int M, int K, int N,\n                                                           int max_threads)\n        TF_LOCKS_EXCLUDED(lock) {\n      tensorflow::mutex_lock ml(lock);\n      auto key = std::make_tuple(M, K, N, max_threads);\n      auto it = entries.find(key);\n      if (it != entries.end()) {\n        auto val = std::move(it->second);\n        entries.erase(it);\n        return val;\n      } else {\n        std::unique_ptr<TensorInfoCacheEntry> e{\n            new TensorInfoCacheEntry{M, K, N, max_threads, {}, nullptr}};\n        // setup scoped allocator, which uses cpu_allocator() for this scope\n        const libxsmm_tf_allocator<libxsmm_scratch_allocator> tf_allocator;\n        libxsmm_spmdm_init(M, N, K, max_threads, &e->handle, &e->output_csr);\n        return e;\n      }\n    }\n    // Add a cache entry with certain parameters\n    void return_cache_entry(std::unique_ptr<TensorInfoCacheEntry> e)\n        TF_LOCKS_EXCLUDED(lock) {\n      tensorflow::mutex_lock ml(lock);\n      auto key = std::make_tuple(e->M, e->K, e->N, e->max_threads);\n      entries.insert(std::make_pair(key, std::move(e)));\n    }\n    ~TensorInfoCache() {\n      tensorflow::mutex_lock ml(lock);\n      for (auto& p : entries) {\n        libxsmm_spmdm_destroy(&p.second->handle);\n      }\n      entries.clear();\n    }\n\n   private:\n    TF_DISALLOW_COPY_AND_ASSIGN(TensorInfoCache);\n  };\n\n  // Perform matrix multiplication of \"left\" and \"right\", and store the result\n  // in *\"output\".\n public:\n  static inline void Compute(TensorInfoCache* cache,\n                             const ConstMatrixMapL& left,\n                             const ConstMatrixMapR& right, bool transpose_left,\n                             const DeviceBase::CpuWorkerThreads* thread_pool,\n                             bool transpose_output, MatrixMap* output);\n\n private:\n  TF_DISALLOW_COPY_AND_ASSIGN(LibxsmmSparseMatMul);\n};\n#endif\n\ntemplate <typename TL, typename TR,\n          template <typename TL2, typename TR2> class DoMatMul>\nclass SparseMatMulOp : public OpKernel {\n  using MatrixR = BasicMatrix<TR>;\n  using ConstMatrixMapR = BasicMatrixMap<const TR>;\n\n public:\n  explicit SparseMatMulOp(OpKernelConstruction* ctx) : OpKernel(ctx) {\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"transpose_a\", &transpose_a_));\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"transpose_b\", &transpose_b_));\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"a_is_sparse\", &a_is_sparse_));\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"b_is_sparse\", &b_is_sparse_));\n  }\n\n  void Compute(OpKernelContext* ctx) override {\n    const Tensor& a = ctx->input(0);\n    const Tensor& b = ctx->input(1);\n    OP_REQUIRES(ctx, TensorShapeUtils::IsMatrix(a.shape()),\n                errors::InvalidArgument(\"a is not a matrix\"));\n    OP_REQUIRES(ctx, TensorShapeUtils::IsMatrix(b.shape()),\n                errors::InvalidArgument(\"b is not a matrix\"));\n\n    const int m = transpose_a_ ? a.dim_size(1) : a.dim_size(0);\n    const int k = transpose_a_ ? a.dim_size(0) : a.dim_size(1);\n    const int n = transpose_b_ ? b.dim_size(0) : b.dim_size(1);\n    const int k2 = transpose_b_ ? b.dim_size(1) : b.dim_size(0);\n\n    OP_REQUIRES(ctx, k == k2,\n                errors::InvalidArgument(\n                    \"Matrix size incompatible: a: \", a.shape().DebugString(),\n                    \", b: \", b.shape().DebugString()));\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, TensorShape({m, n}), &output));\n\n    if (k == 0) {\n      // If the inner dimension k in the matrix multiplication is zero, we fill\n      // the output with zeros.\n      functor::SetZeroFunctor<CPUDevice, float> f;\n      f(ctx->eigen_device<CPUDevice>(), output->flat<float>());\n      return;\n    }\n\n    auto out = output->matrix<float>();\n\n    std::unique_ptr<Tensor> a_float;\n    std::unique_ptr<Tensor> b_float;\n    if (!a_is_sparse_ && !b_is_sparse_) {\n      auto left = &a;\n      auto right = &b;\n      // TODO(agarwal): multi-thread the conversions from bfloat16 to float.\n      if (std::is_same<TL, bfloat16>::value) {\n        a_float.reset(new Tensor(DT_FLOAT, a.shape()));\n        BFloat16ToFloat(a.flat<bfloat16>().data(),\n                        a_float->flat<float>().data(), a.NumElements());\n        left = a_float.get();\n      }\n      if (std::is_same<TR, bfloat16>::value) {\n        b_float.reset(new Tensor(DT_FLOAT, b.shape()));\n        BFloat16ToFloat(b.flat<bfloat16>().data(),\n                        b_float->flat<float>().data(), b.NumElements());\n        right = b_float.get();\n      }\n      Eigen::array<Eigen::IndexPair<Eigen::DenseIndex>, 1> dim_pair;\n      dim_pair[0].first = transpose_a_ ? 0 : 1;\n      dim_pair[0].second = transpose_b_ ? 1 : 0;\n\n      out.device(ctx->template eigen_device<CPUDevice>()) =\n          left->matrix<float>().contract(right->matrix<float>(), dim_pair);\n      return;\n    }\n\n    auto left = &a;\n    auto right = &b;\n    bool transpose_output = false;\n    bool transpose_a = transpose_a_;\n    bool transpose_b = transpose_b_;\n    if (!a_is_sparse_) {\n      // Swap the order of multiplications using the identity:\n      // A * B = (B' *  A')'.\n      std::swap(left, right);\n      std::swap(transpose_a, transpose_b);\n      transpose_a = !transpose_a;\n      transpose_b = !transpose_b;\n      transpose_output = !transpose_output;\n    }\n\n    std::unique_ptr<Tensor> right_tr;\n    if (transpose_b) {\n      // TODO(agarwal): avoid transposing the matrix here and directly handle\n      // transpose in CreateDenseSlices.\n      OP_REQUIRES(ctx, right->dim_size(0) != 0,\n                  errors::InvalidArgument(\"b has an entry 0 in it's shape.\"));\n      OP_REQUIRES(ctx, right->dim_size(1) != 0,\n                  errors::InvalidArgument(\"b has an entry 0 in it's shape.\"));\n      right_tr.reset(\n          new Tensor(right->dtype(),\n                     TensorShape({right->dim_size(1), right->dim_size(0)})));\n\n      const auto perm = dsizes_10();\n      if (transpose_output) {\n        right_tr->matrix<TL>().device(ctx->template eigen_device<CPUDevice>()) =\n            right->matrix<TL>().shuffle(perm);\n      } else {\n        right_tr->matrix<TR>().device(ctx->template eigen_device<CPUDevice>()) =\n            right->matrix<TR>().shuffle(perm);\n      }\n      right = right_tr.get();\n    }\n\n    if (transpose_output) {\n      DoMatMul<TR, TL>::Compute(&this->cache_tr_, left->matrix<TR>(),\n                                right->matrix<TL>(), transpose_a,\n                                ctx->device()->tensorflow_cpu_worker_threads(),\n                                transpose_output, &out);\n    } else {\n      DoMatMul<TL, TR>::Compute(&this->cache_nt_, left->matrix<TL>(),\n                                right->matrix<TR>(), transpose_a,\n                                ctx->device()->tensorflow_cpu_worker_threads(),\n                                transpose_output, &out);\n    }\n  }\n\n private:\n  bool transpose_a_;\n  bool transpose_b_;\n  bool a_is_sparse_;\n  bool b_is_sparse_;\n\n  // Cache for non-transposed-output multiply\n  typename DoMatMul<TL, TR>::TensorInfoCache cache_nt_;\n  // Cache for transposed-output multiply\n  typename DoMatMul<TR, TL>::TensorInfoCache cache_tr_;\n\n  TF_DISALLOW_COPY_AND_ASSIGN(SparseMatMulOp);\n};\n\ntemplate <typename TL, typename TR>\ninline void SparseMatMul<TL, TR>::ComputeOutputBlock(\n    const std::vector<SparseSlice<TL>*>& left,\n    const typename SparseMatMul<TL, TR>::ConstMatrixMapR& right, int num_cols,\n    int output_row_offset, int output_col_offset, bool assign,\n    bool transpose_output, MatrixMap* output) {\n  const auto perm = dsizes_10();\n  int num_rows = left[0]->num_rows;\n  const int rhs_num_cols = right.dimension(1);\n  DCHECK_LE(num_cols, rhs_num_cols);\n  Matrix out(num_rows, rhs_num_cols);\n  out.setZero();\n  if (num_cols == N) {\n    GEPP<TL, TR, N>(left, right, num_cols, &out);\n  } else {\n    GEPP<TL, TR, -1>(left, right, num_cols, &out);\n  }\n  if (!assign) {\n    const DSizes begin(output_row_offset, output_col_offset);\n    const DSizes sizes(num_rows, num_cols);\n    if (transpose_output) {\n      if (num_cols == rhs_num_cols) {\n        output->shuffle(perm).slice(begin, sizes) += out;\n      } else {\n        const auto zero = dsizes_00();\n        output->shuffle(perm).slice(begin, sizes) += out.slice(zero, sizes);\n      }\n    } else {\n      if (num_cols == rhs_num_cols) {\n        output->slice(begin, sizes) += out;\n      } else {\n        const auto zero = dsizes_00();\n        output->slice(begin, sizes) += out.slice(zero, sizes);\n      }\n    }\n  } else {\n    std::unique_ptr<Matrix> out_tr;\n    if (transpose_output) {\n      out_tr.reset(new Matrix(rhs_num_cols, num_rows));\n      *out_tr = out.shuffle(perm);\n      std::swap(output_row_offset, output_col_offset);\n      std::swap(num_rows, num_cols);\n    }\n    const Matrix& final_out = transpose_output ? *out_tr : out;\n    for (int i = 0; i < num_rows; ++i) {\n      memcpy(&(*output)(output_row_offset + i, output_col_offset),\n             &final_out(i, 0), num_cols * sizeof(float));\n    }\n  }\n}\n\ntemplate <typename TL, typename TR>\ninline std::unique_ptr<BlockingCounter>\nSparseMatMul<TL, TR>::CreateSparseSlices(\n    const typename SparseMatMul<TL, TR>::ConstMatrixMapL& mat, bool transpose,\n    int slice_num_rows, int slice_block_size, int slice_num_cols,\n    std::vector<std::vector<SparseSlice<TL>*>>* mat_slices,\n    const DeviceBase::CpuWorkerThreads* thread_pool) {\n  const int mat_num_rows = transpose ? mat.dimension(1) : mat.dimension(0);\n  const int mat_num_cols = transpose ? mat.dimension(0) : mat.dimension(1);\n  const int num_slices_dim0 =\n      std::max(1, (mat_num_rows + slice_num_rows - 1) / slice_num_rows);\n  const int num_slices_dim1 =\n      std::max(1, (mat_num_cols + slice_num_cols - 1) / slice_num_cols);\n  mat_slices->resize(num_slices_dim0);\n  BlockingCounter* counter =\n      new BlockingCounter(num_slices_dim0 * num_slices_dim1);\n  auto work = [counter, transpose](SparseSlice<TL>* sparse_slice,\n                                   SparseMatMul<TL, TR>::ConstMatrixMapL* slice,\n                                   int col_offset) {\n    if (transpose) {\n      sparse_slice->template Initialize<true>(*slice, col_offset);\n    } else {\n      sparse_slice->template Initialize<false>(*slice, col_offset);\n    }\n    delete slice;\n    counter->DecrementCount();\n  };\n  for (int i = 0; i < num_slices_dim0; ++i) {\n    (*mat_slices)[i].resize(num_slices_dim1);\n    int num_rows =\n        std::min<int>(slice_num_rows, mat_num_rows - i * slice_num_rows);\n    for (int j = 0; j < num_slices_dim1; ++j) {\n      int num_cols =\n          std::min<int>(slice_num_cols, mat_num_cols - j * slice_num_cols);\n      SparseMatMul<TL, TR>::ConstMatrixMapL* slice = nullptr;\n      if (transpose) {\n        slice = new SparseMatMul<TL, TR>::ConstMatrixMapL(\n            &mat(0, i * slice_num_rows), mat.dimensions());\n      } else {\n        DSizes d(num_rows, mat_num_cols);\n        slice = new SparseMatMul<TL, TR>::ConstMatrixMapL(\n            &mat(i * slice_num_rows, 0), d);\n      }\n      auto* sparse_slice =\n          new SparseSlice<TL>(num_rows, num_cols, slice_block_size);\n      (*mat_slices)[i][j] = sparse_slice;\n      thread_pool->workers->Schedule(\n          [=]() { work(sparse_slice, slice, slice_num_cols * j); });\n    }\n  }\n  return std::unique_ptr<BlockingCounter>(counter);\n}\n#define LOAD(x) Eigen::internal::ploadu<Packet>((x));\n#define INTERLEAVE(x) Eigen::internal::pinterleave4x64<Packet>(x);\n#define STORE(x, y) Eigen::internal::pstoreu<float>(x, y);\n\ntemplate <int NUM_ELEM = -1>\nALWAYS_INLINE void CopyAndMayBeInterleaveBfloat16(void* bdst, const void* bsrc,\n                                                  int num_elements) {\n  DCHECK_GE(kNumOperands, 8);\n  static const int kStep = kNumOperands * sizeof(float) / sizeof(bfloat16);\n  const int num = (NUM_ELEM == -1) ? num_elements : NUM_ELEM;\n  DCHECK_EQ(num, num_elements);\n  const float* src = reinterpret_cast<const float*>(bsrc);\n  float* dst = reinterpret_cast<float*>(bdst);\n  for (int index = 0; index + kStep <= num; index += kStep) {\n    auto in = LOAD(src);\n    auto tmp = INTERLEAVE(in);\n    STORE(dst, tmp);\n    src += kNumOperands;\n    dst += kNumOperands;\n  }\n  if (num % kStep != 0) {\n    memcpy(dst, src, (num % kStep) * sizeof(bfloat16));\n  }\n}\n\ntemplate <typename T>\nALWAYS_INLINE void CopyAndMayBeInterleave(void* dst, const void* src,\n                                          int num_elements) {\n  if (std::is_same<T, float>::value || kNumOperands < 8) {\n    memcpy(dst, src, num_elements * sizeof(T));\n  } else if (std::is_same<T, bfloat16>::value) {\n    if (num_elements == N) {\n      CopyAndMayBeInterleaveBfloat16<N>(dst, src, num_elements);\n    } else {\n      CopyAndMayBeInterleaveBfloat16<-1>(dst, src, num_elements);\n    }\n  } else {\n    LOG(FATAL) << \"Unsupported type\";\n  }\n}\n\n#undef LOAD\n#undef Interleave\n#undef Store\n\ntemplate <typename TL, typename TR>\ninline BlockingCounter* SparseMatMul<TL, TR>::ShuffleMatrix(\n    const typename SparseMatMul<TL, TR>::ConstMatrixMapR& mat,\n    int slice_row_start, int slice_num_rows, int slice_col_start,\n    int slice_num_cols, const int N,\n    const DeviceBase::CpuWorkerThreads* thread_pool, MatrixR* buffer) {\n  DCHECK_EQ(N % 2, 0);\n  DCHECK_LE(kNumOperands * sizeof(float) / sizeof(TR), N);\n  // Note(nikhilsarda): This heuristic is optimal in benchmarks as of\n  // Jan 21, 2020.\n  int num_threads = std::min(thread_pool->num_threads, 8);\n  BlockingCounter* counter = new BlockingCounter(num_threads);\n  DCHECK_EQ(N, buffer->dimension(1));\n  auto shuffle_work = [&mat, slice_row_start, slice_num_rows, slice_col_start,\n                       slice_num_cols, N, buffer, counter](int s, int e) {\n    const int row_start = s % slice_num_rows + slice_row_start;\n    const int col_start = s / slice_num_rows * N + slice_col_start;\n    auto* out_start = &(*buffer)(s, 0);\n    const auto* input_start = &mat(row_start, col_start);\n    const auto* input_end = &mat(slice_row_start + slice_num_rows - 1,\n                                 slice_col_start + slice_num_cols - 1);\n    const int mat_num_cols = mat.dimension(1);\n    const int row_slice_size = slice_num_rows * mat_num_cols;\n\n    const int aligned_end = slice_num_cols / N * slice_num_rows;\n    const int e1 = std::min(e, aligned_end);\n    while (s < e1) {\n      CopyAndMayBeInterleave<TR>(out_start, input_start, N);\n      out_start += N;\n      input_start += mat_num_cols;\n      if (input_start > input_end) {\n        input_start = input_start - row_slice_size + N;\n      }\n      ++s;\n    }\n    int s1 = std::max(s, aligned_end);\n    const int copy_num_cols = slice_num_cols % N;\n    while (s1 < e) {\n      CopyAndMayBeInterleave<TR>(out_start, input_start, copy_num_cols);\n      out_start += N;\n      input_start += mat_num_cols;\n      ++s1;\n    }\n    if (counter) counter->DecrementCount();\n  };\n\n  int start = 0;\n  int end = 0;\n  int num_out_rows = (slice_num_cols + N - 1) / N * slice_num_rows;\n  DCHECK_LE(num_out_rows, buffer->dimension(0));\n  for (int i = std::max(1, num_threads); i > 0; --i) {\n    end = start + num_out_rows / i;\n    thread_pool->workers->Schedule([=]() { shuffle_work(start, end); });\n    num_out_rows -= (end - start);\n    start = end;\n  }\n  return counter;\n}\n\ntemplate <typename TL, typename TR>\ninline void SparseMatMul<TL, TR>::SliceMatrix(\n    const MatrixR& mat, const int num_rows, const int num_slices,\n    std::vector<typename SparseMatMul<TL, TR>::ConstMatrixMapR*>* slices) {\n  slices->resize(num_slices);\n  DSizes d(num_rows, mat.dimension(1));\n  DCHECK_LE(num_rows * num_slices, mat.dimension(0));\n  for (int i = 0; i < num_slices; ++i) {\n    (*slices)[i] = new ConstMatrixMapR(&mat(i * num_rows, 0), d);\n  }\n}\n\ntemplate <typename TL, typename TR>\ninline std::unique_ptr<BlockingCounter> SparseMatMul<TL, TR>::CreateDenseSlices(\n    const typename SparseMatMul<TL, TR>::ConstMatrixMapR& mat, int row_start,\n    int num_rows, int col_start, int num_cols,\n    const DeviceBase::CpuWorkerThreads* thread_pool, MatrixR* buffer,\n    std::vector<typename SparseMatMul<TL, TR>::ConstMatrixMapR*>* slices) {\n  std::unique_ptr<BlockingCounter> shuffle_counter(ShuffleMatrix(\n      mat, row_start, num_rows, col_start, num_cols, N, thread_pool, buffer));\n  const int num_slices = (num_cols + N - 1) / N;\n  SliceMatrix(*buffer, num_rows, num_slices, slices);\n  return shuffle_counter;\n}\n\ntemplate <typename TL, typename TR>\ninline void SparseMatMul<TL, TR>::ComputeBlockSizes(\n    const typename SparseMatMul<TL, TR>::ConstMatrixMapL& left,\n    const typename SparseMatMul<TL, TR>::ConstMatrixMapR& right,\n    bool transpose_left, int num_threads, int* KR, int* NR, int* KL, int* JB,\n    int* IB) {\n  // Heuristics for calculating block sizes\n  // Assume two hyperthreads per core.\n  const int est_num_cores = std::max(1, (num_threads + 1) / 2);\n  // Use block of rhs with at most 128K floats per core.\n  const int mem = est_num_cores * 128 * 1024;\n  *KR = std::min(static_cast<int>(right.dimension(0)), mem / 256);\n  *NR = right.dimension(1);\n  if (*KR * *NR > mem) {\n    // 4096 may be enough to amortize the cost of writes.\n    *KR = std::min<int>(*KR, 4096);\n  }\n  // Use sizes that are multiples of K and 256.\n  *KR = std::max(1, *KR / K) * K;\n  *NR = std::max(1, *NR / 256) * 256;\n  if (*KR * *NR > mem) {\n    *NR = mem / *KR;\n  }\n  *NR = std::max(1, *NR / 256) * 256;\n\n  const int left_dim0 = transpose_left ? left.dimension(1) : left.dimension(0);\n  const int left_dim1 = transpose_left ? left.dimension(0) : left.dimension(1);\n  for (*KL = 1024; *KL > K; *KL /= 2) {\n    if (*KR % *KL == 0 &&\n        std::max<int>(1, left_dim0 / 64) * (left_dim1 / *KL) > est_num_cores) {\n      break;\n    }\n  }\n  DCHECK_EQ(*KL % K, 0);\n  DCHECK_GE(*KR, *KL);\n  if (*KR < right.dimension(0)) {\n    CHECK_EQ(*KR % *KL, 0);\n  }\n\n  *JB = std::max(1, static_cast<int>(sqrt(num_threads) / 2.0));\n  *IB = 8 * *JB;\n  DCHECK_EQ(N * sizeof(float) % 64, size_t{0});\n}\n\n#ifdef TENSORFLOW_USE_LIBXSMM\n\ntemplate <typename F>\nvoid do_on_all_threads(const DeviceBase::CpuWorkerThreads* thread_pool,\n                       const F& f) {\n  int num_threads = thread_pool->num_threads;\n  if (num_threads == 0) {\n    LOG(FATAL) << \"Have 0 threads in thread pool\";\n  } else if (num_threads == 1) {\n    f(0);\n  } else {\n    BlockingCounter counter(num_threads - 1);\n    for (int i = 1; i < num_threads; ++i) {\n      thread_pool->workers->Schedule([&, i]() {\n        f(i);\n        counter.DecrementCount();\n      });\n    }\n    f(0);\n    counter.Wait();\n  }\n}\n\ntemplate <typename T>\nstruct empty_type_wrapper {};\n\n// Copies of interface to libxsmm_spmdm_createSparseSlice_*_notrans_thread to\n// allow overloading\nvoid wrapper_libxsmm_spmdm_createSparseSlice_generic_thread(\n    empty_type_wrapper<float>, const libxsmm_spmdm_handle* handle, char transA,\n    const float* A, libxsmm_CSR_sparseslice* libxsmm_output_csr_a, int block_id,\n    int tid, int nthreads) {\n  return libxsmm_spmdm_createSparseSlice_fp32_thread(\n      handle, transA, A, libxsmm_output_csr_a, block_id, tid, nthreads);\n}\nvoid wrapper_libxsmm_spmdm_createSparseSlice_generic_thread(\n    empty_type_wrapper<bfloat16>, const libxsmm_spmdm_handle* handle,\n    char transA, const bfloat16* A,\n    libxsmm_CSR_sparseslice* libxsmm_output_csr_a, int block_id, int tid,\n    int nthreads) {\n  return libxsmm_spmdm_createSparseSlice_bfloat16_thread(\n      handle, transA, reinterpret_cast<const libxsmm_bfloat16*>(A),\n      libxsmm_output_csr_a, block_id, tid, nthreads);\n}\n\nvoid wrapper_libxsmm_spmdm_compute_generic_thread(\n    empty_type_wrapper<bfloat16>, const libxsmm_spmdm_handle* handle,\n    char transA, char transB, const bfloat16* alpha,\n    libxsmm_CSR_sparseslice* A_sparse, const bfloat16* B, char transC,\n    const bfloat16* beta, float* C, int block_id, int tid, int nthreads) {\n  return libxsmm_spmdm_compute_bfloat16_thread(\n      handle, transA, transB, reinterpret_cast<const libxsmm_bfloat16*>(alpha),\n      A_sparse, reinterpret_cast<const libxsmm_bfloat16*>(B), transC,\n      reinterpret_cast<const libxsmm_bfloat16*>(beta), C, block_id, tid,\n      nthreads);\n}\nvoid wrapper_libxsmm_spmdm_compute_generic_thread(\n    empty_type_wrapper<float>, const libxsmm_spmdm_handle* handle, char transA,\n    char transB, const float* alpha, libxsmm_CSR_sparseslice* A_sparse,\n    const float* B, char transC, const float* beta, float* C, int block_id,\n    int tid, int nthreads) {\n  return libxsmm_spmdm_compute_fp32_thread(handle, transA, transB, alpha,\n                                           A_sparse, B, transC, beta, C,\n                                           block_id, tid, nthreads);\n}\n\ntemplate <typename TL, typename TR>\ninline void LibxsmmSparseMatMul<TL, TR>::Compute(\n    typename LibxsmmSparseMatMul<TL, TR>::TensorInfoCache* cache,\n    const typename LibxsmmSparseMatMul<TL, TR>::ConstMatrixMapL& left,\n    const typename LibxsmmSparseMatMul<TL, TR>::ConstMatrixMapR& right,\n    bool transpose_left, const DeviceBase::CpuWorkerThreads* thread_pool,\n    bool transpose_output, MatrixMap* output) {\n  const int num_threads = thread_pool->num_threads;\n  const int left_dim0 = transpose_left ? left.dimension(1) : left.dimension(0);\n  const int left_dim1 = transpose_left ? left.dimension(0) : left.dimension(1);\n  const int right_dim0 = right.dimension(0);\n  const int right_dim1 = right.dimension(1);\n  CHECK_EQ(left_dim1, right_dim0);\n  CHECK_EQ(left_dim0,\n           (transpose_output ? output->dimension(1) : output->dimension(0)));\n  CHECK_EQ(right_dim1,\n           (transpose_output ? output->dimension(0) : output->dimension(1)));\n#if 0  // this issue seems to be resolved\n  if (left_dim0 < 32 || left_dim1 < 32 || right_dim1 < 32) {\n    // Causes problems in libxsmm\n    SparseMatMul<TL, TR>::Compute(\n        nullptr /* Assumes no cached data for fallback */, left, right,\n        transpose_left, thread_pool, transpose_output, output);\n    return;\n  }\n#endif\n  auto left_data = left.data();\n  auto right_data = right.data();\n  auto output_data = output->data();\n  // Initialize libxsmm for this matrix; make sure another thread doesn't use\n  // this handle\n  auto entry =\n      cache->take_cache_entry(left_dim0, right_dim0, right_dim1, num_threads);\n  // Convert the left matrix to compressed sparse row (CSR) format\n  ptrdiff_t total_num_creation_blocks =\n      libxsmm_spmdm_get_num_createSparseSlice_blocks(&entry->handle);\n  std::atomic<int> cur_create_block_number;\n  cur_create_block_number.store(0);\n  do_on_all_threads(thread_pool, [&](int i) {\n    while (true) {\n      int work_item = cur_create_block_number.fetch_add(1);\n      if (work_item >= total_num_creation_blocks) break;\n      wrapper_libxsmm_spmdm_createSparseSlice_generic_thread(\n          empty_type_wrapper<TL>{}, &entry->handle,\n          (transpose_left ? 'T' : 'N'), left_data, entry->output_csr, work_item,\n          i, num_threads);\n    }\n  });\n  // Do matrix-matrix multiplication\n  ptrdiff_t total_num_mult_blocks =\n      libxsmm_spmdm_get_num_compute_blocks(&entry->handle);\n  std::atomic<int> cur_mult_block_number;\n  cur_mult_block_number.store(0);\n  do_on_all_threads(thread_pool, [&](int i) {\n    while (true) {\n      int work_item = cur_mult_block_number.fetch_add(1);\n      if (work_item >= total_num_mult_blocks) break;\n      const TL alpha(1.0);  // Stored in a variable so we can get a pointer\n      const TL beta(0.0);   // Stored in a variable so we can get a pointer\n      wrapper_libxsmm_spmdm_compute_generic_thread(\n          empty_type_wrapper<TL>{}, &entry->handle,\n          (transpose_left ? 'T' : 'N'), 'N', &alpha, entry->output_csr,\n          right_data, (transpose_output ? 'T' : 'N'), &beta, output_data,\n          work_item, i, num_threads);\n    }\n  });\n  // Put handle + CSR storage back into cache\n  cache->return_cache_entry(std::move(entry));\n}\n\n#endif  // TENSORFLOW_USE_LIBXSMM\n\n// Here is an overview of the SparseMatMul code. Note that we assume that the\n// left matrix is sparse.\n//\n// The matrix \"left\" is divided into a grid with blocksize of (M, KL). Each\n// block is encoded as a SparseSlice. These grid elements are stored as\n// std::vector<std::vector<SparseSlice>>. Each element of the outer vector\n// represents M rows of the left matrix. Lets call these elements l_i and lets\n// call each element of the inner vector L_mk.\n//\n// The matrix \"right\" is divided into a grid with block size KR * NR.  Lets\n// denote the blocks on the right as R_kn. Note that we ensure that KL divides\n// KR so that for each element R_kn, we don't need to multiply it with any\n// partial L_mk blocks.\n//\n// We then multiply each right side block R_kn with the full \"left\" matrix and\n// update the output. These iterations are run sequentially since R_kn are\n// packed into the same underlying temporary buffer.\n//\n// In each iteration we do the following:\n// 1. Create slices r_j of R_kn: We split R_kn into vertical blocks with N\n//    (=128) columns and then concatenating these slices into a buffer. This is\n//    done so that each slice r_j of R_kn is stored contiguously in memory. Note\n//    that if R_kj has dimensions (KR, NR), we create NR / N slices, and the\n//    buffer has dimensions (KR * NR / N, N) (assuming N divides NR).\n// 2. For each (l_i, r_j), we compute the inner product using the GEPP function\n//    and update the output block o_ij. These calls are further blocked to\n//    reduce the working set size. In each iteration we take IB elements from\n//    {l_i} and JB elements from {r_j} and compute the IB * JB inner products.\ntemplate <typename TL, typename TR>\ninline void SparseMatMul<TL, TR>::Compute(\n    typename SparseMatMul<TL, TR>::TensorInfoCache* /*cache*/,\n    const typename SparseMatMul<TL, TR>::ConstMatrixMapL& left,\n    const typename SparseMatMul<TL, TR>::ConstMatrixMapR& right,\n    bool transpose_left, const DeviceBase::CpuWorkerThreads* thread_pool,\n    bool transpose_output, MatrixMap* output) {\n  const int num_threads = thread_pool->num_threads;\n  int KR, NR, KL, JB, IB;\n  ComputeBlockSizes(left, right, transpose_left, num_threads, &KR, &NR, &KL,\n                    &JB, &IB);\n  // Slice the left matrix\n  std::vector<std::vector<SparseSlice<TL>*>> left_slices;\n  std::unique_ptr<BlockingCounter> sparse_slice_counter =\n      CreateSparseSlices(ConstMatrixMapL(left.data(), left.dimensions()),\n                         transpose_left, M, K, KL, &left_slices, thread_pool);\n  const int num_left_slices = left_slices.size();\n\n  const int right_dim0 = right.dimension(0);\n  const int right_dim1 = right.dimension(1);\n  // Allocate buffer for storing slices of right matrix.\n  // Note buffer needs enough space to hold at most a KR * NR matrix since that\n  // is the block size per iteration.\n  const int buffer_num_rows =\n      std::min(KR, right_dim0) * ((std::min(NR, right_dim1) + N - 1) / N);\n  MatrixR buffer(buffer_num_rows, N);\n  std::vector<ConstMatrixMapR*> right_slices;\n\n  std::vector<SparseSlice<TL>*> block_left_slices;\n  std::vector<std::function<void(void)>> tasks;\n  // Number of blocks based on block sizes of KR * NR.\n  const int num_k_blocks = (right_dim0 + KR - 1) / KR;\n  const int num_n_blocks = (right_dim1 + NR - 1) / NR;\n  std::unique_ptr<BlockingCounter> dense_slice_counter;\n\n  for (int nb = 0; nb < num_n_blocks; ++nb) {\n    const int right_num_cols =\n        std::min(NR, static_cast<int>(right_dim1 - NR * nb));\n    for (int kb = 0; kb < num_k_blocks; ++kb) {\n      const int right_num_rows =\n          std::min(KR, static_cast<int>(right_dim0 - KR * kb));\n      dense_slice_counter = CreateDenseSlices(\n          right, kb * KR, right_num_rows, nb * NR, right_num_cols, thread_pool,\n          &buffer, &right_slices);\n      const int num_right_slices = right_slices.size();\n      tasks.reserve(num_left_slices * num_right_slices);\n      for (int j_outer = 0; j_outer < num_right_slices; j_outer += JB) {\n        for (int i_outer = 0; i_outer < num_left_slices; i_outer += IB) {\n          for (int j_inner = j_outer;\n               j_inner < std::min(num_right_slices, j_outer + JB); ++j_inner) {\n            const int num_cols = std::min(N, right_num_cols - N * j_inner);\n            for (int i_inner = i_outer;\n                 i_inner < std::min(num_left_slices, i_outer + IB); ++i_inner) {\n              block_left_slices.clear();\n              int begin = kb * KR / KL;\n              int end = std::min<int>((kb + 1) * KR / KL,\n                                      (right.dimension(0) + KL - 1) / KL);\n              DCHECK_LT(begin, end);\n              block_left_slices.insert(block_left_slices.begin(),\n                                       left_slices[i_inner].begin() + begin,\n                                       left_slices[i_inner].begin() + end);\n              tasks.push_back(std::bind(\n                  &ComputeOutputBlock, block_left_slices,\n                  std::ref(*right_slices[j_inner]), num_cols, M * i_inner,\n                  N * j_inner + nb * NR, kb == 0, transpose_output, output));\n            }\n          }\n        }\n      }\n      if (sparse_slice_counter) {\n        sparse_slice_counter->Wait();\n        sparse_slice_counter.reset(nullptr);\n      }\n      if (dense_slice_counter) {\n        dense_slice_counter->Wait();\n        dense_slice_counter.reset(nullptr);\n      }\n      BlockingCounter bc(tasks.size());\n      for (const auto& t : tasks) {\n        thread_pool->workers->Schedule([&bc, &t]() {\n          t();\n          bc.DecrementCount();\n        });\n      }\n      bc.Wait();\n      tasks.clear();\n      for (auto& temp : right_slices) {\n        delete temp;\n      }\n      right_slices.clear();\n    }\n  }\n  for (auto& left_slice : left_slices) {\n    for (auto& temp : left_slice) {\n      delete temp;\n    }\n    left_slice.clear();\n  }\n}\n\n#define REGISTER_SPARSE_MATMUL(TA, TB)                   \\\n  REGISTER_KERNEL_BUILDER(Name(\"SparseMatMul\")           \\\n                              .Device(DEVICE_CPU)        \\\n                              .TypeConstraint<TA>(\"Ta\")  \\\n                              .TypeConstraint<TB>(\"Tb\"), \\\n                          SparseMatMulOp<TA, TB, SparseMatMul>);\n#ifdef TENSORFLOW_USE_LIBXSMM\n#define REGISTER_SPARSE_MATMUL_LIBXSMM(TA, TB)           \\\n  REGISTER_KERNEL_BUILDER(Name(\"SparseMatMul\")           \\\n                              .Device(DEVICE_CPU)        \\\n                              .TypeConstraint<TA>(\"Ta\")  \\\n                              .TypeConstraint<TB>(\"Tb\"), \\\n                          SparseMatMulOp<TA, TB, LibxsmmSparseMatMul>);\n#endif\n\nREGISTER_SPARSE_MATMUL(float, bfloat16);\nREGISTER_SPARSE_MATMUL(bfloat16, float);\n\n#ifdef TENSORFLOW_USE_LIBXSMM\nREGISTER_SPARSE_MATMUL_LIBXSMM(bfloat16, bfloat16);\nREGISTER_SPARSE_MATMUL_LIBXSMM(float, float);\n#else\nREGISTER_SPARSE_MATMUL(bfloat16, bfloat16);\nREGISTER_SPARSE_MATMUL(float, float);\n#endif\n\n#undef REGISTER_SPARSE_MATMUL\n\n}  // end namespace tensorflow\n"], "fixing_code": ["/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n// See docs in ../ops/math_ops.cc.\n\n#define EIGEN_USE_THREADS\n\n#include \"tensorflow/core/kernels/sparse_matmul_op.h\"\n\n#include <map>\n#include <memory>\n#include <vector>\n\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n#include \"tensorflow/core/common_runtime/device.h\"\n#include \"tensorflow/core/framework/bfloat16.h\"\n#include \"tensorflow/core/framework/op.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/types.h\"\n#include \"tensorflow/core/kernels/fill_functor.h\"\n#include \"tensorflow/core/lib/core/blocking_counter.h\"\n#include \"tensorflow/core/lib/core/threadpool.h\"\n#include \"tensorflow/core/platform/errors.h\"\n#include \"tensorflow/core/platform/logging.h\"\n#include \"tensorflow/core/platform/macros.h\"\n#include \"tensorflow/core/platform/mutex.h\"\n#include \"tensorflow/core/platform/thread_annotations.h\"\n#include \"tensorflow/core/platform/types.h\"\n#ifdef TENSORFLOW_USE_LIBXSMM\n#include \"include/libxsmm_intrinsics_x86.h\"\n#include \"include/libxsmm_malloc.h\"\n#include \"include/libxsmm_spmdm.h\"\n#endif\n\n#if defined(TENSORFLOW_USE_CUSTOM_CONTRACTION_KERNEL)\n#include \"tensorflow/core/kernels/eigen_contraction_kernel.h\"\n#endif\n\n#define ALWAYS_INLINE EIGEN_ALWAYS_INLINE\n\nnamespace tensorflow {\nnamespace {\n\ntemplate <typename T>\nusing BasicMatrix = Eigen::Tensor<T, 2, Eigen::RowMajor>;\n\ntemplate <typename T>\nusing BasicMatrixMap =\n    Eigen::TensorMap<Eigen::Tensor<T, 2, Eigen::RowMajor>, Eigen::Aligned>;\n\nusing Matrix = BasicMatrix<float>;\nusing MatrixMap = BasicMatrixMap<float>;\nusing CPUDevice = Eigen::ThreadPoolDevice;\nusing DSizes = Eigen::DSizes<Eigen::DenseIndex, 2>;\n\n// Two commonly used static dsizes. We use Eigen::type2index to allow as much\n// compile time optimization as possible.\n#ifdef EIGEN_HAS_INDEX_LIST\ninline Eigen::IndexList<Eigen::type2index<0>, Eigen::type2index<0>>\ndsizes_00() {\n  return Eigen::IndexList<Eigen::type2index<0>, Eigen::type2index<0>>();\n}\ninline Eigen::IndexList<Eigen::type2index<1>, Eigen::type2index<0>>\ndsizes_10() {\n  return Eigen::IndexList<Eigen::type2index<1>, Eigen::type2index<0>>();\n}\n#else\ninline DSizes dsizes_00() { return DSizes(0, 0); }\ninline DSizes dsizes_10() { return DSizes(1, 0); }\n#endif\n\n// Blocksizes\n// TODO(agarwal): compute these sizes based on cache sizes.\nconst int K = 64;\nconst int M = 64;\nconst int N = 128;\n\n// This stores a sparse representation of a slice of a matrix with size\n// (num_rows, num_cols). The slice is represented as a series of blocks of size\n// (num_rows, b), where b = block_size for all but the last block, which may\n// have fewer columns.\n//\n// num_rows and block_size are assumed to be <= 256. This allows storing\n// different indices as uint8.\n//\n// For each block, we store all the non zero entries in data/data3 vector and\n// the corresponding coordinates of the element in index/index3 vectors. index3\n// vector stores index of 3 elements in the same row so that these elements can\n// share the same row coordinate. Each entry in Index3 corresponds to 3 entries\n// in data3.\n//\n// Note that all the data/indices of all the blocks are stored in the same\n// vectors respectively. To identify block boundaries, we store the block\n// offsets using index3_offset/index_offset. If there are n blocks in the slice,\n// index3_offset and index_offset have n entries. The indices for the ith block\n// are the values in the following range:\n// [index3[index3_offset[i-1]], index3[index3_offset[i]]). Similarly for\n// index_offset.\ntemplate <typename T>\nstruct SparseSlice {\n  using ConstMatrixMap = BasicMatrixMap<const T>;\n\n public:\n  // Indices of three elements on the same row.\n  struct Index3 {\n    uint8 m;  // row\n    // columns\n    uint8 k1;\n    uint8 k2;\n    uint8 k3;\n  };\n\n  // Index of one element.\n  struct Index {\n    uint8 m;\n    uint8 k;\n  };\n\n  SparseSlice(int nrows, int ncols, int bsize)\n      : num_rows(nrows), num_cols(ncols), block_size(bsize) {\n    DCHECK_LE(nrows, 256);\n    DCHECK_LE(block_size, 256);\n  }\n\n  // Initializes the slice with data starting at mat(0, col_offset) and with\n  // size (num_rows, num_cols).\n  // If Transpose is true, implicitly transposes mat.\n  template <bool Transpose = false>\n  void Initialize(const ConstMatrixMap& mat, int col_offset);\n\n  void Clear();\n\n  // See comments above.\n  std::vector<int> index3_offset;\n  std::vector<Index3> index3;\n  std::vector<T> data3;\n\n  // See comments above. Similar to \"index3\" except that each element in \"index\"\n  // corresponds to one element in data.\n  std::vector<int> index_offset;\n  std::vector<Index> index;\n  std::vector<T> data;\n\n  // Number of rows and columns for the slice.\n  const int num_rows;\n  const int num_cols;\n\n  // Block size used to initialize from a matrix.\n  const int block_size;\n};\n\ntemplate <typename T>\nbool IsZero(T v);\n\ntemplate <>\nALWAYS_INLINE bool IsZero(bfloat16 v) {\n  return !static_cast<bool>(v);\n}\n\ntemplate <>\nALWAYS_INLINE bool IsZero(float v) {\n  return v == 0.0f;\n}\n\ntemplate <typename T>\ntemplate <bool Transpose>\nvoid SparseSlice<T>::Initialize(\n    const typename SparseSlice<T>::ConstMatrixMap& mat, int col_offset) {\n  const int mat_rows = Transpose ? mat.dimension(1) : mat.dimension(0);\n  const int mat_cols = Transpose ? mat.dimension(0) : mat.dimension(1);\n  DCHECK_LE(num_rows, mat_rows);\n  DCHECK_LE(num_cols + col_offset, mat_cols);\n\n  int num_blocks = (num_cols + block_size - 1) / block_size;\n  int mat_size = num_rows * num_cols;\n\n  index3_offset.reserve(num_blocks);\n  data3.reserve(mat_size);\n  index3.reserve(mat_size / 3);\n\n  index_offset.reserve(num_blocks);\n  data.reserve(num_blocks * num_rows * 2);\n  index.reserve(num_blocks * num_rows * 2);\n\n  Index3 idx3;\n  const int stride = Transpose ? mat.dimension(1) : 1;\n\n  for (int i = 0; i < num_blocks; ++i) {\n    int num_block_cols = std::min(block_size, num_cols - block_size * i);\n    for (int row = 0; row < num_rows; ++row) {\n      idx3.m = static_cast<uint8>(row);\n      // Safety note: The following code has a race, since it checks whether\n      // *curr is nonzero and then reads it again on use.  However, the result\n      // of the race is only that some of the \"nonzeros\" in the resulting sparse\n      // representation may actually be zero, which is harmless.\n      const auto* start =\n          Transpose ? &mat(col_offset, row) : &mat(row, col_offset);\n      const auto* curr = start;\n      const auto* end = start + stride * num_block_cols;\n      uint8 k = 0;\n#define NEXT_ELEM \\\n  curr += stride; \\\n  ++k;\n#define EAT_ZEROS                          \\\n  while (curr < end && IsZero<T>(*curr)) { \\\n    NEXT_ELEM;                             \\\n  }\n      while (true) {\n        EAT_ZEROS\n        if (curr >= end) break;\n        idx3.k1 = k;\n        const T value1 = *curr;\n        NEXT_ELEM;\n\n        EAT_ZEROS\n        if (curr >= end) {\n          data.push_back(value1);\n          index.push_back({idx3.m, idx3.k1});\n          break;\n        }\n        idx3.k2 = k;\n        const T value2 = *curr;\n        NEXT_ELEM;\n\n        EAT_ZEROS\n        if (curr >= end) {\n          data.push_back(value2);\n          index.push_back({idx3.m, idx3.k2});\n          data.push_back(value1);\n          index.push_back({idx3.m, idx3.k1});\n          break;\n        }\n        idx3.k3 = k;\n        data3.push_back(value1);\n        data3.push_back(value2);\n        data3.push_back(*curr);\n        NEXT_ELEM;\n        index3.push_back(idx3);\n#undef NEXT_ELEM\n#undef EAT_ZEROS\n      }\n    }\n    col_offset += block_size;\n    index3_offset.push_back(index3.size());\n    index_offset.push_back(index.size());\n  }\n  DCHECK_EQ(index3_offset.size(), num_blocks);\n  DCHECK_EQ(index_offset.size(), num_blocks);\n  DCHECK_EQ(3 * index3.size(), data3.size());\n  DCHECK_EQ(index.size(), data.size());\n}\n\ntemplate <typename T>\nvoid SparseSlice<T>::Clear() {\n  index3_offset.clear();\n  index3.clear();\n  data3.clear();\n  index_offset.clear();\n  index.clear();\n  data.clear();\n}\n\nusing Packet = Eigen::internal::packet_traits<float>::type;\nconst int kNumOperands = (sizeof(Packet) / sizeof(float));\n#define LOAD(x) Eigen::internal::pload<Packet>(x);\n#define EXPAND_BFLOAT_L(x, y) \\\n  const auto y = Eigen::internal::pexpand_bf16_l<Packet>(x);\n#define EXPAND_BFLOAT_U(x, y) \\\n  const auto y = Eigen::internal::pexpand_bf16_u<Packet>(x);\n#define STORE(x, y) Eigen::internal::pstore<float>(x, y);\n#define FMA(a, b, c, d) d = Eigen::internal::pmadd<Packet>(a, b, c);\n\nALWAYS_INLINE float ConvertBfloat16ToFloat(const bfloat16* src) {\n  float out = 0;\n  auto tmp = reinterpret_cast<bfloat16*>(&out);\n#if __BYTE_ORDER__ == __ORDER_BIG_ENDIAN__\n  tmp[0] = *src;\n#else\n  tmp[1] = *src;\n#endif\n  return out;\n}\n\nALWAYS_INLINE Packet ConvertFourBfloat16ToFloat(const bfloat16* src) {\n  return Eigen::internal::pload4bf16<Packet>(\n      reinterpret_cast<const float*>(src));\n}\n\nALWAYS_INLINE Packet ConvertTwoBfloat16ToFloat(const bfloat16* src) {\n  return Eigen::internal::pload2bf16<Packet>(\n      reinterpret_cast<const float*>(src));\n}\n\nALWAYS_INLINE void ScalarMulAdd(const float a, const float** inp, float** out) {\n  **out += a * **inp;\n  ++*inp;\n  ++*out;\n}\n\nALWAYS_INLINE void ScalarMulAdd(const float a, const bfloat16** inp,\n                                float** out) {\n  float inp_f = ConvertBfloat16ToFloat(*inp);\n  **out += a * inp_f;\n  ++*inp;\n  ++*out;\n}\nALWAYS_INLINE void ScalarMulAdd3Way(const float a1, const float a2,\n                                    const float a3, const bfloat16** inp1,\n                                    const bfloat16** inp2,\n                                    const bfloat16** inp3, float** out) {\n  float inp1_f = ConvertBfloat16ToFloat(*inp1);\n  float inp2_f = ConvertBfloat16ToFloat(*inp2);\n  float inp3_f = ConvertBfloat16ToFloat(*inp3);\n  **out += a1 * inp1_f + a2 * inp2_f + a3 * inp3_f;\n  ++*out;\n  ++*inp1;\n  ++*inp2;\n  ++*inp3;\n}\n\nALWAYS_INLINE void ScalarMulAdd3Way(const float a1, const float a2,\n                                    const float a3, const float** inp1,\n                                    const float** inp2, const float** inp3,\n                                    float** out) {\n  **out += a1 * **inp1 + a2 * **inp2 + a3 * **inp3;\n  ++*out;\n  ++*inp1;\n  ++*inp2;\n  ++*inp3;\n}\n\nALWAYS_INLINE void LoadSingleScalar(const bfloat16** data, Packet* l) {\n  auto tmp = ConvertBfloat16ToFloat(*data);\n  *l = Eigen::internal::pset1<Packet>(tmp);\n  ++*data;\n}\n\nALWAYS_INLINE void LoadTwoScalars(const bfloat16** data, Packet* l1,\n                                  Packet* l2) {\n  if (kNumOperands >= 2) {\n    auto tmp = ConvertTwoBfloat16ToFloat(*data);\n    *l1 = Eigen::internal::pbroadcast_first<Packet>(tmp);\n    *l2 = Eigen::internal::pbroadcast_second<Packet>(tmp);\n    *data += 2;\n  } else {\n    LoadSingleScalar(data, l1);\n    LoadSingleScalar(data, l2);\n  }\n}\n\nALWAYS_INLINE void LoadFourScalars(const bfloat16** data, Packet* l1,\n                                   Packet* l2, Packet* l3, Packet* l4) {\n  if (kNumOperands >= 4) {\n    auto tmp = ConvertFourBfloat16ToFloat(*data);\n    *l1 = Eigen::internal::pbroadcast_first<Packet>(tmp);\n    *l2 = Eigen::internal::pbroadcast_second<Packet>(tmp);\n    *l3 = Eigen::internal::pbroadcast_third<Packet>(tmp);\n    *l4 = Eigen::internal::pbroadcast_fourth<Packet>(tmp);\n    *data += 4;\n  } else {\n    LoadTwoScalars(data, l1, l2);\n    LoadTwoScalars(data, l3, l4);\n  }\n}\n\nALWAYS_INLINE void LoadSingleScalar(const float** data, Packet* l) {\n  *l = Eigen::internal::pload1<Packet>(*data);\n  ++(*data);\n}\n\nALWAYS_INLINE void LoadTwoScalars(const float** data, Packet* l1, Packet* l2) {\n  LoadSingleScalar(data, l1);\n  LoadSingleScalar(data, l2);\n}\n\nALWAYS_INLINE void LoadFourScalars(const float** data, Packet* l1, Packet* l2,\n                                   Packet* l3, Packet* l4) {\n  LoadTwoScalars(data, l1, l2);\n  LoadTwoScalars(data, l3, l4);\n}\n\ntemplate <typename T>\nALWAYS_INLINE void LoadThreeScalars(const T** data, Packet* l1, Packet* l2,\n                                    Packet* l3) {\n  LoadTwoScalars(data, l1, l2);\n  LoadSingleScalar(data, l3);\n}\n\ntemplate <typename T>\nALWAYS_INLINE void LoadSixScalars(const T** data, Packet* l1, Packet* l2,\n                                  Packet* l3, Packet* l4, Packet* l5,\n                                  Packet* l6) {\n  LoadFourScalars(data, l1, l2, l3, l4);\n  LoadTwoScalars(data, l5, l6);\n}\n\n// Vectorized version of ScalarMulAdd.\nALWAYS_INLINE void MulAdd(const Packet a, const bfloat16** binp, float** out) {\n  auto inp = reinterpret_cast<const float*>(*binp);\n  const auto b = LOAD(inp);\n  EXPAND_BFLOAT_L(b, b_0);\n  EXPAND_BFLOAT_U(b, b_1);\n  *binp += 2 * kNumOperands;\n  auto c1 = LOAD(*out);\n  auto c2 = LOAD(*out + kNumOperands);\n  FMA(a, b_0, c1, c1);\n  FMA(a, b_1, c2, c2);\n  STORE(*out, c1);\n  STORE(*out + kNumOperands, c2);\n  *out += 2 * kNumOperands;\n}\n\n// Vectorized version of ScalarMulAdd3Way.\nALWAYS_INLINE void MulAdd3Way(const Packet a1, const Packet a2, const Packet a3,\n                              const bfloat16** binp1, const bfloat16** binp2,\n                              const bfloat16** binp3, float** out) {\n  auto inp1 = reinterpret_cast<const float*>(*binp1);\n  auto inp2 = reinterpret_cast<const float*>(*binp2);\n  auto inp3 = reinterpret_cast<const float*>(*binp3);\n  auto c1 = LOAD(*out);\n  auto c2 = LOAD(*out + kNumOperands);\n  const auto b1 = LOAD(inp1);\n  EXPAND_BFLOAT_L(b1, b1_0);\n  EXPAND_BFLOAT_U(b1, b1_1);\n  *binp1 += 2 * kNumOperands;\n  const auto b2 = LOAD(inp2);\n  EXPAND_BFLOAT_L(b2, b2_0);\n  EXPAND_BFLOAT_U(b2, b2_1);\n  *binp2 += 2 * kNumOperands;\n  const auto b3 = LOAD(inp3);\n  EXPAND_BFLOAT_L(b3, b3_0);\n  EXPAND_BFLOAT_U(b3, b3_1);\n  *binp3 += 2 * kNumOperands;\n  FMA(a1, b1_0, c1, c1);\n  FMA(a1, b1_1, c2, c2);\n  FMA(a2, b2_0, c1, c1);\n  FMA(a2, b2_1, c2, c2);\n  FMA(a3, b3_0, c1, c1);\n  FMA(a3, b3_1, c2, c2);\n  STORE(*out, c1);\n  STORE(*out + kNumOperands, c2);\n  *out += 2 * kNumOperands;\n}\n\n// Unroll MulAdd3Way for two iterations\nALWAYS_INLINE void TwoMulAdd3Way(const Packet a1, const Packet a2,\n                                 const Packet a3, const bfloat16** binp1,\n                                 const bfloat16** binp2, const bfloat16** binp3,\n                                 float** out) {\n  auto inp1 = reinterpret_cast<const float*>(*binp1);\n  auto inp2 = reinterpret_cast<const float*>(*binp2);\n  auto inp3 = reinterpret_cast<const float*>(*binp3);\n  auto c1 = LOAD(*out);\n  auto c2 = LOAD(*out + kNumOperands);\n  const auto b1 = LOAD(inp1);\n  const auto b2 = LOAD(inp2);\n  const auto b3 = LOAD(inp3);\n\n  EXPAND_BFLOAT_L(b1, b1_0);\n  EXPAND_BFLOAT_U(b1, b1_1);\n  EXPAND_BFLOAT_L(b2, b2_0);\n  EXPAND_BFLOAT_U(b2, b2_1);\n  EXPAND_BFLOAT_L(b3, b3_0);\n  EXPAND_BFLOAT_U(b3, b3_1);\n  auto c3 = LOAD(*out + 2 * kNumOperands);\n  auto c4 = LOAD(*out + 3 * kNumOperands);\n  const auto b4 = LOAD(inp1 + kNumOperands);\n  const auto b5 = LOAD(inp2 + kNumOperands);\n  const auto b6 = LOAD(inp3 + kNumOperands);\n\n  EXPAND_BFLOAT_L(b4, b4_0);\n  EXPAND_BFLOAT_U(b4, b4_1);\n  EXPAND_BFLOAT_L(b5, b5_0);\n  EXPAND_BFLOAT_U(b5, b5_1);\n  EXPAND_BFLOAT_L(b6, b6_0);\n  EXPAND_BFLOAT_U(b6, b6_1);\n\n  FMA(a1, b1_0, c1, c1);\n  FMA(a1, b1_1, c2, c2);\n  FMA(a1, b4_0, c3, c3);\n  FMA(a1, b4_1, c4, c4);\n  FMA(a2, b2_0, c1, c1);\n  FMA(a2, b2_1, c2, c2);\n  FMA(a2, b5_0, c3, c3);\n  FMA(a2, b5_1, c4, c4);\n  FMA(a3, b3_0, c1, c1);\n  FMA(a3, b3_1, c2, c2);\n  FMA(a3, b6_0, c3, c3);\n  FMA(a3, b6_1, c4, c4);\n  STORE(*out, c1);\n  STORE(*out + kNumOperands, c2);\n  STORE(*out + 2 * kNumOperands, c3);\n  STORE(*out + 3 * kNumOperands, c4);\n  *out += 4 * kNumOperands;\n  *binp1 += 4 * kNumOperands;\n  *binp2 += 4 * kNumOperands;\n  *binp3 += 4 * kNumOperands;\n}\n\n// Apply MulAdd3Way on 128 operands.\nALWAYS_INLINE void MulAdd3Way128(const Packet a1, const Packet a2,\n                                 const Packet a3, const bfloat16** inp1,\n                                 const bfloat16** inp2, const bfloat16** inp3,\n                                 float** out) {\n  for (int k = 0; k < 128 / (8 * kNumOperands); ++k) {\n    TwoMulAdd3Way(a1, a2, a3, inp1, inp2, inp3, out);\n    TwoMulAdd3Way(a1, a2, a3, inp1, inp2, inp3, out);\n  }\n}\n\n// Vectorized version of ScalarMulAdd\nALWAYS_INLINE void MulAdd(const Packet a, const float** inp, float** out) {\n  const auto b = LOAD(*inp);\n  *inp += kNumOperands;\n  auto c = LOAD(*out);\n  FMA(a, b, c, c);\n  STORE(*out, c);\n  *out += kNumOperands;\n}\n\n// Vectorized version of ScalarMulAdd3Way\nALWAYS_INLINE void MulAdd3Way(const Packet a1, const Packet a2, const Packet a3,\n                              const float** inp1, const float** inp2,\n                              const float** inp3, float** out) {\n  auto c = LOAD(*out);\n  const auto b1 = LOAD(*inp1);\n  *inp1 += kNumOperands;\n  const auto b2 = LOAD(*inp2);\n  *inp2 += kNumOperands;\n  const auto b3 = LOAD(*inp3);\n  *inp3 += kNumOperands;\n  FMA(a1, b1, c, c);\n  FMA(a2, b2, c, c);\n  FMA(a3, b3, c, c);\n  STORE(*out, c);\n  *out += kNumOperands;\n}\n\n// Unroll MulAdd3Way for two iterations\nALWAYS_INLINE void TwoMulAdd3Way(const Packet a1, const Packet a2,\n                                 const Packet a3, const float** inp1,\n                                 const float** inp2, const float** inp3,\n                                 float** out) {\n  auto c1 = LOAD(*out);\n  const auto b1 = LOAD(*inp1);\n  const auto b2 = LOAD(*inp2);\n  const auto b3 = LOAD(*inp3);\n\n  auto c2 = LOAD(*out + kNumOperands);\n  const auto b4 = LOAD(*inp1 + kNumOperands);\n  const auto b5 = LOAD(*inp2 + kNumOperands);\n  const auto b6 = LOAD(*inp3 + kNumOperands);\n\n  FMA(a1, b1, c1, c1);\n  FMA(a1, b4, c2, c2);\n  FMA(a2, b2, c1, c1);\n  FMA(a2, b5, c2, c2);\n  FMA(a3, b3, c1, c1);\n  FMA(a3, b6, c2, c2);\n  STORE(*out, c1);\n  STORE(*out + kNumOperands, c2);\n  *out += 2 * kNumOperands;\n  *inp1 += 2 * kNumOperands;\n  *inp2 += 2 * kNumOperands;\n  *inp3 += 2 * kNumOperands;\n}\n\n// Unroll MulAdd3Way for four iterations\nALWAYS_INLINE void FourMulAdd3Way(const Packet a1, const Packet a2,\n                                  const Packet a3, const float** inp1,\n                                  const float** inp2, const float** inp3,\n                                  float** out) {\n  TwoMulAdd3Way(a1, a2, a3, inp1, inp2, inp3, out);\n  TwoMulAdd3Way(a1, a2, a3, inp1, inp2, inp3, out);\n}\n\n// Apply MulAdd3Way on 128 operands.\nALWAYS_INLINE void MulAdd3Way128(const Packet a1, const Packet a2,\n                                 const Packet a3, const float** inp1,\n                                 const float** inp2, const float** inp3,\n                                 float** out) {\n  if (kNumOperands == 8) {\n    FourMulAdd3Way(a1, a2, a3, inp1, inp2, inp3, out);\n    FourMulAdd3Way(a1, a2, a3, inp1, inp2, inp3, out);\n    FourMulAdd3Way(a1, a2, a3, inp1, inp2, inp3, out);\n    FourMulAdd3Way(a1, a2, a3, inp1, inp2, inp3, out);\n  } else {\n    DCHECK_LE(4 * kNumOperands, 128);\n    for (int i = 0; i < 128 / (4 * kNumOperands); ++i) {\n      MulAdd3Way(a1, a2, a3, inp1, inp2, inp3, out);\n      MulAdd3Way(a1, a2, a3, inp1, inp2, inp3, out);\n      MulAdd3Way(a1, a2, a3, inp1, inp2, inp3, out);\n      MulAdd3Way(a1, a2, a3, inp1, inp2, inp3, out);\n    }\n  }\n}\n// Computes product of \"left_slices\" with \"num_cols\" columns of \"right\", and\n// stores the output in *\"output\".\n// Note that left_slices is a list of SparseSlices, which are conceptually\n// assumed to be concatenated along the column dimension. Also each SparseSlice\n// is encoded as a list of blocks with upto N columns. See SparseSlice for more\n// details.\ntemplate <typename TL, typename TR, int Cols>\ninline void GEPP(\n    const std::vector<SparseSlice<TL>*>& left_slices,\n    const Eigen::TensorMap<Eigen::Tensor<const TR, 2, Eigen::RowMajor>,\n                           Eigen::Aligned>& right,\n    const int num_cols, Matrix* output) {\n  const int cols = (Cols == -1) ? num_cols : Cols;\n  DCHECK_EQ(num_cols, cols);\n  const int right_num_cols = right.dimension(1);\n  const int output_num_cols = output->dimension(1);\n  static const int kNumOperandsR = kNumOperands * sizeof(float) / sizeof(TR);\n  const int cols_mod = cols % kNumOperandsR;\n  int k_offset = 0;\n  // Pre-compute pointers for output matrix.\n  float* out_ptrs[M];\n  float* const out_start = &(*output)(0, 0);\n  for (int j = 0; j < M; ++j) {\n    out_ptrs[j] = out_start + output_num_cols * j;\n  }\n  for (const auto* left_slice : left_slices) {\n    const auto& left = *left_slice;\n    const auto* data3 = (!left.data3.empty()) ? &left.data3[0] : nullptr;\n    const auto* data = (!left.data.empty()) ? &left.data[0] : nullptr;\n    const int num_blocks = left.index3_offset.size();\n    int begin3 = 0;\n    int begin = 0;\n    for (int i = 0; i < num_blocks; ++i) {\n      // Pre-compute pointers for right matrix\n      const TR* right_ptrs[K];\n      const auto* const right_start = &right(k_offset, 0);\n      DCHECK_LT(k_offset, right.dimension(0));\n      for (int j = 0; j < K; ++j) {\n        right_ptrs[j] = right_start + right_num_cols * j;\n      }\n\n      const int end3 = left.index3_offset[i];\n      int j = begin3;\n      // Loop unrolled for 2 iterations.\n      for (; j + 1 < end3; j += 2) {\n        Packet l1, l2, l3, nl1, nl2, nl3;\n        LoadSixScalars(&data3, &l1, &l2, &l3, &nl1, &nl2, &nl3);\n        const auto& index = left.index3[j];\n        const auto& nindex = left.index3[j + 1];\n        float* out = out_ptrs[index.m];\n        float* nout = out_ptrs[nindex.m];\n        const auto* r1 = right_ptrs[index.k1];\n        const auto* r2 = right_ptrs[index.k2];\n        const auto* r3 = right_ptrs[index.k3];\n\n        const auto* nr1 = right_ptrs[nindex.k1];\n        const auto* nr2 = right_ptrs[nindex.k2];\n        const auto* nr3 = right_ptrs[nindex.k3];\n        if (cols == 128) {\n          MulAdd3Way128(l1, l2, l3, &r1, &r2, &r3, &out);\n          MulAdd3Way128(nl1, nl2, nl3, &nr1, &nr2, &nr3, &nout);\n        } else {\n          for (int n = 0; n < cols / kNumOperandsR; ++n) {\n            MulAdd3Way(l1, l2, l3, &r1, &r2, &r3, &out);\n            MulAdd3Way(nl1, nl2, nl3, &nr1, &nr2, &nr3, &nout);\n          }\n\n          const float sl1 = Eigen::internal::pfirst<Packet>(l1);\n          const float sl2 = Eigen::internal::pfirst<Packet>(l2);\n          const float sl3 = Eigen::internal::pfirst<Packet>(l3);\n          const float nsl1 = Eigen::internal::pfirst<Packet>(nl1);\n          const float nsl2 = Eigen::internal::pfirst<Packet>(nl2);\n          const float nsl3 = Eigen::internal::pfirst<Packet>(nl3);\n          for (int k = 0; k < cols_mod; ++k) {\n            ScalarMulAdd3Way(sl1, sl2, sl3, &r1, &r2, &r3, &out);\n            ScalarMulAdd3Way(nsl1, nsl2, nsl3, &nr1, &nr2, &nr3, &nout);\n          }\n        }\n      }\n      if (j < end3) {\n        Packet l1, l2, l3;\n        LoadThreeScalars(&data3, &l1, &l2, &l3);\n\n        const auto& index = left.index3[j];\n        float* out = out_ptrs[index.m];\n        const auto* r1 = right_ptrs[index.k1];\n        const auto* r2 = right_ptrs[index.k2];\n        const auto* r3 = right_ptrs[index.k3];\n        if (cols == 128) {\n          MulAdd3Way128(l1, l2, l3, &r1, &r2, &r3, &out);\n        } else {\n          for (int n = 0; n < cols / kNumOperandsR; ++n) {\n            MulAdd3Way(l1, l2, l3, &r1, &r2, &r3, &out);\n          }\n          const float sl1 = Eigen::internal::pfirst<Packet>(l1);\n          const float sl2 = Eigen::internal::pfirst<Packet>(l2);\n          const float sl3 = Eigen::internal::pfirst<Packet>(l3);\n          for (int k = 0; k < cols_mod; ++k) {\n            ScalarMulAdd3Way(sl1, sl2, sl3, &r1, &r2, &r3, &out);\n          }\n        }\n      }\n      begin3 = end3;\n      int end = left.index_offset[i];\n      // Loop unrolled for 4 iterations.\n      j = begin;\n      for (; j + 3 < end; j += 4) {\n        Packet l, nl, n2l, n3l;\n        LoadFourScalars(&data, &l, &nl, &n2l, &n3l);\n\n        const auto& index = left.index[j];\n        const auto& nindex = left.index[j + 1];\n        const auto& n2index = left.index[j + 2];\n        const auto& n3index = left.index[j + 3];\n        const auto* r = right_ptrs[index.k];\n        const auto* nr = right_ptrs[nindex.k];\n        const auto* n2r = right_ptrs[n2index.k];\n        const auto* n3r = right_ptrs[n3index.k];\n        float* out = out_ptrs[index.m];\n        float* nout = out_ptrs[nindex.m];\n        float* n2out = out_ptrs[n2index.m];\n        float* n3out = out_ptrs[n3index.m];\n\n        for (int n = 0; n < cols / kNumOperandsR; ++n) {\n          MulAdd(l, &r, &out);\n          MulAdd(nl, &nr, &nout);\n          MulAdd(n2l, &n2r, &n2out);\n          MulAdd(n3l, &n3r, &n3out);\n        }\n\n        const float sl1 = Eigen::internal::pfirst<Packet>(l);\n        const float sl2 = Eigen::internal::pfirst<Packet>(nl);\n        const float sl3 = Eigen::internal::pfirst<Packet>(n2l);\n        const float sl4 = Eigen::internal::pfirst<Packet>(n3l);\n        for (int k = 0; k < cols_mod; ++k) {\n          ScalarMulAdd(sl1, &r, &out);\n          ScalarMulAdd(sl2, &nr, &nout);\n          ScalarMulAdd(sl3, &n2r, &n2out);\n          ScalarMulAdd(sl4, &n3r, &n3out);\n        }\n      }\n      while (j < end) {\n        Packet l;\n        LoadSingleScalar(&data, &l);\n        const auto& index = left.index[j];\n        const auto* r = right_ptrs[index.k];\n        float* out = out_ptrs[index.m];\n        for (int n = 0; n < cols / kNumOperandsR; ++n) {\n          MulAdd(l, &r, &out);\n        }\n        const float sl = Eigen::internal::pfirst<Packet>(l);\n        for (int k = 0; k < cols_mod; ++k) {\n          ScalarMulAdd(sl, &r, &out);\n        }\n        j++;\n      }\n      k_offset += left.block_size;\n      begin = end;\n    }\n  }\n}\n\n#undef LOAD\n#undef EXPAND_BFLOAT_L\n#undef EXPAND_BFLOAT_U\n#undef STORE\n#undef FMA\n\n}  // namespace\n\ntemplate <typename TL, typename TR>\nclass SparseMatMul {\n  using MatrixL = BasicMatrix<TL>;\n  using MatrixR = BasicMatrix<TR>;\n  using ConstMatrixMapL = BasicMatrixMap<const TL>;\n  using ConstMatrixMapR = BasicMatrixMap<const TR>;\n  using MatrixMapR = BasicMatrixMap<TR>;\n\n public:\n  // Not used; added to match interface of LibxsmmSparseMatMul\n  struct TensorInfoCache {};\n\n  // Perform matrix multiplication of \"left\" and \"right\", and store the result\n  // in *\"output\".\n public:\n  static inline void Compute(TensorInfoCache* cache,\n                             const ConstMatrixMapL& left,\n                             const ConstMatrixMapR& right, bool transpose_left,\n                             const DeviceBase::CpuWorkerThreads* thread_pool,\n                             bool transpose_output, MatrixMap* output);\n\n private:\n  // Computes multiplication of left and num_cols columns of right, and stores\n  // the output block in *\"output\" at offsets \"output_row_offset\" and\n  // \"output_col_offset\". If assign is true, assigns the value to that block,\n  // else adds the values to the existing values.\n  static inline void ComputeOutputBlock(\n      const std::vector<SparseSlice<TL>*>& left, const ConstMatrixMapR& right,\n      int num_cols, int output_row_offset, int output_col_offset, bool assign,\n      bool transpose_output, MatrixMap* output);\n\n  // Encodes \"mat\" using a sparse representation and stores that in\n  // \"mat_slices\". \"mat\" is broken into a grid with sizes \"slice_num_rows\" and\n  // \"slice_num_cols\", each grid element is converted into a SparseSlice and\n  // stored in mat_slices. \"slice_block_size\" is used to perform further column\n  // blocking of each slice.\n  static inline std::unique_ptr<BlockingCounter> CreateSparseSlices(\n      const ConstMatrixMapL& mat, bool transpose, int slice_num_rows,\n      int slice_block_size, int slice_num_cols,\n      std::vector<std::vector<SparseSlice<TL>*>>* mat_slices,\n      const DeviceBase::CpuWorkerThreads* thread_pool);\n\n  // This function chops \"mat\" along column dimension into pieces with at most N\n  // columns, and concatenates the pieces one after the other in \"buffer\". It\n  // returns the list of the pieces in \"slices\". It returns a BlockingCounter\n  // which should be used to wait for the shuffle operations to complete.\n  static inline std::unique_ptr<BlockingCounter> CreateDenseSlices(\n      const ConstMatrixMapR& mat, int row_start, int num_rows, int col_start,\n      int num_cols, const DeviceBase::CpuWorkerThreads* thread_pool,\n      MatrixR* buffer, std::vector<ConstMatrixMapR*>* slices);\n\n  // Helper function for CreateDenseSlices to move the data around. It returns a\n  // BlockingCounter which should be used to wait for the shuffle operations to\n  // complete.\n  static inline BlockingCounter* ShuffleMatrix(\n      const ConstMatrixMapR& mat, int slice_row_start, int slice_num_rows,\n      int slice_col_start, int slice_num_cols, const int N,\n      const DeviceBase::CpuWorkerThreads* thread_pool, MatrixR* buffer);\n\n  // Helper function for CreateDenseSlices to create slices.\n  static inline void SliceMatrix(const MatrixR& mat, const int num_rows,\n                                 const int num_slices,\n                                 std::vector<ConstMatrixMapR*>* slices);\n\n  // Heuristics to compute various block sizes.\n  // KR, NR: block sizes for \"right\". We run blocking iterations that operate on\n  // matrices with at most this size.\n  // KL: grid size along the column dimension used while encoding left.\n  // IB, JB: number of left and right slices to multiply together. This is used\n  // for ordering different ComputeBlockOutput operations inside each blocking\n  // iteration so as to potentially reduce the working set size.\n  static inline void ComputeBlockSizes(const ConstMatrixMapL& left,\n                                       const ConstMatrixMapR& right,\n                                       bool transpose_left, int num_threads,\n                                       int* KR, int* NR, int* KL, int* JB,\n                                       int* IB);\n\n  TF_DISALLOW_COPY_AND_ASSIGN(SparseMatMul);\n};\n\n#ifdef TENSORFLOW_USE_LIBXSMM\ntemplate <typename TL, typename TR>\nclass LibxsmmSparseMatMul {\n  using MatrixL = BasicMatrix<TL>;\n  using MatrixR = BasicMatrix<TR>;\n  using ConstMatrixMapL = BasicMatrixMap<const TL>;\n  using ConstMatrixMapR = BasicMatrixMap<const TR>;\n  using MatrixMapR = BasicMatrixMap<TR>;\n\n public:\n  // This structure contains a set of libxsmm kernels for sizes that have been\n  // encountered previously by this operator so that libxsmm does not need to\n  // reallocate its scratchpad memory each time (which hurts performance\n  // substantially).\n  struct TensorInfoCache {\n    struct TensorInfoCacheEntry {\n      // Parameters for kernel\n      int M;\n      int K;\n      int N;\n      int max_threads;\n      // libxsmm handle and matrix data\n      libxsmm_spmdm_handle handle;\n      libxsmm_CSR_sparseslice* output_csr;\n      // Chain to non-libxsmm implementation's cache in case that ever becomes\n      // useful (it is an empty struct right now)\n      typename SparseMatMul<TL, TR>::TensorInfoCache\n          non_libxsmm_cache;  // Currently not used\n    };\n    // protects entries; invariant: entries is a valid std::multimap\n    tensorflow::mutex lock;\n    // Because there could be multiple matrix multiplies with the same sizes\n    // going on at the same time, we need to allow multiple cache entries for a\n    // given set of parameters. Taking and returning entries is used to make\n    // sure the same cache entry is not used from two threads at a time.\n    std::multimap<std::tuple<int, int, int, int>,\n                  std::unique_ptr<TensorInfoCacheEntry>>\n        entries TF_GUARDED_BY(lock);\n\n    TensorInfoCache() : lock(), entries() {}\n    // Look up and remove first entry with these parameters, creating one if\n    // there isn't one\n    std::unique_ptr<TensorInfoCacheEntry> take_cache_entry(int M, int K, int N,\n                                                           int max_threads)\n        TF_LOCKS_EXCLUDED(lock) {\n      tensorflow::mutex_lock ml(lock);\n      auto key = std::make_tuple(M, K, N, max_threads);\n      auto it = entries.find(key);\n      if (it != entries.end()) {\n        auto val = std::move(it->second);\n        entries.erase(it);\n        return val;\n      } else {\n        std::unique_ptr<TensorInfoCacheEntry> e{\n            new TensorInfoCacheEntry{M, K, N, max_threads, {}, nullptr}};\n        // setup scoped allocator, which uses cpu_allocator() for this scope\n        const libxsmm_tf_allocator<libxsmm_scratch_allocator> tf_allocator;\n        libxsmm_spmdm_init(M, N, K, max_threads, &e->handle, &e->output_csr);\n        return e;\n      }\n    }\n    // Add a cache entry with certain parameters\n    void return_cache_entry(std::unique_ptr<TensorInfoCacheEntry> e)\n        TF_LOCKS_EXCLUDED(lock) {\n      tensorflow::mutex_lock ml(lock);\n      auto key = std::make_tuple(e->M, e->K, e->N, e->max_threads);\n      entries.insert(std::make_pair(key, std::move(e)));\n    }\n    ~TensorInfoCache() {\n      tensorflow::mutex_lock ml(lock);\n      for (auto& p : entries) {\n        libxsmm_spmdm_destroy(&p.second->handle);\n      }\n      entries.clear();\n    }\n\n   private:\n    TF_DISALLOW_COPY_AND_ASSIGN(TensorInfoCache);\n  };\n\n  // Perform matrix multiplication of \"left\" and \"right\", and store the result\n  // in *\"output\".\n public:\n  static inline void Compute(TensorInfoCache* cache,\n                             const ConstMatrixMapL& left,\n                             const ConstMatrixMapR& right, bool transpose_left,\n                             const DeviceBase::CpuWorkerThreads* thread_pool,\n                             bool transpose_output, MatrixMap* output);\n\n private:\n  TF_DISALLOW_COPY_AND_ASSIGN(LibxsmmSparseMatMul);\n};\n#endif\n\ntemplate <typename TL, typename TR,\n          template <typename TL2, typename TR2> class DoMatMul>\nclass SparseMatMulOp : public OpKernel {\n  using MatrixR = BasicMatrix<TR>;\n  using ConstMatrixMapR = BasicMatrixMap<const TR>;\n\n public:\n  explicit SparseMatMulOp(OpKernelConstruction* ctx) : OpKernel(ctx) {\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"transpose_a\", &transpose_a_));\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"transpose_b\", &transpose_b_));\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"a_is_sparse\", &a_is_sparse_));\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"b_is_sparse\", &b_is_sparse_));\n  }\n\n  void Compute(OpKernelContext* ctx) override {\n    const Tensor& a = ctx->input(0);\n    const Tensor& b = ctx->input(1);\n    OP_REQUIRES(ctx, TensorShapeUtils::IsMatrix(a.shape()),\n                errors::InvalidArgument(\"a is not a matrix\"));\n    OP_REQUIRES(ctx, TensorShapeUtils::IsMatrix(b.shape()),\n                errors::InvalidArgument(\"b is not a matrix\"));\n\n    const int m = transpose_a_ ? a.dim_size(1) : a.dim_size(0);\n    const int k = transpose_a_ ? a.dim_size(0) : a.dim_size(1);\n    const int n = transpose_b_ ? b.dim_size(0) : b.dim_size(1);\n    const int k2 = transpose_b_ ? b.dim_size(1) : b.dim_size(0);\n\n    OP_REQUIRES(ctx, k == k2,\n                errors::InvalidArgument(\n                    \"Matrix size incompatible: a: \", a.shape().DebugString(),\n                    \", b: \", b.shape().DebugString()));\n    OP_REQUIRES(ctx, m >= 0 && n >= 0 && k >= 0,\n                errors::InvalidArgument(\n                    \"Matrix dimensions cannot be negative: a: \",\n                    a.shape().DebugString(), \", b: \", b.shape().DebugString()));\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, TensorShape({m, n}), &output));\n\n    // Return early if at least one of the output dimension size is 0.\n    if (m == 0 || n == 0) {\n      return;\n    }\n\n    if (k == 0) {\n      // If the inner dimension k in the matrix multiplication is zero, we fill\n      // the output with zeros.\n      functor::SetZeroFunctor<CPUDevice, float> f;\n      f(ctx->eigen_device<CPUDevice>(), output->flat<float>());\n      return;\n    }\n\n    auto out = output->matrix<float>();\n\n    std::unique_ptr<Tensor> a_float;\n    std::unique_ptr<Tensor> b_float;\n    if (!a_is_sparse_ && !b_is_sparse_) {\n      auto left = &a;\n      auto right = &b;\n      // TODO(agarwal): multi-thread the conversions from bfloat16 to float.\n      if (std::is_same<TL, bfloat16>::value) {\n        a_float.reset(new Tensor(DT_FLOAT, a.shape()));\n        BFloat16ToFloat(a.flat<bfloat16>().data(),\n                        a_float->flat<float>().data(), a.NumElements());\n        left = a_float.get();\n      }\n      if (std::is_same<TR, bfloat16>::value) {\n        b_float.reset(new Tensor(DT_FLOAT, b.shape()));\n        BFloat16ToFloat(b.flat<bfloat16>().data(),\n                        b_float->flat<float>().data(), b.NumElements());\n        right = b_float.get();\n      }\n      Eigen::array<Eigen::IndexPair<Eigen::DenseIndex>, 1> dim_pair;\n      dim_pair[0].first = transpose_a_ ? 0 : 1;\n      dim_pair[0].second = transpose_b_ ? 1 : 0;\n\n      out.device(ctx->template eigen_device<CPUDevice>()) =\n          left->matrix<float>().contract(right->matrix<float>(), dim_pair);\n      return;\n    }\n\n    auto left = &a;\n    auto right = &b;\n    bool transpose_output = false;\n    bool transpose_a = transpose_a_;\n    bool transpose_b = transpose_b_;\n    if (!a_is_sparse_) {\n      // Swap the order of multiplications using the identity:\n      // A * B = (B' *  A')'.\n      std::swap(left, right);\n      std::swap(transpose_a, transpose_b);\n      transpose_a = !transpose_a;\n      transpose_b = !transpose_b;\n      transpose_output = !transpose_output;\n    }\n\n    std::unique_ptr<Tensor> right_tr;\n    if (transpose_b) {\n      // TODO(agarwal): avoid transposing the matrix here and directly handle\n      // transpose in CreateDenseSlices.\n      OP_REQUIRES(ctx, right->dim_size(0) != 0,\n                  errors::InvalidArgument(\"b has an entry 0 in it's shape.\"));\n      OP_REQUIRES(ctx, right->dim_size(1) != 0,\n                  errors::InvalidArgument(\"b has an entry 0 in it's shape.\"));\n      right_tr.reset(\n          new Tensor(right->dtype(),\n                     TensorShape({right->dim_size(1), right->dim_size(0)})));\n\n      const auto perm = dsizes_10();\n      if (transpose_output) {\n        right_tr->matrix<TL>().device(ctx->template eigen_device<CPUDevice>()) =\n            right->matrix<TL>().shuffle(perm);\n      } else {\n        right_tr->matrix<TR>().device(ctx->template eigen_device<CPUDevice>()) =\n            right->matrix<TR>().shuffle(perm);\n      }\n      right = right_tr.get();\n    }\n\n    if (transpose_output) {\n      DoMatMul<TR, TL>::Compute(&this->cache_tr_, left->matrix<TR>(),\n                                right->matrix<TL>(), transpose_a,\n                                ctx->device()->tensorflow_cpu_worker_threads(),\n                                transpose_output, &out);\n    } else {\n      DoMatMul<TL, TR>::Compute(&this->cache_nt_, left->matrix<TL>(),\n                                right->matrix<TR>(), transpose_a,\n                                ctx->device()->tensorflow_cpu_worker_threads(),\n                                transpose_output, &out);\n    }\n  }\n\n private:\n  bool transpose_a_;\n  bool transpose_b_;\n  bool a_is_sparse_;\n  bool b_is_sparse_;\n\n  // Cache for non-transposed-output multiply\n  typename DoMatMul<TL, TR>::TensorInfoCache cache_nt_;\n  // Cache for transposed-output multiply\n  typename DoMatMul<TR, TL>::TensorInfoCache cache_tr_;\n\n  TF_DISALLOW_COPY_AND_ASSIGN(SparseMatMulOp);\n};\n\ntemplate <typename TL, typename TR>\ninline void SparseMatMul<TL, TR>::ComputeOutputBlock(\n    const std::vector<SparseSlice<TL>*>& left,\n    const typename SparseMatMul<TL, TR>::ConstMatrixMapR& right, int num_cols,\n    int output_row_offset, int output_col_offset, bool assign,\n    bool transpose_output, MatrixMap* output) {\n  const auto perm = dsizes_10();\n  int num_rows = left[0]->num_rows;\n  const int rhs_num_cols = right.dimension(1);\n  DCHECK_LE(num_cols, rhs_num_cols);\n  Matrix out(num_rows, rhs_num_cols);\n  out.setZero();\n  if (num_cols == N) {\n    GEPP<TL, TR, N>(left, right, num_cols, &out);\n  } else {\n    GEPP<TL, TR, -1>(left, right, num_cols, &out);\n  }\n  if (!assign) {\n    const DSizes begin(output_row_offset, output_col_offset);\n    const DSizes sizes(num_rows, num_cols);\n    if (transpose_output) {\n      if (num_cols == rhs_num_cols) {\n        output->shuffle(perm).slice(begin, sizes) += out;\n      } else {\n        const auto zero = dsizes_00();\n        output->shuffle(perm).slice(begin, sizes) += out.slice(zero, sizes);\n      }\n    } else {\n      if (num_cols == rhs_num_cols) {\n        output->slice(begin, sizes) += out;\n      } else {\n        const auto zero = dsizes_00();\n        output->slice(begin, sizes) += out.slice(zero, sizes);\n      }\n    }\n  } else {\n    std::unique_ptr<Matrix> out_tr;\n    if (transpose_output) {\n      out_tr.reset(new Matrix(rhs_num_cols, num_rows));\n      *out_tr = out.shuffle(perm);\n      std::swap(output_row_offset, output_col_offset);\n      std::swap(num_rows, num_cols);\n    }\n    const Matrix& final_out = transpose_output ? *out_tr : out;\n    for (int i = 0; i < num_rows; ++i) {\n      memcpy(&(*output)(output_row_offset + i, output_col_offset),\n             &final_out(i, 0), num_cols * sizeof(float));\n    }\n  }\n}\n\ntemplate <typename TL, typename TR>\ninline std::unique_ptr<BlockingCounter>\nSparseMatMul<TL, TR>::CreateSparseSlices(\n    const typename SparseMatMul<TL, TR>::ConstMatrixMapL& mat, bool transpose,\n    int slice_num_rows, int slice_block_size, int slice_num_cols,\n    std::vector<std::vector<SparseSlice<TL>*>>* mat_slices,\n    const DeviceBase::CpuWorkerThreads* thread_pool) {\n  const int mat_num_rows = transpose ? mat.dimension(1) : mat.dimension(0);\n  const int mat_num_cols = transpose ? mat.dimension(0) : mat.dimension(1);\n  const int num_slices_dim0 =\n      std::max(1, (mat_num_rows + slice_num_rows - 1) / slice_num_rows);\n  const int num_slices_dim1 =\n      std::max(1, (mat_num_cols + slice_num_cols - 1) / slice_num_cols);\n  mat_slices->resize(num_slices_dim0);\n  BlockingCounter* counter =\n      new BlockingCounter(num_slices_dim0 * num_slices_dim1);\n  auto work = [counter, transpose](SparseSlice<TL>* sparse_slice,\n                                   SparseMatMul<TL, TR>::ConstMatrixMapL* slice,\n                                   int col_offset) {\n    if (transpose) {\n      sparse_slice->template Initialize<true>(*slice, col_offset);\n    } else {\n      sparse_slice->template Initialize<false>(*slice, col_offset);\n    }\n    delete slice;\n    counter->DecrementCount();\n  };\n  for (int i = 0; i < num_slices_dim0; ++i) {\n    (*mat_slices)[i].resize(num_slices_dim1);\n    int num_rows =\n        std::min<int>(slice_num_rows, mat_num_rows - i * slice_num_rows);\n    for (int j = 0; j < num_slices_dim1; ++j) {\n      int num_cols =\n          std::min<int>(slice_num_cols, mat_num_cols - j * slice_num_cols);\n      SparseMatMul<TL, TR>::ConstMatrixMapL* slice = nullptr;\n      if (transpose) {\n        slice = new SparseMatMul<TL, TR>::ConstMatrixMapL(\n            &mat(0, i * slice_num_rows), mat.dimensions());\n      } else {\n        DSizes d(num_rows, mat_num_cols);\n        slice = new SparseMatMul<TL, TR>::ConstMatrixMapL(\n            &mat(i * slice_num_rows, 0), d);\n      }\n      auto* sparse_slice =\n          new SparseSlice<TL>(num_rows, num_cols, slice_block_size);\n      (*mat_slices)[i][j] = sparse_slice;\n      thread_pool->workers->Schedule(\n          [=]() { work(sparse_slice, slice, slice_num_cols * j); });\n    }\n  }\n  return std::unique_ptr<BlockingCounter>(counter);\n}\n#define LOAD(x) Eigen::internal::ploadu<Packet>((x));\n#define INTERLEAVE(x) Eigen::internal::pinterleave4x64<Packet>(x);\n#define STORE(x, y) Eigen::internal::pstoreu<float>(x, y);\n\ntemplate <int NUM_ELEM = -1>\nALWAYS_INLINE void CopyAndMayBeInterleaveBfloat16(void* bdst, const void* bsrc,\n                                                  int num_elements) {\n  DCHECK_GE(kNumOperands, 8);\n  static const int kStep = kNumOperands * sizeof(float) / sizeof(bfloat16);\n  const int num = (NUM_ELEM == -1) ? num_elements : NUM_ELEM;\n  DCHECK_EQ(num, num_elements);\n  const float* src = reinterpret_cast<const float*>(bsrc);\n  float* dst = reinterpret_cast<float*>(bdst);\n  for (int index = 0; index + kStep <= num; index += kStep) {\n    auto in = LOAD(src);\n    auto tmp = INTERLEAVE(in);\n    STORE(dst, tmp);\n    src += kNumOperands;\n    dst += kNumOperands;\n  }\n  if (num % kStep != 0) {\n    memcpy(dst, src, (num % kStep) * sizeof(bfloat16));\n  }\n}\n\ntemplate <typename T>\nALWAYS_INLINE void CopyAndMayBeInterleave(void* dst, const void* src,\n                                          int num_elements) {\n  if (std::is_same<T, float>::value || kNumOperands < 8) {\n    memcpy(dst, src, num_elements * sizeof(T));\n  } else if (std::is_same<T, bfloat16>::value) {\n    if (num_elements == N) {\n      CopyAndMayBeInterleaveBfloat16<N>(dst, src, num_elements);\n    } else {\n      CopyAndMayBeInterleaveBfloat16<-1>(dst, src, num_elements);\n    }\n  } else {\n    LOG(FATAL) << \"Unsupported type\";\n  }\n}\n\n#undef LOAD\n#undef Interleave\n#undef Store\n\ntemplate <typename TL, typename TR>\ninline BlockingCounter* SparseMatMul<TL, TR>::ShuffleMatrix(\n    const typename SparseMatMul<TL, TR>::ConstMatrixMapR& mat,\n    int slice_row_start, int slice_num_rows, int slice_col_start,\n    int slice_num_cols, const int N,\n    const DeviceBase::CpuWorkerThreads* thread_pool, MatrixR* buffer) {\n  DCHECK_EQ(N % 2, 0);\n  DCHECK_LE(kNumOperands * sizeof(float) / sizeof(TR), N);\n  // Note(nikhilsarda): This heuristic is optimal in benchmarks as of\n  // Jan 21, 2020.\n  int num_threads = std::min(thread_pool->num_threads, 8);\n  BlockingCounter* counter = new BlockingCounter(num_threads);\n  DCHECK_EQ(N, buffer->dimension(1));\n  auto shuffle_work = [&mat, slice_row_start, slice_num_rows, slice_col_start,\n                       slice_num_cols, N, buffer, counter](int s, int e) {\n    const int row_start = s % slice_num_rows + slice_row_start;\n    const int col_start = s / slice_num_rows * N + slice_col_start;\n    auto* out_start = &(*buffer)(s, 0);\n    const auto* input_start = &mat(row_start, col_start);\n    const auto* input_end = &mat(slice_row_start + slice_num_rows - 1,\n                                 slice_col_start + slice_num_cols - 1);\n    const int mat_num_cols = mat.dimension(1);\n    const int row_slice_size = slice_num_rows * mat_num_cols;\n\n    const int aligned_end = slice_num_cols / N * slice_num_rows;\n    const int e1 = std::min(e, aligned_end);\n    while (s < e1) {\n      CopyAndMayBeInterleave<TR>(out_start, input_start, N);\n      out_start += N;\n      input_start += mat_num_cols;\n      if (input_start > input_end) {\n        input_start = input_start - row_slice_size + N;\n      }\n      ++s;\n    }\n    int s1 = std::max(s, aligned_end);\n    const int copy_num_cols = slice_num_cols % N;\n    while (s1 < e) {\n      CopyAndMayBeInterleave<TR>(out_start, input_start, copy_num_cols);\n      out_start += N;\n      input_start += mat_num_cols;\n      ++s1;\n    }\n    if (counter) counter->DecrementCount();\n  };\n\n  int start = 0;\n  int end = 0;\n  int num_out_rows = (slice_num_cols + N - 1) / N * slice_num_rows;\n  DCHECK_LE(num_out_rows, buffer->dimension(0));\n  for (int i = std::max(1, num_threads); i > 0; --i) {\n    end = start + num_out_rows / i;\n    thread_pool->workers->Schedule([=]() { shuffle_work(start, end); });\n    num_out_rows -= (end - start);\n    start = end;\n  }\n  return counter;\n}\n\ntemplate <typename TL, typename TR>\ninline void SparseMatMul<TL, TR>::SliceMatrix(\n    const MatrixR& mat, const int num_rows, const int num_slices,\n    std::vector<typename SparseMatMul<TL, TR>::ConstMatrixMapR*>* slices) {\n  slices->resize(num_slices);\n  DSizes d(num_rows, mat.dimension(1));\n  DCHECK_LE(num_rows * num_slices, mat.dimension(0));\n  for (int i = 0; i < num_slices; ++i) {\n    (*slices)[i] = new ConstMatrixMapR(&mat(i * num_rows, 0), d);\n  }\n}\n\ntemplate <typename TL, typename TR>\ninline std::unique_ptr<BlockingCounter> SparseMatMul<TL, TR>::CreateDenseSlices(\n    const typename SparseMatMul<TL, TR>::ConstMatrixMapR& mat, int row_start,\n    int num_rows, int col_start, int num_cols,\n    const DeviceBase::CpuWorkerThreads* thread_pool, MatrixR* buffer,\n    std::vector<typename SparseMatMul<TL, TR>::ConstMatrixMapR*>* slices) {\n  std::unique_ptr<BlockingCounter> shuffle_counter(ShuffleMatrix(\n      mat, row_start, num_rows, col_start, num_cols, N, thread_pool, buffer));\n  const int num_slices = (num_cols + N - 1) / N;\n  SliceMatrix(*buffer, num_rows, num_slices, slices);\n  return shuffle_counter;\n}\n\ntemplate <typename TL, typename TR>\ninline void SparseMatMul<TL, TR>::ComputeBlockSizes(\n    const typename SparseMatMul<TL, TR>::ConstMatrixMapL& left,\n    const typename SparseMatMul<TL, TR>::ConstMatrixMapR& right,\n    bool transpose_left, int num_threads, int* KR, int* NR, int* KL, int* JB,\n    int* IB) {\n  // Heuristics for calculating block sizes\n  // Assume two hyperthreads per core.\n  const int est_num_cores = std::max(1, (num_threads + 1) / 2);\n  // Use block of rhs with at most 128K floats per core.\n  const int mem = est_num_cores * 128 * 1024;\n  *KR = std::min(static_cast<int>(right.dimension(0)), mem / 256);\n  *NR = right.dimension(1);\n  if (*KR * *NR > mem) {\n    // 4096 may be enough to amortize the cost of writes.\n    *KR = std::min<int>(*KR, 4096);\n  }\n  // Use sizes that are multiples of K and 256.\n  *KR = std::max(1, *KR / K) * K;\n  *NR = std::max(1, *NR / 256) * 256;\n  if (*KR * *NR > mem) {\n    *NR = mem / *KR;\n  }\n  *NR = std::max(1, *NR / 256) * 256;\n\n  const int left_dim0 = transpose_left ? left.dimension(1) : left.dimension(0);\n  const int left_dim1 = transpose_left ? left.dimension(0) : left.dimension(1);\n  for (*KL = 1024; *KL > K; *KL /= 2) {\n    if (*KR % *KL == 0 &&\n        std::max<int>(1, left_dim0 / 64) * (left_dim1 / *KL) > est_num_cores) {\n      break;\n    }\n  }\n  DCHECK_EQ(*KL % K, 0);\n  DCHECK_GE(*KR, *KL);\n  if (*KR < right.dimension(0)) {\n    CHECK_EQ(*KR % *KL, 0);\n  }\n\n  *JB = std::max(1, static_cast<int>(sqrt(num_threads) / 2.0));\n  *IB = 8 * *JB;\n  DCHECK_EQ(N * sizeof(float) % 64, size_t{0});\n}\n\n#ifdef TENSORFLOW_USE_LIBXSMM\n\ntemplate <typename F>\nvoid do_on_all_threads(const DeviceBase::CpuWorkerThreads* thread_pool,\n                       const F& f) {\n  int num_threads = thread_pool->num_threads;\n  if (num_threads == 0) {\n    LOG(FATAL) << \"Have 0 threads in thread pool\";\n  } else if (num_threads == 1) {\n    f(0);\n  } else {\n    BlockingCounter counter(num_threads - 1);\n    for (int i = 1; i < num_threads; ++i) {\n      thread_pool->workers->Schedule([&, i]() {\n        f(i);\n        counter.DecrementCount();\n      });\n    }\n    f(0);\n    counter.Wait();\n  }\n}\n\ntemplate <typename T>\nstruct empty_type_wrapper {};\n\n// Copies of interface to libxsmm_spmdm_createSparseSlice_*_notrans_thread to\n// allow overloading\nvoid wrapper_libxsmm_spmdm_createSparseSlice_generic_thread(\n    empty_type_wrapper<float>, const libxsmm_spmdm_handle* handle, char transA,\n    const float* A, libxsmm_CSR_sparseslice* libxsmm_output_csr_a, int block_id,\n    int tid, int nthreads) {\n  return libxsmm_spmdm_createSparseSlice_fp32_thread(\n      handle, transA, A, libxsmm_output_csr_a, block_id, tid, nthreads);\n}\nvoid wrapper_libxsmm_spmdm_createSparseSlice_generic_thread(\n    empty_type_wrapper<bfloat16>, const libxsmm_spmdm_handle* handle,\n    char transA, const bfloat16* A,\n    libxsmm_CSR_sparseslice* libxsmm_output_csr_a, int block_id, int tid,\n    int nthreads) {\n  return libxsmm_spmdm_createSparseSlice_bfloat16_thread(\n      handle, transA, reinterpret_cast<const libxsmm_bfloat16*>(A),\n      libxsmm_output_csr_a, block_id, tid, nthreads);\n}\n\nvoid wrapper_libxsmm_spmdm_compute_generic_thread(\n    empty_type_wrapper<bfloat16>, const libxsmm_spmdm_handle* handle,\n    char transA, char transB, const bfloat16* alpha,\n    libxsmm_CSR_sparseslice* A_sparse, const bfloat16* B, char transC,\n    const bfloat16* beta, float* C, int block_id, int tid, int nthreads) {\n  return libxsmm_spmdm_compute_bfloat16_thread(\n      handle, transA, transB, reinterpret_cast<const libxsmm_bfloat16*>(alpha),\n      A_sparse, reinterpret_cast<const libxsmm_bfloat16*>(B), transC,\n      reinterpret_cast<const libxsmm_bfloat16*>(beta), C, block_id, tid,\n      nthreads);\n}\nvoid wrapper_libxsmm_spmdm_compute_generic_thread(\n    empty_type_wrapper<float>, const libxsmm_spmdm_handle* handle, char transA,\n    char transB, const float* alpha, libxsmm_CSR_sparseslice* A_sparse,\n    const float* B, char transC, const float* beta, float* C, int block_id,\n    int tid, int nthreads) {\n  return libxsmm_spmdm_compute_fp32_thread(handle, transA, transB, alpha,\n                                           A_sparse, B, transC, beta, C,\n                                           block_id, tid, nthreads);\n}\n\ntemplate <typename TL, typename TR>\ninline void LibxsmmSparseMatMul<TL, TR>::Compute(\n    typename LibxsmmSparseMatMul<TL, TR>::TensorInfoCache* cache,\n    const typename LibxsmmSparseMatMul<TL, TR>::ConstMatrixMapL& left,\n    const typename LibxsmmSparseMatMul<TL, TR>::ConstMatrixMapR& right,\n    bool transpose_left, const DeviceBase::CpuWorkerThreads* thread_pool,\n    bool transpose_output, MatrixMap* output) {\n  const int num_threads = thread_pool->num_threads;\n  const int left_dim0 = transpose_left ? left.dimension(1) : left.dimension(0);\n  const int left_dim1 = transpose_left ? left.dimension(0) : left.dimension(1);\n  const int right_dim0 = right.dimension(0);\n  const int right_dim1 = right.dimension(1);\n  CHECK_EQ(left_dim1, right_dim0);\n  CHECK_EQ(left_dim0,\n           (transpose_output ? output->dimension(1) : output->dimension(0)));\n  CHECK_EQ(right_dim1,\n           (transpose_output ? output->dimension(0) : output->dimension(1)));\n#if 0  // this issue seems to be resolved\n  if (left_dim0 < 32 || left_dim1 < 32 || right_dim1 < 32) {\n    // Causes problems in libxsmm\n    SparseMatMul<TL, TR>::Compute(\n        nullptr /* Assumes no cached data for fallback */, left, right,\n        transpose_left, thread_pool, transpose_output, output);\n    return;\n  }\n#endif\n  auto left_data = left.data();\n  auto right_data = right.data();\n  auto output_data = output->data();\n  // Initialize libxsmm for this matrix; make sure another thread doesn't use\n  // this handle\n  auto entry =\n      cache->take_cache_entry(left_dim0, right_dim0, right_dim1, num_threads);\n  // Convert the left matrix to compressed sparse row (CSR) format\n  ptrdiff_t total_num_creation_blocks =\n      libxsmm_spmdm_get_num_createSparseSlice_blocks(&entry->handle);\n  std::atomic<int> cur_create_block_number;\n  cur_create_block_number.store(0);\n  do_on_all_threads(thread_pool, [&](int i) {\n    while (true) {\n      int work_item = cur_create_block_number.fetch_add(1);\n      if (work_item >= total_num_creation_blocks) break;\n      wrapper_libxsmm_spmdm_createSparseSlice_generic_thread(\n          empty_type_wrapper<TL>{}, &entry->handle,\n          (transpose_left ? 'T' : 'N'), left_data, entry->output_csr, work_item,\n          i, num_threads);\n    }\n  });\n  // Do matrix-matrix multiplication\n  ptrdiff_t total_num_mult_blocks =\n      libxsmm_spmdm_get_num_compute_blocks(&entry->handle);\n  std::atomic<int> cur_mult_block_number;\n  cur_mult_block_number.store(0);\n  do_on_all_threads(thread_pool, [&](int i) {\n    while (true) {\n      int work_item = cur_mult_block_number.fetch_add(1);\n      if (work_item >= total_num_mult_blocks) break;\n      const TL alpha(1.0);  // Stored in a variable so we can get a pointer\n      const TL beta(0.0);   // Stored in a variable so we can get a pointer\n      wrapper_libxsmm_spmdm_compute_generic_thread(\n          empty_type_wrapper<TL>{}, &entry->handle,\n          (transpose_left ? 'T' : 'N'), 'N', &alpha, entry->output_csr,\n          right_data, (transpose_output ? 'T' : 'N'), &beta, output_data,\n          work_item, i, num_threads);\n    }\n  });\n  // Put handle + CSR storage back into cache\n  cache->return_cache_entry(std::move(entry));\n}\n\n#endif  // TENSORFLOW_USE_LIBXSMM\n\n// Here is an overview of the SparseMatMul code. Note that we assume that the\n// left matrix is sparse.\n//\n// The matrix \"left\" is divided into a grid with blocksize of (M, KL). Each\n// block is encoded as a SparseSlice. These grid elements are stored as\n// std::vector<std::vector<SparseSlice>>. Each element of the outer vector\n// represents M rows of the left matrix. Lets call these elements l_i and lets\n// call each element of the inner vector L_mk.\n//\n// The matrix \"right\" is divided into a grid with block size KR * NR.  Lets\n// denote the blocks on the right as R_kn. Note that we ensure that KL divides\n// KR so that for each element R_kn, we don't need to multiply it with any\n// partial L_mk blocks.\n//\n// We then multiply each right side block R_kn with the full \"left\" matrix and\n// update the output. These iterations are run sequentially since R_kn are\n// packed into the same underlying temporary buffer.\n//\n// In each iteration we do the following:\n// 1. Create slices r_j of R_kn: We split R_kn into vertical blocks with N\n//    (=128) columns and then concatenating these slices into a buffer. This is\n//    done so that each slice r_j of R_kn is stored contiguously in memory. Note\n//    that if R_kj has dimensions (KR, NR), we create NR / N slices, and the\n//    buffer has dimensions (KR * NR / N, N) (assuming N divides NR).\n// 2. For each (l_i, r_j), we compute the inner product using the GEPP function\n//    and update the output block o_ij. These calls are further blocked to\n//    reduce the working set size. In each iteration we take IB elements from\n//    {l_i} and JB elements from {r_j} and compute the IB * JB inner products.\ntemplate <typename TL, typename TR>\ninline void SparseMatMul<TL, TR>::Compute(\n    typename SparseMatMul<TL, TR>::TensorInfoCache* /*cache*/,\n    const typename SparseMatMul<TL, TR>::ConstMatrixMapL& left,\n    const typename SparseMatMul<TL, TR>::ConstMatrixMapR& right,\n    bool transpose_left, const DeviceBase::CpuWorkerThreads* thread_pool,\n    bool transpose_output, MatrixMap* output) {\n  const int num_threads = thread_pool->num_threads;\n  int KR, NR, KL, JB, IB;\n  ComputeBlockSizes(left, right, transpose_left, num_threads, &KR, &NR, &KL,\n                    &JB, &IB);\n  // Slice the left matrix\n  std::vector<std::vector<SparseSlice<TL>*>> left_slices;\n  std::unique_ptr<BlockingCounter> sparse_slice_counter =\n      CreateSparseSlices(ConstMatrixMapL(left.data(), left.dimensions()),\n                         transpose_left, M, K, KL, &left_slices, thread_pool);\n  const int num_left_slices = left_slices.size();\n\n  const int right_dim0 = right.dimension(0);\n  const int right_dim1 = right.dimension(1);\n  // Allocate buffer for storing slices of right matrix.\n  // Note buffer needs enough space to hold at most a KR * NR matrix since that\n  // is the block size per iteration.\n  const int buffer_num_rows =\n      std::min(KR, right_dim0) * ((std::min(NR, right_dim1) + N - 1) / N);\n  MatrixR buffer(buffer_num_rows, N);\n  std::vector<ConstMatrixMapR*> right_slices;\n\n  std::vector<SparseSlice<TL>*> block_left_slices;\n  std::vector<std::function<void(void)>> tasks;\n  // Number of blocks based on block sizes of KR * NR.\n  const int num_k_blocks = (right_dim0 + KR - 1) / KR;\n  const int num_n_blocks = (right_dim1 + NR - 1) / NR;\n  std::unique_ptr<BlockingCounter> dense_slice_counter;\n\n  for (int nb = 0; nb < num_n_blocks; ++nb) {\n    const int right_num_cols =\n        std::min(NR, static_cast<int>(right_dim1 - NR * nb));\n    for (int kb = 0; kb < num_k_blocks; ++kb) {\n      const int right_num_rows =\n          std::min(KR, static_cast<int>(right_dim0 - KR * kb));\n      dense_slice_counter = CreateDenseSlices(\n          right, kb * KR, right_num_rows, nb * NR, right_num_cols, thread_pool,\n          &buffer, &right_slices);\n      const int num_right_slices = right_slices.size();\n      tasks.reserve(num_left_slices * num_right_slices);\n      for (int j_outer = 0; j_outer < num_right_slices; j_outer += JB) {\n        for (int i_outer = 0; i_outer < num_left_slices; i_outer += IB) {\n          for (int j_inner = j_outer;\n               j_inner < std::min(num_right_slices, j_outer + JB); ++j_inner) {\n            const int num_cols = std::min(N, right_num_cols - N * j_inner);\n            for (int i_inner = i_outer;\n                 i_inner < std::min(num_left_slices, i_outer + IB); ++i_inner) {\n              block_left_slices.clear();\n              int begin = kb * KR / KL;\n              int end = std::min<int>((kb + 1) * KR / KL,\n                                      (right.dimension(0) + KL - 1) / KL);\n              DCHECK_LT(begin, end);\n              block_left_slices.insert(block_left_slices.begin(),\n                                       left_slices[i_inner].begin() + begin,\n                                       left_slices[i_inner].begin() + end);\n              tasks.push_back(std::bind(\n                  &ComputeOutputBlock, block_left_slices,\n                  std::ref(*right_slices[j_inner]), num_cols, M * i_inner,\n                  N * j_inner + nb * NR, kb == 0, transpose_output, output));\n            }\n          }\n        }\n      }\n      if (sparse_slice_counter) {\n        sparse_slice_counter->Wait();\n        sparse_slice_counter.reset(nullptr);\n      }\n      if (dense_slice_counter) {\n        dense_slice_counter->Wait();\n        dense_slice_counter.reset(nullptr);\n      }\n      BlockingCounter bc(tasks.size());\n      for (const auto& t : tasks) {\n        thread_pool->workers->Schedule([&bc, &t]() {\n          t();\n          bc.DecrementCount();\n        });\n      }\n      bc.Wait();\n      tasks.clear();\n      for (auto& temp : right_slices) {\n        delete temp;\n      }\n      right_slices.clear();\n    }\n  }\n  for (auto& left_slice : left_slices) {\n    for (auto& temp : left_slice) {\n      delete temp;\n    }\n    left_slice.clear();\n  }\n}\n\n#define REGISTER_SPARSE_MATMUL(TA, TB)                   \\\n  REGISTER_KERNEL_BUILDER(Name(\"SparseMatMul\")           \\\n                              .Device(DEVICE_CPU)        \\\n                              .TypeConstraint<TA>(\"Ta\")  \\\n                              .TypeConstraint<TB>(\"Tb\"), \\\n                          SparseMatMulOp<TA, TB, SparseMatMul>);\n#ifdef TENSORFLOW_USE_LIBXSMM\n#define REGISTER_SPARSE_MATMUL_LIBXSMM(TA, TB)           \\\n  REGISTER_KERNEL_BUILDER(Name(\"SparseMatMul\")           \\\n                              .Device(DEVICE_CPU)        \\\n                              .TypeConstraint<TA>(\"Ta\")  \\\n                              .TypeConstraint<TB>(\"Tb\"), \\\n                          SparseMatMulOp<TA, TB, LibxsmmSparseMatMul>);\n#endif\n\nREGISTER_SPARSE_MATMUL(float, bfloat16);\nREGISTER_SPARSE_MATMUL(bfloat16, float);\n\n#ifdef TENSORFLOW_USE_LIBXSMM\nREGISTER_SPARSE_MATMUL_LIBXSMM(bfloat16, bfloat16);\nREGISTER_SPARSE_MATMUL_LIBXSMM(float, float);\n#else\nREGISTER_SPARSE_MATMUL(bfloat16, bfloat16);\nREGISTER_SPARSE_MATMUL(float, float);\n#endif\n\n#undef REGISTER_SPARSE_MATMUL\n\n}  // end namespace tensorflow\n"], "filenames": ["tensorflow/core/kernels/sparse_matmul_op.cc"], "buggy_code_start_loc": [34], "buggy_code_end_loc": [984], "fixing_code_start_loc": [35], "fixing_code_end_loc": [995], "type": "CWE-125", "message": "TensorFlow is an open source platform for machine learning. In affected versions the code for sparse matrix multiplication is vulnerable to undefined behavior via binding a reference to `nullptr`. This occurs whenever the dimensions of `a` or `b` are 0 or less. In the case on one of these is 0, an empty output tensor should be allocated (to conserve the invariant that output tensors are always allocated when the operation is successful) but nothing should be written to it (that is, we should return early from the kernel implementation). Otherwise, attempts to write to this empty tensor would result in heap OOB access. The fix will be included in TensorFlow 2.7.0. We will also cherrypick this commit on TensorFlow 2.6.1, TensorFlow 2.5.2, and TensorFlow 2.4.4, as these are also affected and still in supported range.", "other": {"cve": {"id": "CVE-2021-41219", "sourceIdentifier": "security-advisories@github.com", "published": "2021-11-05T21:15:09.137", "lastModified": "2021-11-09T16:12:56.930", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "TensorFlow is an open source platform for machine learning. In affected versions the code for sparse matrix multiplication is vulnerable to undefined behavior via binding a reference to `nullptr`. This occurs whenever the dimensions of `a` or `b` are 0 or less. In the case on one of these is 0, an empty output tensor should be allocated (to conserve the invariant that output tensors are always allocated when the operation is successful) but nothing should be written to it (that is, we should return early from the kernel implementation). Otherwise, attempts to write to this empty tensor would result in heap OOB access. The fix will be included in TensorFlow 2.7.0. We will also cherrypick this commit on TensorFlow 2.6.1, TensorFlow 2.5.2, and TensorFlow 2.4.4, as these are also affected and still in supported range."}, {"lang": "es", "value": "TensorFlow es una plataforma de c\u00f3digo abierto para el aprendizaje autom\u00e1tico. En las versiones afectadas el c\u00f3digo para la multiplicaci\u00f3n de matrices dispersas es vulnerable a un comportamiento indefinido por medio de la vinculaci\u00f3n de una referencia a \"nullptr\". Esto ocurre siempre que las dimensiones de \"a\" o \"b\" sean 0 o menos. En el caso de que una de ellas sea 0, es debido asignar un tensor de salida vac\u00edo (para conservar el invariante de que los tensores de salida siempre se asignan cuando la operaci\u00f3n presenta \u00e9xito) pero no es debido escribir nada en \u00e9l (es decir, debemos regresar antes de tiempo de la implementaci\u00f3n del n\u00facleo). De lo contrario, los intentos de escribir en este tensor vac\u00edo resultar\u00edan en un acceso OOB a la pila. La correcci\u00f3n ser\u00e1 incluida en TensorFlow versi\u00f3n 2.7.0. Tambi\u00e9n ser\u00e1 incluida este commit en TensorFlow versi\u00f3n 2.6.1, TensorFlow versi\u00f3n 2.5.2, y TensorFlow versi\u00f3n 2.4.4, ya que estos tambi\u00e9n est\u00e1n afectados y todav\u00eda est\u00e1n en el rango admitido"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.9}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:P/I:P/A:P", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "PARTIAL", "availabilityImpact": "PARTIAL", "baseScore": 4.6}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.9, "impactScore": 6.4, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-125"}]}, {"source": "security-advisories@github.com", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-824"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionEndExcluding": "2.4.4", "matchCriteriaId": "455FB550-4C9C-4BD6-9F76-A627B62AB332"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.5.0", "versionEndExcluding": "2.5.2", "matchCriteriaId": "035CDF63-1548-4FB4-B8A9-B8D328FAF910"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.6.0:*:*:*:*:*:*:*", "matchCriteriaId": "651EA851-E660-4E53-9F3E-B6B69D91326B"}]}]}], "references": [{"url": "https://github.com/tensorflow/tensorflow/commit/e6cf28c72ba2eb949ca950d834dd6d66bb01cfae", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/security/advisories/GHSA-4f99-p9c2-3j8x", "source": "security-advisories@github.com", "tags": ["Exploit", "Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/tensorflow/tensorflow/commit/e6cf28c72ba2eb949ca950d834dd6d66bb01cfae"}}