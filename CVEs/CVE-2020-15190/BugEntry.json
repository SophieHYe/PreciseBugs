{"buggy_code": ["/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include \"tensorflow/core/common_runtime/eager/kernel_and_device.h\"\n\n#include <memory>\n\n#include \"absl/strings/match.h\"\n#include \"tensorflow/core/common_runtime/device_factory.h\"\n#include \"tensorflow/core/common_runtime/eager/attr_builder.h\"\n#include \"tensorflow/core/common_runtime/process_function_library_runtime.h\"\n#include \"tensorflow/core/common_runtime/rendezvous_mgr.h\"\n#include \"tensorflow/core/framework/allocator.h\"\n#include \"tensorflow/core/framework/cancellation.h\"\n#include \"tensorflow/core/framework/function.h\"\n#include \"tensorflow/core/framework/node_def.pb.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/resource_mgr.h\"\n#include \"tensorflow/core/framework/types.h\"\n#include \"tensorflow/core/framework/types.pb.h\"\n#include \"tensorflow/core/lib/core/errors.h\"\n#include \"tensorflow/core/lib/core/refcount.h\"\n#include \"tensorflow/core/lib/gtl/cleanup.h\"\n#include \"tensorflow/core/lib/gtl/map_util.h\"\n#include \"tensorflow/core/lib/random/random.h\"\n#include \"tensorflow/core/platform/denormal.h\"\n#include \"tensorflow/core/platform/errors.h\"\n#include \"tensorflow/core/platform/fingerprint.h\"\n#include \"tensorflow/core/platform/setround.h\"\n#include \"tensorflow/core/profiler/lib/annotated_traceme.h\"\n#include \"tensorflow/core/profiler/lib/traceme.h\"\n#include \"tensorflow/core/public/version.h\"\n#include \"tensorflow/core/util/tensor_slice_reader_cache.h\"\n#if !defined(IS_MOBILE_PLATFORM)\n#include \"tensorflow/core/grappler/optimizers/meta_optimizer.h\"\n#endif  // !IS_MOBILE_PLATFORM\n\nnamespace tensorflow {\n\nStatus EagerKernelArgs::GetLocalArg(const FunctionArgIndex& index,\n                                    Tensor* val) const {\n  if (index.sub_index >= 0) {\n    return errors::InvalidArgument(\"Got unexpected sub_index \", index.sub_index,\n                                   \" for argument \", index.index);\n  }\n  Tensor* arg = tensor_args_.at(index.index).tensor;\n  if (arg) {\n    *val = *arg;\n    return Status::OK();\n  } else {\n    return errors::NotFound(\"Argument \", index.index, \" has no local tensor.\");\n  }\n}\n\nstd::vector<Tensor> EagerKernelArgs::GetLocalTensors() const {\n  std::vector<Tensor> lcoal_inputs;\n  lcoal_inputs.reserve(tensor_args_.size());\n  for (const TensorValue& tensor_value : tensor_args_) {\n    lcoal_inputs.push_back(*tensor_value.tensor);\n  }\n  return lcoal_inputs;\n}\n\nstd::function<void(std::function<void()>)>* KernelAndDevice::get_runner()\n    const {\n  if (runner_) {\n    return runner_;\n  } else {\n    static auto* default_runner =\n        new std::function<void(std::function<void()>)>(\n            [](const std::function<void()>& f) { f(); });\n    return default_runner;\n  }\n}\n\nKernelAndDeviceFunc::~KernelAndDeviceFunc() {\n  if (handle_ != kInvalidHandle) {\n    Status status = pflr_->ReleaseHandle(handle_);\n    if (!status.ok()) {\n      LOG(INFO) << \"Ignoring error status when releasing multi-device function \"\n                   \"handle \"\n                << status.ToString();\n    }\n  }\n}\n\nStatus KernelAndDeviceOp::Init(const Context& ctx, const NodeDef& ndef,\n                               GraphCollector* graph_collector) {\n  OpKernel* k = nullptr;\n  if (flr_ == nullptr) {\n    return errors::Internal(\n        \"A valid FunctionLibraryRuntime must be provided when running ops \"\n        \"based on OpKernel.\");\n  }\n  std::shared_ptr<const NodeProperties> props;\n  TF_RETURN_IF_ERROR(NodeProperties::CreateFromNodeDef(\n      ndef, flr_->GetFunctionLibraryDefinition(), &props));\n  TF_RETURN_IF_ERROR(flr_->CreateKernel(props, &k));\n  kernel_.reset(k);\n\n  input_alloc_attrs_.resize(kernel_->num_inputs());\n  input_devices_.resize(kernel_->num_inputs(), device_);\n  for (size_t i = 0; i < input_alloc_attrs_.size(); ++i) {\n    bool host = kernel_->input_memory_types()[i] == tensorflow::HOST_MEMORY;\n    input_alloc_attrs_[i].set_on_host(host);\n    if (host) {\n      input_devices_[i] = host_cpu_device_;\n    }\n  }\n  output_alloc_attrs_.resize(kernel_->num_outputs());\n  for (size_t i = 0; i < output_alloc_attrs_.size(); ++i) {\n    output_alloc_attrs_[i].set_on_host(kernel_->output_memory_types()[i] ==\n                                       tensorflow::HOST_MEMORY);\n  }\n\n  return Status::OK();\n}\n\nStatus KernelAndDeviceFunc::InstantiateFunc(const Context& ctx,\n                                            const NodeDef& ndef,\n                                            GraphCollector* graph_collector) {\n  const OpDef* op_def = nullptr;\n  const FunctionDef* function_def;\n  if (flr_ == nullptr) {\n    // If function is being executed without an explicit device request,\n    // lookup the FunctionDef in the CPU's FLR. All FLRs share the same\n    // library.\n    function_def = pflr_->GetFLR(host_cpu_device_->name())\n                       ->GetFunctionLibraryDefinition()\n                       ->Find(ndef.op());\n  } else {\n    function_def = flr_->GetFunctionLibraryDefinition()->Find(ndef.op());\n  }\n\n  if (function_def != nullptr) {\n    op_def = &(function_def->signature());\n  } else {\n    TF_RETURN_IF_ERROR(OpDefForOp(ndef.op(), &op_def));\n  }\n  TF_RETURN_IF_ERROR(\n      InOutTypesForNode(ndef, *op_def, &input_dtypes_, &output_dtypes_));\n\n  FunctionLibraryRuntime::InstantiateOptions options;\n  options.target = device_ == nullptr ? \"\" : device_->name();\n  options.is_multi_device_function = true;\n  for (const Device* device : input_devices_) {\n    options.input_devices.push_back(device->name());\n  }\n  options.composite_devices = composite_devices_;\n  options.input_resource_dtypes_and_shapes = input_resource_dtypes_and_shapes_;\n\n  const auto& it = ndef.attr().find(\"executor_type\");\n  if (it != ndef.attr().end()) {\n    options.executor_type = it->second.s();\n  }\n  const auto& is_component_fn_it = ndef.attr().find(\"is_component_function\");\n  if (is_component_fn_it != ndef.attr().end()) {\n    options.is_component_function = is_component_fn_it->second.b();\n  }\n#if !defined(IS_MOBILE_PLATFORM)\n  // Android tf library does not include grappler.\n  const auto& config_it = ndef.attr().find(\"config_proto\");\n  if (it != ndef.attr().end()) {\n    if (!options.config_proto.ParseFromString(config_it->second.s())) {\n      return errors::InvalidArgument(\n          \"Failed to parse config_proto attribute as tensorflow::ConfigProto \"\n          \"proto.\");\n    }\n    grappler::GrapplerItem::OptimizationOptions optimization_options;\n\n    // Tensorflow 2.0 in eager mode with automatic control dependencies will\n    // prune all nodes that are not in the transitive fanin of the fetch nodes.\n    // However because the function will be executed via FunctionLibraryRuntime,\n    // and current function implementation does not prune stateful and dataset\n    // ops, we rely on Grappler to do the correct graph pruning.\n    optimization_options.allow_pruning_stateful_and_dataset_ops = true;\n\n    optimization_options.is_eager_mode = true;\n\n    // All the nested function calls will be executed and optimized via\n    // PartitionedCallOp, there is no need to optimize functions now.\n    optimization_options.optimize_function_library = false;\n\n    options.optimize_graph_fn = std::bind(\n        grappler::OptimizeGraph, std::placeholders::_1, std::placeholders::_2,\n        std::placeholders::_3, std::placeholders::_4, std::placeholders::_5,\n        options.config_proto, function_def->signature().name(),\n        optimization_options, std::placeholders::_6);\n  }\n#endif  // !IS_MOBILE_PLATFORM\n  options.graph_collector = graph_collector;\n\n  // In Eager mode we always inline all functions into the top-level\n  // function body graph, to get a single executable graph, that could be\n  // optimized across function boundaries (e.g. prune unused inputs and outputs\n  // in a function call chain). This is required to mimic graph mode execution,\n  // with aggressive pruning of nodes not in the transitive fanin of fetches.\n  options.config_proto.mutable_graph_options()\n      ->mutable_optimizer_options()\n      ->set_do_function_inlining(true);\n\n  options.config_proto.set_log_device_placement(ctx.log_device_placement);\n\n  TF_RETURN_IF_ERROR(\n      pflr_->Instantiate(ndef.op(), AttrSlice(ndef), options, &handle_));\n  return pflr_->IsCrossProcess(handle_, &is_cross_process_);\n}\n\nStatus KernelAndDeviceFunc::Init(const Context& ctx, const NodeDef& ndef,\n                                 GraphCollector* graph_collector) {\n  TF_RETURN_IF_ERROR(InstantiateFunc(ctx, ndef, graph_collector));\n  return pflr_->GetOutputDevices(handle_, &output_devices_,\n                                 ctx.eager_lazy_copy);\n}\n\nnamespace {\n// In certain contexts (e.g. TPU async executions), the CancellationManager is\n// used to shut down the device in error scenarios (as opposed to using the\n// AsyncCompute's DoneCallback). This is handled through the\n// {inc,dec}_num_deferred_ops_function.\nstruct OpExecutionState : public core::RefCounted {\n  // TODO(nareshmodi): consider refcounting the cancellation_manager.\n  CancellationManager cancellation_manager;\n};\n}  // anonymous namespace\n\nStatus KernelAndDeviceOp::Run(\n    ScopedStepContainer* step_container, const EagerKernelArgs& inputs,\n    std::vector<EagerKernelRet>* outputs,\n    CancellationManager* cancellation_manager,\n    const absl::optional<EagerRemoteFunctionParams>& remote_func_params) {\n  OpKernelContext::Params params;\n  params.device = device_;\n  params.frame_iter = FrameAndIter(0, 0);\n  params.inputs = inputs.GetTensorValues();\n  params.op_kernel = kernel_.get();\n  params.resource_manager = device_->resource_manager();\n  params.input_alloc_attrs = &input_alloc_attrs_;\n  params.output_attr_array = output_alloc_attrs_.data();\n  params.function_library = flr_;\n  params.slice_reader_cache = &slice_reader_cache_;\n  params.rendezvous = rendezvous_;\n  OpExecutionState* op_execution_state = nullptr;\n\n  CancellationManager default_cancellation_manager;\n  if (cancellation_manager) {\n    params.cancellation_manager = cancellation_manager;\n  } else if (kernel_->is_deferred()) {\n    op_execution_state = new OpExecutionState;\n    params.cancellation_manager = &op_execution_state->cancellation_manager;\n    params.inc_num_deferred_ops_function = [op_execution_state]() {\n      op_execution_state->Ref();\n    };\n    params.dec_num_deferred_ops_function = [op_execution_state]() {\n      op_execution_state->Unref();\n    };\n  } else {\n    params.cancellation_manager = &default_cancellation_manager;\n  }\n\n  params.log_memory = log_memory_;\n\n  params.runner = get_runner();\n\n  params.step_container =\n      step_container == nullptr ? &step_container_ : step_container;\n  auto step_container_cleanup = gtl::MakeCleanup([step_container, this] {\n    if (step_container == nullptr) {\n      this->step_container_.CleanUp();\n    }\n  });\n\n  params.collective_executor =\n      collective_executor_ ? collective_executor_->get() : nullptr;\n\n  OpKernelContext context(&params);\n\n  {\n    port::ScopedFlushDenormal flush;\n    port::ScopedSetRound round(FE_TONEAREST);\n    // 'AnnotatedTraceMe' will trace both scheduling time on host and execution\n    // time on device of the OpKernel.\n    profiler::AnnotatedTraceMe activity(\n        [&] { return kernel_->TraceString(context, /*verbose=*/false); },\n        profiler::TraceMeLevel::kInfo);\n    device_->Compute(kernel_.get(), &context);\n  }\n\n  // Clean up execution op_execution_state if deferred ops aren't running.\n  if (op_execution_state != nullptr) {\n    op_execution_state->Unref();\n  }\n\n  if (!context.status().ok()) return context.status();\n\n  if (outputs != nullptr) {\n    outputs->clear();\n    for (int i = 0; i < context.num_outputs(); ++i) {\n      outputs->push_back(Tensor(*context.mutable_output(i)));\n    }\n  }\n  return Status::OK();\n}\n\nStatus KernelAndDeviceFunc::Run(\n    ScopedStepContainer* step_container, const EagerKernelArgs& inputs,\n    std::vector<EagerKernelRet>* outputs,\n    CancellationManager* cancellation_manager,\n    const absl::optional<EagerRemoteFunctionParams>& remote_func_params) {\n  Notification n;\n  Status status;\n  RunAsync(step_container, inputs, outputs, cancellation_manager,\n           remote_func_params, [&status, &n](const Status& s) {\n             status = s;\n             n.Notify();\n           });\n  n.WaitForNotification();\n  return status;\n}\n\nvoid KernelAndDeviceFunc::RunAsync(\n    ScopedStepContainer* step_container, const EagerKernelArgs& inputs,\n    std::vector<EagerKernelRet>* outputs,\n    CancellationManager* cancellation_manager,\n    const absl::optional<EagerRemoteFunctionParams>& remote_func_params,\n    std::function<void(const Status&)> done) {\n  std::shared_ptr<FunctionLibraryRuntime::Options> opts = nullptr;\n  if (remote_func_params.has_value()) {\n    const EagerRemoteFunctionParams& params = remote_func_params.value();\n    if (params.step_id.has_value()) {\n      // If the function is a remote component of a cross-process function,\n      // re-use the step id as its parent function's.\n      opts = std::make_shared<FunctionLibraryRuntime::Options>(\n          params.step_id.value());\n    } else {\n      opts = std::make_shared<FunctionLibraryRuntime::Options>();\n    }\n    // Reuse the op id if it exists.\n    opts->op_id = params.op_id;\n  } else {\n    opts = std::make_shared<FunctionLibraryRuntime::Options>();\n    if (get_op_id_ && is_cross_process_) {\n      // If the function is a cross-process function and the remote execution\n      // goes through eager service, create an eager op id for the function.\n      opts->op_id = get_op_id_();\n    }\n  }\n\n  // We don't pass rendezvous from eager context because we can get tensor\n  // name collisions in send/recv ops when running multiple instances\n  // of the same multi-device function concurrently.\n  Rendezvous* rendezvous = rendezvous_creator_(opts->step_id);\n  opts->rendezvous = rendezvous;\n  opts->create_rendezvous = false;\n\n  // Create a cancellation manager to be used by FLR options if caller does not\n  // pass in one. If the caller does provide one, pass it to process FLR and the\n  // locally created one will be unused.\n  std::shared_ptr<CancellationManager> local_cm;\n  if (cancellation_manager) {\n    opts->cancellation_manager = cancellation_manager;\n  } else {\n    local_cm = std::make_shared<CancellationManager>();\n    opts->cancellation_manager = local_cm.get();\n  }\n  opts->allow_dead_tensors = true;\n  opts->step_container =\n      step_container == nullptr ? &step_container_ : step_container;\n  opts->collective_executor =\n      collective_executor_ ? collective_executor_->get() : nullptr;\n\n  opts->stats_collector = nullptr;\n  opts->runner = get_runner();\n\n  outputs->clear();\n\n  pflr_->Run(*opts, handle_, inputs, outputs,\n             [opts, rendezvous, local_cm, step_container, this,\n              done = std::move(done)](const Status& s) {\n               rendezvous->Unref();\n               if (step_container == nullptr) {\n                 this->step_container_.CleanUp();\n               }\n               done(s);\n             });\n}\n\ntensorflow::Device* KernelAndDeviceOp::OutputDevice(int idx) const {\n  if (kernel_->output_memory_types()[idx] == HOST_MEMORY) {\n    return nullptr;\n  }\n  return device_;\n}\n\ntensorflow::Device* KernelAndDeviceFunc::OutputDevice(int idx) const {\n  if (output_dtypes_[idx] == DT_RESOURCE) {\n    return nullptr;\n  }\n  return output_devices_[idx];\n}\n\ntensorflow::Device* KernelAndDeviceOp::OutputResourceDevice(int idx) const {\n  if (kernel_->output_type(idx) == DT_RESOURCE) {\n    return device_;\n  }\n  return nullptr;\n}\n\ntensorflow::Device* KernelAndDeviceFunc::OutputResourceDevice(int idx) const {\n  if (output_dtypes_[idx] == DT_RESOURCE) {\n    return output_devices_[idx];\n  }\n  return nullptr;\n}\n\nDevice* KernelAndDeviceOp::InputDevice(int i) const {\n  return input_devices_[i];\n}\n\nDevice* KernelAndDeviceFunc::InputDevice(int i) const {\n  if ((input_dtypes_[i] == DT_RESOURCE) &&\n      (composite_devices_.find(input_devices_[i]->name()) ==\n       composite_devices_.end())) {\n    return host_cpu_device_;\n  } else {\n    return input_devices_[i];\n  }\n}\n\n}  // namespace tensorflow\n", "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OiR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n# pylint: disable=g-long-lambda\n\"\"\"Tests for tensorflow.ops.control_flow_ops.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport math\nimport re\nimport sys\nimport time\n\nfrom absl.testing import parameterized\nimport numpy as np\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\n\nfrom tensorflow.core.protobuf import config_pb2\nfrom tensorflow.python import tf2\nfrom tensorflow.python.client import device_lib\nfrom tensorflow.python.client import session\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.eager import def_function\nfrom tensorflow.python.eager import function as eager_function\nfrom tensorflow.python.eager import wrap_function\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors_impl\nfrom tensorflow.python.framework import function\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import sparse_tensor\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.framework import tensor_spec\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import control_flow_util\nfrom tensorflow.python.ops import data_flow_ops\nfrom tensorflow.python.ops import functional_ops\nfrom tensorflow.python.ops import gen_array_ops\nfrom tensorflow.python.ops import gen_control_flow_ops\nfrom tensorflow.python.ops import gen_data_flow_ops\nfrom tensorflow.python.ops import gen_logging_ops\nfrom tensorflow.python.ops import gen_state_ops\nfrom tensorflow.python.ops import gradient_checker_v2\nfrom tensorflow.python.ops import gradients_impl\nfrom tensorflow.python.ops import init_ops\nfrom tensorflow.python.ops import linalg_ops\nfrom tensorflow.python.ops import logging_ops\nfrom tensorflow.python.ops import map_fn\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import nn_grad  # pylint: disable=unused-import\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.ops import random_ops\nfrom tensorflow.python.ops import resource_variable_ops\nfrom tensorflow.python.ops import script_ops\nfrom tensorflow.python.ops import sparse_ops\nfrom tensorflow.python.ops import state_ops\nfrom tensorflow.python.ops import tensor_array_grad  # pylint: disable=unused-import\nfrom tensorflow.python.ops import tensor_array_ops\nfrom tensorflow.python.ops import variable_scope\nfrom tensorflow.python.ops import variables\nfrom tensorflow.python.ops import while_v2  # pylint: disable=unused-import\n# pylint: disable=unused-import\nfrom tensorflow.python.ops.ragged import ragged_factory_ops\nfrom tensorflow.python.ops.ragged import ragged_tensor\nimport tensorflow.python.ops.tensor_array_grad\n# pylint: enable=unused-import\nfrom tensorflow.python.platform import test\nfrom tensorflow.python.training import adam\nfrom tensorflow.python.training import gradient_descent\nfrom tensorflow.python.util import nest\n\n\ndef check_consumers(graph):\n  \"\"\"Sanity check on the consumer list of the tensors.\"\"\"\n\n  consumer_count = {}\n  for op in graph.get_operations():\n    for v in op.inputs:\n      cnt = consumer_count.get(v, 0)\n      consumer_count[v] = cnt + 1\n  for k, v in consumer_count.items():\n    if len(k.consumers()) != v:\n      return False\n  return True\n\n\ndef all_fetchables():\n  tensor_names = []\n  graph = ops.get_default_graph()\n  for op in graph.get_operations():\n    for t in op.outputs:\n      if graph.is_fetchable(t):\n        tensor_names.append(t.name)\n  return tensor_names\n\n\ndef all_feedables():\n  feedable_tensors = []\n  graph = ops.get_default_graph()\n  for op in graph.get_operations():\n    for t in op.inputs:\n      if graph.is_feedable(t):\n        feedable_tensors.append(t)\n  return feedable_tensors\n\n\ndef opt_cfg(do_constant_folding=True):\n  return config_pb2.ConfigProto(\n      allow_soft_placement=True,\n      graph_options=config_pb2.GraphOptions(\n          optimizer_options=config_pb2.OptimizerOptions(\n              opt_level=config_pb2.OptimizerOptions.L1,\n              do_function_inlining=True,\n              do_constant_folding=do_constant_folding)))\n\n\ndef isum(s, maximum_iterations=None):\n  i = constant_op.constant(0, name=\"i\")\n  c = lambda i, s: math_ops.less(i, 10)\n  b = lambda i, s: [math_ops.add(i, 1), math_ops.add(i, s)]\n  _, r_s = control_flow_ops.while_loop(\n      c, b, [i, s], maximum_iterations=maximum_iterations)\n  return r_s\n\n\ndef enqueue_print_op(s):\n  \"\"\"Enqueues an op that prints a message to be captured in the test.\"\"\"\n  return logging_ops.print_v2(\"ControlFlowOpsTest: \" + s)\n\n\ndef filter_test_messages(s):\n  \"\"\"Returns a list of messages printed by enqueue_print_op.\"\"\"\n  prefix = \"ControlFlowOpsTest: \"\n  return [l[len(prefix):] for l in s.split(\"\\n\") if l.startswith(prefix)]\n\n\ndef tf_function_in_tf2(f):\n  if tf2.enabled():\n    # In TF1 do not wrap with tf.function so that we can test the v1 control\n    # flow code path.\n    return def_function.function(f)\n  return f\n\n\n@test_util.with_control_flow_v2\nclass ControlFlowTest(test.TestCase, parameterized.TestCase):\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testRefIdentity(self):\n    with self.cached_session():\n      v = variables.VariableV1(7)\n\n      v = control_flow_ops._Identity(v)\n      op = state_ops.assign(v, 9)\n      v2 = control_flow_ops.with_dependencies([op], v)\n\n      self.assertTrue(isinstance(v2, ops.Tensor))\n      self.evaluate(variables.global_variables_initializer())\n      self.assertEqual(9, self.evaluate(v2))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testRefEnter(self):\n    with self.cached_session():\n      v = variables.VariableV1(7)\n\n      enter_v = control_flow_ops._Enter(v, \"foo_1\", is_constant=True)\n      nine = constant_op.constant(9)\n      enter_nine = gen_control_flow_ops.enter(nine, \"foo_1\")\n      op = state_ops.assign(enter_v, enter_nine)\n      v2 = control_flow_ops.with_dependencies([op], enter_v)\n      v3 = control_flow_ops.exit(v2)\n      self.evaluate(variables.global_variables_initializer())\n      self.assertEqual(9, self.evaluate(v3))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testRefSwitch(self):\n    with self.cached_session():\n      v = variables.VariableV1(7)\n\n      p = constant_op.constant(True)\n      v1 = control_flow_ops._SwitchRefOrTensor(v._ref(), p)  # pylint: disable=protected-access\n      v2 = state_ops.assign(v1[1], 9)\n      self.evaluate(variables.global_variables_initializer())\n      self.assertEqual(9, self.evaluate(v2))\n\n  def testEnterMulExit(self):\n    with self.cached_session():\n      data = constant_op.constant([1, 2, 3, 4, 5, 6], name=\"data\")\n      enter_data = gen_control_flow_ops.enter(data, \"foo_1\", False)\n      five = constant_op.constant(5)\n      enter_five = gen_control_flow_ops.enter(five, \"foo_1\", False)\n      mul_op = math_ops.multiply(enter_data, enter_five)\n      exit_op = control_flow_ops.exit(mul_op)\n\n      result = self.evaluate(exit_op)\n    self.assertAllEqual(np.array([x * 5 for x in [1, 2, 3, 4, 5, 6]]), result)\n\n  @test_util.run_deprecated_v1\n  def testEnterShapePropagation(self):\n    with self.cached_session():\n      v = variables.Variable([0.0, 0.0], dtype=dtypes.float32)\n\n      # If is_constant=True, the shape information should be propagated.\n      enter_v_constant = gen_control_flow_ops.enter(\n          v, \"frame1\", is_constant=True)\n      self.assertEqual(enter_v_constant.shape, [2])\n\n      # Otherwise, the shape should be unknown.\n      enter_v_non_constant = gen_control_flow_ops.enter(\n          v, \"frame2\", is_constant=False)\n      self.assertEqual(enter_v_non_constant.shape, None)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testSwitchMergeIndexedSlices(self):\n    with self.cached_session():\n      values = constant_op.constant([1, 2, 3, 4, 5, 6])\n      indices = constant_op.constant([0, 2, 4, 6, 8, 10])\n      data = ops.IndexedSlices(values, indices)\n      pred = ops.convert_to_tensor(True)\n      switch_op = control_flow_ops.switch(data, pred)\n      merge_op = control_flow_ops.merge(switch_op)[0]\n\n      val = merge_op.values\n      ind = merge_op.indices\n    self.assertAllEqual(np.arange(1, 7), val)\n    self.assertAllEqual(np.arange(0, 12, 2), ind)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testSwitchDeadBranch(self):\n    with self.cached_session():\n      data = constant_op.constant([1, 2, 3, 4, 5, 6], name=\"data\")\n      ports = ops.convert_to_tensor(True, name=\"ports\")\n      switch_op = control_flow_ops.switch(data, ports)\n      dead_branch = array_ops.identity(switch_op[0])\n\n      with self.assertRaisesWithPredicateMatch(\n          errors_impl.InvalidArgumentError,\n          lambda e: \"Retval[0] does not have value\" in str(e)):\n        self.evaluate(dead_branch)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testSwitchMergeLess(self):\n    with self.cached_session():\n      data = constant_op.constant([1, 2, 3, 4, 5, 6], name=\"data\")\n      zero = ops.convert_to_tensor(0)\n      one = ops.convert_to_tensor(1)\n      less_op = math_ops.less(zero, one)\n      switch_op = control_flow_ops.switch(data, less_op)\n      merge_op = control_flow_ops.merge(switch_op)[0]\n\n      result = self.evaluate(merge_op)\n    self.assertAllEqual(np.arange(1, 7), result)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testSwitchMergeAddIdentity(self):\n    with self.cached_session():\n      data = constant_op.constant([1, 2, 3, 4, 5, 6], name=\"data\")\n      ports = ops.convert_to_tensor(False, name=\"ports\")\n      switch_op = control_flow_ops.switch(data, ports)\n      one = constant_op.constant(1)\n      add_op = math_ops.add(switch_op[0], one)\n      id_op = array_ops.identity(switch_op[1])\n      merge_op = control_flow_ops.merge([add_op, id_op])[0]\n\n      result = self.evaluate(merge_op)\n    self.assertAllEqual(np.array([x + 1 for x in [1, 2, 3, 4, 5, 6]]), result)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testSwitchMergeAddMul(self):\n    with self.cached_session():\n      data = constant_op.constant([1, 2, 3, 4, 5, 6], name=\"data\")\n      ports = ops.convert_to_tensor(True, name=\"ports\")\n      switch_op = control_flow_ops.switch(data, ports)\n      one = constant_op.constant(1)\n      add_op = math_ops.add(switch_op[0], one)\n      five = constant_op.constant(5)\n      mul_op = math_ops.multiply(switch_op[1], five)\n      merge_op = control_flow_ops.merge([add_op, mul_op])[0]\n\n      result = self.evaluate(merge_op)\n    self.assertAllEqual(np.array([x * 5 for x in [1, 2, 3, 4, 5, 6]]), result)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testLoop_false(self):\n    with self.cached_session():\n      false = ops.convert_to_tensor(False)\n      n = constant_op.constant(10)\n\n      enter_false = gen_control_flow_ops.enter(false, \"foo_1\", False)\n      enter_n = gen_control_flow_ops.enter(n, \"foo_1\", False)\n\n      merge_n = control_flow_ops.merge([enter_n, enter_n], name=\"merge_n\")[0]\n      switch_n = control_flow_ops.switch(merge_n, enter_false)\n      exit_n = control_flow_ops.exit(switch_n[0])\n      next_n = control_flow_ops.next_iteration(switch_n[0])\n      merge_n.op._update_input(1, next_n)\n\n      result = self.evaluate(exit_n)\n    self.assertAllEqual(10, result)\n\n  @test_util.run_deprecated_v1\n  def testLoop_1(self):\n    with self.cached_session():\n      zero = constant_op.constant(0)\n      one = constant_op.constant(1)\n      n = constant_op.constant(10)\n\n      enter_i = gen_control_flow_ops.enter(zero, \"foo\", False)\n      enter_one = gen_control_flow_ops.enter(one, \"foo\", True)\n      enter_n = gen_control_flow_ops.enter(n, \"foo\", True)\n\n      with ops.device(test.gpu_device_name()):\n        merge_i = control_flow_ops.merge([enter_i, enter_i])[0]\n\n      less_op = math_ops.less(merge_i, enter_n)\n      cond_op = control_flow_ops.loop_cond(less_op)\n      switch_i = control_flow_ops.switch(merge_i, cond_op)\n\n      add_i = math_ops.add(switch_i[1], enter_one)\n\n      next_i = control_flow_ops.next_iteration(add_i)\n      merge_i.op._update_input(1, next_i)\n\n      exit_i = control_flow_ops.exit(switch_i[0])\n      result = self.evaluate(exit_i)\n    self.assertAllEqual(10, result)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testLoop_2(self):\n    with self.cached_session():\n      zero = constant_op.constant(0)\n      one = constant_op.constant(1)\n      n = constant_op.constant(10)\n\n      enter_i = gen_control_flow_ops.enter(zero, \"foo\", False)\n      enter_one = gen_control_flow_ops.enter(one, \"foo\", True)\n      enter_n = gen_control_flow_ops.enter(n, \"foo\", True)\n\n      merge_i = control_flow_ops.merge([enter_i, enter_i])[0]\n\n      less_op = math_ops.less(merge_i, enter_n)\n      cond_op = control_flow_ops.loop_cond(less_op)\n      switch_i = control_flow_ops.switch(merge_i, cond_op)\n\n      add_i = math_ops.add(switch_i[1], enter_one)\n\n      with ops.device(test.gpu_device_name()):\n        next_i = control_flow_ops.next_iteration(add_i)\n      merge_i.op._update_input(1, next_i)\n\n      exit_i = control_flow_ops.exit(switch_i[0])\n      result = self.evaluate(exit_i)\n    self.assertAllEqual(10, result)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testDifferentFrame(self):\n    with self.cached_session():\n      data = array_ops.placeholder(dtypes.float32, shape=[])\n      enter_1 = gen_control_flow_ops.enter(data, \"foo_1\", False)\n      enter_2 = gen_control_flow_ops.enter(data, \"foo_2\", False)\n      res = math_ops.add(enter_1, enter_2)\n      with self.assertRaisesOpError(\"has inputs from different frames\"):\n        res.eval(feed_dict={data: 1.0})\n\n  @test_util.run_deprecated_v1\n  def testCondBool(self):\n    values = constant_op.constant(10)\n    fn1 = lambda: math_ops.add(values, 1)\n    fn2 = lambda: math_ops.subtract(values, 1)\n    with self.assertRaisesRegex(TypeError, \"must not be a Python bool\"):\n      _ = control_flow_ops.cond(False, fn1, fn2)\n\n  @test_util.run_deprecated_v1\n  def testCondInt(self):\n    p = array_ops.placeholder(dtypes.bool, shape=[])\n    v = constant_op.constant(10)\n    fn1 = lambda: math_ops.add(v, 1)\n    fn2 = lambda: math_ops.subtract(v, 1)\n    y = control_flow_ops.cond(p, fn1, fn2)\n    grad = gradients_impl.gradients(y, [v])\n    self.assertAllEqual([None], grad)\n\n  def testCondOutputShape(self):\n    x = constant_op.constant(1.0)\n    b = control_flow_ops.cond(\n        constant_op.constant(True), lambda: math_ops.square(x),\n        lambda: math_ops.subtract(x, 1.))\n    self.assertEqual(b.shape, tensor_shape.TensorShape([]))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testFetchable(self):\n    with self.cached_session() as sess:\n      x = array_ops.placeholder(dtypes.float32)\n      control_flow_ops.cond(\n          constant_op.constant(True), lambda: x + 2, lambda: x + 0)\n      graph = ops.get_default_graph()\n      for op in graph.get_operations():\n        for t in op.inputs:\n          if graph.is_fetchable(t.op):\n            sess.run(t, feed_dict={x: 3})\n          else:\n            with self.assertRaisesRegex(ValueError,\n                                        \"has been marked as not fetchable\"):\n              sess.run(t, feed_dict={x: 3})\n\n  @test_util.disable_control_flow_v2(\"Not relevant\")\n  @test_util.run_v1_only(\"b/120545219\")\n  def testFeedable(self):\n    with self.cached_session() as sess:\n      c = constant_op.constant(2)\n      i0 = constant_op.constant(0)\n      r = control_flow_ops.while_loop(lambda i: i < 1000,\n                                      lambda i: math_ops.square(c) + i, [i0])\n      self.assertEqual(1000, r.eval(feed_dict={i0: 0}))\n      feedable_tensors = all_feedables()\n      for t in feedable_tensors:\n        sess.run(r, feed_dict={t: 3})\n      graph = ops.get_default_graph()\n      for op in graph.get_operations():\n        for t in op.inputs:\n          if t not in feedable_tensors and t.dtype is dtypes.int32:\n            with self.assertRaisesRegex(ValueError, \"may not be fed\"):\n              sess.run(r, feed_dict={t: 3})\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testCondIndexedSlices(self):\n    with self.cached_session():\n      values = constant_op.constant([10])\n      indices = constant_op.constant([0])\n      x = ops.IndexedSlices(values, indices)\n      pred = math_ops.less(1, 2)\n      fn1 = lambda: ops.IndexedSlices(math_ops.add(x.values, 1), indices)\n      fn2 = lambda: ops.IndexedSlices(math_ops.subtract(x.values, 1), indices)\n      r = control_flow_ops.cond(pred, fn1, fn2)\n\n      val = r.values\n      ind = r.indices\n    self.assertAllEqual([11], val)\n    self.assertAllEqual([0], ind)\n\n  def testCondMismatchedIndexedSlices(self):\n    @def_function.function\n    def foo():\n      values = constant_op.constant([10])\n      indices = constant_op.constant([0])\n      x = ops.IndexedSlices(values, indices)\n      with self.assertRaisesRegex(TypeError,\n                                  \"Cannot reconcile tf.cond 0-th outputs\"):\n        control_flow_ops.cond(\n            constant_op.constant(True),\n            lambda: ops.IndexedSlices(math_ops.add(x.values, 1), indices),\n            lambda: math_ops.add(x.values, 1), indices)\n    foo()\n\n  def testCondSparseTensor(self):\n    values = constant_op.constant([2.0, 4.0], name=\"values\")\n    indices = constant_op.constant([[0], [3]],\n                                   dtype=dtypes.int64,\n                                   name=\"indices\")\n    shape = constant_op.constant([10], dtype=dtypes.int64, name=\"dense_shape\")\n    x = sparse_tensor.SparseTensor(indices, values, dense_shape=shape)\n    pred = math_ops.less(1, 2)\n    fn1 = lambda: sparse_tensor.SparseTensor(\n        indices + 1, x.values + 1, dense_shape=shape)\n    fn2 = lambda: sparse_tensor.SparseTensor(\n        indices, x.values - 1, dense_shape=shape)\n    r = control_flow_ops.cond(pred, fn1, fn2)\n    self.assertAllEqual([3.0, 5.0], r.values)\n    self.assertAllEqual([[1], [4]], r.indices)\n    self.assertAllEqual(r.values.get_shape(), (2,))\n\n  def testCondRaggedTensor(self):\n    rt = ragged_factory_ops.constant([[1, 2], [3], [4, 5, 6]])\n    pred = math_ops.less(1, 2)\n    fn1 = lambda: array_ops.concat([rt + 2, [[100]]], axis=0)\n    fn2 = lambda: rt[:2] - 2\n    result = control_flow_ops.cond(pred, fn1, fn2)\n    self.assertAllEqual([3, 4, 5, 6, 7, 8, 100], result.values)\n    self.assertAllEqual([0, 2, 3, 6, 7], result.row_splits)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testCondResource(self):\n\n    with self.cached_session():\n      rv = resource_variable_ops.ResourceVariable(True)\n      self.evaluate(variables.global_variables_initializer())\n      t = ops.convert_to_tensor(1.0)\n\n      def case():\n        assign = resource_variable_ops.assign_variable_op(rv.handle, False)\n        with ops.control_dependencies([assign]):\n          return array_ops.identity(t)\n\n      self.assertEqual(\n          1.0, self.evaluate(control_flow_ops.cond(rv, case, lambda: t)))\n\n  @test_util.run_deprecated_v1\n  def testCondResourceGradShape(self):\n    rv1 = resource_variable_ops.ResourceVariable([1.0, 2.0])\n    rv2 = resource_variable_ops.ResourceVariable([3.0, 4.0])\n    pred = constant_op.constant(True)\n    result = control_flow_ops.cond(pred, lambda: rv1, lambda: rv2)\n    grads = gradients_impl.gradients(result, [rv1, rv2])\n    self.assertAllEqual(grads[0].shape.as_list(), [2])\n    self.assertAllEqual(grads[1].shape.as_list(), [2])\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testCondWithTensorArrayGrad(self):\n    with self.cached_session() as sess:\n      with ops.device(test.gpu_device_name()):\n        pred = array_ops.placeholder(dtypes.bool, [])\n        x = constant_op.constant([1.0, 2.0, 3.0])\n        y = control_flow_ops.cond(\n            pred, lambda: map_fn.map_fn(lambda z: z * 2.0, x),\n            lambda: constant_op.constant([1.0, 1.0, 1.0]))\n        g = gradients_impl.gradients(y, x)[0]\n\n      self.assertAllEqual(sess.run(g, {pred: True}), [2.0, 2.0, 2.0])\n      self.assertAllEqual(sess.run(g, {pred: False}), [0.0, 0.0, 0.0])\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testCondIndexedSlicesDifferentTypes(self):\n    with self.cached_session():\n      values = constant_op.constant([10])\n      i_32 = ops.convert_to_tensor([0], name=\"one\", dtype=dtypes.int32)\n      i_64 = ops.convert_to_tensor([0], name=\"one\", dtype=dtypes.int64)\n      x = ops.IndexedSlices(values, i_32)\n      pred = math_ops.less(1, 2)\n      fn1 = lambda: ops.IndexedSlices(math_ops.add(x.values, 1), i_32)\n      fn2 = lambda: ops.IndexedSlices(math_ops.subtract(x.values, 1), i_64)\n      r = control_flow_ops.cond(pred, fn1, fn2)\n\n      val = r.values\n      ind = r.indices\n    self.assertAllEqual([11], val)\n    self.assertAllEqual([0], ind)\n    self.assertTrue(ind.dtype == np.int64)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testCondColocation(self):\n    with self.session(use_gpu=True):\n      with ops.device(\"/cpu:0\"):\n        v = variables.Variable(7.0)\n\n      x = constant_op.constant(10.0)\n      pred = math_ops.less(1.0, 2.0)\n      fn1 = lambda: math_ops.add(v, 1.0)\n      fn2 = lambda: math_ops.subtract(x, 1.0)\n      r = control_flow_ops.cond(pred, fn1, fn2)\n\n      for op in x.graph.get_operations():\n        if op.name == \"cond/Add/Switch\":\n          self.assertDeviceEqual(op.device, \"/cpu:0\")\n\n  def _testCond_1(self, use_gpu):\n    with self.cached_session(use_gpu=use_gpu):\n      x = constant_op.constant(10)\n      pred = math_ops.less(1, 2)\n      fn1 = lambda: math_ops.add(x, 1)\n      fn2 = lambda: math_ops.subtract(x, 1)\n      r = control_flow_ops.cond(pred, fn1, fn2)\n\n      result = self.evaluate(r)\n    self.assertAllEqual(11, result)\n\n  def testCond_1(self):\n\n    self._testCond_1(use_gpu=False)\n    # TODO(b/116526896): Enable GPU tests.\n    # self._testCond_1(use_gpu=True)\n\n  def testCond_2(self):\n\n    with self.cached_session():\n      x = constant_op.constant(10)\n      r = control_flow_ops.cond(\n          math_ops.less(1, 0), lambda: math_ops.add(x, 1),\n          lambda: math_ops.subtract(x, 1))\n      result = self.evaluate(r)\n    self.assertAllEqual(9, result)\n\n  def testCond_3(self):\n\n    with self.cached_session():\n      x = constant_op.constant(10)\n      pred = math_ops.less(1, 2)\n      fn1 = lambda: math_ops.add(x, 1)\n      fn2 = lambda: math_ops.subtract(x, 1)\n      fn3 = lambda: math_ops.add(control_flow_ops.cond(pred, fn1, fn2), 1)\n      r = control_flow_ops.cond(pred, fn3, fn2)\n\n      result = self.evaluate(r)\n    self.assertAllEqual(12, result)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testCondPruning(self):\n    v1 = variables.Variable(7)\n    v2 = variables.Variable(7)\n    v3 = variables.Variable(7)\n\n    def f():\n      age = constant_op.constant(3)\n      max_age = constant_op.constant(2)\n      pred = math_ops.greater(age, max_age)\n      fn1 = lambda: [state_ops.assign(v1, 1).op, state_ops.assign(v2, 2).op]\n      fn2 = lambda: [state_ops.assign(v3, 3).op, constant_op.constant(10).op]\n      r = control_flow_ops.cond(pred, fn1, fn2)\n      self.assertEqual(len(r), 2)\n      return r[1]\n\n    f_defun = eager_function.defun(f)\n\n    if not context.executing_eagerly():\n      with self.cached_session():\n        self.evaluate(variables.global_variables_initializer())\n        result = self.evaluate(f())\n        self.assertEqual(True, result)\n        # Only second cond result was fetched, so v1 assign shouldn't run.\n        self.assertEqual(7, self.evaluate(v1))\n        self.assertEqual(2, self.evaluate(v2))\n        self.assertEqual(7, self.evaluate(v3))\n\n    result = f_defun()\n    self.assertEqual(True, self.evaluate(result))\n    # Both v1 and v2 branch assignments should be run in defun.\n    self.assertEqual(1, self.evaluate(v1))\n    self.assertEqual(2, self.evaluate(v2))\n    self.assertEqual(7, self.evaluate(v3))\n\n  def testCond_5(self):\n    with self.cached_session():\n      alive = constant_op.constant(True, name=\"alive\")\n      count = constant_op.constant(0, name=\"count\")\n\n      def body(i):\n        return control_flow_ops.cond(\n            alive, lambda: [math_ops.less(i, 3), math_ops.add(count, 1)],\n            lambda: [alive, count])\n\n      for i in range(10):\n        alive, count = body(i)\n      self.assertAllEqual(4, self.evaluate(count))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testCond_6(self):\n    with self.cached_session():\n      v1 = variables.Variable([7])\n\n      age = constant_op.constant(3)\n      pred = math_ops.greater(age, 4)\n      fn1 = lambda: age\n      fn2 = lambda: v1\n      r = control_flow_ops.cond(pred, fn1, fn2)\n\n      self.evaluate(variables.global_variables_initializer())\n      result = self.evaluate(r)\n      self.assertAllEqual(np.array([7]), result)\n\n  def testCond_7(self):\n    with self.cached_session() as sess:\n      x = constant_op.constant(10)\n      y = constant_op.constant(200)\n      pred = math_ops.less(1, 2)\n      fn1 = lambda: [math_ops.add(x, 1), math_ops.add(x, 2)]\n      fn2 = lambda: [y, y]\n      r = control_flow_ops.cond(pred, fn1, fn2)\n      self.assertAllEqual([11, 12], self.evaluate(r))\n\n  @parameterized.parameters(dtypes.float32, dtypes.float64)\n  @test_util.run_v1_only(\"Uses tf.gradients\")\n  def testCondResourceGrad(self, dtype):\n    init = constant_op.constant([7.], dtype=dtype)\n    v1 = variables.Variable(init)\n\n    age = constant_op.constant(3., dtype=dtype)\n    pred = math_ops.greater(age, 4.)\n    fn1 = lambda: age\n    fn2 = lambda: v1\n    r = control_flow_ops.cond(pred, fn1, fn2)\n\n    grad = gradients_impl.gradients(r, v1)[0]\n    self.evaluate(variables.global_variables_initializer())\n    self.assertAllEqual(grad, [1.])\n\n  @test_util.run_gpu_only\n  @test_util.run_deprecated_v1\n  def testCond_Device(self):\n    x = constant_op.constant(-10.)\n\n    # True branch function defined outside of device scope\n    def true_fn():\n      return math_ops.exp(x)\n\n    with ops.device(\"CPU:0\"):\n      r = control_flow_ops.cond(\n          constant_op.constant(True), true_fn, lambda: 0.)\n      self.assertIn(\"cpu\", r.device.lower())\n\n    with session.Session() as sess:\n      options = config_pb2.RunOptions(output_partition_graphs=True)\n      run_metadata = config_pb2.RunMetadata()\n      sess.run(r, options=options, run_metadata=run_metadata)\n      # We expect that everything runs on CPU, even if GPU is available.\n      self.assertEqual(len(run_metadata.partition_graphs), 1)\n\n  def _count_matching_switch_nodes_on_device(self, run_metadata, device_str):\n    # Returns the number of Switch nodes with type float32 placed on\n    # `device_str`.\n    device_graphs = [\n        g for g in run_metadata.partition_graphs\n        if device_str in g.node[0].device\n    ]\n    self.assertLen(device_graphs, 1)\n    switch_nodes = [\n        n for n in device_graphs[0].node if n.op == \"Switch\" and\n        n.attr[\"T\"].type == dtypes.float32.as_datatype_enum\n    ]\n    return len(switch_nodes)\n\n  @test_util.run_gpu_only\n  @test_util.run_deprecated_v1\n  def testCondSwitchColocatedWithInputWhenInputOnCPU(self):\n    x = array_ops.placeholder(dtypes.float32)\n\n    # `arg` is used in the cond then branch so a Switch node is created for it.\n    # We test that the Switch node gets placed on the same device as `arg`.\n    # We force `arg` to be on CPU here.\n    with ops.device(\"CPU:0\"):\n      arg = x + 10.\n\n    def true_fn():\n      with ops.device(\"CPU:0\"):\n        return arg + 1\n\n    r = control_flow_ops.cond(constant_op.constant(True), true_fn, lambda: 0.)\n\n    with session.Session() as sess:\n      run_metadata = config_pb2.RunMetadata()\n      options = config_pb2.RunOptions(output_partition_graphs=True)\n      sess.run(\n          r, feed_dict={x: -10.}, options=options, run_metadata=run_metadata)\n      self.assertEqual(len(run_metadata.partition_graphs), 2)\n      # Check that the Switch for `arg` gets placed on CPU.\n      self.assertEqual(\n          self._count_matching_switch_nodes_on_device(run_metadata, \"CPU\"), 1)\n      self.assertEqual(\n          self._count_matching_switch_nodes_on_device(run_metadata, \"GPU\"), 0)\n\n  @test_util.run_gpu_only\n  @test_util.run_deprecated_v1\n  def testCondSwitchColocatedWithInputWhenInputOnGPU(self):\n    x = array_ops.placeholder(dtypes.float32)\n\n    # `arg` is used in the cond then branch so a Switch node is created for it.\n    # We test that the Switch node gets placed on the same device as `arg`.\n    # Note: `arg` gets placed on GPU by default by the placer.\n    arg = x + 10.\n\n    def true_fn():\n      with ops.device(\"CPU:0\"):\n        return arg + 1\n\n    r = control_flow_ops.cond(constant_op.constant(True), true_fn, lambda: 0.)\n\n    with session.Session() as sess:\n      run_metadata = config_pb2.RunMetadata()\n      options = config_pb2.RunOptions(output_partition_graphs=True)\n      sess.run(\n          r, feed_dict={x: -10.}, options=options, run_metadata=run_metadata)\n      self.assertEqual(len(run_metadata.partition_graphs), 2)\n      # Check that the Switch for `arg` gets placed on GPU.\n      self.assertEqual(\n          self._count_matching_switch_nodes_on_device(run_metadata, \"CPU\"), 0)\n      self.assertEqual(\n          self._count_matching_switch_nodes_on_device(run_metadata, \"GPU\"), 1)\n\n  def testCondAccessTrueBranchTensorInFalseBranchRaises(self):\n\n    @def_function.function\n    def f():\n      c = constant_op.constant(1.)\n      inputs = {\"c\": c}\n\n      def true_fn(inputs):\n        inputs[\"c\"] = array_ops.identity(inputs[\"c\"], name=\"true_branch\")\n        return inputs[\"c\"]\n\n      def false_fn(inputs):\n        return array_ops.identity(inputs[\"c\"])\n\n      pred = constant_op.constant(True)\n      return control_flow_ops.cond(\n          pred, lambda: true_fn(inputs), lambda: false_fn(inputs))\n\n    # This was needed for backwards compatibility with TF2 Estimators which\n    # rely on variable names.\n    prefix = \"cond/\" if context.executing_eagerly() else \"\"\n\n    with self.assertRaisesRegex(\n        ValueError,\n        \"Tensor %strue_branch:0 in true_fn is accessed from false_fn.\" %\n        prefix):\n      f()\n\n  def testSwitchCaseAccessBranch1TensorInBranch4Raises(self):\n\n    @def_function.function\n    def f():\n      c = constant_op.constant(1.)\n      inputs = {\"c\": c}\n\n      def br1_fn(inputs):\n        inputs[\"c\"] = array_ops.identity(inputs[\"c\"], name=\"br1_identity\")\n        return inputs[\"c\"]\n\n      def br4_fn(inputs):\n        return array_ops.identity(inputs[\"c\"])\n\n      def other_fn():\n        return array_ops.identity(c)\n\n      return control_flow_ops.switch_case(\n          constant_op.constant(2),\n          [other_fn, lambda: br1_fn(inputs), other_fn, other_fn,\n           lambda: br4_fn(inputs)])\n\n    # This was needed for backwards compatibility with TF2 Estimators which\n    # rely on variable names.\n    prefix = \"switch_case/indexed_case/\" if context.executing_eagerly() else \"\"\n    with self.assertRaisesRegex(\n        ValueError, \"Tensor %sbr1_identity:0 in branch 1 is \"\n        \"accessed from branch 4.\" % prefix):\n      f()\n\n  def testCondListOutput(self):\n    with self.cached_session() as sess:\n      x = constant_op.constant(10)\n      y = constant_op.constant(200)\n      pred = math_ops.less(1, 2)\n      fn1 = lambda: [math_ops.add(x, y), math_ops.add(x, y)]\n      fn2 = lambda: [y, y]\n      r = control_flow_ops.cond(pred, fn1, fn2)\n      test_result = self.evaluate(r)\n      self.assertListEqual([210, 210], test_result)\n\n  def testTupleOutput(self):\n    with self.cached_session() as sess:\n      x = constant_op.constant(10)\n      y = constant_op.constant(200)\n      pred = math_ops.less(1, 2)\n      fn1 = lambda: (math_ops.add(x, y), math_ops.add(x, y))\n      fn2 = lambda: (y, y)\n      r = control_flow_ops.cond(pred, fn1, fn2)\n      test_result = self.evaluate(r)\n      self.assertTupleEqual((210, 210), test_result)\n\n  def testDictOutput(self):\n    with self.cached_session() as sess:\n      x = constant_op.constant(10)\n      y = constant_op.constant(200)\n      pred = math_ops.less(1, 2)\n      fn1 = lambda: {\"a\": math_ops.add(x, y), \"b\": math_ops.add(x, y)}\n      fn2 = lambda: {\"a\": y, \"b\": y}\n      r = control_flow_ops.cond(pred, fn1, fn2)\n      test_result = self.evaluate(r)\n      self.assertDictEqual({\"a\": 210, \"b\": 210}, test_result)\n\n  def testEmbeddedListOutput(self):\n    x = constant_op.constant(10)\n    y = constant_op.constant(200)\n    pred = math_ops.less(1, 2)\n    fn1 = lambda: [[math_ops.add(x, y), math_ops.add(x, y)]]\n    fn2 = lambda: [[y, y]]\n    # Pass strict=True flag as cond_v2 allows for tensors to be\n    # in nested output structures as singletons\n    r = control_flow_ops.cond(pred, fn1, fn2, strict=True)\n    test_result = self.evaluate(r)\n    self.assertListEqual([[210, 210]], test_result)\n\n  def testEmbeddedTupleOutput(self):\n    with self.cached_session() as sess:\n      x = constant_op.constant(10)\n      y = constant_op.constant(200)\n      pred = math_ops.less(1, 2)\n      fn1 = lambda: ((math_ops.add(x, y), math_ops.add(x, y)))\n      fn2 = lambda: ((y, y))\n      r = control_flow_ops.cond(pred, fn1, fn2)\n      test_result = self.evaluate(r)\n      self.assertTupleEqual(((210, 210)), test_result)\n\n  def testEmbeddedDictOutput(self):\n    with self.cached_session() as sess:\n      x = constant_op.constant(10)\n      y = constant_op.constant(200)\n      pred = math_ops.less(1, 2)\n      fn1 = lambda: {\"a\": {\"c\": math_ops.add(x, y)},\n                     \"b\": {\"d\": math_ops.add(x, y)}}\n      fn2 = lambda: {\"a\": {\"c\": y},\n                     \"b\": {\"d\": y}}\n      r = control_flow_ops.cond(pred, fn1, fn2)\n      test_result = self.evaluate(r)\n      self.assertDictEqual({\"a\": {\"c\": 210}, \"b\": {\"d\": 210}}, test_result)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testCheckNestedOutputStruct(self):\n    with self.cached_session() as sess:\n      x = constant_op.constant(10)\n      y = constant_op.constant(200)\n      pred = math_ops.less(1, 2)\n      fn1 = lambda: {\"a\": math_ops.add(x, y), \"b\": math_ops.add(x, y)}\n      fn2 = lambda: {\"c\": y, \"d\": y}\n      v1_msg = \"The two structures don't have the same nested structure\"\n      v2_msg = (\"true_fn and false_fn arguments to tf.cond must have the same \"\n                \"number, type, and overall structure of return values.\")\n      with self.assertRaisesRegex(\n          TypeError if control_flow_util.ENABLE_CONTROL_FLOW_V2 else ValueError,\n          v2_msg if control_flow_util.ENABLE_CONTROL_FLOW_V2 else v1_msg):\n        control_flow_ops.cond(pred, fn1, fn2)\n\n  @test_util.run_deprecated_v1\n  def testCondRef(self):\n\n    with self.cached_session():\n      x = gen_state_ops.variable(\n          shape=[1],\n          dtype=dtypes.float32,\n          name=\"x\",\n          container=\"\",\n          shared_name=\"\")\n      true_fn = lambda: x\n      false_fn = lambda: constant_op.constant([2.0])\n      r = control_flow_ops.cond(constant_op.constant(False), true_fn, false_fn)\n      self.assertAllEqual([2.0], self.evaluate(r))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testCondWithControl(self):\n    with self.cached_session() as sess:\n      control_holder = array_ops.placeholder(dtypes.float32, shape=())\n      a = constant_op.constant(3)\n\n      def true_branch():\n        with ops.control_dependencies([control_holder]):\n          _ = a + 1\n        return a + 2\n\n      r = control_flow_ops.cond(\n          constant_op.constant(True), true_branch,\n          lambda: constant_op.constant(1))\n      result = sess.run(r, feed_dict={control_holder: 5.})\n      self.assertEqual(5, result)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testUninitializedRefIdentity(self):\n    with self.cached_session() as sess:\n      v = gen_state_ops.variable(\n          shape=[1],\n          dtype=dtypes.float32,\n          name=\"v\",\n          container=\"\",\n          shared_name=\"\")\n      inited = state_ops.is_variable_initialized(v)\n      v_f, v_t = control_flow_ops.ref_switch(v, inited)\n      # Both v_f and v_t are uninitialized references. However, an actual use\n      # of the reference in the 'true' branch in the 'tf.identity' op will\n      # not 'fire' when v is uninitialized, so this is a valid construction.\n      # This test tests that ref_identity allows uninitialized ref as input\n      # so that this construction is allowed.\n      v_f_op = gen_array_ops.ref_identity(v_f)\n      v_t_op = gen_array_ops.ref_identity(v_t)\n      with ops.control_dependencies([v_f_op]):\n        assign_v = state_ops.assign(v, [1.0])\n      with ops.control_dependencies([v_t_op]):\n        orig_v = array_ops.identity(v)\n      merged_op = control_flow_ops.merge([assign_v, orig_v])\n      self.assertAllEqual([1.0], self.evaluate(merged_op.output))\n\n  def testCondSwitchIdentity(self):\n    # Make sure the recv identity is not removed by optimization.\n    with session.Session(config=opt_cfg()) as sess:\n      pred = constant_op.constant(True)\n\n      def fn1():\n        return control_flow_ops.no_op()\n\n      def fn2():\n        return control_flow_ops.Assert(False, [\"Wrong branch!!!\"])\n\n      r = control_flow_ops.cond(pred, fn1, fn2)\n      self.evaluate(r)\n\n  def testCondRecvIdentity(self):\n    # Make sure the switch identity is not removed by optimization.\n    with session.Session(config=opt_cfg()) as sess:\n      with ops.device(test.gpu_device_name()):\n        pred = constant_op.constant(True)\n\n      def fn1():\n        return control_flow_ops.no_op()\n\n      def fn2():\n        with ops.device(\"/cpu:0\"):\n          return control_flow_ops.Assert(False, [\"Wrong branch!!!\"])\n\n      r = control_flow_ops.cond(pred, fn1, fn2)\n      self.evaluate(r)\n\n  @test_util.run_deprecated_v1\n  @test_util.enable_control_flow_v2\n  def testDisableLoweringSwitchMerge(self):\n    if test_util.is_gpu_available():\n      self.skipTest(\n          \"Single threaded executor doesn't support partitioned graphs.  \"\n          \"Skipping GPU test.\")\n    # Make pred feedable to ensure we don't constant-fold it out.\n    run_opts = config_pb2.RunOptions(\n        trace_level=config_pb2.RunOptions.FULL_TRACE)\n    run_metadata_no_lowering = config_pb2.RunMetadata()\n    run_metadata_with_lowering = config_pb2.RunMetadata()\n\n    config = opt_cfg(do_constant_folding=False)\n\n    pred = array_ops.placeholder_with_default(\n        constant_op.constant(True), shape=())\n    r = control_flow_ops.cond(pred, lambda: True, lambda: False)\n\n    with session.Session(config=config) as sess:\n      r_value = sess.run(\n          r, options=run_opts, run_metadata=run_metadata_with_lowering)\n      self.assertEqual(r_value, True)\n\n    # Use the single threaded executor, which disables control flow lowering.\n    config.experimental.executor_type = \"SINGLE_THREADED_EXECUTOR\"\n    with session.Session(config=config) as sess:\n      r_value = sess.run(\n          r, options=run_opts, run_metadata=run_metadata_no_lowering)\n      self.assertEqual(r_value, True)\n\n    self.assertTrue(  # pylint: disable=g-complex-comprehension\n        any(\"switch\" in ns.node_name\n            for dev_stat in run_metadata_with_lowering.step_stats.dev_stats\n            for ns in dev_stat.node_stats))\n\n    self.assertTrue(  # pylint: disable=g-complex-comprehension\n        all(\"switch\" not in ns.node_name\n            for dev_stat in run_metadata_no_lowering.step_stats.dev_stats\n            for ns in dev_stat.node_stats))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testCondGrad_1(self):\n    with self.cached_session():\n      x = constant_op.constant(10.0, name=\"x\")\n      pred = math_ops.less(1, 2)\n      fn1 = lambda: array_ops.identity(x)\n      fn2 = lambda: array_ops.identity(x)\n      r = control_flow_ops.cond(pred, fn1, fn2)\n\n      grad = gradients_impl.gradients(r, [x])[0]\n      self.assertAllEqual(1.0, self.evaluate(grad))\n\n  @test_util.run_deprecated_v1\n  @test_util.enable_control_flow_v2\n  def testCondComputeGradAfterSessRunFails(self):\n    with self.cached_session():\n      x = constant_op.constant(10.0, name=\"x\")\n      pred = math_ops.less(1, 2)\n\n      def true_fn():\n        a = x * x\n        return a * a\n\n      def false_fn():\n        return x * x\n\n      r = control_flow_ops.cond(pred, true_fn, false_fn)\n\n      self.assertAllEqual(r, 10000.)\n      grad = gradients_impl.gradients(r, [x])[0]\n      with self.assertRaisesRegex(\n          errors_impl.InvalidArgumentError,\n          r\"Connecting to invalid output 1 of source node cond which has 1 \"\n          r\"outputs. Try using \"\n          \"tf.compat.v1.experimental.output_all_intermediates\\(True\\).\"):\n        self.evaluate(grad)\n\n  @test_util.run_deprecated_v1\n  @test_util.enable_output_all_intermediates\n  def testCondComputeGradAfterSessRun(self):\n    with self.cached_session():\n      x = constant_op.constant(10.0, name=\"x\")\n      pred = math_ops.less(1, 2)\n\n      def true_fn():\n        a = x * x\n        return a * a\n\n      def false_fn():\n        return x * x\n\n      r = control_flow_ops.cond(pred, true_fn, false_fn)\n\n      self.assertAllEqual(r, 10000.)\n      grad = gradients_impl.gradients(r, [x])[0]\n      self.assertAllEqual(grad, 4000.)\n\n  @test_util.run_deprecated_v1\n  @test_util.enable_output_all_intermediates\n  def testNestedCondComputeGradAfterSessRun(self):\n    with self.cached_session():\n      x = constant_op.constant(10.0, name=\"x\")\n      pred = math_ops.less(1, 2)\n\n      def true_fn():\n\n        def inner_true_fn():\n          a = x * x\n          return a * a\n\n        def inner_false_fn():\n          return x * x\n\n        return control_flow_ops.cond(\n            constant_op.constant(True), inner_true_fn, inner_false_fn)\n\n      def false_fn():\n        return x * x\n\n      r = control_flow_ops.cond(pred, true_fn, false_fn)\n\n      self.assertAllEqual(r, 10000.)\n      grad = gradients_impl.gradients(r, [x])[0]\n      self.assertAllEqual(grad, 4000.)\n\n  @test_util.run_deprecated_v1\n  def testCondGrad_2(self):\n    with self.cached_session():\n      c = array_ops.placeholder(dtypes.int32, shape=[])\n      x = constant_op.constant(10.0)\n      pred = math_ops.less(c, 2)\n      fn1 = lambda: math_ops.multiply(x, 42.0)\n      fn2 = lambda: math_ops.multiply(x, 3.0)\n      r = control_flow_ops.cond(pred, fn1, fn2)\n\n      grad = gradients_impl.gradients(r, [x])[0]\n      self.assertAllEqual(42.0, grad.eval(feed_dict={c: 1}))\n      self.assertAllEqual(3.0, grad.eval(feed_dict={c: 3}))\n\n  @test_util.disable_control_flow_v2(\n      \"b/110550782 (gradient w.r.t external variable)\")\n  @test_util.run_deprecated_v1\n  def testCondGrad_3(self):\n    with self.cached_session():\n      c = array_ops.placeholder(dtypes.int32, shape=[])\n      ox = constant_op.constant(10.0)\n      pred = math_ops.less(c, 2)\n\n      def fn1(x):\n        m = x * x\n        return gradients_impl.gradients(m, [ox])[0]\n\n      fn2 = lambda: math_ops.multiply(ox, 3.0)\n      y = math_ops.multiply(7.0, ox)\n      r = control_flow_ops.cond(pred, lambda: fn1(y), fn2)\n\n      self.assertAllEqual(980.0, r.eval(feed_dict={c: 1}))\n      self.assertAllEqual(30.0, r.eval(feed_dict={c: 3}))\n\n  @test_util.run_deprecated_v1\n  def testCondGradMultiDevice(self):\n    config = config_pb2.ConfigProto(device_count={\"CPU\": 2},\n                                    allow_soft_placement=True)\n    with self.cached_session(use_gpu=True, config=config) as sess:\n      pred = array_ops.placeholder(dtypes.bool, [])\n      x = array_ops.placeholder(dtypes.float32)\n      y = array_ops.placeholder(dtypes.float32)\n\n      with ops.device(\"/cpu:0\"):\n        z = control_flow_ops.cond(pred, lambda: x * y * 2.0, lambda: 2.0)\n\n      with ops.device(\"/cpu:1\"):\n        grad = gradients_impl.gradients(z, x)[0]\n\n      with ops.device(\"/cpu:0\"):\n        grad_grad = gradients_impl.gradients(grad, x)[0]\n\n      self.assertEqual(sess.run(grad, {pred: True, x: 1.0, y: 2.0}), 4.0)\n      self.assertEqual(sess.run(grad, {pred: False, x: 1.0, y: 2.0}), 0.0)\n\n      # v1 control flow gets None second derivative for some reason.\n      if not control_flow_util.ENABLE_CONTROL_FLOW_V2:\n        self.assertIsNone(grad_grad)\n        return\n\n      self.assertEqual(sess.run(grad_grad, {pred: True, x: 1.0, y: 2.0}), 0.0)\n      self.assertEqual(sess.run(grad_grad, {pred: False, x: 1.0, y: 2.0}), 0.0)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testNestedCond_Simple(self):\n    with self.cached_session():\n      x = constant_op.constant(0., name=\"X\")\n      y = control_flow_ops.cond(\n          constant_op.constant(True), lambda: x,\n          lambda: control_flow_ops.cond(x < 1., lambda: x, lambda: x))\n      result = gradients_impl.gradients(y, x)[0]\n      self.assertEqual(1.0, self.evaluate(result))\n\n      z = control_flow_ops.cond(\n          constant_op.constant(False), lambda: x,\n          lambda: control_flow_ops.cond(x < 1., lambda: x, lambda: x))\n      result = gradients_impl.gradients(z, x)[0]\n      self.assertEqual(1.0, self.evaluate(result))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testCondGrad_Gather(self):\n    with self.cached_session() as sess:\n      v1 = variables.Variable([1.0, 42.0])\n      c = array_ops.placeholder(dtypes.int32, shape=[])\n      pred = math_ops.less(c, 2)\n      fn1 = lambda: array_ops.identity(v1)\n      fn2 = lambda: array_ops.gather(v1, [1, 1])\n      r = control_flow_ops.cond(pred, fn1, fn2)\n      # The following `grad` is a Tensor since it is the aggregation of an\n      # IndexedSlice and a Tensor. It is an `IndexedSlices` with control flow\n      # v2.\n      grad = gradients_impl.gradients(r, [v1])[0]\n      self.evaluate(variables.global_variables_initializer())\n\n      if control_flow_util.ENABLE_CONTROL_FLOW_V2:\n        self.assertIsInstance(grad, ops.IndexedSlices)\n\n      grad_value = sess.run(grad, feed_dict={c: 1})\n      self.assertAllEqual(gradient_checker_v2._to_numpy(grad_value), [1.0, 1.0])\n\n      grad_value = sess.run(grad, feed_dict={c: 3})\n      self.assertAllEqual(gradient_checker_v2._to_numpy(grad_value), [0.0, 2.0])\n\n  @test_util.run_deprecated_v1\n  def testCondGrad_ResourceVarSparseRead(self):\n    # NOTE(skyewm): this test is interesting because the\n    # ResourceVariable.sparse_read gradient function returns IndexedSlices.\n    var = resource_variable_ops.ResourceVariable(\n        np.ones((4, 2), dtype=np.float32))\n    x = constant_op.constant(1.0)\n    r = control_flow_ops.cond(\n        constant_op.constant(True),\n        lambda: x * math_ops.reduce_sum(var.sparse_read([1, 2])),\n        lambda: constant_op.constant(np.zeros((2, 3)),\n                                     dtype=dtypes.float32))\n    grad = gradients_impl.gradients(r, var)[0]\n\n    self.evaluate(variables.global_variables_initializer())\n    grad_val = self.evaluate(grad)\n    self.assertIsInstance(grad_val, ops.IndexedSlicesValue)\n    self.assertAllEqual(gradient_checker_v2._to_numpy(grad_val), [[0., 0.],\n                                                                  [1., 1.],\n                                                                  [1., 1.],\n                                                                  [0., 0.]])\n\n  def testCondGrad_MultiGather(self):\n    # NOTE(skyewm): this test is interesting because the array_ops.gather and\n    # ResourceVariable.sparse_read gradient functions returns IndexedSlices.\n    var = resource_variable_ops.ResourceVariable(\n        np.ones((4, 2), dtype=np.float32))\n    x1 = constant_op.constant(np.ones((3, 3), dtype=np.float32))\n    x2 = constant_op.constant(2.0)\n\n    def true_fn():\n      y1 = var.sparse_read([1, 2])\n      y2 = array_ops.gather(x1, [2]) * x2\n      y3 = x2 * [1., 1., 1.]\n      return y1, y2, y3\n\n    def false_fn():\n      y1 = np.zeros((2, 2), dtype=np.float32)\n      y2 = array_ops.gather(x1, [2]) * x2\n      y3 = array_ops.gather(x1, [2])\n      return y1, y2, y3\n\n    @def_function.function\n    def foo():\n      r = control_flow_ops.cond(constant_op.constant(True), true_fn, false_fn)\n      return gradients_impl.gradients(r, [var, x1, x2])\n\n    grad = foo()\n    self.evaluate(variables.global_variables_initializer())\n    var_grad, x1_grad, x2_grad = self.evaluate(grad)\n    self.assertIsInstance(var_grad, ops.IndexedSlicesValue)\n    self.assertAllEqual(gradient_checker_v2._to_numpy(var_grad), [[0., 0.],\n                                                                  [1., 1.],\n                                                                  [1., 1.],\n                                                                  [0., 0]])\n    self.assertIsInstance(x1_grad, ops.IndexedSlicesValue)\n    self.assertAllEqual(gradient_checker_v2._to_numpy(x1_grad), [[0., 0., 0.],\n                                                                 [0., 0., 0.],\n                                                                 [2., 2., 2.]])\n    self.assertIsInstance(x1_grad, ops.IndexedSlicesValue)\n    self.assertEqual(gradient_checker_v2._to_numpy(x2_grad), 6.)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testCondPredicateTensor(self):\n    \"\"\"Regression test for lowering predicate from non-first output of an op.\"\"\"\n\n    @eager_function.defun\n    def foo():\n      return constant_op.constant(\"foo\"), constant_op.constant(True)\n\n    r = control_flow_ops.cond(foo()[1], lambda: 1.0, lambda: 2.0)\n    self.assertEqual(self.evaluate(r), 1.0)\n\n  @test_util.run_v1_only(\"Tests Session.run() pruning logic.\")\n  def testCondFeedConstantPredicate(self):\n    with self.cached_session() as sess:\n      value = constant_op.constant(37.0)\n      predicate = constant_op.constant(True)\n      cond_output = control_flow_ops.cond(\n          predicate, lambda: constant_op.constant(0.0), lambda: value)\n      result = array_ops.identity(cond_output)\n      self.assertEqual(37.0, sess.run(result, feed_dict={predicate: False}))\n      self.assertEqual(0.0, sess.run(result, feed_dict={predicate: True}))\n      self.assertEqual(0.0, sess.run(result))\n\n  @test_util.run_v1_only(\"Tests Session.run() pruning logic.\")\n  def testCondFeedPlaceholderWithDefaultPredicate(self):\n    with self.cached_session() as sess:\n      value = constant_op.constant(37.0)\n      predicate = array_ops.placeholder_with_default(\n          constant_op.constant(True), [])\n      cond_output = control_flow_ops.cond(\n          predicate, lambda: constant_op.constant(0.0), lambda: value)\n      result = array_ops.identity(cond_output)\n      self.assertAllEqual(37.0, sess.run(result, feed_dict={predicate: False}))\n      self.assertAllEqual(0.0, sess.run(result, feed_dict={predicate: True}))\n      self.assertAllEqual(0.0, sess.run(result))\n\n  @test_util.run_in_graph_and_eager_modes\n  def testCondAutoControlDeps(self):\n    if test_util.is_gpu_available():\n      self.skipTest(\"b/128676188 causes OOM on opensource gpu tests\")\n\n    print_prefix = \"testCondAutoControlDeps: \"\n\n    def branch_fn():\n      enqueue_print_op(\"A\")\n      enqueue_print_op(\"B\")\n      with ops.control_dependencies([enqueue_print_op(\"C\")]):\n        return constant_op.constant(10)\n\n    def build_cond():\n      return control_flow_ops.cond(\n          constant_op.constant(True), branch_fn, lambda: 0)\n\n    def build_nested_cond():\n      return control_flow_ops.cond(\n          constant_op.constant(True), build_cond, lambda: 0)\n\n    # In v1 graph mode, pruning should make only \"C\" print.\n    if not context.executing_eagerly():\n      with self.cached_session():\n        with self.captureWritesToStream(sys.stderr) as printed:\n          self.assertEqual(self.evaluate(build_cond()), 10)\n        self.assertEqual([\"C\"], filter_test_messages(printed.contents()))\n\n        with self.captureWritesToStream(sys.stderr) as printed:\n          self.assertEqual(self.evaluate(build_nested_cond()), 10)\n        self.assertEqual([\"C\"], filter_test_messages(printed.contents()))\n\n    # In defuns, all prints should execute in program order.\n    # This doesn't work with legacy control flow.\n    if control_flow_util.ENABLE_CONTROL_FLOW_V2:\n\n      @eager_function.defun\n      def cond():\n        return build_cond()\n\n      with self.captureWritesToStream(sys.stderr) as printed:\n        self.assertEqual(self.evaluate(cond()), 10)\n      self.assertEqual([\"A\", \"B\", \"C\"],\n                       filter_test_messages(printed.contents()))\n\n      @eager_function.defun\n      def nested_cond():\n        return build_nested_cond()\n\n      with self.captureWritesToStream(sys.stderr) as printed:\n        self.assertEqual(self.evaluate(nested_cond()), 10)\n      self.assertEqual([\"A\", \"B\", \"C\"],\n                       filter_test_messages(printed.contents()))\n\n    # wrap_function should prune.\n    def pruned_cond():\n      return build_cond()\n    pruned_cond = wrap_function.wrap_function(pruned_cond, [])\n\n    with self.captureWritesToStream(sys.stderr) as printed:\n      self.assertEqual(self.evaluate(pruned_cond()), 10)\n    self.assertEqual([\"C\"], filter_test_messages(printed.contents()))\n\n    def pruned_nested_cond():\n      return build_nested_cond()\n    pruned_nested_cond = wrap_function.wrap_function(pruned_nested_cond, [])\n\n    with self.captureWritesToStream(sys.stderr) as printed:\n      self.assertEqual(self.evaluate(pruned_nested_cond()), 10)\n    self.assertEqual([\"C\"], filter_test_messages(printed.contents()))\n\n\n  @test_util.run_in_graph_and_eager_modes\n  def testWhileAutoControlDeps(self):\n    # Legacy while_loop fails this test because it produces deprecation notices\n    # in stderr.\n    if not control_flow_util.ENABLE_CONTROL_FLOW_V2: return\n\n    def cond(i, unused_x):\n      enqueue_print_op(\"A\")\n      return i < 2\n\n    def body(i, x):\n      enqueue_print_op(\"B\")\n      with ops.control_dependencies([enqueue_print_op(\"C\")]):\n        x = array_ops.identity(x)\n      with ops.control_dependencies([enqueue_print_op(\"D\")]):\n        return i + 1, x\n\n    def build_while():\n      return control_flow_ops.while_loop(\n          cond, body, [constant_op.constant(0), constant_op.constant(0)])\n\n    def build_nested_while():\n      return control_flow_ops.cond(\n          constant_op.constant(True), build_while, lambda: [0, 0])\n\n    # In v1 graph mode, pruning should make only \"D\" print.\n    if not context.executing_eagerly():\n      with self.cached_session():\n        with self.captureWritesToStream(sys.stderr) as printed:\n          self.assertEqual(self.evaluate(build_while()[0]), 2)\n        self.assertEqual([\"D\", \"D\"], filter_test_messages(printed.contents()))\n\n        with self.captureWritesToStream(sys.stderr) as printed:\n          self.assertEqual(self.evaluate(build_nested_while()[0]), 2)\n        self.assertEqual([\"D\", \"D\"], filter_test_messages(printed.contents()))\n\n    # In defuns, all prints should execute in program order.\n    @eager_function.defun\n    def while_loop():\n      return build_while()[0]\n\n    with self.captureWritesToStream(sys.stderr) as printed:\n      self.assertEqual(self.evaluate(while_loop()), 2)\n    self.assertEqual([\"A\", \"B\", \"C\", \"D\", \"A\", \"B\", \"C\", \"D\", \"A\"],\n                     filter_test_messages(printed.contents()))\n\n    @eager_function.defun\n    def nested_while_loop():\n      return build_nested_while()[0]\n\n    with self.captureWritesToStream(sys.stderr) as printed:\n      self.assertEqual(self.evaluate(nested_while_loop()), 2)\n    self.assertEqual([\"A\", \"B\", \"C\", \"D\", \"A\", \"B\", \"C\", \"D\", \"A\"],\n                     filter_test_messages(printed.contents()))\n\n    # wrap_function should prune.\n    def pruned_while():\n      return build_while()[0]\n    pruned_while = wrap_function.wrap_function(pruned_while, [])\n\n    with self.captureWritesToStream(sys.stderr) as printed:\n      self.assertEqual(self.evaluate(pruned_while()), 2)\n    self.assertEqual([\"D\", \"D\"], filter_test_messages(printed.contents()))\n\n    def pruned_nested_while():\n      return build_nested_while()[0]\n    pruned_nested_while = wrap_function.wrap_function(pruned_nested_while, [])\n\n    with self.captureWritesToStream(sys.stderr) as printed:\n      self.assertEqual(self.evaluate(pruned_nested_while()), 2)\n    self.assertEqual([\"D\", \"D\"], filter_test_messages(printed.contents()))\n\n  # Microbenchmark: 256,000 iterations/s.\n  def testWhile_1(self):\n    with self.cached_session():\n      n = constant_op.constant(0)\n      c = lambda x: math_ops.less(x, 10000)\n      b = lambda x: math_ops.add(x, 1)\n      r = control_flow_ops.while_loop(c, b, [n], parallel_iterations=20)\n      self.assertEqual(10000, self.evaluate(r))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileExternalControlDependencies(self):\n    with self.cached_session():\n      v = variables.Variable(0.0)\n      self.evaluate(v.initializer)\n      increment = v.assign_add(1.0).read_value()\n\n      def body_fn(i):\n        with ops.control_dependencies([increment]):\n          return i + 1\n\n      result = control_flow_ops.while_loop(cond=lambda i: i < 2,\n                                           body=body_fn, loop_vars=[1])\n      self.assertAllEqual(result, 2)\n      self.assertAllEqual(v.read_value(), 1.0)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileExternalControlDependenciesNoInput(self):\n    with self.cached_session():\n      v = variables.Variable(0.0)\n      self.evaluate(v.initializer)\n      # TODO(apassos): figure out why the reading is necessary here.\n      increment = v.assign_add(1.0).read_value()\n\n      def body_fn(unused_i):\n        with ops.control_dependencies([increment]):\n          return constant_op.constant(5, name=\"five\")\n\n      result = control_flow_ops.while_loop(cond=lambda i: i < 5,\n                                           body=body_fn, loop_vars=[0])\n      self.evaluate(result)\n      self.assertAllEqual(self.evaluate(v), 1.0)\n\n  @test_util.disable_control_flow_v2(\"b/113324949 (RefVariable)\")\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileWithRefs_1(self):\n    with self.cached_session() as sess:\n      x = variables.VariableV1(0)._ref()  # pylint: disable=protected-access\n      i = constant_op.constant(0)\n      c = lambda i, x: math_ops.less(i, 100)\n\n      self.assertEqual(x.dtype, dtypes.int32_ref)\n\n      def b(i, x):\n        self.assertEqual(x.dtype, dtypes.int32_ref)\n        return (i + 1, gen_array_ops.ref_identity(x))\n\n      r = control_flow_ops.while_loop(c, b, [i, x], parallel_iterations=5)\n\n      self.evaluate(variables.global_variables_initializer())\n\n      self.assertEqual(r[0].dtype, dtypes.int32)\n      self.assertEqual(r[1].dtype, dtypes.int32_ref)\n\n      value_i, value_x = self.evaluate(r)\n\n    self.assertEqual(100, value_i)\n    self.assertEqual(0, value_x)\n\n  def testWhile_2(self):\n    with self.cached_session():\n      s = constant_op.constant(0)\n      r = isum(s)\n      self.assertAllEqual(45, self.evaluate(r))\n\n  def testWhileWithMaximumIterations(self):\n    with self.cached_session():\n      s = constant_op.constant([1, 2, 3, 4, 5])\n      r = isum(s, maximum_iterations=3)\n      self.assertAllEqual([1 + 3, 2 + 3, 3 + 3, 4 + 3, 5 + 3], self.evaluate(r))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileWithMaximumIterationsAndSingleArgument(self):\n    with self.cached_session():\n      r = control_flow_ops.while_loop(\n          lambda i: i < 3, lambda i: i + 1, [0], maximum_iterations=1)\n      self.assertEqual(1, self.evaluate(r))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testXLAGradInLoop(self):\n    # We have an optimization that moves certain reduction ops, this test makes\n    # sure we don't do that for XLA ops.\n\n    # Use dynamic inputs, which triggers the creation of \"BroadcastGradientArgs\"\n    # and \"Shape\" op.\n    input1 = array_ops.placeholder(dtype=dtypes.float32, shape=[None, None])\n    input2 = array_ops.placeholder(dtype=dtypes.float32, shape=[None, None])\n    def cond(i1, i2):\n      return False\n\n    def body(i1, i2):\n      return math_ops.add(i1, i2), math_ops.add(i1, i2)\n\n    xla_context = control_flow_ops.XLAControlFlowContext()\n    xla_context.Enter()\n\n    out1, _ = control_flow_ops.while_loop(\n        cond, body, (input1, input2), maximum_iterations=2)\n    g = gradients_impl.gradients(out1, [input1])\n\n    for op in out1.graph.get_operations():\n      # Test that the \"Shape\" is directly passed to BroadcastGradientArgs\n      # instead of being pushed to the stack.\n      if op.type == \"BroadcastGradientArgs\":\n        self.assertEqual(op.inputs[0].op.type, \"Shape\")\n        self.assertEqual(op.inputs[1].op.type, \"Shape\")\n    xla_context.Exit()\n\n\n  @test_util.disable_control_flow_v2(\"b/115776323 (max_iters)\")\n  @test_util.run_v1_only(\"b/120545219\")\n  def testSingleNestedMaximumIterationsWhileLoopGradientInXLAContext(self):\n    v = constant_op.constant(1.0)\n\n    def training_loop_with_gradient(i):\n      out = control_flow_ops.while_loop(\n          lambda i_, _: i_ < 3,\n          lambda i_, j: [i_ + 1, j * v], [0, 1.0],\n          maximum_iterations=i)\n      g = gradients_impl.gradients(out, v)\n      with ops.control_dependencies(g):\n        return i + 1\n\n    xla_context = control_flow_ops.XLAControlFlowContext()\n    xla_context.Enter()\n    # Create training loop, ensure we can call gradient() of\n    # while_loop inside the training loop.\n    loop = control_flow_ops.while_loop(lambda i: i < 3,\n                                       training_loop_with_gradient, [0])\n    xla_context.Exit()\n\n    loop_execute = array_ops.identity(loop)  # Because loop is not fetchable.\n\n    # Should execute without issue.\n    self.assertEqual(3, self.evaluate(loop_execute))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testInvalidMaximumIterationsWhileLoopGradientInXLAContext(self):\n    if control_flow_util.ENABLE_CONTROL_FLOW_V2:\n      self.skipTest(\"WhileV2 does lazy evaluation of maximum_iterations\")\n    v = constant_op.constant(1.0)\n\n    def inner_body(i, x):\n      out = control_flow_ops.while_loop(\n          lambda i, _: i < 3,\n          lambda i, j: [i + 1, j * v], [0, x],\n          maximum_iterations=i)\n      return out\n\n    def create_while_loop(maximum_iterations=None):\n      return control_flow_ops.while_loop(\n          lambda i, _: i < 3,\n          inner_body, [0, 1.0],\n          maximum_iterations=maximum_iterations)\n\n    loop_no_xla = create_while_loop(maximum_iterations=5)\n    # maximum_iterations is fine outside of an XLA scope\n    gs = gradients_impl.gradients(loop_no_xla, v)\n    self.evaluate(gs)  # This should execute without error.\n\n    xla_context = control_flow_ops.XLAControlFlowContext()\n    xla_context.Enter()\n    loop_no_maxiter = create_while_loop()\n    loop_with_maxiter = create_while_loop(maximum_iterations=2)\n    xla_context.Exit()\n\n    with self.assertRaisesRegex(\n        ValueError,\n        r\"Cannot create a gradient accumulator for tensor '.+' inside \"\n        r\"XLA while_loop because maximum_iterations was not passed to \"\n        r\"the tf.while_loop call \\('.+'\\).\"):\n      _ = gradients_impl.gradients(loop_no_maxiter, v)\n\n    with self.assertRaisesRegex(\n        ValueError,\n        r\"Cannot create a gradient accumulator for tensor '.+' inside XLA \"\n        r\"while_loop. maximum_iterations tensor '.+' for while_loop context \"\n        r\"'.+' must be statically known \\(e.g. a constant value or known \"\n        r\"shape dimension\\), or be defined at or outside the while loop \"\n        r\"context '.*' \\(currently defined in '.*'\\)\"):\n      _ = gradients_impl.gradients(loop_with_maxiter, v)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testInvalidMaximumIterationsFromSiblingContextWhileLoopInXLAContext(self):\n    v = constant_op.constant(1.0)\n\n    def create_while_loop():\n      max_iter_holder = []\n\n      def create_mi():\n        max_iter_holder.append(array_ops.placeholder(dtypes.int32, shape=()))\n        return 1.0\n\n      _ = control_flow_ops.cond(\n          constant_op.constant(True), create_mi, create_mi)\n\n      return control_flow_ops.while_loop(\n          lambda i, _: i < 3,\n          lambda i, x: (i + 1, v * x), (0, 1.0),\n          maximum_iterations=max_iter_holder[0])\n\n    if control_flow_util.ENABLE_CONTROL_FLOW_V2:\n      xla_context = control_flow_ops.XLAControlFlowContext()\n      xla_context.Enter()\n      with self.assertRaisesRegex(ValueError, r\"must be from the same graph.*\"):\n        loop = create_while_loop()\n      xla_context.Exit()\n    else:\n      xla_context = control_flow_ops.XLAControlFlowContext()\n      xla_context.Enter()\n      loop = create_while_loop()\n      xla_context.Exit()\n      with self.assertRaisesRegex(\n          ValueError,\n          r\"Cannot create a gradient accumulator for tensor '.+' inside XLA \"\n          r\"while_loop. maximum_iterations tensor '.*Placeholder:0' for \"\n          r\"while_loop context '.+' must be statically known \\(e.g. a constant \"\n          r\"value or known shape dimension\\), or be defined at or outside the \"\n          r\"while loop context '' \\(currently defined in 'cond/.+'\\)\"):\n        _ = gradients_impl.gradients(loop, v)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testNestedWhileLoopWithMaxItersFromOuterContextInXLAContext(self):\n    if test_util.is_gpu_available():\n      self.skipTest(\"b/128646372, b/128645947 fails in opensource build\")\n\n    v = constant_op.constant(1.0)\n\n    p = array_ops.placeholder(dtype=dtypes.int32)\n\n    def mid_body_builder(iterations):\n\n      def mid_body(i, x):\n        r = control_flow_ops.while_loop(\n            lambda *_: True,\n            lambda i, x: (i + 1, v * x), (0, x),\n            maximum_iterations=iterations,\n            name=\"inner\")\n        return (i + 1, gradients_impl.gradients(x + r[1], v)[0])\n\n      return mid_body\n\n    def outer_body(i, x):\n      iterations = array_ops.size(p, name=\"iterations\")\n      return (i + 1, x + control_flow_ops.while_loop(\n          lambda *_: True,\n          mid_body_builder(iterations), (0, x),\n          maximum_iterations=iterations,\n          name=\"mid\")[1])\n\n    def create_while_loop():\n      with ops.device(\"/cpu:0\"):\n        r = control_flow_ops.while_loop(\n            lambda *_: True,\n            outer_body, (0, 1.0),\n            maximum_iterations=5,\n            name=\"outer\")\n        return array_ops.identity(r[1])\n\n    xla_context = control_flow_ops.XLAControlFlowContext()\n    xla_context.Enter()\n    final_with_xla_context = create_while_loop()\n    xla_context.Exit()\n\n    final_without_xla_context = create_while_loop()\n\n    with self.session(use_gpu=False) as sess:\n      opts = config_pb2.RunOptions(trace_level=config_pb2.RunOptions.FULL_TRACE)\n      run_metadata_without_xla_context = config_pb2.RunMetadata()\n      run_metadata = config_pb2.RunMetadata()\n\n      final_value_without_xla_context = sess.run(\n          final_without_xla_context,\n          feed_dict={p: [0, 0, 0]},\n          options=opts,\n          run_metadata=run_metadata_without_xla_context)\n\n      final_value_with_xla_context = sess.run(\n          final_with_xla_context,\n          feed_dict={p: [0, 0, 0]},\n          options=opts,\n          run_metadata=run_metadata)\n\n      if control_flow_util.ENABLE_CONTROL_FLOW_V2:\n        # With while_v2 on xla, run_metadata only contains the unlowered While\n        # op so node_stats does not have statistics for the pushes. So as a\n        # loose check we check the pushes in the lowered version.\n        for dev in run_metadata_without_xla_context.step_stats.dev_stats:\n          if \"/device:CPU\" in dev.device:\n            node_stats = dev.node_stats\n        stack_push_count = len([\n            x for x in node_stats\n            if re.match(r\".*TensorListPushBack_?\\d*\", x.node_name)\n        ])\n      else:\n        for dev in run_metadata.step_stats.dev_stats:\n          if \"/device:CPU\" in dev.device:\n            node_stats = dev.node_stats\n        stack_push_op = \"StackPushV2\"\n        stack_push_count = len(\n            [x for x in node_stats if x.node_name.endswith(\"StackPushV2\")])\n      # Pushes to the stack = product of maximum_iterations values;\n      # the last two \"3\"s comes from size(p), when p == [0, 0, 0].\n      self.assertEqual(stack_push_count, 5 * 3 * 3, str(node_stats))\n\n      self.assertAllClose(final_value_with_xla_context,\n                          final_value_without_xla_context)\n\n  # Have more than 10 parallel iterations and hence exercise k-bound\n  # most of the time.\n  @test_util.run_deprecated_v1\n  def testWhile_3(self):\n    with self.cached_session():\n\n      def compute(i, m, c, o):\n        m, c = [math_ops.add(m, 1), math_ops.add(c, 1)]\n        o = math_ops.add(o, m)\n        o = math_ops.add(o, c)\n        i = math_ops.add(i, 1)\n        return [i, m, c, o]\n\n      i = ops.convert_to_tensor(0)\n      m = ops.convert_to_tensor(0)\n      c = ops.convert_to_tensor(0)\n      o = ops.convert_to_tensor(0)\n      d = ops.convert_to_tensor(100)\n      r = control_flow_ops.while_loop(lambda i, m, c, o: math_ops.less(i, d),\n                                      compute, [i, m, c, o])\n      result = r[3]\n    self.assertAllEqual(10100, result)\n\n  @test_util.run_deprecated_v1\n  def testWhile_4(self):\n    with self.cached_session():\n\n      def compute(i, m, c, o):\n        m, c = [array_ops.gather(x, i), array_ops.gather(x, i)]\n        o = math_ops.add(o, m)\n        o = math_ops.add(o, c)\n        i = math_ops.add(i, 1)\n        return [i, m, c, o]\n\n      i = ops.convert_to_tensor(0)\n      m = ops.convert_to_tensor(0)\n      c = ops.convert_to_tensor(0)\n      o = ops.convert_to_tensor(0)\n      x = ops.convert_to_tensor([1, 2, 3, 4, 5, 6])\n      s = array_ops.size(x)\n      r = control_flow_ops.while_loop(lambda i, m, c, o: math_ops.less(i, s),\n                                      compute, [i, m, c, o])\n      result = r[3]\n    self.assertAllEqual(42, result)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhile_5(self):\n    with self.cached_session():\n\n      def compute(i, c, o):\n        c = array_ops.strided_slice(x, array_ops.expand_dims(i, 0),\n                                    [1] + array_ops.expand_dims(i, 0))\n        o = array_ops.concat([o, c], 0)\n        i = math_ops.add(i, 1)\n        return [i, c, o]\n\n      i = ops.convert_to_tensor(0)\n      c = ops.convert_to_tensor([0])\n      o = ops.convert_to_tensor([0])\n      x = ops.convert_to_tensor([1, 2, 3, 4, 5, 6])\n      s = array_ops.size(x)\n      r = control_flow_ops.while_loop(lambda i, c, o: math_ops.less(i, s),\n                                      compute, [i, c, o], [\n                                          i.get_shape(),\n                                          tensor_shape.unknown_shape(),\n                                          tensor_shape.unknown_shape()\n                                      ])\n      result = r[2]\n    self.assertAllEqual(np.array([0, 1, 2, 3, 4, 5, 6]), result)\n\n  @test_util.run_gpu_only\n  @test_util.run_deprecated_v1\n  def testWhile_Device(self):\n\n    # Body function defined outside of device scope\n    def body(x):\n      return math_ops.exp(x)\n\n    with ops.device(\"CPU:0\"):\n      r = control_flow_ops.while_loop(\n          lambda x: x < 10, body, [constant_op.constant(-10.)])\n      self.assertIn(\"cpu\", r.device.lower())\n\n    with session.Session() as sess:\n      options = config_pb2.RunOptions(output_partition_graphs=True)\n      run_metadata = config_pb2.RunMetadata()\n      sess.run(r, options=options, run_metadata=run_metadata)\n      # We expect that everything runs on CPU, even if GPU is available.\n      self.assertEqual(len(run_metadata.partition_graphs), 1)\n\n  @test_util.disable_control_flow_v2(\"b/116338794 (buffer_reuse)\")\n  @test_util.run_v1_only(\"b/120545219\")\n  def testBufferForwarding(self):\n    run_options = config_pb2.RunOptions(\n        trace_level=config_pb2.RunOptions.FULL_TRACE)\n    run_metadata = config_pb2.RunMetadata()\n\n    with self.cached_session() as sess:\n      with ops.device(\"/cpu:0\"):\n        c = constant_op.constant(2)\n        i0 = constant_op.constant(0)\n        r = control_flow_ops.while_loop(lambda i: i < 1000,\n                                        lambda i: math_ops.square(c) + i, [i0])\n      r_val = sess.run(r, options=run_options, run_metadata=run_metadata)\n      self.assertEqual(1000, r_val)\n      self.assertTrue(run_metadata.HasField(\"step_stats\"))\n      unique_allocs = set()\n      for node_stat in run_metadata.step_stats.dev_stats[0].node_stats:\n        for output in node_stat.output:\n          unique_allocs.add(\n              output.tensor_description.allocation_description.ptr)\n      # Prior to cl/147536680, the number of unique allocations was about 1005.\n      self.assertLess(len(unique_allocs), 756)\n\n  def _testWhile_Gpu_1(self, use_gpu):\n    with self.cached_session(use_gpu=use_gpu):\n      n = constant_op.constant(1.0)\n      c = lambda x: math_ops.less(x, 10.0)\n      b = lambda x: math_ops.add(x, 1.0)\n      r = control_flow_ops.while_loop(c, b, [n])\n      self.assertAllClose(10.0, self.evaluate(r))\n\n  def testWhile_Gpu_1(self):\n    self._testWhile_Gpu_1(use_gpu=False)\n    self._testWhile_Gpu_1(use_gpu=True)\n\n  def _testWhile_Gpu_2(self, use_gpu):\n    with self.cached_session(use_gpu=use_gpu):\n      n = constant_op.constant(1.0)\n      c = lambda x: math_ops.less(x, 10.0)\n\n      def b(x):\n        with ops.device(\"/cpu:0\"):\n          return math_ops.add(x, 1.0)\n\n      r = control_flow_ops.while_loop(c, b, [n])\n      self.assertAllClose(10.0, self.evaluate(r))\n\n  def testWhile_Gpu_2(self):\n    self._testWhile_Gpu_2(use_gpu=False)\n    self._testWhile_Gpu_2(use_gpu=True)\n\n  def testWhileShape(self):\n    with self.cached_session():\n      i = constant_op.constant(0)\n      m = array_ops.ones([2, 2])\n      c = lambda i, j: math_ops.less(i, 2)\n\n      def _b(i, j):\n        new_i = math_ops.add(i, 1)\n        new_j = array_ops.tile(j, [2, 2])\n        return [new_i, new_j]\n\n      r = control_flow_ops.while_loop(\n          c, _b, [i, m],\n          [i.get_shape(), tensor_shape.unknown_shape()])\n      r = r[1] * array_ops.ones([8, 8])\n      self.assertAllEqual(np.ones((8, 8)), self.evaluate(r))\n\n  @test_util.disable_control_flow_v2(\"b/131265085\")\n  @test_util.run_v1_only(\"b/131265085\")\n  def testWhileBadShape(self):\n    x = constant_op.constant([2.0, 4.0], name=\"values\")\n    i = constant_op.constant(0)\n    c = lambda i, _: math_ops.less(i, 10)\n    b = lambda i, x: [i + 1, x + 1]\n    with self.assertRaisesRegex(ValueError, \"is not compatible with\"):\n      # Shape of x is [2], but we specify a shape of [5].\n      control_flow_ops.while_loop(\n          c, b, [i, x], [i.shape, tensor_shape.TensorShape([5])])\n\n  @test_util.run_in_graph_and_eager_modes\n  def testWhileBadBodyReturn(self):\n    x = constant_op.constant([2.0, 4.0], name=\"values\")\n    i = constant_op.constant(0)\n    c = lambda i, *x: math_ops.less(i, 10)\n\n    # body accepts N values and returns N+1 values.\n    b = lambda i, *x: (i, i) + x\n\n    with self.assertRaisesRegex(\n        ValueError, \"The two structures don't have the same nested structure.\"):\n      control_flow_ops.while_loop(c, b, [i, x])\n\n  @test_util.run_deprecated_v1\n  def testWhileWithNonTensorInput_Scalar(self):\n    with self.cached_session():\n      n = 0\n      c = lambda x: x < 10000\n      b = lambda x: x + 1\n      r = control_flow_ops.while_loop(c, b, [n], parallel_iterations=20)\n      self.assertEqual(10000, self.evaluate(r))\n\n  def testWhileWithNonTensorInput_Vector(self):\n    with self.cached_session():\n      n = np.array([0])  # Note, [0] would not work here; that is a list\n      c = lambda x: x[0] < 10000\n      b = lambda x: array_ops.stack([x[0] + 1])\n      r = control_flow_ops.while_loop(c, b, [n], parallel_iterations=20)\n      self.assertEqual([10000], self.evaluate(r))\n\n  def testWhileShapeInference(self):\n    with self.cached_session():\n      i = constant_op.constant(0)\n      m = array_ops.ones([2, 2])\n      c = lambda i, j: math_ops.less(i, 2)\n\n      def b(i, j):\n        new_i = math_ops.add(i, 1)\n        new_j = array_ops.concat([j, j], 0)\n        return [new_i, new_j]\n\n      r = control_flow_ops.while_loop(\n          c, b, [i, m],\n          [i.get_shape(), tensor_shape.TensorShape([None, 2])])\n      self.assertTrue(r[1].shape.is_compatible_with([8, 2]))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileShapeInferenceBadShape(self):\n    with self.cached_session():\n      i = constant_op.constant(0)\n      m = array_ops.ones([2, 2])\n      c = lambda i, j: math_ops.less(i, 2)\n      b = lambda i, j: [i + 1, array_ops.concat([j, j], 0)]\n      with self.assertRaisesRegex(\n          ValueError,\n          r\"Input tensor 'ones:0' enters the loop with shape \\(2, 2\\), but has \"\n          r\"shape \\(4, 2\\) after one iteration. To allow the shape to vary \"\n          r\"across iterations, use the `shape_invariants` argument of \"\n          r\"tf.while_loop to specify a less-specific shape.\"):\n        control_flow_ops.while_loop(c, b, [i, m])\n\n  def testWhileShapeInferenceSparseTensor(self):\n    values = constant_op.constant([2.0, 4.0], name=\"values\")\n    indices = constant_op.constant([[0], [3]],\n                                   dtype=dtypes.int64,\n                                   name=\"indices\")\n    shape = constant_op.constant([10], dtype=dtypes.int64, name=\"dense_shape\")\n    i = constant_op.constant(0)\n    x = sparse_tensor.SparseTensor(indices, values, dense_shape=shape)\n\n    def c(i, _):\n      return i < 10\n\n    def b1(i, x):  # modifies values.  (shape of components is not changed.)\n      return [\n          i + 1,\n          sparse_tensor.SparseTensor(x.indices, x.values * 2.0, x.dense_shape)\n      ]\n\n    def b2(i, x):  # adds new values.  (shape of components is changed.)\n      return [\n          i + 1,\n          sparse_ops.sparse_add(\n              x,\n              sparse_tensor.SparseTensor(\n                  indices=math_ops.cast(\n                      array_ops.fill([1, 1], i), dtypes.int64),\n                  values=array_ops.fill([1], 1.0),\n                  dense_shape=x.dense_shape))\n      ]\n\n    def b3(i, x):  # modifies rank.  (shape of all components is changed.)\n      return [\n          i + 1,\n          sparse_tensor.SparseTensor(\n              array_ops.concat([x.indices, [[i], [i]]], axis=1), x.values * 2.0,\n              array_ops.concat([x.dense_shape, [10]], axis=0))\n      ]\n\n    def check_shapes(r, indices, values, dense_shape):\n      self.assertTrue(r.indices.shape.is_compatible_with(indices))\n      self.assertTrue(r.values.shape.is_compatible_with(values))\n      self.assertTrue(r.dense_shape.shape.is_compatible_with(dense_shape))\n\n    # Default shape invariant; b1 only modifies values.\n    _, r = control_flow_ops.while_loop(c, b1, [i, x])\n    check_shapes(r, indices=[None, 1], values=[None], dense_shape=[1])\n\n    # Default shape invariant; b2 adds new values\n    _, r = control_flow_ops.while_loop(c, b2, [i, x])\n    check_shapes(r, indices=[None, 1], values=[None], dense_shape=[1])\n\n    # Explicit shape invariant, allowing any rank; b1 only modifies values.\n    _, r = control_flow_ops.while_loop(\n        c, b1, [i, x],\n        [i.get_shape(), tensor_shape.TensorShape([None])])\n    check_shapes(r, indices=[None, None], values=[None], dense_shape=[None])\n\n    # Explicit shape invariant, allowing any rank; b3 modifies rank.\n    _, r = control_flow_ops.while_loop(\n        c, b3, [i, x],\n        [i.get_shape(), tensor_shape.TensorShape([None])])\n    check_shapes(r, indices=[None, None], values=[None], dense_shape=[None])\n\n    # Shape invariant with ndims=None.  Technically, this isn't supported\n    # according to the docs, but we support it for backwards compatibility.\n    _, r = control_flow_ops.while_loop(\n        c, b1, [i, x],\n        [i.get_shape(), tensor_shape.TensorShape(None)])\n    check_shapes(r, indices=[None, None], values=[None], dense_shape=[None])\n    _, r = control_flow_ops.while_loop(\n        c, b3, [i, x],\n        [i.get_shape(), tensor_shape.TensorShape(None)])\n    check_shapes(r, indices=[None, None], values=[None], dense_shape=[None])\n\n  @test_util.disable_control_flow_v2(\"b/131265085\")\n  @test_util.run_v1_only(\"b/131265085\")\n  def testWhileBadShapeSparseTensor(self):\n    values = constant_op.constant([2.0, 4.0], name=\"values\")\n    indices = constant_op.constant([[0], [3]],\n                                   dtype=dtypes.int64,\n                                   name=\"indices\")\n    shape = constant_op.constant([10], dtype=dtypes.int64, name=\"dense_shape\")\n    i = constant_op.constant(0)\n    x = sparse_tensor.SparseTensor(indices, values, dense_shape=shape)\n    c = lambda i, _: i < 10\n    b1 = lambda i, x: [i+1, x]\n    def b2(i, x):  # modifies rank.  (shape of all components is changed.)\n      return [\n          i + 1,\n          sparse_tensor.SparseTensor(\n              array_ops.concat([x.indices, [[i], [i]]], axis=1), x.values * 2.0,\n              array_ops.concat([x.dense_shape, [10]], axis=0))\n      ]\n\n    # Explicit shape invariant, with a specific (incompatible) rank.\n    with self.assertRaisesRegex(ValueError, \"is not compatible with\"):\n      control_flow_ops.while_loop(\n          c, b1, [i, x],\n          [i.get_shape(), tensor_shape.TensorShape([5])])\n\n    # Default shape invariant, but b2 modifies rank (which is not allowed).\n    with self.assertRaises(ValueError):\n      control_flow_ops.while_loop(c, b2, [i, x])\n\n  def testWhileShapeInferenceIndexedSlices(self):\n    with self.cached_session():\n      values = constant_op.constant([[2.0, 4.0], [3.0, 5.0]], name=\"values\")\n      indices = constant_op.constant([0, 3], name=\"indices\")\n      shape = constant_op.constant([10, 2], name=\"dense_shape\")\n      i = constant_op.constant(0)\n      x = ops.IndexedSlices(values, indices, dense_shape=shape)\n\n      def c(i, _):\n        return i < 10\n\n      def b(i, x):\n        return [\n            i + 1,\n            ops.IndexedSlices(x.values * 2.0, x.indices, x.dense_shape)\n        ]\n\n      _, r = control_flow_ops.while_loop(c, b, [i, x])\n      self.assertEqual(r.dense_shape.get_shape()[0], 2)\n      self.assertEqual(r.values.get_shape(), tensor_shape.TensorShape([2, 2]))\n\n      _, r = control_flow_ops.while_loop(\n          c, b, [i, x],\n          [i.get_shape(), tensor_shape.TensorShape([None, 2])])\n      self.assertEqual(r.dense_shape.get_shape()[0], 2)\n      self.assertTrue(r.values.get_shape().is_compatible_with([None, 2]))\n\n  @test_util.disable_control_flow_v2(\"b/131265085\")\n  @test_util.run_v1_only(\"b/131265085\")\n  def testWhileBadShapeIndexedSlices(self):\n    values = constant_op.constant([2.0, 4.0], name=\"values\")\n    indices = constant_op.constant([[0], [3]],\n                                   dtype=dtypes.int64,\n                                   name=\"indices\")\n    shape = constant_op.constant([10], dtype=dtypes.int64, name=\"dense_shape\")\n    i = constant_op.constant(0)\n    x = sparse_tensor.SparseTensor(indices, values, dense_shape=shape)\n    c = lambda i, _: 10\n    b = lambda i, x: [i+1, x]\n\n    # Explicit shape invariant, with a specific (incompatible) rank.\n    with self.assertRaisesRegex(ValueError, \"is not compatible with\"):\n      control_flow_ops.while_loop(\n          c, b, [i, x],\n          [i.get_shape(), tensor_shape.TensorShape([5])])\n\n  def testWhileShapeInferenceRaggedTensor(self):\n    i = constant_op.constant(0)\n    x = ragged_factory_ops.constant([[1, 2], [3], [4, 5, 6]])\n    c = lambda i, _: i < 10\n\n    def b1(i, x):  # Adds new values to rows (but doesn't create new rows)\n      return [\n          i + 1,\n          array_ops.concat([x, x], axis=1)\n      ]\n\n    def b2(i, x):  # Adds new rows.\n      return [\n          i + 1,\n          array_ops.concat([x, x], axis=0)\n      ]\n\n    def check_shapes(r, values, splits):\n      self.assertTrue(r.values.shape.is_compatible_with(values))\n      self.assertTrue(r.row_splits.shape.is_compatible_with(splits))\n\n    # Default shape invariant; b1 adds new values to rows.\n    _, r = control_flow_ops.while_loop(c, b1, [i, x])\n    check_shapes(r, values=[None], splits=[4])\n\n    # Default shape invariant; b2 adds new rows (not allowed).\n    if not context.executing_eagerly():\n      with self.assertRaises(ValueError):\n        _, r = control_flow_ops.while_loop(c, b2, [i, x])\n\n    # Explicit shape invariant; b1 adds new values to rows.\n    # (deprecated: use TensorShape instead of RaggedTensorSpec)\n    _, r = control_flow_ops.while_loop(\n        c, b1, [i, x],\n        [i.get_shape(), tensor_shape.TensorShape([None, None])])\n    check_shapes(r, values=[None], splits=[None])\n\n    # Explicit shape invariant; b1 adds new values to rows.\n    _, r = control_flow_ops.while_loop(\n        c, b1, [i, x],\n        [i.get_shape(), ragged_tensor.RaggedTensorSpec([None, None],\n                                                       dtypes.int32)])\n    check_shapes(r, values=[None], splits=[None])\n\n    # Explicit shape invariant; b2 adds new rows.\n    _, r = control_flow_ops.while_loop(\n        c, b2, [i, x],\n        [i.get_shape(), ragged_tensor.RaggedTensorSpec([None, None],\n                                                       dtypes.int32)])\n    check_shapes(r, values=[None], splits=[None])\n\n  def testWhileShapeInferenceRaggedTensorRaggedRank2(self):\n    i = constant_op.constant(0)\n    x = ragged_factory_ops.constant([[[1, 2], [3], [4, 5, 6]],\n                                     [[], [8, 9, 10]]])\n    c = lambda i, _: i < 10\n    def b(i, x):\n      return [\n          i + 1,\n          array_ops.concat([x, x[..., i:i+1]], axis=-1)\n      ]\n    _, r = control_flow_ops.while_loop(c, b, [i, x])\n    self.assertEqual(r.row_splits.shape.as_list(), [3])\n    self.assertTrue(r.values.row_splits.shape.as_list() in ([6], [None]))\n    self.assertTrue(r.values.values.shape.as_list() in ([49], [None]))\n\n  def testWhileShapeInvariantTensorSpec(self):\n    i = constant_op.constant(0)\n    x = constant_op.constant([1])\n    c = lambda i, _: i < 10\n    b = lambda i, x: (i + 1, array_ops.stack([x, x]))\n    shape_invariants = [\n        tensor_spec.TensorSpec([], dtype=dtypes.int32),\n        tensor_spec.TensorSpec(None, dtype=dtypes.int32)]\n    control_flow_ops.while_loop(c, b, [i, x], shape_invariants)\n\n  # TODO(b/131265085) Remove this decorator when bug is fixed.\n  @test_util.build_as_function_and_v1_graph\n  def testWhileShapeInvariantWrongTypeSpecType(self):\n    c = lambda i, _: i < 10\n    b = lambda i, x: (i + 1, x)\n    i = constant_op.constant(0)\n    x = sparse_tensor.SparseTensor([[0]], [1.0], [10])\n    shape_invariants = [\n        tensor_spec.TensorSpec([], dtype=dtypes.int32),\n        sparse_tensor.SparseTensorSpec([None])]\n    control_flow_ops.while_loop(c, b, [i, x], shape_invariants)\n\n    x2 = constant_op.constant([1])\n    with self.assertRaises(TypeError):\n      control_flow_ops.while_loop(c, b, [i, x2], shape_invariants)\n\n    x3 = ragged_factory_ops.constant([[1, 2], [3]])\n    with self.assertRaises(TypeError):\n      control_flow_ops.while_loop(c, b, [i, x3], shape_invariants)\n\n    i2 = constant_op.constant(0.0)\n    with self.assertRaises(TypeError):\n      control_flow_ops.while_loop(c, b, [i2, x], shape_invariants)\n\n  # TODO(b/131265085) Remove this decorator when bug is fixed.\n  @test_util.build_as_function_and_v1_graph\n  def testWhileShapeInvariantBadType(self):\n    i = constant_op.constant(0)\n    x = constant_op.constant([1])\n    c = lambda i, _: i < 10\n    b = lambda i, x: (i + 1, x)\n    with self.assertRaises((ValueError, TypeError)):\n      control_flow_ops.while_loop(c, b, [i, x], [\"foo\", \"bar\"])\n\n  def _testNestedWhile_1(self, use_gpu):\n    with self.cached_session(use_gpu=use_gpu):\n      n = constant_op.constant(0)\n\n      def cpu_sum(s):\n        c = lambda i, s: math_ops.less(i, 10)\n\n        def b(i, s):\n          i1 = math_ops.add(i, 1)\n          with ops.device(\"/cpu:0\"):\n            s1 = math_ops.add(i, s)\n          return i1, s1\n\n        _, r_s = control_flow_ops.while_loop(c, b, [n, s])\n        return r_s\n\n      c = lambda x: math_ops.less(x, 200)\n      b = lambda x: math_ops.add(x, cpu_sum(n))\n      r = control_flow_ops.while_loop(c, b, [n])\n      self.assertEqual(225, self.evaluate(r))\n\n  def testNestedWhile_1(self):\n    self._testNestedWhile_1(use_gpu=False)\n    self._testNestedWhile_1(use_gpu=True)\n\n  def _testNestedWhile_2(self, use_gpu):\n    # Test the cases that A -> Enter and Exit -> A are partitioned.\n    with self.cached_session(use_gpu=use_gpu):\n      s0 = constant_op.constant(2.0)\n\n      def inner_loop(s):\n        c = lambda s: math_ops.less(s, 20.0)\n\n        def b(s):\n          s1 = math_ops.add(s, s)\n          return s1\n\n        r_s = control_flow_ops.while_loop(c, b, [s], parallel_iterations=1)\n        return r_s\n\n      outer_c = lambda x: math_ops.less(x, 3000.0)\n\n      def outer_b(x):\n        x = logging_ops.Print(x, [x])  # Edge \"Print -> Enter\" is partitioned\n        x = inner_loop(x)\n        with ops.device(\"/cpu:0\"):\n          x = math_ops.square(x)  # Edge \"Exit -> Square\" is partitioned\n        return x\n\n      r = control_flow_ops.while_loop(\n          outer_c, outer_b, [s0], parallel_iterations=1)\n      self.assertEqual(1048576.0, self.evaluate(r))\n\n  def testNestedWhile_2(self):\n    self._testNestedWhile_2(use_gpu=False)\n    self._testNestedWhile_2(use_gpu=True)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileWithControl_1(self):\n    with self.cached_session():\n      n = constant_op.constant(0)\n      r = constant_op.constant(0)\n      condition = lambda n_, r_: math_ops.less(n_, 10)\n\n      def body(n_, r_):\n        n_ = math_ops.add(n_, 1)\n        with r_.graph.control_dependencies([r_]):\n          r_ = constant_op.constant(12)\n        return [n_, r_]\n\n      res = control_flow_ops.while_loop(\n          condition, body, [n, r], parallel_iterations=1)\n      self.assertAllEqual(12, res[1])\n\n  @test_util.run_deprecated_v1\n  def testWhileWithControl_2(self):\n    with self.cached_session():\n      r = constant_op.constant(0)\n      condition = lambda r_: math_ops.less(r_, 10)\n\n      def body(r_):\n        with r_.graph.control_dependencies([r_]):\n          r_ = constant_op.constant(12)\n        return [r_]\n\n      res = control_flow_ops.while_loop(\n          condition, body, [r], parallel_iterations=1)\n      self.assertAllEqual(12, self.evaluate(res))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileWithControl_3(self):\n    with self.cached_session() as sess:\n      b = array_ops.placeholder(dtypes.bool)\n      c = constant_op.constant(1)\n      x0 = constant_op.constant(0)\n      with ops.control_dependencies([b]):\n        r = control_flow_ops.while_loop(lambda x: x < 10, lambda x: x + c, [x0])\n      self.assertEqual(10, sess.run(r, {b: True}))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileWithControl_4(self):\n    with self.cached_session() as sess:\n      b = array_ops.placeholder(dtypes.bool)\n      c = constant_op.constant(1)\n      x0 = constant_op.constant(0)\n      with ops.control_dependencies([b]):\n        r = control_flow_ops.while_loop(\n            lambda x: x < 10, lambda x: x + array_ops.identity(c), [x0])\n      self.assertEqual(10, sess.run(r, {b: True}))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileWithControl_5(self):\n    with self.cached_session() as sess:\n      b = array_ops.placeholder(dtypes.bool)\n      c = constant_op.constant(1)\n      x0 = constant_op.constant(0)\n\n      def body(x):\n        with ops.control_dependencies([b]):\n          return x + c\n\n      r = control_flow_ops.while_loop(lambda x: x < 10, body, [x0])\n      self.assertEqual(10, sess.run(r, {b: True}))\n\n  def testWhileCondWithControl(self):\n    # Ensure that no control edges by an outer control dependency context are\n    # added to nodes inside cond/while contexts.\n    with self.cached_session() as sess:\n      const_true = lambda: constant_op.constant(True)\n      const_false = lambda: constant_op.constant(False)\n      cond = lambda i: control_flow_ops.cond(i > 0, const_true, const_false)\n      body = lambda i: control_flow_ops.cond(i > 0, lambda: i - 1, lambda: i)\n\n      with ops.control_dependencies([control_flow_ops.no_op()]):\n        loop = control_flow_ops.while_loop(cond, body,\n                                           (constant_op.constant(5),))\n      self.assertEqual(0, self.evaluate(loop))\n\n  @test_util.disable_control_flow_v2(\"b/113324949 (ref vars)\")\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileCondWithControl_1(self):\n    with self.cached_session():\n      v = variable_scope.get_variable(\n          \"v\", [], initializer=init_ops.constant_initializer(2))\n      i0 = constant_op.constant(0)\n      with ops.control_dependencies([i0]):\n\n        def loop_condition(i):\n          return i < 4\n\n        def loop_body(i):\n          some_cond = control_flow_ops.cond(\n              constant_op.constant(True),\n              lambda: state_ops.assign(v, math_ops.square(v)), lambda: v)\n          with ops.control_dependencies([some_cond]):\n            return i + 1\n\n      r = control_flow_ops.while_loop(loop_condition, loop_body, (i0,))\n      self.evaluate(variables.global_variables_initializer())\n      self.assertEqual(4, self.evaluate(r))\n      self.assertAllClose(65536.0, self.evaluate(v))\n\n  @test_util.disable_control_flow_v2(\"b/113324949 (ref vars)\")\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileCondExitControl(self):\n\n    with self.cached_session():\n      v = variables.Variable(1)\n\n      def false_branch():\n        cond = lambda i: i < 100\n\n        def body(i):\n          x = state_ops.assign(v, i)\n          return x + 1\n\n        loop = control_flow_ops.while_loop(cond, body, [0])\n        # Make sure to handle correctly control edge from Exit to a node.\n        with ops.control_dependencies([loop]):\n          return constant_op.constant(6.0)\n\n      r = control_flow_ops.cond(\n          constant_op.constant(False), lambda: constant_op.constant(1.0),\n          false_branch)\n      self.evaluate(variables.global_variables_initializer())\n      self.assertEqual(6.0, self.evaluate(r))\n      self.assertEqual(99, self.evaluate(v))\n\n  def testCondWhile_1(self):\n\n    with self.cached_session():\n      n = ops.convert_to_tensor(0, name=\"n\")\n      c = lambda x: math_ops.less(x, 10)\n      b = lambda x: math_ops.add(x, 1)\n      r = control_flow_ops.cond(\n          math_ops.less(0, 1), lambda: control_flow_ops.while_loop(c, b, [n]),\n          lambda: n)\n      self.assertAllEqual(10, self.evaluate(r))\n\n  def testCondWhile_2(self):\n\n    with self.cached_session():\n      n = ops.convert_to_tensor(0)\n      c = lambda x: math_ops.less(x, 10)\n      b = lambda x: math_ops.add(x, 1)\n      r = control_flow_ops.cond(\n          math_ops.less(1, 0), lambda: math_ops.add(n, 1),\n          lambda: control_flow_ops.while_loop(c, b, [n]))\n      self.assertAllEqual(10, self.evaluate(r))\n\n  def _testCondWhile_3(self, use_gpu):\n    with self.cached_session(use_gpu=use_gpu) as sess:\n      p = array_ops.placeholder(dtypes.bool)\n      n = constant_op.constant(0.0)\n\n      def c(x):\n        return math_ops.less(x, 10.0)\n\n      def b(x):\n        with ops.device(\"/cpu:0\"):\n          x1 = math_ops.add(x, 1.0)\n        return x1\n\n      r = control_flow_ops.cond(p,\n                                lambda: control_flow_ops.while_loop(c, b, [n]),\n                                lambda: math_ops.multiply(n, 2.0))\n      r1 = gradients_impl.gradients(r, [n])\n      self.assertEqual(10., sess.run(r, {p: True}))\n      self.assertEqual([1.0], sess.run(r1, {p: True}))\n      self.assertEqual(0.0, sess.run(r, {p: False}))\n      self.assertEqual([2.0], sess.run(r1, {p: False}))\n\n  @test_util.run_deprecated_v1\n  def testCondWhile_3(self):\n    self._testCondWhile_3(use_gpu=False)\n    self._testCondWhile_3(use_gpu=True)\n\n  def testWhileCond_1(self):\n\n    with self.cached_session():\n      i = ops.convert_to_tensor(0, name=\"i\")\n      n = ops.convert_to_tensor(10, name=\"n\")\n      one = ops.convert_to_tensor(1, name=\"one\")\n      c = lambda x: math_ops.less(x, n)\n      # pylint: disable=undefined-variable\n      # for OSS build\n      b = lambda x: control_flow_ops.cond(\n          constant_op.constant(True),\n          lambda: math_ops.add(x, one), lambda: math_ops.subtract(x, one))\n      # pylint: enable=undefined-variable\n      r = control_flow_ops.while_loop(c, b, [i])\n      self.assertAllEqual(10, self.evaluate(r))\n\n  def testWhileCond_2(self):\n\n    with self.cached_session():\n      n = ops.convert_to_tensor(0, name=\"n\")\n      c = lambda x: math_ops.less(x, 10)\n      b = lambda x: control_flow_ops.cond(constant_op.constant(True), lambda: math_ops.add(x, 1), lambda: n)\n      r = control_flow_ops.while_loop(c, b, [n])\n      self.assertAllEqual(10, self.evaluate(r))\n\n  def testWhileCond_3(self):\n\n    with self.cached_session():\n      n = ops.convert_to_tensor(0)\n      c = lambda x: math_ops.less(x, 10)\n      # pylint: disable=undefined-variable\n      # for OSS build\n      b = lambda x: control_flow_ops.cond(math_ops.less(0, 1),\n                                          lambda: math_ops.add(x, 1),\n                                          lambda: math_ops.subtract(x, 1))\n      # pylint: enable=undefined-variable\n      r = control_flow_ops.while_loop(c, b, [n])\n      self.assertAllEqual(10, self.evaluate(r))\n\n  @test_util.run_deprecated_v1\n  def testWhileCondGradMultiDevice(self):\n    config = config_pb2.ConfigProto(device_count={\"CPU\": 2},\n                                    allow_soft_placement=True)\n    with self.cached_session(use_gpu=True, config=config) as sess:\n      pred = array_ops.placeholder(dtypes.bool, [])\n      x_init = constant_op.constant(1.0)\n\n      with ops.device(\"/cpu:0\"):\n        z = control_flow_ops.while_loop(\n            lambda i, _: i < 3,\n            lambda i, x: (i + 1, control_flow_ops.cond(\n                pred, lambda: x * 2.0, lambda: 10.0)),\n            [0, x_init])\n\n      with ops.device(\"/cpu:1\"):\n        grad = gradients_impl.gradients(z, x_init)[0]\n\n      with ops.device(\"/cpu:0\"):\n        grad_grad = gradients_impl.gradients(grad, x_init)[0]\n\n      self.assertEqual(sess.run(grad, {pred: True}), 8.0)\n      self.assertEqual(sess.run(grad, {pred: False}), 0.0)\n\n      if not control_flow_util.ENABLE_CONTROL_FLOW_V2:\n        return\n\n      self.assertEqual(sess.run(grad_grad, {pred: True}), 0.0)\n      self.assertEqual(sess.run(grad_grad, {pred: False}), 0.0)\n\n  # NOTE: It is ok to have parallel_iterations > 1\n  @test_util.disable_control_flow_v2(\"b/113324949 (RefVariable)\")\n  @test_util.run_deprecated_v1\n  def testWhileUpdateVariable_1(self):\n    with self.cached_session():\n      select = variables.Variable([3.0, 4.0, 5.0])\n      n = constant_op.constant(0)\n\n      def loop_iterator(j):\n        return math_ops.less(j, 3)\n\n      def loop_body(j):\n        ns = state_ops.scatter_update(select, j, 10.0)\n        nj = math_ops.add(j, 1)\n        op = control_flow_ops.group(ns)\n        nj = control_flow_ops.with_dependencies([op], nj)\n        return [nj]\n\n      r = control_flow_ops.while_loop(\n          loop_iterator, loop_body, [n], parallel_iterations=1)\n      self.evaluate(variables.global_variables_initializer())\n      self.assertEqual(3, self.evaluate(r))\n      result = self.evaluate(select)\n      self.assertAllClose(np.array([10.0, 10.0, 10.0]), result)\n\n  @test_util.disable_control_flow_v2(\"b/113324949 (RefVariable)\")\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileUpdateVariable_2(self):\n    with self.cached_session():\n      select1 = variables.Variable([3.0, 4.0, 5.0])\n      select2 = variables.Variable([3.0, 4.0, 5.0])\n      n = constant_op.constant(0)\n\n      def loop_iterator(j):\n        return math_ops.less(j, 3)\n\n      def loop_body(j):\n        ns1 = state_ops.scatter_update(select1, j, 10.0)\n        ns2 = state_ops.scatter_update(select2, j, 10.0)\n        nj = math_ops.add(j, 1)\n        op = control_flow_ops.group(ns1, ns2)\n        nj = control_flow_ops.with_dependencies([op], nj)\n        return [nj]\n\n      r = control_flow_ops.while_loop(\n          loop_iterator, loop_body, [n], parallel_iterations=1)\n      self.evaluate(variables.global_variables_initializer())\n      self.assertEqual(3, self.evaluate(r))\n      result1 = self.evaluate(select1)\n      self.assertAllClose(np.array([10.0, 10.0, 10.0]), result1)\n      result2 = self.evaluate(select2)\n      self.assertAllClose(np.array([10.0, 10.0, 10.0]), result2)\n\n  @test_util.disable_control_flow_v2(\"b/113324949 (RefVariable)\")\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileUpdateVariable_3(self):\n    with self.cached_session():\n      select = variables.Variable([3.0, 4.0, 5.0])\n      n = constant_op.constant(0)\n\n      def loop_iterator(j, _):\n        return math_ops.less(j, 3)\n\n      def loop_body(j, _):\n        ns = state_ops.scatter_update(select, j, 10.0)\n        nj = math_ops.add(j, 1)\n        return [nj, ns]\n\n      r = control_flow_ops.while_loop(\n          loop_iterator,\n          loop_body, [n, array_ops.identity(select)],\n          parallel_iterations=1)\n      self.evaluate(variables.global_variables_initializer())\n      result = r[1]\n    self.assertAllClose(np.array([10.0, 10.0, 10.0]), result)\n\n  @test_util.disable_control_flow_v2(\"b/113324949 (RefVariable)\")\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileUpdateVariable_4(self):\n    with self.cached_session():\n      var_a = variables.Variable(0, name=\"a\")\n      var_b = variables.Variable(0, name=\"b\")\n      self.evaluate(variables.global_variables_initializer())\n\n      c = constant_op.constant(0, name=\"c\")\n      asn1 = state_ops.assign_add(var_a, 1, name=\"a_add\")\n\n      # Loop condition\n      def pred(i):\n        return math_ops.less(i, 10)\n\n      # Loop body\n      def loop_body(i):\n        asn2 = state_ops.assign_add(var_b, asn1, name=\"b_add\")\n        with ops.control_dependencies([asn2]):\n          ni = math_ops.add(i, 1, name=\"i_add\")\n        return ni\n\n      lpa = control_flow_ops.while_loop(\n          pred, loop_body, [c], parallel_iterations=1)\n\n      self.assertEqual(0, self.evaluate(var_b))\n      self.evaluate(lpa)  # Run the loop\n      self.assertEqual(10, self.evaluate(var_b))\n\n  @test_util.disable_control_flow_v2(\"b/113324949 (RefVariable)\")\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileUpdateVariable_5(self):\n    with self.cached_session():\n      # Create some variables.\n      var_a = variables.Variable(0, name=\"a\")\n      var_b = variables.Variable(0, name=\"b\")\n      self.evaluate(variables.global_variables_initializer())\n\n      # Change condition to check var_b\n      def pred(_):\n        return math_ops.less(var_b, 10)\n\n      # Change body to increment var_b\n      def loop_body(i):\n        asn1 = state_ops.assign_add(\n            var_a, constant_op.constant(1), name=\"a_add\")\n        asn2 = state_ops.assign_add(\n            var_b, constant_op.constant(1), name=\"b_add\")\n        with ops.control_dependencies([asn1, asn2]):\n          inc_b = array_ops.identity(var_b)\n        return inc_b\n\n      lpa = control_flow_ops.while_loop(\n          pred, loop_body, [var_b], parallel_iterations=1, name=\"loop\")\n\n      self.assertEqual(0, self.evaluate(var_b))\n      self.evaluate(lpa)  # Run the loop\n      self.assertEqual(10, self.evaluate(var_a))\n      self.assertEqual(10, self.evaluate(var_b))\n\n  @test_util.disable_control_flow_v2(\"b/113324949 (RefVariable)\")\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileUpdateVariable_6(self):\n    with self.cached_session():\n      # Create some variables.\n      var_a = variables.Variable(0, name=\"a\")\n      var_b = variables.Variable(0, name=\"b\")\n      c = constant_op.constant(0)\n      self.evaluate(variables.global_variables_initializer())\n\n      # Loop condition\n      def pred(i):\n        return math_ops.less(i, 10)\n\n      # Loop body\n      def loop_body(i):\n        asn1 = state_ops.assign_add(var_a, 1, name=\"a_add\")\n        with ops.control_dependencies([asn1]):\n          asn2 = state_ops.assign_add(var_b, var_a, name=\"b_add\")\n        with ops.control_dependencies([asn2]):\n          ni = math_ops.add(i, 1, name=\"i_add\")\n          return ni\n\n      lpa = control_flow_ops.while_loop(\n          pred, loop_body, [c], parallel_iterations=1, name=\"loop\")\n\n      self.assertEqual(0, self.evaluate(var_b))\n      self.evaluate(lpa)  # Run the loop\n      self.assertEqual(55, self.evaluate(var_b))\n      self.assertEqual(10, self.evaluate(var_a))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileQueue_1(self):\n    with self.cached_session():\n      q = data_flow_ops.FIFOQueue(-1, dtypes.int32)\n      i = constant_op.constant(0)\n\n      def c(i):\n        return math_ops.less(i, 10)\n\n      def b(i):\n        ni = math_ops.add(i, 1)\n        ni = control_flow_ops.with_dependencies([q.enqueue((i,))], ni)\n        return ni\n\n      r = control_flow_ops.while_loop(c, b, [i], parallel_iterations=1)\n      self.assertEqual([10], self.evaluate(r))\n      for i in xrange(10):\n        self.assertEqual([i], self.evaluate(q.dequeue()))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileTimeOut(self):\n    run_options = config_pb2.RunOptions(timeout_in_ms=1)\n    with self.cached_session() as sess:\n      n = constant_op.constant(0)\n      c = lambda x: True\n      b = lambda x: math_ops.add(x, 1)\n      r = control_flow_ops.while_loop(c, b, [n])\n      with self.assertRaises(errors_impl.DeadlineExceededError):\n        sess.run(r, options=run_options)\n\n  @test_util.disable_control_flow_v2(\"b/117119329 (stack)\")\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileStack_1(self):\n    with self.cached_session():\n      s = gen_data_flow_ops.stack_v2(-1, dtypes.int32, stack_name=\"foo\")\n      i = constant_op.constant(0)\n\n      def c(i):\n        return math_ops.less(i, 10)\n\n      def b(i):\n        ni = math_ops.add(i, 1)\n        ni = control_flow_ops.with_dependencies(\n            [gen_data_flow_ops.stack_push_v2(s, i)], ni)\n        return ni\n\n      r = control_flow_ops.while_loop(c, b, [i], parallel_iterations=1)\n\n      x = constant_op.constant(0)\n\n      def c1(i, _):\n        return math_ops.greater(i, 0)\n\n      def b1(i, x):\n        ni = math_ops.subtract(i, 1)\n        nx = x + gen_data_flow_ops.stack_pop_v2(s, dtypes.int32)\n        return [ni, nx]\n\n      _, rx = control_flow_ops.while_loop(\n          c1,\n          b1, [r, x],\n          [r.get_shape(), tensor_shape.unknown_shape()],\n          parallel_iterations=1)\n      self.assertEqual(45, self.evaluate(rx))\n\n  def _testWhileGrad_ColocateGradients(self, colocate):\n    gpu_dev_name = test.gpu_device_name() if test.is_gpu_available(\n    ) else \"/device:CPU:0\"\n\n    graph = ops.Graph()\n    with graph.as_default():\n      v = constant_op.constant(2.0, name=\"v\")\n      c = lambda v: math_ops.less(v, 100.0)\n\n      def b(x):\n        with ops.device(gpu_dev_name):\n          return math_ops.square(x)\n\n      loop = control_flow_ops.while_loop(c, b, [v], parallel_iterations=1)\n      r = gradients_impl.gradients(\n          loop, v, colocate_gradients_with_ops=colocate)[0]\n\n    r_ops = graph.get_operations()\n    r_devices = [(op.name, op.device) for op in r_ops]\n\n    self.assertTrue(any(\"Square\" in op.name for op in r_ops))\n\n    for (name, dev) in r_devices:\n      if not colocate and name.endswith(\"Square\"):\n        # Only forward graph contain gpu in Square device\n        self.assertTrue(gpu_dev_name in dev)\n      elif colocate and \"Square\" in name:\n        # Forward and backward graphs contain gpu in Square/Square_grad devices\n        self.assertTrue(gpu_dev_name in dev)\n      else:\n        self.assertFalse(gpu_dev_name in dev)\n\n    with self.session(graph=graph) as sess:\n      self.assertAllClose(1024.0, self.evaluate(r))\n\n  @test_util.disable_control_flow_v2(\"b/116351701 (colocation)\")\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileGrad_ColocateGradients(self):\n    self._testWhileGrad_ColocateGradients(colocate=False)\n    self._testWhileGrad_ColocateGradients(colocate=True)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileGrad_Square(self):\n    with self.cached_session():\n      v = constant_op.constant(2.0, name=\"v\")\n      c = lambda v: math_ops.less(v, 100.0)\n      b = math_ops.square\n      r = control_flow_ops.while_loop(c, b, [v], parallel_iterations=1)\n      r = control_flow_ops.cond(math_ops.less(1, 2), lambda: r, lambda: v)\n\n      r = gradients_impl.gradients(r, v)[0]\n      self.assertAllClose(1024.0, self.evaluate(r))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileGrad_Shape(self):\n    with self.cached_session():\n      x = array_ops.placeholder(dtypes.float32, shape=[None])\n      v = constant_op.constant([2.0], name=\"v\")\n      n = constant_op.constant(0, name=\"n\")\n      c = lambda i, v: math_ops.less(i, 5)\n      b = lambda i, v: [i + 1, math_ops.multiply(x, v)]\n      r = control_flow_ops.while_loop(\n          c,\n          b, [n, v],\n          [n.get_shape(), tensor_shape.unknown_shape()],\n          parallel_iterations=1)\n\n      r = gradients_impl.gradients(r[1], x)[0]\n      self.assertEqual([None], r.get_shape().as_list())\n      self.assertAllClose([810.0, 2560.0], r.eval(feed_dict={x: [3.0, 4.0]}))\n\n  @test_util.run_deprecated_v1\n  def testWhileGrad_BaseShape(self):\n    with self.cached_session() as sess:\n      x = array_ops.placeholder(dtypes.float32, [None])\n      v0 = constant_op.constant([2.0, 2.0], name=\"v\")\n      c = lambda v: constant_op.constant(False)\n      b = lambda v: math_ops.multiply(v, x)\n      r = control_flow_ops.while_loop(c, b, [v0])\n      y = math_ops.square(x)\n\n      r = gradients_impl.gradients([r, y], x)[0]\n      self.assertAllClose([2.0, 4.0], sess.run(r, feed_dict={x: [1.0, 2.0]}))\n\n  @test_util.run_deprecated_v1\n  @test_util.enable_output_all_intermediates\n  def testWhileGradAfterSessionRun(self):\n    v0 = constant_op.constant(2.)\n    r = control_flow_ops.while_loop(\n        lambda _: True, lambda v: v * v, [v0], maximum_iterations=3)\n\n    self.assertAllEqual(r, 256.)\n    grad = gradients_impl.gradients(r, v0)[0]\n    self.assertAllClose(grad, 1024.)\n\n  @test_util.run_deprecated_v1\n  @test_util.enable_output_all_intermediates\n  def testNestedWhileGradAfterSessionRun(self):\n    v0 = constant_op.constant(2.)\n\n    def body(v):\n      inner_v0 = constant_op.constant(1.)\n      return control_flow_ops.while_loop(\n          lambda _: True, lambda x: x * v, [inner_v0], maximum_iterations=2)\n\n    r = control_flow_ops.while_loop(\n        lambda _: True, body, [v0], maximum_iterations=3)\n\n    self.assertAllEqual(r, 256.)\n    grad = gradients_impl.gradients(r, v0)[0]\n    self.assertAllClose(grad, 1024.)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileGrad_MultipleUses(self):\n    with self.cached_session():\n      v = constant_op.constant(2.0, name=\"v\")\n      c = lambda v: math_ops.less(v, 100.0)\n      b = math_ops.square\n      r = control_flow_ops.while_loop(c, b, [v], parallel_iterations=1)\n      r = math_ops.multiply(r, r)\n\n      r = gradients_impl.gradients(r, v)[0]\n      self.assertEqual(524288.0, self.evaluate(r))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileGrad_LoopAdd(self):\n    with self.cached_session():\n      v = constant_op.constant(2.0, name=\"v\")\n      c = lambda v: math_ops.less(v, 100.0)\n      b = math_ops.square\n      r = control_flow_ops.while_loop(c, b, [v], parallel_iterations=1)\n      r = math_ops.add(r, r)\n\n      r = gradients_impl.gradients(r, v)[0]\n      self.assertAllClose(2048.0, self.evaluate(r))\n\n  def _testWhileGrad_Mul(self, use_gpu, p_iters):\n    with self.cached_session(use_gpu=use_gpu) as sess:\n      a = constant_op.constant(3.0, name=\"a\")\n      v = constant_op.constant(2.0, name=\"v\")\n      c = lambda v: math_ops.less(v, 100.0)\n      b = lambda v: math_ops.multiply(v, a)\n      r = control_flow_ops.while_loop(c, b, [v], parallel_iterations=p_iters)\n\n      grad_a, grad_v = gradients_impl.gradients(r, [a, v])\n      grad_a_val, grad_v_val = self.evaluate([grad_a, grad_v])\n      self.assertAllClose(216.0, grad_a_val)\n      self.assertAllClose(81.0, grad_v_val)\n\n  @test_util.run_deprecated_v1\n  def testWhileGrad_Mul(self):\n    self._testWhileGrad_Mul(use_gpu=False, p_iters=1)\n    self._testWhileGrad_Mul(use_gpu=False, p_iters=10)\n    self._testWhileGrad_Mul(use_gpu=True, p_iters=1)\n    self._testWhileGrad_Mul(use_gpu=True, p_iters=10)\n\n  def testWhileGradInControlDeps(self):\n\n    @def_function.function\n    def f():\n      x_init = constant_op.constant(2.)\n      loop_cond = lambda i, x: math_ops.less(i, 2)\n      loop_body = lambda i, x: [i + 1, x**2]\n      _, x = control_flow_ops.while_loop(loop_cond, loop_body, [0, x_init])\n      with ops.control_dependencies([x]):\n        (grad,) = gradients_impl.gradients(x, x_init)\n        return grad\n\n    self.assertAllEqual(f(), 4. * 2.**3)  # 4 * x_init ^ 3\n\n  @test_util.run_deprecated_v1\n  def testTfFunctionInV1WhileLoop(self):\n\n    # This test specifically tests that creating a Const node inside a\n    # tf.function inside a v1 while_loop while inlining is turned on works.\n    config = opt_cfg()\n    assert config.graph_options.optimizer_options.do_function_inlining\n    with session.Session(config=config):\n\n      @def_function.function\n      def loop_body(i):\n        # Here we create the const.\n        return i + 1.\n\n      loop_cond = lambda i: True\n      x = control_flow_ops.while_loop(\n          loop_cond, loop_body, [0.], maximum_iterations=5)\n      self.assertAllEqual(x, 5.)\n\n  def _testNestedWhileCondWhileGrad(self, use_gpu):\n\n    with self.cached_session(use_gpu=use_gpu):\n      v = constant_op.constant(1.0)\n\n      def inner_loop(s):\n        z = constant_op.constant(0)\n        c = lambda i, x: math_ops.less(i, 4)\n        b = lambda i, x: [math_ops.add(i, 1), math_ops.multiply(x, 2.0)]\n        return control_flow_ops.while_loop(c, b, [z, s])\n\n      c = lambda x: math_ops.less(x, 128.0)\n\n      def b(x):\n        return control_flow_ops.cond(\n            constant_op.constant(True),\n            lambda: math_ops.square(inner_loop(x)[1]),\n            lambda: math_ops.multiply(x, 2.0))\n\n      r = control_flow_ops.while_loop(c, b, [v])\n      r = gradients_impl.gradients(r, v)[0]\n      self.assertAllClose(512.0, self.evaluate(r))\n\n  @test_util.run_deprecated_v1\n  def testNestedWhileCondWhileGrad(self):\n    self._testNestedWhileCondWhileGrad(use_gpu=False)\n\n  @test_util.run_deprecated_v1\n  def testNestedWhileCondWhileGradGpu(self):\n    self._testNestedWhileCondWhileGrad(use_gpu=True)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileGrad_Variable(self):\n    with self.cached_session():\n      a = variables.Variable(3.0)\n      v = constant_op.constant(2.0, name=\"v\")\n      c = lambda v: math_ops.less(v, 100.0)\n      b = lambda v: math_ops.multiply(v, a)\n      r = control_flow_ops.while_loop(c, b, [v], parallel_iterations=1)\n\n      r = gradients_impl.gradients(r, a)\n      self.evaluate(variables.global_variables_initializer())\n      self.assertAllClose(216.0, r[0])\n\n  @test_util.run_deprecated_v1\n  def testWhileGrad_ResourceVariable(self):\n    with self.cached_session():\n      a = resource_variable_ops.ResourceVariable(3.0)\n      v = constant_op.constant(2.0, name=\"v\")\n      c = lambda v: math_ops.less(v, 100.0)\n      b = lambda v: math_ops.multiply(v, a)\n      r = control_flow_ops.while_loop(c, b, [v], parallel_iterations=1)\n\n      g = gradients_impl.gradients(r, a)\n      self.evaluate(variables.global_variables_initializer())\n      self.assertAllClose(216.0, g[0])\n\n  def testWhileGrad_EagerResourceVariable(self):\n    with context.eager_mode():\n      a = resource_variable_ops.ResourceVariable(\n          np.ones([2, 2], dtype=np.float32))\n      v = constant_op.constant(1.0)\n\n      @eager_function.defun\n      def fn():\n        r = control_flow_ops.while_loop(\n            lambda i, _: i < 2,\n            lambda i, x: (i + 1, x * math_ops.reduce_sum(a) * v),\n            [0, 1.0])[1]\n        return gradients_impl.gradients(r, [v])[0]\n\n      self.assertEqual(self.evaluate(fn()), 32.)\n\n  def testWhileGrad_ResourceVarInFunctionCall(self):\n\n    @def_function.function\n    def foo(x, var):\n      return x + math_ops.reduce_sum(var.sparse_read([1, 3]))\n\n    @def_function.function\n    def bar(var):\n      r = control_flow_ops.while_loop(\n          lambda i, _: i < 2,\n          lambda i, x: (i + 1, foo(x, var)),\n          [0, 0.0])[1]\n      return gradients_impl.gradients(r, var)[0]\n\n    var = resource_variable_ops.ResourceVariable([1., 2., 3., 4.])\n    self.evaluate(variables.global_variables_initializer())\n    grad = self.evaluate(bar(var))\n    self.assertAllEqual(gradient_checker_v2._to_numpy(grad), [0., 2., 0., 2.])\n\n  def testWhileGrad_ResourceVarInNestedFunctionCall(self):\n\n    @def_function.function\n    def foo(x, var):\n      return x + math_ops.reduce_sum(var.sparse_read([1, 3]))\n\n    @def_function.function\n    def foo2(x, var):\n      return foo(x, var)\n\n    @def_function.function\n    def bar(var):\n      r = control_flow_ops.while_loop(\n          lambda i, _: i < 2,\n          lambda i, x: (i + 1, foo2(x, var)),\n          [0, 0.0])[1]\n      return gradients_impl.gradients(r, var)[0]\n\n    var = resource_variable_ops.ResourceVariable([1., 1., 1., 1.])\n    self.evaluate(variables.global_variables_initializer())\n    grad = self.evaluate(bar(var))\n    self.assertAllEqual(gradient_checker_v2._to_numpy(grad), [0., 2., 0., 2.])\n\n  def testWhileGrad_ResourceVarInLoopInFunctionCall(self):\n    if test.is_gpu_available():\n      self.skipTest(\"b/128635252\")\n\n    @def_function.function\n    def foo(x, var):\n      return control_flow_ops.while_loop(\n          lambda j, _: j < 3,\n          lambda j, y: (j + 1,\n                        y + math_ops.reduce_sum(var.sparse_read([1, 2]))),\n          [0, x])[1]\n\n    @def_function.function\n    def bar(var):\n      r = control_flow_ops.while_loop(\n          lambda i, _: i < 2,\n          lambda i, x: (i + 1, foo(x, var)),\n          [0, 0.0])[1]\n      return gradients_impl.gradients(r, var)[0]\n\n    var = resource_variable_ops.ResourceVariable([1., 1., 1., 1.])\n    self.evaluate(variables.global_variables_initializer())\n    grad = self.evaluate(bar(var))\n    self.assertAllEqual(gradient_checker_v2._to_numpy(grad), [0., 6., 6., 0.])\n\n  def testWhileCondGrad_ResourceVarInFunctionCall(self):\n\n    @def_function.function\n    def foo(x, var):\n      return x + var.sparse_read([1])[0]\n\n    def body(i, x):\n      return (i + 1, control_flow_ops.cond(\n          math_ops.equal(i % 2, 0),\n          lambda: foo(x, var1),\n          lambda: foo(x, var2)))\n\n    @def_function.function\n    def bar(var1, var2):\n      r = control_flow_ops.while_loop(\n          lambda i, _: i < 4, body, [0, 0.0])\n      return gradients_impl.gradients(r, [var1, var2])\n\n    var1 = resource_variable_ops.ResourceVariable([1., 2., 3.])\n    var2 = resource_variable_ops.ResourceVariable([4., 5.])\n    self.evaluate(variables.global_variables_initializer())\n    grads = self.evaluate(bar(var1, var2))\n    self.assertAllEqual(gradient_checker_v2._to_numpy(grads[0]), [0., 2., 0.])\n    self.assertAllEqual(gradient_checker_v2._to_numpy(grads[1]), [0., 2.])\n\n  @test_util.run_deprecated_v1\n  def testWhileGrad_ResourceVarSparseRead(self):\n    # NOTE(skyewm): this test is interesting because the gradient is the\n    # aggregation result of IndexedSlices and Tensors.\n    var = resource_variable_ops.ResourceVariable(np.ones(5),\n                                                 dtype=dtypes.float32)\n    r = control_flow_ops.while_loop(\n        lambda i, _: i < 3,\n        lambda i, x: (i + 1, x * math_ops.reduce_sum(var.sparse_read([1, 3]))),\n        [0, constant_op.constant(1.0)])[1]\n    grad = gradients_impl.gradients(r, var)[0]\n\n    self.evaluate(variables.global_variables_initializer())\n    grad_val = self.evaluate(grad)\n    arr = gradient_checker_v2._to_numpy(grad_val)\n    self.assertAllEqual(arr, [0., 12., 0., 12., 0.])\n\n  @test_util.run_deprecated_v1\n  def testWhileGrad_MultiResourceVarSparseRead(self):\n    # NOTE(skyewm): this test is interesting because the gradient is the\n    # aggregation result of IndexedSlices and Tensors.\n    var1 = resource_variable_ops.ResourceVariable(np.ones(5),\n                                                  dtype=dtypes.float32)\n    var2 = resource_variable_ops.ResourceVariable(np.ones(3),\n                                                  dtype=dtypes.float32)\n    x1_init = constant_op.constant([0., 0.])\n    x2_init = constant_op.constant(1.)\n    x3_init = constant_op.constant(1.)\n\n    def body(i, unused_x1, x2, x3):\n      y1 = var1.sparse_read([1, 3])\n      y2 = x2 * 2\n      y3 = x3 * math_ops.reduce_sum(var2.sparse_read([0]))\n      return i + 1, y1, y2, y3\n\n    r = control_flow_ops.while_loop(\n        lambda i, x1, x2, x3: i < 3, body,\n        [0, x1_init, x2_init, x3_init])[1:]\n    var1_grad, var2_grad = gradients_impl.gradients(r, [var1, var2])\n\n    self.evaluate(variables.global_variables_initializer())\n    var1_grad_val = self.evaluate(var1_grad)\n    var2_grad_val = self.evaluate(var2_grad)\n    self.assertAllEqual(gradient_checker_v2._to_numpy(var1_grad_val),\n                        [0., 1., 0., 1., 0.])\n    self.assertAllEqual(gradient_checker_v2._to_numpy(var2_grad_val),\n                        [3., 0., 0.])\n\n  def testWhileGrad_Gather(self):\n    # NOTE(skyewm): this test is interesting because the gather gradient\n    # function returns an IndexedSlices.\n    @tf_function_in_tf2\n    def fn():\n      x = constant_op.constant([1., 1., 1., 1., 1.])\n      y = control_flow_ops.while_loop(\n          lambda i, _: i < 3,\n          lambda i, x: (i + 1, x + array_ops.gather(x, [0])),\n          [0, x[:1]])[1]\n      z = y * 3.0\n      grad = gradients_impl.gradients(z, x)[0]\n      return y, grad\n    y, grad = fn()\n    self.assertEqual(self.evaluate(y), 8.)\n    self.assertAllEqual(self.evaluate(grad), [24., 0., 0., 0., 0.])\n\n  def testWhileGrad_GatherNoFanOut(self):\n    # NOTE(skyewm): this test is interesting because the gather gradient\n    # function returns an IndexedSlices.\n    @tf_function_in_tf2\n    def fn():\n      x = constant_op.constant([1., 1., 1., 1., 1.])\n      y = control_flow_ops.while_loop(\n          lambda i, _: i < 3,\n          lambda i, x: (i + 1, array_ops.gather(x, [0])),\n          [0, x[:1]])[1]\n      z = y * 3.0\n      grad = gradients_impl.gradients(z, x)[0]\n      return y, grad\n    y, grad = fn()\n    self.assertEqual(self.evaluate(y), 1.)\n    self.assertAllEqual(self.evaluate(grad), [3., 0., 0., 0., 0.])\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileGradInCond(self):\n\n    with self.cached_session():\n      n = ops.convert_to_tensor(1.0, name=\"n\")\n      x = array_ops.placeholder(dtypes.float32, shape=None)\n      c = lambda n: math_ops.less(n, 10.0)\n      b = lambda n: math_ops.add(n, x)\n\n      def fn1():\n        r = control_flow_ops.while_loop(c, b, [n],\n                                        [tensor_shape.unknown_shape()])\n        return gradients_impl.gradients(r, x)[0]\n\n      r = control_flow_ops.cond(math_ops.less(1, 2), fn1, lambda: x)\n      self.assertAllClose(9.0, r.eval(feed_dict={x: 1.0}))\n\n  @test_util.disable_control_flow_v2(\"b/116340060\")\n  @test_util.run_v1_only(\"b/120545219\")\n  def testGradInWhileWrtInitialLoopVal(self):\n    with self.cached_session():\n      x = array_ops.placeholder(dtypes.float32, shape=(), name=\"x\")\n      y = x + 1\n\n      def body(i, v):\n        z = v * 2\n        return i + 1, gradients_impl.gradients(z, x)[0]\n\n      with self.assertRaisesRegex(\n          ValueError,\n          \"Cannot compute gradient inside while loop with respect to op 'x'. \"\n          \"We do not support taking the gradient wrt or through the initial \"\n          \"value of a loop variable. Gradients can be computed through \"\n          \"loop invariants or wrt the input parameters to the loop body.\"):\n        control_flow_ops.while_loop(lambda i, x: i < 3, body, [0, y])\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileGradInWhile(self):\n    with self.cached_session():\n      n = ops.convert_to_tensor(1.0, name=\"n\")\n      x = array_ops.placeholder(dtypes.float32, shape=None)\n      c = lambda n: math_ops.less(n, 10.0)\n      b = lambda n: math_ops.add(n, x)\n\n      def b1(n):\n        r = control_flow_ops.while_loop(c, b, [n],\n                                        [tensor_shape.unknown_shape()])\n        return gradients_impl.gradients(r, x)\n\n      r = control_flow_ops.while_loop(lambda n: n < 6.0, b1, [n],\n                                      [tensor_shape.unknown_shape()])\n      self.assertAllClose(9.0, r.eval(feed_dict={x: 1.0}))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testCondGradInNestedWhiles(self):\n\n    def outer_body(i, x):\n      _, x = control_flow_ops.while_loop(\n          lambda j, x: j < 3, inner_body, [0, 0.0])\n      return i + 1, x\n\n    def inner_body(j, x):\n      y = control_flow_ops.cond(math_ops.less(x, 1), lambda: 2 * x, lambda: x)\n      return j + 1, gradients_impl.gradients(y, x)[0]\n\n    i, x = control_flow_ops.while_loop(lambda i, x: i < 3, outer_body, [0, 0.0])\n\n    with self.cached_session() as sess:\n      i_val, x_val = self.evaluate([i, x])\n      self.assertEqual(i_val, 3)\n      self.assertAllClose(x_val, 1.0)\n\n  @test_util.run_gpu_only\n  def testGpuResourceAccess(self):\n    with ops.device(test.gpu_device_name()):\n      var = resource_variable_ops.ResourceVariable(constant_op.constant(3.0))\n\n    @def_function.function\n    def foo():\n      return control_flow_ops.while_loop(\n          lambda i, _: i < 3,\n          lambda i, x: (i + 1, control_flow_ops.cond(\n              constant_op.constant(True),\n              lambda: x + var,\n              lambda: x)),\n          [0, 0.0])[1]\n\n    self.evaluate(variables.global_variables_initializer())\n    self.assertEqual(self.evaluate(foo()), 9.0)\n\n  def testNestedResourceAccess(self):\n    var = resource_variable_ops.ResourceVariable(constant_op.constant(3.0))\n\n    @eager_function.defun\n    def test_fn():\n      x = constant_op.constant(0.0)\n      r = control_flow_ops.while_loop(\n          # Outer loop condition\n          lambda i, y: i < 2,\n          # Outer loop body\n          lambda i, y: (i + 1, y + control_flow_ops.cond(\n              constant_op.constant(True),\n              # True branch\n              lambda: control_flow_ops.while_loop(\n                  # Inner loop condition\n                  lambda j, z: j < 3,\n                  # Inner loop body\n                  lambda j, z: (j + 1, z + math_ops.square(var)),\n                  # Inner initial loop value\n                  [0, y])[1],\n              # False branch\n              lambda: (0.0))),\n          # Outer initial loop value\n          [0, x])[1]\n\n      grad = gradients_impl.gradients(r, x)[0]\n      return r, grad\n\n    self.evaluate(variables.global_variables_initializer())\n    r, grad = self.evaluate(test_fn())\n    # 2 * 3 * 3^2\n    self.assertEqual(r, 81.0)\n    # v1 control flow gets the wrong answer!!!\n    # Gradient computation:\n    #   f(x) = x + 3^2\n    #   inner_loop(x) = f(f(f(x))) = x + 3*3^2 = x + 27\n    #   g(x) = x + inner_loop(x) = 2x + 27\n    #   outer_loop(x) = g(g(x)) = 4x + 81\n    #   outer_loop'(x) = 4\n    # Note that v1 control flow gets 4.0 as well if the cond is removed.\n    if control_flow_util.ENABLE_CONTROL_FLOW_V2:\n      self.assertEqual(grad, 4.0)\n\n  def testWhile_NestedInput(self):\n    with self.cached_session() as sess:\n      named = collections.namedtuple(\"named\", (\"a\", \"b\"))\n      loop_vars = [\n          named(a=constant_op.constant(0.0), b=constant_op.constant(1.0)),\n          (constant_op.constant(2.0), constant_op.constant(3.0)),\n          constant_op.constant(4.0)\n      ]\n      c = lambda lv0, _1, _2: lv0.a < 100.0\n\n      def b(lv0, lv1, lv2):\n        lv0 = named(a=lv0.a + 1, b=lv0.b)\n        lv1 = (lv1[0] + 1, lv1[1])\n        lv2 += 2\n        return [lv0, lv1, lv2]\n\n      r = control_flow_ops.while_loop(c, b, loop_vars)\n\n      self.assertTrue(isinstance(r, list))\n      self.assertTrue(isinstance(r[0], named))\n      self.assertTrue(isinstance(r[1], tuple))\n      self.assertTrue(isinstance(r[2], ops.Tensor))\n\n      r_flattened = nest.flatten(r)\n      self.assertEqual([100.0, 1.0, 102.0, 3.0, 4.0 + 100 * 2.0],\n                       self.evaluate(r_flattened))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhile_NestedBadArityFails(self):\n    with self.cached_session():\n      named = collections.namedtuple(\"named\", (\"a\", \"b\"))\n      loop_vars = [\n          named(a=constant_op.constant(0.0), b=constant_op.constant(1.0)),\n          (constant_op.constant(2.0), constant_op.constant(3.0)),\n          constant_op.constant(4.0)\n      ]\n      c = lambda lv0, _1, _2: lv0.a < 100.0\n\n      def b(lv0, lv1, _):\n        return [lv0, lv1]\n\n      with self.assertRaisesRegex(ValueError, \"the same number of elements\"):\n        control_flow_ops.while_loop(c, b, loop_vars)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileGrad_ys_xs(self):\n    with self.cached_session():\n      x = constant_op.constant(3.0, name=\"x\")\n      y = constant_op.constant(2.0, name=\"y\")\n\n      c = lambda x, y: math_ops.less(x, 100.0)\n\n      def b(x, y):\n        y1 = math_ops.add(x, y)\n        x1 = math_ops.multiply(x, y1)\n        return x1, y1\n\n      rx, ry = control_flow_ops.while_loop(c, b, [x, y], parallel_iterations=1)\n\n      r = gradients_impl.gradients([rx, ry], x)\n      self.assertAllClose(304.0, r[0])\n      r = gradients_impl.gradients([rx, ry], y)\n      self.assertAllClose(124.0, r[0])\n      r = gradients_impl.gradients([rx], x)\n      self.assertAllClose(295.0, r[0])\n      r = gradients_impl.gradients([rx], y)\n      self.assertAllClose(120.0, r[0])\n\n  @test_util.run_deprecated_v1\n  def testWhileGrad_Dependency(self):\n    with self.cached_session():\n      i = constant_op.constant(0, name=\"i\")\n      x = constant_op.constant(2.0, name=\"x\")\n\n      c = lambda i, x: math_ops.less(i, 10)\n\n      def b(i, x):\n        x = math_ops.multiply(x, 2.0)\n        i = math_ops.add(i, 1)\n        return i, x\n\n      ri, rx = control_flow_ops.while_loop(c, b, [i, x], parallel_iterations=1)\n\n      r = gradients_impl.gradients([ri, rx], x)\n      self.assertAllClose(1024.0, r[0])\n      r = gradients_impl.gradients([rx], x)\n      self.assertAllClose(1024.0, r[0])\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileGrad_NoGradient(self):\n    with self.cached_session():\n      v = constant_op.constant(2.0, name=\"v\")\n      c = lambda v: math_ops.less(v, 100.0)\n      b = math_ops.square\n      r = control_flow_ops.while_loop(c, b, [v], back_prop=False)\n      r = math_ops.add(r, v)\n      r = gradients_impl.gradients(r, v)\n      self.assertAllClose(1.0, r[0])\n\n  @test_util.disable_control_flow_v2(\"b/113324949 (RefVariable)\")\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileGrad_NoDependency(self):\n    with self.cached_session() as sess:\n      variable = variables.Variable(array_ops.ones([2, 3]))\n      duration = array_ops.zeros([], dtype=dtypes.int32)\n\n      def cond(duration, tensor, _):\n        del tensor\n        return duration < 10\n\n      def body(duration, tensor, _):\n        return (duration + 1, tensor, tensor)\n\n      loop_vars = [duration, variable, variable]\n      tensors = control_flow_ops.while_loop(\n          cond=cond, body=body, loop_vars=loop_vars)\n      cost = math_ops.reduce_sum(tensors[2])\n      grad = gradients_impl.gradients(cost, [variable])\n      self.evaluate(variables.global_variables_initializer())\n      self.assertAllClose(np.ones([2, 3]), sess.run(grad[0]))\n\n  @test_util.run_deprecated_v1\n  def testWhileGrad_Const(self):\n    with self.cached_session() as sess:\n      c0 = constant_op.constant(0.0, name=\"c0\")\n      c1 = constant_op.constant(1.0, name=\"c1\")\n      duration = constant_op.constant(0, name=\"t\")\n\n      def cond(duration, _):\n        return duration < 1\n\n      def body(duration, _):\n        return duration + 1, c1\n\n      loop_vars = [duration, c0]\n      tensors = control_flow_ops.while_loop(\n          cond=cond, body=body, loop_vars=loop_vars)\n      cost = math_ops.reduce_sum(tensors[1])\n      grad = gradients_impl.gradients(cost, [c0])\n      self.assertAllClose(0.0, sess.run(grad[0]))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileGrad_SerialTwoLoops(self):\n    with self.cached_session():\n      i = constant_op.constant(0, name=\"i\")\n      x = constant_op.constant(2.0, name=\"x\")\n\n      c = lambda i, x: math_ops.less(i, 5)\n\n      def b(i, x):\n        x = math_ops.multiply(x, 2.0)\n        i = math_ops.add(i, 1)\n        return i, x\n\n      _, rx = control_flow_ops.while_loop(c, b, [i, x], parallel_iterations=1)\n      _, rx = control_flow_ops.while_loop(c, b, [i, rx], parallel_iterations=1)\n\n      r = gradients_impl.gradients([rx], x)\n      self.assertAllClose(1024.0, r[0])\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileGrad_ParallelTwoLoops(self):\n    with self.cached_session():\n      i = constant_op.constant(0, name=\"i\")\n      x = constant_op.constant(2.0, name=\"x\")\n\n      c = lambda i, x: math_ops.less(i, 5)\n\n      def b(i, x):\n        x = math_ops.multiply(x, 2.0)\n        i = math_ops.add(i, 1)\n        return i, x\n\n      _, r1 = control_flow_ops.while_loop(c, b, [i, x], parallel_iterations=1)\n      _, r2 = control_flow_ops.while_loop(c, b, [i, x], parallel_iterations=1)\n      rx = math_ops.add(r1, r2)\n\n      r = gradients_impl.gradients([rx], x)\n      self.assertAllClose(64.0, r[0])\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileGrad_OneOutputWithControlDependencyOnSecond(self):\n    with self.cached_session():\n      i = constant_op.constant(0, name=\"i\")\n      x = constant_op.constant(1.0, name=\"x\")\n      y = constant_op.constant(1.0, name=\"y\")\n      c = lambda i, *_: math_ops.less(i, 1, name=\"cond_less\")\n\n      def b(i, xi, yi):\n        # return (i + 1, xi, xi + yi)\n        return (math_ops.add(i, 1, name=\"inc\"), array_ops.identity(\n            xi, name=\"xi\"), math_ops.add(xi, yi, name=\"xi_plus_yi\"))\n\n      _, x_f, y_f = control_flow_ops.while_loop(c, b, [i, x, y])\n      with ops.control_dependencies([x_f]):\n        y_f_d = array_ops.identity(y_f, name=\"y_f_d\")\n\n      self.assertAllClose(2.0, self.evaluate(y_f_d))  # y_f_d = 1.0 + 1.0\n      g = gradients_impl.gradients([y_f_d], [x])[0]\n      self.assertTrue(g is not None)\n      self.assertAllClose(1.0,\n                          self.evaluate(g))  # y_f_d = x + 1.0, dy_f_d/dx = 1.0\n\n  def _testNestedWhileGrad_Simple(self, use_gpu):\n    with self.cached_session(use_gpu=use_gpu):\n      v = constant_op.constant(1.0)\n\n      def inner_loop(s):\n        c = lambda x: math_ops.less(x, 4.0)\n        b = lambda x: math_ops.multiply(x, 2.0)\n        return control_flow_ops.while_loop(c, b, [s])\n\n      c = lambda x: math_ops.less(x, 2.0)\n      b = lambda x: math_ops.multiply(inner_loop(x), 2.0)\n      r = control_flow_ops.while_loop(c, b, [v])\n\n      r = gradients_impl.gradients(r, v)[0]\n      self.assertAllClose(8.0, self.evaluate(r))\n\n  @test_util.run_deprecated_v1\n  def testNestedWhileGrad_Simple(self):\n    self._testNestedWhileGrad_Simple(use_gpu=False)\n    self._testNestedWhileGrad_Simple(use_gpu=True)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testNestedWhileGrad_SerialInner(self):\n    with self.cached_session():\n      v = constant_op.constant(1.0)\n\n      def inner_loop1(s):\n        z = constant_op.constant(0)\n        c = lambda i, x: math_ops.less(i, 4)\n        b = lambda i, x: [math_ops.add(i, 1), math_ops.multiply(x, 2.0)]\n        return control_flow_ops.while_loop(c, b, [z, s])\n\n      def inner_loop2(s):\n        z = constant_op.constant(0)\n        c = lambda i, x: math_ops.less(i, 4)\n        b = lambda i, x: [math_ops.add(i, 1), math_ops.multiply(x, 2.0)]\n        return control_flow_ops.while_loop(c, b, [z, s])\n\n      c = lambda x: math_ops.less(x, 128.0)\n      b = lambda x: inner_loop2(inner_loop1(x)[1])[1]\n      r = control_flow_ops.while_loop(c, b, [v])\n\n      r = gradients_impl.gradients(r, v)[0]\n      self.assertAllClose(256.0, self.evaluate(r))\n\n  @test_util.run_deprecated_v1\n  def testNestedWhileGrad_ParallelInner(self):\n    with self.cached_session():\n      v = constant_op.constant(1.0)\n\n      def inner_loop1(s):\n        z = constant_op.constant(0)\n        c = lambda i, x: math_ops.less(i, 4)\n        b = lambda i, x: [math_ops.add(i, 1), math_ops.multiply(x, 2.0)]\n        return control_flow_ops.while_loop(c, b, [z, s])\n\n      def inner_loop2(s):\n        z = constant_op.constant(0)\n        c = lambda i, x: math_ops.less(i, 4)\n        b = lambda i, x: [math_ops.add(i, 1), math_ops.multiply(x, 2.0)]\n        return control_flow_ops.while_loop(c, b, [z, s])\n\n      c = lambda x: math_ops.less(x, 128.0)\n      b = lambda x: math_ops.multiply(inner_loop1(x)[1], inner_loop2(x)[1])\n      r = control_flow_ops.while_loop(c, b, [v])\n\n      r = gradients_impl.gradients(r, v)[0]\n      self.assertAllClose(512.0, self.evaluate(r))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testNestedWhileGrad_ParallelIterations(self):\n    # Make sure the stack pushes and pops of an inner loop are executed in\n    # the sequential order of the iterations of its outer loop.\n    with self.cached_session() as sess:\n\n      def inner_loop(t):\n        fn = lambda n: n + math_ops.square(var)\n        return map_fn.map_fn(fn=fn, elems=t, parallel_iterations=10)\n\n      def outer_loop(inp):\n        return map_fn.map_fn(\n            fn=inner_loop, elems=inp, parallel_iterations=10)\n\n      var = variables.Variable(constant_op.constant(3.0))\n      inp = constant_op.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n      res = outer_loop(inp)\n      optimizer = adam.AdamOptimizer(learning_rate=0.001)\n      train_op = optimizer.minimize(math_ops.reduce_mean(math_ops.square(res)))\n      self.evaluate(variables.global_variables_initializer())\n      self.evaluate(train_op)\n      self.assertAllClose(2.999, var.read_value())\n\n  def _testWhileCondGrad_Simple(self, use_gpu):\n    with self.cached_session(use_gpu=use_gpu):\n      v = ops.convert_to_tensor(2.0, name=\"v\")\n      n = ops.convert_to_tensor(100.0, name=\"n\")\n      one = ops.convert_to_tensor(1.0, name=\"one\")\n      c = lambda x: math_ops.less(x, n)\n      # pylint: disable=undefined-variable\n      # for OSS build\n      b = lambda x: control_flow_ops.cond(constant_op.constant(True),\n                                          lambda: math_ops.square(x),\n                                          lambda: math_ops.subtract(x, one))\n      # pylint: enable=undefined-variable\n      r = control_flow_ops.while_loop(c, b, [v])\n      r = gradients_impl.gradients(r, v)[0]\n      self.assertAllClose(1024.0, self.evaluate(r))\n\n  @test_util.run_deprecated_v1\n  def testWhileCondGrad_Simple(self):\n    self._testWhileCondGrad_Simple(use_gpu=False)\n    self._testWhileCondGrad_Simple(use_gpu=True)\n\n  @test_util.run_deprecated_v1\n  def testWhileCondGrad_UnknownShape(self):\n    with self.cached_session() as sess:\n      v = array_ops.placeholder(dtypes.float32)\n      n = ops.convert_to_tensor(100.0, name=\"n\")\n      one = ops.convert_to_tensor(1.0, name=\"one\")\n      c = lambda x: math_ops.less(x, n)\n      # pylint: disable=undefined-variable\n      # for OSS build\n      b = lambda x: control_flow_ops.cond(constant_op.constant(True),\n                                          lambda: math_ops.square(x),\n                                          lambda: math_ops.subtract(x, one))\n      # pylint: enable=undefined-variable\n      r = control_flow_ops.while_loop(c, b, [v])\n      r = gradients_impl.gradients(r, v)[0]\n      r = sess.run(r, feed_dict={v: 2.0})\n      self.assertAllClose(1024.0, r)\n\n  @test_util.run_deprecated_v1\n  def testWhileGrad_Concat(self):\n    with self.cached_session() as sess:\n      x = variable_scope.get_variable(\"x\", initializer=[[1., 2.]])\n      i0 = constant_op.constant(0)\n      h0 = array_ops.zeros([0, 2])\n\n      def condition(i, _):\n        return i < 2\n\n      def body(i, h):\n        return i + 1, array_ops.concat([h, x], 0)\n\n      _, h = control_flow_ops.while_loop(\n          condition, body, [i0, h0],\n          [i0.get_shape(), tensor_shape.TensorShape([None, 2])])\n      s = math_ops.reduce_sum(h)\n\n      optimizer = gradient_descent.GradientDescentOptimizer(0.01)\n      op = optimizer.minimize(s)\n\n      self.evaluate(variables.global_variables_initializer())\n      self.evaluate(op)\n      self.assertAllClose([[0.98000002, 1.98000002]], self.evaluate(x))\n\n  @test_util.disable_control_flow_v2(\"b/113324949 (RefVariable)\")\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileWithRefsWithGradients_1(self):\n    with self.cached_session() as sess:\n      x = variables.VariableV1(0.)._ref()  # pylint: disable=protected-access\n      i = constant_op.constant(0)\n      c = lambda i, x: math_ops.less(i, 10)\n\n      self.assertEqual(x.dtype, dtypes.float32_ref)\n\n      def body(i, x):\n        self.assertEqual(x.dtype, dtypes.float32_ref)\n        return [i + 1, gen_array_ops.ref_identity(x)]\n\n      r = control_flow_ops.while_loop(c, body, [i, x], parallel_iterations=5)\n\n      grad_ys = [variables.VariableV1(73)._ref()]  # pylint: disable=protected-access\n      grad = gradients_impl.gradients([r[1]], [x], grad_ys=grad_ys)\n\n      self.evaluate(variables.global_variables_initializer())\n\n      self.assertEqual(r[0].dtype, dtypes.int32)\n      self.assertEqual(r[1].dtype, dtypes.float32_ref)\n\n      value_i, value_x, value_x_grad = sess.run(r + grad)\n\n    self.assertEqual(10, value_i)\n    self.assertEqual(0, value_x)\n    self.assertEqual(73, value_x_grad)\n\n  @test_util.deprecated_graph_mode_only\n  def testWhileGrad_IndexedSlices(self):\n    with self.cached_session():\n      values = constant_op.constant([2.0, 4.0], name=\"values\")\n      indices = constant_op.constant([0, 3], name=\"indices\")\n      shape = constant_op.constant([10], name=\"dense_shape\")\n      i = constant_op.constant(0)\n      x = ops.IndexedSlices(values, indices, dense_shape=shape)\n\n      def c(i, _):\n        return i < 10\n\n      def b(i, x):\n        return [\n            i + 1,\n            ops.IndexedSlices(x.values * 2.0, x.indices, x.dense_shape)\n        ]\n\n      _, r = control_flow_ops.while_loop(c, b, [i, x])\n      r = gradients_impl.gradients(r.values, values)[0]\n      self.assertAllClose(np.array([1024.0, 1024.0]), self.evaluate(r))\n\n  @test_util.deprecated_graph_mode_only\n  def testWhileGrad_SparseTensor(self):\n    with self.cached_session():\n      values = constant_op.constant([2.0, 4.0], name=\"values\")\n      indices = constant_op.constant(\n          [[0], [3]], dtype=dtypes.int64, name=\"indices\")\n      shape = constant_op.constant([10], dtype=dtypes.int64, name=\"dense_shape\")\n      i = constant_op.constant(0)\n      x = sparse_tensor.SparseTensor(indices, values, dense_shape=shape)\n\n      def c(i, _):\n        return i < 10\n\n      def b(i, x):\n        return [\n            i + 1,\n            sparse_tensor.SparseTensor(x.indices, x.values * 2.0, x.dense_shape)\n        ]\n\n      _, r = control_flow_ops.while_loop(c, b, [i, x])\n      r = gradients_impl.gradients(r.values, values)[0]\n      self.assertAllClose(np.array([1024.0, 1024.0]), self.evaluate(r))\n\n  @test_util.deprecated_graph_mode_only\n  def testCallGradInLoop(self):\n    with self.cached_session() as sess:\n      i0 = constant_op.constant(0)\n      params = constant_op.constant(5.0)\n      params_1 = math_ops.square(params)\n\n      def c(i, _):\n        return i < 10\n\n      def b(i, x):\n        data = constant_op.constant([1.0, 2.0, 3.0])\n        data = math_ops.multiply(data, params_1)\n        x1 = x + gradients_impl.gradients(data, params)[0]\n        return i + 1, x1\n\n      output_grad = control_flow_ops.while_loop(\n          c, b, [i0, constant_op.constant(0.0)])\n      self.assertAllClose(600.0, self.evaluate(output_grad)[1])\n\n  @test_util.run_deprecated_v1\n  def testWhileAndTensorArray(self):\n    with self.cached_session() as sess:\n      param = constant_op.constant(2.0)\n      n0 = constant_op.constant(0)\n      y0 = constant_op.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], name=\"elems\")\n\n      def c(i, _):\n        return i < 10\n\n      def b(i, y):\n        return [\n            i + 1,\n            map_fn.map_fn(lambda x: math_ops.multiply(x, param), y)\n        ]\n\n      r = control_flow_ops.while_loop(c, b, [n0, y0], parallel_iterations=1)\n      r = gradients_impl.gradients(r, param)[0]\n      self.assertAllClose(107520.0, self.evaluate(r))\n\n  @test_util.run_deprecated_v1\n  def testNestedWhileAndTensorArray(self):\n    n = constant_op.constant(3.0)\n\n    def Body(row, ta):\n\n      def InnerBody(row, col, ta):\n        # Note: row and col are 1-based.\n        ta = ta.write(\n            math_ops.cast(n * (row - 1.) + col - 1., dtypes.int32), row * col)\n        return row, col + 1., ta\n\n      ta = control_flow_ops.while_loop(\n          lambda _, col, _1: col <= n,\n          InnerBody, [row, constant_op.constant(1.), ta],\n          return_same_structure=False)[2]\n      return row + 1., ta\n\n    ta = tensor_array_ops.TensorArray(dtype=dtypes.float32, size=9)\n    ta = control_flow_ops.while_loop(\n        lambda row, _: row <= n,\n        Body, [constant_op.constant(1.), ta],\n        return_same_structure=False)[1]\n\n    output = array_ops.reshape(ta.stack(), [3, 3])\n    self.assertAllEqual(\n        self.evaluate(output), [[1., 2., 3.], [2., 4., 6.], [3., 6., 9.]])\n    # TODO(b/117675481): This does not work with current TA. Enable with new TA.\n    # grad = gradients_impl.gradients(output, [n])\n    # self.assertEqual(self.evaluate(grad), 3.5)\n\n  @test_util.run_deprecated_v1\n  def testWhileGrad_StopGrad(self):\n    with self.cached_session():\n      x = constant_op.constant(3.0, name=\"x\")\n      y = constant_op.constant(2.0, name=\"y\")\n\n      c = lambda x, y: math_ops.less(x, 100.0)\n\n      def b(x, y):\n        y1 = math_ops.square(y)\n        x1 = math_ops.add(math_ops.square(x), y1)\n        return x1, y1\n\n      rx, ry = control_flow_ops.while_loop(c, b, [x, y])\n\n      r = gradients_impl.gradients(rx, y)[0]\n      self.assertEqual(136.0, self.evaluate(r))\n      r = gradients_impl.gradients(ry, y)[0]\n      self.assertEqual(32.0, self.evaluate(r))\n\n      r = gradients_impl.gradients(array_ops.stop_gradient(rx), y)[0]\n      self.assertEqual(r, None)\n      r = gradients_impl.gradients(array_ops.stop_gradient(ry), y)[0]\n      self.assertEqual(r, None)\n\n      r = gradients_impl.gradients(\n          array_ops.stop_gradient(math_ops.square(rx)), y)[0]\n      self.assertEqual(r, None)\n      r = gradients_impl.gradients(\n          array_ops.stop_gradient(math_ops.add(rx, ry)), x)[0]\n      self.assertEqual(r, None)\n      r = gradients_impl.gradients(\n          array_ops.stop_gradient(math_ops.add(rx, ry)), y)[0]\n      self.assertEqual(r, None)\n\n      r = gradients_impl.gradients(math_ops.add(rx, ry), y)[0]\n      self.assertEqual(168.0, self.evaluate(r))\n      r = gradients_impl.gradients(\n          math_ops.add(rx, array_ops.stop_gradient(ry)), y)[0]\n      self.assertEqual(136.0, self.evaluate(r))\n      r = gradients_impl.gradients(\n          math_ops.add(array_ops.stop_gradient(rx), ry), y)[0]\n      self.assertEqual(32.0, self.evaluate(r))\n\n  @test_util.run_deprecated_v1\n  def testWhileGrad_StopGradInside(self):\n    with self.cached_session():\n      x = constant_op.constant(3.0, name=\"x\")\n      y = constant_op.constant(2.0, name=\"y\")\n\n      c = lambda x, y: math_ops.less(x, 100.0)\n\n      def b(x, y):\n        y1 = array_ops.stop_gradient(math_ops.square(y))\n        x1 = math_ops.add(math_ops.square(x), y1)\n        return x1, y1\n\n      rx, _ = control_flow_ops.while_loop(c, b, [x, y])\n\n      r = gradients_impl.gradients(rx, y)[0]\n      self.assertAllClose(0.0, self.evaluate(r))\n      r = gradients_impl.gradients(rx, x)[0]\n      self.assertAllClose(156.0, self.evaluate(r))\n\n  @test_util.run_deprecated_v1\n  def testWhileGrad_StopGradInsideNoShape(self):\n    with self.cached_session() as sess:\n      x = array_ops.placeholder(dtypes.float32)\n      y = array_ops.placeholder(dtypes.float32)\n\n      c = lambda x, y: math_ops.less(math_ops.reduce_sum(x), 100.0)\n\n      def b(x, y):\n        y1 = array_ops.stop_gradient(math_ops.square(y, name=\"stopped\"))\n        x1 = math_ops.add(math_ops.square(x), y1)\n        return x1, y1\n\n      rx, _ = control_flow_ops.while_loop(c, b, [x, y])\n\n      grad_y = gradients_impl.gradients(rx, y)[0]\n      grad_x = gradients_impl.gradients(rx, x)[0]\n      feed_dict = {x: [3.0, 4.0], y: [2.0, 3.0]}\n      self.assertAllClose([0.0, 0.0], sess.run(grad_y, feed_dict=feed_dict))\n      self.assertAllClose([156.0, 400.0], sess.run(grad_x, feed_dict=feed_dict))\n      name = \"gradients/while/stopped_grad\"\n      all_ops = x.graph.get_operations()\n      self.assertFalse(any(name in op.name for op in all_ops))\n\n  @test_util.run_deprecated_v1\n  def testWhileGradGradFail(self):\n    theta = variables.Variable(initial_value=1.)\n\n    def fn(prev, x):\n      return prev + x * theta\n\n    result = functional_ops.scan(fn, np.array([1., 2., 3.], dtype=np.float32))\n    grad_theta = gradients_impl.gradients(result, theta)\n    if not control_flow_util.ENABLE_CONTROL_FLOW_V2:\n      with self.assertRaisesRegex(TypeError, \"Second-order gradient\"):\n        gradients_impl.gradients(grad_theta, theta)\n    grad_theta_stopped = array_ops.stop_gradient(grad_theta)\n    gradients_impl.gradients(grad_theta_stopped, theta)\n\n  @test_util.run_deprecated_v1\n  def testStopGradOnWhileGrad(self):\n    with self.cached_session():\n      x = constant_op.constant(2.0, name=\"x\")\n      y = constant_op.constant(2.0, name=\"y\")\n\n      c = lambda x: math_ops.less(x, 100.0)\n      b = lambda x: math_ops.multiply(x, y)\n      rx = control_flow_ops.while_loop(c, b, [x])\n\n      rg = gradients_impl.gradients(rx, y)[0]\n      rg = array_ops.stop_gradient(rg)\n      r = math_ops.add(math_ops.square(y), rx)\n      r = math_ops.add(r, rg)\n      r = gradients_impl.gradients(r, y)[0]\n      self.assertEqual(388.0, self.evaluate(r))\n\n  @test_util.disable_control_flow_v2(\"b/113324949 (RefVariable)\")\n  @test_util.run_deprecated_v1\n  def testWhileGradientWithNontrainablePath1(self):\n    q = variables.Variable([7., 8.])\n\n    def cond(_, y):\n      del y\n      return False\n\n    def body(x, _):\n      return x, math_ops.cast(x, dtypes.float32) + math_ops.reduce_sum(q)\n\n    _, y = control_flow_ops.while_loop(cond, body, (math_ops.argmin(q), 0.))\n    dy_dq, = gradients_impl.gradients(y, q)\n    self.assertIsNotNone(dy_dq)\n    with self.cached_session() as sess:\n      self.evaluate(q.initializer)\n      self.assertAllClose([0., 0.], self.evaluate(dy_dq))\n\n  @test_util.disable_control_flow_v2(\"b/113324949 (RefVariable)\")\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileGradientWithNontrainablePath2(self):\n    q = variables.Variable([7., 8.])\n\n    def cond(_, y):\n      return math_ops.equal(y, 0.)\n\n    def body(x, _):\n      zero = constant_op.constant(0, dtype=dtypes.int64)\n      return zero, math_ops.cast(x, dtypes.float32) + math_ops.reduce_sum(q)\n\n    _, y = control_flow_ops.while_loop(cond, body, (math_ops.argmin(q), 0.))\n    dy_dq, = gradients_impl.gradients(y, q)\n    self.assertIsNotNone(dy_dq)\n    with self.cached_session() as sess:\n      self.evaluate(q.initializer)\n      self.assertAllClose([1., 1.], self.evaluate(dy_dq))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testIssue16504(self):\n    c = constant_op.constant(np.arange(100), dtype=dtypes.float32)\n    w = variables.Variable(\n        initial_value=np.ones(100), dtype=dtypes.float32) / 100\n    k = variables.Variable(0, dtype=dtypes.int32)\n    chg_w = constant_op.constant(np.inf, dtype=dtypes.float32)\n\n    def cond(k, _, chg_w):\n      return math_ops.logical_and(k < 10, chg_w > 1e-3)\n\n    def body(k, w, chg_w):\n      grad, = gradients_impl.gradients(-math_ops.reduce_sum(w * c), w)\n      w_n = w * math_ops.exp(-0.1 * grad)\n      w_n /= math_ops.reduce_sum(w_n)\n      chg_w = (\n          math_ops.reduce_sum(math_ops.abs(w_n - w)) / math_ops.reduce_sum(\n              math_ops.abs(w)))\n      return k + 1, w_n, chg_w\n\n    _, w, _ = control_flow_ops.while_loop(cond, body, [k, w, chg_w])\n    grad, = gradients_impl.gradients(w, c)\n    self.assertIsNotNone(grad)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testStopGradMultiFlows(self):\n    with self.cached_session():\n\n      def body(i, y, r):\n        x = variable_scope.get_variable(\n            \"x\",\n            shape=(),\n            dtype=dtypes.float32,\n            initializer=init_ops.ones_initializer())\n        y *= x\n        return [i + 1, y, r + math_ops.reduce_sum(y)]\n\n      i0 = constant_op.constant(0)\n      y0 = array_ops.ones(5)\n      r0 = constant_op.constant(0.0)\n      cond = lambda i, y, r: i < 1\n      _, _, r = control_flow_ops.while_loop(\n          cond, body, [i0, y0, r0], back_prop=True)\n\n      vars_ = variables.global_variables()\n      grads = linalg_ops.norm(gradients_impl.gradients(r, vars_)[0])\n      z = math_ops.add(r, array_ops.stop_gradient(math_ops.reduce_sum(grads)))\n      result = gradients_impl.gradients(z, vars_)[0]\n      self.evaluate(variables.global_variables_initializer())\n      self.assertEqual(5.0, self.evaluate(result))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testOneValueCond(self):\n\n    with self.cached_session():\n      c = array_ops.placeholder(dtypes.int32, shape=[])\n      one = ops.convert_to_tensor(1, name=\"one\")\n      two = ops.convert_to_tensor(2, name=\"two\")\n      p = math_ops.greater_equal(c, 1)\n      i = control_flow_ops.cond(p, lambda: one, lambda: two)\n      self.assertTrue(isinstance(i, ops.Tensor))\n\n      # True case: c = 2 is >= 1\n      self.assertEqual([1], i.eval(feed_dict={c: 2}))\n\n      # False case: c = 0 is not >= 1\n      self.assertEqual([2], i.eval(feed_dict={c: 0}))\n\n  @test_util.run_deprecated_v1\n  def testExampleCond(self):\n\n    with self.cached_session():\n      x = ops.convert_to_tensor([-2.0, 2.0], name=\"x\")\n      d = array_ops.placeholder(dtypes.int32, shape=[])\n\n      def l2():\n        return math_ops.sqrt(math_ops.reduce_sum(math_ops.square(x)))\n\n      def l1():\n        return math_ops.reduce_sum(math_ops.abs(x))\n\n      i = control_flow_ops.cond(math_ops.equal(d, 2), l2, l1)\n      self.assertAllClose(4.0, i.eval(feed_dict={d: 1}))\n      self.assertAllClose(2.0 * math.sqrt(2), i.eval(feed_dict={d: 2}))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testCase(self):\n    with self.cached_session():\n      x = constant_op.constant(1)\n      y = constant_op.constant(2)\n      z = constant_op.constant(3)\n      f1 = lambda: constant_op.constant(17)\n      f2 = lambda: constant_op.constant(23)\n      f3 = lambda: constant_op.constant(-1)\n\n      r1 = control_flow_ops.case(\n          {\n              x < y: f1,\n              x > z: f2\n          }, default=f3, exclusive=True)\n      self.assertAllEqual(r1, 17)\n\n      r2 = control_flow_ops.case([(y > z, f1), (y > x, f2)], default=f3)\n      self.assertAllEqual(r2, 23)\n\n      # Duplicate events can happen, first one is selected\n      r3 = control_flow_ops.case([(x < y, f1), (x < y, f2)], default=f3)\n      self.assertAllEqual(r3, 17)\n\n      # Duplicate events cause an error if exclusive = True\n      r4 = control_flow_ops.case(\n          [(x < y, f1), (x < y, f2)], default=f3, exclusive=True)\n      with self.assertRaisesOpError(\"Input error:\"):\n        self.evaluate(r4)\n\n      # Check that the default is called if none of the others are\n      r5 = control_flow_ops.case({x > y: f1}, default=f3)\n      self.assertAllEqual(r5, -1)\n\n      ran_once = [False, False, False]\n\n      def break_run_twice(ix):\n\n        def _break():\n          ran_once[ix] = True\n          return constant_op.constant(ix)\n\n        return _break\n\n      # Should not fail - each conditional gets called exactly once\n      # except default.  Default gets called twice: once to create an\n      # empty output and once for the actual cond switch.\n      r6 = control_flow_ops.case(\n          [(x < y, break_run_twice(0)), (x > y, break_run_twice(1))],\n          default=lambda: constant_op.constant(2))\n\n      self.assertAllEqual(r6, 0)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testCaseSideEffects(self):\n    with self.cached_session() as sess:\n      v0 = variables.Variable(-1)\n      v1 = variables.Variable(-1)\n      v2 = variables.Variable(-1)\n\n      a = lambda: control_flow_ops.with_dependencies([state_ops.assign(v0, 0)], 0)\n      b = lambda: control_flow_ops.with_dependencies([state_ops.assign(v1, 1)], 1)\n      c = lambda: control_flow_ops.with_dependencies([state_ops.assign(v2, 2)], 2)\n\n      x = constant_op.constant(1)\n      y = constant_op.constant(2)\n\n      r0 = control_flow_ops.case(\n          ((x < y, a), (x > y, b)), default=c, exclusive=True)\n      r1 = control_flow_ops.case(\n          ((x > y, a), (x < y, b)), default=c, exclusive=True)\n      r2 = control_flow_ops.case(\n          ((x > y, a), (x > y, b)), default=c, exclusive=True)\n\n      self.evaluate(variables.global_variables_initializer())\n      self.assertAllEqual(self.evaluate([v0, v1, v2]), [-1] * 3)\n      self.assertEqual(2, self.evaluate(r2))\n      self.assertAllEqual(self.evaluate([v0, v1, v2]), [-1, -1, 2])\n\n      self.evaluate(variables.global_variables_initializer())\n      self.assertAllEqual(self.evaluate([v0, v1, v2]), [-1] * 3)\n      self.assertEqual(1, self.evaluate(r1))\n      self.assertAllEqual(self.evaluate([v0, v1, v2]), [-1, 1, -1])\n\n      self.evaluate(variables.global_variables_initializer())\n      self.assertAllEqual(self.evaluate([v0, v1, v2]), [-1] * 3)\n      self.assertEqual(0, self.evaluate(r0))\n      self.assertAllEqual(self.evaluate([v0, v1, v2]), [0, -1, -1])\n\n  @test_util.disable_control_flow_v2(\"b/113324949 (ref vars)\")\n  @test_util.run_v1_only(\"b/120545219\")\n  def testOneOpCond(self):\n    with self.cached_session():\n      v = variables.Variable(0)\n      c = ops.convert_to_tensor(0)\n      one = ops.convert_to_tensor(1)\n      two = ops.convert_to_tensor(2)\n      p = math_ops.greater_equal(c, 1)\n\n      def a():\n        return state_ops.assign(v, one)\n\n      def b():\n        return state_ops.assign(v, two)\n\n      i = control_flow_ops.cond(p, a, b)\n      self.assertTrue(isinstance(i, ops.Tensor))\n      self.evaluate(variables.global_variables_initializer())\n\n      self.assertEqual(0, self.evaluate(v))\n\n      # True case: c = 2 is >= 1, v is set to 1.\n      self.assertEqual(1, i.eval(feed_dict={c.name: 2}))\n      self.assertEqual(1, self.evaluate(v))\n\n      # False case: c = 0 is not >= 1, v is set to 2.\n      self.assertEqual(2, i.eval(feed_dict={c.name: 0}))\n      self.assertEqual(2, self.evaluate(v))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWithOpsDependencies(self):\n    with self.cached_session() as sess:\n      v = variables.VariableV1(0.0)\n      c = constant_op.constant(10)\n\n      # Fetching v directly will result in an uninitialized error\n      with self.assertRaisesOpError(\"Attempting to use uninitialized value\"):\n        self.evaluate([c, v])\n\n      # Use a control dependency to ensure init_variable is run\n      # while asking for c\n      real_v = control_flow_ops.with_dependencies(\n          name=\"real_tensor\",\n          output_tensor=v._ref(),  # pylint: disable=protected-access\n          dependencies=[v.initializer])\n      c_val, real_v_val = self.evaluate([c, real_v])\n\n    # Ensure the result of 'real_c' is the same as 'c'\n    self.assertAllEqual(10, c_val)\n\n    # Ensure that 'v' is initialized\n    self.assertAllClose(0.0, real_v_val)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWithTensorDependencies(self):\n    with self.cached_session():\n      v = variables.VariableV1(0.0)\n      c1 = constant_op.constant(10)\n      c2 = constant_op.constant(20)\n\n      # c1_with_init_v depends on the init op for v\n      c1_with_init_v = control_flow_ops.with_dependencies(\n          name=\"c1_with_init_v\", output_tensor=c1, dependencies=[v.initializer])\n      # c2_with_c1 depends on the value of c1_with_init_v\n      c2_with_c1_dep = control_flow_ops.with_dependencies(\n          name=\"c2_with_c1_dep\",\n          output_tensor=c2,\n          dependencies=[c1_with_init_v])\n\n      # Fetching v directly will result in an uninitialized error\n      with self.assertRaisesOpError(\"Attempting to use uninitialized value\"):\n        self.evaluate(v)\n\n      # Get the value of 'c2_with_c1_dep', which should cause 'v'\n      # to be initialized.\n      self.assertAllEqual(20, self.evaluate(c2_with_c1_dep))\n\n      # Ensure that 'v' is initialized\n      self.assertAllClose(0.0, self.evaluate(v))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWithIndexedSlicesDependencies(self):\n    with self.cached_session():\n      v = variables.VariableV1(\n          np.array([[0.0, 1.0], [10.0, 11.0], [20.0, 21.0]]).astype(np.float32))\n      v_at_1 = ops.IndexedSlices(v, constant_op.constant([1]))\n      gather_v_at_1 = array_ops.gather(v_at_1.values, v_at_1.indices)\n      v_at_1_after_init = control_flow_ops.with_dependencies([v.initializer],\n                                                             v_at_1)\n      gather_v_at_1_after_init = array_ops.gather(v_at_1_after_init.values,\n                                                  v_at_1_after_init.indices)\n\n      # Fetching gather_v_at_1 will result in an uninitialized error\n      with self.assertRaisesOpError(\"Attempting to use uninitialized value\"):\n        self.evaluate(gather_v_at_1)\n\n      # Getting gather_v_at_1_after_init will work, and initialize v.\n      self.assertAllEqual([[10.0, 11.0]],\n                          self.evaluate(gather_v_at_1_after_init))\n\n      # Double check that 'v' is initialized\n      self.assertAllClose([[0.0, 1.0], [10.0, 11.0], [20.0, 21.0]],\n                          self.evaluate(v))\n\n  def testDependenciesDevice(self):\n    with ops.Graph().as_default():\n      # device set on tensor => same device on dep.\n      with ops.device(\"/job:ps\"):\n        vd = variables.VariableV1([0.0])\n      with_vd_dep = control_flow_ops.with_dependencies([vd.initializer], vd)\n      self.assertTrue(\"/job:ps\" in with_vd_dep.device)\n\n      # No device set on tensor => no device on dep.\n      vnod = variables.VariableV1([0.0])\n      with_vnod_dep = control_flow_ops.with_dependencies([vnod.initializer],\n                                                         vnod)\n      self.assertDeviceEqual(None, with_vnod_dep.device)\n\n      # device set on tensor, default device on graph => default device on dep.\n      vdef = variables.VariableV1([0.0], name=\"vdef\")\n      with ops.device(\"/job:worker/device:GPU:1\"):\n        with_vdef_dep = control_flow_ops.with_dependencies([vdef.initializer],\n                                                           vdef)\n        # The device is empty, but the colocation constraint is set.\n        self.assertDeviceEqual(\"\", with_vdef_dep.device)\n        self.assertEqual([b\"loc:@vdef\"], with_vdef_dep.op.colocation_groups())\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testGroup(self):\n    with self.cached_session() as sess:\n      v1 = variables.VariableV1([0.0])\n      v2 = variables.VariableV1([1.0])\n\n      # Group init1 and init2 and run.\n      init = control_flow_ops.group(v1.initializer, v2.initializer)\n      # Fetching v1 directly will result in an uninitialized error\n      with self.assertRaisesOpError(\"Attempting to use uninitialized value\"):\n        self.evaluate(v1)\n\n      # Runs \"init\" before fetching v1 and v2.\n      init.run()\n      v1_val, v2_val = self.evaluate([v1, v2])\n\n    # Ensure that v1 and v2 are initialized\n    self.assertAllClose([0.0], v1_val)\n    self.assertAllClose([1.0], v2_val)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testGroupEmpty(self):\n    op = control_flow_ops.group()\n    self.assertEqual(op.type, \"NoOp\")\n    self.assertEqual(op.control_inputs, [])\n\n  @test_util.run_deprecated_v1\n  def testMergeShapes(self):\n    # All inputs unknown.\n    p1 = array_ops.placeholder(dtypes.float32)\n    p2 = array_ops.placeholder(dtypes.float32)\n    p3 = array_ops.placeholder(dtypes.float32)\n    m, index = control_flow_ops.merge([p1, p2, p3])\n    self.assertIs(None, m.get_shape().ndims)\n    self.assertEqual([], index.get_shape())\n\n    # All inputs known with different ranks.\n    p1 = array_ops.placeholder(dtypes.float32, shape=[1, 2])\n    p2 = array_ops.placeholder(dtypes.float32, shape=[1, 2, 3])\n    m, index = control_flow_ops.merge([p1, p2])\n    self.assertIs(None, m.get_shape().ndims)\n    self.assertEqual([], index.get_shape())\n\n    # All inputs known with some dimensions different.\n    p1 = array_ops.placeholder(dtypes.float32, shape=[1, 2])\n    p2 = array_ops.placeholder(dtypes.float32, shape=[2, 1])\n    m, index = control_flow_ops.merge([p1, p2])\n    self.assertEqual([None, None], m.get_shape().as_list())\n    self.assertEqual([], index.get_shape())\n\n    p1 = array_ops.placeholder(dtypes.float32, shape=[1, 2])\n    p2 = array_ops.placeholder(dtypes.float32, shape=[None, 2])\n    m, index = control_flow_ops.merge([p1, p2])\n    self.assertEqual([None, 2], m.get_shape().as_list())\n    self.assertEqual([], index.get_shape())\n\n    p1 = array_ops.placeholder(dtypes.float32, shape=[1, 2])\n    p2 = array_ops.placeholder(dtypes.float32, shape=[2, 2])\n    m, index = control_flow_ops.merge([p1, p2])\n    self.assertEqual([None, 2], m.get_shape().as_list())\n    self.assertEqual([], index.get_shape())\n\n    # All inputs known with same dimensions.\n    p1 = array_ops.placeholder(dtypes.float32, shape=[1, 2])\n    p2 = array_ops.placeholder(dtypes.float32, shape=[1, 2])\n    m, index = control_flow_ops.merge([p1, p2])\n    self.assertEqual([1, 2], m.get_shape().as_list())\n    self.assertEqual([], index.get_shape())\n\n    p1 = array_ops.placeholder(dtypes.float32, shape=[None, 2])\n    p2 = array_ops.placeholder(dtypes.float32, shape=[None, 2])\n    m, index = control_flow_ops.merge([p1, p2])\n    self.assertEqual([None, 2], m.get_shape().as_list())\n    self.assertEqual([], index.get_shape())\n\n    p1 = array_ops.placeholder(dtypes.float32, shape=[None, None])\n    p2 = array_ops.placeholder(dtypes.float32, shape=[None, None])\n    m, index = control_flow_ops.merge([p1, p2])\n    self.assertEqual([None, None], m.get_shape().as_list())\n    self.assertEqual([], index.get_shape())\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testRefSelect(self):\n    index = array_ops.placeholder(dtypes.int32)\n\n    # All inputs unknown.\n    p1 = array_ops.placeholder(dtypes.float32)\n    p2 = array_ops.placeholder(dtypes.float32)\n    p3 = array_ops.placeholder(dtypes.float32)\n    v1 = variables.VariableV1(p1, validate_shape=False)\n    v2 = variables.VariableV1(p2, validate_shape=False)\n    v3 = variables.VariableV1(p3, validate_shape=False)\n    self.assertIs(None, v1.get_shape().ndims)\n    s = control_flow_ops.ref_select(index, [v1, v2, v3])\n    self.assertIs(None, s.get_shape().ndims)\n\n    # All inputs known but different.\n    v1 = variables.VariableV1([[1, 2]])\n    v2 = variables.VariableV1([[2], [1]])\n    s = control_flow_ops.ref_select(index, [v1, v2])\n    self.assertIs(None, s.get_shape().ndims)\n\n    # All inputs known and same.\n    v1 = variables.VariableV1([[1, 2]])\n    v2 = variables.VariableV1([[1, 2]])\n    s = control_flow_ops.ref_select(index, [v1, v2])\n    self.assertEqual([1, 2], s.get_shape())\n\n    # Possibly the same but not guaranteed.\n    v1 = variables.VariableV1([[1., 2.]])\n    p2 = array_ops.placeholder(dtypes.float32, shape=[None, 2])\n    v2 = variables.VariableV1(p2, validate_shape=False)\n    s = control_flow_ops.ref_select(index, [v1, v2])\n    self.assertEqual(None, s.get_shape())\n\n  @test_util.run_deprecated_v1\n  def testRunLoopTensor(self):\n    with self.cached_session() as sess:\n      tensor_list = []\n\n      def condition(t):\n        return t < constant_op.constant(5)\n\n      def body(_):\n        tensor_list.append(constant_op.constant(5))\n        return constant_op.constant(10)\n\n      result = control_flow_ops.while_loop(condition, body,\n                                           [constant_op.constant(4)])\n      self.assertEqual(10, self.evaluate(result))\n\n      # Ensure that we cannot run a tensor that escapes the loop body\n      # accidentally.\n      with self.assertRaises(ValueError):\n        sess.run(tensor_list[0])\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhilePyFuncBasic(self):\n\n    def func(x):\n      return np.square(x)\n\n    with self.cached_session():\n      r = control_flow_ops.while_loop(\n          lambda i, v: i < 4,\n          lambda i, v: [i + 1, script_ops.py_func(func, [v], [dtypes.float32])[0]],\n          [constant_op.constant(0), constant_op.constant(2.0, dtypes.float32)],\n          [tensor_shape.unknown_shape(), tensor_shape.unknown_shape()])\n      self.assertEqual(self.evaluate(r[1]), 65536.0)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileFuncBasic(self):\n\n    @function.Defun(dtypes.float32)\n    def func(x):\n      return math_ops.square(math_ops.square(x))\n\n    with self.cached_session():\n      x = constant_op.constant(2.0, dtypes.float32)\n      r = control_flow_ops.while_loop(\n          lambda i, v: i < 2, lambda i, v: [i + 1, func(v)],\n          [constant_op.constant(0), x],\n          [tensor_shape.unknown_shape(),\n           tensor_shape.unknown_shape()])\n      grad = gradients_impl.gradients(r, x)[0]\n      self.assertEqual(self.evaluate(r[1]), 65536.0)\n      self.assertEqual(self.evaluate(grad), 524288.0)\n      # while_v2 does not have stacks.\n      if not control_flow_util.ENABLE_CONTROL_FLOW_V2:\n        self.assertEqual(\n            len([op for op in x.graph.get_operations() if op.type == \"StackV2\"\n                ]), 1)\n\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testQIntSwitchMerge(self):\n    with self.cached_session(force_gpu=test.is_gpu_available()) as sess:\n      constant_qint = constant_op.constant(np.array([42]), dtypes.qint8)\n      cond = constant_op.constant(True, dtypes.bool)\n      v_f, v_t = control_flow_ops.switch(constant_qint, cond)\n      result = control_flow_ops.merge([v_f, v_t])\n      self.evaluate(result)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testQIntRefSwitchMerge(self):\n    with self.cached_session(use_gpu=test.is_gpu_available()) as sess:\n      var_qint = gen_state_ops.variable(\n          shape=[1], dtype=dtypes.qint8, name=\"v\", container=\"\", shared_name=\"\")\n      assign_op = state_ops.assign(\n          var_qint, constant_op.constant(np.array([42]), dtypes.qint8))\n      self.evaluate(assign_op)\n\n      cond = constant_op.constant(True, dtypes.bool)\n      v_f, v_t = control_flow_ops.ref_switch(var_qint, cond)\n      result = control_flow_ops.ref_merge([v_f, v_t])\n      self.evaluate(result)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testUInt64SwitchMerge(self):\n    with self.cached_session(force_gpu=test.is_gpu_available()) as sess:\n      constant_uint64 = constant_op.constant(np.array([42]), dtypes.uint64)\n      cond = constant_op.constant(True, dtypes.bool)\n      v_f, v_t = control_flow_ops.switch(constant_uint64, cond)\n      result = control_flow_ops.merge([v_f, v_t])\n      self.evaluate(result)\n\n  @test_util.run_deprecated_v1\n  def testQIntArgAndRet(self):\n\n    @function.Defun(dtypes.qint8)\n    def func(x):\n      return x\n\n    with self.cached_session(force_gpu=test.is_gpu_available()) as sess:\n      qint = constant_op.constant(np.array([42]), dtypes.qint8)\n      result = func(qint)\n      self.evaluate(result)\n\n  def testSparseIdentity(self):\n    st1 = sparse_tensor.SparseTensor([[0, 5]], ['x'], [10, 10])\n    st2 = control_flow_ops._Identity(st1)\n    self.assertAllEqual(st1.indices, st2.indices)\n    self.assertAllEqual(st1.values, st2.values)\n    self.assertAllEqual(st1.dense_shape, st2.dense_shape)\n\n  def testSparseEnterExit(self):\n    st1 = sparse_tensor.SparseTensor([[0, 5]], ['x'], [10, 10])\n    st2 = control_flow_ops._Enter(st1, \"foo_1\")\n    st3 = control_flow_ops.exit(st2)\n    self.assertAllEqual(st1.indices, st3.indices)\n    self.assertAllEqual(st1.values, st3.values)\n    self.assertAllEqual(st1.dense_shape, st3.dense_shape)\n\n  def _buildWhileWithShapeInvariants(self, shape_invariants):\n    r = constant_op.constant([1, 2])\n\n    def cond(_):\n      return False\n\n    def body(_):\n      return constant_op.constant([1])\n\n    return control_flow_ops.while_loop(\n        cond, body, [r], shape_invariants=shape_invariants)\n\n  def testWhileOutputShapeWithShapeInvariantsUnknownRank(self):\n    @def_function.function\n    def runTest():\n      while_output = self._buildWhileWithShapeInvariants(\n          [tensor_shape.TensorShape(None)])\n      self.assertIsNone(while_output.shape.rank)\n    runTest()\n\n  def testWhileOutputShapeWithShapeInvariantsPartialShape(self):\n    @def_function.function\n    def runTest():\n      while_output = self._buildWhileWithShapeInvariants(\n          [tensor_shape.TensorShape([None])])\n      self.assertAllEqual(while_output.shape.as_list(), [None])\n    runTest()\n\n  def testFunctionInWhile(self):\n\n    @def_function.function\n    def body(x):\n      return x + 1\n\n    r = control_flow_ops.while_loop(lambda x: x < 5, body, [0])\n    self.assertAllEqual(r, 5.)\n\n\nclass ControlFlowContextCheckTest(test.TestCase):\n\n  def _getWhileTensor(self):\n    \"\"\"Creates and returns a tensor from a while context.\"\"\"\n    tensor = []\n\n    def body(i):\n      if not tensor:\n        tensor.append(constant_op.constant(1))\n      return i + tensor[0]\n\n    control_flow_ops.while_loop(lambda i: i < 10, body, [0])\n    return tensor[0]\n\n  def _getCondTensor(self):\n    cond_tensor = []\n\n    def true_fn():\n      if not cond_tensor:\n        cond_tensor.append(constant_op.constant(1))\n      return cond_tensor[0]\n\n    control_flow_ops.cond(\n        math_ops.less(1, 2), true_fn, lambda: constant_op.constant(0))\n    return cond_tensor[0]\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testInvalidContext(self):\n    # Accessing a while loop tensor outside of control flow is illegal.\n    while_tensor = self._getWhileTensor()\n    with self.assertRaisesRegex(\n        ValueError,\n        \"Cannot use 'while/Const_1' as input to 'Add' because 'while/Const_1' \"\n        \"is in a while loop. See info log for more details.\"):\n      math_ops.add(1, while_tensor)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testInvalidContextInCond(self):\n    # Accessing a while loop tensor in cond is illegal.\n    while_tensor = self._getWhileTensor()\n    with self.assertRaisesRegex(\n        ValueError, \"Cannot use 'while/Const_1' as input to 'cond/Add' because \"\n        \"'while/Const_1' is in a while loop. See info log for more details.\"):\n      # TODO(skyewm): this passes if we return while_tensor directly instead\n      # of using it as input to another op.\n      control_flow_ops.cond(\n          math_ops.less(1, 2), lambda: math_ops.add(1, while_tensor),\n          lambda: constant_op.constant(0))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testInvalidContextInWhile(self):\n    # Accessing a while loop tensor in a different while loop is illegal.\n    while_tensor = self._getWhileTensor()\n    with self.assertRaisesRegex(\n        ValueError,\n        \"Cannot use 'while/Const_1' as input to 'while_1/Add' because they are \"\n        \"in different while loops. See info log for more details.\"):\n      control_flow_ops.while_loop(lambda i: i < 10,\n                                  lambda x: math_ops.add(1, while_tensor), [0])\n\n    with self.assertRaisesRegex(\n        ValueError,\n        \"Cannot use 'while/Const_1' as input to 'while_2/NextIteration' \"\n        \"because they are in different while loops. See info log for more \"\n        \"details.\"):\n      control_flow_ops.while_loop(lambda i: i < 10, lambda i: while_tensor, [0])\n\n  def testValidCondContext(self):\n    # Accessing a tensor from a cond context is OK (although dangerous).\n    cond_tensor = self._getCondTensor()\n    math_ops.add(1, cond_tensor)\n\n  def testValidCondContextBranches(self):\n    # Accessing a tensor from a cond context from the other branch's cond\n    # context is OK (although dangerous).\n    cond_tensor = []\n\n    def branch_fn():\n      if not cond_tensor:\n        cond_tensor.append(constant_op.constant(1))\n      return cond_tensor[0]\n\n    control_flow_ops.cond(math_ops.less(1, 2), branch_fn, branch_fn)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testValidWhileContext(self):\n    # Accessing a tensor in a nested while is OK.\n    def body(_):\n      c = constant_op.constant(1)\n      return control_flow_ops.while_loop(lambda i: i < 3, lambda i: i + c, [0])\n\n    control_flow_ops.while_loop(lambda i: i < 5, body, [0])\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testValidNestedContexts(self):\n    # Accessing a tensor from a cond context in a while context, all inside an\n    # outer while context, is OK.\n    def body(_):\n      cond_tensor = self._getCondTensor()\n      # Create another cond containing the while loop for good measure\n      return control_flow_ops.cond(\n          math_ops.less(1, 2),\n          lambda: control_flow_ops.while_loop(lambda i: i < 3,\n                                              lambda i: i + cond_tensor, [0]),\n          lambda: constant_op.constant(0))\n\n    control_flow_ops.while_loop(lambda i: i < 5, body, [0])\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testInvalidNestedContexts(self):\n    # Accessing a tensor from a while context in a different while context, all\n    # inside a cond context, is illegal.\n    def true_fn():\n      while_tensor = self._getWhileTensor()\n      return control_flow_ops.while_loop(lambda i: i < 3,\n                                         lambda i: i + while_tensor, [0])\n\n    with self.assertRaisesRegex(\n        ValueError,\n        \"Cannot use 'cond/while/Const_1' as input to 'cond/while_1/add' because\"\n        \" they are in different while loops. See info log for more details.\"):\n      control_flow_ops.cond(\n          math_ops.less(1, 2), true_fn, lambda: constant_op.constant(0))\n\n\nclass TupleTest(test.TestCase):\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testTensors(self):\n    for v1_first in [True, False]:\n      with self.cached_session():\n        v1 = variables.VariableV1([1.0])\n        add1 = math_ops.add(\n            control_flow_ops.with_dependencies([v1.initializer], v1._ref()),  # pylint: disable=protected-access\n            2.0)\n        v2 = variables.VariableV1([10.0])\n        add2 = math_ops.add(\n            control_flow_ops.with_dependencies([v2.initializer], v2._ref()),  # pylint: disable=protected-access\n            20.0)\n        t1, _, t2 = control_flow_ops.tuple([add1, None, add2])\n\n        # v1 is not initialized.\n        with self.assertRaisesOpError(\"Attempting to use uninitialized value\"):\n          self.evaluate(v1)\n\n        # v2 is not initialized.\n        with self.assertRaisesOpError(\"Attempting to use uninitialized value\"):\n          self.evaluate(v2)\n\n        if v1_first:\n          # Getting t1 initializes v2.\n          self.assertAllClose([3.0], self.evaluate(t1))\n          self.assertAllClose([10.0], self.evaluate(v2))\n        else:\n          # Getting t2 initializes v1.\n          self.assertAllClose([30.0], self.evaluate(t2))\n          self.assertAllClose([1.0], self.evaluate(v1))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testIndexedSlices(self):\n    for v1_first in [True, False]:\n      with self.cached_session():\n        v1 = variables.VariableV1(\n            np.array([[0.0, 1.0], [10.0, 11.0], [20.0, 21.0]]).astype(\n                np.float32))\n        v1_at_1 = ops.IndexedSlices(\n            control_flow_ops.with_dependencies([v1.initializer], v1._ref()),  # pylint: disable=protected-access\n            constant_op.constant([1]))\n\n        v2 = variables.VariableV1(\n            np.array([[0.1, 1.1], [10.1, 11.1], [20.1, 21.1]]).astype(\n                np.float32))\n        v2_at_1 = ops.IndexedSlices(\n            control_flow_ops.with_dependencies([v2.initializer], v2._ref()),  # pylint: disable=protected-access\n            constant_op.constant([1]))\n\n        st1, st2 = control_flow_ops.tuple([v1_at_1, v2_at_1])\n        g1 = array_ops.gather(st1.values, st1.indices)\n        g2 = array_ops.gather(st2.values, st2.indices)\n\n        # v1 is not initialized.\n        with self.assertRaisesOpError(\"Attempting to use uninitialized value\"):\n          self.evaluate(v1)\n\n        # v2 is not initialized.\n        with self.assertRaisesOpError(\"Attempting to use uninitialized value\"):\n          self.evaluate(v2)\n\n        if v1_first:\n          # Getting g1 initializes v2.\n          self.assertAllClose([[10.0, 11.0]], self.evaluate(g1))\n          self.assertAllClose([[0.1, 1.1], [10.1, 11.1], [20.1, 21.1]],\n                              self.evaluate(v2))\n        else:\n          # Getting g2 initializes v1.\n          self.assertAllClose([[10.1, 11.1]], self.evaluate(g2))\n          self.assertAllClose([[0.0, 1.0], [10.0, 11.0], [20.0, 21.0]],\n                              self.evaluate(v1))\n\n  def testAcceptTensorsAsControlInputs(self):\n    with self.cached_session():\n      var = variables.VariableV1(0)\n      assign = state_ops.assign(var, 1)\n      t, = control_flow_ops.tuple(\n          [constant_op.constant(0)], control_inputs=[assign])\n\n      # Should trigger the assign.\n      self.evaluate(t)\n\n      self.assertEqual(1, self.evaluate(var))\n\n\nclass AssertTest(test.TestCase):\n\n  @test_util.run_deprecated_v1\n  def testGuardedAssertDoesNotCopyWhenTrue(self):\n    if test_util.is_gpu_available():\n      self.skipTest(\"b/128646478 fails in opensource\")\n\n    with self.session(use_gpu=True) as sess:\n      with ops.device(test.gpu_device_name()):\n        value = constant_op.constant(1.0)\n      with ops.device(\"/cpu:0\"):\n        true = constant_op.constant(True)\n        guarded_assert = control_flow_ops.Assert(true, [value], name=\"guarded\")\n        unguarded_assert = gen_logging_ops._assert(\n            true, [value], name=\"unguarded\")\n      opts = config_pb2.RunOptions(trace_level=config_pb2.RunOptions.FULL_TRACE)\n      guarded_metadata = config_pb2.RunMetadata()\n      sess.run(guarded_assert, options=opts, run_metadata=guarded_metadata)\n      unguarded_metadata = config_pb2.RunMetadata()\n      sess.run(unguarded_assert, options=opts, run_metadata=unguarded_metadata)\n      guarded_nodestat_names = [\n          n.node_name\n          for d in guarded_metadata.step_stats.dev_stats\n          for n in d.node_stats\n      ]\n      unguarded_nodestat_names = [\n          n.node_name\n          for d in unguarded_metadata.step_stats.dev_stats\n          for n in d.node_stats\n      ]\n      guarded_memcpy_nodestat_names = [\n          n for n in guarded_nodestat_names if \"MEMCPYDtoH\" in n\n      ]\n      unguarded_memcpy_nodestat_names = [\n          n for n in unguarded_nodestat_names if \"MEMCPYDtoH\" in n\n      ]\n      if \"GPU\" in [d.device_type for d in device_lib.list_local_devices()]:\n        # A copy was performed for the unguarded assert\n        self.assertLess(0, len(unguarded_memcpy_nodestat_names),\n                        str(unguarded_nodestat_names))\n      # No copy was performed for the guarded assert\n      self.assertEqual([], guarded_memcpy_nodestat_names)\n\n\nclass WhileOpBenchmark(test.Benchmark):\n  \"\"\"Evaluate the performance of while_loop op.\"\"\"\n\n  def _getInitVariables(self):\n    batch_size = 10\n    image_size = 256\n    kernel_size = 3\n    depth = 16\n\n    init_step = constant_op.constant(-1)\n    image = variable_scope.get_variable(\n        \"image\",\n        initializer=random_ops.random_normal(\n            [batch_size, image_size, image_size, depth],\n            dtype=dtypes.float32,\n            stddev=1e-1))\n    kernel = variable_scope.get_variable(\n        \"weights\",\n        initializer=random_ops.truncated_normal(\n            [kernel_size, kernel_size, depth, depth],\n            dtype=dtypes.float32,\n            stddev=1e-1))\n    return init_step, image, kernel\n\n  def _runOneBenchmark(self,\n                       default_device,\n                       num_iters=10,\n                       static_unroll=False,\n                       steps=10):\n    \"\"\"Evaluate the while loop performance.\n\n    Args:\n      default_device: The default device to run all ops except the loop_body.\n        loop_body is always run on GPU.\n      num_iters: Number of iterations to run.\n      static_unroll: If true, run unrolled version; otherwise, run while_loop.\n      steps: Total number of repeated steps to run the loop.\n\n    Returns:\n      The duration of the run in seconds.\n    \"\"\"\n\n    def loop_body(i, x):\n      with ops.device(\"/gpu:0\"):\n        # Always put loop body on GPU.\n        nx = nn_ops.conv2d(\n            input=x,\n            filter=kernel,\n            strides=[1, 1, 1, 1],\n            padding=\"SAME\",\n            data_format=\"NHWC\",\n            name=\"conv2d\")\n        ni = math_ops.add(i, 1)\n        return ni, nx\n\n    ops.reset_default_graph()\n    with session.Session() as sess, ops.device(default_device):\n      # Get the initial id i, input x, and kernel.\n      i, x, kernel = self._getInitVariables()\n      self.evaluate(variables.global_variables_initializer())\n\n      if static_unroll:\n        for _ in xrange(steps):\n          i, x = loop_body(i, x)\n      else:\n        i, x = control_flow_ops.while_loop(\n            lambda i, _: i < steps,\n            loop_body, [i, x],\n            parallel_iterations=steps,\n            swap_memory=True)\n\n      r = math_ops.reduce_sum(x)\n      dx, dk = gradients_impl.gradients(r, [x, kernel])\n      # Use group to avoid fetching back results.\n      r = control_flow_ops.group(dx, dk)\n\n      for _ in xrange(3):\n        # exclude warm up time\n        self.evaluate(r)\n\n      start_time = time.time()\n      for _ in xrange(num_iters):\n        self.evaluate(r)\n      return (time.time() - start_time) / num_iters\n\n  def benchmarkWhileOpCrossDevicePlacement(self):\n    iters = 10\n    # Run loop body on GPU, but other ops on CPU.\n    duration = self._runOneBenchmark(\"cpu\", iters, static_unroll=False)\n    self.report_benchmark(\n        name=\"while_op_cross_device\", iters=iters, wall_time=duration)\n\n  def benchmarkWhileOpSameDevicePlacement(self):\n    iters = 10\n    # Run all ops on the same GPU device.\n    duration = self._runOneBenchmark(\"gpu\", iters, static_unroll=False)\n    self.report_benchmark(\n        name=\"while_op_same_device\", iters=iters, wall_time=duration)\n\n  def benchmarkWhileOpUnrollCrossDevicePlacement(self):\n    iters = 10\n    # Run loop body on GPU, but other ops on CPU.\n    duration = self._runOneBenchmark(\"cpu\", iters, static_unroll=True)\n    self.report_benchmark(\n        name=\"unroll_cross_device_cpu\", iters=iters, wall_time=duration)\n\n  def benchmarkWhileOpUnrollSameDevicePlacement(self):\n    iters = 10\n    # Run all ops on GPU.\n    duration = self._runOneBenchmark(\"gpu\", iters, static_unroll=True)\n    self.report_benchmark(\n        name=\"unroll_same_device\", iters=iters, wall_time=duration)\n\n\n@test_util.with_control_flow_v2\nclass EagerTest(test.TestCase):\n\n  def testCond(self):\n    with context.eager_mode():\n      pred = math_ops.less(1, 2)\n      fn1 = lambda: [constant_op.constant(10)]\n      fn2 = lambda: [constant_op.constant(20)]\n      r = control_flow_ops.cond(pred, fn1, fn2)\n\n      self.assertAllEqual(r.numpy(), 10)\n      self.assertFalse(isinstance(r, list))\n\n  # TODO(b/117279927): Re-enable once msan failure is fixed.\n  def DISABLED_testCondInDefun(self):\n    with context.eager_mode():\n\n      @eager_function.defun\n      def foo(pred):\n        # TODO(b/111124878): this only needs to output one element.\n        fn1 = lambda: (constant_op.constant(10), constant_op.constant(100))\n        fn2 = lambda: (constant_op.constant(20), constant_op.constant(200))\n        return control_flow_ops.cond(constant_op.constant(pred), fn1, fn2)\n\n      r = foo(True)\n      self.assertAllEqual(r[0].numpy(), 10)\n      self.assertNotIsInstance(r, list)\n\n      r = foo(False)\n      self.assertAllEqual(r[0].numpy(), 20)\n      self.assertFalse(isinstance(r, list))\n\n  def testWhileLoop(self):\n    with context.eager_mode():\n      tensor = constant_op.constant([1, 2, 3, 4, 5])\n      self.assertAllEqual(isum(tensor).numpy(), [46, 47, 48, 49, 50])\n\n  def testWhileLoopWithMaxIterations(self):\n    with context.eager_mode():\n      tensor = constant_op.constant([1, 2, 3, 4, 5])\n      self.assertAllEqual(\n          isum(tensor, maximum_iterations=3).numpy(),\n          [1 + 3, 2 + 3, 3 + 3, 4 + 3, 5 + 3])\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileWithMaximumIterationsAndSingleArgument(self):\n    with context.eager_mode():\n      tensor = constant_op.constant(0)\n      r = control_flow_ops.while_loop(\n          lambda i: i < 3, lambda i: i + 1, [tensor], maximum_iterations=1)\n      self.assertEqual(1, r.numpy())\n\n  def testWithDependencies(self):\n    with context.eager_mode():\n      t1 = constant_op.constant(1)\n      t2 = constant_op.constant(2)\n      t3 = control_flow_ops.with_dependencies(t1, t2)\n      self.assertAllEqual(t2.numpy(), t3.numpy())\n\n  def testTuple(self):\n    with context.eager_mode():\n      t1 = constant_op.constant(1)\n      t2 = constant_op.constant(2)\n      tup1, tup2 = control_flow_ops.tuple([t1, t2])\n      self.assertAllEqual(t1.numpy(), tup1.numpy())\n      self.assertAllEqual(t2.numpy(), tup2.numpy())\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testCase(self):\n    with context.eager_mode():\n      x = constant_op.constant(1)\n      y = constant_op.constant(2)\n      z = constant_op.constant(3)\n      f1 = lambda: constant_op.constant(17)\n      f2 = lambda: constant_op.constant(23)\n      f3 = lambda: constant_op.constant(-1)\n\n      r1 = control_flow_ops.case(\n          [(x < y, f1), (x > z, f2)], default=f3, exclusive=True)\n      self.assertAllEqual(r1.numpy(), 17)\n\n\nif __name__ == \"__main__\":\n  test.main()\n"], "fixing_code": ["/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include \"tensorflow/core/common_runtime/eager/kernel_and_device.h\"\n\n#include <memory>\n\n#include \"absl/strings/match.h\"\n#include \"tensorflow/core/common_runtime/device_factory.h\"\n#include \"tensorflow/core/common_runtime/eager/attr_builder.h\"\n#include \"tensorflow/core/common_runtime/process_function_library_runtime.h\"\n#include \"tensorflow/core/common_runtime/rendezvous_mgr.h\"\n#include \"tensorflow/core/framework/allocator.h\"\n#include \"tensorflow/core/framework/cancellation.h\"\n#include \"tensorflow/core/framework/function.h\"\n#include \"tensorflow/core/framework/node_def.pb.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/resource_mgr.h\"\n#include \"tensorflow/core/framework/types.h\"\n#include \"tensorflow/core/framework/types.pb.h\"\n#include \"tensorflow/core/lib/core/errors.h\"\n#include \"tensorflow/core/lib/core/refcount.h\"\n#include \"tensorflow/core/lib/gtl/cleanup.h\"\n#include \"tensorflow/core/lib/gtl/map_util.h\"\n#include \"tensorflow/core/lib/random/random.h\"\n#include \"tensorflow/core/platform/denormal.h\"\n#include \"tensorflow/core/platform/errors.h\"\n#include \"tensorflow/core/platform/fingerprint.h\"\n#include \"tensorflow/core/platform/setround.h\"\n#include \"tensorflow/core/profiler/lib/annotated_traceme.h\"\n#include \"tensorflow/core/profiler/lib/traceme.h\"\n#include \"tensorflow/core/public/version.h\"\n#include \"tensorflow/core/util/tensor_slice_reader_cache.h\"\n#if !defined(IS_MOBILE_PLATFORM)\n#include \"tensorflow/core/grappler/optimizers/meta_optimizer.h\"\n#endif  // !IS_MOBILE_PLATFORM\n\nnamespace tensorflow {\n\nStatus EagerKernelArgs::GetLocalArg(const FunctionArgIndex& index,\n                                    Tensor* val) const {\n  if (index.sub_index >= 0) {\n    return errors::InvalidArgument(\"Got unexpected sub_index \", index.sub_index,\n                                   \" for argument \", index.index);\n  }\n  Tensor* arg = tensor_args_.at(index.index).tensor;\n  if (arg) {\n    *val = *arg;\n    return Status::OK();\n  } else {\n    return errors::NotFound(\"Argument \", index.index, \" has no local tensor.\");\n  }\n}\n\nstd::vector<Tensor> EagerKernelArgs::GetLocalTensors() const {\n  std::vector<Tensor> lcoal_inputs;\n  lcoal_inputs.reserve(tensor_args_.size());\n  for (const TensorValue& tensor_value : tensor_args_) {\n    lcoal_inputs.push_back(*tensor_value.tensor);\n  }\n  return lcoal_inputs;\n}\n\nstd::function<void(std::function<void()>)>* KernelAndDevice::get_runner()\n    const {\n  if (runner_) {\n    return runner_;\n  } else {\n    static auto* default_runner =\n        new std::function<void(std::function<void()>)>(\n            [](const std::function<void()>& f) { f(); });\n    return default_runner;\n  }\n}\n\nKernelAndDeviceFunc::~KernelAndDeviceFunc() {\n  if (handle_ != kInvalidHandle) {\n    Status status = pflr_->ReleaseHandle(handle_);\n    if (!status.ok()) {\n      LOG(INFO) << \"Ignoring error status when releasing multi-device function \"\n                   \"handle \"\n                << status.ToString();\n    }\n  }\n}\n\nStatus KernelAndDeviceOp::Init(const Context& ctx, const NodeDef& ndef,\n                               GraphCollector* graph_collector) {\n  OpKernel* k = nullptr;\n  if (flr_ == nullptr) {\n    return errors::Internal(\n        \"A valid FunctionLibraryRuntime must be provided when running ops \"\n        \"based on OpKernel.\");\n  }\n  std::shared_ptr<const NodeProperties> props;\n  TF_RETURN_IF_ERROR(NodeProperties::CreateFromNodeDef(\n      ndef, flr_->GetFunctionLibraryDefinition(), &props));\n  TF_RETURN_IF_ERROR(flr_->CreateKernel(props, &k));\n  kernel_.reset(k);\n\n  input_alloc_attrs_.resize(kernel_->num_inputs());\n  input_devices_.resize(kernel_->num_inputs(), device_);\n  for (size_t i = 0; i < input_alloc_attrs_.size(); ++i) {\n    bool host = kernel_->input_memory_types()[i] == tensorflow::HOST_MEMORY;\n    input_alloc_attrs_[i].set_on_host(host);\n    if (host) {\n      input_devices_[i] = host_cpu_device_;\n    }\n  }\n  output_alloc_attrs_.resize(kernel_->num_outputs());\n  for (size_t i = 0; i < output_alloc_attrs_.size(); ++i) {\n    output_alloc_attrs_[i].set_on_host(kernel_->output_memory_types()[i] ==\n                                       tensorflow::HOST_MEMORY);\n  }\n\n  return Status::OK();\n}\n\nStatus KernelAndDeviceFunc::InstantiateFunc(const Context& ctx,\n                                            const NodeDef& ndef,\n                                            GraphCollector* graph_collector) {\n  const OpDef* op_def = nullptr;\n  const FunctionDef* function_def;\n  if (flr_ == nullptr) {\n    // If function is being executed without an explicit device request,\n    // lookup the FunctionDef in the CPU's FLR. All FLRs share the same\n    // library.\n    function_def = pflr_->GetFLR(host_cpu_device_->name())\n                       ->GetFunctionLibraryDefinition()\n                       ->Find(ndef.op());\n  } else {\n    function_def = flr_->GetFunctionLibraryDefinition()->Find(ndef.op());\n  }\n\n  if (function_def != nullptr) {\n    op_def = &(function_def->signature());\n  } else {\n    TF_RETURN_IF_ERROR(OpDefForOp(ndef.op(), &op_def));\n  }\n  TF_RETURN_IF_ERROR(\n      InOutTypesForNode(ndef, *op_def, &input_dtypes_, &output_dtypes_));\n\n  FunctionLibraryRuntime::InstantiateOptions options;\n  options.target = device_ == nullptr ? \"\" : device_->name();\n  options.is_multi_device_function = true;\n  for (const Device* device : input_devices_) {\n    options.input_devices.push_back(device->name());\n  }\n  options.composite_devices = composite_devices_;\n  options.input_resource_dtypes_and_shapes = input_resource_dtypes_and_shapes_;\n\n  const auto& it = ndef.attr().find(\"executor_type\");\n  if (it != ndef.attr().end()) {\n    options.executor_type = it->second.s();\n  }\n  const auto& is_component_fn_it = ndef.attr().find(\"is_component_function\");\n  if (is_component_fn_it != ndef.attr().end()) {\n    options.is_component_function = is_component_fn_it->second.b();\n  }\n#if !defined(IS_MOBILE_PLATFORM)\n  // Android tf library does not include grappler.\n  const auto& config_it = ndef.attr().find(\"config_proto\");\n  if (it != ndef.attr().end()) {\n    if (!options.config_proto.ParseFromString(config_it->second.s())) {\n      return errors::InvalidArgument(\n          \"Failed to parse config_proto attribute as tensorflow::ConfigProto \"\n          \"proto.\");\n    }\n    grappler::GrapplerItem::OptimizationOptions optimization_options;\n\n    // Tensorflow 2.0 in eager mode with automatic control dependencies will\n    // prune all nodes that are not in the transitive fanin of the fetch nodes.\n    // However because the function will be executed via FunctionLibraryRuntime,\n    // and current function implementation does not prune stateful and dataset\n    // ops, we rely on Grappler to do the correct graph pruning.\n    optimization_options.allow_pruning_stateful_and_dataset_ops = true;\n\n    optimization_options.is_eager_mode = true;\n\n    // All the nested function calls will be executed and optimized via\n    // PartitionedCallOp, there is no need to optimize functions now.\n    optimization_options.optimize_function_library = false;\n\n    options.optimize_graph_fn = std::bind(\n        grappler::OptimizeGraph, std::placeholders::_1, std::placeholders::_2,\n        std::placeholders::_3, std::placeholders::_4, std::placeholders::_5,\n        options.config_proto, function_def->signature().name(),\n        optimization_options, std::placeholders::_6);\n  }\n#endif  // !IS_MOBILE_PLATFORM\n  options.graph_collector = graph_collector;\n\n  // In Eager mode we always inline all functions into the top-level\n  // function body graph, to get a single executable graph, that could be\n  // optimized across function boundaries (e.g. prune unused inputs and outputs\n  // in a function call chain). This is required to mimic graph mode execution,\n  // with aggressive pruning of nodes not in the transitive fanin of fetches.\n  options.config_proto.mutable_graph_options()\n      ->mutable_optimizer_options()\n      ->set_do_function_inlining(true);\n\n  options.config_proto.set_log_device_placement(ctx.log_device_placement);\n\n  TF_RETURN_IF_ERROR(\n      pflr_->Instantiate(ndef.op(), AttrSlice(ndef), options, &handle_));\n  return pflr_->IsCrossProcess(handle_, &is_cross_process_);\n}\n\nStatus KernelAndDeviceFunc::Init(const Context& ctx, const NodeDef& ndef,\n                                 GraphCollector* graph_collector) {\n  TF_RETURN_IF_ERROR(InstantiateFunc(ctx, ndef, graph_collector));\n  return pflr_->GetOutputDevices(handle_, &output_devices_,\n                                 ctx.eager_lazy_copy);\n}\n\nnamespace {\n// In certain contexts (e.g. TPU async executions), the CancellationManager is\n// used to shut down the device in error scenarios (as opposed to using the\n// AsyncCompute's DoneCallback). This is handled through the\n// {inc,dec}_num_deferred_ops_function.\nstruct OpExecutionState : public core::RefCounted {\n  // TODO(nareshmodi): consider refcounting the cancellation_manager.\n  CancellationManager cancellation_manager;\n};\n}  // anonymous namespace\n\nStatus KernelAndDeviceOp::Run(\n    ScopedStepContainer* step_container, const EagerKernelArgs& inputs,\n    std::vector<EagerKernelRet>* outputs,\n    CancellationManager* cancellation_manager,\n    const absl::optional<EagerRemoteFunctionParams>& remote_func_params) {\n  OpKernelContext::Params params;\n  params.device = device_;\n  params.frame_iter = FrameAndIter(0, 0);\n  params.inputs = inputs.GetTensorValues();\n  params.op_kernel = kernel_.get();\n  params.resource_manager = device_->resource_manager();\n  params.input_alloc_attrs = &input_alloc_attrs_;\n  params.output_attr_array = output_alloc_attrs_.data();\n  params.function_library = flr_;\n  params.slice_reader_cache = &slice_reader_cache_;\n  params.rendezvous = rendezvous_;\n  OpExecutionState* op_execution_state = nullptr;\n\n  CancellationManager default_cancellation_manager;\n  if (cancellation_manager) {\n    params.cancellation_manager = cancellation_manager;\n  } else if (kernel_->is_deferred()) {\n    op_execution_state = new OpExecutionState;\n    params.cancellation_manager = &op_execution_state->cancellation_manager;\n    params.inc_num_deferred_ops_function = [op_execution_state]() {\n      op_execution_state->Ref();\n    };\n    params.dec_num_deferred_ops_function = [op_execution_state]() {\n      op_execution_state->Unref();\n    };\n  } else {\n    params.cancellation_manager = &default_cancellation_manager;\n  }\n\n  params.log_memory = log_memory_;\n\n  params.runner = get_runner();\n\n  params.step_container =\n      step_container == nullptr ? &step_container_ : step_container;\n  auto step_container_cleanup = gtl::MakeCleanup([step_container, this] {\n    if (step_container == nullptr) {\n      this->step_container_.CleanUp();\n    }\n  });\n\n  params.collective_executor =\n      collective_executor_ ? collective_executor_->get() : nullptr;\n\n  OpKernelContext context(&params);\n\n  {\n    port::ScopedFlushDenormal flush;\n    port::ScopedSetRound round(FE_TONEAREST);\n    // 'AnnotatedTraceMe' will trace both scheduling time on host and execution\n    // time on device of the OpKernel.\n    profiler::AnnotatedTraceMe activity(\n        [&] { return kernel_->TraceString(context, /*verbose=*/false); },\n        profiler::TraceMeLevel::kInfo);\n    device_->Compute(kernel_.get(), &context);\n  }\n\n  // Clean up execution op_execution_state if deferred ops aren't running.\n  if (op_execution_state != nullptr) {\n    op_execution_state->Unref();\n  }\n\n  if (!context.status().ok()) return context.status();\n\n  if (outputs != nullptr) {\n    outputs->clear();\n    for (int i = 0; i < context.num_outputs(); ++i) {\n      const auto* output_tensor = context.mutable_output(i);\n      if (output_tensor != nullptr) {\n        outputs->push_back(Tensor(*output_tensor));\n      } else {\n        outputs->push_back(Tensor());\n      }\n    }\n  }\n  return Status::OK();\n}\n\nStatus KernelAndDeviceFunc::Run(\n    ScopedStepContainer* step_container, const EagerKernelArgs& inputs,\n    std::vector<EagerKernelRet>* outputs,\n    CancellationManager* cancellation_manager,\n    const absl::optional<EagerRemoteFunctionParams>& remote_func_params) {\n  Notification n;\n  Status status;\n  RunAsync(step_container, inputs, outputs, cancellation_manager,\n           remote_func_params, [&status, &n](const Status& s) {\n             status = s;\n             n.Notify();\n           });\n  n.WaitForNotification();\n  return status;\n}\n\nvoid KernelAndDeviceFunc::RunAsync(\n    ScopedStepContainer* step_container, const EagerKernelArgs& inputs,\n    std::vector<EagerKernelRet>* outputs,\n    CancellationManager* cancellation_manager,\n    const absl::optional<EagerRemoteFunctionParams>& remote_func_params,\n    std::function<void(const Status&)> done) {\n  std::shared_ptr<FunctionLibraryRuntime::Options> opts = nullptr;\n  if (remote_func_params.has_value()) {\n    const EagerRemoteFunctionParams& params = remote_func_params.value();\n    if (params.step_id.has_value()) {\n      // If the function is a remote component of a cross-process function,\n      // re-use the step id as its parent function's.\n      opts = std::make_shared<FunctionLibraryRuntime::Options>(\n          params.step_id.value());\n    } else {\n      opts = std::make_shared<FunctionLibraryRuntime::Options>();\n    }\n    // Reuse the op id if it exists.\n    opts->op_id = params.op_id;\n  } else {\n    opts = std::make_shared<FunctionLibraryRuntime::Options>();\n    if (get_op_id_ && is_cross_process_) {\n      // If the function is a cross-process function and the remote execution\n      // goes through eager service, create an eager op id for the function.\n      opts->op_id = get_op_id_();\n    }\n  }\n\n  // We don't pass rendezvous from eager context because we can get tensor\n  // name collisions in send/recv ops when running multiple instances\n  // of the same multi-device function concurrently.\n  Rendezvous* rendezvous = rendezvous_creator_(opts->step_id);\n  opts->rendezvous = rendezvous;\n  opts->create_rendezvous = false;\n\n  // Create a cancellation manager to be used by FLR options if caller does not\n  // pass in one. If the caller does provide one, pass it to process FLR and the\n  // locally created one will be unused.\n  std::shared_ptr<CancellationManager> local_cm;\n  if (cancellation_manager) {\n    opts->cancellation_manager = cancellation_manager;\n  } else {\n    local_cm = std::make_shared<CancellationManager>();\n    opts->cancellation_manager = local_cm.get();\n  }\n  opts->allow_dead_tensors = true;\n  opts->step_container =\n      step_container == nullptr ? &step_container_ : step_container;\n  opts->collective_executor =\n      collective_executor_ ? collective_executor_->get() : nullptr;\n\n  opts->stats_collector = nullptr;\n  opts->runner = get_runner();\n\n  outputs->clear();\n\n  pflr_->Run(*opts, handle_, inputs, outputs,\n             [opts, rendezvous, local_cm, step_container, this,\n              done = std::move(done)](const Status& s) {\n               rendezvous->Unref();\n               if (step_container == nullptr) {\n                 this->step_container_.CleanUp();\n               }\n               done(s);\n             });\n}\n\ntensorflow::Device* KernelAndDeviceOp::OutputDevice(int idx) const {\n  if (kernel_->output_memory_types()[idx] == HOST_MEMORY) {\n    return nullptr;\n  }\n  return device_;\n}\n\ntensorflow::Device* KernelAndDeviceFunc::OutputDevice(int idx) const {\n  if (output_dtypes_[idx] == DT_RESOURCE) {\n    return nullptr;\n  }\n  return output_devices_[idx];\n}\n\ntensorflow::Device* KernelAndDeviceOp::OutputResourceDevice(int idx) const {\n  if (kernel_->output_type(idx) == DT_RESOURCE) {\n    return device_;\n  }\n  return nullptr;\n}\n\ntensorflow::Device* KernelAndDeviceFunc::OutputResourceDevice(int idx) const {\n  if (output_dtypes_[idx] == DT_RESOURCE) {\n    return output_devices_[idx];\n  }\n  return nullptr;\n}\n\nDevice* KernelAndDeviceOp::InputDevice(int i) const {\n  return input_devices_[i];\n}\n\nDevice* KernelAndDeviceFunc::InputDevice(int i) const {\n  if ((input_dtypes_[i] == DT_RESOURCE) &&\n      (composite_devices_.find(input_devices_[i]->name()) ==\n       composite_devices_.end())) {\n    return host_cpu_device_;\n  } else {\n    return input_devices_[i];\n  }\n}\n\n}  // namespace tensorflow\n", "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OiR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n# pylint: disable=g-long-lambda\n\"\"\"Tests for tensorflow.ops.control_flow_ops.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport math\nimport re\nimport sys\nimport time\n\nfrom absl.testing import parameterized\nimport numpy as np\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\n\nfrom tensorflow.core.protobuf import config_pb2\nfrom tensorflow.python import tf2\nfrom tensorflow.python.client import device_lib\nfrom tensorflow.python.client import session\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.eager import def_function\nfrom tensorflow.python.eager import function as eager_function\nfrom tensorflow.python.eager import wrap_function\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors_impl\nfrom tensorflow.python.framework import function\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import sparse_tensor\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.framework import tensor_spec\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import control_flow_util\nfrom tensorflow.python.ops import data_flow_ops\nfrom tensorflow.python.ops import functional_ops\nfrom tensorflow.python.ops import gen_array_ops\nfrom tensorflow.python.ops import gen_control_flow_ops\nfrom tensorflow.python.ops import gen_data_flow_ops\nfrom tensorflow.python.ops import gen_logging_ops\nfrom tensorflow.python.ops import gen_state_ops\nfrom tensorflow.python.ops import gradient_checker_v2\nfrom tensorflow.python.ops import gradients_impl\nfrom tensorflow.python.ops import init_ops\nfrom tensorflow.python.ops import linalg_ops\nfrom tensorflow.python.ops import logging_ops\nfrom tensorflow.python.ops import map_fn\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import nn_grad  # pylint: disable=unused-import\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.ops import random_ops\nfrom tensorflow.python.ops import resource_variable_ops\nfrom tensorflow.python.ops import script_ops\nfrom tensorflow.python.ops import sparse_ops\nfrom tensorflow.python.ops import state_ops\nfrom tensorflow.python.ops import tensor_array_grad  # pylint: disable=unused-import\nfrom tensorflow.python.ops import tensor_array_ops\nfrom tensorflow.python.ops import variable_scope\nfrom tensorflow.python.ops import variables\nfrom tensorflow.python.ops import while_v2  # pylint: disable=unused-import\n# pylint: disable=unused-import\nfrom tensorflow.python.ops.ragged import ragged_factory_ops\nfrom tensorflow.python.ops.ragged import ragged_tensor\nimport tensorflow.python.ops.tensor_array_grad\n# pylint: enable=unused-import\nfrom tensorflow.python.platform import test\nfrom tensorflow.python.training import adam\nfrom tensorflow.python.training import gradient_descent\nfrom tensorflow.python.util import nest\n\n\ndef check_consumers(graph):\n  \"\"\"Sanity check on the consumer list of the tensors.\"\"\"\n\n  consumer_count = {}\n  for op in graph.get_operations():\n    for v in op.inputs:\n      cnt = consumer_count.get(v, 0)\n      consumer_count[v] = cnt + 1\n  for k, v in consumer_count.items():\n    if len(k.consumers()) != v:\n      return False\n  return True\n\n\ndef all_fetchables():\n  tensor_names = []\n  graph = ops.get_default_graph()\n  for op in graph.get_operations():\n    for t in op.outputs:\n      if graph.is_fetchable(t):\n        tensor_names.append(t.name)\n  return tensor_names\n\n\ndef all_feedables():\n  feedable_tensors = []\n  graph = ops.get_default_graph()\n  for op in graph.get_operations():\n    for t in op.inputs:\n      if graph.is_feedable(t):\n        feedable_tensors.append(t)\n  return feedable_tensors\n\n\ndef opt_cfg(do_constant_folding=True):\n  return config_pb2.ConfigProto(\n      allow_soft_placement=True,\n      graph_options=config_pb2.GraphOptions(\n          optimizer_options=config_pb2.OptimizerOptions(\n              opt_level=config_pb2.OptimizerOptions.L1,\n              do_function_inlining=True,\n              do_constant_folding=do_constant_folding)))\n\n\ndef isum(s, maximum_iterations=None):\n  i = constant_op.constant(0, name=\"i\")\n  c = lambda i, s: math_ops.less(i, 10)\n  b = lambda i, s: [math_ops.add(i, 1), math_ops.add(i, s)]\n  _, r_s = control_flow_ops.while_loop(\n      c, b, [i, s], maximum_iterations=maximum_iterations)\n  return r_s\n\n\ndef enqueue_print_op(s):\n  \"\"\"Enqueues an op that prints a message to be captured in the test.\"\"\"\n  return logging_ops.print_v2(\"ControlFlowOpsTest: \" + s)\n\n\ndef filter_test_messages(s):\n  \"\"\"Returns a list of messages printed by enqueue_print_op.\"\"\"\n  prefix = \"ControlFlowOpsTest: \"\n  return [l[len(prefix):] for l in s.split(\"\\n\") if l.startswith(prefix)]\n\n\ndef tf_function_in_tf2(f):\n  if tf2.enabled():\n    # In TF1 do not wrap with tf.function so that we can test the v1 control\n    # flow code path.\n    return def_function.function(f)\n  return f\n\n\n@test_util.with_control_flow_v2\nclass ControlFlowTest(test.TestCase, parameterized.TestCase):\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testRefIdentity(self):\n    with self.cached_session():\n      v = variables.VariableV1(7)\n\n      v = control_flow_ops._Identity(v)\n      op = state_ops.assign(v, 9)\n      v2 = control_flow_ops.with_dependencies([op], v)\n\n      self.assertTrue(isinstance(v2, ops.Tensor))\n      self.evaluate(variables.global_variables_initializer())\n      self.assertEqual(9, self.evaluate(v2))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testRefEnter(self):\n    with self.cached_session():\n      v = variables.VariableV1(7)\n\n      enter_v = control_flow_ops._Enter(v, \"foo_1\", is_constant=True)\n      nine = constant_op.constant(9)\n      enter_nine = gen_control_flow_ops.enter(nine, \"foo_1\")\n      op = state_ops.assign(enter_v, enter_nine)\n      v2 = control_flow_ops.with_dependencies([op], enter_v)\n      v3 = control_flow_ops.exit(v2)\n      self.evaluate(variables.global_variables_initializer())\n      self.assertEqual(9, self.evaluate(v3))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testRefSwitch(self):\n    with self.cached_session():\n      v = variables.VariableV1(7)\n\n      p = constant_op.constant(True)\n      v1 = control_flow_ops._SwitchRefOrTensor(v._ref(), p)  # pylint: disable=protected-access\n      v2 = state_ops.assign(v1[1], 9)\n      self.evaluate(variables.global_variables_initializer())\n      self.assertEqual(9, self.evaluate(v2))\n\n  def testEnterMulExit(self):\n    with self.cached_session():\n      data = constant_op.constant([1, 2, 3, 4, 5, 6], name=\"data\")\n      enter_data = gen_control_flow_ops.enter(data, \"foo_1\", False)\n      five = constant_op.constant(5)\n      enter_five = gen_control_flow_ops.enter(five, \"foo_1\", False)\n      mul_op = math_ops.multiply(enter_data, enter_five)\n      exit_op = control_flow_ops.exit(mul_op)\n\n      result = self.evaluate(exit_op)\n    self.assertAllEqual(np.array([x * 5 for x in [1, 2, 3, 4, 5, 6]]), result)\n\n  @test_util.run_deprecated_v1\n  def testEnterShapePropagation(self):\n    with self.cached_session():\n      v = variables.Variable([0.0, 0.0], dtype=dtypes.float32)\n\n      # If is_constant=True, the shape information should be propagated.\n      enter_v_constant = gen_control_flow_ops.enter(\n          v, \"frame1\", is_constant=True)\n      self.assertEqual(enter_v_constant.shape, [2])\n\n      # Otherwise, the shape should be unknown.\n      enter_v_non_constant = gen_control_flow_ops.enter(\n          v, \"frame2\", is_constant=False)\n      self.assertEqual(enter_v_non_constant.shape, None)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testSwitchMergeIndexedSlices(self):\n    with self.cached_session():\n      values = constant_op.constant([1, 2, 3, 4, 5, 6])\n      indices = constant_op.constant([0, 2, 4, 6, 8, 10])\n      data = ops.IndexedSlices(values, indices)\n      pred = ops.convert_to_tensor(True)\n      switch_op = control_flow_ops.switch(data, pred)\n      merge_op = control_flow_ops.merge(switch_op)[0]\n\n      val = merge_op.values\n      ind = merge_op.indices\n    self.assertAllEqual(np.arange(1, 7), val)\n    self.assertAllEqual(np.arange(0, 12, 2), ind)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testSwitchDeadBranch(self):\n    with self.cached_session():\n      data = constant_op.constant([1, 2, 3, 4, 5, 6], name=\"data\")\n      ports = ops.convert_to_tensor(True, name=\"ports\")\n      switch_op = control_flow_ops.switch(data, ports)\n      dead_branch = array_ops.identity(switch_op[0])\n\n      with self.assertRaisesWithPredicateMatch(\n          errors_impl.InvalidArgumentError,\n          lambda e: \"Retval[0] does not have value\" in str(e)):\n        self.evaluate(dead_branch)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testSwitchMergeLess(self):\n    with self.cached_session():\n      data = constant_op.constant([1, 2, 3, 4, 5, 6], name=\"data\")\n      zero = ops.convert_to_tensor(0)\n      one = ops.convert_to_tensor(1)\n      less_op = math_ops.less(zero, one)\n      switch_op = control_flow_ops.switch(data, less_op)\n      merge_op = control_flow_ops.merge(switch_op)[0]\n\n      result = self.evaluate(merge_op)\n    self.assertAllEqual(np.arange(1, 7), result)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testSwitchMergeAddIdentity(self):\n    with self.cached_session():\n      data = constant_op.constant([1, 2, 3, 4, 5, 6], name=\"data\")\n      ports = ops.convert_to_tensor(False, name=\"ports\")\n      switch_op = control_flow_ops.switch(data, ports)\n      one = constant_op.constant(1)\n      add_op = math_ops.add(switch_op[0], one)\n      id_op = array_ops.identity(switch_op[1])\n      merge_op = control_flow_ops.merge([add_op, id_op])[0]\n\n      result = self.evaluate(merge_op)\n    self.assertAllEqual(np.array([x + 1 for x in [1, 2, 3, 4, 5, 6]]), result)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testSwitchMergeAddMul(self):\n    with self.cached_session():\n      data = constant_op.constant([1, 2, 3, 4, 5, 6], name=\"data\")\n      ports = ops.convert_to_tensor(True, name=\"ports\")\n      switch_op = control_flow_ops.switch(data, ports)\n      one = constant_op.constant(1)\n      add_op = math_ops.add(switch_op[0], one)\n      five = constant_op.constant(5)\n      mul_op = math_ops.multiply(switch_op[1], five)\n      merge_op = control_flow_ops.merge([add_op, mul_op])[0]\n\n      result = self.evaluate(merge_op)\n    self.assertAllEqual(np.array([x * 5 for x in [1, 2, 3, 4, 5, 6]]), result)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testLoop_false(self):\n    with self.cached_session():\n      false = ops.convert_to_tensor(False)\n      n = constant_op.constant(10)\n\n      enter_false = gen_control_flow_ops.enter(false, \"foo_1\", False)\n      enter_n = gen_control_flow_ops.enter(n, \"foo_1\", False)\n\n      merge_n = control_flow_ops.merge([enter_n, enter_n], name=\"merge_n\")[0]\n      switch_n = control_flow_ops.switch(merge_n, enter_false)\n      exit_n = control_flow_ops.exit(switch_n[0])\n      next_n = control_flow_ops.next_iteration(switch_n[0])\n      merge_n.op._update_input(1, next_n)\n\n      result = self.evaluate(exit_n)\n    self.assertAllEqual(10, result)\n\n  @test_util.run_deprecated_v1\n  def testLoop_1(self):\n    with self.cached_session():\n      zero = constant_op.constant(0)\n      one = constant_op.constant(1)\n      n = constant_op.constant(10)\n\n      enter_i = gen_control_flow_ops.enter(zero, \"foo\", False)\n      enter_one = gen_control_flow_ops.enter(one, \"foo\", True)\n      enter_n = gen_control_flow_ops.enter(n, \"foo\", True)\n\n      with ops.device(test.gpu_device_name()):\n        merge_i = control_flow_ops.merge([enter_i, enter_i])[0]\n\n      less_op = math_ops.less(merge_i, enter_n)\n      cond_op = control_flow_ops.loop_cond(less_op)\n      switch_i = control_flow_ops.switch(merge_i, cond_op)\n\n      add_i = math_ops.add(switch_i[1], enter_one)\n\n      next_i = control_flow_ops.next_iteration(add_i)\n      merge_i.op._update_input(1, next_i)\n\n      exit_i = control_flow_ops.exit(switch_i[0])\n      result = self.evaluate(exit_i)\n    self.assertAllEqual(10, result)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testLoop_2(self):\n    with self.cached_session():\n      zero = constant_op.constant(0)\n      one = constant_op.constant(1)\n      n = constant_op.constant(10)\n\n      enter_i = gen_control_flow_ops.enter(zero, \"foo\", False)\n      enter_one = gen_control_flow_ops.enter(one, \"foo\", True)\n      enter_n = gen_control_flow_ops.enter(n, \"foo\", True)\n\n      merge_i = control_flow_ops.merge([enter_i, enter_i])[0]\n\n      less_op = math_ops.less(merge_i, enter_n)\n      cond_op = control_flow_ops.loop_cond(less_op)\n      switch_i = control_flow_ops.switch(merge_i, cond_op)\n\n      add_i = math_ops.add(switch_i[1], enter_one)\n\n      with ops.device(test.gpu_device_name()):\n        next_i = control_flow_ops.next_iteration(add_i)\n      merge_i.op._update_input(1, next_i)\n\n      exit_i = control_flow_ops.exit(switch_i[0])\n      result = self.evaluate(exit_i)\n    self.assertAllEqual(10, result)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testDifferentFrame(self):\n    with self.cached_session():\n      data = array_ops.placeholder(dtypes.float32, shape=[])\n      enter_1 = gen_control_flow_ops.enter(data, \"foo_1\", False)\n      enter_2 = gen_control_flow_ops.enter(data, \"foo_2\", False)\n      res = math_ops.add(enter_1, enter_2)\n      with self.assertRaisesOpError(\"has inputs from different frames\"):\n        res.eval(feed_dict={data: 1.0})\n\n  @test_util.run_deprecated_v1\n  def testCondBool(self):\n    values = constant_op.constant(10)\n    fn1 = lambda: math_ops.add(values, 1)\n    fn2 = lambda: math_ops.subtract(values, 1)\n    with self.assertRaisesRegex(TypeError, \"must not be a Python bool\"):\n      _ = control_flow_ops.cond(False, fn1, fn2)\n\n  @test_util.run_deprecated_v1\n  def testCondInt(self):\n    p = array_ops.placeholder(dtypes.bool, shape=[])\n    v = constant_op.constant(10)\n    fn1 = lambda: math_ops.add(v, 1)\n    fn2 = lambda: math_ops.subtract(v, 1)\n    y = control_flow_ops.cond(p, fn1, fn2)\n    grad = gradients_impl.gradients(y, [v])\n    self.assertAllEqual([None], grad)\n\n  def testCondOutputShape(self):\n    x = constant_op.constant(1.0)\n    b = control_flow_ops.cond(\n        constant_op.constant(True), lambda: math_ops.square(x),\n        lambda: math_ops.subtract(x, 1.))\n    self.assertEqual(b.shape, tensor_shape.TensorShape([]))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testFetchable(self):\n    with self.cached_session() as sess:\n      x = array_ops.placeholder(dtypes.float32)\n      control_flow_ops.cond(\n          constant_op.constant(True), lambda: x + 2, lambda: x + 0)\n      graph = ops.get_default_graph()\n      for op in graph.get_operations():\n        for t in op.inputs:\n          if graph.is_fetchable(t.op):\n            sess.run(t, feed_dict={x: 3})\n          else:\n            with self.assertRaisesRegex(ValueError,\n                                        \"has been marked as not fetchable\"):\n              sess.run(t, feed_dict={x: 3})\n\n  @test_util.disable_control_flow_v2(\"Not relevant\")\n  @test_util.run_v1_only(\"b/120545219\")\n  def testFeedable(self):\n    with self.cached_session() as sess:\n      c = constant_op.constant(2)\n      i0 = constant_op.constant(0)\n      r = control_flow_ops.while_loop(lambda i: i < 1000,\n                                      lambda i: math_ops.square(c) + i, [i0])\n      self.assertEqual(1000, r.eval(feed_dict={i0: 0}))\n      feedable_tensors = all_feedables()\n      for t in feedable_tensors:\n        sess.run(r, feed_dict={t: 3})\n      graph = ops.get_default_graph()\n      for op in graph.get_operations():\n        for t in op.inputs:\n          if t not in feedable_tensors and t.dtype is dtypes.int32:\n            with self.assertRaisesRegex(ValueError, \"may not be fed\"):\n              sess.run(r, feed_dict={t: 3})\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testCondIndexedSlices(self):\n    with self.cached_session():\n      values = constant_op.constant([10])\n      indices = constant_op.constant([0])\n      x = ops.IndexedSlices(values, indices)\n      pred = math_ops.less(1, 2)\n      fn1 = lambda: ops.IndexedSlices(math_ops.add(x.values, 1), indices)\n      fn2 = lambda: ops.IndexedSlices(math_ops.subtract(x.values, 1), indices)\n      r = control_flow_ops.cond(pred, fn1, fn2)\n\n      val = r.values\n      ind = r.indices\n    self.assertAllEqual([11], val)\n    self.assertAllEqual([0], ind)\n\n  def testCondMismatchedIndexedSlices(self):\n    @def_function.function\n    def foo():\n      values = constant_op.constant([10])\n      indices = constant_op.constant([0])\n      x = ops.IndexedSlices(values, indices)\n      with self.assertRaisesRegex(TypeError,\n                                  \"Cannot reconcile tf.cond 0-th outputs\"):\n        control_flow_ops.cond(\n            constant_op.constant(True),\n            lambda: ops.IndexedSlices(math_ops.add(x.values, 1), indices),\n            lambda: math_ops.add(x.values, 1), indices)\n    foo()\n\n  def testCondSparseTensor(self):\n    values = constant_op.constant([2.0, 4.0], name=\"values\")\n    indices = constant_op.constant([[0], [3]],\n                                   dtype=dtypes.int64,\n                                   name=\"indices\")\n    shape = constant_op.constant([10], dtype=dtypes.int64, name=\"dense_shape\")\n    x = sparse_tensor.SparseTensor(indices, values, dense_shape=shape)\n    pred = math_ops.less(1, 2)\n    fn1 = lambda: sparse_tensor.SparseTensor(\n        indices + 1, x.values + 1, dense_shape=shape)\n    fn2 = lambda: sparse_tensor.SparseTensor(\n        indices, x.values - 1, dense_shape=shape)\n    r = control_flow_ops.cond(pred, fn1, fn2)\n    self.assertAllEqual([3.0, 5.0], r.values)\n    self.assertAllEqual([[1], [4]], r.indices)\n    self.assertAllEqual(r.values.get_shape(), (2,))\n\n  def testCondRaggedTensor(self):\n    rt = ragged_factory_ops.constant([[1, 2], [3], [4, 5, 6]])\n    pred = math_ops.less(1, 2)\n    fn1 = lambda: array_ops.concat([rt + 2, [[100]]], axis=0)\n    fn2 = lambda: rt[:2] - 2\n    result = control_flow_ops.cond(pred, fn1, fn2)\n    self.assertAllEqual([3, 4, 5, 6, 7, 8, 100], result.values)\n    self.assertAllEqual([0, 2, 3, 6, 7], result.row_splits)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testCondResource(self):\n\n    with self.cached_session():\n      rv = resource_variable_ops.ResourceVariable(True)\n      self.evaluate(variables.global_variables_initializer())\n      t = ops.convert_to_tensor(1.0)\n\n      def case():\n        assign = resource_variable_ops.assign_variable_op(rv.handle, False)\n        with ops.control_dependencies([assign]):\n          return array_ops.identity(t)\n\n      self.assertEqual(\n          1.0, self.evaluate(control_flow_ops.cond(rv, case, lambda: t)))\n\n  @test_util.run_deprecated_v1\n  def testCondResourceGradShape(self):\n    rv1 = resource_variable_ops.ResourceVariable([1.0, 2.0])\n    rv2 = resource_variable_ops.ResourceVariable([3.0, 4.0])\n    pred = constant_op.constant(True)\n    result = control_flow_ops.cond(pred, lambda: rv1, lambda: rv2)\n    grads = gradients_impl.gradients(result, [rv1, rv2])\n    self.assertAllEqual(grads[0].shape.as_list(), [2])\n    self.assertAllEqual(grads[1].shape.as_list(), [2])\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testCondWithTensorArrayGrad(self):\n    with self.cached_session() as sess:\n      with ops.device(test.gpu_device_name()):\n        pred = array_ops.placeholder(dtypes.bool, [])\n        x = constant_op.constant([1.0, 2.0, 3.0])\n        y = control_flow_ops.cond(\n            pred, lambda: map_fn.map_fn(lambda z: z * 2.0, x),\n            lambda: constant_op.constant([1.0, 1.0, 1.0]))\n        g = gradients_impl.gradients(y, x)[0]\n\n      self.assertAllEqual(sess.run(g, {pred: True}), [2.0, 2.0, 2.0])\n      self.assertAllEqual(sess.run(g, {pred: False}), [0.0, 0.0, 0.0])\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testCondIndexedSlicesDifferentTypes(self):\n    with self.cached_session():\n      values = constant_op.constant([10])\n      i_32 = ops.convert_to_tensor([0], name=\"one\", dtype=dtypes.int32)\n      i_64 = ops.convert_to_tensor([0], name=\"one\", dtype=dtypes.int64)\n      x = ops.IndexedSlices(values, i_32)\n      pred = math_ops.less(1, 2)\n      fn1 = lambda: ops.IndexedSlices(math_ops.add(x.values, 1), i_32)\n      fn2 = lambda: ops.IndexedSlices(math_ops.subtract(x.values, 1), i_64)\n      r = control_flow_ops.cond(pred, fn1, fn2)\n\n      val = r.values\n      ind = r.indices\n    self.assertAllEqual([11], val)\n    self.assertAllEqual([0], ind)\n    self.assertTrue(ind.dtype == np.int64)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testCondColocation(self):\n    with self.session(use_gpu=True):\n      with ops.device(\"/cpu:0\"):\n        v = variables.Variable(7.0)\n\n      x = constant_op.constant(10.0)\n      pred = math_ops.less(1.0, 2.0)\n      fn1 = lambda: math_ops.add(v, 1.0)\n      fn2 = lambda: math_ops.subtract(x, 1.0)\n      r = control_flow_ops.cond(pred, fn1, fn2)\n\n      for op in x.graph.get_operations():\n        if op.name == \"cond/Add/Switch\":\n          self.assertDeviceEqual(op.device, \"/cpu:0\")\n\n  def _testCond_1(self, use_gpu):\n    with self.cached_session(use_gpu=use_gpu):\n      x = constant_op.constant(10)\n      pred = math_ops.less(1, 2)\n      fn1 = lambda: math_ops.add(x, 1)\n      fn2 = lambda: math_ops.subtract(x, 1)\n      r = control_flow_ops.cond(pred, fn1, fn2)\n\n      result = self.evaluate(r)\n    self.assertAllEqual(11, result)\n\n  def testCond_1(self):\n\n    self._testCond_1(use_gpu=False)\n    # TODO(b/116526896): Enable GPU tests.\n    # self._testCond_1(use_gpu=True)\n\n  def testCond_2(self):\n\n    with self.cached_session():\n      x = constant_op.constant(10)\n      r = control_flow_ops.cond(\n          math_ops.less(1, 0), lambda: math_ops.add(x, 1),\n          lambda: math_ops.subtract(x, 1))\n      result = self.evaluate(r)\n    self.assertAllEqual(9, result)\n\n  def testCond_3(self):\n\n    with self.cached_session():\n      x = constant_op.constant(10)\n      pred = math_ops.less(1, 2)\n      fn1 = lambda: math_ops.add(x, 1)\n      fn2 = lambda: math_ops.subtract(x, 1)\n      fn3 = lambda: math_ops.add(control_flow_ops.cond(pred, fn1, fn2), 1)\n      r = control_flow_ops.cond(pred, fn3, fn2)\n\n      result = self.evaluate(r)\n    self.assertAllEqual(12, result)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testCondPruning(self):\n    v1 = variables.Variable(7)\n    v2 = variables.Variable(7)\n    v3 = variables.Variable(7)\n\n    def f():\n      age = constant_op.constant(3)\n      max_age = constant_op.constant(2)\n      pred = math_ops.greater(age, max_age)\n      fn1 = lambda: [state_ops.assign(v1, 1).op, state_ops.assign(v2, 2).op]\n      fn2 = lambda: [state_ops.assign(v3, 3).op, constant_op.constant(10).op]\n      r = control_flow_ops.cond(pred, fn1, fn2)\n      self.assertEqual(len(r), 2)\n      return r[1]\n\n    f_defun = eager_function.defun(f)\n\n    if not context.executing_eagerly():\n      with self.cached_session():\n        self.evaluate(variables.global_variables_initializer())\n        result = self.evaluate(f())\n        self.assertEqual(True, result)\n        # Only second cond result was fetched, so v1 assign shouldn't run.\n        self.assertEqual(7, self.evaluate(v1))\n        self.assertEqual(2, self.evaluate(v2))\n        self.assertEqual(7, self.evaluate(v3))\n\n    result = f_defun()\n    self.assertEqual(True, self.evaluate(result))\n    # Both v1 and v2 branch assignments should be run in defun.\n    self.assertEqual(1, self.evaluate(v1))\n    self.assertEqual(2, self.evaluate(v2))\n    self.assertEqual(7, self.evaluate(v3))\n\n  def testCond_5(self):\n    with self.cached_session():\n      alive = constant_op.constant(True, name=\"alive\")\n      count = constant_op.constant(0, name=\"count\")\n\n      def body(i):\n        return control_flow_ops.cond(\n            alive, lambda: [math_ops.less(i, 3), math_ops.add(count, 1)],\n            lambda: [alive, count])\n\n      for i in range(10):\n        alive, count = body(i)\n      self.assertAllEqual(4, self.evaluate(count))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testCond_6(self):\n    with self.cached_session():\n      v1 = variables.Variable([7])\n\n      age = constant_op.constant(3)\n      pred = math_ops.greater(age, 4)\n      fn1 = lambda: age\n      fn2 = lambda: v1\n      r = control_flow_ops.cond(pred, fn1, fn2)\n\n      self.evaluate(variables.global_variables_initializer())\n      result = self.evaluate(r)\n      self.assertAllEqual(np.array([7]), result)\n\n  def testCond_7(self):\n    with self.cached_session() as sess:\n      x = constant_op.constant(10)\n      y = constant_op.constant(200)\n      pred = math_ops.less(1, 2)\n      fn1 = lambda: [math_ops.add(x, 1), math_ops.add(x, 2)]\n      fn2 = lambda: [y, y]\n      r = control_flow_ops.cond(pred, fn1, fn2)\n      self.assertAllEqual([11, 12], self.evaluate(r))\n\n  @parameterized.parameters(dtypes.float32, dtypes.float64)\n  @test_util.run_v1_only(\"Uses tf.gradients\")\n  def testCondResourceGrad(self, dtype):\n    init = constant_op.constant([7.], dtype=dtype)\n    v1 = variables.Variable(init)\n\n    age = constant_op.constant(3., dtype=dtype)\n    pred = math_ops.greater(age, 4.)\n    fn1 = lambda: age\n    fn2 = lambda: v1\n    r = control_flow_ops.cond(pred, fn1, fn2)\n\n    grad = gradients_impl.gradients(r, v1)[0]\n    self.evaluate(variables.global_variables_initializer())\n    self.assertAllEqual(grad, [1.])\n\n  @test_util.run_gpu_only\n  @test_util.run_deprecated_v1\n  def testCond_Device(self):\n    x = constant_op.constant(-10.)\n\n    # True branch function defined outside of device scope\n    def true_fn():\n      return math_ops.exp(x)\n\n    with ops.device(\"CPU:0\"):\n      r = control_flow_ops.cond(\n          constant_op.constant(True), true_fn, lambda: 0.)\n      self.assertIn(\"cpu\", r.device.lower())\n\n    with session.Session() as sess:\n      options = config_pb2.RunOptions(output_partition_graphs=True)\n      run_metadata = config_pb2.RunMetadata()\n      sess.run(r, options=options, run_metadata=run_metadata)\n      # We expect that everything runs on CPU, even if GPU is available.\n      self.assertEqual(len(run_metadata.partition_graphs), 1)\n\n  def _count_matching_switch_nodes_on_device(self, run_metadata, device_str):\n    # Returns the number of Switch nodes with type float32 placed on\n    # `device_str`.\n    device_graphs = [\n        g for g in run_metadata.partition_graphs\n        if device_str in g.node[0].device\n    ]\n    self.assertLen(device_graphs, 1)\n    switch_nodes = [\n        n for n in device_graphs[0].node if n.op == \"Switch\" and\n        n.attr[\"T\"].type == dtypes.float32.as_datatype_enum\n    ]\n    return len(switch_nodes)\n\n  @test_util.run_gpu_only\n  @test_util.run_deprecated_v1\n  def testCondSwitchColocatedWithInputWhenInputOnCPU(self):\n    x = array_ops.placeholder(dtypes.float32)\n\n    # `arg` is used in the cond then branch so a Switch node is created for it.\n    # We test that the Switch node gets placed on the same device as `arg`.\n    # We force `arg` to be on CPU here.\n    with ops.device(\"CPU:0\"):\n      arg = x + 10.\n\n    def true_fn():\n      with ops.device(\"CPU:0\"):\n        return arg + 1\n\n    r = control_flow_ops.cond(constant_op.constant(True), true_fn, lambda: 0.)\n\n    with session.Session() as sess:\n      run_metadata = config_pb2.RunMetadata()\n      options = config_pb2.RunOptions(output_partition_graphs=True)\n      sess.run(\n          r, feed_dict={x: -10.}, options=options, run_metadata=run_metadata)\n      self.assertEqual(len(run_metadata.partition_graphs), 2)\n      # Check that the Switch for `arg` gets placed on CPU.\n      self.assertEqual(\n          self._count_matching_switch_nodes_on_device(run_metadata, \"CPU\"), 1)\n      self.assertEqual(\n          self._count_matching_switch_nodes_on_device(run_metadata, \"GPU\"), 0)\n\n  @test_util.run_gpu_only\n  @test_util.run_deprecated_v1\n  def testCondSwitchColocatedWithInputWhenInputOnGPU(self):\n    x = array_ops.placeholder(dtypes.float32)\n\n    # `arg` is used in the cond then branch so a Switch node is created for it.\n    # We test that the Switch node gets placed on the same device as `arg`.\n    # Note: `arg` gets placed on GPU by default by the placer.\n    arg = x + 10.\n\n    def true_fn():\n      with ops.device(\"CPU:0\"):\n        return arg + 1\n\n    r = control_flow_ops.cond(constant_op.constant(True), true_fn, lambda: 0.)\n\n    with session.Session() as sess:\n      run_metadata = config_pb2.RunMetadata()\n      options = config_pb2.RunOptions(output_partition_graphs=True)\n      sess.run(\n          r, feed_dict={x: -10.}, options=options, run_metadata=run_metadata)\n      self.assertEqual(len(run_metadata.partition_graphs), 2)\n      # Check that the Switch for `arg` gets placed on GPU.\n      self.assertEqual(\n          self._count_matching_switch_nodes_on_device(run_metadata, \"CPU\"), 0)\n      self.assertEqual(\n          self._count_matching_switch_nodes_on_device(run_metadata, \"GPU\"), 1)\n\n  def testCondAccessTrueBranchTensorInFalseBranchRaises(self):\n\n    @def_function.function\n    def f():\n      c = constant_op.constant(1.)\n      inputs = {\"c\": c}\n\n      def true_fn(inputs):\n        inputs[\"c\"] = array_ops.identity(inputs[\"c\"], name=\"true_branch\")\n        return inputs[\"c\"]\n\n      def false_fn(inputs):\n        return array_ops.identity(inputs[\"c\"])\n\n      pred = constant_op.constant(True)\n      return control_flow_ops.cond(\n          pred, lambda: true_fn(inputs), lambda: false_fn(inputs))\n\n    # This was needed for backwards compatibility with TF2 Estimators which\n    # rely on variable names.\n    prefix = \"cond/\" if context.executing_eagerly() else \"\"\n\n    with self.assertRaisesRegex(\n        ValueError,\n        \"Tensor %strue_branch:0 in true_fn is accessed from false_fn.\" %\n        prefix):\n      f()\n\n  def testSwitchCaseAccessBranch1TensorInBranch4Raises(self):\n\n    @def_function.function\n    def f():\n      c = constant_op.constant(1.)\n      inputs = {\"c\": c}\n\n      def br1_fn(inputs):\n        inputs[\"c\"] = array_ops.identity(inputs[\"c\"], name=\"br1_identity\")\n        return inputs[\"c\"]\n\n      def br4_fn(inputs):\n        return array_ops.identity(inputs[\"c\"])\n\n      def other_fn():\n        return array_ops.identity(c)\n\n      return control_flow_ops.switch_case(\n          constant_op.constant(2),\n          [other_fn, lambda: br1_fn(inputs), other_fn, other_fn,\n           lambda: br4_fn(inputs)])\n\n    # This was needed for backwards compatibility with TF2 Estimators which\n    # rely on variable names.\n    prefix = \"switch_case/indexed_case/\" if context.executing_eagerly() else \"\"\n    with self.assertRaisesRegex(\n        ValueError, \"Tensor %sbr1_identity:0 in branch 1 is \"\n        \"accessed from branch 4.\" % prefix):\n      f()\n\n  def testCondListOutput(self):\n    with self.cached_session() as sess:\n      x = constant_op.constant(10)\n      y = constant_op.constant(200)\n      pred = math_ops.less(1, 2)\n      fn1 = lambda: [math_ops.add(x, y), math_ops.add(x, y)]\n      fn2 = lambda: [y, y]\n      r = control_flow_ops.cond(pred, fn1, fn2)\n      test_result = self.evaluate(r)\n      self.assertListEqual([210, 210], test_result)\n\n  def testTupleOutput(self):\n    with self.cached_session() as sess:\n      x = constant_op.constant(10)\n      y = constant_op.constant(200)\n      pred = math_ops.less(1, 2)\n      fn1 = lambda: (math_ops.add(x, y), math_ops.add(x, y))\n      fn2 = lambda: (y, y)\n      r = control_flow_ops.cond(pred, fn1, fn2)\n      test_result = self.evaluate(r)\n      self.assertTupleEqual((210, 210), test_result)\n\n  def testDictOutput(self):\n    with self.cached_session() as sess:\n      x = constant_op.constant(10)\n      y = constant_op.constant(200)\n      pred = math_ops.less(1, 2)\n      fn1 = lambda: {\"a\": math_ops.add(x, y), \"b\": math_ops.add(x, y)}\n      fn2 = lambda: {\"a\": y, \"b\": y}\n      r = control_flow_ops.cond(pred, fn1, fn2)\n      test_result = self.evaluate(r)\n      self.assertDictEqual({\"a\": 210, \"b\": 210}, test_result)\n\n  def testEmbeddedListOutput(self):\n    x = constant_op.constant(10)\n    y = constant_op.constant(200)\n    pred = math_ops.less(1, 2)\n    fn1 = lambda: [[math_ops.add(x, y), math_ops.add(x, y)]]\n    fn2 = lambda: [[y, y]]\n    # Pass strict=True flag as cond_v2 allows for tensors to be\n    # in nested output structures as singletons\n    r = control_flow_ops.cond(pred, fn1, fn2, strict=True)\n    test_result = self.evaluate(r)\n    self.assertListEqual([[210, 210]], test_result)\n\n  def testEmbeddedTupleOutput(self):\n    with self.cached_session() as sess:\n      x = constant_op.constant(10)\n      y = constant_op.constant(200)\n      pred = math_ops.less(1, 2)\n      fn1 = lambda: ((math_ops.add(x, y), math_ops.add(x, y)))\n      fn2 = lambda: ((y, y))\n      r = control_flow_ops.cond(pred, fn1, fn2)\n      test_result = self.evaluate(r)\n      self.assertTupleEqual(((210, 210)), test_result)\n\n  def testEmbeddedDictOutput(self):\n    with self.cached_session() as sess:\n      x = constant_op.constant(10)\n      y = constant_op.constant(200)\n      pred = math_ops.less(1, 2)\n      fn1 = lambda: {\"a\": {\"c\": math_ops.add(x, y)},\n                     \"b\": {\"d\": math_ops.add(x, y)}}\n      fn2 = lambda: {\"a\": {\"c\": y},\n                     \"b\": {\"d\": y}}\n      r = control_flow_ops.cond(pred, fn1, fn2)\n      test_result = self.evaluate(r)\n      self.assertDictEqual({\"a\": {\"c\": 210}, \"b\": {\"d\": 210}}, test_result)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testCheckNestedOutputStruct(self):\n    with self.cached_session() as sess:\n      x = constant_op.constant(10)\n      y = constant_op.constant(200)\n      pred = math_ops.less(1, 2)\n      fn1 = lambda: {\"a\": math_ops.add(x, y), \"b\": math_ops.add(x, y)}\n      fn2 = lambda: {\"c\": y, \"d\": y}\n      v1_msg = \"The two structures don't have the same nested structure\"\n      v2_msg = (\"true_fn and false_fn arguments to tf.cond must have the same \"\n                \"number, type, and overall structure of return values.\")\n      with self.assertRaisesRegex(\n          TypeError if control_flow_util.ENABLE_CONTROL_FLOW_V2 else ValueError,\n          v2_msg if control_flow_util.ENABLE_CONTROL_FLOW_V2 else v1_msg):\n        control_flow_ops.cond(pred, fn1, fn2)\n\n  @test_util.run_deprecated_v1\n  def testCondRef(self):\n\n    with self.cached_session():\n      x = gen_state_ops.variable(\n          shape=[1],\n          dtype=dtypes.float32,\n          name=\"x\",\n          container=\"\",\n          shared_name=\"\")\n      true_fn = lambda: x\n      false_fn = lambda: constant_op.constant([2.0])\n      r = control_flow_ops.cond(constant_op.constant(False), true_fn, false_fn)\n      self.assertAllEqual([2.0], self.evaluate(r))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testCondWithControl(self):\n    with self.cached_session() as sess:\n      control_holder = array_ops.placeholder(dtypes.float32, shape=())\n      a = constant_op.constant(3)\n\n      def true_branch():\n        with ops.control_dependencies([control_holder]):\n          _ = a + 1\n        return a + 2\n\n      r = control_flow_ops.cond(\n          constant_op.constant(True), true_branch,\n          lambda: constant_op.constant(1))\n      result = sess.run(r, feed_dict={control_holder: 5.})\n      self.assertEqual(5, result)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testUninitializedRefIdentity(self):\n    with self.cached_session() as sess:\n      v = gen_state_ops.variable(\n          shape=[1],\n          dtype=dtypes.float32,\n          name=\"v\",\n          container=\"\",\n          shared_name=\"\")\n      inited = state_ops.is_variable_initialized(v)\n      v_f, v_t = control_flow_ops.ref_switch(v, inited)\n      # Both v_f and v_t are uninitialized references. However, an actual use\n      # of the reference in the 'true' branch in the 'tf.identity' op will\n      # not 'fire' when v is uninitialized, so this is a valid construction.\n      # This test tests that ref_identity allows uninitialized ref as input\n      # so that this construction is allowed.\n      v_f_op = gen_array_ops.ref_identity(v_f)\n      v_t_op = gen_array_ops.ref_identity(v_t)\n      with ops.control_dependencies([v_f_op]):\n        assign_v = state_ops.assign(v, [1.0])\n      with ops.control_dependencies([v_t_op]):\n        orig_v = array_ops.identity(v)\n      merged_op = control_flow_ops.merge([assign_v, orig_v])\n      self.assertAllEqual([1.0], self.evaluate(merged_op.output))\n\n  def testCondSwitchIdentity(self):\n    # Make sure the recv identity is not removed by optimization.\n    with session.Session(config=opt_cfg()) as sess:\n      pred = constant_op.constant(True)\n\n      def fn1():\n        return control_flow_ops.no_op()\n\n      def fn2():\n        return control_flow_ops.Assert(False, [\"Wrong branch!!!\"])\n\n      r = control_flow_ops.cond(pred, fn1, fn2)\n      self.evaluate(r)\n\n  def testCondRecvIdentity(self):\n    # Make sure the switch identity is not removed by optimization.\n    with session.Session(config=opt_cfg()) as sess:\n      with ops.device(test.gpu_device_name()):\n        pred = constant_op.constant(True)\n\n      def fn1():\n        return control_flow_ops.no_op()\n\n      def fn2():\n        with ops.device(\"/cpu:0\"):\n          return control_flow_ops.Assert(False, [\"Wrong branch!!!\"])\n\n      r = control_flow_ops.cond(pred, fn1, fn2)\n      self.evaluate(r)\n\n  @test_util.run_deprecated_v1\n  @test_util.enable_control_flow_v2\n  def testDisableLoweringSwitchMerge(self):\n    if test_util.is_gpu_available():\n      self.skipTest(\n          \"Single threaded executor doesn't support partitioned graphs.  \"\n          \"Skipping GPU test.\")\n    # Make pred feedable to ensure we don't constant-fold it out.\n    run_opts = config_pb2.RunOptions(\n        trace_level=config_pb2.RunOptions.FULL_TRACE)\n    run_metadata_no_lowering = config_pb2.RunMetadata()\n    run_metadata_with_lowering = config_pb2.RunMetadata()\n\n    config = opt_cfg(do_constant_folding=False)\n\n    pred = array_ops.placeholder_with_default(\n        constant_op.constant(True), shape=())\n    r = control_flow_ops.cond(pred, lambda: True, lambda: False)\n\n    with session.Session(config=config) as sess:\n      r_value = sess.run(\n          r, options=run_opts, run_metadata=run_metadata_with_lowering)\n      self.assertEqual(r_value, True)\n\n    # Use the single threaded executor, which disables control flow lowering.\n    config.experimental.executor_type = \"SINGLE_THREADED_EXECUTOR\"\n    with session.Session(config=config) as sess:\n      r_value = sess.run(\n          r, options=run_opts, run_metadata=run_metadata_no_lowering)\n      self.assertEqual(r_value, True)\n\n    self.assertTrue(  # pylint: disable=g-complex-comprehension\n        any(\"switch\" in ns.node_name\n            for dev_stat in run_metadata_with_lowering.step_stats.dev_stats\n            for ns in dev_stat.node_stats))\n\n    self.assertTrue(  # pylint: disable=g-complex-comprehension\n        all(\"switch\" not in ns.node_name\n            for dev_stat in run_metadata_no_lowering.step_stats.dev_stats\n            for ns in dev_stat.node_stats))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testCondGrad_1(self):\n    with self.cached_session():\n      x = constant_op.constant(10.0, name=\"x\")\n      pred = math_ops.less(1, 2)\n      fn1 = lambda: array_ops.identity(x)\n      fn2 = lambda: array_ops.identity(x)\n      r = control_flow_ops.cond(pred, fn1, fn2)\n\n      grad = gradients_impl.gradients(r, [x])[0]\n      self.assertAllEqual(1.0, self.evaluate(grad))\n\n  @test_util.run_deprecated_v1\n  @test_util.enable_control_flow_v2\n  def testCondComputeGradAfterSessRunFails(self):\n    with self.cached_session():\n      x = constant_op.constant(10.0, name=\"x\")\n      pred = math_ops.less(1, 2)\n\n      def true_fn():\n        a = x * x\n        return a * a\n\n      def false_fn():\n        return x * x\n\n      r = control_flow_ops.cond(pred, true_fn, false_fn)\n\n      self.assertAllEqual(r, 10000.)\n      grad = gradients_impl.gradients(r, [x])[0]\n      with self.assertRaisesRegex(\n          errors_impl.InvalidArgumentError,\n          r\"Connecting to invalid output 1 of source node cond which has 1 \"\n          r\"outputs. Try using \"\n          \"tf.compat.v1.experimental.output_all_intermediates\\(True\\).\"):\n        self.evaluate(grad)\n\n  @test_util.run_deprecated_v1\n  @test_util.enable_output_all_intermediates\n  def testCondComputeGradAfterSessRun(self):\n    with self.cached_session():\n      x = constant_op.constant(10.0, name=\"x\")\n      pred = math_ops.less(1, 2)\n\n      def true_fn():\n        a = x * x\n        return a * a\n\n      def false_fn():\n        return x * x\n\n      r = control_flow_ops.cond(pred, true_fn, false_fn)\n\n      self.assertAllEqual(r, 10000.)\n      grad = gradients_impl.gradients(r, [x])[0]\n      self.assertAllEqual(grad, 4000.)\n\n  @test_util.run_deprecated_v1\n  @test_util.enable_output_all_intermediates\n  def testNestedCondComputeGradAfterSessRun(self):\n    with self.cached_session():\n      x = constant_op.constant(10.0, name=\"x\")\n      pred = math_ops.less(1, 2)\n\n      def true_fn():\n\n        def inner_true_fn():\n          a = x * x\n          return a * a\n\n        def inner_false_fn():\n          return x * x\n\n        return control_flow_ops.cond(\n            constant_op.constant(True), inner_true_fn, inner_false_fn)\n\n      def false_fn():\n        return x * x\n\n      r = control_flow_ops.cond(pred, true_fn, false_fn)\n\n      self.assertAllEqual(r, 10000.)\n      grad = gradients_impl.gradients(r, [x])[0]\n      self.assertAllEqual(grad, 4000.)\n\n  @test_util.run_deprecated_v1\n  def testCondGrad_2(self):\n    with self.cached_session():\n      c = array_ops.placeholder(dtypes.int32, shape=[])\n      x = constant_op.constant(10.0)\n      pred = math_ops.less(c, 2)\n      fn1 = lambda: math_ops.multiply(x, 42.0)\n      fn2 = lambda: math_ops.multiply(x, 3.0)\n      r = control_flow_ops.cond(pred, fn1, fn2)\n\n      grad = gradients_impl.gradients(r, [x])[0]\n      self.assertAllEqual(42.0, grad.eval(feed_dict={c: 1}))\n      self.assertAllEqual(3.0, grad.eval(feed_dict={c: 3}))\n\n  @test_util.disable_control_flow_v2(\n      \"b/110550782 (gradient w.r.t external variable)\")\n  @test_util.run_deprecated_v1\n  def testCondGrad_3(self):\n    with self.cached_session():\n      c = array_ops.placeholder(dtypes.int32, shape=[])\n      ox = constant_op.constant(10.0)\n      pred = math_ops.less(c, 2)\n\n      def fn1(x):\n        m = x * x\n        return gradients_impl.gradients(m, [ox])[0]\n\n      fn2 = lambda: math_ops.multiply(ox, 3.0)\n      y = math_ops.multiply(7.0, ox)\n      r = control_flow_ops.cond(pred, lambda: fn1(y), fn2)\n\n      self.assertAllEqual(980.0, r.eval(feed_dict={c: 1}))\n      self.assertAllEqual(30.0, r.eval(feed_dict={c: 3}))\n\n  @test_util.run_deprecated_v1\n  def testCondGradMultiDevice(self):\n    config = config_pb2.ConfigProto(device_count={\"CPU\": 2},\n                                    allow_soft_placement=True)\n    with self.cached_session(use_gpu=True, config=config) as sess:\n      pred = array_ops.placeholder(dtypes.bool, [])\n      x = array_ops.placeholder(dtypes.float32)\n      y = array_ops.placeholder(dtypes.float32)\n\n      with ops.device(\"/cpu:0\"):\n        z = control_flow_ops.cond(pred, lambda: x * y * 2.0, lambda: 2.0)\n\n      with ops.device(\"/cpu:1\"):\n        grad = gradients_impl.gradients(z, x)[0]\n\n      with ops.device(\"/cpu:0\"):\n        grad_grad = gradients_impl.gradients(grad, x)[0]\n\n      self.assertEqual(sess.run(grad, {pred: True, x: 1.0, y: 2.0}), 4.0)\n      self.assertEqual(sess.run(grad, {pred: False, x: 1.0, y: 2.0}), 0.0)\n\n      # v1 control flow gets None second derivative for some reason.\n      if not control_flow_util.ENABLE_CONTROL_FLOW_V2:\n        self.assertIsNone(grad_grad)\n        return\n\n      self.assertEqual(sess.run(grad_grad, {pred: True, x: 1.0, y: 2.0}), 0.0)\n      self.assertEqual(sess.run(grad_grad, {pred: False, x: 1.0, y: 2.0}), 0.0)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testNestedCond_Simple(self):\n    with self.cached_session():\n      x = constant_op.constant(0., name=\"X\")\n      y = control_flow_ops.cond(\n          constant_op.constant(True), lambda: x,\n          lambda: control_flow_ops.cond(x < 1., lambda: x, lambda: x))\n      result = gradients_impl.gradients(y, x)[0]\n      self.assertEqual(1.0, self.evaluate(result))\n\n      z = control_flow_ops.cond(\n          constant_op.constant(False), lambda: x,\n          lambda: control_flow_ops.cond(x < 1., lambda: x, lambda: x))\n      result = gradients_impl.gradients(z, x)[0]\n      self.assertEqual(1.0, self.evaluate(result))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testCondGrad_Gather(self):\n    with self.cached_session() as sess:\n      v1 = variables.Variable([1.0, 42.0])\n      c = array_ops.placeholder(dtypes.int32, shape=[])\n      pred = math_ops.less(c, 2)\n      fn1 = lambda: array_ops.identity(v1)\n      fn2 = lambda: array_ops.gather(v1, [1, 1])\n      r = control_flow_ops.cond(pred, fn1, fn2)\n      # The following `grad` is a Tensor since it is the aggregation of an\n      # IndexedSlice and a Tensor. It is an `IndexedSlices` with control flow\n      # v2.\n      grad = gradients_impl.gradients(r, [v1])[0]\n      self.evaluate(variables.global_variables_initializer())\n\n      if control_flow_util.ENABLE_CONTROL_FLOW_V2:\n        self.assertIsInstance(grad, ops.IndexedSlices)\n\n      grad_value = sess.run(grad, feed_dict={c: 1})\n      self.assertAllEqual(gradient_checker_v2._to_numpy(grad_value), [1.0, 1.0])\n\n      grad_value = sess.run(grad, feed_dict={c: 3})\n      self.assertAllEqual(gradient_checker_v2._to_numpy(grad_value), [0.0, 2.0])\n\n  @test_util.run_deprecated_v1\n  def testCondGrad_ResourceVarSparseRead(self):\n    # NOTE(skyewm): this test is interesting because the\n    # ResourceVariable.sparse_read gradient function returns IndexedSlices.\n    var = resource_variable_ops.ResourceVariable(\n        np.ones((4, 2), dtype=np.float32))\n    x = constant_op.constant(1.0)\n    r = control_flow_ops.cond(\n        constant_op.constant(True),\n        lambda: x * math_ops.reduce_sum(var.sparse_read([1, 2])),\n        lambda: constant_op.constant(np.zeros((2, 3)),\n                                     dtype=dtypes.float32))\n    grad = gradients_impl.gradients(r, var)[0]\n\n    self.evaluate(variables.global_variables_initializer())\n    grad_val = self.evaluate(grad)\n    self.assertIsInstance(grad_val, ops.IndexedSlicesValue)\n    self.assertAllEqual(gradient_checker_v2._to_numpy(grad_val), [[0., 0.],\n                                                                  [1., 1.],\n                                                                  [1., 1.],\n                                                                  [0., 0.]])\n\n  def testCondGrad_MultiGather(self):\n    # NOTE(skyewm): this test is interesting because the array_ops.gather and\n    # ResourceVariable.sparse_read gradient functions returns IndexedSlices.\n    var = resource_variable_ops.ResourceVariable(\n        np.ones((4, 2), dtype=np.float32))\n    x1 = constant_op.constant(np.ones((3, 3), dtype=np.float32))\n    x2 = constant_op.constant(2.0)\n\n    def true_fn():\n      y1 = var.sparse_read([1, 2])\n      y2 = array_ops.gather(x1, [2]) * x2\n      y3 = x2 * [1., 1., 1.]\n      return y1, y2, y3\n\n    def false_fn():\n      y1 = np.zeros((2, 2), dtype=np.float32)\n      y2 = array_ops.gather(x1, [2]) * x2\n      y3 = array_ops.gather(x1, [2])\n      return y1, y2, y3\n\n    @def_function.function\n    def foo():\n      r = control_flow_ops.cond(constant_op.constant(True), true_fn, false_fn)\n      return gradients_impl.gradients(r, [var, x1, x2])\n\n    grad = foo()\n    self.evaluate(variables.global_variables_initializer())\n    var_grad, x1_grad, x2_grad = self.evaluate(grad)\n    self.assertIsInstance(var_grad, ops.IndexedSlicesValue)\n    self.assertAllEqual(gradient_checker_v2._to_numpy(var_grad), [[0., 0.],\n                                                                  [1., 1.],\n                                                                  [1., 1.],\n                                                                  [0., 0]])\n    self.assertIsInstance(x1_grad, ops.IndexedSlicesValue)\n    self.assertAllEqual(gradient_checker_v2._to_numpy(x1_grad), [[0., 0., 0.],\n                                                                 [0., 0., 0.],\n                                                                 [2., 2., 2.]])\n    self.assertIsInstance(x1_grad, ops.IndexedSlicesValue)\n    self.assertEqual(gradient_checker_v2._to_numpy(x2_grad), 6.)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testCondPredicateTensor(self):\n    \"\"\"Regression test for lowering predicate from non-first output of an op.\"\"\"\n\n    @eager_function.defun\n    def foo():\n      return constant_op.constant(\"foo\"), constant_op.constant(True)\n\n    r = control_flow_ops.cond(foo()[1], lambda: 1.0, lambda: 2.0)\n    self.assertEqual(self.evaluate(r), 1.0)\n\n  @test_util.run_v1_only(\"Tests Session.run() pruning logic.\")\n  def testCondFeedConstantPredicate(self):\n    with self.cached_session() as sess:\n      value = constant_op.constant(37.0)\n      predicate = constant_op.constant(True)\n      cond_output = control_flow_ops.cond(\n          predicate, lambda: constant_op.constant(0.0), lambda: value)\n      result = array_ops.identity(cond_output)\n      self.assertEqual(37.0, sess.run(result, feed_dict={predicate: False}))\n      self.assertEqual(0.0, sess.run(result, feed_dict={predicate: True}))\n      self.assertEqual(0.0, sess.run(result))\n\n  @test_util.run_v1_only(\"Tests Session.run() pruning logic.\")\n  def testCondFeedPlaceholderWithDefaultPredicate(self):\n    with self.cached_session() as sess:\n      value = constant_op.constant(37.0)\n      predicate = array_ops.placeholder_with_default(\n          constant_op.constant(True), [])\n      cond_output = control_flow_ops.cond(\n          predicate, lambda: constant_op.constant(0.0), lambda: value)\n      result = array_ops.identity(cond_output)\n      self.assertAllEqual(37.0, sess.run(result, feed_dict={predicate: False}))\n      self.assertAllEqual(0.0, sess.run(result, feed_dict={predicate: True}))\n      self.assertAllEqual(0.0, sess.run(result))\n\n  @test_util.run_in_graph_and_eager_modes\n  def testCondAutoControlDeps(self):\n    if test_util.is_gpu_available():\n      self.skipTest(\"b/128676188 causes OOM on opensource gpu tests\")\n\n    print_prefix = \"testCondAutoControlDeps: \"\n\n    def branch_fn():\n      enqueue_print_op(\"A\")\n      enqueue_print_op(\"B\")\n      with ops.control_dependencies([enqueue_print_op(\"C\")]):\n        return constant_op.constant(10)\n\n    def build_cond():\n      return control_flow_ops.cond(\n          constant_op.constant(True), branch_fn, lambda: 0)\n\n    def build_nested_cond():\n      return control_flow_ops.cond(\n          constant_op.constant(True), build_cond, lambda: 0)\n\n    # In v1 graph mode, pruning should make only \"C\" print.\n    if not context.executing_eagerly():\n      with self.cached_session():\n        with self.captureWritesToStream(sys.stderr) as printed:\n          self.assertEqual(self.evaluate(build_cond()), 10)\n        self.assertEqual([\"C\"], filter_test_messages(printed.contents()))\n\n        with self.captureWritesToStream(sys.stderr) as printed:\n          self.assertEqual(self.evaluate(build_nested_cond()), 10)\n        self.assertEqual([\"C\"], filter_test_messages(printed.contents()))\n\n    # In defuns, all prints should execute in program order.\n    # This doesn't work with legacy control flow.\n    if control_flow_util.ENABLE_CONTROL_FLOW_V2:\n\n      @eager_function.defun\n      def cond():\n        return build_cond()\n\n      with self.captureWritesToStream(sys.stderr) as printed:\n        self.assertEqual(self.evaluate(cond()), 10)\n      self.assertEqual([\"A\", \"B\", \"C\"],\n                       filter_test_messages(printed.contents()))\n\n      @eager_function.defun\n      def nested_cond():\n        return build_nested_cond()\n\n      with self.captureWritesToStream(sys.stderr) as printed:\n        self.assertEqual(self.evaluate(nested_cond()), 10)\n      self.assertEqual([\"A\", \"B\", \"C\"],\n                       filter_test_messages(printed.contents()))\n\n    # wrap_function should prune.\n    def pruned_cond():\n      return build_cond()\n    pruned_cond = wrap_function.wrap_function(pruned_cond, [])\n\n    with self.captureWritesToStream(sys.stderr) as printed:\n      self.assertEqual(self.evaluate(pruned_cond()), 10)\n    self.assertEqual([\"C\"], filter_test_messages(printed.contents()))\n\n    def pruned_nested_cond():\n      return build_nested_cond()\n    pruned_nested_cond = wrap_function.wrap_function(pruned_nested_cond, [])\n\n    with self.captureWritesToStream(sys.stderr) as printed:\n      self.assertEqual(self.evaluate(pruned_nested_cond()), 10)\n    self.assertEqual([\"C\"], filter_test_messages(printed.contents()))\n\n\n  @test_util.run_in_graph_and_eager_modes\n  def testWhileAutoControlDeps(self):\n    # Legacy while_loop fails this test because it produces deprecation notices\n    # in stderr.\n    if not control_flow_util.ENABLE_CONTROL_FLOW_V2: return\n\n    def cond(i, unused_x):\n      enqueue_print_op(\"A\")\n      return i < 2\n\n    def body(i, x):\n      enqueue_print_op(\"B\")\n      with ops.control_dependencies([enqueue_print_op(\"C\")]):\n        x = array_ops.identity(x)\n      with ops.control_dependencies([enqueue_print_op(\"D\")]):\n        return i + 1, x\n\n    def build_while():\n      return control_flow_ops.while_loop(\n          cond, body, [constant_op.constant(0), constant_op.constant(0)])\n\n    def build_nested_while():\n      return control_flow_ops.cond(\n          constant_op.constant(True), build_while, lambda: [0, 0])\n\n    # In v1 graph mode, pruning should make only \"D\" print.\n    if not context.executing_eagerly():\n      with self.cached_session():\n        with self.captureWritesToStream(sys.stderr) as printed:\n          self.assertEqual(self.evaluate(build_while()[0]), 2)\n        self.assertEqual([\"D\", \"D\"], filter_test_messages(printed.contents()))\n\n        with self.captureWritesToStream(sys.stderr) as printed:\n          self.assertEqual(self.evaluate(build_nested_while()[0]), 2)\n        self.assertEqual([\"D\", \"D\"], filter_test_messages(printed.contents()))\n\n    # In defuns, all prints should execute in program order.\n    @eager_function.defun\n    def while_loop():\n      return build_while()[0]\n\n    with self.captureWritesToStream(sys.stderr) as printed:\n      self.assertEqual(self.evaluate(while_loop()), 2)\n    self.assertEqual([\"A\", \"B\", \"C\", \"D\", \"A\", \"B\", \"C\", \"D\", \"A\"],\n                     filter_test_messages(printed.contents()))\n\n    @eager_function.defun\n    def nested_while_loop():\n      return build_nested_while()[0]\n\n    with self.captureWritesToStream(sys.stderr) as printed:\n      self.assertEqual(self.evaluate(nested_while_loop()), 2)\n    self.assertEqual([\"A\", \"B\", \"C\", \"D\", \"A\", \"B\", \"C\", \"D\", \"A\"],\n                     filter_test_messages(printed.contents()))\n\n    # wrap_function should prune.\n    def pruned_while():\n      return build_while()[0]\n    pruned_while = wrap_function.wrap_function(pruned_while, [])\n\n    with self.captureWritesToStream(sys.stderr) as printed:\n      self.assertEqual(self.evaluate(pruned_while()), 2)\n    self.assertEqual([\"D\", \"D\"], filter_test_messages(printed.contents()))\n\n    def pruned_nested_while():\n      return build_nested_while()[0]\n    pruned_nested_while = wrap_function.wrap_function(pruned_nested_while, [])\n\n    with self.captureWritesToStream(sys.stderr) as printed:\n      self.assertEqual(self.evaluate(pruned_nested_while()), 2)\n    self.assertEqual([\"D\", \"D\"], filter_test_messages(printed.contents()))\n\n  # Microbenchmark: 256,000 iterations/s.\n  def testWhile_1(self):\n    with self.cached_session():\n      n = constant_op.constant(0)\n      c = lambda x: math_ops.less(x, 10000)\n      b = lambda x: math_ops.add(x, 1)\n      r = control_flow_ops.while_loop(c, b, [n], parallel_iterations=20)\n      self.assertEqual(10000, self.evaluate(r))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileExternalControlDependencies(self):\n    with self.cached_session():\n      v = variables.Variable(0.0)\n      self.evaluate(v.initializer)\n      increment = v.assign_add(1.0).read_value()\n\n      def body_fn(i):\n        with ops.control_dependencies([increment]):\n          return i + 1\n\n      result = control_flow_ops.while_loop(cond=lambda i: i < 2,\n                                           body=body_fn, loop_vars=[1])\n      self.assertAllEqual(result, 2)\n      self.assertAllEqual(v.read_value(), 1.0)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileExternalControlDependenciesNoInput(self):\n    with self.cached_session():\n      v = variables.Variable(0.0)\n      self.evaluate(v.initializer)\n      # TODO(apassos): figure out why the reading is necessary here.\n      increment = v.assign_add(1.0).read_value()\n\n      def body_fn(unused_i):\n        with ops.control_dependencies([increment]):\n          return constant_op.constant(5, name=\"five\")\n\n      result = control_flow_ops.while_loop(cond=lambda i: i < 5,\n                                           body=body_fn, loop_vars=[0])\n      self.evaluate(result)\n      self.assertAllEqual(self.evaluate(v), 1.0)\n\n  @test_util.disable_control_flow_v2(\"b/113324949 (RefVariable)\")\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileWithRefs_1(self):\n    with self.cached_session() as sess:\n      x = variables.VariableV1(0)._ref()  # pylint: disable=protected-access\n      i = constant_op.constant(0)\n      c = lambda i, x: math_ops.less(i, 100)\n\n      self.assertEqual(x.dtype, dtypes.int32_ref)\n\n      def b(i, x):\n        self.assertEqual(x.dtype, dtypes.int32_ref)\n        return (i + 1, gen_array_ops.ref_identity(x))\n\n      r = control_flow_ops.while_loop(c, b, [i, x], parallel_iterations=5)\n\n      self.evaluate(variables.global_variables_initializer())\n\n      self.assertEqual(r[0].dtype, dtypes.int32)\n      self.assertEqual(r[1].dtype, dtypes.int32_ref)\n\n      value_i, value_x = self.evaluate(r)\n\n    self.assertEqual(100, value_i)\n    self.assertEqual(0, value_x)\n\n  def testWhile_2(self):\n    with self.cached_session():\n      s = constant_op.constant(0)\n      r = isum(s)\n      self.assertAllEqual(45, self.evaluate(r))\n\n  def testWhileWithMaximumIterations(self):\n    with self.cached_session():\n      s = constant_op.constant([1, 2, 3, 4, 5])\n      r = isum(s, maximum_iterations=3)\n      self.assertAllEqual([1 + 3, 2 + 3, 3 + 3, 4 + 3, 5 + 3], self.evaluate(r))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileWithMaximumIterationsAndSingleArgument(self):\n    with self.cached_session():\n      r = control_flow_ops.while_loop(\n          lambda i: i < 3, lambda i: i + 1, [0], maximum_iterations=1)\n      self.assertEqual(1, self.evaluate(r))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testXLAGradInLoop(self):\n    # We have an optimization that moves certain reduction ops, this test makes\n    # sure we don't do that for XLA ops.\n\n    # Use dynamic inputs, which triggers the creation of \"BroadcastGradientArgs\"\n    # and \"Shape\" op.\n    input1 = array_ops.placeholder(dtype=dtypes.float32, shape=[None, None])\n    input2 = array_ops.placeholder(dtype=dtypes.float32, shape=[None, None])\n    def cond(i1, i2):\n      return False\n\n    def body(i1, i2):\n      return math_ops.add(i1, i2), math_ops.add(i1, i2)\n\n    xla_context = control_flow_ops.XLAControlFlowContext()\n    xla_context.Enter()\n\n    out1, _ = control_flow_ops.while_loop(\n        cond, body, (input1, input2), maximum_iterations=2)\n    g = gradients_impl.gradients(out1, [input1])\n\n    for op in out1.graph.get_operations():\n      # Test that the \"Shape\" is directly passed to BroadcastGradientArgs\n      # instead of being pushed to the stack.\n      if op.type == \"BroadcastGradientArgs\":\n        self.assertEqual(op.inputs[0].op.type, \"Shape\")\n        self.assertEqual(op.inputs[1].op.type, \"Shape\")\n    xla_context.Exit()\n\n\n  @test_util.disable_control_flow_v2(\"b/115776323 (max_iters)\")\n  @test_util.run_v1_only(\"b/120545219\")\n  def testSingleNestedMaximumIterationsWhileLoopGradientInXLAContext(self):\n    v = constant_op.constant(1.0)\n\n    def training_loop_with_gradient(i):\n      out = control_flow_ops.while_loop(\n          lambda i_, _: i_ < 3,\n          lambda i_, j: [i_ + 1, j * v], [0, 1.0],\n          maximum_iterations=i)\n      g = gradients_impl.gradients(out, v)\n      with ops.control_dependencies(g):\n        return i + 1\n\n    xla_context = control_flow_ops.XLAControlFlowContext()\n    xla_context.Enter()\n    # Create training loop, ensure we can call gradient() of\n    # while_loop inside the training loop.\n    loop = control_flow_ops.while_loop(lambda i: i < 3,\n                                       training_loop_with_gradient, [0])\n    xla_context.Exit()\n\n    loop_execute = array_ops.identity(loop)  # Because loop is not fetchable.\n\n    # Should execute without issue.\n    self.assertEqual(3, self.evaluate(loop_execute))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testInvalidMaximumIterationsWhileLoopGradientInXLAContext(self):\n    if control_flow_util.ENABLE_CONTROL_FLOW_V2:\n      self.skipTest(\"WhileV2 does lazy evaluation of maximum_iterations\")\n    v = constant_op.constant(1.0)\n\n    def inner_body(i, x):\n      out = control_flow_ops.while_loop(\n          lambda i, _: i < 3,\n          lambda i, j: [i + 1, j * v], [0, x],\n          maximum_iterations=i)\n      return out\n\n    def create_while_loop(maximum_iterations=None):\n      return control_flow_ops.while_loop(\n          lambda i, _: i < 3,\n          inner_body, [0, 1.0],\n          maximum_iterations=maximum_iterations)\n\n    loop_no_xla = create_while_loop(maximum_iterations=5)\n    # maximum_iterations is fine outside of an XLA scope\n    gs = gradients_impl.gradients(loop_no_xla, v)\n    self.evaluate(gs)  # This should execute without error.\n\n    xla_context = control_flow_ops.XLAControlFlowContext()\n    xla_context.Enter()\n    loop_no_maxiter = create_while_loop()\n    loop_with_maxiter = create_while_loop(maximum_iterations=2)\n    xla_context.Exit()\n\n    with self.assertRaisesRegex(\n        ValueError,\n        r\"Cannot create a gradient accumulator for tensor '.+' inside \"\n        r\"XLA while_loop because maximum_iterations was not passed to \"\n        r\"the tf.while_loop call \\('.+'\\).\"):\n      _ = gradients_impl.gradients(loop_no_maxiter, v)\n\n    with self.assertRaisesRegex(\n        ValueError,\n        r\"Cannot create a gradient accumulator for tensor '.+' inside XLA \"\n        r\"while_loop. maximum_iterations tensor '.+' for while_loop context \"\n        r\"'.+' must be statically known \\(e.g. a constant value or known \"\n        r\"shape dimension\\), or be defined at or outside the while loop \"\n        r\"context '.*' \\(currently defined in '.*'\\)\"):\n      _ = gradients_impl.gradients(loop_with_maxiter, v)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testInvalidMaximumIterationsFromSiblingContextWhileLoopInXLAContext(self):\n    v = constant_op.constant(1.0)\n\n    def create_while_loop():\n      max_iter_holder = []\n\n      def create_mi():\n        max_iter_holder.append(array_ops.placeholder(dtypes.int32, shape=()))\n        return 1.0\n\n      _ = control_flow_ops.cond(\n          constant_op.constant(True), create_mi, create_mi)\n\n      return control_flow_ops.while_loop(\n          lambda i, _: i < 3,\n          lambda i, x: (i + 1, v * x), (0, 1.0),\n          maximum_iterations=max_iter_holder[0])\n\n    if control_flow_util.ENABLE_CONTROL_FLOW_V2:\n      xla_context = control_flow_ops.XLAControlFlowContext()\n      xla_context.Enter()\n      with self.assertRaisesRegex(ValueError, r\"must be from the same graph.*\"):\n        loop = create_while_loop()\n      xla_context.Exit()\n    else:\n      xla_context = control_flow_ops.XLAControlFlowContext()\n      xla_context.Enter()\n      loop = create_while_loop()\n      xla_context.Exit()\n      with self.assertRaisesRegex(\n          ValueError,\n          r\"Cannot create a gradient accumulator for tensor '.+' inside XLA \"\n          r\"while_loop. maximum_iterations tensor '.*Placeholder:0' for \"\n          r\"while_loop context '.+' must be statically known \\(e.g. a constant \"\n          r\"value or known shape dimension\\), or be defined at or outside the \"\n          r\"while loop context '' \\(currently defined in 'cond/.+'\\)\"):\n        _ = gradients_impl.gradients(loop, v)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testNestedWhileLoopWithMaxItersFromOuterContextInXLAContext(self):\n    if test_util.is_gpu_available():\n      self.skipTest(\"b/128646372, b/128645947 fails in opensource build\")\n\n    v = constant_op.constant(1.0)\n\n    p = array_ops.placeholder(dtype=dtypes.int32)\n\n    def mid_body_builder(iterations):\n\n      def mid_body(i, x):\n        r = control_flow_ops.while_loop(\n            lambda *_: True,\n            lambda i, x: (i + 1, v * x), (0, x),\n            maximum_iterations=iterations,\n            name=\"inner\")\n        return (i + 1, gradients_impl.gradients(x + r[1], v)[0])\n\n      return mid_body\n\n    def outer_body(i, x):\n      iterations = array_ops.size(p, name=\"iterations\")\n      return (i + 1, x + control_flow_ops.while_loop(\n          lambda *_: True,\n          mid_body_builder(iterations), (0, x),\n          maximum_iterations=iterations,\n          name=\"mid\")[1])\n\n    def create_while_loop():\n      with ops.device(\"/cpu:0\"):\n        r = control_flow_ops.while_loop(\n            lambda *_: True,\n            outer_body, (0, 1.0),\n            maximum_iterations=5,\n            name=\"outer\")\n        return array_ops.identity(r[1])\n\n    xla_context = control_flow_ops.XLAControlFlowContext()\n    xla_context.Enter()\n    final_with_xla_context = create_while_loop()\n    xla_context.Exit()\n\n    final_without_xla_context = create_while_loop()\n\n    with self.session(use_gpu=False) as sess:\n      opts = config_pb2.RunOptions(trace_level=config_pb2.RunOptions.FULL_TRACE)\n      run_metadata_without_xla_context = config_pb2.RunMetadata()\n      run_metadata = config_pb2.RunMetadata()\n\n      final_value_without_xla_context = sess.run(\n          final_without_xla_context,\n          feed_dict={p: [0, 0, 0]},\n          options=opts,\n          run_metadata=run_metadata_without_xla_context)\n\n      final_value_with_xla_context = sess.run(\n          final_with_xla_context,\n          feed_dict={p: [0, 0, 0]},\n          options=opts,\n          run_metadata=run_metadata)\n\n      if control_flow_util.ENABLE_CONTROL_FLOW_V2:\n        # With while_v2 on xla, run_metadata only contains the unlowered While\n        # op so node_stats does not have statistics for the pushes. So as a\n        # loose check we check the pushes in the lowered version.\n        for dev in run_metadata_without_xla_context.step_stats.dev_stats:\n          if \"/device:CPU\" in dev.device:\n            node_stats = dev.node_stats\n        stack_push_count = len([\n            x for x in node_stats\n            if re.match(r\".*TensorListPushBack_?\\d*\", x.node_name)\n        ])\n      else:\n        for dev in run_metadata.step_stats.dev_stats:\n          if \"/device:CPU\" in dev.device:\n            node_stats = dev.node_stats\n        stack_push_op = \"StackPushV2\"\n        stack_push_count = len(\n            [x for x in node_stats if x.node_name.endswith(\"StackPushV2\")])\n      # Pushes to the stack = product of maximum_iterations values;\n      # the last two \"3\"s comes from size(p), when p == [0, 0, 0].\n      self.assertEqual(stack_push_count, 5 * 3 * 3, str(node_stats))\n\n      self.assertAllClose(final_value_with_xla_context,\n                          final_value_without_xla_context)\n\n  # Have more than 10 parallel iterations and hence exercise k-bound\n  # most of the time.\n  @test_util.run_deprecated_v1\n  def testWhile_3(self):\n    with self.cached_session():\n\n      def compute(i, m, c, o):\n        m, c = [math_ops.add(m, 1), math_ops.add(c, 1)]\n        o = math_ops.add(o, m)\n        o = math_ops.add(o, c)\n        i = math_ops.add(i, 1)\n        return [i, m, c, o]\n\n      i = ops.convert_to_tensor(0)\n      m = ops.convert_to_tensor(0)\n      c = ops.convert_to_tensor(0)\n      o = ops.convert_to_tensor(0)\n      d = ops.convert_to_tensor(100)\n      r = control_flow_ops.while_loop(lambda i, m, c, o: math_ops.less(i, d),\n                                      compute, [i, m, c, o])\n      result = r[3]\n    self.assertAllEqual(10100, result)\n\n  @test_util.run_deprecated_v1\n  def testWhile_4(self):\n    with self.cached_session():\n\n      def compute(i, m, c, o):\n        m, c = [array_ops.gather(x, i), array_ops.gather(x, i)]\n        o = math_ops.add(o, m)\n        o = math_ops.add(o, c)\n        i = math_ops.add(i, 1)\n        return [i, m, c, o]\n\n      i = ops.convert_to_tensor(0)\n      m = ops.convert_to_tensor(0)\n      c = ops.convert_to_tensor(0)\n      o = ops.convert_to_tensor(0)\n      x = ops.convert_to_tensor([1, 2, 3, 4, 5, 6])\n      s = array_ops.size(x)\n      r = control_flow_ops.while_loop(lambda i, m, c, o: math_ops.less(i, s),\n                                      compute, [i, m, c, o])\n      result = r[3]\n    self.assertAllEqual(42, result)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhile_5(self):\n    with self.cached_session():\n\n      def compute(i, c, o):\n        c = array_ops.strided_slice(x, array_ops.expand_dims(i, 0),\n                                    [1] + array_ops.expand_dims(i, 0))\n        o = array_ops.concat([o, c], 0)\n        i = math_ops.add(i, 1)\n        return [i, c, o]\n\n      i = ops.convert_to_tensor(0)\n      c = ops.convert_to_tensor([0])\n      o = ops.convert_to_tensor([0])\n      x = ops.convert_to_tensor([1, 2, 3, 4, 5, 6])\n      s = array_ops.size(x)\n      r = control_flow_ops.while_loop(lambda i, c, o: math_ops.less(i, s),\n                                      compute, [i, c, o], [\n                                          i.get_shape(),\n                                          tensor_shape.unknown_shape(),\n                                          tensor_shape.unknown_shape()\n                                      ])\n      result = r[2]\n    self.assertAllEqual(np.array([0, 1, 2, 3, 4, 5, 6]), result)\n\n  @test_util.run_gpu_only\n  @test_util.run_deprecated_v1\n  def testWhile_Device(self):\n\n    # Body function defined outside of device scope\n    def body(x):\n      return math_ops.exp(x)\n\n    with ops.device(\"CPU:0\"):\n      r = control_flow_ops.while_loop(\n          lambda x: x < 10, body, [constant_op.constant(-10.)])\n      self.assertIn(\"cpu\", r.device.lower())\n\n    with session.Session() as sess:\n      options = config_pb2.RunOptions(output_partition_graphs=True)\n      run_metadata = config_pb2.RunMetadata()\n      sess.run(r, options=options, run_metadata=run_metadata)\n      # We expect that everything runs on CPU, even if GPU is available.\n      self.assertEqual(len(run_metadata.partition_graphs), 1)\n\n  @test_util.disable_control_flow_v2(\"b/116338794 (buffer_reuse)\")\n  @test_util.run_v1_only(\"b/120545219\")\n  def testBufferForwarding(self):\n    run_options = config_pb2.RunOptions(\n        trace_level=config_pb2.RunOptions.FULL_TRACE)\n    run_metadata = config_pb2.RunMetadata()\n\n    with self.cached_session() as sess:\n      with ops.device(\"/cpu:0\"):\n        c = constant_op.constant(2)\n        i0 = constant_op.constant(0)\n        r = control_flow_ops.while_loop(lambda i: i < 1000,\n                                        lambda i: math_ops.square(c) + i, [i0])\n      r_val = sess.run(r, options=run_options, run_metadata=run_metadata)\n      self.assertEqual(1000, r_val)\n      self.assertTrue(run_metadata.HasField(\"step_stats\"))\n      unique_allocs = set()\n      for node_stat in run_metadata.step_stats.dev_stats[0].node_stats:\n        for output in node_stat.output:\n          unique_allocs.add(\n              output.tensor_description.allocation_description.ptr)\n      # Prior to cl/147536680, the number of unique allocations was about 1005.\n      self.assertLess(len(unique_allocs), 756)\n\n  def _testWhile_Gpu_1(self, use_gpu):\n    with self.cached_session(use_gpu=use_gpu):\n      n = constant_op.constant(1.0)\n      c = lambda x: math_ops.less(x, 10.0)\n      b = lambda x: math_ops.add(x, 1.0)\n      r = control_flow_ops.while_loop(c, b, [n])\n      self.assertAllClose(10.0, self.evaluate(r))\n\n  def testWhile_Gpu_1(self):\n    self._testWhile_Gpu_1(use_gpu=False)\n    self._testWhile_Gpu_1(use_gpu=True)\n\n  def _testWhile_Gpu_2(self, use_gpu):\n    with self.cached_session(use_gpu=use_gpu):\n      n = constant_op.constant(1.0)\n      c = lambda x: math_ops.less(x, 10.0)\n\n      def b(x):\n        with ops.device(\"/cpu:0\"):\n          return math_ops.add(x, 1.0)\n\n      r = control_flow_ops.while_loop(c, b, [n])\n      self.assertAllClose(10.0, self.evaluate(r))\n\n  def testWhile_Gpu_2(self):\n    self._testWhile_Gpu_2(use_gpu=False)\n    self._testWhile_Gpu_2(use_gpu=True)\n\n  def testWhileShape(self):\n    with self.cached_session():\n      i = constant_op.constant(0)\n      m = array_ops.ones([2, 2])\n      c = lambda i, j: math_ops.less(i, 2)\n\n      def _b(i, j):\n        new_i = math_ops.add(i, 1)\n        new_j = array_ops.tile(j, [2, 2])\n        return [new_i, new_j]\n\n      r = control_flow_ops.while_loop(\n          c, _b, [i, m],\n          [i.get_shape(), tensor_shape.unknown_shape()])\n      r = r[1] * array_ops.ones([8, 8])\n      self.assertAllEqual(np.ones((8, 8)), self.evaluate(r))\n\n  @test_util.disable_control_flow_v2(\"b/131265085\")\n  @test_util.run_v1_only(\"b/131265085\")\n  def testWhileBadShape(self):\n    x = constant_op.constant([2.0, 4.0], name=\"values\")\n    i = constant_op.constant(0)\n    c = lambda i, _: math_ops.less(i, 10)\n    b = lambda i, x: [i + 1, x + 1]\n    with self.assertRaisesRegex(ValueError, \"is not compatible with\"):\n      # Shape of x is [2], but we specify a shape of [5].\n      control_flow_ops.while_loop(\n          c, b, [i, x], [i.shape, tensor_shape.TensorShape([5])])\n\n  @test_util.run_in_graph_and_eager_modes\n  def testWhileBadBodyReturn(self):\n    x = constant_op.constant([2.0, 4.0], name=\"values\")\n    i = constant_op.constant(0)\n    c = lambda i, *x: math_ops.less(i, 10)\n\n    # body accepts N values and returns N+1 values.\n    b = lambda i, *x: (i, i) + x\n\n    with self.assertRaisesRegex(\n        ValueError, \"The two structures don't have the same nested structure.\"):\n      control_flow_ops.while_loop(c, b, [i, x])\n\n  @test_util.run_deprecated_v1\n  def testWhileWithNonTensorInput_Scalar(self):\n    with self.cached_session():\n      n = 0\n      c = lambda x: x < 10000\n      b = lambda x: x + 1\n      r = control_flow_ops.while_loop(c, b, [n], parallel_iterations=20)\n      self.assertEqual(10000, self.evaluate(r))\n\n  def testWhileWithNonTensorInput_Vector(self):\n    with self.cached_session():\n      n = np.array([0])  # Note, [0] would not work here; that is a list\n      c = lambda x: x[0] < 10000\n      b = lambda x: array_ops.stack([x[0] + 1])\n      r = control_flow_ops.while_loop(c, b, [n], parallel_iterations=20)\n      self.assertEqual([10000], self.evaluate(r))\n\n  def testWhileShapeInference(self):\n    with self.cached_session():\n      i = constant_op.constant(0)\n      m = array_ops.ones([2, 2])\n      c = lambda i, j: math_ops.less(i, 2)\n\n      def b(i, j):\n        new_i = math_ops.add(i, 1)\n        new_j = array_ops.concat([j, j], 0)\n        return [new_i, new_j]\n\n      r = control_flow_ops.while_loop(\n          c, b, [i, m],\n          [i.get_shape(), tensor_shape.TensorShape([None, 2])])\n      self.assertTrue(r[1].shape.is_compatible_with([8, 2]))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileShapeInferenceBadShape(self):\n    with self.cached_session():\n      i = constant_op.constant(0)\n      m = array_ops.ones([2, 2])\n      c = lambda i, j: math_ops.less(i, 2)\n      b = lambda i, j: [i + 1, array_ops.concat([j, j], 0)]\n      with self.assertRaisesRegex(\n          ValueError,\n          r\"Input tensor 'ones:0' enters the loop with shape \\(2, 2\\), but has \"\n          r\"shape \\(4, 2\\) after one iteration. To allow the shape to vary \"\n          r\"across iterations, use the `shape_invariants` argument of \"\n          r\"tf.while_loop to specify a less-specific shape.\"):\n        control_flow_ops.while_loop(c, b, [i, m])\n\n  def testWhileShapeInferenceSparseTensor(self):\n    values = constant_op.constant([2.0, 4.0], name=\"values\")\n    indices = constant_op.constant([[0], [3]],\n                                   dtype=dtypes.int64,\n                                   name=\"indices\")\n    shape = constant_op.constant([10], dtype=dtypes.int64, name=\"dense_shape\")\n    i = constant_op.constant(0)\n    x = sparse_tensor.SparseTensor(indices, values, dense_shape=shape)\n\n    def c(i, _):\n      return i < 10\n\n    def b1(i, x):  # modifies values.  (shape of components is not changed.)\n      return [\n          i + 1,\n          sparse_tensor.SparseTensor(x.indices, x.values * 2.0, x.dense_shape)\n      ]\n\n    def b2(i, x):  # adds new values.  (shape of components is changed.)\n      return [\n          i + 1,\n          sparse_ops.sparse_add(\n              x,\n              sparse_tensor.SparseTensor(\n                  indices=math_ops.cast(\n                      array_ops.fill([1, 1], i), dtypes.int64),\n                  values=array_ops.fill([1], 1.0),\n                  dense_shape=x.dense_shape))\n      ]\n\n    def b3(i, x):  # modifies rank.  (shape of all components is changed.)\n      return [\n          i + 1,\n          sparse_tensor.SparseTensor(\n              array_ops.concat([x.indices, [[i], [i]]], axis=1), x.values * 2.0,\n              array_ops.concat([x.dense_shape, [10]], axis=0))\n      ]\n\n    def check_shapes(r, indices, values, dense_shape):\n      self.assertTrue(r.indices.shape.is_compatible_with(indices))\n      self.assertTrue(r.values.shape.is_compatible_with(values))\n      self.assertTrue(r.dense_shape.shape.is_compatible_with(dense_shape))\n\n    # Default shape invariant; b1 only modifies values.\n    _, r = control_flow_ops.while_loop(c, b1, [i, x])\n    check_shapes(r, indices=[None, 1], values=[None], dense_shape=[1])\n\n    # Default shape invariant; b2 adds new values\n    _, r = control_flow_ops.while_loop(c, b2, [i, x])\n    check_shapes(r, indices=[None, 1], values=[None], dense_shape=[1])\n\n    # Explicit shape invariant, allowing any rank; b1 only modifies values.\n    _, r = control_flow_ops.while_loop(\n        c, b1, [i, x],\n        [i.get_shape(), tensor_shape.TensorShape([None])])\n    check_shapes(r, indices=[None, None], values=[None], dense_shape=[None])\n\n    # Explicit shape invariant, allowing any rank; b3 modifies rank.\n    _, r = control_flow_ops.while_loop(\n        c, b3, [i, x],\n        [i.get_shape(), tensor_shape.TensorShape([None])])\n    check_shapes(r, indices=[None, None], values=[None], dense_shape=[None])\n\n    # Shape invariant with ndims=None.  Technically, this isn't supported\n    # according to the docs, but we support it for backwards compatibility.\n    _, r = control_flow_ops.while_loop(\n        c, b1, [i, x],\n        [i.get_shape(), tensor_shape.TensorShape(None)])\n    check_shapes(r, indices=[None, None], values=[None], dense_shape=[None])\n    _, r = control_flow_ops.while_loop(\n        c, b3, [i, x],\n        [i.get_shape(), tensor_shape.TensorShape(None)])\n    check_shapes(r, indices=[None, None], values=[None], dense_shape=[None])\n\n  @test_util.disable_control_flow_v2(\"b/131265085\")\n  @test_util.run_v1_only(\"b/131265085\")\n  def testWhileBadShapeSparseTensor(self):\n    values = constant_op.constant([2.0, 4.0], name=\"values\")\n    indices = constant_op.constant([[0], [3]],\n                                   dtype=dtypes.int64,\n                                   name=\"indices\")\n    shape = constant_op.constant([10], dtype=dtypes.int64, name=\"dense_shape\")\n    i = constant_op.constant(0)\n    x = sparse_tensor.SparseTensor(indices, values, dense_shape=shape)\n    c = lambda i, _: i < 10\n    b1 = lambda i, x: [i+1, x]\n    def b2(i, x):  # modifies rank.  (shape of all components is changed.)\n      return [\n          i + 1,\n          sparse_tensor.SparseTensor(\n              array_ops.concat([x.indices, [[i], [i]]], axis=1), x.values * 2.0,\n              array_ops.concat([x.dense_shape, [10]], axis=0))\n      ]\n\n    # Explicit shape invariant, with a specific (incompatible) rank.\n    with self.assertRaisesRegex(ValueError, \"is not compatible with\"):\n      control_flow_ops.while_loop(\n          c, b1, [i, x],\n          [i.get_shape(), tensor_shape.TensorShape([5])])\n\n    # Default shape invariant, but b2 modifies rank (which is not allowed).\n    with self.assertRaises(ValueError):\n      control_flow_ops.while_loop(c, b2, [i, x])\n\n  def testWhileShapeInferenceIndexedSlices(self):\n    with self.cached_session():\n      values = constant_op.constant([[2.0, 4.0], [3.0, 5.0]], name=\"values\")\n      indices = constant_op.constant([0, 3], name=\"indices\")\n      shape = constant_op.constant([10, 2], name=\"dense_shape\")\n      i = constant_op.constant(0)\n      x = ops.IndexedSlices(values, indices, dense_shape=shape)\n\n      def c(i, _):\n        return i < 10\n\n      def b(i, x):\n        return [\n            i + 1,\n            ops.IndexedSlices(x.values * 2.0, x.indices, x.dense_shape)\n        ]\n\n      _, r = control_flow_ops.while_loop(c, b, [i, x])\n      self.assertEqual(r.dense_shape.get_shape()[0], 2)\n      self.assertEqual(r.values.get_shape(), tensor_shape.TensorShape([2, 2]))\n\n      _, r = control_flow_ops.while_loop(\n          c, b, [i, x],\n          [i.get_shape(), tensor_shape.TensorShape([None, 2])])\n      self.assertEqual(r.dense_shape.get_shape()[0], 2)\n      self.assertTrue(r.values.get_shape().is_compatible_with([None, 2]))\n\n  @test_util.disable_control_flow_v2(\"b/131265085\")\n  @test_util.run_v1_only(\"b/131265085\")\n  def testWhileBadShapeIndexedSlices(self):\n    values = constant_op.constant([2.0, 4.0], name=\"values\")\n    indices = constant_op.constant([[0], [3]],\n                                   dtype=dtypes.int64,\n                                   name=\"indices\")\n    shape = constant_op.constant([10], dtype=dtypes.int64, name=\"dense_shape\")\n    i = constant_op.constant(0)\n    x = sparse_tensor.SparseTensor(indices, values, dense_shape=shape)\n    c = lambda i, _: 10\n    b = lambda i, x: [i+1, x]\n\n    # Explicit shape invariant, with a specific (incompatible) rank.\n    with self.assertRaisesRegex(ValueError, \"is not compatible with\"):\n      control_flow_ops.while_loop(\n          c, b, [i, x],\n          [i.get_shape(), tensor_shape.TensorShape([5])])\n\n  def testWhileShapeInferenceRaggedTensor(self):\n    i = constant_op.constant(0)\n    x = ragged_factory_ops.constant([[1, 2], [3], [4, 5, 6]])\n    c = lambda i, _: i < 10\n\n    def b1(i, x):  # Adds new values to rows (but doesn't create new rows)\n      return [\n          i + 1,\n          array_ops.concat([x, x], axis=1)\n      ]\n\n    def b2(i, x):  # Adds new rows.\n      return [\n          i + 1,\n          array_ops.concat([x, x], axis=0)\n      ]\n\n    def check_shapes(r, values, splits):\n      self.assertTrue(r.values.shape.is_compatible_with(values))\n      self.assertTrue(r.row_splits.shape.is_compatible_with(splits))\n\n    # Default shape invariant; b1 adds new values to rows.\n    _, r = control_flow_ops.while_loop(c, b1, [i, x])\n    check_shapes(r, values=[None], splits=[4])\n\n    # Default shape invariant; b2 adds new rows (not allowed).\n    if not context.executing_eagerly():\n      with self.assertRaises(ValueError):\n        _, r = control_flow_ops.while_loop(c, b2, [i, x])\n\n    # Explicit shape invariant; b1 adds new values to rows.\n    # (deprecated: use TensorShape instead of RaggedTensorSpec)\n    _, r = control_flow_ops.while_loop(\n        c, b1, [i, x],\n        [i.get_shape(), tensor_shape.TensorShape([None, None])])\n    check_shapes(r, values=[None], splits=[None])\n\n    # Explicit shape invariant; b1 adds new values to rows.\n    _, r = control_flow_ops.while_loop(\n        c, b1, [i, x],\n        [i.get_shape(), ragged_tensor.RaggedTensorSpec([None, None],\n                                                       dtypes.int32)])\n    check_shapes(r, values=[None], splits=[None])\n\n    # Explicit shape invariant; b2 adds new rows.\n    _, r = control_flow_ops.while_loop(\n        c, b2, [i, x],\n        [i.get_shape(), ragged_tensor.RaggedTensorSpec([None, None],\n                                                       dtypes.int32)])\n    check_shapes(r, values=[None], splits=[None])\n\n  def testWhileShapeInferenceRaggedTensorRaggedRank2(self):\n    i = constant_op.constant(0)\n    x = ragged_factory_ops.constant([[[1, 2], [3], [4, 5, 6]],\n                                     [[], [8, 9, 10]]])\n    c = lambda i, _: i < 10\n    def b(i, x):\n      return [\n          i + 1,\n          array_ops.concat([x, x[..., i:i+1]], axis=-1)\n      ]\n    _, r = control_flow_ops.while_loop(c, b, [i, x])\n    self.assertEqual(r.row_splits.shape.as_list(), [3])\n    self.assertTrue(r.values.row_splits.shape.as_list() in ([6], [None]))\n    self.assertTrue(r.values.values.shape.as_list() in ([49], [None]))\n\n  def testWhileShapeInvariantTensorSpec(self):\n    i = constant_op.constant(0)\n    x = constant_op.constant([1])\n    c = lambda i, _: i < 10\n    b = lambda i, x: (i + 1, array_ops.stack([x, x]))\n    shape_invariants = [\n        tensor_spec.TensorSpec([], dtype=dtypes.int32),\n        tensor_spec.TensorSpec(None, dtype=dtypes.int32)]\n    control_flow_ops.while_loop(c, b, [i, x], shape_invariants)\n\n  # TODO(b/131265085) Remove this decorator when bug is fixed.\n  @test_util.build_as_function_and_v1_graph\n  def testWhileShapeInvariantWrongTypeSpecType(self):\n    c = lambda i, _: i < 10\n    b = lambda i, x: (i + 1, x)\n    i = constant_op.constant(0)\n    x = sparse_tensor.SparseTensor([[0]], [1.0], [10])\n    shape_invariants = [\n        tensor_spec.TensorSpec([], dtype=dtypes.int32),\n        sparse_tensor.SparseTensorSpec([None])]\n    control_flow_ops.while_loop(c, b, [i, x], shape_invariants)\n\n    x2 = constant_op.constant([1])\n    with self.assertRaises(TypeError):\n      control_flow_ops.while_loop(c, b, [i, x2], shape_invariants)\n\n    x3 = ragged_factory_ops.constant([[1, 2], [3]])\n    with self.assertRaises(TypeError):\n      control_flow_ops.while_loop(c, b, [i, x3], shape_invariants)\n\n    i2 = constant_op.constant(0.0)\n    with self.assertRaises(TypeError):\n      control_flow_ops.while_loop(c, b, [i2, x], shape_invariants)\n\n  # TODO(b/131265085) Remove this decorator when bug is fixed.\n  @test_util.build_as_function_and_v1_graph\n  def testWhileShapeInvariantBadType(self):\n    i = constant_op.constant(0)\n    x = constant_op.constant([1])\n    c = lambda i, _: i < 10\n    b = lambda i, x: (i + 1, x)\n    with self.assertRaises((ValueError, TypeError)):\n      control_flow_ops.while_loop(c, b, [i, x], [\"foo\", \"bar\"])\n\n  def _testNestedWhile_1(self, use_gpu):\n    with self.cached_session(use_gpu=use_gpu):\n      n = constant_op.constant(0)\n\n      def cpu_sum(s):\n        c = lambda i, s: math_ops.less(i, 10)\n\n        def b(i, s):\n          i1 = math_ops.add(i, 1)\n          with ops.device(\"/cpu:0\"):\n            s1 = math_ops.add(i, s)\n          return i1, s1\n\n        _, r_s = control_flow_ops.while_loop(c, b, [n, s])\n        return r_s\n\n      c = lambda x: math_ops.less(x, 200)\n      b = lambda x: math_ops.add(x, cpu_sum(n))\n      r = control_flow_ops.while_loop(c, b, [n])\n      self.assertEqual(225, self.evaluate(r))\n\n  def testNestedWhile_1(self):\n    self._testNestedWhile_1(use_gpu=False)\n    self._testNestedWhile_1(use_gpu=True)\n\n  def _testNestedWhile_2(self, use_gpu):\n    # Test the cases that A -> Enter and Exit -> A are partitioned.\n    with self.cached_session(use_gpu=use_gpu):\n      s0 = constant_op.constant(2.0)\n\n      def inner_loop(s):\n        c = lambda s: math_ops.less(s, 20.0)\n\n        def b(s):\n          s1 = math_ops.add(s, s)\n          return s1\n\n        r_s = control_flow_ops.while_loop(c, b, [s], parallel_iterations=1)\n        return r_s\n\n      outer_c = lambda x: math_ops.less(x, 3000.0)\n\n      def outer_b(x):\n        x = logging_ops.Print(x, [x])  # Edge \"Print -> Enter\" is partitioned\n        x = inner_loop(x)\n        with ops.device(\"/cpu:0\"):\n          x = math_ops.square(x)  # Edge \"Exit -> Square\" is partitioned\n        return x\n\n      r = control_flow_ops.while_loop(\n          outer_c, outer_b, [s0], parallel_iterations=1)\n      self.assertEqual(1048576.0, self.evaluate(r))\n\n  def testNestedWhile_2(self):\n    self._testNestedWhile_2(use_gpu=False)\n    self._testNestedWhile_2(use_gpu=True)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileWithControl_1(self):\n    with self.cached_session():\n      n = constant_op.constant(0)\n      r = constant_op.constant(0)\n      condition = lambda n_, r_: math_ops.less(n_, 10)\n\n      def body(n_, r_):\n        n_ = math_ops.add(n_, 1)\n        with r_.graph.control_dependencies([r_]):\n          r_ = constant_op.constant(12)\n        return [n_, r_]\n\n      res = control_flow_ops.while_loop(\n          condition, body, [n, r], parallel_iterations=1)\n      self.assertAllEqual(12, res[1])\n\n  @test_util.run_deprecated_v1\n  def testWhileWithControl_2(self):\n    with self.cached_session():\n      r = constant_op.constant(0)\n      condition = lambda r_: math_ops.less(r_, 10)\n\n      def body(r_):\n        with r_.graph.control_dependencies([r_]):\n          r_ = constant_op.constant(12)\n        return [r_]\n\n      res = control_flow_ops.while_loop(\n          condition, body, [r], parallel_iterations=1)\n      self.assertAllEqual(12, self.evaluate(res))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileWithControl_3(self):\n    with self.cached_session() as sess:\n      b = array_ops.placeholder(dtypes.bool)\n      c = constant_op.constant(1)\n      x0 = constant_op.constant(0)\n      with ops.control_dependencies([b]):\n        r = control_flow_ops.while_loop(lambda x: x < 10, lambda x: x + c, [x0])\n      self.assertEqual(10, sess.run(r, {b: True}))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileWithControl_4(self):\n    with self.cached_session() as sess:\n      b = array_ops.placeholder(dtypes.bool)\n      c = constant_op.constant(1)\n      x0 = constant_op.constant(0)\n      with ops.control_dependencies([b]):\n        r = control_flow_ops.while_loop(\n            lambda x: x < 10, lambda x: x + array_ops.identity(c), [x0])\n      self.assertEqual(10, sess.run(r, {b: True}))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileWithControl_5(self):\n    with self.cached_session() as sess:\n      b = array_ops.placeholder(dtypes.bool)\n      c = constant_op.constant(1)\n      x0 = constant_op.constant(0)\n\n      def body(x):\n        with ops.control_dependencies([b]):\n          return x + c\n\n      r = control_flow_ops.while_loop(lambda x: x < 10, body, [x0])\n      self.assertEqual(10, sess.run(r, {b: True}))\n\n  def testWhileCondWithControl(self):\n    # Ensure that no control edges by an outer control dependency context are\n    # added to nodes inside cond/while contexts.\n    with self.cached_session() as sess:\n      const_true = lambda: constant_op.constant(True)\n      const_false = lambda: constant_op.constant(False)\n      cond = lambda i: control_flow_ops.cond(i > 0, const_true, const_false)\n      body = lambda i: control_flow_ops.cond(i > 0, lambda: i - 1, lambda: i)\n\n      with ops.control_dependencies([control_flow_ops.no_op()]):\n        loop = control_flow_ops.while_loop(cond, body,\n                                           (constant_op.constant(5),))\n      self.assertEqual(0, self.evaluate(loop))\n\n  @test_util.disable_control_flow_v2(\"b/113324949 (ref vars)\")\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileCondWithControl_1(self):\n    with self.cached_session():\n      v = variable_scope.get_variable(\n          \"v\", [], initializer=init_ops.constant_initializer(2))\n      i0 = constant_op.constant(0)\n      with ops.control_dependencies([i0]):\n\n        def loop_condition(i):\n          return i < 4\n\n        def loop_body(i):\n          some_cond = control_flow_ops.cond(\n              constant_op.constant(True),\n              lambda: state_ops.assign(v, math_ops.square(v)), lambda: v)\n          with ops.control_dependencies([some_cond]):\n            return i + 1\n\n      r = control_flow_ops.while_loop(loop_condition, loop_body, (i0,))\n      self.evaluate(variables.global_variables_initializer())\n      self.assertEqual(4, self.evaluate(r))\n      self.assertAllClose(65536.0, self.evaluate(v))\n\n  @test_util.disable_control_flow_v2(\"b/113324949 (ref vars)\")\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileCondExitControl(self):\n\n    with self.cached_session():\n      v = variables.Variable(1)\n\n      def false_branch():\n        cond = lambda i: i < 100\n\n        def body(i):\n          x = state_ops.assign(v, i)\n          return x + 1\n\n        loop = control_flow_ops.while_loop(cond, body, [0])\n        # Make sure to handle correctly control edge from Exit to a node.\n        with ops.control_dependencies([loop]):\n          return constant_op.constant(6.0)\n\n      r = control_flow_ops.cond(\n          constant_op.constant(False), lambda: constant_op.constant(1.0),\n          false_branch)\n      self.evaluate(variables.global_variables_initializer())\n      self.assertEqual(6.0, self.evaluate(r))\n      self.assertEqual(99, self.evaluate(v))\n\n  def testCondWhile_1(self):\n\n    with self.cached_session():\n      n = ops.convert_to_tensor(0, name=\"n\")\n      c = lambda x: math_ops.less(x, 10)\n      b = lambda x: math_ops.add(x, 1)\n      r = control_flow_ops.cond(\n          math_ops.less(0, 1), lambda: control_flow_ops.while_loop(c, b, [n]),\n          lambda: n)\n      self.assertAllEqual(10, self.evaluate(r))\n\n  def testCondWhile_2(self):\n\n    with self.cached_session():\n      n = ops.convert_to_tensor(0)\n      c = lambda x: math_ops.less(x, 10)\n      b = lambda x: math_ops.add(x, 1)\n      r = control_flow_ops.cond(\n          math_ops.less(1, 0), lambda: math_ops.add(n, 1),\n          lambda: control_flow_ops.while_loop(c, b, [n]))\n      self.assertAllEqual(10, self.evaluate(r))\n\n  def _testCondWhile_3(self, use_gpu):\n    with self.cached_session(use_gpu=use_gpu) as sess:\n      p = array_ops.placeholder(dtypes.bool)\n      n = constant_op.constant(0.0)\n\n      def c(x):\n        return math_ops.less(x, 10.0)\n\n      def b(x):\n        with ops.device(\"/cpu:0\"):\n          x1 = math_ops.add(x, 1.0)\n        return x1\n\n      r = control_flow_ops.cond(p,\n                                lambda: control_flow_ops.while_loop(c, b, [n]),\n                                lambda: math_ops.multiply(n, 2.0))\n      r1 = gradients_impl.gradients(r, [n])\n      self.assertEqual(10., sess.run(r, {p: True}))\n      self.assertEqual([1.0], sess.run(r1, {p: True}))\n      self.assertEqual(0.0, sess.run(r, {p: False}))\n      self.assertEqual([2.0], sess.run(r1, {p: False}))\n\n  @test_util.run_deprecated_v1\n  def testCondWhile_3(self):\n    self._testCondWhile_3(use_gpu=False)\n    self._testCondWhile_3(use_gpu=True)\n\n  def testWhileCond_1(self):\n\n    with self.cached_session():\n      i = ops.convert_to_tensor(0, name=\"i\")\n      n = ops.convert_to_tensor(10, name=\"n\")\n      one = ops.convert_to_tensor(1, name=\"one\")\n      c = lambda x: math_ops.less(x, n)\n      # pylint: disable=undefined-variable\n      # for OSS build\n      b = lambda x: control_flow_ops.cond(\n          constant_op.constant(True),\n          lambda: math_ops.add(x, one), lambda: math_ops.subtract(x, one))\n      # pylint: enable=undefined-variable\n      r = control_flow_ops.while_loop(c, b, [i])\n      self.assertAllEqual(10, self.evaluate(r))\n\n  def testWhileCond_2(self):\n\n    with self.cached_session():\n      n = ops.convert_to_tensor(0, name=\"n\")\n      c = lambda x: math_ops.less(x, 10)\n      b = lambda x: control_flow_ops.cond(constant_op.constant(True), lambda: math_ops.add(x, 1), lambda: n)\n      r = control_flow_ops.while_loop(c, b, [n])\n      self.assertAllEqual(10, self.evaluate(r))\n\n  def testWhileCond_3(self):\n\n    with self.cached_session():\n      n = ops.convert_to_tensor(0)\n      c = lambda x: math_ops.less(x, 10)\n      # pylint: disable=undefined-variable\n      # for OSS build\n      b = lambda x: control_flow_ops.cond(math_ops.less(0, 1),\n                                          lambda: math_ops.add(x, 1),\n                                          lambda: math_ops.subtract(x, 1))\n      # pylint: enable=undefined-variable\n      r = control_flow_ops.while_loop(c, b, [n])\n      self.assertAllEqual(10, self.evaluate(r))\n\n  @test_util.run_deprecated_v1\n  def testWhileCondGradMultiDevice(self):\n    config = config_pb2.ConfigProto(device_count={\"CPU\": 2},\n                                    allow_soft_placement=True)\n    with self.cached_session(use_gpu=True, config=config) as sess:\n      pred = array_ops.placeholder(dtypes.bool, [])\n      x_init = constant_op.constant(1.0)\n\n      with ops.device(\"/cpu:0\"):\n        z = control_flow_ops.while_loop(\n            lambda i, _: i < 3,\n            lambda i, x: (i + 1, control_flow_ops.cond(\n                pred, lambda: x * 2.0, lambda: 10.0)),\n            [0, x_init])\n\n      with ops.device(\"/cpu:1\"):\n        grad = gradients_impl.gradients(z, x_init)[0]\n\n      with ops.device(\"/cpu:0\"):\n        grad_grad = gradients_impl.gradients(grad, x_init)[0]\n\n      self.assertEqual(sess.run(grad, {pred: True}), 8.0)\n      self.assertEqual(sess.run(grad, {pred: False}), 0.0)\n\n      if not control_flow_util.ENABLE_CONTROL_FLOW_V2:\n        return\n\n      self.assertEqual(sess.run(grad_grad, {pred: True}), 0.0)\n      self.assertEqual(sess.run(grad_grad, {pred: False}), 0.0)\n\n  # NOTE: It is ok to have parallel_iterations > 1\n  @test_util.disable_control_flow_v2(\"b/113324949 (RefVariable)\")\n  @test_util.run_deprecated_v1\n  def testWhileUpdateVariable_1(self):\n    with self.cached_session():\n      select = variables.Variable([3.0, 4.0, 5.0])\n      n = constant_op.constant(0)\n\n      def loop_iterator(j):\n        return math_ops.less(j, 3)\n\n      def loop_body(j):\n        ns = state_ops.scatter_update(select, j, 10.0)\n        nj = math_ops.add(j, 1)\n        op = control_flow_ops.group(ns)\n        nj = control_flow_ops.with_dependencies([op], nj)\n        return [nj]\n\n      r = control_flow_ops.while_loop(\n          loop_iterator, loop_body, [n], parallel_iterations=1)\n      self.evaluate(variables.global_variables_initializer())\n      self.assertEqual(3, self.evaluate(r))\n      result = self.evaluate(select)\n      self.assertAllClose(np.array([10.0, 10.0, 10.0]), result)\n\n  @test_util.disable_control_flow_v2(\"b/113324949 (RefVariable)\")\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileUpdateVariable_2(self):\n    with self.cached_session():\n      select1 = variables.Variable([3.0, 4.0, 5.0])\n      select2 = variables.Variable([3.0, 4.0, 5.0])\n      n = constant_op.constant(0)\n\n      def loop_iterator(j):\n        return math_ops.less(j, 3)\n\n      def loop_body(j):\n        ns1 = state_ops.scatter_update(select1, j, 10.0)\n        ns2 = state_ops.scatter_update(select2, j, 10.0)\n        nj = math_ops.add(j, 1)\n        op = control_flow_ops.group(ns1, ns2)\n        nj = control_flow_ops.with_dependencies([op], nj)\n        return [nj]\n\n      r = control_flow_ops.while_loop(\n          loop_iterator, loop_body, [n], parallel_iterations=1)\n      self.evaluate(variables.global_variables_initializer())\n      self.assertEqual(3, self.evaluate(r))\n      result1 = self.evaluate(select1)\n      self.assertAllClose(np.array([10.0, 10.0, 10.0]), result1)\n      result2 = self.evaluate(select2)\n      self.assertAllClose(np.array([10.0, 10.0, 10.0]), result2)\n\n  @test_util.disable_control_flow_v2(\"b/113324949 (RefVariable)\")\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileUpdateVariable_3(self):\n    with self.cached_session():\n      select = variables.Variable([3.0, 4.0, 5.0])\n      n = constant_op.constant(0)\n\n      def loop_iterator(j, _):\n        return math_ops.less(j, 3)\n\n      def loop_body(j, _):\n        ns = state_ops.scatter_update(select, j, 10.0)\n        nj = math_ops.add(j, 1)\n        return [nj, ns]\n\n      r = control_flow_ops.while_loop(\n          loop_iterator,\n          loop_body, [n, array_ops.identity(select)],\n          parallel_iterations=1)\n      self.evaluate(variables.global_variables_initializer())\n      result = r[1]\n    self.assertAllClose(np.array([10.0, 10.0, 10.0]), result)\n\n  @test_util.disable_control_flow_v2(\"b/113324949 (RefVariable)\")\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileUpdateVariable_4(self):\n    with self.cached_session():\n      var_a = variables.Variable(0, name=\"a\")\n      var_b = variables.Variable(0, name=\"b\")\n      self.evaluate(variables.global_variables_initializer())\n\n      c = constant_op.constant(0, name=\"c\")\n      asn1 = state_ops.assign_add(var_a, 1, name=\"a_add\")\n\n      # Loop condition\n      def pred(i):\n        return math_ops.less(i, 10)\n\n      # Loop body\n      def loop_body(i):\n        asn2 = state_ops.assign_add(var_b, asn1, name=\"b_add\")\n        with ops.control_dependencies([asn2]):\n          ni = math_ops.add(i, 1, name=\"i_add\")\n        return ni\n\n      lpa = control_flow_ops.while_loop(\n          pred, loop_body, [c], parallel_iterations=1)\n\n      self.assertEqual(0, self.evaluate(var_b))\n      self.evaluate(lpa)  # Run the loop\n      self.assertEqual(10, self.evaluate(var_b))\n\n  @test_util.disable_control_flow_v2(\"b/113324949 (RefVariable)\")\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileUpdateVariable_5(self):\n    with self.cached_session():\n      # Create some variables.\n      var_a = variables.Variable(0, name=\"a\")\n      var_b = variables.Variable(0, name=\"b\")\n      self.evaluate(variables.global_variables_initializer())\n\n      # Change condition to check var_b\n      def pred(_):\n        return math_ops.less(var_b, 10)\n\n      # Change body to increment var_b\n      def loop_body(i):\n        asn1 = state_ops.assign_add(\n            var_a, constant_op.constant(1), name=\"a_add\")\n        asn2 = state_ops.assign_add(\n            var_b, constant_op.constant(1), name=\"b_add\")\n        with ops.control_dependencies([asn1, asn2]):\n          inc_b = array_ops.identity(var_b)\n        return inc_b\n\n      lpa = control_flow_ops.while_loop(\n          pred, loop_body, [var_b], parallel_iterations=1, name=\"loop\")\n\n      self.assertEqual(0, self.evaluate(var_b))\n      self.evaluate(lpa)  # Run the loop\n      self.assertEqual(10, self.evaluate(var_a))\n      self.assertEqual(10, self.evaluate(var_b))\n\n  @test_util.disable_control_flow_v2(\"b/113324949 (RefVariable)\")\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileUpdateVariable_6(self):\n    with self.cached_session():\n      # Create some variables.\n      var_a = variables.Variable(0, name=\"a\")\n      var_b = variables.Variable(0, name=\"b\")\n      c = constant_op.constant(0)\n      self.evaluate(variables.global_variables_initializer())\n\n      # Loop condition\n      def pred(i):\n        return math_ops.less(i, 10)\n\n      # Loop body\n      def loop_body(i):\n        asn1 = state_ops.assign_add(var_a, 1, name=\"a_add\")\n        with ops.control_dependencies([asn1]):\n          asn2 = state_ops.assign_add(var_b, var_a, name=\"b_add\")\n        with ops.control_dependencies([asn2]):\n          ni = math_ops.add(i, 1, name=\"i_add\")\n          return ni\n\n      lpa = control_flow_ops.while_loop(\n          pred, loop_body, [c], parallel_iterations=1, name=\"loop\")\n\n      self.assertEqual(0, self.evaluate(var_b))\n      self.evaluate(lpa)  # Run the loop\n      self.assertEqual(55, self.evaluate(var_b))\n      self.assertEqual(10, self.evaluate(var_a))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileQueue_1(self):\n    with self.cached_session():\n      q = data_flow_ops.FIFOQueue(-1, dtypes.int32)\n      i = constant_op.constant(0)\n\n      def c(i):\n        return math_ops.less(i, 10)\n\n      def b(i):\n        ni = math_ops.add(i, 1)\n        ni = control_flow_ops.with_dependencies([q.enqueue((i,))], ni)\n        return ni\n\n      r = control_flow_ops.while_loop(c, b, [i], parallel_iterations=1)\n      self.assertEqual([10], self.evaluate(r))\n      for i in xrange(10):\n        self.assertEqual([i], self.evaluate(q.dequeue()))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileTimeOut(self):\n    run_options = config_pb2.RunOptions(timeout_in_ms=1)\n    with self.cached_session() as sess:\n      n = constant_op.constant(0)\n      c = lambda x: True\n      b = lambda x: math_ops.add(x, 1)\n      r = control_flow_ops.while_loop(c, b, [n])\n      with self.assertRaises(errors_impl.DeadlineExceededError):\n        sess.run(r, options=run_options)\n\n  @test_util.disable_control_flow_v2(\"b/117119329 (stack)\")\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileStack_1(self):\n    with self.cached_session():\n      s = gen_data_flow_ops.stack_v2(-1, dtypes.int32, stack_name=\"foo\")\n      i = constant_op.constant(0)\n\n      def c(i):\n        return math_ops.less(i, 10)\n\n      def b(i):\n        ni = math_ops.add(i, 1)\n        ni = control_flow_ops.with_dependencies(\n            [gen_data_flow_ops.stack_push_v2(s, i)], ni)\n        return ni\n\n      r = control_flow_ops.while_loop(c, b, [i], parallel_iterations=1)\n\n      x = constant_op.constant(0)\n\n      def c1(i, _):\n        return math_ops.greater(i, 0)\n\n      def b1(i, x):\n        ni = math_ops.subtract(i, 1)\n        nx = x + gen_data_flow_ops.stack_pop_v2(s, dtypes.int32)\n        return [ni, nx]\n\n      _, rx = control_flow_ops.while_loop(\n          c1,\n          b1, [r, x],\n          [r.get_shape(), tensor_shape.unknown_shape()],\n          parallel_iterations=1)\n      self.assertEqual(45, self.evaluate(rx))\n\n  def _testWhileGrad_ColocateGradients(self, colocate):\n    gpu_dev_name = test.gpu_device_name() if test.is_gpu_available(\n    ) else \"/device:CPU:0\"\n\n    graph = ops.Graph()\n    with graph.as_default():\n      v = constant_op.constant(2.0, name=\"v\")\n      c = lambda v: math_ops.less(v, 100.0)\n\n      def b(x):\n        with ops.device(gpu_dev_name):\n          return math_ops.square(x)\n\n      loop = control_flow_ops.while_loop(c, b, [v], parallel_iterations=1)\n      r = gradients_impl.gradients(\n          loop, v, colocate_gradients_with_ops=colocate)[0]\n\n    r_ops = graph.get_operations()\n    r_devices = [(op.name, op.device) for op in r_ops]\n\n    self.assertTrue(any(\"Square\" in op.name for op in r_ops))\n\n    for (name, dev) in r_devices:\n      if not colocate and name.endswith(\"Square\"):\n        # Only forward graph contain gpu in Square device\n        self.assertTrue(gpu_dev_name in dev)\n      elif colocate and \"Square\" in name:\n        # Forward and backward graphs contain gpu in Square/Square_grad devices\n        self.assertTrue(gpu_dev_name in dev)\n      else:\n        self.assertFalse(gpu_dev_name in dev)\n\n    with self.session(graph=graph) as sess:\n      self.assertAllClose(1024.0, self.evaluate(r))\n\n  @test_util.disable_control_flow_v2(\"b/116351701 (colocation)\")\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileGrad_ColocateGradients(self):\n    self._testWhileGrad_ColocateGradients(colocate=False)\n    self._testWhileGrad_ColocateGradients(colocate=True)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileGrad_Square(self):\n    with self.cached_session():\n      v = constant_op.constant(2.0, name=\"v\")\n      c = lambda v: math_ops.less(v, 100.0)\n      b = math_ops.square\n      r = control_flow_ops.while_loop(c, b, [v], parallel_iterations=1)\n      r = control_flow_ops.cond(math_ops.less(1, 2), lambda: r, lambda: v)\n\n      r = gradients_impl.gradients(r, v)[0]\n      self.assertAllClose(1024.0, self.evaluate(r))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileGrad_Shape(self):\n    with self.cached_session():\n      x = array_ops.placeholder(dtypes.float32, shape=[None])\n      v = constant_op.constant([2.0], name=\"v\")\n      n = constant_op.constant(0, name=\"n\")\n      c = lambda i, v: math_ops.less(i, 5)\n      b = lambda i, v: [i + 1, math_ops.multiply(x, v)]\n      r = control_flow_ops.while_loop(\n          c,\n          b, [n, v],\n          [n.get_shape(), tensor_shape.unknown_shape()],\n          parallel_iterations=1)\n\n      r = gradients_impl.gradients(r[1], x)[0]\n      self.assertEqual([None], r.get_shape().as_list())\n      self.assertAllClose([810.0, 2560.0], r.eval(feed_dict={x: [3.0, 4.0]}))\n\n  @test_util.run_deprecated_v1\n  def testWhileGrad_BaseShape(self):\n    with self.cached_session() as sess:\n      x = array_ops.placeholder(dtypes.float32, [None])\n      v0 = constant_op.constant([2.0, 2.0], name=\"v\")\n      c = lambda v: constant_op.constant(False)\n      b = lambda v: math_ops.multiply(v, x)\n      r = control_flow_ops.while_loop(c, b, [v0])\n      y = math_ops.square(x)\n\n      r = gradients_impl.gradients([r, y], x)[0]\n      self.assertAllClose([2.0, 4.0], sess.run(r, feed_dict={x: [1.0, 2.0]}))\n\n  @test_util.run_deprecated_v1\n  @test_util.enable_output_all_intermediates\n  def testWhileGradAfterSessionRun(self):\n    v0 = constant_op.constant(2.)\n    r = control_flow_ops.while_loop(\n        lambda _: True, lambda v: v * v, [v0], maximum_iterations=3)\n\n    self.assertAllEqual(r, 256.)\n    grad = gradients_impl.gradients(r, v0)[0]\n    self.assertAllClose(grad, 1024.)\n\n  @test_util.run_deprecated_v1\n  @test_util.enable_output_all_intermediates\n  def testNestedWhileGradAfterSessionRun(self):\n    v0 = constant_op.constant(2.)\n\n    def body(v):\n      inner_v0 = constant_op.constant(1.)\n      return control_flow_ops.while_loop(\n          lambda _: True, lambda x: x * v, [inner_v0], maximum_iterations=2)\n\n    r = control_flow_ops.while_loop(\n        lambda _: True, body, [v0], maximum_iterations=3)\n\n    self.assertAllEqual(r, 256.)\n    grad = gradients_impl.gradients(r, v0)[0]\n    self.assertAllClose(grad, 1024.)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileGrad_MultipleUses(self):\n    with self.cached_session():\n      v = constant_op.constant(2.0, name=\"v\")\n      c = lambda v: math_ops.less(v, 100.0)\n      b = math_ops.square\n      r = control_flow_ops.while_loop(c, b, [v], parallel_iterations=1)\n      r = math_ops.multiply(r, r)\n\n      r = gradients_impl.gradients(r, v)[0]\n      self.assertEqual(524288.0, self.evaluate(r))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileGrad_LoopAdd(self):\n    with self.cached_session():\n      v = constant_op.constant(2.0, name=\"v\")\n      c = lambda v: math_ops.less(v, 100.0)\n      b = math_ops.square\n      r = control_flow_ops.while_loop(c, b, [v], parallel_iterations=1)\n      r = math_ops.add(r, r)\n\n      r = gradients_impl.gradients(r, v)[0]\n      self.assertAllClose(2048.0, self.evaluate(r))\n\n  def _testWhileGrad_Mul(self, use_gpu, p_iters):\n    with self.cached_session(use_gpu=use_gpu) as sess:\n      a = constant_op.constant(3.0, name=\"a\")\n      v = constant_op.constant(2.0, name=\"v\")\n      c = lambda v: math_ops.less(v, 100.0)\n      b = lambda v: math_ops.multiply(v, a)\n      r = control_flow_ops.while_loop(c, b, [v], parallel_iterations=p_iters)\n\n      grad_a, grad_v = gradients_impl.gradients(r, [a, v])\n      grad_a_val, grad_v_val = self.evaluate([grad_a, grad_v])\n      self.assertAllClose(216.0, grad_a_val)\n      self.assertAllClose(81.0, grad_v_val)\n\n  @test_util.run_deprecated_v1\n  def testWhileGrad_Mul(self):\n    self._testWhileGrad_Mul(use_gpu=False, p_iters=1)\n    self._testWhileGrad_Mul(use_gpu=False, p_iters=10)\n    self._testWhileGrad_Mul(use_gpu=True, p_iters=1)\n    self._testWhileGrad_Mul(use_gpu=True, p_iters=10)\n\n  def testWhileGradInControlDeps(self):\n\n    @def_function.function\n    def f():\n      x_init = constant_op.constant(2.)\n      loop_cond = lambda i, x: math_ops.less(i, 2)\n      loop_body = lambda i, x: [i + 1, x**2]\n      _, x = control_flow_ops.while_loop(loop_cond, loop_body, [0, x_init])\n      with ops.control_dependencies([x]):\n        (grad,) = gradients_impl.gradients(x, x_init)\n        return grad\n\n    self.assertAllEqual(f(), 4. * 2.**3)  # 4 * x_init ^ 3\n\n  @test_util.run_deprecated_v1\n  def testTfFunctionInV1WhileLoop(self):\n\n    # This test specifically tests that creating a Const node inside a\n    # tf.function inside a v1 while_loop while inlining is turned on works.\n    config = opt_cfg()\n    assert config.graph_options.optimizer_options.do_function_inlining\n    with session.Session(config=config):\n\n      @def_function.function\n      def loop_body(i):\n        # Here we create the const.\n        return i + 1.\n\n      loop_cond = lambda i: True\n      x = control_flow_ops.while_loop(\n          loop_cond, loop_body, [0.], maximum_iterations=5)\n      self.assertAllEqual(x, 5.)\n\n  def _testNestedWhileCondWhileGrad(self, use_gpu):\n\n    with self.cached_session(use_gpu=use_gpu):\n      v = constant_op.constant(1.0)\n\n      def inner_loop(s):\n        z = constant_op.constant(0)\n        c = lambda i, x: math_ops.less(i, 4)\n        b = lambda i, x: [math_ops.add(i, 1), math_ops.multiply(x, 2.0)]\n        return control_flow_ops.while_loop(c, b, [z, s])\n\n      c = lambda x: math_ops.less(x, 128.0)\n\n      def b(x):\n        return control_flow_ops.cond(\n            constant_op.constant(True),\n            lambda: math_ops.square(inner_loop(x)[1]),\n            lambda: math_ops.multiply(x, 2.0))\n\n      r = control_flow_ops.while_loop(c, b, [v])\n      r = gradients_impl.gradients(r, v)[0]\n      self.assertAllClose(512.0, self.evaluate(r))\n\n  @test_util.run_deprecated_v1\n  def testNestedWhileCondWhileGrad(self):\n    self._testNestedWhileCondWhileGrad(use_gpu=False)\n\n  @test_util.run_deprecated_v1\n  def testNestedWhileCondWhileGradGpu(self):\n    self._testNestedWhileCondWhileGrad(use_gpu=True)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileGrad_Variable(self):\n    with self.cached_session():\n      a = variables.Variable(3.0)\n      v = constant_op.constant(2.0, name=\"v\")\n      c = lambda v: math_ops.less(v, 100.0)\n      b = lambda v: math_ops.multiply(v, a)\n      r = control_flow_ops.while_loop(c, b, [v], parallel_iterations=1)\n\n      r = gradients_impl.gradients(r, a)\n      self.evaluate(variables.global_variables_initializer())\n      self.assertAllClose(216.0, r[0])\n\n  @test_util.run_deprecated_v1\n  def testWhileGrad_ResourceVariable(self):\n    with self.cached_session():\n      a = resource_variable_ops.ResourceVariable(3.0)\n      v = constant_op.constant(2.0, name=\"v\")\n      c = lambda v: math_ops.less(v, 100.0)\n      b = lambda v: math_ops.multiply(v, a)\n      r = control_flow_ops.while_loop(c, b, [v], parallel_iterations=1)\n\n      g = gradients_impl.gradients(r, a)\n      self.evaluate(variables.global_variables_initializer())\n      self.assertAllClose(216.0, g[0])\n\n  def testWhileGrad_EagerResourceVariable(self):\n    with context.eager_mode():\n      a = resource_variable_ops.ResourceVariable(\n          np.ones([2, 2], dtype=np.float32))\n      v = constant_op.constant(1.0)\n\n      @eager_function.defun\n      def fn():\n        r = control_flow_ops.while_loop(\n            lambda i, _: i < 2,\n            lambda i, x: (i + 1, x * math_ops.reduce_sum(a) * v),\n            [0, 1.0])[1]\n        return gradients_impl.gradients(r, [v])[0]\n\n      self.assertEqual(self.evaluate(fn()), 32.)\n\n  def testWhileGrad_ResourceVarInFunctionCall(self):\n\n    @def_function.function\n    def foo(x, var):\n      return x + math_ops.reduce_sum(var.sparse_read([1, 3]))\n\n    @def_function.function\n    def bar(var):\n      r = control_flow_ops.while_loop(\n          lambda i, _: i < 2,\n          lambda i, x: (i + 1, foo(x, var)),\n          [0, 0.0])[1]\n      return gradients_impl.gradients(r, var)[0]\n\n    var = resource_variable_ops.ResourceVariable([1., 2., 3., 4.])\n    self.evaluate(variables.global_variables_initializer())\n    grad = self.evaluate(bar(var))\n    self.assertAllEqual(gradient_checker_v2._to_numpy(grad), [0., 2., 0., 2.])\n\n  def testWhileGrad_ResourceVarInNestedFunctionCall(self):\n\n    @def_function.function\n    def foo(x, var):\n      return x + math_ops.reduce_sum(var.sparse_read([1, 3]))\n\n    @def_function.function\n    def foo2(x, var):\n      return foo(x, var)\n\n    @def_function.function\n    def bar(var):\n      r = control_flow_ops.while_loop(\n          lambda i, _: i < 2,\n          lambda i, x: (i + 1, foo2(x, var)),\n          [0, 0.0])[1]\n      return gradients_impl.gradients(r, var)[0]\n\n    var = resource_variable_ops.ResourceVariable([1., 1., 1., 1.])\n    self.evaluate(variables.global_variables_initializer())\n    grad = self.evaluate(bar(var))\n    self.assertAllEqual(gradient_checker_v2._to_numpy(grad), [0., 2., 0., 2.])\n\n  def testWhileGrad_ResourceVarInLoopInFunctionCall(self):\n    if test.is_gpu_available():\n      self.skipTest(\"b/128635252\")\n\n    @def_function.function\n    def foo(x, var):\n      return control_flow_ops.while_loop(\n          lambda j, _: j < 3,\n          lambda j, y: (j + 1,\n                        y + math_ops.reduce_sum(var.sparse_read([1, 2]))),\n          [0, x])[1]\n\n    @def_function.function\n    def bar(var):\n      r = control_flow_ops.while_loop(\n          lambda i, _: i < 2,\n          lambda i, x: (i + 1, foo(x, var)),\n          [0, 0.0])[1]\n      return gradients_impl.gradients(r, var)[0]\n\n    var = resource_variable_ops.ResourceVariable([1., 1., 1., 1.])\n    self.evaluate(variables.global_variables_initializer())\n    grad = self.evaluate(bar(var))\n    self.assertAllEqual(gradient_checker_v2._to_numpy(grad), [0., 6., 6., 0.])\n\n  def testWhileCondGrad_ResourceVarInFunctionCall(self):\n\n    @def_function.function\n    def foo(x, var):\n      return x + var.sparse_read([1])[0]\n\n    def body(i, x):\n      return (i + 1, control_flow_ops.cond(\n          math_ops.equal(i % 2, 0),\n          lambda: foo(x, var1),\n          lambda: foo(x, var2)))\n\n    @def_function.function\n    def bar(var1, var2):\n      r = control_flow_ops.while_loop(\n          lambda i, _: i < 4, body, [0, 0.0])\n      return gradients_impl.gradients(r, [var1, var2])\n\n    var1 = resource_variable_ops.ResourceVariable([1., 2., 3.])\n    var2 = resource_variable_ops.ResourceVariable([4., 5.])\n    self.evaluate(variables.global_variables_initializer())\n    grads = self.evaluate(bar(var1, var2))\n    self.assertAllEqual(gradient_checker_v2._to_numpy(grads[0]), [0., 2., 0.])\n    self.assertAllEqual(gradient_checker_v2._to_numpy(grads[1]), [0., 2.])\n\n  @test_util.run_deprecated_v1\n  def testWhileGrad_ResourceVarSparseRead(self):\n    # NOTE(skyewm): this test is interesting because the gradient is the\n    # aggregation result of IndexedSlices and Tensors.\n    var = resource_variable_ops.ResourceVariable(np.ones(5),\n                                                 dtype=dtypes.float32)\n    r = control_flow_ops.while_loop(\n        lambda i, _: i < 3,\n        lambda i, x: (i + 1, x * math_ops.reduce_sum(var.sparse_read([1, 3]))),\n        [0, constant_op.constant(1.0)])[1]\n    grad = gradients_impl.gradients(r, var)[0]\n\n    self.evaluate(variables.global_variables_initializer())\n    grad_val = self.evaluate(grad)\n    arr = gradient_checker_v2._to_numpy(grad_val)\n    self.assertAllEqual(arr, [0., 12., 0., 12., 0.])\n\n  @test_util.run_deprecated_v1\n  def testWhileGrad_MultiResourceVarSparseRead(self):\n    # NOTE(skyewm): this test is interesting because the gradient is the\n    # aggregation result of IndexedSlices and Tensors.\n    var1 = resource_variable_ops.ResourceVariable(np.ones(5),\n                                                  dtype=dtypes.float32)\n    var2 = resource_variable_ops.ResourceVariable(np.ones(3),\n                                                  dtype=dtypes.float32)\n    x1_init = constant_op.constant([0., 0.])\n    x2_init = constant_op.constant(1.)\n    x3_init = constant_op.constant(1.)\n\n    def body(i, unused_x1, x2, x3):\n      y1 = var1.sparse_read([1, 3])\n      y2 = x2 * 2\n      y3 = x3 * math_ops.reduce_sum(var2.sparse_read([0]))\n      return i + 1, y1, y2, y3\n\n    r = control_flow_ops.while_loop(\n        lambda i, x1, x2, x3: i < 3, body,\n        [0, x1_init, x2_init, x3_init])[1:]\n    var1_grad, var2_grad = gradients_impl.gradients(r, [var1, var2])\n\n    self.evaluate(variables.global_variables_initializer())\n    var1_grad_val = self.evaluate(var1_grad)\n    var2_grad_val = self.evaluate(var2_grad)\n    self.assertAllEqual(gradient_checker_v2._to_numpy(var1_grad_val),\n                        [0., 1., 0., 1., 0.])\n    self.assertAllEqual(gradient_checker_v2._to_numpy(var2_grad_val),\n                        [3., 0., 0.])\n\n  def testWhileGrad_Gather(self):\n    # NOTE(skyewm): this test is interesting because the gather gradient\n    # function returns an IndexedSlices.\n    @tf_function_in_tf2\n    def fn():\n      x = constant_op.constant([1., 1., 1., 1., 1.])\n      y = control_flow_ops.while_loop(\n          lambda i, _: i < 3,\n          lambda i, x: (i + 1, x + array_ops.gather(x, [0])),\n          [0, x[:1]])[1]\n      z = y * 3.0\n      grad = gradients_impl.gradients(z, x)[0]\n      return y, grad\n    y, grad = fn()\n    self.assertEqual(self.evaluate(y), 8.)\n    self.assertAllEqual(self.evaluate(grad), [24., 0., 0., 0., 0.])\n\n  def testWhileGrad_GatherNoFanOut(self):\n    # NOTE(skyewm): this test is interesting because the gather gradient\n    # function returns an IndexedSlices.\n    @tf_function_in_tf2\n    def fn():\n      x = constant_op.constant([1., 1., 1., 1., 1.])\n      y = control_flow_ops.while_loop(\n          lambda i, _: i < 3,\n          lambda i, x: (i + 1, array_ops.gather(x, [0])),\n          [0, x[:1]])[1]\n      z = y * 3.0\n      grad = gradients_impl.gradients(z, x)[0]\n      return y, grad\n    y, grad = fn()\n    self.assertEqual(self.evaluate(y), 1.)\n    self.assertAllEqual(self.evaluate(grad), [3., 0., 0., 0., 0.])\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileGradInCond(self):\n\n    with self.cached_session():\n      n = ops.convert_to_tensor(1.0, name=\"n\")\n      x = array_ops.placeholder(dtypes.float32, shape=None)\n      c = lambda n: math_ops.less(n, 10.0)\n      b = lambda n: math_ops.add(n, x)\n\n      def fn1():\n        r = control_flow_ops.while_loop(c, b, [n],\n                                        [tensor_shape.unknown_shape()])\n        return gradients_impl.gradients(r, x)[0]\n\n      r = control_flow_ops.cond(math_ops.less(1, 2), fn1, lambda: x)\n      self.assertAllClose(9.0, r.eval(feed_dict={x: 1.0}))\n\n  @test_util.disable_control_flow_v2(\"b/116340060\")\n  @test_util.run_v1_only(\"b/120545219\")\n  def testGradInWhileWrtInitialLoopVal(self):\n    with self.cached_session():\n      x = array_ops.placeholder(dtypes.float32, shape=(), name=\"x\")\n      y = x + 1\n\n      def body(i, v):\n        z = v * 2\n        return i + 1, gradients_impl.gradients(z, x)[0]\n\n      with self.assertRaisesRegex(\n          ValueError,\n          \"Cannot compute gradient inside while loop with respect to op 'x'. \"\n          \"We do not support taking the gradient wrt or through the initial \"\n          \"value of a loop variable. Gradients can be computed through \"\n          \"loop invariants or wrt the input parameters to the loop body.\"):\n        control_flow_ops.while_loop(lambda i, x: i < 3, body, [0, y])\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileGradInWhile(self):\n    with self.cached_session():\n      n = ops.convert_to_tensor(1.0, name=\"n\")\n      x = array_ops.placeholder(dtypes.float32, shape=None)\n      c = lambda n: math_ops.less(n, 10.0)\n      b = lambda n: math_ops.add(n, x)\n\n      def b1(n):\n        r = control_flow_ops.while_loop(c, b, [n],\n                                        [tensor_shape.unknown_shape()])\n        return gradients_impl.gradients(r, x)\n\n      r = control_flow_ops.while_loop(lambda n: n < 6.0, b1, [n],\n                                      [tensor_shape.unknown_shape()])\n      self.assertAllClose(9.0, r.eval(feed_dict={x: 1.0}))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testCondGradInNestedWhiles(self):\n\n    def outer_body(i, x):\n      _, x = control_flow_ops.while_loop(\n          lambda j, x: j < 3, inner_body, [0, 0.0])\n      return i + 1, x\n\n    def inner_body(j, x):\n      y = control_flow_ops.cond(math_ops.less(x, 1), lambda: 2 * x, lambda: x)\n      return j + 1, gradients_impl.gradients(y, x)[0]\n\n    i, x = control_flow_ops.while_loop(lambda i, x: i < 3, outer_body, [0, 0.0])\n\n    with self.cached_session() as sess:\n      i_val, x_val = self.evaluate([i, x])\n      self.assertEqual(i_val, 3)\n      self.assertAllClose(x_val, 1.0)\n\n  @test_util.run_gpu_only\n  def testGpuResourceAccess(self):\n    with ops.device(test.gpu_device_name()):\n      var = resource_variable_ops.ResourceVariable(constant_op.constant(3.0))\n\n    @def_function.function\n    def foo():\n      return control_flow_ops.while_loop(\n          lambda i, _: i < 3,\n          lambda i, x: (i + 1, control_flow_ops.cond(\n              constant_op.constant(True),\n              lambda: x + var,\n              lambda: x)),\n          [0, 0.0])[1]\n\n    self.evaluate(variables.global_variables_initializer())\n    self.assertEqual(self.evaluate(foo()), 9.0)\n\n  def testNestedResourceAccess(self):\n    var = resource_variable_ops.ResourceVariable(constant_op.constant(3.0))\n\n    @eager_function.defun\n    def test_fn():\n      x = constant_op.constant(0.0)\n      r = control_flow_ops.while_loop(\n          # Outer loop condition\n          lambda i, y: i < 2,\n          # Outer loop body\n          lambda i, y: (i + 1, y + control_flow_ops.cond(\n              constant_op.constant(True),\n              # True branch\n              lambda: control_flow_ops.while_loop(\n                  # Inner loop condition\n                  lambda j, z: j < 3,\n                  # Inner loop body\n                  lambda j, z: (j + 1, z + math_ops.square(var)),\n                  # Inner initial loop value\n                  [0, y])[1],\n              # False branch\n              lambda: (0.0))),\n          # Outer initial loop value\n          [0, x])[1]\n\n      grad = gradients_impl.gradients(r, x)[0]\n      return r, grad\n\n    self.evaluate(variables.global_variables_initializer())\n    r, grad = self.evaluate(test_fn())\n    # 2 * 3 * 3^2\n    self.assertEqual(r, 81.0)\n    # v1 control flow gets the wrong answer!!!\n    # Gradient computation:\n    #   f(x) = x + 3^2\n    #   inner_loop(x) = f(f(f(x))) = x + 3*3^2 = x + 27\n    #   g(x) = x + inner_loop(x) = 2x + 27\n    #   outer_loop(x) = g(g(x)) = 4x + 81\n    #   outer_loop'(x) = 4\n    # Note that v1 control flow gets 4.0 as well if the cond is removed.\n    if control_flow_util.ENABLE_CONTROL_FLOW_V2:\n      self.assertEqual(grad, 4.0)\n\n  def testWhile_NestedInput(self):\n    with self.cached_session() as sess:\n      named = collections.namedtuple(\"named\", (\"a\", \"b\"))\n      loop_vars = [\n          named(a=constant_op.constant(0.0), b=constant_op.constant(1.0)),\n          (constant_op.constant(2.0), constant_op.constant(3.0)),\n          constant_op.constant(4.0)\n      ]\n      c = lambda lv0, _1, _2: lv0.a < 100.0\n\n      def b(lv0, lv1, lv2):\n        lv0 = named(a=lv0.a + 1, b=lv0.b)\n        lv1 = (lv1[0] + 1, lv1[1])\n        lv2 += 2\n        return [lv0, lv1, lv2]\n\n      r = control_flow_ops.while_loop(c, b, loop_vars)\n\n      self.assertTrue(isinstance(r, list))\n      self.assertTrue(isinstance(r[0], named))\n      self.assertTrue(isinstance(r[1], tuple))\n      self.assertTrue(isinstance(r[2], ops.Tensor))\n\n      r_flattened = nest.flatten(r)\n      self.assertEqual([100.0, 1.0, 102.0, 3.0, 4.0 + 100 * 2.0],\n                       self.evaluate(r_flattened))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhile_NestedBadArityFails(self):\n    with self.cached_session():\n      named = collections.namedtuple(\"named\", (\"a\", \"b\"))\n      loop_vars = [\n          named(a=constant_op.constant(0.0), b=constant_op.constant(1.0)),\n          (constant_op.constant(2.0), constant_op.constant(3.0)),\n          constant_op.constant(4.0)\n      ]\n      c = lambda lv0, _1, _2: lv0.a < 100.0\n\n      def b(lv0, lv1, _):\n        return [lv0, lv1]\n\n      with self.assertRaisesRegex(ValueError, \"the same number of elements\"):\n        control_flow_ops.while_loop(c, b, loop_vars)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileGrad_ys_xs(self):\n    with self.cached_session():\n      x = constant_op.constant(3.0, name=\"x\")\n      y = constant_op.constant(2.0, name=\"y\")\n\n      c = lambda x, y: math_ops.less(x, 100.0)\n\n      def b(x, y):\n        y1 = math_ops.add(x, y)\n        x1 = math_ops.multiply(x, y1)\n        return x1, y1\n\n      rx, ry = control_flow_ops.while_loop(c, b, [x, y], parallel_iterations=1)\n\n      r = gradients_impl.gradients([rx, ry], x)\n      self.assertAllClose(304.0, r[0])\n      r = gradients_impl.gradients([rx, ry], y)\n      self.assertAllClose(124.0, r[0])\n      r = gradients_impl.gradients([rx], x)\n      self.assertAllClose(295.0, r[0])\n      r = gradients_impl.gradients([rx], y)\n      self.assertAllClose(120.0, r[0])\n\n  @test_util.run_deprecated_v1\n  def testWhileGrad_Dependency(self):\n    with self.cached_session():\n      i = constant_op.constant(0, name=\"i\")\n      x = constant_op.constant(2.0, name=\"x\")\n\n      c = lambda i, x: math_ops.less(i, 10)\n\n      def b(i, x):\n        x = math_ops.multiply(x, 2.0)\n        i = math_ops.add(i, 1)\n        return i, x\n\n      ri, rx = control_flow_ops.while_loop(c, b, [i, x], parallel_iterations=1)\n\n      r = gradients_impl.gradients([ri, rx], x)\n      self.assertAllClose(1024.0, r[0])\n      r = gradients_impl.gradients([rx], x)\n      self.assertAllClose(1024.0, r[0])\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileGrad_NoGradient(self):\n    with self.cached_session():\n      v = constant_op.constant(2.0, name=\"v\")\n      c = lambda v: math_ops.less(v, 100.0)\n      b = math_ops.square\n      r = control_flow_ops.while_loop(c, b, [v], back_prop=False)\n      r = math_ops.add(r, v)\n      r = gradients_impl.gradients(r, v)\n      self.assertAllClose(1.0, r[0])\n\n  @test_util.disable_control_flow_v2(\"b/113324949 (RefVariable)\")\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileGrad_NoDependency(self):\n    with self.cached_session() as sess:\n      variable = variables.Variable(array_ops.ones([2, 3]))\n      duration = array_ops.zeros([], dtype=dtypes.int32)\n\n      def cond(duration, tensor, _):\n        del tensor\n        return duration < 10\n\n      def body(duration, tensor, _):\n        return (duration + 1, tensor, tensor)\n\n      loop_vars = [duration, variable, variable]\n      tensors = control_flow_ops.while_loop(\n          cond=cond, body=body, loop_vars=loop_vars)\n      cost = math_ops.reduce_sum(tensors[2])\n      grad = gradients_impl.gradients(cost, [variable])\n      self.evaluate(variables.global_variables_initializer())\n      self.assertAllClose(np.ones([2, 3]), sess.run(grad[0]))\n\n  @test_util.run_deprecated_v1\n  def testWhileGrad_Const(self):\n    with self.cached_session() as sess:\n      c0 = constant_op.constant(0.0, name=\"c0\")\n      c1 = constant_op.constant(1.0, name=\"c1\")\n      duration = constant_op.constant(0, name=\"t\")\n\n      def cond(duration, _):\n        return duration < 1\n\n      def body(duration, _):\n        return duration + 1, c1\n\n      loop_vars = [duration, c0]\n      tensors = control_flow_ops.while_loop(\n          cond=cond, body=body, loop_vars=loop_vars)\n      cost = math_ops.reduce_sum(tensors[1])\n      grad = gradients_impl.gradients(cost, [c0])\n      self.assertAllClose(0.0, sess.run(grad[0]))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileGrad_SerialTwoLoops(self):\n    with self.cached_session():\n      i = constant_op.constant(0, name=\"i\")\n      x = constant_op.constant(2.0, name=\"x\")\n\n      c = lambda i, x: math_ops.less(i, 5)\n\n      def b(i, x):\n        x = math_ops.multiply(x, 2.0)\n        i = math_ops.add(i, 1)\n        return i, x\n\n      _, rx = control_flow_ops.while_loop(c, b, [i, x], parallel_iterations=1)\n      _, rx = control_flow_ops.while_loop(c, b, [i, rx], parallel_iterations=1)\n\n      r = gradients_impl.gradients([rx], x)\n      self.assertAllClose(1024.0, r[0])\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileGrad_ParallelTwoLoops(self):\n    with self.cached_session():\n      i = constant_op.constant(0, name=\"i\")\n      x = constant_op.constant(2.0, name=\"x\")\n\n      c = lambda i, x: math_ops.less(i, 5)\n\n      def b(i, x):\n        x = math_ops.multiply(x, 2.0)\n        i = math_ops.add(i, 1)\n        return i, x\n\n      _, r1 = control_flow_ops.while_loop(c, b, [i, x], parallel_iterations=1)\n      _, r2 = control_flow_ops.while_loop(c, b, [i, x], parallel_iterations=1)\n      rx = math_ops.add(r1, r2)\n\n      r = gradients_impl.gradients([rx], x)\n      self.assertAllClose(64.0, r[0])\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileGrad_OneOutputWithControlDependencyOnSecond(self):\n    with self.cached_session():\n      i = constant_op.constant(0, name=\"i\")\n      x = constant_op.constant(1.0, name=\"x\")\n      y = constant_op.constant(1.0, name=\"y\")\n      c = lambda i, *_: math_ops.less(i, 1, name=\"cond_less\")\n\n      def b(i, xi, yi):\n        # return (i + 1, xi, xi + yi)\n        return (math_ops.add(i, 1, name=\"inc\"), array_ops.identity(\n            xi, name=\"xi\"), math_ops.add(xi, yi, name=\"xi_plus_yi\"))\n\n      _, x_f, y_f = control_flow_ops.while_loop(c, b, [i, x, y])\n      with ops.control_dependencies([x_f]):\n        y_f_d = array_ops.identity(y_f, name=\"y_f_d\")\n\n      self.assertAllClose(2.0, self.evaluate(y_f_d))  # y_f_d = 1.0 + 1.0\n      g = gradients_impl.gradients([y_f_d], [x])[0]\n      self.assertTrue(g is not None)\n      self.assertAllClose(1.0,\n                          self.evaluate(g))  # y_f_d = x + 1.0, dy_f_d/dx = 1.0\n\n  def _testNestedWhileGrad_Simple(self, use_gpu):\n    with self.cached_session(use_gpu=use_gpu):\n      v = constant_op.constant(1.0)\n\n      def inner_loop(s):\n        c = lambda x: math_ops.less(x, 4.0)\n        b = lambda x: math_ops.multiply(x, 2.0)\n        return control_flow_ops.while_loop(c, b, [s])\n\n      c = lambda x: math_ops.less(x, 2.0)\n      b = lambda x: math_ops.multiply(inner_loop(x), 2.0)\n      r = control_flow_ops.while_loop(c, b, [v])\n\n      r = gradients_impl.gradients(r, v)[0]\n      self.assertAllClose(8.0, self.evaluate(r))\n\n  @test_util.run_deprecated_v1\n  def testNestedWhileGrad_Simple(self):\n    self._testNestedWhileGrad_Simple(use_gpu=False)\n    self._testNestedWhileGrad_Simple(use_gpu=True)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testNestedWhileGrad_SerialInner(self):\n    with self.cached_session():\n      v = constant_op.constant(1.0)\n\n      def inner_loop1(s):\n        z = constant_op.constant(0)\n        c = lambda i, x: math_ops.less(i, 4)\n        b = lambda i, x: [math_ops.add(i, 1), math_ops.multiply(x, 2.0)]\n        return control_flow_ops.while_loop(c, b, [z, s])\n\n      def inner_loop2(s):\n        z = constant_op.constant(0)\n        c = lambda i, x: math_ops.less(i, 4)\n        b = lambda i, x: [math_ops.add(i, 1), math_ops.multiply(x, 2.0)]\n        return control_flow_ops.while_loop(c, b, [z, s])\n\n      c = lambda x: math_ops.less(x, 128.0)\n      b = lambda x: inner_loop2(inner_loop1(x)[1])[1]\n      r = control_flow_ops.while_loop(c, b, [v])\n\n      r = gradients_impl.gradients(r, v)[0]\n      self.assertAllClose(256.0, self.evaluate(r))\n\n  @test_util.run_deprecated_v1\n  def testNestedWhileGrad_ParallelInner(self):\n    with self.cached_session():\n      v = constant_op.constant(1.0)\n\n      def inner_loop1(s):\n        z = constant_op.constant(0)\n        c = lambda i, x: math_ops.less(i, 4)\n        b = lambda i, x: [math_ops.add(i, 1), math_ops.multiply(x, 2.0)]\n        return control_flow_ops.while_loop(c, b, [z, s])\n\n      def inner_loop2(s):\n        z = constant_op.constant(0)\n        c = lambda i, x: math_ops.less(i, 4)\n        b = lambda i, x: [math_ops.add(i, 1), math_ops.multiply(x, 2.0)]\n        return control_flow_ops.while_loop(c, b, [z, s])\n\n      c = lambda x: math_ops.less(x, 128.0)\n      b = lambda x: math_ops.multiply(inner_loop1(x)[1], inner_loop2(x)[1])\n      r = control_flow_ops.while_loop(c, b, [v])\n\n      r = gradients_impl.gradients(r, v)[0]\n      self.assertAllClose(512.0, self.evaluate(r))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testNestedWhileGrad_ParallelIterations(self):\n    # Make sure the stack pushes and pops of an inner loop are executed in\n    # the sequential order of the iterations of its outer loop.\n    with self.cached_session() as sess:\n\n      def inner_loop(t):\n        fn = lambda n: n + math_ops.square(var)\n        return map_fn.map_fn(fn=fn, elems=t, parallel_iterations=10)\n\n      def outer_loop(inp):\n        return map_fn.map_fn(\n            fn=inner_loop, elems=inp, parallel_iterations=10)\n\n      var = variables.Variable(constant_op.constant(3.0))\n      inp = constant_op.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n      res = outer_loop(inp)\n      optimizer = adam.AdamOptimizer(learning_rate=0.001)\n      train_op = optimizer.minimize(math_ops.reduce_mean(math_ops.square(res)))\n      self.evaluate(variables.global_variables_initializer())\n      self.evaluate(train_op)\n      self.assertAllClose(2.999, var.read_value())\n\n  def _testWhileCondGrad_Simple(self, use_gpu):\n    with self.cached_session(use_gpu=use_gpu):\n      v = ops.convert_to_tensor(2.0, name=\"v\")\n      n = ops.convert_to_tensor(100.0, name=\"n\")\n      one = ops.convert_to_tensor(1.0, name=\"one\")\n      c = lambda x: math_ops.less(x, n)\n      # pylint: disable=undefined-variable\n      # for OSS build\n      b = lambda x: control_flow_ops.cond(constant_op.constant(True),\n                                          lambda: math_ops.square(x),\n                                          lambda: math_ops.subtract(x, one))\n      # pylint: enable=undefined-variable\n      r = control_flow_ops.while_loop(c, b, [v])\n      r = gradients_impl.gradients(r, v)[0]\n      self.assertAllClose(1024.0, self.evaluate(r))\n\n  @test_util.run_deprecated_v1\n  def testWhileCondGrad_Simple(self):\n    self._testWhileCondGrad_Simple(use_gpu=False)\n    self._testWhileCondGrad_Simple(use_gpu=True)\n\n  @test_util.run_deprecated_v1\n  def testWhileCondGrad_UnknownShape(self):\n    with self.cached_session() as sess:\n      v = array_ops.placeholder(dtypes.float32)\n      n = ops.convert_to_tensor(100.0, name=\"n\")\n      one = ops.convert_to_tensor(1.0, name=\"one\")\n      c = lambda x: math_ops.less(x, n)\n      # pylint: disable=undefined-variable\n      # for OSS build\n      b = lambda x: control_flow_ops.cond(constant_op.constant(True),\n                                          lambda: math_ops.square(x),\n                                          lambda: math_ops.subtract(x, one))\n      # pylint: enable=undefined-variable\n      r = control_flow_ops.while_loop(c, b, [v])\n      r = gradients_impl.gradients(r, v)[0]\n      r = sess.run(r, feed_dict={v: 2.0})\n      self.assertAllClose(1024.0, r)\n\n  @test_util.run_deprecated_v1\n  def testWhileGrad_Concat(self):\n    with self.cached_session() as sess:\n      x = variable_scope.get_variable(\"x\", initializer=[[1., 2.]])\n      i0 = constant_op.constant(0)\n      h0 = array_ops.zeros([0, 2])\n\n      def condition(i, _):\n        return i < 2\n\n      def body(i, h):\n        return i + 1, array_ops.concat([h, x], 0)\n\n      _, h = control_flow_ops.while_loop(\n          condition, body, [i0, h0],\n          [i0.get_shape(), tensor_shape.TensorShape([None, 2])])\n      s = math_ops.reduce_sum(h)\n\n      optimizer = gradient_descent.GradientDescentOptimizer(0.01)\n      op = optimizer.minimize(s)\n\n      self.evaluate(variables.global_variables_initializer())\n      self.evaluate(op)\n      self.assertAllClose([[0.98000002, 1.98000002]], self.evaluate(x))\n\n  @test_util.disable_control_flow_v2(\"b/113324949 (RefVariable)\")\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileWithRefsWithGradients_1(self):\n    with self.cached_session() as sess:\n      x = variables.VariableV1(0.)._ref()  # pylint: disable=protected-access\n      i = constant_op.constant(0)\n      c = lambda i, x: math_ops.less(i, 10)\n\n      self.assertEqual(x.dtype, dtypes.float32_ref)\n\n      def body(i, x):\n        self.assertEqual(x.dtype, dtypes.float32_ref)\n        return [i + 1, gen_array_ops.ref_identity(x)]\n\n      r = control_flow_ops.while_loop(c, body, [i, x], parallel_iterations=5)\n\n      grad_ys = [variables.VariableV1(73)._ref()]  # pylint: disable=protected-access\n      grad = gradients_impl.gradients([r[1]], [x], grad_ys=grad_ys)\n\n      self.evaluate(variables.global_variables_initializer())\n\n      self.assertEqual(r[0].dtype, dtypes.int32)\n      self.assertEqual(r[1].dtype, dtypes.float32_ref)\n\n      value_i, value_x, value_x_grad = sess.run(r + grad)\n\n    self.assertEqual(10, value_i)\n    self.assertEqual(0, value_x)\n    self.assertEqual(73, value_x_grad)\n\n  @test_util.deprecated_graph_mode_only\n  def testWhileGrad_IndexedSlices(self):\n    with self.cached_session():\n      values = constant_op.constant([2.0, 4.0], name=\"values\")\n      indices = constant_op.constant([0, 3], name=\"indices\")\n      shape = constant_op.constant([10], name=\"dense_shape\")\n      i = constant_op.constant(0)\n      x = ops.IndexedSlices(values, indices, dense_shape=shape)\n\n      def c(i, _):\n        return i < 10\n\n      def b(i, x):\n        return [\n            i + 1,\n            ops.IndexedSlices(x.values * 2.0, x.indices, x.dense_shape)\n        ]\n\n      _, r = control_flow_ops.while_loop(c, b, [i, x])\n      r = gradients_impl.gradients(r.values, values)[0]\n      self.assertAllClose(np.array([1024.0, 1024.0]), self.evaluate(r))\n\n  @test_util.deprecated_graph_mode_only\n  def testWhileGrad_SparseTensor(self):\n    with self.cached_session():\n      values = constant_op.constant([2.0, 4.0], name=\"values\")\n      indices = constant_op.constant(\n          [[0], [3]], dtype=dtypes.int64, name=\"indices\")\n      shape = constant_op.constant([10], dtype=dtypes.int64, name=\"dense_shape\")\n      i = constant_op.constant(0)\n      x = sparse_tensor.SparseTensor(indices, values, dense_shape=shape)\n\n      def c(i, _):\n        return i < 10\n\n      def b(i, x):\n        return [\n            i + 1,\n            sparse_tensor.SparseTensor(x.indices, x.values * 2.0, x.dense_shape)\n        ]\n\n      _, r = control_flow_ops.while_loop(c, b, [i, x])\n      r = gradients_impl.gradients(r.values, values)[0]\n      self.assertAllClose(np.array([1024.0, 1024.0]), self.evaluate(r))\n\n  @test_util.deprecated_graph_mode_only\n  def testCallGradInLoop(self):\n    with self.cached_session() as sess:\n      i0 = constant_op.constant(0)\n      params = constant_op.constant(5.0)\n      params_1 = math_ops.square(params)\n\n      def c(i, _):\n        return i < 10\n\n      def b(i, x):\n        data = constant_op.constant([1.0, 2.0, 3.0])\n        data = math_ops.multiply(data, params_1)\n        x1 = x + gradients_impl.gradients(data, params)[0]\n        return i + 1, x1\n\n      output_grad = control_flow_ops.while_loop(\n          c, b, [i0, constant_op.constant(0.0)])\n      self.assertAllClose(600.0, self.evaluate(output_grad)[1])\n\n  @test_util.run_deprecated_v1\n  def testWhileAndTensorArray(self):\n    with self.cached_session() as sess:\n      param = constant_op.constant(2.0)\n      n0 = constant_op.constant(0)\n      y0 = constant_op.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], name=\"elems\")\n\n      def c(i, _):\n        return i < 10\n\n      def b(i, y):\n        return [\n            i + 1,\n            map_fn.map_fn(lambda x: math_ops.multiply(x, param), y)\n        ]\n\n      r = control_flow_ops.while_loop(c, b, [n0, y0], parallel_iterations=1)\n      r = gradients_impl.gradients(r, param)[0]\n      self.assertAllClose(107520.0, self.evaluate(r))\n\n  @test_util.run_deprecated_v1\n  def testNestedWhileAndTensorArray(self):\n    n = constant_op.constant(3.0)\n\n    def Body(row, ta):\n\n      def InnerBody(row, col, ta):\n        # Note: row and col are 1-based.\n        ta = ta.write(\n            math_ops.cast(n * (row - 1.) + col - 1., dtypes.int32), row * col)\n        return row, col + 1., ta\n\n      ta = control_flow_ops.while_loop(\n          lambda _, col, _1: col <= n,\n          InnerBody, [row, constant_op.constant(1.), ta],\n          return_same_structure=False)[2]\n      return row + 1., ta\n\n    ta = tensor_array_ops.TensorArray(dtype=dtypes.float32, size=9)\n    ta = control_flow_ops.while_loop(\n        lambda row, _: row <= n,\n        Body, [constant_op.constant(1.), ta],\n        return_same_structure=False)[1]\n\n    output = array_ops.reshape(ta.stack(), [3, 3])\n    self.assertAllEqual(\n        self.evaluate(output), [[1., 2., 3.], [2., 4., 6.], [3., 6., 9.]])\n    # TODO(b/117675481): This does not work with current TA. Enable with new TA.\n    # grad = gradients_impl.gradients(output, [n])\n    # self.assertEqual(self.evaluate(grad), 3.5)\n\n  @test_util.run_deprecated_v1\n  def testWhileGrad_StopGrad(self):\n    with self.cached_session():\n      x = constant_op.constant(3.0, name=\"x\")\n      y = constant_op.constant(2.0, name=\"y\")\n\n      c = lambda x, y: math_ops.less(x, 100.0)\n\n      def b(x, y):\n        y1 = math_ops.square(y)\n        x1 = math_ops.add(math_ops.square(x), y1)\n        return x1, y1\n\n      rx, ry = control_flow_ops.while_loop(c, b, [x, y])\n\n      r = gradients_impl.gradients(rx, y)[0]\n      self.assertEqual(136.0, self.evaluate(r))\n      r = gradients_impl.gradients(ry, y)[0]\n      self.assertEqual(32.0, self.evaluate(r))\n\n      r = gradients_impl.gradients(array_ops.stop_gradient(rx), y)[0]\n      self.assertEqual(r, None)\n      r = gradients_impl.gradients(array_ops.stop_gradient(ry), y)[0]\n      self.assertEqual(r, None)\n\n      r = gradients_impl.gradients(\n          array_ops.stop_gradient(math_ops.square(rx)), y)[0]\n      self.assertEqual(r, None)\n      r = gradients_impl.gradients(\n          array_ops.stop_gradient(math_ops.add(rx, ry)), x)[0]\n      self.assertEqual(r, None)\n      r = gradients_impl.gradients(\n          array_ops.stop_gradient(math_ops.add(rx, ry)), y)[0]\n      self.assertEqual(r, None)\n\n      r = gradients_impl.gradients(math_ops.add(rx, ry), y)[0]\n      self.assertEqual(168.0, self.evaluate(r))\n      r = gradients_impl.gradients(\n          math_ops.add(rx, array_ops.stop_gradient(ry)), y)[0]\n      self.assertEqual(136.0, self.evaluate(r))\n      r = gradients_impl.gradients(\n          math_ops.add(array_ops.stop_gradient(rx), ry), y)[0]\n      self.assertEqual(32.0, self.evaluate(r))\n\n  @test_util.run_deprecated_v1\n  def testWhileGrad_StopGradInside(self):\n    with self.cached_session():\n      x = constant_op.constant(3.0, name=\"x\")\n      y = constant_op.constant(2.0, name=\"y\")\n\n      c = lambda x, y: math_ops.less(x, 100.0)\n\n      def b(x, y):\n        y1 = array_ops.stop_gradient(math_ops.square(y))\n        x1 = math_ops.add(math_ops.square(x), y1)\n        return x1, y1\n\n      rx, _ = control_flow_ops.while_loop(c, b, [x, y])\n\n      r = gradients_impl.gradients(rx, y)[0]\n      self.assertAllClose(0.0, self.evaluate(r))\n      r = gradients_impl.gradients(rx, x)[0]\n      self.assertAllClose(156.0, self.evaluate(r))\n\n  @test_util.run_deprecated_v1\n  def testWhileGrad_StopGradInsideNoShape(self):\n    with self.cached_session() as sess:\n      x = array_ops.placeholder(dtypes.float32)\n      y = array_ops.placeholder(dtypes.float32)\n\n      c = lambda x, y: math_ops.less(math_ops.reduce_sum(x), 100.0)\n\n      def b(x, y):\n        y1 = array_ops.stop_gradient(math_ops.square(y, name=\"stopped\"))\n        x1 = math_ops.add(math_ops.square(x), y1)\n        return x1, y1\n\n      rx, _ = control_flow_ops.while_loop(c, b, [x, y])\n\n      grad_y = gradients_impl.gradients(rx, y)[0]\n      grad_x = gradients_impl.gradients(rx, x)[0]\n      feed_dict = {x: [3.0, 4.0], y: [2.0, 3.0]}\n      self.assertAllClose([0.0, 0.0], sess.run(grad_y, feed_dict=feed_dict))\n      self.assertAllClose([156.0, 400.0], sess.run(grad_x, feed_dict=feed_dict))\n      name = \"gradients/while/stopped_grad\"\n      all_ops = x.graph.get_operations()\n      self.assertFalse(any(name in op.name for op in all_ops))\n\n  @test_util.run_deprecated_v1\n  def testWhileGradGradFail(self):\n    theta = variables.Variable(initial_value=1.)\n\n    def fn(prev, x):\n      return prev + x * theta\n\n    result = functional_ops.scan(fn, np.array([1., 2., 3.], dtype=np.float32))\n    grad_theta = gradients_impl.gradients(result, theta)\n    if not control_flow_util.ENABLE_CONTROL_FLOW_V2:\n      with self.assertRaisesRegex(TypeError, \"Second-order gradient\"):\n        gradients_impl.gradients(grad_theta, theta)\n    grad_theta_stopped = array_ops.stop_gradient(grad_theta)\n    gradients_impl.gradients(grad_theta_stopped, theta)\n\n  @test_util.run_deprecated_v1\n  def testStopGradOnWhileGrad(self):\n    with self.cached_session():\n      x = constant_op.constant(2.0, name=\"x\")\n      y = constant_op.constant(2.0, name=\"y\")\n\n      c = lambda x: math_ops.less(x, 100.0)\n      b = lambda x: math_ops.multiply(x, y)\n      rx = control_flow_ops.while_loop(c, b, [x])\n\n      rg = gradients_impl.gradients(rx, y)[0]\n      rg = array_ops.stop_gradient(rg)\n      r = math_ops.add(math_ops.square(y), rx)\n      r = math_ops.add(r, rg)\n      r = gradients_impl.gradients(r, y)[0]\n      self.assertEqual(388.0, self.evaluate(r))\n\n  @test_util.disable_control_flow_v2(\"b/113324949 (RefVariable)\")\n  @test_util.run_deprecated_v1\n  def testWhileGradientWithNontrainablePath1(self):\n    q = variables.Variable([7., 8.])\n\n    def cond(_, y):\n      del y\n      return False\n\n    def body(x, _):\n      return x, math_ops.cast(x, dtypes.float32) + math_ops.reduce_sum(q)\n\n    _, y = control_flow_ops.while_loop(cond, body, (math_ops.argmin(q), 0.))\n    dy_dq, = gradients_impl.gradients(y, q)\n    self.assertIsNotNone(dy_dq)\n    with self.cached_session() as sess:\n      self.evaluate(q.initializer)\n      self.assertAllClose([0., 0.], self.evaluate(dy_dq))\n\n  @test_util.disable_control_flow_v2(\"b/113324949 (RefVariable)\")\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileGradientWithNontrainablePath2(self):\n    q = variables.Variable([7., 8.])\n\n    def cond(_, y):\n      return math_ops.equal(y, 0.)\n\n    def body(x, _):\n      zero = constant_op.constant(0, dtype=dtypes.int64)\n      return zero, math_ops.cast(x, dtypes.float32) + math_ops.reduce_sum(q)\n\n    _, y = control_flow_ops.while_loop(cond, body, (math_ops.argmin(q), 0.))\n    dy_dq, = gradients_impl.gradients(y, q)\n    self.assertIsNotNone(dy_dq)\n    with self.cached_session() as sess:\n      self.evaluate(q.initializer)\n      self.assertAllClose([1., 1.], self.evaluate(dy_dq))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testIssue16504(self):\n    c = constant_op.constant(np.arange(100), dtype=dtypes.float32)\n    w = variables.Variable(\n        initial_value=np.ones(100), dtype=dtypes.float32) / 100\n    k = variables.Variable(0, dtype=dtypes.int32)\n    chg_w = constant_op.constant(np.inf, dtype=dtypes.float32)\n\n    def cond(k, _, chg_w):\n      return math_ops.logical_and(k < 10, chg_w > 1e-3)\n\n    def body(k, w, chg_w):\n      grad, = gradients_impl.gradients(-math_ops.reduce_sum(w * c), w)\n      w_n = w * math_ops.exp(-0.1 * grad)\n      w_n /= math_ops.reduce_sum(w_n)\n      chg_w = (\n          math_ops.reduce_sum(math_ops.abs(w_n - w)) / math_ops.reduce_sum(\n              math_ops.abs(w)))\n      return k + 1, w_n, chg_w\n\n    _, w, _ = control_flow_ops.while_loop(cond, body, [k, w, chg_w])\n    grad, = gradients_impl.gradients(w, c)\n    self.assertIsNotNone(grad)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testStopGradMultiFlows(self):\n    with self.cached_session():\n\n      def body(i, y, r):\n        x = variable_scope.get_variable(\n            \"x\",\n            shape=(),\n            dtype=dtypes.float32,\n            initializer=init_ops.ones_initializer())\n        y *= x\n        return [i + 1, y, r + math_ops.reduce_sum(y)]\n\n      i0 = constant_op.constant(0)\n      y0 = array_ops.ones(5)\n      r0 = constant_op.constant(0.0)\n      cond = lambda i, y, r: i < 1\n      _, _, r = control_flow_ops.while_loop(\n          cond, body, [i0, y0, r0], back_prop=True)\n\n      vars_ = variables.global_variables()\n      grads = linalg_ops.norm(gradients_impl.gradients(r, vars_)[0])\n      z = math_ops.add(r, array_ops.stop_gradient(math_ops.reduce_sum(grads)))\n      result = gradients_impl.gradients(z, vars_)[0]\n      self.evaluate(variables.global_variables_initializer())\n      self.assertEqual(5.0, self.evaluate(result))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testOneValueCond(self):\n\n    with self.cached_session():\n      c = array_ops.placeholder(dtypes.int32, shape=[])\n      one = ops.convert_to_tensor(1, name=\"one\")\n      two = ops.convert_to_tensor(2, name=\"two\")\n      p = math_ops.greater_equal(c, 1)\n      i = control_flow_ops.cond(p, lambda: one, lambda: two)\n      self.assertTrue(isinstance(i, ops.Tensor))\n\n      # True case: c = 2 is >= 1\n      self.assertEqual([1], i.eval(feed_dict={c: 2}))\n\n      # False case: c = 0 is not >= 1\n      self.assertEqual([2], i.eval(feed_dict={c: 0}))\n\n  @test_util.run_deprecated_v1\n  def testExampleCond(self):\n\n    with self.cached_session():\n      x = ops.convert_to_tensor([-2.0, 2.0], name=\"x\")\n      d = array_ops.placeholder(dtypes.int32, shape=[])\n\n      def l2():\n        return math_ops.sqrt(math_ops.reduce_sum(math_ops.square(x)))\n\n      def l1():\n        return math_ops.reduce_sum(math_ops.abs(x))\n\n      i = control_flow_ops.cond(math_ops.equal(d, 2), l2, l1)\n      self.assertAllClose(4.0, i.eval(feed_dict={d: 1}))\n      self.assertAllClose(2.0 * math.sqrt(2), i.eval(feed_dict={d: 2}))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testCase(self):\n    with self.cached_session():\n      x = constant_op.constant(1)\n      y = constant_op.constant(2)\n      z = constant_op.constant(3)\n      f1 = lambda: constant_op.constant(17)\n      f2 = lambda: constant_op.constant(23)\n      f3 = lambda: constant_op.constant(-1)\n\n      r1 = control_flow_ops.case(\n          {\n              x < y: f1,\n              x > z: f2\n          }, default=f3, exclusive=True)\n      self.assertAllEqual(r1, 17)\n\n      r2 = control_flow_ops.case([(y > z, f1), (y > x, f2)], default=f3)\n      self.assertAllEqual(r2, 23)\n\n      # Duplicate events can happen, first one is selected\n      r3 = control_flow_ops.case([(x < y, f1), (x < y, f2)], default=f3)\n      self.assertAllEqual(r3, 17)\n\n      # Duplicate events cause an error if exclusive = True\n      r4 = control_flow_ops.case(\n          [(x < y, f1), (x < y, f2)], default=f3, exclusive=True)\n      with self.assertRaisesOpError(\"Input error:\"):\n        self.evaluate(r4)\n\n      # Check that the default is called if none of the others are\n      r5 = control_flow_ops.case({x > y: f1}, default=f3)\n      self.assertAllEqual(r5, -1)\n\n      ran_once = [False, False, False]\n\n      def break_run_twice(ix):\n\n        def _break():\n          ran_once[ix] = True\n          return constant_op.constant(ix)\n\n        return _break\n\n      # Should not fail - each conditional gets called exactly once\n      # except default.  Default gets called twice: once to create an\n      # empty output and once for the actual cond switch.\n      r6 = control_flow_ops.case(\n          [(x < y, break_run_twice(0)), (x > y, break_run_twice(1))],\n          default=lambda: constant_op.constant(2))\n\n      self.assertAllEqual(r6, 0)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testCaseSideEffects(self):\n    with self.cached_session() as sess:\n      v0 = variables.Variable(-1)\n      v1 = variables.Variable(-1)\n      v2 = variables.Variable(-1)\n\n      a = lambda: control_flow_ops.with_dependencies([state_ops.assign(v0, 0)], 0)\n      b = lambda: control_flow_ops.with_dependencies([state_ops.assign(v1, 1)], 1)\n      c = lambda: control_flow_ops.with_dependencies([state_ops.assign(v2, 2)], 2)\n\n      x = constant_op.constant(1)\n      y = constant_op.constant(2)\n\n      r0 = control_flow_ops.case(\n          ((x < y, a), (x > y, b)), default=c, exclusive=True)\n      r1 = control_flow_ops.case(\n          ((x > y, a), (x < y, b)), default=c, exclusive=True)\n      r2 = control_flow_ops.case(\n          ((x > y, a), (x > y, b)), default=c, exclusive=True)\n\n      self.evaluate(variables.global_variables_initializer())\n      self.assertAllEqual(self.evaluate([v0, v1, v2]), [-1] * 3)\n      self.assertEqual(2, self.evaluate(r2))\n      self.assertAllEqual(self.evaluate([v0, v1, v2]), [-1, -1, 2])\n\n      self.evaluate(variables.global_variables_initializer())\n      self.assertAllEqual(self.evaluate([v0, v1, v2]), [-1] * 3)\n      self.assertEqual(1, self.evaluate(r1))\n      self.assertAllEqual(self.evaluate([v0, v1, v2]), [-1, 1, -1])\n\n      self.evaluate(variables.global_variables_initializer())\n      self.assertAllEqual(self.evaluate([v0, v1, v2]), [-1] * 3)\n      self.assertEqual(0, self.evaluate(r0))\n      self.assertAllEqual(self.evaluate([v0, v1, v2]), [0, -1, -1])\n\n  @test_util.disable_control_flow_v2(\"b/113324949 (ref vars)\")\n  @test_util.run_v1_only(\"b/120545219\")\n  def testOneOpCond(self):\n    with self.cached_session():\n      v = variables.Variable(0)\n      c = ops.convert_to_tensor(0)\n      one = ops.convert_to_tensor(1)\n      two = ops.convert_to_tensor(2)\n      p = math_ops.greater_equal(c, 1)\n\n      def a():\n        return state_ops.assign(v, one)\n\n      def b():\n        return state_ops.assign(v, two)\n\n      i = control_flow_ops.cond(p, a, b)\n      self.assertTrue(isinstance(i, ops.Tensor))\n      self.evaluate(variables.global_variables_initializer())\n\n      self.assertEqual(0, self.evaluate(v))\n\n      # True case: c = 2 is >= 1, v is set to 1.\n      self.assertEqual(1, i.eval(feed_dict={c.name: 2}))\n      self.assertEqual(1, self.evaluate(v))\n\n      # False case: c = 0 is not >= 1, v is set to 2.\n      self.assertEqual(2, i.eval(feed_dict={c.name: 0}))\n      self.assertEqual(2, self.evaluate(v))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWithOpsDependencies(self):\n    with self.cached_session() as sess:\n      v = variables.VariableV1(0.0)\n      c = constant_op.constant(10)\n\n      # Fetching v directly will result in an uninitialized error\n      with self.assertRaisesOpError(\"Attempting to use uninitialized value\"):\n        self.evaluate([c, v])\n\n      # Use a control dependency to ensure init_variable is run\n      # while asking for c\n      real_v = control_flow_ops.with_dependencies(\n          name=\"real_tensor\",\n          output_tensor=v._ref(),  # pylint: disable=protected-access\n          dependencies=[v.initializer])\n      c_val, real_v_val = self.evaluate([c, real_v])\n\n    # Ensure the result of 'real_c' is the same as 'c'\n    self.assertAllEqual(10, c_val)\n\n    # Ensure that 'v' is initialized\n    self.assertAllClose(0.0, real_v_val)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWithTensorDependencies(self):\n    with self.cached_session():\n      v = variables.VariableV1(0.0)\n      c1 = constant_op.constant(10)\n      c2 = constant_op.constant(20)\n\n      # c1_with_init_v depends on the init op for v\n      c1_with_init_v = control_flow_ops.with_dependencies(\n          name=\"c1_with_init_v\", output_tensor=c1, dependencies=[v.initializer])\n      # c2_with_c1 depends on the value of c1_with_init_v\n      c2_with_c1_dep = control_flow_ops.with_dependencies(\n          name=\"c2_with_c1_dep\",\n          output_tensor=c2,\n          dependencies=[c1_with_init_v])\n\n      # Fetching v directly will result in an uninitialized error\n      with self.assertRaisesOpError(\"Attempting to use uninitialized value\"):\n        self.evaluate(v)\n\n      # Get the value of 'c2_with_c1_dep', which should cause 'v'\n      # to be initialized.\n      self.assertAllEqual(20, self.evaluate(c2_with_c1_dep))\n\n      # Ensure that 'v' is initialized\n      self.assertAllClose(0.0, self.evaluate(v))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWithIndexedSlicesDependencies(self):\n    with self.cached_session():\n      v = variables.VariableV1(\n          np.array([[0.0, 1.0], [10.0, 11.0], [20.0, 21.0]]).astype(np.float32))\n      v_at_1 = ops.IndexedSlices(v, constant_op.constant([1]))\n      gather_v_at_1 = array_ops.gather(v_at_1.values, v_at_1.indices)\n      v_at_1_after_init = control_flow_ops.with_dependencies([v.initializer],\n                                                             v_at_1)\n      gather_v_at_1_after_init = array_ops.gather(v_at_1_after_init.values,\n                                                  v_at_1_after_init.indices)\n\n      # Fetching gather_v_at_1 will result in an uninitialized error\n      with self.assertRaisesOpError(\"Attempting to use uninitialized value\"):\n        self.evaluate(gather_v_at_1)\n\n      # Getting gather_v_at_1_after_init will work, and initialize v.\n      self.assertAllEqual([[10.0, 11.0]],\n                          self.evaluate(gather_v_at_1_after_init))\n\n      # Double check that 'v' is initialized\n      self.assertAllClose([[0.0, 1.0], [10.0, 11.0], [20.0, 21.0]],\n                          self.evaluate(v))\n\n  def testDependenciesDevice(self):\n    with ops.Graph().as_default():\n      # device set on tensor => same device on dep.\n      with ops.device(\"/job:ps\"):\n        vd = variables.VariableV1([0.0])\n      with_vd_dep = control_flow_ops.with_dependencies([vd.initializer], vd)\n      self.assertTrue(\"/job:ps\" in with_vd_dep.device)\n\n      # No device set on tensor => no device on dep.\n      vnod = variables.VariableV1([0.0])\n      with_vnod_dep = control_flow_ops.with_dependencies([vnod.initializer],\n                                                         vnod)\n      self.assertDeviceEqual(None, with_vnod_dep.device)\n\n      # device set on tensor, default device on graph => default device on dep.\n      vdef = variables.VariableV1([0.0], name=\"vdef\")\n      with ops.device(\"/job:worker/device:GPU:1\"):\n        with_vdef_dep = control_flow_ops.with_dependencies([vdef.initializer],\n                                                           vdef)\n        # The device is empty, but the colocation constraint is set.\n        self.assertDeviceEqual(\"\", with_vdef_dep.device)\n        self.assertEqual([b\"loc:@vdef\"], with_vdef_dep.op.colocation_groups())\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testGroup(self):\n    with self.cached_session() as sess:\n      v1 = variables.VariableV1([0.0])\n      v2 = variables.VariableV1([1.0])\n\n      # Group init1 and init2 and run.\n      init = control_flow_ops.group(v1.initializer, v2.initializer)\n      # Fetching v1 directly will result in an uninitialized error\n      with self.assertRaisesOpError(\"Attempting to use uninitialized value\"):\n        self.evaluate(v1)\n\n      # Runs \"init\" before fetching v1 and v2.\n      init.run()\n      v1_val, v2_val = self.evaluate([v1, v2])\n\n    # Ensure that v1 and v2 are initialized\n    self.assertAllClose([0.0], v1_val)\n    self.assertAllClose([1.0], v2_val)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testGroupEmpty(self):\n    op = control_flow_ops.group()\n    self.assertEqual(op.type, \"NoOp\")\n    self.assertEqual(op.control_inputs, [])\n\n  @test_util.run_deprecated_v1\n  def testMergeShapes(self):\n    # All inputs unknown.\n    p1 = array_ops.placeholder(dtypes.float32)\n    p2 = array_ops.placeholder(dtypes.float32)\n    p3 = array_ops.placeholder(dtypes.float32)\n    m, index = control_flow_ops.merge([p1, p2, p3])\n    self.assertIs(None, m.get_shape().ndims)\n    self.assertEqual([], index.get_shape())\n\n    # All inputs known with different ranks.\n    p1 = array_ops.placeholder(dtypes.float32, shape=[1, 2])\n    p2 = array_ops.placeholder(dtypes.float32, shape=[1, 2, 3])\n    m, index = control_flow_ops.merge([p1, p2])\n    self.assertIs(None, m.get_shape().ndims)\n    self.assertEqual([], index.get_shape())\n\n    # All inputs known with some dimensions different.\n    p1 = array_ops.placeholder(dtypes.float32, shape=[1, 2])\n    p2 = array_ops.placeholder(dtypes.float32, shape=[2, 1])\n    m, index = control_flow_ops.merge([p1, p2])\n    self.assertEqual([None, None], m.get_shape().as_list())\n    self.assertEqual([], index.get_shape())\n\n    p1 = array_ops.placeholder(dtypes.float32, shape=[1, 2])\n    p2 = array_ops.placeholder(dtypes.float32, shape=[None, 2])\n    m, index = control_flow_ops.merge([p1, p2])\n    self.assertEqual([None, 2], m.get_shape().as_list())\n    self.assertEqual([], index.get_shape())\n\n    p1 = array_ops.placeholder(dtypes.float32, shape=[1, 2])\n    p2 = array_ops.placeholder(dtypes.float32, shape=[2, 2])\n    m, index = control_flow_ops.merge([p1, p2])\n    self.assertEqual([None, 2], m.get_shape().as_list())\n    self.assertEqual([], index.get_shape())\n\n    # All inputs known with same dimensions.\n    p1 = array_ops.placeholder(dtypes.float32, shape=[1, 2])\n    p2 = array_ops.placeholder(dtypes.float32, shape=[1, 2])\n    m, index = control_flow_ops.merge([p1, p2])\n    self.assertEqual([1, 2], m.get_shape().as_list())\n    self.assertEqual([], index.get_shape())\n\n    p1 = array_ops.placeholder(dtypes.float32, shape=[None, 2])\n    p2 = array_ops.placeholder(dtypes.float32, shape=[None, 2])\n    m, index = control_flow_ops.merge([p1, p2])\n    self.assertEqual([None, 2], m.get_shape().as_list())\n    self.assertEqual([], index.get_shape())\n\n    p1 = array_ops.placeholder(dtypes.float32, shape=[None, None])\n    p2 = array_ops.placeholder(dtypes.float32, shape=[None, None])\n    m, index = control_flow_ops.merge([p1, p2])\n    self.assertEqual([None, None], m.get_shape().as_list())\n    self.assertEqual([], index.get_shape())\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testRefSelect(self):\n    index = array_ops.placeholder(dtypes.int32)\n\n    # All inputs unknown.\n    p1 = array_ops.placeholder(dtypes.float32)\n    p2 = array_ops.placeholder(dtypes.float32)\n    p3 = array_ops.placeholder(dtypes.float32)\n    v1 = variables.VariableV1(p1, validate_shape=False)\n    v2 = variables.VariableV1(p2, validate_shape=False)\n    v3 = variables.VariableV1(p3, validate_shape=False)\n    self.assertIs(None, v1.get_shape().ndims)\n    s = control_flow_ops.ref_select(index, [v1, v2, v3])\n    self.assertIs(None, s.get_shape().ndims)\n\n    # All inputs known but different.\n    v1 = variables.VariableV1([[1, 2]])\n    v2 = variables.VariableV1([[2], [1]])\n    s = control_flow_ops.ref_select(index, [v1, v2])\n    self.assertIs(None, s.get_shape().ndims)\n\n    # All inputs known and same.\n    v1 = variables.VariableV1([[1, 2]])\n    v2 = variables.VariableV1([[1, 2]])\n    s = control_flow_ops.ref_select(index, [v1, v2])\n    self.assertEqual([1, 2], s.get_shape())\n\n    # Possibly the same but not guaranteed.\n    v1 = variables.VariableV1([[1., 2.]])\n    p2 = array_ops.placeholder(dtypes.float32, shape=[None, 2])\n    v2 = variables.VariableV1(p2, validate_shape=False)\n    s = control_flow_ops.ref_select(index, [v1, v2])\n    self.assertEqual(None, s.get_shape())\n\n  @test_util.run_deprecated_v1\n  def testRunLoopTensor(self):\n    with self.cached_session() as sess:\n      tensor_list = []\n\n      def condition(t):\n        return t < constant_op.constant(5)\n\n      def body(_):\n        tensor_list.append(constant_op.constant(5))\n        return constant_op.constant(10)\n\n      result = control_flow_ops.while_loop(condition, body,\n                                           [constant_op.constant(4)])\n      self.assertEqual(10, self.evaluate(result))\n\n      # Ensure that we cannot run a tensor that escapes the loop body\n      # accidentally.\n      with self.assertRaises(ValueError):\n        sess.run(tensor_list[0])\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhilePyFuncBasic(self):\n\n    def func(x):\n      return np.square(x)\n\n    with self.cached_session():\n      r = control_flow_ops.while_loop(\n          lambda i, v: i < 4,\n          lambda i, v: [i + 1, script_ops.py_func(func, [v], [dtypes.float32])[0]],\n          [constant_op.constant(0), constant_op.constant(2.0, dtypes.float32)],\n          [tensor_shape.unknown_shape(), tensor_shape.unknown_shape()])\n      self.assertEqual(self.evaluate(r[1]), 65536.0)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileFuncBasic(self):\n\n    @function.Defun(dtypes.float32)\n    def func(x):\n      return math_ops.square(math_ops.square(x))\n\n    with self.cached_session():\n      x = constant_op.constant(2.0, dtypes.float32)\n      r = control_flow_ops.while_loop(\n          lambda i, v: i < 2, lambda i, v: [i + 1, func(v)],\n          [constant_op.constant(0), x],\n          [tensor_shape.unknown_shape(),\n           tensor_shape.unknown_shape()])\n      grad = gradients_impl.gradients(r, x)[0]\n      self.assertEqual(self.evaluate(r[1]), 65536.0)\n      self.assertEqual(self.evaluate(grad), 524288.0)\n      # while_v2 does not have stacks.\n      if not control_flow_util.ENABLE_CONTROL_FLOW_V2:\n        self.assertEqual(\n            len([op for op in x.graph.get_operations() if op.type == \"StackV2\"\n                ]), 1)\n\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testQIntSwitchMerge(self):\n    with self.cached_session(force_gpu=test.is_gpu_available()) as sess:\n      constant_qint = constant_op.constant(np.array([42]), dtypes.qint8)\n      cond = constant_op.constant(True, dtypes.bool)\n      v_f, v_t = control_flow_ops.switch(constant_qint, cond)\n      result = control_flow_ops.merge([v_f, v_t])\n      self.evaluate(result)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testQIntRefSwitchMerge(self):\n    with self.cached_session(use_gpu=test.is_gpu_available()) as sess:\n      var_qint = gen_state_ops.variable(\n          shape=[1], dtype=dtypes.qint8, name=\"v\", container=\"\", shared_name=\"\")\n      assign_op = state_ops.assign(\n          var_qint, constant_op.constant(np.array([42]), dtypes.qint8))\n      self.evaluate(assign_op)\n\n      cond = constant_op.constant(True, dtypes.bool)\n      v_f, v_t = control_flow_ops.ref_switch(var_qint, cond)\n      result = control_flow_ops.ref_merge([v_f, v_t])\n      self.evaluate(result)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testUInt64SwitchMerge(self):\n    with self.cached_session(force_gpu=test.is_gpu_available()) as sess:\n      constant_uint64 = constant_op.constant(np.array([42]), dtypes.uint64)\n      cond = constant_op.constant(True, dtypes.bool)\n      v_f, v_t = control_flow_ops.switch(constant_uint64, cond)\n      result = control_flow_ops.merge([v_f, v_t])\n      self.evaluate(result)\n\n  def testSwitchEagerMode(self):\n    if not context.executing_eagerly():\n      return\n    input_data = [1, 2, 3, 4]\n    vf, vt = control_flow_ops.switch(input_data, False)\n    self.assertAllEqual(vf, input_data)\n    self.assertAllEqual(vt, [])\n\n  @test_util.run_deprecated_v1\n  def testQIntArgAndRet(self):\n\n    @function.Defun(dtypes.qint8)\n    def func(x):\n      return x\n\n    with self.cached_session(force_gpu=test.is_gpu_available()) as sess:\n      qint = constant_op.constant(np.array([42]), dtypes.qint8)\n      result = func(qint)\n      self.evaluate(result)\n\n  def testSparseIdentity(self):\n    st1 = sparse_tensor.SparseTensor([[0, 5]], ['x'], [10, 10])\n    st2 = control_flow_ops._Identity(st1)\n    self.assertAllEqual(st1.indices, st2.indices)\n    self.assertAllEqual(st1.values, st2.values)\n    self.assertAllEqual(st1.dense_shape, st2.dense_shape)\n\n  def testSparseEnterExit(self):\n    st1 = sparse_tensor.SparseTensor([[0, 5]], ['x'], [10, 10])\n    st2 = control_flow_ops._Enter(st1, \"foo_1\")\n    st3 = control_flow_ops.exit(st2)\n    self.assertAllEqual(st1.indices, st3.indices)\n    self.assertAllEqual(st1.values, st3.values)\n    self.assertAllEqual(st1.dense_shape, st3.dense_shape)\n\n  def _buildWhileWithShapeInvariants(self, shape_invariants):\n    r = constant_op.constant([1, 2])\n\n    def cond(_):\n      return False\n\n    def body(_):\n      return constant_op.constant([1])\n\n    return control_flow_ops.while_loop(\n        cond, body, [r], shape_invariants=shape_invariants)\n\n  def testWhileOutputShapeWithShapeInvariantsUnknownRank(self):\n    @def_function.function\n    def runTest():\n      while_output = self._buildWhileWithShapeInvariants(\n          [tensor_shape.TensorShape(None)])\n      self.assertIsNone(while_output.shape.rank)\n    runTest()\n\n  def testWhileOutputShapeWithShapeInvariantsPartialShape(self):\n    @def_function.function\n    def runTest():\n      while_output = self._buildWhileWithShapeInvariants(\n          [tensor_shape.TensorShape([None])])\n      self.assertAllEqual(while_output.shape.as_list(), [None])\n    runTest()\n\n  def testFunctionInWhile(self):\n\n    @def_function.function\n    def body(x):\n      return x + 1\n\n    r = control_flow_ops.while_loop(lambda x: x < 5, body, [0])\n    self.assertAllEqual(r, 5.)\n\n\nclass ControlFlowContextCheckTest(test.TestCase):\n\n  def _getWhileTensor(self):\n    \"\"\"Creates and returns a tensor from a while context.\"\"\"\n    tensor = []\n\n    def body(i):\n      if not tensor:\n        tensor.append(constant_op.constant(1))\n      return i + tensor[0]\n\n    control_flow_ops.while_loop(lambda i: i < 10, body, [0])\n    return tensor[0]\n\n  def _getCondTensor(self):\n    cond_tensor = []\n\n    def true_fn():\n      if not cond_tensor:\n        cond_tensor.append(constant_op.constant(1))\n      return cond_tensor[0]\n\n    control_flow_ops.cond(\n        math_ops.less(1, 2), true_fn, lambda: constant_op.constant(0))\n    return cond_tensor[0]\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testInvalidContext(self):\n    # Accessing a while loop tensor outside of control flow is illegal.\n    while_tensor = self._getWhileTensor()\n    with self.assertRaisesRegex(\n        ValueError,\n        \"Cannot use 'while/Const_1' as input to 'Add' because 'while/Const_1' \"\n        \"is in a while loop. See info log for more details.\"):\n      math_ops.add(1, while_tensor)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testInvalidContextInCond(self):\n    # Accessing a while loop tensor in cond is illegal.\n    while_tensor = self._getWhileTensor()\n    with self.assertRaisesRegex(\n        ValueError, \"Cannot use 'while/Const_1' as input to 'cond/Add' because \"\n        \"'while/Const_1' is in a while loop. See info log for more details.\"):\n      # TODO(skyewm): this passes if we return while_tensor directly instead\n      # of using it as input to another op.\n      control_flow_ops.cond(\n          math_ops.less(1, 2), lambda: math_ops.add(1, while_tensor),\n          lambda: constant_op.constant(0))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testInvalidContextInWhile(self):\n    # Accessing a while loop tensor in a different while loop is illegal.\n    while_tensor = self._getWhileTensor()\n    with self.assertRaisesRegex(\n        ValueError,\n        \"Cannot use 'while/Const_1' as input to 'while_1/Add' because they are \"\n        \"in different while loops. See info log for more details.\"):\n      control_flow_ops.while_loop(lambda i: i < 10,\n                                  lambda x: math_ops.add(1, while_tensor), [0])\n\n    with self.assertRaisesRegex(\n        ValueError,\n        \"Cannot use 'while/Const_1' as input to 'while_2/NextIteration' \"\n        \"because they are in different while loops. See info log for more \"\n        \"details.\"):\n      control_flow_ops.while_loop(lambda i: i < 10, lambda i: while_tensor, [0])\n\n  def testValidCondContext(self):\n    # Accessing a tensor from a cond context is OK (although dangerous).\n    cond_tensor = self._getCondTensor()\n    math_ops.add(1, cond_tensor)\n\n  def testValidCondContextBranches(self):\n    # Accessing a tensor from a cond context from the other branch's cond\n    # context is OK (although dangerous).\n    cond_tensor = []\n\n    def branch_fn():\n      if not cond_tensor:\n        cond_tensor.append(constant_op.constant(1))\n      return cond_tensor[0]\n\n    control_flow_ops.cond(math_ops.less(1, 2), branch_fn, branch_fn)\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testValidWhileContext(self):\n    # Accessing a tensor in a nested while is OK.\n    def body(_):\n      c = constant_op.constant(1)\n      return control_flow_ops.while_loop(lambda i: i < 3, lambda i: i + c, [0])\n\n    control_flow_ops.while_loop(lambda i: i < 5, body, [0])\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testValidNestedContexts(self):\n    # Accessing a tensor from a cond context in a while context, all inside an\n    # outer while context, is OK.\n    def body(_):\n      cond_tensor = self._getCondTensor()\n      # Create another cond containing the while loop for good measure\n      return control_flow_ops.cond(\n          math_ops.less(1, 2),\n          lambda: control_flow_ops.while_loop(lambda i: i < 3,\n                                              lambda i: i + cond_tensor, [0]),\n          lambda: constant_op.constant(0))\n\n    control_flow_ops.while_loop(lambda i: i < 5, body, [0])\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testInvalidNestedContexts(self):\n    # Accessing a tensor from a while context in a different while context, all\n    # inside a cond context, is illegal.\n    def true_fn():\n      while_tensor = self._getWhileTensor()\n      return control_flow_ops.while_loop(lambda i: i < 3,\n                                         lambda i: i + while_tensor, [0])\n\n    with self.assertRaisesRegex(\n        ValueError,\n        \"Cannot use 'cond/while/Const_1' as input to 'cond/while_1/add' because\"\n        \" they are in different while loops. See info log for more details.\"):\n      control_flow_ops.cond(\n          math_ops.less(1, 2), true_fn, lambda: constant_op.constant(0))\n\n\nclass TupleTest(test.TestCase):\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testTensors(self):\n    for v1_first in [True, False]:\n      with self.cached_session():\n        v1 = variables.VariableV1([1.0])\n        add1 = math_ops.add(\n            control_flow_ops.with_dependencies([v1.initializer], v1._ref()),  # pylint: disable=protected-access\n            2.0)\n        v2 = variables.VariableV1([10.0])\n        add2 = math_ops.add(\n            control_flow_ops.with_dependencies([v2.initializer], v2._ref()),  # pylint: disable=protected-access\n            20.0)\n        t1, _, t2 = control_flow_ops.tuple([add1, None, add2])\n\n        # v1 is not initialized.\n        with self.assertRaisesOpError(\"Attempting to use uninitialized value\"):\n          self.evaluate(v1)\n\n        # v2 is not initialized.\n        with self.assertRaisesOpError(\"Attempting to use uninitialized value\"):\n          self.evaluate(v2)\n\n        if v1_first:\n          # Getting t1 initializes v2.\n          self.assertAllClose([3.0], self.evaluate(t1))\n          self.assertAllClose([10.0], self.evaluate(v2))\n        else:\n          # Getting t2 initializes v1.\n          self.assertAllClose([30.0], self.evaluate(t2))\n          self.assertAllClose([1.0], self.evaluate(v1))\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testIndexedSlices(self):\n    for v1_first in [True, False]:\n      with self.cached_session():\n        v1 = variables.VariableV1(\n            np.array([[0.0, 1.0], [10.0, 11.0], [20.0, 21.0]]).astype(\n                np.float32))\n        v1_at_1 = ops.IndexedSlices(\n            control_flow_ops.with_dependencies([v1.initializer], v1._ref()),  # pylint: disable=protected-access\n            constant_op.constant([1]))\n\n        v2 = variables.VariableV1(\n            np.array([[0.1, 1.1], [10.1, 11.1], [20.1, 21.1]]).astype(\n                np.float32))\n        v2_at_1 = ops.IndexedSlices(\n            control_flow_ops.with_dependencies([v2.initializer], v2._ref()),  # pylint: disable=protected-access\n            constant_op.constant([1]))\n\n        st1, st2 = control_flow_ops.tuple([v1_at_1, v2_at_1])\n        g1 = array_ops.gather(st1.values, st1.indices)\n        g2 = array_ops.gather(st2.values, st2.indices)\n\n        # v1 is not initialized.\n        with self.assertRaisesOpError(\"Attempting to use uninitialized value\"):\n          self.evaluate(v1)\n\n        # v2 is not initialized.\n        with self.assertRaisesOpError(\"Attempting to use uninitialized value\"):\n          self.evaluate(v2)\n\n        if v1_first:\n          # Getting g1 initializes v2.\n          self.assertAllClose([[10.0, 11.0]], self.evaluate(g1))\n          self.assertAllClose([[0.1, 1.1], [10.1, 11.1], [20.1, 21.1]],\n                              self.evaluate(v2))\n        else:\n          # Getting g2 initializes v1.\n          self.assertAllClose([[10.1, 11.1]], self.evaluate(g2))\n          self.assertAllClose([[0.0, 1.0], [10.0, 11.0], [20.0, 21.0]],\n                              self.evaluate(v1))\n\n  def testAcceptTensorsAsControlInputs(self):\n    with self.cached_session():\n      var = variables.VariableV1(0)\n      assign = state_ops.assign(var, 1)\n      t, = control_flow_ops.tuple(\n          [constant_op.constant(0)], control_inputs=[assign])\n\n      # Should trigger the assign.\n      self.evaluate(t)\n\n      self.assertEqual(1, self.evaluate(var))\n\n\nclass AssertTest(test.TestCase):\n\n  @test_util.run_deprecated_v1\n  def testGuardedAssertDoesNotCopyWhenTrue(self):\n    if test_util.is_gpu_available():\n      self.skipTest(\"b/128646478 fails in opensource\")\n\n    with self.session(use_gpu=True) as sess:\n      with ops.device(test.gpu_device_name()):\n        value = constant_op.constant(1.0)\n      with ops.device(\"/cpu:0\"):\n        true = constant_op.constant(True)\n        guarded_assert = control_flow_ops.Assert(true, [value], name=\"guarded\")\n        unguarded_assert = gen_logging_ops._assert(\n            true, [value], name=\"unguarded\")\n      opts = config_pb2.RunOptions(trace_level=config_pb2.RunOptions.FULL_TRACE)\n      guarded_metadata = config_pb2.RunMetadata()\n      sess.run(guarded_assert, options=opts, run_metadata=guarded_metadata)\n      unguarded_metadata = config_pb2.RunMetadata()\n      sess.run(unguarded_assert, options=opts, run_metadata=unguarded_metadata)\n      guarded_nodestat_names = [\n          n.node_name\n          for d in guarded_metadata.step_stats.dev_stats\n          for n in d.node_stats\n      ]\n      unguarded_nodestat_names = [\n          n.node_name\n          for d in unguarded_metadata.step_stats.dev_stats\n          for n in d.node_stats\n      ]\n      guarded_memcpy_nodestat_names = [\n          n for n in guarded_nodestat_names if \"MEMCPYDtoH\" in n\n      ]\n      unguarded_memcpy_nodestat_names = [\n          n for n in unguarded_nodestat_names if \"MEMCPYDtoH\" in n\n      ]\n      if \"GPU\" in [d.device_type for d in device_lib.list_local_devices()]:\n        # A copy was performed for the unguarded assert\n        self.assertLess(0, len(unguarded_memcpy_nodestat_names),\n                        str(unguarded_nodestat_names))\n      # No copy was performed for the guarded assert\n      self.assertEqual([], guarded_memcpy_nodestat_names)\n\n\nclass WhileOpBenchmark(test.Benchmark):\n  \"\"\"Evaluate the performance of while_loop op.\"\"\"\n\n  def _getInitVariables(self):\n    batch_size = 10\n    image_size = 256\n    kernel_size = 3\n    depth = 16\n\n    init_step = constant_op.constant(-1)\n    image = variable_scope.get_variable(\n        \"image\",\n        initializer=random_ops.random_normal(\n            [batch_size, image_size, image_size, depth],\n            dtype=dtypes.float32,\n            stddev=1e-1))\n    kernel = variable_scope.get_variable(\n        \"weights\",\n        initializer=random_ops.truncated_normal(\n            [kernel_size, kernel_size, depth, depth],\n            dtype=dtypes.float32,\n            stddev=1e-1))\n    return init_step, image, kernel\n\n  def _runOneBenchmark(self,\n                       default_device,\n                       num_iters=10,\n                       static_unroll=False,\n                       steps=10):\n    \"\"\"Evaluate the while loop performance.\n\n    Args:\n      default_device: The default device to run all ops except the loop_body.\n        loop_body is always run on GPU.\n      num_iters: Number of iterations to run.\n      static_unroll: If true, run unrolled version; otherwise, run while_loop.\n      steps: Total number of repeated steps to run the loop.\n\n    Returns:\n      The duration of the run in seconds.\n    \"\"\"\n\n    def loop_body(i, x):\n      with ops.device(\"/gpu:0\"):\n        # Always put loop body on GPU.\n        nx = nn_ops.conv2d(\n            input=x,\n            filter=kernel,\n            strides=[1, 1, 1, 1],\n            padding=\"SAME\",\n            data_format=\"NHWC\",\n            name=\"conv2d\")\n        ni = math_ops.add(i, 1)\n        return ni, nx\n\n    ops.reset_default_graph()\n    with session.Session() as sess, ops.device(default_device):\n      # Get the initial id i, input x, and kernel.\n      i, x, kernel = self._getInitVariables()\n      self.evaluate(variables.global_variables_initializer())\n\n      if static_unroll:\n        for _ in xrange(steps):\n          i, x = loop_body(i, x)\n      else:\n        i, x = control_flow_ops.while_loop(\n            lambda i, _: i < steps,\n            loop_body, [i, x],\n            parallel_iterations=steps,\n            swap_memory=True)\n\n      r = math_ops.reduce_sum(x)\n      dx, dk = gradients_impl.gradients(r, [x, kernel])\n      # Use group to avoid fetching back results.\n      r = control_flow_ops.group(dx, dk)\n\n      for _ in xrange(3):\n        # exclude warm up time\n        self.evaluate(r)\n\n      start_time = time.time()\n      for _ in xrange(num_iters):\n        self.evaluate(r)\n      return (time.time() - start_time) / num_iters\n\n  def benchmarkWhileOpCrossDevicePlacement(self):\n    iters = 10\n    # Run loop body on GPU, but other ops on CPU.\n    duration = self._runOneBenchmark(\"cpu\", iters, static_unroll=False)\n    self.report_benchmark(\n        name=\"while_op_cross_device\", iters=iters, wall_time=duration)\n\n  def benchmarkWhileOpSameDevicePlacement(self):\n    iters = 10\n    # Run all ops on the same GPU device.\n    duration = self._runOneBenchmark(\"gpu\", iters, static_unroll=False)\n    self.report_benchmark(\n        name=\"while_op_same_device\", iters=iters, wall_time=duration)\n\n  def benchmarkWhileOpUnrollCrossDevicePlacement(self):\n    iters = 10\n    # Run loop body on GPU, but other ops on CPU.\n    duration = self._runOneBenchmark(\"cpu\", iters, static_unroll=True)\n    self.report_benchmark(\n        name=\"unroll_cross_device_cpu\", iters=iters, wall_time=duration)\n\n  def benchmarkWhileOpUnrollSameDevicePlacement(self):\n    iters = 10\n    # Run all ops on GPU.\n    duration = self._runOneBenchmark(\"gpu\", iters, static_unroll=True)\n    self.report_benchmark(\n        name=\"unroll_same_device\", iters=iters, wall_time=duration)\n\n\n@test_util.with_control_flow_v2\nclass EagerTest(test.TestCase):\n\n  def testCond(self):\n    with context.eager_mode():\n      pred = math_ops.less(1, 2)\n      fn1 = lambda: [constant_op.constant(10)]\n      fn2 = lambda: [constant_op.constant(20)]\n      r = control_flow_ops.cond(pred, fn1, fn2)\n\n      self.assertAllEqual(r.numpy(), 10)\n      self.assertFalse(isinstance(r, list))\n\n  # TODO(b/117279927): Re-enable once msan failure is fixed.\n  def DISABLED_testCondInDefun(self):\n    with context.eager_mode():\n\n      @eager_function.defun\n      def foo(pred):\n        # TODO(b/111124878): this only needs to output one element.\n        fn1 = lambda: (constant_op.constant(10), constant_op.constant(100))\n        fn2 = lambda: (constant_op.constant(20), constant_op.constant(200))\n        return control_flow_ops.cond(constant_op.constant(pred), fn1, fn2)\n\n      r = foo(True)\n      self.assertAllEqual(r[0].numpy(), 10)\n      self.assertNotIsInstance(r, list)\n\n      r = foo(False)\n      self.assertAllEqual(r[0].numpy(), 20)\n      self.assertFalse(isinstance(r, list))\n\n  def testWhileLoop(self):\n    with context.eager_mode():\n      tensor = constant_op.constant([1, 2, 3, 4, 5])\n      self.assertAllEqual(isum(tensor).numpy(), [46, 47, 48, 49, 50])\n\n  def testWhileLoopWithMaxIterations(self):\n    with context.eager_mode():\n      tensor = constant_op.constant([1, 2, 3, 4, 5])\n      self.assertAllEqual(\n          isum(tensor, maximum_iterations=3).numpy(),\n          [1 + 3, 2 + 3, 3 + 3, 4 + 3, 5 + 3])\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testWhileWithMaximumIterationsAndSingleArgument(self):\n    with context.eager_mode():\n      tensor = constant_op.constant(0)\n      r = control_flow_ops.while_loop(\n          lambda i: i < 3, lambda i: i + 1, [tensor], maximum_iterations=1)\n      self.assertEqual(1, r.numpy())\n\n  def testWithDependencies(self):\n    with context.eager_mode():\n      t1 = constant_op.constant(1)\n      t2 = constant_op.constant(2)\n      t3 = control_flow_ops.with_dependencies(t1, t2)\n      self.assertAllEqual(t2.numpy(), t3.numpy())\n\n  def testTuple(self):\n    with context.eager_mode():\n      t1 = constant_op.constant(1)\n      t2 = constant_op.constant(2)\n      tup1, tup2 = control_flow_ops.tuple([t1, t2])\n      self.assertAllEqual(t1.numpy(), tup1.numpy())\n      self.assertAllEqual(t2.numpy(), tup2.numpy())\n\n  @test_util.run_v1_only(\"b/120545219\")\n  def testCase(self):\n    with context.eager_mode():\n      x = constant_op.constant(1)\n      y = constant_op.constant(2)\n      z = constant_op.constant(3)\n      f1 = lambda: constant_op.constant(17)\n      f2 = lambda: constant_op.constant(23)\n      f3 = lambda: constant_op.constant(-1)\n\n      r1 = control_flow_ops.case(\n          [(x < y, f1), (x > z, f2)], default=f3, exclusive=True)\n      self.assertAllEqual(r1.numpy(), 17)\n\n\nif __name__ == \"__main__\":\n  test.main()\n"], "filenames": ["tensorflow/core/common_runtime/eager/kernel_and_device.cc", "tensorflow/python/kernel_tests/control_flow_ops_py_test.py"], "buggy_code_start_loc": [311, 4581], "buggy_code_end_loc": [312, 4581], "fixing_code_start_loc": [311, 4582], "fixing_code_end_loc": [317, 4590], "type": "CWE-20", "message": "In Tensorflow before versions 1.15.4, 2.0.3, 2.1.2, 2.2.1 and 2.3.1, the `tf.raw_ops.Switch` operation takes as input a tensor and a boolean and outputs two tensors. Depending on the boolean value, one of the tensors is exactly the input tensor whereas the other one should be an empty tensor. However, the eager runtime traverses all tensors in the output. Since only one of the tensors is defined, the other one is `nullptr`, hence we are binding a reference to `nullptr`. This is undefined behavior and reported as an error if compiling with `-fsanitize=null`. In this case, this results in a segmentation fault The issue is patched in commit da8558533d925694483d2c136a9220d6d49d843c, and is released in TensorFlow versions 1.15.4, 2.0.3, 2.1.2, 2.2.1, or 2.3.1.", "other": {"cve": {"id": "CVE-2020-15190", "sourceIdentifier": "security-advisories@github.com", "published": "2020-09-25T19:15:14.337", "lastModified": "2021-11-18T17:18:14.627", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "In Tensorflow before versions 1.15.4, 2.0.3, 2.1.2, 2.2.1 and 2.3.1, the `tf.raw_ops.Switch` operation takes as input a tensor and a boolean and outputs two tensors. Depending on the boolean value, one of the tensors is exactly the input tensor whereas the other one should be an empty tensor. However, the eager runtime traverses all tensors in the output. Since only one of the tensors is defined, the other one is `nullptr`, hence we are binding a reference to `nullptr`. This is undefined behavior and reported as an error if compiling with `-fsanitize=null`. In this case, this results in a segmentation fault The issue is patched in commit da8558533d925694483d2c136a9220d6d49d843c, and is released in TensorFlow versions 1.15.4, 2.0.3, 2.1.2, 2.2.1, or 2.3.1."}, {"lang": "es", "value": "En Tensorflow anteriores a las versiones 1.15.4, 2.0.3, 2.1.2, 2.2.1 y 2.3.1, la operaci\u00f3n \"tf.raw_ops.Switch\" toma como entrada un tensor y un booleano y genera dos tensores.&#xa0;Dependiendo del valor booleano, uno de los tensores es exactamente el tensor de entrada mientras que el otro deber\u00eda ser un tensor vac\u00edo.&#xa0;Sin embargo, el tiempo de ejecuci\u00f3n de eager salta todos los tensores en la salida.&#xa0;Dado que solo se define uno de los tensores, el otro es \"nullptr\", por lo que vinculamos una referencia a \"nullptr\".&#xa0;Este es un comportamiento indefinido y se reporta como un error si se compila con \"-fsanitize=null\".&#xa0;En este caso, esto resulta en un fallo de segmentaci\u00f3n. El problema es parcheado en el commit da8558533d925694483d2c136a9220d6d49d843c, y es publicado en TensorFlow versiones 1.15.4, 2.0.3, 2.1.2, 2.2.1 o 2.3.1"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:L", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "LOW", "baseScore": 5.3, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 3.9, "impactScore": 1.4}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:L", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "LOW", "baseScore": 5.3, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 3.9, "impactScore": 1.4}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:L/Au:N/C:N/I:N/A:P", "accessVector": "NETWORK", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "PARTIAL", "baseScore": 5.0}, "baseSeverity": "MEDIUM", "exploitabilityScore": 10.0, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "security-advisories@github.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-20"}, {"lang": "en", "value": "CWE-476"}]}, {"source": "nvd@nist.gov", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-476"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:-:*:*:*", "versionEndExcluding": "1.15.4", "matchCriteriaId": "EC688B44-17B7-462D-B6E3-BAAF99334782"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:-:*:*:*", "versionStartIncluding": "2.0.0", "versionEndExcluding": "2.0.3", "matchCriteriaId": "B6271763-8DFA-4A8F-9596-F1148961ECC5"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:-:*:*:*", "versionStartIncluding": "2.1.0", "versionEndExcluding": "2.1.2", "matchCriteriaId": "AA3FD62B-13CB-4EB5-939F-C848DE9AE071"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:-:*:*:*", "versionStartIncluding": "2.2.0", "versionEndExcluding": "2.2.1", "matchCriteriaId": "029CB8A9-ED3D-486D-967C-4CE0AF8D8FAD"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:-:*:*:*", "versionStartIncluding": "2.3.0", "versionEndExcluding": "2.3.1", "matchCriteriaId": "B617650A-B5A1-44BB-BB3A-2EF83648B100"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:opensuse:leap:15.2:*:*:*:*:*:*:*", "matchCriteriaId": "B009C22E-30A4-4288-BCF6-C3E81DEAF45A"}]}]}], "references": [{"url": "http://lists.opensuse.org/opensuse-security-announce/2020-10/msg00065.html", "source": "security-advisories@github.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/commit/da8558533d925694483d2c136a9220d6d49d843c", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/releases/tag/v2.3.1", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/security/advisories/GHSA-4g9f-63rx-5cw4", "source": "security-advisories@github.com", "tags": ["Exploit", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/tensorflow/tensorflow/commit/da8558533d925694483d2c136a9220d6d49d843c"}}