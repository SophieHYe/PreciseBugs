{"buggy_code": ["/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n// See docs in ../ops/math_ops.cc.\n\n#include <cmath>\n\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/register_types.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/tensor_shape.h\"\n#include \"tensorflow/core/framework/types.h\"\n\nnamespace tensorflow {\n\nint32 GetValue(int32_t v) { return v; }\n\ntemplate <typename T>\nclass RangeOp : public OpKernel {\n public:\n  explicit RangeOp(OpKernelConstruction* context) : OpKernel(context) {}\n\n  void Compute(OpKernelContext* context) override {\n    const Tensor& start_in = context->input(0);\n    const Tensor& limit_in = context->input(1);\n    const Tensor& delta_in = context->input(2);\n    // TODO(rmlarsen): Disallow legacy use of length-1 vectors as scalars.\n    OP_REQUIRES(context,\n                TensorShapeUtils::IsScalar(start_in.shape()) ||\n                    (TensorShapeUtils::IsVector(start_in.shape()) &&\n                     start_in.shape().dim_size(0) == 1),\n                errors::InvalidArgument(\"start must be a scalar, not shape \",\n                                        start_in.shape().DebugString()));\n    OP_REQUIRES(context,\n                TensorShapeUtils::IsScalar(limit_in.shape()) ||\n                    (TensorShapeUtils::IsVector(limit_in.shape()) &&\n                     limit_in.shape().dim_size(0) == 1),\n                errors::InvalidArgument(\"limit must be a scalar, not shape \",\n                                        limit_in.shape().DebugString()));\n    OP_REQUIRES(context,\n                TensorShapeUtils::IsScalar(delta_in.shape()) ||\n                    (TensorShapeUtils::IsVector(delta_in.shape()) &&\n                     delta_in.shape().dim_size(0) == 1),\n                errors::InvalidArgument(\"delta must be a scalar, not shape \",\n                                        delta_in.shape().DebugString()));\n    const T start = start_in.scalar<T>()();\n    const T limit = limit_in.scalar<T>()();\n    const T delta = delta_in.scalar<T>()();\n    OP_REQUIRES(context, delta != 0,\n                errors::InvalidArgument(\"Requires delta != 0: \", delta));\n    if (delta > 0) {\n      OP_REQUIRES(\n          context, start <= limit,\n          errors::InvalidArgument(\n              \"Requires start <= limit when delta > 0: \", start, \"/\", limit));\n    } else {\n      OP_REQUIRES(\n          context, start >= limit,\n          errors::InvalidArgument(\n              \"Requires start >= limit when delta < 0: \", start, \"/\", limit));\n    }\n    int64_t size = 0;\n    if (std::is_integral<T>::value) {\n      size = static_cast<int64>(\n          (std::abs(limit - start) + std::abs(delta) - 1) / std::abs(delta));\n    } else {\n      size = static_cast<int64>(std::ceil(std::abs((limit - start) / delta)));\n    }\n    Tensor* out = nullptr;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(0, TensorShape({size}), &out));\n    auto flat = out->flat<T>();\n    T val = start;\n    for (int64_t i = 0; i < size; ++i) {\n      flat(i) = T(val);\n      val += delta;\n    }\n  }\n};\n\n#define REGISTER_KERNEL(DEV, TYPE)                           \\\n  REGISTER_KERNEL_BUILDER(Name(\"Range\")                      \\\n                              .Device(DEV)                   \\\n                              .HostMemory(\"start\")           \\\n                              .HostMemory(\"limit\")           \\\n                              .HostMemory(\"delta\")           \\\n                              .HostMemory(\"output\")          \\\n                              .TypeConstraint<TYPE>(\"Tidx\"), \\\n                          RangeOp<TYPE>);\n\n#define REGISTER_CPU_KERNEL(T) REGISTER_KERNEL(DEVICE_CPU, T)\n#define REGISTER_GPU_KERNEL(T) REGISTER_KERNEL(DEVICE_GPU, T)\n\nTF_CALL_float(REGISTER_CPU_KERNEL);\nTF_CALL_double(REGISTER_CPU_KERNEL);\nTF_CALL_int32(REGISTER_CPU_KERNEL);\nTF_CALL_int64(REGISTER_CPU_KERNEL);\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\nTF_CALL_float(REGISTER_GPU_KERNEL);\nTF_CALL_double(REGISTER_GPU_KERNEL);\nTF_CALL_int32(REGISTER_GPU_KERNEL);\nTF_CALL_int64(REGISTER_GPU_KERNEL);\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n#undef REGISTER_KERNEL\n#undef REGISTER_CPU_KERNEL\n#undef REGISTER_GPU_KERNEL\n\ntemplate <typename T, typename Tnum>\nclass LinSpaceOp : public OpKernel {\n public:\n  explicit LinSpaceOp(OpKernelConstruction* context) : OpKernel(context) {}\n\n  void Compute(OpKernelContext* context) override {\n    const Tensor& start_in = context->input(0);\n    const Tensor& stop_in = context->input(1);\n    const Tensor& num_in = context->input(2);\n    OP_REQUIRES(context, TensorShapeUtils::IsScalar(start_in.shape()),\n                errors::InvalidArgument(\"start must be a scalar, not shape \",\n                                        start_in.shape().DebugString()));\n    OP_REQUIRES(context, TensorShapeUtils::IsScalar(stop_in.shape()),\n                errors::InvalidArgument(\"stop must be a scalar, not shape \",\n                                        stop_in.shape().DebugString()));\n    OP_REQUIRES(context, TensorShapeUtils::IsScalar(num_in.shape()),\n                errors::InvalidArgument(\"num must be a scalar, not shape \",\n                                        num_in.shape().DebugString()));\n    const T start = start_in.scalar<T>()();\n    const T stop = stop_in.scalar<T>()();\n    const Tnum num = num_in.scalar<Tnum>()();\n    OP_REQUIRES(context, num > 0,\n                errors::InvalidArgument(\"Requires num > 0: \", num));\n    Tensor* out = nullptr;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(0, TensorShape({num}), &out));\n    auto flat = out->flat<T>();\n    flat(0) = start;\n    if (num > 1) {\n      const T step = (stop - start) / (num - 1);\n      for (Tnum i = 1; i < num - 1; ++i) flat(i) = start + step * i;\n      // Ensure final value == stop; float arithmetic won't guarantee this.\n      flat(num - 1) = stop;\n    }\n  }\n};\n\n#define REGISTER_KERNEL(DEV, T, Tidx)                       \\\n  REGISTER_KERNEL_BUILDER(Name(\"LinSpace\")                  \\\n                              .Device(DEV)                  \\\n                              .TypeConstraint<T>(\"T\")       \\\n                              .TypeConstraint<Tidx>(\"Tidx\") \\\n                              .HostMemory(\"start\")          \\\n                              .HostMemory(\"stop\")           \\\n                              .HostMemory(\"num\")            \\\n                              .HostMemory(\"output\"),        \\\n                          LinSpaceOp<T, Tidx>);\n\n#define REGISTER_KERNEL_ALL_NUMS(dev, T) \\\n  REGISTER_KERNEL(dev, T, int32);        \\\n  REGISTER_KERNEL(dev, T, int64_t)\n\n#define REGISTER_CPU_KERNEL(T) REGISTER_KERNEL_ALL_NUMS(DEVICE_CPU, T)\nTF_CALL_float(REGISTER_CPU_KERNEL);\nTF_CALL_double(REGISTER_CPU_KERNEL);\n\n// NOTE(touts): We register the op on GPU but it still runs on CPU\n// because its inputs and outputs are tagged as HostMemory.\n#define REGISTER_GPU_KERNEL(T) REGISTER_KERNEL_ALL_NUMS(DEVICE_GPU, T)\nTF_CALL_float(REGISTER_GPU_KERNEL);\nTF_CALL_double(REGISTER_GPU_KERNEL);\n#undef REGISTER_GPU_KERNEL\n\n\n#undef REGISTER_CPU_KERNEL\n#undef REGISTER_KERNEL_ALL_NUMS\n#undef REGISTER_KERNEL\n\n}  // namespace tensorflow\n", "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for tensorflow.ops.ops.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\n\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors_impl\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import random_seed\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.layers import convolutional\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import init_ops\nfrom tensorflow.python.ops import linalg_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import partitioned_variables\nfrom tensorflow.python.ops import random_ops\nfrom tensorflow.python.ops import variable_scope\nfrom tensorflow.python.ops import variables\nfrom tensorflow.python.platform import test\n\n\n# Returns true iff the two initializers produce the same tensor to\n# within a tiny tolerance.\ndef identicaltest(tc, init1, init2, shape=None):\n  \"\"\"Tests if two initializations are identical to within tiny tolerances.\n\n  Args:\n    tc: An instance of TensorFlowTestCase.\n    init1: An Initializer that generates a tensor of a given shape\n    init2: An Initializer that generates a tensor of a given shape\n    shape: Shape of the tensor to initialize or `None` to use a vector of length\n      100.\n\n  Returns:\n    True or False as determined by test.\n  \"\"\"\n  if shape is None:\n    shape = [100]\n  with tc.test_session(graph=ops.Graph()):\n    t1 = init1(shape).eval()\n  with tc.test_session(graph=ops.Graph()):\n    t2 = init2(shape).eval()\n  return np.allclose(t1, t2, rtol=1e-15, atol=1e-15)\n\n\ndef duplicated_initializer(tc, init, graph_seed, shape=None):\n  \"\"\"Tests duplicated random initializer within the same graph.\n\n  This test generates two random kernels from the same initializer to the same\n  graph, and checks if the results are close enough. Even given the same global,\n  seed, two different instances of random kernels should generate different\n  results.\n\n  Args:\n    tc: An instance of TensorFlowTestCase.\n    init: An Initializer that generates a tensor of a given shape\n    graph_seed: A graph-level seed to use.\n    shape: Shape of the tensor to initialize or `None` to use a vector of length\n      100.\n\n  Returns:\n    True or False as determined by test.\n  \"\"\"\n  if shape is None:\n    shape = [100]\n  with tc.test_session(graph=ops.Graph()):\n    random_seed.set_random_seed(graph_seed)\n    t1 = init(shape).eval()\n    t2 = init(shape).eval()\n    return np.allclose(t1, t2, rtol=1e-15, atol=1e-15)\n\n\ndef _init_sampler(tc, init, num):\n  \"\"\"Returns a func to generate a random tensor of shape [num].\n\n  Args:\n    tc: An instance of TensorFlowTestCase.\n    init: An Initializer that generates a tensor of a given shape\n    num: Size of 1D tensor to create.\n\n  Returns:\n    Function to generate a random tensor.\n  \"\"\"\n\n  def func():\n    with tc.test_session():\n      return init([num]).eval()\n\n  return func\n\n\nclass ConstantInitializersTest(test.TestCase):\n\n  @test_util.run_deprecated_v1\n  def testZerosInitializer(self):\n    with self.session():\n      shape = [2, 3]\n      x = variable_scope.get_variable(\n          \"x\", shape=shape, initializer=init_ops.zeros_initializer())\n      self.evaluate(x.initializer)\n      self.assertAllEqual(x, np.zeros(shape))\n\n  @test_util.run_deprecated_v1\n  def testOnesInitializer(self):\n    with self.session():\n      shape = [2, 3]\n      x = variable_scope.get_variable(\n          \"x\", shape=shape, initializer=init_ops.ones_initializer())\n      self.evaluate(x.initializer)\n      self.assertAllEqual(x, np.ones(shape))\n\n  @test_util.run_deprecated_v1\n  def testConstantZeroInitializer(self):\n    with self.session():\n      shape = [2, 3]\n      x = variable_scope.get_variable(\n          \"x\", shape=shape, initializer=init_ops.constant_initializer(0.0))\n      self.evaluate(x.initializer)\n      self.assertAllEqual(x, np.zeros(shape))\n\n  @test_util.run_deprecated_v1\n  def testConstantOneInitializer(self):\n    with self.session():\n      shape = [2, 3]\n      x = variable_scope.get_variable(\n          \"x\", shape=shape, initializer=init_ops.constant_initializer(1.0))\n      self.evaluate(x.initializer)\n      self.assertAllEqual(x, np.ones(shape))\n\n  @test_util.run_deprecated_v1\n  def testConstantIntInitializer(self):\n    with self.session():\n      shape = [2, 3]\n      x = variable_scope.get_variable(\n          \"x\",\n          shape=shape,\n          dtype=dtypes.int32,\n          initializer=init_ops.constant_initializer(7))\n      self.evaluate(x.initializer)\n      self.assertEqual(x.dtype.base_dtype, dtypes.int32)\n      self.assertAllEqual(x, 7 * np.ones(shape, dtype=np.int32))\n\n  @test_util.run_deprecated_v1\n  def testConstantTupleInitializer(self):\n    with self.session():\n      shape = [3]\n      x = variable_scope.get_variable(\n          \"x\",\n          shape=shape,\n          dtype=dtypes.int32,\n          initializer=init_ops.constant_initializer((10, 20, 30)))\n      self.evaluate(x.initializer)\n      self.assertEqual(x.dtype.base_dtype, dtypes.int32)\n      self.assertAllEqual(x, [10, 20, 30])\n\n  def _testNDimConstantInitializer(self, name, value, shape, expected):\n    with self.cached_session():\n      init = init_ops.constant_initializer(value, dtype=dtypes.int32)\n      x = variable_scope.get_variable(name, shape=shape, initializer=init)\n      self.evaluate(x.initializer)\n\n      actual = array_ops.reshape(x, [-1]).eval()\n      self.assertEqual(len(actual), len(expected))\n      for a, e in zip(actual, expected):\n        self.assertEqual(a, e)\n\n  @test_util.run_deprecated_v1\n  def testNDimConstantInitializer(self):\n    value = [0, 1, 2, 3, 4, 5]\n    shape = [2, 3]\n    expected = list(value)\n\n    self._testNDimConstantInitializer(\"list\", value, shape, expected)\n    self._testNDimConstantInitializer(\"ndarray\", np.asarray(value), shape,\n                                      expected)\n    self._testNDimConstantInitializer(\"2D-ndarray\",\n                                      np.asarray(value).reshape(tuple(shape)),\n                                      shape, expected)\n\n  def _testNDimConstantInitializerLessValues(self, name, value, shape,\n                                             expected):\n    with self.cached_session():\n      init = init_ops.constant_initializer(value, dtype=dtypes.int32)\n      x = variable_scope.get_variable(name, shape=shape, initializer=init)\n      self.evaluate(x.initializer)\n\n      actual = array_ops.reshape(x, [-1]).eval()\n      self.assertGreater(len(actual), len(expected))\n      for i in xrange(len(actual)):\n        a = actual[i]\n        e = expected[i] if i < len(expected) else expected[-1]\n        self.assertEqual(a, e)\n\n  @test_util.run_deprecated_v1\n  def testNDimConstantInitializerLessValues(self):\n    value = [0, 1, 2, 3, 4, 5]\n    shape = [2, 4]\n    expected = list(value)\n\n    self._testNDimConstantInitializerLessValues(\"list\", value, shape, expected)\n    self._testNDimConstantInitializerLessValues(\"ndarray\", np.asarray(value),\n                                                shape, expected)\n    self._testNDimConstantInitializerLessValues(\n        \"2D-ndarray\",\n        np.asarray(value).reshape(tuple([2, 3])), shape, expected)\n\n  def _testNDimConstantInitializerMoreValues(self, value, shape):\n    ops.reset_default_graph()\n    with self.cached_session():\n      init = init_ops.constant_initializer(value, dtype=dtypes.int32)\n      self.assertRaises(\n          ValueError,\n          variable_scope.get_variable,\n          \"x\",\n          shape=shape,\n          initializer=init)\n\n  @test_util.run_deprecated_v1\n  def testNDimConstantInitializerMoreValues(self):\n    value = [0, 1, 2, 3, 4, 5, 6, 7]\n    shape = [2, 3]\n    self._testNDimConstantInitializerMoreValues(value, shape)\n    self._testNDimConstantInitializerMoreValues(np.asarray(value), shape)\n    self._testNDimConstantInitializerMoreValues(\n        np.asarray(value).reshape(tuple([2, 4])), shape)\n\n  def testInvalidValueTypeForConstantInitializerCausesTypeError(self):\n    c = constant_op.constant([1.0, 2.0, 3.0])\n    with self.assertRaisesRegex(TypeError,\n                                r\"Invalid type for initial value: .*Tensor.*\"):\n      init_ops.constant_initializer(c, dtype=dtypes.float32)\n    v = variables.Variable([3.0, 2.0, 1.0])\n    with self.assertRaisesRegex(\n        TypeError, r\"Invalid type for initial value: .*Variable.*\"):\n      init_ops.constant_initializer(v, dtype=dtypes.float32)\n\n\nclass RandomNormalInitializationTest(test.TestCase):\n\n  @test_util.run_deprecated_v1\n  def testInitializerIdentical(self):\n    for dtype in [dtypes.float32, dtypes.float64]:\n      init1 = init_ops.random_normal_initializer(0.0, 1.0, seed=1, dtype=dtype)\n      init2 = init_ops.random_normal_initializer(0.0, 1.0, seed=1, dtype=dtype)\n      self.assertTrue(identicaltest(self, init1, init2))\n\n  @test_util.run_deprecated_v1\n  def testInitializerDifferent(self):\n    for dtype in [dtypes.float32, dtypes.float64]:\n      init1 = init_ops.random_normal_initializer(0.0, 1.0, seed=1, dtype=dtype)\n      init2 = init_ops.random_normal_initializer(0.0, 1.0, seed=2, dtype=dtype)\n      self.assertFalse(identicaltest(self, init1, init2))\n\n  @test_util.run_deprecated_v1\n  def testDuplicatedInitializer(self):\n    init = init_ops.random_normal_initializer(0.0, 1.0)\n    self.assertFalse(duplicated_initializer(self, init, 1))\n\n  def testInvalidDataType(self):\n    self.assertRaises(\n        ValueError,\n        init_ops.random_normal_initializer,\n        0.0,\n        1.0,\n        dtype=dtypes.string)\n\n\nclass TruncatedNormalInitializationTest(test.TestCase):\n\n  @test_util.run_deprecated_v1\n  def testInitializerIdentical(self):\n    for dtype in [dtypes.float32, dtypes.float64]:\n      init1 = init_ops.truncated_normal_initializer(\n          0.0, 1.0, seed=1, dtype=dtype)\n      init2 = init_ops.truncated_normal_initializer(\n          0.0, 1.0, seed=1, dtype=dtype)\n      self.assertTrue(identicaltest(self, init1, init2))\n\n  @test_util.run_deprecated_v1\n  def testInitializerDifferent(self):\n    for dtype in [dtypes.float32, dtypes.float64]:\n      init1 = init_ops.truncated_normal_initializer(\n          0.0, 1.0, seed=1, dtype=dtype)\n      init2 = init_ops.truncated_normal_initializer(\n          0.0, 1.0, seed=2, dtype=dtype)\n      self.assertFalse(identicaltest(self, init1, init2))\n\n  @test_util.run_deprecated_v1\n  def testDuplicatedInitializer(self):\n    init = init_ops.truncated_normal_initializer(0.0, 1.0)\n    self.assertFalse(duplicated_initializer(self, init, 1))\n\n  def testInvalidDataType(self):\n    self.assertRaises(\n        ValueError,\n        init_ops.truncated_normal_initializer,\n        0.0,\n        1.0,\n        dtype=dtypes.string)\n\n\nclass RandomUniformInitializationTest(test.TestCase):\n\n  @test_util.run_deprecated_v1\n  def testInitializerIdentical(self):\n    for dtype in [dtypes.float32, dtypes.float64, dtypes.int64]:\n      init1 = init_ops.random_uniform_initializer(0, 7, seed=1, dtype=dtype)\n      init2 = init_ops.random_uniform_initializer(0, 7, seed=1, dtype=dtype)\n      self.assertTrue(identicaltest(self, init1, init2))\n\n  @test_util.run_deprecated_v1\n  def testInitializerDifferent(self):\n    for dtype in [dtypes.float32, dtypes.float64, dtypes.int32, dtypes.int64]:\n      init1 = init_ops.random_uniform_initializer(0, 7, seed=1, dtype=dtype)\n      init2 = init_ops.random_uniform_initializer(0, 7, seed=2, dtype=dtype)\n      self.assertFalse(identicaltest(self, init1, init2))\n\n  @test_util.run_deprecated_v1\n  def testDuplicatedInitializer(self):\n    init = init_ops.random_uniform_initializer(0.0, 1.0)\n    self.assertFalse(duplicated_initializer(self, init, 1))\n\n\nclass UniformUnitScalingInitializationTest(test.TestCase):\n\n  @test_util.run_deprecated_v1\n  def testInitializerIdentical(self):\n    for dtype in [dtypes.float32, dtypes.float64]:\n      init1 = init_ops.uniform_unit_scaling_initializer(seed=1, dtype=dtype)\n      init2 = init_ops.uniform_unit_scaling_initializer(seed=1, dtype=dtype)\n      self.assertTrue(identicaltest(self, init1, init2))\n      init3 = init_ops.uniform_unit_scaling_initializer(\n          1.5, seed=1, dtype=dtype)\n      init4 = init_ops.uniform_unit_scaling_initializer(\n          1.5, seed=1, dtype=dtype)\n      self.assertTrue(identicaltest(self, init3, init4))\n\n  @test_util.run_deprecated_v1\n  def testInitializerDifferent(self):\n    for dtype in [dtypes.float32, dtypes.float64]:\n      init1 = init_ops.uniform_unit_scaling_initializer(seed=1, dtype=dtype)\n      init2 = init_ops.uniform_unit_scaling_initializer(seed=2, dtype=dtype)\n      init3 = init_ops.uniform_unit_scaling_initializer(\n          1.5, seed=1, dtype=dtype)\n      self.assertFalse(identicaltest(self, init1, init2))\n      self.assertFalse(identicaltest(self, init1, init3))\n      self.assertFalse(identicaltest(self, init2, init3))\n\n  @test_util.run_deprecated_v1\n  def testZeroSize(self):\n    shape = [0, 2]\n    with self.cached_session():\n      x = variable_scope.get_variable(\n          \"x\",\n          shape=shape,\n          initializer=init_ops.uniform_unit_scaling_initializer())\n      self.evaluate(variables.global_variables_initializer())\n      self.assertAllEqual(shape, self.evaluate(x).shape)\n\n  @test_util.run_deprecated_v1\n  def testDuplicatedInitializer(self):\n    init = init_ops.uniform_unit_scaling_initializer()\n    self.assertFalse(duplicated_initializer(self, init, 1))\n\n  def testInvalidDataType(self):\n    self.assertRaises(\n        ValueError,\n        init_ops.uniform_unit_scaling_initializer,\n        dtype=dtypes.string)\n\n\nclass VarianceScalingInitializationTest(test.TestCase):\n\n  @test_util.run_deprecated_v1\n  def testTruncatedNormalDistribution(self):\n    shape = [100, 100]\n    expect_mean = 0.\n    expect_var = 1. / shape[0]\n    init = init_ops.variance_scaling_initializer(\n        distribution=\"truncated_normal\")\n\n    with self.session(), \\\n      test.mock.patch.object(\n          random_ops, \"truncated_normal\", wraps=random_ops.truncated_normal) \\\n          as mock_truncated_normal:\n      x = init(shape).eval()\n      self.assertTrue(mock_truncated_normal.called)\n\n    self.assertNear(np.mean(x), expect_mean, err=1e-2)\n    self.assertNear(np.var(x), expect_var, err=1e-2)\n\n  @test_util.run_deprecated_v1\n  def testNormalDistribution(self):\n    shape = [100, 100]\n    expect_mean = 0.\n    expect_var = 1. / shape[0]\n    init = init_ops.variance_scaling_initializer(distribution=\"normal\")\n\n    with self.session(), \\\n      test.mock.patch.object(\n          random_ops, \"truncated_normal\", wraps=random_ops.truncated_normal) \\\n          as mock_truncated_normal:\n      x = init(shape).eval()\n      self.assertTrue(mock_truncated_normal.called)\n\n    self.assertNear(np.mean(x), expect_mean, err=1e-2)\n    self.assertNear(np.var(x), expect_var, err=1e-2)\n\n  @test_util.run_deprecated_v1\n  def testUntruncatedNormalDistribution(self):\n    shape = [100, 100]\n    expect_mean = 0.\n    expect_var = 1. / shape[0]\n    init = init_ops.variance_scaling_initializer(\n        distribution=\"untruncated_normal\")\n\n    with self.session(), \\\n      test.mock.patch.object(\n          random_ops, \"random_normal\", wraps=random_ops.random_normal) \\\n          as mock_random_normal:\n      x = init(shape).eval()\n      self.assertTrue(mock_random_normal.called)\n\n    self.assertNear(np.mean(x), expect_mean, err=1e-2)\n    self.assertNear(np.var(x), expect_var, err=1e-2)\n\n  @test_util.run_deprecated_v1\n  def testUniformDistribution(self):\n    shape = [100, 100]\n    expect_mean = 0.\n    expect_var = 1. / shape[0]\n    init = init_ops.variance_scaling_initializer(distribution=\"uniform\")\n\n    with self.session():\n      x = init(shape).eval()\n\n    self.assertNear(np.mean(x), expect_mean, err=1e-2)\n    self.assertNear(np.var(x), expect_var, err=1e-2)\n\n\n# TODO(vrv): move to sequence_ops_test?\nclass RangeTest(test.TestCase):\n\n  def _Range(self, start, limit, delta):\n    with self.cached_session():\n      tf_ans = math_ops.range(start, limit, delta, name=\"range\")\n      self.assertEqual([len(np.arange(start, limit, delta))],\n                       tf_ans.get_shape())\n      return self.evaluate(tf_ans)\n\n  def testBasic(self):\n    self.assertTrue(\n        np.array_equal(self._Range(0, 5, 1), np.array([0, 1, 2, 3, 4])))\n    self.assertTrue(np.array_equal(self._Range(0, 5, 2), np.array([0, 2, 4])))\n    self.assertTrue(np.array_equal(self._Range(0, 6, 2), np.array([0, 2, 4])))\n    self.assertTrue(\n        np.array_equal(self._Range(13, 32, 7), np.array([13, 20, 27])))\n    self.assertTrue(\n        np.array_equal(\n            self._Range(100, 500, 100), np.array([100, 200, 300, 400])))\n    self.assertEqual(math_ops.range(0, 5, 1).dtype, dtypes.int32)\n\n  @test_util.run_deprecated_v1\n  def testLimitOnly(self):\n    with self.session():\n      self.assertAllEqual(np.arange(5), math_ops.range(5))\n\n  def testEmpty(self):\n    for start in 0, 5:\n      self.assertTrue(np.array_equal(self._Range(start, start, 1), []))\n\n  def testNonInteger(self):\n    self.assertTrue(\n        np.allclose(self._Range(0, 2, 0.5), np.array([0, 0.5, 1, 1.5])))\n    self.assertTrue(np.allclose(self._Range(0, 5, 2.5), np.array([0, 2.5])))\n    self.assertTrue(\n        np.allclose(self._Range(0, 3, 0.9), np.array([0, 0.9, 1.8, 2.7])))\n    self.assertTrue(\n        np.allclose(\n            self._Range(100., 500., 100.), np.array([100, 200, 300, 400])))\n    self.assertEqual(math_ops.range(0., 5., 1.).dtype, dtypes.float32)\n\n  def testNegativeDelta(self):\n    self.assertTrue(\n        np.array_equal(self._Range(5, -1, -1), np.array([5, 4, 3, 2, 1, 0])))\n    self.assertTrue(\n        np.allclose(self._Range(2.5, 0, -0.5), np.array([2.5, 2, 1.5, 1, 0.5])))\n    self.assertTrue(\n        np.array_equal(self._Range(-5, -10, -3), np.array([-5, -8])))\n\n  def testDType(self):\n    zero_int32 = math_ops.cast(0, dtypes.int32)\n    zero_int64 = math_ops.cast(0, dtypes.int64)\n    zero_float32 = math_ops.cast(0, dtypes.float32)\n    zero_float64 = math_ops.cast(0, dtypes.float64)\n\n    self.assertEqual(math_ops.range(zero_int32, 0, 1).dtype, dtypes.int32)\n    self.assertEqual(math_ops.range(zero_int64, 0, 1).dtype, dtypes.int64)\n    self.assertEqual(math_ops.range(zero_float32, 0, 1).dtype, dtypes.float32)\n    self.assertEqual(math_ops.range(zero_float64, 0, 1).dtype, dtypes.float64)\n\n    self.assertEqual(\n        math_ops.range(zero_int32, zero_int64, 1).dtype, dtypes.int64)\n    self.assertEqual(\n        math_ops.range(zero_int64, zero_float32, 1).dtype, dtypes.float32)\n    self.assertEqual(\n        math_ops.range(zero_float32, zero_float64, 1).dtype, dtypes.float64)\n    self.assertEqual(\n        math_ops.range(zero_float64, zero_int32, 1).dtype, dtypes.float64)\n\n    self.assertEqual(\n        math_ops.range(0, 0, 1, dtype=dtypes.int32).dtype, dtypes.int32)\n    self.assertEqual(\n        math_ops.range(0, 0, 1, dtype=dtypes.int64).dtype, dtypes.int64)\n    self.assertEqual(\n        math_ops.range(0, 0, 1, dtype=dtypes.float32).dtype, dtypes.float32)\n    self.assertEqual(\n        math_ops.range(0, 0, 1, dtype=dtypes.float64).dtype, dtypes.float64)\n\n  def testMixedDType(self):\n    # Test case for GitHub issue 35710\n    tf_ans = math_ops.range(\n        constant_op.constant(4, dtype=dtypes.int32), dtype=dtypes.int64)\n    self.assertAllEqual(self.evaluate(tf_ans), np.array([0, 1, 2, 3]))\n\n  def testLargeLimits(self):\n    # Test case for GitHub issue 46913.\n    with self.session():\n      with self.assertRaises(errors_impl.ResourceExhaustedError):\n        v = math_ops.range(0, 9223372036854775807)\n        self.evaluate(v)\n\n\n# TODO(vrv): move to sequence_ops_test?\nclass LinSpaceTest(test.TestCase):\n\n  def _gpu_modes(self):\n    if test.is_gpu_available():\n      return [False, True]\n    else:\n      return [False]\n\n  def _LinSpace(self, start, stop, num):\n    with ops.Graph().as_default() as graph:\n      with self.session(graph=graph, force_gpu=self.force_gpu):\n        tf_ans = math_ops.linspace(start, stop, num, name=\"linspace\")\n        self.assertEqual([num], tf_ans.get_shape())\n        return self.evaluate(tf_ans)\n\n  def testPositive(self):\n    for self.force_gpu in self._gpu_modes():\n      self.assertArrayNear(self._LinSpace(1., 5., 1), np.array([1.]), 1e-5)\n      self.assertArrayNear(self._LinSpace(1., 5., 2), np.array([1., 5.]), 1e-5)\n      self.assertArrayNear(\n          self._LinSpace(1., 5., 3), np.array([1., 3., 5.]), 1e-5)\n      self.assertArrayNear(\n          self._LinSpace(1., 5., 4), np.array([1., 7. / 3., 11. / 3., 5.]),\n          1e-5)\n\n  def testNegative(self):\n    for self.force_gpu in self._gpu_modes():\n      self.assertArrayNear(self._LinSpace(-1., -5., 1), np.array([-1.]), 1e-5)\n      self.assertArrayNear(\n          self._LinSpace(-1., -5., 2), np.array([-1., -5.]), 1e-5)\n      self.assertArrayNear(\n          self._LinSpace(-1., -5., 3), np.array([-1., -3., -5.]), 1e-5)\n      self.assertArrayNear(\n          self._LinSpace(-1., -5., 4), np.array([-1., -7. / 3., -11. / 3.,\n                                                 -5.]), 1e-5)\n\n  def testNegativeToPositive(self):\n    for self.force_gpu in self._gpu_modes():\n      self.assertArrayNear(self._LinSpace(-1., 5., 1), np.array([-1.]), 1e-5)\n      self.assertArrayNear(\n          self._LinSpace(-1., 5., 2), np.array([-1., 5.]), 1e-5)\n      self.assertArrayNear(\n          self._LinSpace(-1., 5., 3), np.array([-1., 2., 5.]), 1e-5)\n      self.assertArrayNear(\n          self._LinSpace(-1., 5., 4), np.array([-1., 1., 3., 5.]), 1e-5)\n\n  def testPoint(self):\n    for self.force_gpu in self._gpu_modes():\n      self.assertArrayNear(self._LinSpace(5., 5., 1), np.array([5.]), 1e-5)\n      self.assertArrayNear(self._LinSpace(5., 5., 2), np.array([5.] * 2), 1e-5)\n      self.assertArrayNear(self._LinSpace(5., 5., 3), np.array([5.] * 3), 1e-5)\n      self.assertArrayNear(self._LinSpace(5., 5., 4), np.array([5.] * 4), 1e-5)\n\n  def testEndpointsAreExact(self):\n    for self.force_gpu in self._gpu_modes():\n      # Test some cases that produce last values not equal to \"stop\" when\n      # computed via start + (num - 1) * ((stop - start) / (num - 1)), since\n      # float arithmetic will introduce error through precision loss.\n      self.assertAllEqual(\n          self._LinSpace(0., 1., 42)[[0, -1]], np.array([0., 1.], np.float32))\n      self.assertAllEqual(\n          self._LinSpace(-1., 0., 42)[[0, -1]], np.array([-1., 0.], np.float32))\n      self.assertAllEqual(\n          self._LinSpace(.1, .2, 4)[[0, -1]], np.array([.1, .2], np.float32))\n      # Check a case for float64 error too.\n      self.assertAllEqual(\n          self._LinSpace(np.array(0., np.float64), .1, 12)[[0, -1]],\n          np.array([0., .1], np.float64))\n\n\nclass LinSpaceNdTest(test.TestCase):\n\n  def _gpu_modes(self):\n    if test.is_gpu_available():\n      return [False, True]\n    else:\n      return [False]\n\n  def _LinSpace(self, start, stop, num, axis=0):\n    with ops.Graph().as_default() as graph:\n      with self.session(graph=graph, force_gpu=self.force_gpu):\n        tf_ans = math_ops.linspace_nd(start, stop, num, axis=axis)\n        return self.evaluate(tf_ans)\n\n  def _LinSpaceNumConstant(self, start, stop, num, axis=0):\n    with ops.Graph().as_default() as graph:\n      num_constant = constant_op.constant(num)\n      with self.session(graph=graph, force_gpu=self.force_gpu):\n        tf_ans = math_ops.linspace_nd(start, stop, num_constant, axis=axis)\n        return self.evaluate(tf_ans)\n\n  def _LinspaceNoneShape(self, start, stop, num, graph_shape=None, axis=0):\n    with ops.Graph().as_default() as graph:\n      num_tensor = array_ops.placeholder(dtypes.int32)\n      start_t = array_ops.placeholder(dtypes.float32, shape=graph_shape)\n      stop_t = array_ops.placeholder(dtypes.float32, shape=graph_shape)\n      ans_tensor = math_ops.linspace_nd(start_t, stop_t, num_tensor, axis=axis)\n\n      with self.session(graph=graph, force_gpu=self.force_gpu) as sess:\n        feed_dict = {start_t: start, stop_t: stop, num_tensor: num}\n        return sess.run(ans_tensor, feed_dict=feed_dict)\n\n  def testPositive(self):\n    for self.force_gpu in self._gpu_modes():\n      self.assertArrayNear(self._LinSpace(1., 5., 1), np.array([1.]), 1e-5)\n      self.assertArrayNear(self._LinSpace(1., 5., 2), np.array([1., 5.]), 1e-5)\n      self.assertArrayNear(\n          self._LinSpace(1., 5., 3), np.array([1., 3., 5.]), 1e-5)\n      self.assertArrayNear(\n          self._LinSpace(1., 5., 4), np.array([1., 7. / 3., 11. / 3., 5.]),\n          1e-5)\n\n  def testNegative(self):\n    for self.force_gpu in self._gpu_modes():\n      self.assertArrayNear(self._LinSpace(-1., -5., 1), np.array([-1.]), 1e-5)\n      self.assertArrayNear(\n          self._LinSpace(-1., -5., 2), np.array([-1., -5.]), 1e-5)\n      self.assertArrayNear(\n          self._LinSpace(-1., -5., 3), np.array([-1., -3., -5.]), 1e-5)\n      self.assertArrayNear(\n          self._LinSpace(-1., -5., 4), np.array([-1., -7. / 3., -11. / 3.,\n                                                 -5.]), 1e-5)\n\n  def testNegativeToPositive(self):\n    for self.force_gpu in self._gpu_modes():\n      self.assertArrayNear(self._LinSpace(-1., 5., 1), np.array([-1.]), 1e-5)\n      self.assertArrayNear(\n          self._LinSpace(-1., 5., 2), np.array([-1., 5.]), 1e-5)\n      self.assertArrayNear(\n          self._LinSpace(-1., 5., 3), np.array([-1., 2., 5.]), 1e-5)\n      self.assertArrayNear(\n          self._LinSpace(-1., 5., 4), np.array([-1., 1., 3., 5.]), 1e-5)\n\n  def testPoint(self):\n    for self.force_gpu in self._gpu_modes():\n      self.assertArrayNear(self._LinSpace(5., 5., 1), np.array([5.]), 1e-5)\n      self.assertArrayNear(self._LinSpace(5., 5., 2), np.array([5.] * 2), 1e-5)\n      self.assertArrayNear(self._LinSpace(5., 5., 3), np.array([5.] * 3), 1e-5)\n      self.assertArrayNear(self._LinSpace(5., 5., 4), np.array([5.] * 4), 1e-5)\n\n  def testEndpointsAreExact(self):\n    for self.force_gpu in self._gpu_modes():\n      # Test some cases that produce last values not equal to \"stop\" when\n      # computed via start + (num - 1) * ((stop - start) / (num - 1)), since\n      # float arithmetic will introduce error through precision loss.\n      self.assertAllEqual(\n          self._LinSpace(0., 1., 42)[[0, -1]], np.array([0., 1.], np.float32))\n      self.assertAllEqual(\n          self._LinSpace(-1., 0., 42)[[0, -1]], np.array([-1., 0.], np.float32))\n      self.assertAllEqual(\n          self._LinSpace(.1, .2, 4)[[0, -1]], np.array([.1, .2], np.float32))\n      # Check a case for float64 error too.\n      self.assertAllEqual(\n          self._LinSpace(np.array(0., np.float64), .1, 12)[[0, -1]],\n          np.array([0., .1], np.float64))\n\n  def testScalarsCompareToNumpy(self):\n    for self.force_gpu in self._gpu_modes():\n      actual = self._LinSpace(0., 1., 32)\n      expected = np.linspace(0., 1., 32)\n      self.assertArrayNear(expected, actual, 1e-5)\n\n  def _baseNDArrayCompareToNumpy(self, axis):\n    for self.force_gpu in self._gpu_modes():\n      a, b, expected, num = self.create_nd_inputs_and_expected_output(axis)\n      actual = self._LinSpace(a, b, num, axis=axis)\n      self.assert_close(actual, expected)\n\n  def assert_close(self, actual, expected):\n    wrong_indices = np.where(~np.allclose(actual, expected))\n    mess = \"Wrong float answer. Wrong indices: {}\".format(wrong_indices)\n    self.assertTrue(np.allclose(actual, expected), mess)\n\n  def create_nd_inputs_and_expected_output(self, axis):\n    a = np.arange(2, dtype=np.float32)\n    b = a * 5\n    num = 5\n\n    res = np.array([[0., 0., 0., 0., 0.], [1., 2., 3., 4., 5.]])\n    expected = res if axis != 0 else res.T\n    return a, b, expected, num\n\n  def testNDArrayCompareToNumpyDefaultAxis(self):\n    self._baseNDArrayCompareToNumpy(0)\n\n  def testNDArrayAxisStrictlyPositive(self):\n    self._baseNDArrayCompareToNumpy(1)\n\n  def testNDArrayAxisStrictlyNegative(self):\n    self._baseNDArrayCompareToNumpy(-1)\n\n  def testNumConstant(self):\n    for self.force_gpu in self._gpu_modes():\n      actual = self._LinSpaceNumConstant(0., 1., 32)\n      expected = np.linspace(0., 1., 32)\n      self.assertArrayNear(expected, actual, 1e-5)\n\n  def testUnknownShapeAtGraphCreationTime(self):\n    self.base_test_unknown_shape((2))\n\n  def testNoneValuesInShapeAtGraphCreationTime(self):\n    self.base_test_unknown_shape((None))\n\n  def testNoneShapeAtGraphCreationTime(self):\n    self.base_test_unknown_shape(None)\n\n  def base_test_unknown_shape(self, graph_shape):\n    for self.force_gpu in self._gpu_modes():\n      axis = 1\n      a, b, expected, num = self.create_nd_inputs_and_expected_output(axis)\n      actual = self._LinspaceNoneShape(a, b, num, graph_shape, axis)\n      self.assert_close(actual, expected)\n\n\nclass DeviceTest(test.TestCase):\n\n  def testNoDevice(self):\n    with ops.Graph().as_default():\n      var = variables.Variable([[1.0, 1.0]])\n    self.assertDeviceEqual(None, var.device)\n    self.assertDeviceEqual(None, var.initializer.device)\n\n  def testDevice(self):\n    with ops.Graph().as_default():\n      with ops.device(\"/job:ps\"):\n        var = variables.Variable([[1.0, 1.0]])\n    self.assertDeviceEqual(\"/job:ps\", var.device)\n    self.assertDeviceEqual(\"/job:ps\", var.initializer.device)\n\n\nclass OrthogonalInitializerTest(test.TestCase):\n\n  @test_util.run_deprecated_v1\n  def testInitializerIdentical(self):\n    for dtype in [dtypes.float32, dtypes.float64]:\n      init1 = init_ops.orthogonal_initializer(seed=1, dtype=dtype)\n      init2 = init_ops.orthogonal_initializer(seed=1, dtype=dtype)\n      self.assertTrue(identicaltest(self, init1, init2, (10, 10)))\n\n  @test_util.run_deprecated_v1\n  def testInitializerDifferent(self):\n    for dtype in [dtypes.float32, dtypes.float64]:\n      init1 = init_ops.orthogonal_initializer(seed=1, dtype=dtype)\n      init2 = init_ops.orthogonal_initializer(seed=2, dtype=dtype)\n      self.assertFalse(identicaltest(self, init1, init2, (10, 10)))\n\n  @test_util.run_deprecated_v1\n  def testDuplicatedInitializer(self):\n    init = init_ops.orthogonal_initializer()\n    self.assertFalse(duplicated_initializer(self, init, 1, (10, 10)))\n\n  def testInvalidDataType(self):\n    self.assertRaises(\n        ValueError, init_ops.orthogonal_initializer, dtype=dtypes.string)\n\n  def testInvalidShape(self):\n    init1 = init_ops.orthogonal_initializer()\n    with self.session(graph=ops.Graph(), use_gpu=True):\n      self.assertRaises(ValueError, init1, shape=[5])\n\n  @test_util.run_deprecated_v1\n  def testGain(self):\n    shape = (10, 10)\n    for dtype in [dtypes.float32, dtypes.float64]:\n      init1 = init_ops.orthogonal_initializer(seed=1, dtype=dtype)\n      init2 = init_ops.orthogonal_initializer(gain=3.14, seed=1, dtype=dtype)\n      with self.session(graph=ops.Graph(), use_gpu=True):\n        t1 = init1(shape).eval()\n        t2 = init2(shape).eval()\n      self.assertAllClose(t1, t2 / 3.14)\n\n  @test_util.run_deprecated_v1\n  def testShapesValues(self):\n    for dtype in [dtypes.float32, dtypes.float64]:\n      for shape in [(10, 10), (10, 9, 8), (100, 5, 5), (50, 40), (40, 50)]:\n        init = init_ops.orthogonal_initializer(dtype=dtype)\n        tol = 1e-5 if dtype == dtypes.float32 else 1e-12\n        with self.session(graph=ops.Graph(), use_gpu=True):\n          # Check the shape\n          t = init(shape).eval()\n          self.assertAllEqual(shape, t.shape)\n          # Check orthogonality by computing the inner product\n          t = t.reshape((np.prod(t.shape[:-1]), t.shape[-1]))\n          if t.shape[0] > t.shape[1]:\n            self.assertAllClose(\n                np.dot(t.T, t), np.eye(t.shape[1]), rtol=tol, atol=tol)\n          else:\n            self.assertAllClose(\n                np.dot(t, t.T), np.eye(t.shape[0]), rtol=tol, atol=tol)\n\n\nclass ConvolutionDeltaOrthogonalInitializerTest(test.TestCase):\n\n  @test_util.run_deprecated_v1\n  def testInitializerIdentical(self):\n    for dtype in [dtypes.float32, dtypes.float64]:\n      init1 = init_ops.convolutional_delta_orthogonal(seed=1, dtype=dtype)\n      init2 = init_ops.convolutional_delta_orthogonal(seed=1, dtype=dtype)\n      self.assertTrue(identicaltest(self, init1, init2, (3, 3, 10, 10)))\n\n  @test_util.run_deprecated_v1\n  def testInitializerDifferent(self):\n    for dtype in [dtypes.float32, dtypes.float64]:\n      init1 = init_ops.convolutional_delta_orthogonal(seed=1, dtype=dtype)\n      init2 = init_ops.convolutional_delta_orthogonal(seed=2, dtype=dtype)\n      self.assertFalse(identicaltest(self, init1, init2, (3, 3, 10, 10)))\n\n  @test_util.run_deprecated_v1\n  def testDuplicatedInitializer(self):\n    init = init_ops.convolutional_delta_orthogonal()\n    self.assertFalse(duplicated_initializer(self, init, 1, (3, 3, 10, 10)))\n\n  def testInvalidDataType(self):\n    self.assertRaises(\n        ValueError,\n        init_ops.convolutional_delta_orthogonal,\n        dtype=dtypes.string)\n\n  def testInvalidShape(self):\n    init1 = init_ops.convolutional_delta_orthogonal()\n    with self.session(graph=ops.Graph(), use_gpu=True):\n      self.assertRaises(ValueError, init1, shape=[3, 3, 6, 5])\n\n  @test_util.run_deprecated_v1\n  def testGain(self):\n    shape = (3, 3, 10, 10)\n    for dtype in [dtypes.float32, dtypes.float64]:\n      init1 = init_ops.convolutional_delta_orthogonal(seed=1, dtype=dtype)\n      init2 = init_ops.convolutional_delta_orthogonal(\n          gain=3.14, seed=1, dtype=dtype)\n      with self.session(graph=ops.Graph(), use_gpu=True):\n        t1 = init1(shape).eval()\n        t2 = init2(shape).eval()\n      self.assertAllClose(t1, t2 / 3.14)\n\n  @test_util.run_deprecated_v1\n  def testShapesValues(self):\n    gain = 3.14\n    for dtype in [dtypes.float32]:\n      for kernel_size in [[3], [8], [3, 5], [2, 4], [3, 3, 3], [2, 2, 2]]:\n        tol = 1e-2\n        # Check orthogonality by computing ratio between\n        # the 2-norms of the inputs and outputs.\n        if len(kernel_size) == 1:\n          shape = [4, 32, 64]\n          convolution = convolutional.conv1d\n        elif len(kernel_size) == 2:\n          convolution = convolutional.conv2d\n          shape = [4, 32, 32, 64]\n        else:\n          shape = [4, 16, 16, 16, 64]\n          convolution = convolutional.conv3d\n        inputs = random_ops.random_normal(shape, dtype=dtype)\n        inputs_2norm = linalg_ops.norm(inputs)\n        outputs = convolution(\n            inputs,\n            padding=\"same\",\n            filters=128,\n            kernel_size=kernel_size,\n            use_bias=False,\n            kernel_initializer=init_ops.convolutional_delta_orthogonal(\n                gain=gain))\n        outputs_shape = shape[0:-1] + [128]\n        outputs_2norm = linalg_ops.norm(outputs)\n        ratio = outputs_2norm / inputs_2norm\n        my_ops = variables.global_variables_initializer()\n        with self.session():\n          self.evaluate(my_ops)\n          # Check the shape of the outputs\n          t = self.evaluate(outputs)\n          self.assertAllEqual(t.shape, outputs_shape)\n          # Check isometry of the delta-orthogonal kernel.\n          self.assertAllClose(self.evaluate(ratio), gain, rtol=tol, atol=tol)\n\n  @test_util.run_deprecated_v1\n  def testNonuniformity(self):\n    value = 0\n    abs_value = 0\n    shape = [3, 3, 10, 10]\n    count = 70\n    tol = 1e-5\n    with self.session():\n      for i in range(count):\n        x = variable_scope.get_variable(\n            \"{}\".format(i),\n            shape=shape,\n            initializer=init_ops.convolutional_delta_orthogonal)\n        self.evaluate(x.initializer)\n        y = self.evaluate(x)[1, 1, :, :]\n        determinant = np.linalg.det(y)\n        value += determinant\n        abs_value += np.abs(determinant)\n\n      # Check there is some variation in the signs of the determinants\n      self.assertLess(value, count - tol)\n      self.assertLess(-count + tol, value)\n      # Check all determinants have absolute value 1\n      # Compute the sum of the absolute values of 'count' determinants\n      self.assertAllClose(abs_value, count, rtol=tol, atol=tol)\n\n\n@test_util.run_all_without_tensor_float_32(\n    \"Tests convolutional_orthogonal_1d, which calls matmul\")\nclass ConvolutionOrthogonal1dInitializerTest(test.TestCase):\n\n  @test_util.run_deprecated_v1\n  def testInitializerIdentical(self):\n    for dtype in [dtypes.float32, dtypes.float64]:\n      init1 = init_ops.convolutional_orthogonal_1d(seed=1, dtype=dtype)\n      init2 = init_ops.convolutional_orthogonal_1d(seed=1, dtype=dtype)\n      self.assertTrue(identicaltest(self, init1, init2, (3, 10, 10)))\n\n  @test_util.run_deprecated_v1\n  def testInitializerDifferent(self):\n    for dtype in [dtypes.float32, dtypes.float64]:\n      init1 = init_ops.convolutional_orthogonal_1d(seed=1, dtype=dtype)\n      init2 = init_ops.convolutional_orthogonal_1d(seed=2, dtype=dtype)\n      self.assertFalse(identicaltest(self, init1, init2, (3, 10, 10)))\n\n  @test_util.run_deprecated_v1\n  def testDuplicatedInitializer(self):\n    init = init_ops.convolutional_orthogonal_1d()\n    self.assertFalse(duplicated_initializer(self, init, 1, (3, 10, 10)))\n\n  def testInvalidDataType(self):\n    self.assertRaises(\n        ValueError, init_ops.convolutional_orthogonal_1d, dtype=dtypes.string)\n\n  def testInvalidShape(self):\n    init1 = init_ops.convolutional_orthogonal_1d()\n    with self.session(graph=ops.Graph(), use_gpu=True):\n      self.assertRaises(ValueError, init1, shape=[3, 6, 5])\n\n  @test_util.run_deprecated_v1\n  def testGain(self):\n    shape = (3, 10, 10)\n    for dtype in [dtypes.float32, dtypes.float64]:\n      init1 = init_ops.convolutional_orthogonal_1d(seed=1, dtype=dtype)\n      init2 = init_ops.convolutional_orthogonal_1d(\n          gain=3.14, seed=1, dtype=dtype)\n      with self.session(graph=ops.Graph(), use_gpu=True):\n        t1 = init1(shape).eval()\n        t2 = init2(shape).eval()\n      self.assertAllClose(t1, t2 / 3.14)\n\n  @test_util.run_deprecated_v1\n  def testNonuniformity(self):\n    value = 0\n    abs_value = 0\n    shape = [3, 10, 10]\n    count = 70\n    tol = 1e-5\n    with self.session():\n      for i in range(count):\n        x = variable_scope.get_variable(\n            \"{}\".format(i),\n            shape=shape,\n            initializer=init_ops.convolutional_orthogonal_1d)\n        self.evaluate(x.initializer)\n        y = np.sum(self.evaluate(x), axis=0)\n        determinant = np.linalg.det(y)\n        value += determinant\n        abs_value += np.abs(determinant)\n\n      # Check there is some variation in the signs of the determinants.\n      self.assertLess(value, count - tol)\n      self.assertLess(-count + tol, value)\n      # Check all determinants have absolute value 1\n      # Compute the sum of the absolute values of 'count' determinants\n      self.assertAllClose(abs_value, count, rtol=tol, atol=tol)\n\n  @test_util.run_deprecated_v1\n  def testShapesValues(self):\n\n    def circular_pad(input_, width, kernel_size):\n      \"\"\"Pad input_ for computing (circular) convolution.\n\n      Args:\n        input_: the input tensor\n        width: the width of the tensor.\n        kernel_size: the kernel size of the filter.\n\n      Returns:\n        a tensor whose width is (width + kernel_size - 1).\n      \"\"\"\n\n      beginning = kernel_size // 2\n      end = kernel_size - 1 - beginning\n\n      tmp_up = array_ops.slice(input_, [0, width - beginning, 0],\n                               [-1, beginning, -1])\n      tmp_down = array_ops.slice(input_, [0, 0, 0], [-1, end, -1])\n      tmp = array_ops.concat([tmp_up, input_, tmp_down], 1)\n\n      return tmp\n\n    cout = 64\n    shape = [10, 20, 32]\n    outputs_shape = shape[0:-1] + [cout]\n    dtype = dtypes.float32\n    tol = 1e-3\n    gain = 3.14\n    # Check orthogonality/isometry by computing the ratio between\n    # the 2-norms of the inputs and outputs.\n    for kernel_size in [[1], [2], [3], [4], [5], [6]]:\n      convolution = convolutional.conv1d\n      inputs = random_ops.random_normal(shape, dtype=dtype)\n      inputs_2norm = linalg_ops.norm(inputs)\n      input_with_circular_pad = circular_pad(inputs, shape[1], kernel_size[0])\n      outputs = convolution(\n          input_with_circular_pad,\n          padding=\"valid\",\n          filters=cout,\n          kernel_size=kernel_size[0],\n          use_bias=False,\n          kernel_initializer=init_ops.convolutional_orthogonal_1d(gain=gain))\n      outputs_2norm = linalg_ops.norm(outputs)\n      ratio = outputs_2norm / inputs_2norm\n      my_ops = variables.global_variables_initializer()\n      with self.session():\n        self.evaluate(my_ops)\n        # Check the shape of the outputs\n        t = self.evaluate(outputs)\n        self.assertAllEqual(t.shape, outputs_shape)\n        # Check isometry of the orthogonal kernel.\n        self.assertAllClose(self.evaluate(ratio), gain, rtol=tol, atol=tol)\n\n\nclass ConvolutionOrthogonal2dInitializerTest(test.TestCase):\n\n  @test_util.run_deprecated_v1\n  def testInitializerIdentical(self):\n    for dtype in [dtypes.float32, dtypes.float64]:\n      init1 = init_ops.convolutional_orthogonal_2d(seed=1, dtype=dtype)\n      init2 = init_ops.convolutional_orthogonal_2d(seed=1, dtype=dtype)\n      self.assertTrue(identicaltest(self, init1, init2, (3, 3, 10, 10)))\n\n  @test_util.run_deprecated_v1\n  def testInitializerDifferent(self):\n    for dtype in [dtypes.float32, dtypes.float64]:\n      init1 = init_ops.convolutional_orthogonal_2d(seed=1, dtype=dtype)\n      init2 = init_ops.convolutional_orthogonal_2d(seed=2, dtype=dtype)\n      self.assertFalse(identicaltest(self, init1, init2, (3, 3, 10, 10)))\n\n  @test_util.run_deprecated_v1\n  def testDuplicatedInitializer(self):\n    init = init_ops.convolutional_orthogonal_2d()\n    self.assertFalse(duplicated_initializer(self, init, 1, (3, 3, 10, 10)))\n\n  def testInvalidDataType(self):\n    self.assertRaises(\n        ValueError, init_ops.convolutional_orthogonal_2d, dtype=dtypes.string)\n\n  def testInvalidShape(self):\n    init1 = init_ops.convolutional_orthogonal_2d()\n    with self.session(graph=ops.Graph(), use_gpu=True):\n      self.assertRaises(ValueError, init1, shape=[3, 3, 6, 5])\n\n  @test_util.run_deprecated_v1\n  def testGain(self):\n    shape = (3, 3, 10, 10)\n    for dtype in [dtypes.float32, dtypes.float64]:\n      init1 = init_ops.convolutional_orthogonal_2d(seed=1, dtype=dtype)\n      init2 = init_ops.convolutional_orthogonal_2d(\n          gain=3.14, seed=1, dtype=dtype)\n      with self.session(graph=ops.Graph(), use_gpu=True):\n        t1 = init1(shape).eval()\n        t2 = init2(shape).eval()\n      self.assertAllClose(t1, t2 / 3.14)\n\n  @test_util.run_deprecated_v1\n  def testShapesValues(self):\n\n    def circular_pad(input_, width, kernel_size):\n      \"\"\"Pad input_ for computing (circular) convolution.\n\n      Args:\n        input_: the input tensor\n        width: the width of the tensor.\n        kernel_size: the kernel size of the filter.\n\n      Returns:\n        a tensor whose width is (width + kernel_size - 1).\n      \"\"\"\n      beginning = kernel_size // 2\n      end = kernel_size - 1 - beginning\n\n      tmp_up = array_ops.slice(input_, [0, width - beginning, 0, 0],\n                               [-1, beginning, width, -1])\n      tmp_down = array_ops.slice(input_, [0, 0, 0, 0], [-1, end, width, -1])\n      tmp = array_ops.concat([tmp_up, input_, tmp_down], 1)\n\n      new_width = width + kernel_size - 1\n      tmp_left = array_ops.slice(tmp, [0, 0, width - beginning, 0],\n                                 [-1, new_width, beginning, -1])\n      tmp_right = array_ops.slice(tmp, [0, 0, 0, 0], [-1, new_width, end, -1])\n\n      final = array_ops.concat([tmp_left, tmp, tmp_right], 2)\n      return final\n\n    cout = 45\n    shape = [64, 28, 28, 32]\n    outputs_shape = shape[0:-1] + [cout]\n    dtype = dtypes.float32\n    tol = 1e-3\n    gain = 3.14\n    # Check orthogonality/isometry by computing the ratio between\n    # the 2-norms of the inputs and outputs.\n    for kernel_size in [[1, 1], [2, 2], [3, 3], [4, 4], [5, 5]]:\n      convolution = convolutional.conv2d\n      inputs = random_ops.random_normal(shape, dtype=dtype)\n      inputs_2norm = linalg_ops.norm(inputs)\n      input_with_circular_pad = circular_pad(inputs, shape[1], kernel_size[0])\n      outputs = convolution(\n          input_with_circular_pad,\n          padding=\"valid\",\n          filters=cout,\n          kernel_size=kernel_size,\n          use_bias=False,\n          kernel_initializer=init_ops.convolutional_orthogonal_2d(gain=gain))\n      outputs_2norm = linalg_ops.norm(outputs)\n      ratio = outputs_2norm / inputs_2norm\n      my_ops = variables.global_variables_initializer()\n      with self.session():\n        self.evaluate(my_ops)\n        # Check the shape of the outputs\n        t = self.evaluate(outputs)\n        self.assertAllEqual(t.shape, outputs_shape)\n        # Check isometry of the orthogonal kernel.\n        self.assertAllClose(self.evaluate(ratio), gain, rtol=tol, atol=tol)\n\n\n@test_util.run_all_without_tensor_float_32(\n    \"Tests convolutional_orthogonal_3d, which calls matmul\")\nclass ConvolutionOrthogonal3dInitializerTest(test.TestCase):\n\n  @test_util.run_deprecated_v1\n  def testInitializerIdentical(self):\n    for dtype in [dtypes.float32, dtypes.float64]:\n      init1 = init_ops.convolutional_orthogonal_3d(seed=1, dtype=dtype)\n      init2 = init_ops.convolutional_orthogonal_3d(seed=1, dtype=dtype)\n      self.assertTrue(identicaltest(self, init1, init2, (3, 3, 3, 10, 10)))\n\n  @test_util.run_deprecated_v1\n  def testInitializerDifferent(self):\n    for dtype in [dtypes.float32, dtypes.float64]:\n      init1 = init_ops.convolutional_orthogonal_3d(seed=1, dtype=dtype)\n      init2 = init_ops.convolutional_orthogonal_3d(seed=2, dtype=dtype)\n      self.assertFalse(identicaltest(self, init1, init2, (3, 3, 3, 10, 10)))\n\n  @test_util.run_deprecated_v1\n  def testDuplicatedInitializer(self):\n    init = init_ops.convolutional_orthogonal_3d()\n    self.assertFalse(duplicated_initializer(self, init, 1, (3, 3, 3, 10, 10)))\n\n  def testInvalidDataType(self):\n    self.assertRaises(\n        ValueError, init_ops.convolutional_orthogonal_3d, dtype=dtypes.string)\n\n  def testInvalidShape(self):\n    init1 = init_ops.convolutional_orthogonal_3d()\n    with self.session(graph=ops.Graph(), use_gpu=True):\n      self.assertRaises(ValueError, init1, shape=[3, 3, 3, 6, 5])\n\n  @test_util.run_deprecated_v1\n  def testGain(self):\n    shape = (3, 3, 3, 10, 10)\n    for dtype in [dtypes.float32, dtypes.float64]:\n      init1 = init_ops.convolutional_orthogonal_3d(seed=1, dtype=dtype)\n      init2 = init_ops.convolutional_orthogonal_3d(\n          gain=3.14, seed=1, dtype=dtype)\n      with self.session(graph=ops.Graph(), use_gpu=True):\n        t1 = init1(shape).eval()\n        t2 = init2(shape).eval()\n      self.assertAllClose(t1, t2 / 3.14)\n\n  @test_util.run_deprecated_v1\n  def testNonuniformity(self):\n    value = 0\n    abs_value = 0\n    shape = [3, 3, 3, 5, 5]\n    count = 20\n    tol = 1e-5\n    with self.session():\n      for i in range(count):\n        x = variable_scope.get_variable(\n            \"{}\".format(i),\n            shape=shape,\n            initializer=init_ops.convolutional_orthogonal_3d)\n        self.evaluate(x.initializer)\n        y = np.sum(self.evaluate(x), axis=(0, 1, 2))\n        determinant = np.linalg.det(y)\n        value += determinant\n        abs_value += np.abs(determinant)\n\n      # Check there is some variation in the signs of the determinants\n      self.assertLess(value, count - tol)\n      self.assertLess(-count + tol, value)\n      # Check all determinants have absolute value 1\n      # Compute the sum of the absolute values of 'count' determinants\n      self.assertAllClose(abs_value, count, rtol=tol, atol=tol)\n\n  @test_util.run_deprecated_v1\n  def testShapesValues(self):\n\n    def circular_pad(input_, width, kernel_size):\n      \"\"\"Padding input_ for computing circular convolution.\n\n      Args:\n        input_: the input tensor\n        width: the width of the tensor.\n        kernel_size: the kernel size of the filter.\n\n      Returns:\n        a tensor whose width is (width + kernel_size - 1).\n      \"\"\"\n\n      beginning = kernel_size // 2\n      end = kernel_size - 1 - beginning\n\n      tmp_up = array_ops.slice(input_, [0, width - beginning, 0, 0, 0],\n                               [-1, beginning, -1, -1, -1])\n      tmp_down = array_ops.slice(input_, [0, 0, 0, 0, 0], [-1, end, -1, -1, -1])\n      tmp = array_ops.concat([tmp_up, input_, tmp_down], 1)\n\n      tmp_left = array_ops.slice(tmp, [0, 0, width - beginning, 0, 0],\n                                 [-1, -1, beginning, -1, -1])\n      tmp_right = array_ops.slice(tmp, [0, 0, 0, 0, 0], [-1, -1, end, -1, -1])\n      tmp = array_ops.concat([tmp_left, tmp, tmp_right], 2)\n\n      tmp_front = array_ops.slice(tmp, [0, 0, 0, width - beginning, 0],\n                                  [-1, -1, -1, beginning, -1])\n      tmp_back = array_ops.slice(tmp, [0, 0, 0, 0, 0], [-1, -1, -1, end, -1])\n      return array_ops.concat([tmp_front, tmp, tmp_back], 3)\n\n    cout = 32\n    shape = [1, 7, 7, 7, 16]\n    outputs_shape = shape[0:-1] + [cout]\n    dtype = dtypes.float32\n    tol = 1e-3\n    gain = 3.14\n    # Check orthogonality/isometry by computing the ratio between\n    # the 2-norms of the inputs and outputs.\n    for kernel_size in [[1, 1, 1], [2, 2, 2], [3, 3, 3]]:\n      convolution = convolutional.conv3d\n      inputs = random_ops.random_normal(shape, dtype=dtype)\n      inputs_2norm = linalg_ops.norm(inputs)\n      input_with_circular_pad = circular_pad(inputs, shape[1], kernel_size[0])\n      outputs = convolution(\n          input_with_circular_pad,\n          padding=\"valid\",\n          filters=cout,\n          kernel_size=kernel_size[0],\n          use_bias=False,\n          kernel_initializer=init_ops.convolutional_orthogonal_3d(gain=gain))\n      outputs_2norm = linalg_ops.norm(outputs)\n      ratio = outputs_2norm / inputs_2norm\n      my_ops = variables.global_variables_initializer()\n      with self.cached_session():\n        self.evaluate(my_ops)\n        # Check the shape of the outputs\n        t = self.evaluate(outputs)\n        self.assertAllEqual(t.shape, outputs_shape)\n        # Check isometry of the orthogonal kernel.\n        self.assertAllClose(self.evaluate(ratio), gain, rtol=tol, atol=tol)\n\n\nclass IdentityInitializerTest(test.TestCase):\n\n  def testInvalidDataType(self):\n    self.assertRaises(\n        ValueError, init_ops.orthogonal_initializer, dtype=dtypes.string)\n\n  def testInvalidShape(self):\n    init = init_ops.identity_initializer()\n    with self.session(graph=ops.Graph(), use_gpu=True):\n      self.assertRaises(ValueError, init, shape=[5, 7, 7])\n      self.assertRaises(ValueError, init, shape=[5])\n      self.assertRaises(ValueError, init, shape=[])\n\n  @test_util.run_deprecated_v1\n  def testNonSquare(self):\n    init = init_ops.identity_initializer()\n    shape = (10, 5)\n    with self.session(graph=ops.Graph(), use_gpu=True):\n      self.assertAllClose(init(shape), np.eye(*shape))\n\n  @test_util.run_deprecated_v1\n  def testGain(self):\n    shape = (10, 10)\n    for dtype in [dtypes.float32, dtypes.float64]:\n      init_default = init_ops.identity_initializer(dtype=dtype)\n      init_custom = init_ops.identity_initializer(gain=0.9, dtype=dtype)\n      with self.session(graph=ops.Graph(), use_gpu=True):\n        self.assertAllClose(init_default(shape), np.eye(*shape))\n      with self.session(graph=ops.Graph(), use_gpu=True):\n        self.assertAllClose(init_custom(shape), np.eye(*shape) * 0.9)\n\n  @test_util.run_deprecated_v1\n  def testPartitions(self):\n    shape = (10, 10)\n    init = init_ops.identity_initializer()\n    partitioner = partitioned_variables.variable_axis_size_partitioner(1)\n    with self.session(graph=ops.Graph(), use_gpu=True):\n      with variable_scope.variable_scope(\n          \"foo\", partitioner=partitioner, initializer=init):\n        v = array_ops.identity(variable_scope.get_variable(\"bar\", shape=shape))\n      self.evaluate(variables.global_variables_initializer())\n      self.assertAllClose(v, np.eye(*shape))\n\n\nif __name__ == \"__main__\":\n  test.main()\n"], "fixing_code": ["/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n// See docs in ../ops/math_ops.cc.\n\n#include <cmath>\n\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/register_types.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/tensor_shape.h\"\n#include \"tensorflow/core/framework/types.h\"\n\nnamespace tensorflow {\n\nint32 GetValue(int32_t v) { return v; }\n\ntemplate <typename T>\nclass RangeOp : public OpKernel {\n public:\n  explicit RangeOp(OpKernelConstruction* context) : OpKernel(context) {}\n\n  void Compute(OpKernelContext* context) override {\n    const Tensor& start_in = context->input(0);\n    const Tensor& limit_in = context->input(1);\n    const Tensor& delta_in = context->input(2);\n    // TODO(rmlarsen): Disallow legacy use of length-1 vectors as scalars.\n    OP_REQUIRES(context,\n                TensorShapeUtils::IsScalar(start_in.shape()) ||\n                    (TensorShapeUtils::IsVector(start_in.shape()) &&\n                     start_in.shape().dim_size(0) == 1),\n                errors::InvalidArgument(\"start must be a scalar, not shape \",\n                                        start_in.shape().DebugString()));\n    OP_REQUIRES(context,\n                TensorShapeUtils::IsScalar(limit_in.shape()) ||\n                    (TensorShapeUtils::IsVector(limit_in.shape()) &&\n                     limit_in.shape().dim_size(0) == 1),\n                errors::InvalidArgument(\"limit must be a scalar, not shape \",\n                                        limit_in.shape().DebugString()));\n    OP_REQUIRES(context,\n                TensorShapeUtils::IsScalar(delta_in.shape()) ||\n                    (TensorShapeUtils::IsVector(delta_in.shape()) &&\n                     delta_in.shape().dim_size(0) == 1),\n                errors::InvalidArgument(\"delta must be a scalar, not shape \",\n                                        delta_in.shape().DebugString()));\n    const T start = start_in.scalar<T>()();\n    const T limit = limit_in.scalar<T>()();\n    const T delta = delta_in.scalar<T>()();\n    OP_REQUIRES(context, delta != 0,\n                errors::InvalidArgument(\"Requires delta != 0: \", delta));\n    if (delta > 0) {\n      OP_REQUIRES(\n          context, start <= limit,\n          errors::InvalidArgument(\n              \"Requires start <= limit when delta > 0: \", start, \"/\", limit));\n    } else {\n      OP_REQUIRES(\n          context, start >= limit,\n          errors::InvalidArgument(\n              \"Requires start >= limit when delta < 0: \", start, \"/\", limit));\n    }\n    int64_t size = 0;\n    if (std::is_integral<T>::value) {\n      size = static_cast<int64>(\n          (std::abs(limit - start) + std::abs(delta) - 1) / std::abs(delta));\n    } else {\n      size = static_cast<int64>(std::ceil(std::abs((limit - start) / delta)));\n    }\n    TensorShape shape;\n    OP_REQUIRES_OK(context, shape.AddDimWithStatus(size));\n    Tensor* out = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(0, shape, &out));\n    auto flat = out->flat<T>();\n    T val = start;\n    for (int64_t i = 0; i < size; ++i) {\n      flat(i) = T(val);\n      val += delta;\n    }\n  }\n};\n\n#define REGISTER_KERNEL(DEV, TYPE)                           \\\n  REGISTER_KERNEL_BUILDER(Name(\"Range\")                      \\\n                              .Device(DEV)                   \\\n                              .HostMemory(\"start\")           \\\n                              .HostMemory(\"limit\")           \\\n                              .HostMemory(\"delta\")           \\\n                              .HostMemory(\"output\")          \\\n                              .TypeConstraint<TYPE>(\"Tidx\"), \\\n                          RangeOp<TYPE>);\n\n#define REGISTER_CPU_KERNEL(T) REGISTER_KERNEL(DEVICE_CPU, T)\n#define REGISTER_GPU_KERNEL(T) REGISTER_KERNEL(DEVICE_GPU, T)\n\nTF_CALL_float(REGISTER_CPU_KERNEL);\nTF_CALL_double(REGISTER_CPU_KERNEL);\nTF_CALL_int32(REGISTER_CPU_KERNEL);\nTF_CALL_int64(REGISTER_CPU_KERNEL);\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\nTF_CALL_float(REGISTER_GPU_KERNEL);\nTF_CALL_double(REGISTER_GPU_KERNEL);\nTF_CALL_int32(REGISTER_GPU_KERNEL);\nTF_CALL_int64(REGISTER_GPU_KERNEL);\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n#undef REGISTER_KERNEL\n#undef REGISTER_CPU_KERNEL\n#undef REGISTER_GPU_KERNEL\n\ntemplate <typename T, typename Tnum>\nclass LinSpaceOp : public OpKernel {\n public:\n  explicit LinSpaceOp(OpKernelConstruction* context) : OpKernel(context) {}\n\n  void Compute(OpKernelContext* context) override {\n    const Tensor& start_in = context->input(0);\n    const Tensor& stop_in = context->input(1);\n    const Tensor& num_in = context->input(2);\n    OP_REQUIRES(context, TensorShapeUtils::IsScalar(start_in.shape()),\n                errors::InvalidArgument(\"start must be a scalar, not shape \",\n                                        start_in.shape().DebugString()));\n    OP_REQUIRES(context, TensorShapeUtils::IsScalar(stop_in.shape()),\n                errors::InvalidArgument(\"stop must be a scalar, not shape \",\n                                        stop_in.shape().DebugString()));\n    OP_REQUIRES(context, TensorShapeUtils::IsScalar(num_in.shape()),\n                errors::InvalidArgument(\"num must be a scalar, not shape \",\n                                        num_in.shape().DebugString()));\n    const T start = start_in.scalar<T>()();\n    const T stop = stop_in.scalar<T>()();\n    const Tnum num = num_in.scalar<Tnum>()();\n    OP_REQUIRES(context, num > 0,\n                errors::InvalidArgument(\"Requires num > 0: \", num));\n    Tensor* out = nullptr;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(0, TensorShape({num}), &out));\n    auto flat = out->flat<T>();\n    flat(0) = start;\n    if (num > 1) {\n      const T step = (stop - start) / (num - 1);\n      for (Tnum i = 1; i < num - 1; ++i) flat(i) = start + step * i;\n      // Ensure final value == stop; float arithmetic won't guarantee this.\n      flat(num - 1) = stop;\n    }\n  }\n};\n\n#define REGISTER_KERNEL(DEV, T, Tidx)                       \\\n  REGISTER_KERNEL_BUILDER(Name(\"LinSpace\")                  \\\n                              .Device(DEV)                  \\\n                              .TypeConstraint<T>(\"T\")       \\\n                              .TypeConstraint<Tidx>(\"Tidx\") \\\n                              .HostMemory(\"start\")          \\\n                              .HostMemory(\"stop\")           \\\n                              .HostMemory(\"num\")            \\\n                              .HostMemory(\"output\"),        \\\n                          LinSpaceOp<T, Tidx>);\n\n#define REGISTER_KERNEL_ALL_NUMS(dev, T) \\\n  REGISTER_KERNEL(dev, T, int32);        \\\n  REGISTER_KERNEL(dev, T, int64_t)\n\n#define REGISTER_CPU_KERNEL(T) REGISTER_KERNEL_ALL_NUMS(DEVICE_CPU, T)\nTF_CALL_float(REGISTER_CPU_KERNEL);\nTF_CALL_double(REGISTER_CPU_KERNEL);\n\n// NOTE(touts): We register the op on GPU but it still runs on CPU\n// because its inputs and outputs are tagged as HostMemory.\n#define REGISTER_GPU_KERNEL(T) REGISTER_KERNEL_ALL_NUMS(DEVICE_GPU, T)\nTF_CALL_float(REGISTER_GPU_KERNEL);\nTF_CALL_double(REGISTER_GPU_KERNEL);\n#undef REGISTER_GPU_KERNEL\n\n\n#undef REGISTER_CPU_KERNEL\n#undef REGISTER_KERNEL_ALL_NUMS\n#undef REGISTER_KERNEL\n\n}  // namespace tensorflow\n", "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for tensorflow.ops.ops.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\n\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors_impl\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import random_seed\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.layers import convolutional\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import init_ops\nfrom tensorflow.python.ops import linalg_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import partitioned_variables\nfrom tensorflow.python.ops import random_ops\nfrom tensorflow.python.ops import variable_scope\nfrom tensorflow.python.ops import variables\nfrom tensorflow.python.platform import test\n\n\n# Returns true iff the two initializers produce the same tensor to\n# within a tiny tolerance.\ndef identicaltest(tc, init1, init2, shape=None):\n  \"\"\"Tests if two initializations are identical to within tiny tolerances.\n\n  Args:\n    tc: An instance of TensorFlowTestCase.\n    init1: An Initializer that generates a tensor of a given shape\n    init2: An Initializer that generates a tensor of a given shape\n    shape: Shape of the tensor to initialize or `None` to use a vector of length\n      100.\n\n  Returns:\n    True or False as determined by test.\n  \"\"\"\n  if shape is None:\n    shape = [100]\n  with tc.test_session(graph=ops.Graph()):\n    t1 = init1(shape).eval()\n  with tc.test_session(graph=ops.Graph()):\n    t2 = init2(shape).eval()\n  return np.allclose(t1, t2, rtol=1e-15, atol=1e-15)\n\n\ndef duplicated_initializer(tc, init, graph_seed, shape=None):\n  \"\"\"Tests duplicated random initializer within the same graph.\n\n  This test generates two random kernels from the same initializer to the same\n  graph, and checks if the results are close enough. Even given the same global,\n  seed, two different instances of random kernels should generate different\n  results.\n\n  Args:\n    tc: An instance of TensorFlowTestCase.\n    init: An Initializer that generates a tensor of a given shape\n    graph_seed: A graph-level seed to use.\n    shape: Shape of the tensor to initialize or `None` to use a vector of length\n      100.\n\n  Returns:\n    True or False as determined by test.\n  \"\"\"\n  if shape is None:\n    shape = [100]\n  with tc.test_session(graph=ops.Graph()):\n    random_seed.set_random_seed(graph_seed)\n    t1 = init(shape).eval()\n    t2 = init(shape).eval()\n    return np.allclose(t1, t2, rtol=1e-15, atol=1e-15)\n\n\ndef _init_sampler(tc, init, num):\n  \"\"\"Returns a func to generate a random tensor of shape [num].\n\n  Args:\n    tc: An instance of TensorFlowTestCase.\n    init: An Initializer that generates a tensor of a given shape\n    num: Size of 1D tensor to create.\n\n  Returns:\n    Function to generate a random tensor.\n  \"\"\"\n\n  def func():\n    with tc.test_session():\n      return init([num]).eval()\n\n  return func\n\n\nclass ConstantInitializersTest(test.TestCase):\n\n  @test_util.run_deprecated_v1\n  def testZerosInitializer(self):\n    with self.session():\n      shape = [2, 3]\n      x = variable_scope.get_variable(\n          \"x\", shape=shape, initializer=init_ops.zeros_initializer())\n      self.evaluate(x.initializer)\n      self.assertAllEqual(x, np.zeros(shape))\n\n  @test_util.run_deprecated_v1\n  def testOnesInitializer(self):\n    with self.session():\n      shape = [2, 3]\n      x = variable_scope.get_variable(\n          \"x\", shape=shape, initializer=init_ops.ones_initializer())\n      self.evaluate(x.initializer)\n      self.assertAllEqual(x, np.ones(shape))\n\n  @test_util.run_deprecated_v1\n  def testConstantZeroInitializer(self):\n    with self.session():\n      shape = [2, 3]\n      x = variable_scope.get_variable(\n          \"x\", shape=shape, initializer=init_ops.constant_initializer(0.0))\n      self.evaluate(x.initializer)\n      self.assertAllEqual(x, np.zeros(shape))\n\n  @test_util.run_deprecated_v1\n  def testConstantOneInitializer(self):\n    with self.session():\n      shape = [2, 3]\n      x = variable_scope.get_variable(\n          \"x\", shape=shape, initializer=init_ops.constant_initializer(1.0))\n      self.evaluate(x.initializer)\n      self.assertAllEqual(x, np.ones(shape))\n\n  @test_util.run_deprecated_v1\n  def testConstantIntInitializer(self):\n    with self.session():\n      shape = [2, 3]\n      x = variable_scope.get_variable(\n          \"x\",\n          shape=shape,\n          dtype=dtypes.int32,\n          initializer=init_ops.constant_initializer(7))\n      self.evaluate(x.initializer)\n      self.assertEqual(x.dtype.base_dtype, dtypes.int32)\n      self.assertAllEqual(x, 7 * np.ones(shape, dtype=np.int32))\n\n  @test_util.run_deprecated_v1\n  def testConstantTupleInitializer(self):\n    with self.session():\n      shape = [3]\n      x = variable_scope.get_variable(\n          \"x\",\n          shape=shape,\n          dtype=dtypes.int32,\n          initializer=init_ops.constant_initializer((10, 20, 30)))\n      self.evaluate(x.initializer)\n      self.assertEqual(x.dtype.base_dtype, dtypes.int32)\n      self.assertAllEqual(x, [10, 20, 30])\n\n  def _testNDimConstantInitializer(self, name, value, shape, expected):\n    with self.cached_session():\n      init = init_ops.constant_initializer(value, dtype=dtypes.int32)\n      x = variable_scope.get_variable(name, shape=shape, initializer=init)\n      self.evaluate(x.initializer)\n\n      actual = array_ops.reshape(x, [-1]).eval()\n      self.assertEqual(len(actual), len(expected))\n      for a, e in zip(actual, expected):\n        self.assertEqual(a, e)\n\n  @test_util.run_deprecated_v1\n  def testNDimConstantInitializer(self):\n    value = [0, 1, 2, 3, 4, 5]\n    shape = [2, 3]\n    expected = list(value)\n\n    self._testNDimConstantInitializer(\"list\", value, shape, expected)\n    self._testNDimConstantInitializer(\"ndarray\", np.asarray(value), shape,\n                                      expected)\n    self._testNDimConstantInitializer(\"2D-ndarray\",\n                                      np.asarray(value).reshape(tuple(shape)),\n                                      shape, expected)\n\n  def _testNDimConstantInitializerLessValues(self, name, value, shape,\n                                             expected):\n    with self.cached_session():\n      init = init_ops.constant_initializer(value, dtype=dtypes.int32)\n      x = variable_scope.get_variable(name, shape=shape, initializer=init)\n      self.evaluate(x.initializer)\n\n      actual = array_ops.reshape(x, [-1]).eval()\n      self.assertGreater(len(actual), len(expected))\n      for i in xrange(len(actual)):\n        a = actual[i]\n        e = expected[i] if i < len(expected) else expected[-1]\n        self.assertEqual(a, e)\n\n  @test_util.run_deprecated_v1\n  def testNDimConstantInitializerLessValues(self):\n    value = [0, 1, 2, 3, 4, 5]\n    shape = [2, 4]\n    expected = list(value)\n\n    self._testNDimConstantInitializerLessValues(\"list\", value, shape, expected)\n    self._testNDimConstantInitializerLessValues(\"ndarray\", np.asarray(value),\n                                                shape, expected)\n    self._testNDimConstantInitializerLessValues(\n        \"2D-ndarray\",\n        np.asarray(value).reshape(tuple([2, 3])), shape, expected)\n\n  def _testNDimConstantInitializerMoreValues(self, value, shape):\n    ops.reset_default_graph()\n    with self.cached_session():\n      init = init_ops.constant_initializer(value, dtype=dtypes.int32)\n      self.assertRaises(\n          ValueError,\n          variable_scope.get_variable,\n          \"x\",\n          shape=shape,\n          initializer=init)\n\n  @test_util.run_deprecated_v1\n  def testNDimConstantInitializerMoreValues(self):\n    value = [0, 1, 2, 3, 4, 5, 6, 7]\n    shape = [2, 3]\n    self._testNDimConstantInitializerMoreValues(value, shape)\n    self._testNDimConstantInitializerMoreValues(np.asarray(value), shape)\n    self._testNDimConstantInitializerMoreValues(\n        np.asarray(value).reshape(tuple([2, 4])), shape)\n\n  def testInvalidValueTypeForConstantInitializerCausesTypeError(self):\n    c = constant_op.constant([1.0, 2.0, 3.0])\n    with self.assertRaisesRegex(TypeError,\n                                r\"Invalid type for initial value: .*Tensor.*\"):\n      init_ops.constant_initializer(c, dtype=dtypes.float32)\n    v = variables.Variable([3.0, 2.0, 1.0])\n    with self.assertRaisesRegex(\n        TypeError, r\"Invalid type for initial value: .*Variable.*\"):\n      init_ops.constant_initializer(v, dtype=dtypes.float32)\n\n\nclass RandomNormalInitializationTest(test.TestCase):\n\n  @test_util.run_deprecated_v1\n  def testInitializerIdentical(self):\n    for dtype in [dtypes.float32, dtypes.float64]:\n      init1 = init_ops.random_normal_initializer(0.0, 1.0, seed=1, dtype=dtype)\n      init2 = init_ops.random_normal_initializer(0.0, 1.0, seed=1, dtype=dtype)\n      self.assertTrue(identicaltest(self, init1, init2))\n\n  @test_util.run_deprecated_v1\n  def testInitializerDifferent(self):\n    for dtype in [dtypes.float32, dtypes.float64]:\n      init1 = init_ops.random_normal_initializer(0.0, 1.0, seed=1, dtype=dtype)\n      init2 = init_ops.random_normal_initializer(0.0, 1.0, seed=2, dtype=dtype)\n      self.assertFalse(identicaltest(self, init1, init2))\n\n  @test_util.run_deprecated_v1\n  def testDuplicatedInitializer(self):\n    init = init_ops.random_normal_initializer(0.0, 1.0)\n    self.assertFalse(duplicated_initializer(self, init, 1))\n\n  def testInvalidDataType(self):\n    self.assertRaises(\n        ValueError,\n        init_ops.random_normal_initializer,\n        0.0,\n        1.0,\n        dtype=dtypes.string)\n\n\nclass TruncatedNormalInitializationTest(test.TestCase):\n\n  @test_util.run_deprecated_v1\n  def testInitializerIdentical(self):\n    for dtype in [dtypes.float32, dtypes.float64]:\n      init1 = init_ops.truncated_normal_initializer(\n          0.0, 1.0, seed=1, dtype=dtype)\n      init2 = init_ops.truncated_normal_initializer(\n          0.0, 1.0, seed=1, dtype=dtype)\n      self.assertTrue(identicaltest(self, init1, init2))\n\n  @test_util.run_deprecated_v1\n  def testInitializerDifferent(self):\n    for dtype in [dtypes.float32, dtypes.float64]:\n      init1 = init_ops.truncated_normal_initializer(\n          0.0, 1.0, seed=1, dtype=dtype)\n      init2 = init_ops.truncated_normal_initializer(\n          0.0, 1.0, seed=2, dtype=dtype)\n      self.assertFalse(identicaltest(self, init1, init2))\n\n  @test_util.run_deprecated_v1\n  def testDuplicatedInitializer(self):\n    init = init_ops.truncated_normal_initializer(0.0, 1.0)\n    self.assertFalse(duplicated_initializer(self, init, 1))\n\n  def testInvalidDataType(self):\n    self.assertRaises(\n        ValueError,\n        init_ops.truncated_normal_initializer,\n        0.0,\n        1.0,\n        dtype=dtypes.string)\n\n\nclass RandomUniformInitializationTest(test.TestCase):\n\n  @test_util.run_deprecated_v1\n  def testInitializerIdentical(self):\n    for dtype in [dtypes.float32, dtypes.float64, dtypes.int64]:\n      init1 = init_ops.random_uniform_initializer(0, 7, seed=1, dtype=dtype)\n      init2 = init_ops.random_uniform_initializer(0, 7, seed=1, dtype=dtype)\n      self.assertTrue(identicaltest(self, init1, init2))\n\n  @test_util.run_deprecated_v1\n  def testInitializerDifferent(self):\n    for dtype in [dtypes.float32, dtypes.float64, dtypes.int32, dtypes.int64]:\n      init1 = init_ops.random_uniform_initializer(0, 7, seed=1, dtype=dtype)\n      init2 = init_ops.random_uniform_initializer(0, 7, seed=2, dtype=dtype)\n      self.assertFalse(identicaltest(self, init1, init2))\n\n  @test_util.run_deprecated_v1\n  def testDuplicatedInitializer(self):\n    init = init_ops.random_uniform_initializer(0.0, 1.0)\n    self.assertFalse(duplicated_initializer(self, init, 1))\n\n\nclass UniformUnitScalingInitializationTest(test.TestCase):\n\n  @test_util.run_deprecated_v1\n  def testInitializerIdentical(self):\n    for dtype in [dtypes.float32, dtypes.float64]:\n      init1 = init_ops.uniform_unit_scaling_initializer(seed=1, dtype=dtype)\n      init2 = init_ops.uniform_unit_scaling_initializer(seed=1, dtype=dtype)\n      self.assertTrue(identicaltest(self, init1, init2))\n      init3 = init_ops.uniform_unit_scaling_initializer(\n          1.5, seed=1, dtype=dtype)\n      init4 = init_ops.uniform_unit_scaling_initializer(\n          1.5, seed=1, dtype=dtype)\n      self.assertTrue(identicaltest(self, init3, init4))\n\n  @test_util.run_deprecated_v1\n  def testInitializerDifferent(self):\n    for dtype in [dtypes.float32, dtypes.float64]:\n      init1 = init_ops.uniform_unit_scaling_initializer(seed=1, dtype=dtype)\n      init2 = init_ops.uniform_unit_scaling_initializer(seed=2, dtype=dtype)\n      init3 = init_ops.uniform_unit_scaling_initializer(\n          1.5, seed=1, dtype=dtype)\n      self.assertFalse(identicaltest(self, init1, init2))\n      self.assertFalse(identicaltest(self, init1, init3))\n      self.assertFalse(identicaltest(self, init2, init3))\n\n  @test_util.run_deprecated_v1\n  def testZeroSize(self):\n    shape = [0, 2]\n    with self.cached_session():\n      x = variable_scope.get_variable(\n          \"x\",\n          shape=shape,\n          initializer=init_ops.uniform_unit_scaling_initializer())\n      self.evaluate(variables.global_variables_initializer())\n      self.assertAllEqual(shape, self.evaluate(x).shape)\n\n  @test_util.run_deprecated_v1\n  def testDuplicatedInitializer(self):\n    init = init_ops.uniform_unit_scaling_initializer()\n    self.assertFalse(duplicated_initializer(self, init, 1))\n\n  def testInvalidDataType(self):\n    self.assertRaises(\n        ValueError,\n        init_ops.uniform_unit_scaling_initializer,\n        dtype=dtypes.string)\n\n\nclass VarianceScalingInitializationTest(test.TestCase):\n\n  @test_util.run_deprecated_v1\n  def testTruncatedNormalDistribution(self):\n    shape = [100, 100]\n    expect_mean = 0.\n    expect_var = 1. / shape[0]\n    init = init_ops.variance_scaling_initializer(\n        distribution=\"truncated_normal\")\n\n    with self.session(), \\\n      test.mock.patch.object(\n          random_ops, \"truncated_normal\", wraps=random_ops.truncated_normal) \\\n          as mock_truncated_normal:\n      x = init(shape).eval()\n      self.assertTrue(mock_truncated_normal.called)\n\n    self.assertNear(np.mean(x), expect_mean, err=1e-2)\n    self.assertNear(np.var(x), expect_var, err=1e-2)\n\n  @test_util.run_deprecated_v1\n  def testNormalDistribution(self):\n    shape = [100, 100]\n    expect_mean = 0.\n    expect_var = 1. / shape[0]\n    init = init_ops.variance_scaling_initializer(distribution=\"normal\")\n\n    with self.session(), \\\n      test.mock.patch.object(\n          random_ops, \"truncated_normal\", wraps=random_ops.truncated_normal) \\\n          as mock_truncated_normal:\n      x = init(shape).eval()\n      self.assertTrue(mock_truncated_normal.called)\n\n    self.assertNear(np.mean(x), expect_mean, err=1e-2)\n    self.assertNear(np.var(x), expect_var, err=1e-2)\n\n  @test_util.run_deprecated_v1\n  def testUntruncatedNormalDistribution(self):\n    shape = [100, 100]\n    expect_mean = 0.\n    expect_var = 1. / shape[0]\n    init = init_ops.variance_scaling_initializer(\n        distribution=\"untruncated_normal\")\n\n    with self.session(), \\\n      test.mock.patch.object(\n          random_ops, \"random_normal\", wraps=random_ops.random_normal) \\\n          as mock_random_normal:\n      x = init(shape).eval()\n      self.assertTrue(mock_random_normal.called)\n\n    self.assertNear(np.mean(x), expect_mean, err=1e-2)\n    self.assertNear(np.var(x), expect_var, err=1e-2)\n\n  @test_util.run_deprecated_v1\n  def testUniformDistribution(self):\n    shape = [100, 100]\n    expect_mean = 0.\n    expect_var = 1. / shape[0]\n    init = init_ops.variance_scaling_initializer(distribution=\"uniform\")\n\n    with self.session():\n      x = init(shape).eval()\n\n    self.assertNear(np.mean(x), expect_mean, err=1e-2)\n    self.assertNear(np.var(x), expect_var, err=1e-2)\n\n\n# TODO(vrv): move to sequence_ops_test?\nclass RangeTest(test.TestCase):\n\n  def _Range(self, start, limit, delta):\n    with self.cached_session():\n      tf_ans = math_ops.range(start, limit, delta, name=\"range\")\n      self.assertEqual([len(np.arange(start, limit, delta))],\n                       tf_ans.get_shape())\n      return self.evaluate(tf_ans)\n\n  def testBasic(self):\n    self.assertTrue(\n        np.array_equal(self._Range(0, 5, 1), np.array([0, 1, 2, 3, 4])))\n    self.assertTrue(np.array_equal(self._Range(0, 5, 2), np.array([0, 2, 4])))\n    self.assertTrue(np.array_equal(self._Range(0, 6, 2), np.array([0, 2, 4])))\n    self.assertTrue(\n        np.array_equal(self._Range(13, 32, 7), np.array([13, 20, 27])))\n    self.assertTrue(\n        np.array_equal(\n            self._Range(100, 500, 100), np.array([100, 200, 300, 400])))\n    self.assertEqual(math_ops.range(0, 5, 1).dtype, dtypes.int32)\n\n  @test_util.run_deprecated_v1\n  def testLimitOnly(self):\n    with self.session():\n      self.assertAllEqual(np.arange(5), math_ops.range(5))\n\n  def testEmpty(self):\n    for start in 0, 5:\n      self.assertTrue(np.array_equal(self._Range(start, start, 1), []))\n\n  def testNonInteger(self):\n    self.assertTrue(\n        np.allclose(self._Range(0, 2, 0.5), np.array([0, 0.5, 1, 1.5])))\n    self.assertTrue(np.allclose(self._Range(0, 5, 2.5), np.array([0, 2.5])))\n    self.assertTrue(\n        np.allclose(self._Range(0, 3, 0.9), np.array([0, 0.9, 1.8, 2.7])))\n    self.assertTrue(\n        np.allclose(\n            self._Range(100., 500., 100.), np.array([100, 200, 300, 400])))\n    self.assertEqual(math_ops.range(0., 5., 1.).dtype, dtypes.float32)\n\n  def testNegativeDelta(self):\n    self.assertTrue(\n        np.array_equal(self._Range(5, -1, -1), np.array([5, 4, 3, 2, 1, 0])))\n    self.assertTrue(\n        np.allclose(self._Range(2.5, 0, -0.5), np.array([2.5, 2, 1.5, 1, 0.5])))\n    self.assertTrue(\n        np.array_equal(self._Range(-5, -10, -3), np.array([-5, -8])))\n\n  def testDType(self):\n    zero_int32 = math_ops.cast(0, dtypes.int32)\n    zero_int64 = math_ops.cast(0, dtypes.int64)\n    zero_float32 = math_ops.cast(0, dtypes.float32)\n    zero_float64 = math_ops.cast(0, dtypes.float64)\n\n    self.assertEqual(math_ops.range(zero_int32, 0, 1).dtype, dtypes.int32)\n    self.assertEqual(math_ops.range(zero_int64, 0, 1).dtype, dtypes.int64)\n    self.assertEqual(math_ops.range(zero_float32, 0, 1).dtype, dtypes.float32)\n    self.assertEqual(math_ops.range(zero_float64, 0, 1).dtype, dtypes.float64)\n\n    self.assertEqual(\n        math_ops.range(zero_int32, zero_int64, 1).dtype, dtypes.int64)\n    self.assertEqual(\n        math_ops.range(zero_int64, zero_float32, 1).dtype, dtypes.float32)\n    self.assertEqual(\n        math_ops.range(zero_float32, zero_float64, 1).dtype, dtypes.float64)\n    self.assertEqual(\n        math_ops.range(zero_float64, zero_int32, 1).dtype, dtypes.float64)\n\n    self.assertEqual(\n        math_ops.range(0, 0, 1, dtype=dtypes.int32).dtype, dtypes.int32)\n    self.assertEqual(\n        math_ops.range(0, 0, 1, dtype=dtypes.int64).dtype, dtypes.int64)\n    self.assertEqual(\n        math_ops.range(0, 0, 1, dtype=dtypes.float32).dtype, dtypes.float32)\n    self.assertEqual(\n        math_ops.range(0, 0, 1, dtype=dtypes.float64).dtype, dtypes.float64)\n\n  def testMixedDType(self):\n    # Test case for GitHub issue 35710\n    tf_ans = math_ops.range(\n        constant_op.constant(4, dtype=dtypes.int32), dtype=dtypes.int64)\n    self.assertAllEqual(self.evaluate(tf_ans), np.array([0, 1, 2, 3]))\n\n  def testLargeLimits(self):\n    # Test case for GitHub issue 46913.\n    with self.session():\n      with self.assertRaises(errors_impl.ResourceExhaustedError):\n        v = math_ops.range(0, 9223372036854775807)\n        self.evaluate(v)\n\n  def testLargeStarts(self):\n    # Test case for GitHub issue 46899.\n    with self.session():\n      with self.assertRaises(errors_impl.InternalError):\n        v = math_ops.range(start=-1e+38, limit=1)\n        self.evaluate(v)\n\n\n# TODO(vrv): move to sequence_ops_test?\nclass LinSpaceTest(test.TestCase):\n\n  def _gpu_modes(self):\n    if test.is_gpu_available():\n      return [False, True]\n    else:\n      return [False]\n\n  def _LinSpace(self, start, stop, num):\n    with ops.Graph().as_default() as graph:\n      with self.session(graph=graph, force_gpu=self.force_gpu):\n        tf_ans = math_ops.linspace(start, stop, num, name=\"linspace\")\n        self.assertEqual([num], tf_ans.get_shape())\n        return self.evaluate(tf_ans)\n\n  def testPositive(self):\n    for self.force_gpu in self._gpu_modes():\n      self.assertArrayNear(self._LinSpace(1., 5., 1), np.array([1.]), 1e-5)\n      self.assertArrayNear(self._LinSpace(1., 5., 2), np.array([1., 5.]), 1e-5)\n      self.assertArrayNear(\n          self._LinSpace(1., 5., 3), np.array([1., 3., 5.]), 1e-5)\n      self.assertArrayNear(\n          self._LinSpace(1., 5., 4), np.array([1., 7. / 3., 11. / 3., 5.]),\n          1e-5)\n\n  def testNegative(self):\n    for self.force_gpu in self._gpu_modes():\n      self.assertArrayNear(self._LinSpace(-1., -5., 1), np.array([-1.]), 1e-5)\n      self.assertArrayNear(\n          self._LinSpace(-1., -5., 2), np.array([-1., -5.]), 1e-5)\n      self.assertArrayNear(\n          self._LinSpace(-1., -5., 3), np.array([-1., -3., -5.]), 1e-5)\n      self.assertArrayNear(\n          self._LinSpace(-1., -5., 4), np.array([-1., -7. / 3., -11. / 3.,\n                                                 -5.]), 1e-5)\n\n  def testNegativeToPositive(self):\n    for self.force_gpu in self._gpu_modes():\n      self.assertArrayNear(self._LinSpace(-1., 5., 1), np.array([-1.]), 1e-5)\n      self.assertArrayNear(\n          self._LinSpace(-1., 5., 2), np.array([-1., 5.]), 1e-5)\n      self.assertArrayNear(\n          self._LinSpace(-1., 5., 3), np.array([-1., 2., 5.]), 1e-5)\n      self.assertArrayNear(\n          self._LinSpace(-1., 5., 4), np.array([-1., 1., 3., 5.]), 1e-5)\n\n  def testPoint(self):\n    for self.force_gpu in self._gpu_modes():\n      self.assertArrayNear(self._LinSpace(5., 5., 1), np.array([5.]), 1e-5)\n      self.assertArrayNear(self._LinSpace(5., 5., 2), np.array([5.] * 2), 1e-5)\n      self.assertArrayNear(self._LinSpace(5., 5., 3), np.array([5.] * 3), 1e-5)\n      self.assertArrayNear(self._LinSpace(5., 5., 4), np.array([5.] * 4), 1e-5)\n\n  def testEndpointsAreExact(self):\n    for self.force_gpu in self._gpu_modes():\n      # Test some cases that produce last values not equal to \"stop\" when\n      # computed via start + (num - 1) * ((stop - start) / (num - 1)), since\n      # float arithmetic will introduce error through precision loss.\n      self.assertAllEqual(\n          self._LinSpace(0., 1., 42)[[0, -1]], np.array([0., 1.], np.float32))\n      self.assertAllEqual(\n          self._LinSpace(-1., 0., 42)[[0, -1]], np.array([-1., 0.], np.float32))\n      self.assertAllEqual(\n          self._LinSpace(.1, .2, 4)[[0, -1]], np.array([.1, .2], np.float32))\n      # Check a case for float64 error too.\n      self.assertAllEqual(\n          self._LinSpace(np.array(0., np.float64), .1, 12)[[0, -1]],\n          np.array([0., .1], np.float64))\n\n\nclass LinSpaceNdTest(test.TestCase):\n\n  def _gpu_modes(self):\n    if test.is_gpu_available():\n      return [False, True]\n    else:\n      return [False]\n\n  def _LinSpace(self, start, stop, num, axis=0):\n    with ops.Graph().as_default() as graph:\n      with self.session(graph=graph, force_gpu=self.force_gpu):\n        tf_ans = math_ops.linspace_nd(start, stop, num, axis=axis)\n        return self.evaluate(tf_ans)\n\n  def _LinSpaceNumConstant(self, start, stop, num, axis=0):\n    with ops.Graph().as_default() as graph:\n      num_constant = constant_op.constant(num)\n      with self.session(graph=graph, force_gpu=self.force_gpu):\n        tf_ans = math_ops.linspace_nd(start, stop, num_constant, axis=axis)\n        return self.evaluate(tf_ans)\n\n  def _LinspaceNoneShape(self, start, stop, num, graph_shape=None, axis=0):\n    with ops.Graph().as_default() as graph:\n      num_tensor = array_ops.placeholder(dtypes.int32)\n      start_t = array_ops.placeholder(dtypes.float32, shape=graph_shape)\n      stop_t = array_ops.placeholder(dtypes.float32, shape=graph_shape)\n      ans_tensor = math_ops.linspace_nd(start_t, stop_t, num_tensor, axis=axis)\n\n      with self.session(graph=graph, force_gpu=self.force_gpu) as sess:\n        feed_dict = {start_t: start, stop_t: stop, num_tensor: num}\n        return sess.run(ans_tensor, feed_dict=feed_dict)\n\n  def testPositive(self):\n    for self.force_gpu in self._gpu_modes():\n      self.assertArrayNear(self._LinSpace(1., 5., 1), np.array([1.]), 1e-5)\n      self.assertArrayNear(self._LinSpace(1., 5., 2), np.array([1., 5.]), 1e-5)\n      self.assertArrayNear(\n          self._LinSpace(1., 5., 3), np.array([1., 3., 5.]), 1e-5)\n      self.assertArrayNear(\n          self._LinSpace(1., 5., 4), np.array([1., 7. / 3., 11. / 3., 5.]),\n          1e-5)\n\n  def testNegative(self):\n    for self.force_gpu in self._gpu_modes():\n      self.assertArrayNear(self._LinSpace(-1., -5., 1), np.array([-1.]), 1e-5)\n      self.assertArrayNear(\n          self._LinSpace(-1., -5., 2), np.array([-1., -5.]), 1e-5)\n      self.assertArrayNear(\n          self._LinSpace(-1., -5., 3), np.array([-1., -3., -5.]), 1e-5)\n      self.assertArrayNear(\n          self._LinSpace(-1., -5., 4), np.array([-1., -7. / 3., -11. / 3.,\n                                                 -5.]), 1e-5)\n\n  def testNegativeToPositive(self):\n    for self.force_gpu in self._gpu_modes():\n      self.assertArrayNear(self._LinSpace(-1., 5., 1), np.array([-1.]), 1e-5)\n      self.assertArrayNear(\n          self._LinSpace(-1., 5., 2), np.array([-1., 5.]), 1e-5)\n      self.assertArrayNear(\n          self._LinSpace(-1., 5., 3), np.array([-1., 2., 5.]), 1e-5)\n      self.assertArrayNear(\n          self._LinSpace(-1., 5., 4), np.array([-1., 1., 3., 5.]), 1e-5)\n\n  def testPoint(self):\n    for self.force_gpu in self._gpu_modes():\n      self.assertArrayNear(self._LinSpace(5., 5., 1), np.array([5.]), 1e-5)\n      self.assertArrayNear(self._LinSpace(5., 5., 2), np.array([5.] * 2), 1e-5)\n      self.assertArrayNear(self._LinSpace(5., 5., 3), np.array([5.] * 3), 1e-5)\n      self.assertArrayNear(self._LinSpace(5., 5., 4), np.array([5.] * 4), 1e-5)\n\n  def testEndpointsAreExact(self):\n    for self.force_gpu in self._gpu_modes():\n      # Test some cases that produce last values not equal to \"stop\" when\n      # computed via start + (num - 1) * ((stop - start) / (num - 1)), since\n      # float arithmetic will introduce error through precision loss.\n      self.assertAllEqual(\n          self._LinSpace(0., 1., 42)[[0, -1]], np.array([0., 1.], np.float32))\n      self.assertAllEqual(\n          self._LinSpace(-1., 0., 42)[[0, -1]], np.array([-1., 0.], np.float32))\n      self.assertAllEqual(\n          self._LinSpace(.1, .2, 4)[[0, -1]], np.array([.1, .2], np.float32))\n      # Check a case for float64 error too.\n      self.assertAllEqual(\n          self._LinSpace(np.array(0., np.float64), .1, 12)[[0, -1]],\n          np.array([0., .1], np.float64))\n\n  def testScalarsCompareToNumpy(self):\n    for self.force_gpu in self._gpu_modes():\n      actual = self._LinSpace(0., 1., 32)\n      expected = np.linspace(0., 1., 32)\n      self.assertArrayNear(expected, actual, 1e-5)\n\n  def _baseNDArrayCompareToNumpy(self, axis):\n    for self.force_gpu in self._gpu_modes():\n      a, b, expected, num = self.create_nd_inputs_and_expected_output(axis)\n      actual = self._LinSpace(a, b, num, axis=axis)\n      self.assert_close(actual, expected)\n\n  def assert_close(self, actual, expected):\n    wrong_indices = np.where(~np.allclose(actual, expected))\n    mess = \"Wrong float answer. Wrong indices: {}\".format(wrong_indices)\n    self.assertTrue(np.allclose(actual, expected), mess)\n\n  def create_nd_inputs_and_expected_output(self, axis):\n    a = np.arange(2, dtype=np.float32)\n    b = a * 5\n    num = 5\n\n    res = np.array([[0., 0., 0., 0., 0.], [1., 2., 3., 4., 5.]])\n    expected = res if axis != 0 else res.T\n    return a, b, expected, num\n\n  def testNDArrayCompareToNumpyDefaultAxis(self):\n    self._baseNDArrayCompareToNumpy(0)\n\n  def testNDArrayAxisStrictlyPositive(self):\n    self._baseNDArrayCompareToNumpy(1)\n\n  def testNDArrayAxisStrictlyNegative(self):\n    self._baseNDArrayCompareToNumpy(-1)\n\n  def testNumConstant(self):\n    for self.force_gpu in self._gpu_modes():\n      actual = self._LinSpaceNumConstant(0., 1., 32)\n      expected = np.linspace(0., 1., 32)\n      self.assertArrayNear(expected, actual, 1e-5)\n\n  def testUnknownShapeAtGraphCreationTime(self):\n    self.base_test_unknown_shape((2))\n\n  def testNoneValuesInShapeAtGraphCreationTime(self):\n    self.base_test_unknown_shape((None))\n\n  def testNoneShapeAtGraphCreationTime(self):\n    self.base_test_unknown_shape(None)\n\n  def base_test_unknown_shape(self, graph_shape):\n    for self.force_gpu in self._gpu_modes():\n      axis = 1\n      a, b, expected, num = self.create_nd_inputs_and_expected_output(axis)\n      actual = self._LinspaceNoneShape(a, b, num, graph_shape, axis)\n      self.assert_close(actual, expected)\n\n\nclass DeviceTest(test.TestCase):\n\n  def testNoDevice(self):\n    with ops.Graph().as_default():\n      var = variables.Variable([[1.0, 1.0]])\n    self.assertDeviceEqual(None, var.device)\n    self.assertDeviceEqual(None, var.initializer.device)\n\n  def testDevice(self):\n    with ops.Graph().as_default():\n      with ops.device(\"/job:ps\"):\n        var = variables.Variable([[1.0, 1.0]])\n    self.assertDeviceEqual(\"/job:ps\", var.device)\n    self.assertDeviceEqual(\"/job:ps\", var.initializer.device)\n\n\nclass OrthogonalInitializerTest(test.TestCase):\n\n  @test_util.run_deprecated_v1\n  def testInitializerIdentical(self):\n    for dtype in [dtypes.float32, dtypes.float64]:\n      init1 = init_ops.orthogonal_initializer(seed=1, dtype=dtype)\n      init2 = init_ops.orthogonal_initializer(seed=1, dtype=dtype)\n      self.assertTrue(identicaltest(self, init1, init2, (10, 10)))\n\n  @test_util.run_deprecated_v1\n  def testInitializerDifferent(self):\n    for dtype in [dtypes.float32, dtypes.float64]:\n      init1 = init_ops.orthogonal_initializer(seed=1, dtype=dtype)\n      init2 = init_ops.orthogonal_initializer(seed=2, dtype=dtype)\n      self.assertFalse(identicaltest(self, init1, init2, (10, 10)))\n\n  @test_util.run_deprecated_v1\n  def testDuplicatedInitializer(self):\n    init = init_ops.orthogonal_initializer()\n    self.assertFalse(duplicated_initializer(self, init, 1, (10, 10)))\n\n  def testInvalidDataType(self):\n    self.assertRaises(\n        ValueError, init_ops.orthogonal_initializer, dtype=dtypes.string)\n\n  def testInvalidShape(self):\n    init1 = init_ops.orthogonal_initializer()\n    with self.session(graph=ops.Graph(), use_gpu=True):\n      self.assertRaises(ValueError, init1, shape=[5])\n\n  @test_util.run_deprecated_v1\n  def testGain(self):\n    shape = (10, 10)\n    for dtype in [dtypes.float32, dtypes.float64]:\n      init1 = init_ops.orthogonal_initializer(seed=1, dtype=dtype)\n      init2 = init_ops.orthogonal_initializer(gain=3.14, seed=1, dtype=dtype)\n      with self.session(graph=ops.Graph(), use_gpu=True):\n        t1 = init1(shape).eval()\n        t2 = init2(shape).eval()\n      self.assertAllClose(t1, t2 / 3.14)\n\n  @test_util.run_deprecated_v1\n  def testShapesValues(self):\n    for dtype in [dtypes.float32, dtypes.float64]:\n      for shape in [(10, 10), (10, 9, 8), (100, 5, 5), (50, 40), (40, 50)]:\n        init = init_ops.orthogonal_initializer(dtype=dtype)\n        tol = 1e-5 if dtype == dtypes.float32 else 1e-12\n        with self.session(graph=ops.Graph(), use_gpu=True):\n          # Check the shape\n          t = init(shape).eval()\n          self.assertAllEqual(shape, t.shape)\n          # Check orthogonality by computing the inner product\n          t = t.reshape((np.prod(t.shape[:-1]), t.shape[-1]))\n          if t.shape[0] > t.shape[1]:\n            self.assertAllClose(\n                np.dot(t.T, t), np.eye(t.shape[1]), rtol=tol, atol=tol)\n          else:\n            self.assertAllClose(\n                np.dot(t, t.T), np.eye(t.shape[0]), rtol=tol, atol=tol)\n\n\nclass ConvolutionDeltaOrthogonalInitializerTest(test.TestCase):\n\n  @test_util.run_deprecated_v1\n  def testInitializerIdentical(self):\n    for dtype in [dtypes.float32, dtypes.float64]:\n      init1 = init_ops.convolutional_delta_orthogonal(seed=1, dtype=dtype)\n      init2 = init_ops.convolutional_delta_orthogonal(seed=1, dtype=dtype)\n      self.assertTrue(identicaltest(self, init1, init2, (3, 3, 10, 10)))\n\n  @test_util.run_deprecated_v1\n  def testInitializerDifferent(self):\n    for dtype in [dtypes.float32, dtypes.float64]:\n      init1 = init_ops.convolutional_delta_orthogonal(seed=1, dtype=dtype)\n      init2 = init_ops.convolutional_delta_orthogonal(seed=2, dtype=dtype)\n      self.assertFalse(identicaltest(self, init1, init2, (3, 3, 10, 10)))\n\n  @test_util.run_deprecated_v1\n  def testDuplicatedInitializer(self):\n    init = init_ops.convolutional_delta_orthogonal()\n    self.assertFalse(duplicated_initializer(self, init, 1, (3, 3, 10, 10)))\n\n  def testInvalidDataType(self):\n    self.assertRaises(\n        ValueError,\n        init_ops.convolutional_delta_orthogonal,\n        dtype=dtypes.string)\n\n  def testInvalidShape(self):\n    init1 = init_ops.convolutional_delta_orthogonal()\n    with self.session(graph=ops.Graph(), use_gpu=True):\n      self.assertRaises(ValueError, init1, shape=[3, 3, 6, 5])\n\n  @test_util.run_deprecated_v1\n  def testGain(self):\n    shape = (3, 3, 10, 10)\n    for dtype in [dtypes.float32, dtypes.float64]:\n      init1 = init_ops.convolutional_delta_orthogonal(seed=1, dtype=dtype)\n      init2 = init_ops.convolutional_delta_orthogonal(\n          gain=3.14, seed=1, dtype=dtype)\n      with self.session(graph=ops.Graph(), use_gpu=True):\n        t1 = init1(shape).eval()\n        t2 = init2(shape).eval()\n      self.assertAllClose(t1, t2 / 3.14)\n\n  @test_util.run_deprecated_v1\n  def testShapesValues(self):\n    gain = 3.14\n    for dtype in [dtypes.float32]:\n      for kernel_size in [[3], [8], [3, 5], [2, 4], [3, 3, 3], [2, 2, 2]]:\n        tol = 1e-2\n        # Check orthogonality by computing ratio between\n        # the 2-norms of the inputs and outputs.\n        if len(kernel_size) == 1:\n          shape = [4, 32, 64]\n          convolution = convolutional.conv1d\n        elif len(kernel_size) == 2:\n          convolution = convolutional.conv2d\n          shape = [4, 32, 32, 64]\n        else:\n          shape = [4, 16, 16, 16, 64]\n          convolution = convolutional.conv3d\n        inputs = random_ops.random_normal(shape, dtype=dtype)\n        inputs_2norm = linalg_ops.norm(inputs)\n        outputs = convolution(\n            inputs,\n            padding=\"same\",\n            filters=128,\n            kernel_size=kernel_size,\n            use_bias=False,\n            kernel_initializer=init_ops.convolutional_delta_orthogonal(\n                gain=gain))\n        outputs_shape = shape[0:-1] + [128]\n        outputs_2norm = linalg_ops.norm(outputs)\n        ratio = outputs_2norm / inputs_2norm\n        my_ops = variables.global_variables_initializer()\n        with self.session():\n          self.evaluate(my_ops)\n          # Check the shape of the outputs\n          t = self.evaluate(outputs)\n          self.assertAllEqual(t.shape, outputs_shape)\n          # Check isometry of the delta-orthogonal kernel.\n          self.assertAllClose(self.evaluate(ratio), gain, rtol=tol, atol=tol)\n\n  @test_util.run_deprecated_v1\n  def testNonuniformity(self):\n    value = 0\n    abs_value = 0\n    shape = [3, 3, 10, 10]\n    count = 70\n    tol = 1e-5\n    with self.session():\n      for i in range(count):\n        x = variable_scope.get_variable(\n            \"{}\".format(i),\n            shape=shape,\n            initializer=init_ops.convolutional_delta_orthogonal)\n        self.evaluate(x.initializer)\n        y = self.evaluate(x)[1, 1, :, :]\n        determinant = np.linalg.det(y)\n        value += determinant\n        abs_value += np.abs(determinant)\n\n      # Check there is some variation in the signs of the determinants\n      self.assertLess(value, count - tol)\n      self.assertLess(-count + tol, value)\n      # Check all determinants have absolute value 1\n      # Compute the sum of the absolute values of 'count' determinants\n      self.assertAllClose(abs_value, count, rtol=tol, atol=tol)\n\n\n@test_util.run_all_without_tensor_float_32(\n    \"Tests convolutional_orthogonal_1d, which calls matmul\")\nclass ConvolutionOrthogonal1dInitializerTest(test.TestCase):\n\n  @test_util.run_deprecated_v1\n  def testInitializerIdentical(self):\n    for dtype in [dtypes.float32, dtypes.float64]:\n      init1 = init_ops.convolutional_orthogonal_1d(seed=1, dtype=dtype)\n      init2 = init_ops.convolutional_orthogonal_1d(seed=1, dtype=dtype)\n      self.assertTrue(identicaltest(self, init1, init2, (3, 10, 10)))\n\n  @test_util.run_deprecated_v1\n  def testInitializerDifferent(self):\n    for dtype in [dtypes.float32, dtypes.float64]:\n      init1 = init_ops.convolutional_orthogonal_1d(seed=1, dtype=dtype)\n      init2 = init_ops.convolutional_orthogonal_1d(seed=2, dtype=dtype)\n      self.assertFalse(identicaltest(self, init1, init2, (3, 10, 10)))\n\n  @test_util.run_deprecated_v1\n  def testDuplicatedInitializer(self):\n    init = init_ops.convolutional_orthogonal_1d()\n    self.assertFalse(duplicated_initializer(self, init, 1, (3, 10, 10)))\n\n  def testInvalidDataType(self):\n    self.assertRaises(\n        ValueError, init_ops.convolutional_orthogonal_1d, dtype=dtypes.string)\n\n  def testInvalidShape(self):\n    init1 = init_ops.convolutional_orthogonal_1d()\n    with self.session(graph=ops.Graph(), use_gpu=True):\n      self.assertRaises(ValueError, init1, shape=[3, 6, 5])\n\n  @test_util.run_deprecated_v1\n  def testGain(self):\n    shape = (3, 10, 10)\n    for dtype in [dtypes.float32, dtypes.float64]:\n      init1 = init_ops.convolutional_orthogonal_1d(seed=1, dtype=dtype)\n      init2 = init_ops.convolutional_orthogonal_1d(\n          gain=3.14, seed=1, dtype=dtype)\n      with self.session(graph=ops.Graph(), use_gpu=True):\n        t1 = init1(shape).eval()\n        t2 = init2(shape).eval()\n      self.assertAllClose(t1, t2 / 3.14)\n\n  @test_util.run_deprecated_v1\n  def testNonuniformity(self):\n    value = 0\n    abs_value = 0\n    shape = [3, 10, 10]\n    count = 70\n    tol = 1e-5\n    with self.session():\n      for i in range(count):\n        x = variable_scope.get_variable(\n            \"{}\".format(i),\n            shape=shape,\n            initializer=init_ops.convolutional_orthogonal_1d)\n        self.evaluate(x.initializer)\n        y = np.sum(self.evaluate(x), axis=0)\n        determinant = np.linalg.det(y)\n        value += determinant\n        abs_value += np.abs(determinant)\n\n      # Check there is some variation in the signs of the determinants.\n      self.assertLess(value, count - tol)\n      self.assertLess(-count + tol, value)\n      # Check all determinants have absolute value 1\n      # Compute the sum of the absolute values of 'count' determinants\n      self.assertAllClose(abs_value, count, rtol=tol, atol=tol)\n\n  @test_util.run_deprecated_v1\n  def testShapesValues(self):\n\n    def circular_pad(input_, width, kernel_size):\n      \"\"\"Pad input_ for computing (circular) convolution.\n\n      Args:\n        input_: the input tensor\n        width: the width of the tensor.\n        kernel_size: the kernel size of the filter.\n\n      Returns:\n        a tensor whose width is (width + kernel_size - 1).\n      \"\"\"\n\n      beginning = kernel_size // 2\n      end = kernel_size - 1 - beginning\n\n      tmp_up = array_ops.slice(input_, [0, width - beginning, 0],\n                               [-1, beginning, -1])\n      tmp_down = array_ops.slice(input_, [0, 0, 0], [-1, end, -1])\n      tmp = array_ops.concat([tmp_up, input_, tmp_down], 1)\n\n      return tmp\n\n    cout = 64\n    shape = [10, 20, 32]\n    outputs_shape = shape[0:-1] + [cout]\n    dtype = dtypes.float32\n    tol = 1e-3\n    gain = 3.14\n    # Check orthogonality/isometry by computing the ratio between\n    # the 2-norms of the inputs and outputs.\n    for kernel_size in [[1], [2], [3], [4], [5], [6]]:\n      convolution = convolutional.conv1d\n      inputs = random_ops.random_normal(shape, dtype=dtype)\n      inputs_2norm = linalg_ops.norm(inputs)\n      input_with_circular_pad = circular_pad(inputs, shape[1], kernel_size[0])\n      outputs = convolution(\n          input_with_circular_pad,\n          padding=\"valid\",\n          filters=cout,\n          kernel_size=kernel_size[0],\n          use_bias=False,\n          kernel_initializer=init_ops.convolutional_orthogonal_1d(gain=gain))\n      outputs_2norm = linalg_ops.norm(outputs)\n      ratio = outputs_2norm / inputs_2norm\n      my_ops = variables.global_variables_initializer()\n      with self.session():\n        self.evaluate(my_ops)\n        # Check the shape of the outputs\n        t = self.evaluate(outputs)\n        self.assertAllEqual(t.shape, outputs_shape)\n        # Check isometry of the orthogonal kernel.\n        self.assertAllClose(self.evaluate(ratio), gain, rtol=tol, atol=tol)\n\n\nclass ConvolutionOrthogonal2dInitializerTest(test.TestCase):\n\n  @test_util.run_deprecated_v1\n  def testInitializerIdentical(self):\n    for dtype in [dtypes.float32, dtypes.float64]:\n      init1 = init_ops.convolutional_orthogonal_2d(seed=1, dtype=dtype)\n      init2 = init_ops.convolutional_orthogonal_2d(seed=1, dtype=dtype)\n      self.assertTrue(identicaltest(self, init1, init2, (3, 3, 10, 10)))\n\n  @test_util.run_deprecated_v1\n  def testInitializerDifferent(self):\n    for dtype in [dtypes.float32, dtypes.float64]:\n      init1 = init_ops.convolutional_orthogonal_2d(seed=1, dtype=dtype)\n      init2 = init_ops.convolutional_orthogonal_2d(seed=2, dtype=dtype)\n      self.assertFalse(identicaltest(self, init1, init2, (3, 3, 10, 10)))\n\n  @test_util.run_deprecated_v1\n  def testDuplicatedInitializer(self):\n    init = init_ops.convolutional_orthogonal_2d()\n    self.assertFalse(duplicated_initializer(self, init, 1, (3, 3, 10, 10)))\n\n  def testInvalidDataType(self):\n    self.assertRaises(\n        ValueError, init_ops.convolutional_orthogonal_2d, dtype=dtypes.string)\n\n  def testInvalidShape(self):\n    init1 = init_ops.convolutional_orthogonal_2d()\n    with self.session(graph=ops.Graph(), use_gpu=True):\n      self.assertRaises(ValueError, init1, shape=[3, 3, 6, 5])\n\n  @test_util.run_deprecated_v1\n  def testGain(self):\n    shape = (3, 3, 10, 10)\n    for dtype in [dtypes.float32, dtypes.float64]:\n      init1 = init_ops.convolutional_orthogonal_2d(seed=1, dtype=dtype)\n      init2 = init_ops.convolutional_orthogonal_2d(\n          gain=3.14, seed=1, dtype=dtype)\n      with self.session(graph=ops.Graph(), use_gpu=True):\n        t1 = init1(shape).eval()\n        t2 = init2(shape).eval()\n      self.assertAllClose(t1, t2 / 3.14)\n\n  @test_util.run_deprecated_v1\n  def testShapesValues(self):\n\n    def circular_pad(input_, width, kernel_size):\n      \"\"\"Pad input_ for computing (circular) convolution.\n\n      Args:\n        input_: the input tensor\n        width: the width of the tensor.\n        kernel_size: the kernel size of the filter.\n\n      Returns:\n        a tensor whose width is (width + kernel_size - 1).\n      \"\"\"\n      beginning = kernel_size // 2\n      end = kernel_size - 1 - beginning\n\n      tmp_up = array_ops.slice(input_, [0, width - beginning, 0, 0],\n                               [-1, beginning, width, -1])\n      tmp_down = array_ops.slice(input_, [0, 0, 0, 0], [-1, end, width, -1])\n      tmp = array_ops.concat([tmp_up, input_, tmp_down], 1)\n\n      new_width = width + kernel_size - 1\n      tmp_left = array_ops.slice(tmp, [0, 0, width - beginning, 0],\n                                 [-1, new_width, beginning, -1])\n      tmp_right = array_ops.slice(tmp, [0, 0, 0, 0], [-1, new_width, end, -1])\n\n      final = array_ops.concat([tmp_left, tmp, tmp_right], 2)\n      return final\n\n    cout = 45\n    shape = [64, 28, 28, 32]\n    outputs_shape = shape[0:-1] + [cout]\n    dtype = dtypes.float32\n    tol = 1e-3\n    gain = 3.14\n    # Check orthogonality/isometry by computing the ratio between\n    # the 2-norms of the inputs and outputs.\n    for kernel_size in [[1, 1], [2, 2], [3, 3], [4, 4], [5, 5]]:\n      convolution = convolutional.conv2d\n      inputs = random_ops.random_normal(shape, dtype=dtype)\n      inputs_2norm = linalg_ops.norm(inputs)\n      input_with_circular_pad = circular_pad(inputs, shape[1], kernel_size[0])\n      outputs = convolution(\n          input_with_circular_pad,\n          padding=\"valid\",\n          filters=cout,\n          kernel_size=kernel_size,\n          use_bias=False,\n          kernel_initializer=init_ops.convolutional_orthogonal_2d(gain=gain))\n      outputs_2norm = linalg_ops.norm(outputs)\n      ratio = outputs_2norm / inputs_2norm\n      my_ops = variables.global_variables_initializer()\n      with self.session():\n        self.evaluate(my_ops)\n        # Check the shape of the outputs\n        t = self.evaluate(outputs)\n        self.assertAllEqual(t.shape, outputs_shape)\n        # Check isometry of the orthogonal kernel.\n        self.assertAllClose(self.evaluate(ratio), gain, rtol=tol, atol=tol)\n\n\n@test_util.run_all_without_tensor_float_32(\n    \"Tests convolutional_orthogonal_3d, which calls matmul\")\nclass ConvolutionOrthogonal3dInitializerTest(test.TestCase):\n\n  @test_util.run_deprecated_v1\n  def testInitializerIdentical(self):\n    for dtype in [dtypes.float32, dtypes.float64]:\n      init1 = init_ops.convolutional_orthogonal_3d(seed=1, dtype=dtype)\n      init2 = init_ops.convolutional_orthogonal_3d(seed=1, dtype=dtype)\n      self.assertTrue(identicaltest(self, init1, init2, (3, 3, 3, 10, 10)))\n\n  @test_util.run_deprecated_v1\n  def testInitializerDifferent(self):\n    for dtype in [dtypes.float32, dtypes.float64]:\n      init1 = init_ops.convolutional_orthogonal_3d(seed=1, dtype=dtype)\n      init2 = init_ops.convolutional_orthogonal_3d(seed=2, dtype=dtype)\n      self.assertFalse(identicaltest(self, init1, init2, (3, 3, 3, 10, 10)))\n\n  @test_util.run_deprecated_v1\n  def testDuplicatedInitializer(self):\n    init = init_ops.convolutional_orthogonal_3d()\n    self.assertFalse(duplicated_initializer(self, init, 1, (3, 3, 3, 10, 10)))\n\n  def testInvalidDataType(self):\n    self.assertRaises(\n        ValueError, init_ops.convolutional_orthogonal_3d, dtype=dtypes.string)\n\n  def testInvalidShape(self):\n    init1 = init_ops.convolutional_orthogonal_3d()\n    with self.session(graph=ops.Graph(), use_gpu=True):\n      self.assertRaises(ValueError, init1, shape=[3, 3, 3, 6, 5])\n\n  @test_util.run_deprecated_v1\n  def testGain(self):\n    shape = (3, 3, 3, 10, 10)\n    for dtype in [dtypes.float32, dtypes.float64]:\n      init1 = init_ops.convolutional_orthogonal_3d(seed=1, dtype=dtype)\n      init2 = init_ops.convolutional_orthogonal_3d(\n          gain=3.14, seed=1, dtype=dtype)\n      with self.session(graph=ops.Graph(), use_gpu=True):\n        t1 = init1(shape).eval()\n        t2 = init2(shape).eval()\n      self.assertAllClose(t1, t2 / 3.14)\n\n  @test_util.run_deprecated_v1\n  def testNonuniformity(self):\n    value = 0\n    abs_value = 0\n    shape = [3, 3, 3, 5, 5]\n    count = 20\n    tol = 1e-5\n    with self.session():\n      for i in range(count):\n        x = variable_scope.get_variable(\n            \"{}\".format(i),\n            shape=shape,\n            initializer=init_ops.convolutional_orthogonal_3d)\n        self.evaluate(x.initializer)\n        y = np.sum(self.evaluate(x), axis=(0, 1, 2))\n        determinant = np.linalg.det(y)\n        value += determinant\n        abs_value += np.abs(determinant)\n\n      # Check there is some variation in the signs of the determinants\n      self.assertLess(value, count - tol)\n      self.assertLess(-count + tol, value)\n      # Check all determinants have absolute value 1\n      # Compute the sum of the absolute values of 'count' determinants\n      self.assertAllClose(abs_value, count, rtol=tol, atol=tol)\n\n  @test_util.run_deprecated_v1\n  def testShapesValues(self):\n\n    def circular_pad(input_, width, kernel_size):\n      \"\"\"Padding input_ for computing circular convolution.\n\n      Args:\n        input_: the input tensor\n        width: the width of the tensor.\n        kernel_size: the kernel size of the filter.\n\n      Returns:\n        a tensor whose width is (width + kernel_size - 1).\n      \"\"\"\n\n      beginning = kernel_size // 2\n      end = kernel_size - 1 - beginning\n\n      tmp_up = array_ops.slice(input_, [0, width - beginning, 0, 0, 0],\n                               [-1, beginning, -1, -1, -1])\n      tmp_down = array_ops.slice(input_, [0, 0, 0, 0, 0], [-1, end, -1, -1, -1])\n      tmp = array_ops.concat([tmp_up, input_, tmp_down], 1)\n\n      tmp_left = array_ops.slice(tmp, [0, 0, width - beginning, 0, 0],\n                                 [-1, -1, beginning, -1, -1])\n      tmp_right = array_ops.slice(tmp, [0, 0, 0, 0, 0], [-1, -1, end, -1, -1])\n      tmp = array_ops.concat([tmp_left, tmp, tmp_right], 2)\n\n      tmp_front = array_ops.slice(tmp, [0, 0, 0, width - beginning, 0],\n                                  [-1, -1, -1, beginning, -1])\n      tmp_back = array_ops.slice(tmp, [0, 0, 0, 0, 0], [-1, -1, -1, end, -1])\n      return array_ops.concat([tmp_front, tmp, tmp_back], 3)\n\n    cout = 32\n    shape = [1, 7, 7, 7, 16]\n    outputs_shape = shape[0:-1] + [cout]\n    dtype = dtypes.float32\n    tol = 1e-3\n    gain = 3.14\n    # Check orthogonality/isometry by computing the ratio between\n    # the 2-norms of the inputs and outputs.\n    for kernel_size in [[1, 1, 1], [2, 2, 2], [3, 3, 3]]:\n      convolution = convolutional.conv3d\n      inputs = random_ops.random_normal(shape, dtype=dtype)\n      inputs_2norm = linalg_ops.norm(inputs)\n      input_with_circular_pad = circular_pad(inputs, shape[1], kernel_size[0])\n      outputs = convolution(\n          input_with_circular_pad,\n          padding=\"valid\",\n          filters=cout,\n          kernel_size=kernel_size[0],\n          use_bias=False,\n          kernel_initializer=init_ops.convolutional_orthogonal_3d(gain=gain))\n      outputs_2norm = linalg_ops.norm(outputs)\n      ratio = outputs_2norm / inputs_2norm\n      my_ops = variables.global_variables_initializer()\n      with self.cached_session():\n        self.evaluate(my_ops)\n        # Check the shape of the outputs\n        t = self.evaluate(outputs)\n        self.assertAllEqual(t.shape, outputs_shape)\n        # Check isometry of the orthogonal kernel.\n        self.assertAllClose(self.evaluate(ratio), gain, rtol=tol, atol=tol)\n\n\nclass IdentityInitializerTest(test.TestCase):\n\n  def testInvalidDataType(self):\n    self.assertRaises(\n        ValueError, init_ops.orthogonal_initializer, dtype=dtypes.string)\n\n  def testInvalidShape(self):\n    init = init_ops.identity_initializer()\n    with self.session(graph=ops.Graph(), use_gpu=True):\n      self.assertRaises(ValueError, init, shape=[5, 7, 7])\n      self.assertRaises(ValueError, init, shape=[5])\n      self.assertRaises(ValueError, init, shape=[])\n\n  @test_util.run_deprecated_v1\n  def testNonSquare(self):\n    init = init_ops.identity_initializer()\n    shape = (10, 5)\n    with self.session(graph=ops.Graph(), use_gpu=True):\n      self.assertAllClose(init(shape), np.eye(*shape))\n\n  @test_util.run_deprecated_v1\n  def testGain(self):\n    shape = (10, 10)\n    for dtype in [dtypes.float32, dtypes.float64]:\n      init_default = init_ops.identity_initializer(dtype=dtype)\n      init_custom = init_ops.identity_initializer(gain=0.9, dtype=dtype)\n      with self.session(graph=ops.Graph(), use_gpu=True):\n        self.assertAllClose(init_default(shape), np.eye(*shape))\n      with self.session(graph=ops.Graph(), use_gpu=True):\n        self.assertAllClose(init_custom(shape), np.eye(*shape) * 0.9)\n\n  @test_util.run_deprecated_v1\n  def testPartitions(self):\n    shape = (10, 10)\n    init = init_ops.identity_initializer()\n    partitioner = partitioned_variables.variable_axis_size_partitioner(1)\n    with self.session(graph=ops.Graph(), use_gpu=True):\n      with variable_scope.variable_scope(\n          \"foo\", partitioner=partitioner, initializer=init):\n        v = array_ops.identity(variable_scope.get_variable(\"bar\", shape=shape))\n      self.evaluate(variables.global_variables_initializer())\n      self.assertAllClose(v, np.eye(*shape))\n\n\nif __name__ == \"__main__\":\n  test.main()\n"], "filenames": ["tensorflow/core/kernels/sequence_ops.cc", "tensorflow/python/kernel_tests/init_ops_test.py"], "buggy_code_start_loc": [80, 550], "buggy_code_end_loc": [84, 550], "fixing_code_start_loc": [81, 551], "fixing_code_end_loc": [85, 558], "type": "CWE-681", "message": "TensorFlow is an open source platform for machine learning. In affected versions while calculating the size of the output within the `tf.range` kernel, there is a conditional statement of type `int64 = condition ? int64 : double`. Due to C++ implicit conversion rules, both branches of the condition will be cast to `double` and the result would be truncated before the assignment. This result in overflows. The fix will be included in TensorFlow 2.7.0. We will also cherrypick this commit on TensorFlow 2.6.1, TensorFlow 2.5.2, and TensorFlow 2.4.4, as these are also affected and still in supported range.", "other": {"cve": {"id": "CVE-2021-41202", "sourceIdentifier": "security-advisories@github.com", "published": "2021-11-05T22:15:08.323", "lastModified": "2021-11-09T15:05:52.520", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "TensorFlow is an open source platform for machine learning. In affected versions while calculating the size of the output within the `tf.range` kernel, there is a conditional statement of type `int64 = condition ? int64 : double`. Due to C++ implicit conversion rules, both branches of the condition will be cast to `double` and the result would be truncated before the assignment. This result in overflows. The fix will be included in TensorFlow 2.7.0. We will also cherrypick this commit on TensorFlow 2.6.1, TensorFlow 2.5.2, and TensorFlow 2.4.4, as these are also affected and still in supported range."}, {"lang": "es", "value": "TensorFlow es una plataforma de c\u00f3digo abierto para el aprendizaje autom\u00e1tico. En las versiones afectadas, mientras se calcula el tama\u00f1o de la salida dentro del n\u00facleo \"tf.range\", se presenta una sentencia condicional de tipo \"int64 = condici\u00f3n ? int64 : double\". Debido a las reglas de conversi\u00f3n impl\u00edcitas de C++, ambas ramas de la condici\u00f3n se convertir\u00e1n en \"double\" y el resultado se truncar\u00e1 antes de la asignaci\u00f3n. Esto resulta en desbordamientos. La correcci\u00f3n ser\u00e1 incluida en TensorFlow versi\u00f3n 2.7.0. Tambi\u00e9n ser\u00e1 incluida este commit en TensorFlow versi\u00f3n 2.6.1, TensorFlow versi\u00f3n 2.5.2, y TensorFlow versi\u00f3n 2.4.4, ya que estos tambi\u00e9n est\u00e1n afectados y todav\u00eda est\u00e1n en el rango admitido"}], "metrics": {"cvssMetricV31": [{"source": "security-advisories@github.com", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:N/I:N/A:P", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "PARTIAL", "baseScore": 2.1}, "baseSeverity": "LOW", "exploitabilityScore": 3.9, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "security-advisories@github.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-681"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.4.0", "versionEndExcluding": "2.4.4", "matchCriteriaId": "0E596567-6F67-4880-8EC4-CB262BF02E0D"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.5.0", "versionEndExcluding": "2.5.2", "matchCriteriaId": "035CDF63-1548-4FB4-B8A9-B8D328FAF910"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.6.0", "versionEndExcluding": "2.6.1", "matchCriteriaId": "5D68D8D1-DB27-4395-9D3D-2BED901B852C"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.7.0:rc0:*:*:*:*:*:*", "matchCriteriaId": "A58EDA5C-66D6-46F1-962E-60AFB7C784A7"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.7.0:rc1:*:*:*:*:*:*", "matchCriteriaId": "89522760-C2DF-400D-9624-626D8F160CBA"}]}]}], "references": [{"url": "https://github.com/tensorflow/tensorflow/commit/1b0e0ec27e7895b9985076eab32445026ae5ca94", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/commit/6d94002a09711d297dbba90390d5482b76113899", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/issues/46889", "source": "security-advisories@github.com", "tags": ["Issue Tracking", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/issues/46912", "source": "security-advisories@github.com", "tags": ["Issue Tracking", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/security/advisories/GHSA-xrqm-fpgr-6hhx", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/tensorflow/tensorflow/commit/1b0e0ec27e7895b9985076eab32445026ae5ca94"}}