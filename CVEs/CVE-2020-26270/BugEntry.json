{"buggy_code": ["/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include \"tensorflow/stream_executor/cuda/cuda_dnn.h\"\n\n#include <functional>\n#include <memory>\n#include <utility>\n\n#include \"absl/memory/memory.h\"\n#include \"absl/strings/str_cat.h\"\n#include \"third_party/eigen3/Eigen/Core\"\n#include \"tensorflow/core/lib/core/errors.h\"\n#include \"tensorflow/core/platform/tensor_float_32_utils.h\"\n#include \"tensorflow/core/util/env_var.h\"\n#include \"tensorflow/stream_executor/cuda/cuda_activation.h\"\n#include \"tensorflow/stream_executor/cuda/cuda_diagnostics.h\"\n#include \"tensorflow/stream_executor/cuda/cuda_driver.h\"\n#include \"tensorflow/stream_executor/cuda/cuda_gpu_executor.h\"\n#include \"tensorflow/stream_executor/cuda/cuda_platform_id.h\"\n#include \"tensorflow/stream_executor/cuda/cuda_stream.h\"\n#include \"tensorflow/stream_executor/cuda/cuda_timer.h\"\n#include \"tensorflow/stream_executor/cuda/cudnn_version.h\"\n#include \"tensorflow/stream_executor/dnn.h\"\n#include \"tensorflow/stream_executor/lib/env.h\"\n#include \"tensorflow/stream_executor/lib/error.h\"\n#include \"tensorflow/stream_executor/lib/initialize.h\"\n#include \"tensorflow/stream_executor/lib/mathutil.h\"\n#include \"tensorflow/stream_executor/lib/threadpool.h\"\n#include \"tensorflow/stream_executor/platform/logging.h\"\n#include \"tensorflow/stream_executor/plugin_registry.h\"\n#include \"tensorflow/stream_executor/scratch_allocator.h\"\n#include \"tensorflow/stream_executor/stream.h\"\n#include \"tensorflow/stream_executor/stream_executor_pimpl.h\"\n// clang-format off\n#include \"third_party/gpus/cudnn/cudnn.h\"\n#include \"absl/strings/string_view.h\"\n// clang-format on\n\n#pragma clang diagnostic push\n\n// Make sure that Eigen::half forward declaration in dnn.h matches the\n// declaration in Eigen.\n#pragma clang diagnostic warning \"-Wmismatched-tags\"\n\nnamespace stream_executor {\nnamespace gpu {\n\nPLUGIN_REGISTRY_DEFINE_PLUGIN_ID(kCuDnnPlugin);\n\nnamespace {\n\nstatic_assert(CUDNN_VERSION >= 7300, \"cuDNN needs to be version 7.3 or higher\");\n\n// Exits the program if 'expr' doesn't return CUDNN_STATUS_SUCCESS.\n#define CHECK_CUDNN_OK(expr) CHECK_EQ(expr, CUDNN_STATUS_SUCCESS)\n\n// If 'expr' doesn't return CUDNN_STATUS_SUCCESS, returns from the current\n// function with a non-successful port::Status.\n#define RETURN_IF_CUDNN_ERROR(expr)                                      \\\n  do {                                                                   \\\n    cudnnStatus_t _status = expr;                                        \\\n    if (!SE_PREDICT_TRUE(_status == CUDNN_STATUS_SUCCESS)) {             \\\n      std::ostringstream oss;                                            \\\n      oss << ToString(_status) << \"\\nin \" << __FILE__ << \"(\" << __LINE__ \\\n          << \"): '\" << #expr << \"'\";                                     \\\n      return port::Status(port::error::UNKNOWN, oss.str().c_str());      \\\n    }                                                                    \\\n  } while (false)\n\n// Converts (via narrowing) a type T value to a type U, and checks that the\n// value has no value change due to the conversion.\ntemplate <typename WideT, typename NarrowT>\nNarrowT CheckedNarrowing(const WideT& wide) {\n  NarrowT narrow = wide;\n  CHECK_EQ(narrow, wide)\n      << \"checked narrowing failed; values not equal post-conversion\";\n  return narrow;\n}\n\nstd::string ToString(cudnnStatus_t status) {\n  switch (status) {\n    case CUDNN_STATUS_SUCCESS:\n      return \"CUDNN_STATUS_SUCCESS\";\n    case CUDNN_STATUS_NOT_INITIALIZED:\n      return \"CUDNN_STATUS_NOT_INITIALIZED\";\n    case CUDNN_STATUS_ALLOC_FAILED:\n      return \"CUDNN_STATUS_ALLOC_FAILED\";\n    case CUDNN_STATUS_BAD_PARAM:\n      return \"CUDNN_STATUS_BAD_PARAM\";\n    case CUDNN_STATUS_INTERNAL_ERROR:\n      return \"CUDNN_STATUS_INTERNAL_ERROR\";\n    case CUDNN_STATUS_INVALID_VALUE:\n      return \"CUDNN_STATUS_INVALID_VALUE\";\n    case CUDNN_STATUS_ARCH_MISMATCH:\n      return \"CUDNN_STATUS_ARCH_MISMATCH\";\n    case CUDNN_STATUS_MAPPING_ERROR:\n      return \"CUDNN_STATUS_MAPPING_ERROR\";\n    case CUDNN_STATUS_EXECUTION_FAILED:\n      return \"CUDNN_STATUS_EXECUTION_FAILED\";\n    case CUDNN_STATUS_NOT_SUPPORTED:\n      return \"CUDNN_STATUS_NOT_SUPPORTED\";\n    case CUDNN_STATUS_LICENSE_ERROR:\n      return \"CUDNN_STATUS_LICENSE_ERROR\";\n    case CUDNN_STATUS_RUNTIME_PREREQUISITE_MISSING:\n      return \"CUDNN_STATUS_RUNTIME_PREREQUISITE_MISSING\";\n    case CUDNN_STATUS_RUNTIME_IN_PROGRESS:\n      return \"CUDNN_STATUS_RUNTIME_IN_PROGRESS\";\n    case CUDNN_STATUS_RUNTIME_FP_OVERFLOW:\n      return \"CUDNN_STATUS_RUNTIME_FP_OVERFLOW\";\n    default:\n      return absl::StrCat(\"<unknown cudnn status: \", static_cast<int>(status),\n                          \">\");\n  }\n}\n\n// RAII wrapper for all calls to cuDNN with a cuDNN handle argument.\n//\n// See CudnnAccess::GetHandle() for details.\nclass CudnnHandle {\n public:\n  // Takes ownership of the executor context and the lock to access cuDNN\n  // using handle.\n  CudnnHandle(gpu::ScopedActivateExecutorContext context,\n              std::unique_ptr<absl::MutexLock> lock, cudnnHandle_t handle)\n      : context_(std::move(context)), lock_(std::move(lock)), handle_(handle) {}\n\n  // Returns cuDNN handle. To be passed directly to cuDNN APIs, don't keep\n  // a copy.\n  cudnnHandle_t handle() const { return handle_; }\n\n private:\n  gpu::ScopedActivateExecutorContext context_;\n  std::unique_ptr<absl::MutexLock> lock_;\n  cudnnHandle_t handle_;  // Not owned.\n};\n\n}  // namespace\n\n// Wraps a cuDNN handle and provides access to it through CudnnHandle\n// instances, which also locks a mutex, acquires the CUDA context, and sets\n// the stream that cuDNN should use to enqueue any work.\n//\n// Note: CudnnSupport::cudnn_ should be the only instantiation of this class.\nclass CudnnAccess {\n public:\n  // Takes ownership of the handle.\n  explicit CudnnAccess(cudnnHandle_t handle) : handle_(handle) {}\n\n  ~CudnnAccess() {\n    absl::MutexLock lock(&mutex_);\n    cudnnDestroy(handle_);\n  }\n\n  // Creates a CudnnHandle instance for stream.\n  //\n  // cuDNN API calls using the same handle instance need to be serialized\n  // across threads. This is guaranteed by CudnnHandle instances locking the\n  // mutex owned by this class.\n  //\n  // Most cuDNN APIs taking a handle perform work on a CUDA stream. The\n  // CudnnHandle instance acquires the executor's CUDA context and sets cuDNN\n  // to use the provided stream.\n  //\n  // The stream argument may be null, which translates to the legacy default\n  // stream. See\n  // https://docs.nvidia.com/cuda/cuda-driver-api/stream-sync-behavior.html.\n  // The legacy default stream synchronizes with all other streams and it is\n  // therefore a bad idea (performance wise) to call any cuDNN APIs that\n  // enqueue work in the stream.\n  CudnnHandle GetHandle(GpuExecutor* executor, Stream* stream) {\n    auto lock = absl::make_unique<absl::MutexLock>(&mutex_);\n    mutex_.AssertHeld();\n    gpu::ScopedActivateExecutorContext context(executor);\n    CUstream cu_stream = stream ? AsGpuStreamValue(stream) : cudaStreamLegacy;\n    const auto status = cudnnSetStream(handle_, cu_stream);\n    CHECK_EQ(status, CUDNN_STATUS_SUCCESS) << \"Failed to set cuDNN stream.\";\n    return CudnnHandle(std::move(context), std::move(lock), handle_);\n  }\n\n private:\n  // Guards the enqueueing of cuDNN operations via the handle_ below.\n  absl::Mutex mutex_;\n\n  // cuDNN library handle.\n  cudnnHandle_t handle_ TF_GUARDED_BY(mutex_);  // Owned.\n};\n\nnamespace {\n\n// A helper function to return the internal compute type for\n// RNNs in cudnn.\ncudnnDataType_t GetRnnComputeType(dnn::DataType data_type);\n\ncudnnConvolutionFwdAlgo_t ToConvForwardAlgo(dnn::AlgorithmDesc algorithm) {\n  cudnnConvolutionFwdAlgo_t algo =\n      cudnnConvolutionFwdAlgo_t(algorithm.algo_id());\n  switch (algo) {\n    case CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM:\n    case CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM:\n    case CUDNN_CONVOLUTION_FWD_ALGO_GEMM:\n    case CUDNN_CONVOLUTION_FWD_ALGO_DIRECT:\n    case CUDNN_CONVOLUTION_FWD_ALGO_FFT:\n    case CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING:\n    case CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD:\n    case CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED:\n      return algo;\n    default:\n      LOG(FATAL) << \"Unsupported Cudnn convolution forward algorithm: \"\n                 << algorithm.algo_id();\n  }\n}\n\ncudnnConvolutionBwdDataAlgo_t ToConvBackwardDataAlgo(\n    dnn::AlgorithmDesc algorithm) {\n  cudnnConvolutionBwdDataAlgo_t algo =\n      cudnnConvolutionBwdDataAlgo_t(algorithm.algo_id());\n  switch (algo) {\n    case CUDNN_CONVOLUTION_BWD_DATA_ALGO_0:\n    case CUDNN_CONVOLUTION_BWD_DATA_ALGO_1:\n    case CUDNN_CONVOLUTION_BWD_DATA_ALGO_FFT:\n    case CUDNN_CONVOLUTION_BWD_DATA_ALGO_FFT_TILING:\n    case CUDNN_CONVOLUTION_BWD_DATA_ALGO_WINOGRAD:\n    case CUDNN_CONVOLUTION_BWD_DATA_ALGO_WINOGRAD_NONFUSED:\n      return algo;\n    default:\n      LOG(FATAL)\n          << \"Unsupported Cudnn convolution backward algorithm for data: \"\n          << algorithm.algo_id();\n  }\n}\n\ncudnnConvolutionBwdFilterAlgo_t ToConvBackwardFilterAlgo(\n    dnn::AlgorithmDesc algorithm) {\n  cudnnConvolutionBwdFilterAlgo_t algo =\n      cudnnConvolutionBwdFilterAlgo_t(algorithm.algo_id());\n  switch (algo) {\n    case CUDNN_CONVOLUTION_BWD_FILTER_ALGO_0:\n    case CUDNN_CONVOLUTION_BWD_FILTER_ALGO_1:\n    case CUDNN_CONVOLUTION_BWD_FILTER_ALGO_FFT:\n    case CUDNN_CONVOLUTION_BWD_FILTER_ALGO_3:\n    // Based on cudnn.h, the following is not implemented.\n    // case CUDNN_CONVOLUTION_BWD_FILTER_ALGO_WINOGRAD:\n    case CUDNN_CONVOLUTION_BWD_FILTER_ALGO_WINOGRAD_NONFUSED:\n    case CUDNN_CONVOLUTION_BWD_FILTER_ALGO_FFT_TILING:\n      return algo;\n    default:\n      LOG(FATAL)\n          << \"Unsupported Cudnn convolution backward algorithm for filter: \"\n          << algorithm.algo_id();\n  }\n}\n\nport::StatusOr<int> GetCudnnProperty(libraryPropertyType type) {\n  int value;\n  RETURN_IF_CUDNN_ERROR(cudnnGetProperty(type, &value));\n  return value;\n}\n\ncudnnRNNAlgo_t ToCudnnRNNAlgo(absl::optional<dnn::AlgorithmDesc> algorithm) {\n  if (!algorithm.has_value()) {\n    return CUDNN_RNN_ALGO_STANDARD;\n  }\n  cudnnRNNAlgo_t algo = static_cast<cudnnRNNAlgo_t>(algorithm->algo_id());\n  switch (algo) {\n    case CUDNN_RNN_ALGO_STANDARD:\n    case CUDNN_RNN_ALGO_PERSIST_STATIC:\n    case CUDNN_RNN_ALGO_PERSIST_DYNAMIC:\n      return algo;\n    default:\n      LOG(FATAL) << \"Unsupported Cudnn RNN algorithm: \" << algorithm->algo_id();\n  }\n}\n\nport::Status GetLoadedCudnnVersion(CudnnVersion* version) {\n  SE_ASSIGN_OR_RETURN(version->major_version, GetCudnnProperty(MAJOR_VERSION));\n  SE_ASSIGN_OR_RETURN(version->minor_version, GetCudnnProperty(MINOR_VERSION));\n  SE_ASSIGN_OR_RETURN(version->patch_level, GetCudnnProperty(PATCH_LEVEL));\n  return port::Status::OK();\n}\n\n#if CUDNN_MAJOR >= 8 && (CUDNN_MINOR > 0 || CUDNN_PATCHLEVEL >= 4)\nvoid PreloadCudnnLibrary(cudnnStatus_t (*version_check_fn)(),\n                         absl::string_view sub_library) {\n  cudnnStatus_t status = version_check_fn();\n  if (status != CUDNN_STATUS_SUCCESS) {\n    VLOG(1) << \"Could not pre-initialize cuDNN sub-library \" << sub_library\n            << \".  Error: \" << cudnnGetErrorString(status) << \".\";\n  }\n}\n#endif\n\n}  // namespace\n\nCudnnSupport::CudnnSupport(GpuExecutor* parent) : parent_(parent) {}\n\nport::Status CudnnSupport::Init() {\n  ScopedActivateExecutorContext context(parent_);\n  cudnnHandle_t cudnn_handle = nullptr;\n  const auto status = cudnnCreate(&cudnn_handle);\n  if (status == CUDNN_STATUS_SUCCESS) {\n    CudnnVersion source_version(CUDNN_MAJOR, CUDNN_MINOR, CUDNN_PATCHLEVEL);\n\n    CudnnVersion loaded_version;\n    TF_RETURN_IF_ERROR(GetLoadedCudnnVersion(&loaded_version));\n    if (!IsSourceCompatibleWithCudnnLibrary(source_version, loaded_version)) {\n      const std::string error = absl::StrCat(\n          \"Loaded runtime CuDNN library: \", loaded_version.ToString(),\n          \" but source was compiled with: \", source_version.ToString(),\n          \".  CuDNN library needs to have matching major version and equal or \"\n          \"higher minor version. If using a binary install, upgrade your CuDNN \"\n          \"library.  If building from sources, make sure the library loaded at \"\n          \"runtime is compatible with the version specified during compile \"\n          \"configuration.\");\n      LOG(ERROR) << error;\n      cudnnDestroy(cudnn_handle);\n      return port::Status(port::error::INTERNAL, error);\n    }\n\n    cudnn_.reset(new CudnnAccess(cudnn_handle));\n\n    LOG(INFO) << \"Loaded cuDNN version \" << cudnnGetVersion();\n    return port::Status::OK();\n  }\n\n  CHECK_EQ(cudnn_handle, nullptr);\n  LOG(ERROR) << \"Could not create cudnn handle: \" << ToString(status);\n  if (status == CUDNN_STATUS_NOT_INITIALIZED) {\n    auto result = gpu::Diagnostician::FindKernelDriverVersion();\n    if (!result.ok()) {\n      LOG(ERROR) << \"Error retrieving driver version: \"\n                 << cuda::DriverVersionStatusToString(result);\n    } else {\n      const auto& version = result.ValueOrDie();\n      LOG(ERROR) << \"Possibly insufficient driver version: \"\n                 << cuda::DriverVersionToString(version);\n    }\n  }\n\n  return port::Status(port::error::INTERNAL,\n                      absl::StrCat(\"cudnn library could not create a handle: \",\n                                   ToString(status)));\n}\n\nport::StatusOr<perftools::gputools::dnn::VersionInfo>\nCudnnSupport::GetVersion() {\n  CudnnVersion version;\n  TF_RETURN_IF_ERROR(GetLoadedCudnnVersion(&version));\n  return perftools::gputools::dnn::VersionInfo(\n      version.major_version, version.minor_version, version.patch_level);\n}\n\nnamespace {\n\n// Deleter functors for cuDNN types that need to be deleted.\nstruct TensorDescriptorDeleter {\n  void operator()(cudnnTensorDescriptor_t descriptor) const {\n    CHECK_CUDNN_OK(cudnnDestroyTensorDescriptor(descriptor));\n  }\n};\nstruct RNNDataDescriptorDeleter {\n  void operator()(cudnnRNNDataDescriptor_t descriptor) const {\n    CHECK_CUDNN_OK(cudnnDestroyRNNDataDescriptor(descriptor));\n  }\n};\nstruct FilterDescriptorDeleter {\n  void operator()(cudnnFilterDescriptor_t descriptor) const {\n    CHECK_CUDNN_OK(cudnnDestroyFilterDescriptor(descriptor));\n  }\n};\nstruct ConvolutionDescriptorDeleter {\n  void operator()(cudnnConvolutionDescriptor_t descriptor) const {\n    CHECK_CUDNN_OK(cudnnDestroyConvolutionDescriptor(descriptor));\n  }\n};\nstruct PoolingDescriptorDeleter {\n  void operator()(cudnnPoolingDescriptor_t descriptor) const {\n    CHECK_CUDNN_OK(cudnnDestroyPoolingDescriptor(descriptor));\n  }\n};\nstruct LrnDescriptorDeleter {\n  void operator()(cudnnLRNDescriptor_t descriptor) const {\n    CHECK_CUDNN_OK(cudnnDestroyLRNDescriptor(descriptor));\n  }\n};\n\nstruct ActivationDescriptorDeleter {\n  void operator()(cudnnActivationDescriptor_t descriptor) const {\n    CHECK_CUDNN_OK(cudnnDestroyActivationDescriptor(descriptor));\n  }\n};\nstruct DropoutDescriptorDeleter {\n  void operator()(cudnnDropoutDescriptor_t descriptor) const {\n    CHECK_CUDNN_OK(cudnnDestroyDropoutDescriptor(descriptor));\n  }\n};\nstruct RnnDescriptorDeleter {\n  void operator()(cudnnRNNDescriptor_t descriptor) const {\n    CHECK_CUDNN_OK(cudnnDestroyRNNDescriptor(descriptor));\n  }\n};\nstruct PersistentRnnPlanDeleter {\n  void operator()(cudnnPersistentRNNPlan_t plan) const {\n    CHECK_CUDNN_OK(cudnnDestroyPersistentRNNPlan(plan));\n  }\n};\n#if CUDNN_VERSION >= 7603\nstruct CtcLossDescriptorDeleter {\n  void operator()(cudnnCTCLossDescriptor_t descriptor) const {\n    CHECK_CUDNN_OK(cudnnDestroyCTCLossDescriptor(descriptor));\n  }\n};\n#endif\n\n// RAII wrappers for cuDNN types.\nusing TensorDescriptor =\n    std::unique_ptr<cudnnTensorStruct, TensorDescriptorDeleter>;\nusing RNNDataDescriptor =\n    std::unique_ptr<cudnnRNNDataStruct, RNNDataDescriptorDeleter>;\nusing FilterDescriptor =\n    std::unique_ptr<cudnnFilterStruct, FilterDescriptorDeleter>;\nusing ConvolutionDescriptor =\n    std::unique_ptr<cudnnConvolutionStruct, ConvolutionDescriptorDeleter>;\nusing PoolingDescriptor =\n    std::unique_ptr<cudnnPoolingStruct, PoolingDescriptorDeleter>;\nusing LrnDescriptor = std::unique_ptr<cudnnLRNStruct, LrnDescriptorDeleter>;\nusing ActivationDescriptor =\n    std::unique_ptr<cudnnActivationStruct, ActivationDescriptorDeleter>;\nusing DropoutDescriptor =\n    std::unique_ptr<cudnnDropoutStruct, DropoutDescriptorDeleter>;\nusing RnnDescriptor = std::unique_ptr<cudnnRNNStruct, RnnDescriptorDeleter>;\nusing PersistentRnnPlan =\n    std::unique_ptr<cudnnPersistentRNNPlan, PersistentRnnPlanDeleter>;\n#if CUDNN_VERSION >= 7603\nusing CtcLossDescriptor =\n    std::unique_ptr<cudnnCTCLossStruct, CtcLossDescriptorDeleter>;\n#endif\n\n// Factory methods for cuDNN types.\nTensorDescriptor CreateTensorDescriptor() {\n  cudnnTensorDescriptor_t result;\n  CHECK_CUDNN_OK(cudnnCreateTensorDescriptor(&result));\n  return TensorDescriptor(result);\n}\nRNNDataDescriptor CreateRNNDataDescriptor() {\n  cudnnRNNDataDescriptor_t result;\n  CHECK_CUDNN_OK(cudnnCreateRNNDataDescriptor(&result));\n  return RNNDataDescriptor(result);\n}\nFilterDescriptor CreateFilterDescriptor() {\n  cudnnFilterDescriptor_t result;\n  CHECK_CUDNN_OK(cudnnCreateFilterDescriptor(&result));\n  return FilterDescriptor(result);\n}\nConvolutionDescriptor CreateConvolutionDescriptor() {\n  cudnnConvolutionDescriptor_t result;\n  CHECK_CUDNN_OK(cudnnCreateConvolutionDescriptor(&result));\n  return ConvolutionDescriptor(result);\n}\nPoolingDescriptor CreatePoolingDescriptor() {\n  cudnnPoolingDescriptor_t result;\n  CHECK_CUDNN_OK(cudnnCreatePoolingDescriptor(&result));\n  return PoolingDescriptor(result);\n}\nLrnDescriptor CreateLrnDescriptor() {\n  cudnnLRNDescriptor_t result;\n  CHECK_CUDNN_OK(cudnnCreateLRNDescriptor(&result));\n  return LrnDescriptor(result);\n}\nActivationDescriptor CreateActivationDescriptor() {\n  cudnnActivationDescriptor_t result;\n  CHECK_CUDNN_OK(cudnnCreateActivationDescriptor(&result));\n  return ActivationDescriptor(result);\n}\nDropoutDescriptor CreateDropoutDescriptor() {\n  cudnnDropoutDescriptor_t result;\n  CHECK_CUDNN_OK(cudnnCreateDropoutDescriptor(&result));\n  return DropoutDescriptor(result);\n}\nRnnDescriptor CreateRnnDescriptor() {\n  cudnnRNNDescriptor_t result;\n  CHECK_CUDNN_OK(cudnnCreateRNNDescriptor(&result));\n  return RnnDescriptor(result);\n}\n#if CUDNN_VERSION >= 7603\nCtcLossDescriptor CreateCtcLossDescriptor() {\n  cudnnCTCLossDescriptor_t result;\n  CHECK_CUDNN_OK(cudnnCreateCTCLossDescriptor(&result));\n  return CtcLossDescriptor(result);\n}\n#endif\n\nport::StatusOr<PersistentRnnPlan> CreatePersistentRnnPlan(\n    cudnnRNNDescriptor_t rnn_desc, int batch_size, cudnnDataType_t data_type) {\n  cudnnPersistentRNNPlan_t result;\n  RETURN_IF_CUDNN_ERROR(\n      cudnnCreatePersistentRNNPlan(rnn_desc, batch_size, data_type, &result));\n  return port::StatusOr<PersistentRnnPlan>(PersistentRnnPlan(result));\n}\n\n// Turns a BatchDescriptor structure into a cudnn tensor handle within a\n// scope.\nclass CudnnTensorDescriptor {\n public:\n  CudnnTensorDescriptor(const dnn::BatchDescriptor& batch_descriptor,\n                        cudnnDataType_t elem_type)\n      : handle_(CreateTensorDescriptor()) {\n    switch (batch_descriptor.layout()) {\n      case dnn::DataLayout::kBatchYXDepth:\n      case dnn::DataLayout::kBatchDepthYX: {\n        const int nd = batch_descriptor.ndims() + 2;\n        // cuDNN requires the strides and dims to be ordered as BDYX.\n        std::vector<int64> strides64 =\n            batch_descriptor.full_strides(dnn::DataLayout::kBatchDepthYX);\n        std::vector<int64> dims64 =\n            batch_descriptor.full_dims(dnn::DataLayout::kBatchDepthYX);\n\n        // cuDNN requires arrays of ints.\n        std::vector<int> strides(nd);\n        std::vector<int> dims(nd);\n        std::transform(strides64.cbegin(), strides64.cend(), strides.begin(),\n                       &CheckedNarrowing<int64, int>);\n        std::transform(dims64.cbegin(), dims64.cend(), dims.begin(),\n                       &CheckedNarrowing<int64, int>);\n        CHECK_CUDNN_OK(cudnnSetTensorNdDescriptor(handle_.get(), elem_type, nd,\n                                                  dims.data(), strides.data()))\n            << \"batch_descriptor: \" << batch_descriptor.ToString();\n      } break;\n      case dnn::DataLayout::kBatchDepthYX4: {\n        CHECK_CUDNN_OK(cudnnSetTensor4dDescriptor(\n            handle_.get(), CUDNN_TENSOR_NCHW_VECT_C, elem_type,\n            batch_descriptor.count(), batch_descriptor.feature_map_count(),\n            batch_descriptor.height(), batch_descriptor.width()))\n            << \"batch_descriptor: \" << batch_descriptor.ToString();\n      } break;\n      default:\n        LOG(FATAL) << \"Unsupported tensor format \"\n                   << DataLayoutString(batch_descriptor.layout());\n        break;\n    }\n  }\n\n  cudnnTensorDescriptor_t handle() const { return handle_.get(); }\n\n private:\n  TensorDescriptor handle_;\n\n  SE_DISALLOW_COPY_AND_ASSIGN(CudnnTensorDescriptor);\n};\n\n// Turns a FilterDescriptor structure into a cudnn filter handle within a\n// scope.\nclass CudnnFilterDescriptor {\n public:\n  CudnnFilterDescriptor(const dnn::FilterDescriptor& filter_descriptor,\n                        cudnnDataType_t elem_type)\n      : handle_(CreateFilterDescriptor()) {\n    // TODO(b/23032134): Even if the filter layout is not supported,\n    // cudnnSetFilter4DDescriptor_v4 will return CUDNN_STATUS_SUCCESS because\n    // it does not take layout as an input. Maybe force cuDNN by giving wrong\n    // inputs intentionally?\n    cudnnTensorFormat_t format;\n    switch (filter_descriptor.layout()) {\n      case dnn::FilterLayout::kOutputInputYX:\n        format = CUDNN_TENSOR_NCHW;\n        break;\n      case dnn::FilterLayout::kOutputYXInput:\n        format = CUDNN_TENSOR_NHWC;\n        break;\n      case dnn::FilterLayout::kOutputInputYX4:\n        format = CUDNN_TENSOR_NCHW_VECT_C;\n        break;\n      default:\n        LOG(FATAL) << \"Unsupported filter format \"\n                   << FilterLayoutString(filter_descriptor.layout());\n        break;\n    }\n\n    std::vector<int> dims(2 + filter_descriptor.ndims());\n    dims[0] = filter_descriptor.output_feature_map_count();\n    dims[1] = filter_descriptor.input_feature_map_count();\n    absl::Span<const int64> spatial_dims =\n        filter_descriptor.input_filter_dims();\n    std::copy(spatial_dims.begin(), spatial_dims.end(), dims.begin() + 2);\n\n    CHECK_CUDNN_OK(cudnnSetFilterNdDescriptor(handle_.get(), elem_type, format,\n                                              dims.size(), dims.data()));\n  }\n\n  cudnnFilterDescriptor_t handle() const { return handle_.get(); }\n\n private:\n  FilterDescriptor handle_;  // Owned.\n\n  SE_DISALLOW_COPY_AND_ASSIGN(CudnnFilterDescriptor);\n};\n\n// A helper function to decide whether to use\n// CUDNN_BATCHNORM_SPATIAL_PERSISTENT in batchnorm. This mode can be faster in\n// some tasks because an optimized path may be selected for CUDNN_DATA_FLOAT\n// and CUDNN_DATA_HALF data types, compute capability 6.0 or higher. The\n// reason we set it to false by default is that this mode may use scaled\n// atomic integer reduction that may cause a numerical overflow for certain\n// input data range.\n// TODO(yangzihao): Use autotune to choose between this mode and\n// CUDNN_BATCHNORM_SPATIAL mode.\nbool BatchnormSpatialPersistentEnabled() {\n  static bool is_enabled = [] {\n    bool is_enabled = false;\n    TF_CHECK_OK(tensorflow::ReadBoolFromEnvVar(\n        \"TF_USE_CUDNN_BATCHNORM_SPATIAL_PERSISTENT\",\n        /*default_val=*/false, &is_enabled));\n    return is_enabled;\n  }();\n  return is_enabled;\n}\n\n// The following function allows deterministic ops to be implemented relatively\n// quickly using environment variables. It is intended to be temporary. The\n// longer-term intention is to enable deterministic ops via tf.config and\n// appropriate plumbing. See the discussion on PR 34951 for more information:\n// https://github.com/tensorflow/tensorflow/pull/34951#discussion_r355682316\n// This function and associated comment are replicated in the following three\n// places:\n//   1. tensorflow/compiler/xla/service/gpu/gpu_conv_algorithm_picker.cc\n//   2. tensorflow/core/kernels/gpu_utils.cc\n//   3. tensorflow/stream_executor/cuda/cuda_dnn.cc\n// When implementing the plumbing, you should also search for the use of\n// TF_DETERMINISTIC_OPS on its own.\n// TODO(duncanriach): move to an API that uses tf.config and implement the first\n//                    phase of plumbing.\nbool RequireCudnnDeterminism() {\n  static bool require_cudnn_determinism = [] {\n    bool deterministic_ops = false;\n    TF_CHECK_OK(tensorflow::ReadBoolFromEnvVar(\"TF_DETERMINISTIC_OPS\",\n                                               /*default_val=*/false,\n                                               &deterministic_ops));\n    bool cudnn_deterministic = false;\n    TF_CHECK_OK(tensorflow::ReadBoolFromEnvVar(\"TF_CUDNN_DETERMINISTIC\",\n                                               /*default_val=*/false,\n                                               &cudnn_deterministic));\n    return deterministic_ops || cudnn_deterministic;\n  }();\n  return require_cudnn_determinism;\n}\n\n// A helper function to decide whether to force the default conv algorithm.\nbool ConvUseDefaultAlgorithm() {\n  static bool use_default = [] {\n    bool use_default = false;\n    TF_CHECK_OK(tensorflow::ReadBoolFromEnvVar(\"TF_USE_DEFAULT_CONV_ALGO\",\n                                               /*default_val=*/false,\n                                               &use_default));\n    return use_default;\n  }();\n  return use_default;\n}\n\nstd::tuple<int, int> GetCcMajorMinor(Stream* stream) {\n  int cc_major, cc_minor;\n  stream->parent()->GetDeviceDescription().cuda_compute_capability(&cc_major,\n                                                                   &cc_minor);\n  return std::make_tuple(cc_major, cc_minor);\n}\n\n// Turns a ConvolutionDescriptor structure into a cudnn convolution handle\n// within a scope.\nclass CudnnConvolutionDescriptor {\n public:\n  CudnnConvolutionDescriptor(\n      const dnn::ConvolutionDescriptor& convolution_descriptor,\n      cudnnDataType_t data_type)\n      : handle_(CreateConvolutionDescriptor()) {\n    absl::Span<const int64> strides64 = convolution_descriptor.strides();\n    absl::Span<const int64> padding64 = convolution_descriptor.padding();\n    absl::Span<const int64> dilations64 = convolution_descriptor.dilations();\n    CHECK_NE(convolution_descriptor.pad_alignment(),\n             dnn::PadAlignment::kTensorFlowPadding)\n        << \"TensorFlow padding alignment is not supported.\";\n\n    // cuDNN requires arrays of ints.\n    std::vector<int> strides(convolution_descriptor.ndims());\n    std::vector<int> padding(convolution_descriptor.ndims());\n    std::vector<int> dilations(convolution_descriptor.ndims());\n    std::transform(strides64.cbegin(), strides64.cend(), strides.begin(),\n                   &CheckedNarrowing<int64, int>);\n    std::transform(padding64.cbegin(), padding64.cend(), padding.begin(),\n                   &CheckedNarrowing<int64, int>);\n    // TODO(yangzihao): Test with negative dilation to make sure that cudnn\n    // doesn't crash.\n    std::transform(dilations64.cbegin(), dilations64.cend(), dilations.begin(),\n                   &CheckedNarrowing<int64, int>);\n\n    CHECK_CUDNN_OK(cudnnSetConvolutionNdDescriptor(\n        handle_.get(), convolution_descriptor.ndims(), padding.data(),\n        strides.data(), dilations.data(),\n        convolution_descriptor.convolution_not_crosscorr()\n            ? CUDNN_CONVOLUTION\n            : CUDNN_CROSS_CORRELATION,\n        data_type));\n\n#if CUDNN_MAJOR >= 7\n    VLOG(2) << \"Requesting grouped convolution: \"\n            << convolution_descriptor.group_count();\n    CHECK_CUDNN_OK(cudnnSetConvolutionGroupCount(\n        handle_.get(), convolution_descriptor.group_count()));\n#else\n    CHECK_EQ(convolution_descriptor.group_count(), 1)\n        << \"Requested grouped convolution for cuDNN version < 7\";\n#endif\n  }\n\n  void set_use_tensor_op_math(bool use_tensor_op_math) {\n    cudnnMathType_t math_type =\n#if CUDNN_VERSION >= 8000\n        (use_tensor_op_math ? CUDNN_TENSOR_OP_MATH : CUDNN_FMA_MATH);\n#else\n        (use_tensor_op_math ? CUDNN_TENSOR_OP_MATH : CUDNN_DEFAULT_MATH);\n#endif\n    CHECK_CUDNN_OK(cudnnSetConvolutionMathType(handle_.get(), math_type));\n  }\n\n  cudnnConvolutionDescriptor_t handle() const { return handle_.get(); }\n\n private:\n  ConvolutionDescriptor handle_;  // Owned.\n\n  SE_DISALLOW_COPY_AND_ASSIGN(CudnnConvolutionDescriptor);\n};\n\n// A helper function to query if a CudnnConvolutionDescriptor has tensor_op_math\n// set\nstatic bool IsTensorMathOpSet(const CudnnConvolutionDescriptor& conv) {\n  cudnnMathType_t math_type;\n  CHECK_CUDNN_OK(cudnnGetConvolutionMathType(conv.handle(), &math_type));\n#if CUDNN_VERSION >= 8000\n  return math_type != CUDNN_FMA_MATH;\n#else\n  return math_type == CUDNN_TENSOR_OP_MATH;\n#endif\n}\n\nstatic bool TensorOpMathAvailable(int cc_major) { return cc_major >= 7; }\n\nstatic bool IsTensorMathEnabled(Stream* stream, dnn::DataType input_type) {\n  int cc_major, cc_minor;\n  std::tie(cc_major, cc_minor) = GetCcMajorMinor(stream);\n  if (!TensorOpMathAvailable(cc_major)) {\n    return false;\n  }\n  if (input_type == dnn::DataType::kFloat) {\n#if CUDNN_VERSION < 8000\n    return false;\n#else\n    if (!tensorflow::tensor_float_32_execution_enabled()) {\n      return false;\n    }\n#endif\n  }\n  return true;\n}\n\n// Turns a PoolingDescriptor structure into a cudnn pooling descriptor handle\n// within a scope.\nclass CudnnPoolingDescriptor {\n public:\n  explicit CudnnPoolingDescriptor(\n      const dnn::PoolingDescriptor& pooling_descriptor)\n      : handle_(CreatePoolingDescriptor()) {\n    absl::Span<const int64> strides64 = pooling_descriptor.strides();\n    absl::Span<const int64> padding64 = pooling_descriptor.padding();\n    absl::Span<const int64> shape64 = pooling_descriptor.window();\n\n    const int nd = pooling_descriptor.ndims();\n    std::vector<int> shape(nd);\n    std::vector<int> padding(nd);\n    std::vector<int> strides(nd);\n    std::transform(strides64.cbegin(), strides64.cend(), strides.begin(),\n                   &CheckedNarrowing<int64, int>);\n    std::transform(padding64.cbegin(), padding64.cend(), padding.begin(),\n                   &CheckedNarrowing<int64, int>);\n    std::transform(shape64.cbegin(), shape64.cend(), shape.begin(),\n                   &CheckedNarrowing<int64, int>);\n    bool propagate_nans = pooling_descriptor.propagate_nans();\n    const auto cudnn_max_pooling_mode = RequireCudnnDeterminism()\n                                            ? CUDNN_POOLING_MAX_DETERMINISTIC\n                                            : CUDNN_POOLING_MAX;\n    CHECK_CUDNN_OK(cudnnSetPoolingNdDescriptor(\n        handle_.get(),\n        (pooling_descriptor.mode() == dnn::PoolingMode::kMaximum\n             ? cudnn_max_pooling_mode\n             : CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING),\n        propagate_nans ? CUDNN_PROPAGATE_NAN : CUDNN_NOT_PROPAGATE_NAN, nd,\n        shape.data(), padding.data(), strides.data()));\n  }\n\n  cudnnPoolingDescriptor_t handle() const { return handle_.get(); }\n\n private:\n  PoolingDescriptor handle_;  // Owned.\n\n  SE_DISALLOW_COPY_AND_ASSIGN(CudnnPoolingDescriptor);\n};\n\n// Turns a NormalizeDescriptor structure into a cudnn LRN descriptor handle.\nclass CudnnNormalizeDescriptor {\n public:\n  explicit CudnnNormalizeDescriptor(\n      const dnn::NormalizeDescriptor& normalize_descriptor)\n      : handle_(CreateLrnDescriptor()) {\n    // The range specifies that the indices in the closed range\n    // [i - range, i + range] should be included in the normalization for index\n    // i. The lrnN value is the total number of elements in the range, so\n    // lrnN = 2*range + 1.\n    unsigned lrnN = 2 * normalize_descriptor.range() + 1;\n\n    // Note that SE defines the normalization operation as\n    //\n    //  U_i = V_i / ((bias +  alpha      * (sum_j V_j^2)) ^ beta)\n    //\n    // but cuDNN defines it as\n    //\n    //  U_i = V_i / ((bias + (alpha / n) * (sum_j V_j^2)) ^ beta)\n    //\n    // i.e. there is a factor of n difference between the meaning of the alphas\n    // in the two contexts. The cuDNN alpha is n times the SE alpha.\n    double lrnAlpha = lrnN * normalize_descriptor.alpha();\n\n    double lrnBeta = normalize_descriptor.beta();\n    double lrnK = normalize_descriptor.bias();\n    CHECK_CUDNN_OK(\n        cudnnSetLRNDescriptor(handle_.get(), lrnN, lrnAlpha, lrnBeta, lrnK));\n  }\n\n  cudnnLRNDescriptor_t handle() const { return handle_.get(); }\n\n private:\n  LrnDescriptor handle_;  // Owned.\n\n  SE_DISALLOW_COPY_AND_ASSIGN(CudnnNormalizeDescriptor);\n};\n\n// Turns a ActivationDescriptor structure into a cudnn activation\n// descriptor handle within a scope.\nclass CudnnActivationDescriptor {\n public:\n  CudnnActivationDescriptor(dnn::ActivationMode activation_mode,\n                            cudnnNanPropagation_t nan_propagation,\n                            double value_max)\n      : handle_(CreateActivationDescriptor()) {\n    double relu_ceiling = 0.0;\n    cudnnActivationMode_t mode;\n    switch (activation_mode) {\n      case dnn::ActivationMode::kNone:\n        mode = CUDNN_ACTIVATION_IDENTITY;\n        break;\n      case dnn::ActivationMode::kRelu6:\n        relu_ceiling = 6.0;\n        mode = CUDNN_ACTIVATION_CLIPPED_RELU;\n        break;\n      case dnn::ActivationMode::kReluX:\n        relu_ceiling = value_max;\n        mode = CUDNN_ACTIVATION_CLIPPED_RELU;\n        break;\n      case dnn::ActivationMode::kRelu:\n        mode = CUDNN_ACTIVATION_RELU;\n        break;\n      case dnn::ActivationMode::kSigmoid:\n        mode = CUDNN_ACTIVATION_SIGMOID;\n        break;\n      case dnn::ActivationMode::kTanh:\n        mode = CUDNN_ACTIVATION_TANH;\n        break;\n      default:\n        LOG(FATAL) << \"unrecognized activation mode: \"\n                   << static_cast<int>(activation_mode);\n    }\n\n    CHECK_CUDNN_OK(cudnnSetActivationDescriptor(handle_.get(), mode,\n                                                nan_propagation, relu_ceiling));\n  }\n\n  cudnnActivationDescriptor_t handle() const { return handle_.get(); }\n\n private:\n  ActivationDescriptor handle_;  // Owned.\n\n  SE_DISALLOW_COPY_AND_ASSIGN(CudnnActivationDescriptor);\n};\n\ncudnnDataType_t ToCudnnDataType(\n    dnn::DataType data_type,\n    dnn::DataLayout data_layout = dnn::DataLayout::kBatchDepthYX) {\n  switch (data_type) {\n    case dnn::DataType::kFloat:\n      return CUDNN_DATA_FLOAT;\n    case dnn::DataType::kDouble:\n      return CUDNN_DATA_DOUBLE;\n    case dnn::DataType::kHalf:\n      return CUDNN_DATA_HALF;\n    case dnn::DataType::kInt8:\n      return data_layout == dnn::DataLayout::kBatchDepthYX4 ? CUDNN_DATA_INT8x4\n                                                            : CUDNN_DATA_INT8;\n    case dnn::DataType::kInt32:\n      return CUDNN_DATA_INT32;\n    default:\n      LOG(FATAL) << \"Invalid DNN data type: \" << static_cast<int>(data_type);\n  }\n}\n\ncudnnDataType_t ToCudnnDataType(dnn::DataType data_type,\n                                dnn::FilterLayout filter_layout) {\n  if (data_type == dnn::DataType::kInt8 &&\n      filter_layout == dnn::FilterLayout::kOutputInputYX4) {\n    return CUDNN_DATA_INT8x4;\n  }\n  return ToCudnnDataType(data_type);\n}\n\ntemplate <typename T>\ncudnnDataType_t GetCudnnDataType(\n    dnn::DataLayout data_layout = dnn::DataLayout::kBatchDepthYX) {\n  return ToCudnnDataType(dnn::ToDataType<T>::value, data_layout);\n}\n\ncudnnRNNInputMode_t ToCudnnRnnInputMode(dnn::RnnInputMode input_mode) {\n  switch (input_mode) {\n    case dnn::RnnInputMode::kRnnLinearSkip:\n    case dnn::RnnInputMode::kRnnSkipInput:\n      return static_cast<cudnnRNNInputMode_t>(input_mode);\n    default:\n      LOG(FATAL) << \"Invalid RNN input mode: \" << static_cast<int>(input_mode);\n  }\n}\n\ncudnnDirectionMode_t ToCudnnRnnDirectionMode(\n    dnn::RnnDirectionMode direction_mode) {\n  switch (direction_mode) {\n    case dnn::RnnDirectionMode::kRnnUnidirectional:\n    case dnn::RnnDirectionMode::kRnnBidirectional:\n      return static_cast<cudnnDirectionMode_t>(direction_mode);\n    default:\n      LOG(FATAL) << \"Invalid RNN direction mode: \"\n                 << static_cast<int>(direction_mode);\n  }\n}\n\ncudnnRNNMode_t ToCudnnRnnMode(dnn::RnnMode rnn_mode) {\n  switch (rnn_mode) {\n    case dnn::RnnMode::kRnnRelu:\n    case dnn::RnnMode::kRnnTanh:\n    case dnn::RnnMode::kRnnLstm:\n    case dnn::RnnMode::kRnnGru:\n      return static_cast<cudnnRNNMode_t>(rnn_mode);\n    default:\n      LOG(FATAL) << \"Invalid RNN Mode: \" << static_cast<int>(rnn_mode);\n  }\n}\n\nint CudnnDataTypeToByteSize(cudnnDataType_t data_type) {\n  switch (data_type) {\n    case CUDNN_DATA_FLOAT:\n      return sizeof(float);\n    case CUDNN_DATA_DOUBLE:\n      return sizeof(double);\n    case CUDNN_DATA_HALF:\n      return sizeof(Eigen::half);\n    default:\n      LOG(FATAL) << \"Invalid DNN data type: \" << static_cast<int>(data_type);\n  }\n}\n\nclass CudnnDropoutDescriptor {\n  explicit CudnnDropoutDescriptor(DropoutDescriptor handle)\n      : handle_(std::move(handle)) {}\n\n public:\n  CudnnDropoutDescriptor(CudnnDropoutDescriptor&&) = default;\n\n  static port::StatusOr<CudnnDropoutDescriptor> Create(\n      const CudnnHandle& cudnn, float dropout, uint64 seed,\n      ScratchAllocator* state_allocator) {\n    DropoutDescriptor handle = CreateDropoutDescriptor();\n\n    if (dropout == 0.0f) {\n      // Return 'empty' dropout descriptor.\n      return CudnnDropoutDescriptor(std::move(handle));\n    }\n\n    DeviceMemory<uint8> state_memory;\n    if (state_allocator) {\n      size_t state_sizes_in_bytes = 0;\n      RETURN_IF_CUDNN_ERROR(\n          cudnnDropoutGetStatesSize(cudnn.handle(), &state_sizes_in_bytes));\n      SE_ASSIGN_OR_RETURN(state_memory,\n                          state_allocator->AllocateBytes(state_sizes_in_bytes));\n    }\n    RETURN_IF_CUDNN_ERROR(cudnnSetDropoutDescriptor(\n        handle.get(), cudnn.handle(), dropout, state_memory.opaque(),\n        state_memory.size(), seed));\n\n    return CudnnDropoutDescriptor(std::move(handle));\n  }\n\n  cudnnDropoutDescriptor_t handle() const { return handle_.get(); }\n\n private:\n  DropoutDescriptor handle_;  // Owned.\n  SE_DISALLOW_COPY_AND_ASSIGN(CudnnDropoutDescriptor);\n};\n\nclass CudnnRnnParamsDescriptor {\n  typedef dnn::RnnDescriptor::ParamsRegions ParamsRegions;\n\n  CudnnRnnParamsDescriptor(FilterDescriptor handle, int64 params_size_in_bytes,\n                           ParamsRegions weights, ParamsRegions biases)\n      : handle_(std::move(handle)),\n        params_size_in_bytes_(params_size_in_bytes),\n        weights_(std::move(weights)),\n        biases_(std::move(biases)) {}\n\n public:\n  CudnnRnnParamsDescriptor(CudnnRnnParamsDescriptor&&) = default;\n\n  static port::StatusOr<CudnnRnnParamsDescriptor> Create(\n      const CudnnHandle& cudnn, int input_size, cudnnDataType_t data_type,\n      cudnnRNNDescriptor_t rnn_desc, cudnnRNNMode_t rnn_mode,\n      cudnnDirectionMode_t direction_mode, int num_layers);\n\n  cudnnFilterDescriptor_t handle() const { return handle_.get(); }\n  int64 params_size_in_bytes() const { return params_size_in_bytes_; }\n  ParamsRegions params_weights() const { return weights_; }\n  ParamsRegions params_biases() const { return biases_; }\n\n private:\n  FilterDescriptor handle_;\n  int64 params_size_in_bytes_;\n  ParamsRegions weights_;\n  ParamsRegions biases_;\n  SE_DISALLOW_COPY_AND_ASSIGN(CudnnRnnParamsDescriptor);\n};\n\n}  // namespace\n\nclass CudnnRnnDescriptor : public dnn::RnnDescriptor {\n  CudnnRnnDescriptor(const CudnnHandle& cudnn, gpu::RnnDescriptor rnn_desc,\n                     PersistentRnnPlan rnn_plan, int num_layers,\n                     int hidden_size, int input_size, int cell_size,\n                     int batch_size, cudnnRNNInputMode_t input_mode,\n                     cudnnDirectionMode_t direction_mode,\n                     cudnnRNNMode_t rnn_mode, cudnnDataType_t data_type,\n                     cudnnDataType_t compute_type,\n                     const dnn::AlgorithmConfig& algorithm_config,\n                     CudnnDropoutDescriptor dropout_desc,\n                     CudnnRnnParamsDescriptor params_desc)\n      : rnn_desc_(std::move(rnn_desc)),\n        rnn_plan_(std::move(rnn_plan)),\n        num_layers_(num_layers),\n        hidden_size_(hidden_size),\n        input_size_(input_size),\n        cell_size_(cell_size),\n        batch_size_(batch_size),\n        rnn_algo_(ToCudnnRNNAlgo(algorithm_config.algorithm())),\n        input_mode_(input_mode),\n        direction_mode_(direction_mode),\n        rnn_mode_(rnn_mode),\n        data_type_(data_type),\n        compute_type_(compute_type),\n        algorithm_config_(algorithm_config),\n        dropout_desc_(std::move(dropout_desc)),\n        params_desc_(std::move(params_desc)) {}\n\n public:\n  CudnnRnnDescriptor(CudnnRnnDescriptor&& other) = default;\n\n  static port::StatusOr<CudnnRnnDescriptor> Create(\n      const CudnnHandle& cudnn, int num_layers, int hidden_size, int input_size,\n      int cell_size, int batch_size, cudnnRNNInputMode_t input_mode,\n      cudnnDirectionMode_t direction_mode, cudnnRNNMode_t rnn_mode,\n      cudnnDataType_t data_type, cudnnDataType_t compute_type,\n      const dnn::AlgorithmConfig& algorithm_config, float dropout, uint64 seed,\n      ScratchAllocator* state_allocator, bool use_padded_io) {\n    SE_ASSIGN_OR_RETURN(\n        CudnnDropoutDescriptor dropout_desc,\n        CudnnDropoutDescriptor::Create(cudnn, dropout, seed, state_allocator));\n\n    gpu::RnnDescriptor rnn_desc = CreateRnnDescriptor();\n    cudnnRNNAlgo_t rnn_algo = ToCudnnRNNAlgo(algorithm_config.algorithm());\n\n    // TODO: allow the user to choose an algorithm.\n    auto proj_size = hidden_size;\n    hidden_size = std::max(hidden_size, cell_size);\n\n    // Require explicit algorithm config to enable tensor cores. Some configs\n    // return CUDNN_NOT_SUPPORTED when tensor ops are enabled (which is against\n    // the idiom that enabling tensor ops is only a hint: see nvbugs/2172799).\n    // We can only reasonably expect the user to handle the subsequent failure\n    // in profile mode, which is run with algorithms returned from\n    // GetRnnAlgorithms() (which are non-default and explicitly set whether to\n    // use tensor ops). CuDNN 7.2.1 fixed this issue.\n    // TODO(csigg): Minimal support cuDNN version is 7.3, clean up.\n    bool allow_tensor_ops = data_type == CUDNN_DATA_HALF;\n    if (data_type == CUDNN_DATA_FLOAT)\n      allow_tensor_ops = tensorflow::tensor_float_32_execution_enabled();\n    bool use_tensor_ops =\n        algorithm_config.algorithm().has_value()\n            ? algorithm_config.algorithm()->tensor_ops_enabled()\n            : allow_tensor_ops;\n    if (use_tensor_ops && !allow_tensor_ops) {\n      return port::Status(port::error::INVALID_ARGUMENT,\n                          \"Algo requests disallowed tensor op evaluation.\");\n    }\n\n    cudnnMathType_t math_type =\n        use_tensor_ops ? CUDNN_TENSOR_OP_MATH : CUDNN_DEFAULT_MATH;\n\n#if CUDNN_VERSION >= 8000\n    cudnnRNNBiasMode_t bias_mode = CUDNN_RNN_DOUBLE_BIAS;\n    uint32_t aux_flags = 0;\n    if (use_padded_io) aux_flags |= CUDNN_RNN_PADDED_IO_ENABLED;\n    RETURN_IF_CUDNN_ERROR(cudnnSetRNNDescriptor_v8(\n        /*rnnDesc=*/rnn_desc.get(), /*algo=*/rnn_algo, /*cellMode=*/rnn_mode,\n        /*biasMode=*/bias_mode, /*dirMode=*/direction_mode,\n        /*inputMode=*/input_mode,\n        /*dataType=*/data_type, /*mathPrec=*/compute_type,\n        /*mathType=*/math_type,\n        /*inputSize=*/input_size,\n        /*hiddenSize=*/hidden_size, /*projSize=*/proj_size,\n        /*numLayers=*/num_layers,\n        /*dropoutDesc=*/dropout_desc.handle(),\n        /*auxFlags=*/aux_flags));\n#else\n    RETURN_IF_CUDNN_ERROR(cudnnSetRNNDescriptor_v6(\n        cudnn.handle(), /*rnnDesc=*/rnn_desc.get(),\n        /*hiddenSize=*/hidden_size, /*numLayers=*/num_layers,\n        /*dropoutDesc=*/dropout_desc.handle(), /*inputMode=*/input_mode,\n        /*direction=*/direction_mode, /*mode=*/rnn_mode, /*algo=*/rnn_algo,\n        /*dataType=*/compute_type));\n    CHECK_CUDNN_OK(cudnnSetRNNMatrixMathType(rnn_desc.get(), math_type));\n\n    if (proj_size < hidden_size) {\n      RETURN_IF_CUDNN_ERROR(cudnnSetRNNProjectionLayers(\n          cudnn.handle(), /*rnnDesc=*/rnn_desc.get(),\n          /*recProjSize=*/proj_size, /*outProjSize=*/0));\n    }\n\n    // TODO: For now, we only use cudnnRNN**Ex API to process padded inputs.\n    // But in the future if these APIs are used to process full length arrays,\n    // we need to distinguish when to set it.\n    if (use_padded_io) {\n      RETURN_IF_CUDNN_ERROR(\n          cudnnSetRNNPaddingMode(rnn_desc.get(), CUDNN_RNN_PADDED_IO_ENABLED));\n    }\n#endif\n\n    port::StatusOr<PersistentRnnPlan> rnn_plan_wrapper;\n    PersistentRnnPlan rnn_plan;\n    if (rnn_algo == CUDNN_RNN_ALGO_PERSIST_DYNAMIC) {\n      CHECK_GE(batch_size, 0);\n      rnn_plan_wrapper =\n          CreatePersistentRnnPlan(rnn_desc.get(), batch_size, data_type);\n      if (!rnn_plan_wrapper.ok()) {\n        return port::StatusOr<CudnnRnnDescriptor>(rnn_plan_wrapper.status());\n      } else {\n        rnn_plan = rnn_plan_wrapper.ConsumeValueOrDie();\n        RETURN_IF_CUDNN_ERROR(\n            cudnnSetPersistentRNNPlan(rnn_desc.get(), rnn_plan.get()));\n      }\n    }\n\n    // Create the params handle.\n    SE_ASSIGN_OR_RETURN(auto params_desc,\n                        CudnnRnnParamsDescriptor::Create(\n                            cudnn, input_size, data_type, rnn_desc.get(),\n                            rnn_mode, direction_mode, num_layers));\n\n    return CudnnRnnDescriptor(cudnn, std::move(rnn_desc), std::move(rnn_plan),\n                              num_layers, hidden_size, input_size, cell_size,\n                              batch_size, input_mode, direction_mode, rnn_mode,\n                              data_type, compute_type, algorithm_config,\n                              std::move(dropout_desc), std::move(params_desc));\n  }\n\n  cudnnRNNDescriptor_t handle() const { return rnn_desc_.get(); }\n  int num_layers() const { return num_layers_; }\n  int hidden_size() const { return hidden_size_; }\n  int input_size() const { return input_size_; }\n  int cell_size() const { return cell_size_; }\n  int batch_size() const { return batch_size_; }\n  cudnnRNNInputMode_t input_mode() const { return input_mode_; }\n  cudnnDirectionMode_t direction_mode() const { return direction_mode_; }\n  cudnnRNNMode_t rnn_mode() const { return rnn_mode_; }\n  cudnnDataType_t data_type() const { return data_type_; }\n  cudnnDataType_t compute_type() const { return compute_type_; }\n  const dnn::AlgorithmConfig& algorithm_config() const {\n    return algorithm_config_;\n  }\n  int64 ParamsSizeInBytes() const override {\n    return params_desc_.params_size_in_bytes();\n  }\n  cudnnFilterDescriptor_t params_handle() const {\n    return params_desc_.handle();\n  }\n  ParamsRegions ParamsWeightRegions() const override {\n    return params_desc_.params_weights();\n  }\n  ParamsRegions ParamsBiasRegions() const override {\n    return params_desc_.params_biases();\n  }\n\n private:\n  gpu::RnnDescriptor rnn_desc_;\n  PersistentRnnPlan rnn_plan_;\n  int num_layers_;\n  int hidden_size_;\n  int input_size_;\n  // cell_size_ is the size of cell state, which will be different from\n  // hidden_size_ if the projection is used.\n  int cell_size_;\n  // batch_size_ is set to -1 when not using CUDNN_RNN_ALGO_PERSIST_DYNAMIC\n  // algorithm.\n  int batch_size_;\n  cudnnRNNAlgo_t rnn_algo_;\n  cudnnRNNInputMode_t input_mode_;\n  cudnnDirectionMode_t direction_mode_;\n  cudnnRNNMode_t rnn_mode_;\n  cudnnDataType_t data_type_;\n  cudnnDataType_t compute_type_;\n  dnn::AlgorithmConfig algorithm_config_;\n  CudnnDropoutDescriptor dropout_desc_;\n  CudnnRnnParamsDescriptor params_desc_;\n  SE_DISALLOW_COPY_AND_ASSIGN(CudnnRnnDescriptor);\n};\n\n#if CUDNN_VERSION >= 7603\nclass CudnnCtcLossDescriptor {\n public:\n  explicit CudnnCtcLossDescriptor(cudnnDataType_t data_type)\n      : handle_(CreateCtcLossDescriptor()) {\n    CHECK_CUDNN_OK(cudnnSetCTCLossDescriptorEx(\n        /*ctcLossDesc=*/handle_.get(),\n        /*compType=*/data_type,\n        /*normMode=*/CUDNN_LOSS_NORMALIZATION_SOFTMAX,\n        /*gradMode=*/CUDNN_NOT_PROPAGATE_NAN));\n  }\n\n  cudnnCTCLossDescriptor_t handle() const { return handle_.get(); }\n\n private:\n  CtcLossDescriptor handle_;  // Owned\n\n  SE_DISALLOW_COPY_AND_ASSIGN(CudnnCtcLossDescriptor);\n};\n#else\n// dummy class\nclass CudnnCtcLossDescriptor {\n public:\n  CudnnCtcLossDescriptor(cudnnDataType_t data_type) {}\n};\n#endif\n\nnamespace {\n\n// Check if the LSTM projection is used. If yes, an additional weight matrix\n// (projection matrix) will be fetched to the 'weights'. Otherwise, nothing will\n// be done.\nport::Status CheckAndFetchProjectionWeights(\n    const CudnnHandle& cudnn, cudnnRNNDescriptor_t rnn_desc, const int layer,\n    const TensorDescriptor& input_desc, const FilterDescriptor& filter_desc,\n    const FilterDescriptor& region_desc_handle,\n    dnn::RnnDescriptor::ParamsRegions* weights) {\n  int hidden_size_v;\n  int num_layers_v;\n  cudnnDropoutDescriptor_t dropout_desc;\n  cudnnRNNInputMode_t input_mode;\n  cudnnDirectionMode_t direction;\n  cudnnRNNMode_t mode;\n  cudnnRNNAlgo_t algo;\n  cudnnDataType_t data_type;\n#if CUDNN_VERSION >= 8000\n  RETURN_IF_CUDNN_ERROR(cudnnGetRNNDescriptor_v6(\n      /*handle=*/cudnn.handle(), /*rnnDesc=*/rnn_desc,\n      /*hiddenSize=*/&hidden_size_v,\n      /*numLayers=*/&num_layers_v,\n      /*dropoutDesc=*/&dropout_desc,\n      /*inputMode=*/&input_mode,\n      /*direction=*/&direction,\n      /*mode=*/&mode,\n      /*algo=*/&algo,\n      /*mathPrec=*/&data_type));\n#else\n  RETURN_IF_CUDNN_ERROR(cudnnGetRNNDescriptor(\n      /*handle=*/cudnn.handle(), /*rnnDesc=*/rnn_desc,\n      /*hiddenSize=*/&hidden_size_v,\n      /*numLayers=*/&num_layers_v,\n      /*dropoutDesc=*/&dropout_desc,\n      /*inputMode=*/&input_mode,\n      /*direction=*/&direction,\n      /*mode=*/&mode,\n      /*algo=*/&algo,\n      /*mathPrec=*/&data_type));\n#endif\n  int rec_proj_size_v;\n  int out_proj_size_v;\n  RETURN_IF_CUDNN_ERROR(cudnnGetRNNProjectionLayers(\n      /*handle=*/cudnn.handle(),\n      /*rnnDesc=*/rnn_desc,\n      /*recProjSize*/ &rec_proj_size_v,\n      /*outProjSize*/ &out_proj_size_v));\n  if (rec_proj_size_v != hidden_size_v) {\n    void* offset = nullptr;\n    int region_id = 8;\n    RETURN_IF_CUDNN_ERROR(cudnnGetRNNLinLayerMatrixParams(\n        /*handle=*/cudnn.handle(), /*rnnDesc=*/rnn_desc,\n        /*layer=*/layer, /*xDesc=*/input_desc.get(),\n        /*wDesc=*/filter_desc.get(),\n        /*w=*/nullptr, /*linLayerID=*/region_id,\n        /*linLayerMatDesc=*/region_desc_handle.get(),\n        /*linLayerMat or linLayerBias=*/&offset));\n    int dims[] = {1, 1, 1};\n    cudnnDataType_t data_type;\n    cudnnTensorFormat_t tensor_format;\n    int n_dims;\n    RETURN_IF_CUDNN_ERROR(cudnnGetFilterNdDescriptor(\n        /*filterDesc=*/region_desc_handle.get(),\n        /*nbDimsRequested=*/sizeof(dims) / sizeof(dims[0]),\n        /*dataType=*/&data_type, /*format=*/&tensor_format,\n        /*nbDims=*/&n_dims, /*filterDimA=*/dims));\n    int64 size =\n        dims[0] * dims[1] * dims[2] * CudnnDataTypeToByteSize(data_type);\n    dnn::RnnDescriptor::ParamsRegion region = {reinterpret_cast<int64>(offset),\n                                               size};\n    weights->push_back(region);\n  }\n  return port::Status::OK();\n}\n\nport::StatusOr<CudnnRnnParamsDescriptor> CudnnRnnParamsDescriptor::Create(\n    const CudnnHandle& cudnn, int input_size, cudnnDataType_t data_type,\n    cudnnRNNDescriptor_t rnn_desc, cudnnRNNMode_t rnn_mode,\n    cudnnDirectionMode_t direction_mode, int num_layers) {\n  // Query the params size.\n  TensorDescriptor input_desc = CreateTensorDescriptor();\n  int tensor_dims[] = {1, input_size, 1};\n  int strides[] = {tensor_dims[1] * tensor_dims[2], tensor_dims[2], 1};\n  RETURN_IF_CUDNN_ERROR(cudnnSetTensorNdDescriptor(\n      /*tensorDesc=*/input_desc.get(), /*dataType=*/data_type,\n      /*nbDims=*/sizeof(tensor_dims) / sizeof(tensor_dims[0]),\n      /*dimA=*/tensor_dims,\n      /*strideA=*/strides));\n\n  size_t params_size = 0;\n  RETURN_IF_CUDNN_ERROR(cudnnGetRNNParamsSize(\n      /*handle=*/cudnn.handle(), /*rnnDesc=*/rnn_desc,\n      /*xDesc=*/input_desc.get(), /*sizeInBytes=*/&params_size,\n      /*dataType=*/data_type));\n  int64 params_size_in_bytes = static_cast<int64>(params_size);\n\n  FilterDescriptor filter_desc = CreateFilterDescriptor();\n  int filter_dims[] = {static_cast<int>(params_size_in_bytes), 1, 1};\n  RETURN_IF_CUDNN_ERROR(cudnnSetFilterNdDescriptor(\n      /*filterDesc=*/filter_desc.get(), /*dataType=*/data_type,\n      /*format=*/CUDNN_TENSOR_NCHW,\n      /*nbDims=*/sizeof(filter_dims) / sizeof(filter_dims[0]),\n      /*filterDimA=*/filter_dims));\n\n  // Create the weights and biases into the params buffer\n  int region_count_per_layer = [&] {\n    switch (rnn_mode) {\n      case CUDNN_RNN_RELU:\n      case CUDNN_RNN_TANH:\n        return 2;\n      case CUDNN_LSTM:\n        return 8;\n      case CUDNN_GRU:\n        return 6;\n      default:\n        LOG(FATAL) << \"Invalid RNN Mode: \" << static_cast<int>(rnn_mode);\n        return 0;\n    }\n  }();\n\n  FilterDescriptor region_desc_handle = CreateFilterDescriptor();\n  const int layer_count =\n      direction_mode == CUDNN_UNIDIRECTIONAL ? num_layers : 2 * num_layers;\n\n  ParamsRegions weights;\n  ParamsRegions biases;\n\n  for (int layer = 0; layer < layer_count; layer++) {\n    for (int region = 0; region < region_count_per_layer; region++) {\n      for (int type = 0; type < 2; type++) {\n        void* offset = nullptr;\n        RETURN_IF_CUDNN_ERROR(\n            type == 0 ? cudnnGetRNNLinLayerMatrixParams(\n                            /*handle=*/cudnn.handle(), /*rnnDesc=*/rnn_desc,\n                            /*layer=*/layer, /*xDesc=*/input_desc.get(),\n                            /*wDesc=*/filter_desc.get(),\n                            /*w=*/nullptr, /*linLayerID=*/region,\n                            /*linLayerMatDesc=*/region_desc_handle.get(),\n                            /*linLayerMat or linLayerBias=*/&offset)\n                      : cudnnGetRNNLinLayerBiasParams(\n                            /*handle=*/cudnn.handle(), /*rnnDesc=*/rnn_desc,\n                            /*layer=*/layer, /*xDesc=*/input_desc.get(),\n                            /*wDesc=*/filter_desc.get(),\n                            /*w=*/nullptr, /*linLayerID=*/region,\n                            /*linLayerMatDesc=*/region_desc_handle.get(),\n                            /*linLayerMat or linLayerBias=*/&offset));\n        int dims[] = {1, 1, 1};\n        cudnnDataType_t data_type;\n        cudnnTensorFormat_t tensor_format;\n        int n_dims;\n        RETURN_IF_CUDNN_ERROR(cudnnGetFilterNdDescriptor(\n            /*filterDesc=*/region_desc_handle.get(),\n            /*nbDimsRequested=*/sizeof(dims) / sizeof(dims[0]),\n            /*dataType=*/&data_type, /*format=*/&tensor_format,\n            /*nbDims=*/&n_dims, /*filterDimA=*/dims));\n        int64 size =\n            dims[0] * dims[1] * dims[2] * CudnnDataTypeToByteSize(data_type);\n        dnn::RnnDescriptor::ParamsRegion region = {\n            reinterpret_cast<int64>(offset), size};\n        (type == 0 ? weights : biases).push_back(region);\n      }\n    }\n    TF_RETURN_IF_ERROR(CheckAndFetchProjectionWeights(\n        cudnn, rnn_desc, layer, input_desc, filter_desc, region_desc_handle,\n        &weights));\n  }\n\n  return CudnnRnnParamsDescriptor(std::move(filter_desc), params_size_in_bytes,\n                                  weights, biases);\n}\n\n}  // namespace\n\nclass CudnnRnnSequenceTensorDescriptor\n    : public dnn::RnnSequenceTensorDescriptor {\n  CudnnRnnSequenceTensorDescriptor(GpuExecutor* parent, int max_seq_length,\n                                   int batch_size, int data_size,\n                                   cudnnDataType_t data_type,\n                                   RNNDataDescriptor data_handle,\n                                   TensorDescriptor handle)\n      : max_seq_length_(max_seq_length),\n        batch_size_(batch_size),\n        data_size_(data_size),\n        data_type_(data_type),\n        handle_(std::move(handle)),\n        rnn_data_handle_(std::move(data_handle)),\n        handles_(max_seq_length, handle_.get()) {\n  }\n\n public:\n  CudnnRnnSequenceTensorDescriptor(CudnnRnnSequenceTensorDescriptor&&) =\n      default;\n\n  static port::StatusOr<CudnnRnnSequenceTensorDescriptor> Create(\n      GpuExecutor* parent, int max_seq_length, int batch_size, int data_size,\n      cudnnDataType_t data_type) {\n    CHECK_GT(max_seq_length, 0);\n    int dims[] = {batch_size, data_size, 1};\n    int strides[] = {dims[1] * dims[2], dims[2], 1};\n    TensorDescriptor tensor_desc = CreateTensorDescriptor();\n    RETURN_IF_CUDNN_ERROR(cudnnSetTensorNdDescriptor(\n        /*tensorDesc=*/tensor_desc.get(), /*dataType=*/data_type,\n        /*nbDims=*/sizeof(dims) / sizeof(dims[0]), /*dimA=*/dims,\n        /*strideA=*/strides));\n    return CudnnRnnSequenceTensorDescriptor(parent, max_seq_length, batch_size,\n                                            data_size, data_type,\n                                            nullptr,\n                                            std::move(tensor_desc));\n  }\n\n  static port::StatusOr<CudnnRnnSequenceTensorDescriptor> Create(\n      GpuExecutor* parent, int max_seq_length, int batch_size, int data_size,\n      const absl::Span<const int>& seq_lengths, bool time_major,\n      cudnnDataType_t data_type) {\n    CHECK_GT(max_seq_length, 0);\n    int dims[] = {batch_size, data_size, 1};\n    int strides[] = {dims[1] * dims[2], dims[2], 1};\n    TensorDescriptor tensor_desc = CreateTensorDescriptor();\n    RETURN_IF_CUDNN_ERROR(cudnnSetTensorNdDescriptor(\n        /*tensorDesc=*/tensor_desc.get(), /*dataType=*/data_type,\n        /*nbDims=*/sizeof(dims) / sizeof(dims[0]), /*dimA=*/dims,\n        /*strideA=*/strides));\n    const int* seq_lengths_array = seq_lengths.data();\n    RNNDataDescriptor data_desc = CreateRNNDataDescriptor();\n    float padding_fill = 0.0f;\n    cudnnRNNDataLayout_t layout;\n    if (time_major) {\n      layout = CUDNN_RNN_DATA_LAYOUT_SEQ_MAJOR_UNPACKED;\n    } else {\n      layout = CUDNN_RNN_DATA_LAYOUT_BATCH_MAJOR_UNPACKED;\n    }\n    RETURN_IF_CUDNN_ERROR(cudnnSetRNNDataDescriptor(\n        /*RNNDataDesc=*/data_desc.get(), /*dataType*/ data_type,\n        /*layout=*/layout,\n        /*maxSeqLength=*/max_seq_length,\n        /*batchSize=*/batch_size, /*vectorSize=*/data_size,\n        /*seqLengthArray=*/seq_lengths_array,\n        /*paddingFill*/ (void*)&padding_fill));\n    return CudnnRnnSequenceTensorDescriptor(\n        parent, max_seq_length, batch_size, data_size, data_type,\n        std::move(data_desc), std::move(tensor_desc));\n  }\n\n  const cudnnTensorDescriptor_t* handles() const { return handles_.data(); }\n  const cudnnRNNDataDescriptor_t data_handle() const {\n    return rnn_data_handle_.get();\n  }\n\n  int max_seq_length() const { return max_seq_length_; }\n  int batch_size() const { return batch_size_; }\n  int data_size() const { return data_size_; }\n  bool is_var_seq_lengths() const {\n    return rnn_data_handle_ != nullptr;\n  }\n\n private:\n  int max_seq_length_;\n  int batch_size_;\n  int data_size_;\n  cudnnDataType_t data_type_;\n  TensorDescriptor handle_;\n  RNNDataDescriptor rnn_data_handle_;\n  std::vector<cudnnTensorDescriptor_t> handles_;  // Copies of handle_.\n  SE_DISALLOW_COPY_AND_ASSIGN(CudnnRnnSequenceTensorDescriptor);\n};\n\nclass CudnnRnnStateTensorDescriptor : public dnn::RnnStateTensorDescriptor {\n public:\n  CudnnRnnStateTensorDescriptor(GpuExecutor* parent, int num_layers,\n                                int batch_size, int data_size,\n                                cudnnDataType_t data_type)\n      : handle_(CreateTensorDescriptor()),\n        num_layers_(num_layers),\n        batch_size_(batch_size),\n        data_size_(data_size),\n        data_type_(data_type) {\n    int dims[] = {num_layers, batch_size, data_size};\n    int strides[] = {dims[1] * dims[2], dims[2], 1};\n    CHECK_CUDNN_OK(cudnnSetTensorNdDescriptor(\n        /*tensorDesc=*/handle_.get(), /*dataType=*/data_type,\n        /*nbDims=*/sizeof(dims) / sizeof(dims[0]), /*dimA=*/dims,\n        /*strideA=*/strides));\n  }\n\n  cudnnTensorDescriptor_t handle() const { return handle_.get(); }\n\n  int num_layers() const { return num_layers_; }\n  int batch_size() const { return batch_size_; }\n  int data_size() const { return data_size_; }\n\n private:\n  TensorDescriptor handle_;\n  int num_layers_;\n  int batch_size_;\n  int data_size_;\n  cudnnDataType_t data_type_;\n  SE_DISALLOW_COPY_AND_ASSIGN(CudnnRnnStateTensorDescriptor);\n};\n\nnamespace {\n\nstruct RnnModelDims {\n  int num_layers = 0;\n  int batch_size = 0;\n  int max_seq_length = 0;\n  int hidden_size = 0;\n  int input_size = 0;\n  int cell_size = 0;\n  int dir_count = 0;\n};\n\ntemplate <class T>\nport::StatusOr<RnnModelDims> ExtractAndCheckRnnForward(\n    const CudnnRnnDescriptor& rnn_desc,\n    const CudnnRnnSequenceTensorDescriptor& input_desc,\n    const DeviceMemory<T>& input_data,\n    const CudnnRnnStateTensorDescriptor& input_h_desc,\n    const DeviceMemory<T>& input_h_data,\n    const CudnnRnnStateTensorDescriptor& input_c_desc,\n    const DeviceMemory<T>& input_c_data, const DeviceMemory<T>& params,\n    const CudnnRnnSequenceTensorDescriptor& output_desc,\n    const DeviceMemory<T>& output_data,\n    const CudnnRnnStateTensorDescriptor& output_h_desc,\n    const DeviceMemory<T>& output_h_data,\n    const CudnnRnnStateTensorDescriptor& output_c_desc,\n    const DeviceMemory<T>& output_c_data) {\n  // extract model parameters\n  RnnModelDims model_dims;\n  model_dims.num_layers = rnn_desc.num_layers();\n  model_dims.batch_size = input_desc.batch_size();\n  model_dims.max_seq_length = input_desc.max_seq_length();\n  model_dims.hidden_size = rnn_desc.hidden_size();\n  model_dims.input_size = input_desc.data_size();\n  model_dims.cell_size = rnn_desc.cell_size();\n  model_dims.dir_count =\n      (rnn_desc.direction_mode() == CUDNN_BIDIRECTIONAL) ? 2 : 1;\n\n  // check parameters\n  if (!(input_h_desc.num_layers() ==\n            model_dims.num_layers * model_dims.dir_count &&\n        input_h_desc.batch_size() == model_dims.batch_size &&\n        input_h_desc.data_size() == model_dims.hidden_size)) {\n    return port::Status(port::error::INVALID_ARGUMENT, \"Invalid input_h shape\");\n  }\n  // The LSTM projection will be used if input_h_desc.data_size() <\n  // input_c_desc.data_size()\n  if (!(input_h_desc.num_layers() == input_c_desc.num_layers() &&\n        input_h_desc.batch_size() == input_c_desc.batch_size() &&\n        input_h_desc.data_size() <= input_c_desc.data_size())) {\n    return port::Status(port::error::INVALID_ARGUMENT, \"Invalid input_c shape\");\n  }\n  if (!(output_desc.max_seq_length() == model_dims.max_seq_length &&\n        output_desc.batch_size() == model_dims.batch_size &&\n        output_desc.data_size() ==\n            model_dims.hidden_size * model_dims.dir_count)) {\n    return port::Status(port::error::INVALID_ARGUMENT, \"Invalid output shape\");\n  }\n  if (!(input_h_desc.num_layers() == output_h_desc.num_layers() &&\n        input_h_desc.batch_size() == output_h_desc.batch_size() &&\n        input_h_desc.data_size() == output_h_desc.data_size())) {\n    return port::Status(port::error::INVALID_ARGUMENT,\n                        \"Invalid output_h shape\");\n  }\n  if (!(input_h_desc.num_layers() == output_c_desc.num_layers() &&\n        input_h_desc.batch_size() == output_c_desc.batch_size() &&\n        input_h_desc.data_size() <= output_c_desc.data_size())) {\n    return port::Status(port::error::INVALID_ARGUMENT,\n                        \"Invalid output_c shape\");\n  }\n\n  return model_dims;\n}\n\nport::Status CheckRNNParameterSize(\n    const CudnnHandle& cudnn, const CudnnRnnDescriptor& rnn_desc,\n    const CudnnRnnSequenceTensorDescriptor& input_desc) {\n  size_t params_size_in_bytes = 0;\n  RETURN_IF_CUDNN_ERROR(cudnnGetRNNParamsSize(\n      /*handle=*/cudnn.handle(), /*rnnDesc=*/rnn_desc.handle(),\n      /*xDesc=*/input_desc.handles()[0], /*sizeInBytes=*/&params_size_in_bytes,\n      /*dataType=*/rnn_desc.data_type()));\n  if (static_cast<int64>(params_size_in_bytes) !=\n      rnn_desc.ParamsSizeInBytes()) {\n    return port::Status(port::error::INVALID_ARGUMENT,\n                        \"Mismatching RNN parameter size\");\n  }\n  return port::Status::OK();\n}\n\nport::StatusOr<DeviceMemory<uint8>> CreateRnnWorkspace(\n    Stream* stream, const CudnnHandle& cudnn,\n    const CudnnRnnDescriptor& rnn_desc,\n    const CudnnRnnSequenceTensorDescriptor& input_desc,\n    ScratchAllocator* workspace_allocator) {\n  // Query the workspace size.\n  size_t workspace_size_in_bytes = 0;\n  RETURN_IF_CUDNN_ERROR(cudnnGetRNNWorkspaceSize(\n      /*handle=*/cudnn.handle(), /*rnnDesc=*/rnn_desc.handle(),\n      /*seqLength=*/input_desc.max_seq_length(), /*xDesc=*/input_desc.handles(),\n      /*sizeInBytes=*/&workspace_size_in_bytes));\n  // Allocate the workspace.\n  if (workspace_size_in_bytes == 0) {\n    return DeviceMemory<uint8>();\n  }\n  return workspace_allocator->AllocateBytes(workspace_size_in_bytes);\n}\n\n#if CUDNN_VERSION >= 7402\nport::StatusOr<DeviceMemory<uint8>> CreateBatchNormForwardWorkspace(\n    Stream* stream, const CudnnHandle& cudnn, const cudnnBatchNormMode_t& mode,\n    const cudnnBatchNormOps_t& bn_ops,\n    const cudnnActivationDescriptor_t& activation_desc,\n    const CudnnTensorDescriptor& x_descriptor,\n    const CudnnTensorDescriptor& scale_offset_descriptor,\n    ScratchAllocator* workspace_allocator) {\n  // Query the workspace size.\n  size_t workspace_size_in_bytes = 0;\n  RETURN_IF_CUDNN_ERROR(\n      cudnnGetBatchNormalizationForwardTrainingExWorkspaceSize(\n          /*handle=*/cudnn.handle(), /*mode=*/mode, /*bnOps=*/bn_ops,\n          /*xDesc=*/x_descriptor.handle(), /*zDesc=*/x_descriptor.handle(),\n          /*yDesc=*/x_descriptor.handle(),\n          /*bnScaleBiasMeanVarDesc=*/scale_offset_descriptor.handle(),\n          /*activationDesc=*/activation_desc,\n          /*sizeInBytes=*/&workspace_size_in_bytes));\n  // Allocate the workspace.\n  if (workspace_size_in_bytes == 0) {\n    return DeviceMemory<uint8>();\n  }\n  return workspace_allocator->AllocateBytes(workspace_size_in_bytes);\n}\n\nport::StatusOr<DeviceMemory<uint8>> CreateBatchNormBackwardWorkspace(\n    Stream* stream, const CudnnHandle& cudnn, const cudnnBatchNormMode_t& mode,\n    const cudnnBatchNormOps_t& bn_ops,\n    const CudnnTensorDescriptor& x_descriptor,\n    const CudnnTensorDescriptor& scale_offset_descriptor,\n    ScratchAllocator* workspace_allocator) {\n  // Query the workspace size.\n  size_t workspace_size_in_bytes = 0;\n  RETURN_IF_CUDNN_ERROR(cudnnGetBatchNormalizationBackwardExWorkspaceSize(\n      /*handle=*/cudnn.handle(), /*mode=*/mode, /*bnOps=*/bn_ops,\n      /*xDesc=*/x_descriptor.handle(),\n      /*yDesc=*/x_descriptor.handle(),\n      /*dyDesc=*/x_descriptor.handle(),\n      /*dzDesc=*/nullptr,\n      /*dxDesc=*/x_descriptor.handle(),\n      /*dBnScaleBiasDesc=*/scale_offset_descriptor.handle(),\n      /*activationDesc=*/nullptr, /*sizeInBytes=*/&workspace_size_in_bytes));\n  // Allocate the workspace.\n  if (workspace_size_in_bytes == 0) {\n    return DeviceMemory<uint8>();\n  }\n  return workspace_allocator->AllocateBytes(workspace_size_in_bytes);\n}\n\n#endif\n\n}  // namespace\n\ntemplate <class T>\nport::Status CudnnSupport::DoRnnForwardImpl(\n    Stream* stream, const CudnnRnnDescriptor& rnn_desc,\n    const CudnnRnnSequenceTensorDescriptor& input_desc,\n    const DeviceMemory<T>& input_data,\n    const CudnnRnnStateTensorDescriptor& input_h_desc,\n    const DeviceMemory<T>& input_h_data,\n    const CudnnRnnStateTensorDescriptor& input_c_desc,\n    const DeviceMemory<T>& input_c_data, const DeviceMemory<T>& params,\n    const CudnnRnnSequenceTensorDescriptor& output_desc,\n    DeviceMemory<T>* output_data,\n    const CudnnRnnStateTensorDescriptor& output_h_desc,\n    DeviceMemory<T>* output_h_data,\n    const CudnnRnnStateTensorDescriptor& output_c_desc,\n    DeviceMemory<T>* output_c_data, bool is_training,\n    ScratchAllocator* reserve_space_allocator,\n    ScratchAllocator* workspace_allocator,\n    dnn::ProfileResult* output_profile_result) {\n  SE_ASSIGN_OR_RETURN(\n      RnnModelDims model_dims,\n      ExtractAndCheckRnnForward(\n          rnn_desc, input_desc, input_data, input_h_desc, input_h_data,\n          input_c_desc, input_c_data, params, output_desc, *output_data,\n          output_h_desc, *output_h_data, output_c_desc, *output_c_data));\n\n  auto cudnn = cudnn_->GetHandle(parent_, stream);\n\n  SE_RETURN_IF_ERROR(CheckRNNParameterSize(cudnn, rnn_desc, input_desc));\n  SE_ASSIGN_OR_RETURN(DeviceMemory<uint8> workspace,\n                      CreateRnnWorkspace(stream, cudnn, rnn_desc, input_desc,\n                                         workspace_allocator))\n\n  // query the reserve space size\n  // allocate the reserve space\n  DeviceMemory<uint8> reserve_space;\n  if (is_training) {\n    size_t reserve_space_size_in_bytes = 0;\n    RETURN_IF_CUDNN_ERROR(cudnnGetRNNTrainingReserveSize(\n        /*handle=*/cudnn.handle(), /*rnnDesc=*/rnn_desc.handle(),\n        /*seqLength=*/model_dims.max_seq_length, /*xDesc=*/input_desc.handles(),\n        /*sizeInBytes=*/&reserve_space_size_in_bytes));\n\n    if (reserve_space_size_in_bytes > 0) {\n      SE_ASSIGN_OR_RETURN(reserve_space, reserve_space_allocator->AllocateBytes(\n                                             reserve_space_size_in_bytes));\n    }\n  }\n\n  std::unique_ptr<GpuTimer, GpuTimerDeleter> timer;\n  const bool is_profiling = output_profile_result != nullptr;\n  if (is_profiling) {\n    timer.reset(new GpuTimer(parent_));\n    // The start and stop of the timer should be as close to the Cudnn call as\n    // possible. It is still possible for other threads to issue workload on\n    // to this stream. So it could take multiple profiling measurements.\n    if (!timer->Init() || !timer->Start(AsGpuStream(stream))) {\n      return port::Status(port::error::INTERNAL, \"Failed to start timer\");\n    }\n  }\n\n  if (!is_training) {\n    if (input_desc.is_var_seq_lengths()) {\n      RETURN_IF_CUDNN_ERROR(cudnnRNNForwardInferenceEx(\n          /*handle=*/cudnn.handle(), /*rnnDesc=*/rnn_desc.handle(),\n          /*xDesc=*/input_desc.data_handle(), /*x=*/input_data.opaque(),\n          /*hxDesc=*/input_h_desc.handle(), /*hx=*/input_h_data.opaque(),\n          /*cxDesc=*/input_c_desc.handle(), /*cx=*/input_c_data.opaque(),\n          /*wDesc=*/rnn_desc.params_handle(), /*w=*/params.opaque(),\n          /*yDesc=*/output_desc.data_handle(),\n          /*y=*/output_data->opaque(),\n          /*hyDesc=*/output_h_desc.handle(), /*hy=*/output_h_data->opaque(),\n          /*cyDesc=*/output_c_desc.handle(), /*cy=*/output_c_data->opaque(),\n          nullptr, nullptr, nullptr, nullptr, nullptr, nullptr, nullptr,\n          nullptr,\n          /*workspace=*/workspace.opaque(),\n          /*workSpaceSizeInBytes=*/workspace.size()));\n    } else {\n      RETURN_IF_CUDNN_ERROR(cudnnRNNForwardInference(\n          /*handle=*/cudnn.handle(), /*rnnDesc=*/rnn_desc.handle(),\n          /*seqLength=*/model_dims.max_seq_length,\n          /*xDesc=*/input_desc.handles(),\n          /*x=*/input_data.opaque(), /*hxDesc=*/input_h_desc.handle(),\n          /*hx=*/input_h_data.opaque(), /*cxDesc=*/input_c_desc.handle(),\n          /*cx=*/input_c_data.opaque(), /*wDesc=*/rnn_desc.params_handle(),\n          /*w=*/params.opaque(), /*yDesc=*/output_desc.handles(),\n          /*y=*/output_data->opaque(), /*hyDesc=*/output_h_desc.handle(),\n          /*hy=*/output_h_data->opaque(), /*cyDesc=*/output_c_desc.handle(),\n          /*cy=*/output_c_data->opaque(), /*workspace=*/workspace.opaque(),\n          /*workSpaceSizeInBytes=*/workspace.size()));\n    }\n  } else {\n    if (input_desc.is_var_seq_lengths()) {\n      // cudnnSetRNNPaddingMode(rnn_desc.handle(), CUDNN_RNN_PADDED_IO_ENABLED);\n      RETURN_IF_CUDNN_ERROR(cudnnRNNForwardTrainingEx(\n          /*handle=*/cudnn.handle(), /*rnnDesc=*/rnn_desc.handle(),\n          /*xDesc=*/input_desc.data_handle(), /*x=*/input_data.opaque(),\n          /*hxDesc=*/input_h_desc.handle(), /*hx=*/input_h_data.opaque(),\n          /*cxDesc=*/input_c_desc.handle(), /*cx=*/input_c_data.opaque(),\n          /*wDesc=*/rnn_desc.params_handle(), /*w=*/params.opaque(),\n          /*yDesc=*/output_desc.data_handle(),\n          /*y=*/output_data->opaque(),\n          /*hyDesc=*/output_h_desc.handle(), /*hy=*/output_h_data->opaque(),\n          /*cyDesc=*/output_c_desc.handle(), /*cy=*/output_c_data->opaque(),\n          nullptr, nullptr, nullptr, nullptr, nullptr, nullptr, nullptr,\n          nullptr,\n          /*workspace=*/workspace.opaque(),\n          /*workSpaceSizeInBytes=*/workspace.size(),\n          /*reserveSpace=*/reserve_space.opaque(),\n          /*reserveSpaceSizeInBytes=*/reserve_space.size()));\n    } else {\n      RETURN_IF_CUDNN_ERROR(cudnnRNNForwardTraining(\n          /*handle=*/cudnn.handle(), /*rnnDesc=*/rnn_desc.handle(),\n          /*seqLength=*/model_dims.max_seq_length,\n          /*xDesc=*/input_desc.handles(),\n          /*x=*/input_data.opaque(), /*hxDesc=*/input_h_desc.handle(),\n          /*hx=*/input_h_data.opaque(), /*cxDesc=*/input_c_desc.handle(),\n          /*cx=*/input_c_data.opaque(), /*wDesc=*/rnn_desc.params_handle(),\n          /*w=*/params.opaque(), /*yDesc=*/output_desc.handles(),\n          /*y=*/output_data->opaque(), /*hyDesc=*/output_h_desc.handle(),\n          /*hy=*/output_h_data->opaque(), /*cyDesc=*/output_c_desc.handle(),\n          /*cy=*/output_c_data->opaque(), /*workspace=*/workspace.opaque(),\n          /*workSpaceSizeInBytes=*/workspace.size(),\n          /*reserveSpace=*/reserve_space.opaque(),\n          /*reserveSpaceSizeInBytes=*/reserve_space.size()));\n    }\n  }\n\n  if (is_profiling) {\n    if (!timer->Stop(AsGpuStream(stream))) {\n      return port::Status(port::error::INTERNAL, \"Failed to stop timer\");\n    }\n    auto algo_desc = *rnn_desc.algorithm_config().algorithm();\n    output_profile_result->set_algorithm(algo_desc);\n    output_profile_result->set_elapsed_time_in_ms(\n        timer->GetElapsedMilliseconds());\n  }\n\n  return port::Status::OK();\n}\n\ntemplate <class T>\nport::Status CudnnSupport::DoRnnBackwardImpl(\n    Stream* stream, const CudnnRnnDescriptor& rnn_desc,\n    const CudnnRnnSequenceTensorDescriptor& input_desc,\n    const DeviceMemory<T>& input_data,\n    const CudnnRnnStateTensorDescriptor& input_h_desc,\n    const DeviceMemory<T>& input_h_data,\n    const CudnnRnnStateTensorDescriptor& input_c_desc,\n    const DeviceMemory<T>& input_c_data, const DeviceMemory<T>& params,\n    const CudnnRnnSequenceTensorDescriptor& output_desc,\n    const DeviceMemory<T>& output_data,\n    const CudnnRnnStateTensorDescriptor& output_h_desc,\n    const DeviceMemory<T>& output_h_data,\n    const CudnnRnnStateTensorDescriptor& output_c_desc,\n    const DeviceMemory<T>& output_c_data,\n    const DeviceMemory<T>& output_backprop_data,\n    const DeviceMemory<T>& output_h_backprop_data,\n    const DeviceMemory<T>& output_c_backprop_data,\n    DeviceMemory<T>* input_backprop_data,\n    DeviceMemory<T>* input_h_backprop_data,\n    DeviceMemory<T>* input_c_backprop_data,\n    DeviceMemory<T>* params_backprop_data,\n    DeviceMemory<uint8>* reserve_space_data,\n    ScratchAllocator* workspace_allocator,\n    dnn::ProfileResult* output_profile_result) {\n  SE_ASSIGN_OR_RETURN(\n      RnnModelDims model_dims,\n      ExtractAndCheckRnnForward(rnn_desc, input_desc, input_data, input_h_desc,\n                                input_h_data, input_c_desc, input_c_data,\n                                params, output_desc, output_data, output_h_desc,\n                                output_h_data, output_c_desc, output_c_data));\n\n  auto cudnn = cudnn_->GetHandle(parent_, stream);\n\n  SE_RETURN_IF_ERROR(CheckRNNParameterSize(cudnn, rnn_desc, input_desc));\n  SE_ASSIGN_OR_RETURN(DeviceMemory<uint8> workspace,\n                      CreateRnnWorkspace(stream, cudnn, rnn_desc, input_desc,\n                                         workspace_allocator));\n\n  std::unique_ptr<GpuTimer, GpuTimerDeleter> timer;\n  const bool is_profiling = output_profile_result != nullptr;\n  if (is_profiling) {\n    timer.reset(new GpuTimer(parent_));\n    // The start and stop of the timer should be as close to the Cudnn call as\n    // possible. It is still possible for other threads to issue workload on\n    // to this stream. So it could take multiple profiling measurements.\n    if (!timer->Init() || !timer->Start(AsGpuStream(stream))) {\n      return port::Status(port::error::INTERNAL, \"Failed to start timer\");\n    }\n  }\n\n  if (input_desc.is_var_seq_lengths()) {\n    RETURN_IF_CUDNN_ERROR(cudnnRNNBackwardDataEx(\n        /*handle=*/cudnn.handle(), /*rnnDesc=*/rnn_desc.handle(),\n        /*yDesc=*/output_desc.data_handle(), /*y=*/output_data.opaque(),\n        /*dyDesc=*/output_desc.data_handle(),\n        /*dy=*/output_backprop_data.opaque(), nullptr, nullptr,\n        /*dhyDesc=*/output_h_desc.handle(),\n        /*dhy=*/output_h_backprop_data.opaque(),\n        /*dcyDesc=*/output_c_desc.handle(),\n        /*dcy=*/output_c_backprop_data.opaque(),\n        /*wDesc=*/rnn_desc.params_handle(), /*w=*/params.opaque(),\n        /*hxDesc=*/input_h_desc.handle(), /*hx=*/input_h_data.opaque(),\n        /*cxDesc=*/input_c_desc.handle(), /*cx=*/input_c_data.opaque(),\n        /*dxDesc=*/input_desc.data_handle(),\n        /*dx=*/input_backprop_data->opaque(),\n        /*dhxDesc=*/input_h_desc.handle(),\n        /*dhx=*/input_h_backprop_data->opaque(),\n        /*dcxDesc=*/input_c_desc.handle(),\n        /*dcx=*/input_c_backprop_data->opaque(), nullptr, nullptr,\n        /*workspace=*/workspace.opaque(),\n        /*workSpaceSizeInBytes=*/workspace.size(),\n        /*reserveSpace=*/reserve_space_data->opaque(),\n        /*reserveSpaceSizeInBytes=*/reserve_space_data->size()));\n  } else {\n    RETURN_IF_CUDNN_ERROR(cudnnRNNBackwardData(\n        /*handle=*/cudnn.handle(), /*rnnDesc=*/rnn_desc.handle(),\n        /*seqLength=*/model_dims.max_seq_length,\n        /*yDesc=*/output_desc.handles(),\n        /*y=*/output_data.opaque(), /*dyDesc=*/output_desc.handles(),\n        /*dy=*/output_backprop_data.opaque(),\n        /*dhyDesc=*/output_h_desc.handle(),\n        /*dhy=*/output_h_backprop_data.opaque(),\n        /*dcyDesc=*/output_c_desc.handle(),\n        /*dcy=*/output_c_backprop_data.opaque(),\n        /*wDesc=*/rnn_desc.params_handle(), /*w=*/params.opaque(),\n        /*hxDesc=*/input_h_desc.handle(), /*hx=*/input_h_data.opaque(),\n        /*cxDesc=*/input_c_desc.handle(), /*cx=*/input_c_data.opaque(),\n        /*dxDesc=*/input_desc.handles(), /*dx=*/input_backprop_data->opaque(),\n        /*dhxDesc=*/input_h_desc.handle(),\n        /*dhx=*/input_h_backprop_data->opaque(),\n        /*dcxDesc=*/input_c_desc.handle(),\n        /*dcx=*/input_c_backprop_data->opaque(),\n        /*workspace=*/workspace.opaque(),\n        /*workSpaceSizeInBytes=*/workspace.size(),\n        /*reserveSpace=*/reserve_space_data->opaque(),\n        /*reserveSpaceSizeInBytes=*/reserve_space_data->size()));\n  }\n\n  if (params_backprop_data != nullptr) {\n    // Clear the dw to zeros.\n    stream->ThenMemZero(params_backprop_data, params_backprop_data->size());\n    if (input_desc.is_var_seq_lengths()) {\n      RETURN_IF_CUDNN_ERROR(cudnnRNNBackwardWeightsEx(\n          /*handle=*/cudnn.handle(), /*rnnDesc=*/rnn_desc.handle(),\n          /*xDesc=*/input_desc.data_handle(), /*x=*/input_data.opaque(),\n          /*hxDesc=*/input_h_desc.handle(), /*hx=*/input_h_data.opaque(),\n          /*yDesc=*/output_desc.data_handle(),\n          /*y=*/output_data.opaque(),\n          /*workspace=*/workspace.opaque(),\n          /*workSpaceSizeInBytes=*/workspace.size(),\n          /*dwDesc=*/rnn_desc.params_handle(),\n          /*dw=*/params_backprop_data->opaque(),\n          /*reserveSpace=*/reserve_space_data->opaque(),\n          /*reserveSpaceSizeInBytes=*/reserve_space_data->size()));\n    } else {\n      // make the backward weight call\n      RETURN_IF_CUDNN_ERROR(cudnnRNNBackwardWeights(\n          /*handle=*/cudnn.handle(), /*rnnDesc=*/rnn_desc.handle(),\n          /*seqLength=*/model_dims.max_seq_length,\n          /*xDesc=*/input_desc.handles(),\n          /*x=*/input_data.opaque(), /*hxDesc=*/input_h_desc.handle(),\n          /*hx=*/input_h_data.opaque(), /*yDesc=*/output_desc.handles(),\n          /*y=*/output_data.opaque(), /*workspace=*/workspace.opaque(),\n          /*workSpaceSizeInBytes=*/workspace.size(),\n          /*dwDesc=*/rnn_desc.params_handle(),\n          /*dw=*/params_backprop_data->opaque(),\n          /*reserveSpace=*/reserve_space_data->opaque(),\n          /*reserveSpaceSizeInBytes=*/reserve_space_data->size()));\n    }\n  }\n\n  if (is_profiling) {\n    if (!timer->Stop(AsGpuStream(stream))) {\n      return port::Status(port::error::INTERNAL, \"Failed to stop timer\");\n    }\n    auto algo_desc = *rnn_desc.algorithm_config().algorithm();\n    output_profile_result->set_algorithm(algo_desc);\n    output_profile_result->set_elapsed_time_in_ms(\n        timer->GetElapsedMilliseconds());\n  }\n\n  return port::Status::OK();\n}\n\nport::Status CudnnSupport::DoCtcLossImpl(\n    Stream* stream, const CudnnRnnStateTensorDescriptor& probs_desc,\n    const DeviceMemoryBase probs_data, absl::Span<const int> labels_data,\n    absl::Span<const int> labels_lengths_data,\n    absl::Span<const int> input_lengths_data, DeviceMemoryBase costs_data,\n    const CudnnRnnStateTensorDescriptor& grads_desc,\n    DeviceMemoryBase grads_data, const CudnnCtcLossDescriptor& ctc_loss_desc,\n    DeviceMemory<uint8> scratch_memory, int ctc_loss_algo_id) {\n  auto cudnn = cudnn_->GetHandle(parent_, stream);\n\n  int kNumTimestamps = probs_desc.num_layers();\n  int kBatchSize = probs_desc.batch_size();\n  int kNumLabels = probs_desc.data_size();\n  int total_size = kNumLabels * kNumTimestamps * kBatchSize;\n  (void)total_size;\n\n#if CUDNN_VERSION >= 7603\n  cudnnCTCLossAlgo_t ctc_loss_algo =\n      static_cast<cudnnCTCLossAlgo_t>(ctc_loss_algo_id);\n  RETURN_IF_CUDNN_ERROR(cudnnCTCLoss(\n      /*handle=*/cudnn.handle(), /*probsDesc=*/probs_desc.handle(),\n      /*probs=*/probs_data.opaque(), /*labels=*/labels_data.data(),\n      /*labelLengths=*/labels_lengths_data.data(),\n      /*inputLengths=*/input_lengths_data.data(),\n      /*costs=*/costs_data.opaque(), /*gradientsDesc=*/grads_desc.handle(),\n      /*gradients=*/grads_data.opaque(),\n      /*algo=*/ctc_loss_algo,\n      /*ctcLossDesc=*/ctc_loss_desc.handle(),\n      /*workspace=*/scratch_memory.opaque(),\n      /*workSpaceSizeInBytes=*/scratch_memory.size()));\n#else\n  return port::Status(port::error::INVALID_ARGUMENT,\n                      \"No supported cudnnCTCLoss when \"\n                      \"CUDNN_VERSION < 7.6.3\");\n#endif\n\n  return port::Status::OK();\n}\n\nport::StatusOr<std::unique_ptr<dnn::RnnDescriptor>>\nCudnnSupport::createRnnDescriptor(\n    int num_layers, int hidden_size, int input_size, int cell_size,\n    int batch_size, dnn::RnnInputMode input_mode,\n    dnn::RnnDirectionMode direction_mode, dnn::RnnMode rnn_mode,\n    dnn::DataType data_type, const dnn::AlgorithmConfig& algorithm_config,\n    float dropout, uint64 seed, ScratchAllocator* state_allocator,\n    bool use_padded_io) {\n  // Setting up a cudnnRNNDescriptor requires a cuDNN handle, but because it's\n  // not enqueueing anything into a stream, we pass in the null stream.\n  auto cudnn = cudnn_->GetHandle(parent_, /*stream=*/nullptr);\n  SE_ASSIGN_OR_RETURN(\n      CudnnRnnDescriptor rnn_desc,\n      CudnnRnnDescriptor::Create(\n          cudnn, num_layers, hidden_size, input_size, cell_size, batch_size,\n          ToCudnnRnnInputMode(input_mode),\n          ToCudnnRnnDirectionMode(direction_mode), ToCudnnRnnMode(rnn_mode),\n          ToCudnnDataType(data_type), GetRnnComputeType(data_type),\n          algorithm_config, dropout, seed, state_allocator, use_padded_io));\n  return std::unique_ptr<dnn::RnnDescriptor>(\n      new CudnnRnnDescriptor(std::move(rnn_desc)));\n}\n\nport::StatusOr<std::unique_ptr<dnn::RnnSequenceTensorDescriptor>>\nCudnnSupport::createRnnSequenceTensorDescriptor(int max_seq_length,\n                                                int batch_size, int data_size,\n                                                dnn::DataType data_type) {\n  SE_ASSIGN_OR_RETURN(CudnnRnnSequenceTensorDescriptor descriptor,\n                      CudnnRnnSequenceTensorDescriptor::Create(\n                          parent_, max_seq_length, batch_size, data_size,\n                          ToCudnnDataType(data_type)));\n  return std::unique_ptr<dnn::RnnSequenceTensorDescriptor>(\n      new CudnnRnnSequenceTensorDescriptor(std::move(descriptor)));\n}\n\nport::StatusOr<std::unique_ptr<dnn::RnnSequenceTensorDescriptor>>\nCudnnSupport::createRnnSequenceTensorDescriptor(\n    int max_seq_length, int batch_size, int data_size,\n    const absl::Span<const int>& seq_lengths, bool time_major,\n    dnn::DataType data_type) {\n  SE_ASSIGN_OR_RETURN(CudnnRnnSequenceTensorDescriptor descriptor,\n                      CudnnRnnSequenceTensorDescriptor::Create(\n                          parent_, max_seq_length, batch_size, data_size,\n                          seq_lengths, time_major, ToCudnnDataType(data_type)));\n  return std::unique_ptr<dnn::RnnSequenceTensorDescriptor>(\n      new CudnnRnnSequenceTensorDescriptor(std::move(descriptor)));\n}\n\nport::StatusOr<std::unique_ptr<dnn::RnnStateTensorDescriptor>>\nCudnnSupport::createRnnStateTensorDescriptor(int num_layer, int batch_size,\n                                             int data_size,\n                                             dnn::DataType data_type) {\n  return std::unique_ptr<dnn::RnnStateTensorDescriptor>(\n      new CudnnRnnStateTensorDescriptor(parent_, num_layer, batch_size,\n                                        data_size, ToCudnnDataType(data_type)));\n}\n\nbool CudnnSupport::DoRnnForward(\n    Stream* stream, const dnn::RnnDescriptor& rnn_desc,\n    const dnn::RnnSequenceTensorDescriptor& input_desc,\n    const DeviceMemory<Eigen::half>& input_data,\n    const dnn::RnnStateTensorDescriptor& input_h_desc,\n    const DeviceMemory<Eigen::half>& input_h_data,\n    const dnn::RnnStateTensorDescriptor& input_c_desc,\n    const DeviceMemory<Eigen::half>& input_c_data,\n    const DeviceMemory<Eigen::half>& params,\n    const dnn::RnnSequenceTensorDescriptor& output_desc,\n    DeviceMemory<Eigen::half>* output_data,\n    const dnn::RnnStateTensorDescriptor& output_h_desc,\n    DeviceMemory<Eigen::half>* output_h_data,\n    const dnn::RnnStateTensorDescriptor& output_c_desc,\n    DeviceMemory<Eigen::half>* output_c_data, bool is_training,\n    ScratchAllocator* reserve_space_allocator,\n    ScratchAllocator* workspace_allocator,\n    dnn::ProfileResult* output_profile_result) {\n  const CudnnRnnDescriptor& cudnn_rnn_desc =\n      static_cast<const CudnnRnnDescriptor&>(rnn_desc);\n  const CudnnRnnSequenceTensorDescriptor& cudnn_input_desc =\n      static_cast<const CudnnRnnSequenceTensorDescriptor&>(input_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_input_h_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(input_h_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_input_c_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(input_c_desc);\n  const CudnnRnnSequenceTensorDescriptor& cudnn_output_desc =\n      static_cast<const CudnnRnnSequenceTensorDescriptor&>(output_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_output_h_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(output_h_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_output_c_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(output_c_desc);\n  return IsStatusOk(\n      DoRnnForwardImpl<Eigen::half>(\n          stream, cudnn_rnn_desc, cudnn_input_desc, input_data,\n          cudnn_input_h_desc, input_h_data, cudnn_input_c_desc, input_c_data,\n          params, cudnn_output_desc, output_data, cudnn_output_h_desc,\n          output_h_data, cudnn_output_c_desc, output_c_data, is_training,\n          reserve_space_allocator, workspace_allocator, output_profile_result),\n      /*report_error=*/!output_profile_result);\n}\n\nbool CudnnSupport::DoRnnForward(\n    Stream* stream, const dnn::RnnDescriptor& rnn_desc,\n    const dnn::RnnSequenceTensorDescriptor& input_desc,\n    const DeviceMemory<float>& input_data,\n    const dnn::RnnStateTensorDescriptor& input_h_desc,\n    const DeviceMemory<float>& input_h_data,\n    const dnn::RnnStateTensorDescriptor& input_c_desc,\n    const DeviceMemory<float>& input_c_data, const DeviceMemory<float>& params,\n    const dnn::RnnSequenceTensorDescriptor& output_desc,\n    DeviceMemory<float>* output_data,\n    const dnn::RnnStateTensorDescriptor& output_h_desc,\n    DeviceMemory<float>* output_h_data,\n    const dnn::RnnStateTensorDescriptor& output_c_desc,\n    DeviceMemory<float>* output_c_data, bool is_training,\n    ScratchAllocator* reserve_space_allocator,\n    ScratchAllocator* workspace_allocator,\n    dnn::ProfileResult* output_profile_result) {\n  const CudnnRnnDescriptor& cudnn_rnn_desc =\n      static_cast<const CudnnRnnDescriptor&>(rnn_desc);\n  const CudnnRnnSequenceTensorDescriptor& cudnn_input_desc =\n      static_cast<const CudnnRnnSequenceTensorDescriptor&>(input_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_input_h_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(input_h_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_input_c_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(input_c_desc);\n  const CudnnRnnSequenceTensorDescriptor& cudnn_output_desc =\n      static_cast<const CudnnRnnSequenceTensorDescriptor&>(output_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_output_h_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(output_h_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_output_c_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(output_c_desc);\n  return IsStatusOk(\n      DoRnnForwardImpl<float>(\n          stream, cudnn_rnn_desc, cudnn_input_desc, input_data,\n          cudnn_input_h_desc, input_h_data, cudnn_input_c_desc, input_c_data,\n          params, cudnn_output_desc, output_data, cudnn_output_h_desc,\n          output_h_data, cudnn_output_c_desc, output_c_data, is_training,\n          reserve_space_allocator, workspace_allocator, output_profile_result),\n      /*report_error=*/!output_profile_result);\n}\n\nbool CudnnSupport::DoRnnForward(\n    Stream* stream, const dnn::RnnDescriptor& rnn_desc,\n    const dnn::RnnSequenceTensorDescriptor& input_desc,\n    const DeviceMemory<double>& input_data,\n    const dnn::RnnStateTensorDescriptor& input_h_desc,\n    const DeviceMemory<double>& input_h_data,\n    const dnn::RnnStateTensorDescriptor& input_c_desc,\n    const DeviceMemory<double>& input_c_data,\n    const DeviceMemory<double>& params,\n    const dnn::RnnSequenceTensorDescriptor& output_desc,\n    DeviceMemory<double>* output_data,\n    const dnn::RnnStateTensorDescriptor& output_h_desc,\n    DeviceMemory<double>* output_h_data,\n    const dnn::RnnStateTensorDescriptor& output_c_desc,\n    DeviceMemory<double>* output_c_data, bool is_training,\n    ScratchAllocator* reserve_space_allocator,\n    ScratchAllocator* workspace_allocator,\n    dnn::ProfileResult* output_profile_result) {\n  const CudnnRnnDescriptor& cudnn_rnn_desc =\n      static_cast<const CudnnRnnDescriptor&>(rnn_desc);\n  const CudnnRnnSequenceTensorDescriptor& cudnn_input_desc =\n      static_cast<const CudnnRnnSequenceTensorDescriptor&>(input_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_input_h_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(input_h_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_input_c_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(input_c_desc);\n  const CudnnRnnSequenceTensorDescriptor& cudnn_output_desc =\n      static_cast<const CudnnRnnSequenceTensorDescriptor&>(output_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_output_h_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(output_h_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_output_c_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(output_c_desc);\n  return IsStatusOk(\n      DoRnnForwardImpl<double>(\n          stream, cudnn_rnn_desc, cudnn_input_desc, input_data,\n          cudnn_input_h_desc, input_h_data, cudnn_input_c_desc, input_c_data,\n          params, cudnn_output_desc, output_data, cudnn_output_h_desc,\n          output_h_data, cudnn_output_c_desc, output_c_data, is_training,\n          reserve_space_allocator, workspace_allocator, output_profile_result),\n      /*report_error=*/!output_profile_result);\n}\n\nbool CudnnSupport::DoRnnBackward(\n    Stream* stream, const dnn::RnnDescriptor& rnn_desc,\n    const dnn::RnnSequenceTensorDescriptor& input_desc,\n    const DeviceMemory<Eigen::half>& input_data,\n    const dnn::RnnStateTensorDescriptor& input_h_desc,\n    const DeviceMemory<Eigen::half>& input_h_data,\n    const dnn::RnnStateTensorDescriptor& input_c_desc,\n    const DeviceMemory<Eigen::half>& input_c_data,\n    const DeviceMemory<Eigen::half>& params,\n    const dnn::RnnSequenceTensorDescriptor& output_desc,\n    const DeviceMemory<Eigen::half>& output_data,\n    const dnn::RnnStateTensorDescriptor& output_h_desc,\n    const DeviceMemory<Eigen::half>& output_h_data,\n    const dnn::RnnStateTensorDescriptor& output_c_desc,\n    const DeviceMemory<Eigen::half>& output_c_data,\n    const DeviceMemory<Eigen::half>& output_backprop_data,\n    const DeviceMemory<Eigen::half>& output_h_backprop_data,\n    const DeviceMemory<Eigen::half>& output_c_backprop_data,\n    DeviceMemory<Eigen::half>* input_backprop_data,\n    DeviceMemory<Eigen::half>* input_h_backprop_data,\n    DeviceMemory<Eigen::half>* input_c_backprop_data,\n    DeviceMemory<Eigen::half>* params_backprop_data,\n    DeviceMemory<uint8>* reserve_space_data,\n    ScratchAllocator* workspace_allocator,\n    dnn::ProfileResult* output_profile_result) {\n  const CudnnRnnDescriptor& cudnn_rnn_desc =\n      static_cast<const CudnnRnnDescriptor&>(rnn_desc);\n  const CudnnRnnSequenceTensorDescriptor& cudnn_input_desc =\n      static_cast<const CudnnRnnSequenceTensorDescriptor&>(input_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_input_h_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(input_h_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_input_c_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(input_c_desc);\n  const CudnnRnnSequenceTensorDescriptor& cudnn_output_desc =\n      static_cast<const CudnnRnnSequenceTensorDescriptor&>(output_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_output_h_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(output_h_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_output_c_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(output_c_desc);\n  return IsStatusOk(\n      DoRnnBackwardImpl<Eigen::half>(\n          stream, cudnn_rnn_desc, cudnn_input_desc, input_data,\n          cudnn_input_h_desc, input_h_data, cudnn_input_c_desc, input_c_data,\n          params, cudnn_output_desc, output_data, cudnn_output_h_desc,\n          output_h_data, cudnn_output_c_desc, output_c_data,\n          output_backprop_data, output_h_backprop_data, output_c_backprop_data,\n          input_backprop_data, input_h_backprop_data, input_c_backprop_data,\n          params_backprop_data, reserve_space_data, workspace_allocator,\n          output_profile_result),\n      /*report_error=*/!output_profile_result);\n}\n\nbool CudnnSupport::DoRnnBackward(\n    Stream* stream, const dnn::RnnDescriptor& rnn_desc,\n    const dnn::RnnSequenceTensorDescriptor& input_desc,\n    const DeviceMemory<float>& input_data,\n    const dnn::RnnStateTensorDescriptor& input_h_desc,\n    const DeviceMemory<float>& input_h_data,\n    const dnn::RnnStateTensorDescriptor& input_c_desc,\n    const DeviceMemory<float>& input_c_data, const DeviceMemory<float>& params,\n    const dnn::RnnSequenceTensorDescriptor& output_desc,\n    const DeviceMemory<float>& output_data,\n    const dnn::RnnStateTensorDescriptor& output_h_desc,\n    const DeviceMemory<float>& output_h_data,\n    const dnn::RnnStateTensorDescriptor& output_c_desc,\n    const DeviceMemory<float>& output_c_data,\n    const DeviceMemory<float>& output_backprop_data,\n    const DeviceMemory<float>& output_h_backprop_data,\n    const DeviceMemory<float>& output_c_backprop_data,\n    DeviceMemory<float>* input_backprop_data,\n    DeviceMemory<float>* input_h_backprop_data,\n    DeviceMemory<float>* input_c_backprop_data,\n    DeviceMemory<float>* params_backprop_data,\n    DeviceMemory<uint8>* reserve_space_data,\n    ScratchAllocator* workspace_allocator,\n    dnn::ProfileResult* output_profile_result) {\n  const CudnnRnnDescriptor& cudnn_rnn_desc =\n      static_cast<const CudnnRnnDescriptor&>(rnn_desc);\n  const CudnnRnnSequenceTensorDescriptor& cudnn_input_desc =\n      static_cast<const CudnnRnnSequenceTensorDescriptor&>(input_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_input_h_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(input_h_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_input_c_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(input_c_desc);\n  const CudnnRnnSequenceTensorDescriptor& cudnn_output_desc =\n      static_cast<const CudnnRnnSequenceTensorDescriptor&>(output_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_output_h_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(output_h_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_output_c_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(output_c_desc);\n  return IsStatusOk(\n      DoRnnBackwardImpl<float>(\n          stream, cudnn_rnn_desc, cudnn_input_desc, input_data,\n          cudnn_input_h_desc, input_h_data, cudnn_input_c_desc, input_c_data,\n          params, cudnn_output_desc, output_data, cudnn_output_h_desc,\n          output_h_data, cudnn_output_c_desc, output_c_data,\n          output_backprop_data, output_h_backprop_data, output_c_backprop_data,\n          input_backprop_data, input_h_backprop_data, input_c_backprop_data,\n          params_backprop_data, reserve_space_data, workspace_allocator,\n          output_profile_result),\n      /*report_error=*/!output_profile_result);\n}\n\nbool CudnnSupport::DoRnnBackward(\n    Stream* stream, const dnn::RnnDescriptor& rnn_desc,\n    const dnn::RnnSequenceTensorDescriptor& input_desc,\n    const DeviceMemory<double>& input_data,\n    const dnn::RnnStateTensorDescriptor& input_h_desc,\n    const DeviceMemory<double>& input_h_data,\n    const dnn::RnnStateTensorDescriptor& input_c_desc,\n    const DeviceMemory<double>& input_c_data,\n    const DeviceMemory<double>& params,\n    const dnn::RnnSequenceTensorDescriptor& output_desc,\n    const DeviceMemory<double>& output_data,\n    const dnn::RnnStateTensorDescriptor& output_h_desc,\n    const DeviceMemory<double>& output_h_data,\n    const dnn::RnnStateTensorDescriptor& output_c_desc,\n    const DeviceMemory<double>& output_c_data,\n    const DeviceMemory<double>& output_backprop_data,\n    const DeviceMemory<double>& output_h_backprop_data,\n    const DeviceMemory<double>& output_c_backprop_data,\n    DeviceMemory<double>* input_backprop_data,\n    DeviceMemory<double>* input_h_backprop_data,\n    DeviceMemory<double>* input_c_backprop_data,\n    DeviceMemory<double>* params_backprop_data,\n    DeviceMemory<uint8>* reserve_space_data,\n    ScratchAllocator* workspace_allocator,\n    dnn::ProfileResult* output_profile_result) {\n  const CudnnRnnDescriptor& cudnn_rnn_desc =\n      static_cast<const CudnnRnnDescriptor&>(rnn_desc);\n  const CudnnRnnSequenceTensorDescriptor& cudnn_input_desc =\n      static_cast<const CudnnRnnSequenceTensorDescriptor&>(input_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_input_h_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(input_h_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_input_c_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(input_c_desc);\n  const CudnnRnnSequenceTensorDescriptor& cudnn_output_desc =\n      static_cast<const CudnnRnnSequenceTensorDescriptor&>(output_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_output_h_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(output_h_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_output_c_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(output_c_desc);\n  return IsStatusOk(\n      DoRnnBackwardImpl<double>(\n          stream, cudnn_rnn_desc, cudnn_input_desc, input_data,\n          cudnn_input_h_desc, input_h_data, cudnn_input_c_desc, input_c_data,\n          params, cudnn_output_desc, output_data, cudnn_output_h_desc,\n          output_h_data, cudnn_output_c_desc, output_c_data,\n          output_backprop_data, output_h_backprop_data, output_c_backprop_data,\n          input_backprop_data, input_h_backprop_data, input_c_backprop_data,\n          params_backprop_data, reserve_space_data, workspace_allocator,\n          output_profile_result),\n      /*report_error=*/!output_profile_result);\n}\n\nnamespace {\n\n// TODO(csigg): Merge a lot of duplicate code below for forward, backward data,\n// and backward filter.\n\nport::StatusOr<cudnnConvolutionFwdAlgo_t> GetCudnnConvolutionForwardAlgo(\n    const CudnnHandle& cudnn, const CudnnTensorDescriptor& input_nd,\n    const CudnnFilterDescriptor& filter, const CudnnConvolutionDescriptor& conv,\n    const CudnnTensorDescriptor& output_nd, bool specify_workspace_limit,\n    size_t memory_limit_bytes) {\n#if CUDNN_VERSION >= 8000\n  const int num_requested_algos = 5;\n  int num_returned_algos = 0;\n  cudnnConvolutionFwdAlgoPerf_t perf_results[num_requested_algos];\n\n  RETURN_IF_CUDNN_ERROR(cudnnGetConvolutionForwardAlgorithm_v7(\n      cudnn.handle(), input_nd.handle(), filter.handle(), conv.handle(),\n      output_nd.handle(), num_requested_algos, &num_returned_algos,\n      perf_results));\n\n  size_t mem_limit = specify_workspace_limit ? memory_limit_bytes : 0ULL;\n  for (int r = 0; r < num_returned_algos; r++) {\n    if (perf_results[r].status == CUDNN_STATUS_SUCCESS &&\n        perf_results[r].algo != CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED &&\n        perf_results[r].memory <= mem_limit) {\n      return perf_results[r].algo;\n    }\n  }\n  return port::Status(port::error::INTERNAL,\n                      \"cudnnGetConvolutionForwardAlgorithm_v7 returned \"\n                      \"no suitable algorithms. This could be a cudnn bug.\");\n#else\n  cudnnConvolutionFwdPreference_t preference =\n      specify_workspace_limit ? CUDNN_CONVOLUTION_FWD_SPECIFY_WORKSPACE_LIMIT\n                              : CUDNN_CONVOLUTION_FWD_NO_WORKSPACE;\n  cudnnConvolutionFwdAlgo_t algo_to_use;\n  RETURN_IF_CUDNN_ERROR(cudnnGetConvolutionForwardAlgorithm(\n      cudnn.handle(), input_nd.handle(), filter.handle(), conv.handle(),\n      output_nd.handle(), preference, memory_limit_bytes, &algo_to_use));\n  return algo_to_use;\n#endif\n}\n\nport::StatusOr<cudnnConvolutionBwdDataAlgo_t>\nGetCudnnConvolutionBackwardDataAlgo(const CudnnHandle& cudnn,\n                                    const CudnnTensorDescriptor& input_nd,\n                                    const CudnnFilterDescriptor& filter,\n                                    const CudnnConvolutionDescriptor& conv,\n                                    const CudnnTensorDescriptor& output_nd,\n                                    bool specify_workspace_limit,\n                                    size_t memory_limit_bytes) {\n#if CUDNN_VERSION >= 8000\n  const int num_requested_algos = 5;\n  int num_returned_algos = 0;\n  cudnnConvolutionBwdDataAlgoPerf_t perf_results[num_requested_algos];\n\n  RETURN_IF_CUDNN_ERROR(cudnnGetConvolutionBackwardDataAlgorithm_v7(\n      cudnn.handle(), filter.handle(), output_nd.handle(), conv.handle(),\n      input_nd.handle(), num_requested_algos, &num_returned_algos,\n      perf_results));\n\n  size_t mem_limit = specify_workspace_limit ? memory_limit_bytes : 0ULL;\n  for (int r = 0; r < num_returned_algos; r++) {\n    if (perf_results[r].status == CUDNN_STATUS_SUCCESS &&\n        perf_results[r].algo !=\n            CUDNN_CONVOLUTION_BWD_DATA_ALGO_WINOGRAD_NONFUSED &&\n        perf_results[r].memory <= mem_limit) {\n      return perf_results[r].algo;\n    }\n  }\n  return port::Status(port::error::INTERNAL,\n                      \"cudnnGetConvolutionBackwardDataAlgorithm_v7 returned \"\n                      \"no suitable algorithms. This could be a cudnn bug.\");\n#else\n  cudnnConvolutionBwdDataPreference_t preference =\n      specify_workspace_limit\n          ? CUDNN_CONVOLUTION_BWD_DATA_SPECIFY_WORKSPACE_LIMIT\n          : CUDNN_CONVOLUTION_BWD_DATA_NO_WORKSPACE;\n  cudnnConvolutionBwdDataAlgo_t algo_to_use;\n  RETURN_IF_CUDNN_ERROR(cudnnGetConvolutionBackwardDataAlgorithm(\n      cudnn.handle(), filter.handle(), output_nd.handle(), conv.handle(),\n      input_nd.handle(), preference, memory_limit_bytes, &algo_to_use));\n  return algo_to_use;\n#endif\n}\n\nport::StatusOr<cudnnConvolutionBwdFilterAlgo_t>\nGetCudnnConvolutionBackwardFilterAlgo(const CudnnHandle& cudnn,\n                                      const CudnnTensorDescriptor& input_nd,\n                                      const CudnnFilterDescriptor& filter,\n                                      const CudnnConvolutionDescriptor& conv,\n                                      const CudnnTensorDescriptor& output_nd,\n                                      bool specify_workspace_limit,\n                                      size_t memory_limit_bytes) {\n#if CUDNN_VERSION >= 8000\n  const int num_requested_algos = 5;\n  int num_returned_algos = 0;\n  cudnnConvolutionBwdFilterAlgoPerf_t perf_results[num_requested_algos];\n\n  RETURN_IF_CUDNN_ERROR(cudnnGetConvolutionBackwardFilterAlgorithm_v7(\n      cudnn.handle(), input_nd.handle(), output_nd.handle(), conv.handle(),\n      filter.handle(), num_requested_algos, &num_returned_algos, perf_results));\n\n  size_t mem_limit = specify_workspace_limit ? memory_limit_bytes : 0ULL;\n  for (int r = 0; r < num_returned_algos; r++) {\n    if (perf_results[r].status == CUDNN_STATUS_SUCCESS &&\n        perf_results[r].algo !=\n            CUDNN_CONVOLUTION_BWD_FILTER_ALGO_WINOGRAD_NONFUSED &&\n        perf_results[r].memory <= mem_limit) {\n      return perf_results[r].algo;\n    }\n  }\n  return port::Status(port::error::INTERNAL,\n                      \"cudnnGetConvolutionBackwardFilterAlgorithm_v7 returned \"\n                      \"no suitable algorithms. This could be a cudnn bug.\");\n#else\n  cudnnConvolutionBwdFilterPreference_t preference =\n      specify_workspace_limit\n          ? CUDNN_CONVOLUTION_BWD_FILTER_SPECIFY_WORKSPACE_LIMIT\n          : CUDNN_CONVOLUTION_BWD_FILTER_NO_WORKSPACE;\n  cudnnConvolutionBwdFilterAlgo_t algo_to_use;\n  RETURN_IF_CUDNN_ERROR(cudnnGetConvolutionBackwardFilterAlgorithm(\n      cudnn.handle(), input_nd.handle(), output_nd.handle(), conv.handle(),\n      filter.handle(), preference, memory_limit_bytes, &algo_to_use));\n  return algo_to_use;\n#endif\n}\n\nport::StatusOr<DeviceMemory<uint8>> AllocateCudnnConvolutionForwardWorkspace(\n    Stream* stream, const CudnnHandle& cudnn,\n    const CudnnTensorDescriptor& input_nd, const CudnnFilterDescriptor& filter,\n    const CudnnConvolutionDescriptor& conv,\n    const CudnnTensorDescriptor& output_nd,\n    const dnn::AlgorithmDesc& algorithm_desc,\n    ScratchAllocator* scratch_allocator) {\n  if (IsTensorMathOpSet(conv) != algorithm_desc.tensor_ops_enabled()) {\n    return port::Status(\n        port::error::INTERNAL,\n        \"Mismatch between cudnn conv and algorithm descriptors.\");\n  }\n\n  // Query the size of the workspace and allocate it.\n  size_t size_in_bytes;\n  RETURN_IF_CUDNN_ERROR(cudnnGetConvolutionForwardWorkspaceSize(\n      cudnn.handle(),\n      /*xDesc=*/input_nd.handle(),\n      /*wDesc=*/filter.handle(), /*convDesc=*/conv.handle(),\n      /*yDesc=*/output_nd.handle(), /*algo=*/ToConvForwardAlgo(algorithm_desc),\n      /*sizeInBytes=*/&size_in_bytes));\n\n  int64 size_in_bytes_int64 = size_in_bytes;\n\n  if (TF_PREDICT_FALSE(size_in_bytes_int64 < 0)) {\n    return port::Status(\n        port::error::INTERNAL,\n        \"cudnnGetConvolutionForwardWorkspaceSize() returned \"\n        \"negative sizeInBytes value. This could be a cudnn bug.\");\n  }\n\n  if (size_in_bytes_int64 == 0) {\n    return DeviceMemory<uint8>();\n  }\n\n  if (TF_PREDICT_FALSE(!scratch_allocator)) {\n    return port::Status(port::error::INVALID_ARGUMENT,\n                        \"No scratch allocator provided\");\n  }\n\n  return scratch_allocator->AllocateBytes(size_in_bytes);\n}\n\nport::StatusOr<DeviceMemory<uint8>>\nAllocateCudnnConvolutionBackwardDataWorkspace(\n    Stream* stream, const CudnnHandle& cudnn,\n    const CudnnTensorDescriptor& input_nd, const CudnnFilterDescriptor& filter,\n    const CudnnConvolutionDescriptor& conv,\n    const CudnnTensorDescriptor& output_nd,\n    const dnn::AlgorithmDesc& algorithm_desc,\n    ScratchAllocator* scratch_allocator) {\n  if (IsTensorMathOpSet(conv) != algorithm_desc.tensor_ops_enabled()) {\n    return port::Status(\n        port::error::INTERNAL,\n        \"Mismatch between cudnn conv and algorithm descriptors.\");\n  }\n\n  // Query the size of the workspace and allocate it.\n  size_t size_in_bytes;\n  RETURN_IF_CUDNN_ERROR(cudnnGetConvolutionBackwardDataWorkspaceSize(\n      cudnn.handle(),\n      /*wDesc=*/filter.handle(),\n      /*dyDesc=*/output_nd.handle(),\n      /*convDesc=*/conv.handle(),\n      /*dxDesc=*/input_nd.handle(),\n      /*algo=*/ToConvBackwardDataAlgo(algorithm_desc),\n      /*sizeInBytes=*/&size_in_bytes));\n\n  int64 size_in_bytes_int64 = size_in_bytes;\n\n  if (TF_PREDICT_FALSE(size_in_bytes_int64 < 0)) {\n    return port::Status(\n        port::error::INTERNAL,\n        \"cudnnGetConvolutionBackwardDataWorkspaceSize() returned \"\n        \"negative sizeInBytes value. This could be a cudnn bug.\");\n  }\n\n  if (size_in_bytes_int64 == 0) {\n    return DeviceMemory<uint8>();\n  }\n\n  if (TF_PREDICT_FALSE(!scratch_allocator)) {\n    return port::Status(port::error::INVALID_ARGUMENT,\n                        \"No scratch allocator provided\");\n  }\n\n  return scratch_allocator->AllocateBytes(size_in_bytes);\n}\n\nport::StatusOr<DeviceMemory<uint8>>\nAllocateCudnnConvolutionBackwardFilterWorkspace(\n    Stream* stream, const CudnnHandle& cudnn,\n    const CudnnTensorDescriptor& input_nd, const CudnnFilterDescriptor& filter,\n    const CudnnConvolutionDescriptor& conv,\n    const CudnnTensorDescriptor& output_nd,\n    const dnn::AlgorithmDesc& algorithm_desc,\n    ScratchAllocator* scratch_allocator) {\n  if (IsTensorMathOpSet(conv) != algorithm_desc.tensor_ops_enabled()) {\n    return port::Status(\n        port::error::INTERNAL,\n        \"Mismatch between cudnn conv and algorithm descriptors.\");\n  }\n\n  // Query the size of the workspace and allocate it.\n  size_t size_in_bytes;\n  RETURN_IF_CUDNN_ERROR(cudnnGetConvolutionBackwardFilterWorkspaceSize(\n      cudnn.handle(),\n      /*xDesc=*/input_nd.handle(),\n      /*dyDesc=*/output_nd.handle(),\n      /*convDesc=*/conv.handle(),\n      /*gradDesc=*/filter.handle(),\n      /*algo=*/ToConvBackwardFilterAlgo(algorithm_desc),\n      /*sizeInBytes=*/&size_in_bytes));\n\n  int64 size_in_bytes_int64 = size_in_bytes;\n\n  if (TF_PREDICT_FALSE(size_in_bytes_int64 < 0)) {\n    return port::Status(\n        port::error::INTERNAL,\n        \"cudnnGetConvolutionBackwardFilterWorkspaceSize() returned \"\n        \"negative sizeInBytes value. This could be a cudnn bug.\");\n  }\n\n  if (size_in_bytes_int64 == 0) {\n    return DeviceMemory<uint8>();\n  }\n\n  if (TF_PREDICT_FALSE(!scratch_allocator)) {\n    return port::Status(port::error::INVALID_ARGUMENT,\n                        \"No scratch allocator provided\");\n  }\n\n  return scratch_allocator->AllocateBytes(size_in_bytes);\n}\n\nport::StatusOr<bool> UseTensorOps(Stream* stream, dnn::DataType type,\n                                  absl::optional<dnn::AlgorithmDesc> desc) {\n  bool use_tensor_ops;\n  if (desc.has_value()) {\n    use_tensor_ops = desc->tensor_ops_enabled();\n    if (use_tensor_ops && !IsTensorMathEnabled(stream, type)) {\n      return port::Status(port::error::INVALID_ARGUMENT,\n                          \"Algo requests disabled tensor op evaluation.\");\n    }\n  } else {\n    use_tensor_ops = IsTensorMathEnabled(stream, type);\n  }\n  return use_tensor_ops;\n}\n\ncudnnDataType_t GetRnnComputeType(dnn::DataType data_type);\ndnn::DataType GetConvAccumulatorType(dnn::DataType data_type);\n\nport::StatusOr<dnn::AlgorithmDesc> GetCudnnConvolutionForwardAlgorithm(\n    Stream* stream, const CudnnHandle& cudnn,\n    const dnn::AlgorithmConfig& algorithm_config,\n    const CudnnTensorDescriptor& input_nd, const CudnnFilterDescriptor& filter,\n    dnn::DataType element_type,\n    const dnn::ConvolutionDescriptor& convolution_descriptor,\n    const CudnnTensorDescriptor& output_nd, ScratchAllocator* scratch_allocator,\n    DeviceMemory<uint8>* scratch) {\n  absl::optional<dnn::AlgorithmDesc> algo_desc = algorithm_config.algorithm();\n\n  CudnnConvolutionDescriptor conv(\n      convolution_descriptor,\n      ToCudnnDataType(GetConvAccumulatorType(element_type)));\n  bool use_tensor_ops;\n  SE_ASSIGN_OR_RETURN(use_tensor_ops,\n                      UseTensorOps(stream, element_type, algo_desc));\n  conv.set_use_tensor_op_math(use_tensor_ops);\n\n  if (!algo_desc.has_value()) {\n    // Pick fastest algorithm within memory limit according to cuDNN's\n    // heuristics.\n    bool specify_workspace_limit = scratch_allocator != nullptr;\n    auto memory_limit_bytes =\n        specify_workspace_limit\n            ? std::max(scratch_allocator->GetMemoryLimitInBytes(), int64{0})\n            : int64{0};\n    SE_ASSIGN_OR_RETURN(cudnnConvolutionFwdAlgo_t algo,\n                        GetCudnnConvolutionForwardAlgo(\n                            cudnn, input_nd, filter, conv, output_nd,\n                            specify_workspace_limit, memory_limit_bytes));\n    algo_desc = dnn::AlgorithmDesc(algo, use_tensor_ops);\n  }\n\n  const auto scratch_or = AllocateCudnnConvolutionForwardWorkspace(\n      stream, cudnn, input_nd, filter, conv, output_nd, *algo_desc,\n      scratch_allocator);\n\n  if (scratch_or.ok()) {\n    *scratch = scratch_or.ValueOrDie();\n    return *algo_desc;\n  }\n\n  algo_desc = algorithm_config.algorithm_no_scratch();\n\n  // Failed to allocate workspace for the first algorithm, fall back to the\n  // no_scratch algorithm.\n  if (!algo_desc.has_value()) {\n    return port::Status(\n        scratch_or.status().code(),\n        absl::StrCat(\"The primary convolution algorithm failed, \",\n                     \"while a secondary algorithm is not provided. \",\n                     \"Returned status: \", scratch_or.status().ToString()));\n  }\n\n  SE_ASSIGN_OR_RETURN(use_tensor_ops,\n                      UseTensorOps(stream, element_type, algo_desc));\n  conv.set_use_tensor_op_math(use_tensor_ops);\n  SE_ASSIGN_OR_RETURN(*scratch, AllocateCudnnConvolutionForwardWorkspace(\n                                    stream, cudnn, input_nd, filter, conv,\n                                    output_nd, *algo_desc, scratch_allocator));\n  return *algo_desc;\n}\n\nport::StatusOr<dnn::AlgorithmDesc> GetCudnnConvolutionBackwardDataAlgorithm(\n    Stream* stream, const CudnnHandle& cudnn,\n    const dnn::AlgorithmConfig& algorithm_config,\n    const CudnnTensorDescriptor& input_nd, const CudnnFilterDescriptor& filter,\n    dnn::DataType element_type,\n    const dnn::ConvolutionDescriptor& convolution_descriptor,\n    const CudnnTensorDescriptor& output_nd, ScratchAllocator* scratch_allocator,\n    DeviceMemory<uint8>* scratch) {\n  absl::optional<dnn::AlgorithmDesc> algo_desc = algorithm_config.algorithm();\n  CudnnConvolutionDescriptor conv(\n      convolution_descriptor,\n      ToCudnnDataType(GetConvAccumulatorType(element_type)));\n  bool use_tensor_ops;\n  SE_ASSIGN_OR_RETURN(use_tensor_ops,\n                      UseTensorOps(stream, element_type, algo_desc));\n  conv.set_use_tensor_op_math(use_tensor_ops);\n\n  if (!algo_desc.has_value()) {\n    // Pick fastest algorithm within memory limit according to cuDNN's\n    // heuristics.\n    bool specify_workspace_limit = scratch_allocator != nullptr;\n    auto memory_limit_bytes =\n        specify_workspace_limit\n            ? std::max(scratch_allocator->GetMemoryLimitInBytes(), int64{0})\n            : int64{0};\n    SE_ASSIGN_OR_RETURN(cudnnConvolutionBwdDataAlgo_t algo,\n                        GetCudnnConvolutionBackwardDataAlgo(\n                            cudnn, input_nd, filter, conv, output_nd,\n                            specify_workspace_limit, memory_limit_bytes));\n    algo_desc = dnn::AlgorithmDesc(algo, use_tensor_ops);\n  }\n\n  const auto scratch_or = AllocateCudnnConvolutionBackwardDataWorkspace(\n      stream, cudnn, input_nd, filter, conv, output_nd, *algo_desc,\n      scratch_allocator);\n\n  if (scratch_or.ok()) {\n    *scratch = scratch_or.ValueOrDie();\n    return *algo_desc;\n  }\n\n  algo_desc = algorithm_config.algorithm_no_scratch();\n\n  // Failed to allocate workspace for the first algorithm, fall back to the\n  // no_scratch algorithm.\n  if (!algo_desc.has_value()) {\n    return port::Status(\n        port::error::INVALID_ARGUMENT,\n        \"The primary convolution algorithm failed memory allocation, \"\n        \"while a secondary algorithm is not provided.\");\n  }\n\n  SE_ASSIGN_OR_RETURN(use_tensor_ops,\n                      UseTensorOps(stream, element_type, algo_desc));\n  conv.set_use_tensor_op_math(use_tensor_ops);\n  SE_ASSIGN_OR_RETURN(*scratch, AllocateCudnnConvolutionBackwardDataWorkspace(\n                                    stream, cudnn, input_nd, filter, conv,\n                                    output_nd, *algo_desc, scratch_allocator));\n  return *algo_desc;\n}\n\nport::StatusOr<dnn::AlgorithmDesc> GetCudnnConvolutionBackwardFilterAlgorithm(\n    Stream* stream, const CudnnHandle& cudnn,\n    const dnn::AlgorithmConfig& algorithm_config,\n    const CudnnTensorDescriptor& input_nd, const CudnnFilterDescriptor& filter,\n    dnn::DataType element_type,\n    const dnn::ConvolutionDescriptor& convolution_descriptor,\n    const CudnnTensorDescriptor& output_nd, ScratchAllocator* scratch_allocator,\n    DeviceMemory<uint8>* scratch) {\n  absl::optional<dnn::AlgorithmDesc> algo_desc = algorithm_config.algorithm();\n  CudnnConvolutionDescriptor conv(\n      convolution_descriptor,\n      ToCudnnDataType(GetConvAccumulatorType(element_type)));\n  bool use_tensor_ops;\n  SE_ASSIGN_OR_RETURN(use_tensor_ops,\n                      UseTensorOps(stream, element_type, algo_desc));\n  conv.set_use_tensor_op_math(use_tensor_ops);\n\n  if (!algo_desc.has_value()) {\n    // Pick fastest algorithm within memory limit according to cuDNN's\n    // heuristics.\n    bool specify_workspace_limit = scratch_allocator != nullptr;\n    auto memory_limit_bytes =\n        specify_workspace_limit\n            ? std::max(scratch_allocator->GetMemoryLimitInBytes(), int64{0})\n            : int64{0};\n    SE_ASSIGN_OR_RETURN(cudnnConvolutionBwdFilterAlgo_t algo,\n                        GetCudnnConvolutionBackwardFilterAlgo(\n                            cudnn, input_nd, filter, conv, output_nd,\n                            specify_workspace_limit, memory_limit_bytes));\n    algo_desc = dnn::AlgorithmDesc(algo, use_tensor_ops);\n  }\n\n  auto scratch_or = AllocateCudnnConvolutionBackwardFilterWorkspace(\n      stream, cudnn, input_nd, filter, conv, output_nd, *algo_desc,\n      scratch_allocator);\n\n  if (scratch_or.ok()) {\n    *scratch = scratch_or.ValueOrDie();\n    return *algo_desc;\n  }\n\n  algo_desc = algorithm_config.algorithm_no_scratch();\n\n  // Failed to allocate workspace for the first algorithm, fall back to the\n  // no_scratch algorithm.\n  if (!algo_desc.has_value()) {\n    return port::Status(\n        port::error::INVALID_ARGUMENT,\n        \"The primary convolution algorithm failed memory allocation, \"\n        \"while a secondary algorithm is not provided.\");\n  }\n\n  SE_ASSIGN_OR_RETURN(use_tensor_ops,\n                      UseTensorOps(stream, element_type, algo_desc));\n  conv.set_use_tensor_op_math(use_tensor_ops);\n  SE_ASSIGN_OR_RETURN(*scratch, AllocateCudnnConvolutionBackwardFilterWorkspace(\n                                    stream, cudnn, input_nd, filter, conv,\n                                    output_nd, *algo_desc, scratch_allocator));\n  return *algo_desc;\n}\n\n// A helper class to set env-vars and choose options for cudnn-related\n// algorithms.\ntemplate <typename EnvVar>\nclass CudnnEnvVar {\n public:\n  static bool IsEnabled() {\n    static bool is_enabled = IsEnabledImpl();\n    return is_enabled;\n  }\n\n private:\n  static bool IsEnabledImpl() {\n    const char* tf_env_var_val = getenv(EnvVar::kName);\n    if (tf_env_var_val != nullptr) {\n      absl::string_view tf_env_var_val_str(tf_env_var_val);\n      if (tf_env_var_val_str == \"0\") {\n        return false;\n      }\n      return true;\n    }\n    return EnvVar::kDefaultFlag;\n  }\n};\n\n// A helper struct to decide whether to enable the FFT_TILING algorithms for\n// forward convolution. It is disabled for cuDNN < 7 due to memory corruption\n// caused by some shapes with this algorithm. Users can explicitly enable the\n// algorithm through an env-var \"TF_ENABLE_FFT_TILING_FORWARD=1\".\nstruct FftTilingForward {\n  static constexpr const char* kName = \"TF_ENABLE_FFT_TILING_FORWARD\";\n  static constexpr bool kDefaultFlag = true;\n};\n\n// A helper struct to decide whether to enable the WINOGRAD_NONFUSED algorithms.\n// By default it is turned on, users can explicitly disable them through an\n// env-var \"TF_ENABLE_WINOGRAD_NONFUSED=0\".\n// https://github.com/tensorflow/tensorflow/pull/4901\nstruct WinogradNonfused {\n  static constexpr const char* kName = \"TF_ENABLE_WINOGRAD_NONFUSED\";\n  // NVIDIA has fixed winograd nonfused bug for cudnn v>=7. For older versions,\n  // we have a workaround.\n  static constexpr bool kDefaultFlag = true;\n};\n\n// A helper struct to decide whether to use FP32 as the internal compute type\n// for convolution when the input data type is FP16. By default it is turned on,\n// users can explicitly disable them (choose to use FP16 as the internal compute\n// type) through an env-var \"TF_FP16_CONV_USE_FP32_COMPUTE=0\".\nstruct ConvDoFP32ComputationFP16Input {\n  static constexpr const char* kName = \"TF_FP16_CONV_USE_FP32_COMPUTE\";\n  // Using FP16 as the internal compute type for convolution when the input data\n  // type is FP16 is only supported on architectures with true fp16 support\n  // (compute capability 5.3 and 6.0). Setting this to false in an unsupported\n  // architecture will cause internal errors.\n  static constexpr bool kDefaultFlag = true;\n};\n\n// A helper struct to decide whether to use FP32 as the internal compute type\n// for rnn when the input data type is FP16. At present it is turned off,\n// users can explicitly control them through an env-var\n// TF_FP16_RNN_USE_FP32_COMPUTE.\n// After the TODO below is fixed, users should almost always use fp32 compute\n// type for training. Using fp16 might suffer suboptimal accuracy due to loss\n// in precision.\nstruct RnnDoFP32ComputationFP16Input {\n  static constexpr const char* kName = \"TF_FP16_RNN_USE_FP32_COMPUTE\";\n  // TODO(jamesqin): b/78182362 flip to true when cudnn 7.1.4 fixes the bug.\n  // Before cudnn 7.1.4 RNN are always done in fp32, no matter what math\n  // precision is set.\n  // Set it temporary to false s.t. no error is raised when using fp16 inputs,\n  // fp32 math precision.\n  //\n  // cuDNN == 7.5.0 is verified to have this fixed.\n  static constexpr bool kDefaultFlag = CUDNN_VERSION >= 7500;\n};\n\ncudnnDataType_t GetRnnComputeType(dnn::DataType data_type) {\n  switch (data_type) {\n    case dnn::DataType::kFloat:\n      return CUDNN_DATA_FLOAT;\n    case dnn::DataType::kDouble:\n      return CUDNN_DATA_DOUBLE;\n    case dnn::DataType::kHalf:\n      if (CudnnEnvVar<RnnDoFP32ComputationFP16Input>::IsEnabled()) {\n        return CUDNN_DATA_FLOAT;\n      } else {\n        return CUDNN_DATA_HALF;\n      }\n    default:\n      LOG(FATAL) << \"Invalid RNN data type: \" << static_cast<int>(data_type);\n  }\n}\n\ndnn::DataType GetConvAccumulatorType(dnn::DataType data_type) {\n  switch (data_type) {\n    case dnn::DataType::kFloat:\n    case dnn::DataType::kDouble:\n      return data_type;\n    case dnn::DataType::kHalf:\n      return CudnnEnvVar<ConvDoFP32ComputationFP16Input>::IsEnabled()\n                 ? dnn::DataType::kFloat\n                 : dnn::DataType::kHalf;\n    case dnn::DataType::kInt8:\n    case dnn::DataType::kInt32:\n      return dnn::DataType::kInt32;\n    default:\n      LOG(FATAL) << \"Invalid DNN data type: \" << static_cast<int>(data_type);\n  }\n}\n}  // namespace\n\nport::Status CudnnSupport::DoPrepareForConvolution(\n    dnn::ConvolutionKind kind, dnn::DataType element_type, Stream* stream,\n    const dnn::BatchDescriptor& input_descriptor, DeviceMemoryBase input_data,\n    const dnn::FilterDescriptor& filter_descriptor,\n    DeviceMemoryBase filter_data, const dnn::BatchDescriptor& output_descriptor,\n    DeviceMemoryBase output_data,\n    const dnn::ConvolutionDescriptor& convolution_descriptor,\n    const dnn::AlgorithmConfig& algorithm_config,\n    ScratchAllocator* scratch_allocator, dnn::AlgorithmDesc* algorithm_desc,\n    DeviceMemory<uint8>* scratch_memory) {\n  CudnnTensorDescriptor input_nd(\n      input_descriptor,\n      ToCudnnDataType(element_type, input_descriptor.layout()));\n  CudnnFilterDescriptor filter_nd(\n      filter_descriptor,\n      ToCudnnDataType(element_type, filter_descriptor.layout()));\n  CudnnTensorDescriptor output_nd(\n      output_descriptor,\n      ToCudnnDataType(element_type, output_descriptor.layout()));\n\n  auto cudnn = cudnn_->GetHandle(parent_, stream);\n\n  switch (kind) {\n    case dnn::ConvolutionKind::FORWARD: {\n      SE_ASSIGN_OR_RETURN(*algorithm_desc,\n                          GetCudnnConvolutionForwardAlgorithm(\n                              stream, cudnn, algorithm_config, input_nd,\n                              filter_nd, element_type, convolution_descriptor,\n                              output_nd, scratch_allocator, scratch_memory));\n      break;\n    }\n    case dnn::ConvolutionKind::BACKWARD_DATA: {\n      SE_ASSIGN_OR_RETURN(*algorithm_desc,\n                          GetCudnnConvolutionBackwardDataAlgorithm(\n                              stream, cudnn, algorithm_config, input_nd,\n                              filter_nd, element_type, convolution_descriptor,\n                              output_nd, scratch_allocator, scratch_memory));\n      break;\n    }\n    case dnn::ConvolutionKind::BACKWARD_FILTER: {\n      SE_ASSIGN_OR_RETURN(*algorithm_desc,\n                          GetCudnnConvolutionBackwardFilterAlgorithm(\n                              stream, cudnn, algorithm_config, input_nd,\n                              filter_nd, element_type, convolution_descriptor,\n                              output_nd, scratch_allocator, scratch_memory));\n      break;\n    }\n    default:\n      return port::InternalError(\n          absl::StrCat(\"Unexpected convolution kind \", static_cast<int>(kind)));\n  }\n\n  return port::Status::OK();\n}\n\nport::Status CudnnSupport::DoConvolve(\n    dnn::ConvolutionKind kind, dnn::DataType element_type,\n    dnn::DataType output_type, Stream* stream,\n    const dnn::BatchDescriptor& input_descriptor, DeviceMemoryBase input_data,\n    const dnn::FilterDescriptor& filter_descriptor,\n    DeviceMemoryBase filter_data, const dnn::BatchDescriptor& output_descriptor,\n    DeviceMemoryBase output_data,\n    const dnn::ConvolutionDescriptor& convolution_descriptor,\n    dnn::AlgorithmDesc algorithm_desc, DeviceMemory<uint8> scratch_memory,\n    dnn::ProfileResult* output_profile_result) {\n  cudnnDataType_t cudnn_type = ToCudnnDataType(element_type);\n  CudnnTensorDescriptor input_nd(input_descriptor, cudnn_type);\n  CudnnTensorDescriptor output_nd(output_descriptor,\n                                  ToCudnnDataType(output_type));\n  CudnnFilterDescriptor filter_nd(filter_descriptor, cudnn_type);\n  auto accumulator_type = GetConvAccumulatorType(element_type);\n  CudnnConvolutionDescriptor conv(convolution_descriptor,\n                                  ToCudnnDataType(accumulator_type));\n  SE_ASSIGN_OR_RETURN(bool use_tensor_ops,\n                      UseTensorOps(stream, element_type, algorithm_desc));\n  conv.set_use_tensor_op_math(use_tensor_ops);\n\n  auto cudnn = cudnn_->GetHandle(parent_, stream);\n  // Alpha is the scaling factor for input.\n  float falpha = 1.0;\n  double dalpha = 1.0;\n  void* alpha = cudnn_type == CUDNN_DATA_DOUBLE ? static_cast<void*>(&dalpha)\n                                                : static_cast<void*>(&falpha);\n  // Beta is the scaling factor for output.\n  float fbeta = 0.0;\n  double dbeta = 0.0;\n  void* beta = cudnn_type == CUDNN_DATA_DOUBLE ? static_cast<void*>(&dbeta)\n                                               : static_cast<void*>(&fbeta);\n\n  const bool is_profiling = output_profile_result != nullptr;\n\n  std::unique_ptr<GpuTimer, GpuTimerDeleter> timer;\n  if (is_profiling) {\n    timer.reset(new GpuTimer(parent_));  // NOLINT\n    // The start and stop of the timer should be as close to the Cudnn call as\n    // possible. It is still possible for other threads to issue workload on\n    // to this stream. So it could take multiple profiling measurements.\n    if (!timer->Init() || !timer->Start(AsGpuStream(stream))) {\n      return port::Status(port::error::INTERNAL, \"Failed to start timer\");\n    }\n  }\n\n  const auto get_fwd_bugs = [&]() -> port::Status {\n    if (CUDNN_VERSION < 8000) {\n      if (algorithm_desc.algo_id() ==\n              CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM &&\n          ToCudnnDataType(element_type) == CUDNN_DATA_INT8 &&\n          ToCudnnDataType(output_type) == CUDNN_DATA_FLOAT) {\n        return port::Status(\n            port::error::FAILED_PRECONDITION,\n            \"This configuration potentially produces incorrect results.\");\n      }\n    }\n    return port::Status::OK();\n  };\n\n  auto get_bwd_data_bugs = [&]() -> port::Status {\n    return port::Status::OK();\n  };\n\n  const auto get_bwd_filter_bugs = [&]() -> port::Status {\n    return port::Status::OK();\n  };\n\n  switch (kind) {\n    case dnn::ConvolutionKind::FORWARD: {\n      SE_RETURN_IF_ERROR(get_fwd_bugs());\n      RETURN_IF_CUDNN_ERROR(cudnnConvolutionForward(\n          cudnn.handle(),\n          /*alpha=*/alpha, /*srcDesc=*/input_nd.handle(),\n          /*srcData=*/input_data.opaque(), /*filterDesc=*/filter_nd.handle(),\n          /*filterData=*/filter_data.opaque(), /*convDesc=*/conv.handle(),\n          /*algo=*/ToConvForwardAlgo(algorithm_desc),\n          /*workSpace=*/scratch_memory.opaque(),\n          /*workSpaceSizeInBytes=*/scratch_memory.size(), /*beta=*/beta,\n          /*yDesc=*/output_nd.handle(), /*y=*/output_data.opaque()));\n      break;\n    }\n    case dnn::ConvolutionKind::BACKWARD_DATA: {\n      SE_RETURN_IF_ERROR(get_bwd_data_bugs());\n      RETURN_IF_CUDNN_ERROR(cudnnConvolutionBackwardData(\n          cudnn.handle(),\n          /*alpha=*/alpha,\n          /*wDesc=*/filter_nd.handle(),\n          /*w=*/filter_data.opaque(),\n          /*dyDesc=*/output_nd.handle(),\n          /*dy=*/output_data.opaque(),\n          /*convDesc=*/conv.handle(),\n          /*algo=*/ToConvBackwardDataAlgo(algorithm_desc),\n          /*workSpace=*/scratch_memory.opaque(),\n          /*workSpaceSizeInBytes=*/scratch_memory.size(),\n          /*beta=*/beta,\n          /*dxDesc=*/input_nd.handle(),\n          /*dx=*/input_data.opaque()));\n      break;\n    }\n    case dnn::ConvolutionKind::BACKWARD_FILTER: {\n      SE_RETURN_IF_ERROR(get_bwd_filter_bugs());\n      RETURN_IF_CUDNN_ERROR(cudnnConvolutionBackwardFilter(\n          cudnn.handle(),\n          /*alpha=*/alpha,\n          /*srcDesc=*/input_nd.handle(),\n          /*srcData=*/input_data.opaque(),\n          /*diffDesc=*/output_nd.handle(),\n          /*diffData=*/output_data.opaque(),\n          /*convDesc=*/conv.handle(),\n          /*algo=*/ToConvBackwardFilterAlgo(algorithm_desc),\n          /*workSpace=*/scratch_memory.opaque(),\n          /*workSpaceSizeInBytes=*/scratch_memory.size(),\n          /*beta=*/beta,\n          /*gradDesc=*/filter_nd.handle(),\n          /*dw=*/filter_data.opaque()));\n      break;\n    }\n    default:\n      return port::InternalError(\n          absl::StrCat(\"Unexpected convolution kind \", static_cast<int>(kind)));\n  }\n\n  if (is_profiling) {\n    if (!timer->Stop(AsGpuStream(stream))) {\n      return port::Status(port::error::INTERNAL, \"Failed to stop timer\");\n    }\n    output_profile_result->set_algorithm(algorithm_desc);\n    output_profile_result->set_elapsed_time_in_ms(\n        timer->GetElapsedMilliseconds());\n    output_profile_result->set_scratch_size(scratch_memory.size());\n  }\n\n  return port::Status::OK();\n}\n\ntemplate <typename ElementType, typename BiasType, typename ScaleType,\n          typename OutputType>\nport::Status CudnnSupport::DoFusedConvolveImpl(\n    Stream* stream, const dnn::BatchDescriptor& conv_input_descriptor,\n    const DeviceMemory<ElementType>& conv_input_data,\n    ScaleType conv_input_scale, const dnn::FilterDescriptor& filter_descriptor,\n    const DeviceMemory<ElementType>& filter_data,\n    const dnn::ConvolutionDescriptor& convolution_descriptor,\n    const DeviceMemory<OutputType>& side_input_data, ScaleType side_input_scale,\n    const dnn::BatchDescriptor& bias_descriptor,\n    const DeviceMemory<BiasType>& biases, dnn::ActivationMode activation_mode,\n    const dnn::BatchDescriptor& output_descriptor,\n    DeviceMemory<OutputType>* output_data, dnn::DataType accumulator_type,\n    ScratchAllocator* scratch_allocator,\n    const dnn::AlgorithmConfig& algorithm_config,\n    dnn::ProfileResult* output_profile_result) {\n  if (activation_mode != dnn::ActivationMode::kRelu &&\n      activation_mode != dnn::ActivationMode::kNone) {\n    return port::Status(port::error::INVALID_ARGUMENT,\n                        \"cudnnConvolutionBiasActivationForward() only supports \"\n                        \"Relu or None activation.\");\n  }\n\n  CudnnTensorDescriptor conv_input_nd(\n      conv_input_descriptor,\n      GetCudnnDataType<ElementType>(conv_input_descriptor.layout()));\n  CudnnTensorDescriptor output_nd(\n      output_descriptor,\n      GetCudnnDataType<OutputType>(conv_input_descriptor.layout()));\n  CudnnFilterDescriptor filter(\n      filter_descriptor,\n      GetCudnnDataType<ElementType>(conv_input_descriptor.layout()));\n  CudnnTensorDescriptor bias_nd(bias_descriptor, GetCudnnDataType<BiasType>());\n\n  auto cudnn = cudnn_->GetHandle(parent_, stream);\n\n  const bool is_profiling = output_profile_result != nullptr;\n\n  DeviceMemory<uint8> scratch;\n  SE_ASSIGN_OR_RETURN(\n      dnn::AlgorithmDesc algo_desc,\n      GetCudnnConvolutionForwardAlgorithm(\n          stream, cudnn, algorithm_config, conv_input_nd, filter,\n          dnn::ToDataType<ElementType>::value, convolution_descriptor,\n          output_nd, scratch_allocator, &scratch));\n\n  CudnnConvolutionDescriptor conv(convolution_descriptor,\n                                  ToCudnnDataType(accumulator_type));\n  conv.set_use_tensor_op_math(algo_desc.tensor_ops_enabled());\n\n  std::unique_ptr<GpuTimer, GpuTimerDeleter> timer;\n  if (is_profiling) {\n    timer.reset(new GpuTimer(parent_));  // NOLINT\n    // The start and stop of the timer should be as close to the Cudnn call as\n    // possible. It is still possible for other threads to issue workload on\n    // to this stream. So it could take multiple profiling measurements.\n    if (!timer->Init() || !timer->Start(AsGpuStream(stream))) {\n      return port::Status(port::error::INTERNAL, \"Failed to start timer\");\n    }\n  }\n  // CUDNN v6 only supports CUDNN_NOT_PROPAGATE_NAN as the reluNanOpt for\n  // activation descriptor. Note that this will change the nan propagation\n  // behavior from separate conv, bias, and relu (which by default is\n  // CUDNN_PROPAGATE_NAN.\n  CudnnActivationDescriptor activation_desc(\n      activation_mode, CUDNN_NOT_PROPAGATE_NAN, output_descriptor.value_max());\n  auto side_input_data_ptr = (side_input_scale == 0) ? output_data->opaque()\n                                                     : side_input_data.opaque();\n\n  VLOG(2) << \"\\nconv_input_scale = \" << conv_input_scale\n          << \"\\nconv_input_nd.handle() = \" << conv_input_nd.handle()\n          << \"\\nconv_input_data.opaque() = \" << conv_input_data.opaque()\n          << \"\\nfilter.handle() = \" << filter.handle()\n          << \"\\nfilter_data.opaque() = \" << filter_data.opaque()\n          << \"\\nconv.handle() = \" << conv.handle()\n          << \"\\nalgo = \" << algo_desc.algo_id()\n          << \"\\nscratch.opaque() = \" << scratch.opaque()\n          << \"\\nscratch.size() = \" << scratch.size()\n          << \"\\nside_input_scale = \" << side_input_scale\n          << \"\\noutput_nd.handle() = \" << output_nd.handle()\n          << \"\\nside_input_data_ptr = \" << side_input_data_ptr\n          << \"\\nbias_nd.handle() = \" << bias_nd.handle()\n          << \"\\nbiases.opaque() = \" << biases.opaque()\n          << \"\\nactivation_desc.handle() = \" << activation_desc.handle()\n          << \"\\noutput_nd.handle() = \" << output_nd.handle()\n          << \"\\noutput_data->opaque() = \" << output_data->opaque();\n\n  if (IsTensorMathOpSet(conv) != algo_desc.tensor_ops_enabled()) {\n    return port::Status(port::error::FAILED_PRECONDITION,\n                        \"Tensor op math type in dnn::AlgorithmDesc does not \"\n                        \"match that of the CudnnConvolutionDescriptor\");\n  }\n\n  RETURN_IF_CUDNN_ERROR(cudnnConvolutionBiasActivationForward(\n      cudnn.handle(),\n      /*alpha1=*/&conv_input_scale,\n      /*srcDesc=*/conv_input_nd.handle(), /*srcData=*/conv_input_data.opaque(),\n      /*filterDesc=*/filter.handle(), /*filterData=*/filter_data.opaque(),\n      /*convDesc=*/conv.handle(), ToConvForwardAlgo(algo_desc),\n      /*workSpace=*/scratch.opaque(),\n      /*workSpaceSizeInBytes=*/scratch.size(), /*alpha2=*/&side_input_scale,\n      /*zDesc=*/output_nd.handle(), /*z=*/side_input_data_ptr,\n      /*biasDesc=*/bias_nd.handle(), /*bias=*/biases.opaque(),\n      /*activationDesc=*/activation_desc.handle(),\n      /*yDesc=*/output_nd.handle(), /*y=*/output_data->opaque()));\n\n  if (is_profiling) {\n    if (!timer->Stop(AsGpuStream(stream))) {\n      return port::Status(port::error::INTERNAL, \"Failed to stop timer\");\n    }\n    output_profile_result->set_algorithm(algo_desc);\n    output_profile_result->set_elapsed_time_in_ms(\n        timer->GetElapsedMilliseconds());\n    output_profile_result->set_scratch_size(scratch.size());\n  }\n\n  return port::Status::OK();\n}\n\nbool CudnnSupport::GetConvolveAlgorithms(\n    bool with_winograd_nonfused, int cc_major, int cc_minor,\n    std::vector<dnn::AlgorithmDesc>* out_algorithms) {\n  // Preload sub libs for cudnn 8.0.4+\n#if CUDNN_MAJOR >= 8 && (CUDNN_MINOR > 0 || CUDNN_PATCHLEVEL >= 4)\n  cudnnOpsInferVersionCheck();\n  cudnnCnnInferVersionCheck();\n#endif\n  bool tensor_op_math_available = TensorOpMathAvailable(cc_major);\n  out_algorithms->clear();\n\n  std::vector<dnn::AlgorithmDesc::Index> algo_types;\n  if (ConvUseDefaultAlgorithm()) {\n    // Force a fallback algorithm.\n    algo_types = {CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM};\n  } else {\n    algo_types = {\n        // clang-format off\n    CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM,\n    CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM,\n    CUDNN_CONVOLUTION_FWD_ALGO_GEMM,\n    CUDNN_CONVOLUTION_FWD_ALGO_DIRECT,\n    CUDNN_CONVOLUTION_FWD_ALGO_FFT,\n    CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD,\n        // clang-format on\n    };\n    if (CudnnEnvVar<FftTilingForward>::IsEnabled()) {\n      algo_types.push_back(CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING);\n    }\n    if (CudnnEnvVar<WinogradNonfused>::IsEnabled() && with_winograd_nonfused) {\n      algo_types.push_back(CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED);\n    }\n  }\n\n  // The algorithms are intentionally ordered for deterministic operation\n  for (auto i : algo_types) {\n    if (tensor_op_math_available) {\n      out_algorithms->push_back({i, /*use_tensor_ops=*/true});\n    }\n    out_algorithms->push_back({i, /*use_tensor_ops=*/false});\n  }\n\n  return true;\n}\n\nbool CudnnSupport::GetRnnAlgorithms(\n    std::vector<dnn::AlgorithmDesc>* out_algorithms) {\n  // Preload sub libs for cudnn 8.0.4+\n#if CUDNN_MAJOR >= 8 && (CUDNN_MINOR > 0 || CUDNN_PATCHLEVEL >= 4)\n  cudnnOpsInferVersionCheck();\n  cudnnOpsTrainVersionCheck();\n  cudnnAdvInferVersionCheck();\n  cudnnAdvTrainVersionCheck();\n#endif\n  std::vector<dnn::AlgorithmDesc::Index> algo_types = {\n      // clang-format off\n    CUDNN_RNN_ALGO_STANDARD,\n    CUDNN_RNN_ALGO_PERSIST_STATIC,\n    CUDNN_RNN_ALGO_PERSIST_DYNAMIC,\n      // clang-format on\n  };\n\n  out_algorithms->clear();\n  for (auto i : algo_types) {\n    out_algorithms->push_back({i, /*use_tensor_ops=*/false});\n    out_algorithms->push_back({i, /*use_tensor_ops=*/true});\n  }\n  return true;\n}\n\nbool CudnnSupport::GetConvolveBackwardDataAlgorithms(\n    bool with_winograd_nonfused, int cc_major, int cc_minor,\n    std::vector<dnn::AlgorithmDesc>* out_algorithms) {\n  // Preload sub libs for cudnn 8.0.4+\n#if CUDNN_MAJOR >= 8 && (CUDNN_MINOR > 0 || CUDNN_PATCHLEVEL >= 4)\n  cudnnOpsInferVersionCheck();\n  cudnnOpsTrainVersionCheck();\n  cudnnCnnInferVersionCheck();\n  cudnnCnnTrainVersionCheck();\n#endif\n  bool tensor_op_math_available = TensorOpMathAvailable(cc_major);\n  out_algorithms->clear();\n\n  std::vector<dnn::AlgorithmDesc::Index> algo_types = {\n      // clang-format off\n    CUDNN_CONVOLUTION_BWD_DATA_ALGO_1,\n    CUDNN_CONVOLUTION_BWD_DATA_ALGO_FFT,\n    CUDNN_CONVOLUTION_BWD_DATA_ALGO_FFT_TILING,\n    CUDNN_CONVOLUTION_BWD_DATA_ALGO_WINOGRAD,\n      // clang-format on\n  };\n  if (CudnnEnvVar<WinogradNonfused>::IsEnabled() && with_winograd_nonfused) {\n    algo_types.push_back(CUDNN_CONVOLUTION_BWD_DATA_ALGO_WINOGRAD_NONFUSED);\n  }\n  if (!RequireCudnnDeterminism()) {\n    algo_types.push_back(CUDNN_CONVOLUTION_BWD_DATA_ALGO_0);\n  }\n\n  // The algorithms are intentionally ordered for deterministic operation\n  for (auto i : algo_types) {\n    if (tensor_op_math_available) {\n      out_algorithms->push_back({i, /*use_tensor_ops=*/true});\n    }\n    out_algorithms->push_back({i, /*use_tensor_ops=*/false});\n  }\n\n  return true;\n}\n\nbool CudnnSupport::GetConvolveBackwardFilterAlgorithms(\n    bool with_winograd_nonfused, int cc_major, int cc_minor,\n    std::vector<dnn::AlgorithmDesc>* out_algorithms) {\n  // Preload sub libs for cudnn 8.0.4+\n#if CUDNN_MAJOR >= 8 && (CUDNN_MINOR > 0 || CUDNN_PATCHLEVEL >= 4)\n  cudnnOpsInferVersionCheck();\n  cudnnOpsTrainVersionCheck();\n  cudnnCnnInferVersionCheck();\n  cudnnCnnTrainVersionCheck();\n#endif\n  bool tensor_op_math_available = TensorOpMathAvailable(cc_major);\n  out_algorithms->clear();\n\n  std::vector<dnn::AlgorithmDesc::Index> algo_types = {\n      // clang-format off\n      CUDNN_CONVOLUTION_BWD_FILTER_ALGO_1,\n      CUDNN_CONVOLUTION_BWD_FILTER_ALGO_FFT,\n      // Based on cudnn.h, the following is not implemented.\n      // CUDNN_CONVOLUTION_BWD_FILTER_ALGO_WINOGRAD,\n\n      // Produces incorrect results for some shapes. Disabled for now, see\n      // NVIDIA bug 2072856. TODO(csigg): Only disable for subset of shapes.\n      // CUDNN_CONVOLUTION_BWD_FILTER_ALGO_FFT_TILING,\n      // clang-format on\n  };\n  if (CudnnEnvVar<WinogradNonfused>::IsEnabled() && with_winograd_nonfused) {\n    algo_types.push_back(CUDNN_CONVOLUTION_BWD_FILTER_ALGO_WINOGRAD_NONFUSED);\n  }\n  if (!RequireCudnnDeterminism()) {\n    algo_types.push_back(CUDNN_CONVOLUTION_BWD_FILTER_ALGO_0);\n    algo_types.push_back(CUDNN_CONVOLUTION_BWD_FILTER_ALGO_3);\n  }\n\n  // The algorithms are intentionally ordered for deterministic operation\n  for (auto i : algo_types) {\n    if (tensor_op_math_available) {\n      out_algorithms->push_back({i, /*use_tensor_ops=*/true});\n    }\n    out_algorithms->push_back({i, /*use_tensor_ops=*/false});\n  }\n\n  return true;\n}\n\nbool CudnnSupport::DoBatchNormalizationForward(\n    Stream* stream, const DeviceMemory<float>& x,\n    const DeviceMemory<float>& scale, const DeviceMemory<float>& offset,\n    const DeviceMemory<float>& estimated_mean,\n    const DeviceMemory<float>& estimated_variance,\n    const DeviceMemory<float>& side_input, const dnn::BatchDescriptor& x_desc,\n    const dnn::BatchDescriptor& scale_offset_desc, const double epsilon,\n    const double exponential_average_factor,\n    dnn::ActivationMode activation_mode, DeviceMemory<float>* y,\n    DeviceMemory<float>* batch_mean, DeviceMemory<float>* batch_var,\n    DeviceMemory<float>* saved_mean, DeviceMemory<float>* saved_inv_var,\n    bool is_training, ScratchAllocator* reserve_space_allocator,\n    ScratchAllocator* workspace_allocator) {\n  return IsStatusOk(\n      DoBatchNormalizationForwardImpl<float, float>(\n          stream, dnn::DataType::kFloat, dnn::DataType::kFloat, x, scale,\n          offset, estimated_mean, estimated_variance, side_input, x_desc,\n          scale_offset_desc, epsilon, exponential_average_factor,\n          activation_mode, y, batch_mean, batch_var, saved_mean, saved_inv_var,\n          is_training, reserve_space_allocator, workspace_allocator),\n      /*report_error=*/true);\n}\n\nbool CudnnSupport::DoBatchNormalizationForward(\n    Stream* stream, const DeviceMemory<Eigen::half>& x,\n    const DeviceMemory<float>& scale, const DeviceMemory<float>& offset,\n    const DeviceMemory<float>& estimated_mean,\n    const DeviceMemory<float>& estimated_variance,\n    const DeviceMemory<float>& side_input, const dnn::BatchDescriptor& x_desc,\n    const dnn::BatchDescriptor& scale_offset_desc, const double epsilon,\n    const double exponential_average_factor,\n    dnn::ActivationMode activation_mode, DeviceMemory<Eigen::half>* y,\n    DeviceMemory<float>* batch_mean, DeviceMemory<float>* batch_var,\n    DeviceMemory<float>* saved_mean, DeviceMemory<float>* saved_inv_var,\n    bool is_training, ScratchAllocator* reserve_space_allocator,\n    ScratchAllocator* workspace_allocator) {\n  return IsStatusOk(\n      DoBatchNormalizationForwardImpl<Eigen::half, float>(\n          stream, dnn::DataType::kHalf, dnn::DataType::kFloat, x, scale, offset,\n          estimated_mean, estimated_variance, side_input, x_desc,\n          scale_offset_desc, epsilon, exponential_average_factor,\n          activation_mode, y, batch_mean, batch_var, saved_mean, saved_inv_var,\n          is_training, reserve_space_allocator, workspace_allocator),\n      /*report_error=*/true);\n}\n\ntemplate <class T, class U>\nport::Status CudnnSupport::DoBatchNormalizationForwardImpl(\n    Stream* stream, dnn::DataType input_data_type,\n    dnn::DataType scale_data_type, const DeviceMemory<T>& x,\n    const DeviceMemory<U>& scale, const DeviceMemory<U>& offset,\n    const DeviceMemory<U>& estimated_mean,\n    const DeviceMemory<U>& estimated_variance,\n    const DeviceMemory<U>& side_input, const dnn::BatchDescriptor& x_desc,\n    const dnn::BatchDescriptor& scale_offset_desc, const double epsilon,\n    const double exponential_average_factor,\n    dnn::ActivationMode activation_mode, DeviceMemory<T>* y,\n    DeviceMemory<U>* batch_mean, DeviceMemory<U>* batch_var,\n    DeviceMemory<U>* saved_mean, DeviceMemory<U>* saved_inv_var,\n    bool is_training, ScratchAllocator* reserve_space_allocator,\n    ScratchAllocator* workspace_allocator) {\n  CudnnTensorDescriptor x_descriptor(x_desc, ToCudnnDataType(input_data_type));\n  CudnnTensorDescriptor scale_offset_descriptor(\n      scale_offset_desc, ToCudnnDataType(scale_data_type));\n  cudnnBatchNormMode_t mode = CUDNN_BATCHNORM_SPATIAL;\n  if (BatchnormSpatialPersistentEnabled() && is_training) {\n    mode = CUDNN_BATCHNORM_SPATIAL_PERSISTENT;\n  }\n  float one = 1.0;\n  float zero = 0.0;\n  auto cudnn = cudnn_->GetHandle(parent_, stream);\n\n  DeviceMemory<uint8> workspace;\n  DeviceMemory<uint8> reserve_space;\n\n#if CUDNN_VERSION >= 7402\n  const auto get_bn_ops = [&]() -> cudnnBatchNormOps_t {\n    if (side_input.is_null()) {\n      return activation_mode == dnn::ActivationMode::kNone\n                 ? CUDNN_BATCHNORM_OPS_BN\n                 : CUDNN_BATCHNORM_OPS_BN_ACTIVATION;\n    } else {\n      return CUDNN_BATCHNORM_OPS_BN_ADD_ACTIVATION;\n    }\n  };\n  const cudnnBatchNormOps_t bn_ops = get_bn_ops();\n\n  // We use Nan propagation to be consistent with CudnnSupport::DoActivate(...).\n  CudnnActivationDescriptor activation_desc(\n      activation_mode, CUDNN_PROPAGATE_NAN, x_desc.value_max());\n\n  if (reserve_space_allocator != nullptr && workspace_allocator != nullptr) {\n    SE_ASSIGN_OR_RETURN(\n        workspace,\n        CreateBatchNormForwardWorkspace(\n            stream, cudnn, mode, bn_ops, activation_desc.handle(), x_descriptor,\n            scale_offset_descriptor, workspace_allocator))\n    if (is_training) {\n      size_t reserve_space_size_in_bytes = 0;\n      RETURN_IF_CUDNN_ERROR(\n          cudnnGetBatchNormalizationTrainingExReserveSpaceSize(\n              /*handle=*/cudnn.handle(), /*mode=*/mode, /*bnOps=*/bn_ops,\n              /*activationDesc=*/activation_desc.handle(),\n              /*xDesc=*/x_descriptor.handle(),\n              /*sizeInBytes=*/&reserve_space_size_in_bytes));\n      SE_ASSIGN_OR_RETURN(reserve_space, reserve_space_allocator->AllocateBytes(\n                                             reserve_space_size_in_bytes));\n    }\n  }\n#endif\n\n  auto check_no_side_input_or_activation = [&]() -> port::Status {\n    if (activation_mode != dnn::ActivationMode::kNone ||\n        !side_input.is_null()) {\n      return port::Status(\n          port::error::INTERNAL,\n          absl::StrCat(\n              \"Side input and activation are not supported by cuDNN version: \",\n              CUDNN_VERSION));\n    } else {\n      return port::Status::OK();\n    }\n  };\n\n  if (is_training) {\n    CHECK_EQ(batch_mean->is_null(), batch_var->is_null())\n        << \"batch_mean and batch_var must both be null or both be non-null\";\n\n    void* batch_mean_opaque;\n    void* batch_var_opaque;\n    if (!batch_mean->is_null() && !batch_var->is_null()) {\n      if (exponential_average_factor == 1.0) {\n        stream->ThenMemZero(batch_mean, batch_mean->size());\n        stream->ThenMemZero(batch_var, batch_var->size());\n      }\n      batch_mean_opaque = batch_mean->opaque();\n      batch_var_opaque = batch_var->opaque();\n    } else {\n      batch_mean_opaque = nullptr;\n      batch_var_opaque = nullptr;\n    }\n\n    bool called = false;\n#if CUDNN_VERSION >= 7402\n    if (reserve_space_allocator != nullptr && workspace_allocator != nullptr) {\n      called = true;\n      RETURN_IF_CUDNN_ERROR(cudnnBatchNormalizationForwardTrainingEx(\n          /*handle=*/cudnn.handle(),\n          /*mode=*/mode,\n          /*bnOps=*/bn_ops,\n          /*alpha=*/&one,\n          /*beta=*/&zero,\n          /*xDesc=*/x_descriptor.handle(),\n          /*xData=*/x.opaque(),\n          /*zDesc=*/x_descriptor.handle(),\n          /*zData=*/side_input.opaque(),\n          /*yDesc=*/x_descriptor.handle(),\n          /*yData=*/y->opaque(),\n          /*bnScaleBiasMeanVarDesc=*/scale_offset_descriptor.handle(),\n          /*bnScale=*/scale.opaque(),\n          /*bnBias=*/offset.opaque(),\n          /*exponentialAverageFactor=*/exponential_average_factor,\n          /*resultRunningMean=*/batch_mean_opaque,\n          /*resultRunningVariance=*/batch_var_opaque,\n          /*epsilon=*/epsilon,\n          /*resultSaveMean=*/saved_mean->opaque(),\n          /*resultSaveInvVariance=*/saved_inv_var->opaque(),\n          /*activationDesc=*/activation_desc.handle(),\n          /*workspace=*/workspace.opaque(),\n          /*workSpaceSizeInBytes=*/workspace.size(),\n          /*reserveSpace=*/reserve_space.opaque(),\n          /*reserveSpaceSizeInBytes=*/reserve_space.size()));\n    }\n#endif\n    if (!called) {\n      SE_RETURN_IF_ERROR(check_no_side_input_or_activation());\n      RETURN_IF_CUDNN_ERROR(cudnnBatchNormalizationForwardTraining(\n          cudnn.handle(), mode, &one, &zero, x_descriptor.handle(), x.opaque(),\n          x_descriptor.handle(), y->opaque(), scale_offset_descriptor.handle(),\n          scale.opaque(), offset.opaque(), exponential_average_factor,\n          batch_mean_opaque, batch_var_opaque, epsilon, saved_mean->opaque(),\n          saved_inv_var->opaque()));\n    }\n  } else {\n    const void* maybe_inv_var = estimated_variance.opaque();\n    SE_RETURN_IF_ERROR(check_no_side_input_or_activation());\n    RETURN_IF_CUDNN_ERROR(cudnnBatchNormalizationForwardInference(\n        cudnn.handle(), mode, &one, &zero, x_descriptor.handle(), x.opaque(),\n        x_descriptor.handle(), y->opaque(), scale_offset_descriptor.handle(),\n        scale.opaque(), offset.opaque(), estimated_mean.opaque(), maybe_inv_var,\n        epsilon));\n  }\n  return port::Status::OK();\n}\n\nbool CudnnSupport::DoBatchNormalizationBackward(\n    Stream* stream, const DeviceMemory<float>& y_backprop,\n    const DeviceMemory<float>& x, const DeviceMemory<float>& scale,\n    const DeviceMemory<float>& mean, const DeviceMemory<float>& inv_var,\n    const dnn::BatchDescriptor& x_desc,\n    const dnn::BatchDescriptor& scale_offset_desc, const double epsilon,\n    DeviceMemory<float>* x_backprop, DeviceMemory<float>* scale_backprop,\n    DeviceMemory<float>* offset_backprop,\n    DeviceMemory<uint8>* reserve_space_data,\n    ScratchAllocator* workspace_allocator) {\n  return IsStatusOk(DoBatchNormalizationBackwardImpl(\n                        stream, CUDNN_DATA_FLOAT, CUDNN_DATA_FLOAT, y_backprop,\n                        x, scale, mean, inv_var, x_desc, scale_offset_desc,\n                        epsilon, x_backprop, scale_backprop, offset_backprop,\n                        reserve_space_data, workspace_allocator),\n                    /*report_error=*/true);\n}\n\nbool CudnnSupport::DoBatchNormalizationBackward(\n    Stream* stream, const DeviceMemory<Eigen::half>& y_backprop,\n    const DeviceMemory<Eigen::half>& x, const DeviceMemory<float>& scale,\n    const DeviceMemory<float>& mean, const DeviceMemory<float>& inv_var,\n    const dnn::BatchDescriptor& x_desc,\n    const dnn::BatchDescriptor& scale_offset_desc, const double epsilon,\n    DeviceMemory<Eigen::half>* x_backprop, DeviceMemory<float>* scale_backprop,\n    DeviceMemory<float>* offset_backprop,\n    DeviceMemory<uint8>* reserve_space_data,\n    ScratchAllocator* workspace_allocator) {\n  return IsStatusOk(DoBatchNormalizationBackwardImpl(\n                        stream, CUDNN_DATA_HALF, CUDNN_DATA_FLOAT, y_backprop,\n                        x, scale, mean, inv_var, x_desc, scale_offset_desc,\n                        epsilon, x_backprop, scale_backprop, offset_backprop,\n                        reserve_space_data, workspace_allocator),\n                    /*report_error=*/true);\n}\n\ntemplate <class T, class U>\nport::Status CudnnSupport::DoBatchNormalizationBackwardImpl(\n    Stream* stream, int cudnn_input_type, int cudnn_scale_type,\n    const DeviceMemory<T>& y_backprop, const DeviceMemory<T>& x,\n    const DeviceMemory<U>& scale, const DeviceMemory<U>& mean,\n    const DeviceMemory<U>& inv_var, const dnn::BatchDescriptor& x_desc,\n    const dnn::BatchDescriptor& scale_offset_desc, const double epsilon,\n    DeviceMemory<T>* x_backprop, DeviceMemory<U>* scale_backprop,\n    DeviceMemory<U>* offset_backprop, DeviceMemory<uint8>* reserve_space_data,\n    ScratchAllocator* workspace_allocator) {\n  CudnnTensorDescriptor x_descriptor(\n      x_desc, static_cast<cudnnDataType_t>(cudnn_input_type));\n  CudnnTensorDescriptor scale_offset_descriptor(\n      scale_offset_desc, static_cast<cudnnDataType_t>(cudnn_scale_type));\n  cudnnBatchNormMode_t mode = CUDNN_BATCHNORM_SPATIAL;\n  if (BatchnormSpatialPersistentEnabled()) {\n    mode = CUDNN_BATCHNORM_SPATIAL_PERSISTENT;\n  }\n  float one = 1.0;\n  float zero = 0.0;\n\n  auto cudnn = cudnn_->GetHandle(parent_, stream);\n\n  bool called = false;\n#if CUDNN_VERSION >= 7402\n  if (reserve_space_data != nullptr && workspace_allocator != nullptr) {\n    called = true;\n    const cudnnBatchNormOps_t bn_ops = CUDNN_BATCHNORM_OPS_BN;\n    SE_ASSIGN_OR_RETURN(DeviceMemory<uint8> workspace,\n                        CreateBatchNormBackwardWorkspace(\n                            stream, cudnn, mode, bn_ops, x_descriptor,\n                            scale_offset_descriptor, workspace_allocator))\n    RETURN_IF_CUDNN_ERROR(cudnnBatchNormalizationBackwardEx(\n        /*handle=*/cudnn.handle(),\n        /*mode=*/mode,\n        /*bnOps=*/bn_ops,\n        /*alphaDataDiff=*/&one,\n        /*betaDataDiff=*/&zero,\n        /*alphaParamDiff=*/&one,\n        /*betaParamDiff=*/&zero,\n        /*xDesc=*/x_descriptor.handle(),\n        /*xData=*/x.opaque(),\n        /*yDesc=*/nullptr,\n        /*yData=*/nullptr,\n        /*dyDesc=*/x_descriptor.handle(),\n        /*dyData=*/y_backprop.opaque(),\n        /*dzDesc=*/nullptr,\n        /*dzData=*/nullptr,\n        /*dxDesc=*/x_descriptor.handle(),\n        /*dxData=*/x_backprop->opaque(),\n        /*dBnScaleBiasDesc=*/scale_offset_descriptor.handle(),\n        /*bnScaleData=*/scale.opaque(),\n        /*bnBiasData=*/nullptr,\n        /*dBnScaleData=*/scale_backprop->opaque(),\n        /*dBnBiasData=*/offset_backprop->opaque(),\n        /*epsilon=*/epsilon,\n        /*savedMean=*/mean.opaque(),\n        /*savedInvVariance=*/inv_var.opaque(),\n        /*activationDesc=*/nullptr,\n        /*workspace=*/workspace.opaque(),\n        /*workSpaceSizeInBytes=*/workspace.size(),\n        /*reserveSpace=*/reserve_space_data->opaque(),\n        /*reserveSpaceSizeInBytes=*/reserve_space_data->size()));\n  }\n#endif\n  if (!called) {\n    RETURN_IF_CUDNN_ERROR(cudnnBatchNormalizationBackward(\n        cudnn.handle(), mode, &one, &zero, &one, &zero, x_descriptor.handle(),\n        x.opaque(), x_descriptor.handle(), y_backprop.opaque(),\n        x_descriptor.handle(), x_backprop->opaque(),\n        scale_offset_descriptor.handle(), scale.opaque(),\n        scale_backprop->opaque(), offset_backprop->opaque(), epsilon,\n        mean.opaque(), inv_var.opaque()));\n  }\n\n  return port::Status::OK();\n}\n\nport::Status CudnnSupport::DoFusedConvolve(\n    Stream* stream, const dnn::BatchDescriptor& conv_input_descriptor,\n    const DeviceMemory<double>& conv_input_data, double conv_input_scale,\n    const dnn::FilterDescriptor& filter_descriptor,\n    const DeviceMemory<double>& filter_data,\n    const dnn::ConvolutionDescriptor& convolution_descriptor,\n    const DeviceMemory<double>& side_input_data, double side_input_scale,\n    const dnn::BatchDescriptor& bias_descriptor,\n    const DeviceMemory<double>& biases, dnn::ActivationMode activation_mode,\n    const dnn::BatchDescriptor& output_descriptor,\n    DeviceMemory<double>* output_data, ScratchAllocator* scratch_allocator,\n    const dnn::AlgorithmConfig& algorithm_config,\n    dnn::ProfileResult* output_profile_result) {\n  return DoFusedConvolveImpl(\n      stream, conv_input_descriptor, conv_input_data, conv_input_scale,\n      filter_descriptor, filter_data, convolution_descriptor, side_input_data,\n      side_input_scale, bias_descriptor, biases, activation_mode,\n      output_descriptor, output_data,\n      GetConvAccumulatorType(dnn::DataType::kDouble), scratch_allocator,\n      algorithm_config, output_profile_result);\n}\n\nport::Status CudnnSupport::DoFusedConvolve(\n    Stream* stream, const dnn::BatchDescriptor& conv_input_descriptor,\n    const DeviceMemory<float>& conv_input_data, float conv_input_scale,\n    const dnn::FilterDescriptor& filter_descriptor,\n    const DeviceMemory<float>& filter_data,\n    const dnn::ConvolutionDescriptor& convolution_descriptor,\n    const DeviceMemory<float>& side_input_data, float side_input_scale,\n    const dnn::BatchDescriptor& bias_descriptor,\n    const DeviceMemory<float>& biases, dnn::ActivationMode activation_mode,\n    const dnn::BatchDescriptor& output_descriptor,\n    DeviceMemory<float>* output_data, ScratchAllocator* scratch_allocator,\n    const dnn::AlgorithmConfig& algorithm_config,\n    dnn::ProfileResult* output_profile_result) {\n  return DoFusedConvolveImpl(\n      stream, conv_input_descriptor, conv_input_data, conv_input_scale,\n      filter_descriptor, filter_data, convolution_descriptor, side_input_data,\n      side_input_scale, bias_descriptor, biases, activation_mode,\n      output_descriptor, output_data,\n      GetConvAccumulatorType(dnn::DataType::kFloat), scratch_allocator,\n      algorithm_config, output_profile_result);\n}\n\nport::Status CudnnSupport::DoFusedConvolve(\n    Stream* stream, const dnn::BatchDescriptor& conv_input_descriptor,\n    const DeviceMemory<Eigen::half>& conv_input_data, float conv_input_scale,\n    const dnn::FilterDescriptor& filter_descriptor,\n    const DeviceMemory<Eigen::half>& filter_data,\n    const dnn::ConvolutionDescriptor& convolution_descriptor,\n    const DeviceMemory<Eigen::half>& side_input_data, float side_input_scale,\n    const dnn::BatchDescriptor& bias_descriptor,\n    const DeviceMemory<Eigen::half>& biases,\n    dnn::ActivationMode activation_mode,\n    const dnn::BatchDescriptor& output_descriptor,\n    DeviceMemory<Eigen::half>* output_data, ScratchAllocator* scratch_allocator,\n    const dnn::AlgorithmConfig& algorithm_config,\n    dnn::ProfileResult* output_profile_result) {\n  return DoFusedConvolveImpl(\n      stream, conv_input_descriptor, conv_input_data, conv_input_scale,\n      filter_descriptor, filter_data, convolution_descriptor, side_input_data,\n      side_input_scale, bias_descriptor, biases, activation_mode,\n      output_descriptor, output_data,\n      GetConvAccumulatorType(dnn::DataType::kHalf), scratch_allocator,\n      algorithm_config, output_profile_result);\n}\n\nport::Status CudnnSupport::DoFusedConvolve(\n    Stream* stream, const dnn::BatchDescriptor& conv_input_descriptor,\n    const DeviceMemory<int8>& conv_input_data, float conv_input_scale,\n    const dnn::FilterDescriptor& filter_descriptor,\n    const DeviceMemory<int8>& filter_data,\n    const dnn::ConvolutionDescriptor& convolution_descriptor,\n    const DeviceMemory<int8>& side_input_data, float side_input_scale,\n    const dnn::BatchDescriptor& bias_descriptor,\n    const DeviceMemory<float>& biases, dnn::ActivationMode activation_mode,\n    const dnn::BatchDescriptor& output_descriptor,\n    DeviceMemory<int8>* output_data, ScratchAllocator* scratch_allocator,\n    const dnn::AlgorithmConfig& algorithm_config,\n    dnn::ProfileResult* output_profile_result) {\n  int cc_major, cc_minor;\n  std::tie(cc_major, cc_minor) = GetCcMajorMinor(stream);\n\n  if (cc_major < 6 || (cc_major == 6 && cc_minor < 1)) {\n    return port::UnimplementedError(\n        \"cudnnConvolutionBiasActivationForward() for int8 is only supported on \"\n        \"GPUs with compute capability 6.1 or later.\");\n  }\n\n  return DoFusedConvolveImpl(\n      stream, conv_input_descriptor, conv_input_data, conv_input_scale,\n      filter_descriptor, filter_data, convolution_descriptor, side_input_data,\n      side_input_scale, bias_descriptor, biases, activation_mode,\n      output_descriptor, output_data,\n      GetConvAccumulatorType(dnn::DataType::kInt8), scratch_allocator,\n      algorithm_config, output_profile_result);\n}\n\nport::Status CudnnSupport::DoFusedConvolve(\n    Stream* stream, const dnn::BatchDescriptor& conv_input_descriptor,\n    const DeviceMemory<int8>& conv_input_data, float conv_input_scale,\n    const dnn::FilterDescriptor& filter_descriptor,\n    const DeviceMemory<int8>& filter_data,\n    const dnn::ConvolutionDescriptor& convolution_descriptor,\n    const DeviceMemory<float>& side_input_data, float side_input_scale,\n    const dnn::BatchDescriptor& bias_descriptor,\n    const DeviceMemory<float>& biases, dnn::ActivationMode activation_mode,\n    const dnn::BatchDescriptor& output_descriptor,\n    DeviceMemory<float>* output_data, ScratchAllocator* scratch_allocator,\n    const dnn::AlgorithmConfig& algorithm_config,\n    dnn::ProfileResult* output_profile_result) {\n  int cc_major, cc_minor;\n  stream->parent()->GetDeviceDescription().cuda_compute_capability(&cc_major,\n                                                                   &cc_minor);\n  if (cc_major < 6 || (cc_major == 6 && cc_minor < 1)) {\n    return port::UnimplementedError(\n        \"cudnnConvolutionBiasActivationForward() for int8 is only supported on \"\n        \"GPUs with compute capability 6.1 or later.\");\n  }\n\n  return DoFusedConvolveImpl(\n      stream, conv_input_descriptor, conv_input_data, conv_input_scale,\n      filter_descriptor, filter_data, convolution_descriptor, side_input_data,\n      side_input_scale, bias_descriptor, biases, activation_mode,\n      output_descriptor, output_data,\n      GetConvAccumulatorType(dnn::DataType::kInt8), scratch_allocator,\n      algorithm_config, output_profile_result);\n}\n\nport::Status CudnnSupport::DoPrepareForCtcLoss(\n    Stream* stream, dnn::DataType element_type,\n    const dnn::RnnStateTensorDescriptor& probs_desc,\n    const dnn::RnnStateTensorDescriptor& grads_desc,\n    absl::Span<const int> labels_data,\n    absl::Span<const int> labels_lengths_data,\n    absl::Span<const int> input_lengths_data,\n    ScratchAllocator* scratch_allocator, DeviceMemory<uint8>* scratch_memory,\n    int* ctc_loss_algo_id) {\n  auto cudnn = cudnn_->GetHandle(parent_, stream);\n  // Query the workspace size.\n  size_t workspace_size_in_bytes = 0;\n#if CUDNN_VERSION >= 7603\n  CudnnCtcLossDescriptor cudnn_ctc_loss_desc(ToCudnnDataType(element_type));\n  const CudnnRnnStateTensorDescriptor& cudnn_probs_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(probs_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_grads_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(grads_desc);\n\n  // Try running with `algo`, if successful then pick it. The non-deterministic\n  // algorithm is first and thus preferentially picked when determinism is not\n  // required.\n  auto algo = RequireCudnnDeterminism() ? CUDNN_CTC_LOSS_ALGO_DETERMINISTIC\n                                        : CUDNN_CTC_LOSS_ALGO_NON_DETERMINISTIC;\n  cudnnStatus_t status = cudnnGetCTCLossWorkspaceSize(\n      /*handle=*/cudnn.handle(), /*probsDesc=*/cudnn_probs_desc.handle(),\n      /*gradientsDesc=*/cudnn_grads_desc.handle(),\n      /*labels=*/labels_data.data(),\n      /*labelLengths=*/labels_lengths_data.data(),\n      /*inputLengths=*/input_lengths_data.data(),\n      /*algo=*/algo,\n      /*ctcLossDesc=*/cudnn_ctc_loss_desc.handle(),\n      /*sizeInBytes=*/&workspace_size_in_bytes);\n  if (RequireCudnnDeterminism()) {\n    RETURN_IF_CUDNN_ERROR(status);\n  }\n\n  if (status != CUDNN_STATUS_SUCCESS) {\n    algo = CUDNN_CTC_LOSS_ALGO_DETERMINISTIC;\n    RETURN_IF_CUDNN_ERROR(cudnnGetCTCLossWorkspaceSize(\n        /*handle=*/cudnn.handle(), /*probsDesc=*/cudnn_probs_desc.handle(),\n        /*gradientsDesc=*/cudnn_grads_desc.handle(),\n        /*labels=*/labels_data.data(),\n        /*labelLengths=*/labels_lengths_data.data(),\n        /*inputLengths=*/input_lengths_data.data(),\n        /*algo=*/algo,\n        /*ctcLossDesc=*/cudnn_ctc_loss_desc.handle(),\n        /*sizeInBytes=*/&workspace_size_in_bytes));\n  }\n  *ctc_loss_algo_id = algo;\n#else\n  return port::Status(port::error::INVALID_ARGUMENT,\n                      \"No supported cudnnGetCTCLossWorkspaceSize when \"\n                      \"CUDNN_VERSION < 7.6.3\");\n#endif\n  // Allocate the workspace.\n  if (workspace_size_in_bytes == 0) {\n    *scratch_memory = DeviceMemory<uint8>();\n    return port::Status::OK();\n  }\n  const auto scratch_or =\n      scratch_allocator->AllocateBytes(workspace_size_in_bytes);\n  if (scratch_or.ok()) {\n    *scratch_memory = scratch_or.ValueOrDie();\n    return port::Status::OK();\n  }\n  return port::InternalError(\n      \"Failed to allocate scratch memory for the CuDNN CTC Loss\");\n}\n\nport::Status CudnnSupport::DoCtcLoss(\n    Stream* stream, dnn::DataType element_type,\n    const dnn::RnnStateTensorDescriptor& probs_desc,\n    const DeviceMemoryBase probs_data, absl::Span<const int> labels_data,\n    absl::Span<const int> labels_lengths_data,\n    absl::Span<const int> input_lengths_data, DeviceMemoryBase costs_data,\n    const dnn::RnnStateTensorDescriptor& grads_desc,\n    DeviceMemoryBase grads_data, DeviceMemory<uint8> scratch_memory,\n    int ctc_loss_algo_id) {\n  // Current cuDNN CTC Loss only supports the float datatype\n  if (CUDNN_VERSION < 7603 || element_type != dnn::DataType::kFloat) {\n    return port::Status(port::error::INVALID_ARGUMENT,\n                        \"CudnnCtcLossDescriptor is supported only when the \"\n                        \"CUDNN_VERSION >= 7.6.3 and DataType is float\");\n  }\n  CudnnCtcLossDescriptor cudnn_ctc_loss_desc(ToCudnnDataType(element_type));\n  const CudnnRnnStateTensorDescriptor& cudnn_probs_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(probs_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_grads_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(grads_desc);\n  return DoCtcLossImpl(stream, cudnn_probs_desc, probs_data, labels_data,\n                       labels_lengths_data, input_lengths_data, costs_data,\n                       cudnn_grads_desc, grads_data, cudnn_ctc_loss_desc,\n                       scratch_memory, ctc_loss_algo_id);\n}\n\nbool CudnnSupport::DoTransformTensor(Stream* stream,\n                                     const dnn::BatchDescriptor& input_desc,\n                                     dnn::DataType input_type,\n                                     const DeviceMemoryBase& input_data,\n                                     const dnn::BatchDescriptor& output_desc,\n                                     dnn::DataType output_type, float scale,\n                                     DeviceMemoryBase* output_data) {\n  float beta = 0.0f;\n  CudnnTensorDescriptor input_tensor_desc(\n      input_desc, ToCudnnDataType(input_type, input_desc.layout()));\n  CudnnTensorDescriptor output_tensor_desc(\n      output_desc, ToCudnnDataType(output_type, output_desc.layout()));\n  auto cudnn = cudnn_->GetHandle(parent_, stream);\n  const auto status = [&] {\n    RETURN_IF_CUDNN_ERROR(cudnnTransformTensor(\n        cudnn.handle(), &scale, input_tensor_desc.handle(), input_data.opaque(),\n        &beta, output_tensor_desc.handle(), output_data->opaque()));\n    return port::Status::OK();\n  }();\n  return IsStatusOk(status, /*report_error=*/true);\n}\n\ntemplate <class T>\nport::Status CudnnSupport::DoConvolveBackwardBiasImpl(\n    Stream* stream, const dnn::BatchDescriptor& input_descriptor,\n    const DeviceMemory<T>& input_data,\n    const dnn::BatchDescriptor& bias_descriptor,\n    DeviceMemory<T>* backward_bias_data) {\n  cudnnDataType_t cudnn_type = GetCudnnDataType<T>();\n  CudnnTensorDescriptor input_nd(input_descriptor, cudnn_type);\n  CudnnTensorDescriptor bias_nd(bias_descriptor, cudnn_type);\n\n  // Alpha is the scaling factor for input.\n  float alpha = 1.0;\n  // Beta is the scaling factor for output.\n  float beta = 0.0;\n\n  auto cudnn = cudnn_->GetHandle(parent_, stream);\n  RETURN_IF_CUDNN_ERROR(cudnnConvolutionBackwardBias(\n      cudnn.handle(), &alpha, input_nd.handle(), input_data.opaque(), &beta,\n      bias_nd.handle(), backward_bias_data->opaque()));\n  return port::Status::OK();\n}\n\nbool CudnnSupport::DoConvolveBackwardBias(\n    Stream* stream, const dnn::BatchDescriptor& input_descriptor,\n    const DeviceMemory<double>& input_data,\n    const dnn::BatchDescriptor& bias_descriptor,\n    DeviceMemory<double>* backward_bias_data) {\n  return IsStatusOk(\n      DoConvolveBackwardBiasImpl(stream, input_descriptor, input_data,\n                                 bias_descriptor, backward_bias_data),\n      /*report_error=*/true);\n}\n\nbool CudnnSupport::DoConvolveBackwardBias(\n    Stream* stream, const dnn::BatchDescriptor& input_descriptor,\n    const DeviceMemory<float>& input_data,\n    const dnn::BatchDescriptor& bias_descriptor,\n    DeviceMemory<float>* backward_bias_data) {\n  return IsStatusOk(\n      DoConvolveBackwardBiasImpl(stream, input_descriptor, input_data,\n                                 bias_descriptor, backward_bias_data),\n      /*report_error=*/true);\n}\n\nbool CudnnSupport::DoConvolveBackwardBias(\n    Stream* stream, const dnn::BatchDescriptor& input_descriptor,\n    const DeviceMemory<Eigen::half>& input_data,\n    const dnn::BatchDescriptor& bias_descriptor,\n    DeviceMemory<Eigen::half>* backward_bias_data) {\n  return IsStatusOk(\n      DoConvolveBackwardBiasImpl(stream, input_descriptor, input_data,\n                                 bias_descriptor, backward_bias_data),\n      /*report_error=*/true);\n}\n\nbool CudnnSupport::DoMatMul(Stream* stream,\n                            const DeviceMemory<float>& input_data,\n                            const DeviceMemory<float>& weights,\n                            const dnn::BatchDescriptor& input_dimensions,\n                            const dnn::BatchDescriptor& output_dimensions,\n                            DeviceMemory<float>* output_data) {\n  if (input_dimensions.count() != output_dimensions.count()) {\n    LOG(ERROR) << \"MatMul input and output dimensions are not compatible.\";\n    return false;\n  }\n\n  // We do not permute the input or output, instead we just\n  // reinterpret the layout. We are working with row-major matrices\n  // and the rows of the input and output correspond to batch, so\n  // batch has to be outermost in both the input and output.\n  //\n  // By adding transposes to the BLAS gemm call we could perhaps make\n  // the kYXDepthBatch layout work as well, but there has been no need\n  // for that so far.\n  if (input_dimensions.layout() != dnn::DataLayout::kBatchYXDepth &&\n      input_dimensions.layout() != dnn::DataLayout::kBatchDepthYX) {\n    LOG(ERROR) << \"Unsupported MatMul input layout.\";\n    return false;\n  }\n  if (output_dimensions.layout() != dnn::DataLayout::kBatchYXDepth &&\n      output_dimensions.layout() != dnn::DataLayout::kBatchDepthYX) {\n    LOG(ERROR) << \"Unsupported MatMul output layout.\";\n    return false;\n  }\n\n  if (output_dimensions.width() == 1 && output_dimensions.height() == 1) {\n    // This is a fast path that also supports the kBatchYXDepth layout.\n\n    // The matrices here are in row-major format while BLAS expects\n    // column-major, i.e. our matrices are transposed as far as BLAS\n    // is concerned. So we need to compute output^T =\n    // input^T*weights^T. There is no parameter for transposing the\n    // output in BLAS gemm, but instead we can transpose both sides of\n    // the equality to see that this is equivalent to\n    // output=weights*input. So we only need to swap the order of\n    // weights and input in the matrix product to correct for the\n    // row-major versus column-major difference.\n    const float alpha = 1.0f;  // Take the matrix product without scaling it.\n    const float beta = 0.0f;   // Ignore the original values in output_data.\n    const int64 m = output_dimensions.NodesAcrossFeatureMaps();\n    const int64 n = input_dimensions.count();\n    const int64 k = input_dimensions.NodesAcrossFeatureMaps();\n    stream->ThenBlasGemm(blas::Transpose::kNoTranspose,\n                         blas::Transpose::kNoTranspose, m, n, k, alpha, weights,\n                         m, input_data, k, beta, output_data, m);\n  } else {\n    // This is a slower and more complex path that supports output\n    // width() * height() > 1, though it only supports the\n    // kBatchYXDepth layout. Does support kBatchDepthYX if output\n    // feature_map_count() == 1, as then there is no difference\n    // between the two layouts.\n    //\n    // The operation here is the same as above, except that we have to\n    // do the matrix multiplication for each (y,x) output coordinate\n    // separately. We then interpret weights as containing K = width()\n    // * height() different matrices, which we all multiply onto the\n    // matrix from input_data, yielding K matrix products. We then\n    // combine these together into one matrix by concatenating all the\n    // first rows of these matrices, then all the seconds rows and so\n    // on. We can do this with a batched matrix multiplication, where\n    // the result is written to a different submatrix of the output\n    // for each matrix multiplication.\n    //\n    // The reason that we only support the kBatchYXDepth output layout\n    // is that we have to do something in the depth for each (y,x)\n    // coordinate. The kBatchYXDepth layout has the depth information\n    // for each point (y,x) in contiguous memory while the\n    // kBatchDepthYX layout does not.\n    //\n    // TODO(broune): Consider a special case for when output depth ==\n    // 1, as then possibly this could all be done as one matrix\n    // multiplication instead of a batched one, which should be\n    // faster. Another possibility would be to add a weights layout\n    // parameter and then support kBatchDepthYX for a different\n    // weights layout.\n    if (output_dimensions.layout() != dnn::DataLayout::kBatchYXDepth &&\n        !(output_dimensions.layout() == dnn::DataLayout::kBatchDepthYX &&\n          output_dimensions.feature_map_count() == 1)) {\n      LOG(ERROR) << \"Unsupported MatMul output layout.\";\n      return false;\n    }\n\n    const float alpha = 1.0f;  // Take the matrix product without scaling it.\n    const float beta = 0.0f;   // Ignore the original values in output_data.\n    const uint64 m = output_dimensions.feature_map_count();\n    const uint64 n = input_dimensions.count();\n    const uint64 k = input_dimensions.NodesAcrossFeatureMaps();\n    const int lda = m;\n    const int ldb = k;\n    const int ldc = output_dimensions.NodesAcrossFeatureMaps();\n    const int batch_count = output_dimensions.NodesPerFeatureMap();\n\n    std::vector<DeviceMemory<float>> a(batch_count);\n    std::vector<DeviceMemory<float>> b(batch_count);\n    std::vector<DeviceMemory<float>> c(batch_count);\n    for (int i = 0; i < batch_count; ++i) {\n      const int weights_offset = i * input_dimensions.NodesAcrossFeatureMaps() *\n                                 output_dimensions.feature_map_count();\n      a[i] = DeviceMemory<float>::MakeFromByteSize(\n          const_cast<float*>(reinterpret_cast<const float*>(weights.opaque())) +\n              weights_offset,\n          weights.ElementCount() - weights_offset);\n\n      b[i] = input_data;\n\n      const int output_offset = i * output_dimensions.feature_map_count();\n      c[i] = DeviceMemory<float>::MakeFromByteSize(\n          const_cast<float*>(\n              reinterpret_cast<const float*>(output_data->opaque())) +\n              output_offset,\n          output_data->ElementCount() - output_offset);\n    }\n    const auto toPtrs = [](std::vector<DeviceMemory<float>>& v) {\n      std::vector<DeviceMemory<float>*> ptrs;\n      ptrs.reserve(v.size());\n      for (auto& mem : v) {\n        ptrs.push_back(&mem);\n      }\n      return ptrs;\n    };\n\n    stream->ThenBlasGemmBatched(blas::Transpose::kNoTranspose,\n                                blas::Transpose::kNoTranspose, m, n, k, alpha,\n                                toPtrs(a), lda, toPtrs(b), ldb, beta, toPtrs(c),\n                                ldc, batch_count);\n  }\n\n  return stream->ok();\n}\n\nbool CudnnSupport::DoBiasAdd(Stream* stream,\n                             const DeviceMemory<float>& input_data,\n                             const DeviceMemory<float>& biases,\n                             const dnn::BatchDescriptor& dimensions,\n                             DeviceMemory<float>* output_data) {\n  CudnnTensorDescriptor input_descriptor(dimensions, CUDNN_DATA_FLOAT);\n\n  dnn::BatchDescriptor bias_dimensions;\n  bias_dimensions.set_count(1)\n      .set_feature_map_count(dimensions.feature_map_count())\n      .set_height(1)\n      .set_width(1)\n      .set_layout(dnn::DataLayout::kBatchYXDepth);\n  CudnnTensorDescriptor bias_descriptor(bias_dimensions, CUDNN_DATA_FLOAT);\n\n  // cudnnAddTensor after R3 is in-place, so we need to copy input_data to\n  // output_data before doing the addition, unless the input and\n  // output are at the same address.\n  if (input_data.opaque() != output_data->opaque()) {\n    stream->ThenMemcpy(output_data, input_data,\n                       dimensions.ElementCount() * sizeof(float));\n    if (!stream->ok()) {\n      LOG(ERROR)\n          << \"stream \" << stream\n          << \" could not enqueue a tensor copy as part of bias addition.\";\n      return false;\n    }\n  }\n\n  const float alpha = 1.0f;\n  const float beta = 1.0f;\n\n  auto cudnn = cudnn_->GetHandle(parent_, stream);\n\n  const auto status = [&] {\n    RETURN_IF_CUDNN_ERROR(cudnnAddTensor(\n        cudnn.handle(), &alpha, bias_descriptor.handle(), biases.opaque(),\n        &beta, input_descriptor.handle(), output_data->opaque()));\n    return port::Status::OK();\n  }();\n  return IsStatusOk(status, /*report_error=*/true);\n}\n\nbool CudnnSupport::DoActivate(Stream* stream,\n                              dnn::ActivationMode activation_mode,\n                              const dnn::BatchDescriptor& dimensions,\n                              const DeviceMemory<float>& input_data,\n                              DeviceMemory<float>* output_data,\n                              uint64 options) {\n  CudnnActivationDescriptor activation_desc(\n      activation_mode, CUDNN_PROPAGATE_NAN, dimensions.value_max());\n\n  CudnnTensorDescriptor input_nd(dimensions, CUDNN_DATA_FLOAT);\n  // Alpha is the input scaling factor.\n  float alpha = 1.0;\n  // Beta is the output scaling factor.\n  float beta = 0.0;\n\n  auto cudnn = cudnn_->GetHandle(parent_, stream);\n  const auto status = [&] {\n    RETURN_IF_CUDNN_ERROR(cudnnActivationForward(\n        cudnn.handle(), activation_desc.handle(), &alpha, input_nd.handle(),\n        input_data.opaque(), &beta, input_nd.handle(), output_data->opaque()));\n    return port::Status::OK();\n  }();\n  return IsStatusOk(status, /*report_error=*/true);\n}\n\nbool CudnnSupport::DoPoolForward(\n    Stream* stream, const dnn::PoolingDescriptor& pooling_dimensions,\n    const dnn::BatchDescriptor& input_dimensions,\n    const DeviceMemory<double>& input_data,\n    const dnn::BatchDescriptor& output_dimensions,\n    DeviceMemory<double>* output_data, ScratchAllocator* workspace_allocator) {\n  // Alpha is the scaling factor for input.\n  double alpha = 1.0;\n  // Beta is the scaling factor for output.\n  double beta = 0.0;\n\n  CudnnTensorDescriptor src_desc(input_dimensions, CUDNN_DATA_DOUBLE);\n  CudnnTensorDescriptor dest_desc(output_dimensions, CUDNN_DATA_DOUBLE);\n  CudnnPoolingDescriptor pooling_desc(pooling_dimensions);\n\n  auto cudnn = cudnn_->GetHandle(parent_, stream);\n  const auto status = [&] {\n    RETURN_IF_CUDNN_ERROR(cudnnPoolingForward(\n        cudnn.handle(), pooling_desc.handle(), &alpha, src_desc.handle(),\n        input_data.opaque(), &beta, dest_desc.handle(), output_data->opaque()));\n    return port::Status::OK();\n  }();\n  return IsStatusOk(status, /*report_error=*/true);\n}\n\nbool CudnnSupport::DoPoolForward(\n    Stream* stream, const dnn::PoolingDescriptor& pooling_dimensions,\n    const dnn::BatchDescriptor& input_dimensions,\n    const DeviceMemory<float>& input_data,\n    const dnn::BatchDescriptor& output_dimensions,\n    DeviceMemory<float>* output_data, ScratchAllocator* workspace_allocator) {\n  // Alpha is the scaling factor for input.\n  float alpha = 1.0;\n  // Beta is the scaling factor for output.\n  float beta = 0.0;\n\n  CudnnTensorDescriptor src_desc(input_dimensions, CUDNN_DATA_FLOAT);\n  CudnnTensorDescriptor dest_desc(output_dimensions, CUDNN_DATA_FLOAT);\n  CudnnPoolingDescriptor pooling_desc(pooling_dimensions);\n\n  auto cudnn = cudnn_->GetHandle(parent_, stream);\n  const auto status = [&] {\n    RETURN_IF_CUDNN_ERROR(cudnnPoolingForward(\n        cudnn.handle(), pooling_desc.handle(), &alpha, src_desc.handle(),\n        input_data.opaque(), &beta, dest_desc.handle(), output_data->opaque()));\n    return port::Status::OK();\n  }();\n  return IsStatusOk(status, /*report_error=*/true);\n}\n\nbool CudnnSupport::DoPoolForward(\n    Stream* stream, const dnn::PoolingDescriptor& pooling_dimensions,\n    const dnn::BatchDescriptor& input_dimensions,\n    const DeviceMemory<Eigen::half>& input_data,\n    const dnn::BatchDescriptor& output_dimensions,\n    DeviceMemory<Eigen::half>* output_data,\n    ScratchAllocator* workspace_allocator) {\n  // Alpha is the scaling factor for input.\n  float alpha = 1.0;\n  // Beta is the scaling factor for output.\n  float beta = 0.0;\n\n  CudnnTensorDescriptor src_desc(input_dimensions, CUDNN_DATA_HALF);\n  CudnnTensorDescriptor dest_desc(output_dimensions, CUDNN_DATA_HALF);\n  CudnnPoolingDescriptor pooling_desc(pooling_dimensions);\n  auto cudnn = cudnn_->GetHandle(parent_, stream);\n  const auto status = [&] {\n    RETURN_IF_CUDNN_ERROR(cudnnPoolingForward(\n        cudnn.handle(), pooling_desc.handle(), &alpha, src_desc.handle(),\n        input_data.opaque(), &beta, dest_desc.handle(), output_data->opaque()));\n    return port::Status::OK();\n  }();\n  return IsStatusOk(status, /*report_error=*/true);\n}\n\nbool CudnnSupport::DoPoolForward(\n    Stream* stream, const dnn::PoolingDescriptor& pooling_dimensions,\n    const dnn::BatchDescriptor& input_dimensions,\n    const DeviceMemory<int8>& input_data,\n    const dnn::BatchDescriptor& output_dimensions,\n    DeviceMemory<int8>* output_data, ScratchAllocator* workspace_allocator) {\n  // Alpha is the scaling factor for input.\n  float alpha = 1.0;\n  // Beta is the scaling factor for output.\n  float beta = 0.0;\n\n  CudnnTensorDescriptor src_desc(input_dimensions, CUDNN_DATA_INT8);\n  CudnnTensorDescriptor dest_desc(output_dimensions, CUDNN_DATA_INT8);\n  CudnnPoolingDescriptor pooling_desc(pooling_dimensions);\n\n  auto cudnn = cudnn_->GetHandle(parent_, stream);\n  const auto status = [&] {\n    RETURN_IF_CUDNN_ERROR(cudnnPoolingForward(\n        cudnn.handle(), pooling_desc.handle(), &alpha, src_desc.handle(),\n        input_data.opaque(), &beta, dest_desc.handle(), output_data->opaque()));\n    return port::Status::OK();\n  }();\n  return IsStatusOk(status, /*report_error=*/true);\n}\n\nbool CudnnSupport::DoPoolBackward(\n    Stream* stream, const dnn::PoolingDescriptor& pooling_dimensions,\n    const dnn::BatchDescriptor& input_dimensions,\n    const DeviceMemory<double>& input_data,\n    const dnn::BatchDescriptor& output_dimensions,\n    const DeviceMemory<double>& output_data,\n    const DeviceMemory<double>& input_diff_data,\n    DeviceMemory<double>* output_diff_data,\n    ScratchAllocator* workspace_allocator) {\n  // Alpha is the scaling factor for input.\n  double alpha = 1.0;\n  // Beta is the scaling factor for output.\n  double beta = 0.0;\n\n  CudnnTensorDescriptor src_desc(input_dimensions, CUDNN_DATA_DOUBLE);\n  CudnnTensorDescriptor dest_desc(output_dimensions, CUDNN_DATA_DOUBLE);\n  CudnnPoolingDescriptor pooling_desc(pooling_dimensions);\n\n  auto cudnn = cudnn_->GetHandle(parent_, stream);\n  const auto status = [&] {\n    RETURN_IF_CUDNN_ERROR(cudnnPoolingBackward(\n        cudnn.handle(), pooling_desc.handle(), &alpha, dest_desc.handle(),\n        output_data.opaque(), dest_desc.handle(), input_diff_data.opaque(),\n        src_desc.handle(), input_data.opaque(), &beta, src_desc.handle(),\n        output_diff_data->opaque()));\n    return port::Status::OK();\n  }();\n  return IsStatusOk(status, /*report_error=*/true);\n}\n\nbool CudnnSupport::DoPoolBackward(\n    Stream* stream, const dnn::PoolingDescriptor& pooling_dimensions,\n    const dnn::BatchDescriptor& input_dimensions,\n    const DeviceMemory<float>& input_data,\n    const dnn::BatchDescriptor& output_dimensions,\n    const DeviceMemory<float>& output_data,\n    const DeviceMemory<float>& input_diff_data,\n    DeviceMemory<float>* output_diff_data,\n    ScratchAllocator* workspace_allocator) {\n  // Alpha is the scaling factor for input.\n  float alpha = 1.0;\n  // Beta is the scaling factor for output.\n  float beta = 0.0;\n\n  CudnnTensorDescriptor src_desc(input_dimensions, CUDNN_DATA_FLOAT);\n  CudnnTensorDescriptor dest_desc(output_dimensions, CUDNN_DATA_FLOAT);\n  CudnnPoolingDescriptor pooling_desc(pooling_dimensions);\n\n  auto cudnn = cudnn_->GetHandle(parent_, stream);\n  const auto status = [&] {\n    RETURN_IF_CUDNN_ERROR(cudnnPoolingBackward(\n        cudnn.handle(), pooling_desc.handle(), &alpha, dest_desc.handle(),\n        output_data.opaque(), dest_desc.handle(), input_diff_data.opaque(),\n        src_desc.handle(), input_data.opaque(), &beta, src_desc.handle(),\n        output_diff_data->opaque()));\n    return port::Status::OK();\n  }();\n  return IsStatusOk(status, /*report_error=*/true);\n}\n\nbool CudnnSupport::DoPoolBackward(\n    Stream* stream, const dnn::PoolingDescriptor& pooling_dimensions,\n    const dnn::BatchDescriptor& input_dimensions,\n    const DeviceMemory<Eigen::half>& input_data,\n    const dnn::BatchDescriptor& output_dimensions,\n    const DeviceMemory<Eigen::half>& output_data,\n    const DeviceMemory<Eigen::half>& input_diff_data,\n    DeviceMemory<Eigen::half>* output_diff_data,\n    ScratchAllocator* workspace_allocator) {\n  // Alpha is the scaling factor for input.\n  float alpha = 1.0;\n  // Beta is the scaling factor for output.\n  float beta = 0.0;\n\n  CudnnTensorDescriptor src_desc(input_dimensions, CUDNN_DATA_HALF);\n  CudnnTensorDescriptor dest_desc(output_dimensions, CUDNN_DATA_HALF);\n  CudnnPoolingDescriptor pooling_desc(pooling_dimensions);\n\n  auto cudnn = cudnn_->GetHandle(parent_, stream);\n  const auto status = [&] {\n    RETURN_IF_CUDNN_ERROR(cudnnPoolingBackward(\n        cudnn.handle(), pooling_desc.handle(), &alpha, dest_desc.handle(),\n        output_data.opaque(), dest_desc.handle(), input_diff_data.opaque(),\n        src_desc.handle(), input_data.opaque(), &beta, src_desc.handle(),\n        output_diff_data->opaque()));\n    return port::Status::OK();\n  }();\n  return IsStatusOk(status, /*report_error=*/true);\n}\n\nbool CudnnSupport::DoNormalizeWithDimensions(\n    Stream* stream, const dnn::NormalizeDescriptor& normalize_descriptor,\n    const dnn::BatchDescriptor& dimensions,\n    const DeviceMemory<float>& input_data, DeviceMemory<float>* output_data) {\n  // Check for unsupported modes.\n  if (normalize_descriptor.wrap_around()) {\n    LOG(ERROR) << \"CUDA LRN does not support cudnn-around mode\";\n    return false;\n  }\n  if (normalize_descriptor.segment_size()) {\n    LOG(ERROR) << \"CUDA LRN does not support segmentation\";\n    return false;\n  }\n\n  CudnnTensorDescriptor dims(dimensions, CUDNN_DATA_FLOAT);\n  CudnnNormalizeDescriptor normalize(normalize_descriptor);\n\n  // Alpha is the scaling factor for input.\n  float alpha = 1.0f;\n  // Beta is the scaling factor for output.\n  float beta = 0.0f;\n\n  auto cudnn = cudnn_->GetHandle(parent_, stream);\n\n  // Launch the normalization.\n  const auto status = [&] {\n    RETURN_IF_CUDNN_ERROR(cudnnLRNCrossChannelForward(\n        cudnn.handle(), normalize.handle(), CUDNN_LRN_CROSS_CHANNEL_DIM1,\n        &alpha, dims.handle(), input_data.opaque(), &beta, dims.handle(),\n        output_data->opaque()));\n    return port::Status::OK();\n  }();\n  return IsStatusOk(status, /*report_error=*/true);\n}\n\nbool CudnnSupport::DoNormalizeBackwardWithDimensions(\n    Stream* stream, const dnn::NormalizeDescriptor& normalize_descriptor,\n    const dnn::BatchDescriptor& dimensions, const DeviceMemory<float>& raw_data,\n    const DeviceMemory<float>& normalized_data,\n    const DeviceMemory<float>& normalized_variable_gradient,\n    DeviceMemory<float>* raw_variable_gradient,\n    ScratchAllocator* workspace_allocator) {\n  // Check for unsupported modes.\n  if (normalize_descriptor.wrap_around()) {\n    LOG(ERROR) << \"CUDA LRN does not support cudnn-around mode\";\n    return false;\n  }\n  if (normalize_descriptor.segment_size()) {\n    LOG(ERROR) << \"CUDA LRN does not support segmentation\";\n    return false;\n  }\n\n  CudnnTensorDescriptor dims(dimensions, CUDNN_DATA_FLOAT);\n  CudnnNormalizeDescriptor normalize(normalize_descriptor);\n\n  float alpha = 1.0f;\n  float beta = 0.0f;\n\n  auto cudnn = cudnn_->GetHandle(parent_, stream);\n  const auto status = [&] {\n    RETURN_IF_CUDNN_ERROR(cudnnLRNCrossChannelBackward(\n        cudnn.handle(), normalize.handle(), CUDNN_LRN_CROSS_CHANNEL_DIM1,\n        &alpha, dims.handle(), normalized_data.opaque(), dims.handle(),\n        normalized_variable_gradient.opaque(), dims.handle(), raw_data.opaque(),\n        &beta, dims.handle(), raw_variable_gradient->opaque()));\n    return port::Status::OK();\n  }();\n  return IsStatusOk(status, /*report_error=*/true);\n}\n\nbool CudnnSupport::DoDepthConcatenate(\n    Stream* stream, port::ArraySlice<dnn::BatchDescriptor> input_dimensions,\n    port::ArraySlice<const DeviceMemory<float>*> input_data,\n    DeviceMemory<float>* output_data) {\n  CHECK_EQ(input_dimensions.size(), input_data.size());\n\n  for (const auto& dimensions : input_dimensions) {\n    if (dimensions.layout() != dnn::DataLayout::kBatchDepthYX) {\n      LOG(ERROR) << \"CudnnSupport::DoDepthConcatenate currently only \"\n                    \"supports the kBatchDepthYX layout.\";\n      return false;\n    }\n  }\n\n  if (input_dimensions.empty()) {\n    return true;  // Nothing to do.\n  }\n\n  dnn::BatchDescriptor output_dimensions =\n      dnn::BatchDescriptor::DepthConcatenateOutputDescriptor(input_dimensions);\n\n  const int64 area = output_dimensions.width() * output_dimensions.height();\n  const auto index = [area](int64 batch, int64 depth, int64 yx,\n                            int64 max_depth) {\n    return (batch * max_depth + depth) * area + yx;\n  };\n\n  std::vector<float> output_host(output_dimensions.ElementCount());\n  std::vector<float> tmp;\n  int64 depth_sum = 0;\n  for (size_t i = 0; i < input_data.size(); ++i) {\n    const auto& dimensions = input_dimensions[i];\n    tmp.resize(dimensions.ElementCount());\n    stream->ThenMemcpyD2H<float>(*input_data[i], absl::MakeSpan(tmp));\n    port::Status block_status = stream->BlockHostUntilDone();\n    if (!block_status.ok()) {\n      LOG(ERROR) << \"BlockHostUntilDone failed: \" << block_status;\n      return false;\n    }\n\n    for (int64 batch = 0; batch < output_dimensions.count(); ++batch) {\n      for (int64 yx = 0; yx < area; ++yx) {\n        for (int64 depth = 0; depth < dimensions.feature_map_count(); ++depth) {\n          LOG(INFO) << output_dimensions.ElementCount() << ' ' << batch << ' '\n                    << yx << ' ' << depth;\n          output_host[index(batch, depth + depth_sum, yx,\n                            output_dimensions.feature_map_count())] =\n              tmp[index(batch, depth, yx, dimensions.feature_map_count())];\n        }\n      }\n    }\n    depth_sum += dimensions.feature_map_count();\n  }\n  stream->ThenMemcpyH2D<float>(output_host, output_data);\n  return true;\n}\n\nbool CudnnSupport::DoElementwiseOperate(\n    Stream* stream, dnn::ElementwiseOperation operation,\n    port::ArraySlice<dnn::BatchDescriptor> input_dimensions,\n    port::ArraySlice<const DeviceMemory<float>*> input_data,\n    const dnn::BatchDescriptor& output_dimensions,\n    DeviceMemory<float>* output_data) {\n  LOG(FATAL) << \"not yet implemented\";  // TODO(leary)\n  return false;\n}\n\nbool CudnnSupport::DoXYPad(Stream* stream,\n                           const dnn::BatchDescriptor& dimensions,\n                           const DeviceMemory<float>& input_data,\n                           int64 left_pad, int64 right_pad, int64 top_pad,\n                           int64 bottom_pad, DeviceMemory<float>* output_data) {\n  LOG(FATAL) << \"not yet implemented\";  // TODO(leary)\n  return false;\n}\n\nbool CudnnSupport::DoXYSlice(Stream* stream,\n                             const dnn::BatchDescriptor& dimensions,\n                             const DeviceMemory<float>& input_data,\n                             int64 left_trim, int64 right_trim, int64 top_trim,\n                             int64 bottom_trim,\n                             DeviceMemory<float>* output_data) {\n  LOG(FATAL) << \"not yet implemented\";  // TODO(leary)\n  return false;\n}\n\nbool CudnnSupport::DoMemcpyD2HQuantized(\n    Stream* stream, const DeviceMemory<float>& gpu_unquantized_src,\n    dnn::QuantizedActivationMode mode, void* host_dst, int64 size) {\n  LOG(ERROR) << \"quantized memcpy not supported by cuDNN\";\n  return false;\n}\n\nbool CudnnSupport::DoMemcpyH2DQuantized(\n    Stream* stream, const void* host_src, int64 size,\n    dnn::QuantizedActivationMode mode,\n    DeviceMemory<float>* gpu_unquantized_dst) {\n  LOG(ERROR) << \"quantized memcpy not supported by cuDNN\";\n  return false;\n}\n\nbool CudnnSupport::DeriveOutputBatchDescriptor(\n    const dnn::BatchDescriptor& batch_descriptor,\n    const dnn::FilterDescriptor& filter_descriptor,\n    const dnn::ConvolutionDescriptor& convolution_descriptor,\n    dnn::BatchDescriptor* output_batch_descriptor) {\n  CudnnTensorDescriptor input_nd(batch_descriptor, CUDNN_DATA_FLOAT);\n  CudnnFilterDescriptor filter(filter_descriptor, CUDNN_DATA_FLOAT);\n  CudnnConvolutionDescriptor conv(convolution_descriptor, CUDNN_DATA_FLOAT);\n\n  int dn = batch_descriptor.ndims() + 2;\n  std::vector<int> dims(dn);  // in BDYX\n  const auto status = [&] {\n    RETURN_IF_CUDNN_ERROR(cudnnGetConvolutionNdForwardOutputDim(\n        conv.handle(), input_nd.handle(), filter.handle(), dn, dims.data()));\n    output_batch_descriptor->set_count(dims[0])\n        .set_feature_map_count(dims[1])\n        .set_layout(batch_descriptor.layout());\n\n    for (int i = 0; i < batch_descriptor.ndims(); i++) {\n      output_batch_descriptor->set_spatial_dim(static_cast<dnn::DimIndex>(i),\n                                               dims.rbegin()[i]);\n    }\n    return port::Status::OK();\n  }();\n  return IsStatusOk(status, /*report_error=*/true);\n}\n\n}  // namespace gpu\n\nvoid initialize_cudnn() {\n  port::Status status =\n      PluginRegistry::Instance()->RegisterFactory<PluginRegistry::DnnFactory>(\n          cuda::kCudaPlatformId, gpu::kCuDnnPlugin, \"cuDNN\",\n          [](internal::StreamExecutorInterface* parent) -> dnn::DnnSupport* {\n            gpu::GpuExecutor* cuda_executor =\n                dynamic_cast<gpu::GpuExecutor*>(parent);\n            if (cuda_executor == nullptr) {\n              LOG(ERROR) << \"Attempting to initialize an instance of the cuDNN \"\n                         << \"support library with a non-CUDA StreamExecutor\";\n              return nullptr;\n            }\n\n            gpu::CudnnSupport* dnn = new gpu::CudnnSupport(cuda_executor);\n            if (!dnn->Init().ok()) {\n              // Note: Init() will log a more specific error.\n              delete dnn;\n              return nullptr;\n            }\n            return dnn;\n          });\n\n  if (!status.ok()) {\n    LOG(ERROR) << \"Unable to register cuDNN factory: \"\n               << status.error_message();\n  }\n\n  PluginRegistry::Instance()->SetDefaultFactory(\n      cuda::kCudaPlatformId, PluginKind::kDnn, gpu::kCuDnnPlugin);\n}\n\n}  // namespace stream_executor\n\n#pragma clang diagnostic pop\n\nREGISTER_MODULE_INITIALIZER(register_cudnn,\n                            { stream_executor::initialize_cudnn(); });\n"], "fixing_code": ["/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include \"tensorflow/stream_executor/cuda/cuda_dnn.h\"\n\n#include <functional>\n#include <memory>\n#include <utility>\n\n#include \"absl/memory/memory.h\"\n#include \"absl/strings/str_cat.h\"\n#include \"third_party/eigen3/Eigen/Core\"\n#include \"tensorflow/core/lib/core/errors.h\"\n#include \"tensorflow/core/platform/tensor_float_32_utils.h\"\n#include \"tensorflow/core/util/env_var.h\"\n#include \"tensorflow/stream_executor/cuda/cuda_activation.h\"\n#include \"tensorflow/stream_executor/cuda/cuda_diagnostics.h\"\n#include \"tensorflow/stream_executor/cuda/cuda_driver.h\"\n#include \"tensorflow/stream_executor/cuda/cuda_gpu_executor.h\"\n#include \"tensorflow/stream_executor/cuda/cuda_platform_id.h\"\n#include \"tensorflow/stream_executor/cuda/cuda_stream.h\"\n#include \"tensorflow/stream_executor/cuda/cuda_timer.h\"\n#include \"tensorflow/stream_executor/cuda/cudnn_version.h\"\n#include \"tensorflow/stream_executor/dnn.h\"\n#include \"tensorflow/stream_executor/lib/env.h\"\n#include \"tensorflow/stream_executor/lib/error.h\"\n#include \"tensorflow/stream_executor/lib/initialize.h\"\n#include \"tensorflow/stream_executor/lib/mathutil.h\"\n#include \"tensorflow/stream_executor/lib/threadpool.h\"\n#include \"tensorflow/stream_executor/platform/logging.h\"\n#include \"tensorflow/stream_executor/plugin_registry.h\"\n#include \"tensorflow/stream_executor/scratch_allocator.h\"\n#include \"tensorflow/stream_executor/stream.h\"\n#include \"tensorflow/stream_executor/stream_executor_pimpl.h\"\n// clang-format off\n#include \"third_party/gpus/cudnn/cudnn.h\"\n#include \"absl/strings/string_view.h\"\n// clang-format on\n\n#pragma clang diagnostic push\n\n// Make sure that Eigen::half forward declaration in dnn.h matches the\n// declaration in Eigen.\n#pragma clang diagnostic warning \"-Wmismatched-tags\"\n\nnamespace stream_executor {\nnamespace gpu {\n\nPLUGIN_REGISTRY_DEFINE_PLUGIN_ID(kCuDnnPlugin);\n\nnamespace {\n\nstatic_assert(CUDNN_VERSION >= 7300, \"cuDNN needs to be version 7.3 or higher\");\n\n// Exits the program if 'expr' doesn't return CUDNN_STATUS_SUCCESS.\n#define CHECK_CUDNN_OK(expr) CHECK_EQ(expr, CUDNN_STATUS_SUCCESS)\n\n// If 'expr' doesn't return CUDNN_STATUS_SUCCESS, returns from the current\n// function with a non-successful port::Status.\n#define RETURN_IF_CUDNN_ERROR(expr)                                      \\\n  do {                                                                   \\\n    cudnnStatus_t _status = expr;                                        \\\n    if (!SE_PREDICT_TRUE(_status == CUDNN_STATUS_SUCCESS)) {             \\\n      std::ostringstream oss;                                            \\\n      oss << ToString(_status) << \"\\nin \" << __FILE__ << \"(\" << __LINE__ \\\n          << \"): '\" << #expr << \"'\";                                     \\\n      return port::Status(port::error::UNKNOWN, oss.str().c_str());      \\\n    }                                                                    \\\n  } while (false)\n\n// Converts (via narrowing) a type T value to a type U, and checks that the\n// value has no value change due to the conversion.\ntemplate <typename WideT, typename NarrowT>\nNarrowT CheckedNarrowing(const WideT& wide) {\n  NarrowT narrow = wide;\n  CHECK_EQ(narrow, wide)\n      << \"checked narrowing failed; values not equal post-conversion\";\n  return narrow;\n}\n\nstd::string ToString(cudnnStatus_t status) {\n  switch (status) {\n    case CUDNN_STATUS_SUCCESS:\n      return \"CUDNN_STATUS_SUCCESS\";\n    case CUDNN_STATUS_NOT_INITIALIZED:\n      return \"CUDNN_STATUS_NOT_INITIALIZED\";\n    case CUDNN_STATUS_ALLOC_FAILED:\n      return \"CUDNN_STATUS_ALLOC_FAILED\";\n    case CUDNN_STATUS_BAD_PARAM:\n      return \"CUDNN_STATUS_BAD_PARAM\";\n    case CUDNN_STATUS_INTERNAL_ERROR:\n      return \"CUDNN_STATUS_INTERNAL_ERROR\";\n    case CUDNN_STATUS_INVALID_VALUE:\n      return \"CUDNN_STATUS_INVALID_VALUE\";\n    case CUDNN_STATUS_ARCH_MISMATCH:\n      return \"CUDNN_STATUS_ARCH_MISMATCH\";\n    case CUDNN_STATUS_MAPPING_ERROR:\n      return \"CUDNN_STATUS_MAPPING_ERROR\";\n    case CUDNN_STATUS_EXECUTION_FAILED:\n      return \"CUDNN_STATUS_EXECUTION_FAILED\";\n    case CUDNN_STATUS_NOT_SUPPORTED:\n      return \"CUDNN_STATUS_NOT_SUPPORTED\";\n    case CUDNN_STATUS_LICENSE_ERROR:\n      return \"CUDNN_STATUS_LICENSE_ERROR\";\n    case CUDNN_STATUS_RUNTIME_PREREQUISITE_MISSING:\n      return \"CUDNN_STATUS_RUNTIME_PREREQUISITE_MISSING\";\n    case CUDNN_STATUS_RUNTIME_IN_PROGRESS:\n      return \"CUDNN_STATUS_RUNTIME_IN_PROGRESS\";\n    case CUDNN_STATUS_RUNTIME_FP_OVERFLOW:\n      return \"CUDNN_STATUS_RUNTIME_FP_OVERFLOW\";\n    default:\n      return absl::StrCat(\"<unknown cudnn status: \", static_cast<int>(status),\n                          \">\");\n  }\n}\n\n// RAII wrapper for all calls to cuDNN with a cuDNN handle argument.\n//\n// See CudnnAccess::GetHandle() for details.\nclass CudnnHandle {\n public:\n  // Takes ownership of the executor context and the lock to access cuDNN\n  // using handle.\n  CudnnHandle(gpu::ScopedActivateExecutorContext context,\n              std::unique_ptr<absl::MutexLock> lock, cudnnHandle_t handle)\n      : context_(std::move(context)), lock_(std::move(lock)), handle_(handle) {}\n\n  // Returns cuDNN handle. To be passed directly to cuDNN APIs, don't keep\n  // a copy.\n  cudnnHandle_t handle() const { return handle_; }\n\n private:\n  gpu::ScopedActivateExecutorContext context_;\n  std::unique_ptr<absl::MutexLock> lock_;\n  cudnnHandle_t handle_;  // Not owned.\n};\n\n}  // namespace\n\n// Wraps a cuDNN handle and provides access to it through CudnnHandle\n// instances, which also locks a mutex, acquires the CUDA context, and sets\n// the stream that cuDNN should use to enqueue any work.\n//\n// Note: CudnnSupport::cudnn_ should be the only instantiation of this class.\nclass CudnnAccess {\n public:\n  // Takes ownership of the handle.\n  explicit CudnnAccess(cudnnHandle_t handle) : handle_(handle) {}\n\n  ~CudnnAccess() {\n    absl::MutexLock lock(&mutex_);\n    cudnnDestroy(handle_);\n  }\n\n  // Creates a CudnnHandle instance for stream.\n  //\n  // cuDNN API calls using the same handle instance need to be serialized\n  // across threads. This is guaranteed by CudnnHandle instances locking the\n  // mutex owned by this class.\n  //\n  // Most cuDNN APIs taking a handle perform work on a CUDA stream. The\n  // CudnnHandle instance acquires the executor's CUDA context and sets cuDNN\n  // to use the provided stream.\n  //\n  // The stream argument may be null, which translates to the legacy default\n  // stream. See\n  // https://docs.nvidia.com/cuda/cuda-driver-api/stream-sync-behavior.html.\n  // The legacy default stream synchronizes with all other streams and it is\n  // therefore a bad idea (performance wise) to call any cuDNN APIs that\n  // enqueue work in the stream.\n  CudnnHandle GetHandle(GpuExecutor* executor, Stream* stream) {\n    auto lock = absl::make_unique<absl::MutexLock>(&mutex_);\n    mutex_.AssertHeld();\n    gpu::ScopedActivateExecutorContext context(executor);\n    CUstream cu_stream = stream ? AsGpuStreamValue(stream) : cudaStreamLegacy;\n    const auto status = cudnnSetStream(handle_, cu_stream);\n    CHECK_EQ(status, CUDNN_STATUS_SUCCESS) << \"Failed to set cuDNN stream.\";\n    return CudnnHandle(std::move(context), std::move(lock), handle_);\n  }\n\n private:\n  // Guards the enqueueing of cuDNN operations via the handle_ below.\n  absl::Mutex mutex_;\n\n  // cuDNN library handle.\n  cudnnHandle_t handle_ TF_GUARDED_BY(mutex_);  // Owned.\n};\n\nnamespace {\n\n// A helper function to return the internal compute type for\n// RNNs in cudnn.\ncudnnDataType_t GetRnnComputeType(dnn::DataType data_type);\n\ncudnnConvolutionFwdAlgo_t ToConvForwardAlgo(dnn::AlgorithmDesc algorithm) {\n  cudnnConvolutionFwdAlgo_t algo =\n      cudnnConvolutionFwdAlgo_t(algorithm.algo_id());\n  switch (algo) {\n    case CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM:\n    case CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM:\n    case CUDNN_CONVOLUTION_FWD_ALGO_GEMM:\n    case CUDNN_CONVOLUTION_FWD_ALGO_DIRECT:\n    case CUDNN_CONVOLUTION_FWD_ALGO_FFT:\n    case CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING:\n    case CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD:\n    case CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED:\n      return algo;\n    default:\n      LOG(FATAL) << \"Unsupported Cudnn convolution forward algorithm: \"\n                 << algorithm.algo_id();\n  }\n}\n\ncudnnConvolutionBwdDataAlgo_t ToConvBackwardDataAlgo(\n    dnn::AlgorithmDesc algorithm) {\n  cudnnConvolutionBwdDataAlgo_t algo =\n      cudnnConvolutionBwdDataAlgo_t(algorithm.algo_id());\n  switch (algo) {\n    case CUDNN_CONVOLUTION_BWD_DATA_ALGO_0:\n    case CUDNN_CONVOLUTION_BWD_DATA_ALGO_1:\n    case CUDNN_CONVOLUTION_BWD_DATA_ALGO_FFT:\n    case CUDNN_CONVOLUTION_BWD_DATA_ALGO_FFT_TILING:\n    case CUDNN_CONVOLUTION_BWD_DATA_ALGO_WINOGRAD:\n    case CUDNN_CONVOLUTION_BWD_DATA_ALGO_WINOGRAD_NONFUSED:\n      return algo;\n    default:\n      LOG(FATAL)\n          << \"Unsupported Cudnn convolution backward algorithm for data: \"\n          << algorithm.algo_id();\n  }\n}\n\ncudnnConvolutionBwdFilterAlgo_t ToConvBackwardFilterAlgo(\n    dnn::AlgorithmDesc algorithm) {\n  cudnnConvolutionBwdFilterAlgo_t algo =\n      cudnnConvolutionBwdFilterAlgo_t(algorithm.algo_id());\n  switch (algo) {\n    case CUDNN_CONVOLUTION_BWD_FILTER_ALGO_0:\n    case CUDNN_CONVOLUTION_BWD_FILTER_ALGO_1:\n    case CUDNN_CONVOLUTION_BWD_FILTER_ALGO_FFT:\n    case CUDNN_CONVOLUTION_BWD_FILTER_ALGO_3:\n    // Based on cudnn.h, the following is not implemented.\n    // case CUDNN_CONVOLUTION_BWD_FILTER_ALGO_WINOGRAD:\n    case CUDNN_CONVOLUTION_BWD_FILTER_ALGO_WINOGRAD_NONFUSED:\n    case CUDNN_CONVOLUTION_BWD_FILTER_ALGO_FFT_TILING:\n      return algo;\n    default:\n      LOG(FATAL)\n          << \"Unsupported Cudnn convolution backward algorithm for filter: \"\n          << algorithm.algo_id();\n  }\n}\n\nport::StatusOr<int> GetCudnnProperty(libraryPropertyType type) {\n  int value;\n  RETURN_IF_CUDNN_ERROR(cudnnGetProperty(type, &value));\n  return value;\n}\n\ncudnnRNNAlgo_t ToCudnnRNNAlgo(absl::optional<dnn::AlgorithmDesc> algorithm) {\n  if (!algorithm.has_value()) {\n    return CUDNN_RNN_ALGO_STANDARD;\n  }\n  cudnnRNNAlgo_t algo = static_cast<cudnnRNNAlgo_t>(algorithm->algo_id());\n  switch (algo) {\n    case CUDNN_RNN_ALGO_STANDARD:\n    case CUDNN_RNN_ALGO_PERSIST_STATIC:\n    case CUDNN_RNN_ALGO_PERSIST_DYNAMIC:\n      return algo;\n    default:\n      LOG(FATAL) << \"Unsupported Cudnn RNN algorithm: \" << algorithm->algo_id();\n  }\n}\n\nport::Status GetLoadedCudnnVersion(CudnnVersion* version) {\n  SE_ASSIGN_OR_RETURN(version->major_version, GetCudnnProperty(MAJOR_VERSION));\n  SE_ASSIGN_OR_RETURN(version->minor_version, GetCudnnProperty(MINOR_VERSION));\n  SE_ASSIGN_OR_RETURN(version->patch_level, GetCudnnProperty(PATCH_LEVEL));\n  return port::Status::OK();\n}\n\n#if CUDNN_MAJOR >= 8 && (CUDNN_MINOR > 0 || CUDNN_PATCHLEVEL >= 4)\nvoid PreloadCudnnLibrary(cudnnStatus_t (*version_check_fn)(),\n                         absl::string_view sub_library) {\n  cudnnStatus_t status = version_check_fn();\n  if (status != CUDNN_STATUS_SUCCESS) {\n    VLOG(1) << \"Could not pre-initialize cuDNN sub-library \" << sub_library\n            << \".  Error: \" << cudnnGetErrorString(status) << \".\";\n  }\n}\n#endif\n\n}  // namespace\n\nCudnnSupport::CudnnSupport(GpuExecutor* parent) : parent_(parent) {}\n\nport::Status CudnnSupport::Init() {\n  ScopedActivateExecutorContext context(parent_);\n  cudnnHandle_t cudnn_handle = nullptr;\n  const auto status = cudnnCreate(&cudnn_handle);\n  if (status == CUDNN_STATUS_SUCCESS) {\n    CudnnVersion source_version(CUDNN_MAJOR, CUDNN_MINOR, CUDNN_PATCHLEVEL);\n\n    CudnnVersion loaded_version;\n    TF_RETURN_IF_ERROR(GetLoadedCudnnVersion(&loaded_version));\n    if (!IsSourceCompatibleWithCudnnLibrary(source_version, loaded_version)) {\n      const std::string error = absl::StrCat(\n          \"Loaded runtime CuDNN library: \", loaded_version.ToString(),\n          \" but source was compiled with: \", source_version.ToString(),\n          \".  CuDNN library needs to have matching major version and equal or \"\n          \"higher minor version. If using a binary install, upgrade your CuDNN \"\n          \"library.  If building from sources, make sure the library loaded at \"\n          \"runtime is compatible with the version specified during compile \"\n          \"configuration.\");\n      LOG(ERROR) << error;\n      cudnnDestroy(cudnn_handle);\n      return port::Status(port::error::INTERNAL, error);\n    }\n\n    cudnn_.reset(new CudnnAccess(cudnn_handle));\n\n    LOG(INFO) << \"Loaded cuDNN version \" << cudnnGetVersion();\n    return port::Status::OK();\n  }\n\n  CHECK_EQ(cudnn_handle, nullptr);\n  LOG(ERROR) << \"Could not create cudnn handle: \" << ToString(status);\n  if (status == CUDNN_STATUS_NOT_INITIALIZED) {\n    auto result = gpu::Diagnostician::FindKernelDriverVersion();\n    if (!result.ok()) {\n      LOG(ERROR) << \"Error retrieving driver version: \"\n                 << cuda::DriverVersionStatusToString(result);\n    } else {\n      const auto& version = result.ValueOrDie();\n      LOG(ERROR) << \"Possibly insufficient driver version: \"\n                 << cuda::DriverVersionToString(version);\n    }\n  }\n\n  return port::Status(port::error::INTERNAL,\n                      absl::StrCat(\"cudnn library could not create a handle: \",\n                                   ToString(status)));\n}\n\nport::StatusOr<perftools::gputools::dnn::VersionInfo>\nCudnnSupport::GetVersion() {\n  CudnnVersion version;\n  TF_RETURN_IF_ERROR(GetLoadedCudnnVersion(&version));\n  return perftools::gputools::dnn::VersionInfo(\n      version.major_version, version.minor_version, version.patch_level);\n}\n\nnamespace {\n\n// Deleter functors for cuDNN types that need to be deleted.\nstruct TensorDescriptorDeleter {\n  void operator()(cudnnTensorDescriptor_t descriptor) const {\n    CHECK_CUDNN_OK(cudnnDestroyTensorDescriptor(descriptor));\n  }\n};\nstruct RNNDataDescriptorDeleter {\n  void operator()(cudnnRNNDataDescriptor_t descriptor) const {\n    CHECK_CUDNN_OK(cudnnDestroyRNNDataDescriptor(descriptor));\n  }\n};\nstruct FilterDescriptorDeleter {\n  void operator()(cudnnFilterDescriptor_t descriptor) const {\n    CHECK_CUDNN_OK(cudnnDestroyFilterDescriptor(descriptor));\n  }\n};\nstruct ConvolutionDescriptorDeleter {\n  void operator()(cudnnConvolutionDescriptor_t descriptor) const {\n    CHECK_CUDNN_OK(cudnnDestroyConvolutionDescriptor(descriptor));\n  }\n};\nstruct PoolingDescriptorDeleter {\n  void operator()(cudnnPoolingDescriptor_t descriptor) const {\n    CHECK_CUDNN_OK(cudnnDestroyPoolingDescriptor(descriptor));\n  }\n};\nstruct LrnDescriptorDeleter {\n  void operator()(cudnnLRNDescriptor_t descriptor) const {\n    CHECK_CUDNN_OK(cudnnDestroyLRNDescriptor(descriptor));\n  }\n};\n\nstruct ActivationDescriptorDeleter {\n  void operator()(cudnnActivationDescriptor_t descriptor) const {\n    CHECK_CUDNN_OK(cudnnDestroyActivationDescriptor(descriptor));\n  }\n};\nstruct DropoutDescriptorDeleter {\n  void operator()(cudnnDropoutDescriptor_t descriptor) const {\n    CHECK_CUDNN_OK(cudnnDestroyDropoutDescriptor(descriptor));\n  }\n};\nstruct RnnDescriptorDeleter {\n  void operator()(cudnnRNNDescriptor_t descriptor) const {\n    CHECK_CUDNN_OK(cudnnDestroyRNNDescriptor(descriptor));\n  }\n};\nstruct PersistentRnnPlanDeleter {\n  void operator()(cudnnPersistentRNNPlan_t plan) const {\n    CHECK_CUDNN_OK(cudnnDestroyPersistentRNNPlan(plan));\n  }\n};\n#if CUDNN_VERSION >= 7603\nstruct CtcLossDescriptorDeleter {\n  void operator()(cudnnCTCLossDescriptor_t descriptor) const {\n    CHECK_CUDNN_OK(cudnnDestroyCTCLossDescriptor(descriptor));\n  }\n};\n#endif\n\n// RAII wrappers for cuDNN types.\nusing TensorDescriptor =\n    std::unique_ptr<cudnnTensorStruct, TensorDescriptorDeleter>;\nusing RNNDataDescriptor =\n    std::unique_ptr<cudnnRNNDataStruct, RNNDataDescriptorDeleter>;\nusing FilterDescriptor =\n    std::unique_ptr<cudnnFilterStruct, FilterDescriptorDeleter>;\nusing ConvolutionDescriptor =\n    std::unique_ptr<cudnnConvolutionStruct, ConvolutionDescriptorDeleter>;\nusing PoolingDescriptor =\n    std::unique_ptr<cudnnPoolingStruct, PoolingDescriptorDeleter>;\nusing LrnDescriptor = std::unique_ptr<cudnnLRNStruct, LrnDescriptorDeleter>;\nusing ActivationDescriptor =\n    std::unique_ptr<cudnnActivationStruct, ActivationDescriptorDeleter>;\nusing DropoutDescriptor =\n    std::unique_ptr<cudnnDropoutStruct, DropoutDescriptorDeleter>;\nusing RnnDescriptor = std::unique_ptr<cudnnRNNStruct, RnnDescriptorDeleter>;\nusing PersistentRnnPlan =\n    std::unique_ptr<cudnnPersistentRNNPlan, PersistentRnnPlanDeleter>;\n#if CUDNN_VERSION >= 7603\nusing CtcLossDescriptor =\n    std::unique_ptr<cudnnCTCLossStruct, CtcLossDescriptorDeleter>;\n#endif\n\n// Factory methods for cuDNN types.\nTensorDescriptor CreateTensorDescriptor() {\n  cudnnTensorDescriptor_t result;\n  CHECK_CUDNN_OK(cudnnCreateTensorDescriptor(&result));\n  return TensorDescriptor(result);\n}\nRNNDataDescriptor CreateRNNDataDescriptor() {\n  cudnnRNNDataDescriptor_t result;\n  CHECK_CUDNN_OK(cudnnCreateRNNDataDescriptor(&result));\n  return RNNDataDescriptor(result);\n}\nFilterDescriptor CreateFilterDescriptor() {\n  cudnnFilterDescriptor_t result;\n  CHECK_CUDNN_OK(cudnnCreateFilterDescriptor(&result));\n  return FilterDescriptor(result);\n}\nConvolutionDescriptor CreateConvolutionDescriptor() {\n  cudnnConvolutionDescriptor_t result;\n  CHECK_CUDNN_OK(cudnnCreateConvolutionDescriptor(&result));\n  return ConvolutionDescriptor(result);\n}\nPoolingDescriptor CreatePoolingDescriptor() {\n  cudnnPoolingDescriptor_t result;\n  CHECK_CUDNN_OK(cudnnCreatePoolingDescriptor(&result));\n  return PoolingDescriptor(result);\n}\nLrnDescriptor CreateLrnDescriptor() {\n  cudnnLRNDescriptor_t result;\n  CHECK_CUDNN_OK(cudnnCreateLRNDescriptor(&result));\n  return LrnDescriptor(result);\n}\nActivationDescriptor CreateActivationDescriptor() {\n  cudnnActivationDescriptor_t result;\n  CHECK_CUDNN_OK(cudnnCreateActivationDescriptor(&result));\n  return ActivationDescriptor(result);\n}\nDropoutDescriptor CreateDropoutDescriptor() {\n  cudnnDropoutDescriptor_t result;\n  CHECK_CUDNN_OK(cudnnCreateDropoutDescriptor(&result));\n  return DropoutDescriptor(result);\n}\nRnnDescriptor CreateRnnDescriptor() {\n  cudnnRNNDescriptor_t result;\n  CHECK_CUDNN_OK(cudnnCreateRNNDescriptor(&result));\n  return RnnDescriptor(result);\n}\n#if CUDNN_VERSION >= 7603\nCtcLossDescriptor CreateCtcLossDescriptor() {\n  cudnnCTCLossDescriptor_t result;\n  CHECK_CUDNN_OK(cudnnCreateCTCLossDescriptor(&result));\n  return CtcLossDescriptor(result);\n}\n#endif\n\nport::StatusOr<PersistentRnnPlan> CreatePersistentRnnPlan(\n    cudnnRNNDescriptor_t rnn_desc, int batch_size, cudnnDataType_t data_type) {\n  cudnnPersistentRNNPlan_t result;\n  RETURN_IF_CUDNN_ERROR(\n      cudnnCreatePersistentRNNPlan(rnn_desc, batch_size, data_type, &result));\n  return port::StatusOr<PersistentRnnPlan>(PersistentRnnPlan(result));\n}\n\n// Turns a BatchDescriptor structure into a cudnn tensor handle within a\n// scope.\nclass CudnnTensorDescriptor {\n public:\n  CudnnTensorDescriptor(const dnn::BatchDescriptor& batch_descriptor,\n                        cudnnDataType_t elem_type)\n      : handle_(CreateTensorDescriptor()) {\n    switch (batch_descriptor.layout()) {\n      case dnn::DataLayout::kBatchYXDepth:\n      case dnn::DataLayout::kBatchDepthYX: {\n        const int nd = batch_descriptor.ndims() + 2;\n        // cuDNN requires the strides and dims to be ordered as BDYX.\n        std::vector<int64> strides64 =\n            batch_descriptor.full_strides(dnn::DataLayout::kBatchDepthYX);\n        std::vector<int64> dims64 =\n            batch_descriptor.full_dims(dnn::DataLayout::kBatchDepthYX);\n\n        // cuDNN requires arrays of ints.\n        std::vector<int> strides(nd);\n        std::vector<int> dims(nd);\n        std::transform(strides64.cbegin(), strides64.cend(), strides.begin(),\n                       &CheckedNarrowing<int64, int>);\n        std::transform(dims64.cbegin(), dims64.cend(), dims.begin(),\n                       &CheckedNarrowing<int64, int>);\n        CHECK_CUDNN_OK(cudnnSetTensorNdDescriptor(handle_.get(), elem_type, nd,\n                                                  dims.data(), strides.data()))\n            << \"batch_descriptor: \" << batch_descriptor.ToString();\n      } break;\n      case dnn::DataLayout::kBatchDepthYX4: {\n        CHECK_CUDNN_OK(cudnnSetTensor4dDescriptor(\n            handle_.get(), CUDNN_TENSOR_NCHW_VECT_C, elem_type,\n            batch_descriptor.count(), batch_descriptor.feature_map_count(),\n            batch_descriptor.height(), batch_descriptor.width()))\n            << \"batch_descriptor: \" << batch_descriptor.ToString();\n      } break;\n      default:\n        LOG(FATAL) << \"Unsupported tensor format \"\n                   << DataLayoutString(batch_descriptor.layout());\n        break;\n    }\n  }\n\n  cudnnTensorDescriptor_t handle() const { return handle_.get(); }\n\n private:\n  TensorDescriptor handle_;\n\n  SE_DISALLOW_COPY_AND_ASSIGN(CudnnTensorDescriptor);\n};\n\n// Turns a FilterDescriptor structure into a cudnn filter handle within a\n// scope.\nclass CudnnFilterDescriptor {\n public:\n  CudnnFilterDescriptor(const dnn::FilterDescriptor& filter_descriptor,\n                        cudnnDataType_t elem_type)\n      : handle_(CreateFilterDescriptor()) {\n    // TODO(b/23032134): Even if the filter layout is not supported,\n    // cudnnSetFilter4DDescriptor_v4 will return CUDNN_STATUS_SUCCESS because\n    // it does not take layout as an input. Maybe force cuDNN by giving wrong\n    // inputs intentionally?\n    cudnnTensorFormat_t format;\n    switch (filter_descriptor.layout()) {\n      case dnn::FilterLayout::kOutputInputYX:\n        format = CUDNN_TENSOR_NCHW;\n        break;\n      case dnn::FilterLayout::kOutputYXInput:\n        format = CUDNN_TENSOR_NHWC;\n        break;\n      case dnn::FilterLayout::kOutputInputYX4:\n        format = CUDNN_TENSOR_NCHW_VECT_C;\n        break;\n      default:\n        LOG(FATAL) << \"Unsupported filter format \"\n                   << FilterLayoutString(filter_descriptor.layout());\n        break;\n    }\n\n    std::vector<int> dims(2 + filter_descriptor.ndims());\n    dims[0] = filter_descriptor.output_feature_map_count();\n    dims[1] = filter_descriptor.input_feature_map_count();\n    absl::Span<const int64> spatial_dims =\n        filter_descriptor.input_filter_dims();\n    std::copy(spatial_dims.begin(), spatial_dims.end(), dims.begin() + 2);\n\n    CHECK_CUDNN_OK(cudnnSetFilterNdDescriptor(handle_.get(), elem_type, format,\n                                              dims.size(), dims.data()));\n  }\n\n  cudnnFilterDescriptor_t handle() const { return handle_.get(); }\n\n private:\n  FilterDescriptor handle_;  // Owned.\n\n  SE_DISALLOW_COPY_AND_ASSIGN(CudnnFilterDescriptor);\n};\n\n// A helper function to decide whether to use\n// CUDNN_BATCHNORM_SPATIAL_PERSISTENT in batchnorm. This mode can be faster in\n// some tasks because an optimized path may be selected for CUDNN_DATA_FLOAT\n// and CUDNN_DATA_HALF data types, compute capability 6.0 or higher. The\n// reason we set it to false by default is that this mode may use scaled\n// atomic integer reduction that may cause a numerical overflow for certain\n// input data range.\n// TODO(yangzihao): Use autotune to choose between this mode and\n// CUDNN_BATCHNORM_SPATIAL mode.\nbool BatchnormSpatialPersistentEnabled() {\n  static bool is_enabled = [] {\n    bool is_enabled = false;\n    TF_CHECK_OK(tensorflow::ReadBoolFromEnvVar(\n        \"TF_USE_CUDNN_BATCHNORM_SPATIAL_PERSISTENT\",\n        /*default_val=*/false, &is_enabled));\n    return is_enabled;\n  }();\n  return is_enabled;\n}\n\n// The following function allows deterministic ops to be implemented relatively\n// quickly using environment variables. It is intended to be temporary. The\n// longer-term intention is to enable deterministic ops via tf.config and\n// appropriate plumbing. See the discussion on PR 34951 for more information:\n// https://github.com/tensorflow/tensorflow/pull/34951#discussion_r355682316\n// This function and associated comment are replicated in the following three\n// places:\n//   1. tensorflow/compiler/xla/service/gpu/gpu_conv_algorithm_picker.cc\n//   2. tensorflow/core/kernels/gpu_utils.cc\n//   3. tensorflow/stream_executor/cuda/cuda_dnn.cc\n// When implementing the plumbing, you should also search for the use of\n// TF_DETERMINISTIC_OPS on its own.\n// TODO(duncanriach): move to an API that uses tf.config and implement the first\n//                    phase of plumbing.\nbool RequireCudnnDeterminism() {\n  static bool require_cudnn_determinism = [] {\n    bool deterministic_ops = false;\n    TF_CHECK_OK(tensorflow::ReadBoolFromEnvVar(\"TF_DETERMINISTIC_OPS\",\n                                               /*default_val=*/false,\n                                               &deterministic_ops));\n    bool cudnn_deterministic = false;\n    TF_CHECK_OK(tensorflow::ReadBoolFromEnvVar(\"TF_CUDNN_DETERMINISTIC\",\n                                               /*default_val=*/false,\n                                               &cudnn_deterministic));\n    return deterministic_ops || cudnn_deterministic;\n  }();\n  return require_cudnn_determinism;\n}\n\n// A helper function to decide whether to force the default conv algorithm.\nbool ConvUseDefaultAlgorithm() {\n  static bool use_default = [] {\n    bool use_default = false;\n    TF_CHECK_OK(tensorflow::ReadBoolFromEnvVar(\"TF_USE_DEFAULT_CONV_ALGO\",\n                                               /*default_val=*/false,\n                                               &use_default));\n    return use_default;\n  }();\n  return use_default;\n}\n\nstd::tuple<int, int> GetCcMajorMinor(Stream* stream) {\n  int cc_major, cc_minor;\n  stream->parent()->GetDeviceDescription().cuda_compute_capability(&cc_major,\n                                                                   &cc_minor);\n  return std::make_tuple(cc_major, cc_minor);\n}\n\n// Turns a ConvolutionDescriptor structure into a cudnn convolution handle\n// within a scope.\nclass CudnnConvolutionDescriptor {\n public:\n  CudnnConvolutionDescriptor(\n      const dnn::ConvolutionDescriptor& convolution_descriptor,\n      cudnnDataType_t data_type)\n      : handle_(CreateConvolutionDescriptor()) {\n    absl::Span<const int64> strides64 = convolution_descriptor.strides();\n    absl::Span<const int64> padding64 = convolution_descriptor.padding();\n    absl::Span<const int64> dilations64 = convolution_descriptor.dilations();\n    CHECK_NE(convolution_descriptor.pad_alignment(),\n             dnn::PadAlignment::kTensorFlowPadding)\n        << \"TensorFlow padding alignment is not supported.\";\n\n    // cuDNN requires arrays of ints.\n    std::vector<int> strides(convolution_descriptor.ndims());\n    std::vector<int> padding(convolution_descriptor.ndims());\n    std::vector<int> dilations(convolution_descriptor.ndims());\n    std::transform(strides64.cbegin(), strides64.cend(), strides.begin(),\n                   &CheckedNarrowing<int64, int>);\n    std::transform(padding64.cbegin(), padding64.cend(), padding.begin(),\n                   &CheckedNarrowing<int64, int>);\n    // TODO(yangzihao): Test with negative dilation to make sure that cudnn\n    // doesn't crash.\n    std::transform(dilations64.cbegin(), dilations64.cend(), dilations.begin(),\n                   &CheckedNarrowing<int64, int>);\n\n    CHECK_CUDNN_OK(cudnnSetConvolutionNdDescriptor(\n        handle_.get(), convolution_descriptor.ndims(), padding.data(),\n        strides.data(), dilations.data(),\n        convolution_descriptor.convolution_not_crosscorr()\n            ? CUDNN_CONVOLUTION\n            : CUDNN_CROSS_CORRELATION,\n        data_type));\n\n#if CUDNN_MAJOR >= 7\n    VLOG(2) << \"Requesting grouped convolution: \"\n            << convolution_descriptor.group_count();\n    CHECK_CUDNN_OK(cudnnSetConvolutionGroupCount(\n        handle_.get(), convolution_descriptor.group_count()));\n#else\n    CHECK_EQ(convolution_descriptor.group_count(), 1)\n        << \"Requested grouped convolution for cuDNN version < 7\";\n#endif\n  }\n\n  void set_use_tensor_op_math(bool use_tensor_op_math) {\n    cudnnMathType_t math_type =\n#if CUDNN_VERSION >= 8000\n        (use_tensor_op_math ? CUDNN_TENSOR_OP_MATH : CUDNN_FMA_MATH);\n#else\n        (use_tensor_op_math ? CUDNN_TENSOR_OP_MATH : CUDNN_DEFAULT_MATH);\n#endif\n    CHECK_CUDNN_OK(cudnnSetConvolutionMathType(handle_.get(), math_type));\n  }\n\n  cudnnConvolutionDescriptor_t handle() const { return handle_.get(); }\n\n private:\n  ConvolutionDescriptor handle_;  // Owned.\n\n  SE_DISALLOW_COPY_AND_ASSIGN(CudnnConvolutionDescriptor);\n};\n\n// A helper function to query if a CudnnConvolutionDescriptor has tensor_op_math\n// set\nstatic bool IsTensorMathOpSet(const CudnnConvolutionDescriptor& conv) {\n  cudnnMathType_t math_type;\n  CHECK_CUDNN_OK(cudnnGetConvolutionMathType(conv.handle(), &math_type));\n#if CUDNN_VERSION >= 8000\n  return math_type != CUDNN_FMA_MATH;\n#else\n  return math_type == CUDNN_TENSOR_OP_MATH;\n#endif\n}\n\nstatic bool TensorOpMathAvailable(int cc_major) { return cc_major >= 7; }\n\nstatic bool IsTensorMathEnabled(Stream* stream, dnn::DataType input_type) {\n  int cc_major, cc_minor;\n  std::tie(cc_major, cc_minor) = GetCcMajorMinor(stream);\n  if (!TensorOpMathAvailable(cc_major)) {\n    return false;\n  }\n  if (input_type == dnn::DataType::kFloat) {\n#if CUDNN_VERSION < 8000\n    return false;\n#else\n    if (!tensorflow::tensor_float_32_execution_enabled()) {\n      return false;\n    }\n#endif\n  }\n  return true;\n}\n\n// Turns a PoolingDescriptor structure into a cudnn pooling descriptor handle\n// within a scope.\nclass CudnnPoolingDescriptor {\n public:\n  explicit CudnnPoolingDescriptor(\n      const dnn::PoolingDescriptor& pooling_descriptor)\n      : handle_(CreatePoolingDescriptor()) {\n    absl::Span<const int64> strides64 = pooling_descriptor.strides();\n    absl::Span<const int64> padding64 = pooling_descriptor.padding();\n    absl::Span<const int64> shape64 = pooling_descriptor.window();\n\n    const int nd = pooling_descriptor.ndims();\n    std::vector<int> shape(nd);\n    std::vector<int> padding(nd);\n    std::vector<int> strides(nd);\n    std::transform(strides64.cbegin(), strides64.cend(), strides.begin(),\n                   &CheckedNarrowing<int64, int>);\n    std::transform(padding64.cbegin(), padding64.cend(), padding.begin(),\n                   &CheckedNarrowing<int64, int>);\n    std::transform(shape64.cbegin(), shape64.cend(), shape.begin(),\n                   &CheckedNarrowing<int64, int>);\n    bool propagate_nans = pooling_descriptor.propagate_nans();\n    const auto cudnn_max_pooling_mode = RequireCudnnDeterminism()\n                                            ? CUDNN_POOLING_MAX_DETERMINISTIC\n                                            : CUDNN_POOLING_MAX;\n    CHECK_CUDNN_OK(cudnnSetPoolingNdDescriptor(\n        handle_.get(),\n        (pooling_descriptor.mode() == dnn::PoolingMode::kMaximum\n             ? cudnn_max_pooling_mode\n             : CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING),\n        propagate_nans ? CUDNN_PROPAGATE_NAN : CUDNN_NOT_PROPAGATE_NAN, nd,\n        shape.data(), padding.data(), strides.data()));\n  }\n\n  cudnnPoolingDescriptor_t handle() const { return handle_.get(); }\n\n private:\n  PoolingDescriptor handle_;  // Owned.\n\n  SE_DISALLOW_COPY_AND_ASSIGN(CudnnPoolingDescriptor);\n};\n\n// Turns a NormalizeDescriptor structure into a cudnn LRN descriptor handle.\nclass CudnnNormalizeDescriptor {\n public:\n  explicit CudnnNormalizeDescriptor(\n      const dnn::NormalizeDescriptor& normalize_descriptor)\n      : handle_(CreateLrnDescriptor()) {\n    // The range specifies that the indices in the closed range\n    // [i - range, i + range] should be included in the normalization for index\n    // i. The lrnN value is the total number of elements in the range, so\n    // lrnN = 2*range + 1.\n    unsigned lrnN = 2 * normalize_descriptor.range() + 1;\n\n    // Note that SE defines the normalization operation as\n    //\n    //  U_i = V_i / ((bias +  alpha      * (sum_j V_j^2)) ^ beta)\n    //\n    // but cuDNN defines it as\n    //\n    //  U_i = V_i / ((bias + (alpha / n) * (sum_j V_j^2)) ^ beta)\n    //\n    // i.e. there is a factor of n difference between the meaning of the alphas\n    // in the two contexts. The cuDNN alpha is n times the SE alpha.\n    double lrnAlpha = lrnN * normalize_descriptor.alpha();\n\n    double lrnBeta = normalize_descriptor.beta();\n    double lrnK = normalize_descriptor.bias();\n    CHECK_CUDNN_OK(\n        cudnnSetLRNDescriptor(handle_.get(), lrnN, lrnAlpha, lrnBeta, lrnK));\n  }\n\n  cudnnLRNDescriptor_t handle() const { return handle_.get(); }\n\n private:\n  LrnDescriptor handle_;  // Owned.\n\n  SE_DISALLOW_COPY_AND_ASSIGN(CudnnNormalizeDescriptor);\n};\n\n// Turns a ActivationDescriptor structure into a cudnn activation\n// descriptor handle within a scope.\nclass CudnnActivationDescriptor {\n public:\n  CudnnActivationDescriptor(dnn::ActivationMode activation_mode,\n                            cudnnNanPropagation_t nan_propagation,\n                            double value_max)\n      : handle_(CreateActivationDescriptor()) {\n    double relu_ceiling = 0.0;\n    cudnnActivationMode_t mode;\n    switch (activation_mode) {\n      case dnn::ActivationMode::kNone:\n        mode = CUDNN_ACTIVATION_IDENTITY;\n        break;\n      case dnn::ActivationMode::kRelu6:\n        relu_ceiling = 6.0;\n        mode = CUDNN_ACTIVATION_CLIPPED_RELU;\n        break;\n      case dnn::ActivationMode::kReluX:\n        relu_ceiling = value_max;\n        mode = CUDNN_ACTIVATION_CLIPPED_RELU;\n        break;\n      case dnn::ActivationMode::kRelu:\n        mode = CUDNN_ACTIVATION_RELU;\n        break;\n      case dnn::ActivationMode::kSigmoid:\n        mode = CUDNN_ACTIVATION_SIGMOID;\n        break;\n      case dnn::ActivationMode::kTanh:\n        mode = CUDNN_ACTIVATION_TANH;\n        break;\n      default:\n        LOG(FATAL) << \"unrecognized activation mode: \"\n                   << static_cast<int>(activation_mode);\n    }\n\n    CHECK_CUDNN_OK(cudnnSetActivationDescriptor(handle_.get(), mode,\n                                                nan_propagation, relu_ceiling));\n  }\n\n  cudnnActivationDescriptor_t handle() const { return handle_.get(); }\n\n private:\n  ActivationDescriptor handle_;  // Owned.\n\n  SE_DISALLOW_COPY_AND_ASSIGN(CudnnActivationDescriptor);\n};\n\ncudnnDataType_t ToCudnnDataType(\n    dnn::DataType data_type,\n    dnn::DataLayout data_layout = dnn::DataLayout::kBatchDepthYX) {\n  switch (data_type) {\n    case dnn::DataType::kFloat:\n      return CUDNN_DATA_FLOAT;\n    case dnn::DataType::kDouble:\n      return CUDNN_DATA_DOUBLE;\n    case dnn::DataType::kHalf:\n      return CUDNN_DATA_HALF;\n    case dnn::DataType::kInt8:\n      return data_layout == dnn::DataLayout::kBatchDepthYX4 ? CUDNN_DATA_INT8x4\n                                                            : CUDNN_DATA_INT8;\n    case dnn::DataType::kInt32:\n      return CUDNN_DATA_INT32;\n    default:\n      LOG(FATAL) << \"Invalid DNN data type: \" << static_cast<int>(data_type);\n  }\n}\n\ncudnnDataType_t ToCudnnDataType(dnn::DataType data_type,\n                                dnn::FilterLayout filter_layout) {\n  if (data_type == dnn::DataType::kInt8 &&\n      filter_layout == dnn::FilterLayout::kOutputInputYX4) {\n    return CUDNN_DATA_INT8x4;\n  }\n  return ToCudnnDataType(data_type);\n}\n\ntemplate <typename T>\ncudnnDataType_t GetCudnnDataType(\n    dnn::DataLayout data_layout = dnn::DataLayout::kBatchDepthYX) {\n  return ToCudnnDataType(dnn::ToDataType<T>::value, data_layout);\n}\n\ncudnnRNNInputMode_t ToCudnnRnnInputMode(dnn::RnnInputMode input_mode) {\n  switch (input_mode) {\n    case dnn::RnnInputMode::kRnnLinearSkip:\n    case dnn::RnnInputMode::kRnnSkipInput:\n      return static_cast<cudnnRNNInputMode_t>(input_mode);\n    default:\n      LOG(FATAL) << \"Invalid RNN input mode: \" << static_cast<int>(input_mode);\n  }\n}\n\ncudnnDirectionMode_t ToCudnnRnnDirectionMode(\n    dnn::RnnDirectionMode direction_mode) {\n  switch (direction_mode) {\n    case dnn::RnnDirectionMode::kRnnUnidirectional:\n    case dnn::RnnDirectionMode::kRnnBidirectional:\n      return static_cast<cudnnDirectionMode_t>(direction_mode);\n    default:\n      LOG(FATAL) << \"Invalid RNN direction mode: \"\n                 << static_cast<int>(direction_mode);\n  }\n}\n\ncudnnRNNMode_t ToCudnnRnnMode(dnn::RnnMode rnn_mode) {\n  switch (rnn_mode) {\n    case dnn::RnnMode::kRnnRelu:\n    case dnn::RnnMode::kRnnTanh:\n    case dnn::RnnMode::kRnnLstm:\n    case dnn::RnnMode::kRnnGru:\n      return static_cast<cudnnRNNMode_t>(rnn_mode);\n    default:\n      LOG(FATAL) << \"Invalid RNN Mode: \" << static_cast<int>(rnn_mode);\n  }\n}\n\nint CudnnDataTypeToByteSize(cudnnDataType_t data_type) {\n  switch (data_type) {\n    case CUDNN_DATA_FLOAT:\n      return sizeof(float);\n    case CUDNN_DATA_DOUBLE:\n      return sizeof(double);\n    case CUDNN_DATA_HALF:\n      return sizeof(Eigen::half);\n    default:\n      LOG(FATAL) << \"Invalid DNN data type: \" << static_cast<int>(data_type);\n  }\n}\n\nclass CudnnDropoutDescriptor {\n  explicit CudnnDropoutDescriptor(DropoutDescriptor handle)\n      : handle_(std::move(handle)) {}\n\n public:\n  CudnnDropoutDescriptor(CudnnDropoutDescriptor&&) = default;\n\n  static port::StatusOr<CudnnDropoutDescriptor> Create(\n      const CudnnHandle& cudnn, float dropout, uint64 seed,\n      ScratchAllocator* state_allocator) {\n    DropoutDescriptor handle = CreateDropoutDescriptor();\n\n    if (dropout == 0.0f) {\n      // Return 'empty' dropout descriptor.\n      return CudnnDropoutDescriptor(std::move(handle));\n    }\n\n    DeviceMemory<uint8> state_memory;\n    if (state_allocator) {\n      size_t state_sizes_in_bytes = 0;\n      RETURN_IF_CUDNN_ERROR(\n          cudnnDropoutGetStatesSize(cudnn.handle(), &state_sizes_in_bytes));\n      SE_ASSIGN_OR_RETURN(state_memory,\n                          state_allocator->AllocateBytes(state_sizes_in_bytes));\n    }\n    RETURN_IF_CUDNN_ERROR(cudnnSetDropoutDescriptor(\n        handle.get(), cudnn.handle(), dropout, state_memory.opaque(),\n        state_memory.size(), seed));\n\n    return CudnnDropoutDescriptor(std::move(handle));\n  }\n\n  cudnnDropoutDescriptor_t handle() const { return handle_.get(); }\n\n private:\n  DropoutDescriptor handle_;  // Owned.\n  SE_DISALLOW_COPY_AND_ASSIGN(CudnnDropoutDescriptor);\n};\n\nclass CudnnRnnParamsDescriptor {\n  typedef dnn::RnnDescriptor::ParamsRegions ParamsRegions;\n\n  CudnnRnnParamsDescriptor(FilterDescriptor handle, int64 params_size_in_bytes,\n                           ParamsRegions weights, ParamsRegions biases)\n      : handle_(std::move(handle)),\n        params_size_in_bytes_(params_size_in_bytes),\n        weights_(std::move(weights)),\n        biases_(std::move(biases)) {}\n\n public:\n  CudnnRnnParamsDescriptor(CudnnRnnParamsDescriptor&&) = default;\n\n  static port::StatusOr<CudnnRnnParamsDescriptor> Create(\n      const CudnnHandle& cudnn, int input_size, cudnnDataType_t data_type,\n      cudnnRNNDescriptor_t rnn_desc, cudnnRNNMode_t rnn_mode,\n      cudnnDirectionMode_t direction_mode, int num_layers);\n\n  cudnnFilterDescriptor_t handle() const { return handle_.get(); }\n  int64 params_size_in_bytes() const { return params_size_in_bytes_; }\n  ParamsRegions params_weights() const { return weights_; }\n  ParamsRegions params_biases() const { return biases_; }\n\n private:\n  FilterDescriptor handle_;\n  int64 params_size_in_bytes_;\n  ParamsRegions weights_;\n  ParamsRegions biases_;\n  SE_DISALLOW_COPY_AND_ASSIGN(CudnnRnnParamsDescriptor);\n};\n\n}  // namespace\n\nclass CudnnRnnDescriptor : public dnn::RnnDescriptor {\n  CudnnRnnDescriptor(const CudnnHandle& cudnn, gpu::RnnDescriptor rnn_desc,\n                     PersistentRnnPlan rnn_plan, int num_layers,\n                     int hidden_size, int input_size, int cell_size,\n                     int batch_size, cudnnRNNInputMode_t input_mode,\n                     cudnnDirectionMode_t direction_mode,\n                     cudnnRNNMode_t rnn_mode, cudnnDataType_t data_type,\n                     cudnnDataType_t compute_type,\n                     const dnn::AlgorithmConfig& algorithm_config,\n                     CudnnDropoutDescriptor dropout_desc,\n                     CudnnRnnParamsDescriptor params_desc)\n      : rnn_desc_(std::move(rnn_desc)),\n        rnn_plan_(std::move(rnn_plan)),\n        num_layers_(num_layers),\n        hidden_size_(hidden_size),\n        input_size_(input_size),\n        cell_size_(cell_size),\n        batch_size_(batch_size),\n        rnn_algo_(ToCudnnRNNAlgo(algorithm_config.algorithm())),\n        input_mode_(input_mode),\n        direction_mode_(direction_mode),\n        rnn_mode_(rnn_mode),\n        data_type_(data_type),\n        compute_type_(compute_type),\n        algorithm_config_(algorithm_config),\n        dropout_desc_(std::move(dropout_desc)),\n        params_desc_(std::move(params_desc)) {}\n\n public:\n  CudnnRnnDescriptor(CudnnRnnDescriptor&& other) = default;\n\n  static port::StatusOr<CudnnRnnDescriptor> Create(\n      const CudnnHandle& cudnn, int num_layers, int hidden_size, int input_size,\n      int cell_size, int batch_size, cudnnRNNInputMode_t input_mode,\n      cudnnDirectionMode_t direction_mode, cudnnRNNMode_t rnn_mode,\n      cudnnDataType_t data_type, cudnnDataType_t compute_type,\n      const dnn::AlgorithmConfig& algorithm_config, float dropout, uint64 seed,\n      ScratchAllocator* state_allocator, bool use_padded_io) {\n    SE_ASSIGN_OR_RETURN(\n        CudnnDropoutDescriptor dropout_desc,\n        CudnnDropoutDescriptor::Create(cudnn, dropout, seed, state_allocator));\n\n    gpu::RnnDescriptor rnn_desc = CreateRnnDescriptor();\n    cudnnRNNAlgo_t rnn_algo = ToCudnnRNNAlgo(algorithm_config.algorithm());\n\n    // TODO: allow the user to choose an algorithm.\n    auto proj_size = hidden_size;\n    hidden_size = std::max(hidden_size, cell_size);\n\n    // Require explicit algorithm config to enable tensor cores. Some configs\n    // return CUDNN_NOT_SUPPORTED when tensor ops are enabled (which is against\n    // the idiom that enabling tensor ops is only a hint: see nvbugs/2172799).\n    // We can only reasonably expect the user to handle the subsequent failure\n    // in profile mode, which is run with algorithms returned from\n    // GetRnnAlgorithms() (which are non-default and explicitly set whether to\n    // use tensor ops). CuDNN 7.2.1 fixed this issue.\n    // TODO(csigg): Minimal support cuDNN version is 7.3, clean up.\n    bool allow_tensor_ops = data_type == CUDNN_DATA_HALF;\n    if (data_type == CUDNN_DATA_FLOAT)\n      allow_tensor_ops = tensorflow::tensor_float_32_execution_enabled();\n    bool use_tensor_ops =\n        algorithm_config.algorithm().has_value()\n            ? algorithm_config.algorithm()->tensor_ops_enabled()\n            : allow_tensor_ops;\n    if (use_tensor_ops && !allow_tensor_ops) {\n      return port::Status(port::error::INVALID_ARGUMENT,\n                          \"Algo requests disallowed tensor op evaluation.\");\n    }\n\n    cudnnMathType_t math_type =\n        use_tensor_ops ? CUDNN_TENSOR_OP_MATH : CUDNN_DEFAULT_MATH;\n\n#if CUDNN_VERSION >= 8000\n    cudnnRNNBiasMode_t bias_mode = CUDNN_RNN_DOUBLE_BIAS;\n    uint32_t aux_flags = 0;\n    if (use_padded_io) aux_flags |= CUDNN_RNN_PADDED_IO_ENABLED;\n    RETURN_IF_CUDNN_ERROR(cudnnSetRNNDescriptor_v8(\n        /*rnnDesc=*/rnn_desc.get(), /*algo=*/rnn_algo, /*cellMode=*/rnn_mode,\n        /*biasMode=*/bias_mode, /*dirMode=*/direction_mode,\n        /*inputMode=*/input_mode,\n        /*dataType=*/data_type, /*mathPrec=*/compute_type,\n        /*mathType=*/math_type,\n        /*inputSize=*/input_size,\n        /*hiddenSize=*/hidden_size, /*projSize=*/proj_size,\n        /*numLayers=*/num_layers,\n        /*dropoutDesc=*/dropout_desc.handle(),\n        /*auxFlags=*/aux_flags));\n#else\n    RETURN_IF_CUDNN_ERROR(cudnnSetRNNDescriptor_v6(\n        cudnn.handle(), /*rnnDesc=*/rnn_desc.get(),\n        /*hiddenSize=*/hidden_size, /*numLayers=*/num_layers,\n        /*dropoutDesc=*/dropout_desc.handle(), /*inputMode=*/input_mode,\n        /*direction=*/direction_mode, /*mode=*/rnn_mode, /*algo=*/rnn_algo,\n        /*dataType=*/compute_type));\n    CHECK_CUDNN_OK(cudnnSetRNNMatrixMathType(rnn_desc.get(), math_type));\n\n    if (proj_size < hidden_size) {\n      RETURN_IF_CUDNN_ERROR(cudnnSetRNNProjectionLayers(\n          cudnn.handle(), /*rnnDesc=*/rnn_desc.get(),\n          /*recProjSize=*/proj_size, /*outProjSize=*/0));\n    }\n\n    // TODO: For now, we only use cudnnRNN**Ex API to process padded inputs.\n    // But in the future if these APIs are used to process full length arrays,\n    // we need to distinguish when to set it.\n    if (use_padded_io) {\n      RETURN_IF_CUDNN_ERROR(\n          cudnnSetRNNPaddingMode(rnn_desc.get(), CUDNN_RNN_PADDED_IO_ENABLED));\n    }\n#endif\n\n    port::StatusOr<PersistentRnnPlan> rnn_plan_wrapper;\n    PersistentRnnPlan rnn_plan;\n    if (rnn_algo == CUDNN_RNN_ALGO_PERSIST_DYNAMIC) {\n      CHECK_GE(batch_size, 0);\n      rnn_plan_wrapper =\n          CreatePersistentRnnPlan(rnn_desc.get(), batch_size, data_type);\n      if (!rnn_plan_wrapper.ok()) {\n        return port::StatusOr<CudnnRnnDescriptor>(rnn_plan_wrapper.status());\n      } else {\n        rnn_plan = rnn_plan_wrapper.ConsumeValueOrDie();\n        RETURN_IF_CUDNN_ERROR(\n            cudnnSetPersistentRNNPlan(rnn_desc.get(), rnn_plan.get()));\n      }\n    }\n\n    // Create the params handle.\n    SE_ASSIGN_OR_RETURN(auto params_desc,\n                        CudnnRnnParamsDescriptor::Create(\n                            cudnn, input_size, data_type, rnn_desc.get(),\n                            rnn_mode, direction_mode, num_layers));\n\n    return CudnnRnnDescriptor(cudnn, std::move(rnn_desc), std::move(rnn_plan),\n                              num_layers, hidden_size, input_size, cell_size,\n                              batch_size, input_mode, direction_mode, rnn_mode,\n                              data_type, compute_type, algorithm_config,\n                              std::move(dropout_desc), std::move(params_desc));\n  }\n\n  cudnnRNNDescriptor_t handle() const { return rnn_desc_.get(); }\n  int num_layers() const { return num_layers_; }\n  int hidden_size() const { return hidden_size_; }\n  int input_size() const { return input_size_; }\n  int cell_size() const { return cell_size_; }\n  int batch_size() const { return batch_size_; }\n  cudnnRNNInputMode_t input_mode() const { return input_mode_; }\n  cudnnDirectionMode_t direction_mode() const { return direction_mode_; }\n  cudnnRNNMode_t rnn_mode() const { return rnn_mode_; }\n  cudnnDataType_t data_type() const { return data_type_; }\n  cudnnDataType_t compute_type() const { return compute_type_; }\n  const dnn::AlgorithmConfig& algorithm_config() const {\n    return algorithm_config_;\n  }\n  int64 ParamsSizeInBytes() const override {\n    return params_desc_.params_size_in_bytes();\n  }\n  cudnnFilterDescriptor_t params_handle() const {\n    return params_desc_.handle();\n  }\n  ParamsRegions ParamsWeightRegions() const override {\n    return params_desc_.params_weights();\n  }\n  ParamsRegions ParamsBiasRegions() const override {\n    return params_desc_.params_biases();\n  }\n\n private:\n  gpu::RnnDescriptor rnn_desc_;\n  PersistentRnnPlan rnn_plan_;\n  int num_layers_;\n  int hidden_size_;\n  int input_size_;\n  // cell_size_ is the size of cell state, which will be different from\n  // hidden_size_ if the projection is used.\n  int cell_size_;\n  // batch_size_ is set to -1 when not using CUDNN_RNN_ALGO_PERSIST_DYNAMIC\n  // algorithm.\n  int batch_size_;\n  cudnnRNNAlgo_t rnn_algo_;\n  cudnnRNNInputMode_t input_mode_;\n  cudnnDirectionMode_t direction_mode_;\n  cudnnRNNMode_t rnn_mode_;\n  cudnnDataType_t data_type_;\n  cudnnDataType_t compute_type_;\n  dnn::AlgorithmConfig algorithm_config_;\n  CudnnDropoutDescriptor dropout_desc_;\n  CudnnRnnParamsDescriptor params_desc_;\n  SE_DISALLOW_COPY_AND_ASSIGN(CudnnRnnDescriptor);\n};\n\n#if CUDNN_VERSION >= 7603\nclass CudnnCtcLossDescriptor {\n public:\n  explicit CudnnCtcLossDescriptor(cudnnDataType_t data_type)\n      : handle_(CreateCtcLossDescriptor()) {\n    CHECK_CUDNN_OK(cudnnSetCTCLossDescriptorEx(\n        /*ctcLossDesc=*/handle_.get(),\n        /*compType=*/data_type,\n        /*normMode=*/CUDNN_LOSS_NORMALIZATION_SOFTMAX,\n        /*gradMode=*/CUDNN_NOT_PROPAGATE_NAN));\n  }\n\n  cudnnCTCLossDescriptor_t handle() const { return handle_.get(); }\n\n private:\n  CtcLossDescriptor handle_;  // Owned\n\n  SE_DISALLOW_COPY_AND_ASSIGN(CudnnCtcLossDescriptor);\n};\n#else\n// dummy class\nclass CudnnCtcLossDescriptor {\n public:\n  CudnnCtcLossDescriptor(cudnnDataType_t data_type) {}\n};\n#endif\n\nnamespace {\n\n// Check if the LSTM projection is used. If yes, an additional weight matrix\n// (projection matrix) will be fetched to the 'weights'. Otherwise, nothing will\n// be done.\nport::Status CheckAndFetchProjectionWeights(\n    const CudnnHandle& cudnn, cudnnRNNDescriptor_t rnn_desc, const int layer,\n    const TensorDescriptor& input_desc, const FilterDescriptor& filter_desc,\n    const FilterDescriptor& region_desc_handle,\n    dnn::RnnDescriptor::ParamsRegions* weights) {\n  int hidden_size_v;\n  int num_layers_v;\n  cudnnDropoutDescriptor_t dropout_desc;\n  cudnnRNNInputMode_t input_mode;\n  cudnnDirectionMode_t direction;\n  cudnnRNNMode_t mode;\n  cudnnRNNAlgo_t algo;\n  cudnnDataType_t data_type;\n#if CUDNN_VERSION >= 8000\n  RETURN_IF_CUDNN_ERROR(cudnnGetRNNDescriptor_v6(\n      /*handle=*/cudnn.handle(), /*rnnDesc=*/rnn_desc,\n      /*hiddenSize=*/&hidden_size_v,\n      /*numLayers=*/&num_layers_v,\n      /*dropoutDesc=*/&dropout_desc,\n      /*inputMode=*/&input_mode,\n      /*direction=*/&direction,\n      /*mode=*/&mode,\n      /*algo=*/&algo,\n      /*mathPrec=*/&data_type));\n#else\n  RETURN_IF_CUDNN_ERROR(cudnnGetRNNDescriptor(\n      /*handle=*/cudnn.handle(), /*rnnDesc=*/rnn_desc,\n      /*hiddenSize=*/&hidden_size_v,\n      /*numLayers=*/&num_layers_v,\n      /*dropoutDesc=*/&dropout_desc,\n      /*inputMode=*/&input_mode,\n      /*direction=*/&direction,\n      /*mode=*/&mode,\n      /*algo=*/&algo,\n      /*mathPrec=*/&data_type));\n#endif\n  int rec_proj_size_v;\n  int out_proj_size_v;\n  RETURN_IF_CUDNN_ERROR(cudnnGetRNNProjectionLayers(\n      /*handle=*/cudnn.handle(),\n      /*rnnDesc=*/rnn_desc,\n      /*recProjSize*/ &rec_proj_size_v,\n      /*outProjSize*/ &out_proj_size_v));\n  if (rec_proj_size_v != hidden_size_v) {\n    void* offset = nullptr;\n    int region_id = 8;\n    RETURN_IF_CUDNN_ERROR(cudnnGetRNNLinLayerMatrixParams(\n        /*handle=*/cudnn.handle(), /*rnnDesc=*/rnn_desc,\n        /*layer=*/layer, /*xDesc=*/input_desc.get(),\n        /*wDesc=*/filter_desc.get(),\n        /*w=*/nullptr, /*linLayerID=*/region_id,\n        /*linLayerMatDesc=*/region_desc_handle.get(),\n        /*linLayerMat or linLayerBias=*/&offset));\n    int dims[] = {1, 1, 1};\n    cudnnDataType_t data_type;\n    cudnnTensorFormat_t tensor_format;\n    int n_dims;\n    RETURN_IF_CUDNN_ERROR(cudnnGetFilterNdDescriptor(\n        /*filterDesc=*/region_desc_handle.get(),\n        /*nbDimsRequested=*/sizeof(dims) / sizeof(dims[0]),\n        /*dataType=*/&data_type, /*format=*/&tensor_format,\n        /*nbDims=*/&n_dims, /*filterDimA=*/dims));\n    int64 size =\n        dims[0] * dims[1] * dims[2] * CudnnDataTypeToByteSize(data_type);\n    dnn::RnnDescriptor::ParamsRegion region = {reinterpret_cast<int64>(offset),\n                                               size};\n    weights->push_back(region);\n  }\n  return port::Status::OK();\n}\n\nport::StatusOr<CudnnRnnParamsDescriptor> CudnnRnnParamsDescriptor::Create(\n    const CudnnHandle& cudnn, int input_size, cudnnDataType_t data_type,\n    cudnnRNNDescriptor_t rnn_desc, cudnnRNNMode_t rnn_mode,\n    cudnnDirectionMode_t direction_mode, int num_layers) {\n  // Query the params size.\n  TensorDescriptor input_desc = CreateTensorDescriptor();\n  int tensor_dims[] = {1, input_size, 1};\n  int strides[] = {tensor_dims[1] * tensor_dims[2], tensor_dims[2], 1};\n  RETURN_IF_CUDNN_ERROR(cudnnSetTensorNdDescriptor(\n      /*tensorDesc=*/input_desc.get(), /*dataType=*/data_type,\n      /*nbDims=*/sizeof(tensor_dims) / sizeof(tensor_dims[0]),\n      /*dimA=*/tensor_dims,\n      /*strideA=*/strides));\n\n  size_t params_size = 0;\n  RETURN_IF_CUDNN_ERROR(cudnnGetRNNParamsSize(\n      /*handle=*/cudnn.handle(), /*rnnDesc=*/rnn_desc,\n      /*xDesc=*/input_desc.get(), /*sizeInBytes=*/&params_size,\n      /*dataType=*/data_type));\n  int64 params_size_in_bytes = static_cast<int64>(params_size);\n\n  FilterDescriptor filter_desc = CreateFilterDescriptor();\n  int filter_dims[] = {static_cast<int>(params_size_in_bytes), 1, 1};\n  RETURN_IF_CUDNN_ERROR(cudnnSetFilterNdDescriptor(\n      /*filterDesc=*/filter_desc.get(), /*dataType=*/data_type,\n      /*format=*/CUDNN_TENSOR_NCHW,\n      /*nbDims=*/sizeof(filter_dims) / sizeof(filter_dims[0]),\n      /*filterDimA=*/filter_dims));\n\n  // Create the weights and biases into the params buffer\n  int region_count_per_layer = [&] {\n    switch (rnn_mode) {\n      case CUDNN_RNN_RELU:\n      case CUDNN_RNN_TANH:\n        return 2;\n      case CUDNN_LSTM:\n        return 8;\n      case CUDNN_GRU:\n        return 6;\n      default:\n        LOG(FATAL) << \"Invalid RNN Mode: \" << static_cast<int>(rnn_mode);\n        return 0;\n    }\n  }();\n\n  FilterDescriptor region_desc_handle = CreateFilterDescriptor();\n  const int layer_count =\n      direction_mode == CUDNN_UNIDIRECTIONAL ? num_layers : 2 * num_layers;\n\n  ParamsRegions weights;\n  ParamsRegions biases;\n\n  for (int layer = 0; layer < layer_count; layer++) {\n    for (int region = 0; region < region_count_per_layer; region++) {\n      for (int type = 0; type < 2; type++) {\n        void* offset = nullptr;\n        RETURN_IF_CUDNN_ERROR(\n            type == 0 ? cudnnGetRNNLinLayerMatrixParams(\n                            /*handle=*/cudnn.handle(), /*rnnDesc=*/rnn_desc,\n                            /*layer=*/layer, /*xDesc=*/input_desc.get(),\n                            /*wDesc=*/filter_desc.get(),\n                            /*w=*/nullptr, /*linLayerID=*/region,\n                            /*linLayerMatDesc=*/region_desc_handle.get(),\n                            /*linLayerMat or linLayerBias=*/&offset)\n                      : cudnnGetRNNLinLayerBiasParams(\n                            /*handle=*/cudnn.handle(), /*rnnDesc=*/rnn_desc,\n                            /*layer=*/layer, /*xDesc=*/input_desc.get(),\n                            /*wDesc=*/filter_desc.get(),\n                            /*w=*/nullptr, /*linLayerID=*/region,\n                            /*linLayerMatDesc=*/region_desc_handle.get(),\n                            /*linLayerMat or linLayerBias=*/&offset));\n        int dims[] = {1, 1, 1};\n        cudnnDataType_t data_type;\n        cudnnTensorFormat_t tensor_format;\n        int n_dims;\n        RETURN_IF_CUDNN_ERROR(cudnnGetFilterNdDescriptor(\n            /*filterDesc=*/region_desc_handle.get(),\n            /*nbDimsRequested=*/sizeof(dims) / sizeof(dims[0]),\n            /*dataType=*/&data_type, /*format=*/&tensor_format,\n            /*nbDims=*/&n_dims, /*filterDimA=*/dims));\n        int64 size =\n            dims[0] * dims[1] * dims[2] * CudnnDataTypeToByteSize(data_type);\n        dnn::RnnDescriptor::ParamsRegion region = {\n            reinterpret_cast<int64>(offset), size};\n        (type == 0 ? weights : biases).push_back(region);\n      }\n    }\n    TF_RETURN_IF_ERROR(CheckAndFetchProjectionWeights(\n        cudnn, rnn_desc, layer, input_desc, filter_desc, region_desc_handle,\n        &weights));\n  }\n\n  return CudnnRnnParamsDescriptor(std::move(filter_desc), params_size_in_bytes,\n                                  weights, biases);\n}\n\n}  // namespace\n\nclass CudnnRnnSequenceTensorDescriptor\n    : public dnn::RnnSequenceTensorDescriptor {\n  CudnnRnnSequenceTensorDescriptor(GpuExecutor* parent, int max_seq_length,\n                                   int batch_size, int data_size,\n                                   cudnnDataType_t data_type,\n                                   RNNDataDescriptor data_handle,\n                                   TensorDescriptor handle)\n      : max_seq_length_(max_seq_length),\n        batch_size_(batch_size),\n        data_size_(data_size),\n        data_type_(data_type),\n        handle_(std::move(handle)),\n        rnn_data_handle_(std::move(data_handle)),\n        handles_(max_seq_length, handle_.get()) {\n  }\n\n public:\n  CudnnRnnSequenceTensorDescriptor(CudnnRnnSequenceTensorDescriptor&&) =\n      default;\n\n  static port::StatusOr<CudnnRnnSequenceTensorDescriptor> Create(\n      GpuExecutor* parent, int max_seq_length, int batch_size, int data_size,\n      cudnnDataType_t data_type) {\n    if (max_seq_length <= 0) {\n      return port::Status(port::error::INVALID_ARGUMENT, \"max_seq_length <= 0\");\n    }\n    int dims[] = {batch_size, data_size, 1};\n    int strides[] = {dims[1] * dims[2], dims[2], 1};\n    TensorDescriptor tensor_desc = CreateTensorDescriptor();\n    RETURN_IF_CUDNN_ERROR(cudnnSetTensorNdDescriptor(\n        /*tensorDesc=*/tensor_desc.get(), /*dataType=*/data_type,\n        /*nbDims=*/sizeof(dims) / sizeof(dims[0]), /*dimA=*/dims,\n        /*strideA=*/strides));\n    return CudnnRnnSequenceTensorDescriptor(parent, max_seq_length, batch_size,\n                                            data_size, data_type,\n                                            nullptr,\n                                            std::move(tensor_desc));\n  }\n\n  static port::StatusOr<CudnnRnnSequenceTensorDescriptor> Create(\n      GpuExecutor* parent, int max_seq_length, int batch_size, int data_size,\n      const absl::Span<const int>& seq_lengths, bool time_major,\n      cudnnDataType_t data_type) {\n    if (max_seq_length <= 0) {\n      return port::Status(port::error::INVALID_ARGUMENT, \"max_seq_length <= 0\");\n    }\n    int dims[] = {batch_size, data_size, 1};\n    int strides[] = {dims[1] * dims[2], dims[2], 1};\n    TensorDescriptor tensor_desc = CreateTensorDescriptor();\n    RETURN_IF_CUDNN_ERROR(cudnnSetTensorNdDescriptor(\n        /*tensorDesc=*/tensor_desc.get(), /*dataType=*/data_type,\n        /*nbDims=*/sizeof(dims) / sizeof(dims[0]), /*dimA=*/dims,\n        /*strideA=*/strides));\n    const int* seq_lengths_array = seq_lengths.data();\n    RNNDataDescriptor data_desc = CreateRNNDataDescriptor();\n    float padding_fill = 0.0f;\n    cudnnRNNDataLayout_t layout;\n    if (time_major) {\n      layout = CUDNN_RNN_DATA_LAYOUT_SEQ_MAJOR_UNPACKED;\n    } else {\n      layout = CUDNN_RNN_DATA_LAYOUT_BATCH_MAJOR_UNPACKED;\n    }\n    RETURN_IF_CUDNN_ERROR(cudnnSetRNNDataDescriptor(\n        /*RNNDataDesc=*/data_desc.get(), /*dataType*/ data_type,\n        /*layout=*/layout,\n        /*maxSeqLength=*/max_seq_length,\n        /*batchSize=*/batch_size, /*vectorSize=*/data_size,\n        /*seqLengthArray=*/seq_lengths_array,\n        /*paddingFill*/ (void*)&padding_fill));\n    return CudnnRnnSequenceTensorDescriptor(\n        parent, max_seq_length, batch_size, data_size, data_type,\n        std::move(data_desc), std::move(tensor_desc));\n  }\n\n  const cudnnTensorDescriptor_t* handles() const { return handles_.data(); }\n  const cudnnRNNDataDescriptor_t data_handle() const {\n    return rnn_data_handle_.get();\n  }\n\n  int max_seq_length() const { return max_seq_length_; }\n  int batch_size() const { return batch_size_; }\n  int data_size() const { return data_size_; }\n  bool is_var_seq_lengths() const {\n    return rnn_data_handle_ != nullptr;\n  }\n\n private:\n  int max_seq_length_;\n  int batch_size_;\n  int data_size_;\n  cudnnDataType_t data_type_;\n  TensorDescriptor handle_;\n  RNNDataDescriptor rnn_data_handle_;\n  std::vector<cudnnTensorDescriptor_t> handles_;  // Copies of handle_.\n  SE_DISALLOW_COPY_AND_ASSIGN(CudnnRnnSequenceTensorDescriptor);\n};\n\nclass CudnnRnnStateTensorDescriptor : public dnn::RnnStateTensorDescriptor {\n public:\n  CudnnRnnStateTensorDescriptor(GpuExecutor* parent, int num_layers,\n                                int batch_size, int data_size,\n                                cudnnDataType_t data_type)\n      : handle_(CreateTensorDescriptor()),\n        num_layers_(num_layers),\n        batch_size_(batch_size),\n        data_size_(data_size),\n        data_type_(data_type) {\n    int dims[] = {num_layers, batch_size, data_size};\n    int strides[] = {dims[1] * dims[2], dims[2], 1};\n    CHECK_CUDNN_OK(cudnnSetTensorNdDescriptor(\n        /*tensorDesc=*/handle_.get(), /*dataType=*/data_type,\n        /*nbDims=*/sizeof(dims) / sizeof(dims[0]), /*dimA=*/dims,\n        /*strideA=*/strides));\n  }\n\n  cudnnTensorDescriptor_t handle() const { return handle_.get(); }\n\n  int num_layers() const { return num_layers_; }\n  int batch_size() const { return batch_size_; }\n  int data_size() const { return data_size_; }\n\n private:\n  TensorDescriptor handle_;\n  int num_layers_;\n  int batch_size_;\n  int data_size_;\n  cudnnDataType_t data_type_;\n  SE_DISALLOW_COPY_AND_ASSIGN(CudnnRnnStateTensorDescriptor);\n};\n\nnamespace {\n\nstruct RnnModelDims {\n  int num_layers = 0;\n  int batch_size = 0;\n  int max_seq_length = 0;\n  int hidden_size = 0;\n  int input_size = 0;\n  int cell_size = 0;\n  int dir_count = 0;\n};\n\ntemplate <class T>\nport::StatusOr<RnnModelDims> ExtractAndCheckRnnForward(\n    const CudnnRnnDescriptor& rnn_desc,\n    const CudnnRnnSequenceTensorDescriptor& input_desc,\n    const DeviceMemory<T>& input_data,\n    const CudnnRnnStateTensorDescriptor& input_h_desc,\n    const DeviceMemory<T>& input_h_data,\n    const CudnnRnnStateTensorDescriptor& input_c_desc,\n    const DeviceMemory<T>& input_c_data, const DeviceMemory<T>& params,\n    const CudnnRnnSequenceTensorDescriptor& output_desc,\n    const DeviceMemory<T>& output_data,\n    const CudnnRnnStateTensorDescriptor& output_h_desc,\n    const DeviceMemory<T>& output_h_data,\n    const CudnnRnnStateTensorDescriptor& output_c_desc,\n    const DeviceMemory<T>& output_c_data) {\n  // extract model parameters\n  RnnModelDims model_dims;\n  model_dims.num_layers = rnn_desc.num_layers();\n  model_dims.batch_size = input_desc.batch_size();\n  model_dims.max_seq_length = input_desc.max_seq_length();\n  model_dims.hidden_size = rnn_desc.hidden_size();\n  model_dims.input_size = input_desc.data_size();\n  model_dims.cell_size = rnn_desc.cell_size();\n  model_dims.dir_count =\n      (rnn_desc.direction_mode() == CUDNN_BIDIRECTIONAL) ? 2 : 1;\n\n  // check parameters\n  if (!(input_h_desc.num_layers() ==\n            model_dims.num_layers * model_dims.dir_count &&\n        input_h_desc.batch_size() == model_dims.batch_size &&\n        input_h_desc.data_size() == model_dims.hidden_size)) {\n    return port::Status(port::error::INVALID_ARGUMENT, \"Invalid input_h shape\");\n  }\n  // The LSTM projection will be used if input_h_desc.data_size() <\n  // input_c_desc.data_size()\n  if (!(input_h_desc.num_layers() == input_c_desc.num_layers() &&\n        input_h_desc.batch_size() == input_c_desc.batch_size() &&\n        input_h_desc.data_size() <= input_c_desc.data_size())) {\n    return port::Status(port::error::INVALID_ARGUMENT, \"Invalid input_c shape\");\n  }\n  if (!(output_desc.max_seq_length() == model_dims.max_seq_length &&\n        output_desc.batch_size() == model_dims.batch_size &&\n        output_desc.data_size() ==\n            model_dims.hidden_size * model_dims.dir_count)) {\n    return port::Status(port::error::INVALID_ARGUMENT, \"Invalid output shape\");\n  }\n  if (!(input_h_desc.num_layers() == output_h_desc.num_layers() &&\n        input_h_desc.batch_size() == output_h_desc.batch_size() &&\n        input_h_desc.data_size() == output_h_desc.data_size())) {\n    return port::Status(port::error::INVALID_ARGUMENT,\n                        \"Invalid output_h shape\");\n  }\n  if (!(input_h_desc.num_layers() == output_c_desc.num_layers() &&\n        input_h_desc.batch_size() == output_c_desc.batch_size() &&\n        input_h_desc.data_size() <= output_c_desc.data_size())) {\n    return port::Status(port::error::INVALID_ARGUMENT,\n                        \"Invalid output_c shape\");\n  }\n\n  return model_dims;\n}\n\nport::Status CheckRNNParameterSize(\n    const CudnnHandle& cudnn, const CudnnRnnDescriptor& rnn_desc,\n    const CudnnRnnSequenceTensorDescriptor& input_desc) {\n  size_t params_size_in_bytes = 0;\n  RETURN_IF_CUDNN_ERROR(cudnnGetRNNParamsSize(\n      /*handle=*/cudnn.handle(), /*rnnDesc=*/rnn_desc.handle(),\n      /*xDesc=*/input_desc.handles()[0], /*sizeInBytes=*/&params_size_in_bytes,\n      /*dataType=*/rnn_desc.data_type()));\n  if (static_cast<int64>(params_size_in_bytes) !=\n      rnn_desc.ParamsSizeInBytes()) {\n    return port::Status(port::error::INVALID_ARGUMENT,\n                        \"Mismatching RNN parameter size\");\n  }\n  return port::Status::OK();\n}\n\nport::StatusOr<DeviceMemory<uint8>> CreateRnnWorkspace(\n    Stream* stream, const CudnnHandle& cudnn,\n    const CudnnRnnDescriptor& rnn_desc,\n    const CudnnRnnSequenceTensorDescriptor& input_desc,\n    ScratchAllocator* workspace_allocator) {\n  // Query the workspace size.\n  size_t workspace_size_in_bytes = 0;\n  RETURN_IF_CUDNN_ERROR(cudnnGetRNNWorkspaceSize(\n      /*handle=*/cudnn.handle(), /*rnnDesc=*/rnn_desc.handle(),\n      /*seqLength=*/input_desc.max_seq_length(), /*xDesc=*/input_desc.handles(),\n      /*sizeInBytes=*/&workspace_size_in_bytes));\n  // Allocate the workspace.\n  if (workspace_size_in_bytes == 0) {\n    return DeviceMemory<uint8>();\n  }\n  return workspace_allocator->AllocateBytes(workspace_size_in_bytes);\n}\n\n#if CUDNN_VERSION >= 7402\nport::StatusOr<DeviceMemory<uint8>> CreateBatchNormForwardWorkspace(\n    Stream* stream, const CudnnHandle& cudnn, const cudnnBatchNormMode_t& mode,\n    const cudnnBatchNormOps_t& bn_ops,\n    const cudnnActivationDescriptor_t& activation_desc,\n    const CudnnTensorDescriptor& x_descriptor,\n    const CudnnTensorDescriptor& scale_offset_descriptor,\n    ScratchAllocator* workspace_allocator) {\n  // Query the workspace size.\n  size_t workspace_size_in_bytes = 0;\n  RETURN_IF_CUDNN_ERROR(\n      cudnnGetBatchNormalizationForwardTrainingExWorkspaceSize(\n          /*handle=*/cudnn.handle(), /*mode=*/mode, /*bnOps=*/bn_ops,\n          /*xDesc=*/x_descriptor.handle(), /*zDesc=*/x_descriptor.handle(),\n          /*yDesc=*/x_descriptor.handle(),\n          /*bnScaleBiasMeanVarDesc=*/scale_offset_descriptor.handle(),\n          /*activationDesc=*/activation_desc,\n          /*sizeInBytes=*/&workspace_size_in_bytes));\n  // Allocate the workspace.\n  if (workspace_size_in_bytes == 0) {\n    return DeviceMemory<uint8>();\n  }\n  return workspace_allocator->AllocateBytes(workspace_size_in_bytes);\n}\n\nport::StatusOr<DeviceMemory<uint8>> CreateBatchNormBackwardWorkspace(\n    Stream* stream, const CudnnHandle& cudnn, const cudnnBatchNormMode_t& mode,\n    const cudnnBatchNormOps_t& bn_ops,\n    const CudnnTensorDescriptor& x_descriptor,\n    const CudnnTensorDescriptor& scale_offset_descriptor,\n    ScratchAllocator* workspace_allocator) {\n  // Query the workspace size.\n  size_t workspace_size_in_bytes = 0;\n  RETURN_IF_CUDNN_ERROR(cudnnGetBatchNormalizationBackwardExWorkspaceSize(\n      /*handle=*/cudnn.handle(), /*mode=*/mode, /*bnOps=*/bn_ops,\n      /*xDesc=*/x_descriptor.handle(),\n      /*yDesc=*/x_descriptor.handle(),\n      /*dyDesc=*/x_descriptor.handle(),\n      /*dzDesc=*/nullptr,\n      /*dxDesc=*/x_descriptor.handle(),\n      /*dBnScaleBiasDesc=*/scale_offset_descriptor.handle(),\n      /*activationDesc=*/nullptr, /*sizeInBytes=*/&workspace_size_in_bytes));\n  // Allocate the workspace.\n  if (workspace_size_in_bytes == 0) {\n    return DeviceMemory<uint8>();\n  }\n  return workspace_allocator->AllocateBytes(workspace_size_in_bytes);\n}\n\n#endif\n\n}  // namespace\n\ntemplate <class T>\nport::Status CudnnSupport::DoRnnForwardImpl(\n    Stream* stream, const CudnnRnnDescriptor& rnn_desc,\n    const CudnnRnnSequenceTensorDescriptor& input_desc,\n    const DeviceMemory<T>& input_data,\n    const CudnnRnnStateTensorDescriptor& input_h_desc,\n    const DeviceMemory<T>& input_h_data,\n    const CudnnRnnStateTensorDescriptor& input_c_desc,\n    const DeviceMemory<T>& input_c_data, const DeviceMemory<T>& params,\n    const CudnnRnnSequenceTensorDescriptor& output_desc,\n    DeviceMemory<T>* output_data,\n    const CudnnRnnStateTensorDescriptor& output_h_desc,\n    DeviceMemory<T>* output_h_data,\n    const CudnnRnnStateTensorDescriptor& output_c_desc,\n    DeviceMemory<T>* output_c_data, bool is_training,\n    ScratchAllocator* reserve_space_allocator,\n    ScratchAllocator* workspace_allocator,\n    dnn::ProfileResult* output_profile_result) {\n  SE_ASSIGN_OR_RETURN(\n      RnnModelDims model_dims,\n      ExtractAndCheckRnnForward(\n          rnn_desc, input_desc, input_data, input_h_desc, input_h_data,\n          input_c_desc, input_c_data, params, output_desc, *output_data,\n          output_h_desc, *output_h_data, output_c_desc, *output_c_data));\n\n  auto cudnn = cudnn_->GetHandle(parent_, stream);\n\n  SE_RETURN_IF_ERROR(CheckRNNParameterSize(cudnn, rnn_desc, input_desc));\n  SE_ASSIGN_OR_RETURN(DeviceMemory<uint8> workspace,\n                      CreateRnnWorkspace(stream, cudnn, rnn_desc, input_desc,\n                                         workspace_allocator))\n\n  // query the reserve space size\n  // allocate the reserve space\n  DeviceMemory<uint8> reserve_space;\n  if (is_training) {\n    size_t reserve_space_size_in_bytes = 0;\n    RETURN_IF_CUDNN_ERROR(cudnnGetRNNTrainingReserveSize(\n        /*handle=*/cudnn.handle(), /*rnnDesc=*/rnn_desc.handle(),\n        /*seqLength=*/model_dims.max_seq_length, /*xDesc=*/input_desc.handles(),\n        /*sizeInBytes=*/&reserve_space_size_in_bytes));\n\n    if (reserve_space_size_in_bytes > 0) {\n      SE_ASSIGN_OR_RETURN(reserve_space, reserve_space_allocator->AllocateBytes(\n                                             reserve_space_size_in_bytes));\n    }\n  }\n\n  std::unique_ptr<GpuTimer, GpuTimerDeleter> timer;\n  const bool is_profiling = output_profile_result != nullptr;\n  if (is_profiling) {\n    timer.reset(new GpuTimer(parent_));\n    // The start and stop of the timer should be as close to the Cudnn call as\n    // possible. It is still possible for other threads to issue workload on\n    // to this stream. So it could take multiple profiling measurements.\n    if (!timer->Init() || !timer->Start(AsGpuStream(stream))) {\n      return port::Status(port::error::INTERNAL, \"Failed to start timer\");\n    }\n  }\n\n  if (!is_training) {\n    if (input_desc.is_var_seq_lengths()) {\n      RETURN_IF_CUDNN_ERROR(cudnnRNNForwardInferenceEx(\n          /*handle=*/cudnn.handle(), /*rnnDesc=*/rnn_desc.handle(),\n          /*xDesc=*/input_desc.data_handle(), /*x=*/input_data.opaque(),\n          /*hxDesc=*/input_h_desc.handle(), /*hx=*/input_h_data.opaque(),\n          /*cxDesc=*/input_c_desc.handle(), /*cx=*/input_c_data.opaque(),\n          /*wDesc=*/rnn_desc.params_handle(), /*w=*/params.opaque(),\n          /*yDesc=*/output_desc.data_handle(),\n          /*y=*/output_data->opaque(),\n          /*hyDesc=*/output_h_desc.handle(), /*hy=*/output_h_data->opaque(),\n          /*cyDesc=*/output_c_desc.handle(), /*cy=*/output_c_data->opaque(),\n          nullptr, nullptr, nullptr, nullptr, nullptr, nullptr, nullptr,\n          nullptr,\n          /*workspace=*/workspace.opaque(),\n          /*workSpaceSizeInBytes=*/workspace.size()));\n    } else {\n      RETURN_IF_CUDNN_ERROR(cudnnRNNForwardInference(\n          /*handle=*/cudnn.handle(), /*rnnDesc=*/rnn_desc.handle(),\n          /*seqLength=*/model_dims.max_seq_length,\n          /*xDesc=*/input_desc.handles(),\n          /*x=*/input_data.opaque(), /*hxDesc=*/input_h_desc.handle(),\n          /*hx=*/input_h_data.opaque(), /*cxDesc=*/input_c_desc.handle(),\n          /*cx=*/input_c_data.opaque(), /*wDesc=*/rnn_desc.params_handle(),\n          /*w=*/params.opaque(), /*yDesc=*/output_desc.handles(),\n          /*y=*/output_data->opaque(), /*hyDesc=*/output_h_desc.handle(),\n          /*hy=*/output_h_data->opaque(), /*cyDesc=*/output_c_desc.handle(),\n          /*cy=*/output_c_data->opaque(), /*workspace=*/workspace.opaque(),\n          /*workSpaceSizeInBytes=*/workspace.size()));\n    }\n  } else {\n    if (input_desc.is_var_seq_lengths()) {\n      // cudnnSetRNNPaddingMode(rnn_desc.handle(), CUDNN_RNN_PADDED_IO_ENABLED);\n      RETURN_IF_CUDNN_ERROR(cudnnRNNForwardTrainingEx(\n          /*handle=*/cudnn.handle(), /*rnnDesc=*/rnn_desc.handle(),\n          /*xDesc=*/input_desc.data_handle(), /*x=*/input_data.opaque(),\n          /*hxDesc=*/input_h_desc.handle(), /*hx=*/input_h_data.opaque(),\n          /*cxDesc=*/input_c_desc.handle(), /*cx=*/input_c_data.opaque(),\n          /*wDesc=*/rnn_desc.params_handle(), /*w=*/params.opaque(),\n          /*yDesc=*/output_desc.data_handle(),\n          /*y=*/output_data->opaque(),\n          /*hyDesc=*/output_h_desc.handle(), /*hy=*/output_h_data->opaque(),\n          /*cyDesc=*/output_c_desc.handle(), /*cy=*/output_c_data->opaque(),\n          nullptr, nullptr, nullptr, nullptr, nullptr, nullptr, nullptr,\n          nullptr,\n          /*workspace=*/workspace.opaque(),\n          /*workSpaceSizeInBytes=*/workspace.size(),\n          /*reserveSpace=*/reserve_space.opaque(),\n          /*reserveSpaceSizeInBytes=*/reserve_space.size()));\n    } else {\n      RETURN_IF_CUDNN_ERROR(cudnnRNNForwardTraining(\n          /*handle=*/cudnn.handle(), /*rnnDesc=*/rnn_desc.handle(),\n          /*seqLength=*/model_dims.max_seq_length,\n          /*xDesc=*/input_desc.handles(),\n          /*x=*/input_data.opaque(), /*hxDesc=*/input_h_desc.handle(),\n          /*hx=*/input_h_data.opaque(), /*cxDesc=*/input_c_desc.handle(),\n          /*cx=*/input_c_data.opaque(), /*wDesc=*/rnn_desc.params_handle(),\n          /*w=*/params.opaque(), /*yDesc=*/output_desc.handles(),\n          /*y=*/output_data->opaque(), /*hyDesc=*/output_h_desc.handle(),\n          /*hy=*/output_h_data->opaque(), /*cyDesc=*/output_c_desc.handle(),\n          /*cy=*/output_c_data->opaque(), /*workspace=*/workspace.opaque(),\n          /*workSpaceSizeInBytes=*/workspace.size(),\n          /*reserveSpace=*/reserve_space.opaque(),\n          /*reserveSpaceSizeInBytes=*/reserve_space.size()));\n    }\n  }\n\n  if (is_profiling) {\n    if (!timer->Stop(AsGpuStream(stream))) {\n      return port::Status(port::error::INTERNAL, \"Failed to stop timer\");\n    }\n    auto algo_desc = *rnn_desc.algorithm_config().algorithm();\n    output_profile_result->set_algorithm(algo_desc);\n    output_profile_result->set_elapsed_time_in_ms(\n        timer->GetElapsedMilliseconds());\n  }\n\n  return port::Status::OK();\n}\n\ntemplate <class T>\nport::Status CudnnSupport::DoRnnBackwardImpl(\n    Stream* stream, const CudnnRnnDescriptor& rnn_desc,\n    const CudnnRnnSequenceTensorDescriptor& input_desc,\n    const DeviceMemory<T>& input_data,\n    const CudnnRnnStateTensorDescriptor& input_h_desc,\n    const DeviceMemory<T>& input_h_data,\n    const CudnnRnnStateTensorDescriptor& input_c_desc,\n    const DeviceMemory<T>& input_c_data, const DeviceMemory<T>& params,\n    const CudnnRnnSequenceTensorDescriptor& output_desc,\n    const DeviceMemory<T>& output_data,\n    const CudnnRnnStateTensorDescriptor& output_h_desc,\n    const DeviceMemory<T>& output_h_data,\n    const CudnnRnnStateTensorDescriptor& output_c_desc,\n    const DeviceMemory<T>& output_c_data,\n    const DeviceMemory<T>& output_backprop_data,\n    const DeviceMemory<T>& output_h_backprop_data,\n    const DeviceMemory<T>& output_c_backprop_data,\n    DeviceMemory<T>* input_backprop_data,\n    DeviceMemory<T>* input_h_backprop_data,\n    DeviceMemory<T>* input_c_backprop_data,\n    DeviceMemory<T>* params_backprop_data,\n    DeviceMemory<uint8>* reserve_space_data,\n    ScratchAllocator* workspace_allocator,\n    dnn::ProfileResult* output_profile_result) {\n  SE_ASSIGN_OR_RETURN(\n      RnnModelDims model_dims,\n      ExtractAndCheckRnnForward(rnn_desc, input_desc, input_data, input_h_desc,\n                                input_h_data, input_c_desc, input_c_data,\n                                params, output_desc, output_data, output_h_desc,\n                                output_h_data, output_c_desc, output_c_data));\n\n  auto cudnn = cudnn_->GetHandle(parent_, stream);\n\n  SE_RETURN_IF_ERROR(CheckRNNParameterSize(cudnn, rnn_desc, input_desc));\n  SE_ASSIGN_OR_RETURN(DeviceMemory<uint8> workspace,\n                      CreateRnnWorkspace(stream, cudnn, rnn_desc, input_desc,\n                                         workspace_allocator));\n\n  std::unique_ptr<GpuTimer, GpuTimerDeleter> timer;\n  const bool is_profiling = output_profile_result != nullptr;\n  if (is_profiling) {\n    timer.reset(new GpuTimer(parent_));\n    // The start and stop of the timer should be as close to the Cudnn call as\n    // possible. It is still possible for other threads to issue workload on\n    // to this stream. So it could take multiple profiling measurements.\n    if (!timer->Init() || !timer->Start(AsGpuStream(stream))) {\n      return port::Status(port::error::INTERNAL, \"Failed to start timer\");\n    }\n  }\n\n  if (input_desc.is_var_seq_lengths()) {\n    RETURN_IF_CUDNN_ERROR(cudnnRNNBackwardDataEx(\n        /*handle=*/cudnn.handle(), /*rnnDesc=*/rnn_desc.handle(),\n        /*yDesc=*/output_desc.data_handle(), /*y=*/output_data.opaque(),\n        /*dyDesc=*/output_desc.data_handle(),\n        /*dy=*/output_backprop_data.opaque(), nullptr, nullptr,\n        /*dhyDesc=*/output_h_desc.handle(),\n        /*dhy=*/output_h_backprop_data.opaque(),\n        /*dcyDesc=*/output_c_desc.handle(),\n        /*dcy=*/output_c_backprop_data.opaque(),\n        /*wDesc=*/rnn_desc.params_handle(), /*w=*/params.opaque(),\n        /*hxDesc=*/input_h_desc.handle(), /*hx=*/input_h_data.opaque(),\n        /*cxDesc=*/input_c_desc.handle(), /*cx=*/input_c_data.opaque(),\n        /*dxDesc=*/input_desc.data_handle(),\n        /*dx=*/input_backprop_data->opaque(),\n        /*dhxDesc=*/input_h_desc.handle(),\n        /*dhx=*/input_h_backprop_data->opaque(),\n        /*dcxDesc=*/input_c_desc.handle(),\n        /*dcx=*/input_c_backprop_data->opaque(), nullptr, nullptr,\n        /*workspace=*/workspace.opaque(),\n        /*workSpaceSizeInBytes=*/workspace.size(),\n        /*reserveSpace=*/reserve_space_data->opaque(),\n        /*reserveSpaceSizeInBytes=*/reserve_space_data->size()));\n  } else {\n    RETURN_IF_CUDNN_ERROR(cudnnRNNBackwardData(\n        /*handle=*/cudnn.handle(), /*rnnDesc=*/rnn_desc.handle(),\n        /*seqLength=*/model_dims.max_seq_length,\n        /*yDesc=*/output_desc.handles(),\n        /*y=*/output_data.opaque(), /*dyDesc=*/output_desc.handles(),\n        /*dy=*/output_backprop_data.opaque(),\n        /*dhyDesc=*/output_h_desc.handle(),\n        /*dhy=*/output_h_backprop_data.opaque(),\n        /*dcyDesc=*/output_c_desc.handle(),\n        /*dcy=*/output_c_backprop_data.opaque(),\n        /*wDesc=*/rnn_desc.params_handle(), /*w=*/params.opaque(),\n        /*hxDesc=*/input_h_desc.handle(), /*hx=*/input_h_data.opaque(),\n        /*cxDesc=*/input_c_desc.handle(), /*cx=*/input_c_data.opaque(),\n        /*dxDesc=*/input_desc.handles(), /*dx=*/input_backprop_data->opaque(),\n        /*dhxDesc=*/input_h_desc.handle(),\n        /*dhx=*/input_h_backprop_data->opaque(),\n        /*dcxDesc=*/input_c_desc.handle(),\n        /*dcx=*/input_c_backprop_data->opaque(),\n        /*workspace=*/workspace.opaque(),\n        /*workSpaceSizeInBytes=*/workspace.size(),\n        /*reserveSpace=*/reserve_space_data->opaque(),\n        /*reserveSpaceSizeInBytes=*/reserve_space_data->size()));\n  }\n\n  if (params_backprop_data != nullptr) {\n    // Clear the dw to zeros.\n    stream->ThenMemZero(params_backprop_data, params_backprop_data->size());\n    if (input_desc.is_var_seq_lengths()) {\n      RETURN_IF_CUDNN_ERROR(cudnnRNNBackwardWeightsEx(\n          /*handle=*/cudnn.handle(), /*rnnDesc=*/rnn_desc.handle(),\n          /*xDesc=*/input_desc.data_handle(), /*x=*/input_data.opaque(),\n          /*hxDesc=*/input_h_desc.handle(), /*hx=*/input_h_data.opaque(),\n          /*yDesc=*/output_desc.data_handle(),\n          /*y=*/output_data.opaque(),\n          /*workspace=*/workspace.opaque(),\n          /*workSpaceSizeInBytes=*/workspace.size(),\n          /*dwDesc=*/rnn_desc.params_handle(),\n          /*dw=*/params_backprop_data->opaque(),\n          /*reserveSpace=*/reserve_space_data->opaque(),\n          /*reserveSpaceSizeInBytes=*/reserve_space_data->size()));\n    } else {\n      // make the backward weight call\n      RETURN_IF_CUDNN_ERROR(cudnnRNNBackwardWeights(\n          /*handle=*/cudnn.handle(), /*rnnDesc=*/rnn_desc.handle(),\n          /*seqLength=*/model_dims.max_seq_length,\n          /*xDesc=*/input_desc.handles(),\n          /*x=*/input_data.opaque(), /*hxDesc=*/input_h_desc.handle(),\n          /*hx=*/input_h_data.opaque(), /*yDesc=*/output_desc.handles(),\n          /*y=*/output_data.opaque(), /*workspace=*/workspace.opaque(),\n          /*workSpaceSizeInBytes=*/workspace.size(),\n          /*dwDesc=*/rnn_desc.params_handle(),\n          /*dw=*/params_backprop_data->opaque(),\n          /*reserveSpace=*/reserve_space_data->opaque(),\n          /*reserveSpaceSizeInBytes=*/reserve_space_data->size()));\n    }\n  }\n\n  if (is_profiling) {\n    if (!timer->Stop(AsGpuStream(stream))) {\n      return port::Status(port::error::INTERNAL, \"Failed to stop timer\");\n    }\n    auto algo_desc = *rnn_desc.algorithm_config().algorithm();\n    output_profile_result->set_algorithm(algo_desc);\n    output_profile_result->set_elapsed_time_in_ms(\n        timer->GetElapsedMilliseconds());\n  }\n\n  return port::Status::OK();\n}\n\nport::Status CudnnSupport::DoCtcLossImpl(\n    Stream* stream, const CudnnRnnStateTensorDescriptor& probs_desc,\n    const DeviceMemoryBase probs_data, absl::Span<const int> labels_data,\n    absl::Span<const int> labels_lengths_data,\n    absl::Span<const int> input_lengths_data, DeviceMemoryBase costs_data,\n    const CudnnRnnStateTensorDescriptor& grads_desc,\n    DeviceMemoryBase grads_data, const CudnnCtcLossDescriptor& ctc_loss_desc,\n    DeviceMemory<uint8> scratch_memory, int ctc_loss_algo_id) {\n  auto cudnn = cudnn_->GetHandle(parent_, stream);\n\n  int kNumTimestamps = probs_desc.num_layers();\n  int kBatchSize = probs_desc.batch_size();\n  int kNumLabels = probs_desc.data_size();\n  int total_size = kNumLabels * kNumTimestamps * kBatchSize;\n  (void)total_size;\n\n#if CUDNN_VERSION >= 7603\n  cudnnCTCLossAlgo_t ctc_loss_algo =\n      static_cast<cudnnCTCLossAlgo_t>(ctc_loss_algo_id);\n  RETURN_IF_CUDNN_ERROR(cudnnCTCLoss(\n      /*handle=*/cudnn.handle(), /*probsDesc=*/probs_desc.handle(),\n      /*probs=*/probs_data.opaque(), /*labels=*/labels_data.data(),\n      /*labelLengths=*/labels_lengths_data.data(),\n      /*inputLengths=*/input_lengths_data.data(),\n      /*costs=*/costs_data.opaque(), /*gradientsDesc=*/grads_desc.handle(),\n      /*gradients=*/grads_data.opaque(),\n      /*algo=*/ctc_loss_algo,\n      /*ctcLossDesc=*/ctc_loss_desc.handle(),\n      /*workspace=*/scratch_memory.opaque(),\n      /*workSpaceSizeInBytes=*/scratch_memory.size()));\n#else\n  return port::Status(port::error::INVALID_ARGUMENT,\n                      \"No supported cudnnCTCLoss when \"\n                      \"CUDNN_VERSION < 7.6.3\");\n#endif\n\n  return port::Status::OK();\n}\n\nport::StatusOr<std::unique_ptr<dnn::RnnDescriptor>>\nCudnnSupport::createRnnDescriptor(\n    int num_layers, int hidden_size, int input_size, int cell_size,\n    int batch_size, dnn::RnnInputMode input_mode,\n    dnn::RnnDirectionMode direction_mode, dnn::RnnMode rnn_mode,\n    dnn::DataType data_type, const dnn::AlgorithmConfig& algorithm_config,\n    float dropout, uint64 seed, ScratchAllocator* state_allocator,\n    bool use_padded_io) {\n  // Setting up a cudnnRNNDescriptor requires a cuDNN handle, but because it's\n  // not enqueueing anything into a stream, we pass in the null stream.\n  auto cudnn = cudnn_->GetHandle(parent_, /*stream=*/nullptr);\n  SE_ASSIGN_OR_RETURN(\n      CudnnRnnDescriptor rnn_desc,\n      CudnnRnnDescriptor::Create(\n          cudnn, num_layers, hidden_size, input_size, cell_size, batch_size,\n          ToCudnnRnnInputMode(input_mode),\n          ToCudnnRnnDirectionMode(direction_mode), ToCudnnRnnMode(rnn_mode),\n          ToCudnnDataType(data_type), GetRnnComputeType(data_type),\n          algorithm_config, dropout, seed, state_allocator, use_padded_io));\n  return std::unique_ptr<dnn::RnnDescriptor>(\n      new CudnnRnnDescriptor(std::move(rnn_desc)));\n}\n\nport::StatusOr<std::unique_ptr<dnn::RnnSequenceTensorDescriptor>>\nCudnnSupport::createRnnSequenceTensorDescriptor(int max_seq_length,\n                                                int batch_size, int data_size,\n                                                dnn::DataType data_type) {\n  SE_ASSIGN_OR_RETURN(CudnnRnnSequenceTensorDescriptor descriptor,\n                      CudnnRnnSequenceTensorDescriptor::Create(\n                          parent_, max_seq_length, batch_size, data_size,\n                          ToCudnnDataType(data_type)));\n  return std::unique_ptr<dnn::RnnSequenceTensorDescriptor>(\n      new CudnnRnnSequenceTensorDescriptor(std::move(descriptor)));\n}\n\nport::StatusOr<std::unique_ptr<dnn::RnnSequenceTensorDescriptor>>\nCudnnSupport::createRnnSequenceTensorDescriptor(\n    int max_seq_length, int batch_size, int data_size,\n    const absl::Span<const int>& seq_lengths, bool time_major,\n    dnn::DataType data_type) {\n  SE_ASSIGN_OR_RETURN(CudnnRnnSequenceTensorDescriptor descriptor,\n                      CudnnRnnSequenceTensorDescriptor::Create(\n                          parent_, max_seq_length, batch_size, data_size,\n                          seq_lengths, time_major, ToCudnnDataType(data_type)));\n  return std::unique_ptr<dnn::RnnSequenceTensorDescriptor>(\n      new CudnnRnnSequenceTensorDescriptor(std::move(descriptor)));\n}\n\nport::StatusOr<std::unique_ptr<dnn::RnnStateTensorDescriptor>>\nCudnnSupport::createRnnStateTensorDescriptor(int num_layer, int batch_size,\n                                             int data_size,\n                                             dnn::DataType data_type) {\n  return std::unique_ptr<dnn::RnnStateTensorDescriptor>(\n      new CudnnRnnStateTensorDescriptor(parent_, num_layer, batch_size,\n                                        data_size, ToCudnnDataType(data_type)));\n}\n\nbool CudnnSupport::DoRnnForward(\n    Stream* stream, const dnn::RnnDescriptor& rnn_desc,\n    const dnn::RnnSequenceTensorDescriptor& input_desc,\n    const DeviceMemory<Eigen::half>& input_data,\n    const dnn::RnnStateTensorDescriptor& input_h_desc,\n    const DeviceMemory<Eigen::half>& input_h_data,\n    const dnn::RnnStateTensorDescriptor& input_c_desc,\n    const DeviceMemory<Eigen::half>& input_c_data,\n    const DeviceMemory<Eigen::half>& params,\n    const dnn::RnnSequenceTensorDescriptor& output_desc,\n    DeviceMemory<Eigen::half>* output_data,\n    const dnn::RnnStateTensorDescriptor& output_h_desc,\n    DeviceMemory<Eigen::half>* output_h_data,\n    const dnn::RnnStateTensorDescriptor& output_c_desc,\n    DeviceMemory<Eigen::half>* output_c_data, bool is_training,\n    ScratchAllocator* reserve_space_allocator,\n    ScratchAllocator* workspace_allocator,\n    dnn::ProfileResult* output_profile_result) {\n  const CudnnRnnDescriptor& cudnn_rnn_desc =\n      static_cast<const CudnnRnnDescriptor&>(rnn_desc);\n  const CudnnRnnSequenceTensorDescriptor& cudnn_input_desc =\n      static_cast<const CudnnRnnSequenceTensorDescriptor&>(input_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_input_h_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(input_h_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_input_c_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(input_c_desc);\n  const CudnnRnnSequenceTensorDescriptor& cudnn_output_desc =\n      static_cast<const CudnnRnnSequenceTensorDescriptor&>(output_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_output_h_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(output_h_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_output_c_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(output_c_desc);\n  return IsStatusOk(\n      DoRnnForwardImpl<Eigen::half>(\n          stream, cudnn_rnn_desc, cudnn_input_desc, input_data,\n          cudnn_input_h_desc, input_h_data, cudnn_input_c_desc, input_c_data,\n          params, cudnn_output_desc, output_data, cudnn_output_h_desc,\n          output_h_data, cudnn_output_c_desc, output_c_data, is_training,\n          reserve_space_allocator, workspace_allocator, output_profile_result),\n      /*report_error=*/!output_profile_result);\n}\n\nbool CudnnSupport::DoRnnForward(\n    Stream* stream, const dnn::RnnDescriptor& rnn_desc,\n    const dnn::RnnSequenceTensorDescriptor& input_desc,\n    const DeviceMemory<float>& input_data,\n    const dnn::RnnStateTensorDescriptor& input_h_desc,\n    const DeviceMemory<float>& input_h_data,\n    const dnn::RnnStateTensorDescriptor& input_c_desc,\n    const DeviceMemory<float>& input_c_data, const DeviceMemory<float>& params,\n    const dnn::RnnSequenceTensorDescriptor& output_desc,\n    DeviceMemory<float>* output_data,\n    const dnn::RnnStateTensorDescriptor& output_h_desc,\n    DeviceMemory<float>* output_h_data,\n    const dnn::RnnStateTensorDescriptor& output_c_desc,\n    DeviceMemory<float>* output_c_data, bool is_training,\n    ScratchAllocator* reserve_space_allocator,\n    ScratchAllocator* workspace_allocator,\n    dnn::ProfileResult* output_profile_result) {\n  const CudnnRnnDescriptor& cudnn_rnn_desc =\n      static_cast<const CudnnRnnDescriptor&>(rnn_desc);\n  const CudnnRnnSequenceTensorDescriptor& cudnn_input_desc =\n      static_cast<const CudnnRnnSequenceTensorDescriptor&>(input_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_input_h_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(input_h_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_input_c_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(input_c_desc);\n  const CudnnRnnSequenceTensorDescriptor& cudnn_output_desc =\n      static_cast<const CudnnRnnSequenceTensorDescriptor&>(output_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_output_h_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(output_h_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_output_c_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(output_c_desc);\n  return IsStatusOk(\n      DoRnnForwardImpl<float>(\n          stream, cudnn_rnn_desc, cudnn_input_desc, input_data,\n          cudnn_input_h_desc, input_h_data, cudnn_input_c_desc, input_c_data,\n          params, cudnn_output_desc, output_data, cudnn_output_h_desc,\n          output_h_data, cudnn_output_c_desc, output_c_data, is_training,\n          reserve_space_allocator, workspace_allocator, output_profile_result),\n      /*report_error=*/!output_profile_result);\n}\n\nbool CudnnSupport::DoRnnForward(\n    Stream* stream, const dnn::RnnDescriptor& rnn_desc,\n    const dnn::RnnSequenceTensorDescriptor& input_desc,\n    const DeviceMemory<double>& input_data,\n    const dnn::RnnStateTensorDescriptor& input_h_desc,\n    const DeviceMemory<double>& input_h_data,\n    const dnn::RnnStateTensorDescriptor& input_c_desc,\n    const DeviceMemory<double>& input_c_data,\n    const DeviceMemory<double>& params,\n    const dnn::RnnSequenceTensorDescriptor& output_desc,\n    DeviceMemory<double>* output_data,\n    const dnn::RnnStateTensorDescriptor& output_h_desc,\n    DeviceMemory<double>* output_h_data,\n    const dnn::RnnStateTensorDescriptor& output_c_desc,\n    DeviceMemory<double>* output_c_data, bool is_training,\n    ScratchAllocator* reserve_space_allocator,\n    ScratchAllocator* workspace_allocator,\n    dnn::ProfileResult* output_profile_result) {\n  const CudnnRnnDescriptor& cudnn_rnn_desc =\n      static_cast<const CudnnRnnDescriptor&>(rnn_desc);\n  const CudnnRnnSequenceTensorDescriptor& cudnn_input_desc =\n      static_cast<const CudnnRnnSequenceTensorDescriptor&>(input_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_input_h_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(input_h_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_input_c_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(input_c_desc);\n  const CudnnRnnSequenceTensorDescriptor& cudnn_output_desc =\n      static_cast<const CudnnRnnSequenceTensorDescriptor&>(output_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_output_h_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(output_h_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_output_c_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(output_c_desc);\n  return IsStatusOk(\n      DoRnnForwardImpl<double>(\n          stream, cudnn_rnn_desc, cudnn_input_desc, input_data,\n          cudnn_input_h_desc, input_h_data, cudnn_input_c_desc, input_c_data,\n          params, cudnn_output_desc, output_data, cudnn_output_h_desc,\n          output_h_data, cudnn_output_c_desc, output_c_data, is_training,\n          reserve_space_allocator, workspace_allocator, output_profile_result),\n      /*report_error=*/!output_profile_result);\n}\n\nbool CudnnSupport::DoRnnBackward(\n    Stream* stream, const dnn::RnnDescriptor& rnn_desc,\n    const dnn::RnnSequenceTensorDescriptor& input_desc,\n    const DeviceMemory<Eigen::half>& input_data,\n    const dnn::RnnStateTensorDescriptor& input_h_desc,\n    const DeviceMemory<Eigen::half>& input_h_data,\n    const dnn::RnnStateTensorDescriptor& input_c_desc,\n    const DeviceMemory<Eigen::half>& input_c_data,\n    const DeviceMemory<Eigen::half>& params,\n    const dnn::RnnSequenceTensorDescriptor& output_desc,\n    const DeviceMemory<Eigen::half>& output_data,\n    const dnn::RnnStateTensorDescriptor& output_h_desc,\n    const DeviceMemory<Eigen::half>& output_h_data,\n    const dnn::RnnStateTensorDescriptor& output_c_desc,\n    const DeviceMemory<Eigen::half>& output_c_data,\n    const DeviceMemory<Eigen::half>& output_backprop_data,\n    const DeviceMemory<Eigen::half>& output_h_backprop_data,\n    const DeviceMemory<Eigen::half>& output_c_backprop_data,\n    DeviceMemory<Eigen::half>* input_backprop_data,\n    DeviceMemory<Eigen::half>* input_h_backprop_data,\n    DeviceMemory<Eigen::half>* input_c_backprop_data,\n    DeviceMemory<Eigen::half>* params_backprop_data,\n    DeviceMemory<uint8>* reserve_space_data,\n    ScratchAllocator* workspace_allocator,\n    dnn::ProfileResult* output_profile_result) {\n  const CudnnRnnDescriptor& cudnn_rnn_desc =\n      static_cast<const CudnnRnnDescriptor&>(rnn_desc);\n  const CudnnRnnSequenceTensorDescriptor& cudnn_input_desc =\n      static_cast<const CudnnRnnSequenceTensorDescriptor&>(input_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_input_h_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(input_h_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_input_c_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(input_c_desc);\n  const CudnnRnnSequenceTensorDescriptor& cudnn_output_desc =\n      static_cast<const CudnnRnnSequenceTensorDescriptor&>(output_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_output_h_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(output_h_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_output_c_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(output_c_desc);\n  return IsStatusOk(\n      DoRnnBackwardImpl<Eigen::half>(\n          stream, cudnn_rnn_desc, cudnn_input_desc, input_data,\n          cudnn_input_h_desc, input_h_data, cudnn_input_c_desc, input_c_data,\n          params, cudnn_output_desc, output_data, cudnn_output_h_desc,\n          output_h_data, cudnn_output_c_desc, output_c_data,\n          output_backprop_data, output_h_backprop_data, output_c_backprop_data,\n          input_backprop_data, input_h_backprop_data, input_c_backprop_data,\n          params_backprop_data, reserve_space_data, workspace_allocator,\n          output_profile_result),\n      /*report_error=*/!output_profile_result);\n}\n\nbool CudnnSupport::DoRnnBackward(\n    Stream* stream, const dnn::RnnDescriptor& rnn_desc,\n    const dnn::RnnSequenceTensorDescriptor& input_desc,\n    const DeviceMemory<float>& input_data,\n    const dnn::RnnStateTensorDescriptor& input_h_desc,\n    const DeviceMemory<float>& input_h_data,\n    const dnn::RnnStateTensorDescriptor& input_c_desc,\n    const DeviceMemory<float>& input_c_data, const DeviceMemory<float>& params,\n    const dnn::RnnSequenceTensorDescriptor& output_desc,\n    const DeviceMemory<float>& output_data,\n    const dnn::RnnStateTensorDescriptor& output_h_desc,\n    const DeviceMemory<float>& output_h_data,\n    const dnn::RnnStateTensorDescriptor& output_c_desc,\n    const DeviceMemory<float>& output_c_data,\n    const DeviceMemory<float>& output_backprop_data,\n    const DeviceMemory<float>& output_h_backprop_data,\n    const DeviceMemory<float>& output_c_backprop_data,\n    DeviceMemory<float>* input_backprop_data,\n    DeviceMemory<float>* input_h_backprop_data,\n    DeviceMemory<float>* input_c_backprop_data,\n    DeviceMemory<float>* params_backprop_data,\n    DeviceMemory<uint8>* reserve_space_data,\n    ScratchAllocator* workspace_allocator,\n    dnn::ProfileResult* output_profile_result) {\n  const CudnnRnnDescriptor& cudnn_rnn_desc =\n      static_cast<const CudnnRnnDescriptor&>(rnn_desc);\n  const CudnnRnnSequenceTensorDescriptor& cudnn_input_desc =\n      static_cast<const CudnnRnnSequenceTensorDescriptor&>(input_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_input_h_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(input_h_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_input_c_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(input_c_desc);\n  const CudnnRnnSequenceTensorDescriptor& cudnn_output_desc =\n      static_cast<const CudnnRnnSequenceTensorDescriptor&>(output_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_output_h_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(output_h_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_output_c_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(output_c_desc);\n  return IsStatusOk(\n      DoRnnBackwardImpl<float>(\n          stream, cudnn_rnn_desc, cudnn_input_desc, input_data,\n          cudnn_input_h_desc, input_h_data, cudnn_input_c_desc, input_c_data,\n          params, cudnn_output_desc, output_data, cudnn_output_h_desc,\n          output_h_data, cudnn_output_c_desc, output_c_data,\n          output_backprop_data, output_h_backprop_data, output_c_backprop_data,\n          input_backprop_data, input_h_backprop_data, input_c_backprop_data,\n          params_backprop_data, reserve_space_data, workspace_allocator,\n          output_profile_result),\n      /*report_error=*/!output_profile_result);\n}\n\nbool CudnnSupport::DoRnnBackward(\n    Stream* stream, const dnn::RnnDescriptor& rnn_desc,\n    const dnn::RnnSequenceTensorDescriptor& input_desc,\n    const DeviceMemory<double>& input_data,\n    const dnn::RnnStateTensorDescriptor& input_h_desc,\n    const DeviceMemory<double>& input_h_data,\n    const dnn::RnnStateTensorDescriptor& input_c_desc,\n    const DeviceMemory<double>& input_c_data,\n    const DeviceMemory<double>& params,\n    const dnn::RnnSequenceTensorDescriptor& output_desc,\n    const DeviceMemory<double>& output_data,\n    const dnn::RnnStateTensorDescriptor& output_h_desc,\n    const DeviceMemory<double>& output_h_data,\n    const dnn::RnnStateTensorDescriptor& output_c_desc,\n    const DeviceMemory<double>& output_c_data,\n    const DeviceMemory<double>& output_backprop_data,\n    const DeviceMemory<double>& output_h_backprop_data,\n    const DeviceMemory<double>& output_c_backprop_data,\n    DeviceMemory<double>* input_backprop_data,\n    DeviceMemory<double>* input_h_backprop_data,\n    DeviceMemory<double>* input_c_backprop_data,\n    DeviceMemory<double>* params_backprop_data,\n    DeviceMemory<uint8>* reserve_space_data,\n    ScratchAllocator* workspace_allocator,\n    dnn::ProfileResult* output_profile_result) {\n  const CudnnRnnDescriptor& cudnn_rnn_desc =\n      static_cast<const CudnnRnnDescriptor&>(rnn_desc);\n  const CudnnRnnSequenceTensorDescriptor& cudnn_input_desc =\n      static_cast<const CudnnRnnSequenceTensorDescriptor&>(input_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_input_h_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(input_h_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_input_c_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(input_c_desc);\n  const CudnnRnnSequenceTensorDescriptor& cudnn_output_desc =\n      static_cast<const CudnnRnnSequenceTensorDescriptor&>(output_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_output_h_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(output_h_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_output_c_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(output_c_desc);\n  return IsStatusOk(\n      DoRnnBackwardImpl<double>(\n          stream, cudnn_rnn_desc, cudnn_input_desc, input_data,\n          cudnn_input_h_desc, input_h_data, cudnn_input_c_desc, input_c_data,\n          params, cudnn_output_desc, output_data, cudnn_output_h_desc,\n          output_h_data, cudnn_output_c_desc, output_c_data,\n          output_backprop_data, output_h_backprop_data, output_c_backprop_data,\n          input_backprop_data, input_h_backprop_data, input_c_backprop_data,\n          params_backprop_data, reserve_space_data, workspace_allocator,\n          output_profile_result),\n      /*report_error=*/!output_profile_result);\n}\n\nnamespace {\n\n// TODO(csigg): Merge a lot of duplicate code below for forward, backward data,\n// and backward filter.\n\nport::StatusOr<cudnnConvolutionFwdAlgo_t> GetCudnnConvolutionForwardAlgo(\n    const CudnnHandle& cudnn, const CudnnTensorDescriptor& input_nd,\n    const CudnnFilterDescriptor& filter, const CudnnConvolutionDescriptor& conv,\n    const CudnnTensorDescriptor& output_nd, bool specify_workspace_limit,\n    size_t memory_limit_bytes) {\n#if CUDNN_VERSION >= 8000\n  const int num_requested_algos = 5;\n  int num_returned_algos = 0;\n  cudnnConvolutionFwdAlgoPerf_t perf_results[num_requested_algos];\n\n  RETURN_IF_CUDNN_ERROR(cudnnGetConvolutionForwardAlgorithm_v7(\n      cudnn.handle(), input_nd.handle(), filter.handle(), conv.handle(),\n      output_nd.handle(), num_requested_algos, &num_returned_algos,\n      perf_results));\n\n  size_t mem_limit = specify_workspace_limit ? memory_limit_bytes : 0ULL;\n  for (int r = 0; r < num_returned_algos; r++) {\n    if (perf_results[r].status == CUDNN_STATUS_SUCCESS &&\n        perf_results[r].algo != CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED &&\n        perf_results[r].memory <= mem_limit) {\n      return perf_results[r].algo;\n    }\n  }\n  return port::Status(port::error::INTERNAL,\n                      \"cudnnGetConvolutionForwardAlgorithm_v7 returned \"\n                      \"no suitable algorithms. This could be a cudnn bug.\");\n#else\n  cudnnConvolutionFwdPreference_t preference =\n      specify_workspace_limit ? CUDNN_CONVOLUTION_FWD_SPECIFY_WORKSPACE_LIMIT\n                              : CUDNN_CONVOLUTION_FWD_NO_WORKSPACE;\n  cudnnConvolutionFwdAlgo_t algo_to_use;\n  RETURN_IF_CUDNN_ERROR(cudnnGetConvolutionForwardAlgorithm(\n      cudnn.handle(), input_nd.handle(), filter.handle(), conv.handle(),\n      output_nd.handle(), preference, memory_limit_bytes, &algo_to_use));\n  return algo_to_use;\n#endif\n}\n\nport::StatusOr<cudnnConvolutionBwdDataAlgo_t>\nGetCudnnConvolutionBackwardDataAlgo(const CudnnHandle& cudnn,\n                                    const CudnnTensorDescriptor& input_nd,\n                                    const CudnnFilterDescriptor& filter,\n                                    const CudnnConvolutionDescriptor& conv,\n                                    const CudnnTensorDescriptor& output_nd,\n                                    bool specify_workspace_limit,\n                                    size_t memory_limit_bytes) {\n#if CUDNN_VERSION >= 8000\n  const int num_requested_algos = 5;\n  int num_returned_algos = 0;\n  cudnnConvolutionBwdDataAlgoPerf_t perf_results[num_requested_algos];\n\n  RETURN_IF_CUDNN_ERROR(cudnnGetConvolutionBackwardDataAlgorithm_v7(\n      cudnn.handle(), filter.handle(), output_nd.handle(), conv.handle(),\n      input_nd.handle(), num_requested_algos, &num_returned_algos,\n      perf_results));\n\n  size_t mem_limit = specify_workspace_limit ? memory_limit_bytes : 0ULL;\n  for (int r = 0; r < num_returned_algos; r++) {\n    if (perf_results[r].status == CUDNN_STATUS_SUCCESS &&\n        perf_results[r].algo !=\n            CUDNN_CONVOLUTION_BWD_DATA_ALGO_WINOGRAD_NONFUSED &&\n        perf_results[r].memory <= mem_limit) {\n      return perf_results[r].algo;\n    }\n  }\n  return port::Status(port::error::INTERNAL,\n                      \"cudnnGetConvolutionBackwardDataAlgorithm_v7 returned \"\n                      \"no suitable algorithms. This could be a cudnn bug.\");\n#else\n  cudnnConvolutionBwdDataPreference_t preference =\n      specify_workspace_limit\n          ? CUDNN_CONVOLUTION_BWD_DATA_SPECIFY_WORKSPACE_LIMIT\n          : CUDNN_CONVOLUTION_BWD_DATA_NO_WORKSPACE;\n  cudnnConvolutionBwdDataAlgo_t algo_to_use;\n  RETURN_IF_CUDNN_ERROR(cudnnGetConvolutionBackwardDataAlgorithm(\n      cudnn.handle(), filter.handle(), output_nd.handle(), conv.handle(),\n      input_nd.handle(), preference, memory_limit_bytes, &algo_to_use));\n  return algo_to_use;\n#endif\n}\n\nport::StatusOr<cudnnConvolutionBwdFilterAlgo_t>\nGetCudnnConvolutionBackwardFilterAlgo(const CudnnHandle& cudnn,\n                                      const CudnnTensorDescriptor& input_nd,\n                                      const CudnnFilterDescriptor& filter,\n                                      const CudnnConvolutionDescriptor& conv,\n                                      const CudnnTensorDescriptor& output_nd,\n                                      bool specify_workspace_limit,\n                                      size_t memory_limit_bytes) {\n#if CUDNN_VERSION >= 8000\n  const int num_requested_algos = 5;\n  int num_returned_algos = 0;\n  cudnnConvolutionBwdFilterAlgoPerf_t perf_results[num_requested_algos];\n\n  RETURN_IF_CUDNN_ERROR(cudnnGetConvolutionBackwardFilterAlgorithm_v7(\n      cudnn.handle(), input_nd.handle(), output_nd.handle(), conv.handle(),\n      filter.handle(), num_requested_algos, &num_returned_algos, perf_results));\n\n  size_t mem_limit = specify_workspace_limit ? memory_limit_bytes : 0ULL;\n  for (int r = 0; r < num_returned_algos; r++) {\n    if (perf_results[r].status == CUDNN_STATUS_SUCCESS &&\n        perf_results[r].algo !=\n            CUDNN_CONVOLUTION_BWD_FILTER_ALGO_WINOGRAD_NONFUSED &&\n        perf_results[r].memory <= mem_limit) {\n      return perf_results[r].algo;\n    }\n  }\n  return port::Status(port::error::INTERNAL,\n                      \"cudnnGetConvolutionBackwardFilterAlgorithm_v7 returned \"\n                      \"no suitable algorithms. This could be a cudnn bug.\");\n#else\n  cudnnConvolutionBwdFilterPreference_t preference =\n      specify_workspace_limit\n          ? CUDNN_CONVOLUTION_BWD_FILTER_SPECIFY_WORKSPACE_LIMIT\n          : CUDNN_CONVOLUTION_BWD_FILTER_NO_WORKSPACE;\n  cudnnConvolutionBwdFilterAlgo_t algo_to_use;\n  RETURN_IF_CUDNN_ERROR(cudnnGetConvolutionBackwardFilterAlgorithm(\n      cudnn.handle(), input_nd.handle(), output_nd.handle(), conv.handle(),\n      filter.handle(), preference, memory_limit_bytes, &algo_to_use));\n  return algo_to_use;\n#endif\n}\n\nport::StatusOr<DeviceMemory<uint8>> AllocateCudnnConvolutionForwardWorkspace(\n    Stream* stream, const CudnnHandle& cudnn,\n    const CudnnTensorDescriptor& input_nd, const CudnnFilterDescriptor& filter,\n    const CudnnConvolutionDescriptor& conv,\n    const CudnnTensorDescriptor& output_nd,\n    const dnn::AlgorithmDesc& algorithm_desc,\n    ScratchAllocator* scratch_allocator) {\n  if (IsTensorMathOpSet(conv) != algorithm_desc.tensor_ops_enabled()) {\n    return port::Status(\n        port::error::INTERNAL,\n        \"Mismatch between cudnn conv and algorithm descriptors.\");\n  }\n\n  // Query the size of the workspace and allocate it.\n  size_t size_in_bytes;\n  RETURN_IF_CUDNN_ERROR(cudnnGetConvolutionForwardWorkspaceSize(\n      cudnn.handle(),\n      /*xDesc=*/input_nd.handle(),\n      /*wDesc=*/filter.handle(), /*convDesc=*/conv.handle(),\n      /*yDesc=*/output_nd.handle(), /*algo=*/ToConvForwardAlgo(algorithm_desc),\n      /*sizeInBytes=*/&size_in_bytes));\n\n  int64 size_in_bytes_int64 = size_in_bytes;\n\n  if (TF_PREDICT_FALSE(size_in_bytes_int64 < 0)) {\n    return port::Status(\n        port::error::INTERNAL,\n        \"cudnnGetConvolutionForwardWorkspaceSize() returned \"\n        \"negative sizeInBytes value. This could be a cudnn bug.\");\n  }\n\n  if (size_in_bytes_int64 == 0) {\n    return DeviceMemory<uint8>();\n  }\n\n  if (TF_PREDICT_FALSE(!scratch_allocator)) {\n    return port::Status(port::error::INVALID_ARGUMENT,\n                        \"No scratch allocator provided\");\n  }\n\n  return scratch_allocator->AllocateBytes(size_in_bytes);\n}\n\nport::StatusOr<DeviceMemory<uint8>>\nAllocateCudnnConvolutionBackwardDataWorkspace(\n    Stream* stream, const CudnnHandle& cudnn,\n    const CudnnTensorDescriptor& input_nd, const CudnnFilterDescriptor& filter,\n    const CudnnConvolutionDescriptor& conv,\n    const CudnnTensorDescriptor& output_nd,\n    const dnn::AlgorithmDesc& algorithm_desc,\n    ScratchAllocator* scratch_allocator) {\n  if (IsTensorMathOpSet(conv) != algorithm_desc.tensor_ops_enabled()) {\n    return port::Status(\n        port::error::INTERNAL,\n        \"Mismatch between cudnn conv and algorithm descriptors.\");\n  }\n\n  // Query the size of the workspace and allocate it.\n  size_t size_in_bytes;\n  RETURN_IF_CUDNN_ERROR(cudnnGetConvolutionBackwardDataWorkspaceSize(\n      cudnn.handle(),\n      /*wDesc=*/filter.handle(),\n      /*dyDesc=*/output_nd.handle(),\n      /*convDesc=*/conv.handle(),\n      /*dxDesc=*/input_nd.handle(),\n      /*algo=*/ToConvBackwardDataAlgo(algorithm_desc),\n      /*sizeInBytes=*/&size_in_bytes));\n\n  int64 size_in_bytes_int64 = size_in_bytes;\n\n  if (TF_PREDICT_FALSE(size_in_bytes_int64 < 0)) {\n    return port::Status(\n        port::error::INTERNAL,\n        \"cudnnGetConvolutionBackwardDataWorkspaceSize() returned \"\n        \"negative sizeInBytes value. This could be a cudnn bug.\");\n  }\n\n  if (size_in_bytes_int64 == 0) {\n    return DeviceMemory<uint8>();\n  }\n\n  if (TF_PREDICT_FALSE(!scratch_allocator)) {\n    return port::Status(port::error::INVALID_ARGUMENT,\n                        \"No scratch allocator provided\");\n  }\n\n  return scratch_allocator->AllocateBytes(size_in_bytes);\n}\n\nport::StatusOr<DeviceMemory<uint8>>\nAllocateCudnnConvolutionBackwardFilterWorkspace(\n    Stream* stream, const CudnnHandle& cudnn,\n    const CudnnTensorDescriptor& input_nd, const CudnnFilterDescriptor& filter,\n    const CudnnConvolutionDescriptor& conv,\n    const CudnnTensorDescriptor& output_nd,\n    const dnn::AlgorithmDesc& algorithm_desc,\n    ScratchAllocator* scratch_allocator) {\n  if (IsTensorMathOpSet(conv) != algorithm_desc.tensor_ops_enabled()) {\n    return port::Status(\n        port::error::INTERNAL,\n        \"Mismatch between cudnn conv and algorithm descriptors.\");\n  }\n\n  // Query the size of the workspace and allocate it.\n  size_t size_in_bytes;\n  RETURN_IF_CUDNN_ERROR(cudnnGetConvolutionBackwardFilterWorkspaceSize(\n      cudnn.handle(),\n      /*xDesc=*/input_nd.handle(),\n      /*dyDesc=*/output_nd.handle(),\n      /*convDesc=*/conv.handle(),\n      /*gradDesc=*/filter.handle(),\n      /*algo=*/ToConvBackwardFilterAlgo(algorithm_desc),\n      /*sizeInBytes=*/&size_in_bytes));\n\n  int64 size_in_bytes_int64 = size_in_bytes;\n\n  if (TF_PREDICT_FALSE(size_in_bytes_int64 < 0)) {\n    return port::Status(\n        port::error::INTERNAL,\n        \"cudnnGetConvolutionBackwardFilterWorkspaceSize() returned \"\n        \"negative sizeInBytes value. This could be a cudnn bug.\");\n  }\n\n  if (size_in_bytes_int64 == 0) {\n    return DeviceMemory<uint8>();\n  }\n\n  if (TF_PREDICT_FALSE(!scratch_allocator)) {\n    return port::Status(port::error::INVALID_ARGUMENT,\n                        \"No scratch allocator provided\");\n  }\n\n  return scratch_allocator->AllocateBytes(size_in_bytes);\n}\n\nport::StatusOr<bool> UseTensorOps(Stream* stream, dnn::DataType type,\n                                  absl::optional<dnn::AlgorithmDesc> desc) {\n  bool use_tensor_ops;\n  if (desc.has_value()) {\n    use_tensor_ops = desc->tensor_ops_enabled();\n    if (use_tensor_ops && !IsTensorMathEnabled(stream, type)) {\n      return port::Status(port::error::INVALID_ARGUMENT,\n                          \"Algo requests disabled tensor op evaluation.\");\n    }\n  } else {\n    use_tensor_ops = IsTensorMathEnabled(stream, type);\n  }\n  return use_tensor_ops;\n}\n\ncudnnDataType_t GetRnnComputeType(dnn::DataType data_type);\ndnn::DataType GetConvAccumulatorType(dnn::DataType data_type);\n\nport::StatusOr<dnn::AlgorithmDesc> GetCudnnConvolutionForwardAlgorithm(\n    Stream* stream, const CudnnHandle& cudnn,\n    const dnn::AlgorithmConfig& algorithm_config,\n    const CudnnTensorDescriptor& input_nd, const CudnnFilterDescriptor& filter,\n    dnn::DataType element_type,\n    const dnn::ConvolutionDescriptor& convolution_descriptor,\n    const CudnnTensorDescriptor& output_nd, ScratchAllocator* scratch_allocator,\n    DeviceMemory<uint8>* scratch) {\n  absl::optional<dnn::AlgorithmDesc> algo_desc = algorithm_config.algorithm();\n\n  CudnnConvolutionDescriptor conv(\n      convolution_descriptor,\n      ToCudnnDataType(GetConvAccumulatorType(element_type)));\n  bool use_tensor_ops;\n  SE_ASSIGN_OR_RETURN(use_tensor_ops,\n                      UseTensorOps(stream, element_type, algo_desc));\n  conv.set_use_tensor_op_math(use_tensor_ops);\n\n  if (!algo_desc.has_value()) {\n    // Pick fastest algorithm within memory limit according to cuDNN's\n    // heuristics.\n    bool specify_workspace_limit = scratch_allocator != nullptr;\n    auto memory_limit_bytes =\n        specify_workspace_limit\n            ? std::max(scratch_allocator->GetMemoryLimitInBytes(), int64{0})\n            : int64{0};\n    SE_ASSIGN_OR_RETURN(cudnnConvolutionFwdAlgo_t algo,\n                        GetCudnnConvolutionForwardAlgo(\n                            cudnn, input_nd, filter, conv, output_nd,\n                            specify_workspace_limit, memory_limit_bytes));\n    algo_desc = dnn::AlgorithmDesc(algo, use_tensor_ops);\n  }\n\n  const auto scratch_or = AllocateCudnnConvolutionForwardWorkspace(\n      stream, cudnn, input_nd, filter, conv, output_nd, *algo_desc,\n      scratch_allocator);\n\n  if (scratch_or.ok()) {\n    *scratch = scratch_or.ValueOrDie();\n    return *algo_desc;\n  }\n\n  algo_desc = algorithm_config.algorithm_no_scratch();\n\n  // Failed to allocate workspace for the first algorithm, fall back to the\n  // no_scratch algorithm.\n  if (!algo_desc.has_value()) {\n    return port::Status(\n        scratch_or.status().code(),\n        absl::StrCat(\"The primary convolution algorithm failed, \",\n                     \"while a secondary algorithm is not provided. \",\n                     \"Returned status: \", scratch_or.status().ToString()));\n  }\n\n  SE_ASSIGN_OR_RETURN(use_tensor_ops,\n                      UseTensorOps(stream, element_type, algo_desc));\n  conv.set_use_tensor_op_math(use_tensor_ops);\n  SE_ASSIGN_OR_RETURN(*scratch, AllocateCudnnConvolutionForwardWorkspace(\n                                    stream, cudnn, input_nd, filter, conv,\n                                    output_nd, *algo_desc, scratch_allocator));\n  return *algo_desc;\n}\n\nport::StatusOr<dnn::AlgorithmDesc> GetCudnnConvolutionBackwardDataAlgorithm(\n    Stream* stream, const CudnnHandle& cudnn,\n    const dnn::AlgorithmConfig& algorithm_config,\n    const CudnnTensorDescriptor& input_nd, const CudnnFilterDescriptor& filter,\n    dnn::DataType element_type,\n    const dnn::ConvolutionDescriptor& convolution_descriptor,\n    const CudnnTensorDescriptor& output_nd, ScratchAllocator* scratch_allocator,\n    DeviceMemory<uint8>* scratch) {\n  absl::optional<dnn::AlgorithmDesc> algo_desc = algorithm_config.algorithm();\n  CudnnConvolutionDescriptor conv(\n      convolution_descriptor,\n      ToCudnnDataType(GetConvAccumulatorType(element_type)));\n  bool use_tensor_ops;\n  SE_ASSIGN_OR_RETURN(use_tensor_ops,\n                      UseTensorOps(stream, element_type, algo_desc));\n  conv.set_use_tensor_op_math(use_tensor_ops);\n\n  if (!algo_desc.has_value()) {\n    // Pick fastest algorithm within memory limit according to cuDNN's\n    // heuristics.\n    bool specify_workspace_limit = scratch_allocator != nullptr;\n    auto memory_limit_bytes =\n        specify_workspace_limit\n            ? std::max(scratch_allocator->GetMemoryLimitInBytes(), int64{0})\n            : int64{0};\n    SE_ASSIGN_OR_RETURN(cudnnConvolutionBwdDataAlgo_t algo,\n                        GetCudnnConvolutionBackwardDataAlgo(\n                            cudnn, input_nd, filter, conv, output_nd,\n                            specify_workspace_limit, memory_limit_bytes));\n    algo_desc = dnn::AlgorithmDesc(algo, use_tensor_ops);\n  }\n\n  const auto scratch_or = AllocateCudnnConvolutionBackwardDataWorkspace(\n      stream, cudnn, input_nd, filter, conv, output_nd, *algo_desc,\n      scratch_allocator);\n\n  if (scratch_or.ok()) {\n    *scratch = scratch_or.ValueOrDie();\n    return *algo_desc;\n  }\n\n  algo_desc = algorithm_config.algorithm_no_scratch();\n\n  // Failed to allocate workspace for the first algorithm, fall back to the\n  // no_scratch algorithm.\n  if (!algo_desc.has_value()) {\n    return port::Status(\n        port::error::INVALID_ARGUMENT,\n        \"The primary convolution algorithm failed memory allocation, \"\n        \"while a secondary algorithm is not provided.\");\n  }\n\n  SE_ASSIGN_OR_RETURN(use_tensor_ops,\n                      UseTensorOps(stream, element_type, algo_desc));\n  conv.set_use_tensor_op_math(use_tensor_ops);\n  SE_ASSIGN_OR_RETURN(*scratch, AllocateCudnnConvolutionBackwardDataWorkspace(\n                                    stream, cudnn, input_nd, filter, conv,\n                                    output_nd, *algo_desc, scratch_allocator));\n  return *algo_desc;\n}\n\nport::StatusOr<dnn::AlgorithmDesc> GetCudnnConvolutionBackwardFilterAlgorithm(\n    Stream* stream, const CudnnHandle& cudnn,\n    const dnn::AlgorithmConfig& algorithm_config,\n    const CudnnTensorDescriptor& input_nd, const CudnnFilterDescriptor& filter,\n    dnn::DataType element_type,\n    const dnn::ConvolutionDescriptor& convolution_descriptor,\n    const CudnnTensorDescriptor& output_nd, ScratchAllocator* scratch_allocator,\n    DeviceMemory<uint8>* scratch) {\n  absl::optional<dnn::AlgorithmDesc> algo_desc = algorithm_config.algorithm();\n  CudnnConvolutionDescriptor conv(\n      convolution_descriptor,\n      ToCudnnDataType(GetConvAccumulatorType(element_type)));\n  bool use_tensor_ops;\n  SE_ASSIGN_OR_RETURN(use_tensor_ops,\n                      UseTensorOps(stream, element_type, algo_desc));\n  conv.set_use_tensor_op_math(use_tensor_ops);\n\n  if (!algo_desc.has_value()) {\n    // Pick fastest algorithm within memory limit according to cuDNN's\n    // heuristics.\n    bool specify_workspace_limit = scratch_allocator != nullptr;\n    auto memory_limit_bytes =\n        specify_workspace_limit\n            ? std::max(scratch_allocator->GetMemoryLimitInBytes(), int64{0})\n            : int64{0};\n    SE_ASSIGN_OR_RETURN(cudnnConvolutionBwdFilterAlgo_t algo,\n                        GetCudnnConvolutionBackwardFilterAlgo(\n                            cudnn, input_nd, filter, conv, output_nd,\n                            specify_workspace_limit, memory_limit_bytes));\n    algo_desc = dnn::AlgorithmDesc(algo, use_tensor_ops);\n  }\n\n  auto scratch_or = AllocateCudnnConvolutionBackwardFilterWorkspace(\n      stream, cudnn, input_nd, filter, conv, output_nd, *algo_desc,\n      scratch_allocator);\n\n  if (scratch_or.ok()) {\n    *scratch = scratch_or.ValueOrDie();\n    return *algo_desc;\n  }\n\n  algo_desc = algorithm_config.algorithm_no_scratch();\n\n  // Failed to allocate workspace for the first algorithm, fall back to the\n  // no_scratch algorithm.\n  if (!algo_desc.has_value()) {\n    return port::Status(\n        port::error::INVALID_ARGUMENT,\n        \"The primary convolution algorithm failed memory allocation, \"\n        \"while a secondary algorithm is not provided.\");\n  }\n\n  SE_ASSIGN_OR_RETURN(use_tensor_ops,\n                      UseTensorOps(stream, element_type, algo_desc));\n  conv.set_use_tensor_op_math(use_tensor_ops);\n  SE_ASSIGN_OR_RETURN(*scratch, AllocateCudnnConvolutionBackwardFilterWorkspace(\n                                    stream, cudnn, input_nd, filter, conv,\n                                    output_nd, *algo_desc, scratch_allocator));\n  return *algo_desc;\n}\n\n// A helper class to set env-vars and choose options for cudnn-related\n// algorithms.\ntemplate <typename EnvVar>\nclass CudnnEnvVar {\n public:\n  static bool IsEnabled() {\n    static bool is_enabled = IsEnabledImpl();\n    return is_enabled;\n  }\n\n private:\n  static bool IsEnabledImpl() {\n    const char* tf_env_var_val = getenv(EnvVar::kName);\n    if (tf_env_var_val != nullptr) {\n      absl::string_view tf_env_var_val_str(tf_env_var_val);\n      if (tf_env_var_val_str == \"0\") {\n        return false;\n      }\n      return true;\n    }\n    return EnvVar::kDefaultFlag;\n  }\n};\n\n// A helper struct to decide whether to enable the FFT_TILING algorithms for\n// forward convolution. It is disabled for cuDNN < 7 due to memory corruption\n// caused by some shapes with this algorithm. Users can explicitly enable the\n// algorithm through an env-var \"TF_ENABLE_FFT_TILING_FORWARD=1\".\nstruct FftTilingForward {\n  static constexpr const char* kName = \"TF_ENABLE_FFT_TILING_FORWARD\";\n  static constexpr bool kDefaultFlag = true;\n};\n\n// A helper struct to decide whether to enable the WINOGRAD_NONFUSED algorithms.\n// By default it is turned on, users can explicitly disable them through an\n// env-var \"TF_ENABLE_WINOGRAD_NONFUSED=0\".\n// https://github.com/tensorflow/tensorflow/pull/4901\nstruct WinogradNonfused {\n  static constexpr const char* kName = \"TF_ENABLE_WINOGRAD_NONFUSED\";\n  // NVIDIA has fixed winograd nonfused bug for cudnn v>=7. For older versions,\n  // we have a workaround.\n  static constexpr bool kDefaultFlag = true;\n};\n\n// A helper struct to decide whether to use FP32 as the internal compute type\n// for convolution when the input data type is FP16. By default it is turned on,\n// users can explicitly disable them (choose to use FP16 as the internal compute\n// type) through an env-var \"TF_FP16_CONV_USE_FP32_COMPUTE=0\".\nstruct ConvDoFP32ComputationFP16Input {\n  static constexpr const char* kName = \"TF_FP16_CONV_USE_FP32_COMPUTE\";\n  // Using FP16 as the internal compute type for convolution when the input data\n  // type is FP16 is only supported on architectures with true fp16 support\n  // (compute capability 5.3 and 6.0). Setting this to false in an unsupported\n  // architecture will cause internal errors.\n  static constexpr bool kDefaultFlag = true;\n};\n\n// A helper struct to decide whether to use FP32 as the internal compute type\n// for rnn when the input data type is FP16. At present it is turned off,\n// users can explicitly control them through an env-var\n// TF_FP16_RNN_USE_FP32_COMPUTE.\n// After the TODO below is fixed, users should almost always use fp32 compute\n// type for training. Using fp16 might suffer suboptimal accuracy due to loss\n// in precision.\nstruct RnnDoFP32ComputationFP16Input {\n  static constexpr const char* kName = \"TF_FP16_RNN_USE_FP32_COMPUTE\";\n  // TODO(jamesqin): b/78182362 flip to true when cudnn 7.1.4 fixes the bug.\n  // Before cudnn 7.1.4 RNN are always done in fp32, no matter what math\n  // precision is set.\n  // Set it temporary to false s.t. no error is raised when using fp16 inputs,\n  // fp32 math precision.\n  //\n  // cuDNN == 7.5.0 is verified to have this fixed.\n  static constexpr bool kDefaultFlag = CUDNN_VERSION >= 7500;\n};\n\ncudnnDataType_t GetRnnComputeType(dnn::DataType data_type) {\n  switch (data_type) {\n    case dnn::DataType::kFloat:\n      return CUDNN_DATA_FLOAT;\n    case dnn::DataType::kDouble:\n      return CUDNN_DATA_DOUBLE;\n    case dnn::DataType::kHalf:\n      if (CudnnEnvVar<RnnDoFP32ComputationFP16Input>::IsEnabled()) {\n        return CUDNN_DATA_FLOAT;\n      } else {\n        return CUDNN_DATA_HALF;\n      }\n    default:\n      LOG(FATAL) << \"Invalid RNN data type: \" << static_cast<int>(data_type);\n  }\n}\n\ndnn::DataType GetConvAccumulatorType(dnn::DataType data_type) {\n  switch (data_type) {\n    case dnn::DataType::kFloat:\n    case dnn::DataType::kDouble:\n      return data_type;\n    case dnn::DataType::kHalf:\n      return CudnnEnvVar<ConvDoFP32ComputationFP16Input>::IsEnabled()\n                 ? dnn::DataType::kFloat\n                 : dnn::DataType::kHalf;\n    case dnn::DataType::kInt8:\n    case dnn::DataType::kInt32:\n      return dnn::DataType::kInt32;\n    default:\n      LOG(FATAL) << \"Invalid DNN data type: \" << static_cast<int>(data_type);\n  }\n}\n}  // namespace\n\nport::Status CudnnSupport::DoPrepareForConvolution(\n    dnn::ConvolutionKind kind, dnn::DataType element_type, Stream* stream,\n    const dnn::BatchDescriptor& input_descriptor, DeviceMemoryBase input_data,\n    const dnn::FilterDescriptor& filter_descriptor,\n    DeviceMemoryBase filter_data, const dnn::BatchDescriptor& output_descriptor,\n    DeviceMemoryBase output_data,\n    const dnn::ConvolutionDescriptor& convolution_descriptor,\n    const dnn::AlgorithmConfig& algorithm_config,\n    ScratchAllocator* scratch_allocator, dnn::AlgorithmDesc* algorithm_desc,\n    DeviceMemory<uint8>* scratch_memory) {\n  CudnnTensorDescriptor input_nd(\n      input_descriptor,\n      ToCudnnDataType(element_type, input_descriptor.layout()));\n  CudnnFilterDescriptor filter_nd(\n      filter_descriptor,\n      ToCudnnDataType(element_type, filter_descriptor.layout()));\n  CudnnTensorDescriptor output_nd(\n      output_descriptor,\n      ToCudnnDataType(element_type, output_descriptor.layout()));\n\n  auto cudnn = cudnn_->GetHandle(parent_, stream);\n\n  switch (kind) {\n    case dnn::ConvolutionKind::FORWARD: {\n      SE_ASSIGN_OR_RETURN(*algorithm_desc,\n                          GetCudnnConvolutionForwardAlgorithm(\n                              stream, cudnn, algorithm_config, input_nd,\n                              filter_nd, element_type, convolution_descriptor,\n                              output_nd, scratch_allocator, scratch_memory));\n      break;\n    }\n    case dnn::ConvolutionKind::BACKWARD_DATA: {\n      SE_ASSIGN_OR_RETURN(*algorithm_desc,\n                          GetCudnnConvolutionBackwardDataAlgorithm(\n                              stream, cudnn, algorithm_config, input_nd,\n                              filter_nd, element_type, convolution_descriptor,\n                              output_nd, scratch_allocator, scratch_memory));\n      break;\n    }\n    case dnn::ConvolutionKind::BACKWARD_FILTER: {\n      SE_ASSIGN_OR_RETURN(*algorithm_desc,\n                          GetCudnnConvolutionBackwardFilterAlgorithm(\n                              stream, cudnn, algorithm_config, input_nd,\n                              filter_nd, element_type, convolution_descriptor,\n                              output_nd, scratch_allocator, scratch_memory));\n      break;\n    }\n    default:\n      return port::InternalError(\n          absl::StrCat(\"Unexpected convolution kind \", static_cast<int>(kind)));\n  }\n\n  return port::Status::OK();\n}\n\nport::Status CudnnSupport::DoConvolve(\n    dnn::ConvolutionKind kind, dnn::DataType element_type,\n    dnn::DataType output_type, Stream* stream,\n    const dnn::BatchDescriptor& input_descriptor, DeviceMemoryBase input_data,\n    const dnn::FilterDescriptor& filter_descriptor,\n    DeviceMemoryBase filter_data, const dnn::BatchDescriptor& output_descriptor,\n    DeviceMemoryBase output_data,\n    const dnn::ConvolutionDescriptor& convolution_descriptor,\n    dnn::AlgorithmDesc algorithm_desc, DeviceMemory<uint8> scratch_memory,\n    dnn::ProfileResult* output_profile_result) {\n  cudnnDataType_t cudnn_type = ToCudnnDataType(element_type);\n  CudnnTensorDescriptor input_nd(input_descriptor, cudnn_type);\n  CudnnTensorDescriptor output_nd(output_descriptor,\n                                  ToCudnnDataType(output_type));\n  CudnnFilterDescriptor filter_nd(filter_descriptor, cudnn_type);\n  auto accumulator_type = GetConvAccumulatorType(element_type);\n  CudnnConvolutionDescriptor conv(convolution_descriptor,\n                                  ToCudnnDataType(accumulator_type));\n  SE_ASSIGN_OR_RETURN(bool use_tensor_ops,\n                      UseTensorOps(stream, element_type, algorithm_desc));\n  conv.set_use_tensor_op_math(use_tensor_ops);\n\n  auto cudnn = cudnn_->GetHandle(parent_, stream);\n  // Alpha is the scaling factor for input.\n  float falpha = 1.0;\n  double dalpha = 1.0;\n  void* alpha = cudnn_type == CUDNN_DATA_DOUBLE ? static_cast<void*>(&dalpha)\n                                                : static_cast<void*>(&falpha);\n  // Beta is the scaling factor for output.\n  float fbeta = 0.0;\n  double dbeta = 0.0;\n  void* beta = cudnn_type == CUDNN_DATA_DOUBLE ? static_cast<void*>(&dbeta)\n                                               : static_cast<void*>(&fbeta);\n\n  const bool is_profiling = output_profile_result != nullptr;\n\n  std::unique_ptr<GpuTimer, GpuTimerDeleter> timer;\n  if (is_profiling) {\n    timer.reset(new GpuTimer(parent_));  // NOLINT\n    // The start and stop of the timer should be as close to the Cudnn call as\n    // possible. It is still possible for other threads to issue workload on\n    // to this stream. So it could take multiple profiling measurements.\n    if (!timer->Init() || !timer->Start(AsGpuStream(stream))) {\n      return port::Status(port::error::INTERNAL, \"Failed to start timer\");\n    }\n  }\n\n  const auto get_fwd_bugs = [&]() -> port::Status {\n    if (CUDNN_VERSION < 8000) {\n      if (algorithm_desc.algo_id() ==\n              CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM &&\n          ToCudnnDataType(element_type) == CUDNN_DATA_INT8 &&\n          ToCudnnDataType(output_type) == CUDNN_DATA_FLOAT) {\n        return port::Status(\n            port::error::FAILED_PRECONDITION,\n            \"This configuration potentially produces incorrect results.\");\n      }\n    }\n    return port::Status::OK();\n  };\n\n  auto get_bwd_data_bugs = [&]() -> port::Status {\n    return port::Status::OK();\n  };\n\n  const auto get_bwd_filter_bugs = [&]() -> port::Status {\n    return port::Status::OK();\n  };\n\n  switch (kind) {\n    case dnn::ConvolutionKind::FORWARD: {\n      SE_RETURN_IF_ERROR(get_fwd_bugs());\n      RETURN_IF_CUDNN_ERROR(cudnnConvolutionForward(\n          cudnn.handle(),\n          /*alpha=*/alpha, /*srcDesc=*/input_nd.handle(),\n          /*srcData=*/input_data.opaque(), /*filterDesc=*/filter_nd.handle(),\n          /*filterData=*/filter_data.opaque(), /*convDesc=*/conv.handle(),\n          /*algo=*/ToConvForwardAlgo(algorithm_desc),\n          /*workSpace=*/scratch_memory.opaque(),\n          /*workSpaceSizeInBytes=*/scratch_memory.size(), /*beta=*/beta,\n          /*yDesc=*/output_nd.handle(), /*y=*/output_data.opaque()));\n      break;\n    }\n    case dnn::ConvolutionKind::BACKWARD_DATA: {\n      SE_RETURN_IF_ERROR(get_bwd_data_bugs());\n      RETURN_IF_CUDNN_ERROR(cudnnConvolutionBackwardData(\n          cudnn.handle(),\n          /*alpha=*/alpha,\n          /*wDesc=*/filter_nd.handle(),\n          /*w=*/filter_data.opaque(),\n          /*dyDesc=*/output_nd.handle(),\n          /*dy=*/output_data.opaque(),\n          /*convDesc=*/conv.handle(),\n          /*algo=*/ToConvBackwardDataAlgo(algorithm_desc),\n          /*workSpace=*/scratch_memory.opaque(),\n          /*workSpaceSizeInBytes=*/scratch_memory.size(),\n          /*beta=*/beta,\n          /*dxDesc=*/input_nd.handle(),\n          /*dx=*/input_data.opaque()));\n      break;\n    }\n    case dnn::ConvolutionKind::BACKWARD_FILTER: {\n      SE_RETURN_IF_ERROR(get_bwd_filter_bugs());\n      RETURN_IF_CUDNN_ERROR(cudnnConvolutionBackwardFilter(\n          cudnn.handle(),\n          /*alpha=*/alpha,\n          /*srcDesc=*/input_nd.handle(),\n          /*srcData=*/input_data.opaque(),\n          /*diffDesc=*/output_nd.handle(),\n          /*diffData=*/output_data.opaque(),\n          /*convDesc=*/conv.handle(),\n          /*algo=*/ToConvBackwardFilterAlgo(algorithm_desc),\n          /*workSpace=*/scratch_memory.opaque(),\n          /*workSpaceSizeInBytes=*/scratch_memory.size(),\n          /*beta=*/beta,\n          /*gradDesc=*/filter_nd.handle(),\n          /*dw=*/filter_data.opaque()));\n      break;\n    }\n    default:\n      return port::InternalError(\n          absl::StrCat(\"Unexpected convolution kind \", static_cast<int>(kind)));\n  }\n\n  if (is_profiling) {\n    if (!timer->Stop(AsGpuStream(stream))) {\n      return port::Status(port::error::INTERNAL, \"Failed to stop timer\");\n    }\n    output_profile_result->set_algorithm(algorithm_desc);\n    output_profile_result->set_elapsed_time_in_ms(\n        timer->GetElapsedMilliseconds());\n    output_profile_result->set_scratch_size(scratch_memory.size());\n  }\n\n  return port::Status::OK();\n}\n\ntemplate <typename ElementType, typename BiasType, typename ScaleType,\n          typename OutputType>\nport::Status CudnnSupport::DoFusedConvolveImpl(\n    Stream* stream, const dnn::BatchDescriptor& conv_input_descriptor,\n    const DeviceMemory<ElementType>& conv_input_data,\n    ScaleType conv_input_scale, const dnn::FilterDescriptor& filter_descriptor,\n    const DeviceMemory<ElementType>& filter_data,\n    const dnn::ConvolutionDescriptor& convolution_descriptor,\n    const DeviceMemory<OutputType>& side_input_data, ScaleType side_input_scale,\n    const dnn::BatchDescriptor& bias_descriptor,\n    const DeviceMemory<BiasType>& biases, dnn::ActivationMode activation_mode,\n    const dnn::BatchDescriptor& output_descriptor,\n    DeviceMemory<OutputType>* output_data, dnn::DataType accumulator_type,\n    ScratchAllocator* scratch_allocator,\n    const dnn::AlgorithmConfig& algorithm_config,\n    dnn::ProfileResult* output_profile_result) {\n  if (activation_mode != dnn::ActivationMode::kRelu &&\n      activation_mode != dnn::ActivationMode::kNone) {\n    return port::Status(port::error::INVALID_ARGUMENT,\n                        \"cudnnConvolutionBiasActivationForward() only supports \"\n                        \"Relu or None activation.\");\n  }\n\n  CudnnTensorDescriptor conv_input_nd(\n      conv_input_descriptor,\n      GetCudnnDataType<ElementType>(conv_input_descriptor.layout()));\n  CudnnTensorDescriptor output_nd(\n      output_descriptor,\n      GetCudnnDataType<OutputType>(conv_input_descriptor.layout()));\n  CudnnFilterDescriptor filter(\n      filter_descriptor,\n      GetCudnnDataType<ElementType>(conv_input_descriptor.layout()));\n  CudnnTensorDescriptor bias_nd(bias_descriptor, GetCudnnDataType<BiasType>());\n\n  auto cudnn = cudnn_->GetHandle(parent_, stream);\n\n  const bool is_profiling = output_profile_result != nullptr;\n\n  DeviceMemory<uint8> scratch;\n  SE_ASSIGN_OR_RETURN(\n      dnn::AlgorithmDesc algo_desc,\n      GetCudnnConvolutionForwardAlgorithm(\n          stream, cudnn, algorithm_config, conv_input_nd, filter,\n          dnn::ToDataType<ElementType>::value, convolution_descriptor,\n          output_nd, scratch_allocator, &scratch));\n\n  CudnnConvolutionDescriptor conv(convolution_descriptor,\n                                  ToCudnnDataType(accumulator_type));\n  conv.set_use_tensor_op_math(algo_desc.tensor_ops_enabled());\n\n  std::unique_ptr<GpuTimer, GpuTimerDeleter> timer;\n  if (is_profiling) {\n    timer.reset(new GpuTimer(parent_));  // NOLINT\n    // The start and stop of the timer should be as close to the Cudnn call as\n    // possible. It is still possible for other threads to issue workload on\n    // to this stream. So it could take multiple profiling measurements.\n    if (!timer->Init() || !timer->Start(AsGpuStream(stream))) {\n      return port::Status(port::error::INTERNAL, \"Failed to start timer\");\n    }\n  }\n  // CUDNN v6 only supports CUDNN_NOT_PROPAGATE_NAN as the reluNanOpt for\n  // activation descriptor. Note that this will change the nan propagation\n  // behavior from separate conv, bias, and relu (which by default is\n  // CUDNN_PROPAGATE_NAN.\n  CudnnActivationDescriptor activation_desc(\n      activation_mode, CUDNN_NOT_PROPAGATE_NAN, output_descriptor.value_max());\n  auto side_input_data_ptr = (side_input_scale == 0) ? output_data->opaque()\n                                                     : side_input_data.opaque();\n\n  VLOG(2) << \"\\nconv_input_scale = \" << conv_input_scale\n          << \"\\nconv_input_nd.handle() = \" << conv_input_nd.handle()\n          << \"\\nconv_input_data.opaque() = \" << conv_input_data.opaque()\n          << \"\\nfilter.handle() = \" << filter.handle()\n          << \"\\nfilter_data.opaque() = \" << filter_data.opaque()\n          << \"\\nconv.handle() = \" << conv.handle()\n          << \"\\nalgo = \" << algo_desc.algo_id()\n          << \"\\nscratch.opaque() = \" << scratch.opaque()\n          << \"\\nscratch.size() = \" << scratch.size()\n          << \"\\nside_input_scale = \" << side_input_scale\n          << \"\\noutput_nd.handle() = \" << output_nd.handle()\n          << \"\\nside_input_data_ptr = \" << side_input_data_ptr\n          << \"\\nbias_nd.handle() = \" << bias_nd.handle()\n          << \"\\nbiases.opaque() = \" << biases.opaque()\n          << \"\\nactivation_desc.handle() = \" << activation_desc.handle()\n          << \"\\noutput_nd.handle() = \" << output_nd.handle()\n          << \"\\noutput_data->opaque() = \" << output_data->opaque();\n\n  if (IsTensorMathOpSet(conv) != algo_desc.tensor_ops_enabled()) {\n    return port::Status(port::error::FAILED_PRECONDITION,\n                        \"Tensor op math type in dnn::AlgorithmDesc does not \"\n                        \"match that of the CudnnConvolutionDescriptor\");\n  }\n\n  RETURN_IF_CUDNN_ERROR(cudnnConvolutionBiasActivationForward(\n      cudnn.handle(),\n      /*alpha1=*/&conv_input_scale,\n      /*srcDesc=*/conv_input_nd.handle(), /*srcData=*/conv_input_data.opaque(),\n      /*filterDesc=*/filter.handle(), /*filterData=*/filter_data.opaque(),\n      /*convDesc=*/conv.handle(), ToConvForwardAlgo(algo_desc),\n      /*workSpace=*/scratch.opaque(),\n      /*workSpaceSizeInBytes=*/scratch.size(), /*alpha2=*/&side_input_scale,\n      /*zDesc=*/output_nd.handle(), /*z=*/side_input_data_ptr,\n      /*biasDesc=*/bias_nd.handle(), /*bias=*/biases.opaque(),\n      /*activationDesc=*/activation_desc.handle(),\n      /*yDesc=*/output_nd.handle(), /*y=*/output_data->opaque()));\n\n  if (is_profiling) {\n    if (!timer->Stop(AsGpuStream(stream))) {\n      return port::Status(port::error::INTERNAL, \"Failed to stop timer\");\n    }\n    output_profile_result->set_algorithm(algo_desc);\n    output_profile_result->set_elapsed_time_in_ms(\n        timer->GetElapsedMilliseconds());\n    output_profile_result->set_scratch_size(scratch.size());\n  }\n\n  return port::Status::OK();\n}\n\nbool CudnnSupport::GetConvolveAlgorithms(\n    bool with_winograd_nonfused, int cc_major, int cc_minor,\n    std::vector<dnn::AlgorithmDesc>* out_algorithms) {\n  // Preload sub libs for cudnn 8.0.4+\n#if CUDNN_MAJOR >= 8 && (CUDNN_MINOR > 0 || CUDNN_PATCHLEVEL >= 4)\n  cudnnOpsInferVersionCheck();\n  cudnnCnnInferVersionCheck();\n#endif\n  bool tensor_op_math_available = TensorOpMathAvailable(cc_major);\n  out_algorithms->clear();\n\n  std::vector<dnn::AlgorithmDesc::Index> algo_types;\n  if (ConvUseDefaultAlgorithm()) {\n    // Force a fallback algorithm.\n    algo_types = {CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM};\n  } else {\n    algo_types = {\n        // clang-format off\n    CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM,\n    CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM,\n    CUDNN_CONVOLUTION_FWD_ALGO_GEMM,\n    CUDNN_CONVOLUTION_FWD_ALGO_DIRECT,\n    CUDNN_CONVOLUTION_FWD_ALGO_FFT,\n    CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD,\n        // clang-format on\n    };\n    if (CudnnEnvVar<FftTilingForward>::IsEnabled()) {\n      algo_types.push_back(CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING);\n    }\n    if (CudnnEnvVar<WinogradNonfused>::IsEnabled() && with_winograd_nonfused) {\n      algo_types.push_back(CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED);\n    }\n  }\n\n  // The algorithms are intentionally ordered for deterministic operation\n  for (auto i : algo_types) {\n    if (tensor_op_math_available) {\n      out_algorithms->push_back({i, /*use_tensor_ops=*/true});\n    }\n    out_algorithms->push_back({i, /*use_tensor_ops=*/false});\n  }\n\n  return true;\n}\n\nbool CudnnSupport::GetRnnAlgorithms(\n    std::vector<dnn::AlgorithmDesc>* out_algorithms) {\n  // Preload sub libs for cudnn 8.0.4+\n#if CUDNN_MAJOR >= 8 && (CUDNN_MINOR > 0 || CUDNN_PATCHLEVEL >= 4)\n  cudnnOpsInferVersionCheck();\n  cudnnOpsTrainVersionCheck();\n  cudnnAdvInferVersionCheck();\n  cudnnAdvTrainVersionCheck();\n#endif\n  std::vector<dnn::AlgorithmDesc::Index> algo_types = {\n      // clang-format off\n    CUDNN_RNN_ALGO_STANDARD,\n    CUDNN_RNN_ALGO_PERSIST_STATIC,\n    CUDNN_RNN_ALGO_PERSIST_DYNAMIC,\n      // clang-format on\n  };\n\n  out_algorithms->clear();\n  for (auto i : algo_types) {\n    out_algorithms->push_back({i, /*use_tensor_ops=*/false});\n    out_algorithms->push_back({i, /*use_tensor_ops=*/true});\n  }\n  return true;\n}\n\nbool CudnnSupport::GetConvolveBackwardDataAlgorithms(\n    bool with_winograd_nonfused, int cc_major, int cc_minor,\n    std::vector<dnn::AlgorithmDesc>* out_algorithms) {\n  // Preload sub libs for cudnn 8.0.4+\n#if CUDNN_MAJOR >= 8 && (CUDNN_MINOR > 0 || CUDNN_PATCHLEVEL >= 4)\n  cudnnOpsInferVersionCheck();\n  cudnnOpsTrainVersionCheck();\n  cudnnCnnInferVersionCheck();\n  cudnnCnnTrainVersionCheck();\n#endif\n  bool tensor_op_math_available = TensorOpMathAvailable(cc_major);\n  out_algorithms->clear();\n\n  std::vector<dnn::AlgorithmDesc::Index> algo_types = {\n      // clang-format off\n    CUDNN_CONVOLUTION_BWD_DATA_ALGO_1,\n    CUDNN_CONVOLUTION_BWD_DATA_ALGO_FFT,\n    CUDNN_CONVOLUTION_BWD_DATA_ALGO_FFT_TILING,\n    CUDNN_CONVOLUTION_BWD_DATA_ALGO_WINOGRAD,\n      // clang-format on\n  };\n  if (CudnnEnvVar<WinogradNonfused>::IsEnabled() && with_winograd_nonfused) {\n    algo_types.push_back(CUDNN_CONVOLUTION_BWD_DATA_ALGO_WINOGRAD_NONFUSED);\n  }\n  if (!RequireCudnnDeterminism()) {\n    algo_types.push_back(CUDNN_CONVOLUTION_BWD_DATA_ALGO_0);\n  }\n\n  // The algorithms are intentionally ordered for deterministic operation\n  for (auto i : algo_types) {\n    if (tensor_op_math_available) {\n      out_algorithms->push_back({i, /*use_tensor_ops=*/true});\n    }\n    out_algorithms->push_back({i, /*use_tensor_ops=*/false});\n  }\n\n  return true;\n}\n\nbool CudnnSupport::GetConvolveBackwardFilterAlgorithms(\n    bool with_winograd_nonfused, int cc_major, int cc_minor,\n    std::vector<dnn::AlgorithmDesc>* out_algorithms) {\n  // Preload sub libs for cudnn 8.0.4+\n#if CUDNN_MAJOR >= 8 && (CUDNN_MINOR > 0 || CUDNN_PATCHLEVEL >= 4)\n  cudnnOpsInferVersionCheck();\n  cudnnOpsTrainVersionCheck();\n  cudnnCnnInferVersionCheck();\n  cudnnCnnTrainVersionCheck();\n#endif\n  bool tensor_op_math_available = TensorOpMathAvailable(cc_major);\n  out_algorithms->clear();\n\n  std::vector<dnn::AlgorithmDesc::Index> algo_types = {\n      // clang-format off\n      CUDNN_CONVOLUTION_BWD_FILTER_ALGO_1,\n      CUDNN_CONVOLUTION_BWD_FILTER_ALGO_FFT,\n      // Based on cudnn.h, the following is not implemented.\n      // CUDNN_CONVOLUTION_BWD_FILTER_ALGO_WINOGRAD,\n\n      // Produces incorrect results for some shapes. Disabled for now, see\n      // NVIDIA bug 2072856. TODO(csigg): Only disable for subset of shapes.\n      // CUDNN_CONVOLUTION_BWD_FILTER_ALGO_FFT_TILING,\n      // clang-format on\n  };\n  if (CudnnEnvVar<WinogradNonfused>::IsEnabled() && with_winograd_nonfused) {\n    algo_types.push_back(CUDNN_CONVOLUTION_BWD_FILTER_ALGO_WINOGRAD_NONFUSED);\n  }\n  if (!RequireCudnnDeterminism()) {\n    algo_types.push_back(CUDNN_CONVOLUTION_BWD_FILTER_ALGO_0);\n    algo_types.push_back(CUDNN_CONVOLUTION_BWD_FILTER_ALGO_3);\n  }\n\n  // The algorithms are intentionally ordered for deterministic operation\n  for (auto i : algo_types) {\n    if (tensor_op_math_available) {\n      out_algorithms->push_back({i, /*use_tensor_ops=*/true});\n    }\n    out_algorithms->push_back({i, /*use_tensor_ops=*/false});\n  }\n\n  return true;\n}\n\nbool CudnnSupport::DoBatchNormalizationForward(\n    Stream* stream, const DeviceMemory<float>& x,\n    const DeviceMemory<float>& scale, const DeviceMemory<float>& offset,\n    const DeviceMemory<float>& estimated_mean,\n    const DeviceMemory<float>& estimated_variance,\n    const DeviceMemory<float>& side_input, const dnn::BatchDescriptor& x_desc,\n    const dnn::BatchDescriptor& scale_offset_desc, const double epsilon,\n    const double exponential_average_factor,\n    dnn::ActivationMode activation_mode, DeviceMemory<float>* y,\n    DeviceMemory<float>* batch_mean, DeviceMemory<float>* batch_var,\n    DeviceMemory<float>* saved_mean, DeviceMemory<float>* saved_inv_var,\n    bool is_training, ScratchAllocator* reserve_space_allocator,\n    ScratchAllocator* workspace_allocator) {\n  return IsStatusOk(\n      DoBatchNormalizationForwardImpl<float, float>(\n          stream, dnn::DataType::kFloat, dnn::DataType::kFloat, x, scale,\n          offset, estimated_mean, estimated_variance, side_input, x_desc,\n          scale_offset_desc, epsilon, exponential_average_factor,\n          activation_mode, y, batch_mean, batch_var, saved_mean, saved_inv_var,\n          is_training, reserve_space_allocator, workspace_allocator),\n      /*report_error=*/true);\n}\n\nbool CudnnSupport::DoBatchNormalizationForward(\n    Stream* stream, const DeviceMemory<Eigen::half>& x,\n    const DeviceMemory<float>& scale, const DeviceMemory<float>& offset,\n    const DeviceMemory<float>& estimated_mean,\n    const DeviceMemory<float>& estimated_variance,\n    const DeviceMemory<float>& side_input, const dnn::BatchDescriptor& x_desc,\n    const dnn::BatchDescriptor& scale_offset_desc, const double epsilon,\n    const double exponential_average_factor,\n    dnn::ActivationMode activation_mode, DeviceMemory<Eigen::half>* y,\n    DeviceMemory<float>* batch_mean, DeviceMemory<float>* batch_var,\n    DeviceMemory<float>* saved_mean, DeviceMemory<float>* saved_inv_var,\n    bool is_training, ScratchAllocator* reserve_space_allocator,\n    ScratchAllocator* workspace_allocator) {\n  return IsStatusOk(\n      DoBatchNormalizationForwardImpl<Eigen::half, float>(\n          stream, dnn::DataType::kHalf, dnn::DataType::kFloat, x, scale, offset,\n          estimated_mean, estimated_variance, side_input, x_desc,\n          scale_offset_desc, epsilon, exponential_average_factor,\n          activation_mode, y, batch_mean, batch_var, saved_mean, saved_inv_var,\n          is_training, reserve_space_allocator, workspace_allocator),\n      /*report_error=*/true);\n}\n\ntemplate <class T, class U>\nport::Status CudnnSupport::DoBatchNormalizationForwardImpl(\n    Stream* stream, dnn::DataType input_data_type,\n    dnn::DataType scale_data_type, const DeviceMemory<T>& x,\n    const DeviceMemory<U>& scale, const DeviceMemory<U>& offset,\n    const DeviceMemory<U>& estimated_mean,\n    const DeviceMemory<U>& estimated_variance,\n    const DeviceMemory<U>& side_input, const dnn::BatchDescriptor& x_desc,\n    const dnn::BatchDescriptor& scale_offset_desc, const double epsilon,\n    const double exponential_average_factor,\n    dnn::ActivationMode activation_mode, DeviceMemory<T>* y,\n    DeviceMemory<U>* batch_mean, DeviceMemory<U>* batch_var,\n    DeviceMemory<U>* saved_mean, DeviceMemory<U>* saved_inv_var,\n    bool is_training, ScratchAllocator* reserve_space_allocator,\n    ScratchAllocator* workspace_allocator) {\n  CudnnTensorDescriptor x_descriptor(x_desc, ToCudnnDataType(input_data_type));\n  CudnnTensorDescriptor scale_offset_descriptor(\n      scale_offset_desc, ToCudnnDataType(scale_data_type));\n  cudnnBatchNormMode_t mode = CUDNN_BATCHNORM_SPATIAL;\n  if (BatchnormSpatialPersistentEnabled() && is_training) {\n    mode = CUDNN_BATCHNORM_SPATIAL_PERSISTENT;\n  }\n  float one = 1.0;\n  float zero = 0.0;\n  auto cudnn = cudnn_->GetHandle(parent_, stream);\n\n  DeviceMemory<uint8> workspace;\n  DeviceMemory<uint8> reserve_space;\n\n#if CUDNN_VERSION >= 7402\n  const auto get_bn_ops = [&]() -> cudnnBatchNormOps_t {\n    if (side_input.is_null()) {\n      return activation_mode == dnn::ActivationMode::kNone\n                 ? CUDNN_BATCHNORM_OPS_BN\n                 : CUDNN_BATCHNORM_OPS_BN_ACTIVATION;\n    } else {\n      return CUDNN_BATCHNORM_OPS_BN_ADD_ACTIVATION;\n    }\n  };\n  const cudnnBatchNormOps_t bn_ops = get_bn_ops();\n\n  // We use Nan propagation to be consistent with CudnnSupport::DoActivate(...).\n  CudnnActivationDescriptor activation_desc(\n      activation_mode, CUDNN_PROPAGATE_NAN, x_desc.value_max());\n\n  if (reserve_space_allocator != nullptr && workspace_allocator != nullptr) {\n    SE_ASSIGN_OR_RETURN(\n        workspace,\n        CreateBatchNormForwardWorkspace(\n            stream, cudnn, mode, bn_ops, activation_desc.handle(), x_descriptor,\n            scale_offset_descriptor, workspace_allocator))\n    if (is_training) {\n      size_t reserve_space_size_in_bytes = 0;\n      RETURN_IF_CUDNN_ERROR(\n          cudnnGetBatchNormalizationTrainingExReserveSpaceSize(\n              /*handle=*/cudnn.handle(), /*mode=*/mode, /*bnOps=*/bn_ops,\n              /*activationDesc=*/activation_desc.handle(),\n              /*xDesc=*/x_descriptor.handle(),\n              /*sizeInBytes=*/&reserve_space_size_in_bytes));\n      SE_ASSIGN_OR_RETURN(reserve_space, reserve_space_allocator->AllocateBytes(\n                                             reserve_space_size_in_bytes));\n    }\n  }\n#endif\n\n  auto check_no_side_input_or_activation = [&]() -> port::Status {\n    if (activation_mode != dnn::ActivationMode::kNone ||\n        !side_input.is_null()) {\n      return port::Status(\n          port::error::INTERNAL,\n          absl::StrCat(\n              \"Side input and activation are not supported by cuDNN version: \",\n              CUDNN_VERSION));\n    } else {\n      return port::Status::OK();\n    }\n  };\n\n  if (is_training) {\n    CHECK_EQ(batch_mean->is_null(), batch_var->is_null())\n        << \"batch_mean and batch_var must both be null or both be non-null\";\n\n    void* batch_mean_opaque;\n    void* batch_var_opaque;\n    if (!batch_mean->is_null() && !batch_var->is_null()) {\n      if (exponential_average_factor == 1.0) {\n        stream->ThenMemZero(batch_mean, batch_mean->size());\n        stream->ThenMemZero(batch_var, batch_var->size());\n      }\n      batch_mean_opaque = batch_mean->opaque();\n      batch_var_opaque = batch_var->opaque();\n    } else {\n      batch_mean_opaque = nullptr;\n      batch_var_opaque = nullptr;\n    }\n\n    bool called = false;\n#if CUDNN_VERSION >= 7402\n    if (reserve_space_allocator != nullptr && workspace_allocator != nullptr) {\n      called = true;\n      RETURN_IF_CUDNN_ERROR(cudnnBatchNormalizationForwardTrainingEx(\n          /*handle=*/cudnn.handle(),\n          /*mode=*/mode,\n          /*bnOps=*/bn_ops,\n          /*alpha=*/&one,\n          /*beta=*/&zero,\n          /*xDesc=*/x_descriptor.handle(),\n          /*xData=*/x.opaque(),\n          /*zDesc=*/x_descriptor.handle(),\n          /*zData=*/side_input.opaque(),\n          /*yDesc=*/x_descriptor.handle(),\n          /*yData=*/y->opaque(),\n          /*bnScaleBiasMeanVarDesc=*/scale_offset_descriptor.handle(),\n          /*bnScale=*/scale.opaque(),\n          /*bnBias=*/offset.opaque(),\n          /*exponentialAverageFactor=*/exponential_average_factor,\n          /*resultRunningMean=*/batch_mean_opaque,\n          /*resultRunningVariance=*/batch_var_opaque,\n          /*epsilon=*/epsilon,\n          /*resultSaveMean=*/saved_mean->opaque(),\n          /*resultSaveInvVariance=*/saved_inv_var->opaque(),\n          /*activationDesc=*/activation_desc.handle(),\n          /*workspace=*/workspace.opaque(),\n          /*workSpaceSizeInBytes=*/workspace.size(),\n          /*reserveSpace=*/reserve_space.opaque(),\n          /*reserveSpaceSizeInBytes=*/reserve_space.size()));\n    }\n#endif\n    if (!called) {\n      SE_RETURN_IF_ERROR(check_no_side_input_or_activation());\n      RETURN_IF_CUDNN_ERROR(cudnnBatchNormalizationForwardTraining(\n          cudnn.handle(), mode, &one, &zero, x_descriptor.handle(), x.opaque(),\n          x_descriptor.handle(), y->opaque(), scale_offset_descriptor.handle(),\n          scale.opaque(), offset.opaque(), exponential_average_factor,\n          batch_mean_opaque, batch_var_opaque, epsilon, saved_mean->opaque(),\n          saved_inv_var->opaque()));\n    }\n  } else {\n    const void* maybe_inv_var = estimated_variance.opaque();\n    SE_RETURN_IF_ERROR(check_no_side_input_or_activation());\n    RETURN_IF_CUDNN_ERROR(cudnnBatchNormalizationForwardInference(\n        cudnn.handle(), mode, &one, &zero, x_descriptor.handle(), x.opaque(),\n        x_descriptor.handle(), y->opaque(), scale_offset_descriptor.handle(),\n        scale.opaque(), offset.opaque(), estimated_mean.opaque(), maybe_inv_var,\n        epsilon));\n  }\n  return port::Status::OK();\n}\n\nbool CudnnSupport::DoBatchNormalizationBackward(\n    Stream* stream, const DeviceMemory<float>& y_backprop,\n    const DeviceMemory<float>& x, const DeviceMemory<float>& scale,\n    const DeviceMemory<float>& mean, const DeviceMemory<float>& inv_var,\n    const dnn::BatchDescriptor& x_desc,\n    const dnn::BatchDescriptor& scale_offset_desc, const double epsilon,\n    DeviceMemory<float>* x_backprop, DeviceMemory<float>* scale_backprop,\n    DeviceMemory<float>* offset_backprop,\n    DeviceMemory<uint8>* reserve_space_data,\n    ScratchAllocator* workspace_allocator) {\n  return IsStatusOk(DoBatchNormalizationBackwardImpl(\n                        stream, CUDNN_DATA_FLOAT, CUDNN_DATA_FLOAT, y_backprop,\n                        x, scale, mean, inv_var, x_desc, scale_offset_desc,\n                        epsilon, x_backprop, scale_backprop, offset_backprop,\n                        reserve_space_data, workspace_allocator),\n                    /*report_error=*/true);\n}\n\nbool CudnnSupport::DoBatchNormalizationBackward(\n    Stream* stream, const DeviceMemory<Eigen::half>& y_backprop,\n    const DeviceMemory<Eigen::half>& x, const DeviceMemory<float>& scale,\n    const DeviceMemory<float>& mean, const DeviceMemory<float>& inv_var,\n    const dnn::BatchDescriptor& x_desc,\n    const dnn::BatchDescriptor& scale_offset_desc, const double epsilon,\n    DeviceMemory<Eigen::half>* x_backprop, DeviceMemory<float>* scale_backprop,\n    DeviceMemory<float>* offset_backprop,\n    DeviceMemory<uint8>* reserve_space_data,\n    ScratchAllocator* workspace_allocator) {\n  return IsStatusOk(DoBatchNormalizationBackwardImpl(\n                        stream, CUDNN_DATA_HALF, CUDNN_DATA_FLOAT, y_backprop,\n                        x, scale, mean, inv_var, x_desc, scale_offset_desc,\n                        epsilon, x_backprop, scale_backprop, offset_backprop,\n                        reserve_space_data, workspace_allocator),\n                    /*report_error=*/true);\n}\n\ntemplate <class T, class U>\nport::Status CudnnSupport::DoBatchNormalizationBackwardImpl(\n    Stream* stream, int cudnn_input_type, int cudnn_scale_type,\n    const DeviceMemory<T>& y_backprop, const DeviceMemory<T>& x,\n    const DeviceMemory<U>& scale, const DeviceMemory<U>& mean,\n    const DeviceMemory<U>& inv_var, const dnn::BatchDescriptor& x_desc,\n    const dnn::BatchDescriptor& scale_offset_desc, const double epsilon,\n    DeviceMemory<T>* x_backprop, DeviceMemory<U>* scale_backprop,\n    DeviceMemory<U>* offset_backprop, DeviceMemory<uint8>* reserve_space_data,\n    ScratchAllocator* workspace_allocator) {\n  CudnnTensorDescriptor x_descriptor(\n      x_desc, static_cast<cudnnDataType_t>(cudnn_input_type));\n  CudnnTensorDescriptor scale_offset_descriptor(\n      scale_offset_desc, static_cast<cudnnDataType_t>(cudnn_scale_type));\n  cudnnBatchNormMode_t mode = CUDNN_BATCHNORM_SPATIAL;\n  if (BatchnormSpatialPersistentEnabled()) {\n    mode = CUDNN_BATCHNORM_SPATIAL_PERSISTENT;\n  }\n  float one = 1.0;\n  float zero = 0.0;\n\n  auto cudnn = cudnn_->GetHandle(parent_, stream);\n\n  bool called = false;\n#if CUDNN_VERSION >= 7402\n  if (reserve_space_data != nullptr && workspace_allocator != nullptr) {\n    called = true;\n    const cudnnBatchNormOps_t bn_ops = CUDNN_BATCHNORM_OPS_BN;\n    SE_ASSIGN_OR_RETURN(DeviceMemory<uint8> workspace,\n                        CreateBatchNormBackwardWorkspace(\n                            stream, cudnn, mode, bn_ops, x_descriptor,\n                            scale_offset_descriptor, workspace_allocator))\n    RETURN_IF_CUDNN_ERROR(cudnnBatchNormalizationBackwardEx(\n        /*handle=*/cudnn.handle(),\n        /*mode=*/mode,\n        /*bnOps=*/bn_ops,\n        /*alphaDataDiff=*/&one,\n        /*betaDataDiff=*/&zero,\n        /*alphaParamDiff=*/&one,\n        /*betaParamDiff=*/&zero,\n        /*xDesc=*/x_descriptor.handle(),\n        /*xData=*/x.opaque(),\n        /*yDesc=*/nullptr,\n        /*yData=*/nullptr,\n        /*dyDesc=*/x_descriptor.handle(),\n        /*dyData=*/y_backprop.opaque(),\n        /*dzDesc=*/nullptr,\n        /*dzData=*/nullptr,\n        /*dxDesc=*/x_descriptor.handle(),\n        /*dxData=*/x_backprop->opaque(),\n        /*dBnScaleBiasDesc=*/scale_offset_descriptor.handle(),\n        /*bnScaleData=*/scale.opaque(),\n        /*bnBiasData=*/nullptr,\n        /*dBnScaleData=*/scale_backprop->opaque(),\n        /*dBnBiasData=*/offset_backprop->opaque(),\n        /*epsilon=*/epsilon,\n        /*savedMean=*/mean.opaque(),\n        /*savedInvVariance=*/inv_var.opaque(),\n        /*activationDesc=*/nullptr,\n        /*workspace=*/workspace.opaque(),\n        /*workSpaceSizeInBytes=*/workspace.size(),\n        /*reserveSpace=*/reserve_space_data->opaque(),\n        /*reserveSpaceSizeInBytes=*/reserve_space_data->size()));\n  }\n#endif\n  if (!called) {\n    RETURN_IF_CUDNN_ERROR(cudnnBatchNormalizationBackward(\n        cudnn.handle(), mode, &one, &zero, &one, &zero, x_descriptor.handle(),\n        x.opaque(), x_descriptor.handle(), y_backprop.opaque(),\n        x_descriptor.handle(), x_backprop->opaque(),\n        scale_offset_descriptor.handle(), scale.opaque(),\n        scale_backprop->opaque(), offset_backprop->opaque(), epsilon,\n        mean.opaque(), inv_var.opaque()));\n  }\n\n  return port::Status::OK();\n}\n\nport::Status CudnnSupport::DoFusedConvolve(\n    Stream* stream, const dnn::BatchDescriptor& conv_input_descriptor,\n    const DeviceMemory<double>& conv_input_data, double conv_input_scale,\n    const dnn::FilterDescriptor& filter_descriptor,\n    const DeviceMemory<double>& filter_data,\n    const dnn::ConvolutionDescriptor& convolution_descriptor,\n    const DeviceMemory<double>& side_input_data, double side_input_scale,\n    const dnn::BatchDescriptor& bias_descriptor,\n    const DeviceMemory<double>& biases, dnn::ActivationMode activation_mode,\n    const dnn::BatchDescriptor& output_descriptor,\n    DeviceMemory<double>* output_data, ScratchAllocator* scratch_allocator,\n    const dnn::AlgorithmConfig& algorithm_config,\n    dnn::ProfileResult* output_profile_result) {\n  return DoFusedConvolveImpl(\n      stream, conv_input_descriptor, conv_input_data, conv_input_scale,\n      filter_descriptor, filter_data, convolution_descriptor, side_input_data,\n      side_input_scale, bias_descriptor, biases, activation_mode,\n      output_descriptor, output_data,\n      GetConvAccumulatorType(dnn::DataType::kDouble), scratch_allocator,\n      algorithm_config, output_profile_result);\n}\n\nport::Status CudnnSupport::DoFusedConvolve(\n    Stream* stream, const dnn::BatchDescriptor& conv_input_descriptor,\n    const DeviceMemory<float>& conv_input_data, float conv_input_scale,\n    const dnn::FilterDescriptor& filter_descriptor,\n    const DeviceMemory<float>& filter_data,\n    const dnn::ConvolutionDescriptor& convolution_descriptor,\n    const DeviceMemory<float>& side_input_data, float side_input_scale,\n    const dnn::BatchDescriptor& bias_descriptor,\n    const DeviceMemory<float>& biases, dnn::ActivationMode activation_mode,\n    const dnn::BatchDescriptor& output_descriptor,\n    DeviceMemory<float>* output_data, ScratchAllocator* scratch_allocator,\n    const dnn::AlgorithmConfig& algorithm_config,\n    dnn::ProfileResult* output_profile_result) {\n  return DoFusedConvolveImpl(\n      stream, conv_input_descriptor, conv_input_data, conv_input_scale,\n      filter_descriptor, filter_data, convolution_descriptor, side_input_data,\n      side_input_scale, bias_descriptor, biases, activation_mode,\n      output_descriptor, output_data,\n      GetConvAccumulatorType(dnn::DataType::kFloat), scratch_allocator,\n      algorithm_config, output_profile_result);\n}\n\nport::Status CudnnSupport::DoFusedConvolve(\n    Stream* stream, const dnn::BatchDescriptor& conv_input_descriptor,\n    const DeviceMemory<Eigen::half>& conv_input_data, float conv_input_scale,\n    const dnn::FilterDescriptor& filter_descriptor,\n    const DeviceMemory<Eigen::half>& filter_data,\n    const dnn::ConvolutionDescriptor& convolution_descriptor,\n    const DeviceMemory<Eigen::half>& side_input_data, float side_input_scale,\n    const dnn::BatchDescriptor& bias_descriptor,\n    const DeviceMemory<Eigen::half>& biases,\n    dnn::ActivationMode activation_mode,\n    const dnn::BatchDescriptor& output_descriptor,\n    DeviceMemory<Eigen::half>* output_data, ScratchAllocator* scratch_allocator,\n    const dnn::AlgorithmConfig& algorithm_config,\n    dnn::ProfileResult* output_profile_result) {\n  return DoFusedConvolveImpl(\n      stream, conv_input_descriptor, conv_input_data, conv_input_scale,\n      filter_descriptor, filter_data, convolution_descriptor, side_input_data,\n      side_input_scale, bias_descriptor, biases, activation_mode,\n      output_descriptor, output_data,\n      GetConvAccumulatorType(dnn::DataType::kHalf), scratch_allocator,\n      algorithm_config, output_profile_result);\n}\n\nport::Status CudnnSupport::DoFusedConvolve(\n    Stream* stream, const dnn::BatchDescriptor& conv_input_descriptor,\n    const DeviceMemory<int8>& conv_input_data, float conv_input_scale,\n    const dnn::FilterDescriptor& filter_descriptor,\n    const DeviceMemory<int8>& filter_data,\n    const dnn::ConvolutionDescriptor& convolution_descriptor,\n    const DeviceMemory<int8>& side_input_data, float side_input_scale,\n    const dnn::BatchDescriptor& bias_descriptor,\n    const DeviceMemory<float>& biases, dnn::ActivationMode activation_mode,\n    const dnn::BatchDescriptor& output_descriptor,\n    DeviceMemory<int8>* output_data, ScratchAllocator* scratch_allocator,\n    const dnn::AlgorithmConfig& algorithm_config,\n    dnn::ProfileResult* output_profile_result) {\n  int cc_major, cc_minor;\n  std::tie(cc_major, cc_minor) = GetCcMajorMinor(stream);\n\n  if (cc_major < 6 || (cc_major == 6 && cc_minor < 1)) {\n    return port::UnimplementedError(\n        \"cudnnConvolutionBiasActivationForward() for int8 is only supported on \"\n        \"GPUs with compute capability 6.1 or later.\");\n  }\n\n  return DoFusedConvolveImpl(\n      stream, conv_input_descriptor, conv_input_data, conv_input_scale,\n      filter_descriptor, filter_data, convolution_descriptor, side_input_data,\n      side_input_scale, bias_descriptor, biases, activation_mode,\n      output_descriptor, output_data,\n      GetConvAccumulatorType(dnn::DataType::kInt8), scratch_allocator,\n      algorithm_config, output_profile_result);\n}\n\nport::Status CudnnSupport::DoFusedConvolve(\n    Stream* stream, const dnn::BatchDescriptor& conv_input_descriptor,\n    const DeviceMemory<int8>& conv_input_data, float conv_input_scale,\n    const dnn::FilterDescriptor& filter_descriptor,\n    const DeviceMemory<int8>& filter_data,\n    const dnn::ConvolutionDescriptor& convolution_descriptor,\n    const DeviceMemory<float>& side_input_data, float side_input_scale,\n    const dnn::BatchDescriptor& bias_descriptor,\n    const DeviceMemory<float>& biases, dnn::ActivationMode activation_mode,\n    const dnn::BatchDescriptor& output_descriptor,\n    DeviceMemory<float>* output_data, ScratchAllocator* scratch_allocator,\n    const dnn::AlgorithmConfig& algorithm_config,\n    dnn::ProfileResult* output_profile_result) {\n  int cc_major, cc_minor;\n  stream->parent()->GetDeviceDescription().cuda_compute_capability(&cc_major,\n                                                                   &cc_minor);\n  if (cc_major < 6 || (cc_major == 6 && cc_minor < 1)) {\n    return port::UnimplementedError(\n        \"cudnnConvolutionBiasActivationForward() for int8 is only supported on \"\n        \"GPUs with compute capability 6.1 or later.\");\n  }\n\n  return DoFusedConvolveImpl(\n      stream, conv_input_descriptor, conv_input_data, conv_input_scale,\n      filter_descriptor, filter_data, convolution_descriptor, side_input_data,\n      side_input_scale, bias_descriptor, biases, activation_mode,\n      output_descriptor, output_data,\n      GetConvAccumulatorType(dnn::DataType::kInt8), scratch_allocator,\n      algorithm_config, output_profile_result);\n}\n\nport::Status CudnnSupport::DoPrepareForCtcLoss(\n    Stream* stream, dnn::DataType element_type,\n    const dnn::RnnStateTensorDescriptor& probs_desc,\n    const dnn::RnnStateTensorDescriptor& grads_desc,\n    absl::Span<const int> labels_data,\n    absl::Span<const int> labels_lengths_data,\n    absl::Span<const int> input_lengths_data,\n    ScratchAllocator* scratch_allocator, DeviceMemory<uint8>* scratch_memory,\n    int* ctc_loss_algo_id) {\n  auto cudnn = cudnn_->GetHandle(parent_, stream);\n  // Query the workspace size.\n  size_t workspace_size_in_bytes = 0;\n#if CUDNN_VERSION >= 7603\n  CudnnCtcLossDescriptor cudnn_ctc_loss_desc(ToCudnnDataType(element_type));\n  const CudnnRnnStateTensorDescriptor& cudnn_probs_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(probs_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_grads_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(grads_desc);\n\n  // Try running with `algo`, if successful then pick it. The non-deterministic\n  // algorithm is first and thus preferentially picked when determinism is not\n  // required.\n  auto algo = RequireCudnnDeterminism() ? CUDNN_CTC_LOSS_ALGO_DETERMINISTIC\n                                        : CUDNN_CTC_LOSS_ALGO_NON_DETERMINISTIC;\n  cudnnStatus_t status = cudnnGetCTCLossWorkspaceSize(\n      /*handle=*/cudnn.handle(), /*probsDesc=*/cudnn_probs_desc.handle(),\n      /*gradientsDesc=*/cudnn_grads_desc.handle(),\n      /*labels=*/labels_data.data(),\n      /*labelLengths=*/labels_lengths_data.data(),\n      /*inputLengths=*/input_lengths_data.data(),\n      /*algo=*/algo,\n      /*ctcLossDesc=*/cudnn_ctc_loss_desc.handle(),\n      /*sizeInBytes=*/&workspace_size_in_bytes);\n  if (RequireCudnnDeterminism()) {\n    RETURN_IF_CUDNN_ERROR(status);\n  }\n\n  if (status != CUDNN_STATUS_SUCCESS) {\n    algo = CUDNN_CTC_LOSS_ALGO_DETERMINISTIC;\n    RETURN_IF_CUDNN_ERROR(cudnnGetCTCLossWorkspaceSize(\n        /*handle=*/cudnn.handle(), /*probsDesc=*/cudnn_probs_desc.handle(),\n        /*gradientsDesc=*/cudnn_grads_desc.handle(),\n        /*labels=*/labels_data.data(),\n        /*labelLengths=*/labels_lengths_data.data(),\n        /*inputLengths=*/input_lengths_data.data(),\n        /*algo=*/algo,\n        /*ctcLossDesc=*/cudnn_ctc_loss_desc.handle(),\n        /*sizeInBytes=*/&workspace_size_in_bytes));\n  }\n  *ctc_loss_algo_id = algo;\n#else\n  return port::Status(port::error::INVALID_ARGUMENT,\n                      \"No supported cudnnGetCTCLossWorkspaceSize when \"\n                      \"CUDNN_VERSION < 7.6.3\");\n#endif\n  // Allocate the workspace.\n  if (workspace_size_in_bytes == 0) {\n    *scratch_memory = DeviceMemory<uint8>();\n    return port::Status::OK();\n  }\n  const auto scratch_or =\n      scratch_allocator->AllocateBytes(workspace_size_in_bytes);\n  if (scratch_or.ok()) {\n    *scratch_memory = scratch_or.ValueOrDie();\n    return port::Status::OK();\n  }\n  return port::InternalError(\n      \"Failed to allocate scratch memory for the CuDNN CTC Loss\");\n}\n\nport::Status CudnnSupport::DoCtcLoss(\n    Stream* stream, dnn::DataType element_type,\n    const dnn::RnnStateTensorDescriptor& probs_desc,\n    const DeviceMemoryBase probs_data, absl::Span<const int> labels_data,\n    absl::Span<const int> labels_lengths_data,\n    absl::Span<const int> input_lengths_data, DeviceMemoryBase costs_data,\n    const dnn::RnnStateTensorDescriptor& grads_desc,\n    DeviceMemoryBase grads_data, DeviceMemory<uint8> scratch_memory,\n    int ctc_loss_algo_id) {\n  // Current cuDNN CTC Loss only supports the float datatype\n  if (CUDNN_VERSION < 7603 || element_type != dnn::DataType::kFloat) {\n    return port::Status(port::error::INVALID_ARGUMENT,\n                        \"CudnnCtcLossDescriptor is supported only when the \"\n                        \"CUDNN_VERSION >= 7.6.3 and DataType is float\");\n  }\n  CudnnCtcLossDescriptor cudnn_ctc_loss_desc(ToCudnnDataType(element_type));\n  const CudnnRnnStateTensorDescriptor& cudnn_probs_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(probs_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_grads_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(grads_desc);\n  return DoCtcLossImpl(stream, cudnn_probs_desc, probs_data, labels_data,\n                       labels_lengths_data, input_lengths_data, costs_data,\n                       cudnn_grads_desc, grads_data, cudnn_ctc_loss_desc,\n                       scratch_memory, ctc_loss_algo_id);\n}\n\nbool CudnnSupport::DoTransformTensor(Stream* stream,\n                                     const dnn::BatchDescriptor& input_desc,\n                                     dnn::DataType input_type,\n                                     const DeviceMemoryBase& input_data,\n                                     const dnn::BatchDescriptor& output_desc,\n                                     dnn::DataType output_type, float scale,\n                                     DeviceMemoryBase* output_data) {\n  float beta = 0.0f;\n  CudnnTensorDescriptor input_tensor_desc(\n      input_desc, ToCudnnDataType(input_type, input_desc.layout()));\n  CudnnTensorDescriptor output_tensor_desc(\n      output_desc, ToCudnnDataType(output_type, output_desc.layout()));\n  auto cudnn = cudnn_->GetHandle(parent_, stream);\n  const auto status = [&] {\n    RETURN_IF_CUDNN_ERROR(cudnnTransformTensor(\n        cudnn.handle(), &scale, input_tensor_desc.handle(), input_data.opaque(),\n        &beta, output_tensor_desc.handle(), output_data->opaque()));\n    return port::Status::OK();\n  }();\n  return IsStatusOk(status, /*report_error=*/true);\n}\n\ntemplate <class T>\nport::Status CudnnSupport::DoConvolveBackwardBiasImpl(\n    Stream* stream, const dnn::BatchDescriptor& input_descriptor,\n    const DeviceMemory<T>& input_data,\n    const dnn::BatchDescriptor& bias_descriptor,\n    DeviceMemory<T>* backward_bias_data) {\n  cudnnDataType_t cudnn_type = GetCudnnDataType<T>();\n  CudnnTensorDescriptor input_nd(input_descriptor, cudnn_type);\n  CudnnTensorDescriptor bias_nd(bias_descriptor, cudnn_type);\n\n  // Alpha is the scaling factor for input.\n  float alpha = 1.0;\n  // Beta is the scaling factor for output.\n  float beta = 0.0;\n\n  auto cudnn = cudnn_->GetHandle(parent_, stream);\n  RETURN_IF_CUDNN_ERROR(cudnnConvolutionBackwardBias(\n      cudnn.handle(), &alpha, input_nd.handle(), input_data.opaque(), &beta,\n      bias_nd.handle(), backward_bias_data->opaque()));\n  return port::Status::OK();\n}\n\nbool CudnnSupport::DoConvolveBackwardBias(\n    Stream* stream, const dnn::BatchDescriptor& input_descriptor,\n    const DeviceMemory<double>& input_data,\n    const dnn::BatchDescriptor& bias_descriptor,\n    DeviceMemory<double>* backward_bias_data) {\n  return IsStatusOk(\n      DoConvolveBackwardBiasImpl(stream, input_descriptor, input_data,\n                                 bias_descriptor, backward_bias_data),\n      /*report_error=*/true);\n}\n\nbool CudnnSupport::DoConvolveBackwardBias(\n    Stream* stream, const dnn::BatchDescriptor& input_descriptor,\n    const DeviceMemory<float>& input_data,\n    const dnn::BatchDescriptor& bias_descriptor,\n    DeviceMemory<float>* backward_bias_data) {\n  return IsStatusOk(\n      DoConvolveBackwardBiasImpl(stream, input_descriptor, input_data,\n                                 bias_descriptor, backward_bias_data),\n      /*report_error=*/true);\n}\n\nbool CudnnSupport::DoConvolveBackwardBias(\n    Stream* stream, const dnn::BatchDescriptor& input_descriptor,\n    const DeviceMemory<Eigen::half>& input_data,\n    const dnn::BatchDescriptor& bias_descriptor,\n    DeviceMemory<Eigen::half>* backward_bias_data) {\n  return IsStatusOk(\n      DoConvolveBackwardBiasImpl(stream, input_descriptor, input_data,\n                                 bias_descriptor, backward_bias_data),\n      /*report_error=*/true);\n}\n\nbool CudnnSupport::DoMatMul(Stream* stream,\n                            const DeviceMemory<float>& input_data,\n                            const DeviceMemory<float>& weights,\n                            const dnn::BatchDescriptor& input_dimensions,\n                            const dnn::BatchDescriptor& output_dimensions,\n                            DeviceMemory<float>* output_data) {\n  if (input_dimensions.count() != output_dimensions.count()) {\n    LOG(ERROR) << \"MatMul input and output dimensions are not compatible.\";\n    return false;\n  }\n\n  // We do not permute the input or output, instead we just\n  // reinterpret the layout. We are working with row-major matrices\n  // and the rows of the input and output correspond to batch, so\n  // batch has to be outermost in both the input and output.\n  //\n  // By adding transposes to the BLAS gemm call we could perhaps make\n  // the kYXDepthBatch layout work as well, but there has been no need\n  // for that so far.\n  if (input_dimensions.layout() != dnn::DataLayout::kBatchYXDepth &&\n      input_dimensions.layout() != dnn::DataLayout::kBatchDepthYX) {\n    LOG(ERROR) << \"Unsupported MatMul input layout.\";\n    return false;\n  }\n  if (output_dimensions.layout() != dnn::DataLayout::kBatchYXDepth &&\n      output_dimensions.layout() != dnn::DataLayout::kBatchDepthYX) {\n    LOG(ERROR) << \"Unsupported MatMul output layout.\";\n    return false;\n  }\n\n  if (output_dimensions.width() == 1 && output_dimensions.height() == 1) {\n    // This is a fast path that also supports the kBatchYXDepth layout.\n\n    // The matrices here are in row-major format while BLAS expects\n    // column-major, i.e. our matrices are transposed as far as BLAS\n    // is concerned. So we need to compute output^T =\n    // input^T*weights^T. There is no parameter for transposing the\n    // output in BLAS gemm, but instead we can transpose both sides of\n    // the equality to see that this is equivalent to\n    // output=weights*input. So we only need to swap the order of\n    // weights and input in the matrix product to correct for the\n    // row-major versus column-major difference.\n    const float alpha = 1.0f;  // Take the matrix product without scaling it.\n    const float beta = 0.0f;   // Ignore the original values in output_data.\n    const int64 m = output_dimensions.NodesAcrossFeatureMaps();\n    const int64 n = input_dimensions.count();\n    const int64 k = input_dimensions.NodesAcrossFeatureMaps();\n    stream->ThenBlasGemm(blas::Transpose::kNoTranspose,\n                         blas::Transpose::kNoTranspose, m, n, k, alpha, weights,\n                         m, input_data, k, beta, output_data, m);\n  } else {\n    // This is a slower and more complex path that supports output\n    // width() * height() > 1, though it only supports the\n    // kBatchYXDepth layout. Does support kBatchDepthYX if output\n    // feature_map_count() == 1, as then there is no difference\n    // between the two layouts.\n    //\n    // The operation here is the same as above, except that we have to\n    // do the matrix multiplication for each (y,x) output coordinate\n    // separately. We then interpret weights as containing K = width()\n    // * height() different matrices, which we all multiply onto the\n    // matrix from input_data, yielding K matrix products. We then\n    // combine these together into one matrix by concatenating all the\n    // first rows of these matrices, then all the seconds rows and so\n    // on. We can do this with a batched matrix multiplication, where\n    // the result is written to a different submatrix of the output\n    // for each matrix multiplication.\n    //\n    // The reason that we only support the kBatchYXDepth output layout\n    // is that we have to do something in the depth for each (y,x)\n    // coordinate. The kBatchYXDepth layout has the depth information\n    // for each point (y,x) in contiguous memory while the\n    // kBatchDepthYX layout does not.\n    //\n    // TODO(broune): Consider a special case for when output depth ==\n    // 1, as then possibly this could all be done as one matrix\n    // multiplication instead of a batched one, which should be\n    // faster. Another possibility would be to add a weights layout\n    // parameter and then support kBatchDepthYX for a different\n    // weights layout.\n    if (output_dimensions.layout() != dnn::DataLayout::kBatchYXDepth &&\n        !(output_dimensions.layout() == dnn::DataLayout::kBatchDepthYX &&\n          output_dimensions.feature_map_count() == 1)) {\n      LOG(ERROR) << \"Unsupported MatMul output layout.\";\n      return false;\n    }\n\n    const float alpha = 1.0f;  // Take the matrix product without scaling it.\n    const float beta = 0.0f;   // Ignore the original values in output_data.\n    const uint64 m = output_dimensions.feature_map_count();\n    const uint64 n = input_dimensions.count();\n    const uint64 k = input_dimensions.NodesAcrossFeatureMaps();\n    const int lda = m;\n    const int ldb = k;\n    const int ldc = output_dimensions.NodesAcrossFeatureMaps();\n    const int batch_count = output_dimensions.NodesPerFeatureMap();\n\n    std::vector<DeviceMemory<float>> a(batch_count);\n    std::vector<DeviceMemory<float>> b(batch_count);\n    std::vector<DeviceMemory<float>> c(batch_count);\n    for (int i = 0; i < batch_count; ++i) {\n      const int weights_offset = i * input_dimensions.NodesAcrossFeatureMaps() *\n                                 output_dimensions.feature_map_count();\n      a[i] = DeviceMemory<float>::MakeFromByteSize(\n          const_cast<float*>(reinterpret_cast<const float*>(weights.opaque())) +\n              weights_offset,\n          weights.ElementCount() - weights_offset);\n\n      b[i] = input_data;\n\n      const int output_offset = i * output_dimensions.feature_map_count();\n      c[i] = DeviceMemory<float>::MakeFromByteSize(\n          const_cast<float*>(\n              reinterpret_cast<const float*>(output_data->opaque())) +\n              output_offset,\n          output_data->ElementCount() - output_offset);\n    }\n    const auto toPtrs = [](std::vector<DeviceMemory<float>>& v) {\n      std::vector<DeviceMemory<float>*> ptrs;\n      ptrs.reserve(v.size());\n      for (auto& mem : v) {\n        ptrs.push_back(&mem);\n      }\n      return ptrs;\n    };\n\n    stream->ThenBlasGemmBatched(blas::Transpose::kNoTranspose,\n                                blas::Transpose::kNoTranspose, m, n, k, alpha,\n                                toPtrs(a), lda, toPtrs(b), ldb, beta, toPtrs(c),\n                                ldc, batch_count);\n  }\n\n  return stream->ok();\n}\n\nbool CudnnSupport::DoBiasAdd(Stream* stream,\n                             const DeviceMemory<float>& input_data,\n                             const DeviceMemory<float>& biases,\n                             const dnn::BatchDescriptor& dimensions,\n                             DeviceMemory<float>* output_data) {\n  CudnnTensorDescriptor input_descriptor(dimensions, CUDNN_DATA_FLOAT);\n\n  dnn::BatchDescriptor bias_dimensions;\n  bias_dimensions.set_count(1)\n      .set_feature_map_count(dimensions.feature_map_count())\n      .set_height(1)\n      .set_width(1)\n      .set_layout(dnn::DataLayout::kBatchYXDepth);\n  CudnnTensorDescriptor bias_descriptor(bias_dimensions, CUDNN_DATA_FLOAT);\n\n  // cudnnAddTensor after R3 is in-place, so we need to copy input_data to\n  // output_data before doing the addition, unless the input and\n  // output are at the same address.\n  if (input_data.opaque() != output_data->opaque()) {\n    stream->ThenMemcpy(output_data, input_data,\n                       dimensions.ElementCount() * sizeof(float));\n    if (!stream->ok()) {\n      LOG(ERROR)\n          << \"stream \" << stream\n          << \" could not enqueue a tensor copy as part of bias addition.\";\n      return false;\n    }\n  }\n\n  const float alpha = 1.0f;\n  const float beta = 1.0f;\n\n  auto cudnn = cudnn_->GetHandle(parent_, stream);\n\n  const auto status = [&] {\n    RETURN_IF_CUDNN_ERROR(cudnnAddTensor(\n        cudnn.handle(), &alpha, bias_descriptor.handle(), biases.opaque(),\n        &beta, input_descriptor.handle(), output_data->opaque()));\n    return port::Status::OK();\n  }();\n  return IsStatusOk(status, /*report_error=*/true);\n}\n\nbool CudnnSupport::DoActivate(Stream* stream,\n                              dnn::ActivationMode activation_mode,\n                              const dnn::BatchDescriptor& dimensions,\n                              const DeviceMemory<float>& input_data,\n                              DeviceMemory<float>* output_data,\n                              uint64 options) {\n  CudnnActivationDescriptor activation_desc(\n      activation_mode, CUDNN_PROPAGATE_NAN, dimensions.value_max());\n\n  CudnnTensorDescriptor input_nd(dimensions, CUDNN_DATA_FLOAT);\n  // Alpha is the input scaling factor.\n  float alpha = 1.0;\n  // Beta is the output scaling factor.\n  float beta = 0.0;\n\n  auto cudnn = cudnn_->GetHandle(parent_, stream);\n  const auto status = [&] {\n    RETURN_IF_CUDNN_ERROR(cudnnActivationForward(\n        cudnn.handle(), activation_desc.handle(), &alpha, input_nd.handle(),\n        input_data.opaque(), &beta, input_nd.handle(), output_data->opaque()));\n    return port::Status::OK();\n  }();\n  return IsStatusOk(status, /*report_error=*/true);\n}\n\nbool CudnnSupport::DoPoolForward(\n    Stream* stream, const dnn::PoolingDescriptor& pooling_dimensions,\n    const dnn::BatchDescriptor& input_dimensions,\n    const DeviceMemory<double>& input_data,\n    const dnn::BatchDescriptor& output_dimensions,\n    DeviceMemory<double>* output_data, ScratchAllocator* workspace_allocator) {\n  // Alpha is the scaling factor for input.\n  double alpha = 1.0;\n  // Beta is the scaling factor for output.\n  double beta = 0.0;\n\n  CudnnTensorDescriptor src_desc(input_dimensions, CUDNN_DATA_DOUBLE);\n  CudnnTensorDescriptor dest_desc(output_dimensions, CUDNN_DATA_DOUBLE);\n  CudnnPoolingDescriptor pooling_desc(pooling_dimensions);\n\n  auto cudnn = cudnn_->GetHandle(parent_, stream);\n  const auto status = [&] {\n    RETURN_IF_CUDNN_ERROR(cudnnPoolingForward(\n        cudnn.handle(), pooling_desc.handle(), &alpha, src_desc.handle(),\n        input_data.opaque(), &beta, dest_desc.handle(), output_data->opaque()));\n    return port::Status::OK();\n  }();\n  return IsStatusOk(status, /*report_error=*/true);\n}\n\nbool CudnnSupport::DoPoolForward(\n    Stream* stream, const dnn::PoolingDescriptor& pooling_dimensions,\n    const dnn::BatchDescriptor& input_dimensions,\n    const DeviceMemory<float>& input_data,\n    const dnn::BatchDescriptor& output_dimensions,\n    DeviceMemory<float>* output_data, ScratchAllocator* workspace_allocator) {\n  // Alpha is the scaling factor for input.\n  float alpha = 1.0;\n  // Beta is the scaling factor for output.\n  float beta = 0.0;\n\n  CudnnTensorDescriptor src_desc(input_dimensions, CUDNN_DATA_FLOAT);\n  CudnnTensorDescriptor dest_desc(output_dimensions, CUDNN_DATA_FLOAT);\n  CudnnPoolingDescriptor pooling_desc(pooling_dimensions);\n\n  auto cudnn = cudnn_->GetHandle(parent_, stream);\n  const auto status = [&] {\n    RETURN_IF_CUDNN_ERROR(cudnnPoolingForward(\n        cudnn.handle(), pooling_desc.handle(), &alpha, src_desc.handle(),\n        input_data.opaque(), &beta, dest_desc.handle(), output_data->opaque()));\n    return port::Status::OK();\n  }();\n  return IsStatusOk(status, /*report_error=*/true);\n}\n\nbool CudnnSupport::DoPoolForward(\n    Stream* stream, const dnn::PoolingDescriptor& pooling_dimensions,\n    const dnn::BatchDescriptor& input_dimensions,\n    const DeviceMemory<Eigen::half>& input_data,\n    const dnn::BatchDescriptor& output_dimensions,\n    DeviceMemory<Eigen::half>* output_data,\n    ScratchAllocator* workspace_allocator) {\n  // Alpha is the scaling factor for input.\n  float alpha = 1.0;\n  // Beta is the scaling factor for output.\n  float beta = 0.0;\n\n  CudnnTensorDescriptor src_desc(input_dimensions, CUDNN_DATA_HALF);\n  CudnnTensorDescriptor dest_desc(output_dimensions, CUDNN_DATA_HALF);\n  CudnnPoolingDescriptor pooling_desc(pooling_dimensions);\n  auto cudnn = cudnn_->GetHandle(parent_, stream);\n  const auto status = [&] {\n    RETURN_IF_CUDNN_ERROR(cudnnPoolingForward(\n        cudnn.handle(), pooling_desc.handle(), &alpha, src_desc.handle(),\n        input_data.opaque(), &beta, dest_desc.handle(), output_data->opaque()));\n    return port::Status::OK();\n  }();\n  return IsStatusOk(status, /*report_error=*/true);\n}\n\nbool CudnnSupport::DoPoolForward(\n    Stream* stream, const dnn::PoolingDescriptor& pooling_dimensions,\n    const dnn::BatchDescriptor& input_dimensions,\n    const DeviceMemory<int8>& input_data,\n    const dnn::BatchDescriptor& output_dimensions,\n    DeviceMemory<int8>* output_data, ScratchAllocator* workspace_allocator) {\n  // Alpha is the scaling factor for input.\n  float alpha = 1.0;\n  // Beta is the scaling factor for output.\n  float beta = 0.0;\n\n  CudnnTensorDescriptor src_desc(input_dimensions, CUDNN_DATA_INT8);\n  CudnnTensorDescriptor dest_desc(output_dimensions, CUDNN_DATA_INT8);\n  CudnnPoolingDescriptor pooling_desc(pooling_dimensions);\n\n  auto cudnn = cudnn_->GetHandle(parent_, stream);\n  const auto status = [&] {\n    RETURN_IF_CUDNN_ERROR(cudnnPoolingForward(\n        cudnn.handle(), pooling_desc.handle(), &alpha, src_desc.handle(),\n        input_data.opaque(), &beta, dest_desc.handle(), output_data->opaque()));\n    return port::Status::OK();\n  }();\n  return IsStatusOk(status, /*report_error=*/true);\n}\n\nbool CudnnSupport::DoPoolBackward(\n    Stream* stream, const dnn::PoolingDescriptor& pooling_dimensions,\n    const dnn::BatchDescriptor& input_dimensions,\n    const DeviceMemory<double>& input_data,\n    const dnn::BatchDescriptor& output_dimensions,\n    const DeviceMemory<double>& output_data,\n    const DeviceMemory<double>& input_diff_data,\n    DeviceMemory<double>* output_diff_data,\n    ScratchAllocator* workspace_allocator) {\n  // Alpha is the scaling factor for input.\n  double alpha = 1.0;\n  // Beta is the scaling factor for output.\n  double beta = 0.0;\n\n  CudnnTensorDescriptor src_desc(input_dimensions, CUDNN_DATA_DOUBLE);\n  CudnnTensorDescriptor dest_desc(output_dimensions, CUDNN_DATA_DOUBLE);\n  CudnnPoolingDescriptor pooling_desc(pooling_dimensions);\n\n  auto cudnn = cudnn_->GetHandle(parent_, stream);\n  const auto status = [&] {\n    RETURN_IF_CUDNN_ERROR(cudnnPoolingBackward(\n        cudnn.handle(), pooling_desc.handle(), &alpha, dest_desc.handle(),\n        output_data.opaque(), dest_desc.handle(), input_diff_data.opaque(),\n        src_desc.handle(), input_data.opaque(), &beta, src_desc.handle(),\n        output_diff_data->opaque()));\n    return port::Status::OK();\n  }();\n  return IsStatusOk(status, /*report_error=*/true);\n}\n\nbool CudnnSupport::DoPoolBackward(\n    Stream* stream, const dnn::PoolingDescriptor& pooling_dimensions,\n    const dnn::BatchDescriptor& input_dimensions,\n    const DeviceMemory<float>& input_data,\n    const dnn::BatchDescriptor& output_dimensions,\n    const DeviceMemory<float>& output_data,\n    const DeviceMemory<float>& input_diff_data,\n    DeviceMemory<float>* output_diff_data,\n    ScratchAllocator* workspace_allocator) {\n  // Alpha is the scaling factor for input.\n  float alpha = 1.0;\n  // Beta is the scaling factor for output.\n  float beta = 0.0;\n\n  CudnnTensorDescriptor src_desc(input_dimensions, CUDNN_DATA_FLOAT);\n  CudnnTensorDescriptor dest_desc(output_dimensions, CUDNN_DATA_FLOAT);\n  CudnnPoolingDescriptor pooling_desc(pooling_dimensions);\n\n  auto cudnn = cudnn_->GetHandle(parent_, stream);\n  const auto status = [&] {\n    RETURN_IF_CUDNN_ERROR(cudnnPoolingBackward(\n        cudnn.handle(), pooling_desc.handle(), &alpha, dest_desc.handle(),\n        output_data.opaque(), dest_desc.handle(), input_diff_data.opaque(),\n        src_desc.handle(), input_data.opaque(), &beta, src_desc.handle(),\n        output_diff_data->opaque()));\n    return port::Status::OK();\n  }();\n  return IsStatusOk(status, /*report_error=*/true);\n}\n\nbool CudnnSupport::DoPoolBackward(\n    Stream* stream, const dnn::PoolingDescriptor& pooling_dimensions,\n    const dnn::BatchDescriptor& input_dimensions,\n    const DeviceMemory<Eigen::half>& input_data,\n    const dnn::BatchDescriptor& output_dimensions,\n    const DeviceMemory<Eigen::half>& output_data,\n    const DeviceMemory<Eigen::half>& input_diff_data,\n    DeviceMemory<Eigen::half>* output_diff_data,\n    ScratchAllocator* workspace_allocator) {\n  // Alpha is the scaling factor for input.\n  float alpha = 1.0;\n  // Beta is the scaling factor for output.\n  float beta = 0.0;\n\n  CudnnTensorDescriptor src_desc(input_dimensions, CUDNN_DATA_HALF);\n  CudnnTensorDescriptor dest_desc(output_dimensions, CUDNN_DATA_HALF);\n  CudnnPoolingDescriptor pooling_desc(pooling_dimensions);\n\n  auto cudnn = cudnn_->GetHandle(parent_, stream);\n  const auto status = [&] {\n    RETURN_IF_CUDNN_ERROR(cudnnPoolingBackward(\n        cudnn.handle(), pooling_desc.handle(), &alpha, dest_desc.handle(),\n        output_data.opaque(), dest_desc.handle(), input_diff_data.opaque(),\n        src_desc.handle(), input_data.opaque(), &beta, src_desc.handle(),\n        output_diff_data->opaque()));\n    return port::Status::OK();\n  }();\n  return IsStatusOk(status, /*report_error=*/true);\n}\n\nbool CudnnSupport::DoNormalizeWithDimensions(\n    Stream* stream, const dnn::NormalizeDescriptor& normalize_descriptor,\n    const dnn::BatchDescriptor& dimensions,\n    const DeviceMemory<float>& input_data, DeviceMemory<float>* output_data) {\n  // Check for unsupported modes.\n  if (normalize_descriptor.wrap_around()) {\n    LOG(ERROR) << \"CUDA LRN does not support cudnn-around mode\";\n    return false;\n  }\n  if (normalize_descriptor.segment_size()) {\n    LOG(ERROR) << \"CUDA LRN does not support segmentation\";\n    return false;\n  }\n\n  CudnnTensorDescriptor dims(dimensions, CUDNN_DATA_FLOAT);\n  CudnnNormalizeDescriptor normalize(normalize_descriptor);\n\n  // Alpha is the scaling factor for input.\n  float alpha = 1.0f;\n  // Beta is the scaling factor for output.\n  float beta = 0.0f;\n\n  auto cudnn = cudnn_->GetHandle(parent_, stream);\n\n  // Launch the normalization.\n  const auto status = [&] {\n    RETURN_IF_CUDNN_ERROR(cudnnLRNCrossChannelForward(\n        cudnn.handle(), normalize.handle(), CUDNN_LRN_CROSS_CHANNEL_DIM1,\n        &alpha, dims.handle(), input_data.opaque(), &beta, dims.handle(),\n        output_data->opaque()));\n    return port::Status::OK();\n  }();\n  return IsStatusOk(status, /*report_error=*/true);\n}\n\nbool CudnnSupport::DoNormalizeBackwardWithDimensions(\n    Stream* stream, const dnn::NormalizeDescriptor& normalize_descriptor,\n    const dnn::BatchDescriptor& dimensions, const DeviceMemory<float>& raw_data,\n    const DeviceMemory<float>& normalized_data,\n    const DeviceMemory<float>& normalized_variable_gradient,\n    DeviceMemory<float>* raw_variable_gradient,\n    ScratchAllocator* workspace_allocator) {\n  // Check for unsupported modes.\n  if (normalize_descriptor.wrap_around()) {\n    LOG(ERROR) << \"CUDA LRN does not support cudnn-around mode\";\n    return false;\n  }\n  if (normalize_descriptor.segment_size()) {\n    LOG(ERROR) << \"CUDA LRN does not support segmentation\";\n    return false;\n  }\n\n  CudnnTensorDescriptor dims(dimensions, CUDNN_DATA_FLOAT);\n  CudnnNormalizeDescriptor normalize(normalize_descriptor);\n\n  float alpha = 1.0f;\n  float beta = 0.0f;\n\n  auto cudnn = cudnn_->GetHandle(parent_, stream);\n  const auto status = [&] {\n    RETURN_IF_CUDNN_ERROR(cudnnLRNCrossChannelBackward(\n        cudnn.handle(), normalize.handle(), CUDNN_LRN_CROSS_CHANNEL_DIM1,\n        &alpha, dims.handle(), normalized_data.opaque(), dims.handle(),\n        normalized_variable_gradient.opaque(), dims.handle(), raw_data.opaque(),\n        &beta, dims.handle(), raw_variable_gradient->opaque()));\n    return port::Status::OK();\n  }();\n  return IsStatusOk(status, /*report_error=*/true);\n}\n\nbool CudnnSupport::DoDepthConcatenate(\n    Stream* stream, port::ArraySlice<dnn::BatchDescriptor> input_dimensions,\n    port::ArraySlice<const DeviceMemory<float>*> input_data,\n    DeviceMemory<float>* output_data) {\n  CHECK_EQ(input_dimensions.size(), input_data.size());\n\n  for (const auto& dimensions : input_dimensions) {\n    if (dimensions.layout() != dnn::DataLayout::kBatchDepthYX) {\n      LOG(ERROR) << \"CudnnSupport::DoDepthConcatenate currently only \"\n                    \"supports the kBatchDepthYX layout.\";\n      return false;\n    }\n  }\n\n  if (input_dimensions.empty()) {\n    return true;  // Nothing to do.\n  }\n\n  dnn::BatchDescriptor output_dimensions =\n      dnn::BatchDescriptor::DepthConcatenateOutputDescriptor(input_dimensions);\n\n  const int64 area = output_dimensions.width() * output_dimensions.height();\n  const auto index = [area](int64 batch, int64 depth, int64 yx,\n                            int64 max_depth) {\n    return (batch * max_depth + depth) * area + yx;\n  };\n\n  std::vector<float> output_host(output_dimensions.ElementCount());\n  std::vector<float> tmp;\n  int64 depth_sum = 0;\n  for (size_t i = 0; i < input_data.size(); ++i) {\n    const auto& dimensions = input_dimensions[i];\n    tmp.resize(dimensions.ElementCount());\n    stream->ThenMemcpyD2H<float>(*input_data[i], absl::MakeSpan(tmp));\n    port::Status block_status = stream->BlockHostUntilDone();\n    if (!block_status.ok()) {\n      LOG(ERROR) << \"BlockHostUntilDone failed: \" << block_status;\n      return false;\n    }\n\n    for (int64 batch = 0; batch < output_dimensions.count(); ++batch) {\n      for (int64 yx = 0; yx < area; ++yx) {\n        for (int64 depth = 0; depth < dimensions.feature_map_count(); ++depth) {\n          LOG(INFO) << output_dimensions.ElementCount() << ' ' << batch << ' '\n                    << yx << ' ' << depth;\n          output_host[index(batch, depth + depth_sum, yx,\n                            output_dimensions.feature_map_count())] =\n              tmp[index(batch, depth, yx, dimensions.feature_map_count())];\n        }\n      }\n    }\n    depth_sum += dimensions.feature_map_count();\n  }\n  stream->ThenMemcpyH2D<float>(output_host, output_data);\n  return true;\n}\n\nbool CudnnSupport::DoElementwiseOperate(\n    Stream* stream, dnn::ElementwiseOperation operation,\n    port::ArraySlice<dnn::BatchDescriptor> input_dimensions,\n    port::ArraySlice<const DeviceMemory<float>*> input_data,\n    const dnn::BatchDescriptor& output_dimensions,\n    DeviceMemory<float>* output_data) {\n  LOG(FATAL) << \"not yet implemented\";  // TODO(leary)\n  return false;\n}\n\nbool CudnnSupport::DoXYPad(Stream* stream,\n                           const dnn::BatchDescriptor& dimensions,\n                           const DeviceMemory<float>& input_data,\n                           int64 left_pad, int64 right_pad, int64 top_pad,\n                           int64 bottom_pad, DeviceMemory<float>* output_data) {\n  LOG(FATAL) << \"not yet implemented\";  // TODO(leary)\n  return false;\n}\n\nbool CudnnSupport::DoXYSlice(Stream* stream,\n                             const dnn::BatchDescriptor& dimensions,\n                             const DeviceMemory<float>& input_data,\n                             int64 left_trim, int64 right_trim, int64 top_trim,\n                             int64 bottom_trim,\n                             DeviceMemory<float>* output_data) {\n  LOG(FATAL) << \"not yet implemented\";  // TODO(leary)\n  return false;\n}\n\nbool CudnnSupport::DoMemcpyD2HQuantized(\n    Stream* stream, const DeviceMemory<float>& gpu_unquantized_src,\n    dnn::QuantizedActivationMode mode, void* host_dst, int64 size) {\n  LOG(ERROR) << \"quantized memcpy not supported by cuDNN\";\n  return false;\n}\n\nbool CudnnSupport::DoMemcpyH2DQuantized(\n    Stream* stream, const void* host_src, int64 size,\n    dnn::QuantizedActivationMode mode,\n    DeviceMemory<float>* gpu_unquantized_dst) {\n  LOG(ERROR) << \"quantized memcpy not supported by cuDNN\";\n  return false;\n}\n\nbool CudnnSupport::DeriveOutputBatchDescriptor(\n    const dnn::BatchDescriptor& batch_descriptor,\n    const dnn::FilterDescriptor& filter_descriptor,\n    const dnn::ConvolutionDescriptor& convolution_descriptor,\n    dnn::BatchDescriptor* output_batch_descriptor) {\n  CudnnTensorDescriptor input_nd(batch_descriptor, CUDNN_DATA_FLOAT);\n  CudnnFilterDescriptor filter(filter_descriptor, CUDNN_DATA_FLOAT);\n  CudnnConvolutionDescriptor conv(convolution_descriptor, CUDNN_DATA_FLOAT);\n\n  int dn = batch_descriptor.ndims() + 2;\n  std::vector<int> dims(dn);  // in BDYX\n  const auto status = [&] {\n    RETURN_IF_CUDNN_ERROR(cudnnGetConvolutionNdForwardOutputDim(\n        conv.handle(), input_nd.handle(), filter.handle(), dn, dims.data()));\n    output_batch_descriptor->set_count(dims[0])\n        .set_feature_map_count(dims[1])\n        .set_layout(batch_descriptor.layout());\n\n    for (int i = 0; i < batch_descriptor.ndims(); i++) {\n      output_batch_descriptor->set_spatial_dim(static_cast<dnn::DimIndex>(i),\n                                               dims.rbegin()[i]);\n    }\n    return port::Status::OK();\n  }();\n  return IsStatusOk(status, /*report_error=*/true);\n}\n\n}  // namespace gpu\n\nvoid initialize_cudnn() {\n  port::Status status =\n      PluginRegistry::Instance()->RegisterFactory<PluginRegistry::DnnFactory>(\n          cuda::kCudaPlatformId, gpu::kCuDnnPlugin, \"cuDNN\",\n          [](internal::StreamExecutorInterface* parent) -> dnn::DnnSupport* {\n            gpu::GpuExecutor* cuda_executor =\n                dynamic_cast<gpu::GpuExecutor*>(parent);\n            if (cuda_executor == nullptr) {\n              LOG(ERROR) << \"Attempting to initialize an instance of the cuDNN \"\n                         << \"support library with a non-CUDA StreamExecutor\";\n              return nullptr;\n            }\n\n            gpu::CudnnSupport* dnn = new gpu::CudnnSupport(cuda_executor);\n            if (!dnn->Init().ok()) {\n              // Note: Init() will log a more specific error.\n              delete dnn;\n              return nullptr;\n            }\n            return dnn;\n          });\n\n  if (!status.ok()) {\n    LOG(ERROR) << \"Unable to register cuDNN factory: \"\n               << status.error_message();\n  }\n\n  PluginRegistry::Instance()->SetDefaultFactory(\n      cuda::kCudaPlatformId, PluginKind::kDnn, gpu::kCuDnnPlugin);\n}\n\n}  // namespace stream_executor\n\n#pragma clang diagnostic pop\n\nREGISTER_MODULE_INITIALIZER(register_cudnn,\n                            { stream_executor::initialize_cudnn(); });\n"], "filenames": ["tensorflow/stream_executor/cuda/cuda_dnn.cc"], "buggy_code_start_loc": [1471], "buggy_code_end_loc": [1490], "fixing_code_start_loc": [1471], "fixing_code_end_loc": [1494], "type": "CWE-20", "message": "In affected versions of TensorFlow running an LSTM/GRU model where the LSTM/GRU layer receives an input with zero-length results in a CHECK failure when using the CUDA backend. This can result in a query-of-death vulnerability, via denial of service, if users can control the input to the layer. This is fixed in versions 1.15.5, 2.0.4, 2.1.3, 2.2.2, 2.3.2, and 2.4.0.", "other": {"cve": {"id": "CVE-2020-26270", "sourceIdentifier": "security-advisories@github.com", "published": "2020-12-10T23:15:12.973", "lastModified": "2020-12-14T17:33:30.010", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "In affected versions of TensorFlow running an LSTM/GRU model where the LSTM/GRU layer receives an input with zero-length results in a CHECK failure when using the CUDA backend. This can result in a query-of-death vulnerability, via denial of service, if users can control the input to the layer. This is fixed in versions 1.15.5, 2.0.4, 2.1.3, 2.2.2, 2.3.2, and 2.4.0."}, {"lang": "es", "value": "En las versiones afectadas de TensorFlow que ejecutan un modelo LSTM/GRU donde la capa LSTM/GRU recibe una entrada con longitud cero, se produce un fallo de COMPROBACI\u00d3N cuando se usa el backend CUDA.&#xa0;Esto puede resultar en una vulnerabilidad query-of-death, por medio de la denegaci\u00f3n de servicio, si los usuarios pueden controlar la entrada a la capa.&#xa0;Esto es corregido en las versiones 1.15.5, 2.0.4, 2.1.3, 2.2.2, 2.3.2 y 2.4.0."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:L", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "LOW", "baseScore": 3.3, "baseSeverity": "LOW"}, "exploitabilityScore": 1.8, "impactScore": 1.4}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:L/A:L", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "LOW", "availabilityImpact": "LOW", "baseScore": 4.4, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 2.5}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:N/I:N/A:P", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "PARTIAL", "baseScore": 2.1}, "baseSeverity": "LOW", "exploitabilityScore": 3.9, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-20"}]}, {"source": "security-advisories@github.com", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-20"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionEndExcluding": "1.15.5", "matchCriteriaId": "CA3A54AC-E0F8-4741-8A80-04EEF746B14B"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.0.0", "versionEndExcluding": "2.0.4", "matchCriteriaId": "989E4548-7823-436F-A9FE-04158ED41C48"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.1.0", "versionEndExcluding": "2.1.3", "matchCriteriaId": "46417CA8-E666-4E12-B2A8-BB0E97D49BF4"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.2.0", "versionEndExcluding": "2.2.2", "matchCriteriaId": "57B24744-0D81-41E9-9ED0-7296368DEF00"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.3.0", "versionEndExcluding": "2.3.2", "matchCriteriaId": "DBEA56AF-3495-4883-9721-0FA9F08E7F6D"}]}]}], "references": [{"url": "https://github.com/tensorflow/tensorflow/commit/14755416e364f17fb1870882fa778c7fec7f16e3", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/security/advisories/GHSA-m648-33qf-v3gp", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/tensorflow/tensorflow/commit/14755416e364f17fb1870882fa778c7fec7f16e3"}}