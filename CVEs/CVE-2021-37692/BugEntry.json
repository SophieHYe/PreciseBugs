{"buggy_code": ["/*\nCopyright 2016 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n*/\n\npackage tensorflow\n\n/*\n#include <stdlib.h>\n#include <string.h>\n#include \"tensorflow/c/c_api.h\"\n\nvoid toNewTString(_GoString_ gstr, TF_TString *tstr) {\n    TF_TString_Init(tstr);\n    TF_TString_Copy(tstr, _GoStringPtr(gstr), _GoStringLen(gstr));\n}\n*/\nimport \"C\"\n\nimport (\n\t\"bytes\"\n\t\"fmt\"\n\t\"io\"\n\t\"math/bits\"\n\t\"reflect\"\n\t\"runtime\"\n\t\"unsafe\"\n)\n\n// DataType holds the type for a scalar value.  E.g., one slot in a tensor.\ntype DataType C.TF_DataType\n\n// Types of scalar values in the TensorFlow type system.\nconst (\n\tFloat      DataType = C.TF_FLOAT\n\tDouble     DataType = C.TF_DOUBLE\n\tInt32      DataType = C.TF_INT32\n\tUint32     DataType = C.TF_UINT32\n\tUint8      DataType = C.TF_UINT8\n\tInt16      DataType = C.TF_INT16\n\tInt8       DataType = C.TF_INT8\n\tString     DataType = C.TF_STRING\n\tComplex64  DataType = C.TF_COMPLEX64\n\tComplex    DataType = C.TF_COMPLEX\n\tInt64      DataType = C.TF_INT64\n\tUint64     DataType = C.TF_UINT64\n\tBool       DataType = C.TF_BOOL\n\tQint8      DataType = C.TF_QINT8\n\tQuint8     DataType = C.TF_QUINT8\n\tQint32     DataType = C.TF_QINT32\n\tBfloat16   DataType = C.TF_BFLOAT16\n\tQint16     DataType = C.TF_QINT16\n\tQuint16    DataType = C.TF_QUINT16\n\tUint16     DataType = C.TF_UINT16\n\tComplex128 DataType = C.TF_COMPLEX128\n\tHalf       DataType = C.TF_HALF\n)\n\n// Tensor holds a multi-dimensional array of elements of a single data type.\ntype Tensor struct {\n\tc     *C.TF_Tensor\n\tshape []int64\n}\n\n// NewTensor converts from a Go value to a Tensor. Valid values are scalars,\n// slices, and arrays. Every element of a slice must have the same length so\n// that the resulting Tensor has a valid shape.\nfunc NewTensor(value interface{}) (*Tensor, error) {\n\tval := reflect.ValueOf(value)\n\tshape, dataType, err := shapeAndDataTypeOf(val)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tnflattened := numElements(shape)\n\tnbytes := TypeOf(dataType, nil).Size() * uintptr(nflattened)\n\tif dataType == String {\n\t\tnbytes = uintptr(nflattened) * C.sizeof_TF_TString\n\t}\n\tvar shapePtr *C.int64_t\n\tif len(shape) > 0 {\n\t\tshapePtr = (*C.int64_t)(unsafe.Pointer(&shape[0]))\n\t}\n\tt := &Tensor{\n\t\tc:     C.TF_AllocateTensor(C.TF_DataType(dataType), shapePtr, C.int(len(shape)), C.size_t(nbytes)),\n\t\tshape: shape,\n\t}\n\n\traw := tensorData(t.c)\n\n\truntime.SetFinalizer(t, func(t *Tensor) {\n\t\tif dataType == String {\n\t\t\tt.clearTStrings(raw, nflattened)\n\t\t}\n\n\t\tt.finalize()\n\t})\n\n\tbuf := bytes.NewBuffer(raw[:0:len(raw)])\n\n\tif isAllArray(val.Type()) {\n\t\t// We have arrays all the way down, or just primitive types. We can\n\t\t// just copy the memory in as it is all contiguous.\n\t\tif err := copyPtr(buf, unpackEFace(value).data, int(val.Type().Size())); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t} else {\n\t\t// When there are slices involved the memory for each leaf slice may\n\t\t// not be contiguous with the others or in the order we might\n\t\t// expect, so we need to work our way down to each slice of\n\t\t// primitives and copy them individually\n\t\tif err := encodeTensorWithSlices(buf, val, shape); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\tif uintptr(buf.Len()) != nbytes {\n\t\treturn nil, bug(\"NewTensor incorrectly calculated the size of a tensor with type %v and shape %v as %v bytes instead of %v\", dataType, shape, nbytes, buf.Len())\n\t}\n\treturn t, nil\n}\n\n// isAllArray returns true if type is a primitive type or an array of primitive\n// types or an array of ... etc.. When this is true the data we want is\n// contiguous in RAM.\nfunc isAllArray(typ reflect.Type) bool {\n\tswitch typ.Kind() {\n\tcase reflect.String:\n\t\treturn false\n\tcase reflect.Slice:\n\t\treturn false\n\tcase reflect.Array:\n\t\treturn isAllArray(typ.Elem())\n\tdefault:\n\t\t// We know the type is slices/arrays of slices/arrays of primitive types.\n\t\treturn true\n\t}\n}\n\n// eface defines what an interface type actually is: a pointer to type\n// information about the encapsulated type and a pointer to the encapsulated\n// value.\ntype eface struct {\n\trtype unsafe.Pointer\n\tdata  unsafe.Pointer\n}\n\n// unpackEFace gives us an effient way to get us a pointer to the value carried\n// in an interface. If you wrap a pointer type in an interface then the pointer\n// is directly stored in the interface struct. If you wrap a value type in an\n// interface then the compiler copies the value into a newly allocated piece of\n// memory and stores a pointer to that memory in the interface. So we're\n// guaranteed to get a pointer. Go reflection doesn't expose the pointer to\n// value types straightforwardly as it doesn't want you to think you have a\n// reference to the original value. But we just want a pointer to make it\n// efficient to read the value, so cheating like this should be safe and\n// reasonable.\nfunc unpackEFace(obj interface{}) *eface {\n\treturn (*eface)(unsafe.Pointer(&obj))\n}\n\n// ReadTensor constructs a Tensor with the provided type and shape from the\n// serialized tensor contents in r.\n//\n// See also WriteContentsTo.\nfunc ReadTensor(dataType DataType, shape []int64, r io.Reader) (*Tensor, error) {\n\tif err := isTensorSerializable(dataType); err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar shapePtr *C.int64_t\n\tif len(shape) > 0 {\n\t\tfor _, dim := range shape {\n\t\t\tif dim < 0 {\n\t\t\t\treturn nil, fmt.Errorf(\"all shape dimentions should be non-negative: %v\", shape)\n\t\t\t}\n\t\t}\n\t\tshapePtr = (*C.int64_t)(unsafe.Pointer(&shape[0]))\n\t}\n\n\tnbytes := TypeOf(dataType, nil).Size() * uintptr(numElements(shape))\n\tt := &Tensor{\n\t\tc:     C.TF_AllocateTensor(C.TF_DataType(dataType), shapePtr, C.int(len(shape)), C.size_t(nbytes)),\n\t\tshape: shape,\n\t}\n\truntime.SetFinalizer(t, (*Tensor).finalize)\n\traw := tensorData(t.c)\n\tif _, err := io.ReadFull(r, raw); err != nil {\n\t\treturn nil, err\n\t}\n\treturn t, nil\n}\n\n// newTensorFromC takes ownership of c and returns the owning Tensor.\nfunc newTensorFromC(c *C.TF_Tensor) *Tensor {\n\tvar shape []int64\n\tif ndims := int(C.TF_NumDims(c)); ndims > 0 {\n\t\tshape = make([]int64, ndims)\n\t}\n\tfor i := range shape {\n\t\tshape[i] = int64(C.TF_Dim(c, C.int(i)))\n\t}\n\tt := &Tensor{c: c, shape: shape}\n\truntime.SetFinalizer(t, (*Tensor).finalize)\n\treturn t\n}\n\nfunc (t *Tensor) clearTStrings(raw []byte, n int64) {\n\ttstrs := (*(*[]C.TF_TString)(unsafe.Pointer(&raw)))[:n]\n\n\tfor _, tstr := range tstrs {\n\t\tC.TF_TString_Dealloc(&tstr)\n\t}\n}\n\nfunc (t *Tensor) finalize() { C.TF_DeleteTensor(t.c) }\n\n// DataType returns the scalar datatype of the Tensor.\nfunc (t *Tensor) DataType() DataType { return DataType(C.TF_TensorType(t.c)) }\n\n// Shape returns the shape of the Tensor.\nfunc (t *Tensor) Shape() []int64 { return t.shape }\n\n// Reshape  updates tensor's shape in place if this is possible or returns an error otherwise.\nfunc (t *Tensor) Reshape(newShape []int64) error {\n\toldShapeSize := numElements(t.shape)\n\tnewShapeSize := numElements(newShape)\n\n\tif oldShapeSize != newShapeSize {\n\t\treturn fmt.Errorf(\"unable to convert shape %v (num_elements: %d) into shape %v (num_elements: %d)\", t.shape, oldShapeSize, newShape, newShapeSize)\n\t}\n\n\tif len(newShape) == 0 {\n\t\treturn nil\n\t}\n\n\tvar shapePtr *C.int64_t\n\tshapePtr = (*C.int64_t)(unsafe.Pointer(&newShape[0]))\n\n\tstatus := newStatus()\n\tC.TF_TensorBitcastFrom(t.c, C.TF_TensorType(t.c), t.c, shapePtr, C.int(len(newShape)), status.c)\n\n\tif err := status.Err(); err != nil {\n\t\treturn err\n\t}\n\tt.shape = newShape\n\treturn nil\n}\n\n// Value converts the Tensor to a Go value. For now, not all Tensor types are\n// supported, and this function may panic if it encounters an unsupported\n// DataType.\n//\n// The type of the output depends on the Tensor type and dimensions.\n// For example:\n// Tensor(int64, 0): int64\n// Tensor(float64, 3): [][][]float64\nfunc (t *Tensor) Value() interface{} {\n\traw := tensorData(t.c)\n\tshape := t.Shape()\n\tdt := t.DataType()\n\treturn decodeTensor(raw, shape, dt).Interface()\n}\n\nfunc decodeTensor(raw []byte, shape []int64, dt DataType) reflect.Value {\n\t// Create a 1-dimensional slice of the base large enough for the data and\n\t// copy the data in.\n\tn := int(numElements(shape))\n\n\tvar (\n\t\tslice reflect.Value\n\t\ttyp   reflect.Type\n\t)\n\tif dt == String {\n\t\tstrs, err := decodeOneDimString(raw, n)\n\t\tif err != nil {\n\t\t\tpanic(bug(\"unable to decode string with shape %v: %v\", shape, err))\n\t\t}\n\t\tslice = reflect.ValueOf(strs)\n\t\ttyp = slice.Type()\n\t} else {\n\t\ttyp = typeForDataType(dt)\n\t\tl := n * int(typ.Size())\n\t\ttyp = reflect.SliceOf(typ)\n\t\tslice = reflect.MakeSlice(typ, n, n)\n\t\tbaseBytes := *(*[]byte)(unsafe.Pointer(&sliceHeader{\n\t\t\tData: unsafe.Pointer(slice.Pointer()),\n\t\t\tLen:  l,\n\t\t\tCap:  l,\n\t\t}))\n\t\tcopy(baseBytes, raw)\n\t}\n\n\t// Now we have the data in place in the base slice we can add the\n\t// dimensions. We want to walk backwards through the shape. If the shape is\n\t// length 1 or 0 then we're already done.\n\tif len(shape) == 0 {\n\t\treturn slice.Index(0)\n\t}\n\tif len(shape) == 1 {\n\t\treturn slice\n\t}\n\t// We have a special case if the tensor has no data. Our backing slice is\n\t// empty, but we still want to create slices following the shape. In this\n\t// case only the final part of the shape will be 0 and we want to recalculate\n\t// n at this point ignoring that 0.\n\t// For example if our shape is 3 * 2 * 0 then n will be zero, but we still\n\t// want 6 zero length slices to group as follows.\n\t// {{} {}} {{} {}} {{} {}}\n\tif n == 0 {\n\t\tn = int(numElements(shape[:len(shape)-1]))\n\t}\n\tfor i := len(shape) - 2; i >= 0; i-- {\n\t\tunderlyingSize := typ.Elem().Size()\n\t\ttyp = reflect.SliceOf(typ)\n\t\tsubsliceLen := int(shape[i+1])\n\t\tif subsliceLen != 0 {\n\t\t\tn = n / subsliceLen\n\t\t}\n\t\t// Just using reflection it is difficult to avoid unnecessary\n\t\t// allocations while setting up the sub-slices as the Slice function on\n\t\t// a slice Value allocates. So we end up doing pointer arithmetic!\n\t\t// Pointer() on a slice gives us access to the data backing the slice.\n\t\t// We insert slice headers directly into this data.\n\t\tdata := unsafe.Pointer(slice.Pointer())\n\t\tnextSlice := reflect.MakeSlice(typ, n, n)\n\n\t\tfor j := 0; j < n; j++ {\n\t\t\t// This is equivalent to nSlice[j] = slice[j*subsliceLen: (j+1)*subsliceLen]\n\t\t\tsetSliceInSlice(nextSlice, j, sliceHeader{\n\t\t\t\tData: unsafe.Pointer(uintptr(data) + (uintptr(j*subsliceLen) * underlyingSize)),\n\t\t\t\tLen:  subsliceLen,\n\t\t\t\tCap:  subsliceLen,\n\t\t\t})\n\t\t}\n\n\t\tslice = nextSlice\n\t}\n\treturn slice\n}\n\n// setSliceInSlice sets slice[index] = content.\nfunc setSliceInSlice(slice reflect.Value, index int, content sliceHeader) {\n\tconst sliceSize = unsafe.Sizeof(sliceHeader{})\n\t// We must cast slice.Pointer to uninptr & back again to avoid GC issues.\n\t// See https://github.com/google/go-cmp/issues/167#issuecomment-546093202\n\t*(*sliceHeader)(unsafe.Pointer(uintptr(unsafe.Pointer(slice.Pointer())) + (uintptr(index) * sliceSize))) = content\n}\n\n// decodeOneDimString decodes a string tensor into a one-dimensional []string.\nfunc decodeOneDimString(raw []byte, nStrings int) ([]string, error) {\n\tstrs := make([]string, nStrings)\n\ttstrs := (*(*[]C.TF_TString)(unsafe.Pointer(&raw)))[:nStrings]\n\n\tfor i, tstr := range tstrs {\n\t\tdst := C.TF_TString_GetDataPointer(&tstr)\n\t\tdstLen := C.TF_TString_GetSize(&tstr)\n\n\t\tstrs[i] = C.GoStringN(dst, C.int(dstLen))\n\t}\n\n\treturn strs, nil\n}\n\n// WriteContentsTo writes the serialized contents of t to w.\n//\n// Returns the number of bytes written. See ReadTensor for\n// reconstructing a Tensor from the serialized form.\n//\n// WARNING: WriteContentsTo is not comprehensive and will fail\n// if t.DataType() is non-numeric (e.g., String). See\n// https://github.com/tensorflow/tensorflow/issues/6003.\nfunc (t *Tensor) WriteContentsTo(w io.Writer) (int64, error) {\n\tif err := isTensorSerializable(t.DataType()); err != nil {\n\t\treturn 0, err\n\t}\n\treturn io.Copy(w, bytes.NewReader(tensorData(t.c)))\n}\n\nfunc tensorData(c *C.TF_Tensor) []byte {\n\t// See: https://github.com/golang/go/wiki/cgo#turning-c-arrays-into-go-slices\n\tcbytes := C.TF_TensorData(c)\n\tif cbytes == nil {\n\t\treturn nil\n\t}\n\tlength := int(C.TF_TensorByteSize(c))\n\tvar slice []byte\n\tif unsafe.Sizeof(unsafe.Pointer(nil)) == 8 {\n\t\tslice = (*[1<<50 - 1]byte)(unsafe.Pointer(cbytes))[:length:length]\n\t} else {\n\t\tslice = (*[1 << 30]byte)(unsafe.Pointer(cbytes))[:length:length]\n\t}\n\treturn slice\n}\n\nvar types = []struct {\n\ttyp      reflect.Type\n\tdataType C.TF_DataType\n}{\n\t{reflect.TypeOf(float32(0)), C.TF_FLOAT},\n\t{reflect.TypeOf(float64(0)), C.TF_DOUBLE},\n\t{reflect.TypeOf(int32(0)), C.TF_INT32},\n\t{reflect.TypeOf(uint32(0)), C.TF_UINT32},\n\t{reflect.TypeOf(uint8(0)), C.TF_UINT8},\n\t{reflect.TypeOf(int16(0)), C.TF_INT16},\n\t{reflect.TypeOf(int8(0)), C.TF_INT8},\n\t{reflect.TypeOf(\"\"), C.TF_STRING},\n\t{reflect.TypeOf(complex(float32(0), float32(0))), C.TF_COMPLEX64},\n\t{reflect.TypeOf(int64(0)), C.TF_INT64},\n\t{reflect.TypeOf(uint64(0)), C.TF_UINT64},\n\t{reflect.TypeOf(false), C.TF_BOOL},\n\t{reflect.TypeOf(uint16(0)), C.TF_UINT16},\n\t{reflect.TypeOf(complex(float64(0), float64(0))), C.TF_COMPLEX128},\n\t// TODO(apassos): support DT_RESOURCE representation in go.\n\t// TODO(keveman): support DT_VARIANT representation in go.\n}\n\n// shapeAndDataTypeOf returns the data type and shape of the Tensor\n// corresponding to a Go type.\nfunc shapeAndDataTypeOf(val reflect.Value) (shape []int64, dt DataType, err error) {\n\ttyp := val.Type()\n\tfor typ.Kind() == reflect.Array || typ.Kind() == reflect.Slice {\n\t\tshape = append(shape, int64(val.Len()))\n\t\tif val.Len() > 0 {\n\t\t\t// In order to check tensor structure properly in general case we need to iterate over all slices of the tensor to check sizes match\n\t\t\t// Since we already going to iterate over all elements in encodeTensor() let's\n\t\t\t// 1) do the actual check in encodeTensor() to save some cpu cycles here\n\t\t\t// 2) assume the shape is represented by lengths of elements with zero index in each dimension\n\t\t\tval = val.Index(0)\n\t\t}\n\t\ttyp = typ.Elem()\n\t}\n\tfor _, t := range types {\n\t\tif typ.Kind() == t.typ.Kind() {\n\t\t\treturn shape, DataType(t.dataType), nil\n\t\t}\n\t}\n\treturn shape, dt, fmt.Errorf(\"unsupported type %v\", typ)\n}\n\nfunc typeForDataType(dt DataType) reflect.Type {\n\tfor _, t := range types {\n\t\tif dt == DataType(t.dataType) {\n\t\t\treturn t.typ\n\t\t}\n\t}\n\tpanic(bug(\"DataType %v is not supported (see https://www.tensorflow.org/code/tensorflow/core/framework/types.proto)\", dt))\n}\n\n// TypeOf converts from a DataType and Shape to the equivalent Go type.\nfunc TypeOf(dt DataType, shape []int64) reflect.Type {\n\tret := typeForDataType(dt)\n\tfor range shape {\n\t\tret = reflect.SliceOf(ret)\n\t}\n\treturn ret\n}\n\nfunc numElements(shape []int64) int64 {\n\tn := int64(1)\n\tfor _, d := range shape {\n\t\tn *= d\n\t}\n\treturn n\n}\n\n// sizeVarUint determines how many bytes it would take to encode the int v as\n// an unsigned varint\nfunc sizeVarUint(v uint64) int {\n\tif v < 0x80 {\n\t\treturn 1\n\t}\n\tbits := bits.Len64(v)\n\treturn (bits + 6) / 7\n}\n\n// encodeTensorWithSlices writes v to the specified buffer using the format specified in\n// c_api.h. Use stringEncoder for String tensors.\nfunc encodeTensorWithSlices(w *bytes.Buffer, v reflect.Value, shape []int64) error {\n\t// If current dimension is a slice, verify that it has the expected size\n\t// Go's type system makes that guarantee for arrays.\n\tif v.Kind() == reflect.Slice {\n\t\texpected := int(shape[0])\n\t\tif v.Len() != expected {\n\t\t\treturn fmt.Errorf(\"mismatched slice lengths: %d and %d\", v.Len(), expected)\n\t\t}\n\t} else if v.Kind() == reflect.String {\n\t\ts := v.Interface().(string)\n\t\tvar tstr C.TF_TString\n\t\tC.toNewTString(s, &tstr)\n\t\tptr := unsafe.Pointer(&tstr)\n\t\treturn copyPtr(w, ptr, C.sizeof_TF_TString)\n\t} else if v.Kind() != reflect.Array {\n\t\treturn fmt.Errorf(\"unsupported type %v\", v.Type())\n\t}\n\n\t// Once we have just a single dimension we can just copy the data\n\tif len(shape) == 1 && v.Len() > 0 && v.Index(0).Kind() != reflect.String {\n\t\telt := v.Index(0)\n\t\tif !elt.CanAddr() {\n\t\t\tpanic(\"cannot take address\")\n\t\t}\n\t\tptr := unsafe.Pointer(elt.Addr().Pointer())\n\t\treturn copyPtr(w, ptr, v.Len()*int(elt.Type().Size()))\n\t}\n\n\tsubShape := shape[1:]\n\tfor i := 0; i < v.Len(); i++ {\n\t\terr := encodeTensorWithSlices(w, v.Index(i), subShape)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// It isn't safe to use reflect.SliceHeader as it uses a uintptr for Data and\n// this is not inspected by the garbage collector\ntype sliceHeader struct {\n\tData unsafe.Pointer\n\tLen  int\n\tCap  int\n}\n\n// copyPtr copies the backing data for a slice or array directly into w. Note\n// we don't need to worry about byte ordering because we want the natural byte\n// order for the machine we're running on.\nfunc copyPtr(w *bytes.Buffer, ptr unsafe.Pointer, l int) error {\n\t// Convert our slice header into a []byte so we can call w.Write\n\tb := *(*[]byte)(unsafe.Pointer(&sliceHeader{\n\t\tData: ptr,\n\t\tLen:  l,\n\t\tCap:  l,\n\t}))\n\t_, err := w.Write(b)\n\treturn err\n}\n\nfunc bug(format string, args ...interface{}) error {\n\treturn fmt.Errorf(\"BUG: Please report at https://github.com/tensorflow/tensorflow/issues with the note: Go TensorFlow %v: %v\", Version(), fmt.Sprintf(format, args...))\n}\n\nfunc isTensorSerializable(dataType DataType) error {\n\t// For numeric types, the serialized Tensor matches the in-memory\n\t// representation.  See the implementation of Tensor::AsProtoContent in\n\t// https://www.tensorflow.org/code/tensorflow/core/framework/tensor.cc\n\t//\n\t// The more appropriate way to be in sync with Tensor::AsProtoContent\n\t// would be to have the TensorFlow C library export functions for\n\t// serialization and deserialization of Tensors.  Till then capitalize\n\t// on knowledge of the implementation for numeric types.\n\tswitch dataType {\n\tcase Float, Double, Int32, Uint8, Int16, Int8, Complex, Int64, Bool, Quint8, Qint32, Bfloat16, Qint16, Quint16, Uint16, Complex128, Half:\n\t\treturn nil\n\tdefault:\n\t\treturn fmt.Errorf(\"serialization of tensors with the DataType %d is not yet supported, see https://github.com/tensorflow/tensorflow/issues/6003\", dataType)\n\t}\n}\n"], "fixing_code": ["/*\nCopyright 2016 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n*/\n\npackage tensorflow\n\n/*\n#include <stdlib.h>\n#include <string.h>\n#include \"tensorflow/c/c_api.h\"\n\nvoid toNewTString(_GoString_ gstr, TF_TString *tstr) {\n    TF_TString_Init(tstr);\n    TF_TString_Copy(tstr, _GoStringPtr(gstr), _GoStringLen(gstr));\n}\n*/\nimport \"C\"\n\nimport (\n\t\"bytes\"\n\t\"fmt\"\n\t\"io\"\n\t\"math/bits\"\n\t\"reflect\"\n\t\"runtime\"\n\t\"unsafe\"\n)\n\n// DataType holds the type for a scalar value.  E.g., one slot in a tensor.\ntype DataType C.TF_DataType\n\n// Types of scalar values in the TensorFlow type system.\nconst (\n\tFloat      DataType = C.TF_FLOAT\n\tDouble     DataType = C.TF_DOUBLE\n\tInt32      DataType = C.TF_INT32\n\tUint32     DataType = C.TF_UINT32\n\tUint8      DataType = C.TF_UINT8\n\tInt16      DataType = C.TF_INT16\n\tInt8       DataType = C.TF_INT8\n\tString     DataType = C.TF_STRING\n\tComplex64  DataType = C.TF_COMPLEX64\n\tComplex    DataType = C.TF_COMPLEX\n\tInt64      DataType = C.TF_INT64\n\tUint64     DataType = C.TF_UINT64\n\tBool       DataType = C.TF_BOOL\n\tQint8      DataType = C.TF_QINT8\n\tQuint8     DataType = C.TF_QUINT8\n\tQint32     DataType = C.TF_QINT32\n\tBfloat16   DataType = C.TF_BFLOAT16\n\tQint16     DataType = C.TF_QINT16\n\tQuint16    DataType = C.TF_QUINT16\n\tUint16     DataType = C.TF_UINT16\n\tComplex128 DataType = C.TF_COMPLEX128\n\tHalf       DataType = C.TF_HALF\n)\n\n// Tensor holds a multi-dimensional array of elements of a single data type.\ntype Tensor struct {\n\tc     *C.TF_Tensor\n\tshape []int64\n}\n\n// NewTensor converts from a Go value to a Tensor. Valid values are scalars,\n// slices, and arrays. Every element of a slice must have the same length so\n// that the resulting Tensor has a valid shape.\nfunc NewTensor(value interface{}) (*Tensor, error) {\n\tval := reflect.ValueOf(value)\n\tshape, dataType, err := shapeAndDataTypeOf(val)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tnflattened := numElements(shape)\n\tnbytes := TypeOf(dataType, nil).Size() * uintptr(nflattened)\n\tif dataType == String {\n\t\tnbytes = uintptr(nflattened) * C.sizeof_TF_TString\n\t}\n\tvar shapePtr *C.int64_t\n\tif len(shape) > 0 {\n\t\tshapePtr = (*C.int64_t)(unsafe.Pointer(&shape[0]))\n\t}\n\tt := &Tensor{\n\t\tc:     C.TF_AllocateTensor(C.TF_DataType(dataType), shapePtr, C.int(len(shape)), C.size_t(nbytes)),\n\t\tshape: shape,\n\t}\n\n\traw := tensorData(t.c)\n\n\tdefer runtime.SetFinalizer(t, func(t *Tensor) {\n\t\tif dataType == String {\n\t\t\tt.clearTStrings(raw, int64(nbytes/C.sizeof_TF_TString))\n\t\t}\n\n\t\tt.finalize()\n\t})\n\n\tbuf := bytes.NewBuffer(raw[:0:len(raw)])\n\n\tif isAllArray(val.Type()) {\n\t\t// We have arrays all the way down, or just primitive types. We can\n\t\t// just copy the memory in as it is all contiguous.\n\t\tif _, err := copyPtr(buf, unpackEFace(value).data, int(val.Type().Size())); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t} else {\n\t\t// When there are slices involved the memory for each leaf slice may\n\t\t// not be contiguous with the others or in the order we might\n\t\t// expect, so we need to work our way down to each slice of\n\t\t// primitives and copy them individually\n\t\tif n, err := encodeTensorWithSlices(buf, val, shape); err != nil {\n\t\t\t// Set nbytes to count of bytes written for deferred call to\n\t\t\t// runtime.SetFinalizer\n\t\t\tnbytes = uintptr(n)\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\tif uintptr(buf.Len()) != nbytes {\n\t\treturn nil, bug(\"NewTensor incorrectly calculated the size of a tensor with type %v and shape %v as %v bytes instead of %v\", dataType, shape, nbytes, buf.Len())\n\t}\n\treturn t, nil\n}\n\n// isAllArray returns true if type is a primitive type or an array of primitive\n// types or an array of ... etc.. When this is true the data we want is\n// contiguous in RAM.\nfunc isAllArray(typ reflect.Type) bool {\n\tswitch typ.Kind() {\n\tcase reflect.String:\n\t\treturn false\n\tcase reflect.Slice:\n\t\treturn false\n\tcase reflect.Array:\n\t\treturn isAllArray(typ.Elem())\n\tdefault:\n\t\t// We know the type is slices/arrays of slices/arrays of primitive types.\n\t\treturn true\n\t}\n}\n\n// eface defines what an interface type actually is: a pointer to type\n// information about the encapsulated type and a pointer to the encapsulated\n// value.\ntype eface struct {\n\trtype unsafe.Pointer\n\tdata  unsafe.Pointer\n}\n\n// unpackEFace gives us an effient way to get us a pointer to the value carried\n// in an interface. If you wrap a pointer type in an interface then the pointer\n// is directly stored in the interface struct. If you wrap a value type in an\n// interface then the compiler copies the value into a newly allocated piece of\n// memory and stores a pointer to that memory in the interface. So we're\n// guaranteed to get a pointer. Go reflection doesn't expose the pointer to\n// value types straightforwardly as it doesn't want you to think you have a\n// reference to the original value. But we just want a pointer to make it\n// efficient to read the value, so cheating like this should be safe and\n// reasonable.\nfunc unpackEFace(obj interface{}) *eface {\n\treturn (*eface)(unsafe.Pointer(&obj))\n}\n\n// ReadTensor constructs a Tensor with the provided type and shape from the\n// serialized tensor contents in r.\n//\n// See also WriteContentsTo.\nfunc ReadTensor(dataType DataType, shape []int64, r io.Reader) (*Tensor, error) {\n\tif err := isTensorSerializable(dataType); err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar shapePtr *C.int64_t\n\tif len(shape) > 0 {\n\t\tfor _, dim := range shape {\n\t\t\tif dim < 0 {\n\t\t\t\treturn nil, fmt.Errorf(\"all shape dimentions should be non-negative: %v\", shape)\n\t\t\t}\n\t\t}\n\t\tshapePtr = (*C.int64_t)(unsafe.Pointer(&shape[0]))\n\t}\n\n\tnbytes := TypeOf(dataType, nil).Size() * uintptr(numElements(shape))\n\tt := &Tensor{\n\t\tc:     C.TF_AllocateTensor(C.TF_DataType(dataType), shapePtr, C.int(len(shape)), C.size_t(nbytes)),\n\t\tshape: shape,\n\t}\n\truntime.SetFinalizer(t, (*Tensor).finalize)\n\traw := tensorData(t.c)\n\tif _, err := io.ReadFull(r, raw); err != nil {\n\t\treturn nil, err\n\t}\n\treturn t, nil\n}\n\n// newTensorFromC takes ownership of c and returns the owning Tensor.\nfunc newTensorFromC(c *C.TF_Tensor) *Tensor {\n\tvar shape []int64\n\tif ndims := int(C.TF_NumDims(c)); ndims > 0 {\n\t\tshape = make([]int64, ndims)\n\t}\n\tfor i := range shape {\n\t\tshape[i] = int64(C.TF_Dim(c, C.int(i)))\n\t}\n\tt := &Tensor{c: c, shape: shape}\n\truntime.SetFinalizer(t, (*Tensor).finalize)\n\treturn t\n}\n\nfunc (t *Tensor) clearTStrings(raw []byte, n int64) {\n\ttstrs := (*(*[]C.TF_TString)(unsafe.Pointer(&raw)))[:n]\n\n\tfor _, tstr := range tstrs {\n\t\tC.TF_TString_Dealloc(&tstr)\n\t}\n}\n\nfunc (t *Tensor) finalize() { C.TF_DeleteTensor(t.c) }\n\n// DataType returns the scalar datatype of the Tensor.\nfunc (t *Tensor) DataType() DataType { return DataType(C.TF_TensorType(t.c)) }\n\n// Shape returns the shape of the Tensor.\nfunc (t *Tensor) Shape() []int64 { return t.shape }\n\n// Reshape  updates tensor's shape in place if this is possible or returns an error otherwise.\nfunc (t *Tensor) Reshape(newShape []int64) error {\n\toldShapeSize := numElements(t.shape)\n\tnewShapeSize := numElements(newShape)\n\n\tif oldShapeSize != newShapeSize {\n\t\treturn fmt.Errorf(\"unable to convert shape %v (num_elements: %d) into shape %v (num_elements: %d)\", t.shape, oldShapeSize, newShape, newShapeSize)\n\t}\n\n\tif len(newShape) == 0 {\n\t\treturn nil\n\t}\n\n\tvar shapePtr *C.int64_t\n\tshapePtr = (*C.int64_t)(unsafe.Pointer(&newShape[0]))\n\n\tstatus := newStatus()\n\tC.TF_TensorBitcastFrom(t.c, C.TF_TensorType(t.c), t.c, shapePtr, C.int(len(newShape)), status.c)\n\n\tif err := status.Err(); err != nil {\n\t\treturn err\n\t}\n\tt.shape = newShape\n\treturn nil\n}\n\n// Value converts the Tensor to a Go value. For now, not all Tensor types are\n// supported, and this function may panic if it encounters an unsupported\n// DataType.\n//\n// The type of the output depends on the Tensor type and dimensions.\n// For example:\n// Tensor(int64, 0): int64\n// Tensor(float64, 3): [][][]float64\nfunc (t *Tensor) Value() interface{} {\n\traw := tensorData(t.c)\n\tshape := t.Shape()\n\tdt := t.DataType()\n\treturn decodeTensor(raw, shape, dt).Interface()\n}\n\nfunc decodeTensor(raw []byte, shape []int64, dt DataType) reflect.Value {\n\t// Create a 1-dimensional slice of the base large enough for the data and\n\t// copy the data in.\n\tn := int(numElements(shape))\n\n\tvar (\n\t\tslice reflect.Value\n\t\ttyp   reflect.Type\n\t)\n\tif dt == String {\n\t\tstrs, err := decodeOneDimString(raw, n)\n\t\tif err != nil {\n\t\t\tpanic(bug(\"unable to decode string with shape %v: %v\", shape, err))\n\t\t}\n\t\tslice = reflect.ValueOf(strs)\n\t\ttyp = slice.Type()\n\t} else {\n\t\ttyp = typeForDataType(dt)\n\t\tl := n * int(typ.Size())\n\t\ttyp = reflect.SliceOf(typ)\n\t\tslice = reflect.MakeSlice(typ, n, n)\n\t\tbaseBytes := *(*[]byte)(unsafe.Pointer(&sliceHeader{\n\t\t\tData: unsafe.Pointer(slice.Pointer()),\n\t\t\tLen:  l,\n\t\t\tCap:  l,\n\t\t}))\n\t\tcopy(baseBytes, raw)\n\t}\n\n\t// Now we have the data in place in the base slice we can add the\n\t// dimensions. We want to walk backwards through the shape. If the shape is\n\t// length 1 or 0 then we're already done.\n\tif len(shape) == 0 {\n\t\treturn slice.Index(0)\n\t}\n\tif len(shape) == 1 {\n\t\treturn slice\n\t}\n\t// We have a special case if the tensor has no data. Our backing slice is\n\t// empty, but we still want to create slices following the shape. In this\n\t// case only the final part of the shape will be 0 and we want to recalculate\n\t// n at this point ignoring that 0.\n\t// For example if our shape is 3 * 2 * 0 then n will be zero, but we still\n\t// want 6 zero length slices to group as follows.\n\t// {{} {}} {{} {}} {{} {}}\n\tif n == 0 {\n\t\tn = int(numElements(shape[:len(shape)-1]))\n\t}\n\tfor i := len(shape) - 2; i >= 0; i-- {\n\t\tunderlyingSize := typ.Elem().Size()\n\t\ttyp = reflect.SliceOf(typ)\n\t\tsubsliceLen := int(shape[i+1])\n\t\tif subsliceLen != 0 {\n\t\t\tn = n / subsliceLen\n\t\t}\n\t\t// Just using reflection it is difficult to avoid unnecessary\n\t\t// allocations while setting up the sub-slices as the Slice function on\n\t\t// a slice Value allocates. So we end up doing pointer arithmetic!\n\t\t// Pointer() on a slice gives us access to the data backing the slice.\n\t\t// We insert slice headers directly into this data.\n\t\tdata := unsafe.Pointer(slice.Pointer())\n\t\tnextSlice := reflect.MakeSlice(typ, n, n)\n\n\t\tfor j := 0; j < n; j++ {\n\t\t\t// This is equivalent to nSlice[j] = slice[j*subsliceLen: (j+1)*subsliceLen]\n\t\t\tsetSliceInSlice(nextSlice, j, sliceHeader{\n\t\t\t\tData: unsafe.Pointer(uintptr(data) + (uintptr(j*subsliceLen) * underlyingSize)),\n\t\t\t\tLen:  subsliceLen,\n\t\t\t\tCap:  subsliceLen,\n\t\t\t})\n\t\t}\n\n\t\tslice = nextSlice\n\t}\n\treturn slice\n}\n\n// setSliceInSlice sets slice[index] = content.\nfunc setSliceInSlice(slice reflect.Value, index int, content sliceHeader) {\n\tconst sliceSize = unsafe.Sizeof(sliceHeader{})\n\t// We must cast slice.Pointer to uninptr & back again to avoid GC issues.\n\t// See https://github.com/google/go-cmp/issues/167#issuecomment-546093202\n\t*(*sliceHeader)(unsafe.Pointer(uintptr(unsafe.Pointer(slice.Pointer())) + (uintptr(index) * sliceSize))) = content\n}\n\n// decodeOneDimString decodes a string tensor into a one-dimensional []string.\nfunc decodeOneDimString(raw []byte, nStrings int) ([]string, error) {\n\tstrs := make([]string, nStrings)\n\ttstrs := (*(*[]C.TF_TString)(unsafe.Pointer(&raw)))[:nStrings]\n\n\tfor i, tstr := range tstrs {\n\t\tdst := C.TF_TString_GetDataPointer(&tstr)\n\t\tdstLen := C.TF_TString_GetSize(&tstr)\n\n\t\tstrs[i] = C.GoStringN(dst, C.int(dstLen))\n\t}\n\n\treturn strs, nil\n}\n\n// WriteContentsTo writes the serialized contents of t to w.\n//\n// Returns the number of bytes written. See ReadTensor for\n// reconstructing a Tensor from the serialized form.\n//\n// WARNING: WriteContentsTo is not comprehensive and will fail\n// if t.DataType() is non-numeric (e.g., String). See\n// https://github.com/tensorflow/tensorflow/issues/6003.\nfunc (t *Tensor) WriteContentsTo(w io.Writer) (int64, error) {\n\tif err := isTensorSerializable(t.DataType()); err != nil {\n\t\treturn 0, err\n\t}\n\treturn io.Copy(w, bytes.NewReader(tensorData(t.c)))\n}\n\nfunc tensorData(c *C.TF_Tensor) []byte {\n\t// See: https://github.com/golang/go/wiki/cgo#turning-c-arrays-into-go-slices\n\tcbytes := C.TF_TensorData(c)\n\tif cbytes == nil {\n\t\treturn nil\n\t}\n\tlength := int(C.TF_TensorByteSize(c))\n\tvar slice []byte\n\tif unsafe.Sizeof(unsafe.Pointer(nil)) == 8 {\n\t\tslice = (*[1<<50 - 1]byte)(unsafe.Pointer(cbytes))[:length:length]\n\t} else {\n\t\tslice = (*[1 << 30]byte)(unsafe.Pointer(cbytes))[:length:length]\n\t}\n\treturn slice\n}\n\nvar types = []struct {\n\ttyp      reflect.Type\n\tdataType C.TF_DataType\n}{\n\t{reflect.TypeOf(float32(0)), C.TF_FLOAT},\n\t{reflect.TypeOf(float64(0)), C.TF_DOUBLE},\n\t{reflect.TypeOf(int32(0)), C.TF_INT32},\n\t{reflect.TypeOf(uint32(0)), C.TF_UINT32},\n\t{reflect.TypeOf(uint8(0)), C.TF_UINT8},\n\t{reflect.TypeOf(int16(0)), C.TF_INT16},\n\t{reflect.TypeOf(int8(0)), C.TF_INT8},\n\t{reflect.TypeOf(\"\"), C.TF_STRING},\n\t{reflect.TypeOf(complex(float32(0), float32(0))), C.TF_COMPLEX64},\n\t{reflect.TypeOf(int64(0)), C.TF_INT64},\n\t{reflect.TypeOf(uint64(0)), C.TF_UINT64},\n\t{reflect.TypeOf(false), C.TF_BOOL},\n\t{reflect.TypeOf(uint16(0)), C.TF_UINT16},\n\t{reflect.TypeOf(complex(float64(0), float64(0))), C.TF_COMPLEX128},\n\t// TODO(apassos): support DT_RESOURCE representation in go.\n\t// TODO(keveman): support DT_VARIANT representation in go.\n}\n\n// shapeAndDataTypeOf returns the data type and shape of the Tensor\n// corresponding to a Go type.\nfunc shapeAndDataTypeOf(val reflect.Value) (shape []int64, dt DataType, err error) {\n\ttyp := val.Type()\n\tfor typ.Kind() == reflect.Array || typ.Kind() == reflect.Slice {\n\t\tshape = append(shape, int64(val.Len()))\n\t\tif val.Len() > 0 {\n\t\t\t// In order to check tensor structure properly in general case we need to iterate over all slices of the tensor to check sizes match\n\t\t\t// Since we already going to iterate over all elements in encodeTensor() let's\n\t\t\t// 1) do the actual check in encodeTensor() to save some cpu cycles here\n\t\t\t// 2) assume the shape is represented by lengths of elements with zero index in each dimension\n\t\t\tval = val.Index(0)\n\t\t}\n\t\ttyp = typ.Elem()\n\t}\n\tfor _, t := range types {\n\t\tif typ.Kind() == t.typ.Kind() {\n\t\t\treturn shape, DataType(t.dataType), nil\n\t\t}\n\t}\n\treturn shape, dt, fmt.Errorf(\"unsupported type %v\", typ)\n}\n\nfunc typeForDataType(dt DataType) reflect.Type {\n\tfor _, t := range types {\n\t\tif dt == DataType(t.dataType) {\n\t\t\treturn t.typ\n\t\t}\n\t}\n\tpanic(bug(\"DataType %v is not supported (see https://www.tensorflow.org/code/tensorflow/core/framework/types.proto)\", dt))\n}\n\n// TypeOf converts from a DataType and Shape to the equivalent Go type.\nfunc TypeOf(dt DataType, shape []int64) reflect.Type {\n\tret := typeForDataType(dt)\n\tfor range shape {\n\t\tret = reflect.SliceOf(ret)\n\t}\n\treturn ret\n}\n\nfunc numElements(shape []int64) int64 {\n\tn := int64(1)\n\tfor _, d := range shape {\n\t\tn *= d\n\t}\n\treturn n\n}\n\n// sizeVarUint determines how many bytes it would take to encode the int v as\n// an unsigned varint\nfunc sizeVarUint(v uint64) int {\n\tif v < 0x80 {\n\t\treturn 1\n\t}\n\tbits := bits.Len64(v)\n\treturn (bits + 6) / 7\n}\n\n// encodeTensorWithSlices writes v to the specified buffer using the format specified in\n// c_api.h. Use stringEncoder for String tensors.\nfunc encodeTensorWithSlices(w *bytes.Buffer, v reflect.Value, shape []int64) (int, error) {\n\t// If current dimension is a slice, verify that it has the expected size\n\t// Go's type system makes that guarantee for arrays.\n\tif v.Kind() == reflect.Slice {\n\t\texpected := int(shape[0])\n\t\tif v.Len() != expected {\n\t\t\treturn 0, fmt.Errorf(\"mismatched slice lengths: %d and %d\", v.Len(), expected)\n\t\t}\n\t} else if v.Kind() == reflect.String {\n\t\ts := v.Interface().(string)\n\t\tvar tstr C.TF_TString\n\t\tC.toNewTString(s, &tstr)\n\t\tptr := unsafe.Pointer(&tstr)\n\t\treturn copyPtr(w, ptr, C.sizeof_TF_TString)\n\t} else if v.Kind() != reflect.Array {\n\t\treturn 0, fmt.Errorf(\"unsupported type %v\", v.Type())\n\t}\n\n\t// Once we have just a single dimension we can just copy the data\n\tif len(shape) == 1 && v.Len() > 0 && v.Index(0).Kind() != reflect.String {\n\t\telt := v.Index(0)\n\t\tif !elt.CanAddr() {\n\t\t\tpanic(\"cannot take address\")\n\t\t}\n\t\tptr := unsafe.Pointer(elt.Addr().Pointer())\n\t\treturn copyPtr(w, ptr, v.Len()*int(elt.Type().Size()))\n\t}\n\n\tn := 0\n\tsubShape := shape[1:]\n\tfor i := 0; i < v.Len(); i++ {\n\t\tj, err := encodeTensorWithSlices(w, v.Index(i), subShape)\n\t\tif err != nil {\n\t\t\treturn n + j, err\n\t\t}\n\t\tn += j\n\t}\n\n\treturn n, nil\n}\n\n// It isn't safe to use reflect.SliceHeader as it uses a uintptr for Data and\n// this is not inspected by the garbage collector\ntype sliceHeader struct {\n\tData unsafe.Pointer\n\tLen  int\n\tCap  int\n}\n\n// copyPtr copies the backing data for a slice or array directly into w. Note\n// we don't need to worry about byte ordering because we want the natural byte\n// order for the machine we're running on.\nfunc copyPtr(w *bytes.Buffer, ptr unsafe.Pointer, l int) (int, error) {\n\t// Convert our slice header into a []byte so we can call w.Write\n\tb := *(*[]byte)(unsafe.Pointer(&sliceHeader{\n\t\tData: ptr,\n\t\tLen:  l,\n\t\tCap:  l,\n\t}))\n\treturn w.Write(b)\n}\n\nfunc bug(format string, args ...interface{}) error {\n\treturn fmt.Errorf(\"BUG: Please report at https://github.com/tensorflow/tensorflow/issues with the note: Go TensorFlow %v: %v\", Version(), fmt.Sprintf(format, args...))\n}\n\nfunc isTensorSerializable(dataType DataType) error {\n\t// For numeric types, the serialized Tensor matches the in-memory\n\t// representation.  See the implementation of Tensor::AsProtoContent in\n\t// https://www.tensorflow.org/code/tensorflow/core/framework/tensor.cc\n\t//\n\t// The more appropriate way to be in sync with Tensor::AsProtoContent\n\t// would be to have the TensorFlow C library export functions for\n\t// serialization and deserialization of Tensors.  Till then capitalize\n\t// on knowledge of the implementation for numeric types.\n\tswitch dataType {\n\tcase Float, Double, Int32, Uint8, Int16, Int8, Complex, Int64, Bool, Quint8, Qint32, Bfloat16, Qint16, Quint16, Uint16, Complex128, Half:\n\t\treturn nil\n\tdefault:\n\t\treturn fmt.Errorf(\"serialization of tensors with the DataType %d is not yet supported, see https://github.com/tensorflow/tensorflow/issues/6003\", dataType)\n\t}\n}\n"], "filenames": ["tensorflow/go/tensor.go"], "buggy_code_start_loc": [101], "buggy_code_end_loc": [548], "fixing_code_start_loc": [101], "fixing_code_end_loc": [552], "type": "CWE-20", "message": "TensorFlow is an end-to-end open source platform for machine learning. In affected versions under certain conditions, Go code can trigger a segfault in string deallocation. For string tensors, `C.TF_TString_Dealloc` is called during garbage collection within a finalizer function. However, tensor structure isn't checked until encoding to avoid a performance penalty. The current method for dealloc assumes that encoding succeeded, but segfaults when a string tensor is garbage collected whose encoding failed (e.g., due to mismatched dimensions). To fix this, the call to set the finalizer function is deferred until `NewTensor` returns and, if encoding failed for a string tensor, deallocs are determined based on bytes written. We have patched the issue in GitHub commit 8721ba96e5760c229217b594f6d2ba332beedf22. The fix will be included in TensorFlow 2.6.0. We will also cherrypick this commit on TensorFlow 2.5.1, which is the other affected version.", "other": {"cve": {"id": "CVE-2021-37692", "sourceIdentifier": "security-advisories@github.com", "published": "2021-08-12T23:15:08.967", "lastModified": "2021-08-31T16:23:53.087", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "TensorFlow is an end-to-end open source platform for machine learning. In affected versions under certain conditions, Go code can trigger a segfault in string deallocation. For string tensors, `C.TF_TString_Dealloc` is called during garbage collection within a finalizer function. However, tensor structure isn't checked until encoding to avoid a performance penalty. The current method for dealloc assumes that encoding succeeded, but segfaults when a string tensor is garbage collected whose encoding failed (e.g., due to mismatched dimensions). To fix this, the call to set the finalizer function is deferred until `NewTensor` returns and, if encoding failed for a string tensor, deallocs are determined based on bytes written. We have patched the issue in GitHub commit 8721ba96e5760c229217b594f6d2ba332beedf22. The fix will be included in TensorFlow 2.6.0. We will also cherrypick this commit on TensorFlow 2.5.1, which is the other affected version."}, {"lang": "es", "value": "TensorFlow es una plataforma de c\u00f3digo abierto de extremo a extremo para el aprendizaje autom\u00e1tico.&#xa0;En las versiones afectadas bajo determinadas condiciones, el c\u00f3digo Go puede desencadenar un error de segmentaci\u00f3n en la desasignaci\u00f3n de cadenas.&#xa0;Para tensores de cadena, se llama a \"C.TF_TString_Dealloc\" durante la recolecci\u00f3n de basura dentro de una funci\u00f3n finalizer.&#xa0;Sin embargo, la estructura del tensor no es comprobada hasta la codificaci\u00f3n para impedir que se lleve a cabo una penalizaci\u00f3n del rendimiento.&#xa0;El m\u00e9todo actual para dealloc asume que la codificaci\u00f3n tuvo \u00e9xito, pero segfaults cuando se recolecta basura un tensor de cadena cuya codificaci\u00f3n fall\u00f3 (por ejemplo, debido a dimensiones no coincidentes).&#xa0;Para solucionar este problema, la llamada para establecer la funci\u00f3n finalizer se aplaza hasta que regrese \"NewTensor\" y, si la codificaci\u00f3n presenta un fallo para un tensor de cadena, las deslocalizaciones se determinan en funci\u00f3n de los bytes escritos.&#xa0;Hemos solucionado el problema en GitHub commit 8721ba96e5760c229217b594f6d2ba332beedf22.&#xa0;La correcci\u00f3n ser\u00e1 incluida en TensorFlow versi\u00f3n 2.6.0.&#xa0;Tambi\u00e9n seleccionaremos este commit en TensorFlow versi\u00f3n 2.5.1, que es la otra versi\u00f3n afectada."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:N/I:N/A:P", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "PARTIAL", "baseScore": 2.1}, "baseSeverity": "LOW", "exploitabilityScore": 3.9, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "security-advisories@github.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-20"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.5.0", "versionEndExcluding": "2.6.0", "matchCriteriaId": "6AA74D7D-2A3B-4409-8F59-3421CEF5E32A"}]}]}], "references": [{"url": "https://github.com/tensorflow/tensorflow/commit/8721ba96e5760c229217b594f6d2ba332beedf22", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/pull/50508", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/security/advisories/GHSA-cmgw-8vpc-rc59", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/tensorflow/tensorflow/commit/8721ba96e5760c229217b594f6d2ba332beedf22"}}