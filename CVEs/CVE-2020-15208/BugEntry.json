{"buggy_code": ["/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n#ifndef TENSORFLOW_LITE_KERNELS_INTERNAL_TYPES_H_\n#define TENSORFLOW_LITE_KERNELS_INTERNAL_TYPES_H_\n\n#include <algorithm>\n#include <cstdint>\n#include <cstring>\n#include <initializer_list>\n\n#include \"tensorflow/lite/kernels/internal/compatibility.h\"\n\nnamespace tflite {\n\nenum class FusedActivationFunctionType : uint8_t {\n  kNone,\n  kRelu6,\n  kRelu1,\n  kRelu\n};\nenum class PaddingType : uint8_t { kNone, kSame, kValid };\n\nstruct PaddingValues {\n  int16_t width;\n  int16_t height;\n  // offset is used for calculating \"remaining\" padding, for example, `width`\n  // is 1 and `width_offset` is 1, so padding_left is 1 while padding_right is\n  // 1 + 1 = 2.\n  int16_t width_offset;\n  // Same as width_offset except it's over the height dimension.\n  int16_t height_offset;\n};\n\n// This enumeration allows for non-default formats for the weights array\n// of a fully-connected operator, allowing the use of special optimized\n// runtime paths.\nenum class FullyConnectedWeightsFormat : uint8_t {\n  // Default format (flat 2D layout, the inner contiguous dimension\n  // is input_depth, the outer non-contiguous dimension is output_depth)\n  kDefault,\n  // Summary: optimized layout for fast CPU runtime implementation,\n  // aimed specifically at ARM CPUs at the moment, and specialized for\n  // 8-bit quantized layers.\n  //\n  // The use case we're concerned with here is: 8-bit quantization,\n  // large weights matrix that doesn't fit in cache (e.g. 4096x2048 in\n  // a key application that drove this), very small batch size (e.g. 1 -- 4).\n  //\n  // Even with 8-bit quantization of weights, the performance of memory\n  // accesses to the weights can become the dominant issue when\n  // the batch size is small, so each weight value is used in only a few\n  // arithmetic ops, i.e. the fully-connected node has a low arithmetic\n  // intensity. The specific issues that arise are of three kinds:\n  // (1) One may, ideally, max out DRAM bandwidth, i.e. be truly memory\n  //     bound. That's the \"good\" issue to run into.\n  // (2) One may run into sub-optimal pre-fetching: the data hasn't been\n  //     prefetched into the cache by the time we need it.\n  // (3) One may run into cache aliasing: multiple values that are\n  //     pre-fetched, alias each other in the L1 cache (which typically\n  //     has only 4-way set associativity in ARM CPUs) and thus evict\n  //     each other before we get to using them.\n  //\n  // The point of this shuffling is to avoid issues (2) and (3) so that\n  // we get as fast as possible given only the hard constraint (1).\n  // This is achieved by turning the difficulty into a solution: the\n  // difficulty, that each value loaded from memory is used only in\n  // one kernel iteration, making this operation memory-intensive, hints at\n  // the solution, of shuffling the weights so that they are stored in the\n  // exact order as the kernel needs to load them, so that the memory\n  // accesses made by the kernel are trivial. This solves (2) because the\n  // trivial memory access pattern allows the CPU's automatic prefetching\n  // to perform very well (no need even for preload instructions), and this\n  // solves (3) because the values being loaded concurrently are now\n  // contiguous in the address space, thus don't alias each other in the cache.\n  //\n  // On ARM, we typically want our kernel to process a 4x16 block of weights\n  // at a time, because:\n  //   - 16 is the number of bytes in a NEON register.\n  //   - 4 is how many rows we need to handle concurrently in the kernel in\n  //     order to have sufficient mutual independence of instructions to\n  //     maximize arithmetic throughput.\n  //\n  // Finally, the 'Int8' part in the name refers to the fact that this\n  // weights format has each weights value encoded as a signed int8_t value,\n  // even if the data type of the weights buffer is uint8_t.  This is intended\n  // to save runtime kernels the effort to have to XOR the top bit of these\n  // bytes before using them in signed arithmetic, see this file for more\n  // explanations on the 'signed int8_t trick' in matrix multiplication kernels:\n  //\n  //   tensorflow/lite/toco/graph_transformations/ensure_uint8_weights_safe_for_fast_int8_kernels.cc\n  //\n  kShuffled4x16Int8,\n};\n\n// Quantization parameters, determining the mapping of quantized values\n// to real values (i.e. determining how quantized values are mathematically\n// interpreted).\n//\n// The correspondence is as follows:\n//\n//   real_value = scale * (quantized_value - zero_point);\n//\n// In other words, zero_point designates which quantized value corresponds to\n// the real 0 value, and scale designates the difference between the real values\n// corresponding to consecutive quantized values differing by 1.\nstruct QuantizationParams {\n  int32_t zero_point = 0;\n  double scale = 0.0;\n};\n\ninline bool operator==(const QuantizationParams& qp1,\n                       const QuantizationParams& qp2) {\n  return qp1.zero_point == qp2.zero_point && qp1.scale == qp2.scale;\n}\n\ntemplate <int N>\nstruct Dims {\n  int sizes[N];\n  int strides[N];\n};\n\nclass RuntimeShape {\n public:\n  // Shapes with dimensions up to 5 are stored directly in the structure, while\n  // larger shapes are separately allocated.\n  static constexpr int kMaxSmallSize = 5;\n\n  RuntimeShape& operator=(RuntimeShape const&) = delete;\n\n  RuntimeShape() : size_(0) {}\n\n  explicit RuntimeShape(int dimensions_count) : size_(dimensions_count) {\n    if (dimensions_count > kMaxSmallSize) {\n#ifdef TF_LITE_STATIC_MEMORY\n      TFLITE_CHECK(false && \"No shape resizing supported on this platform\");\n#else  // TF_LITE_STATIC_MEMORY\n      dims_pointer_ = new int32_t[dimensions_count];\n#endif  // TF_LITE_STATIC_MEMORY\n    }\n  }\n\n  RuntimeShape(int shape_size, int32_t value) : size_(0) {\n    Resize(shape_size);\n    for (int i = 0; i < shape_size; ++i) {\n      SetDim(i, value);\n    }\n  }\n\n  RuntimeShape(int dimensions_count, const int32_t* dims_data) : size_(0) {\n    ReplaceWith(dimensions_count, dims_data);\n  }\n\n  RuntimeShape(const std::initializer_list<int> init_list) : size_(0) {\n    BuildFrom(init_list);\n  }\n\n  // Avoid using this constructor.  We should be able to delete it when C++17\n  // rolls out.\n  RuntimeShape(RuntimeShape const& other) : size_(other.DimensionsCount()) {\n    if (size_ > kMaxSmallSize) {\n      dims_pointer_ = new int32_t[size_];\n    }\n    std::memcpy(DimsData(), other.DimsData(), sizeof(int32_t) * size_);\n  }\n\n  bool operator==(const RuntimeShape& comp) const {\n    return this->size_ == comp.size_ &&\n           std::memcmp(DimsData(), comp.DimsData(), size_ * sizeof(int32_t)) ==\n               0;\n  }\n\n  ~RuntimeShape() {\n    if (size_ > kMaxSmallSize) {\n#ifdef TF_LITE_STATIC_MEMORY\n      TFLITE_CHECK(false && \"No shape resizing supported on this platform\");\n#else  // TF_LITE_STATIC_MEMORY\n      delete[] dims_pointer_;\n#endif  // TF_LITE_STATIC_MEMORY\n    }\n  }\n\n  inline int32_t DimensionsCount() const { return size_; }\n  inline int32_t Dims(int i) const {\n    TFLITE_DCHECK_GE(i, 0);\n    TFLITE_DCHECK_LT(i, size_);\n    return size_ > kMaxSmallSize ? dims_pointer_[i] : dims_[i];\n  }\n  inline void SetDim(int i, int32_t val) {\n    TFLITE_DCHECK_GE(i, 0);\n    TFLITE_DCHECK_LT(i, size_);\n    if (size_ > kMaxSmallSize) {\n      dims_pointer_[i] = val;\n    } else {\n      dims_[i] = val;\n    }\n  }\n\n  inline int32_t* DimsData() {\n    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;\n  }\n  inline const int32_t* DimsData() const {\n    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;\n  }\n  // The caller must ensure that the shape is no bigger than 5-D.\n  inline const int32_t* DimsDataUpTo5D() const { return dims_; }\n\n  inline void Resize(int dimensions_count) {\n    if (size_ > kMaxSmallSize) {\n#ifdef TF_LITE_STATIC_MEMORY\n      TFLITE_CHECK(false && \"No shape resizing supported on this platform\");\n#else  // TF_LITE_STATIC_MEMORY\n      delete[] dims_pointer_;\n#endif  // TF_LITE_STATIC_MEMORY\n    }\n    size_ = dimensions_count;\n    if (dimensions_count > kMaxSmallSize) {\n#ifdef TF_LITE_STATIC_MEMORY\n      TFLITE_CHECK(false && \"No shape resizing supported on this platform\");\n#else  // TF_LITE_STATIC_MEMORY\n      dims_pointer_ = new int32_t[dimensions_count];\n#endif  // TF_LITE_STATIC_MEMORY\n    }\n  }\n\n  inline void ReplaceWith(int dimensions_count, const int32_t* dims_data) {\n    Resize(dimensions_count);\n    int32_t* dst_dims = DimsData();\n    std::memcpy(dst_dims, dims_data, dimensions_count * sizeof(int32_t));\n  }\n\n  template <typename T>\n  inline void BuildFrom(const T& src_iterable) {\n    const int dimensions_count =\n        std::distance(src_iterable.begin(), src_iterable.end());\n    Resize(dimensions_count);\n    int32_t* data = DimsData();\n    for (auto it : src_iterable) {\n      *data = it;\n      ++data;\n    }\n  }\n\n  // This will probably be factored out. Old code made substantial use of 4-D\n  // shapes, and so this function is used to extend smaller shapes. Note that\n  // (a) as Dims<4>-dependent code is eliminated, the reliance on this should be\n  // reduced, and (b) some kernels are stricly 4-D, but then the shapes of their\n  // inputs should already be 4-D, so this function should not be needed.\n  inline static RuntimeShape ExtendedShape(int new_shape_size,\n                                           const RuntimeShape& shape) {\n    return RuntimeShape(new_shape_size, shape, 1);\n  }\n\n  inline void BuildFrom(const std::initializer_list<int> init_list) {\n    BuildFrom<const std::initializer_list<int>>(init_list);\n  }\n\n  // Returns the total count of elements, that is the size when flattened into a\n  // vector.\n  inline int FlatSize() const {\n    int buffer_size = 1;\n    const int* dims_data = reinterpret_cast<const int*>(DimsData());\n    for (int i = 0; i < size_; i++) {\n      buffer_size *= dims_data[i];\n    }\n    return buffer_size;\n  }\n\n  bool operator!=(const RuntimeShape& comp) const { return !((*this) == comp); }\n\n private:\n  // For use only by ExtendedShape(), written to guarantee (return-value) copy\n  // elision in C++17.\n  // This creates a shape padded to the desired size with the specified value.\n  RuntimeShape(int new_shape_size, const RuntimeShape& shape, int pad_value)\n      : size_(0) {\n    // If the following check fails, it is likely because a 4D-only kernel is\n    // being used with an array of larger dimension count.\n    TFLITE_CHECK_GE(new_shape_size, shape.DimensionsCount());\n    Resize(new_shape_size);\n    const int size_increase = new_shape_size - shape.DimensionsCount();\n    for (int i = 0; i < size_increase; ++i) {\n      SetDim(i, pad_value);\n    }\n    std::memcpy(DimsData() + size_increase, shape.DimsData(),\n                sizeof(int32_t) * shape.DimensionsCount());\n  }\n\n  int32_t size_;\n  union {\n    int32_t dims_[kMaxSmallSize];\n    int32_t* dims_pointer_;\n  };\n};\n\n// Converts inference-style shape to legacy tflite::Dims<4>.\ninline tflite::Dims<4> ToRuntimeDims(const tflite::RuntimeShape& array_shape) {\n  tflite::Dims<4> result;\n  const int dimensions_count = array_shape.DimensionsCount();\n  TFLITE_CHECK_LE(dimensions_count, 4);\n  int cum_prod = 1;\n  for (int i = 0; i < 4; i++) {\n    const int new_dim =\n        (i < dimensions_count) ? array_shape.Dims(dimensions_count - 1 - i) : 1;\n    result.sizes[i] = new_dim;\n    result.strides[i] = cum_prod;\n    cum_prod *= new_dim;\n  }\n  return result;\n}\n\n// TODO(b/80418076): Move to legacy ops file, update invocations.\ninline RuntimeShape DimsToShape(const tflite::Dims<4>& dims) {\n  return RuntimeShape(\n      {dims.sizes[3], dims.sizes[2], dims.sizes[1], dims.sizes[0]});\n}\n\n// Gets next index to iterate through a multidimensional array.\ninline bool NextIndex(const int num_dims, const int* dims, int* current) {\n  if (num_dims == 0) {\n    return false;\n  }\n  TFLITE_DCHECK(dims != nullptr);\n  TFLITE_DCHECK(current != nullptr);\n  int carry = 1;\n  for (int idx = num_dims - 1; idx >= 0; --idx) {\n    int current_val = current[idx] + carry;\n    TFLITE_DCHECK_GE(dims[idx], current_val);\n    if (dims[idx] == current_val) {\n      current[idx] = 0;\n    } else {\n      current[idx] = current_val;\n      carry = 0;\n      break;\n    }\n  }\n  return (carry == 0);\n}\n\n// Gets offset of index if reducing on axis. When reducing, the flattened offset\n// will not change, if the input index changes on the given axis. For example,\n// if you have a 3D tensor and you are reducing to 2D by eliminating axis 0,\n// then index (0, 1, 2) and index (1, 1, 2) will map to the same flattened\n// offset.\n// TODO(kanlig): uses Dims to represent dimensions.\ninline size_t ReducedOutputOffset(const int num_dims, const int* dims,\n                                  const int* index, const int num_axis,\n                                  const int* axis) {\n  if (num_dims == 0) {\n    return 0;\n  }\n  TFLITE_DCHECK(dims != nullptr);\n  TFLITE_DCHECK(index != nullptr);\n  size_t offset = 0;\n  for (int idx = 0; idx < num_dims; ++idx) {\n    // if we need to skip this axis\n    bool is_axis = false;\n    if (axis != nullptr) {\n      for (int axis_idx = 0; axis_idx < num_axis; ++axis_idx) {\n        if (idx == axis[axis_idx]) {\n          is_axis = true;\n          break;\n        }\n      }\n    }\n    if (!is_axis) {\n      offset = offset * static_cast<size_t>(dims[idx]) +\n               static_cast<size_t>(index[idx]);\n    }\n  }\n  return offset;\n}\n\ninline int Offset(const RuntimeShape& shape, int i0, int i1, int i2, int i3) {\n  TFLITE_DCHECK_EQ(shape.DimensionsCount(), 4);\n  const int* dims_data = reinterpret_cast<const int*>(shape.DimsDataUpTo5D());\n  TFLITE_DCHECK(i0 >= 0 && i0 < dims_data[0]);\n  TFLITE_DCHECK(i1 >= 0 && i1 < dims_data[1]);\n  TFLITE_DCHECK(i2 >= 0 && i2 < dims_data[2]);\n  TFLITE_DCHECK(i3 >= 0 && i3 < dims_data[3]);\n  return ((i0 * dims_data[1] + i1) * dims_data[2] + i2) * dims_data[3] + i3;\n}\n\ninline int Offset(const Dims<4>& dims, int i0, int i1, int i2, int i3) {\n  TFLITE_DCHECK(i0 >= 0 && i0 < dims.sizes[0]);\n  TFLITE_DCHECK(i1 >= 0 && i1 < dims.sizes[1]);\n  TFLITE_DCHECK(i2 >= 0 && i2 < dims.sizes[2]);\n  TFLITE_DCHECK(i3 >= 0 && i3 < dims.sizes[3]);\n  return i0 * dims.strides[0] + i1 * dims.strides[1] + i2 * dims.strides[2] +\n         i3 * dims.strides[3];\n}\n\ninline int Offset(const Dims<4>& dims, int* index) {\n  return Offset(dims, index[0], index[1], index[2], index[3]);\n}\n\ninline int Offset(const RuntimeShape& shape, int* index) {\n  return Offset(shape, index[0], index[1], index[2], index[3]);\n}\n\n// Get array size, DCHECKing that the dim index is in range.\n//\n// Note that this will be phased out with Dims<4>, since RuntimeShape::Dims()\n// already performs this check.\ntemplate <int N>\nint ArraySize(const Dims<N>& array, int index) {\n  TFLITE_DCHECK(index >= 0 && index < N);\n  return array.sizes[index];\n}\n\n// Get common array size, DCHECKing that they all agree.\ntemplate <typename ArrayType1, typename ArrayType2>\nint MatchingArraySize(const ArrayType1& array1, int index1,\n                      const ArrayType2& array2, int index2) {\n  TFLITE_DCHECK_EQ(ArraySize(array1, index1), ArraySize(array2, index2));\n  return ArraySize(array1, index1);\n}\n\ntemplate <typename ArrayType1, typename ArrayType2, typename... Args>\nint MatchingArraySize(const ArrayType1& array1, int index1,\n                      const ArrayType2& array2, int index2, Args... args) {\n  TFLITE_DCHECK_EQ(ArraySize(array1, index1), ArraySize(array2, index2));\n  return MatchingArraySize(array1, index1, args...);\n}\n\n// Get common shape dim, DCHECKing that they all agree.\ninline int MatchingDim(const RuntimeShape& shape1, int index1,\n                       const RuntimeShape& shape2, int index2) {\n  TFLITE_DCHECK_EQ(shape1.Dims(index1), shape2.Dims(index2));\n  return shape1.Dims(index1);\n}\n\ntemplate <typename... Args>\nint MatchingDim(const RuntimeShape& shape1, int index1,\n                const RuntimeShape& shape2, int index2, Args... args) {\n  TFLITE_DCHECK_EQ(shape1.Dims(index1), shape2.Dims(index2));\n  return MatchingDim(shape1, index1, args...);\n}\n\n// Will be phased out with Dims<4>, replaced by RuntimeShape::FlatSize().\ntemplate <int N>\ninline int FlatSize(const Dims<N>& dims) {\n  int flat_size = 1;\n  for (int i = 0; i < N; ++i) {\n    flat_size *= dims.sizes[i];\n  }\n  return flat_size;\n}\n\nTFLITE_DEPRECATED(\"Prefer FlatSize.\")\ninline int RequiredBufferSizeForDims(const Dims<4>& dims) {\n  return FlatSize(dims);\n}\n\ninline int MatchingElementsSize(const RuntimeShape& shape,\n                                const RuntimeShape& check_shape_0) {\n  const int size_1 = shape.FlatSize();\n  const int size_2 = check_shape_0.FlatSize();\n  TFLITE_CHECK_EQ(size_1, size_2);\n  return size_1;\n}\n\ninline int MatchingElementsSize(const RuntimeShape& shape,\n                                const RuntimeShape& check_shape_0,\n                                const RuntimeShape& check_shape_1) {\n  const int size_1 = shape.FlatSize();\n  const int size_2 = check_shape_0.FlatSize();\n  const int size_3 = check_shape_1.FlatSize();\n  TFLITE_CHECK_EQ(size_1, size_2);\n  TFLITE_CHECK_EQ(size_2, size_3);\n  return size_1;\n}\n\n// Flat size calculation, checking that dimensions match with one or more other\n// arrays.\ninline int MatchingFlatSize(const RuntimeShape& shape,\n                            const RuntimeShape& check_shape_0) {\n  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());\n  const int dims_count = shape.DimensionsCount();\n  for (int i = 0; i < dims_count; ++i) {\n    TFLITE_DCHECK_EQ(shape.Dims(i), check_shape_0.Dims(i));\n  }\n  return shape.FlatSize();\n}\n\ninline int MatchingFlatSize(const RuntimeShape& shape,\n                            const RuntimeShape& check_shape_0,\n                            const RuntimeShape& check_shape_1) {\n  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());\n  const int dims_count = shape.DimensionsCount();\n  for (int i = 0; i < dims_count; ++i) {\n    TFLITE_DCHECK_EQ(shape.Dims(i), check_shape_0.Dims(i));\n  }\n  return MatchingFlatSize(shape, check_shape_1);\n}\n\ninline int MatchingFlatSize(const RuntimeShape& shape,\n                            const RuntimeShape& check_shape_0,\n                            const RuntimeShape& check_shape_1,\n                            const RuntimeShape& check_shape_2) {\n  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());\n  const int dims_count = shape.DimensionsCount();\n  for (int i = 0; i < dims_count; ++i) {\n    TFLITE_DCHECK_EQ(shape.Dims(i), check_shape_0.Dims(i));\n  }\n  return MatchingFlatSize(shape, check_shape_1, check_shape_2);\n}\n\ninline int MatchingFlatSize(const RuntimeShape& shape,\n                            const RuntimeShape& check_shape_0,\n                            const RuntimeShape& check_shape_1,\n                            const RuntimeShape& check_shape_2,\n                            const RuntimeShape& check_shape_3) {\n  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());\n  const int dims_count = shape.DimensionsCount();\n  for (int i = 0; i < dims_count; ++i) {\n    TFLITE_DCHECK_EQ(shape.Dims(i), check_shape_0.Dims(i));\n  }\n  return MatchingFlatSize(shape, check_shape_1, check_shape_2, check_shape_3);\n}\n\n// Flat size calculation, checking that dimensions match with one or more other\n// arrays.\ntemplate <int N>\ninline int MatchingFlatSize(const Dims<N>& dims, const Dims<N>& check_dims_0) {\n  for (int i = 0; i < N; ++i) {\n    TFLITE_DCHECK_EQ(ArraySize(dims, i), ArraySize(check_dims_0, i));\n  }\n  return FlatSize(dims);\n}\n\ntemplate <int N>\ninline int MatchingFlatSize(const Dims<N>& dims, const Dims<N>& check_dims_0,\n                            const Dims<N>& check_dims_1) {\n  for (int i = 0; i < N; ++i) {\n    TFLITE_DCHECK_EQ(ArraySize(dims, i), ArraySize(check_dims_0, i));\n  }\n  return MatchingFlatSize(dims, check_dims_1);\n}\n\ntemplate <int N>\ninline int MatchingFlatSize(const Dims<N>& dims, const Dims<N>& check_dims_0,\n                            const Dims<N>& check_dims_1,\n                            const Dims<N>& check_dims_2) {\n  for (int i = 0; i < N; ++i) {\n    TFLITE_DCHECK_EQ(ArraySize(dims, i), ArraySize(check_dims_0, i));\n  }\n  return MatchingFlatSize(dims, check_dims_1, check_dims_2);\n}\n\ntemplate <int N>\ninline int MatchingFlatSize(const Dims<N>& dims, const Dims<N>& check_dims_0,\n                            const Dims<N>& check_dims_1,\n                            const Dims<N>& check_dims_2,\n                            const Dims<N>& check_dims_3) {\n  for (int i = 0; i < N; ++i) {\n    TFLITE_DCHECK_EQ(ArraySize(dims, i), ArraySize(check_dims_0, i));\n  }\n  return MatchingFlatSize(dims, check_dims_1, check_dims_2, check_dims_3);\n}\n\n// Data is required to be contiguous, and so many operators can use either the\n// full array flat size or the flat size with one dimension skipped (commonly\n// the depth).\ntemplate <int N>\ninline int FlatSizeSkipDim(const Dims<N>& dims, int skip_dim) {\n  TFLITE_DCHECK(skip_dim >= 0 && skip_dim < N);\n  int flat_size = 1;\n  for (int i = 0; i < N; ++i) {\n    flat_size *= (i == skip_dim) ? 1 : dims.sizes[i];\n  }\n  return flat_size;\n}\n\n// A combination of MatchingFlatSize() and FlatSizeSkipDim().\ntemplate <int N>\ninline int MatchingFlatSizeSkipDim(const Dims<N>& dims, int skip_dim,\n                                   const Dims<N>& check_dims_0) {\n  for (int i = 0; i < N; ++i) {\n    if (i != skip_dim) {\n      TFLITE_DCHECK_EQ(ArraySize(dims, i), ArraySize(check_dims_0, i));\n    }\n  }\n  return FlatSizeSkipDim(dims, skip_dim);\n}\n\ntemplate <int N>\ninline int MatchingFlatSizeSkipDim(const Dims<N>& dims, int skip_dim,\n                                   const Dims<N>& check_dims_0,\n                                   const Dims<N>& check_dims_1) {\n  for (int i = 0; i < N; ++i) {\n    if (i != skip_dim) {\n      TFLITE_DCHECK_EQ(ArraySize(dims, i), ArraySize(check_dims_0, i));\n    }\n  }\n  return MatchingFlatSizeSkipDim(dims, skip_dim, check_dims_1);\n}\n\ntemplate <int N>\ninline int MatchingFlatSizeSkipDim(const Dims<N>& dims, int skip_dim,\n                                   const Dims<N>& check_dims_0,\n                                   const Dims<N>& check_dims_1,\n                                   const Dims<N>& check_dims_2) {\n  for (int i = 0; i < N; ++i) {\n    if (i != skip_dim) {\n      TFLITE_DCHECK_EQ(ArraySize(dims, i), ArraySize(check_dims_0, i));\n    }\n  }\n  return MatchingFlatSizeSkipDim(dims, skip_dim, check_dims_1, check_dims_2);\n}\n\ntemplate <int N>\ninline int MatchingFlatSizeSkipDim(const Dims<N>& dims, int skip_dim,\n                                   const Dims<N>& check_dims_0,\n                                   const Dims<N>& check_dims_1,\n                                   const Dims<N>& check_dims_2,\n                                   const Dims<N>& check_dims_3) {\n  for (int i = 0; i < N; ++i) {\n    if (i != skip_dim) {\n      TFLITE_DCHECK_EQ(ArraySize(dims, i), ArraySize(check_dims_0, i));\n    }\n  }\n  return MatchingFlatSizeSkipDim(dims, skip_dim, check_dims_1, check_dims_2,\n                                 check_dims_3);\n}\n\n// Data is required to be contiguous, and so many operators can use either the\n// full array flat size or the flat size with one dimension skipped (commonly\n// the depth).\ninline int FlatSizeSkipDim(const RuntimeShape& shape, int skip_dim) {\n  const int dims_count = shape.DimensionsCount();\n  TFLITE_DCHECK(skip_dim >= 0 && skip_dim < dims_count);\n  const auto* dims_data = shape.DimsData();\n  int flat_size = 1;\n  for (int i = 0; i < dims_count; ++i) {\n    flat_size *= (i == skip_dim) ? 1 : dims_data[i];\n  }\n  return flat_size;\n}\n\n// A combination of MatchingFlatSize() and FlatSizeSkipDim().\ninline int MatchingFlatSizeSkipDim(const RuntimeShape& shape, int skip_dim,\n                                   const RuntimeShape& check_shape_0) {\n  const int dims_count = shape.DimensionsCount();\n  for (int i = 0; i < dims_count; ++i) {\n    if (i != skip_dim) {\n      TFLITE_DCHECK_EQ(shape.Dims(i), check_shape_0.Dims(i));\n    }\n  }\n  return FlatSizeSkipDim(shape, skip_dim);\n}\n\ninline int MatchingFlatSizeSkipDim(const RuntimeShape& shape, int skip_dim,\n                                   const RuntimeShape& check_shape_0,\n                                   const RuntimeShape& check_shape_1) {\n  const int dims_count = shape.DimensionsCount();\n  for (int i = 0; i < dims_count; ++i) {\n    if (i != skip_dim) {\n      TFLITE_DCHECK_EQ(shape.Dims(i), check_shape_0.Dims(i));\n    }\n  }\n  return MatchingFlatSizeSkipDim(shape, skip_dim, check_shape_1);\n}\n\ninline int MatchingFlatSizeSkipDim(const RuntimeShape& shape, int skip_dim,\n                                   const RuntimeShape& check_shape_0,\n                                   const RuntimeShape& check_shape_1,\n                                   const RuntimeShape& check_shape_2) {\n  const int dims_count = shape.DimensionsCount();\n  for (int i = 0; i < dims_count; ++i) {\n    if (i != skip_dim) {\n      TFLITE_DCHECK_EQ(shape.Dims(i), check_shape_0.Dims(i));\n    }\n  }\n  return MatchingFlatSizeSkipDim(shape, skip_dim, check_shape_1, check_shape_2);\n}\n\ninline int MatchingFlatSizeSkipDim(const RuntimeShape& shape, int skip_dim,\n                                   const RuntimeShape& check_shape_0,\n                                   const RuntimeShape& check_shape_1,\n                                   const RuntimeShape& check_shape_2,\n                                   const RuntimeShape& check_shape_3) {\n  const int dims_count = shape.DimensionsCount();\n  for (int i = 0; i < dims_count; ++i) {\n    if (i != skip_dim) {\n      TFLITE_DCHECK_EQ(shape.Dims(i), check_shape_0.Dims(i));\n    }\n  }\n  return MatchingFlatSizeSkipDim(shape, skip_dim, check_shape_1, check_shape_2,\n                                 check_shape_3);\n}\n\ntemplate <int N>\nbool IsPackedWithoutStrides(const Dims<N>& dims) {\n  int expected_stride = 1;\n  for (int d = 0; d < N; d++) {\n    if (dims.strides[d] != expected_stride) return false;\n    expected_stride *= dims.sizes[d];\n  }\n  return true;\n}\n\ntemplate <int N>\nvoid ComputeStrides(Dims<N>* dims) {\n  dims->strides[0] = 1;\n  for (int d = 1; d < N; d++) {\n    dims->strides[d] = dims->strides[d - 1] * dims->sizes[d - 1];\n  }\n}\n\nenum class BroadcastableOpCategory : uint8_t {\n  kNone,\n  kNonBroadcast,               // Matching input shapes.\n  kFirstInputBroadcastsFast,   // Fivefold nested loops.\n  kSecondInputBroadcastsFast,  // Fivefold nested loops.\n  kGenericBroadcast,           // Fall-back.\n};\n\nstruct MinMax {\n  float min;\n  float max;\n};\nstatic_assert(sizeof(MinMax) == 8, \"\");\n\nstruct ActivationParams {\n  FusedActivationFunctionType activation_type;\n  // uint8_t, etc, activation params.\n  int32_t quantized_activation_min;\n  int32_t quantized_activation_max;\n};\n\nstruct ReluParams : public ActivationParams {\n  int32_t input_offset;\n  int32_t output_offset;\n  int32_t output_multiplier;\n  int output_shift;\n};\n\n// Styles of resizing op usages. For example, kImageStyle can be used with a Pad\n// op for pattern-specific optimization.\nenum class ResizingCategory : uint8_t {\n  kNone,\n  kImageStyle,  // 4D, operating on inner dimensions, say {0, a, b, 0}.\n  kGenericResize,\n};\n\n// For Add, Sub, Mul ops.\nstruct ArithmeticParams {\n  // Shape dependent / common to data / op types.\n  BroadcastableOpCategory broadcast_category;\n  // uint8_t inference params.\n  int32_t input1_offset;\n  int32_t input2_offset;\n  int32_t output_offset;\n  int32_t output_multiplier;\n  int output_shift;\n  // Add / Sub, not Mul, uint8_t inference params.\n  int left_shift;\n  int32_t input1_multiplier;\n  int input1_shift;\n  int32_t input2_multiplier;\n  int input2_shift;\n\n  // TODO(b/158622529): Union the following activation params.\n  // uint8_t, etc, activation params.\n  int32_t quantized_activation_min;\n  int32_t quantized_activation_max;\n  // float activation params.\n  float float_activation_min;\n  float float_activation_max;\n  // int64_t activation params.\n  int64_t int64_activation_min;\n  int64_t int64_activation_max;\n\n  // Processed output dimensions.\n  // Let input \"a\" be the one that broadcasts in the faster-changing dimension.\n  // Then, after coalescing, for shapes {a0, a1, a2, a3, a4} and\n  // {b0, b1, b2, b3, b4},\n  // broadcast_shape[4] = b0 = a0.\n  // broadcast_shape[3] = b1; a1 = 1.\n  // broadcast_shape[2] = b2 = a2.\n  // broadcast_shape[1] = a3; b3 = 1.\n  // broadcast_shape[0] = b4 = a4.\n  int broadcast_shape[5];\n};\n\nstruct ConcatenationParams {\n  int8_t axis;\n  const int32_t* input_zeropoint;\n  const float* input_scale;\n  uint16_t inputs_count;\n  int32_t output_zeropoint;\n  float output_scale;\n};\n\nstruct ComparisonParams {\n  // uint8_t inference params.\n  int left_shift;\n  int32_t input1_offset;\n  int32_t input1_multiplier;\n  int input1_shift;\n  int32_t input2_offset;\n  int32_t input2_multiplier;\n  int input2_shift;\n  // Shape dependent / common to inference types.\n  bool is_broadcast;\n};\n\nstruct ConvParams {\n  PaddingType padding_type;\n  PaddingValues padding_values;\n  // TODO(starka): This was just \"stride\", so check that width+height is OK.\n  int16_t stride_width;\n  int16_t stride_height;\n  int16_t dilation_width_factor;\n  int16_t dilation_height_factor;\n  // uint8_t inference params.\n  // TODO(b/65838351): Use smaller types if appropriate.\n  int32_t input_offset;\n  int32_t weights_offset;\n  int32_t output_offset;\n  int32_t output_multiplier;\n  int output_shift;\n  // uint8_t, etc, activation params.\n  int32_t quantized_activation_min;\n  int32_t quantized_activation_max;\n  // float activation params.\n  float float_activation_min;\n  float float_activation_max;\n};\n\nstruct DepthToSpaceParams {\n  int32_t block_size;\n};\n\nstruct DepthwiseParams {\n  PaddingType padding_type;\n  PaddingValues padding_values;\n  int16_t stride_width;\n  int16_t stride_height;\n  int16_t dilation_width_factor;\n  int16_t dilation_height_factor;\n  int16_t depth_multiplier;\n  // uint8_t inference params.\n  // TODO(b/65838351): Use smaller types if appropriate.\n  int32_t input_offset;\n  int32_t weights_offset;\n  int32_t output_offset;\n  int32_t output_multiplier;\n  int output_shift;\n  // uint8_t, etc, activation params.\n  int32_t quantized_activation_min;\n  int32_t quantized_activation_max;\n  // float activation params.\n  float float_activation_min;\n  float float_activation_max;\n  const int32_t* output_multiplier_per_channel;\n  const int32_t* output_shift_per_channel;\n};\n\nstruct DequantizationParams {\n  double scale;\n  int32_t zero_point;\n};\n\nstruct PerChannelDequantizationParams {\n  const float* scale;\n  const int32_t* zero_point;\n  int32_t quantized_dimension;\n};\n\nstruct FakeQuantParams {\n  MinMax minmax;\n  int32_t num_bits;\n};\n\nstruct FullyConnectedParams {\n  // uint8_t inference params.\n  // TODO(b/65838351): Use smaller types if appropriate.\n  int32_t input_offset;\n  int32_t weights_offset;\n  int32_t output_offset;\n  int32_t output_multiplier;\n  int output_shift;\n  // uint8_t, etc, activation params.\n  int32_t quantized_activation_min;\n  int32_t quantized_activation_max;\n  // float activation params.\n  float float_activation_min;\n  float float_activation_max;\n  // Mark the operands as cacheable if they are unchanging, e.g. weights.\n  bool lhs_cacheable;\n  bool rhs_cacheable;\n  FullyConnectedWeightsFormat weights_format;\n};\n\nstruct GatherParams {\n  int16_t axis;\n};\n\nstruct L2NormalizationParams {\n  // uint8_t inference params.\n  int32_t input_zero_point;\n};\n\nstruct LocalResponseNormalizationParams {\n  int32_t range;\n  double bias;\n  double alpha;\n  double beta;\n};\n\nstruct HardSwishParams {\n  // zero_point of the input activations.\n  int16_t input_zero_point;\n  // zero_point of the output activations.\n  int16_t output_zero_point;\n  // 16bit fixed-point component of the multiplier to apply to go from the\n  // \"high-res input scale\", which is the input scale multiplied by 2^7, to the\n  // \"relu-ish scale\", which 3.0/32768.\n  // See the implementation of HardSwishPrepare.\n  int16_t reluish_multiplier_fixedpoint_int16;\n  // exponent/bit-shift component of the aforementioned multiplier.\n  int reluish_multiplier_exponent;\n  // 16bit fixed-point component of the multiplier to apply to go from the\n  // \"high-res input scale\", which is the input scale multiplied by 2^7, to the\n  // output scale.\n  // See the implementation of HardSwishPrepare.\n  int16_t output_multiplier_fixedpoint_int16;\n  // exponent/bit-shift component of the aforementioned multiplier.\n  int output_multiplier_exponent;\n};\n\nstruct LogisticParams {\n  // uint8_t inference params.\n  int32_t input_zero_point;\n  int32_t input_range_radius;\n  int32_t input_multiplier;\n  int input_left_shift;\n};\n\nstruct LstmCellParams {\n  int32_t weights_zero_point;\n  int32_t accum_multiplier;\n  int accum_shift;\n  int state_integer_bits;\n};\n\nstruct MeanParams {\n  int8_t axis_count;\n  int16_t axis[4];\n};\n\nstruct PackParams {\n  int8_t axis;\n  const int32_t* input_zeropoint;\n  const float* input_scale;\n  uint16_t inputs_count;\n  int32_t output_zeropoint;\n  float output_scale;\n};\n\nstruct PadParams {\n  int8_t left_padding_count;\n  int32_t left_padding[4];\n  int8_t right_padding_count;\n  int32_t right_padding[4];\n  ResizingCategory resizing_category;\n};\n\nstruct PreluParams {\n  int32_t input_offset;\n  int32_t alpha_offset;\n  int32_t output_offset;\n  int32_t output_multiplier_1;\n  int output_shift_1;\n  int32_t output_multiplier_2;\n  int output_shift_2;\n};\n\nstruct PoolParams {\n  FusedActivationFunctionType activation;\n  PaddingType padding_type;\n  PaddingValues padding_values;\n  int stride_height;\n  int stride_width;\n  int filter_height;\n  int filter_width;\n  // uint8_t, etc, activation params.\n  int32_t quantized_activation_min;\n  int32_t quantized_activation_max;\n  // float activation params.\n  float float_activation_min;\n  float float_activation_max;\n};\n\nstruct ReshapeParams {\n  int8_t shape_count;\n  int32_t shape[4];\n};\n\nstruct ResizeBilinearParams {\n  bool align_corners;\n  // half_pixel_centers assumes pixels are of half the actual dimensions, and\n  // yields more accurate resizes. Corresponds to the same argument for the\n  // original TensorFlow op in TF2.0.\n  bool half_pixel_centers;\n};\n\nstruct ResizeNearestNeighborParams {\n  bool align_corners;\n  bool half_pixel_centers;\n};\n\nstruct SliceParams {\n  int8_t begin_count;\n  int32_t begin[4];\n  int8_t size_count;\n  int32_t size[4];\n};\n\nstruct SoftmaxParams {\n  // beta is not really used (not a Tensorflow parameter) and not implemented\n  // for LogSoftmax.\n  double beta;\n  // uint8_t inference params.  Used even when beta defaults to 1.0.\n  int32_t input_multiplier;\n  int32_t input_left_shift;\n  // Reverse scaling is only used by LogSoftmax.\n  int32_t reverse_scaling_divisor;\n  int32_t reverse_scaling_right_shift;\n  int diff_min;\n  int32_t zero_point;\n  float scale;\n  float* table;\n  int16_t* exp_lut;\n  int16_t* one_over_one_plus_x_lut;\n  uint8_t* uint8_table1;\n  uint8_t* uint8_table2;\n};\n\nstruct SpaceToBatchParams {\n  // \"Zero\" padding for uint8_t means padding with the output offset.\n  int32_t output_offset;\n};\n\nstruct SpaceToDepthParams {\n  int32_t block_size;\n};\n\nstruct SplitParams {\n  // Graphs that split into, say, 2000 nodes are encountered.  The indices in\n  // OperatorEdges are of type uint16_t.\n  uint16_t num_split;\n  int16_t axis;\n};\n\nstruct SqueezeParams {\n  int8_t squeeze_dims_count;\n  int32_t squeeze_dims[4];\n};\n\nstruct StridedSliceParams {\n  int8_t start_indices_count;\n  int32_t start_indices[5];\n  int8_t stop_indices_count;\n  int32_t stop_indices[5];\n  int8_t strides_count;\n  int32_t strides[5];\n\n  int16_t begin_mask;\n  int16_t ellipsis_mask;\n  int16_t end_mask;\n  int16_t new_axis_mask;\n  int16_t shrink_axis_mask;\n};\n\nstruct TanhParams {\n  int32_t input_zero_point;\n  int32_t input_range_radius;\n  int32_t input_multiplier;\n  int input_left_shift;\n};\n\nstruct TransposeParams {\n  int8_t perm_count;\n  int32_t perm[5];\n};\n\nstruct UnpackParams {\n  uint16_t num_split;\n  int16_t axis;\n};\n\nstruct LeakyReluParams {\n  float alpha;\n  int32_t input_offset;\n  int32_t output_offset;\n  int32_t output_multiplier_alpha;\n  int32_t output_shift_alpha;\n  int32_t output_multiplier_identity;\n  int32_t output_shift_identity;\n};\n\ntemplate <typename P>\ninline void SetActivationParams(float min, float max, P* params) {\n  params->float_activation_min = min;\n  params->float_activation_max = max;\n}\n\ntemplate <typename P>\ninline void SetActivationParams(int32_t min, int32_t max, P* params) {\n  params->quantized_activation_min = min;\n  params->quantized_activation_max = max;\n}\n\ntemplate <typename P>\ninline void SetActivationParams(int64_t min, int64_t max, P* params) {\n  params->int64_activation_min = min;\n  params->int64_activation_max = max;\n}\n\ntemplate <typename P>\ninline void GetActivationParams(const P& params, int32_t* min, int32_t* max) {\n  *min = params.quantized_activation_min;\n  *max = params.quantized_activation_max;\n}\n\ntemplate <typename P>\ninline void GetActivationParams(const P& params, float* min, float* max) {\n  *min = params.float_activation_min;\n  *max = params.float_activation_max;\n}\n\ntemplate <typename P>\ninline void GetActivationParams(const P& params, int64_t* min, int64_t* max) {\n  *min = params.int64_activation_min;\n  *max = params.int64_activation_max;\n}\n}  // namespace tflite\n\n#endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_TYPES_H_\n"], "fixing_code": ["/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n#ifndef TENSORFLOW_LITE_KERNELS_INTERNAL_TYPES_H_\n#define TENSORFLOW_LITE_KERNELS_INTERNAL_TYPES_H_\n\n#include <algorithm>\n#include <cstdint>\n#include <cstring>\n#include <initializer_list>\n\n#include \"tensorflow/lite/kernels/internal/compatibility.h\"\n\nnamespace tflite {\n\nenum class FusedActivationFunctionType : uint8_t {\n  kNone,\n  kRelu6,\n  kRelu1,\n  kRelu\n};\nenum class PaddingType : uint8_t { kNone, kSame, kValid };\n\nstruct PaddingValues {\n  int16_t width;\n  int16_t height;\n  // offset is used for calculating \"remaining\" padding, for example, `width`\n  // is 1 and `width_offset` is 1, so padding_left is 1 while padding_right is\n  // 1 + 1 = 2.\n  int16_t width_offset;\n  // Same as width_offset except it's over the height dimension.\n  int16_t height_offset;\n};\n\n// This enumeration allows for non-default formats for the weights array\n// of a fully-connected operator, allowing the use of special optimized\n// runtime paths.\nenum class FullyConnectedWeightsFormat : uint8_t {\n  // Default format (flat 2D layout, the inner contiguous dimension\n  // is input_depth, the outer non-contiguous dimension is output_depth)\n  kDefault,\n  // Summary: optimized layout for fast CPU runtime implementation,\n  // aimed specifically at ARM CPUs at the moment, and specialized for\n  // 8-bit quantized layers.\n  //\n  // The use case we're concerned with here is: 8-bit quantization,\n  // large weights matrix that doesn't fit in cache (e.g. 4096x2048 in\n  // a key application that drove this), very small batch size (e.g. 1 -- 4).\n  //\n  // Even with 8-bit quantization of weights, the performance of memory\n  // accesses to the weights can become the dominant issue when\n  // the batch size is small, so each weight value is used in only a few\n  // arithmetic ops, i.e. the fully-connected node has a low arithmetic\n  // intensity. The specific issues that arise are of three kinds:\n  // (1) One may, ideally, max out DRAM bandwidth, i.e. be truly memory\n  //     bound. That's the \"good\" issue to run into.\n  // (2) One may run into sub-optimal pre-fetching: the data hasn't been\n  //     prefetched into the cache by the time we need it.\n  // (3) One may run into cache aliasing: multiple values that are\n  //     pre-fetched, alias each other in the L1 cache (which typically\n  //     has only 4-way set associativity in ARM CPUs) and thus evict\n  //     each other before we get to using them.\n  //\n  // The point of this shuffling is to avoid issues (2) and (3) so that\n  // we get as fast as possible given only the hard constraint (1).\n  // This is achieved by turning the difficulty into a solution: the\n  // difficulty, that each value loaded from memory is used only in\n  // one kernel iteration, making this operation memory-intensive, hints at\n  // the solution, of shuffling the weights so that they are stored in the\n  // exact order as the kernel needs to load them, so that the memory\n  // accesses made by the kernel are trivial. This solves (2) because the\n  // trivial memory access pattern allows the CPU's automatic prefetching\n  // to perform very well (no need even for preload instructions), and this\n  // solves (3) because the values being loaded concurrently are now\n  // contiguous in the address space, thus don't alias each other in the cache.\n  //\n  // On ARM, we typically want our kernel to process a 4x16 block of weights\n  // at a time, because:\n  //   - 16 is the number of bytes in a NEON register.\n  //   - 4 is how many rows we need to handle concurrently in the kernel in\n  //     order to have sufficient mutual independence of instructions to\n  //     maximize arithmetic throughput.\n  //\n  // Finally, the 'Int8' part in the name refers to the fact that this\n  // weights format has each weights value encoded as a signed int8_t value,\n  // even if the data type of the weights buffer is uint8_t.  This is intended\n  // to save runtime kernels the effort to have to XOR the top bit of these\n  // bytes before using them in signed arithmetic, see this file for more\n  // explanations on the 'signed int8_t trick' in matrix multiplication kernels:\n  //\n  //   tensorflow/lite/toco/graph_transformations/ensure_uint8_weights_safe_for_fast_int8_kernels.cc\n  //\n  kShuffled4x16Int8,\n};\n\n// Quantization parameters, determining the mapping of quantized values\n// to real values (i.e. determining how quantized values are mathematically\n// interpreted).\n//\n// The correspondence is as follows:\n//\n//   real_value = scale * (quantized_value - zero_point);\n//\n// In other words, zero_point designates which quantized value corresponds to\n// the real 0 value, and scale designates the difference between the real values\n// corresponding to consecutive quantized values differing by 1.\nstruct QuantizationParams {\n  int32_t zero_point = 0;\n  double scale = 0.0;\n};\n\ninline bool operator==(const QuantizationParams& qp1,\n                       const QuantizationParams& qp2) {\n  return qp1.zero_point == qp2.zero_point && qp1.scale == qp2.scale;\n}\n\ntemplate <int N>\nstruct Dims {\n  int sizes[N];\n  int strides[N];\n};\n\nclass RuntimeShape {\n public:\n  // Shapes with dimensions up to 5 are stored directly in the structure, while\n  // larger shapes are separately allocated.\n  static constexpr int kMaxSmallSize = 5;\n\n  RuntimeShape& operator=(RuntimeShape const&) = delete;\n\n  RuntimeShape() : size_(0) {}\n\n  explicit RuntimeShape(int dimensions_count) : size_(dimensions_count) {\n    if (dimensions_count > kMaxSmallSize) {\n#ifdef TF_LITE_STATIC_MEMORY\n      TFLITE_CHECK(false && \"No shape resizing supported on this platform\");\n#else  // TF_LITE_STATIC_MEMORY\n      dims_pointer_ = new int32_t[dimensions_count];\n#endif  // TF_LITE_STATIC_MEMORY\n    }\n  }\n\n  RuntimeShape(int shape_size, int32_t value) : size_(0) {\n    Resize(shape_size);\n    for (int i = 0; i < shape_size; ++i) {\n      SetDim(i, value);\n    }\n  }\n\n  RuntimeShape(int dimensions_count, const int32_t* dims_data) : size_(0) {\n    ReplaceWith(dimensions_count, dims_data);\n  }\n\n  RuntimeShape(const std::initializer_list<int> init_list) : size_(0) {\n    BuildFrom(init_list);\n  }\n\n  // Avoid using this constructor.  We should be able to delete it when C++17\n  // rolls out.\n  RuntimeShape(RuntimeShape const& other) : size_(other.DimensionsCount()) {\n    if (size_ > kMaxSmallSize) {\n      dims_pointer_ = new int32_t[size_];\n    }\n    std::memcpy(DimsData(), other.DimsData(), sizeof(int32_t) * size_);\n  }\n\n  bool operator==(const RuntimeShape& comp) const {\n    return this->size_ == comp.size_ &&\n           std::memcmp(DimsData(), comp.DimsData(), size_ * sizeof(int32_t)) ==\n               0;\n  }\n\n  ~RuntimeShape() {\n    if (size_ > kMaxSmallSize) {\n#ifdef TF_LITE_STATIC_MEMORY\n      TFLITE_CHECK(false && \"No shape resizing supported on this platform\");\n#else  // TF_LITE_STATIC_MEMORY\n      delete[] dims_pointer_;\n#endif  // TF_LITE_STATIC_MEMORY\n    }\n  }\n\n  inline int32_t DimensionsCount() const { return size_; }\n  inline int32_t Dims(int i) const {\n    TFLITE_DCHECK_GE(i, 0);\n    TFLITE_DCHECK_LT(i, size_);\n    return size_ > kMaxSmallSize ? dims_pointer_[i] : dims_[i];\n  }\n  inline void SetDim(int i, int32_t val) {\n    TFLITE_DCHECK_GE(i, 0);\n    TFLITE_DCHECK_LT(i, size_);\n    if (size_ > kMaxSmallSize) {\n      dims_pointer_[i] = val;\n    } else {\n      dims_[i] = val;\n    }\n  }\n\n  inline int32_t* DimsData() {\n    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;\n  }\n  inline const int32_t* DimsData() const {\n    return size_ > kMaxSmallSize ? dims_pointer_ : dims_;\n  }\n  // The caller must ensure that the shape is no bigger than 5-D.\n  inline const int32_t* DimsDataUpTo5D() const { return dims_; }\n\n  inline void Resize(int dimensions_count) {\n    if (size_ > kMaxSmallSize) {\n#ifdef TF_LITE_STATIC_MEMORY\n      TFLITE_CHECK(false && \"No shape resizing supported on this platform\");\n#else  // TF_LITE_STATIC_MEMORY\n      delete[] dims_pointer_;\n#endif  // TF_LITE_STATIC_MEMORY\n    }\n    size_ = dimensions_count;\n    if (dimensions_count > kMaxSmallSize) {\n#ifdef TF_LITE_STATIC_MEMORY\n      TFLITE_CHECK(false && \"No shape resizing supported on this platform\");\n#else  // TF_LITE_STATIC_MEMORY\n      dims_pointer_ = new int32_t[dimensions_count];\n#endif  // TF_LITE_STATIC_MEMORY\n    }\n  }\n\n  inline void ReplaceWith(int dimensions_count, const int32_t* dims_data) {\n    Resize(dimensions_count);\n    int32_t* dst_dims = DimsData();\n    std::memcpy(dst_dims, dims_data, dimensions_count * sizeof(int32_t));\n  }\n\n  template <typename T>\n  inline void BuildFrom(const T& src_iterable) {\n    const int dimensions_count =\n        std::distance(src_iterable.begin(), src_iterable.end());\n    Resize(dimensions_count);\n    int32_t* data = DimsData();\n    for (auto it : src_iterable) {\n      *data = it;\n      ++data;\n    }\n  }\n\n  // This will probably be factored out. Old code made substantial use of 4-D\n  // shapes, and so this function is used to extend smaller shapes. Note that\n  // (a) as Dims<4>-dependent code is eliminated, the reliance on this should be\n  // reduced, and (b) some kernels are stricly 4-D, but then the shapes of their\n  // inputs should already be 4-D, so this function should not be needed.\n  inline static RuntimeShape ExtendedShape(int new_shape_size,\n                                           const RuntimeShape& shape) {\n    return RuntimeShape(new_shape_size, shape, 1);\n  }\n\n  inline void BuildFrom(const std::initializer_list<int> init_list) {\n    BuildFrom<const std::initializer_list<int>>(init_list);\n  }\n\n  // Returns the total count of elements, that is the size when flattened into a\n  // vector.\n  inline int FlatSize() const {\n    int buffer_size = 1;\n    const int* dims_data = reinterpret_cast<const int*>(DimsData());\n    for (int i = 0; i < size_; i++) {\n      buffer_size *= dims_data[i];\n    }\n    return buffer_size;\n  }\n\n  bool operator!=(const RuntimeShape& comp) const { return !((*this) == comp); }\n\n private:\n  // For use only by ExtendedShape(), written to guarantee (return-value) copy\n  // elision in C++17.\n  // This creates a shape padded to the desired size with the specified value.\n  RuntimeShape(int new_shape_size, const RuntimeShape& shape, int pad_value)\n      : size_(0) {\n    // If the following check fails, it is likely because a 4D-only kernel is\n    // being used with an array of larger dimension count.\n    TFLITE_CHECK_GE(new_shape_size, shape.DimensionsCount());\n    Resize(new_shape_size);\n    const int size_increase = new_shape_size - shape.DimensionsCount();\n    for (int i = 0; i < size_increase; ++i) {\n      SetDim(i, pad_value);\n    }\n    std::memcpy(DimsData() + size_increase, shape.DimsData(),\n                sizeof(int32_t) * shape.DimensionsCount());\n  }\n\n  int32_t size_;\n  union {\n    int32_t dims_[kMaxSmallSize];\n    int32_t* dims_pointer_;\n  };\n};\n\n// Converts inference-style shape to legacy tflite::Dims<4>.\ninline tflite::Dims<4> ToRuntimeDims(const tflite::RuntimeShape& array_shape) {\n  tflite::Dims<4> result;\n  const int dimensions_count = array_shape.DimensionsCount();\n  TFLITE_CHECK_LE(dimensions_count, 4);\n  int cum_prod = 1;\n  for (int i = 0; i < 4; i++) {\n    const int new_dim =\n        (i < dimensions_count) ? array_shape.Dims(dimensions_count - 1 - i) : 1;\n    result.sizes[i] = new_dim;\n    result.strides[i] = cum_prod;\n    cum_prod *= new_dim;\n  }\n  return result;\n}\n\n// TODO(b/80418076): Move to legacy ops file, update invocations.\ninline RuntimeShape DimsToShape(const tflite::Dims<4>& dims) {\n  return RuntimeShape(\n      {dims.sizes[3], dims.sizes[2], dims.sizes[1], dims.sizes[0]});\n}\n\n// Gets next index to iterate through a multidimensional array.\ninline bool NextIndex(const int num_dims, const int* dims, int* current) {\n  if (num_dims == 0) {\n    return false;\n  }\n  TFLITE_DCHECK(dims != nullptr);\n  TFLITE_DCHECK(current != nullptr);\n  int carry = 1;\n  for (int idx = num_dims - 1; idx >= 0; --idx) {\n    int current_val = current[idx] + carry;\n    TFLITE_DCHECK_GE(dims[idx], current_val);\n    if (dims[idx] == current_val) {\n      current[idx] = 0;\n    } else {\n      current[idx] = current_val;\n      carry = 0;\n      break;\n    }\n  }\n  return (carry == 0);\n}\n\n// Gets offset of index if reducing on axis. When reducing, the flattened offset\n// will not change, if the input index changes on the given axis. For example,\n// if you have a 3D tensor and you are reducing to 2D by eliminating axis 0,\n// then index (0, 1, 2) and index (1, 1, 2) will map to the same flattened\n// offset.\n// TODO(kanlig): uses Dims to represent dimensions.\ninline size_t ReducedOutputOffset(const int num_dims, const int* dims,\n                                  const int* index, const int num_axis,\n                                  const int* axis) {\n  if (num_dims == 0) {\n    return 0;\n  }\n  TFLITE_DCHECK(dims != nullptr);\n  TFLITE_DCHECK(index != nullptr);\n  size_t offset = 0;\n  for (int idx = 0; idx < num_dims; ++idx) {\n    // if we need to skip this axis\n    bool is_axis = false;\n    if (axis != nullptr) {\n      for (int axis_idx = 0; axis_idx < num_axis; ++axis_idx) {\n        if (idx == axis[axis_idx]) {\n          is_axis = true;\n          break;\n        }\n      }\n    }\n    if (!is_axis) {\n      offset = offset * static_cast<size_t>(dims[idx]) +\n               static_cast<size_t>(index[idx]);\n    }\n  }\n  return offset;\n}\n\ninline int Offset(const RuntimeShape& shape, int i0, int i1, int i2, int i3) {\n  TFLITE_DCHECK_EQ(shape.DimensionsCount(), 4);\n  const int* dims_data = reinterpret_cast<const int*>(shape.DimsDataUpTo5D());\n  TFLITE_DCHECK(i0 >= 0 && i0 < dims_data[0]);\n  TFLITE_DCHECK(i1 >= 0 && i1 < dims_data[1]);\n  TFLITE_DCHECK(i2 >= 0 && i2 < dims_data[2]);\n  TFLITE_DCHECK(i3 >= 0 && i3 < dims_data[3]);\n  return ((i0 * dims_data[1] + i1) * dims_data[2] + i2) * dims_data[3] + i3;\n}\n\ninline int Offset(const Dims<4>& dims, int i0, int i1, int i2, int i3) {\n  TFLITE_DCHECK(i0 >= 0 && i0 < dims.sizes[0]);\n  TFLITE_DCHECK(i1 >= 0 && i1 < dims.sizes[1]);\n  TFLITE_DCHECK(i2 >= 0 && i2 < dims.sizes[2]);\n  TFLITE_DCHECK(i3 >= 0 && i3 < dims.sizes[3]);\n  return i0 * dims.strides[0] + i1 * dims.strides[1] + i2 * dims.strides[2] +\n         i3 * dims.strides[3];\n}\n\ninline int Offset(const Dims<4>& dims, int* index) {\n  return Offset(dims, index[0], index[1], index[2], index[3]);\n}\n\ninline int Offset(const RuntimeShape& shape, int* index) {\n  return Offset(shape, index[0], index[1], index[2], index[3]);\n}\n\n// Get array size, DCHECKing that the dim index is in range.\n//\n// Note that this will be phased out with Dims<4>, since RuntimeShape::Dims()\n// already performs this check.\ntemplate <int N>\nint ArraySize(const Dims<N>& array, int index) {\n  TFLITE_DCHECK(index >= 0 && index < N);\n  return array.sizes[index];\n}\n\n// Get common array size, DCHECKing that they all agree.\ntemplate <typename ArrayType1, typename ArrayType2>\nint MatchingArraySize(const ArrayType1& array1, int index1,\n                      const ArrayType2& array2, int index2) {\n  TFLITE_DCHECK_EQ(ArraySize(array1, index1), ArraySize(array2, index2));\n  return ArraySize(array1, index1);\n}\n\ntemplate <typename ArrayType1, typename ArrayType2, typename... Args>\nint MatchingArraySize(const ArrayType1& array1, int index1,\n                      const ArrayType2& array2, int index2, Args... args) {\n  TFLITE_DCHECK_EQ(ArraySize(array1, index1), ArraySize(array2, index2));\n  return MatchingArraySize(array1, index1, args...);\n}\n\n// Get common shape dim, DCHECKing that they all agree.\ninline int MatchingDim(const RuntimeShape& shape1, int index1,\n                       const RuntimeShape& shape2, int index2) {\n  TFLITE_DCHECK_EQ(shape1.Dims(index1), shape2.Dims(index2));\n  return std::min(shape1.Dims(index1), shape2.Dims(index2));\n}\n\ntemplate <typename... Args>\nint MatchingDim(const RuntimeShape& shape1, int index1,\n                const RuntimeShape& shape2, int index2, Args... args) {\n  TFLITE_DCHECK_EQ(shape1.Dims(index1), shape2.Dims(index2));\n  return MatchingDim(shape1, index1, args...);\n}\n\n// Will be phased out with Dims<4>, replaced by RuntimeShape::FlatSize().\ntemplate <int N>\ninline int FlatSize(const Dims<N>& dims) {\n  int flat_size = 1;\n  for (int i = 0; i < N; ++i) {\n    flat_size *= dims.sizes[i];\n  }\n  return flat_size;\n}\n\nTFLITE_DEPRECATED(\"Prefer FlatSize.\")\ninline int RequiredBufferSizeForDims(const Dims<4>& dims) {\n  return FlatSize(dims);\n}\n\ninline int MatchingElementsSize(const RuntimeShape& shape,\n                                const RuntimeShape& check_shape_0) {\n  const int size_1 = shape.FlatSize();\n  const int size_2 = check_shape_0.FlatSize();\n  TFLITE_CHECK_EQ(size_1, size_2);\n  return size_1;\n}\n\ninline int MatchingElementsSize(const RuntimeShape& shape,\n                                const RuntimeShape& check_shape_0,\n                                const RuntimeShape& check_shape_1) {\n  const int size_1 = shape.FlatSize();\n  const int size_2 = check_shape_0.FlatSize();\n  const int size_3 = check_shape_1.FlatSize();\n  TFLITE_CHECK_EQ(size_1, size_2);\n  TFLITE_CHECK_EQ(size_2, size_3);\n  return size_1;\n}\n\n// Flat size calculation, checking that dimensions match with one or more other\n// arrays.\ninline int MatchingFlatSize(const RuntimeShape& shape,\n                            const RuntimeShape& check_shape_0) {\n  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());\n  const int dims_count = shape.DimensionsCount();\n  for (int i = 0; i < dims_count; ++i) {\n    TFLITE_DCHECK_EQ(shape.Dims(i), check_shape_0.Dims(i));\n  }\n  return shape.FlatSize();\n}\n\ninline int MatchingFlatSize(const RuntimeShape& shape,\n                            const RuntimeShape& check_shape_0,\n                            const RuntimeShape& check_shape_1) {\n  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());\n  const int dims_count = shape.DimensionsCount();\n  for (int i = 0; i < dims_count; ++i) {\n    TFLITE_DCHECK_EQ(shape.Dims(i), check_shape_0.Dims(i));\n  }\n  return MatchingFlatSize(shape, check_shape_1);\n}\n\ninline int MatchingFlatSize(const RuntimeShape& shape,\n                            const RuntimeShape& check_shape_0,\n                            const RuntimeShape& check_shape_1,\n                            const RuntimeShape& check_shape_2) {\n  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());\n  const int dims_count = shape.DimensionsCount();\n  for (int i = 0; i < dims_count; ++i) {\n    TFLITE_DCHECK_EQ(shape.Dims(i), check_shape_0.Dims(i));\n  }\n  return MatchingFlatSize(shape, check_shape_1, check_shape_2);\n}\n\ninline int MatchingFlatSize(const RuntimeShape& shape,\n                            const RuntimeShape& check_shape_0,\n                            const RuntimeShape& check_shape_1,\n                            const RuntimeShape& check_shape_2,\n                            const RuntimeShape& check_shape_3) {\n  TFLITE_DCHECK_EQ(shape.DimensionsCount(), check_shape_0.DimensionsCount());\n  const int dims_count = shape.DimensionsCount();\n  for (int i = 0; i < dims_count; ++i) {\n    TFLITE_DCHECK_EQ(shape.Dims(i), check_shape_0.Dims(i));\n  }\n  return MatchingFlatSize(shape, check_shape_1, check_shape_2, check_shape_3);\n}\n\n// Flat size calculation, checking that dimensions match with one or more other\n// arrays.\ntemplate <int N>\ninline int MatchingFlatSize(const Dims<N>& dims, const Dims<N>& check_dims_0) {\n  for (int i = 0; i < N; ++i) {\n    TFLITE_DCHECK_EQ(ArraySize(dims, i), ArraySize(check_dims_0, i));\n  }\n  return FlatSize(dims);\n}\n\ntemplate <int N>\ninline int MatchingFlatSize(const Dims<N>& dims, const Dims<N>& check_dims_0,\n                            const Dims<N>& check_dims_1) {\n  for (int i = 0; i < N; ++i) {\n    TFLITE_DCHECK_EQ(ArraySize(dims, i), ArraySize(check_dims_0, i));\n  }\n  return MatchingFlatSize(dims, check_dims_1);\n}\n\ntemplate <int N>\ninline int MatchingFlatSize(const Dims<N>& dims, const Dims<N>& check_dims_0,\n                            const Dims<N>& check_dims_1,\n                            const Dims<N>& check_dims_2) {\n  for (int i = 0; i < N; ++i) {\n    TFLITE_DCHECK_EQ(ArraySize(dims, i), ArraySize(check_dims_0, i));\n  }\n  return MatchingFlatSize(dims, check_dims_1, check_dims_2);\n}\n\ntemplate <int N>\ninline int MatchingFlatSize(const Dims<N>& dims, const Dims<N>& check_dims_0,\n                            const Dims<N>& check_dims_1,\n                            const Dims<N>& check_dims_2,\n                            const Dims<N>& check_dims_3) {\n  for (int i = 0; i < N; ++i) {\n    TFLITE_DCHECK_EQ(ArraySize(dims, i), ArraySize(check_dims_0, i));\n  }\n  return MatchingFlatSize(dims, check_dims_1, check_dims_2, check_dims_3);\n}\n\n// Data is required to be contiguous, and so many operators can use either the\n// full array flat size or the flat size with one dimension skipped (commonly\n// the depth).\ntemplate <int N>\ninline int FlatSizeSkipDim(const Dims<N>& dims, int skip_dim) {\n  TFLITE_DCHECK(skip_dim >= 0 && skip_dim < N);\n  int flat_size = 1;\n  for (int i = 0; i < N; ++i) {\n    flat_size *= (i == skip_dim) ? 1 : dims.sizes[i];\n  }\n  return flat_size;\n}\n\n// A combination of MatchingFlatSize() and FlatSizeSkipDim().\ntemplate <int N>\ninline int MatchingFlatSizeSkipDim(const Dims<N>& dims, int skip_dim,\n                                   const Dims<N>& check_dims_0) {\n  for (int i = 0; i < N; ++i) {\n    if (i != skip_dim) {\n      TFLITE_DCHECK_EQ(ArraySize(dims, i), ArraySize(check_dims_0, i));\n    }\n  }\n  return FlatSizeSkipDim(dims, skip_dim);\n}\n\ntemplate <int N>\ninline int MatchingFlatSizeSkipDim(const Dims<N>& dims, int skip_dim,\n                                   const Dims<N>& check_dims_0,\n                                   const Dims<N>& check_dims_1) {\n  for (int i = 0; i < N; ++i) {\n    if (i != skip_dim) {\n      TFLITE_DCHECK_EQ(ArraySize(dims, i), ArraySize(check_dims_0, i));\n    }\n  }\n  return MatchingFlatSizeSkipDim(dims, skip_dim, check_dims_1);\n}\n\ntemplate <int N>\ninline int MatchingFlatSizeSkipDim(const Dims<N>& dims, int skip_dim,\n                                   const Dims<N>& check_dims_0,\n                                   const Dims<N>& check_dims_1,\n                                   const Dims<N>& check_dims_2) {\n  for (int i = 0; i < N; ++i) {\n    if (i != skip_dim) {\n      TFLITE_DCHECK_EQ(ArraySize(dims, i), ArraySize(check_dims_0, i));\n    }\n  }\n  return MatchingFlatSizeSkipDim(dims, skip_dim, check_dims_1, check_dims_2);\n}\n\ntemplate <int N>\ninline int MatchingFlatSizeSkipDim(const Dims<N>& dims, int skip_dim,\n                                   const Dims<N>& check_dims_0,\n                                   const Dims<N>& check_dims_1,\n                                   const Dims<N>& check_dims_2,\n                                   const Dims<N>& check_dims_3) {\n  for (int i = 0; i < N; ++i) {\n    if (i != skip_dim) {\n      TFLITE_DCHECK_EQ(ArraySize(dims, i), ArraySize(check_dims_0, i));\n    }\n  }\n  return MatchingFlatSizeSkipDim(dims, skip_dim, check_dims_1, check_dims_2,\n                                 check_dims_3);\n}\n\n// Data is required to be contiguous, and so many operators can use either the\n// full array flat size or the flat size with one dimension skipped (commonly\n// the depth).\ninline int FlatSizeSkipDim(const RuntimeShape& shape, int skip_dim) {\n  const int dims_count = shape.DimensionsCount();\n  TFLITE_DCHECK(skip_dim >= 0 && skip_dim < dims_count);\n  const auto* dims_data = shape.DimsData();\n  int flat_size = 1;\n  for (int i = 0; i < dims_count; ++i) {\n    flat_size *= (i == skip_dim) ? 1 : dims_data[i];\n  }\n  return flat_size;\n}\n\n// A combination of MatchingFlatSize() and FlatSizeSkipDim().\ninline int MatchingFlatSizeSkipDim(const RuntimeShape& shape, int skip_dim,\n                                   const RuntimeShape& check_shape_0) {\n  const int dims_count = shape.DimensionsCount();\n  for (int i = 0; i < dims_count; ++i) {\n    if (i != skip_dim) {\n      TFLITE_DCHECK_EQ(shape.Dims(i), check_shape_0.Dims(i));\n    }\n  }\n  return FlatSizeSkipDim(shape, skip_dim);\n}\n\ninline int MatchingFlatSizeSkipDim(const RuntimeShape& shape, int skip_dim,\n                                   const RuntimeShape& check_shape_0,\n                                   const RuntimeShape& check_shape_1) {\n  const int dims_count = shape.DimensionsCount();\n  for (int i = 0; i < dims_count; ++i) {\n    if (i != skip_dim) {\n      TFLITE_DCHECK_EQ(shape.Dims(i), check_shape_0.Dims(i));\n    }\n  }\n  return MatchingFlatSizeSkipDim(shape, skip_dim, check_shape_1);\n}\n\ninline int MatchingFlatSizeSkipDim(const RuntimeShape& shape, int skip_dim,\n                                   const RuntimeShape& check_shape_0,\n                                   const RuntimeShape& check_shape_1,\n                                   const RuntimeShape& check_shape_2) {\n  const int dims_count = shape.DimensionsCount();\n  for (int i = 0; i < dims_count; ++i) {\n    if (i != skip_dim) {\n      TFLITE_DCHECK_EQ(shape.Dims(i), check_shape_0.Dims(i));\n    }\n  }\n  return MatchingFlatSizeSkipDim(shape, skip_dim, check_shape_1, check_shape_2);\n}\n\ninline int MatchingFlatSizeSkipDim(const RuntimeShape& shape, int skip_dim,\n                                   const RuntimeShape& check_shape_0,\n                                   const RuntimeShape& check_shape_1,\n                                   const RuntimeShape& check_shape_2,\n                                   const RuntimeShape& check_shape_3) {\n  const int dims_count = shape.DimensionsCount();\n  for (int i = 0; i < dims_count; ++i) {\n    if (i != skip_dim) {\n      TFLITE_DCHECK_EQ(shape.Dims(i), check_shape_0.Dims(i));\n    }\n  }\n  return MatchingFlatSizeSkipDim(shape, skip_dim, check_shape_1, check_shape_2,\n                                 check_shape_3);\n}\n\ntemplate <int N>\nbool IsPackedWithoutStrides(const Dims<N>& dims) {\n  int expected_stride = 1;\n  for (int d = 0; d < N; d++) {\n    if (dims.strides[d] != expected_stride) return false;\n    expected_stride *= dims.sizes[d];\n  }\n  return true;\n}\n\ntemplate <int N>\nvoid ComputeStrides(Dims<N>* dims) {\n  dims->strides[0] = 1;\n  for (int d = 1; d < N; d++) {\n    dims->strides[d] = dims->strides[d - 1] * dims->sizes[d - 1];\n  }\n}\n\nenum class BroadcastableOpCategory : uint8_t {\n  kNone,\n  kNonBroadcast,               // Matching input shapes.\n  kFirstInputBroadcastsFast,   // Fivefold nested loops.\n  kSecondInputBroadcastsFast,  // Fivefold nested loops.\n  kGenericBroadcast,           // Fall-back.\n};\n\nstruct MinMax {\n  float min;\n  float max;\n};\nstatic_assert(sizeof(MinMax) == 8, \"\");\n\nstruct ActivationParams {\n  FusedActivationFunctionType activation_type;\n  // uint8_t, etc, activation params.\n  int32_t quantized_activation_min;\n  int32_t quantized_activation_max;\n};\n\nstruct ReluParams : public ActivationParams {\n  int32_t input_offset;\n  int32_t output_offset;\n  int32_t output_multiplier;\n  int output_shift;\n};\n\n// Styles of resizing op usages. For example, kImageStyle can be used with a Pad\n// op for pattern-specific optimization.\nenum class ResizingCategory : uint8_t {\n  kNone,\n  kImageStyle,  // 4D, operating on inner dimensions, say {0, a, b, 0}.\n  kGenericResize,\n};\n\n// For Add, Sub, Mul ops.\nstruct ArithmeticParams {\n  // Shape dependent / common to data / op types.\n  BroadcastableOpCategory broadcast_category;\n  // uint8_t inference params.\n  int32_t input1_offset;\n  int32_t input2_offset;\n  int32_t output_offset;\n  int32_t output_multiplier;\n  int output_shift;\n  // Add / Sub, not Mul, uint8_t inference params.\n  int left_shift;\n  int32_t input1_multiplier;\n  int input1_shift;\n  int32_t input2_multiplier;\n  int input2_shift;\n\n  // TODO(b/158622529): Union the following activation params.\n  // uint8_t, etc, activation params.\n  int32_t quantized_activation_min;\n  int32_t quantized_activation_max;\n  // float activation params.\n  float float_activation_min;\n  float float_activation_max;\n  // int64_t activation params.\n  int64_t int64_activation_min;\n  int64_t int64_activation_max;\n\n  // Processed output dimensions.\n  // Let input \"a\" be the one that broadcasts in the faster-changing dimension.\n  // Then, after coalescing, for shapes {a0, a1, a2, a3, a4} and\n  // {b0, b1, b2, b3, b4},\n  // broadcast_shape[4] = b0 = a0.\n  // broadcast_shape[3] = b1; a1 = 1.\n  // broadcast_shape[2] = b2 = a2.\n  // broadcast_shape[1] = a3; b3 = 1.\n  // broadcast_shape[0] = b4 = a4.\n  int broadcast_shape[5];\n};\n\nstruct ConcatenationParams {\n  int8_t axis;\n  const int32_t* input_zeropoint;\n  const float* input_scale;\n  uint16_t inputs_count;\n  int32_t output_zeropoint;\n  float output_scale;\n};\n\nstruct ComparisonParams {\n  // uint8_t inference params.\n  int left_shift;\n  int32_t input1_offset;\n  int32_t input1_multiplier;\n  int input1_shift;\n  int32_t input2_offset;\n  int32_t input2_multiplier;\n  int input2_shift;\n  // Shape dependent / common to inference types.\n  bool is_broadcast;\n};\n\nstruct ConvParams {\n  PaddingType padding_type;\n  PaddingValues padding_values;\n  // TODO(starka): This was just \"stride\", so check that width+height is OK.\n  int16_t stride_width;\n  int16_t stride_height;\n  int16_t dilation_width_factor;\n  int16_t dilation_height_factor;\n  // uint8_t inference params.\n  // TODO(b/65838351): Use smaller types if appropriate.\n  int32_t input_offset;\n  int32_t weights_offset;\n  int32_t output_offset;\n  int32_t output_multiplier;\n  int output_shift;\n  // uint8_t, etc, activation params.\n  int32_t quantized_activation_min;\n  int32_t quantized_activation_max;\n  // float activation params.\n  float float_activation_min;\n  float float_activation_max;\n};\n\nstruct DepthToSpaceParams {\n  int32_t block_size;\n};\n\nstruct DepthwiseParams {\n  PaddingType padding_type;\n  PaddingValues padding_values;\n  int16_t stride_width;\n  int16_t stride_height;\n  int16_t dilation_width_factor;\n  int16_t dilation_height_factor;\n  int16_t depth_multiplier;\n  // uint8_t inference params.\n  // TODO(b/65838351): Use smaller types if appropriate.\n  int32_t input_offset;\n  int32_t weights_offset;\n  int32_t output_offset;\n  int32_t output_multiplier;\n  int output_shift;\n  // uint8_t, etc, activation params.\n  int32_t quantized_activation_min;\n  int32_t quantized_activation_max;\n  // float activation params.\n  float float_activation_min;\n  float float_activation_max;\n  const int32_t* output_multiplier_per_channel;\n  const int32_t* output_shift_per_channel;\n};\n\nstruct DequantizationParams {\n  double scale;\n  int32_t zero_point;\n};\n\nstruct PerChannelDequantizationParams {\n  const float* scale;\n  const int32_t* zero_point;\n  int32_t quantized_dimension;\n};\n\nstruct FakeQuantParams {\n  MinMax minmax;\n  int32_t num_bits;\n};\n\nstruct FullyConnectedParams {\n  // uint8_t inference params.\n  // TODO(b/65838351): Use smaller types if appropriate.\n  int32_t input_offset;\n  int32_t weights_offset;\n  int32_t output_offset;\n  int32_t output_multiplier;\n  int output_shift;\n  // uint8_t, etc, activation params.\n  int32_t quantized_activation_min;\n  int32_t quantized_activation_max;\n  // float activation params.\n  float float_activation_min;\n  float float_activation_max;\n  // Mark the operands as cacheable if they are unchanging, e.g. weights.\n  bool lhs_cacheable;\n  bool rhs_cacheable;\n  FullyConnectedWeightsFormat weights_format;\n};\n\nstruct GatherParams {\n  int16_t axis;\n};\n\nstruct L2NormalizationParams {\n  // uint8_t inference params.\n  int32_t input_zero_point;\n};\n\nstruct LocalResponseNormalizationParams {\n  int32_t range;\n  double bias;\n  double alpha;\n  double beta;\n};\n\nstruct HardSwishParams {\n  // zero_point of the input activations.\n  int16_t input_zero_point;\n  // zero_point of the output activations.\n  int16_t output_zero_point;\n  // 16bit fixed-point component of the multiplier to apply to go from the\n  // \"high-res input scale\", which is the input scale multiplied by 2^7, to the\n  // \"relu-ish scale\", which 3.0/32768.\n  // See the implementation of HardSwishPrepare.\n  int16_t reluish_multiplier_fixedpoint_int16;\n  // exponent/bit-shift component of the aforementioned multiplier.\n  int reluish_multiplier_exponent;\n  // 16bit fixed-point component of the multiplier to apply to go from the\n  // \"high-res input scale\", which is the input scale multiplied by 2^7, to the\n  // output scale.\n  // See the implementation of HardSwishPrepare.\n  int16_t output_multiplier_fixedpoint_int16;\n  // exponent/bit-shift component of the aforementioned multiplier.\n  int output_multiplier_exponent;\n};\n\nstruct LogisticParams {\n  // uint8_t inference params.\n  int32_t input_zero_point;\n  int32_t input_range_radius;\n  int32_t input_multiplier;\n  int input_left_shift;\n};\n\nstruct LstmCellParams {\n  int32_t weights_zero_point;\n  int32_t accum_multiplier;\n  int accum_shift;\n  int state_integer_bits;\n};\n\nstruct MeanParams {\n  int8_t axis_count;\n  int16_t axis[4];\n};\n\nstruct PackParams {\n  int8_t axis;\n  const int32_t* input_zeropoint;\n  const float* input_scale;\n  uint16_t inputs_count;\n  int32_t output_zeropoint;\n  float output_scale;\n};\n\nstruct PadParams {\n  int8_t left_padding_count;\n  int32_t left_padding[4];\n  int8_t right_padding_count;\n  int32_t right_padding[4];\n  ResizingCategory resizing_category;\n};\n\nstruct PreluParams {\n  int32_t input_offset;\n  int32_t alpha_offset;\n  int32_t output_offset;\n  int32_t output_multiplier_1;\n  int output_shift_1;\n  int32_t output_multiplier_2;\n  int output_shift_2;\n};\n\nstruct PoolParams {\n  FusedActivationFunctionType activation;\n  PaddingType padding_type;\n  PaddingValues padding_values;\n  int stride_height;\n  int stride_width;\n  int filter_height;\n  int filter_width;\n  // uint8_t, etc, activation params.\n  int32_t quantized_activation_min;\n  int32_t quantized_activation_max;\n  // float activation params.\n  float float_activation_min;\n  float float_activation_max;\n};\n\nstruct ReshapeParams {\n  int8_t shape_count;\n  int32_t shape[4];\n};\n\nstruct ResizeBilinearParams {\n  bool align_corners;\n  // half_pixel_centers assumes pixels are of half the actual dimensions, and\n  // yields more accurate resizes. Corresponds to the same argument for the\n  // original TensorFlow op in TF2.0.\n  bool half_pixel_centers;\n};\n\nstruct ResizeNearestNeighborParams {\n  bool align_corners;\n  bool half_pixel_centers;\n};\n\nstruct SliceParams {\n  int8_t begin_count;\n  int32_t begin[4];\n  int8_t size_count;\n  int32_t size[4];\n};\n\nstruct SoftmaxParams {\n  // beta is not really used (not a Tensorflow parameter) and not implemented\n  // for LogSoftmax.\n  double beta;\n  // uint8_t inference params.  Used even when beta defaults to 1.0.\n  int32_t input_multiplier;\n  int32_t input_left_shift;\n  // Reverse scaling is only used by LogSoftmax.\n  int32_t reverse_scaling_divisor;\n  int32_t reverse_scaling_right_shift;\n  int diff_min;\n  int32_t zero_point;\n  float scale;\n  float* table;\n  int16_t* exp_lut;\n  int16_t* one_over_one_plus_x_lut;\n  uint8_t* uint8_table1;\n  uint8_t* uint8_table2;\n};\n\nstruct SpaceToBatchParams {\n  // \"Zero\" padding for uint8_t means padding with the output offset.\n  int32_t output_offset;\n};\n\nstruct SpaceToDepthParams {\n  int32_t block_size;\n};\n\nstruct SplitParams {\n  // Graphs that split into, say, 2000 nodes are encountered.  The indices in\n  // OperatorEdges are of type uint16_t.\n  uint16_t num_split;\n  int16_t axis;\n};\n\nstruct SqueezeParams {\n  int8_t squeeze_dims_count;\n  int32_t squeeze_dims[4];\n};\n\nstruct StridedSliceParams {\n  int8_t start_indices_count;\n  int32_t start_indices[5];\n  int8_t stop_indices_count;\n  int32_t stop_indices[5];\n  int8_t strides_count;\n  int32_t strides[5];\n\n  int16_t begin_mask;\n  int16_t ellipsis_mask;\n  int16_t end_mask;\n  int16_t new_axis_mask;\n  int16_t shrink_axis_mask;\n};\n\nstruct TanhParams {\n  int32_t input_zero_point;\n  int32_t input_range_radius;\n  int32_t input_multiplier;\n  int input_left_shift;\n};\n\nstruct TransposeParams {\n  int8_t perm_count;\n  int32_t perm[5];\n};\n\nstruct UnpackParams {\n  uint16_t num_split;\n  int16_t axis;\n};\n\nstruct LeakyReluParams {\n  float alpha;\n  int32_t input_offset;\n  int32_t output_offset;\n  int32_t output_multiplier_alpha;\n  int32_t output_shift_alpha;\n  int32_t output_multiplier_identity;\n  int32_t output_shift_identity;\n};\n\ntemplate <typename P>\ninline void SetActivationParams(float min, float max, P* params) {\n  params->float_activation_min = min;\n  params->float_activation_max = max;\n}\n\ntemplate <typename P>\ninline void SetActivationParams(int32_t min, int32_t max, P* params) {\n  params->quantized_activation_min = min;\n  params->quantized_activation_max = max;\n}\n\ntemplate <typename P>\ninline void SetActivationParams(int64_t min, int64_t max, P* params) {\n  params->int64_activation_min = min;\n  params->int64_activation_max = max;\n}\n\ntemplate <typename P>\ninline void GetActivationParams(const P& params, int32_t* min, int32_t* max) {\n  *min = params.quantized_activation_min;\n  *max = params.quantized_activation_max;\n}\n\ntemplate <typename P>\ninline void GetActivationParams(const P& params, float* min, float* max) {\n  *min = params.float_activation_min;\n  *max = params.float_activation_max;\n}\n\ntemplate <typename P>\ninline void GetActivationParams(const P& params, int64_t* min, int64_t* max) {\n  *min = params.int64_activation_min;\n  *max = params.int64_activation_max;\n}\n}  // namespace tflite\n\n#endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_TYPES_H_\n"], "filenames": ["tensorflow/lite/kernels/internal/types.h"], "buggy_code_start_loc": [441], "buggy_code_end_loc": [442], "fixing_code_start_loc": [441], "fixing_code_end_loc": [442], "type": "CWE-125", "message": "In tensorflow-lite before versions 1.15.4, 2.0.3, 2.1.2, 2.2.1 and 2.3.1, when determining the common dimension size of two tensors, TFLite uses a `DCHECK` which is no-op outside of debug compilation modes. Since the function always returns the dimension of the first tensor, malicious attackers can craft cases where this is larger than that of the second tensor. In turn, this would result in reads/writes outside of bounds since the interpreter will wrongly assume that there is enough data in both tensors. The issue is patched in commit 8ee24e7949a203d234489f9da2c5bf45a7d5157d, and is released in TensorFlow versions 1.15.4, 2.0.3, 2.1.2, 2.2.1, or 2.3.1.", "other": {"cve": {"id": "CVE-2020-15208", "sourceIdentifier": "security-advisories@github.com", "published": "2020-09-25T19:15:16.103", "lastModified": "2021-09-16T15:45:33.860", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "In tensorflow-lite before versions 1.15.4, 2.0.3, 2.1.2, 2.2.1 and 2.3.1, when determining the common dimension size of two tensors, TFLite uses a `DCHECK` which is no-op outside of debug compilation modes. Since the function always returns the dimension of the first tensor, malicious attackers can craft cases where this is larger than that of the second tensor. In turn, this would result in reads/writes outside of bounds since the interpreter will wrongly assume that there is enough data in both tensors. The issue is patched in commit 8ee24e7949a203d234489f9da2c5bf45a7d5157d, and is released in TensorFlow versions 1.15.4, 2.0.3, 2.1.2, 2.2.1, or 2.3.1."}, {"lang": "es", "value": "En tensorflow-lite versiones anteriores a 1.15.4, 2.0.3, 2.1.2, 2.2.1 y 2.3.1, al determinar el tama\u00f1o de dimensi\u00f3n com\u00fan de dos tensores, TFLite usa un \"DCHECK\" que no es operativo fuera de los modos de compilaci\u00f3n de depuraci\u00f3n.&#xa0;Dado que la funci\u00f3n siempre devuelve la dimensi\u00f3n del primer tensor, los atacantes maliciosos pueden crear casos en los que este sea mayor que el del segundo tensor.&#xa0;A su vez, esto resultar\u00eda en lecturas y escrituras fuera de l\u00edmites, ya que el int\u00e9rprete asumir\u00e1 incorrectamente que existen suficientes datos en ambos tensores.&#xa0;El problema es parcheado en el commit 8ee24e7949a203d234489f9da2c5bf45a7d5157d, y es publicado en TensorFlow versiones 1.15.4, 2.0.3, 2.1.2, 2.2.1 o 2.3.1"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 9.8, "baseSeverity": "CRITICAL"}, "exploitabilityScore": 3.9, "impactScore": 5.9}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:H/PR:N/UI:N/S:U/C:H/I:H/A:N", "attackVector": "NETWORK", "attackComplexity": "HIGH", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "NONE", "baseScore": 7.4, "baseSeverity": "HIGH"}, "exploitabilityScore": 2.2, "impactScore": 5.2}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:L/Au:N/C:P/I:P/A:P", "accessVector": "NETWORK", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "PARTIAL", "availabilityImpact": "PARTIAL", "baseScore": 7.5}, "baseSeverity": "HIGH", "exploitabilityScore": 10.0, "impactScore": 6.4, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "security-advisories@github.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-125"}, {"lang": "en", "value": "CWE-787"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:lite:*:*:*", "versionEndExcluding": "1.15.4", "matchCriteriaId": "7A5421A9-693F-472A-9A21-43950C884C77"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:lite:*:*:*", "versionStartIncluding": "2.0.0", "versionEndExcluding": "2.0.3", "matchCriteriaId": "B0FEB74E-5E54-4A2F-910C-FA1812C73DB2"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:lite:*:*:*", "versionStartIncluding": "2.1.0", "versionEndExcluding": "2.1.2", "matchCriteriaId": "47D83682-6615-49BC-8043-F36B9D017578"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:lite:*:*:*", "versionStartIncluding": "2.2.0", "versionEndExcluding": "2.2.1", "matchCriteriaId": "323B716A-E8F7-4CDA-B8FD-A56977D59C02"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:lite:*:*:*", "versionStartIncluding": "2.3.0", "versionEndExcluding": "2.3.1", "matchCriteriaId": "C09502A8-B667-4867-BEBD-40333E98A601"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:opensuse:leap:15.2:*:*:*:*:*:*:*", "matchCriteriaId": "B009C22E-30A4-4288-BCF6-C3E81DEAF45A"}]}]}], "references": [{"url": "http://lists.opensuse.org/opensuse-security-announce/2020-10/msg00065.html", "source": "security-advisories@github.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/commit/8ee24e7949a203d234489f9da2c5bf45a7d5157d", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/releases/tag/v2.3.1", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/security/advisories/GHSA-mxjj-953w-2c2v", "source": "security-advisories@github.com", "tags": ["Exploit", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/tensorflow/tensorflow/commit/8ee24e7949a203d234489f9da2c5bf45a7d5157d"}}