{"buggy_code": ["/* Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include <atomic>\n\n#define EIGEN_USE_THREADS\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n#define EIGEN_USE_GPU\n#if GOOGLE_CUDA\n#include \"third_party/gpus/cudnn/cudnn.h\"\n#endif  // GOOGLE_CUDA\n\n#include \"tensorflow/core/kernels/conv_2d.h\"\n#include \"tensorflow/core/platform/stream_executor.h\"\n#include \"tensorflow/core/util/stream_executor_util.h\"\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/register_types.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/tensor_types.h\"\n#include \"tensorflow/core/kernels/fill_functor.h\"\n#include \"tensorflow/core/kernels/fused_batch_norm_op.h\"\n#include \"tensorflow/core/kernels/redux_functor.h\"\n#include \"tensorflow/core/kernels/transpose_functor.h\"\n#include \"tensorflow/core/lib/core/blocking_counter.h\"\n#include \"tensorflow/core/util/env_var.h\"\n#include \"tensorflow/core/util/tensor_format.h\"\n\nnamespace tensorflow {\nusing CPUDevice = Eigen::ThreadPoolDevice;\nusing GPUDevice = Eigen::GpuDevice;\n\nnamespace functor {\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\nusing se::DeviceMemory;\nusing se::ScratchAllocator;\nusing se::Stream;\nusing se::port::StatusOr;\n#endif\n\nstring ToString(FusedBatchNormActivationMode activation_mode) {\n  switch (activation_mode) {\n    case FusedBatchNormActivationMode::kIdentity:\n      return \"Identity\";\n    case FusedBatchNormActivationMode::kRelu:\n      return \"Relu\";\n  }\n}\n\nStatus ParseActivationMode(OpKernelConstruction* context,\n                           FusedBatchNormActivationMode* activation_mode) {\n  string activation_mode_str;\n  TF_RETURN_IF_ERROR(context->GetAttr(\"activation_mode\", &activation_mode_str));\n\n  if (activation_mode_str == \"Identity\") {\n    *activation_mode = FusedBatchNormActivationMode::kIdentity;\n    return Status::OK();\n  }\n  if (activation_mode_str == \"Relu\") {\n    *activation_mode = FusedBatchNormActivationMode::kRelu;\n    return Status::OK();\n  }\n  return errors::InvalidArgument(\"Unsupported activation mode: \",\n                                 activation_mode_str);\n}\n\n// Functor used by FusedBatchNormOp to do the computations.\ntemplate <typename Device, typename T, typename U, bool is_training>\nstruct FusedBatchNorm;\n// Functor used by FusedBatchNormGradOp to do the computations when\n// is_training=True.\ntemplate <typename Device, typename T, typename U>\nstruct FusedBatchNormGrad;\n\ntemplate <typename T, typename U>\nstruct FusedBatchNorm<CPUDevice, T, U, /* is_training= */ true> {\n  void operator()(OpKernelContext* context, const Tensor& x_input,\n                  const Tensor& scale_input, const Tensor& offset_input,\n                  const Tensor& running_mean_input,\n                  const Tensor& running_variance_input,\n                  const Tensor* side_input, U epsilon, U exponential_avg_factor,\n                  FusedBatchNormActivationMode activation_mode,\n                  Tensor* y_output, Tensor* running_mean_output,\n                  Tensor* running_var_output, Tensor* saved_batch_mean_output,\n                  Tensor* saved_batch_var_output, TensorFormat tensor_format,\n                  bool use_reserved_space) {\n    OP_REQUIRES(context, side_input == nullptr,\n                errors::Internal(\n                    \"The CPU implementation of FusedBatchNorm does not support \"\n                    \"side input.\"));\n    OP_REQUIRES(context,\n                activation_mode == FusedBatchNormActivationMode::kIdentity,\n                errors::Internal(\"The CPU implementation of FusedBatchNorm \"\n                                 \"does not support activations.\"));\n\n    if (use_reserved_space) {\n      Tensor* dummy_reserve_space = nullptr;\n      OP_REQUIRES_OK(context,\n                     context->allocate_output(5, {}, &dummy_reserve_space));\n      // Initialize the memory, to avoid sanitizer alerts.\n      dummy_reserve_space->flat<U>()(0) = U();\n    }\n    Tensor transformed_x;\n    Tensor transformed_y;\n    if (tensor_format == FORMAT_NCHW) {\n      const int64 in_batch = GetTensorDim(x_input, tensor_format, 'N');\n      const int64 in_rows = GetTensorDim(x_input, tensor_format, 'H');\n      const int64 in_cols = GetTensorDim(x_input, tensor_format, 'W');\n      const int64 in_depths = GetTensorDim(x_input, tensor_format, 'C');\n      OP_REQUIRES_OK(context, context->allocate_temp(\n                                  DataTypeToEnum<T>::value,\n                                  ShapeFromFormat(FORMAT_NHWC, in_batch,\n                                                  in_rows, in_cols, in_depths),\n                                  &transformed_x));\n      OP_REQUIRES_OK(context, context->allocate_temp(\n                                  DataTypeToEnum<T>::value,\n                                  ShapeFromFormat(FORMAT_NHWC, in_batch,\n                                                  in_rows, in_cols, in_depths),\n                                  &transformed_y));\n      // Perform NCHW to NHWC\n      std::vector<int32> perm = {0, 2, 3, 1};\n      OP_REQUIRES_OK(\n          context, ::tensorflow::DoTranspose(context->eigen_device<CPUDevice>(),\n                                             x_input, perm, &transformed_x));\n    } else {\n      transformed_x = x_input;\n      transformed_y = *y_output;\n    }\n    typename TTypes<T, 4>::Tensor x(transformed_x.tensor<T, 4>());\n    typename TTypes<U>::ConstVec scale(scale_input.vec<U>());\n    typename TTypes<U>::ConstVec offset(offset_input.vec<U>());\n    typename TTypes<U>::ConstVec old_mean(running_mean_input.vec<U>());\n    typename TTypes<U>::ConstVec old_variance(running_variance_input.vec<U>());\n    typename TTypes<T, 4>::Tensor y(transformed_y.tensor<T, 4>());\n    typename TTypes<U>::Vec new_mean(running_mean_output->vec<U>());\n    typename TTypes<U>::Vec new_variance(running_var_output->vec<U>());\n    typename TTypes<U>::Vec saved_batch_mean(saved_batch_mean_output->vec<U>());\n    typename TTypes<U>::Vec saved_batch_var(saved_batch_var_output->vec<U>());\n\n    const CPUDevice& d = context->eigen_device<CPUDevice>();\n\n    const int depth = x.dimension(3);\n    const int size = x.size();\n    const int rest_size = size / depth;\n    Eigen::DSizes<Eigen::Index, 2> rest_by_depth(rest_size, depth);\n\n#if !defined(EIGEN_HAS_INDEX_LIST)\n    Eigen::DSizes<Eigen::Index, 2> one_by_depth(1, depth);\n    Eigen::array<int, 1> reduce_dims({0});\n    Eigen::array<int, 2> bcast_spec({rest_size, 1});\n#else\n    Eigen::IndexList<Eigen::type2index<1>, Eigen::Index> one_by_depth;\n    one_by_depth.set(1, depth);\n    Eigen::IndexList<Eigen::type2index<0>> reduce_dims;\n    Eigen::IndexList<Eigen::Index, Eigen::type2index<1>> bcast_spec;\n    bcast_spec.set(0, rest_size);\n#endif\n\n    auto x_rest_by_depth = x.reshape(rest_by_depth).template cast<U>();\n    const int rest_size_minus_one = (rest_size > 1) ? (rest_size - 1) : 1;\n    U rest_size_inv = static_cast<U>(1.0f / static_cast<U>(rest_size));\n    // This adjustment is for Bessel's correction\n    U rest_size_adjust =\n        static_cast<U>(rest_size) / static_cast<U>(rest_size_minus_one);\n\n    Eigen::Tensor<U, 1, Eigen::RowMajor> batch_mean(depth);\n    Eigen::Tensor<U, 1, Eigen::RowMajor> batch_variance(depth);\n\n    batch_mean.device(d) = (x_rest_by_depth.sum(reduce_dims) * rest_size_inv);\n    auto x_centered = x_rest_by_depth -\n                      batch_mean.reshape(one_by_depth).broadcast(bcast_spec);\n\n    batch_variance.device(d) =\n        x_centered.square().sum(reduce_dims) * rest_size_inv;\n    auto scaling_factor = ((batch_variance + epsilon).rsqrt() * scale)\n                              .eval()\n                              .reshape(one_by_depth)\n                              .broadcast(bcast_spec);\n    auto x_scaled = x_centered * scaling_factor;\n    auto x_shifted =\n        (x_scaled + offset.reshape(one_by_depth).broadcast(bcast_spec))\n            .template cast<T>();\n\n    y.reshape(rest_by_depth).device(d) = x_shifted;\n    if (exponential_avg_factor == U(1.0)) {\n      saved_batch_var.device(d) = batch_variance;\n      saved_batch_mean.device(d) = batch_mean;\n      new_variance.device(d) = batch_variance * rest_size_adjust;\n      new_mean.device(d) = batch_mean;\n    } else {\n      U one_minus_factor = U(1) - exponential_avg_factor;\n      saved_batch_var.device(d) = batch_variance;\n      saved_batch_mean.device(d) = batch_mean;\n      new_variance.device(d) =\n          one_minus_factor * old_variance +\n          (exponential_avg_factor * rest_size_adjust) * batch_variance;\n      new_mean.device(d) =\n          one_minus_factor * old_mean + exponential_avg_factor * batch_mean;\n    }\n\n    if (tensor_format == FORMAT_NCHW) {\n      // Perform NHWC to NCHW\n      const std::vector<int32> perm = {0, 3, 1, 2};\n      const Status s = ::tensorflow::DoTranspose(\n          context->eigen_device<CPUDevice>(), transformed_y, perm, y_output);\n      if (!s.ok()) {\n        context->SetStatus(errors::InvalidArgument(\"Transpose failed: \", s));\n      }\n    }\n  }\n};\n\ntemplate <typename T, typename U>\nstruct FusedBatchNorm<CPUDevice, T, U, /* is_training= */ false> {\n  void operator()(OpKernelContext* context, const Tensor& x_input,\n                  const Tensor& scale_input, const Tensor& offset_input,\n                  const Tensor& estimated_mean_input,\n                  const Tensor& estimated_variance_input,\n                  const Tensor* side_input, U epsilon, U exponential_avg_factor,\n                  FusedBatchNormActivationMode activation_mode,\n                  Tensor* y_output, Tensor* batch_mean_output,\n                  Tensor* batch_var_output, Tensor* saved_mean_output,\n                  Tensor* saved_var_output, TensorFormat tensor_format,\n                  bool use_reserved_space) {\n    OP_REQUIRES(context, side_input == nullptr,\n                errors::Internal(\n                    \"The CPU implementation of FusedBatchNorm does not support \"\n                    \"side input.\"));\n    OP_REQUIRES(context,\n                activation_mode == FusedBatchNormActivationMode::kIdentity,\n                errors::Internal(\"The CPU implementation of FusedBatchNorm \"\n                                 \"does not support activations.\"));\n\n    if (use_reserved_space) {\n      Tensor* dummy_reserve_space = nullptr;\n      OP_REQUIRES_OK(context,\n                     context->allocate_output(5, {}, &dummy_reserve_space));\n      // Initialize the memory, to avoid sanitizer alerts.\n      dummy_reserve_space->flat<U>()(0) = U();\n    }\n    Tensor transformed_x;\n    Tensor transformed_y;\n    if (tensor_format == FORMAT_NCHW) {\n      const int64 in_batch = GetTensorDim(x_input, tensor_format, 'N');\n      const int64 in_rows = GetTensorDim(x_input, tensor_format, 'H');\n      const int64 in_cols = GetTensorDim(x_input, tensor_format, 'W');\n      const int64 in_depths = GetTensorDim(x_input, tensor_format, 'C');\n      OP_REQUIRES_OK(context, context->allocate_temp(\n                                  DataTypeToEnum<T>::value,\n                                  ShapeFromFormat(FORMAT_NHWC, in_batch,\n                                                  in_rows, in_cols, in_depths),\n                                  &transformed_x));\n      OP_REQUIRES_OK(context, context->allocate_temp(\n                                  DataTypeToEnum<T>::value,\n                                  ShapeFromFormat(FORMAT_NHWC, in_batch,\n                                                  in_rows, in_cols, in_depths),\n                                  &transformed_y));\n      // Perform NCHW to NHWC\n      std::vector<int32> perm = {0, 2, 3, 1};\n      OP_REQUIRES_OK(\n          context, ::tensorflow::DoTranspose(context->eigen_device<CPUDevice>(),\n                                             x_input, perm, &transformed_x));\n    } else {\n      transformed_x = x_input;\n      transformed_y = *y_output;\n    }\n    typename TTypes<T, 4>::Tensor x(transformed_x.tensor<T, 4>());\n    typename TTypes<U>::ConstVec scale(scale_input.vec<U>());\n    typename TTypes<U>::ConstVec offset(offset_input.vec<U>());\n    typename TTypes<U>::ConstVec estimated_mean(estimated_mean_input.vec<U>());\n    typename TTypes<U>::ConstVec estimated_variance(\n        estimated_variance_input.vec<U>());\n    typename TTypes<T, 4>::Tensor y(transformed_y.tensor<T, 4>());\n    typename TTypes<U>::Vec batch_mean(batch_mean_output->vec<U>());\n    typename TTypes<U>::Vec batch_variance(batch_var_output->vec<U>());\n\n    const CPUDevice& d = context->eigen_device<CPUDevice>();\n\n    const int depth = x.dimension(3);\n    OP_REQUIRES(\n        context, depth != 0,\n        errors::Internal(\"The 4th element in the input shape cannot be 0.\"));\n    const int size = x.size();\n    const int rest_size = size / depth;\n    Eigen::DSizes<Eigen::Index, 2> rest_by_depth(rest_size, depth);\n\n#if !defined(EIGEN_HAS_INDEX_LIST)\n    Eigen::DSizes<Eigen::Index, 2> one_by_depth(1, depth);\n    Eigen::array<int, 1> reduce_dims({0});\n    Eigen::array<int, 2> bcast_spec({rest_size, 1});\n#else\n    Eigen::IndexList<Eigen::type2index<1>, Eigen::Index> one_by_depth;\n    one_by_depth.set(1, depth);\n    Eigen::IndexList<Eigen::Index, Eigen::type2index<1>> bcast_spec;\n    bcast_spec.set(0, rest_size);\n#endif\n\n    auto x_rest_by_depth = x.reshape(rest_by_depth).template cast<U>();\n    auto x_centered =\n        x_rest_by_depth -\n        estimated_mean.reshape(one_by_depth).broadcast(bcast_spec);\n    auto scaling_factor = ((estimated_variance + epsilon).rsqrt() * scale)\n                              .eval()\n                              .reshape(one_by_depth)\n                              .broadcast(bcast_spec);\n    auto x_scaled = x_centered * scaling_factor;\n    auto x_shifted =\n        (x_scaled + offset.reshape(one_by_depth).broadcast(bcast_spec))\n            .template cast<T>();\n\n    y.reshape(rest_by_depth).device(d) = x_shifted;\n    batch_mean.device(d) = estimated_mean;\n    batch_variance.device(d) = estimated_variance;\n\n    if (tensor_format == FORMAT_NCHW) {\n      // Perform NHWC to NCHW\n      const std::vector<int32> perm = {0, 3, 1, 2};\n      const Status s = ::tensorflow::DoTranspose(\n          context->eigen_device<CPUDevice>(), transformed_y, perm, y_output);\n      if (!s.ok()) {\n        context->SetStatus(errors::InvalidArgument(\"Transpose failed: \", s));\n      }\n    }\n  }\n};\n\ntemplate <typename T, typename U>\nstruct FusedBatchNormGrad<CPUDevice, T, U> {\n  void operator()(OpKernelContext* context, const Tensor& y_backprop_input,\n                  const Tensor& x_input, const Tensor& scale_input,\n                  const Tensor& mean_input, const Tensor& variance_input,\n                  U epsilon, Tensor* x_backprop_output,\n                  Tensor* scale_backprop_output, Tensor* offset_backprop_output,\n                  bool use_reserved_space, TensorFormat tensor_format) {\n    Tensor transformed_y_backprop_input;\n    Tensor transformed_x_input;\n    Tensor transformed_x_backprop_output;\n    if (tensor_format == FORMAT_NCHW) {\n      const int64 in_batch = GetTensorDim(x_input, tensor_format, 'N');\n      const int64 in_rows = GetTensorDim(x_input, tensor_format, 'H');\n      const int64 in_cols = GetTensorDim(x_input, tensor_format, 'W');\n      const int64 in_depths = GetTensorDim(x_input, tensor_format, 'C');\n      OP_REQUIRES_OK(context, context->allocate_temp(\n                                  DataTypeToEnum<T>::value,\n                                  ShapeFromFormat(FORMAT_NHWC, in_batch,\n                                                  in_rows, in_cols, in_depths),\n                                  &transformed_y_backprop_input));\n      OP_REQUIRES_OK(context, context->allocate_temp(\n                                  DataTypeToEnum<T>::value,\n                                  ShapeFromFormat(FORMAT_NHWC, in_batch,\n                                                  in_rows, in_cols, in_depths),\n                                  &transformed_x_input));\n      OP_REQUIRES_OK(context, context->allocate_temp(\n                                  DataTypeToEnum<T>::value,\n                                  ShapeFromFormat(FORMAT_NHWC, in_batch,\n                                                  in_rows, in_cols, in_depths),\n                                  &transformed_x_backprop_output));\n      // Perform NCHW to NHWC\n      std::vector<int32> perm = {0, 2, 3, 1};\n      OP_REQUIRES_OK(\n          context, ::tensorflow::DoTranspose(context->eigen_device<CPUDevice>(),\n                                             y_backprop_input, perm,\n                                             &transformed_y_backprop_input));\n      OP_REQUIRES_OK(context, ::tensorflow::DoTranspose(\n                                  context->eigen_device<CPUDevice>(), x_input,\n                                  perm, &transformed_x_input));\n    } else {\n      transformed_y_backprop_input = y_backprop_input;\n      transformed_x_input = x_input;\n      transformed_x_backprop_output = *x_backprop_output;\n    }\n    typename TTypes<T, 4>::Tensor y_backprop(\n        transformed_y_backprop_input.tensor<T, 4>());\n    typename TTypes<T, 4>::Tensor x(transformed_x_input.tensor<T, 4>());\n    typename TTypes<U>::ConstVec scale(scale_input.vec<U>());\n    typename TTypes<U>::ConstVec mean(mean_input.vec<U>());\n    typename TTypes<U>::ConstVec variance(variance_input.vec<U>());\n    typename TTypes<T, 4>::Tensor x_backprop(\n        transformed_x_backprop_output.tensor<T, 4>());\n    typename TTypes<U>::Vec offset_backprop(offset_backprop_output->vec<U>());\n\n    // Note: the following formulas are used to compute the gradients for\n    // back propagation.\n    // x_backprop = scale * rsqrt(variance + epsilon) *\n    //              [y_backprop - mean(y_backprop) - (x - mean(x)) *\n    //              mean(y_backprop * (x - mean(x))) / (variance + epsilon)]\n    // scale_backprop = sum(y_backprop *\n    //                  (x - mean(x)) * rsqrt(variance + epsilon))\n    // offset_backprop = sum(y_backprop)\n\n    const CPUDevice& d = context->eigen_device<CPUDevice>();\n    const int depth = x.dimension(3);\n    const int size = x.size();\n    const int rest_size = size / depth;\n    Eigen::DSizes<Eigen::Index, 2> rest_by_depth(rest_size, depth);\n\n#if !defined(EIGEN_HAS_INDEX_LIST)\n    Eigen::DSizes<Eigen::Index, 2> one_by_depth(1, depth);\n    Eigen::array<int, 2> bcast_spec({rest_size, 1});\n#else\n    Eigen::IndexList<Eigen::type2index<1>, Eigen::Index> one_by_depth;\n    one_by_depth.set(1, depth);\n    Eigen::IndexList<Eigen::Index, Eigen::type2index<1>> bcast_spec;\n    bcast_spec.set(0, rest_size);\n#endif\n\n    auto x_rest_by_depth = x.reshape(rest_by_depth).template cast<U>();\n    U rest_size_inv = static_cast<U>(1.0f / static_cast<U>(rest_size));\n\n    // Eigen is notoriously bad at reducing outer dimension, so we materialize\n    // all temporary tensors that require reduction, and then use Eigen redux\n    // functor, that is optimized for this particular task.\n    //\n    // All reductions are of this type: [rest_size, depth] -> [depth].\n    using ScalarSum = Eigen::internal::scalar_sum_op<U>;\n    const functor::ReduceOuterDimensions<T, U, U, ScalarSum> redux_sum_t;\n    const functor::ReduceOuterDimensions<U, U, U, ScalarSum> redux_sum_u;\n\n    auto scratch_dtype = DataTypeToEnum<U>::value;\n\n    // Allocate a temporary workspace of [depth] shape.\n    Tensor scratch_one_by_depth;\n    OP_REQUIRES_OK(context, context->allocate_temp(scratch_dtype, {depth},\n                                                   &scratch_one_by_depth));\n\n    // Maybe allocate a temporary workspace of [rest_size, depth] shape.\n    Tensor scratch_rest_by_depth;\n    if (std::is_same<T, U>::value) {\n      OP_REQUIRES(context,\n                  scratch_rest_by_depth.CopyFrom(transformed_x_backprop_output,\n                                                 {rest_size, depth}),\n                  errors::Internal(\"Failed to copy a tensor\"));\n    } else {\n      OP_REQUIRES_OK(context,\n                     context->allocate_temp(scratch_dtype, {rest_size, depth},\n                                            &scratch_rest_by_depth));\n    }\n\n    typename TTypes<U, 2>::Tensor scratch_tensor(\n        scratch_rest_by_depth.tensor<U, 2>());\n    typename TTypes<U>::Vec scratch_vector(scratch_one_by_depth.vec<U>());\n\n    auto x_mean_rest_by_depth =\n        mean.reshape(one_by_depth).broadcast(bcast_spec);\n    auto x_centered = (x_rest_by_depth - x_mean_rest_by_depth);\n    auto coef0_one_by_depth =\n        (variance.reshape(one_by_depth) + epsilon).rsqrt();\n    auto coef0_rest_by_depth = coef0_one_by_depth.broadcast(bcast_spec);\n    auto x_scaled = x_centered * coef0_rest_by_depth;\n\n    auto y_backprop_rest_by_depth =\n        y_backprop.reshape(rest_by_depth).template cast<U>();\n\n    // Compute `scale_backprop_output`:\n    //   scale_backprop =\n    //     (y_backprop_rest_by_depth * x_scaled).sum(reduce_dims)\n    scratch_tensor.device(d) = y_backprop_rest_by_depth * x_scaled;\n    redux_sum_u(d, rest_by_depth, scratch_rest_by_depth, scale_backprop_output);\n\n    // Compute 'offset_backprop_output':\n    //   offset_backprop =\n    //     y_backprop_rest_by_depth.sum(reduce_dims)\n    redux_sum_t(d, rest_by_depth, transformed_y_backprop_input,\n                offset_backprop_output);\n    auto y_backprop_sum = offset_backprop;\n\n    auto y_backprop_sum_one_by_depth = y_backprop_sum.reshape(one_by_depth);\n    auto y_backprop_mean_one_by_depth =\n        y_backprop_sum_one_by_depth * rest_size_inv;\n    auto y_backprop_mean_rest_by_depth =\n        y_backprop_mean_one_by_depth.broadcast(bcast_spec);\n    auto y_backprop_centered =\n        y_backprop_rest_by_depth - y_backprop_mean_rest_by_depth;\n\n    // Compute expression:\n    //   y_backprop_centered_mean =\n    //     (y_backprop_rest_by_depth * x_centered).mean(reduce_dims)\n    scratch_tensor.device(d) = y_backprop_rest_by_depth * x_centered;\n    redux_sum_u(d, rest_by_depth, scratch_rest_by_depth, &scratch_one_by_depth);\n    auto y_backprop_centered_mean =\n        scratch_vector.reshape(one_by_depth) / static_cast<U>(rest_size);\n\n    auto coef1 = (scale.reshape(one_by_depth) * coef0_one_by_depth)\n                     .broadcast(bcast_spec);\n    auto coef2 = (coef0_one_by_depth.square() * y_backprop_centered_mean)\n                     .broadcast(bcast_spec);\n\n    x_backprop.reshape(rest_by_depth).device(d) =\n        (coef1 * (y_backprop_centered - x_centered * coef2)).template cast<T>();\n\n    if (tensor_format == FORMAT_NCHW) {\n      // Perform NHWC to NCHW\n      std::vector<int32> perm = {0, 3, 1, 2};\n      OP_REQUIRES_OK(\n          context, ::tensorflow::DoTranspose(context->eigen_device<CPUDevice>(),\n                                             transformed_x_backprop_output,\n                                             perm, x_backprop_output));\n    }\n  }\n};\n\ntemplate <typename T, typename U>\nstruct FusedBatchNormFreezeGrad<CPUDevice, T, U> {\n  void operator()(OpKernelContext* context, const Tensor& y_backprop_input,\n                  const Tensor& x_input, const Tensor& scale_input,\n                  const Tensor& pop_mean_input,\n                  const Tensor& pop_variance_input, U epsilon,\n                  Tensor* x_backprop_output, Tensor* scale_backprop_output,\n                  Tensor* offset_backprop_output) {\n    typename TTypes<T, 4>::ConstTensor y_backprop(\n        y_backprop_input.tensor<T, 4>());\n    typename TTypes<T, 4>::ConstTensor input(x_input.tensor<T, 4>());\n    typename TTypes<U>::ConstVec scale(scale_input.vec<U>());\n    typename TTypes<U>::ConstVec pop_mean(pop_mean_input.vec<U>());\n    typename TTypes<U>::ConstVec pop_var(pop_variance_input.vec<U>());\n    typename TTypes<T, 4>::Tensor x_backprop(x_backprop_output->tensor<T, 4>());\n    typename TTypes<U>::Vec scale_backprop(scale_backprop_output->vec<U>());\n\n    const int depth = pop_mean.dimension(0);\n    const int rest_size = input.size() / depth;\n\n    const CPUDevice& d = context->eigen_device<CPUDevice>();\n\n    // Allocate two temporary workspaces of [depth] shape.\n    Tensor scratch1_vec, scratch2_vec;\n    OP_REQUIRES_OK(context, context->allocate_temp(DataTypeToEnum<U>::value,\n                                                   {depth}, &scratch1_vec));\n    OP_REQUIRES_OK(context, context->allocate_temp(DataTypeToEnum<U>::value,\n                                                   {depth}, &scratch2_vec));\n\n    // Maybe allocate a temporary workspace of [rest_size, depth] shape.\n    Tensor scratch3_tensor;\n    if (std::is_same<T, U>::value) {\n      OP_REQUIRES(\n          context,\n          scratch3_tensor.CopyFrom(*x_backprop_output, {rest_size, depth}),\n          errors::Internal(\"Failed to copy a tensor\"));\n    } else {\n      OP_REQUIRES_OK(context, context->allocate_temp(DataTypeToEnum<U>::value,\n                                                     {rest_size, depth},\n                                                     &scratch3_tensor));\n    }\n\n    typename TTypes<U>::Vec scratch1(scratch1_vec.vec<U>());\n    typename TTypes<U>::Vec scratch2(scratch2_vec.vec<U>());\n    typename TTypes<U, 2>::Tensor scratch3(scratch3_tensor.tensor<U, 2>());\n\n    Eigen::DSizes<Eigen::Index, 2> rest_by_depth(rest_size, depth);\n#if !defined(EIGEN_HAS_INDEX_LIST)\n    Eigen::DSizes<Eigen::Index, 2> one_by_depth(1, depth);\n    Eigen::array<int, 2> rest_by_one({rest_size, 1});\n#else\n    Eigen::IndexList<Eigen::type2index<1>, Eigen::Index> one_by_depth;\n    one_by_depth.set(1, depth);\n    Eigen::IndexList<Eigen::Index, Eigen::type2index<1>> rest_by_one;\n    rest_by_one.set(0, rest_size);\n#endif\n\n    // Sum reduction along the 0th dimension using custom CPU functor.\n    using ScalarSum = Eigen::internal::scalar_sum_op<U>;\n    const functor::ReduceOuterDimensions<T, U, U, ScalarSum> redux_sum_t;\n    const functor::ReduceOuterDimensions<U, U, U, ScalarSum> redux_sum_u;\n\n    // offset_backprop  = sum(y_backprop)\n    // scale_backprop = y_backprop * ((x - pop_mean) * rsqrt(pop_var + epsilon))\n    // x_backprop = y_backprop * (scale * rsqrt(pop_var + epsilon))\n\n    // NOTE: DEFAULT DEVICE comment is added to expression assignments that\n    // we don't want to be executed in a thread pool.\n\n    auto y_backprop_rest_by_depth =\n        y_backprop.reshape(rest_by_depth).template cast<U>();\n    auto input_rest_by_depth = input.reshape(rest_by_depth).template cast<U>();\n\n    // offset_backprop  = sum(y_backprop)\n    redux_sum_t(d, rest_by_depth, y_backprop_input, offset_backprop_output);\n\n    // scratch1 = rsqrt(pop_var + epsilon)\n    scratch1 = (pop_var + pop_var.constant(epsilon)).rsqrt();  // DEFAULT DEVICE\n\n    // scratch2 = sum(y_backprop * (x - mean))\n    scratch3.device(d) =\n        y_backprop_rest_by_depth *\n        (input_rest_by_depth -\n         pop_mean.reshape(one_by_depth).broadcast(rest_by_one));\n    redux_sum_u(d, rest_by_depth, scratch3_tensor, &scratch2_vec);\n\n    x_backprop.reshape(rest_by_depth).device(d) =\n        (y_backprop_rest_by_depth *\n         ((scratch1.reshape(one_by_depth) * scale.reshape(one_by_depth))\n              .broadcast(rest_by_one)))\n            .template cast<T>();\n    scale_backprop = scratch2 * scratch1;  // DEFAULT DEVICE\n  }\n};\n\n#if !GOOGLE_CUDA\nnamespace {\n// See implementation under GOOGLE_CUDA #ifdef below.\n// This is a CUDA specific feature, do not enable it for non-CUDA builds\nbool BatchnormSpatialPersistentEnabled() { return false; }\n}  // namespace\n#endif\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\nnamespace {\n\nse::dnn::ActivationMode AsDnnActivationMode(\n    const FusedBatchNormActivationMode activation_mode) {\n  switch (activation_mode) {\n    case FusedBatchNormActivationMode::kIdentity:\n      return se::dnn::ActivationMode::kNone;\n    case FusedBatchNormActivationMode::kRelu:\n      return se::dnn::ActivationMode::kRelu;\n  }\n}\n\n#if GOOGLE_CUDA\n// NOTE(ezhulenev): See `BatchnormSpatialPersistentEnabled` documentation in the\n// `cuda_dnn.cc` for details.\nbool BatchnormSpatialPersistentEnabled() {\n#if CUDNN_VERSION >= 7402\n  static bool is_enabled = [] {\n    bool is_enabled = false;\n    TF_CHECK_OK(tensorflow::ReadBoolFromEnvVar(\n        \"TF_USE_CUDNN_BATCHNORM_SPATIAL_PERSISTENT\",\n        /*default_val=*/false, &is_enabled));\n    return is_enabled;\n  }();\n  return is_enabled;\n#else\n  return false;\n#endif\n}\n#endif\n\n}  // namespace\n\ntemplate <typename U, typename T>\nDeviceMemory<U> CastDeviceMemory(Tensor* tensor) {\n  return DeviceMemory<U>::MakeFromByteSize(\n      tensor->template flat<T>().data(),\n      tensor->template flat<T>().size() * sizeof(T));\n}\n\n// A helper to allocate temporary scratch memory for Cudnn BatchNormEx ops. It\n// takes the ownership of the underlying memory. The expectation is that the\n// memory should be alive for the span of the Cudnn BatchNormEx itself.\ntemplate <typename T>\nclass CudnnBatchNormAllocatorInTemp : public ScratchAllocator {\n public:\n  ~CudnnBatchNormAllocatorInTemp() override = default;\n\n  explicit CudnnBatchNormAllocatorInTemp(OpKernelContext* context)\n      : context_(context) {}\n\n  int64 GetMemoryLimitInBytes() override {\n    return std::numeric_limits<int64>::max();\n  }\n\n  StatusOr<DeviceMemory<uint8>> AllocateBytes(int64 byte_size) override {\n    Tensor temporary_memory;\n    const DataType tf_data_type = DataTypeToEnum<T>::v();\n    int64 allocate_count =\n        Eigen::divup(byte_size, static_cast<int64>(sizeof(T)));\n    Status allocation_status(context_->allocate_temp(\n        tf_data_type, TensorShape({allocate_count}), &temporary_memory));\n    if (!allocation_status.ok()) {\n      return allocation_status;\n    }\n    // Hold the reference of the allocated tensors until the end of the\n    // allocator.\n    allocated_tensors_.push_back(temporary_memory);\n    total_byte_size_ += byte_size;\n    return DeviceMemory<uint8>::MakeFromByteSize(\n        temporary_memory.template flat<T>().data(),\n        temporary_memory.template flat<T>().size() * sizeof(T));\n  }\n\n  int64 TotalByteSize() const { return total_byte_size_; }\n\n  Tensor get_allocated_tensor(int index) const {\n    return allocated_tensors_[index];\n  }\n\n private:\n  int64 total_byte_size_ = 0;\n  OpKernelContext* context_;  // not owned\n  std::vector<Tensor> allocated_tensors_;\n};\n\n// A helper to allocate memory for Cudnn BatchNormEx as a kernel output. It is\n// used by forward pass kernel to feed the output to the backward pass.\n// The memory is expected to live long enough after the backward pass is\n// finished.\ntemplate <typename T>\nclass CudnnBatchNormAllocatorInOutput : public ScratchAllocator {\n public:\n  ~CudnnBatchNormAllocatorInOutput() override {\n    if (!output_allocated) {\n      Tensor* dummy_reserve_space = nullptr;\n      OP_REQUIRES_OK(context_, context_->allocate_output(output_index_, {},\n                                                         &dummy_reserve_space));\n    }\n  }\n\n  CudnnBatchNormAllocatorInOutput(OpKernelContext* context, int output_index)\n      : context_(context), output_index_(output_index) {}\n\n  int64 GetMemoryLimitInBytes() override {\n    return std::numeric_limits<int64>::max();\n  }\n\n  StatusOr<DeviceMemory<uint8>> AllocateBytes(int64 byte_size) override {\n    output_allocated = true;\n    DCHECK(total_byte_size_ == 0)\n        << \"Reserve space allocator can only be called once\";\n    int64 allocate_count =\n        Eigen::divup(byte_size, static_cast<int64>(sizeof(T)));\n\n    Tensor* temporary_memory = nullptr;\n    Status allocation_status(context_->allocate_output(\n        output_index_, TensorShape({allocate_count}), &temporary_memory));\n    if (!allocation_status.ok()) {\n      return allocation_status;\n    }\n    total_byte_size_ += byte_size;\n    auto memory_uint8 = DeviceMemory<uint8>::MakeFromByteSize(\n        temporary_memory->template flat<T>().data(),\n        temporary_memory->template flat<T>().size() * sizeof(T));\n    return StatusOr<DeviceMemory<uint8>>(memory_uint8);\n  }\n\n  int64 TotalByteSize() { return total_byte_size_; }\n\n private:\n  int64 total_byte_size_ = 0;\n  OpKernelContext* context_;  // not owned\n  int output_index_;\n  bool output_allocated = false;\n};\n\ntemplate <typename T, typename U, bool is_training>\nstruct FusedBatchNorm<GPUDevice, T, U, is_training> {\n  void operator()(OpKernelContext* context, const Tensor& x,\n                  const Tensor& scale, const Tensor& offset,\n                  const Tensor& estimated_mean,\n                  const Tensor& estimated_variance, const Tensor* side_input,\n                  U epsilon, U exponential_avg_factor,\n                  FusedBatchNormActivationMode activation_mode, Tensor* y,\n                  Tensor* batch_mean, Tensor* batch_var, Tensor* saved_mean,\n                  Tensor* saved_inv_var, TensorFormat tensor_format,\n                  bool use_reserved_space) {\n    auto* stream = context->op_device_context()->stream();\n    OP_REQUIRES(context, stream, errors::Internal(\"No GPU stream available\"));\n\n    const int64 batch_size = GetTensorDim(x, tensor_format, 'N');\n    const int64 channels = GetTensorDim(x, tensor_format, 'C');\n    const int64 height = GetTensorDim(x, tensor_format, 'H');\n    const int64 width = GetTensorDim(x, tensor_format, 'W');\n\n    // If use_reserved_space we have reserve_space_3 output (only in\n    // FusedBatchNormV3 op).\n\n#if GOOGLE_CUDA\n    // Check if cuDNN batch normalization has a fast NHWC implementation:\n    //   (1) In inference mode it's always fast.\n    //   (2) Tensorflow enabled batchnorm spatial persistence, we are called\n    //   from\n    //       FusedBatchNormV3, i.e. use_reserved_space is true.\n    const bool fast_nhwc_batch_norm =\n        !is_training ||\n        (BatchnormSpatialPersistentEnabled() &&\n         DataTypeToEnum<T>::value == DT_HALF && use_reserved_space);\n#else\n    // fast NHWC implementation is a CUDA only feature\n    const bool fast_nhwc_batch_norm = false;\n#endif\n\n    // If input tensor is in NHWC format, and we have a fast cuDNN\n    // implementation, there is no need to do data format conversion.\n    TensorFormat compute_format =\n        fast_nhwc_batch_norm && tensor_format == FORMAT_NHWC ? FORMAT_NHWC\n                                                             : FORMAT_NCHW;\n\n    VLOG(2) << \"FusedBatchNorm:\"\n            << \" batch_size: \" << batch_size << \" channels: \" << channels\n            << \" height: \" << height << \" width:\" << width\n            << \" x shape: \" << x.shape().DebugString()\n            << \" scale shape: \" << scale.shape().DebugString()\n            << \" offset shape: \" << offset.shape().DebugString()\n            << \" activation mode: \" << ToString(activation_mode)\n            << \" tensor format: \" << ToString(tensor_format)\n            << \" compute format: \" << ToString(compute_format);\n\n    auto maybe_make_dummy_output = [context, use_reserved_space]() -> Status {\n      if (use_reserved_space) {\n        Tensor* dummy_reserve_space = nullptr;\n        return context->allocate_output(5, {}, &dummy_reserve_space);\n      }\n      return Status::OK();\n    };\n\n    // If input is empty, return NaN mean/variance\n    if (x.shape().num_elements() == 0) {\n      OP_REQUIRES_OK(context, maybe_make_dummy_output());\n      functor::SetNanFunctor<U> f;\n      f(context->eigen_device<GPUDevice>(), batch_mean->flat<U>());\n      f(context->eigen_device<GPUDevice>(), batch_var->flat<U>());\n      return;\n    }\n\n    // In inference mode we use custom CUDA kernel, because cuDNN does not\n    // support side input and activations for inference.\n    const bool has_side_input = side_input != nullptr;\n    const bool has_activation =\n        activation_mode != FusedBatchNormActivationMode::kIdentity;\n\n    if (!is_training && (has_side_input || has_activation)) {\n      OP_REQUIRES_OK(context, maybe_make_dummy_output());\n      FusedBatchNormInferenceFunctor<GPUDevice, T, U> inference_functor;\n\n      if (has_side_input) {\n        inference_functor(context, tensor_format, x.tensor<T, 4>(),\n                          scale.vec<U>(), offset.vec<U>(),\n                          estimated_mean.vec<U>(), estimated_variance.vec<U>(),\n                          side_input->tensor<T, 4>(), epsilon, activation_mode,\n                          y->tensor<T, 4>());\n      } else {\n        typename TTypes<T, 4>::ConstTensor empty_tensor(nullptr, 0, 0, 0, 0);\n        inference_functor(context, tensor_format, x.tensor<T, 4>(),\n                          scale.vec<U>(), offset.vec<U>(),\n                          estimated_mean.vec<U>(), estimated_variance.vec<U>(),\n                          empty_tensor, epsilon, activation_mode,\n                          y->tensor<T, 4>());\n      }\n      return;\n    }\n\n    Tensor x_maybe_transformed = x;\n    Tensor x_transformed;\n    Tensor y_transformed;\n    se::DeviceMemory<T> y_ptr;\n\n    if (tensor_format == compute_format) {\n      y_ptr = StreamExecutorUtil::AsDeviceMemory<T>(*y);\n    } else if (tensor_format == FORMAT_NHWC && compute_format == FORMAT_NCHW) {\n      OP_REQUIRES_OK(context, context->allocate_temp(\n                                  DataTypeToEnum<T>::value,\n                                  ShapeFromFormat(compute_format, batch_size,\n                                                  height, width, channels),\n                                  &x_transformed));\n      functor::NHWCToNCHW<GPUDevice, T, 4>()(\n          context->eigen_device<GPUDevice>(),\n          const_cast<const Tensor&>(x_maybe_transformed).tensor<T, 4>(),\n          x_transformed.tensor<T, 4>());\n      x_maybe_transformed = x_transformed;\n\n      OP_REQUIRES_OK(context, context->allocate_temp(\n                                  DataTypeToEnum<T>::value,\n                                  ShapeFromFormat(compute_format, batch_size,\n                                                  height, width, channels),\n                                  &y_transformed));\n      y_ptr = StreamExecutorUtil::AsDeviceMemory<T>(y_transformed);\n    } else {\n      context->SetStatus(errors::Internal(\n          \"Unsupported tensor format: \", ToString(tensor_format),\n          \" and compute format: \", ToString(compute_format)));\n      return;\n    }\n\n    const se::dnn::DataLayout data_layout =\n        compute_format == FORMAT_NHWC ? se::dnn::DataLayout::kBatchYXDepth\n                                      : se::dnn::DataLayout::kBatchDepthYX;\n\n    se::dnn::BatchDescriptor x_desc;\n    x_desc.set_count(batch_size)\n        .set_feature_map_count(channels)\n        .set_height(height)\n        .set_width(width)\n        .set_layout(data_layout);\n\n    se::dnn::BatchDescriptor scale_offset_desc;\n    scale_offset_desc.set_count(1)\n        .set_feature_map_count(channels)\n        .set_height(1)\n        .set_width(1)\n        .set_layout(se::dnn::DataLayout::kBatchDepthYX);\n\n    auto x_ptr = StreamExecutorUtil::AsDeviceMemory<T>(x_maybe_transformed);\n    auto scale_ptr = StreamExecutorUtil::AsDeviceMemory<U>(scale);\n    auto offset_ptr = StreamExecutorUtil::AsDeviceMemory<U>(offset);\n    auto estimated_mean_ptr =\n        StreamExecutorUtil::AsDeviceMemory<U>(estimated_mean);\n    auto estimated_variance_ptr =\n        StreamExecutorUtil::AsDeviceMemory<U>(estimated_variance);\n    auto side_input_ptr =\n        side_input != nullptr\n            ? StreamExecutorUtil::AsDeviceMemory<U>(*side_input)\n            : se::DeviceMemory<U>();\n    auto batch_mean_ptr = StreamExecutorUtil::AsDeviceMemory<U>(*batch_mean);\n\n    auto batch_var_ptr = StreamExecutorUtil::AsDeviceMemory<U>(*batch_var);\n    auto saved_mean_ptr = StreamExecutorUtil::AsDeviceMemory<U>(*saved_mean);\n    auto saved_inv_var_ptr =\n        StreamExecutorUtil::AsDeviceMemory<U>(*saved_inv_var);\n\n    std::unique_ptr<functor::CudnnBatchNormAllocatorInOutput<U>>\n        reserve_space_allocator;\n    std::unique_ptr<functor::CudnnBatchNormAllocatorInTemp<uint8>>\n        workspace_allocator;\n    if (use_reserved_space) {\n      reserve_space_allocator.reset(\n          new functor::CudnnBatchNormAllocatorInOutput<U>(context, 5));\n      workspace_allocator.reset(\n          new functor::CudnnBatchNormAllocatorInTemp<uint8>(context));\n    }\n    if (!batch_mean->SharesBufferWith(estimated_mean) &&\n        exponential_avg_factor != 1.0f) {\n      OP_REQUIRES(\n          context,\n          stream\n              ->ThenMemcpyD2D(&batch_mean_ptr, estimated_mean_ptr,\n                              estimated_mean.NumElements() * sizeof(U))\n              .ok(),\n          errors::Internal(\"MatrixTriangularSolveOp: failed to copy rhs \"\n                           \"from device\"));\n    }\n    if (!batch_var->SharesBufferWith(estimated_variance) &&\n        exponential_avg_factor != 1.0f) {\n      OP_REQUIRES(\n          context,\n          stream\n              ->ThenMemcpyD2D(&batch_var_ptr, estimated_variance_ptr,\n                              estimated_variance.NumElements() * sizeof(U))\n              .ok(),\n          errors::Internal(\"MatrixTriangularSolveOp: failed to copy rhs \"\n                           \"from device\"));\n    }\n    bool cudnn_launch_status =\n        stream\n            ->ThenBatchNormalizationForward(\n                x_ptr, scale_ptr, offset_ptr, estimated_mean_ptr,\n                estimated_variance_ptr, side_input_ptr, x_desc,\n                scale_offset_desc, static_cast<double>(epsilon),\n                static_cast<double>(exponential_avg_factor),\n                AsDnnActivationMode(activation_mode), &y_ptr, &batch_mean_ptr,\n                &batch_var_ptr, &saved_mean_ptr, &saved_inv_var_ptr,\n                is_training, reserve_space_allocator.get(),\n                workspace_allocator.get())\n            .ok();\n\n    if (!cudnn_launch_status) {\n      context->SetStatus(\n          errors::Internal(\"cuDNN launch failure : input shape (\",\n                           x.shape().DebugString(), \")\"));\n      return;\n    }\n\n    if (tensor_format == FORMAT_NHWC && compute_format == FORMAT_NCHW) {\n      functor::NCHWToNHWC<GPUDevice, T, 4>()(\n          context->eigen_device<GPUDevice>(),\n          const_cast<const Tensor&>(y_transformed).tensor<T, 4>(),\n          y->tensor<T, 4>());\n    }\n  }\n};\n\ntemplate <typename T, typename U>\nstruct FusedBatchNormGrad<GPUDevice, T, U> {\n  void operator()(OpKernelContext* context, const Tensor& y_backprop,\n                  const Tensor& x, const Tensor& scale, const Tensor& mean,\n                  const Tensor& inv_variance, U epsilon, Tensor* x_backprop,\n                  Tensor* scale_backprop, Tensor* offset_backprop,\n                  bool use_reserved_space, TensorFormat tensor_format) {\n    auto* stream = context->op_device_context()->stream();\n    OP_REQUIRES(context, stream, errors::Internal(\"No GPU stream available\"));\n\n    const int64 batch_size = GetTensorDim(x, tensor_format, 'N');\n    const int64 channels = GetTensorDim(x, tensor_format, 'C');\n    const int64 height = GetTensorDim(x, tensor_format, 'H');\n    const int64 width = GetTensorDim(x, tensor_format, 'W');\n\n#if GOOGLE_CUDA\n    // Check if cuDNN batch normalization has a fast NHWC implementation:\n    //   (1) Tensorflow enabled batchnorm spatial persistence, and\n    //       FusedBatchNormGradV3 passed non-null reserve space and allocator.\n    const bool fast_nhwc_batch_norm = BatchnormSpatialPersistentEnabled() &&\n                                      DataTypeToEnum<T>::value == DT_HALF &&\n                                      use_reserved_space;\n#else\n    // fast NHWC implementation is a CUDA only feature\n    const bool fast_nhwc_batch_norm = false;\n#endif\n\n    // If input tensor is in NHWC format, and we have a fast cuDNN\n    // implementation, there is no need to do data format conversion.\n    TensorFormat compute_format =\n        fast_nhwc_batch_norm && tensor_format == FORMAT_NHWC ? FORMAT_NHWC\n                                                             : FORMAT_NCHW;\n\n    VLOG(2) << \"FusedBatchNormGrad:\"\n            << \" batch_size: \" << batch_size << \" channels: \" << channels\n            << \" height: \" << height << \" width: \" << width\n            << \" y_backprop shape: \" << y_backprop.shape().DebugString()\n            << \" x shape: \" << x.shape().DebugString()\n            << \" scale shape: \" << scale.shape().DebugString()\n            << \" tensor format: \" << ToString(tensor_format)\n            << \" compute format: \" << ToString(compute_format);\n\n    // Inputs\n    Tensor y_backprop_maybe_transformed = y_backprop;\n    Tensor x_maybe_transformed = x;\n    Tensor y_backprop_transformed;\n    Tensor x_transformed;\n\n    // Outputs\n    Tensor x_backprop_transformed;\n    se::DeviceMemory<T> x_backprop_ptr;\n\n    if (tensor_format == compute_format) {\n      x_backprop_ptr = StreamExecutorUtil::AsDeviceMemory<T>(*x_backprop);\n    } else if (tensor_format == FORMAT_NHWC && compute_format == FORMAT_NCHW) {\n      // Transform inputs from 'NHWC' to 'NCHW'\n      OP_REQUIRES_OK(context, context->allocate_temp(\n                                  DataTypeToEnum<T>::value,\n                                  ShapeFromFormat(FORMAT_NCHW, batch_size,\n                                                  height, width, channels),\n                                  &y_backprop_transformed));\n      functor::NHWCToNCHW<GPUDevice, T, 4>()(\n          context->eigen_device<GPUDevice>(),\n          const_cast<const Tensor&>(y_backprop_maybe_transformed)\n              .tensor<T, 4>(),\n          y_backprop_transformed.tensor<T, 4>());\n      y_backprop_maybe_transformed = y_backprop_transformed;\n\n      OP_REQUIRES_OK(context, context->allocate_temp(\n                                  DataTypeToEnum<T>::value,\n                                  ShapeFromFormat(FORMAT_NCHW, batch_size,\n                                                  height, width, channels),\n                                  &x_transformed));\n      functor::NHWCToNCHW<GPUDevice, T, 4>()(\n          context->eigen_device<GPUDevice>(),\n          const_cast<const Tensor&>(x_maybe_transformed).tensor<T, 4>(),\n          x_transformed.tensor<T, 4>());\n      x_maybe_transformed = x_transformed;\n\n      // Allocate memory for transformed outputs in 'NCHW'\n      OP_REQUIRES_OK(context, context->allocate_temp(\n                                  DataTypeToEnum<T>::value,\n                                  ShapeFromFormat(FORMAT_NCHW, batch_size,\n                                                  height, width, channels),\n                                  &x_backprop_transformed));\n      x_backprop_ptr =\n          StreamExecutorUtil::AsDeviceMemory<T>(x_backprop_transformed);\n    } else {\n      context->SetStatus(errors::Internal(\n          \"Unsupported tensor format: \", ToString(tensor_format),\n          \" and compute format: \", ToString(compute_format)));\n      return;\n    }\n\n    const se::dnn::DataLayout data_layout =\n        compute_format == FORMAT_NHWC ? se::dnn::DataLayout::kBatchYXDepth\n                                      : se::dnn::DataLayout::kBatchDepthYX;\n\n    se::dnn::BatchDescriptor x_desc;\n    x_desc.set_count(batch_size)\n        .set_feature_map_count(channels)\n        .set_height(height)\n        .set_width(width)\n        .set_layout(data_layout);\n\n    se::dnn::BatchDescriptor scale_offset_desc;\n    scale_offset_desc.set_count(1)\n        .set_feature_map_count(channels)\n        .set_height(1)\n        .set_width(1)\n        .set_layout(se::dnn::DataLayout::kBatchDepthYX);\n\n    auto y_backprop_ptr =\n        StreamExecutorUtil::AsDeviceMemory<T>(y_backprop_maybe_transformed);\n    auto x_ptr = StreamExecutorUtil::AsDeviceMemory<T>(x_maybe_transformed);\n    auto scale_ptr = StreamExecutorUtil::AsDeviceMemory<U>(scale);\n    auto mean_ptr = StreamExecutorUtil::AsDeviceMemory<U>(mean);\n    auto inv_variance_ptr = StreamExecutorUtil::AsDeviceMemory<U>(inv_variance);\n    auto scale_backprop_ptr =\n        StreamExecutorUtil::AsDeviceMemory<U>(*scale_backprop);\n    auto offset_backprop_ptr =\n        StreamExecutorUtil::AsDeviceMemory<U>(*offset_backprop);\n\n    std::unique_ptr<functor::CudnnBatchNormAllocatorInTemp<uint8>>\n        workspace_allocator;\n    DeviceMemory<uint8>* reserve_space_data_ptr = nullptr;\n    DeviceMemory<uint8> reserve_space_data;\n#if CUDNN_VERSION >= 7402\n    if (use_reserved_space) {\n      const Tensor& reserve_space = context->input(5);\n      workspace_allocator.reset(\n          new functor::CudnnBatchNormAllocatorInTemp<uint8>(context));\n\n      // the cudnn kernel outputs inverse variance in forward and reuse it in\n      // backward\n      if (reserve_space.dims() != 0) {\n        reserve_space_data = functor::CastDeviceMemory<uint8, U>(\n            const_cast<Tensor*>(&reserve_space));\n        reserve_space_data_ptr = &reserve_space_data;\n      }\n    }\n#endif  // CUDNN_VERSION >= 7402\n\n    bool cudnn_launch_status =\n        stream\n            ->ThenBatchNormalizationBackward(\n                y_backprop_ptr, x_ptr, scale_ptr, mean_ptr, inv_variance_ptr,\n                x_desc, scale_offset_desc, static_cast<double>(epsilon),\n                &x_backprop_ptr, &scale_backprop_ptr, &offset_backprop_ptr,\n                reserve_space_data_ptr, workspace_allocator.get())\n            .ok();\n\n    if (!cudnn_launch_status) {\n      context->SetStatus(\n          errors::Internal(\"cuDNN launch failure : input shape (\",\n                           x.shape().DebugString(), \")\"));\n    }\n    if (tensor_format == FORMAT_NHWC && compute_format == FORMAT_NCHW) {\n      functor::NCHWToNHWC<GPUDevice, T, 4>()(\n          context->eigen_device<GPUDevice>(),\n          const_cast<const Tensor&>(x_backprop_transformed).tensor<T, 4>(),\n          x_backprop->tensor<T, 4>());\n    }\n  }\n};\n\n// Forward declarations of the functor specializations for GPU.\n#define DECLARE_GPU_SPEC(T, U)                                                 \\\n  template <>                                                                  \\\n  void FusedBatchNormFreezeGrad<GPUDevice, T, U>::operator()(                  \\\n      OpKernelContext* context, const Tensor& y_backprop_input,                \\\n      const Tensor& x_input, const Tensor& scale_input,                        \\\n      const Tensor& mean_input, const Tensor& variance_input, U epsilon,       \\\n      Tensor* x_backprop_output, Tensor* scale_backprop_output,                \\\n      Tensor* offset_backprop_output);                                         \\\n  extern template struct FusedBatchNormFreezeGrad<GPUDevice, T, U>;            \\\n  template <>                                                                  \\\n  void FusedBatchNormInferenceFunctor<GPUDevice, T, U>::operator()(            \\\n      OpKernelContext* context, TensorFormat tensor_format,                    \\\n      typename TTypes<T, 4>::ConstTensor in,                                   \\\n      typename TTypes<U>::ConstVec scale, typename TTypes<U>::ConstVec offset, \\\n      typename TTypes<U>::ConstVec estimated_mean,                             \\\n      typename TTypes<U>::ConstVec estimated_variance,                         \\\n      typename TTypes<T, 4>::ConstTensor side_input, U epsilon,                \\\n      FusedBatchNormActivationMode activation_mode,                            \\\n      typename TTypes<T, 4>::Tensor out);                                      \\\n  extern template struct FusedBatchNormInferenceFunctor<GPUDevice, T, U>;\n\nDECLARE_GPU_SPEC(float, float);\nDECLARE_GPU_SPEC(Eigen::half, float);\n\n#undef DECLARE_GPU_SPEC\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n}  // namespace functor\n\ntemplate <typename Device, typename T, typename U>\nclass FusedBatchNormOpBase : public OpKernel {\n  using FbnActivationMode = functor::FusedBatchNormActivationMode;\n\n protected:\n  explicit FusedBatchNormOpBase(OpKernelConstruction* context,\n                                bool is_batch_norm_ex = false)\n      : OpKernel(context) {\n    float epsilon;\n    OP_REQUIRES_OK(context, context->GetAttr(\"epsilon\", &epsilon));\n    epsilon_ = U(epsilon);\n    float exponential_avg_factor;\n    OP_REQUIRES_OK(context, context->GetAttr(\"exponential_avg_factor\",\n                                             &exponential_avg_factor));\n    exponential_avg_factor_ = U(exponential_avg_factor);\n    string tensor_format;\n    OP_REQUIRES_OK(context, context->GetAttr(\"data_format\", &tensor_format));\n    OP_REQUIRES(context, FormatFromString(tensor_format, &tensor_format_),\n                errors::InvalidArgument(\"Invalid data format\"));\n    OP_REQUIRES_OK(context, context->GetAttr(\"is_training\", &is_training_));\n\n    if (!is_batch_norm_ex) {\n      has_side_input_ = false;\n      activation_mode_ = FbnActivationMode::kIdentity;\n    } else {\n      OP_REQUIRES_OK(context, ParseActivationMode(context, &activation_mode_));\n\n      int num_side_inputs;\n      OP_REQUIRES_OK(context,\n                     context->GetAttr(\"num_side_inputs\", &num_side_inputs));\n      OP_REQUIRES(context, num_side_inputs >= 0 && num_side_inputs <= 1,\n                  errors::InvalidArgument(\n                      \"FusedBatchNorm accepts at most one side input.\"));\n      has_side_input_ = (num_side_inputs == 1);\n      if (has_side_input_ && is_training_) {\n        OP_REQUIRES(\n            context, activation_mode_ != FbnActivationMode::kIdentity,\n            errors::InvalidArgument(\"Identity activation is not supported with \"\n                                    \"non-empty side input\"));\n      }\n    }\n\n    if (activation_mode_ != FbnActivationMode::kIdentity && is_training_) {\n      // NOTE(ezhulenev): Following requirements are coming from implementation\n      // details of cudnnBatchNormalizationForwardTrainingEx used in training\n      // mode. In inference mode we call custom CUDA kernel that supports all\n      // data formats and data types.\n      OP_REQUIRES(context, DataTypeToEnum<T>::value == DT_HALF,\n                  errors::InvalidArgument(\"FusedBatchNorm with activation \"\n                                          \"supports only DT_HALF data type.\"));\n      OP_REQUIRES(context, tensor_format_ == FORMAT_NHWC,\n                  errors::InvalidArgument(\"FusedBatchNorm with activation \"\n                                          \"supports only NHWC tensor format.\"));\n      OP_REQUIRES(context, functor::BatchnormSpatialPersistentEnabled(),\n                  errors::InvalidArgument(\n                      \"FusedBatchNorm with activation must run with cuDNN \"\n                      \"spatial persistence mode enabled.\"));\n    }\n  }\n\n  // If use_reserved_space is true, we need to handle the 5th output (a reserved\n  // space) and a new cudnn batch norm will be called if the version > 7.4.2.\n  // If use_reserved_space is false, we don't have 5th output.\n  virtual void ComputeWithReservedSpace(OpKernelContext* context,\n                                        bool use_reserved_space) {\n    Tensor x = context->input(0);\n    const Tensor& scale = context->input(1);\n    const Tensor& offset = context->input(2);\n    const Tensor& estimated_mean = context->input(3);\n    const Tensor& estimated_variance = context->input(4);\n    const Tensor* side_input = has_side_input_ ? &context->input(5) : nullptr;\n\n    OP_REQUIRES(context, x.dims() == 4 || x.dims() == 5,\n                errors::InvalidArgument(\"input must be 4 or 5-dimensional\",\n                                        x.shape().DebugString()));\n    OP_REQUIRES(context, scale.dims() == 1,\n                errors::InvalidArgument(\"scale must be 1-dimensional\",\n                                        scale.shape().DebugString()));\n    OP_REQUIRES(context, offset.dims() == 1,\n                errors::InvalidArgument(\"offset must be 1-dimensional\",\n                                        offset.shape().DebugString()));\n    OP_REQUIRES(context, estimated_mean.dims() == 1,\n                errors::InvalidArgument(\"estimated_mean must be 1-dimensional\",\n                                        estimated_mean.shape().DebugString()));\n    OP_REQUIRES(\n        context, estimated_variance.dims() == 1,\n        errors::InvalidArgument(\"estimated_variance must be 1-dimensional\",\n                                estimated_variance.shape().DebugString()));\n    bool use_reshape = (x.dims() == 5);\n    auto x_shape = x.shape();\n    TensorShape dest_shape;\n    if (use_reshape) {\n      const int64 in_batch = GetTensorDim(x, tensor_format_, 'N');\n      int64 in_planes = GetTensorDim(x, tensor_format_, '0');\n      int64 in_rows = GetTensorDim(x, tensor_format_, '1');\n      int64 in_cols = GetTensorDim(x, tensor_format_, '2');\n      const int64 in_depth = GetTensorDim(x, tensor_format_, 'C');\n      dest_shape = ShapeFromFormat(tensor_format_, in_batch,\n                                   {{in_planes, in_rows * in_cols}}, in_depth);\n      OP_REQUIRES(context, x.CopyFrom(x, dest_shape),\n                  errors::InvalidArgument(\"Error during tensor copy.\"));\n    }\n\n    if (has_side_input_) {\n      OP_REQUIRES(context, side_input->shape() == x.shape(),\n                  errors::InvalidArgument(\n                      \"side_input shape must be equal to input shape: \",\n                      side_input->shape().DebugString(),\n                      \" != \", x.shape().DebugString()));\n    }\n\n    if (activation_mode_ != FbnActivationMode::kIdentity) {\n      // NOTE(ezhulenev): This requirement is coming from implementation\n      // details of cudnnBatchNormalizationForwardTrainingEx.\n      OP_REQUIRES(\n          context, !is_training_ || x.dim_size(3) % 4 == 0,\n          errors::InvalidArgument(\"FusedBatchNorm with activation requires \"\n                                  \"channel dimension to be a multiple of 4.\"));\n    }\n\n    Tensor* y = nullptr;\n    auto alloc_shape = use_reshape ? dest_shape : x_shape;\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                {0}, 0, alloc_shape, &y));\n\n    Tensor* batch_mean = nullptr;\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                {3}, 1, scale.shape(), &batch_mean));\n    Tensor* batch_var = nullptr;\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                {4}, 2, scale.shape(), &batch_var));\n    Tensor* saved_mean = nullptr;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(3, scale.shape(), &saved_mean));\n    Tensor* saved_maybe_inv_var = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(4, scale.shape(),\n                                                     &saved_maybe_inv_var));\n\n    if (is_training_) {\n      functor::FusedBatchNorm<Device, T, U, true>()(\n          context, x, scale, offset, estimated_mean, estimated_variance,\n          side_input, epsilon_, exponential_avg_factor_, activation_mode_, y,\n          batch_mean, batch_var, saved_mean, saved_maybe_inv_var,\n          tensor_format_, use_reserved_space);\n    } else {\n      functor::FusedBatchNorm<Device, T, U, false>()(\n          context, x, scale, offset, estimated_mean, estimated_variance,\n          side_input, epsilon_, exponential_avg_factor_, activation_mode_, y,\n          batch_mean, batch_var, saved_mean, saved_maybe_inv_var,\n          tensor_format_, use_reserved_space);\n    }\n    if (use_reshape) {\n      OP_REQUIRES(context, y->CopyFrom(*y, x_shape),\n                  errors::InvalidArgument(\"Error during tensor copy.\"));\n    }\n  }\n\n private:\n  U epsilon_;\n  U exponential_avg_factor_;\n  TensorFormat tensor_format_;\n  bool is_training_;\n  bool has_side_input_;\n  FbnActivationMode activation_mode_;\n};\n\ntemplate <typename Device, typename T, typename U>\nclass FusedBatchNormOp : public FusedBatchNormOpBase<Device, T, U> {\n public:\n  explicit FusedBatchNormOp(OpKernelConstruction* context)\n      : FusedBatchNormOpBase<Device, T, U>(context) {}\n\n  void Compute(OpKernelContext* context) override {\n    FusedBatchNormOpBase<Device, T, U>::ComputeWithReservedSpace(context,\n                                                                 false);\n  }\n};\n\ntemplate <typename Device, typename T, typename U>\nclass FusedBatchNormOpV3 : public FusedBatchNormOpBase<Device, T, U> {\n public:\n  explicit FusedBatchNormOpV3(OpKernelConstruction* context)\n      : FusedBatchNormOpBase<Device, T, U>(context) {}\n\n  void Compute(OpKernelContext* context) override {\n    FusedBatchNormOpBase<Device, T, U>::ComputeWithReservedSpace(context, true);\n  }\n};\n\ntemplate <typename Device, typename T, typename U>\nclass FusedBatchNormOpEx : public FusedBatchNormOpBase<Device, T, U> {\n  static constexpr bool kWithSideInputAndActivation = true;\n\n public:\n  explicit FusedBatchNormOpEx(OpKernelConstruction* context)\n      : FusedBatchNormOpBase<Device, T, U>(context,\n                                           kWithSideInputAndActivation) {}\n\n  void Compute(OpKernelContext* context) override {\n    FusedBatchNormOpBase<Device, T, U>::ComputeWithReservedSpace(context, true);\n  }\n};\n\ntemplate <typename Device, typename T, typename U>\nclass FusedBatchNormGradOpBase : public OpKernel {\n protected:\n  explicit FusedBatchNormGradOpBase(OpKernelConstruction* context)\n      : OpKernel(context) {\n    float epsilon;\n    OP_REQUIRES_OK(context, context->GetAttr(\"epsilon\", &epsilon));\n    epsilon_ = U(epsilon);\n    string tensor_format;\n    OP_REQUIRES_OK(context, context->GetAttr(\"data_format\", &tensor_format));\n    OP_REQUIRES(context, FormatFromString(tensor_format, &tensor_format_),\n                errors::InvalidArgument(\"Invalid data format\"));\n    OP_REQUIRES_OK(context, context->GetAttr(\"is_training\", &is_training_));\n  }\n\n  virtual void ComputeWithReservedSpace(OpKernelContext* context,\n                                        bool use_reserved_space) {\n    Tensor y_backprop = context->input(0);\n    Tensor x = context->input(1);\n    const Tensor& scale = context->input(2);\n    // When is_training=True, batch mean and variance/inverted variance are\n    // saved in the forward pass to be reused here. When is_training=False,\n    // population mean and variance need to be forwarded here to compute the\n    // gradients.\n    const Tensor& saved_mean_or_pop_mean = context->input(3);\n    // The Eigen implementation saves variance in the forward pass, while cuDNN\n    // saves inverted variance.\n    const Tensor& saved_maybe_inv_var_or_pop_var = context->input(4);\n\n    OP_REQUIRES(context, y_backprop.dims() == 4 || y_backprop.dims() == 5,\n                errors::InvalidArgument(\"input must be 4 or 5-dimensional\",\n                                        y_backprop.shape().DebugString()));\n    OP_REQUIRES(context, x.dims() == 4 || x.dims() == 5,\n                errors::InvalidArgument(\"input must be 4 or 5-dimensional\",\n                                        x.shape().DebugString()));\n    OP_REQUIRES(context, scale.dims() == 1,\n                errors::InvalidArgument(\"scale must be 1-dimensional\",\n                                        scale.shape().DebugString()));\n    OP_REQUIRES(\n        context, saved_mean_or_pop_mean.dims() == 1,\n        errors::InvalidArgument(\"saved mean must be 1-dimensional\",\n                                saved_mean_or_pop_mean.shape().DebugString()));\n    OP_REQUIRES(context, saved_maybe_inv_var_or_pop_var.dims() == 1,\n                errors::InvalidArgument(\n                    \"saved variance must be 1-dimensional\",\n                    saved_maybe_inv_var_or_pop_var.shape().DebugString()));\n    bool use_reshape = (x.dims() == 5);\n    auto x_shape = x.shape();\n    TensorShape dest_shape;\n    if (use_reshape) {\n      const int64 in_batch = GetTensorDim(x, tensor_format_, 'N');\n      int64 in_planes = GetTensorDim(x, tensor_format_, '0');\n      int64 in_rows = GetTensorDim(x, tensor_format_, '1');\n      int64 in_cols = GetTensorDim(x, tensor_format_, '2');\n      const int64 in_depth = GetTensorDim(x, tensor_format_, 'C');\n      dest_shape = ShapeFromFormat(tensor_format_, in_batch,\n                                   {{in_planes, in_rows * in_cols}}, in_depth);\n      OP_REQUIRES(context, x.CopyFrom(x, dest_shape),\n                  errors::InvalidArgument(\"Error during tensor copy.\"));\n      OP_REQUIRES(context, y_backprop.CopyFrom(y_backprop, dest_shape),\n                  errors::InvalidArgument(\"Error during tensor copy.\"));\n    }\n\n    Tensor* x_backprop = nullptr;\n    auto alloc_shape = use_reshape ? dest_shape : x_shape;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(0, alloc_shape, &x_backprop));\n\n    const TensorShape& scale_offset_shape = scale.shape();\n    Tensor* scale_backprop = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(1, scale_offset_shape,\n                                                     &scale_backprop));\n    Tensor* offset_backprop = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(2, scale_offset_shape,\n                                                     &offset_backprop));\n    // Two placeholders for estimated_mean and estimated_variance, which are\n    // used for inference and thus not needed here for gradient computation.\n    // They are filled with zeros so as to avoid NaN outputs.\n    Tensor* placeholder_1 = nullptr;\n    OP_REQUIRES_OK(\n        context, context->allocate_output(3, TensorShape({0}), &placeholder_1));\n    Tensor* placeholder_2 = nullptr;\n    OP_REQUIRES_OK(\n        context, context->allocate_output(4, TensorShape({0}), &placeholder_2));\n\n    // If input is empty, set gradients w.r.t scale/offset to zero.\n    if (x.shape().num_elements() == 0) {\n      functor::SetZeroFunctor<Device, U> f;\n      f(context->eigen_device<Device>(), scale_backprop->flat<U>());\n      f(context->eigen_device<Device>(), offset_backprop->flat<U>());\n      return;\n    }\n\n    if (is_training_) {\n      functor::FusedBatchNormGrad<Device, T, U>()(\n          context, y_backprop, x, scale, saved_mean_or_pop_mean,\n          saved_maybe_inv_var_or_pop_var, epsilon_, x_backprop, scale_backprop,\n          offset_backprop, use_reserved_space, tensor_format_);\n    } else {\n      // Necessary layout conversion is currently done in python.\n      OP_REQUIRES(context, tensor_format_ == FORMAT_NHWC,\n                  errors::InvalidArgument(\n                      \"The implementation of \"\n                      \"FusedBatchNormGrad with is_training=False only support \"\n                      \"NHWC tensor format for now.\"));\n      functor::FusedBatchNormFreezeGrad<Device, T, U>()(\n          context, y_backprop, x, scale, saved_mean_or_pop_mean,\n          saved_maybe_inv_var_or_pop_var, epsilon_, x_backprop, scale_backprop,\n          offset_backprop);\n    }\n    if (use_reshape) {\n      OP_REQUIRES(context, x_backprop->CopyFrom(*x_backprop, x_shape),\n                  errors::InvalidArgument(\"Error during tensor copy.\"));\n    }\n  }\n\n private:\n  U epsilon_;\n  TensorFormat tensor_format_;\n  bool is_training_;\n};\n\ntemplate <typename Device, typename T, typename U>\nclass FusedBatchNormGradOp : public FusedBatchNormGradOpBase<Device, T, U> {\n public:\n  explicit FusedBatchNormGradOp(OpKernelConstruction* context)\n      : FusedBatchNormGradOpBase<Device, T, U>(context) {}\n\n  void Compute(OpKernelContext* context) override {\n    FusedBatchNormGradOpBase<Device, T, U>::ComputeWithReservedSpace(context,\n                                                                     false);\n  }\n};\n\ntemplate <typename Device, typename T, typename U>\nclass FusedBatchNormGradOpV3 : public FusedBatchNormGradOpBase<Device, T, U> {\n public:\n  explicit FusedBatchNormGradOpV3(OpKernelConstruction* context)\n      : FusedBatchNormGradOpBase<Device, T, U>(context) {}\n\n  void Compute(OpKernelContext* context) override {\n    FusedBatchNormGradOpBase<Device, T, U>::ComputeWithReservedSpace(context,\n                                                                     true);\n  }\n};\n\nREGISTER_KERNEL_BUILDER(\n    Name(\"FusedBatchNorm\").Device(DEVICE_CPU).TypeConstraint<float>(\"T\"),\n    FusedBatchNormOp<CPUDevice, float, float>);\n\nREGISTER_KERNEL_BUILDER(\n    Name(\"FusedBatchNormGrad\").Device(DEVICE_CPU).TypeConstraint<float>(\"T\"),\n    FusedBatchNormGradOp<CPUDevice, float, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"FusedBatchNormV2\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<float>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormOp<CPUDevice, float, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"FusedBatchNormGradV2\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<float>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormGradOp<CPUDevice, float, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"FusedBatchNormV2\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<Eigen::half>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormOp<CPUDevice, Eigen::half, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"FusedBatchNormGradV2\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<Eigen::half>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormGradOp<CPUDevice, Eigen::half, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"FusedBatchNormV3\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<float>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormOpV3<CPUDevice, float, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"FusedBatchNormGradV3\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<float>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormGradOpV3<CPUDevice, float, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"FusedBatchNormV3\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<Eigen::half>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormOpV3<CPUDevice, Eigen::half, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"FusedBatchNormGradV3\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<Eigen::half>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormGradOpV3<CPUDevice, Eigen::half, float>);\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\nREGISTER_KERNEL_BUILDER(\n    Name(\"FusedBatchNorm\").Device(DEVICE_GPU).TypeConstraint<float>(\"T\"),\n    FusedBatchNormOp<GPUDevice, float, float>);\n\nREGISTER_KERNEL_BUILDER(\n    Name(\"FusedBatchNormGrad\").Device(DEVICE_GPU).TypeConstraint<float>(\"T\"),\n    FusedBatchNormGradOp<GPUDevice, float, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"FusedBatchNormV2\")\n                            .Device(DEVICE_GPU)\n                            .TypeConstraint<float>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormOp<GPUDevice, float, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"FusedBatchNormGradV2\")\n                            .Device(DEVICE_GPU)\n                            .TypeConstraint<float>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormGradOp<GPUDevice, float, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"FusedBatchNormV2\")\n                            .Device(DEVICE_GPU)\n                            .TypeConstraint<Eigen::half>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormOp<GPUDevice, Eigen::half, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"FusedBatchNormGradV2\")\n                            .Device(DEVICE_GPU)\n                            .TypeConstraint<Eigen::half>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormGradOp<GPUDevice, Eigen::half, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"FusedBatchNormV3\")\n                            .Device(DEVICE_GPU)\n                            .TypeConstraint<float>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormOpV3<GPUDevice, float, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"_FusedBatchNormEx\")\n                            .Device(DEVICE_GPU)\n                            .TypeConstraint<float>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormOpEx<GPUDevice, float, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"FusedBatchNormGradV3\")\n                            .Device(DEVICE_GPU)\n                            .TypeConstraint<float>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormGradOpV3<GPUDevice, float, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"FusedBatchNormV3\")\n                            .Device(DEVICE_GPU)\n                            .TypeConstraint<Eigen::half>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormOpV3<GPUDevice, Eigen::half, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"_FusedBatchNormEx\")\n                            .Device(DEVICE_GPU)\n                            .TypeConstraint<Eigen::half>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormOpEx<GPUDevice, Eigen::half, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"FusedBatchNormGradV3\")\n                            .Device(DEVICE_GPU)\n                            .TypeConstraint<Eigen::half>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormGradOpV3<GPUDevice, Eigen::half, float>);\n\n#endif\n\n}  // namespace tensorflow\n"], "fixing_code": ["/* Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include <atomic>\n\n#define EIGEN_USE_THREADS\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n#define EIGEN_USE_GPU\n#if GOOGLE_CUDA\n#include \"third_party/gpus/cudnn/cudnn.h\"\n#endif  // GOOGLE_CUDA\n\n#include \"tensorflow/core/kernels/conv_2d.h\"\n#include \"tensorflow/core/platform/stream_executor.h\"\n#include \"tensorflow/core/util/stream_executor_util.h\"\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/register_types.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/tensor_types.h\"\n#include \"tensorflow/core/kernels/fill_functor.h\"\n#include \"tensorflow/core/kernels/fused_batch_norm_op.h\"\n#include \"tensorflow/core/kernels/redux_functor.h\"\n#include \"tensorflow/core/kernels/transpose_functor.h\"\n#include \"tensorflow/core/lib/core/blocking_counter.h\"\n#include \"tensorflow/core/util/env_var.h\"\n#include \"tensorflow/core/util/tensor_format.h\"\n\nnamespace tensorflow {\nusing CPUDevice = Eigen::ThreadPoolDevice;\nusing GPUDevice = Eigen::GpuDevice;\n\nnamespace functor {\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\nusing se::DeviceMemory;\nusing se::ScratchAllocator;\nusing se::Stream;\nusing se::port::StatusOr;\n#endif\n\nstring ToString(FusedBatchNormActivationMode activation_mode) {\n  switch (activation_mode) {\n    case FusedBatchNormActivationMode::kIdentity:\n      return \"Identity\";\n    case FusedBatchNormActivationMode::kRelu:\n      return \"Relu\";\n  }\n}\n\nStatus ParseActivationMode(OpKernelConstruction* context,\n                           FusedBatchNormActivationMode* activation_mode) {\n  string activation_mode_str;\n  TF_RETURN_IF_ERROR(context->GetAttr(\"activation_mode\", &activation_mode_str));\n\n  if (activation_mode_str == \"Identity\") {\n    *activation_mode = FusedBatchNormActivationMode::kIdentity;\n    return Status::OK();\n  }\n  if (activation_mode_str == \"Relu\") {\n    *activation_mode = FusedBatchNormActivationMode::kRelu;\n    return Status::OK();\n  }\n  return errors::InvalidArgument(\"Unsupported activation mode: \",\n                                 activation_mode_str);\n}\n\n// Functor used by FusedBatchNormOp to do the computations.\ntemplate <typename Device, typename T, typename U, bool is_training>\nstruct FusedBatchNorm;\n// Functor used by FusedBatchNormGradOp to do the computations when\n// is_training=True.\ntemplate <typename Device, typename T, typename U>\nstruct FusedBatchNormGrad;\n\ntemplate <typename T, typename U>\nstruct FusedBatchNorm<CPUDevice, T, U, /* is_training= */ true> {\n  void operator()(OpKernelContext* context, const Tensor& x_input,\n                  const Tensor& scale_input, const Tensor& offset_input,\n                  const Tensor& running_mean_input,\n                  const Tensor& running_variance_input,\n                  const Tensor* side_input, U epsilon, U exponential_avg_factor,\n                  FusedBatchNormActivationMode activation_mode,\n                  Tensor* y_output, Tensor* running_mean_output,\n                  Tensor* running_var_output, Tensor* saved_batch_mean_output,\n                  Tensor* saved_batch_var_output, TensorFormat tensor_format,\n                  bool use_reserved_space) {\n    OP_REQUIRES(context, side_input == nullptr,\n                errors::Internal(\n                    \"The CPU implementation of FusedBatchNorm does not support \"\n                    \"side input.\"));\n    OP_REQUIRES(context,\n                activation_mode == FusedBatchNormActivationMode::kIdentity,\n                errors::Internal(\"The CPU implementation of FusedBatchNorm \"\n                                 \"does not support activations.\"));\n\n    if (use_reserved_space) {\n      Tensor* dummy_reserve_space = nullptr;\n      OP_REQUIRES_OK(context,\n                     context->allocate_output(5, {}, &dummy_reserve_space));\n      // Initialize the memory, to avoid sanitizer alerts.\n      dummy_reserve_space->flat<U>()(0) = U();\n    }\n    Tensor transformed_x;\n    Tensor transformed_y;\n    if (tensor_format == FORMAT_NCHW) {\n      const int64 in_batch = GetTensorDim(x_input, tensor_format, 'N');\n      const int64 in_rows = GetTensorDim(x_input, tensor_format, 'H');\n      const int64 in_cols = GetTensorDim(x_input, tensor_format, 'W');\n      const int64 in_depths = GetTensorDim(x_input, tensor_format, 'C');\n      OP_REQUIRES_OK(context, context->allocate_temp(\n                                  DataTypeToEnum<T>::value,\n                                  ShapeFromFormat(FORMAT_NHWC, in_batch,\n                                                  in_rows, in_cols, in_depths),\n                                  &transformed_x));\n      OP_REQUIRES_OK(context, context->allocate_temp(\n                                  DataTypeToEnum<T>::value,\n                                  ShapeFromFormat(FORMAT_NHWC, in_batch,\n                                                  in_rows, in_cols, in_depths),\n                                  &transformed_y));\n      // Perform NCHW to NHWC\n      std::vector<int32> perm = {0, 2, 3, 1};\n      OP_REQUIRES_OK(\n          context, ::tensorflow::DoTranspose(context->eigen_device<CPUDevice>(),\n                                             x_input, perm, &transformed_x));\n    } else {\n      transformed_x = x_input;\n      transformed_y = *y_output;\n    }\n    typename TTypes<T, 4>::Tensor x(transformed_x.tensor<T, 4>());\n    typename TTypes<U>::ConstVec scale(scale_input.vec<U>());\n    typename TTypes<U>::ConstVec offset(offset_input.vec<U>());\n    typename TTypes<U>::ConstVec old_mean(running_mean_input.vec<U>());\n    typename TTypes<U>::ConstVec old_variance(running_variance_input.vec<U>());\n    typename TTypes<T, 4>::Tensor y(transformed_y.tensor<T, 4>());\n    typename TTypes<U>::Vec new_mean(running_mean_output->vec<U>());\n    typename TTypes<U>::Vec new_variance(running_var_output->vec<U>());\n    typename TTypes<U>::Vec saved_batch_mean(saved_batch_mean_output->vec<U>());\n    typename TTypes<U>::Vec saved_batch_var(saved_batch_var_output->vec<U>());\n\n    const CPUDevice& d = context->eigen_device<CPUDevice>();\n\n    const int depth = x.dimension(3);\n    const int size = x.size();\n    const int rest_size = size / depth;\n    Eigen::DSizes<Eigen::Index, 2> rest_by_depth(rest_size, depth);\n\n#if !defined(EIGEN_HAS_INDEX_LIST)\n    Eigen::DSizes<Eigen::Index, 2> one_by_depth(1, depth);\n    Eigen::array<int, 1> reduce_dims({0});\n    Eigen::array<int, 2> bcast_spec({rest_size, 1});\n#else\n    Eigen::IndexList<Eigen::type2index<1>, Eigen::Index> one_by_depth;\n    one_by_depth.set(1, depth);\n    Eigen::IndexList<Eigen::type2index<0>> reduce_dims;\n    Eigen::IndexList<Eigen::Index, Eigen::type2index<1>> bcast_spec;\n    bcast_spec.set(0, rest_size);\n#endif\n\n    auto x_rest_by_depth = x.reshape(rest_by_depth).template cast<U>();\n    const int rest_size_minus_one = (rest_size > 1) ? (rest_size - 1) : 1;\n    U rest_size_inv = static_cast<U>(1.0f / static_cast<U>(rest_size));\n    // This adjustment is for Bessel's correction\n    U rest_size_adjust =\n        static_cast<U>(rest_size) / static_cast<U>(rest_size_minus_one);\n\n    Eigen::Tensor<U, 1, Eigen::RowMajor> batch_mean(depth);\n    Eigen::Tensor<U, 1, Eigen::RowMajor> batch_variance(depth);\n\n    batch_mean.device(d) = (x_rest_by_depth.sum(reduce_dims) * rest_size_inv);\n    auto x_centered = x_rest_by_depth -\n                      batch_mean.reshape(one_by_depth).broadcast(bcast_spec);\n\n    batch_variance.device(d) =\n        x_centered.square().sum(reduce_dims) * rest_size_inv;\n    auto scaling_factor = ((batch_variance + epsilon).rsqrt() * scale)\n                              .eval()\n                              .reshape(one_by_depth)\n                              .broadcast(bcast_spec);\n    auto x_scaled = x_centered * scaling_factor;\n    auto x_shifted =\n        (x_scaled + offset.reshape(one_by_depth).broadcast(bcast_spec))\n            .template cast<T>();\n\n    y.reshape(rest_by_depth).device(d) = x_shifted;\n    if (exponential_avg_factor == U(1.0)) {\n      saved_batch_var.device(d) = batch_variance;\n      saved_batch_mean.device(d) = batch_mean;\n      new_variance.device(d) = batch_variance * rest_size_adjust;\n      new_mean.device(d) = batch_mean;\n    } else {\n      U one_minus_factor = U(1) - exponential_avg_factor;\n      saved_batch_var.device(d) = batch_variance;\n      saved_batch_mean.device(d) = batch_mean;\n      new_variance.device(d) =\n          one_minus_factor * old_variance +\n          (exponential_avg_factor * rest_size_adjust) * batch_variance;\n      new_mean.device(d) =\n          one_minus_factor * old_mean + exponential_avg_factor * batch_mean;\n    }\n\n    if (tensor_format == FORMAT_NCHW) {\n      // Perform NHWC to NCHW\n      const std::vector<int32> perm = {0, 3, 1, 2};\n      const Status s = ::tensorflow::DoTranspose(\n          context->eigen_device<CPUDevice>(), transformed_y, perm, y_output);\n      if (!s.ok()) {\n        context->SetStatus(errors::InvalidArgument(\"Transpose failed: \", s));\n      }\n    }\n  }\n};\n\ntemplate <typename T, typename U>\nstruct FusedBatchNorm<CPUDevice, T, U, /* is_training= */ false> {\n  void operator()(OpKernelContext* context, const Tensor& x_input,\n                  const Tensor& scale_input, const Tensor& offset_input,\n                  const Tensor& estimated_mean_input,\n                  const Tensor& estimated_variance_input,\n                  const Tensor* side_input, U epsilon, U exponential_avg_factor,\n                  FusedBatchNormActivationMode activation_mode,\n                  Tensor* y_output, Tensor* batch_mean_output,\n                  Tensor* batch_var_output, Tensor* saved_mean_output,\n                  Tensor* saved_var_output, TensorFormat tensor_format,\n                  bool use_reserved_space) {\n    OP_REQUIRES(context, side_input == nullptr,\n                errors::Internal(\n                    \"The CPU implementation of FusedBatchNorm does not support \"\n                    \"side input.\"));\n    OP_REQUIRES(context,\n                activation_mode == FusedBatchNormActivationMode::kIdentity,\n                errors::Internal(\"The CPU implementation of FusedBatchNorm \"\n                                 \"does not support activations.\"));\n\n    if (use_reserved_space) {\n      Tensor* dummy_reserve_space = nullptr;\n      OP_REQUIRES_OK(context,\n                     context->allocate_output(5, {}, &dummy_reserve_space));\n      // Initialize the memory, to avoid sanitizer alerts.\n      dummy_reserve_space->flat<U>()(0) = U();\n    }\n    Tensor transformed_x;\n    Tensor transformed_y;\n    if (tensor_format == FORMAT_NCHW) {\n      const int64 in_batch = GetTensorDim(x_input, tensor_format, 'N');\n      const int64 in_rows = GetTensorDim(x_input, tensor_format, 'H');\n      const int64 in_cols = GetTensorDim(x_input, tensor_format, 'W');\n      const int64 in_depths = GetTensorDim(x_input, tensor_format, 'C');\n      OP_REQUIRES_OK(context, context->allocate_temp(\n                                  DataTypeToEnum<T>::value,\n                                  ShapeFromFormat(FORMAT_NHWC, in_batch,\n                                                  in_rows, in_cols, in_depths),\n                                  &transformed_x));\n      OP_REQUIRES_OK(context, context->allocate_temp(\n                                  DataTypeToEnum<T>::value,\n                                  ShapeFromFormat(FORMAT_NHWC, in_batch,\n                                                  in_rows, in_cols, in_depths),\n                                  &transformed_y));\n      // Perform NCHW to NHWC\n      std::vector<int32> perm = {0, 2, 3, 1};\n      OP_REQUIRES_OK(\n          context, ::tensorflow::DoTranspose(context->eigen_device<CPUDevice>(),\n                                             x_input, perm, &transformed_x));\n    } else {\n      transformed_x = x_input;\n      transformed_y = *y_output;\n    }\n    typename TTypes<T, 4>::Tensor x(transformed_x.tensor<T, 4>());\n    typename TTypes<U>::ConstVec scale(scale_input.vec<U>());\n    typename TTypes<U>::ConstVec offset(offset_input.vec<U>());\n    typename TTypes<U>::ConstVec estimated_mean(estimated_mean_input.vec<U>());\n    typename TTypes<U>::ConstVec estimated_variance(\n        estimated_variance_input.vec<U>());\n    typename TTypes<T, 4>::Tensor y(transformed_y.tensor<T, 4>());\n    typename TTypes<U>::Vec batch_mean(batch_mean_output->vec<U>());\n    typename TTypes<U>::Vec batch_variance(batch_var_output->vec<U>());\n\n    const CPUDevice& d = context->eigen_device<CPUDevice>();\n\n    const int depth = x.dimension(3);\n    OP_REQUIRES(\n        context, depth != 0,\n        errors::Internal(\"The 4th element in the input shape cannot be 0.\"));\n    const int size = x.size();\n    const int rest_size = size / depth;\n    Eigen::DSizes<Eigen::Index, 2> rest_by_depth(rest_size, depth);\n\n#if !defined(EIGEN_HAS_INDEX_LIST)\n    Eigen::DSizes<Eigen::Index, 2> one_by_depth(1, depth);\n    Eigen::array<int, 1> reduce_dims({0});\n    Eigen::array<int, 2> bcast_spec({rest_size, 1});\n#else\n    Eigen::IndexList<Eigen::type2index<1>, Eigen::Index> one_by_depth;\n    one_by_depth.set(1, depth);\n    Eigen::IndexList<Eigen::Index, Eigen::type2index<1>> bcast_spec;\n    bcast_spec.set(0, rest_size);\n#endif\n\n    auto x_rest_by_depth = x.reshape(rest_by_depth).template cast<U>();\n    auto x_centered =\n        x_rest_by_depth -\n        estimated_mean.reshape(one_by_depth).broadcast(bcast_spec);\n    auto scaling_factor = ((estimated_variance + epsilon).rsqrt() * scale)\n                              .eval()\n                              .reshape(one_by_depth)\n                              .broadcast(bcast_spec);\n    auto x_scaled = x_centered * scaling_factor;\n    auto x_shifted =\n        (x_scaled + offset.reshape(one_by_depth).broadcast(bcast_spec))\n            .template cast<T>();\n\n    y.reshape(rest_by_depth).device(d) = x_shifted;\n    batch_mean.device(d) = estimated_mean;\n    batch_variance.device(d) = estimated_variance;\n\n    if (tensor_format == FORMAT_NCHW) {\n      // Perform NHWC to NCHW\n      const std::vector<int32> perm = {0, 3, 1, 2};\n      const Status s = ::tensorflow::DoTranspose(\n          context->eigen_device<CPUDevice>(), transformed_y, perm, y_output);\n      if (!s.ok()) {\n        context->SetStatus(errors::InvalidArgument(\"Transpose failed: \", s));\n      }\n    }\n  }\n};\n\ntemplate <typename T, typename U>\nstruct FusedBatchNormGrad<CPUDevice, T, U> {\n  void operator()(OpKernelContext* context, const Tensor& y_backprop_input,\n                  const Tensor& x_input, const Tensor& scale_input,\n                  const Tensor& mean_input, const Tensor& variance_input,\n                  U epsilon, Tensor* x_backprop_output,\n                  Tensor* scale_backprop_output, Tensor* offset_backprop_output,\n                  bool use_reserved_space, TensorFormat tensor_format) {\n    Tensor transformed_y_backprop_input;\n    Tensor transformed_x_input;\n    Tensor transformed_x_backprop_output;\n    if (tensor_format == FORMAT_NCHW) {\n      const int64 in_batch = GetTensorDim(x_input, tensor_format, 'N');\n      const int64 in_rows = GetTensorDim(x_input, tensor_format, 'H');\n      const int64 in_cols = GetTensorDim(x_input, tensor_format, 'W');\n      const int64 in_depths = GetTensorDim(x_input, tensor_format, 'C');\n      OP_REQUIRES_OK(context, context->allocate_temp(\n                                  DataTypeToEnum<T>::value,\n                                  ShapeFromFormat(FORMAT_NHWC, in_batch,\n                                                  in_rows, in_cols, in_depths),\n                                  &transformed_y_backprop_input));\n      OP_REQUIRES_OK(context, context->allocate_temp(\n                                  DataTypeToEnum<T>::value,\n                                  ShapeFromFormat(FORMAT_NHWC, in_batch,\n                                                  in_rows, in_cols, in_depths),\n                                  &transformed_x_input));\n      OP_REQUIRES_OK(context, context->allocate_temp(\n                                  DataTypeToEnum<T>::value,\n                                  ShapeFromFormat(FORMAT_NHWC, in_batch,\n                                                  in_rows, in_cols, in_depths),\n                                  &transformed_x_backprop_output));\n      // Perform NCHW to NHWC\n      std::vector<int32> perm = {0, 2, 3, 1};\n      OP_REQUIRES_OK(\n          context, ::tensorflow::DoTranspose(context->eigen_device<CPUDevice>(),\n                                             y_backprop_input, perm,\n                                             &transformed_y_backprop_input));\n      OP_REQUIRES_OK(context, ::tensorflow::DoTranspose(\n                                  context->eigen_device<CPUDevice>(), x_input,\n                                  perm, &transformed_x_input));\n    } else {\n      transformed_y_backprop_input = y_backprop_input;\n      transformed_x_input = x_input;\n      transformed_x_backprop_output = *x_backprop_output;\n    }\n    typename TTypes<T, 4>::Tensor y_backprop(\n        transformed_y_backprop_input.tensor<T, 4>());\n    typename TTypes<T, 4>::Tensor x(transformed_x_input.tensor<T, 4>());\n    typename TTypes<U>::ConstVec scale(scale_input.vec<U>());\n    typename TTypes<U>::ConstVec mean(mean_input.vec<U>());\n    typename TTypes<U>::ConstVec variance(variance_input.vec<U>());\n    typename TTypes<T, 4>::Tensor x_backprop(\n        transformed_x_backprop_output.tensor<T, 4>());\n    typename TTypes<U>::Vec offset_backprop(offset_backprop_output->vec<U>());\n\n    // Note: the following formulas are used to compute the gradients for\n    // back propagation.\n    // x_backprop = scale * rsqrt(variance + epsilon) *\n    //              [y_backprop - mean(y_backprop) - (x - mean(x)) *\n    //              mean(y_backprop * (x - mean(x))) / (variance + epsilon)]\n    // scale_backprop = sum(y_backprop *\n    //                  (x - mean(x)) * rsqrt(variance + epsilon))\n    // offset_backprop = sum(y_backprop)\n\n    const CPUDevice& d = context->eigen_device<CPUDevice>();\n    const int depth = x.dimension(3);\n    const int size = x.size();\n    const int rest_size = size / depth;\n    Eigen::DSizes<Eigen::Index, 2> rest_by_depth(rest_size, depth);\n\n#if !defined(EIGEN_HAS_INDEX_LIST)\n    Eigen::DSizes<Eigen::Index, 2> one_by_depth(1, depth);\n    Eigen::array<int, 2> bcast_spec({rest_size, 1});\n#else\n    Eigen::IndexList<Eigen::type2index<1>, Eigen::Index> one_by_depth;\n    one_by_depth.set(1, depth);\n    Eigen::IndexList<Eigen::Index, Eigen::type2index<1>> bcast_spec;\n    bcast_spec.set(0, rest_size);\n#endif\n\n    auto x_rest_by_depth = x.reshape(rest_by_depth).template cast<U>();\n    U rest_size_inv = static_cast<U>(1.0f / static_cast<U>(rest_size));\n\n    // Eigen is notoriously bad at reducing outer dimension, so we materialize\n    // all temporary tensors that require reduction, and then use Eigen redux\n    // functor, that is optimized for this particular task.\n    //\n    // All reductions are of this type: [rest_size, depth] -> [depth].\n    using ScalarSum = Eigen::internal::scalar_sum_op<U>;\n    const functor::ReduceOuterDimensions<T, U, U, ScalarSum> redux_sum_t;\n    const functor::ReduceOuterDimensions<U, U, U, ScalarSum> redux_sum_u;\n\n    auto scratch_dtype = DataTypeToEnum<U>::value;\n\n    // Allocate a temporary workspace of [depth] shape.\n    Tensor scratch_one_by_depth;\n    OP_REQUIRES_OK(context, context->allocate_temp(scratch_dtype, {depth},\n                                                   &scratch_one_by_depth));\n\n    // Maybe allocate a temporary workspace of [rest_size, depth] shape.\n    Tensor scratch_rest_by_depth;\n    if (std::is_same<T, U>::value) {\n      OP_REQUIRES(context,\n                  scratch_rest_by_depth.CopyFrom(transformed_x_backprop_output,\n                                                 {rest_size, depth}),\n                  errors::Internal(\"Failed to copy a tensor\"));\n    } else {\n      OP_REQUIRES_OK(context,\n                     context->allocate_temp(scratch_dtype, {rest_size, depth},\n                                            &scratch_rest_by_depth));\n    }\n\n    typename TTypes<U, 2>::Tensor scratch_tensor(\n        scratch_rest_by_depth.tensor<U, 2>());\n    typename TTypes<U>::Vec scratch_vector(scratch_one_by_depth.vec<U>());\n\n    auto x_mean_rest_by_depth =\n        mean.reshape(one_by_depth).broadcast(bcast_spec);\n    auto x_centered = (x_rest_by_depth - x_mean_rest_by_depth);\n    auto coef0_one_by_depth =\n        (variance.reshape(one_by_depth) + epsilon).rsqrt();\n    auto coef0_rest_by_depth = coef0_one_by_depth.broadcast(bcast_spec);\n    auto x_scaled = x_centered * coef0_rest_by_depth;\n\n    auto y_backprop_rest_by_depth =\n        y_backprop.reshape(rest_by_depth).template cast<U>();\n\n    // Compute `scale_backprop_output`:\n    //   scale_backprop =\n    //     (y_backprop_rest_by_depth * x_scaled).sum(reduce_dims)\n    scratch_tensor.device(d) = y_backprop_rest_by_depth * x_scaled;\n    redux_sum_u(d, rest_by_depth, scratch_rest_by_depth, scale_backprop_output);\n\n    // Compute 'offset_backprop_output':\n    //   offset_backprop =\n    //     y_backprop_rest_by_depth.sum(reduce_dims)\n    redux_sum_t(d, rest_by_depth, transformed_y_backprop_input,\n                offset_backprop_output);\n    auto y_backprop_sum = offset_backprop;\n\n    auto y_backprop_sum_one_by_depth = y_backprop_sum.reshape(one_by_depth);\n    auto y_backprop_mean_one_by_depth =\n        y_backprop_sum_one_by_depth * rest_size_inv;\n    auto y_backprop_mean_rest_by_depth =\n        y_backprop_mean_one_by_depth.broadcast(bcast_spec);\n    auto y_backprop_centered =\n        y_backprop_rest_by_depth - y_backprop_mean_rest_by_depth;\n\n    // Compute expression:\n    //   y_backprop_centered_mean =\n    //     (y_backprop_rest_by_depth * x_centered).mean(reduce_dims)\n    scratch_tensor.device(d) = y_backprop_rest_by_depth * x_centered;\n    redux_sum_u(d, rest_by_depth, scratch_rest_by_depth, &scratch_one_by_depth);\n    auto y_backprop_centered_mean =\n        scratch_vector.reshape(one_by_depth) / static_cast<U>(rest_size);\n\n    auto coef1 = (scale.reshape(one_by_depth) * coef0_one_by_depth)\n                     .broadcast(bcast_spec);\n    auto coef2 = (coef0_one_by_depth.square() * y_backprop_centered_mean)\n                     .broadcast(bcast_spec);\n\n    x_backprop.reshape(rest_by_depth).device(d) =\n        (coef1 * (y_backprop_centered - x_centered * coef2)).template cast<T>();\n\n    if (tensor_format == FORMAT_NCHW) {\n      // Perform NHWC to NCHW\n      std::vector<int32> perm = {0, 3, 1, 2};\n      OP_REQUIRES_OK(\n          context, ::tensorflow::DoTranspose(context->eigen_device<CPUDevice>(),\n                                             transformed_x_backprop_output,\n                                             perm, x_backprop_output));\n    }\n  }\n};\n\ntemplate <typename T, typename U>\nstruct FusedBatchNormFreezeGrad<CPUDevice, T, U> {\n  void operator()(OpKernelContext* context, const Tensor& y_backprop_input,\n                  const Tensor& x_input, const Tensor& scale_input,\n                  const Tensor& pop_mean_input,\n                  const Tensor& pop_variance_input, U epsilon,\n                  Tensor* x_backprop_output, Tensor* scale_backprop_output,\n                  Tensor* offset_backprop_output) {\n    typename TTypes<T, 4>::ConstTensor y_backprop(\n        y_backprop_input.tensor<T, 4>());\n    typename TTypes<T, 4>::ConstTensor input(x_input.tensor<T, 4>());\n    typename TTypes<U>::ConstVec scale(scale_input.vec<U>());\n    typename TTypes<U>::ConstVec pop_mean(pop_mean_input.vec<U>());\n    typename TTypes<U>::ConstVec pop_var(pop_variance_input.vec<U>());\n    typename TTypes<T, 4>::Tensor x_backprop(x_backprop_output->tensor<T, 4>());\n    typename TTypes<U>::Vec scale_backprop(scale_backprop_output->vec<U>());\n\n    const int depth = pop_mean.dimension(0);\n    const int rest_size = input.size() / depth;\n\n    const CPUDevice& d = context->eigen_device<CPUDevice>();\n\n    // Allocate two temporary workspaces of [depth] shape.\n    Tensor scratch1_vec, scratch2_vec;\n    OP_REQUIRES_OK(context, context->allocate_temp(DataTypeToEnum<U>::value,\n                                                   {depth}, &scratch1_vec));\n    OP_REQUIRES_OK(context, context->allocate_temp(DataTypeToEnum<U>::value,\n                                                   {depth}, &scratch2_vec));\n\n    // Maybe allocate a temporary workspace of [rest_size, depth] shape.\n    Tensor scratch3_tensor;\n    if (std::is_same<T, U>::value) {\n      OP_REQUIRES(\n          context,\n          scratch3_tensor.CopyFrom(*x_backprop_output, {rest_size, depth}),\n          errors::Internal(\"Failed to copy a tensor\"));\n    } else {\n      OP_REQUIRES_OK(context, context->allocate_temp(DataTypeToEnum<U>::value,\n                                                     {rest_size, depth},\n                                                     &scratch3_tensor));\n    }\n\n    typename TTypes<U>::Vec scratch1(scratch1_vec.vec<U>());\n    typename TTypes<U>::Vec scratch2(scratch2_vec.vec<U>());\n    typename TTypes<U, 2>::Tensor scratch3(scratch3_tensor.tensor<U, 2>());\n\n    Eigen::DSizes<Eigen::Index, 2> rest_by_depth(rest_size, depth);\n#if !defined(EIGEN_HAS_INDEX_LIST)\n    Eigen::DSizes<Eigen::Index, 2> one_by_depth(1, depth);\n    Eigen::array<int, 2> rest_by_one({rest_size, 1});\n#else\n    Eigen::IndexList<Eigen::type2index<1>, Eigen::Index> one_by_depth;\n    one_by_depth.set(1, depth);\n    Eigen::IndexList<Eigen::Index, Eigen::type2index<1>> rest_by_one;\n    rest_by_one.set(0, rest_size);\n#endif\n\n    // Sum reduction along the 0th dimension using custom CPU functor.\n    using ScalarSum = Eigen::internal::scalar_sum_op<U>;\n    const functor::ReduceOuterDimensions<T, U, U, ScalarSum> redux_sum_t;\n    const functor::ReduceOuterDimensions<U, U, U, ScalarSum> redux_sum_u;\n\n    // offset_backprop  = sum(y_backprop)\n    // scale_backprop = y_backprop * ((x - pop_mean) * rsqrt(pop_var + epsilon))\n    // x_backprop = y_backprop * (scale * rsqrt(pop_var + epsilon))\n\n    // NOTE: DEFAULT DEVICE comment is added to expression assignments that\n    // we don't want to be executed in a thread pool.\n\n    auto y_backprop_rest_by_depth =\n        y_backprop.reshape(rest_by_depth).template cast<U>();\n    auto input_rest_by_depth = input.reshape(rest_by_depth).template cast<U>();\n\n    // offset_backprop  = sum(y_backprop)\n    redux_sum_t(d, rest_by_depth, y_backprop_input, offset_backprop_output);\n\n    // scratch1 = rsqrt(pop_var + epsilon)\n    scratch1 = (pop_var + pop_var.constant(epsilon)).rsqrt();  // DEFAULT DEVICE\n\n    // scratch2 = sum(y_backprop * (x - mean))\n    scratch3.device(d) =\n        y_backprop_rest_by_depth *\n        (input_rest_by_depth -\n         pop_mean.reshape(one_by_depth).broadcast(rest_by_one));\n    redux_sum_u(d, rest_by_depth, scratch3_tensor, &scratch2_vec);\n\n    x_backprop.reshape(rest_by_depth).device(d) =\n        (y_backprop_rest_by_depth *\n         ((scratch1.reshape(one_by_depth) * scale.reshape(one_by_depth))\n              .broadcast(rest_by_one)))\n            .template cast<T>();\n    scale_backprop = scratch2 * scratch1;  // DEFAULT DEVICE\n  }\n};\n\n#if !GOOGLE_CUDA\nnamespace {\n// See implementation under GOOGLE_CUDA #ifdef below.\n// This is a CUDA specific feature, do not enable it for non-CUDA builds\nbool BatchnormSpatialPersistentEnabled() { return false; }\n}  // namespace\n#endif\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\nnamespace {\n\nse::dnn::ActivationMode AsDnnActivationMode(\n    const FusedBatchNormActivationMode activation_mode) {\n  switch (activation_mode) {\n    case FusedBatchNormActivationMode::kIdentity:\n      return se::dnn::ActivationMode::kNone;\n    case FusedBatchNormActivationMode::kRelu:\n      return se::dnn::ActivationMode::kRelu;\n  }\n}\n\n#if GOOGLE_CUDA\n// NOTE(ezhulenev): See `BatchnormSpatialPersistentEnabled` documentation in the\n// `cuda_dnn.cc` for details.\nbool BatchnormSpatialPersistentEnabled() {\n#if CUDNN_VERSION >= 7402\n  static bool is_enabled = [] {\n    bool is_enabled = false;\n    TF_CHECK_OK(tensorflow::ReadBoolFromEnvVar(\n        \"TF_USE_CUDNN_BATCHNORM_SPATIAL_PERSISTENT\",\n        /*default_val=*/false, &is_enabled));\n    return is_enabled;\n  }();\n  return is_enabled;\n#else\n  return false;\n#endif\n}\n#endif\n\n}  // namespace\n\ntemplate <typename U, typename T>\nDeviceMemory<U> CastDeviceMemory(Tensor* tensor) {\n  return DeviceMemory<U>::MakeFromByteSize(\n      tensor->template flat<T>().data(),\n      tensor->template flat<T>().size() * sizeof(T));\n}\n\n// A helper to allocate temporary scratch memory for Cudnn BatchNormEx ops. It\n// takes the ownership of the underlying memory. The expectation is that the\n// memory should be alive for the span of the Cudnn BatchNormEx itself.\ntemplate <typename T>\nclass CudnnBatchNormAllocatorInTemp : public ScratchAllocator {\n public:\n  ~CudnnBatchNormAllocatorInTemp() override = default;\n\n  explicit CudnnBatchNormAllocatorInTemp(OpKernelContext* context)\n      : context_(context) {}\n\n  int64 GetMemoryLimitInBytes() override {\n    return std::numeric_limits<int64>::max();\n  }\n\n  StatusOr<DeviceMemory<uint8>> AllocateBytes(int64 byte_size) override {\n    Tensor temporary_memory;\n    const DataType tf_data_type = DataTypeToEnum<T>::v();\n    int64 allocate_count =\n        Eigen::divup(byte_size, static_cast<int64>(sizeof(T)));\n    Status allocation_status(context_->allocate_temp(\n        tf_data_type, TensorShape({allocate_count}), &temporary_memory));\n    if (!allocation_status.ok()) {\n      return allocation_status;\n    }\n    // Hold the reference of the allocated tensors until the end of the\n    // allocator.\n    allocated_tensors_.push_back(temporary_memory);\n    total_byte_size_ += byte_size;\n    return DeviceMemory<uint8>::MakeFromByteSize(\n        temporary_memory.template flat<T>().data(),\n        temporary_memory.template flat<T>().size() * sizeof(T));\n  }\n\n  int64 TotalByteSize() const { return total_byte_size_; }\n\n  Tensor get_allocated_tensor(int index) const {\n    return allocated_tensors_[index];\n  }\n\n private:\n  int64 total_byte_size_ = 0;\n  OpKernelContext* context_;  // not owned\n  std::vector<Tensor> allocated_tensors_;\n};\n\n// A helper to allocate memory for Cudnn BatchNormEx as a kernel output. It is\n// used by forward pass kernel to feed the output to the backward pass.\n// The memory is expected to live long enough after the backward pass is\n// finished.\ntemplate <typename T>\nclass CudnnBatchNormAllocatorInOutput : public ScratchAllocator {\n public:\n  ~CudnnBatchNormAllocatorInOutput() override {\n    if (!output_allocated) {\n      Tensor* dummy_reserve_space = nullptr;\n      OP_REQUIRES_OK(context_, context_->allocate_output(output_index_, {},\n                                                         &dummy_reserve_space));\n    }\n  }\n\n  CudnnBatchNormAllocatorInOutput(OpKernelContext* context, int output_index)\n      : context_(context), output_index_(output_index) {}\n\n  int64 GetMemoryLimitInBytes() override {\n    return std::numeric_limits<int64>::max();\n  }\n\n  StatusOr<DeviceMemory<uint8>> AllocateBytes(int64 byte_size) override {\n    output_allocated = true;\n    DCHECK(total_byte_size_ == 0)\n        << \"Reserve space allocator can only be called once\";\n    int64 allocate_count =\n        Eigen::divup(byte_size, static_cast<int64>(sizeof(T)));\n\n    Tensor* temporary_memory = nullptr;\n    Status allocation_status(context_->allocate_output(\n        output_index_, TensorShape({allocate_count}), &temporary_memory));\n    if (!allocation_status.ok()) {\n      return allocation_status;\n    }\n    total_byte_size_ += byte_size;\n    auto memory_uint8 = DeviceMemory<uint8>::MakeFromByteSize(\n        temporary_memory->template flat<T>().data(),\n        temporary_memory->template flat<T>().size() * sizeof(T));\n    return StatusOr<DeviceMemory<uint8>>(memory_uint8);\n  }\n\n  int64 TotalByteSize() { return total_byte_size_; }\n\n private:\n  int64 total_byte_size_ = 0;\n  OpKernelContext* context_;  // not owned\n  int output_index_;\n  bool output_allocated = false;\n};\n\ntemplate <typename T, typename U, bool is_training>\nstruct FusedBatchNorm<GPUDevice, T, U, is_training> {\n  void operator()(OpKernelContext* context, const Tensor& x,\n                  const Tensor& scale, const Tensor& offset,\n                  const Tensor& estimated_mean,\n                  const Tensor& estimated_variance, const Tensor* side_input,\n                  U epsilon, U exponential_avg_factor,\n                  FusedBatchNormActivationMode activation_mode, Tensor* y,\n                  Tensor* batch_mean, Tensor* batch_var, Tensor* saved_mean,\n                  Tensor* saved_inv_var, TensorFormat tensor_format,\n                  bool use_reserved_space) {\n    auto* stream = context->op_device_context()->stream();\n    OP_REQUIRES(context, stream, errors::Internal(\"No GPU stream available\"));\n\n    const int64 batch_size = GetTensorDim(x, tensor_format, 'N');\n    const int64 channels = GetTensorDim(x, tensor_format, 'C');\n    const int64 height = GetTensorDim(x, tensor_format, 'H');\n    const int64 width = GetTensorDim(x, tensor_format, 'W');\n\n    // If use_reserved_space we have reserve_space_3 output (only in\n    // FusedBatchNormV3 op).\n\n#if GOOGLE_CUDA\n    // Check if cuDNN batch normalization has a fast NHWC implementation:\n    //   (1) In inference mode it's always fast.\n    //   (2) Tensorflow enabled batchnorm spatial persistence, we are called\n    //   from\n    //       FusedBatchNormV3, i.e. use_reserved_space is true.\n    const bool fast_nhwc_batch_norm =\n        !is_training ||\n        (BatchnormSpatialPersistentEnabled() &&\n         DataTypeToEnum<T>::value == DT_HALF && use_reserved_space);\n#else\n    // fast NHWC implementation is a CUDA only feature\n    const bool fast_nhwc_batch_norm = false;\n#endif\n\n    // If input tensor is in NHWC format, and we have a fast cuDNN\n    // implementation, there is no need to do data format conversion.\n    TensorFormat compute_format =\n        fast_nhwc_batch_norm && tensor_format == FORMAT_NHWC ? FORMAT_NHWC\n                                                             : FORMAT_NCHW;\n\n    VLOG(2) << \"FusedBatchNorm:\"\n            << \" batch_size: \" << batch_size << \" channels: \" << channels\n            << \" height: \" << height << \" width:\" << width\n            << \" x shape: \" << x.shape().DebugString()\n            << \" scale shape: \" << scale.shape().DebugString()\n            << \" offset shape: \" << offset.shape().DebugString()\n            << \" activation mode: \" << ToString(activation_mode)\n            << \" tensor format: \" << ToString(tensor_format)\n            << \" compute format: \" << ToString(compute_format);\n\n    auto maybe_make_dummy_output = [context, use_reserved_space]() -> Status {\n      if (use_reserved_space) {\n        Tensor* dummy_reserve_space = nullptr;\n        return context->allocate_output(5, {}, &dummy_reserve_space);\n      }\n      return Status::OK();\n    };\n\n    // If input is empty, return NaN mean/variance\n    if (x.shape().num_elements() == 0) {\n      OP_REQUIRES_OK(context, maybe_make_dummy_output());\n      functor::SetNanFunctor<U> f;\n      f(context->eigen_device<GPUDevice>(), batch_mean->flat<U>());\n      f(context->eigen_device<GPUDevice>(), batch_var->flat<U>());\n      return;\n    }\n\n    // In inference mode we use custom CUDA kernel, because cuDNN does not\n    // support side input and activations for inference.\n    const bool has_side_input = side_input != nullptr;\n    const bool has_activation =\n        activation_mode != FusedBatchNormActivationMode::kIdentity;\n\n    if (!is_training && (has_side_input || has_activation)) {\n      OP_REQUIRES_OK(context, maybe_make_dummy_output());\n      FusedBatchNormInferenceFunctor<GPUDevice, T, U> inference_functor;\n\n      if (has_side_input) {\n        inference_functor(context, tensor_format, x.tensor<T, 4>(),\n                          scale.vec<U>(), offset.vec<U>(),\n                          estimated_mean.vec<U>(), estimated_variance.vec<U>(),\n                          side_input->tensor<T, 4>(), epsilon, activation_mode,\n                          y->tensor<T, 4>());\n      } else {\n        typename TTypes<T, 4>::ConstTensor empty_tensor(nullptr, 0, 0, 0, 0);\n        inference_functor(context, tensor_format, x.tensor<T, 4>(),\n                          scale.vec<U>(), offset.vec<U>(),\n                          estimated_mean.vec<U>(), estimated_variance.vec<U>(),\n                          empty_tensor, epsilon, activation_mode,\n                          y->tensor<T, 4>());\n      }\n      return;\n    }\n\n    Tensor x_maybe_transformed = x;\n    Tensor x_transformed;\n    Tensor y_transformed;\n    se::DeviceMemory<T> y_ptr;\n\n    if (tensor_format == compute_format) {\n      y_ptr = StreamExecutorUtil::AsDeviceMemory<T>(*y);\n    } else if (tensor_format == FORMAT_NHWC && compute_format == FORMAT_NCHW) {\n      OP_REQUIRES_OK(context, context->allocate_temp(\n                                  DataTypeToEnum<T>::value,\n                                  ShapeFromFormat(compute_format, batch_size,\n                                                  height, width, channels),\n                                  &x_transformed));\n      functor::NHWCToNCHW<GPUDevice, T, 4>()(\n          context->eigen_device<GPUDevice>(),\n          const_cast<const Tensor&>(x_maybe_transformed).tensor<T, 4>(),\n          x_transformed.tensor<T, 4>());\n      x_maybe_transformed = x_transformed;\n\n      OP_REQUIRES_OK(context, context->allocate_temp(\n                                  DataTypeToEnum<T>::value,\n                                  ShapeFromFormat(compute_format, batch_size,\n                                                  height, width, channels),\n                                  &y_transformed));\n      y_ptr = StreamExecutorUtil::AsDeviceMemory<T>(y_transformed);\n    } else {\n      context->SetStatus(errors::Internal(\n          \"Unsupported tensor format: \", ToString(tensor_format),\n          \" and compute format: \", ToString(compute_format)));\n      return;\n    }\n\n    const se::dnn::DataLayout data_layout =\n        compute_format == FORMAT_NHWC ? se::dnn::DataLayout::kBatchYXDepth\n                                      : se::dnn::DataLayout::kBatchDepthYX;\n\n    se::dnn::BatchDescriptor x_desc;\n    x_desc.set_count(batch_size)\n        .set_feature_map_count(channels)\n        .set_height(height)\n        .set_width(width)\n        .set_layout(data_layout);\n\n    se::dnn::BatchDescriptor scale_offset_desc;\n    scale_offset_desc.set_count(1)\n        .set_feature_map_count(channels)\n        .set_height(1)\n        .set_width(1)\n        .set_layout(se::dnn::DataLayout::kBatchDepthYX);\n\n    auto x_ptr = StreamExecutorUtil::AsDeviceMemory<T>(x_maybe_transformed);\n    auto scale_ptr = StreamExecutorUtil::AsDeviceMemory<U>(scale);\n    auto offset_ptr = StreamExecutorUtil::AsDeviceMemory<U>(offset);\n    auto estimated_mean_ptr =\n        StreamExecutorUtil::AsDeviceMemory<U>(estimated_mean);\n    auto estimated_variance_ptr =\n        StreamExecutorUtil::AsDeviceMemory<U>(estimated_variance);\n    auto side_input_ptr =\n        side_input != nullptr\n            ? StreamExecutorUtil::AsDeviceMemory<U>(*side_input)\n            : se::DeviceMemory<U>();\n    auto batch_mean_ptr = StreamExecutorUtil::AsDeviceMemory<U>(*batch_mean);\n\n    auto batch_var_ptr = StreamExecutorUtil::AsDeviceMemory<U>(*batch_var);\n    auto saved_mean_ptr = StreamExecutorUtil::AsDeviceMemory<U>(*saved_mean);\n    auto saved_inv_var_ptr =\n        StreamExecutorUtil::AsDeviceMemory<U>(*saved_inv_var);\n\n    std::unique_ptr<functor::CudnnBatchNormAllocatorInOutput<U>>\n        reserve_space_allocator;\n    std::unique_ptr<functor::CudnnBatchNormAllocatorInTemp<uint8>>\n        workspace_allocator;\n    if (use_reserved_space) {\n      reserve_space_allocator.reset(\n          new functor::CudnnBatchNormAllocatorInOutput<U>(context, 5));\n      workspace_allocator.reset(\n          new functor::CudnnBatchNormAllocatorInTemp<uint8>(context));\n    }\n    if (!batch_mean->SharesBufferWith(estimated_mean) &&\n        exponential_avg_factor != 1.0f) {\n      OP_REQUIRES(\n          context,\n          stream\n              ->ThenMemcpyD2D(&batch_mean_ptr, estimated_mean_ptr,\n                              estimated_mean.NumElements() * sizeof(U))\n              .ok(),\n          errors::Internal(\"MatrixTriangularSolveOp: failed to copy rhs \"\n                           \"from device\"));\n    }\n    if (!batch_var->SharesBufferWith(estimated_variance) &&\n        exponential_avg_factor != 1.0f) {\n      OP_REQUIRES(\n          context,\n          stream\n              ->ThenMemcpyD2D(&batch_var_ptr, estimated_variance_ptr,\n                              estimated_variance.NumElements() * sizeof(U))\n              .ok(),\n          errors::Internal(\"MatrixTriangularSolveOp: failed to copy rhs \"\n                           \"from device\"));\n    }\n    bool cudnn_launch_status =\n        stream\n            ->ThenBatchNormalizationForward(\n                x_ptr, scale_ptr, offset_ptr, estimated_mean_ptr,\n                estimated_variance_ptr, side_input_ptr, x_desc,\n                scale_offset_desc, static_cast<double>(epsilon),\n                static_cast<double>(exponential_avg_factor),\n                AsDnnActivationMode(activation_mode), &y_ptr, &batch_mean_ptr,\n                &batch_var_ptr, &saved_mean_ptr, &saved_inv_var_ptr,\n                is_training, reserve_space_allocator.get(),\n                workspace_allocator.get())\n            .ok();\n\n    if (!cudnn_launch_status) {\n      context->SetStatus(\n          errors::Internal(\"cuDNN launch failure : input shape (\",\n                           x.shape().DebugString(), \")\"));\n      return;\n    }\n\n    if (tensor_format == FORMAT_NHWC && compute_format == FORMAT_NCHW) {\n      functor::NCHWToNHWC<GPUDevice, T, 4>()(\n          context->eigen_device<GPUDevice>(),\n          const_cast<const Tensor&>(y_transformed).tensor<T, 4>(),\n          y->tensor<T, 4>());\n    }\n  }\n};\n\ntemplate <typename T, typename U>\nstruct FusedBatchNormGrad<GPUDevice, T, U> {\n  void operator()(OpKernelContext* context, const Tensor& y_backprop,\n                  const Tensor& x, const Tensor& scale, const Tensor& mean,\n                  const Tensor& inv_variance, U epsilon, Tensor* x_backprop,\n                  Tensor* scale_backprop, Tensor* offset_backprop,\n                  bool use_reserved_space, TensorFormat tensor_format) {\n    auto* stream = context->op_device_context()->stream();\n    OP_REQUIRES(context, stream, errors::Internal(\"No GPU stream available\"));\n\n    const int64 batch_size = GetTensorDim(x, tensor_format, 'N');\n    const int64 channels = GetTensorDim(x, tensor_format, 'C');\n    const int64 height = GetTensorDim(x, tensor_format, 'H');\n    const int64 width = GetTensorDim(x, tensor_format, 'W');\n\n#if GOOGLE_CUDA\n    // Check if cuDNN batch normalization has a fast NHWC implementation:\n    //   (1) Tensorflow enabled batchnorm spatial persistence, and\n    //       FusedBatchNormGradV3 passed non-null reserve space and allocator.\n    const bool fast_nhwc_batch_norm = BatchnormSpatialPersistentEnabled() &&\n                                      DataTypeToEnum<T>::value == DT_HALF &&\n                                      use_reserved_space;\n#else\n    // fast NHWC implementation is a CUDA only feature\n    const bool fast_nhwc_batch_norm = false;\n#endif\n\n    // If input tensor is in NHWC format, and we have a fast cuDNN\n    // implementation, there is no need to do data format conversion.\n    TensorFormat compute_format =\n        fast_nhwc_batch_norm && tensor_format == FORMAT_NHWC ? FORMAT_NHWC\n                                                             : FORMAT_NCHW;\n\n    VLOG(2) << \"FusedBatchNormGrad:\"\n            << \" batch_size: \" << batch_size << \" channels: \" << channels\n            << \" height: \" << height << \" width: \" << width\n            << \" y_backprop shape: \" << y_backprop.shape().DebugString()\n            << \" x shape: \" << x.shape().DebugString()\n            << \" scale shape: \" << scale.shape().DebugString()\n            << \" tensor format: \" << ToString(tensor_format)\n            << \" compute format: \" << ToString(compute_format);\n\n    // Inputs\n    Tensor y_backprop_maybe_transformed = y_backprop;\n    Tensor x_maybe_transformed = x;\n    Tensor y_backprop_transformed;\n    Tensor x_transformed;\n\n    // Outputs\n    Tensor x_backprop_transformed;\n    se::DeviceMemory<T> x_backprop_ptr;\n\n    if (tensor_format == compute_format) {\n      x_backprop_ptr = StreamExecutorUtil::AsDeviceMemory<T>(*x_backprop);\n    } else if (tensor_format == FORMAT_NHWC && compute_format == FORMAT_NCHW) {\n      // Transform inputs from 'NHWC' to 'NCHW'\n      OP_REQUIRES_OK(context, context->allocate_temp(\n                                  DataTypeToEnum<T>::value,\n                                  ShapeFromFormat(FORMAT_NCHW, batch_size,\n                                                  height, width, channels),\n                                  &y_backprop_transformed));\n      functor::NHWCToNCHW<GPUDevice, T, 4>()(\n          context->eigen_device<GPUDevice>(),\n          const_cast<const Tensor&>(y_backprop_maybe_transformed)\n              .tensor<T, 4>(),\n          y_backprop_transformed.tensor<T, 4>());\n      y_backprop_maybe_transformed = y_backprop_transformed;\n\n      OP_REQUIRES_OK(context, context->allocate_temp(\n                                  DataTypeToEnum<T>::value,\n                                  ShapeFromFormat(FORMAT_NCHW, batch_size,\n                                                  height, width, channels),\n                                  &x_transformed));\n      functor::NHWCToNCHW<GPUDevice, T, 4>()(\n          context->eigen_device<GPUDevice>(),\n          const_cast<const Tensor&>(x_maybe_transformed).tensor<T, 4>(),\n          x_transformed.tensor<T, 4>());\n      x_maybe_transformed = x_transformed;\n\n      // Allocate memory for transformed outputs in 'NCHW'\n      OP_REQUIRES_OK(context, context->allocate_temp(\n                                  DataTypeToEnum<T>::value,\n                                  ShapeFromFormat(FORMAT_NCHW, batch_size,\n                                                  height, width, channels),\n                                  &x_backprop_transformed));\n      x_backprop_ptr =\n          StreamExecutorUtil::AsDeviceMemory<T>(x_backprop_transformed);\n    } else {\n      context->SetStatus(errors::Internal(\n          \"Unsupported tensor format: \", ToString(tensor_format),\n          \" and compute format: \", ToString(compute_format)));\n      return;\n    }\n\n    const se::dnn::DataLayout data_layout =\n        compute_format == FORMAT_NHWC ? se::dnn::DataLayout::kBatchYXDepth\n                                      : se::dnn::DataLayout::kBatchDepthYX;\n\n    se::dnn::BatchDescriptor x_desc;\n    x_desc.set_count(batch_size)\n        .set_feature_map_count(channels)\n        .set_height(height)\n        .set_width(width)\n        .set_layout(data_layout);\n\n    se::dnn::BatchDescriptor scale_offset_desc;\n    scale_offset_desc.set_count(1)\n        .set_feature_map_count(channels)\n        .set_height(1)\n        .set_width(1)\n        .set_layout(se::dnn::DataLayout::kBatchDepthYX);\n\n    auto y_backprop_ptr =\n        StreamExecutorUtil::AsDeviceMemory<T>(y_backprop_maybe_transformed);\n    auto x_ptr = StreamExecutorUtil::AsDeviceMemory<T>(x_maybe_transformed);\n    auto scale_ptr = StreamExecutorUtil::AsDeviceMemory<U>(scale);\n    auto mean_ptr = StreamExecutorUtil::AsDeviceMemory<U>(mean);\n    auto inv_variance_ptr = StreamExecutorUtil::AsDeviceMemory<U>(inv_variance);\n    auto scale_backprop_ptr =\n        StreamExecutorUtil::AsDeviceMemory<U>(*scale_backprop);\n    auto offset_backprop_ptr =\n        StreamExecutorUtil::AsDeviceMemory<U>(*offset_backprop);\n\n    std::unique_ptr<functor::CudnnBatchNormAllocatorInTemp<uint8>>\n        workspace_allocator;\n    DeviceMemory<uint8>* reserve_space_data_ptr = nullptr;\n    DeviceMemory<uint8> reserve_space_data;\n#if CUDNN_VERSION >= 7402\n    if (use_reserved_space) {\n      const Tensor& reserve_space = context->input(5);\n      workspace_allocator.reset(\n          new functor::CudnnBatchNormAllocatorInTemp<uint8>(context));\n\n      // the cudnn kernel outputs inverse variance in forward and reuse it in\n      // backward\n      if (reserve_space.dims() != 0) {\n        reserve_space_data = functor::CastDeviceMemory<uint8, U>(\n            const_cast<Tensor*>(&reserve_space));\n        reserve_space_data_ptr = &reserve_space_data;\n      }\n    }\n#endif  // CUDNN_VERSION >= 7402\n\n    bool cudnn_launch_status =\n        stream\n            ->ThenBatchNormalizationBackward(\n                y_backprop_ptr, x_ptr, scale_ptr, mean_ptr, inv_variance_ptr,\n                x_desc, scale_offset_desc, static_cast<double>(epsilon),\n                &x_backprop_ptr, &scale_backprop_ptr, &offset_backprop_ptr,\n                reserve_space_data_ptr, workspace_allocator.get())\n            .ok();\n\n    if (!cudnn_launch_status) {\n      context->SetStatus(\n          errors::Internal(\"cuDNN launch failure : input shape (\",\n                           x.shape().DebugString(), \")\"));\n    }\n    if (tensor_format == FORMAT_NHWC && compute_format == FORMAT_NCHW) {\n      functor::NCHWToNHWC<GPUDevice, T, 4>()(\n          context->eigen_device<GPUDevice>(),\n          const_cast<const Tensor&>(x_backprop_transformed).tensor<T, 4>(),\n          x_backprop->tensor<T, 4>());\n    }\n  }\n};\n\n// Forward declarations of the functor specializations for GPU.\n#define DECLARE_GPU_SPEC(T, U)                                                 \\\n  template <>                                                                  \\\n  void FusedBatchNormFreezeGrad<GPUDevice, T, U>::operator()(                  \\\n      OpKernelContext* context, const Tensor& y_backprop_input,                \\\n      const Tensor& x_input, const Tensor& scale_input,                        \\\n      const Tensor& mean_input, const Tensor& variance_input, U epsilon,       \\\n      Tensor* x_backprop_output, Tensor* scale_backprop_output,                \\\n      Tensor* offset_backprop_output);                                         \\\n  extern template struct FusedBatchNormFreezeGrad<GPUDevice, T, U>;            \\\n  template <>                                                                  \\\n  void FusedBatchNormInferenceFunctor<GPUDevice, T, U>::operator()(            \\\n      OpKernelContext* context, TensorFormat tensor_format,                    \\\n      typename TTypes<T, 4>::ConstTensor in,                                   \\\n      typename TTypes<U>::ConstVec scale, typename TTypes<U>::ConstVec offset, \\\n      typename TTypes<U>::ConstVec estimated_mean,                             \\\n      typename TTypes<U>::ConstVec estimated_variance,                         \\\n      typename TTypes<T, 4>::ConstTensor side_input, U epsilon,                \\\n      FusedBatchNormActivationMode activation_mode,                            \\\n      typename TTypes<T, 4>::Tensor out);                                      \\\n  extern template struct FusedBatchNormInferenceFunctor<GPUDevice, T, U>;\n\nDECLARE_GPU_SPEC(float, float);\nDECLARE_GPU_SPEC(Eigen::half, float);\n\n#undef DECLARE_GPU_SPEC\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n}  // namespace functor\n\ntemplate <typename Device, typename T, typename U>\nclass FusedBatchNormOpBase : public OpKernel {\n  using FbnActivationMode = functor::FusedBatchNormActivationMode;\n\n protected:\n  explicit FusedBatchNormOpBase(OpKernelConstruction* context,\n                                bool is_batch_norm_ex = false)\n      : OpKernel(context) {\n    float epsilon;\n    OP_REQUIRES_OK(context, context->GetAttr(\"epsilon\", &epsilon));\n    epsilon_ = U(epsilon);\n    float exponential_avg_factor;\n    OP_REQUIRES_OK(context, context->GetAttr(\"exponential_avg_factor\",\n                                             &exponential_avg_factor));\n    exponential_avg_factor_ = U(exponential_avg_factor);\n    string tensor_format;\n    OP_REQUIRES_OK(context, context->GetAttr(\"data_format\", &tensor_format));\n    OP_REQUIRES(context, FormatFromString(tensor_format, &tensor_format_),\n                errors::InvalidArgument(\"Invalid data format\"));\n    OP_REQUIRES_OK(context, context->GetAttr(\"is_training\", &is_training_));\n\n    if (!is_batch_norm_ex) {\n      has_side_input_ = false;\n      activation_mode_ = FbnActivationMode::kIdentity;\n    } else {\n      OP_REQUIRES_OK(context, ParseActivationMode(context, &activation_mode_));\n\n      int num_side_inputs;\n      OP_REQUIRES_OK(context,\n                     context->GetAttr(\"num_side_inputs\", &num_side_inputs));\n      OP_REQUIRES(context, num_side_inputs >= 0 && num_side_inputs <= 1,\n                  errors::InvalidArgument(\n                      \"FusedBatchNorm accepts at most one side input.\"));\n      has_side_input_ = (num_side_inputs == 1);\n      if (has_side_input_ && is_training_) {\n        OP_REQUIRES(\n            context, activation_mode_ != FbnActivationMode::kIdentity,\n            errors::InvalidArgument(\"Identity activation is not supported with \"\n                                    \"non-empty side input\"));\n      }\n    }\n\n    if (activation_mode_ != FbnActivationMode::kIdentity && is_training_) {\n      // NOTE(ezhulenev): Following requirements are coming from implementation\n      // details of cudnnBatchNormalizationForwardTrainingEx used in training\n      // mode. In inference mode we call custom CUDA kernel that supports all\n      // data formats and data types.\n      OP_REQUIRES(context, DataTypeToEnum<T>::value == DT_HALF,\n                  errors::InvalidArgument(\"FusedBatchNorm with activation \"\n                                          \"supports only DT_HALF data type.\"));\n      OP_REQUIRES(context, tensor_format_ == FORMAT_NHWC,\n                  errors::InvalidArgument(\"FusedBatchNorm with activation \"\n                                          \"supports only NHWC tensor format.\"));\n      OP_REQUIRES(context, functor::BatchnormSpatialPersistentEnabled(),\n                  errors::InvalidArgument(\n                      \"FusedBatchNorm with activation must run with cuDNN \"\n                      \"spatial persistence mode enabled.\"));\n    }\n  }\n\n  // If use_reserved_space is true, we need to handle the 5th output (a reserved\n  // space) and a new cudnn batch norm will be called if the version > 7.4.2.\n  // If use_reserved_space is false, we don't have 5th output.\n  virtual void ComputeWithReservedSpace(OpKernelContext* context,\n                                        bool use_reserved_space) {\n    Tensor x = context->input(0);\n    const Tensor& scale = context->input(1);\n    const Tensor& offset = context->input(2);\n    const Tensor& estimated_mean = context->input(3);\n    const Tensor& estimated_variance = context->input(4);\n    const Tensor* side_input = has_side_input_ ? &context->input(5) : nullptr;\n\n    OP_REQUIRES(context, x.dims() == 4 || x.dims() == 5,\n                errors::InvalidArgument(\"input must be 4 or 5-dimensional\",\n                                        x.shape().DebugString()));\n    OP_REQUIRES(context, scale.dims() == 1,\n                errors::InvalidArgument(\"scale must be 1-dimensional\",\n                                        scale.shape().DebugString()));\n    OP_REQUIRES(context, offset.dims() == 1,\n                errors::InvalidArgument(\"offset must be 1-dimensional\",\n                                        offset.shape().DebugString()));\n    OP_REQUIRES(context, estimated_mean.dims() == 1,\n                errors::InvalidArgument(\"estimated_mean must be 1-dimensional\",\n                                        estimated_mean.shape().DebugString()));\n    OP_REQUIRES(\n        context, estimated_variance.dims() == 1,\n        errors::InvalidArgument(\"estimated_variance must be 1-dimensional\",\n                                estimated_variance.shape().DebugString()));\n    bool use_reshape = (x.dims() == 5);\n    auto x_shape = x.shape();\n    TensorShape dest_shape;\n    if (use_reshape) {\n      const int64 in_batch = GetTensorDim(x, tensor_format_, 'N');\n      int64 in_planes = GetTensorDim(x, tensor_format_, '0');\n      int64 in_rows = GetTensorDim(x, tensor_format_, '1');\n      int64 in_cols = GetTensorDim(x, tensor_format_, '2');\n      const int64 in_depth = GetTensorDim(x, tensor_format_, 'C');\n      dest_shape = ShapeFromFormat(tensor_format_, in_batch,\n                                   {{in_planes, in_rows * in_cols}}, in_depth);\n      OP_REQUIRES(context, x.CopyFrom(x, dest_shape),\n                  errors::InvalidArgument(\"Error during tensor copy.\"));\n    }\n\n    const auto num_channels = GetTensorDim(x, tensor_format_, 'C');\n    OP_REQUIRES(\n        context, scale.NumElements() == num_channels,\n        errors::InvalidArgument(\"scale must have the same number of elements \"\n                                \"as the channels of x, got \",\n                                scale.NumElements(), \" and \", num_channels));\n    OP_REQUIRES(\n        context, offset.NumElements() == num_channels,\n        errors::InvalidArgument(\"offset must have the same number of elements \"\n                                \"as the channels of x, got \",\n                                offset.NumElements(), \" and \", num_channels));\n    if (estimated_mean.NumElements() != 0) {\n      OP_REQUIRES(context, estimated_mean.NumElements() == num_channels,\n                  errors::InvalidArgument(\n                      \"mean must be empty or have the same number of \"\n                      \"elements as the channels of x, got \",\n                      estimated_mean.NumElements(), \" and \", num_channels));\n    }\n    if (estimated_variance.NumElements() != 0) {\n      OP_REQUIRES(context, estimated_variance.NumElements() == num_channels,\n                  errors::InvalidArgument(\n                      \"variance must be empty or have the same number of \"\n                      \"elements as the channels of x, got \",\n                      estimated_variance.NumElements(), \" and \", num_channels));\n    }\n\n    if (has_side_input_) {\n      OP_REQUIRES(context, side_input->shape() == x.shape(),\n                  errors::InvalidArgument(\n                      \"side_input shape must be equal to input shape: \",\n                      side_input->shape().DebugString(),\n                      \" != \", x.shape().DebugString()));\n    }\n\n    if (activation_mode_ != FbnActivationMode::kIdentity) {\n      // NOTE(ezhulenev): This requirement is coming from implementation\n      // details of cudnnBatchNormalizationForwardTrainingEx.\n      OP_REQUIRES(\n          context, !is_training_ || num_channels % 4 == 0,\n          errors::InvalidArgument(\"FusedBatchNorm with activation requires \"\n                                  \"channel dimension to be a multiple of 4.\"));\n    }\n\n    Tensor* y = nullptr;\n    auto alloc_shape = use_reshape ? dest_shape : x_shape;\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                {0}, 0, alloc_shape, &y));\n\n    Tensor* batch_mean = nullptr;\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                {3}, 1, scale.shape(), &batch_mean));\n    Tensor* batch_var = nullptr;\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                {4}, 2, scale.shape(), &batch_var));\n    Tensor* saved_mean = nullptr;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(3, scale.shape(), &saved_mean));\n    Tensor* saved_maybe_inv_var = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(4, scale.shape(),\n                                                     &saved_maybe_inv_var));\n\n    if (is_training_) {\n      functor::FusedBatchNorm<Device, T, U, true>()(\n          context, x, scale, offset, estimated_mean, estimated_variance,\n          side_input, epsilon_, exponential_avg_factor_, activation_mode_, y,\n          batch_mean, batch_var, saved_mean, saved_maybe_inv_var,\n          tensor_format_, use_reserved_space);\n    } else {\n      functor::FusedBatchNorm<Device, T, U, false>()(\n          context, x, scale, offset, estimated_mean, estimated_variance,\n          side_input, epsilon_, exponential_avg_factor_, activation_mode_, y,\n          batch_mean, batch_var, saved_mean, saved_maybe_inv_var,\n          tensor_format_, use_reserved_space);\n    }\n    if (use_reshape) {\n      OP_REQUIRES(context, y->CopyFrom(*y, x_shape),\n                  errors::InvalidArgument(\"Error during tensor copy.\"));\n    }\n  }\n\n private:\n  U epsilon_;\n  U exponential_avg_factor_;\n  TensorFormat tensor_format_;\n  bool is_training_;\n  bool has_side_input_;\n  FbnActivationMode activation_mode_;\n};\n\ntemplate <typename Device, typename T, typename U>\nclass FusedBatchNormOp : public FusedBatchNormOpBase<Device, T, U> {\n public:\n  explicit FusedBatchNormOp(OpKernelConstruction* context)\n      : FusedBatchNormOpBase<Device, T, U>(context) {}\n\n  void Compute(OpKernelContext* context) override {\n    FusedBatchNormOpBase<Device, T, U>::ComputeWithReservedSpace(context,\n                                                                 false);\n  }\n};\n\ntemplate <typename Device, typename T, typename U>\nclass FusedBatchNormOpV3 : public FusedBatchNormOpBase<Device, T, U> {\n public:\n  explicit FusedBatchNormOpV3(OpKernelConstruction* context)\n      : FusedBatchNormOpBase<Device, T, U>(context) {}\n\n  void Compute(OpKernelContext* context) override {\n    FusedBatchNormOpBase<Device, T, U>::ComputeWithReservedSpace(context, true);\n  }\n};\n\ntemplate <typename Device, typename T, typename U>\nclass FusedBatchNormOpEx : public FusedBatchNormOpBase<Device, T, U> {\n  static constexpr bool kWithSideInputAndActivation = true;\n\n public:\n  explicit FusedBatchNormOpEx(OpKernelConstruction* context)\n      : FusedBatchNormOpBase<Device, T, U>(context,\n                                           kWithSideInputAndActivation) {}\n\n  void Compute(OpKernelContext* context) override {\n    FusedBatchNormOpBase<Device, T, U>::ComputeWithReservedSpace(context, true);\n  }\n};\n\ntemplate <typename Device, typename T, typename U>\nclass FusedBatchNormGradOpBase : public OpKernel {\n protected:\n  explicit FusedBatchNormGradOpBase(OpKernelConstruction* context)\n      : OpKernel(context) {\n    float epsilon;\n    OP_REQUIRES_OK(context, context->GetAttr(\"epsilon\", &epsilon));\n    epsilon_ = U(epsilon);\n    string tensor_format;\n    OP_REQUIRES_OK(context, context->GetAttr(\"data_format\", &tensor_format));\n    OP_REQUIRES(context, FormatFromString(tensor_format, &tensor_format_),\n                errors::InvalidArgument(\"Invalid data format\"));\n    OP_REQUIRES_OK(context, context->GetAttr(\"is_training\", &is_training_));\n  }\n\n  virtual void ComputeWithReservedSpace(OpKernelContext* context,\n                                        bool use_reserved_space) {\n    Tensor y_backprop = context->input(0);\n    Tensor x = context->input(1);\n    const Tensor& scale = context->input(2);\n    // When is_training=True, batch mean and variance/inverted variance are\n    // saved in the forward pass to be reused here. When is_training=False,\n    // population mean and variance need to be forwarded here to compute the\n    // gradients.\n    const Tensor& saved_mean_or_pop_mean = context->input(3);\n    // The Eigen implementation saves variance in the forward pass, while cuDNN\n    // saves inverted variance.\n    const Tensor& saved_maybe_inv_var_or_pop_var = context->input(4);\n\n    OP_REQUIRES(context, y_backprop.dims() == 4 || y_backprop.dims() == 5,\n                errors::InvalidArgument(\"input must be 4 or 5-dimensional\",\n                                        y_backprop.shape().DebugString()));\n    OP_REQUIRES(context, x.dims() == 4 || x.dims() == 5,\n                errors::InvalidArgument(\"input must be 4 or 5-dimensional\",\n                                        x.shape().DebugString()));\n    OP_REQUIRES(context, scale.dims() == 1,\n                errors::InvalidArgument(\"scale must be 1-dimensional\",\n                                        scale.shape().DebugString()));\n    OP_REQUIRES(\n        context, saved_mean_or_pop_mean.dims() == 1,\n        errors::InvalidArgument(\"saved mean must be 1-dimensional\",\n                                saved_mean_or_pop_mean.shape().DebugString()));\n    OP_REQUIRES(context, saved_maybe_inv_var_or_pop_var.dims() == 1,\n                errors::InvalidArgument(\n                    \"saved variance must be 1-dimensional\",\n                    saved_maybe_inv_var_or_pop_var.shape().DebugString()));\n    bool use_reshape = (x.dims() == 5);\n    auto x_shape = x.shape();\n    TensorShape dest_shape;\n    if (use_reshape) {\n      const int64 in_batch = GetTensorDim(x, tensor_format_, 'N');\n      int64 in_planes = GetTensorDim(x, tensor_format_, '0');\n      int64 in_rows = GetTensorDim(x, tensor_format_, '1');\n      int64 in_cols = GetTensorDim(x, tensor_format_, '2');\n      const int64 in_depth = GetTensorDim(x, tensor_format_, 'C');\n      dest_shape = ShapeFromFormat(tensor_format_, in_batch,\n                                   {{in_planes, in_rows * in_cols}}, in_depth);\n      OP_REQUIRES(context, x.CopyFrom(x, dest_shape),\n                  errors::InvalidArgument(\"Error during tensor copy.\"));\n      OP_REQUIRES(context, y_backprop.CopyFrom(y_backprop, dest_shape),\n                  errors::InvalidArgument(\"Error during tensor copy.\"));\n    }\n\n    Tensor* x_backprop = nullptr;\n    auto alloc_shape = use_reshape ? dest_shape : x_shape;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(0, alloc_shape, &x_backprop));\n\n    const TensorShape& scale_offset_shape = scale.shape();\n    Tensor* scale_backprop = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(1, scale_offset_shape,\n                                                     &scale_backprop));\n    Tensor* offset_backprop = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(2, scale_offset_shape,\n                                                     &offset_backprop));\n    // Two placeholders for estimated_mean and estimated_variance, which are\n    // used for inference and thus not needed here for gradient computation.\n    // They are filled with zeros so as to avoid NaN outputs.\n    Tensor* placeholder_1 = nullptr;\n    OP_REQUIRES_OK(\n        context, context->allocate_output(3, TensorShape({0}), &placeholder_1));\n    Tensor* placeholder_2 = nullptr;\n    OP_REQUIRES_OK(\n        context, context->allocate_output(4, TensorShape({0}), &placeholder_2));\n\n    // If input is empty, set gradients w.r.t scale/offset to zero.\n    if (x.shape().num_elements() == 0) {\n      functor::SetZeroFunctor<Device, U> f;\n      f(context->eigen_device<Device>(), scale_backprop->flat<U>());\n      f(context->eigen_device<Device>(), offset_backprop->flat<U>());\n      return;\n    }\n\n    if (is_training_) {\n      functor::FusedBatchNormGrad<Device, T, U>()(\n          context, y_backprop, x, scale, saved_mean_or_pop_mean,\n          saved_maybe_inv_var_or_pop_var, epsilon_, x_backprop, scale_backprop,\n          offset_backprop, use_reserved_space, tensor_format_);\n    } else {\n      // Necessary layout conversion is currently done in python.\n      OP_REQUIRES(context, tensor_format_ == FORMAT_NHWC,\n                  errors::InvalidArgument(\n                      \"The implementation of \"\n                      \"FusedBatchNormGrad with is_training=False only support \"\n                      \"NHWC tensor format for now.\"));\n      functor::FusedBatchNormFreezeGrad<Device, T, U>()(\n          context, y_backprop, x, scale, saved_mean_or_pop_mean,\n          saved_maybe_inv_var_or_pop_var, epsilon_, x_backprop, scale_backprop,\n          offset_backprop);\n    }\n    if (use_reshape) {\n      OP_REQUIRES(context, x_backprop->CopyFrom(*x_backprop, x_shape),\n                  errors::InvalidArgument(\"Error during tensor copy.\"));\n    }\n  }\n\n private:\n  U epsilon_;\n  TensorFormat tensor_format_;\n  bool is_training_;\n};\n\ntemplate <typename Device, typename T, typename U>\nclass FusedBatchNormGradOp : public FusedBatchNormGradOpBase<Device, T, U> {\n public:\n  explicit FusedBatchNormGradOp(OpKernelConstruction* context)\n      : FusedBatchNormGradOpBase<Device, T, U>(context) {}\n\n  void Compute(OpKernelContext* context) override {\n    FusedBatchNormGradOpBase<Device, T, U>::ComputeWithReservedSpace(context,\n                                                                     false);\n  }\n};\n\ntemplate <typename Device, typename T, typename U>\nclass FusedBatchNormGradOpV3 : public FusedBatchNormGradOpBase<Device, T, U> {\n public:\n  explicit FusedBatchNormGradOpV3(OpKernelConstruction* context)\n      : FusedBatchNormGradOpBase<Device, T, U>(context) {}\n\n  void Compute(OpKernelContext* context) override {\n    FusedBatchNormGradOpBase<Device, T, U>::ComputeWithReservedSpace(context,\n                                                                     true);\n  }\n};\n\nREGISTER_KERNEL_BUILDER(\n    Name(\"FusedBatchNorm\").Device(DEVICE_CPU).TypeConstraint<float>(\"T\"),\n    FusedBatchNormOp<CPUDevice, float, float>);\n\nREGISTER_KERNEL_BUILDER(\n    Name(\"FusedBatchNormGrad\").Device(DEVICE_CPU).TypeConstraint<float>(\"T\"),\n    FusedBatchNormGradOp<CPUDevice, float, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"FusedBatchNormV2\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<float>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormOp<CPUDevice, float, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"FusedBatchNormGradV2\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<float>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormGradOp<CPUDevice, float, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"FusedBatchNormV2\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<Eigen::half>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormOp<CPUDevice, Eigen::half, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"FusedBatchNormGradV2\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<Eigen::half>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormGradOp<CPUDevice, Eigen::half, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"FusedBatchNormV3\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<float>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormOpV3<CPUDevice, float, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"FusedBatchNormGradV3\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<float>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormGradOpV3<CPUDevice, float, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"FusedBatchNormV3\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<Eigen::half>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormOpV3<CPUDevice, Eigen::half, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"FusedBatchNormGradV3\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<Eigen::half>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormGradOpV3<CPUDevice, Eigen::half, float>);\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\nREGISTER_KERNEL_BUILDER(\n    Name(\"FusedBatchNorm\").Device(DEVICE_GPU).TypeConstraint<float>(\"T\"),\n    FusedBatchNormOp<GPUDevice, float, float>);\n\nREGISTER_KERNEL_BUILDER(\n    Name(\"FusedBatchNormGrad\").Device(DEVICE_GPU).TypeConstraint<float>(\"T\"),\n    FusedBatchNormGradOp<GPUDevice, float, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"FusedBatchNormV2\")\n                            .Device(DEVICE_GPU)\n                            .TypeConstraint<float>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormOp<GPUDevice, float, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"FusedBatchNormGradV2\")\n                            .Device(DEVICE_GPU)\n                            .TypeConstraint<float>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormGradOp<GPUDevice, float, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"FusedBatchNormV2\")\n                            .Device(DEVICE_GPU)\n                            .TypeConstraint<Eigen::half>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormOp<GPUDevice, Eigen::half, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"FusedBatchNormGradV2\")\n                            .Device(DEVICE_GPU)\n                            .TypeConstraint<Eigen::half>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormGradOp<GPUDevice, Eigen::half, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"FusedBatchNormV3\")\n                            .Device(DEVICE_GPU)\n                            .TypeConstraint<float>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormOpV3<GPUDevice, float, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"_FusedBatchNormEx\")\n                            .Device(DEVICE_GPU)\n                            .TypeConstraint<float>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormOpEx<GPUDevice, float, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"FusedBatchNormGradV3\")\n                            .Device(DEVICE_GPU)\n                            .TypeConstraint<float>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormGradOpV3<GPUDevice, float, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"FusedBatchNormV3\")\n                            .Device(DEVICE_GPU)\n                            .TypeConstraint<Eigen::half>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormOpV3<GPUDevice, Eigen::half, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"_FusedBatchNormEx\")\n                            .Device(DEVICE_GPU)\n                            .TypeConstraint<Eigen::half>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormOpEx<GPUDevice, Eigen::half, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"FusedBatchNormGradV3\")\n                            .Device(DEVICE_GPU)\n                            .TypeConstraint<Eigen::half>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormGradOpV3<GPUDevice, Eigen::half, float>);\n\n#endif\n\n}  // namespace tensorflow\n"], "filenames": ["tensorflow/core/kernels/fused_batch_norm_op.cc"], "buggy_code_start_loc": [1284], "buggy_code_end_loc": [1298], "fixing_code_start_loc": [1285], "fixing_code_end_loc": [1324], "type": "CWE-125", "message": "TensorFlow is an end-to-end open source platform for machine learning. The implementation of `tf.raw_ops.FusedBatchNorm` is vulnerable to a heap buffer overflow. If the tensors are empty, the same implementation can trigger undefined behavior by dereferencing null pointers. The implementation(https://github.com/tensorflow/tensorflow/blob/57d86e0db5d1365f19adcce848dfc1bf89fdd4c7/tensorflow/core/kernels/fused_batch_norm_op.cc) fails to validate that `scale`, `offset`, `mean` and `variance` (the last two only when required) all have the same number of elements as the number of channels of `x`. This results in heap out of bounds reads when the buffers backing these tensors are indexed past their boundary. If the tensors are empty, the validation mentioned in the above paragraph would also trigger and prevent the undefined behavior. The fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range.", "other": {"cve": {"id": "CVE-2021-29583", "sourceIdentifier": "security-advisories@github.com", "published": "2021-05-14T20:15:14.437", "lastModified": "2022-04-25T20:09:03.600", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "TensorFlow is an end-to-end open source platform for machine learning. The implementation of `tf.raw_ops.FusedBatchNorm` is vulnerable to a heap buffer overflow. If the tensors are empty, the same implementation can trigger undefined behavior by dereferencing null pointers. The implementation(https://github.com/tensorflow/tensorflow/blob/57d86e0db5d1365f19adcce848dfc1bf89fdd4c7/tensorflow/core/kernels/fused_batch_norm_op.cc) fails to validate that `scale`, `offset`, `mean` and `variance` (the last two only when required) all have the same number of elements as the number of channels of `x`. This results in heap out of bounds reads when the buffers backing these tensors are indexed past their boundary. If the tensors are empty, the validation mentioned in the above paragraph would also trigger and prevent the undefined behavior. The fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range."}, {"lang": "es", "value": "TensorFlow es una plataforma de c\u00f3digo abierto de extremo a extremo para el aprendizaje autom\u00e1tico.&#xa0;La implementaci\u00f3n de \"tf.raw_ops.FusedBatchNorm\" es vulnerable a un desbordamiento del b\u00fafer de la pila.&#xa0;Si los tensores est\u00e1n vac\u00edos, la misma implementaci\u00f3n puede desencadenar un comportamiento indefinido al eliminar la desreferencia de punteros nulls.&#xa0;La implementaci\u00f3n (https://github.com/tensorflow/tensorflow/blob/57d86e0db5d1365f19adcce848dfc1bf89fdd4c7/tensorflow/core/kernels/fused_batch_norm_op.cc) no comprueba que \"scale\", \"offset\", \"mean\" y \"varnce\" (los dos \u00faltimos solo cuando sea necesario) todos presentan el mismo n\u00famero de elementos que el n\u00famero de canales de \"x\".&#xa0;Esto resulta en lecturas de pilas fuera de l\u00edmites cuando los b\u00faferes que respaldan estos tensores se indexan m\u00e1s all\u00e1 de su l\u00edmite.&#xa0;Si los tensores est\u00e1n vac\u00edos, Una comprobaci\u00f3n mencionada en el p\u00e1rrafo anterior tambi\u00e9n desencadenar\u00eda y evitar\u00eda el comportamiento indefinido.&#xa0;La correcci\u00f3n ser\u00e1 incluida en TensorFlow versi\u00f3n 2.5.0.&#xa0;Tambi\u00e9n seleccionaremos este compromiso en TensorFlow versi\u00f3n 2.4.2, TensorFlow versi\u00f3n 2.3.3, TensorFlow versi\u00f3n 2.2.3 y TensorFlow versi\u00f3n 2.1.4, ya que estos tambi\u00e9n est\u00e1n afectados y a\u00fan est\u00e1n en el rango admitido"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.9}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:H/PR:L/UI:N/S:U/C:N/I:N/A:L", "attackVector": "LOCAL", "attackComplexity": "HIGH", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "LOW", "baseScore": 2.5, "baseSeverity": "LOW"}, "exploitabilityScore": 1.0, "impactScore": 1.4}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:P/I:P/A:P", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "PARTIAL", "availabilityImpact": "PARTIAL", "baseScore": 4.6}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.9, "impactScore": 6.4, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-125"}, {"lang": "en", "value": "CWE-476"}]}, {"source": "security-advisories@github.com", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-476"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionEndExcluding": "2.1.4", "matchCriteriaId": "323ABCCE-24EB-47CC-87F6-48C101477587"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.2.0", "versionEndExcluding": "2.2.3", "matchCriteriaId": "64ABA90C-0649-4BB0-89C9-83C14BBDCC0F"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.3.0", "versionEndExcluding": "2.3.3", "matchCriteriaId": "0F83E0CF-CBF6-4C24-8683-3E7A5DC95BA9"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.4.0", "versionEndExcluding": "2.4.2", "matchCriteriaId": "8259531B-A8AC-4F8B-B60F-B69DE4767C03"}]}]}], "references": [{"url": "https://github.com/tensorflow/tensorflow/commit/6972f9dfe325636b3db4e0bc517ee22a159365c0", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/security/advisories/GHSA-9xh4-23q4-v6wr", "source": "security-advisories@github.com", "tags": ["Exploit", "Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/tensorflow/tensorflow/commit/6972f9dfe325636b3db4e0bc517ee22a159365c0"}}