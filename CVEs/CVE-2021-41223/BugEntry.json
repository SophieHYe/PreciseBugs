{"buggy_code": ["/* Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include <atomic>\n\n#define EIGEN_USE_THREADS\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n#define EIGEN_USE_GPU\n#if GOOGLE_CUDA\n#include \"third_party/gpus/cudnn/cudnn.h\"\n#endif  // GOOGLE_CUDA\n\n#include \"tensorflow/core/kernels/conv_2d.h\"\n#include \"tensorflow/core/platform/stream_executor.h\"\n#include \"tensorflow/core/util/stream_executor_util.h\"\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/register_types.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/tensor_types.h\"\n#include \"tensorflow/core/kernels/fill_functor.h\"\n#include \"tensorflow/core/kernels/fused_batch_norm_op.h\"\n#include \"tensorflow/core/kernels/redux_functor.h\"\n#include \"tensorflow/core/kernels/transpose_functor.h\"\n#include \"tensorflow/core/lib/core/blocking_counter.h\"\n#include \"tensorflow/core/util/env_var.h\"\n#include \"tensorflow/core/util/tensor_format.h\"\n\nnamespace tensorflow {\nusing CPUDevice = Eigen::ThreadPoolDevice;\nusing GPUDevice = Eigen::GpuDevice;\n\nnamespace functor {\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\nusing se::DeviceMemory;\nusing se::ScratchAllocator;\nusing se::Stream;\nusing se::port::StatusOr;\n#endif\n\nstring ToString(FusedBatchNormActivationMode activation_mode) {\n  switch (activation_mode) {\n    case FusedBatchNormActivationMode::kIdentity:\n      return \"Identity\";\n    case FusedBatchNormActivationMode::kRelu:\n      return \"Relu\";\n  }\n}\n\nStatus ParseActivationMode(OpKernelConstruction* context,\n                           FusedBatchNormActivationMode* activation_mode) {\n  string activation_mode_str;\n  TF_RETURN_IF_ERROR(context->GetAttr(\"activation_mode\", &activation_mode_str));\n\n  if (activation_mode_str == \"Identity\") {\n    *activation_mode = FusedBatchNormActivationMode::kIdentity;\n    return Status::OK();\n  }\n  if (activation_mode_str == \"Relu\") {\n    *activation_mode = FusedBatchNormActivationMode::kRelu;\n    return Status::OK();\n  }\n  return errors::InvalidArgument(\"Unsupported activation mode: \",\n                                 activation_mode_str);\n}\n\n// Functor used by FusedBatchNormOp to do the computations.\ntemplate <typename Device, typename T, typename U, bool is_training>\nstruct FusedBatchNorm;\n// Functor used by FusedBatchNormGradOp to do the computations when\n// is_training=True.\ntemplate <typename Device, typename T, typename U>\nstruct FusedBatchNormGrad;\n\ntemplate <typename T, typename U>\nstruct FusedBatchNorm<CPUDevice, T, U, /* is_training= */ true> {\n  void operator()(OpKernelContext* context, const Tensor& x_input,\n                  const Tensor& scale_input, const Tensor& offset_input,\n                  const Tensor& running_mean_input,\n                  const Tensor& running_variance_input,\n                  const Tensor* side_input, U epsilon, U exponential_avg_factor,\n                  FusedBatchNormActivationMode activation_mode,\n                  Tensor* y_output, Tensor* running_mean_output,\n                  Tensor* running_var_output, Tensor* saved_batch_mean_output,\n                  Tensor* saved_batch_var_output, TensorFormat tensor_format,\n                  bool use_reserved_space) {\n    OP_REQUIRES(context, side_input == nullptr,\n                errors::Internal(\n                    \"The CPU implementation of FusedBatchNorm does not support \"\n                    \"side input.\"));\n    OP_REQUIRES(context,\n                activation_mode == FusedBatchNormActivationMode::kIdentity,\n                errors::Internal(\"The CPU implementation of FusedBatchNorm \"\n                                 \"does not support activations.\"));\n\n    if (use_reserved_space) {\n      Tensor* dummy_reserve_space = nullptr;\n      OP_REQUIRES_OK(context,\n                     context->allocate_output(5, {}, &dummy_reserve_space));\n      // Initialize the memory, to avoid sanitizer alerts.\n      dummy_reserve_space->flat<U>()(0) = U();\n    }\n\n    // If input is empty, return NaN mean/variance\n    if (x_input.shape().num_elements() == 0) {\n      functor::SetNanFunctor<CPUDevice, U> f;\n      f(context->eigen_device<CPUDevice>(), running_mean_output->flat<U>());\n      f(context->eigen_device<CPUDevice>(), running_var_output->flat<U>());\n      return;\n    }\n\n    Tensor transformed_x;\n    Tensor transformed_y;\n    if (tensor_format == FORMAT_NCHW) {\n      const int64_t in_batch = GetTensorDim(x_input, tensor_format, 'N');\n      const int64_t in_rows = GetTensorDim(x_input, tensor_format, 'H');\n      const int64_t in_cols = GetTensorDim(x_input, tensor_format, 'W');\n      const int64_t in_depths = GetTensorDim(x_input, tensor_format, 'C');\n      OP_REQUIRES_OK(context, context->allocate_temp(\n                                  DataTypeToEnum<T>::value,\n                                  ShapeFromFormat(FORMAT_NHWC, in_batch,\n                                                  in_rows, in_cols, in_depths),\n                                  &transformed_x));\n      OP_REQUIRES_OK(context, context->allocate_temp(\n                                  DataTypeToEnum<T>::value,\n                                  ShapeFromFormat(FORMAT_NHWC, in_batch,\n                                                  in_rows, in_cols, in_depths),\n                                  &transformed_y));\n      // Perform NCHW to NHWC\n      std::vector<int32> perm = {0, 2, 3, 1};\n      OP_REQUIRES_OK(\n          context, ::tensorflow::DoTranspose(context->eigen_device<CPUDevice>(),\n                                             x_input, perm, &transformed_x));\n    } else {\n      transformed_x = x_input;\n      transformed_y = *y_output;\n    }\n    typename TTypes<T, 4>::Tensor x(transformed_x.tensor<T, 4>());\n    typename TTypes<U>::ConstVec scale(scale_input.vec<U>());\n    typename TTypes<U>::ConstVec offset(offset_input.vec<U>());\n    typename TTypes<U>::ConstVec old_mean(running_mean_input.vec<U>());\n    typename TTypes<U>::ConstVec old_variance(running_variance_input.vec<U>());\n    typename TTypes<T, 4>::Tensor y(transformed_y.tensor<T, 4>());\n    typename TTypes<U>::Vec new_mean(running_mean_output->vec<U>());\n    typename TTypes<U>::Vec new_variance(running_var_output->vec<U>());\n    typename TTypes<U>::Vec saved_batch_mean(saved_batch_mean_output->vec<U>());\n    typename TTypes<U>::Vec saved_batch_var(saved_batch_var_output->vec<U>());\n\n    const CPUDevice& d = context->eigen_device<CPUDevice>();\n\n    const int depth = x.dimension(3);\n    const int size = x.size();\n    const int rest_size = size / depth;\n    Eigen::DSizes<Eigen::Index, 2> rest_by_depth(rest_size, depth);\n\n#if !defined(EIGEN_HAS_INDEX_LIST)\n    Eigen::DSizes<Eigen::Index, 2> one_by_depth(1, depth);\n    Eigen::array<int, 1> reduce_dims({0});\n    Eigen::array<int, 2> bcast_spec({rest_size, 1});\n#else\n    Eigen::IndexList<Eigen::type2index<1>, Eigen::Index> one_by_depth;\n    one_by_depth.set(1, depth);\n    Eigen::IndexList<Eigen::type2index<0>> reduce_dims;\n    Eigen::IndexList<Eigen::Index, Eigen::type2index<1>> bcast_spec;\n    bcast_spec.set(0, rest_size);\n#endif\n\n    auto x_rest_by_depth = x.reshape(rest_by_depth).template cast<U>();\n    const int rest_size_minus_one = (rest_size > 1) ? (rest_size - 1) : 1;\n    U rest_size_inv = static_cast<U>(1.0f / static_cast<U>(rest_size));\n    // This adjustment is for Bessel's correction\n    U rest_size_adjust =\n        static_cast<U>(rest_size) / static_cast<U>(rest_size_minus_one);\n\n    Eigen::Tensor<U, 1, Eigen::RowMajor> batch_mean(depth);\n    Eigen::Tensor<U, 1, Eigen::RowMajor> batch_variance(depth);\n\n    batch_mean.device(d) = (x_rest_by_depth.sum(reduce_dims) * rest_size_inv);\n    auto x_centered = x_rest_by_depth -\n                      batch_mean.reshape(one_by_depth).broadcast(bcast_spec);\n\n    batch_variance.device(d) =\n        x_centered.square().sum(reduce_dims) * rest_size_inv;\n    auto scaling_factor = ((batch_variance + epsilon).rsqrt() * scale)\n                              .eval()\n                              .reshape(one_by_depth)\n                              .broadcast(bcast_spec);\n    auto x_scaled = x_centered * scaling_factor;\n    auto x_shifted =\n        (x_scaled + offset.reshape(one_by_depth).broadcast(bcast_spec))\n            .template cast<T>();\n\n    y.reshape(rest_by_depth).device(d) = x_shifted;\n    if (exponential_avg_factor == U(1.0)) {\n      saved_batch_var.device(d) = batch_variance;\n      saved_batch_mean.device(d) = batch_mean;\n      new_variance.device(d) = batch_variance * rest_size_adjust;\n      new_mean.device(d) = batch_mean;\n    } else {\n      U one_minus_factor = U(1) - exponential_avg_factor;\n      saved_batch_var.device(d) = batch_variance;\n      saved_batch_mean.device(d) = batch_mean;\n      new_variance.device(d) =\n          one_minus_factor * old_variance +\n          (exponential_avg_factor * rest_size_adjust) * batch_variance;\n      new_mean.device(d) =\n          one_minus_factor * old_mean + exponential_avg_factor * batch_mean;\n    }\n\n    if (tensor_format == FORMAT_NCHW) {\n      // Perform NHWC to NCHW\n      const std::vector<int32> perm = {0, 3, 1, 2};\n      const Status s = ::tensorflow::DoTranspose(\n          context->eigen_device<CPUDevice>(), transformed_y, perm, y_output);\n      if (!s.ok()) {\n        context->SetStatus(errors::InvalidArgument(\"Transpose failed: \", s));\n      }\n    }\n  }\n};\n\ntemplate <typename T, typename U>\nstruct FusedBatchNorm<CPUDevice, T, U, /* is_training= */ false> {\n  void operator()(OpKernelContext* context, const Tensor& x_input,\n                  const Tensor& scale_input, const Tensor& offset_input,\n                  const Tensor& estimated_mean_input,\n                  const Tensor& estimated_variance_input,\n                  const Tensor* side_input, U epsilon, U exponential_avg_factor,\n                  FusedBatchNormActivationMode activation_mode,\n                  Tensor* y_output, Tensor* batch_mean_output,\n                  Tensor* batch_var_output, Tensor* saved_mean_output,\n                  Tensor* saved_var_output, TensorFormat tensor_format,\n                  bool use_reserved_space) {\n    OP_REQUIRES(context, side_input == nullptr,\n                errors::Internal(\n                    \"The CPU implementation of FusedBatchNorm does not support \"\n                    \"side input.\"));\n    OP_REQUIRES(context,\n                activation_mode == FusedBatchNormActivationMode::kIdentity,\n                errors::Internal(\"The CPU implementation of FusedBatchNorm \"\n                                 \"does not support activations.\"));\n\n    if (use_reserved_space) {\n      Tensor* dummy_reserve_space = nullptr;\n      OP_REQUIRES_OK(context,\n                     context->allocate_output(5, {}, &dummy_reserve_space));\n      // Initialize the memory, to avoid sanitizer alerts.\n      dummy_reserve_space->flat<U>()(0) = U();\n    }\n\n    // If input is empty, return NaN mean/variance\n    if (x_input.shape().num_elements() == 0) {\n      functor::SetNanFunctor<CPUDevice, U> f;\n      f(context->eigen_device<CPUDevice>(), batch_mean_output->flat<U>());\n      f(context->eigen_device<CPUDevice>(), batch_var_output->flat<U>());\n      return;\n    }\n\n    Tensor transformed_x;\n    Tensor transformed_y;\n    if (tensor_format == FORMAT_NCHW) {\n      const int64_t in_batch = GetTensorDim(x_input, tensor_format, 'N');\n      const int64_t in_rows = GetTensorDim(x_input, tensor_format, 'H');\n      const int64_t in_cols = GetTensorDim(x_input, tensor_format, 'W');\n      const int64_t in_depths = GetTensorDim(x_input, tensor_format, 'C');\n      OP_REQUIRES_OK(context, context->allocate_temp(\n                                  DataTypeToEnum<T>::value,\n                                  ShapeFromFormat(FORMAT_NHWC, in_batch,\n                                                  in_rows, in_cols, in_depths),\n                                  &transformed_x));\n      OP_REQUIRES_OK(context, context->allocate_temp(\n                                  DataTypeToEnum<T>::value,\n                                  ShapeFromFormat(FORMAT_NHWC, in_batch,\n                                                  in_rows, in_cols, in_depths),\n                                  &transformed_y));\n      // Perform NCHW to NHWC\n      std::vector<int32> perm = {0, 2, 3, 1};\n      OP_REQUIRES_OK(\n          context, ::tensorflow::DoTranspose(context->eigen_device<CPUDevice>(),\n                                             x_input, perm, &transformed_x));\n    } else {\n      transformed_x = x_input;\n      transformed_y = *y_output;\n    }\n    typename TTypes<T, 4>::Tensor x(transformed_x.tensor<T, 4>());\n    typename TTypes<U>::ConstVec scale(scale_input.vec<U>());\n    typename TTypes<U>::ConstVec offset(offset_input.vec<U>());\n    typename TTypes<U>::ConstVec estimated_mean(estimated_mean_input.vec<U>());\n    typename TTypes<U>::ConstVec estimated_variance(\n        estimated_variance_input.vec<U>());\n    typename TTypes<T, 4>::Tensor y(transformed_y.tensor<T, 4>());\n    typename TTypes<U>::Vec batch_mean(batch_mean_output->vec<U>());\n    typename TTypes<U>::Vec batch_variance(batch_var_output->vec<U>());\n\n    const CPUDevice& d = context->eigen_device<CPUDevice>();\n\n    const int depth = x.dimension(3);\n    OP_REQUIRES(\n        context, depth != 0,\n        errors::Internal(\"The 4th element in the input shape cannot be 0.\"));\n    const int size = x.size();\n    const int rest_size = size / depth;\n    Eigen::DSizes<Eigen::Index, 2> rest_by_depth(rest_size, depth);\n\n#if !defined(EIGEN_HAS_INDEX_LIST)\n    Eigen::DSizes<Eigen::Index, 2> one_by_depth(1, depth);\n    Eigen::array<int, 1> reduce_dims({0});\n    Eigen::array<int, 2> bcast_spec({rest_size, 1});\n#else\n    Eigen::IndexList<Eigen::type2index<1>, Eigen::Index> one_by_depth;\n    one_by_depth.set(1, depth);\n    Eigen::IndexList<Eigen::Index, Eigen::type2index<1>> bcast_spec;\n    bcast_spec.set(0, rest_size);\n#endif\n\n    auto x_rest_by_depth = x.reshape(rest_by_depth).template cast<U>();\n    auto x_centered =\n        x_rest_by_depth -\n        estimated_mean.reshape(one_by_depth).broadcast(bcast_spec);\n    auto scaling_factor = ((estimated_variance + epsilon).rsqrt() * scale)\n                              .eval()\n                              .reshape(one_by_depth)\n                              .broadcast(bcast_spec);\n    auto x_scaled = x_centered * scaling_factor;\n    auto x_shifted =\n        (x_scaled + offset.reshape(one_by_depth).broadcast(bcast_spec))\n            .template cast<T>();\n\n    y.reshape(rest_by_depth).device(d) = x_shifted;\n    batch_mean.device(d) = estimated_mean;\n    batch_variance.device(d) = estimated_variance;\n\n    if (tensor_format == FORMAT_NCHW) {\n      // Perform NHWC to NCHW\n      const std::vector<int32> perm = {0, 3, 1, 2};\n      const Status s = ::tensorflow::DoTranspose(\n          context->eigen_device<CPUDevice>(), transformed_y, perm, y_output);\n      if (!s.ok()) {\n        context->SetStatus(errors::InvalidArgument(\"Transpose failed: \", s));\n      }\n    }\n  }\n};\n\ntemplate <typename T, typename U>\nstruct FusedBatchNormGrad<CPUDevice, T, U> {\n  void operator()(OpKernelContext* context, const Tensor& y_backprop_input,\n                  const Tensor& x_input, const Tensor& scale_input,\n                  const Tensor* offset_input, const Tensor& mean_input,\n                  const Tensor& variance_input, const Tensor* y_input,\n                  U epsilon, FusedBatchNormActivationMode activation_mode,\n                  Tensor* x_backprop_output, Tensor* scale_backprop_output,\n                  Tensor* offset_backprop_output,\n                  Tensor* side_input_backprop_output, bool use_reserved_space,\n                  TensorFormat tensor_format) {\n    OP_REQUIRES(context,\n                y_input == nullptr &&\n                    activation_mode == FusedBatchNormActivationMode::kIdentity,\n                errors::Internal(\n                    \"The CPU implementation of FusedBatchNormGrad does not \"\n                    \"support activations.\"));\n    OP_REQUIRES(context, side_input_backprop_output == nullptr,\n                errors::Internal(\"The CPU implementation of FusedBatchNormGrad \"\n                                 \"does not support side input.\"));\n\n    Tensor transformed_y_backprop_input;\n    Tensor transformed_x_input;\n    Tensor transformed_x_backprop_output;\n    if (tensor_format == FORMAT_NCHW) {\n      const int64_t in_batch = GetTensorDim(x_input, tensor_format, 'N');\n      const int64_t in_rows = GetTensorDim(x_input, tensor_format, 'H');\n      const int64_t in_cols = GetTensorDim(x_input, tensor_format, 'W');\n      const int64_t in_depths = GetTensorDim(x_input, tensor_format, 'C');\n      OP_REQUIRES_OK(context, context->allocate_temp(\n                                  DataTypeToEnum<T>::value,\n                                  ShapeFromFormat(FORMAT_NHWC, in_batch,\n                                                  in_rows, in_cols, in_depths),\n                                  &transformed_y_backprop_input));\n      OP_REQUIRES_OK(context, context->allocate_temp(\n                                  DataTypeToEnum<T>::value,\n                                  ShapeFromFormat(FORMAT_NHWC, in_batch,\n                                                  in_rows, in_cols, in_depths),\n                                  &transformed_x_input));\n      OP_REQUIRES_OK(context, context->allocate_temp(\n                                  DataTypeToEnum<T>::value,\n                                  ShapeFromFormat(FORMAT_NHWC, in_batch,\n                                                  in_rows, in_cols, in_depths),\n                                  &transformed_x_backprop_output));\n      // Perform NCHW to NHWC\n      std::vector<int32> perm = {0, 2, 3, 1};\n      OP_REQUIRES_OK(\n          context, ::tensorflow::DoTranspose(context->eigen_device<CPUDevice>(),\n                                             y_backprop_input, perm,\n                                             &transformed_y_backprop_input));\n      OP_REQUIRES_OK(context, ::tensorflow::DoTranspose(\n                                  context->eigen_device<CPUDevice>(), x_input,\n                                  perm, &transformed_x_input));\n    } else {\n      transformed_y_backprop_input = y_backprop_input;\n      transformed_x_input = x_input;\n      transformed_x_backprop_output = *x_backprop_output;\n    }\n    typename TTypes<T, 4>::Tensor y_backprop(\n        transformed_y_backprop_input.tensor<T, 4>());\n    typename TTypes<T, 4>::Tensor x(transformed_x_input.tensor<T, 4>());\n    typename TTypes<U>::ConstVec scale(scale_input.vec<U>());\n    typename TTypes<U>::ConstVec mean(mean_input.vec<U>());\n    typename TTypes<U>::ConstVec variance(variance_input.vec<U>());\n    typename TTypes<T, 4>::Tensor x_backprop(\n        transformed_x_backprop_output.tensor<T, 4>());\n    typename TTypes<U>::Vec offset_backprop(offset_backprop_output->vec<U>());\n\n    // Note: the following formulas are used to compute the gradients for\n    // back propagation.\n    // x_backprop = scale * rsqrt(variance + epsilon) *\n    //              [y_backprop - mean(y_backprop) - (x - mean(x)) *\n    //              mean(y_backprop * (x - mean(x))) / (variance + epsilon)]\n    // scale_backprop = sum(y_backprop *\n    //                  (x - mean(x)) * rsqrt(variance + epsilon))\n    // offset_backprop = sum(y_backprop)\n\n    const CPUDevice& d = context->eigen_device<CPUDevice>();\n    const int depth = x.dimension(3);\n    const int size = x.size();\n    const int rest_size = size / depth;\n    Eigen::DSizes<Eigen::Index, 2> rest_by_depth(rest_size, depth);\n\n#if !defined(EIGEN_HAS_INDEX_LIST)\n    Eigen::DSizes<Eigen::Index, 2> one_by_depth(1, depth);\n    Eigen::array<int, 2> bcast_spec({rest_size, 1});\n#else\n    Eigen::IndexList<Eigen::type2index<1>, Eigen::Index> one_by_depth;\n    one_by_depth.set(1, depth);\n    Eigen::IndexList<Eigen::Index, Eigen::type2index<1>> bcast_spec;\n    bcast_spec.set(0, rest_size);\n#endif\n\n    auto x_rest_by_depth = x.reshape(rest_by_depth).template cast<U>();\n    U rest_size_inv = static_cast<U>(1.0f / static_cast<U>(rest_size));\n\n    // Eigen is notoriously bad at reducing outer dimension, so we materialize\n    // all temporary tensors that require reduction, and then use Eigen redux\n    // functor, that is optimized for this particular task.\n    //\n    // All reductions are of this type: [rest_size, depth] -> [depth].\n    using ScalarSum = Eigen::internal::scalar_sum_op<U>;\n    const functor::ReduceOuterDimensions<T, U, U, ScalarSum> redux_sum_t;\n    const functor::ReduceOuterDimensions<U, U, U, ScalarSum> redux_sum_u;\n\n    auto scratch_dtype = DataTypeToEnum<U>::value;\n\n    // Allocate a temporary workspace of [depth] shape.\n    Tensor scratch_one_by_depth;\n    OP_REQUIRES_OK(context, context->allocate_temp(scratch_dtype, {depth},\n                                                   &scratch_one_by_depth));\n\n    // Maybe allocate a temporary workspace of [rest_size, depth] shape.\n    Tensor scratch_rest_by_depth;\n    if (std::is_same<T, U>::value) {\n      OP_REQUIRES(context,\n                  scratch_rest_by_depth.CopyFrom(transformed_x_backprop_output,\n                                                 {rest_size, depth}),\n                  errors::Internal(\"Failed to copy a tensor\"));\n    } else {\n      OP_REQUIRES_OK(context,\n                     context->allocate_temp(scratch_dtype, {rest_size, depth},\n                                            &scratch_rest_by_depth));\n    }\n\n    typename TTypes<U, 2>::Tensor scratch_tensor(\n        scratch_rest_by_depth.tensor<U, 2>());\n    typename TTypes<U>::Vec scratch_vector(scratch_one_by_depth.vec<U>());\n\n    auto x_mean_rest_by_depth =\n        mean.reshape(one_by_depth).broadcast(bcast_spec);\n    auto x_centered = (x_rest_by_depth - x_mean_rest_by_depth);\n    auto coef0_one_by_depth =\n        (variance.reshape(one_by_depth) + epsilon).rsqrt();\n    auto coef0_rest_by_depth = coef0_one_by_depth.broadcast(bcast_spec);\n    auto x_scaled = x_centered * coef0_rest_by_depth;\n\n    auto y_backprop_rest_by_depth =\n        y_backprop.reshape(rest_by_depth).template cast<U>();\n\n    // Compute `scale_backprop_output`:\n    //   scale_backprop =\n    //     (y_backprop_rest_by_depth * x_scaled).sum(reduce_dims)\n    scratch_tensor.device(d) = y_backprop_rest_by_depth * x_scaled;\n    redux_sum_u(d, rest_by_depth, scratch_rest_by_depth, scale_backprop_output);\n\n    // Compute 'offset_backprop_output':\n    //   offset_backprop =\n    //     y_backprop_rest_by_depth.sum(reduce_dims)\n    redux_sum_t(d, rest_by_depth, transformed_y_backprop_input,\n                offset_backprop_output);\n    auto y_backprop_sum = offset_backprop;\n\n    auto y_backprop_sum_one_by_depth = y_backprop_sum.reshape(one_by_depth);\n    auto y_backprop_mean_one_by_depth =\n        y_backprop_sum_one_by_depth * rest_size_inv;\n    auto y_backprop_mean_rest_by_depth =\n        y_backprop_mean_one_by_depth.broadcast(bcast_spec);\n    auto y_backprop_centered =\n        y_backprop_rest_by_depth - y_backprop_mean_rest_by_depth;\n\n    // Compute expression:\n    //   y_backprop_centered_mean =\n    //     (y_backprop_rest_by_depth * x_centered).mean(reduce_dims)\n    scratch_tensor.device(d) = y_backprop_rest_by_depth * x_centered;\n    redux_sum_u(d, rest_by_depth, scratch_rest_by_depth, &scratch_one_by_depth);\n    auto y_backprop_centered_mean =\n        scratch_vector.reshape(one_by_depth) / static_cast<U>(rest_size);\n\n    auto coef1 = (scale.reshape(one_by_depth) * coef0_one_by_depth)\n                     .broadcast(bcast_spec);\n    auto coef2 = (coef0_one_by_depth.square() * y_backprop_centered_mean)\n                     .broadcast(bcast_spec);\n\n    x_backprop.reshape(rest_by_depth).device(d) =\n        (coef1 * (y_backprop_centered - x_centered * coef2)).template cast<T>();\n\n    if (tensor_format == FORMAT_NCHW) {\n      // Perform NHWC to NCHW\n      std::vector<int32> perm = {0, 3, 1, 2};\n      OP_REQUIRES_OK(\n          context, ::tensorflow::DoTranspose(context->eigen_device<CPUDevice>(),\n                                             transformed_x_backprop_output,\n                                             perm, x_backprop_output));\n    }\n  }\n};\n\ntemplate <typename T, typename U>\nstruct FusedBatchNormFreezeGrad<CPUDevice, T, U> {\n  void operator()(OpKernelContext* context, const Tensor& y_backprop_input,\n                  const Tensor& x_input, const Tensor& scale_input,\n                  const Tensor& pop_mean_input,\n                  const Tensor& pop_variance_input, U epsilon,\n                  Tensor* x_backprop_output, Tensor* scale_backprop_output,\n                  Tensor* offset_backprop_output) {\n    typename TTypes<T, 4>::ConstTensor y_backprop(\n        y_backprop_input.tensor<T, 4>());\n    typename TTypes<T, 4>::ConstTensor input(x_input.tensor<T, 4>());\n    typename TTypes<U>::ConstVec scale(scale_input.vec<U>());\n    typename TTypes<U>::ConstVec pop_mean(pop_mean_input.vec<U>());\n    typename TTypes<U>::ConstVec pop_var(pop_variance_input.vec<U>());\n    typename TTypes<T, 4>::Tensor x_backprop(x_backprop_output->tensor<T, 4>());\n    typename TTypes<U>::Vec scale_backprop(scale_backprop_output->vec<U>());\n\n    const int depth = pop_mean.dimension(0);\n    const int rest_size = input.size() / depth;\n\n    const CPUDevice& d = context->eigen_device<CPUDevice>();\n\n    // Allocate two temporary workspaces of [depth] shape.\n    Tensor scratch1_vec, scratch2_vec;\n    OP_REQUIRES_OK(context, context->allocate_temp(DataTypeToEnum<U>::value,\n                                                   {depth}, &scratch1_vec));\n    OP_REQUIRES_OK(context, context->allocate_temp(DataTypeToEnum<U>::value,\n                                                   {depth}, &scratch2_vec));\n\n    // Maybe allocate a temporary workspace of [rest_size, depth] shape.\n    Tensor scratch3_tensor;\n    if (std::is_same<T, U>::value) {\n      OP_REQUIRES(\n          context,\n          scratch3_tensor.CopyFrom(*x_backprop_output, {rest_size, depth}),\n          errors::Internal(\"Failed to copy a tensor\"));\n    } else {\n      OP_REQUIRES_OK(context, context->allocate_temp(DataTypeToEnum<U>::value,\n                                                     {rest_size, depth},\n                                                     &scratch3_tensor));\n    }\n\n    typename TTypes<U>::Vec scratch1(scratch1_vec.vec<U>());\n    typename TTypes<U>::Vec scratch2(scratch2_vec.vec<U>());\n    typename TTypes<U, 2>::Tensor scratch3(scratch3_tensor.tensor<U, 2>());\n\n    Eigen::DSizes<Eigen::Index, 2> rest_by_depth(rest_size, depth);\n#if !defined(EIGEN_HAS_INDEX_LIST)\n    Eigen::DSizes<Eigen::Index, 2> one_by_depth(1, depth);\n    Eigen::array<int, 2> rest_by_one({rest_size, 1});\n#else\n    Eigen::IndexList<Eigen::type2index<1>, Eigen::Index> one_by_depth;\n    one_by_depth.set(1, depth);\n    Eigen::IndexList<Eigen::Index, Eigen::type2index<1>> rest_by_one;\n    rest_by_one.set(0, rest_size);\n#endif\n\n    // Sum reduction along the 0th dimension using custom CPU functor.\n    using ScalarSum = Eigen::internal::scalar_sum_op<U>;\n    const functor::ReduceOuterDimensions<T, U, U, ScalarSum> redux_sum_t;\n    const functor::ReduceOuterDimensions<U, U, U, ScalarSum> redux_sum_u;\n\n    // offset_backprop  = sum(y_backprop)\n    // scale_backprop = y_backprop * ((x - pop_mean) * rsqrt(pop_var + epsilon))\n    // x_backprop = y_backprop * (scale * rsqrt(pop_var + epsilon))\n\n    // NOTE: DEFAULT DEVICE comment is added to expression assignments that\n    // we don't want to be executed in a thread pool.\n\n    auto y_backprop_rest_by_depth =\n        y_backprop.reshape(rest_by_depth).template cast<U>();\n    auto input_rest_by_depth = input.reshape(rest_by_depth).template cast<U>();\n\n    // offset_backprop  = sum(y_backprop)\n    redux_sum_t(d, rest_by_depth, y_backprop_input, offset_backprop_output);\n\n    // scratch1 = rsqrt(pop_var + epsilon)\n    scratch1 = (pop_var + pop_var.constant(epsilon)).rsqrt();  // DEFAULT DEVICE\n\n    // scratch2 = sum(y_backprop * (x - mean))\n    scratch3.device(d) =\n        y_backprop_rest_by_depth *\n        (input_rest_by_depth -\n         pop_mean.reshape(one_by_depth).broadcast(rest_by_one));\n    redux_sum_u(d, rest_by_depth, scratch3_tensor, &scratch2_vec);\n\n    x_backprop.reshape(rest_by_depth).device(d) =\n        (y_backprop_rest_by_depth *\n         ((scratch1.reshape(one_by_depth) * scale.reshape(one_by_depth))\n              .broadcast(rest_by_one)))\n            .template cast<T>();\n    scale_backprop = scratch2 * scratch1;  // DEFAULT DEVICE\n  }\n};\n\n#if !GOOGLE_CUDA\nnamespace {\n// See implementation under GOOGLE_CUDA #ifdef below.\n// This is a CUDA specific feature, do not enable it for non-CUDA builds\nbool BatchnormSpatialPersistentEnabled() { return false; }\n}  // namespace\n#endif\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\nnamespace {\n\nse::dnn::ActivationMode AsDnnActivationMode(\n    const FusedBatchNormActivationMode activation_mode) {\n  switch (activation_mode) {\n    case FusedBatchNormActivationMode::kIdentity:\n      return se::dnn::ActivationMode::kNone;\n    case FusedBatchNormActivationMode::kRelu:\n      return se::dnn::ActivationMode::kRelu;\n  }\n}\n\n#if GOOGLE_CUDA\n// NOTE(ezhulenev): See `BatchnormSpatialPersistentEnabled` documentation in the\n// `cuda_dnn.cc` for details.\nbool BatchnormSpatialPersistentEnabled() {\n#if CUDNN_VERSION >= 7402\n  static bool is_enabled = [] {\n    bool is_enabled = false;\n    TF_CHECK_OK(tensorflow::ReadBoolFromEnvVar(\n        \"TF_USE_CUDNN_BATCHNORM_SPATIAL_PERSISTENT\",\n        /*default_val=*/false, &is_enabled));\n    return is_enabled;\n  }();\n  return is_enabled;\n#else\n  return false;\n#endif\n}\n#endif\n\n}  // namespace\n\ntemplate <typename U, typename T>\nDeviceMemory<U> CastDeviceMemory(Tensor* tensor) {\n  return DeviceMemory<U>::MakeFromByteSize(\n      tensor->template flat<T>().data(),\n      tensor->template flat<T>().size() * sizeof(T));\n}\n\n// A helper to allocate temporary scratch memory for Cudnn BatchNormEx ops. It\n// takes the ownership of the underlying memory. The expectation is that the\n// memory should be alive for the span of the Cudnn BatchNormEx itself.\ntemplate <typename T>\nclass CudnnBatchNormAllocatorInTemp : public ScratchAllocator {\n public:\n  ~CudnnBatchNormAllocatorInTemp() override = default;\n\n  explicit CudnnBatchNormAllocatorInTemp(OpKernelContext* context)\n      : context_(context) {}\n\n  int64_t GetMemoryLimitInBytes() override {\n    return std::numeric_limits<int64_t>::max();\n  }\n\n  StatusOr<DeviceMemory<uint8>> AllocateBytes(int64_t byte_size) override {\n    Tensor temporary_memory;\n    const DataType tf_data_type = DataTypeToEnum<T>::v();\n    int64_t allocate_count =\n        Eigen::divup(byte_size, static_cast<int64_t>(sizeof(T)));\n    Status allocation_status(context_->allocate_temp(\n        tf_data_type, TensorShape({allocate_count}), &temporary_memory));\n    if (!allocation_status.ok()) {\n      return allocation_status;\n    }\n    // Hold the reference of the allocated tensors until the end of the\n    // allocator.\n    allocated_tensors_.push_back(temporary_memory);\n    total_byte_size_ += byte_size;\n    return DeviceMemory<uint8>::MakeFromByteSize(\n        temporary_memory.template flat<T>().data(),\n        temporary_memory.template flat<T>().size() * sizeof(T));\n  }\n\n  int64_t TotalByteSize() const { return total_byte_size_; }\n\n  Tensor get_allocated_tensor(int index) const {\n    return allocated_tensors_[index];\n  }\n\n private:\n  int64_t total_byte_size_ = 0;\n  OpKernelContext* context_;  // not owned\n  std::vector<Tensor> allocated_tensors_;\n};\n\n// A helper to allocate memory for Cudnn BatchNormEx as a kernel output. It is\n// used by forward pass kernel to feed the output to the backward pass.\n// The memory is expected to live long enough after the backward pass is\n// finished.\ntemplate <typename T>\nclass CudnnBatchNormAllocatorInOutput : public ScratchAllocator {\n public:\n  ~CudnnBatchNormAllocatorInOutput() override {\n    if (!output_allocated) {\n      Tensor* dummy_reserve_space = nullptr;\n      OP_REQUIRES_OK(context_, context_->allocate_output(output_index_, {},\n                                                         &dummy_reserve_space));\n    }\n  }\n\n  CudnnBatchNormAllocatorInOutput(OpKernelContext* context, int output_index)\n      : context_(context), output_index_(output_index) {}\n\n  int64_t GetMemoryLimitInBytes() override {\n    return std::numeric_limits<int64_t>::max();\n  }\n\n  StatusOr<DeviceMemory<uint8>> AllocateBytes(int64_t byte_size) override {\n    output_allocated = true;\n    DCHECK(total_byte_size_ == 0)\n        << \"Reserve space allocator can only be called once\";\n    int64_t allocate_count =\n        Eigen::divup(byte_size, static_cast<int64_t>(sizeof(T)));\n\n    Tensor* temporary_memory = nullptr;\n    Status allocation_status(context_->allocate_output(\n        output_index_, TensorShape({allocate_count}), &temporary_memory));\n    if (!allocation_status.ok()) {\n      return allocation_status;\n    }\n    total_byte_size_ += byte_size;\n    auto memory_uint8 = DeviceMemory<uint8>::MakeFromByteSize(\n        temporary_memory->template flat<T>().data(),\n        temporary_memory->template flat<T>().size() * sizeof(T));\n    return StatusOr<DeviceMemory<uint8>>(memory_uint8);\n  }\n\n  int64_t TotalByteSize() { return total_byte_size_; }\n\n private:\n  int64_t total_byte_size_ = 0;\n  OpKernelContext* context_;  // not owned\n  int output_index_;\n  bool output_allocated = false;\n};\n\ntemplate <typename T, typename U, bool is_training>\nstruct FusedBatchNorm<GPUDevice, T, U, is_training> {\n  void operator()(OpKernelContext* context, const Tensor& x,\n                  const Tensor& scale, const Tensor& offset,\n                  const Tensor& estimated_mean,\n                  const Tensor& estimated_variance, const Tensor* side_input,\n                  U epsilon, U exponential_avg_factor,\n                  FusedBatchNormActivationMode activation_mode, Tensor* y,\n                  Tensor* batch_mean, Tensor* batch_var, Tensor* saved_mean,\n                  Tensor* saved_inv_var, TensorFormat tensor_format,\n                  bool use_reserved_space) {\n    auto* stream = context->op_device_context()->stream();\n    OP_REQUIRES(context, stream, errors::Internal(\"No GPU stream available\"));\n\n    const int64_t batch_size = GetTensorDim(x, tensor_format, 'N');\n    const int64_t channels = GetTensorDim(x, tensor_format, 'C');\n    const int64_t height = GetTensorDim(x, tensor_format, 'H');\n    const int64_t width = GetTensorDim(x, tensor_format, 'W');\n\n    // If use_reserved_space we have reserve_space_3 output (only in\n    // FusedBatchNormV3 op).\n\n#if GOOGLE_CUDA\n    // Check if cuDNN batch normalization has a fast NHWC implementation:\n    //   (1) In inference mode it's always fast.\n    //   (2) Tensorflow enabled batchnorm spatial persistence, we are called\n    //   from\n    //       FusedBatchNormV3, i.e. use_reserved_space is true.\n    const bool fast_nhwc_batch_norm =\n        !is_training ||\n        (BatchnormSpatialPersistentEnabled() &&\n         DataTypeToEnum<T>::value == DT_HALF && use_reserved_space);\n#else\n    // fast NHWC implementation is a CUDA only feature\n    const bool fast_nhwc_batch_norm = false;\n#endif\n\n    // If input tensor is in NHWC format, and we have a fast cuDNN\n    // implementation, there is no need to do data format conversion.\n    TensorFormat compute_format =\n        fast_nhwc_batch_norm && tensor_format == FORMAT_NHWC ? FORMAT_NHWC\n                                                             : FORMAT_NCHW;\n\n    VLOG(2) << \"FusedBatchNorm:\"\n            << \" batch_size: \" << batch_size << \" channels: \" << channels\n            << \" height: \" << height << \" width:\" << width\n            << \" x shape: \" << x.shape().DebugString()\n            << \" scale shape: \" << scale.shape().DebugString()\n            << \" offset shape: \" << offset.shape().DebugString()\n            << \" activation mode: \" << ToString(activation_mode)\n            << \" tensor format: \" << ToString(tensor_format)\n            << \" compute format: \" << ToString(compute_format);\n\n    auto maybe_make_dummy_output = [context, use_reserved_space]() -> Status {\n      if (use_reserved_space) {\n        Tensor* dummy_reserve_space = nullptr;\n        return context->allocate_output(5, {}, &dummy_reserve_space);\n      }\n      return Status::OK();\n    };\n\n    // If input is empty, return NaN mean/variance\n    if (x.shape().num_elements() == 0) {\n      OP_REQUIRES_OK(context, maybe_make_dummy_output());\n      functor::SetNanFunctor<GPUDevice, U> f;\n      f(context->eigen_device<GPUDevice>(), batch_mean->flat<U>());\n      f(context->eigen_device<GPUDevice>(), batch_var->flat<U>());\n      return;\n    }\n\n    // In inference mode we use custom CUDA kernel, because cuDNN does not\n    // support side input and activations for inference.\n    const bool has_side_input = side_input != nullptr;\n    const bool has_activation =\n        activation_mode != FusedBatchNormActivationMode::kIdentity;\n\n    if (!is_training && (has_side_input || has_activation)) {\n      OP_REQUIRES_OK(context, maybe_make_dummy_output());\n      FusedBatchNormInferenceFunctor<GPUDevice, T, U> inference_functor;\n\n      if (has_side_input) {\n        inference_functor(context, tensor_format, x.tensor<T, 4>(),\n                          scale.vec<U>(), offset.vec<U>(),\n                          estimated_mean.vec<U>(), estimated_variance.vec<U>(),\n                          side_input->tensor<T, 4>(), epsilon, activation_mode,\n                          y->tensor<T, 4>());\n      } else {\n        typename TTypes<T, 4>::ConstTensor empty_tensor(nullptr, 0, 0, 0, 0);\n        inference_functor(context, tensor_format, x.tensor<T, 4>(),\n                          scale.vec<U>(), offset.vec<U>(),\n                          estimated_mean.vec<U>(), estimated_variance.vec<U>(),\n                          empty_tensor, epsilon, activation_mode,\n                          y->tensor<T, 4>());\n      }\n      return;\n    }\n\n    Tensor x_maybe_transformed = x;\n    Tensor x_transformed;\n    Tensor y_transformed;\n    se::DeviceMemory<T> y_ptr;\n\n    if (tensor_format == compute_format) {\n      y_ptr = StreamExecutorUtil::AsDeviceMemory<T>(*y);\n    } else if (tensor_format == FORMAT_NHWC && compute_format == FORMAT_NCHW) {\n      OP_REQUIRES_OK(context, context->allocate_temp(\n                                  DataTypeToEnum<T>::value,\n                                  ShapeFromFormat(compute_format, batch_size,\n                                                  height, width, channels),\n                                  &x_transformed));\n      functor::NHWCToNCHW<GPUDevice, T, 4>()(\n          context->eigen_device<GPUDevice>(),\n          const_cast<const Tensor&>(x_maybe_transformed).tensor<T, 4>(),\n          x_transformed.tensor<T, 4>());\n      x_maybe_transformed = x_transformed;\n\n      OP_REQUIRES_OK(context, context->allocate_temp(\n                                  DataTypeToEnum<T>::value,\n                                  ShapeFromFormat(compute_format, batch_size,\n                                                  height, width, channels),\n                                  &y_transformed));\n      y_ptr = StreamExecutorUtil::AsDeviceMemory<T>(y_transformed);\n    } else {\n      context->SetStatus(errors::Internal(\n          \"Unsupported tensor format: \", ToString(tensor_format),\n          \" and compute format: \", ToString(compute_format)));\n      return;\n    }\n\n    const se::dnn::DataLayout data_layout =\n        compute_format == FORMAT_NHWC ? se::dnn::DataLayout::kBatchYXDepth\n                                      : se::dnn::DataLayout::kBatchDepthYX;\n\n    se::dnn::BatchDescriptor x_desc;\n    x_desc.set_count(batch_size)\n        .set_feature_map_count(channels)\n        .set_height(height)\n        .set_width(width)\n        .set_layout(data_layout);\n\n    se::dnn::BatchDescriptor scale_offset_desc;\n    scale_offset_desc.set_count(1)\n        .set_feature_map_count(channels)\n        .set_height(1)\n        .set_width(1)\n        .set_layout(se::dnn::DataLayout::kBatchDepthYX);\n\n    auto x_ptr = StreamExecutorUtil::AsDeviceMemory<T>(x_maybe_transformed);\n    auto scale_ptr = StreamExecutorUtil::AsDeviceMemory<U>(scale);\n    auto offset_ptr = StreamExecutorUtil::AsDeviceMemory<U>(offset);\n    auto estimated_mean_ptr =\n        StreamExecutorUtil::AsDeviceMemory<U>(estimated_mean);\n    auto estimated_variance_ptr =\n        StreamExecutorUtil::AsDeviceMemory<U>(estimated_variance);\n    auto side_input_ptr =\n        side_input != nullptr\n            ? StreamExecutorUtil::AsDeviceMemory<T>(*side_input)\n            : se::DeviceMemory<T>();\n    auto batch_mean_ptr = StreamExecutorUtil::AsDeviceMemory<U>(*batch_mean);\n\n    auto batch_var_ptr = StreamExecutorUtil::AsDeviceMemory<U>(*batch_var);\n    auto saved_mean_ptr = StreamExecutorUtil::AsDeviceMemory<U>(*saved_mean);\n    auto saved_inv_var_ptr =\n        StreamExecutorUtil::AsDeviceMemory<U>(*saved_inv_var);\n\n    std::unique_ptr<functor::CudnnBatchNormAllocatorInOutput<U>>\n        reserve_space_allocator;\n    std::unique_ptr<functor::CudnnBatchNormAllocatorInTemp<uint8>>\n        workspace_allocator;\n    if (use_reserved_space) {\n      reserve_space_allocator.reset(\n          new functor::CudnnBatchNormAllocatorInOutput<U>(context, 5));\n      workspace_allocator.reset(\n          new functor::CudnnBatchNormAllocatorInTemp<uint8>(context));\n    }\n    if (!batch_mean->SharesBufferWith(estimated_mean) &&\n        exponential_avg_factor != 1.0f) {\n      OP_REQUIRES(\n          context,\n          stream\n              ->ThenMemcpyD2D(&batch_mean_ptr, estimated_mean_ptr,\n                              estimated_mean.NumElements() * sizeof(U))\n              .ok(),\n          errors::Internal(\"MatrixTriangularSolveOp: failed to copy rhs \"\n                           \"from device\"));\n    }\n    if (!batch_var->SharesBufferWith(estimated_variance) &&\n        exponential_avg_factor != 1.0f) {\n      OP_REQUIRES(\n          context,\n          stream\n              ->ThenMemcpyD2D(&batch_var_ptr, estimated_variance_ptr,\n                              estimated_variance.NumElements() * sizeof(U))\n              .ok(),\n          errors::Internal(\"MatrixTriangularSolveOp: failed to copy rhs \"\n                           \"from device\"));\n    }\n    bool cudnn_launch_status =\n        stream\n            ->ThenBatchNormalizationForward(\n                x_ptr, scale_ptr, offset_ptr, estimated_mean_ptr,\n                estimated_variance_ptr, side_input_ptr, x_desc,\n                scale_offset_desc, static_cast<double>(epsilon),\n                static_cast<double>(exponential_avg_factor),\n                AsDnnActivationMode(activation_mode), &y_ptr, &batch_mean_ptr,\n                &batch_var_ptr, &saved_mean_ptr, &saved_inv_var_ptr,\n                is_training, reserve_space_allocator.get(),\n                workspace_allocator.get())\n            .ok();\n\n    if (!cudnn_launch_status) {\n      context->SetStatus(\n          errors::Internal(\"cuDNN launch failure : input shape (\",\n                           x.shape().DebugString(), \")\"));\n      return;\n    }\n\n    if (tensor_format == FORMAT_NHWC && compute_format == FORMAT_NCHW) {\n      functor::NCHWToNHWC<GPUDevice, T, 4>()(\n          context->eigen_device<GPUDevice>(),\n          const_cast<const Tensor&>(y_transformed).tensor<T, 4>(),\n          y->tensor<T, 4>());\n    }\n  }\n};\n\ntemplate <typename T, typename U>\nstruct FusedBatchNormGrad<GPUDevice, T, U> {\n  void operator()(OpKernelContext* context, const Tensor& y_backprop,\n                  const Tensor& x, const Tensor& scale, const Tensor* offset,\n                  const Tensor& mean, const Tensor& inv_variance,\n                  const Tensor* y, U epsilon,\n                  FusedBatchNormActivationMode activation_mode,\n                  Tensor* x_backprop, Tensor* scale_backprop,\n                  Tensor* offset_backprop, Tensor* side_input_backprop,\n                  bool use_reserved_space, TensorFormat tensor_format) {\n    auto* stream = context->op_device_context()->stream();\n    OP_REQUIRES(context, stream, errors::Internal(\"No GPU stream available\"));\n\n    const int64_t batch_size = GetTensorDim(x, tensor_format, 'N');\n    const int64_t channels = GetTensorDim(x, tensor_format, 'C');\n    const int64_t height = GetTensorDim(x, tensor_format, 'H');\n    const int64_t width = GetTensorDim(x, tensor_format, 'W');\n\n#if GOOGLE_CUDA\n    // Check if cuDNN batch normalization has a fast NHWC implementation:\n    //   (1) Tensorflow enabled batchnorm spatial persistence, and\n    //       FusedBatchNormGradV3 passed non-null reserve space and allocator.\n    const bool fast_nhwc_batch_norm = BatchnormSpatialPersistentEnabled() &&\n                                      DataTypeToEnum<T>::value == DT_HALF &&\n                                      use_reserved_space;\n#else\n    // fast NHWC implementation is a CUDA only feature\n    const bool fast_nhwc_batch_norm = false;\n#endif\n\n    // If input tensor is in NHWC format, and we have a fast cuDNN\n    // implementation, there is no need to do data format conversion.\n    TensorFormat compute_format =\n        fast_nhwc_batch_norm && tensor_format == FORMAT_NHWC ? FORMAT_NHWC\n                                                             : FORMAT_NCHW;\n\n    VLOG(2) << \"FusedBatchNormGrad:\"\n            << \" batch_size: \" << batch_size << \" channels: \" << channels\n            << \" height: \" << height << \" width: \" << width\n            << \" y_backprop shape: \" << y_backprop.shape().DebugString()\n            << \" x shape: \" << x.shape().DebugString()\n            << \" scale shape: \" << scale.shape().DebugString()\n            << \" activation mode: \" << ToString(activation_mode)\n            << \" tensor format: \" << ToString(tensor_format)\n            << \" compute format: \" << ToString(compute_format);\n\n    // Inputs\n    Tensor y_backprop_maybe_transformed = y_backprop;\n    Tensor x_maybe_transformed = x;\n    Tensor y_backprop_transformed;\n    Tensor x_transformed;\n\n    // Outputs\n    Tensor x_backprop_transformed;\n    se::DeviceMemory<T> x_backprop_ptr;\n\n    if (tensor_format == compute_format) {\n      x_backprop_ptr = StreamExecutorUtil::AsDeviceMemory<T>(*x_backprop);\n    } else if (tensor_format == FORMAT_NHWC && compute_format == FORMAT_NCHW) {\n      // Transform inputs from 'NHWC' to 'NCHW'\n      OP_REQUIRES_OK(context, context->allocate_temp(\n                                  DataTypeToEnum<T>::value,\n                                  ShapeFromFormat(FORMAT_NCHW, batch_size,\n                                                  height, width, channels),\n                                  &y_backprop_transformed));\n      functor::NHWCToNCHW<GPUDevice, T, 4>()(\n          context->eigen_device<GPUDevice>(),\n          const_cast<const Tensor&>(y_backprop_maybe_transformed)\n              .tensor<T, 4>(),\n          y_backprop_transformed.tensor<T, 4>());\n      y_backprop_maybe_transformed = y_backprop_transformed;\n\n      OP_REQUIRES_OK(context, context->allocate_temp(\n                                  DataTypeToEnum<T>::value,\n                                  ShapeFromFormat(FORMAT_NCHW, batch_size,\n                                                  height, width, channels),\n                                  &x_transformed));\n      functor::NHWCToNCHW<GPUDevice, T, 4>()(\n          context->eigen_device<GPUDevice>(),\n          const_cast<const Tensor&>(x_maybe_transformed).tensor<T, 4>(),\n          x_transformed.tensor<T, 4>());\n      x_maybe_transformed = x_transformed;\n\n      // Allocate memory for transformed outputs in 'NCHW'\n      OP_REQUIRES_OK(context, context->allocate_temp(\n                                  DataTypeToEnum<T>::value,\n                                  ShapeFromFormat(FORMAT_NCHW, batch_size,\n                                                  height, width, channels),\n                                  &x_backprop_transformed));\n      x_backprop_ptr =\n          StreamExecutorUtil::AsDeviceMemory<T>(x_backprop_transformed);\n    } else {\n      context->SetStatus(errors::Internal(\n          \"Unsupported tensor format: \", ToString(tensor_format),\n          \" and compute format: \", ToString(compute_format)));\n      return;\n    }\n\n    const se::dnn::DataLayout data_layout =\n        compute_format == FORMAT_NHWC ? se::dnn::DataLayout::kBatchYXDepth\n                                      : se::dnn::DataLayout::kBatchDepthYX;\n\n    se::dnn::BatchDescriptor x_desc;\n    x_desc.set_count(batch_size)\n        .set_feature_map_count(channels)\n        .set_height(height)\n        .set_width(width)\n        .set_layout(data_layout);\n\n    se::dnn::BatchDescriptor scale_offset_desc;\n    scale_offset_desc.set_count(1)\n        .set_feature_map_count(channels)\n        .set_height(1)\n        .set_width(1)\n        .set_layout(se::dnn::DataLayout::kBatchDepthYX);\n\n    auto y_backprop_ptr =\n        StreamExecutorUtil::AsDeviceMemory<T>(y_backprop_maybe_transformed);\n    auto x_ptr = StreamExecutorUtil::AsDeviceMemory<T>(x_maybe_transformed);\n    auto scale_ptr = StreamExecutorUtil::AsDeviceMemory<U>(scale);\n    auto offset_ptr = offset != nullptr\n                          ? StreamExecutorUtil::AsDeviceMemory<U>(*offset)\n                          : se::DeviceMemory<U>();\n    auto mean_ptr = StreamExecutorUtil::AsDeviceMemory<U>(mean);\n    auto inv_variance_ptr = StreamExecutorUtil::AsDeviceMemory<U>(inv_variance);\n    auto y_ptr = y != nullptr ? StreamExecutorUtil::AsDeviceMemory<T>(*y)\n                              : se::DeviceMemory<T>();\n    auto scale_backprop_ptr =\n        StreamExecutorUtil::AsDeviceMemory<U>(*scale_backprop);\n    auto offset_backprop_ptr =\n        StreamExecutorUtil::AsDeviceMemory<U>(*offset_backprop);\n    auto side_input_backprop_ptr =\n        side_input_backprop != nullptr\n            ? StreamExecutorUtil::AsDeviceMemory<T>(*side_input_backprop)\n            : se::DeviceMemory<T>();\n\n    std::unique_ptr<functor::CudnnBatchNormAllocatorInTemp<uint8>>\n        workspace_allocator;\n    DeviceMemory<uint8>* reserve_space_data_ptr = nullptr;\n    DeviceMemory<uint8> reserve_space_data;\n#if CUDNN_VERSION >= 7402\n    if (use_reserved_space) {\n      const Tensor& reserve_space = context->input(5);\n      workspace_allocator.reset(\n          new functor::CudnnBatchNormAllocatorInTemp<uint8>(context));\n\n      // the cudnn kernel outputs inverse variance in forward and reuse it in\n      // backward\n      if (reserve_space.dims() != 0) {\n        reserve_space_data = functor::CastDeviceMemory<uint8, U>(\n            const_cast<Tensor*>(&reserve_space));\n        reserve_space_data_ptr = &reserve_space_data;\n      }\n    }\n#endif  // CUDNN_VERSION >= 7402\n\n    bool cudnn_launch_status =\n        stream\n            ->ThenBatchNormalizationBackward(\n                y_backprop_ptr, x_ptr, scale_ptr, offset_ptr, mean_ptr,\n                inv_variance_ptr, y_ptr, x_desc, scale_offset_desc,\n                static_cast<double>(epsilon),\n                AsDnnActivationMode(activation_mode), &x_backprop_ptr,\n                &scale_backprop_ptr, &offset_backprop_ptr,\n                &side_input_backprop_ptr, reserve_space_data_ptr,\n                workspace_allocator.get())\n            .ok();\n\n    if (!cudnn_launch_status) {\n      context->SetStatus(\n          errors::Internal(\"cuDNN launch failure : input shape (\",\n                           x.shape().DebugString(), \")\"));\n    }\n    if (tensor_format == FORMAT_NHWC && compute_format == FORMAT_NCHW) {\n      functor::NCHWToNHWC<GPUDevice, T, 4>()(\n          context->eigen_device<GPUDevice>(),\n          const_cast<const Tensor&>(x_backprop_transformed).tensor<T, 4>(),\n          x_backprop->tensor<T, 4>());\n    }\n  }\n};\n\n// Forward declarations of the functor specializations for GPU.\n#define DECLARE_GPU_SPEC(T, U)                                                 \\\n  template <>                                                                  \\\n  void FusedBatchNormFreezeGrad<GPUDevice, T, U>::operator()(                  \\\n      OpKernelContext* context, const Tensor& y_backprop_input,                \\\n      const Tensor& x_input, const Tensor& scale_input,                        \\\n      const Tensor& mean_input, const Tensor& variance_input, U epsilon,       \\\n      Tensor* x_backprop_output, Tensor* scale_backprop_output,                \\\n      Tensor* offset_backprop_output);                                         \\\n  extern template struct FusedBatchNormFreezeGrad<GPUDevice, T, U>;            \\\n  template <>                                                                  \\\n  void FusedBatchNormInferenceFunctor<GPUDevice, T, U>::operator()(            \\\n      OpKernelContext* context, TensorFormat tensor_format,                    \\\n      typename TTypes<T, 4>::ConstTensor in,                                   \\\n      typename TTypes<U>::ConstVec scale, typename TTypes<U>::ConstVec offset, \\\n      typename TTypes<U>::ConstVec estimated_mean,                             \\\n      typename TTypes<U>::ConstVec estimated_variance,                         \\\n      typename TTypes<T, 4>::ConstTensor side_input, U epsilon,                \\\n      FusedBatchNormActivationMode activation_mode,                            \\\n      typename TTypes<T, 4>::Tensor out);                                      \\\n  extern template struct FusedBatchNormInferenceFunctor<GPUDevice, T, U>;\n\nDECLARE_GPU_SPEC(float, float);\nDECLARE_GPU_SPEC(Eigen::half, float);\n\n#undef DECLARE_GPU_SPEC\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n}  // namespace functor\n\ntemplate <typename Device, typename T, typename U>\nclass FusedBatchNormOpBase : public OpKernel {\n  using FbnActivationMode = functor::FusedBatchNormActivationMode;\n\n protected:\n  explicit FusedBatchNormOpBase(OpKernelConstruction* context,\n                                bool is_batch_norm_ex = false)\n      : OpKernel(context) {\n    float epsilon;\n    OP_REQUIRES_OK(context, context->GetAttr(\"epsilon\", &epsilon));\n    epsilon_ = U(epsilon);\n    float exponential_avg_factor;\n    OP_REQUIRES_OK(context, context->GetAttr(\"exponential_avg_factor\",\n                                             &exponential_avg_factor));\n    exponential_avg_factor_ = U(exponential_avg_factor);\n    string tensor_format;\n    OP_REQUIRES_OK(context, context->GetAttr(\"data_format\", &tensor_format));\n    OP_REQUIRES(context, FormatFromString(tensor_format, &tensor_format_),\n                errors::InvalidArgument(\"Invalid data format\"));\n    OP_REQUIRES_OK(context, context->GetAttr(\"is_training\", &is_training_));\n\n    if (!is_batch_norm_ex) {\n      has_side_input_ = false;\n      activation_mode_ = FbnActivationMode::kIdentity;\n    } else {\n      OP_REQUIRES_OK(context, ParseActivationMode(context, &activation_mode_));\n\n      int num_side_inputs;\n      OP_REQUIRES_OK(context,\n                     context->GetAttr(\"num_side_inputs\", &num_side_inputs));\n      OP_REQUIRES(context, num_side_inputs >= 0 && num_side_inputs <= 1,\n                  errors::InvalidArgument(\n                      \"FusedBatchNorm accepts at most one side input.\"));\n      has_side_input_ = (num_side_inputs == 1);\n      if (has_side_input_ && is_training_) {\n        OP_REQUIRES(\n            context, activation_mode_ != FbnActivationMode::kIdentity,\n            errors::InvalidArgument(\"Identity activation is not supported with \"\n                                    \"non-empty side input\"));\n      }\n    }\n\n    if (activation_mode_ != FbnActivationMode::kIdentity && is_training_) {\n      // NOTE(ezhulenev): Following requirements are coming from implementation\n      // details of cudnnBatchNormalizationForwardTrainingEx used in training\n      // mode. In inference mode we call custom CUDA kernel that supports all\n      // data formats and data types.\n      OP_REQUIRES(context, DataTypeToEnum<T>::value == DT_HALF,\n                  errors::InvalidArgument(\"FusedBatchNorm with activation \"\n                                          \"supports only DT_HALF data type.\"));\n      OP_REQUIRES(context, tensor_format_ == FORMAT_NHWC,\n                  errors::InvalidArgument(\"FusedBatchNorm with activation \"\n                                          \"supports only NHWC tensor format.\"));\n      OP_REQUIRES(context, functor::BatchnormSpatialPersistentEnabled(),\n                  errors::InvalidArgument(\n                      \"FusedBatchNorm with activation must run with cuDNN \"\n                      \"spatial persistence mode enabled.\"));\n    }\n  }\n\n  // If use_reserved_space is true, we need to handle the 5th output (a reserved\n  // space) and a new cudnn batch norm will be called if the version > 7.4.2.\n  // If use_reserved_space is false, we don't have 5th output.\n  virtual void ComputeWithReservedSpace(OpKernelContext* context,\n                                        bool use_reserved_space) {\n    Tensor x = context->input(0);\n    const Tensor& scale = context->input(1);\n    const Tensor& offset = context->input(2);\n    const Tensor& estimated_mean = context->input(3);\n    const Tensor& estimated_variance = context->input(4);\n    const Tensor* side_input = has_side_input_ ? &context->input(5) : nullptr;\n\n    OP_REQUIRES(context, x.dims() == 4 || x.dims() == 5,\n                errors::InvalidArgument(\"input must be 4 or 5-dimensional\",\n                                        x.shape().DebugString()));\n    OP_REQUIRES(context, scale.dims() == 1,\n                errors::InvalidArgument(\"scale must be 1-dimensional\",\n                                        scale.shape().DebugString()));\n    OP_REQUIRES(context, offset.dims() == 1,\n                errors::InvalidArgument(\"offset must be 1-dimensional\",\n                                        offset.shape().DebugString()));\n    OP_REQUIRES(context, estimated_mean.dims() == 1,\n                errors::InvalidArgument(\"estimated_mean must be 1-dimensional\",\n                                        estimated_mean.shape().DebugString()));\n    OP_REQUIRES(\n        context, estimated_variance.dims() == 1,\n        errors::InvalidArgument(\"estimated_variance must be 1-dimensional\",\n                                estimated_variance.shape().DebugString()));\n    bool use_reshape = (x.dims() == 5);\n    auto x_shape = x.shape();\n    TensorShape dest_shape;\n    if (use_reshape) {\n      const int64_t in_batch = GetTensorDim(x, tensor_format_, 'N');\n      int64_t in_planes = GetTensorDim(x, tensor_format_, '0');\n      int64_t in_rows = GetTensorDim(x, tensor_format_, '1');\n      int64_t in_cols = GetTensorDim(x, tensor_format_, '2');\n      const int64_t in_depth = GetTensorDim(x, tensor_format_, 'C');\n      dest_shape = ShapeFromFormat(tensor_format_, in_batch,\n                                   {{in_planes, in_rows * in_cols}}, in_depth);\n      OP_REQUIRES(context, x.CopyFrom(x, dest_shape),\n                  errors::InvalidArgument(\"Error during tensor copy.\"));\n    }\n\n    const auto num_channels = GetTensorDim(x, tensor_format_, 'C');\n    OP_REQUIRES(\n        context, scale.NumElements() == num_channels,\n        errors::InvalidArgument(\"scale must have the same number of elements \"\n                                \"as the channels of x, got \",\n                                scale.NumElements(), \" and \", num_channels));\n    OP_REQUIRES(\n        context, offset.NumElements() == num_channels,\n        errors::InvalidArgument(\"offset must have the same number of elements \"\n                                \"as the channels of x, got \",\n                                offset.NumElements(), \" and \", num_channels));\n    if (estimated_mean.NumElements() != 0) {\n      OP_REQUIRES(context, estimated_mean.NumElements() == num_channels,\n                  errors::InvalidArgument(\n                      \"mean must be empty or have the same number of \"\n                      \"elements as the channels of x, got \",\n                      estimated_mean.NumElements(), \" and \", num_channels));\n    }\n    if (estimated_variance.NumElements() != 0) {\n      OP_REQUIRES(context, estimated_variance.NumElements() == num_channels,\n                  errors::InvalidArgument(\n                      \"variance must be empty or have the same number of \"\n                      \"elements as the channels of x, got \",\n                      estimated_variance.NumElements(), \" and \", num_channels));\n    }\n\n    if (has_side_input_) {\n      OP_REQUIRES(context, side_input->shape() == x.shape(),\n                  errors::InvalidArgument(\n                      \"side_input shape must be equal to input shape: \",\n                      side_input->shape().DebugString(),\n                      \" != \", x.shape().DebugString()));\n    }\n\n    if (activation_mode_ != FbnActivationMode::kIdentity) {\n      // NOTE(ezhulenev): This requirement is coming from implementation\n      // details of cudnnBatchNormalizationForwardTrainingEx.\n      OP_REQUIRES(\n          context, !is_training_ || num_channels % 4 == 0,\n          errors::InvalidArgument(\"FusedBatchNorm with activation requires \"\n                                  \"channel dimension to be a multiple of 4.\"));\n    }\n\n    Tensor* y = nullptr;\n    auto alloc_shape = use_reshape ? dest_shape : x_shape;\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                {0}, 0, alloc_shape, &y));\n\n    Tensor* batch_mean = nullptr;\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                {3}, 1, scale.shape(), &batch_mean));\n    Tensor* batch_var = nullptr;\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                {4}, 2, scale.shape(), &batch_var));\n    Tensor* saved_mean = nullptr;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(3, scale.shape(), &saved_mean));\n    Tensor* saved_maybe_inv_var = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(4, scale.shape(),\n                                                     &saved_maybe_inv_var));\n\n    if (is_training_) {\n      functor::FusedBatchNorm<Device, T, U, true>()(\n          context, x, scale, offset, estimated_mean, estimated_variance,\n          side_input, epsilon_, exponential_avg_factor_, activation_mode_, y,\n          batch_mean, batch_var, saved_mean, saved_maybe_inv_var,\n          tensor_format_, use_reserved_space);\n    } else {\n      functor::FusedBatchNorm<Device, T, U, false>()(\n          context, x, scale, offset, estimated_mean, estimated_variance,\n          side_input, epsilon_, exponential_avg_factor_, activation_mode_, y,\n          batch_mean, batch_var, saved_mean, saved_maybe_inv_var,\n          tensor_format_, use_reserved_space);\n    }\n    if (use_reshape) {\n      OP_REQUIRES(context, y->CopyFrom(*y, x_shape),\n                  errors::InvalidArgument(\"Error during tensor copy.\"));\n    }\n  }\n\n private:\n  U epsilon_;\n  U exponential_avg_factor_;\n  TensorFormat tensor_format_;\n  bool is_training_;\n  bool has_side_input_;\n  FbnActivationMode activation_mode_;\n};\n\ntemplate <typename Device, typename T, typename U>\nclass FusedBatchNormOp : public FusedBatchNormOpBase<Device, T, U> {\n public:\n  explicit FusedBatchNormOp(OpKernelConstruction* context)\n      : FusedBatchNormOpBase<Device, T, U>(context) {}\n\n  void Compute(OpKernelContext* context) override {\n    FusedBatchNormOpBase<Device, T, U>::ComputeWithReservedSpace(context,\n                                                                 false);\n  }\n};\n\ntemplate <typename Device, typename T, typename U>\nclass FusedBatchNormOpV3 : public FusedBatchNormOpBase<Device, T, U> {\n public:\n  explicit FusedBatchNormOpV3(OpKernelConstruction* context)\n      : FusedBatchNormOpBase<Device, T, U>(context) {}\n\n  void Compute(OpKernelContext* context) override {\n    FusedBatchNormOpBase<Device, T, U>::ComputeWithReservedSpace(context, true);\n  }\n};\n\ntemplate <typename Device, typename T, typename U>\nclass FusedBatchNormOpEx : public FusedBatchNormOpBase<Device, T, U> {\n  static constexpr bool kWithSideInputAndActivation = true;\n\n public:\n  explicit FusedBatchNormOpEx(OpKernelConstruction* context)\n      : FusedBatchNormOpBase<Device, T, U>(context,\n                                           kWithSideInputAndActivation) {}\n\n  void Compute(OpKernelContext* context) override {\n    FusedBatchNormOpBase<Device, T, U>::ComputeWithReservedSpace(context, true);\n  }\n};\n\ntemplate <typename Device, typename T, typename U>\nclass FusedBatchNormGradOpBase : public OpKernel {\n  using FbnActivationMode = functor::FusedBatchNormActivationMode;\n\n protected:\n  explicit FusedBatchNormGradOpBase(OpKernelConstruction* context,\n                                    bool is_batch_norm_grad_ex = false)\n      : OpKernel(context) {\n    float epsilon;\n    OP_REQUIRES_OK(context, context->GetAttr(\"epsilon\", &epsilon));\n    epsilon_ = U(epsilon);\n    string tensor_format;\n    OP_REQUIRES_OK(context, context->GetAttr(\"data_format\", &tensor_format));\n    OP_REQUIRES(context, FormatFromString(tensor_format, &tensor_format_),\n                errors::InvalidArgument(\"Invalid data format\"));\n    OP_REQUIRES_OK(context, context->GetAttr(\"is_training\", &is_training_));\n    if (!is_batch_norm_grad_ex) {\n      has_side_input_ = false;\n      activation_mode_ = FbnActivationMode::kIdentity;\n    } else {\n      OP_REQUIRES_OK(context, ParseActivationMode(context, &activation_mode_));\n\n      int num_side_inputs;\n      OP_REQUIRES_OK(context,\n                     context->GetAttr(\"num_side_inputs\", &num_side_inputs));\n      OP_REQUIRES(context, num_side_inputs >= 0 && num_side_inputs <= 1,\n                  errors::InvalidArgument(\n                      \"FusedBatchNormGrad accepts at most one side input.\"));\n      has_side_input_ = (num_side_inputs == 1);\n      if (has_side_input_ && is_training_) {\n        OP_REQUIRES(\n            context, activation_mode_ != FbnActivationMode::kIdentity,\n            errors::InvalidArgument(\"Identity activation is not supported with \"\n                                    \"non-empty side input\"));\n      }\n    }\n\n    if (activation_mode_ != FbnActivationMode::kIdentity && is_training_) {\n      // NOTE(kaixih@nvidia): Following requirements are coming from\n      // implementation details of cudnnBatchNormalizationBackwardEx used in\n      // training mode.\n      OP_REQUIRES(context, DataTypeToEnum<T>::value == DT_HALF,\n                  errors::InvalidArgument(\"FusedBatchNormGrad with activation \"\n                                          \"supports only DT_HALF data type.\"));\n      OP_REQUIRES(context, tensor_format_ == FORMAT_NHWC,\n                  errors::InvalidArgument(\"FusedBatchNormGrad with activation \"\n                                          \"supports only NHWC tensor format.\"));\n      OP_REQUIRES(context, functor::BatchnormSpatialPersistentEnabled(),\n                  errors::InvalidArgument(\n                      \"FusedBatchNormGrad with activation must run with cuDNN \"\n                      \"spatial persistence mode enabled.\"));\n    }\n  }\n\n  virtual void ComputeWithReservedSpace(OpKernelContext* context,\n                                        bool use_reserved_space) {\n    Tensor y_backprop = context->input(0);\n    Tensor x = context->input(1);\n    const Tensor& scale = context->input(2);\n    // When is_training=True, batch mean and variance/inverted variance are\n    // saved in the forward pass to be reused here. When is_training=False,\n    // population mean and variance need to be forwarded here to compute the\n    // gradients.\n    const Tensor& saved_mean_or_pop_mean = context->input(3);\n    // The Eigen implementation saves variance in the forward pass, while cuDNN\n    // saves inverted variance.\n    const Tensor& saved_maybe_inv_var_or_pop_var = context->input(4);\n    bool use_activation = activation_mode_ != FbnActivationMode::kIdentity;\n    const Tensor* offset = use_activation ? &context->input(6) : nullptr;\n    const Tensor* y = use_activation ? &context->input(7) : nullptr;\n\n    OP_REQUIRES(context, y_backprop.dims() == 4 || y_backprop.dims() == 5,\n                errors::InvalidArgument(\"input must be 4 or 5-dimensional\",\n                                        y_backprop.shape().DebugString()));\n    OP_REQUIRES(context, x.dims() == 4 || x.dims() == 5,\n                errors::InvalidArgument(\"input must be 4 or 5-dimensional\",\n                                        x.shape().DebugString()));\n    OP_REQUIRES(context, scale.dims() == 1,\n                errors::InvalidArgument(\"scale must be 1-dimensional\",\n                                        scale.shape().DebugString()));\n    OP_REQUIRES(\n        context, saved_mean_or_pop_mean.dims() == 1,\n        errors::InvalidArgument(\"saved mean must be 1-dimensional\",\n                                saved_mean_or_pop_mean.shape().DebugString()));\n    OP_REQUIRES(context, saved_maybe_inv_var_or_pop_var.dims() == 1,\n                errors::InvalidArgument(\n                    \"saved variance must be 1-dimensional\",\n                    saved_maybe_inv_var_or_pop_var.shape().DebugString()));\n    if (use_activation) {\n      OP_REQUIRES(\n          context, x.dim_size(3) % 4 == 0,\n          errors::InvalidArgument(\"FusedBatchNormGrad with activation requires \"\n                                  \"channel dimension to be a multiple of 4.\"));\n      OP_REQUIRES(context, offset->dims() == 1,\n                  errors::InvalidArgument(\"offset must be 1-dimensional\",\n                                          offset->shape().DebugString()));\n    }\n    bool use_reshape = (x.dims() == 5);\n    auto x_shape = x.shape();\n    TensorShape dest_shape;\n    if (use_reshape) {\n      const int64_t in_batch = GetTensorDim(x, tensor_format_, 'N');\n      int64_t in_planes = GetTensorDim(x, tensor_format_, '0');\n      int64_t in_rows = GetTensorDim(x, tensor_format_, '1');\n      int64_t in_cols = GetTensorDim(x, tensor_format_, '2');\n      const int64_t in_depth = GetTensorDim(x, tensor_format_, 'C');\n      dest_shape = ShapeFromFormat(tensor_format_, in_batch,\n                                   {{in_planes, in_rows * in_cols}}, in_depth);\n      OP_REQUIRES(context, x.CopyFrom(x, dest_shape),\n                  errors::InvalidArgument(\"Error during tensor copy.\"));\n      OP_REQUIRES(context, y_backprop.CopyFrom(y_backprop, dest_shape),\n                  errors::InvalidArgument(\"Error during tensor copy.\"));\n    }\n\n    Tensor* x_backprop = nullptr;\n    auto alloc_shape = use_reshape ? dest_shape : x_shape;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(0, alloc_shape, &x_backprop));\n\n    const TensorShape& scale_offset_shape = scale.shape();\n    Tensor* scale_backprop = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(1, scale_offset_shape,\n                                                     &scale_backprop));\n    Tensor* offset_backprop = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(2, scale_offset_shape,\n                                                     &offset_backprop));\n    // Two placeholders for estimated_mean and estimated_variance, which are\n    // used for inference and thus not needed here for gradient computation.\n    // They are filled with zeros so as to avoid NaN outputs.\n    Tensor* placeholder_1 = nullptr;\n    OP_REQUIRES_OK(\n        context, context->allocate_output(3, TensorShape({0}), &placeholder_1));\n    Tensor* placeholder_2 = nullptr;\n    OP_REQUIRES_OK(\n        context, context->allocate_output(4, TensorShape({0}), &placeholder_2));\n\n    Tensor* side_input_backprop = nullptr;\n    if (has_side_input_) {\n      OP_REQUIRES_OK(context, context->allocate_output(5, alloc_shape,\n                                                       &side_input_backprop));\n    }\n\n    // If input is empty, set gradients w.r.t scale/offset to zero.\n    if (x.shape().num_elements() == 0) {\n      functor::SetZeroFunctor<Device, U> f;\n      f(context->eigen_device<Device>(), scale_backprop->flat<U>());\n      f(context->eigen_device<Device>(), offset_backprop->flat<U>());\n      return;\n    }\n\n    if (is_training_) {\n      functor::FusedBatchNormGrad<Device, T, U>()(\n          context, y_backprop, x, scale, offset, saved_mean_or_pop_mean,\n          saved_maybe_inv_var_or_pop_var, y, epsilon_, activation_mode_,\n          x_backprop, scale_backprop, offset_backprop, side_input_backprop,\n          use_reserved_space, tensor_format_);\n    } else {\n      OP_REQUIRES(\n          context,\n          activation_mode_ == FbnActivationMode::kIdentity && !has_side_input_,\n          errors::InvalidArgument(\n              \"FusedBatchNormGrad with activation is only supported \"\n              \"when is_training=True.\"));\n      // Necessary layout conversion is currently done in python.\n      OP_REQUIRES(context, tensor_format_ == FORMAT_NHWC,\n                  errors::InvalidArgument(\n                      \"The implementation of \"\n                      \"FusedBatchNormGrad with is_training=False only support \"\n                      \"NHWC tensor format for now.\"));\n      functor::FusedBatchNormFreezeGrad<Device, T, U>()(\n          context, y_backprop, x, scale, saved_mean_or_pop_mean,\n          saved_maybe_inv_var_or_pop_var, epsilon_, x_backprop, scale_backprop,\n          offset_backprop);\n    }\n    if (use_reshape) {\n      OP_REQUIRES(context, x_backprop->CopyFrom(*x_backprop, x_shape),\n                  errors::InvalidArgument(\"Error during tensor copy.\"));\n    }\n  }\n\n private:\n  U epsilon_;\n  TensorFormat tensor_format_;\n  bool is_training_;\n  bool has_side_input_;\n  FbnActivationMode activation_mode_;\n};\n\ntemplate <typename Device, typename T, typename U>\nclass FusedBatchNormGradOp : public FusedBatchNormGradOpBase<Device, T, U> {\n public:\n  explicit FusedBatchNormGradOp(OpKernelConstruction* context)\n      : FusedBatchNormGradOpBase<Device, T, U>(context) {}\n\n  void Compute(OpKernelContext* context) override {\n    FusedBatchNormGradOpBase<Device, T, U>::ComputeWithReservedSpace(context,\n                                                                     false);\n  }\n};\n\ntemplate <typename Device, typename T, typename U>\nclass FusedBatchNormGradOpV3 : public FusedBatchNormGradOpBase<Device, T, U> {\n public:\n  explicit FusedBatchNormGradOpV3(OpKernelConstruction* context)\n      : FusedBatchNormGradOpBase<Device, T, U>(context) {}\n\n  void Compute(OpKernelContext* context) override {\n    FusedBatchNormGradOpBase<Device, T, U>::ComputeWithReservedSpace(context,\n                                                                     true);\n  }\n};\n\ntemplate <typename Device, typename T, typename U>\nclass FusedBatchNormGradOpEx : public FusedBatchNormGradOpBase<Device, T, U> {\n  static constexpr bool kWithSideInputAndActivation = true;\n\n public:\n  explicit FusedBatchNormGradOpEx(OpKernelConstruction* context)\n      : FusedBatchNormGradOpBase<Device, T, U>(context,\n                                               kWithSideInputAndActivation) {}\n\n  void Compute(OpKernelContext* context) override {\n    FusedBatchNormGradOpBase<Device, T, U>::ComputeWithReservedSpace(context,\n                                                                     true);\n  }\n};\n\nREGISTER_KERNEL_BUILDER(\n    Name(\"FusedBatchNorm\").Device(DEVICE_CPU).TypeConstraint<float>(\"T\"),\n    FusedBatchNormOp<CPUDevice, float, float>);\n\nREGISTER_KERNEL_BUILDER(\n    Name(\"FusedBatchNormGrad\").Device(DEVICE_CPU).TypeConstraint<float>(\"T\"),\n    FusedBatchNormGradOp<CPUDevice, float, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"FusedBatchNormV2\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<float>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormOp<CPUDevice, float, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"FusedBatchNormGradV2\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<float>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormGradOp<CPUDevice, float, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"FusedBatchNormV2\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<Eigen::half>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormOp<CPUDevice, Eigen::half, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"FusedBatchNormGradV2\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<Eigen::half>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormGradOp<CPUDevice, Eigen::half, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"FusedBatchNormV3\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<float>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormOpV3<CPUDevice, float, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"FusedBatchNormGradV3\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<float>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormGradOpV3<CPUDevice, float, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"FusedBatchNormV3\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<Eigen::half>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormOpV3<CPUDevice, Eigen::half, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"FusedBatchNormGradV3\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<Eigen::half>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormGradOpV3<CPUDevice, Eigen::half, float>);\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\nREGISTER_KERNEL_BUILDER(\n    Name(\"FusedBatchNorm\").Device(DEVICE_GPU).TypeConstraint<float>(\"T\"),\n    FusedBatchNormOp<GPUDevice, float, float>);\n\nREGISTER_KERNEL_BUILDER(\n    Name(\"FusedBatchNormGrad\").Device(DEVICE_GPU).TypeConstraint<float>(\"T\"),\n    FusedBatchNormGradOp<GPUDevice, float, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"FusedBatchNormV2\")\n                            .Device(DEVICE_GPU)\n                            .TypeConstraint<float>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormOp<GPUDevice, float, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"FusedBatchNormGradV2\")\n                            .Device(DEVICE_GPU)\n                            .TypeConstraint<float>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormGradOp<GPUDevice, float, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"FusedBatchNormV2\")\n                            .Device(DEVICE_GPU)\n                            .TypeConstraint<Eigen::half>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormOp<GPUDevice, Eigen::half, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"FusedBatchNormGradV2\")\n                            .Device(DEVICE_GPU)\n                            .TypeConstraint<Eigen::half>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormGradOp<GPUDevice, Eigen::half, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"FusedBatchNormV3\")\n                            .Device(DEVICE_GPU)\n                            .TypeConstraint<float>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormOpV3<GPUDevice, float, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"_FusedBatchNormEx\")\n                            .Device(DEVICE_GPU)\n                            .TypeConstraint<float>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormOpEx<GPUDevice, float, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"FusedBatchNormGradV3\")\n                            .Device(DEVICE_GPU)\n                            .TypeConstraint<float>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormGradOpV3<GPUDevice, float, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"_FusedBatchNormGradEx\")\n                            .Device(DEVICE_GPU)\n                            .TypeConstraint<float>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormGradOpEx<GPUDevice, float, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"FusedBatchNormV3\")\n                            .Device(DEVICE_GPU)\n                            .TypeConstraint<Eigen::half>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormOpV3<GPUDevice, Eigen::half, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"_FusedBatchNormEx\")\n                            .Device(DEVICE_GPU)\n                            .TypeConstraint<Eigen::half>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormOpEx<GPUDevice, Eigen::half, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"FusedBatchNormGradV3\")\n                            .Device(DEVICE_GPU)\n                            .TypeConstraint<Eigen::half>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormGradOpV3<GPUDevice, Eigen::half, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"_FusedBatchNormGradEx\")\n                            .Device(DEVICE_GPU)\n                            .TypeConstraint<Eigen::half>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormGradOpEx<GPUDevice, Eigen::half, float>);\n\n#endif\n\n}  // namespace tensorflow\n", "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for fused_batch_norm related functionality in tensorflow.ops.nn.\"\"\"\n\nimport numpy as np\n\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import gradient_checker\nfrom tensorflow.python.ops import gradients_impl\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import nn_grad\nfrom tensorflow.python.ops import nn_impl\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.platform import test\n\n\nclass BatchNormalizationTest(test.TestCase):\n\n  def _batch_norm(self, x, mean, var, offset, scale, epsilon):\n    # We compute the batch norm manually in this function because\n    # nn_impl.batch_normalization does not support float16 yet.\n    # TODO(reedwm): Add float16 support to nn_impl.batch_normalization.\n    inv = math_ops.rsqrt(var + epsilon) * scale\n    y = math_ops.cast(x, scale.dtype) * inv + (offset - mean * inv)\n    return math_ops.cast(y, x.dtype)\n\n  def _inference_ref(self, x, scale, offset, mean, var, epsilon, data_format):\n    if data_format not in ['NHWC', 'NCHW', 'NDHWC', 'NCDHW']:\n      raise ValueError('data_format must be NCHW or NHWC for 4D tensors or'\n                       'NCDHW or NDHWC for 5D tensors, got %s.' % data_format)\n    if data_format == 'NCHW':\n      x = array_ops.transpose(x, [0, 2, 3, 1])\n    elif data_format == 'NCDHW':\n      x = array_ops.transpose(x, [0, 2, 3, 4, 1])\n    y = self._batch_norm(x, mean, var, offset, scale, epsilon)\n    if data_format == 'NCHW':\n      y = array_ops.transpose(y, [0, 3, 1, 2])\n    elif data_format == 'NCDHW':\n      y = array_ops.transpose(y, [0, 4, 1, 2, 3])\n    return self.evaluate(y)\n\n  def _test_inference(self,\n                      x_shape,\n                      x_dtype,\n                      scale_shape,\n                      scale_dtype,\n                      use_gpu=True,\n                      exponential_avg_factor=1.0,\n                      data_format='NHWC'):\n    np.random.seed(1)\n    x_val = np.random.random_sample(x_shape).astype(x_dtype)\n    scale_val = np.random.random_sample(scale_shape).astype(scale_dtype)\n    offset_val = np.random.random_sample(scale_shape).astype(scale_dtype)\n    mean_val = np.random.random_sample(scale_shape).astype(scale_dtype)\n    var_val = np.random.random_sample(scale_shape).astype(scale_dtype)\n\n    with self.cached_session(use_gpu=use_gpu) as sess:\n      x = constant_op.constant(x_val, name='x')\n      scale = constant_op.constant(scale_val, name='scale')\n      offset = constant_op.constant(offset_val, name='offset')\n      mean = constant_op.constant(mean_val, name='mean')\n      var = constant_op.constant(var_val, name='variance')\n      epsilon = 0.001\n      y, _, _ = nn_impl.fused_batch_norm(\n          x,\n          scale,\n          offset,\n          mean=mean,\n          variance=var,\n          epsilon=epsilon,\n          exponential_avg_factor=exponential_avg_factor,\n          data_format=data_format,\n          is_training=False)\n      y_val = self.evaluate(y)\n      y_ref = self._inference_ref(x, scale, offset, mean, var, epsilon,\n                                  data_format)\n    # An atol value of 1e-3 is too small for float16's, because some adjacent\n    # float16 values that y_val can take are greater than 1e-3 apart, e.g.\n    # 2.16602 and 2.16797.\n    atol = 2e-3 if x_dtype == np.float16 else 1e-3\n    self.assertAllClose(y_ref, y_val, atol=atol)\n\n  def _running_mean(self, old_mean, new_val, factor):\n    if factor == 1.0:\n      return new_val\n    else:\n      return (1.0 - factor) * old_mean + factor * new_val\n\n  def _training_ref(self, x, scale, offset, old_mean, old_var,\n                    exponential_avg_factor, epsilon, data_format):\n    if data_format not in ['NHWC', 'NCHW', 'NDHWC', 'NCDHW']:\n      raise ValueError('data_format must be NCHW or NHWC for 4D tensors or'\n                       'NCDHW or NDHWC for 5D tensors, got %s.' % data_format)\n    use_4d_tensor = (x.shape.ndims == 4)\n    if data_format == 'NCHW':\n      x = array_ops.transpose(x, [0, 2, 3, 1])\n    elif data_format == 'NCDHW':\n      x = array_ops.transpose(x, [0, 2, 3, 4, 1])\n\n    mean_axis = [0, 1, 2] if use_4d_tensor else [0, 1, 2, 3]\n    batch_mean, batch_var = nn_impl.moments(\n        math_ops.cast(x, scale.dtype), mean_axis, keep_dims=False)\n\n    y = self._batch_norm(x, batch_mean, batch_var, offset, scale, epsilon)\n    if data_format == 'NCHW':\n      y = array_ops.transpose(y, [0, 3, 1, 2])\n    elif data_format == 'NCDHW':\n      y = array_ops.transpose(y, [0, 4, 1, 2, 3])\n\n    # This is for Bessel's correction. tf.nn.moments uses n, instead of n-1, as\n    # the denominator in the formula to calculate variance, while\n    # tf.compat.v1.nn.fused_batch_norm has Bessel's correction built in.\n    sample_size = math_ops.cast(\n        array_ops.size(x) / array_ops.size(scale), scale.dtype)\n    batch_var_corrected = batch_var * sample_size / (\n        math_ops.maximum(sample_size - 1.0, 1.0))\n\n    mean = self._running_mean(old_mean, batch_mean, exponential_avg_factor)\n    var = self._running_mean(old_var, batch_var_corrected,\n                             exponential_avg_factor)\n    return self.evaluate(y), self.evaluate(mean), self.evaluate(var)\n\n  def _test_training(self,\n                     x_shape,\n                     x_dtype,\n                     scale_shape,\n                     scale_dtype,\n                     use_gpu=True,\n                     exponential_avg_factor=1.0,\n                     data_format='NHWC'):\n    np.random.seed(1)\n    x_val = np.random.random_sample(x_shape).astype(x_dtype)\n    scale_val = np.random.random_sample(scale_shape).astype(scale_dtype)\n    offset_val = np.random.random_sample(scale_shape).astype(scale_dtype)\n    if exponential_avg_factor == 1.0:\n      old_mean_val = None\n      old_var_val = None\n    else:\n      old_mean_val = np.random.random_sample(scale_shape).astype(scale_dtype)\n      old_var_val = np.random.random_sample(scale_shape).astype(scale_dtype)\n\n    with self.cached_session(use_gpu=use_gpu) as sess:\n      x = constant_op.constant(x_val, name='x')\n      scale = constant_op.constant(scale_val, name='scale')\n      offset = constant_op.constant(offset_val, name='offset')\n      epsilon = 0.001\n      y, mean, var = nn_impl.fused_batch_norm(\n          x,\n          scale,\n          offset,\n          mean=old_mean_val,\n          variance=old_var_val,\n          epsilon=epsilon,\n          exponential_avg_factor=exponential_avg_factor,\n          data_format=data_format,\n          is_training=True)\n      y_val, mean_val, var_val = self.evaluate([y, mean, var])\n      y_ref, mean_ref, var_ref = self._training_ref(x, scale, offset,\n                                                    old_mean_val, old_var_val,\n                                                    exponential_avg_factor,\n                                                    epsilon, data_format)\n    y_atol = 2e-3 if x_dtype == np.float16 else 1e-3\n    self.assertAllClose(y_ref, y_val, atol=y_atol)\n    self.assertAllClose(mean_ref, mean_val, atol=1e-3)\n    self.assertAllClose(var_ref, var_val, atol=1e-3)\n\n  def _compute_gradient_error_float16(self, x, x32, x_shape, y, y32, y_shape):\n    \"\"\"Computes the gradient error for float16 inputs and/or outputs.\n\n    This returns the same value as gradient_checker.compute_gradient_error. The\n    difference is that gradient_checker.compute_gradient_error does not\n    numerically compute the gradients in a numerically stable way for float16\n    tensors. To fix this, this function requires float32 versions of x and y to\n    numerically compute the gradients, to compare with the float16 symbolically\n    computed gradients.\n\n    Args:\n      x: The input tensor.\n      x32: A float32 version of x.\n      x_shape: The shape of x.\n      y: The output tensor.\n      y32: A float32 version of y. Must be calculated based on x32, not x.\n      y_shape: The shape of y.\n\n    Returns:\n      The maximum error in between the two Jacobians, as in\n      gradient_checker.compute_gradient_error.\n    \"\"\"\n    x_init_val = np.random.random_sample(x_shape).astype(np.float16)\n    x32_init_val = x_init_val.astype(np.float32)\n\n    # TODO(reedwm): Do not perform the unnecessary computations in\n    # compute_gradient, since they double the computation time of this function.\n    theoretical_grad, _ = gradient_checker.compute_gradient(\n        x, x_shape, y, y_shape, delta=1e-3, x_init_value=x_init_val)\n    _, numerical_grad = gradient_checker.compute_gradient(\n        x32, x_shape, y32, y_shape, delta=1e-3, x_init_value=x32_init_val)\n\n    # If grad is empty, no error.\n    if theoretical_grad.size == 0 and numerical_grad.size == 0:\n      return 0\n    return np.fabs(theoretical_grad - numerical_grad).max()\n\n  def _test_gradient(self,\n                     x_shape,\n                     x_dtype,\n                     scale_shape,\n                     scale_dtype,\n                     use_gpu=True,\n                     exponential_avg_factor=1.0,\n                     data_format='NHWC',\n                     is_training=True):\n    np.random.seed(1)\n    x_val = np.random.random_sample(x_shape).astype(x_dtype)\n    scale_val = np.random.random_sample(scale_shape).astype(scale_dtype)\n    offset_val = np.random.random_sample(scale_shape).astype(scale_dtype)\n\n    with self.cached_session(use_gpu=use_gpu):\n      x = constant_op.constant(x_val, name='x')\n      scale = constant_op.constant(scale_val, name='scale')\n      offset = constant_op.constant(offset_val, name='offset')\n      if is_training and exponential_avg_factor == 1.0:\n        pop_mean = None\n        pop_var = None\n      else:\n        pop_mean = np.random.random_sample(scale_shape).astype(scale_dtype)\n        pop_var = np.random.random_sample(scale_shape).astype(scale_dtype)\n      y, _, _ = nn_impl.fused_batch_norm(\n          x,\n          scale,\n          offset,\n          mean=pop_mean,\n          variance=pop_var,\n          exponential_avg_factor=exponential_avg_factor,\n          data_format=data_format,\n          is_training=is_training)\n      if x_dtype != np.float16:\n        err_x = gradient_checker.compute_gradient_error(x, x_shape, y, x_shape)\n        err_scale = gradient_checker.compute_gradient_error(\n            scale, scale_shape, y, x_shape)\n        err_offset = gradient_checker.compute_gradient_error(\n            offset, scale_shape, y, x_shape)\n      else:\n        x32 = constant_op.constant(x_val, name='x32', dtype=dtypes.float32)\n        y32, _, _ = nn_impl.fused_batch_norm(\n            x32,\n            scale,\n            offset,\n            mean=pop_mean,\n            variance=pop_var,\n            data_format=data_format,\n            exponential_avg_factor=exponential_avg_factor,\n            is_training=is_training)\n        err_x = self._compute_gradient_error_float16(x, x32, x_shape, y, y32,\n                                                     x_shape)\n        err_scale = self._compute_gradient_error_float16(\n            scale, scale, scale_shape, y, y32, x_shape)\n        err_offset = self._compute_gradient_error_float16(\n            offset, offset, scale_shape, y, y32, x_shape)\n\n    x_err_tolerance = 2e-3 if x_dtype == np.float16 else 1e-3\n    scale_err_tolerance = 1e-3\n    self.assertLess(err_x, x_err_tolerance)\n    self.assertLess(err_scale, scale_err_tolerance)\n    self.assertLess(err_offset, scale_err_tolerance)\n\n  def _test_grad_grad(self,\n                      x_shape,\n                      x_dtype,\n                      scale_shape,\n                      scale_dtype,\n                      use_gpu=True,\n                      exponential_avg_factor=1.0,\n                      data_format='NHWC',\n                      is_training=True,\n                      err_tolerance=1e-3):\n    np.random.seed(1)\n    x_val = np.random.random_sample(x_shape).astype(x_dtype)\n    grad_y_val = np.random.random_sample(x_shape).astype(x_dtype)\n    scale_val = np.random.random_sample(scale_shape).astype(scale_dtype)\n    offset_val = np.random.random_sample(scale_shape).astype(scale_dtype)\n\n    with self.cached_session(use_gpu=use_gpu) as sess:\n      x = constant_op.constant(x_val, name='x')\n      grad_y = constant_op.constant(grad_y_val, name='grad_y')\n      scale = constant_op.constant(scale_val, name='scale')\n      offset = constant_op.constant(offset_val, name='offset')\n      if is_training and exponential_avg_factor == 1.0:\n        pop_mean = None\n        pop_var = None\n      else:\n        pop_mean = np.random.random_sample(scale_shape).astype(scale_dtype)\n        pop_var = np.random.random_sample(scale_shape).astype(scale_dtype)\n      y, _, _ = nn_impl.fused_batch_norm(\n          x,\n          scale,\n          offset,\n          mean=pop_mean,\n          variance=pop_var,\n          exponential_avg_factor=exponential_avg_factor,\n          data_format=data_format,\n          is_training=is_training)\n      grad_x, grad_scale, grad_offset = gradients_impl.gradients(\n          y, [x, scale, offset], grad_y)\n\n      if is_training:\n        epsilon = y.op.get_attr('epsilon')\n        data_format = y.op.get_attr('data_format')\n        grad_vals = self.evaluate([grad_x, grad_scale, grad_offset])\n        grad_internal = nn_grad._BatchNormGrad(grad_y, x, scale, pop_mean,\n                                               pop_var, epsilon, data_format)\n        grad_internal_vals = self.evaluate(list(grad_internal))\n        for grad_val, grad_internal_val in zip(grad_vals, grad_internal_vals):\n          self.assertAllClose(grad_val, grad_internal_val, atol=err_tolerance)\n\n      if x_dtype != np.float16:\n        err_grad_grad_y_1 = gradient_checker.compute_gradient_error(\n            grad_y, x_shape, grad_x, x_shape)\n        err_grad_grad_y_2 = gradient_checker.compute_gradient_error(\n            grad_y, x_shape, grad_scale, scale_shape)\n        err_grad_grad_y_3 = gradient_checker.compute_gradient_error(\n            grad_y, x_shape, grad_offset, scale_shape)\n        # In freeze mode, grad_x is not a function of x.\n        if is_training:\n          err_grad_x_1 = gradient_checker.compute_gradient_error(\n              x, x_shape, grad_x, x_shape)\n        err_grad_x_2 = gradient_checker.compute_gradient_error(\n            x, x_shape, grad_scale, scale_shape)\n\n        err_grad_scale = gradient_checker.compute_gradient_error(\n            scale, scale_shape, grad_x, x_shape)\n      else:\n        x32 = constant_op.constant(x_val, dtype=dtypes.float32, name='x32')\n        grad_y32 = constant_op.constant(\n            grad_y_val, dtype=dtypes.float32, name='grad_y32')\n        y32, _, _ = nn_impl.fused_batch_norm(\n            x32,\n            scale,\n            offset,\n            mean=pop_mean,\n            variance=pop_var,\n            exponential_avg_factor=exponential_avg_factor,\n            data_format=data_format,\n            is_training=is_training)\n        grad_x32, grad_scale32, grad_offset32 = gradients_impl.gradients(\n            y32, [x32, scale, offset], grad_y32)\n        err_grad_grad_y_1 = self._compute_gradient_error_float16(\n            grad_y, grad_y32, x_shape, grad_x, grad_x32, x_shape)\n        err_grad_grad_y_2 = self._compute_gradient_error_float16(\n            grad_y, grad_y32, x_shape, grad_scale, grad_scale32, scale_shape)\n        err_grad_grad_y_3 = self._compute_gradient_error_float16(\n            grad_y, grad_y32, x_shape, grad_offset, grad_offset32, scale_shape)\n        # In freeze mode, grad_x is not a function of x.\n        if is_training:\n          err_grad_x_1 = self._compute_gradient_error_float16(\n              x, x32, x_shape, grad_x, grad_x32, x_shape)\n        err_grad_x_2 = self._compute_gradient_error_float16(\n            x, x32, x_shape, grad_scale, grad_scale32, scale_shape)\n\n        err_grad_scale = self._compute_gradient_error_float16(\n            scale, scale, scale_shape, grad_x, grad_x32, x_shape)\n\n    self.assertLess(err_grad_grad_y_1, err_tolerance)\n    self.assertLess(err_grad_grad_y_2, err_tolerance)\n    self.assertLess(err_grad_grad_y_3, err_tolerance)\n    if is_training:\n      self.assertLess(err_grad_x_1, err_tolerance)\n    self.assertLess(err_grad_x_2, err_tolerance)\n    self.assertLess(err_grad_scale, err_tolerance)\n\n  def _runtests(self, x_shape, is_training, gradient_test=False,\n                cpu_only=False):\n    if len(x_shape) == 4:\n      data_format_list = ['NHWC', 'NCHW']\n    else:\n      data_format_list = ['NCDHW', 'NDHWC']\n    use_gpu_vals = [False]\n    if test.is_gpu_available(cuda_only=True) and not cpu_only:\n      use_gpu_vals += [True]\n    factors = [1.0, 0.6]\n    for dtype in [np.float16, np.float32]:\n      for use_gpu in use_gpu_vals:\n        for data_format in data_format_list:\n          if data_format == 'NHWC' or data_format == 'NDHWC':\n            scale_shape = x_shape[-1:]\n          else:\n            scale_shape = x_shape[1:2]\n          for exponential_avg_factor in factors:\n            if gradient_test:\n              self._test_gradient(\n                  x_shape,\n                  dtype,\n                  scale_shape,\n                  np.float32,\n                  use_gpu=use_gpu,\n                  data_format=data_format,\n                  is_training=is_training,\n                  exponential_avg_factor=exponential_avg_factor)\n            else:\n              if is_training:\n                self._test_training(\n                    x_shape,\n                    dtype,\n                    scale_shape,\n                    np.float32,\n                    use_gpu=use_gpu,\n                    data_format=data_format,\n                    exponential_avg_factor=exponential_avg_factor)\n              else:\n                self._test_inference(\n                    x_shape,\n                    dtype,\n                    scale_shape,\n                    np.float32,\n                    use_gpu=use_gpu,\n                    data_format=data_format,\n                    exponential_avg_factor=exponential_avg_factor)\n\n  def testInferenceShape1(self):\n    x_shape = [1, 1, 6, 1]\n    self._runtests(x_shape, False)\n\n  def testInferenceShape2(self):\n    x_shape = [1, 1, 6, 2]\n    self._runtests(x_shape, False)\n\n  def testInferenceShape3(self):\n    x_shape = [1, 2, 1, 6]\n    self._runtests(x_shape, False)\n\n  def testInferenceShape4(self):\n    x_shape = [27, 131, 127, 6]\n    self._runtests(x_shape, False)\n\n  def testInferenceShape5(self):\n    x_shape = [0, 131, 127, 6]\n    self._runtests(x_shape, False)\n\n  def testInferenceShape6(self):\n    x_shape = [1, 1, 1, 1]\n    # GPU kernel doesn't properly handle case where non-channel dimensions are 1\n    self._runtests(x_shape, False, cpu_only=True)\n\n  def testInferenceShape7(self):\n    x_shape = [1, 2, 6, 1, 3]\n    self._runtests(x_shape, False)\n\n  def testTrainingShape1(self):\n    x_shape = [1, 1, 6, 1]\n    self._runtests(x_shape, True)\n\n  def testTrainingShape2(self):\n    x_shape = [1, 1, 6, 2]\n    self._runtests(x_shape, True)\n\n  def testTrainingShape3(self):\n    x_shape = [1, 2, 1, 6]\n    self._runtests(x_shape, True)\n\n  def testTrainingShape4(self):\n    x_shape = [27, 131, 127, 6]\n    self._runtests(x_shape, True)\n\n  @test_util.disable_xla('b/141236973: Empty inputs wrong on CPU.')\n  def testTrainingShape5(self):\n    x_shape = [0, 131, 127, 6]\n    self._runtests(x_shape, True)\n\n  @test_util.run_deprecated_v1\n  def testTrainingShape6(self):\n    x_shape = [1, 1, 1, 1]\n    # GPU kernel doesn't properly handle case where non-channel dimensions are 1\n    self._runtests(x_shape, True, cpu_only=True)\n\n  def testTrainingShape7(self):\n    x_shape = [1, 2, 6, 1, 3]\n    self._runtests(x_shape, True)\n\n  @test_util.run_deprecated_v1\n  def testBatchNormGradInferenceShape1(self):\n    x_shape = [1, 1, 6, 1]\n    self._runtests(x_shape, is_training=False, gradient_test=True)\n\n  @test_util.run_deprecated_v1\n  def testBatchNormGradInferenceShape2(self):\n    x_shape = [1, 1, 6, 2]\n    self._runtests(x_shape, is_training=False, gradient_test=True)\n\n  @test_util.run_deprecated_v1\n  def testBatchNormGradInferenceShape3(self):\n    x_shape = [1, 2, 1, 6]\n    self._runtests(x_shape, is_training=False, gradient_test=True)\n\n  @test_util.run_deprecated_v1\n  def testBatchNormGradInferenceShape4(self):\n    x_shape = [5, 7, 11, 4]\n    self._runtests(x_shape, is_training=False, gradient_test=True)\n\n  @test_util.run_deprecated_v1\n  @test_util.disable_xla('This test never passed for XLA')\n  def testBatchNormGradInferenceShape5(self):\n    x_shape = [0, 7, 11, 4]\n    self._runtests(x_shape, is_training=False, gradient_test=True)\n\n  @test_util.run_deprecated_v1\n  def testBatchNormGradInferenceShape6(self):\n    x_shape = [1, 1, 1, 1]\n    # GPU kernel doesn't properly handle case where non-channel dimensions are 1\n    self._runtests(x_shape, is_training=False, gradient_test=True,\n                   cpu_only=True)\n\n  @test_util.run_deprecated_v1\n  def testBatchNormGradInferenceShape7(self):\n    x_shape = [1, 2, 6, 1, 3]\n    self._runtests(x_shape, is_training=False, gradient_test=True)\n\n  @test_util.run_deprecated_v1\n  def testBatchNormGradTrainingShape1(self):\n    x_shape = [1, 1, 6, 1]\n    self._runtests(x_shape, is_training=True, gradient_test=True)\n\n  @test_util.run_deprecated_v1\n  def testBatchNormGradTrainingShape2(self):\n    x_shape = [1, 1, 6, 2]\n    self._runtests(x_shape, is_training=True, gradient_test=True)\n\n  @test_util.run_deprecated_v1\n  def testBatchNormGradTrainingShape3(self):\n    x_shape = [1, 2, 1, 6]\n    self._runtests(x_shape, is_training=True, gradient_test=True)\n\n  @test_util.run_deprecated_v1\n  def testBatchNormGradTrainingShape4(self):\n    x_shape = [5, 7, 11, 4]\n    self._runtests(x_shape, is_training=True, gradient_test=True)\n\n  @test_util.run_deprecated_v1\n  @test_util.disable_xla('This test never passed for XLA')\n  def testBatchNormGradTrainingShape5(self):\n    x_shape = [0, 7, 11, 4]\n    self._runtests(x_shape, is_training=True, gradient_test=True)\n\n  @test_util.run_deprecated_v1\n  def testBatchNormGradTrainingShape6(self):\n    x_shape = [1, 1, 1, 1]\n    # GPU kernel doesn't properly handle case where non-channel dimensions are 1\n    self._runtests(x_shape, is_training=True, gradient_test=True, cpu_only=True)\n\n  @test_util.run_deprecated_v1\n  def testBatchNormGradTrainingShape7(self):\n    x_shape = [1, 2, 6, 1, 3]\n    self._runtests(x_shape, is_training=True, gradient_test=True)\n\n  def _testBatchNormGradGrad(self, config):\n    shape = config['shape']\n    err_tolerance = config['err_tolerance']\n    dtype = config['dtype']\n    rank = len(shape)\n    if rank == 4:\n      data_format_nhwc, features_nhwc = 'NHWC', shape[3]\n      data_format_nchw, features_nchw = 'NCHW', shape[1]\n    else:\n      data_format_nhwc, features_nhwc = 'NDHWC', shape[4]\n      data_format_nchw, features_nchw = 'NCDHW', shape[1]\n    for is_training in [True, False]:\n      if test.is_gpu_available(cuda_only=True):\n        self._test_grad_grad(\n            shape,\n            dtype, [features_nhwc],\n            np.float32,\n            use_gpu=True,\n            data_format=data_format_nhwc,\n            is_training=is_training,\n            err_tolerance=err_tolerance)\n        self._test_grad_grad(\n            shape,\n            dtype, [features_nchw],\n            np.float32,\n            use_gpu=True,\n            data_format=data_format_nchw,\n            is_training=is_training,\n            err_tolerance=err_tolerance)\n      self._test_grad_grad(\n          shape,\n          dtype, [features_nhwc],\n          np.float32,\n          use_gpu=False,\n          data_format=data_format_nhwc,\n          is_training=is_training,\n          err_tolerance=err_tolerance)\n      self._test_grad_grad(\n          shape,\n          dtype, [features_nchw],\n          np.float32,\n          use_gpu=False,\n          data_format=data_format_nchw,\n          is_training=is_training,\n          err_tolerance=err_tolerance)\n\n  @test_util.run_deprecated_v1\n  def testBatchNormGradGradConfig1(self):\n    config = {\n        'shape': [2, 3, 4, 5],\n        'err_tolerance': 1e-2,\n        'dtype': np.float32,\n    }\n    self._testBatchNormGradGrad(config)\n\n  @test_util.run_deprecated_v1\n  def testBatchNormGradGradConfig2(self):\n    config = {\n        'shape': [2, 3, 2, 2],\n        'err_tolerance': 1e-3,\n        'dtype': np.float32,\n    }\n    self._testBatchNormGradGrad(config)\n\n  @test_util.run_deprecated_v1\n  def testBatchNormGradGradConfig3(self):\n    config = {\n        'shape': [2, 3, 4, 5],\n        'err_tolerance': 2e-2,\n        'dtype': np.float16,\n    }\n    self._testBatchNormGradGrad(config)\n\n  @test_util.run_deprecated_v1\n  def testBatchNormGradGradConfig4(self):\n    config = {\n        'shape': [2, 3, 2, 2],\n        'err_tolerance': 2e-3,\n        'dtype': np.float16,\n    }\n    self._testBatchNormGradGrad(config)\n\n  @test_util.run_deprecated_v1\n  def testBatchNormGradGradConfig5(self):\n    config = {\n        'shape': [2, 3, 2, 2, 2],\n        'err_tolerance': 2e-3,\n        'dtype': np.float32,\n    }\n    self._testBatchNormGradGrad(config)\n\n  @test_util.run_deprecated_v1\n  def testBatchNormGradGradConfig6(self):\n    config = {\n        'shape': [2, 3, 2, 2, 2],\n        'err_tolerance': 3e-3,\n        'dtype': np.float16,\n    }\n    self._testBatchNormGradGrad(config)\n\n  def test5dBatchNormFollowedByRelu(self):\n    # The remapper grappler pass previously did not properly handle a 5D\n    # inference FusedBatchNorm followed by Relu. This asserts that this case is\n    # correctly handled.\n    np.random.seed(1)\n    x = np.random.random_sample((2, 3, 2, 2, 3)).astype(np.float32)\n    scale = np.random.random_sample((3,)).astype(np.float32)\n    offset = np.random.random_sample((3,)).astype(np.float32)\n    mean = np.random.random_sample((3,)).astype(np.float32)\n    var = np.random.random_sample((3,)).astype(np.float32)\n\n    epsilon = 0.001\n    y, _, _ = nn_impl.fused_batch_norm(\n        x,\n        scale,\n        offset,\n        mean=mean,\n        variance=var,\n        epsilon=epsilon,\n        data_format='NCDHW',\n        is_training=False)\n    y = nn_ops.relu(y)\n    y_val = self.evaluate(y)\n    y_ref = self._inference_ref(x, scale, offset, mean, var, epsilon,\n                                'NCDHW')\n    y_ref = np.maximum(y_ref, 0.)\n    self.assertAllClose(y_ref, y_val, atol=1e-3)\n\n\nif __name__ == '__main__':\n  test.main()\n"], "fixing_code": ["/* Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include <atomic>\n\n#define EIGEN_USE_THREADS\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n#define EIGEN_USE_GPU\n#if GOOGLE_CUDA\n#include \"third_party/gpus/cudnn/cudnn.h\"\n#endif  // GOOGLE_CUDA\n\n#include \"tensorflow/core/kernels/conv_2d.h\"\n#include \"tensorflow/core/platform/stream_executor.h\"\n#include \"tensorflow/core/util/stream_executor_util.h\"\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/register_types.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/tensor_types.h\"\n#include \"tensorflow/core/kernels/fill_functor.h\"\n#include \"tensorflow/core/kernels/fused_batch_norm_op.h\"\n#include \"tensorflow/core/kernels/redux_functor.h\"\n#include \"tensorflow/core/kernels/transpose_functor.h\"\n#include \"tensorflow/core/lib/core/blocking_counter.h\"\n#include \"tensorflow/core/util/env_var.h\"\n#include \"tensorflow/core/util/tensor_format.h\"\n\nnamespace tensorflow {\nusing CPUDevice = Eigen::ThreadPoolDevice;\nusing GPUDevice = Eigen::GpuDevice;\n\nnamespace functor {\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\nusing se::DeviceMemory;\nusing se::ScratchAllocator;\nusing se::Stream;\nusing se::port::StatusOr;\n#endif\n\nstring ToString(FusedBatchNormActivationMode activation_mode) {\n  switch (activation_mode) {\n    case FusedBatchNormActivationMode::kIdentity:\n      return \"Identity\";\n    case FusedBatchNormActivationMode::kRelu:\n      return \"Relu\";\n  }\n}\n\nStatus ParseActivationMode(OpKernelConstruction* context,\n                           FusedBatchNormActivationMode* activation_mode) {\n  string activation_mode_str;\n  TF_RETURN_IF_ERROR(context->GetAttr(\"activation_mode\", &activation_mode_str));\n\n  if (activation_mode_str == \"Identity\") {\n    *activation_mode = FusedBatchNormActivationMode::kIdentity;\n    return Status::OK();\n  }\n  if (activation_mode_str == \"Relu\") {\n    *activation_mode = FusedBatchNormActivationMode::kRelu;\n    return Status::OK();\n  }\n  return errors::InvalidArgument(\"Unsupported activation mode: \",\n                                 activation_mode_str);\n}\n\n// Functor used by FusedBatchNormOp to do the computations.\ntemplate <typename Device, typename T, typename U, bool is_training>\nstruct FusedBatchNorm;\n// Functor used by FusedBatchNormGradOp to do the computations when\n// is_training=True.\ntemplate <typename Device, typename T, typename U>\nstruct FusedBatchNormGrad;\n\ntemplate <typename T, typename U>\nstruct FusedBatchNorm<CPUDevice, T, U, /* is_training= */ true> {\n  void operator()(OpKernelContext* context, const Tensor& x_input,\n                  const Tensor& scale_input, const Tensor& offset_input,\n                  const Tensor& running_mean_input,\n                  const Tensor& running_variance_input,\n                  const Tensor* side_input, U epsilon, U exponential_avg_factor,\n                  FusedBatchNormActivationMode activation_mode,\n                  Tensor* y_output, Tensor* running_mean_output,\n                  Tensor* running_var_output, Tensor* saved_batch_mean_output,\n                  Tensor* saved_batch_var_output, TensorFormat tensor_format,\n                  bool use_reserved_space) {\n    OP_REQUIRES(context, side_input == nullptr,\n                errors::Internal(\n                    \"The CPU implementation of FusedBatchNorm does not support \"\n                    \"side input.\"));\n    OP_REQUIRES(context,\n                activation_mode == FusedBatchNormActivationMode::kIdentity,\n                errors::Internal(\"The CPU implementation of FusedBatchNorm \"\n                                 \"does not support activations.\"));\n\n    if (use_reserved_space) {\n      Tensor* dummy_reserve_space = nullptr;\n      OP_REQUIRES_OK(context,\n                     context->allocate_output(5, {}, &dummy_reserve_space));\n      // Initialize the memory, to avoid sanitizer alerts.\n      dummy_reserve_space->flat<U>()(0) = U();\n    }\n\n    // If input is empty, return NaN mean/variance\n    if (x_input.shape().num_elements() == 0) {\n      functor::SetNanFunctor<CPUDevice, U> f;\n      f(context->eigen_device<CPUDevice>(), running_mean_output->flat<U>());\n      f(context->eigen_device<CPUDevice>(), running_var_output->flat<U>());\n      return;\n    }\n\n    Tensor transformed_x;\n    Tensor transformed_y;\n    if (tensor_format == FORMAT_NCHW) {\n      const int64_t in_batch = GetTensorDim(x_input, tensor_format, 'N');\n      const int64_t in_rows = GetTensorDim(x_input, tensor_format, 'H');\n      const int64_t in_cols = GetTensorDim(x_input, tensor_format, 'W');\n      const int64_t in_depths = GetTensorDim(x_input, tensor_format, 'C');\n      OP_REQUIRES_OK(context, context->allocate_temp(\n                                  DataTypeToEnum<T>::value,\n                                  ShapeFromFormat(FORMAT_NHWC, in_batch,\n                                                  in_rows, in_cols, in_depths),\n                                  &transformed_x));\n      OP_REQUIRES_OK(context, context->allocate_temp(\n                                  DataTypeToEnum<T>::value,\n                                  ShapeFromFormat(FORMAT_NHWC, in_batch,\n                                                  in_rows, in_cols, in_depths),\n                                  &transformed_y));\n      // Perform NCHW to NHWC\n      std::vector<int32> perm = {0, 2, 3, 1};\n      OP_REQUIRES_OK(\n          context, ::tensorflow::DoTranspose(context->eigen_device<CPUDevice>(),\n                                             x_input, perm, &transformed_x));\n    } else {\n      transformed_x = x_input;\n      transformed_y = *y_output;\n    }\n    typename TTypes<T, 4>::Tensor x(transformed_x.tensor<T, 4>());\n    typename TTypes<U>::ConstVec scale(scale_input.vec<U>());\n    typename TTypes<U>::ConstVec offset(offset_input.vec<U>());\n    typename TTypes<U>::ConstVec old_mean(running_mean_input.vec<U>());\n    typename TTypes<U>::ConstVec old_variance(running_variance_input.vec<U>());\n    typename TTypes<T, 4>::Tensor y(transformed_y.tensor<T, 4>());\n    typename TTypes<U>::Vec new_mean(running_mean_output->vec<U>());\n    typename TTypes<U>::Vec new_variance(running_var_output->vec<U>());\n    typename TTypes<U>::Vec saved_batch_mean(saved_batch_mean_output->vec<U>());\n    typename TTypes<U>::Vec saved_batch_var(saved_batch_var_output->vec<U>());\n\n    const CPUDevice& d = context->eigen_device<CPUDevice>();\n\n    const int depth = x.dimension(3);\n    const int size = x.size();\n    const int rest_size = size / depth;\n    Eigen::DSizes<Eigen::Index, 2> rest_by_depth(rest_size, depth);\n\n#if !defined(EIGEN_HAS_INDEX_LIST)\n    Eigen::DSizes<Eigen::Index, 2> one_by_depth(1, depth);\n    Eigen::array<int, 1> reduce_dims({0});\n    Eigen::array<int, 2> bcast_spec({rest_size, 1});\n#else\n    Eigen::IndexList<Eigen::type2index<1>, Eigen::Index> one_by_depth;\n    one_by_depth.set(1, depth);\n    Eigen::IndexList<Eigen::type2index<0>> reduce_dims;\n    Eigen::IndexList<Eigen::Index, Eigen::type2index<1>> bcast_spec;\n    bcast_spec.set(0, rest_size);\n#endif\n\n    auto x_rest_by_depth = x.reshape(rest_by_depth).template cast<U>();\n    const int rest_size_minus_one = (rest_size > 1) ? (rest_size - 1) : 1;\n    U rest_size_inv = static_cast<U>(1.0f / static_cast<U>(rest_size));\n    // This adjustment is for Bessel's correction\n    U rest_size_adjust =\n        static_cast<U>(rest_size) / static_cast<U>(rest_size_minus_one);\n\n    Eigen::Tensor<U, 1, Eigen::RowMajor> batch_mean(depth);\n    Eigen::Tensor<U, 1, Eigen::RowMajor> batch_variance(depth);\n\n    batch_mean.device(d) = (x_rest_by_depth.sum(reduce_dims) * rest_size_inv);\n    auto x_centered = x_rest_by_depth -\n                      batch_mean.reshape(one_by_depth).broadcast(bcast_spec);\n\n    batch_variance.device(d) =\n        x_centered.square().sum(reduce_dims) * rest_size_inv;\n    auto scaling_factor = ((batch_variance + epsilon).rsqrt() * scale)\n                              .eval()\n                              .reshape(one_by_depth)\n                              .broadcast(bcast_spec);\n    auto x_scaled = x_centered * scaling_factor;\n    auto x_shifted =\n        (x_scaled + offset.reshape(one_by_depth).broadcast(bcast_spec))\n            .template cast<T>();\n\n    y.reshape(rest_by_depth).device(d) = x_shifted;\n    if (exponential_avg_factor == U(1.0)) {\n      saved_batch_var.device(d) = batch_variance;\n      saved_batch_mean.device(d) = batch_mean;\n      new_variance.device(d) = batch_variance * rest_size_adjust;\n      new_mean.device(d) = batch_mean;\n    } else {\n      U one_minus_factor = U(1) - exponential_avg_factor;\n      saved_batch_var.device(d) = batch_variance;\n      saved_batch_mean.device(d) = batch_mean;\n      new_variance.device(d) =\n          one_minus_factor * old_variance +\n          (exponential_avg_factor * rest_size_adjust) * batch_variance;\n      new_mean.device(d) =\n          one_minus_factor * old_mean + exponential_avg_factor * batch_mean;\n    }\n\n    if (tensor_format == FORMAT_NCHW) {\n      // Perform NHWC to NCHW\n      const std::vector<int32> perm = {0, 3, 1, 2};\n      const Status s = ::tensorflow::DoTranspose(\n          context->eigen_device<CPUDevice>(), transformed_y, perm, y_output);\n      if (!s.ok()) {\n        context->SetStatus(errors::InvalidArgument(\"Transpose failed: \", s));\n      }\n    }\n  }\n};\n\ntemplate <typename T, typename U>\nstruct FusedBatchNorm<CPUDevice, T, U, /* is_training= */ false> {\n  void operator()(OpKernelContext* context, const Tensor& x_input,\n                  const Tensor& scale_input, const Tensor& offset_input,\n                  const Tensor& estimated_mean_input,\n                  const Tensor& estimated_variance_input,\n                  const Tensor* side_input, U epsilon, U exponential_avg_factor,\n                  FusedBatchNormActivationMode activation_mode,\n                  Tensor* y_output, Tensor* batch_mean_output,\n                  Tensor* batch_var_output, Tensor* saved_mean_output,\n                  Tensor* saved_var_output, TensorFormat tensor_format,\n                  bool use_reserved_space) {\n    OP_REQUIRES(context, side_input == nullptr,\n                errors::Internal(\n                    \"The CPU implementation of FusedBatchNorm does not support \"\n                    \"side input.\"));\n    OP_REQUIRES(context,\n                activation_mode == FusedBatchNormActivationMode::kIdentity,\n                errors::Internal(\"The CPU implementation of FusedBatchNorm \"\n                                 \"does not support activations.\"));\n\n    if (use_reserved_space) {\n      Tensor* dummy_reserve_space = nullptr;\n      OP_REQUIRES_OK(context,\n                     context->allocate_output(5, {}, &dummy_reserve_space));\n      // Initialize the memory, to avoid sanitizer alerts.\n      dummy_reserve_space->flat<U>()(0) = U();\n    }\n\n    // If input is empty, return NaN mean/variance\n    if (x_input.shape().num_elements() == 0) {\n      functor::SetNanFunctor<CPUDevice, U> f;\n      f(context->eigen_device<CPUDevice>(), batch_mean_output->flat<U>());\n      f(context->eigen_device<CPUDevice>(), batch_var_output->flat<U>());\n      return;\n    }\n\n    Tensor transformed_x;\n    Tensor transformed_y;\n    if (tensor_format == FORMAT_NCHW) {\n      const int64_t in_batch = GetTensorDim(x_input, tensor_format, 'N');\n      const int64_t in_rows = GetTensorDim(x_input, tensor_format, 'H');\n      const int64_t in_cols = GetTensorDim(x_input, tensor_format, 'W');\n      const int64_t in_depths = GetTensorDim(x_input, tensor_format, 'C');\n      OP_REQUIRES_OK(context, context->allocate_temp(\n                                  DataTypeToEnum<T>::value,\n                                  ShapeFromFormat(FORMAT_NHWC, in_batch,\n                                                  in_rows, in_cols, in_depths),\n                                  &transformed_x));\n      OP_REQUIRES_OK(context, context->allocate_temp(\n                                  DataTypeToEnum<T>::value,\n                                  ShapeFromFormat(FORMAT_NHWC, in_batch,\n                                                  in_rows, in_cols, in_depths),\n                                  &transformed_y));\n      // Perform NCHW to NHWC\n      std::vector<int32> perm = {0, 2, 3, 1};\n      OP_REQUIRES_OK(\n          context, ::tensorflow::DoTranspose(context->eigen_device<CPUDevice>(),\n                                             x_input, perm, &transformed_x));\n    } else {\n      transformed_x = x_input;\n      transformed_y = *y_output;\n    }\n    typename TTypes<T, 4>::Tensor x(transformed_x.tensor<T, 4>());\n    typename TTypes<U>::ConstVec scale(scale_input.vec<U>());\n    typename TTypes<U>::ConstVec offset(offset_input.vec<U>());\n    typename TTypes<U>::ConstVec estimated_mean(estimated_mean_input.vec<U>());\n    typename TTypes<U>::ConstVec estimated_variance(\n        estimated_variance_input.vec<U>());\n    typename TTypes<T, 4>::Tensor y(transformed_y.tensor<T, 4>());\n    typename TTypes<U>::Vec batch_mean(batch_mean_output->vec<U>());\n    typename TTypes<U>::Vec batch_variance(batch_var_output->vec<U>());\n\n    const CPUDevice& d = context->eigen_device<CPUDevice>();\n\n    const int depth = x.dimension(3);\n    OP_REQUIRES(\n        context, depth != 0,\n        errors::Internal(\"The 4th element in the input shape cannot be 0.\"));\n    const int size = x.size();\n    const int rest_size = size / depth;\n    Eigen::DSizes<Eigen::Index, 2> rest_by_depth(rest_size, depth);\n\n#if !defined(EIGEN_HAS_INDEX_LIST)\n    Eigen::DSizes<Eigen::Index, 2> one_by_depth(1, depth);\n    Eigen::array<int, 1> reduce_dims({0});\n    Eigen::array<int, 2> bcast_spec({rest_size, 1});\n#else\n    Eigen::IndexList<Eigen::type2index<1>, Eigen::Index> one_by_depth;\n    one_by_depth.set(1, depth);\n    Eigen::IndexList<Eigen::Index, Eigen::type2index<1>> bcast_spec;\n    bcast_spec.set(0, rest_size);\n#endif\n\n    auto x_rest_by_depth = x.reshape(rest_by_depth).template cast<U>();\n    auto x_centered =\n        x_rest_by_depth -\n        estimated_mean.reshape(one_by_depth).broadcast(bcast_spec);\n    auto scaling_factor = ((estimated_variance + epsilon).rsqrt() * scale)\n                              .eval()\n                              .reshape(one_by_depth)\n                              .broadcast(bcast_spec);\n    auto x_scaled = x_centered * scaling_factor;\n    auto x_shifted =\n        (x_scaled + offset.reshape(one_by_depth).broadcast(bcast_spec))\n            .template cast<T>();\n\n    y.reshape(rest_by_depth).device(d) = x_shifted;\n    batch_mean.device(d) = estimated_mean;\n    batch_variance.device(d) = estimated_variance;\n\n    if (tensor_format == FORMAT_NCHW) {\n      // Perform NHWC to NCHW\n      const std::vector<int32> perm = {0, 3, 1, 2};\n      const Status s = ::tensorflow::DoTranspose(\n          context->eigen_device<CPUDevice>(), transformed_y, perm, y_output);\n      if (!s.ok()) {\n        context->SetStatus(errors::InvalidArgument(\"Transpose failed: \", s));\n      }\n    }\n  }\n};\n\ntemplate <typename T, typename U>\nstruct FusedBatchNormGrad<CPUDevice, T, U> {\n  void operator()(OpKernelContext* context, const Tensor& y_backprop_input,\n                  const Tensor& x_input, const Tensor& scale_input,\n                  const Tensor* offset_input, const Tensor& mean_input,\n                  const Tensor& variance_input, const Tensor* y_input,\n                  U epsilon, FusedBatchNormActivationMode activation_mode,\n                  Tensor* x_backprop_output, Tensor* scale_backprop_output,\n                  Tensor* offset_backprop_output,\n                  Tensor* side_input_backprop_output, bool use_reserved_space,\n                  TensorFormat tensor_format) {\n    OP_REQUIRES(context,\n                y_input == nullptr &&\n                    activation_mode == FusedBatchNormActivationMode::kIdentity,\n                errors::Internal(\n                    \"The CPU implementation of FusedBatchNormGrad does not \"\n                    \"support activations.\"));\n    OP_REQUIRES(context, side_input_backprop_output == nullptr,\n                errors::Internal(\"The CPU implementation of FusedBatchNormGrad \"\n                                 \"does not support side input.\"));\n\n    Tensor transformed_y_backprop_input;\n    Tensor transformed_x_input;\n    Tensor transformed_x_backprop_output;\n    if (tensor_format == FORMAT_NCHW) {\n      const int64_t in_batch = GetTensorDim(x_input, tensor_format, 'N');\n      const int64_t in_rows = GetTensorDim(x_input, tensor_format, 'H');\n      const int64_t in_cols = GetTensorDim(x_input, tensor_format, 'W');\n      const int64_t in_depths = GetTensorDim(x_input, tensor_format, 'C');\n      OP_REQUIRES_OK(context, context->allocate_temp(\n                                  DataTypeToEnum<T>::value,\n                                  ShapeFromFormat(FORMAT_NHWC, in_batch,\n                                                  in_rows, in_cols, in_depths),\n                                  &transformed_y_backprop_input));\n      OP_REQUIRES_OK(context, context->allocate_temp(\n                                  DataTypeToEnum<T>::value,\n                                  ShapeFromFormat(FORMAT_NHWC, in_batch,\n                                                  in_rows, in_cols, in_depths),\n                                  &transformed_x_input));\n      OP_REQUIRES_OK(context, context->allocate_temp(\n                                  DataTypeToEnum<T>::value,\n                                  ShapeFromFormat(FORMAT_NHWC, in_batch,\n                                                  in_rows, in_cols, in_depths),\n                                  &transformed_x_backprop_output));\n      // Perform NCHW to NHWC\n      std::vector<int32> perm = {0, 2, 3, 1};\n      OP_REQUIRES_OK(\n          context, ::tensorflow::DoTranspose(context->eigen_device<CPUDevice>(),\n                                             y_backprop_input, perm,\n                                             &transformed_y_backprop_input));\n      OP_REQUIRES_OK(context, ::tensorflow::DoTranspose(\n                                  context->eigen_device<CPUDevice>(), x_input,\n                                  perm, &transformed_x_input));\n    } else {\n      transformed_y_backprop_input = y_backprop_input;\n      transformed_x_input = x_input;\n      transformed_x_backprop_output = *x_backprop_output;\n    }\n    typename TTypes<T, 4>::Tensor y_backprop(\n        transformed_y_backprop_input.tensor<T, 4>());\n    typename TTypes<T, 4>::Tensor x(transformed_x_input.tensor<T, 4>());\n    typename TTypes<U>::ConstVec scale(scale_input.vec<U>());\n    typename TTypes<U>::ConstVec mean(mean_input.vec<U>());\n    typename TTypes<U>::ConstVec variance(variance_input.vec<U>());\n    typename TTypes<T, 4>::Tensor x_backprop(\n        transformed_x_backprop_output.tensor<T, 4>());\n    typename TTypes<U>::Vec offset_backprop(offset_backprop_output->vec<U>());\n\n    // Note: the following formulas are used to compute the gradients for\n    // back propagation.\n    // x_backprop = scale * rsqrt(variance + epsilon) *\n    //              [y_backprop - mean(y_backprop) - (x - mean(x)) *\n    //              mean(y_backprop * (x - mean(x))) / (variance + epsilon)]\n    // scale_backprop = sum(y_backprop *\n    //                  (x - mean(x)) * rsqrt(variance + epsilon))\n    // offset_backprop = sum(y_backprop)\n\n    const CPUDevice& d = context->eigen_device<CPUDevice>();\n    const int depth = x.dimension(3);\n    const int size = x.size();\n    const int rest_size = size / depth;\n    Eigen::DSizes<Eigen::Index, 2> rest_by_depth(rest_size, depth);\n\n#if !defined(EIGEN_HAS_INDEX_LIST)\n    Eigen::DSizes<Eigen::Index, 2> one_by_depth(1, depth);\n    Eigen::array<int, 2> bcast_spec({rest_size, 1});\n#else\n    Eigen::IndexList<Eigen::type2index<1>, Eigen::Index> one_by_depth;\n    one_by_depth.set(1, depth);\n    Eigen::IndexList<Eigen::Index, Eigen::type2index<1>> bcast_spec;\n    bcast_spec.set(0, rest_size);\n#endif\n\n    auto x_rest_by_depth = x.reshape(rest_by_depth).template cast<U>();\n    U rest_size_inv = static_cast<U>(1.0f / static_cast<U>(rest_size));\n\n    // Eigen is notoriously bad at reducing outer dimension, so we materialize\n    // all temporary tensors that require reduction, and then use Eigen redux\n    // functor, that is optimized for this particular task.\n    //\n    // All reductions are of this type: [rest_size, depth] -> [depth].\n    using ScalarSum = Eigen::internal::scalar_sum_op<U>;\n    const functor::ReduceOuterDimensions<T, U, U, ScalarSum> redux_sum_t;\n    const functor::ReduceOuterDimensions<U, U, U, ScalarSum> redux_sum_u;\n\n    auto scratch_dtype = DataTypeToEnum<U>::value;\n\n    // Allocate a temporary workspace of [depth] shape.\n    Tensor scratch_one_by_depth;\n    OP_REQUIRES_OK(context, context->allocate_temp(scratch_dtype, {depth},\n                                                   &scratch_one_by_depth));\n\n    // Maybe allocate a temporary workspace of [rest_size, depth] shape.\n    Tensor scratch_rest_by_depth;\n    if (std::is_same<T, U>::value) {\n      OP_REQUIRES(context,\n                  scratch_rest_by_depth.CopyFrom(transformed_x_backprop_output,\n                                                 {rest_size, depth}),\n                  errors::Internal(\"Failed to copy a tensor\"));\n    } else {\n      OP_REQUIRES_OK(context,\n                     context->allocate_temp(scratch_dtype, {rest_size, depth},\n                                            &scratch_rest_by_depth));\n    }\n\n    typename TTypes<U, 2>::Tensor scratch_tensor(\n        scratch_rest_by_depth.tensor<U, 2>());\n    typename TTypes<U>::Vec scratch_vector(scratch_one_by_depth.vec<U>());\n\n    auto x_mean_rest_by_depth =\n        mean.reshape(one_by_depth).broadcast(bcast_spec);\n    auto x_centered = (x_rest_by_depth - x_mean_rest_by_depth);\n    auto coef0_one_by_depth =\n        (variance.reshape(one_by_depth) + epsilon).rsqrt();\n    auto coef0_rest_by_depth = coef0_one_by_depth.broadcast(bcast_spec);\n    auto x_scaled = x_centered * coef0_rest_by_depth;\n\n    auto y_backprop_rest_by_depth =\n        y_backprop.reshape(rest_by_depth).template cast<U>();\n\n    // Compute `scale_backprop_output`:\n    //   scale_backprop =\n    //     (y_backprop_rest_by_depth * x_scaled).sum(reduce_dims)\n    scratch_tensor.device(d) = y_backprop_rest_by_depth * x_scaled;\n    redux_sum_u(d, rest_by_depth, scratch_rest_by_depth, scale_backprop_output);\n\n    // Compute 'offset_backprop_output':\n    //   offset_backprop =\n    //     y_backprop_rest_by_depth.sum(reduce_dims)\n    redux_sum_t(d, rest_by_depth, transformed_y_backprop_input,\n                offset_backprop_output);\n    auto y_backprop_sum = offset_backprop;\n\n    auto y_backprop_sum_one_by_depth = y_backprop_sum.reshape(one_by_depth);\n    auto y_backprop_mean_one_by_depth =\n        y_backprop_sum_one_by_depth * rest_size_inv;\n    auto y_backprop_mean_rest_by_depth =\n        y_backprop_mean_one_by_depth.broadcast(bcast_spec);\n    auto y_backprop_centered =\n        y_backprop_rest_by_depth - y_backprop_mean_rest_by_depth;\n\n    // Compute expression:\n    //   y_backprop_centered_mean =\n    //     (y_backprop_rest_by_depth * x_centered).mean(reduce_dims)\n    scratch_tensor.device(d) = y_backprop_rest_by_depth * x_centered;\n    redux_sum_u(d, rest_by_depth, scratch_rest_by_depth, &scratch_one_by_depth);\n    auto y_backprop_centered_mean =\n        scratch_vector.reshape(one_by_depth) / static_cast<U>(rest_size);\n\n    auto coef1 = (scale.reshape(one_by_depth) * coef0_one_by_depth)\n                     .broadcast(bcast_spec);\n    auto coef2 = (coef0_one_by_depth.square() * y_backprop_centered_mean)\n                     .broadcast(bcast_spec);\n\n    x_backprop.reshape(rest_by_depth).device(d) =\n        (coef1 * (y_backprop_centered - x_centered * coef2)).template cast<T>();\n\n    if (tensor_format == FORMAT_NCHW) {\n      // Perform NHWC to NCHW\n      std::vector<int32> perm = {0, 3, 1, 2};\n      OP_REQUIRES_OK(\n          context, ::tensorflow::DoTranspose(context->eigen_device<CPUDevice>(),\n                                             transformed_x_backprop_output,\n                                             perm, x_backprop_output));\n    }\n  }\n};\n\ntemplate <typename T, typename U>\nstruct FusedBatchNormFreezeGrad<CPUDevice, T, U> {\n  void operator()(OpKernelContext* context, const Tensor& y_backprop_input,\n                  const Tensor& x_input, const Tensor& scale_input,\n                  const Tensor& pop_mean_input,\n                  const Tensor& pop_variance_input, U epsilon,\n                  Tensor* x_backprop_output, Tensor* scale_backprop_output,\n                  Tensor* offset_backprop_output) {\n    typename TTypes<T, 4>::ConstTensor y_backprop(\n        y_backprop_input.tensor<T, 4>());\n    typename TTypes<T, 4>::ConstTensor input(x_input.tensor<T, 4>());\n    typename TTypes<U>::ConstVec scale(scale_input.vec<U>());\n    typename TTypes<U>::ConstVec pop_mean(pop_mean_input.vec<U>());\n    typename TTypes<U>::ConstVec pop_var(pop_variance_input.vec<U>());\n    typename TTypes<T, 4>::Tensor x_backprop(x_backprop_output->tensor<T, 4>());\n    typename TTypes<U>::Vec scale_backprop(scale_backprop_output->vec<U>());\n\n    const int depth = pop_mean.dimension(0);\n    const int rest_size = input.size() / depth;\n\n    const CPUDevice& d = context->eigen_device<CPUDevice>();\n\n    // Allocate two temporary workspaces of [depth] shape.\n    Tensor scratch1_vec, scratch2_vec;\n    OP_REQUIRES_OK(context, context->allocate_temp(DataTypeToEnum<U>::value,\n                                                   {depth}, &scratch1_vec));\n    OP_REQUIRES_OK(context, context->allocate_temp(DataTypeToEnum<U>::value,\n                                                   {depth}, &scratch2_vec));\n\n    // Maybe allocate a temporary workspace of [rest_size, depth] shape.\n    Tensor scratch3_tensor;\n    if (std::is_same<T, U>::value) {\n      OP_REQUIRES(\n          context,\n          scratch3_tensor.CopyFrom(*x_backprop_output, {rest_size, depth}),\n          errors::Internal(\"Failed to copy a tensor\"));\n    } else {\n      OP_REQUIRES_OK(context, context->allocate_temp(DataTypeToEnum<U>::value,\n                                                     {rest_size, depth},\n                                                     &scratch3_tensor));\n    }\n\n    typename TTypes<U>::Vec scratch1(scratch1_vec.vec<U>());\n    typename TTypes<U>::Vec scratch2(scratch2_vec.vec<U>());\n    typename TTypes<U, 2>::Tensor scratch3(scratch3_tensor.tensor<U, 2>());\n\n    Eigen::DSizes<Eigen::Index, 2> rest_by_depth(rest_size, depth);\n#if !defined(EIGEN_HAS_INDEX_LIST)\n    Eigen::DSizes<Eigen::Index, 2> one_by_depth(1, depth);\n    Eigen::array<int, 2> rest_by_one({rest_size, 1});\n#else\n    Eigen::IndexList<Eigen::type2index<1>, Eigen::Index> one_by_depth;\n    one_by_depth.set(1, depth);\n    Eigen::IndexList<Eigen::Index, Eigen::type2index<1>> rest_by_one;\n    rest_by_one.set(0, rest_size);\n#endif\n\n    // Sum reduction along the 0th dimension using custom CPU functor.\n    using ScalarSum = Eigen::internal::scalar_sum_op<U>;\n    const functor::ReduceOuterDimensions<T, U, U, ScalarSum> redux_sum_t;\n    const functor::ReduceOuterDimensions<U, U, U, ScalarSum> redux_sum_u;\n\n    // offset_backprop  = sum(y_backprop)\n    // scale_backprop = y_backprop * ((x - pop_mean) * rsqrt(pop_var + epsilon))\n    // x_backprop = y_backprop * (scale * rsqrt(pop_var + epsilon))\n\n    // NOTE: DEFAULT DEVICE comment is added to expression assignments that\n    // we don't want to be executed in a thread pool.\n\n    auto y_backprop_rest_by_depth =\n        y_backprop.reshape(rest_by_depth).template cast<U>();\n    auto input_rest_by_depth = input.reshape(rest_by_depth).template cast<U>();\n\n    // offset_backprop  = sum(y_backprop)\n    redux_sum_t(d, rest_by_depth, y_backprop_input, offset_backprop_output);\n\n    // scratch1 = rsqrt(pop_var + epsilon)\n    scratch1 = (pop_var + pop_var.constant(epsilon)).rsqrt();  // DEFAULT DEVICE\n\n    // scratch2 = sum(y_backprop * (x - mean))\n    scratch3.device(d) =\n        y_backprop_rest_by_depth *\n        (input_rest_by_depth -\n         pop_mean.reshape(one_by_depth).broadcast(rest_by_one));\n    redux_sum_u(d, rest_by_depth, scratch3_tensor, &scratch2_vec);\n\n    x_backprop.reshape(rest_by_depth).device(d) =\n        (y_backprop_rest_by_depth *\n         ((scratch1.reshape(one_by_depth) * scale.reshape(one_by_depth))\n              .broadcast(rest_by_one)))\n            .template cast<T>();\n    scale_backprop = scratch2 * scratch1;  // DEFAULT DEVICE\n  }\n};\n\n#if !GOOGLE_CUDA\nnamespace {\n// See implementation under GOOGLE_CUDA #ifdef below.\n// This is a CUDA specific feature, do not enable it for non-CUDA builds\nbool BatchnormSpatialPersistentEnabled() { return false; }\n}  // namespace\n#endif\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\nnamespace {\n\nse::dnn::ActivationMode AsDnnActivationMode(\n    const FusedBatchNormActivationMode activation_mode) {\n  switch (activation_mode) {\n    case FusedBatchNormActivationMode::kIdentity:\n      return se::dnn::ActivationMode::kNone;\n    case FusedBatchNormActivationMode::kRelu:\n      return se::dnn::ActivationMode::kRelu;\n  }\n}\n\n#if GOOGLE_CUDA\n// NOTE(ezhulenev): See `BatchnormSpatialPersistentEnabled` documentation in the\n// `cuda_dnn.cc` for details.\nbool BatchnormSpatialPersistentEnabled() {\n#if CUDNN_VERSION >= 7402\n  static bool is_enabled = [] {\n    bool is_enabled = false;\n    TF_CHECK_OK(tensorflow::ReadBoolFromEnvVar(\n        \"TF_USE_CUDNN_BATCHNORM_SPATIAL_PERSISTENT\",\n        /*default_val=*/false, &is_enabled));\n    return is_enabled;\n  }();\n  return is_enabled;\n#else\n  return false;\n#endif\n}\n#endif\n\n}  // namespace\n\ntemplate <typename U, typename T>\nDeviceMemory<U> CastDeviceMemory(Tensor* tensor) {\n  return DeviceMemory<U>::MakeFromByteSize(\n      tensor->template flat<T>().data(),\n      tensor->template flat<T>().size() * sizeof(T));\n}\n\n// A helper to allocate temporary scratch memory for Cudnn BatchNormEx ops. It\n// takes the ownership of the underlying memory. The expectation is that the\n// memory should be alive for the span of the Cudnn BatchNormEx itself.\ntemplate <typename T>\nclass CudnnBatchNormAllocatorInTemp : public ScratchAllocator {\n public:\n  ~CudnnBatchNormAllocatorInTemp() override = default;\n\n  explicit CudnnBatchNormAllocatorInTemp(OpKernelContext* context)\n      : context_(context) {}\n\n  int64_t GetMemoryLimitInBytes() override {\n    return std::numeric_limits<int64_t>::max();\n  }\n\n  StatusOr<DeviceMemory<uint8>> AllocateBytes(int64_t byte_size) override {\n    Tensor temporary_memory;\n    const DataType tf_data_type = DataTypeToEnum<T>::v();\n    int64_t allocate_count =\n        Eigen::divup(byte_size, static_cast<int64_t>(sizeof(T)));\n    Status allocation_status(context_->allocate_temp(\n        tf_data_type, TensorShape({allocate_count}), &temporary_memory));\n    if (!allocation_status.ok()) {\n      return allocation_status;\n    }\n    // Hold the reference of the allocated tensors until the end of the\n    // allocator.\n    allocated_tensors_.push_back(temporary_memory);\n    total_byte_size_ += byte_size;\n    return DeviceMemory<uint8>::MakeFromByteSize(\n        temporary_memory.template flat<T>().data(),\n        temporary_memory.template flat<T>().size() * sizeof(T));\n  }\n\n  int64_t TotalByteSize() const { return total_byte_size_; }\n\n  Tensor get_allocated_tensor(int index) const {\n    return allocated_tensors_[index];\n  }\n\n private:\n  int64_t total_byte_size_ = 0;\n  OpKernelContext* context_;  // not owned\n  std::vector<Tensor> allocated_tensors_;\n};\n\n// A helper to allocate memory for Cudnn BatchNormEx as a kernel output. It is\n// used by forward pass kernel to feed the output to the backward pass.\n// The memory is expected to live long enough after the backward pass is\n// finished.\ntemplate <typename T>\nclass CudnnBatchNormAllocatorInOutput : public ScratchAllocator {\n public:\n  ~CudnnBatchNormAllocatorInOutput() override {\n    if (!output_allocated) {\n      Tensor* dummy_reserve_space = nullptr;\n      OP_REQUIRES_OK(context_, context_->allocate_output(output_index_, {},\n                                                         &dummy_reserve_space));\n    }\n  }\n\n  CudnnBatchNormAllocatorInOutput(OpKernelContext* context, int output_index)\n      : context_(context), output_index_(output_index) {}\n\n  int64_t GetMemoryLimitInBytes() override {\n    return std::numeric_limits<int64_t>::max();\n  }\n\n  StatusOr<DeviceMemory<uint8>> AllocateBytes(int64_t byte_size) override {\n    output_allocated = true;\n    DCHECK(total_byte_size_ == 0)\n        << \"Reserve space allocator can only be called once\";\n    int64_t allocate_count =\n        Eigen::divup(byte_size, static_cast<int64_t>(sizeof(T)));\n\n    Tensor* temporary_memory = nullptr;\n    Status allocation_status(context_->allocate_output(\n        output_index_, TensorShape({allocate_count}), &temporary_memory));\n    if (!allocation_status.ok()) {\n      return allocation_status;\n    }\n    total_byte_size_ += byte_size;\n    auto memory_uint8 = DeviceMemory<uint8>::MakeFromByteSize(\n        temporary_memory->template flat<T>().data(),\n        temporary_memory->template flat<T>().size() * sizeof(T));\n    return StatusOr<DeviceMemory<uint8>>(memory_uint8);\n  }\n\n  int64_t TotalByteSize() { return total_byte_size_; }\n\n private:\n  int64_t total_byte_size_ = 0;\n  OpKernelContext* context_;  // not owned\n  int output_index_;\n  bool output_allocated = false;\n};\n\ntemplate <typename T, typename U, bool is_training>\nstruct FusedBatchNorm<GPUDevice, T, U, is_training> {\n  void operator()(OpKernelContext* context, const Tensor& x,\n                  const Tensor& scale, const Tensor& offset,\n                  const Tensor& estimated_mean,\n                  const Tensor& estimated_variance, const Tensor* side_input,\n                  U epsilon, U exponential_avg_factor,\n                  FusedBatchNormActivationMode activation_mode, Tensor* y,\n                  Tensor* batch_mean, Tensor* batch_var, Tensor* saved_mean,\n                  Tensor* saved_inv_var, TensorFormat tensor_format,\n                  bool use_reserved_space) {\n    auto* stream = context->op_device_context()->stream();\n    OP_REQUIRES(context, stream, errors::Internal(\"No GPU stream available\"));\n\n    const int64_t batch_size = GetTensorDim(x, tensor_format, 'N');\n    const int64_t channels = GetTensorDim(x, tensor_format, 'C');\n    const int64_t height = GetTensorDim(x, tensor_format, 'H');\n    const int64_t width = GetTensorDim(x, tensor_format, 'W');\n\n    // If use_reserved_space we have reserve_space_3 output (only in\n    // FusedBatchNormV3 op).\n\n#if GOOGLE_CUDA\n    // Check if cuDNN batch normalization has a fast NHWC implementation:\n    //   (1) In inference mode it's always fast.\n    //   (2) Tensorflow enabled batchnorm spatial persistence, we are called\n    //   from\n    //       FusedBatchNormV3, i.e. use_reserved_space is true.\n    const bool fast_nhwc_batch_norm =\n        !is_training ||\n        (BatchnormSpatialPersistentEnabled() &&\n         DataTypeToEnum<T>::value == DT_HALF && use_reserved_space);\n#else\n    // fast NHWC implementation is a CUDA only feature\n    const bool fast_nhwc_batch_norm = false;\n#endif\n\n    // If input tensor is in NHWC format, and we have a fast cuDNN\n    // implementation, there is no need to do data format conversion.\n    TensorFormat compute_format =\n        fast_nhwc_batch_norm && tensor_format == FORMAT_NHWC ? FORMAT_NHWC\n                                                             : FORMAT_NCHW;\n\n    VLOG(2) << \"FusedBatchNorm:\"\n            << \" batch_size: \" << batch_size << \" channels: \" << channels\n            << \" height: \" << height << \" width:\" << width\n            << \" x shape: \" << x.shape().DebugString()\n            << \" scale shape: \" << scale.shape().DebugString()\n            << \" offset shape: \" << offset.shape().DebugString()\n            << \" activation mode: \" << ToString(activation_mode)\n            << \" tensor format: \" << ToString(tensor_format)\n            << \" compute format: \" << ToString(compute_format);\n\n    auto maybe_make_dummy_output = [context, use_reserved_space]() -> Status {\n      if (use_reserved_space) {\n        Tensor* dummy_reserve_space = nullptr;\n        return context->allocate_output(5, {}, &dummy_reserve_space);\n      }\n      return Status::OK();\n    };\n\n    // If input is empty, return NaN mean/variance\n    if (x.shape().num_elements() == 0) {\n      OP_REQUIRES_OK(context, maybe_make_dummy_output());\n      functor::SetNanFunctor<GPUDevice, U> f;\n      f(context->eigen_device<GPUDevice>(), batch_mean->flat<U>());\n      f(context->eigen_device<GPUDevice>(), batch_var->flat<U>());\n      return;\n    }\n\n    // In inference mode we use custom CUDA kernel, because cuDNN does not\n    // support side input and activations for inference.\n    const bool has_side_input = side_input != nullptr;\n    const bool has_activation =\n        activation_mode != FusedBatchNormActivationMode::kIdentity;\n\n    if (!is_training && (has_side_input || has_activation)) {\n      OP_REQUIRES_OK(context, maybe_make_dummy_output());\n      FusedBatchNormInferenceFunctor<GPUDevice, T, U> inference_functor;\n\n      if (has_side_input) {\n        inference_functor(context, tensor_format, x.tensor<T, 4>(),\n                          scale.vec<U>(), offset.vec<U>(),\n                          estimated_mean.vec<U>(), estimated_variance.vec<U>(),\n                          side_input->tensor<T, 4>(), epsilon, activation_mode,\n                          y->tensor<T, 4>());\n      } else {\n        typename TTypes<T, 4>::ConstTensor empty_tensor(nullptr, 0, 0, 0, 0);\n        inference_functor(context, tensor_format, x.tensor<T, 4>(),\n                          scale.vec<U>(), offset.vec<U>(),\n                          estimated_mean.vec<U>(), estimated_variance.vec<U>(),\n                          empty_tensor, epsilon, activation_mode,\n                          y->tensor<T, 4>());\n      }\n      return;\n    }\n\n    Tensor x_maybe_transformed = x;\n    Tensor x_transformed;\n    Tensor y_transformed;\n    se::DeviceMemory<T> y_ptr;\n\n    if (tensor_format == compute_format) {\n      y_ptr = StreamExecutorUtil::AsDeviceMemory<T>(*y);\n    } else if (tensor_format == FORMAT_NHWC && compute_format == FORMAT_NCHW) {\n      OP_REQUIRES_OK(context, context->allocate_temp(\n                                  DataTypeToEnum<T>::value,\n                                  ShapeFromFormat(compute_format, batch_size,\n                                                  height, width, channels),\n                                  &x_transformed));\n      functor::NHWCToNCHW<GPUDevice, T, 4>()(\n          context->eigen_device<GPUDevice>(),\n          const_cast<const Tensor&>(x_maybe_transformed).tensor<T, 4>(),\n          x_transformed.tensor<T, 4>());\n      x_maybe_transformed = x_transformed;\n\n      OP_REQUIRES_OK(context, context->allocate_temp(\n                                  DataTypeToEnum<T>::value,\n                                  ShapeFromFormat(compute_format, batch_size,\n                                                  height, width, channels),\n                                  &y_transformed));\n      y_ptr = StreamExecutorUtil::AsDeviceMemory<T>(y_transformed);\n    } else {\n      context->SetStatus(errors::Internal(\n          \"Unsupported tensor format: \", ToString(tensor_format),\n          \" and compute format: \", ToString(compute_format)));\n      return;\n    }\n\n    const se::dnn::DataLayout data_layout =\n        compute_format == FORMAT_NHWC ? se::dnn::DataLayout::kBatchYXDepth\n                                      : se::dnn::DataLayout::kBatchDepthYX;\n\n    se::dnn::BatchDescriptor x_desc;\n    x_desc.set_count(batch_size)\n        .set_feature_map_count(channels)\n        .set_height(height)\n        .set_width(width)\n        .set_layout(data_layout);\n\n    se::dnn::BatchDescriptor scale_offset_desc;\n    scale_offset_desc.set_count(1)\n        .set_feature_map_count(channels)\n        .set_height(1)\n        .set_width(1)\n        .set_layout(se::dnn::DataLayout::kBatchDepthYX);\n\n    auto x_ptr = StreamExecutorUtil::AsDeviceMemory<T>(x_maybe_transformed);\n    auto scale_ptr = StreamExecutorUtil::AsDeviceMemory<U>(scale);\n    auto offset_ptr = StreamExecutorUtil::AsDeviceMemory<U>(offset);\n    auto estimated_mean_ptr =\n        StreamExecutorUtil::AsDeviceMemory<U>(estimated_mean);\n    auto estimated_variance_ptr =\n        StreamExecutorUtil::AsDeviceMemory<U>(estimated_variance);\n    auto side_input_ptr =\n        side_input != nullptr\n            ? StreamExecutorUtil::AsDeviceMemory<T>(*side_input)\n            : se::DeviceMemory<T>();\n    auto batch_mean_ptr = StreamExecutorUtil::AsDeviceMemory<U>(*batch_mean);\n\n    auto batch_var_ptr = StreamExecutorUtil::AsDeviceMemory<U>(*batch_var);\n    auto saved_mean_ptr = StreamExecutorUtil::AsDeviceMemory<U>(*saved_mean);\n    auto saved_inv_var_ptr =\n        StreamExecutorUtil::AsDeviceMemory<U>(*saved_inv_var);\n\n    std::unique_ptr<functor::CudnnBatchNormAllocatorInOutput<U>>\n        reserve_space_allocator;\n    std::unique_ptr<functor::CudnnBatchNormAllocatorInTemp<uint8>>\n        workspace_allocator;\n    if (use_reserved_space) {\n      reserve_space_allocator.reset(\n          new functor::CudnnBatchNormAllocatorInOutput<U>(context, 5));\n      workspace_allocator.reset(\n          new functor::CudnnBatchNormAllocatorInTemp<uint8>(context));\n    }\n    if (!batch_mean->SharesBufferWith(estimated_mean) &&\n        exponential_avg_factor != 1.0f) {\n      OP_REQUIRES(\n          context,\n          stream\n              ->ThenMemcpyD2D(&batch_mean_ptr, estimated_mean_ptr,\n                              estimated_mean.NumElements() * sizeof(U))\n              .ok(),\n          errors::Internal(\"MatrixTriangularSolveOp: failed to copy rhs \"\n                           \"from device\"));\n    }\n    if (!batch_var->SharesBufferWith(estimated_variance) &&\n        exponential_avg_factor != 1.0f) {\n      OP_REQUIRES(\n          context,\n          stream\n              ->ThenMemcpyD2D(&batch_var_ptr, estimated_variance_ptr,\n                              estimated_variance.NumElements() * sizeof(U))\n              .ok(),\n          errors::Internal(\"MatrixTriangularSolveOp: failed to copy rhs \"\n                           \"from device\"));\n    }\n    bool cudnn_launch_status =\n        stream\n            ->ThenBatchNormalizationForward(\n                x_ptr, scale_ptr, offset_ptr, estimated_mean_ptr,\n                estimated_variance_ptr, side_input_ptr, x_desc,\n                scale_offset_desc, static_cast<double>(epsilon),\n                static_cast<double>(exponential_avg_factor),\n                AsDnnActivationMode(activation_mode), &y_ptr, &batch_mean_ptr,\n                &batch_var_ptr, &saved_mean_ptr, &saved_inv_var_ptr,\n                is_training, reserve_space_allocator.get(),\n                workspace_allocator.get())\n            .ok();\n\n    if (!cudnn_launch_status) {\n      context->SetStatus(\n          errors::Internal(\"cuDNN launch failure : input shape (\",\n                           x.shape().DebugString(), \")\"));\n      return;\n    }\n\n    if (tensor_format == FORMAT_NHWC && compute_format == FORMAT_NCHW) {\n      functor::NCHWToNHWC<GPUDevice, T, 4>()(\n          context->eigen_device<GPUDevice>(),\n          const_cast<const Tensor&>(y_transformed).tensor<T, 4>(),\n          y->tensor<T, 4>());\n    }\n  }\n};\n\ntemplate <typename T, typename U>\nstruct FusedBatchNormGrad<GPUDevice, T, U> {\n  void operator()(OpKernelContext* context, const Tensor& y_backprop,\n                  const Tensor& x, const Tensor& scale, const Tensor* offset,\n                  const Tensor& mean, const Tensor& inv_variance,\n                  const Tensor* y, U epsilon,\n                  FusedBatchNormActivationMode activation_mode,\n                  Tensor* x_backprop, Tensor* scale_backprop,\n                  Tensor* offset_backprop, Tensor* side_input_backprop,\n                  bool use_reserved_space, TensorFormat tensor_format) {\n    auto* stream = context->op_device_context()->stream();\n    OP_REQUIRES(context, stream, errors::Internal(\"No GPU stream available\"));\n\n    const int64_t batch_size = GetTensorDim(x, tensor_format, 'N');\n    const int64_t channels = GetTensorDim(x, tensor_format, 'C');\n    const int64_t height = GetTensorDim(x, tensor_format, 'H');\n    const int64_t width = GetTensorDim(x, tensor_format, 'W');\n\n#if GOOGLE_CUDA\n    // Check if cuDNN batch normalization has a fast NHWC implementation:\n    //   (1) Tensorflow enabled batchnorm spatial persistence, and\n    //       FusedBatchNormGradV3 passed non-null reserve space and allocator.\n    const bool fast_nhwc_batch_norm = BatchnormSpatialPersistentEnabled() &&\n                                      DataTypeToEnum<T>::value == DT_HALF &&\n                                      use_reserved_space;\n#else\n    // fast NHWC implementation is a CUDA only feature\n    const bool fast_nhwc_batch_norm = false;\n#endif\n\n    // If input tensor is in NHWC format, and we have a fast cuDNN\n    // implementation, there is no need to do data format conversion.\n    TensorFormat compute_format =\n        fast_nhwc_batch_norm && tensor_format == FORMAT_NHWC ? FORMAT_NHWC\n                                                             : FORMAT_NCHW;\n\n    VLOG(2) << \"FusedBatchNormGrad:\"\n            << \" batch_size: \" << batch_size << \" channels: \" << channels\n            << \" height: \" << height << \" width: \" << width\n            << \" y_backprop shape: \" << y_backprop.shape().DebugString()\n            << \" x shape: \" << x.shape().DebugString()\n            << \" scale shape: \" << scale.shape().DebugString()\n            << \" activation mode: \" << ToString(activation_mode)\n            << \" tensor format: \" << ToString(tensor_format)\n            << \" compute format: \" << ToString(compute_format);\n\n    // Inputs\n    Tensor y_backprop_maybe_transformed = y_backprop;\n    Tensor x_maybe_transformed = x;\n    Tensor y_backprop_transformed;\n    Tensor x_transformed;\n\n    // Outputs\n    Tensor x_backprop_transformed;\n    se::DeviceMemory<T> x_backprop_ptr;\n\n    if (tensor_format == compute_format) {\n      x_backprop_ptr = StreamExecutorUtil::AsDeviceMemory<T>(*x_backprop);\n    } else if (tensor_format == FORMAT_NHWC && compute_format == FORMAT_NCHW) {\n      // Transform inputs from 'NHWC' to 'NCHW'\n      OP_REQUIRES_OK(context, context->allocate_temp(\n                                  DataTypeToEnum<T>::value,\n                                  ShapeFromFormat(FORMAT_NCHW, batch_size,\n                                                  height, width, channels),\n                                  &y_backprop_transformed));\n      functor::NHWCToNCHW<GPUDevice, T, 4>()(\n          context->eigen_device<GPUDevice>(),\n          const_cast<const Tensor&>(y_backprop_maybe_transformed)\n              .tensor<T, 4>(),\n          y_backprop_transformed.tensor<T, 4>());\n      y_backprop_maybe_transformed = y_backprop_transformed;\n\n      OP_REQUIRES_OK(context, context->allocate_temp(\n                                  DataTypeToEnum<T>::value,\n                                  ShapeFromFormat(FORMAT_NCHW, batch_size,\n                                                  height, width, channels),\n                                  &x_transformed));\n      functor::NHWCToNCHW<GPUDevice, T, 4>()(\n          context->eigen_device<GPUDevice>(),\n          const_cast<const Tensor&>(x_maybe_transformed).tensor<T, 4>(),\n          x_transformed.tensor<T, 4>());\n      x_maybe_transformed = x_transformed;\n\n      // Allocate memory for transformed outputs in 'NCHW'\n      OP_REQUIRES_OK(context, context->allocate_temp(\n                                  DataTypeToEnum<T>::value,\n                                  ShapeFromFormat(FORMAT_NCHW, batch_size,\n                                                  height, width, channels),\n                                  &x_backprop_transformed));\n      x_backprop_ptr =\n          StreamExecutorUtil::AsDeviceMemory<T>(x_backprop_transformed);\n    } else {\n      context->SetStatus(errors::Internal(\n          \"Unsupported tensor format: \", ToString(tensor_format),\n          \" and compute format: \", ToString(compute_format)));\n      return;\n    }\n\n    const se::dnn::DataLayout data_layout =\n        compute_format == FORMAT_NHWC ? se::dnn::DataLayout::kBatchYXDepth\n                                      : se::dnn::DataLayout::kBatchDepthYX;\n\n    se::dnn::BatchDescriptor x_desc;\n    x_desc.set_count(batch_size)\n        .set_feature_map_count(channels)\n        .set_height(height)\n        .set_width(width)\n        .set_layout(data_layout);\n\n    se::dnn::BatchDescriptor scale_offset_desc;\n    scale_offset_desc.set_count(1)\n        .set_feature_map_count(channels)\n        .set_height(1)\n        .set_width(1)\n        .set_layout(se::dnn::DataLayout::kBatchDepthYX);\n\n    auto y_backprop_ptr =\n        StreamExecutorUtil::AsDeviceMemory<T>(y_backprop_maybe_transformed);\n    auto x_ptr = StreamExecutorUtil::AsDeviceMemory<T>(x_maybe_transformed);\n    auto scale_ptr = StreamExecutorUtil::AsDeviceMemory<U>(scale);\n    auto offset_ptr = offset != nullptr\n                          ? StreamExecutorUtil::AsDeviceMemory<U>(*offset)\n                          : se::DeviceMemory<U>();\n    auto mean_ptr = StreamExecutorUtil::AsDeviceMemory<U>(mean);\n    auto inv_variance_ptr = StreamExecutorUtil::AsDeviceMemory<U>(inv_variance);\n    auto y_ptr = y != nullptr ? StreamExecutorUtil::AsDeviceMemory<T>(*y)\n                              : se::DeviceMemory<T>();\n    auto scale_backprop_ptr =\n        StreamExecutorUtil::AsDeviceMemory<U>(*scale_backprop);\n    auto offset_backprop_ptr =\n        StreamExecutorUtil::AsDeviceMemory<U>(*offset_backprop);\n    auto side_input_backprop_ptr =\n        side_input_backprop != nullptr\n            ? StreamExecutorUtil::AsDeviceMemory<T>(*side_input_backprop)\n            : se::DeviceMemory<T>();\n\n    std::unique_ptr<functor::CudnnBatchNormAllocatorInTemp<uint8>>\n        workspace_allocator;\n    DeviceMemory<uint8>* reserve_space_data_ptr = nullptr;\n    DeviceMemory<uint8> reserve_space_data;\n#if CUDNN_VERSION >= 7402\n    if (use_reserved_space) {\n      const Tensor& reserve_space = context->input(5);\n      workspace_allocator.reset(\n          new functor::CudnnBatchNormAllocatorInTemp<uint8>(context));\n\n      // the cudnn kernel outputs inverse variance in forward and reuse it in\n      // backward\n      if (reserve_space.dims() != 0) {\n        reserve_space_data = functor::CastDeviceMemory<uint8, U>(\n            const_cast<Tensor*>(&reserve_space));\n        reserve_space_data_ptr = &reserve_space_data;\n      }\n    }\n#endif  // CUDNN_VERSION >= 7402\n\n    bool cudnn_launch_status =\n        stream\n            ->ThenBatchNormalizationBackward(\n                y_backprop_ptr, x_ptr, scale_ptr, offset_ptr, mean_ptr,\n                inv_variance_ptr, y_ptr, x_desc, scale_offset_desc,\n                static_cast<double>(epsilon),\n                AsDnnActivationMode(activation_mode), &x_backprop_ptr,\n                &scale_backprop_ptr, &offset_backprop_ptr,\n                &side_input_backprop_ptr, reserve_space_data_ptr,\n                workspace_allocator.get())\n            .ok();\n\n    if (!cudnn_launch_status) {\n      context->SetStatus(\n          errors::Internal(\"cuDNN launch failure : input shape (\",\n                           x.shape().DebugString(), \")\"));\n    }\n    if (tensor_format == FORMAT_NHWC && compute_format == FORMAT_NCHW) {\n      functor::NCHWToNHWC<GPUDevice, T, 4>()(\n          context->eigen_device<GPUDevice>(),\n          const_cast<const Tensor&>(x_backprop_transformed).tensor<T, 4>(),\n          x_backprop->tensor<T, 4>());\n    }\n  }\n};\n\n// Forward declarations of the functor specializations for GPU.\n#define DECLARE_GPU_SPEC(T, U)                                                 \\\n  template <>                                                                  \\\n  void FusedBatchNormFreezeGrad<GPUDevice, T, U>::operator()(                  \\\n      OpKernelContext* context, const Tensor& y_backprop_input,                \\\n      const Tensor& x_input, const Tensor& scale_input,                        \\\n      const Tensor& mean_input, const Tensor& variance_input, U epsilon,       \\\n      Tensor* x_backprop_output, Tensor* scale_backprop_output,                \\\n      Tensor* offset_backprop_output);                                         \\\n  extern template struct FusedBatchNormFreezeGrad<GPUDevice, T, U>;            \\\n  template <>                                                                  \\\n  void FusedBatchNormInferenceFunctor<GPUDevice, T, U>::operator()(            \\\n      OpKernelContext* context, TensorFormat tensor_format,                    \\\n      typename TTypes<T, 4>::ConstTensor in,                                   \\\n      typename TTypes<U>::ConstVec scale, typename TTypes<U>::ConstVec offset, \\\n      typename TTypes<U>::ConstVec estimated_mean,                             \\\n      typename TTypes<U>::ConstVec estimated_variance,                         \\\n      typename TTypes<T, 4>::ConstTensor side_input, U epsilon,                \\\n      FusedBatchNormActivationMode activation_mode,                            \\\n      typename TTypes<T, 4>::Tensor out);                                      \\\n  extern template struct FusedBatchNormInferenceFunctor<GPUDevice, T, U>;\n\nDECLARE_GPU_SPEC(float, float);\nDECLARE_GPU_SPEC(Eigen::half, float);\n\n#undef DECLARE_GPU_SPEC\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n}  // namespace functor\n\ntemplate <typename Device, typename T, typename U>\nclass FusedBatchNormOpBase : public OpKernel {\n  using FbnActivationMode = functor::FusedBatchNormActivationMode;\n\n protected:\n  explicit FusedBatchNormOpBase(OpKernelConstruction* context,\n                                bool is_batch_norm_ex = false)\n      : OpKernel(context) {\n    float epsilon;\n    OP_REQUIRES_OK(context, context->GetAttr(\"epsilon\", &epsilon));\n    epsilon_ = U(epsilon);\n    float exponential_avg_factor;\n    OP_REQUIRES_OK(context, context->GetAttr(\"exponential_avg_factor\",\n                                             &exponential_avg_factor));\n    exponential_avg_factor_ = U(exponential_avg_factor);\n    string tensor_format;\n    OP_REQUIRES_OK(context, context->GetAttr(\"data_format\", &tensor_format));\n    OP_REQUIRES(context, FormatFromString(tensor_format, &tensor_format_),\n                errors::InvalidArgument(\"Invalid data format\"));\n    OP_REQUIRES_OK(context, context->GetAttr(\"is_training\", &is_training_));\n\n    if (!is_batch_norm_ex) {\n      has_side_input_ = false;\n      activation_mode_ = FbnActivationMode::kIdentity;\n    } else {\n      OP_REQUIRES_OK(context, ParseActivationMode(context, &activation_mode_));\n\n      int num_side_inputs;\n      OP_REQUIRES_OK(context,\n                     context->GetAttr(\"num_side_inputs\", &num_side_inputs));\n      OP_REQUIRES(context, num_side_inputs >= 0 && num_side_inputs <= 1,\n                  errors::InvalidArgument(\n                      \"FusedBatchNorm accepts at most one side input.\"));\n      has_side_input_ = (num_side_inputs == 1);\n      if (has_side_input_ && is_training_) {\n        OP_REQUIRES(\n            context, activation_mode_ != FbnActivationMode::kIdentity,\n            errors::InvalidArgument(\"Identity activation is not supported with \"\n                                    \"non-empty side input\"));\n      }\n    }\n\n    if (activation_mode_ != FbnActivationMode::kIdentity && is_training_) {\n      // NOTE(ezhulenev): Following requirements are coming from implementation\n      // details of cudnnBatchNormalizationForwardTrainingEx used in training\n      // mode. In inference mode we call custom CUDA kernel that supports all\n      // data formats and data types.\n      OP_REQUIRES(context, DataTypeToEnum<T>::value == DT_HALF,\n                  errors::InvalidArgument(\"FusedBatchNorm with activation \"\n                                          \"supports only DT_HALF data type.\"));\n      OP_REQUIRES(context, tensor_format_ == FORMAT_NHWC,\n                  errors::InvalidArgument(\"FusedBatchNorm with activation \"\n                                          \"supports only NHWC tensor format.\"));\n      OP_REQUIRES(context, functor::BatchnormSpatialPersistentEnabled(),\n                  errors::InvalidArgument(\n                      \"FusedBatchNorm with activation must run with cuDNN \"\n                      \"spatial persistence mode enabled.\"));\n    }\n  }\n\n  // If use_reserved_space is true, we need to handle the 5th output (a reserved\n  // space) and a new cudnn batch norm will be called if the version > 7.4.2.\n  // If use_reserved_space is false, we don't have 5th output.\n  virtual void ComputeWithReservedSpace(OpKernelContext* context,\n                                        bool use_reserved_space) {\n    Tensor x = context->input(0);\n    const Tensor& scale = context->input(1);\n    const Tensor& offset = context->input(2);\n    const Tensor& estimated_mean = context->input(3);\n    const Tensor& estimated_variance = context->input(4);\n    const Tensor* side_input = has_side_input_ ? &context->input(5) : nullptr;\n\n    OP_REQUIRES(context, x.dims() == 4 || x.dims() == 5,\n                errors::InvalidArgument(\"input must be 4 or 5-dimensional\",\n                                        x.shape().DebugString()));\n    OP_REQUIRES(context, scale.dims() == 1,\n                errors::InvalidArgument(\"scale must be 1-dimensional\",\n                                        scale.shape().DebugString()));\n    OP_REQUIRES(context, offset.dims() == 1,\n                errors::InvalidArgument(\"offset must be 1-dimensional\",\n                                        offset.shape().DebugString()));\n    OP_REQUIRES(context, estimated_mean.dims() == 1,\n                errors::InvalidArgument(\"estimated_mean must be 1-dimensional\",\n                                        estimated_mean.shape().DebugString()));\n    OP_REQUIRES(\n        context, estimated_variance.dims() == 1,\n        errors::InvalidArgument(\"estimated_variance must be 1-dimensional\",\n                                estimated_variance.shape().DebugString()));\n    bool use_reshape = (x.dims() == 5);\n    auto x_shape = x.shape();\n    TensorShape dest_shape;\n    if (use_reshape) {\n      const int64_t in_batch = GetTensorDim(x, tensor_format_, 'N');\n      int64_t in_planes = GetTensorDim(x, tensor_format_, '0');\n      int64_t in_rows = GetTensorDim(x, tensor_format_, '1');\n      int64_t in_cols = GetTensorDim(x, tensor_format_, '2');\n      const int64_t in_depth = GetTensorDim(x, tensor_format_, 'C');\n      dest_shape = ShapeFromFormat(tensor_format_, in_batch,\n                                   {{in_planes, in_rows * in_cols}}, in_depth);\n      OP_REQUIRES(context, x.CopyFrom(x, dest_shape),\n                  errors::InvalidArgument(\"Error during tensor copy.\"));\n    }\n\n    const auto num_channels = GetTensorDim(x, tensor_format_, 'C');\n    OP_REQUIRES(\n        context, scale.NumElements() == num_channels,\n        errors::InvalidArgument(\"scale must have the same number of elements \"\n                                \"as the channels of x, got \",\n                                scale.NumElements(), \" and \", num_channels));\n    OP_REQUIRES(\n        context, offset.NumElements() == num_channels,\n        errors::InvalidArgument(\"offset must have the same number of elements \"\n                                \"as the channels of x, got \",\n                                offset.NumElements(), \" and \", num_channels));\n    if (!is_training_ || exponential_avg_factor_ != 1.) {\n      std::string prefix_msg = is_training_ ? \"When exponential_avg_factor != 1\"\n                                            : \"When is_training=false\";\n      OP_REQUIRES(context, estimated_mean.NumElements() == num_channels,\n                  errors::InvalidArgument(\n                      prefix_msg,\n                      \", mean must have the same number \"\n                      \"of elements as the channels of x, got \",\n                      estimated_mean.NumElements(), \" and \", num_channels));\n      OP_REQUIRES(context, estimated_variance.NumElements() == num_channels,\n                  errors::InvalidArgument(\n                      prefix_msg,\n                      \", variance must have the same \"\n                      \"number of elements as the channels of x, got \",\n                      estimated_variance.NumElements(), \" and \", num_channels));\n    }\n\n    if (has_side_input_) {\n      OP_REQUIRES(context, side_input->shape() == x.shape(),\n                  errors::InvalidArgument(\n                      \"side_input shape must be equal to input shape: \",\n                      side_input->shape().DebugString(),\n                      \" != \", x.shape().DebugString()));\n    }\n\n    if (activation_mode_ != FbnActivationMode::kIdentity) {\n      // NOTE(ezhulenev): This requirement is coming from implementation\n      // details of cudnnBatchNormalizationForwardTrainingEx.\n      OP_REQUIRES(\n          context, !is_training_ || num_channels % 4 == 0,\n          errors::InvalidArgument(\"FusedBatchNorm with activation requires \"\n                                  \"channel dimension to be a multiple of 4.\"));\n    }\n\n    Tensor* y = nullptr;\n    auto alloc_shape = use_reshape ? dest_shape : x_shape;\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                {0}, 0, alloc_shape, &y));\n\n    Tensor* batch_mean = nullptr;\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                {3}, 1, scale.shape(), &batch_mean));\n    Tensor* batch_var = nullptr;\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                {4}, 2, scale.shape(), &batch_var));\n    Tensor* saved_mean = nullptr;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(3, scale.shape(), &saved_mean));\n    Tensor* saved_maybe_inv_var = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(4, scale.shape(),\n                                                     &saved_maybe_inv_var));\n\n    if (is_training_) {\n      functor::FusedBatchNorm<Device, T, U, true>()(\n          context, x, scale, offset, estimated_mean, estimated_variance,\n          side_input, epsilon_, exponential_avg_factor_, activation_mode_, y,\n          batch_mean, batch_var, saved_mean, saved_maybe_inv_var,\n          tensor_format_, use_reserved_space);\n    } else {\n      functor::FusedBatchNorm<Device, T, U, false>()(\n          context, x, scale, offset, estimated_mean, estimated_variance,\n          side_input, epsilon_, exponential_avg_factor_, activation_mode_, y,\n          batch_mean, batch_var, saved_mean, saved_maybe_inv_var,\n          tensor_format_, use_reserved_space);\n    }\n    if (use_reshape) {\n      OP_REQUIRES(context, y->CopyFrom(*y, x_shape),\n                  errors::InvalidArgument(\"Error during tensor copy.\"));\n    }\n  }\n\n private:\n  U epsilon_;\n  U exponential_avg_factor_;\n  TensorFormat tensor_format_;\n  bool is_training_;\n  bool has_side_input_;\n  FbnActivationMode activation_mode_;\n};\n\ntemplate <typename Device, typename T, typename U>\nclass FusedBatchNormOp : public FusedBatchNormOpBase<Device, T, U> {\n public:\n  explicit FusedBatchNormOp(OpKernelConstruction* context)\n      : FusedBatchNormOpBase<Device, T, U>(context) {}\n\n  void Compute(OpKernelContext* context) override {\n    FusedBatchNormOpBase<Device, T, U>::ComputeWithReservedSpace(context,\n                                                                 false);\n  }\n};\n\ntemplate <typename Device, typename T, typename U>\nclass FusedBatchNormOpV3 : public FusedBatchNormOpBase<Device, T, U> {\n public:\n  explicit FusedBatchNormOpV3(OpKernelConstruction* context)\n      : FusedBatchNormOpBase<Device, T, U>(context) {}\n\n  void Compute(OpKernelContext* context) override {\n    FusedBatchNormOpBase<Device, T, U>::ComputeWithReservedSpace(context, true);\n  }\n};\n\ntemplate <typename Device, typename T, typename U>\nclass FusedBatchNormOpEx : public FusedBatchNormOpBase<Device, T, U> {\n  static constexpr bool kWithSideInputAndActivation = true;\n\n public:\n  explicit FusedBatchNormOpEx(OpKernelConstruction* context)\n      : FusedBatchNormOpBase<Device, T, U>(context,\n                                           kWithSideInputAndActivation) {}\n\n  void Compute(OpKernelContext* context) override {\n    FusedBatchNormOpBase<Device, T, U>::ComputeWithReservedSpace(context, true);\n  }\n};\n\ntemplate <typename Device, typename T, typename U>\nclass FusedBatchNormGradOpBase : public OpKernel {\n  using FbnActivationMode = functor::FusedBatchNormActivationMode;\n\n protected:\n  explicit FusedBatchNormGradOpBase(OpKernelConstruction* context,\n                                    bool is_batch_norm_grad_ex = false)\n      : OpKernel(context) {\n    float epsilon;\n    OP_REQUIRES_OK(context, context->GetAttr(\"epsilon\", &epsilon));\n    epsilon_ = U(epsilon);\n    string tensor_format;\n    OP_REQUIRES_OK(context, context->GetAttr(\"data_format\", &tensor_format));\n    OP_REQUIRES(context, FormatFromString(tensor_format, &tensor_format_),\n                errors::InvalidArgument(\"Invalid data format\"));\n    OP_REQUIRES_OK(context, context->GetAttr(\"is_training\", &is_training_));\n    if (!is_batch_norm_grad_ex) {\n      has_side_input_ = false;\n      activation_mode_ = FbnActivationMode::kIdentity;\n    } else {\n      OP_REQUIRES_OK(context, ParseActivationMode(context, &activation_mode_));\n\n      int num_side_inputs;\n      OP_REQUIRES_OK(context,\n                     context->GetAttr(\"num_side_inputs\", &num_side_inputs));\n      OP_REQUIRES(context, num_side_inputs >= 0 && num_side_inputs <= 1,\n                  errors::InvalidArgument(\n                      \"FusedBatchNormGrad accepts at most one side input.\"));\n      has_side_input_ = (num_side_inputs == 1);\n      if (has_side_input_ && is_training_) {\n        OP_REQUIRES(\n            context, activation_mode_ != FbnActivationMode::kIdentity,\n            errors::InvalidArgument(\"Identity activation is not supported with \"\n                                    \"non-empty side input\"));\n      }\n    }\n\n    if (activation_mode_ != FbnActivationMode::kIdentity && is_training_) {\n      // NOTE(kaixih@nvidia): Following requirements are coming from\n      // implementation details of cudnnBatchNormalizationBackwardEx used in\n      // training mode.\n      OP_REQUIRES(context, DataTypeToEnum<T>::value == DT_HALF,\n                  errors::InvalidArgument(\"FusedBatchNormGrad with activation \"\n                                          \"supports only DT_HALF data type.\"));\n      OP_REQUIRES(context, tensor_format_ == FORMAT_NHWC,\n                  errors::InvalidArgument(\"FusedBatchNormGrad with activation \"\n                                          \"supports only NHWC tensor format.\"));\n      OP_REQUIRES(context, functor::BatchnormSpatialPersistentEnabled(),\n                  errors::InvalidArgument(\n                      \"FusedBatchNormGrad with activation must run with cuDNN \"\n                      \"spatial persistence mode enabled.\"));\n    }\n  }\n\n  virtual void ComputeWithReservedSpace(OpKernelContext* context,\n                                        bool use_reserved_space) {\n    Tensor y_backprop = context->input(0);\n    Tensor x = context->input(1);\n    const Tensor& scale = context->input(2);\n    // When is_training=True, batch mean and variance/inverted variance are\n    // saved in the forward pass to be reused here. When is_training=False,\n    // population mean and variance need to be forwarded here to compute the\n    // gradients.\n    const Tensor& saved_mean_or_pop_mean = context->input(3);\n    // The Eigen implementation saves variance in the forward pass, while cuDNN\n    // saves inverted variance.\n    const Tensor& saved_maybe_inv_var_or_pop_var = context->input(4);\n    bool use_activation = activation_mode_ != FbnActivationMode::kIdentity;\n    const Tensor* offset = use_activation ? &context->input(6) : nullptr;\n    const Tensor* y = use_activation ? &context->input(7) : nullptr;\n\n    OP_REQUIRES(context, y_backprop.dims() == 4 || y_backprop.dims() == 5,\n                errors::InvalidArgument(\"input must be 4 or 5-dimensional\",\n                                        y_backprop.shape().DebugString()));\n    OP_REQUIRES(context, x.dims() == 4 || x.dims() == 5,\n                errors::InvalidArgument(\"input must be 4 or 5-dimensional\",\n                                        x.shape().DebugString()));\n    OP_REQUIRES(context, scale.dims() == 1,\n                errors::InvalidArgument(\"scale must be 1-dimensional\",\n                                        scale.shape().DebugString()));\n    OP_REQUIRES(\n        context, saved_mean_or_pop_mean.dims() == 1,\n        errors::InvalidArgument(\"saved mean must be 1-dimensional\",\n                                saved_mean_or_pop_mean.shape().DebugString()));\n    OP_REQUIRES(context, saved_maybe_inv_var_or_pop_var.dims() == 1,\n                errors::InvalidArgument(\n                    \"saved variance must be 1-dimensional\",\n                    saved_maybe_inv_var_or_pop_var.shape().DebugString()));\n    OP_REQUIRES(\n        context, x.shape() == y_backprop.shape(),\n        errors::InvalidArgument(\n            \"x and y_backprop must have same shape, but x has shape \",\n            x.shape(), \" and y_backprop has shape \", y_backprop.shape()));\n    if (use_activation) {\n      OP_REQUIRES(\n          context, x.dim_size(3) % 4 == 0,\n          errors::InvalidArgument(\"FusedBatchNormGrad with activation requires \"\n                                  \"channel dimension to be a multiple of 4.\"));\n      OP_REQUIRES(context, offset->dims() == 1,\n                  errors::InvalidArgument(\"offset must be 1-dimensional\",\n                                          offset->shape().DebugString()));\n    }\n    bool use_reshape = (x.dims() == 5);\n    auto x_shape = x.shape();\n    TensorShape dest_shape;\n    if (use_reshape) {\n      const int64_t in_batch = GetTensorDim(x, tensor_format_, 'N');\n      int64_t in_planes = GetTensorDim(x, tensor_format_, '0');\n      int64_t in_rows = GetTensorDim(x, tensor_format_, '1');\n      int64_t in_cols = GetTensorDim(x, tensor_format_, '2');\n      const int64_t in_depth = GetTensorDim(x, tensor_format_, 'C');\n      dest_shape = ShapeFromFormat(tensor_format_, in_batch,\n                                   {{in_planes, in_rows * in_cols}}, in_depth);\n      OP_REQUIRES(context, x.CopyFrom(x, dest_shape),\n                  errors::InvalidArgument(\"Error during tensor copy.\"));\n      OP_REQUIRES(context, y_backprop.CopyFrom(y_backprop, dest_shape),\n                  errors::InvalidArgument(\"Error during tensor copy.\"));\n    }\n\n    const auto num_channels = GetTensorDim(x, tensor_format_, 'C');\n    OP_REQUIRES(\n        context, scale.NumElements() == num_channels,\n        errors::InvalidArgument(\"scale must have the same number of elements \"\n                                \"as the channels of x, got \",\n                                scale.NumElements(), \" and \", num_channels));\n    OP_REQUIRES(\n        context, saved_mean_or_pop_mean.NumElements() == num_channels,\n        errors::InvalidArgument(\"reserve_space_1 must have the same number of \"\n                                \"elements as the channels of x, got \",\n                                scale.NumElements(), \" and \", num_channels));\n    OP_REQUIRES(\n        context, saved_maybe_inv_var_or_pop_var.NumElements() == num_channels,\n        errors::InvalidArgument(\"reserve_space_2 must have the same number of \"\n                                \"elements as the channels of x, got \",\n                                scale.NumElements(), \" and \", num_channels));\n\n    Tensor* x_backprop = nullptr;\n    auto alloc_shape = use_reshape ? dest_shape : x_shape;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(0, alloc_shape, &x_backprop));\n\n    const TensorShape& scale_offset_shape = scale.shape();\n    Tensor* scale_backprop = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(1, scale_offset_shape,\n                                                     &scale_backprop));\n    Tensor* offset_backprop = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(2, scale_offset_shape,\n                                                     &offset_backprop));\n    // Two placeholders for estimated_mean and estimated_variance, which are\n    // used for inference and thus not needed here for gradient computation.\n    // They are filled with zeros so as to avoid NaN outputs.\n    Tensor* placeholder_1 = nullptr;\n    OP_REQUIRES_OK(\n        context, context->allocate_output(3, TensorShape({0}), &placeholder_1));\n    Tensor* placeholder_2 = nullptr;\n    OP_REQUIRES_OK(\n        context, context->allocate_output(4, TensorShape({0}), &placeholder_2));\n\n    Tensor* side_input_backprop = nullptr;\n    if (has_side_input_) {\n      OP_REQUIRES_OK(context, context->allocate_output(5, alloc_shape,\n                                                       &side_input_backprop));\n    }\n\n    // If input is empty, set gradients w.r.t scale/offset to zero.\n    if (x.shape().num_elements() == 0) {\n      functor::SetZeroFunctor<Device, U> f;\n      f(context->eigen_device<Device>(), scale_backprop->flat<U>());\n      f(context->eigen_device<Device>(), offset_backprop->flat<U>());\n      return;\n    }\n\n    if (is_training_) {\n      functor::FusedBatchNormGrad<Device, T, U>()(\n          context, y_backprop, x, scale, offset, saved_mean_or_pop_mean,\n          saved_maybe_inv_var_or_pop_var, y, epsilon_, activation_mode_,\n          x_backprop, scale_backprop, offset_backprop, side_input_backprop,\n          use_reserved_space, tensor_format_);\n    } else {\n      OP_REQUIRES(\n          context,\n          activation_mode_ == FbnActivationMode::kIdentity && !has_side_input_,\n          errors::InvalidArgument(\n              \"FusedBatchNormGrad with activation is only supported \"\n              \"when is_training=True.\"));\n      // Necessary layout conversion is currently done in python.\n      OP_REQUIRES(context, tensor_format_ == FORMAT_NHWC,\n                  errors::InvalidArgument(\n                      \"The implementation of \"\n                      \"FusedBatchNormGrad with is_training=False only support \"\n                      \"NHWC tensor format for now.\"));\n      functor::FusedBatchNormFreezeGrad<Device, T, U>()(\n          context, y_backprop, x, scale, saved_mean_or_pop_mean,\n          saved_maybe_inv_var_or_pop_var, epsilon_, x_backprop, scale_backprop,\n          offset_backprop);\n    }\n    if (use_reshape) {\n      OP_REQUIRES(context, x_backprop->CopyFrom(*x_backprop, x_shape),\n                  errors::InvalidArgument(\"Error during tensor copy.\"));\n    }\n  }\n\n private:\n  U epsilon_;\n  TensorFormat tensor_format_;\n  bool is_training_;\n  bool has_side_input_;\n  FbnActivationMode activation_mode_;\n};\n\ntemplate <typename Device, typename T, typename U>\nclass FusedBatchNormGradOp : public FusedBatchNormGradOpBase<Device, T, U> {\n public:\n  explicit FusedBatchNormGradOp(OpKernelConstruction* context)\n      : FusedBatchNormGradOpBase<Device, T, U>(context) {}\n\n  void Compute(OpKernelContext* context) override {\n    FusedBatchNormGradOpBase<Device, T, U>::ComputeWithReservedSpace(context,\n                                                                     false);\n  }\n};\n\ntemplate <typename Device, typename T, typename U>\nclass FusedBatchNormGradOpV3 : public FusedBatchNormGradOpBase<Device, T, U> {\n public:\n  explicit FusedBatchNormGradOpV3(OpKernelConstruction* context)\n      : FusedBatchNormGradOpBase<Device, T, U>(context) {}\n\n  void Compute(OpKernelContext* context) override {\n    FusedBatchNormGradOpBase<Device, T, U>::ComputeWithReservedSpace(context,\n                                                                     true);\n  }\n};\n\ntemplate <typename Device, typename T, typename U>\nclass FusedBatchNormGradOpEx : public FusedBatchNormGradOpBase<Device, T, U> {\n  static constexpr bool kWithSideInputAndActivation = true;\n\n public:\n  explicit FusedBatchNormGradOpEx(OpKernelConstruction* context)\n      : FusedBatchNormGradOpBase<Device, T, U>(context,\n                                               kWithSideInputAndActivation) {}\n\n  void Compute(OpKernelContext* context) override {\n    FusedBatchNormGradOpBase<Device, T, U>::ComputeWithReservedSpace(context,\n                                                                     true);\n  }\n};\n\nREGISTER_KERNEL_BUILDER(\n    Name(\"FusedBatchNorm\").Device(DEVICE_CPU).TypeConstraint<float>(\"T\"),\n    FusedBatchNormOp<CPUDevice, float, float>);\n\nREGISTER_KERNEL_BUILDER(\n    Name(\"FusedBatchNormGrad\").Device(DEVICE_CPU).TypeConstraint<float>(\"T\"),\n    FusedBatchNormGradOp<CPUDevice, float, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"FusedBatchNormV2\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<float>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormOp<CPUDevice, float, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"FusedBatchNormGradV2\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<float>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormGradOp<CPUDevice, float, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"FusedBatchNormV2\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<Eigen::half>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormOp<CPUDevice, Eigen::half, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"FusedBatchNormGradV2\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<Eigen::half>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormGradOp<CPUDevice, Eigen::half, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"FusedBatchNormV3\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<float>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormOpV3<CPUDevice, float, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"FusedBatchNormGradV3\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<float>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormGradOpV3<CPUDevice, float, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"FusedBatchNormV3\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<Eigen::half>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormOpV3<CPUDevice, Eigen::half, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"FusedBatchNormGradV3\")\n                            .Device(DEVICE_CPU)\n                            .TypeConstraint<Eigen::half>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormGradOpV3<CPUDevice, Eigen::half, float>);\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\nREGISTER_KERNEL_BUILDER(\n    Name(\"FusedBatchNorm\").Device(DEVICE_GPU).TypeConstraint<float>(\"T\"),\n    FusedBatchNormOp<GPUDevice, float, float>);\n\nREGISTER_KERNEL_BUILDER(\n    Name(\"FusedBatchNormGrad\").Device(DEVICE_GPU).TypeConstraint<float>(\"T\"),\n    FusedBatchNormGradOp<GPUDevice, float, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"FusedBatchNormV2\")\n                            .Device(DEVICE_GPU)\n                            .TypeConstraint<float>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormOp<GPUDevice, float, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"FusedBatchNormGradV2\")\n                            .Device(DEVICE_GPU)\n                            .TypeConstraint<float>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormGradOp<GPUDevice, float, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"FusedBatchNormV2\")\n                            .Device(DEVICE_GPU)\n                            .TypeConstraint<Eigen::half>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormOp<GPUDevice, Eigen::half, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"FusedBatchNormGradV2\")\n                            .Device(DEVICE_GPU)\n                            .TypeConstraint<Eigen::half>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormGradOp<GPUDevice, Eigen::half, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"FusedBatchNormV3\")\n                            .Device(DEVICE_GPU)\n                            .TypeConstraint<float>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormOpV3<GPUDevice, float, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"_FusedBatchNormEx\")\n                            .Device(DEVICE_GPU)\n                            .TypeConstraint<float>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormOpEx<GPUDevice, float, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"FusedBatchNormGradV3\")\n                            .Device(DEVICE_GPU)\n                            .TypeConstraint<float>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormGradOpV3<GPUDevice, float, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"_FusedBatchNormGradEx\")\n                            .Device(DEVICE_GPU)\n                            .TypeConstraint<float>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormGradOpEx<GPUDevice, float, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"FusedBatchNormV3\")\n                            .Device(DEVICE_GPU)\n                            .TypeConstraint<Eigen::half>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormOpV3<GPUDevice, Eigen::half, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"_FusedBatchNormEx\")\n                            .Device(DEVICE_GPU)\n                            .TypeConstraint<Eigen::half>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormOpEx<GPUDevice, Eigen::half, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"FusedBatchNormGradV3\")\n                            .Device(DEVICE_GPU)\n                            .TypeConstraint<Eigen::half>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormGradOpV3<GPUDevice, Eigen::half, float>);\n\nREGISTER_KERNEL_BUILDER(Name(\"_FusedBatchNormGradEx\")\n                            .Device(DEVICE_GPU)\n                            .TypeConstraint<Eigen::half>(\"T\")\n                            .TypeConstraint<float>(\"U\"),\n                        FusedBatchNormGradOpEx<GPUDevice, Eigen::half, float>);\n\n#endif\n\n}  // namespace tensorflow\n", "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for fused_batch_norm related functionality in tensorflow.ops.nn.\"\"\"\n\nimport numpy as np\n\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors_impl\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import gen_nn_ops\nfrom tensorflow.python.ops import gradient_checker\nfrom tensorflow.python.ops import gradients_impl\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import nn_grad\nfrom tensorflow.python.ops import nn_impl\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.platform import test\n\n\nclass BatchNormalizationTest(test.TestCase):\n\n  def _batch_norm(self, x, mean, var, offset, scale, epsilon):\n    # We compute the batch norm manually in this function because\n    # nn_impl.batch_normalization does not support float16 yet.\n    # TODO(reedwm): Add float16 support to nn_impl.batch_normalization.\n    inv = math_ops.rsqrt(var + epsilon) * scale\n    y = math_ops.cast(x, scale.dtype) * inv + (offset - mean * inv)\n    return math_ops.cast(y, x.dtype)\n\n  def _inference_ref(self, x, scale, offset, mean, var, epsilon, data_format):\n    if data_format not in ['NHWC', 'NCHW', 'NDHWC', 'NCDHW']:\n      raise ValueError('data_format must be NCHW or NHWC for 4D tensors or'\n                       'NCDHW or NDHWC for 5D tensors, got %s.' % data_format)\n    if data_format == 'NCHW':\n      x = array_ops.transpose(x, [0, 2, 3, 1])\n    elif data_format == 'NCDHW':\n      x = array_ops.transpose(x, [0, 2, 3, 4, 1])\n    y = self._batch_norm(x, mean, var, offset, scale, epsilon)\n    if data_format == 'NCHW':\n      y = array_ops.transpose(y, [0, 3, 1, 2])\n    elif data_format == 'NCDHW':\n      y = array_ops.transpose(y, [0, 4, 1, 2, 3])\n    return self.evaluate(y)\n\n  def _test_inference(self,\n                      x_shape,\n                      x_dtype,\n                      scale_shape,\n                      scale_dtype,\n                      use_gpu=True,\n                      exponential_avg_factor=1.0,\n                      data_format='NHWC'):\n    np.random.seed(1)\n    x_val = np.random.random_sample(x_shape).astype(x_dtype)\n    scale_val = np.random.random_sample(scale_shape).astype(scale_dtype)\n    offset_val = np.random.random_sample(scale_shape).astype(scale_dtype)\n    mean_val = np.random.random_sample(scale_shape).astype(scale_dtype)\n    var_val = np.random.random_sample(scale_shape).astype(scale_dtype)\n\n    with self.cached_session(use_gpu=use_gpu) as sess:\n      x = constant_op.constant(x_val, name='x')\n      scale = constant_op.constant(scale_val, name='scale')\n      offset = constant_op.constant(offset_val, name='offset')\n      mean = constant_op.constant(mean_val, name='mean')\n      var = constant_op.constant(var_val, name='variance')\n      epsilon = 0.001\n      y, _, _ = nn_impl.fused_batch_norm(\n          x,\n          scale,\n          offset,\n          mean=mean,\n          variance=var,\n          epsilon=epsilon,\n          exponential_avg_factor=exponential_avg_factor,\n          data_format=data_format,\n          is_training=False)\n      y_val = self.evaluate(y)\n      y_ref = self._inference_ref(x, scale, offset, mean, var, epsilon,\n                                  data_format)\n    # An atol value of 1e-3 is too small for float16's, because some adjacent\n    # float16 values that y_val can take are greater than 1e-3 apart, e.g.\n    # 2.16602 and 2.16797.\n    atol = 2e-3 if x_dtype == np.float16 else 1e-3\n    self.assertAllClose(y_ref, y_val, atol=atol)\n\n  def _running_mean(self, old_mean, new_val, factor):\n    if factor == 1.0:\n      return new_val\n    else:\n      return (1.0 - factor) * old_mean + factor * new_val\n\n  def _training_ref(self, x, scale, offset, old_mean, old_var,\n                    exponential_avg_factor, epsilon, data_format):\n    if data_format not in ['NHWC', 'NCHW', 'NDHWC', 'NCDHW']:\n      raise ValueError('data_format must be NCHW or NHWC for 4D tensors or'\n                       'NCDHW or NDHWC for 5D tensors, got %s.' % data_format)\n    use_4d_tensor = (x.shape.ndims == 4)\n    if data_format == 'NCHW':\n      x = array_ops.transpose(x, [0, 2, 3, 1])\n    elif data_format == 'NCDHW':\n      x = array_ops.transpose(x, [0, 2, 3, 4, 1])\n\n    mean_axis = [0, 1, 2] if use_4d_tensor else [0, 1, 2, 3]\n    batch_mean, batch_var = nn_impl.moments(\n        math_ops.cast(x, scale.dtype), mean_axis, keep_dims=False)\n\n    y = self._batch_norm(x, batch_mean, batch_var, offset, scale, epsilon)\n    if data_format == 'NCHW':\n      y = array_ops.transpose(y, [0, 3, 1, 2])\n    elif data_format == 'NCDHW':\n      y = array_ops.transpose(y, [0, 4, 1, 2, 3])\n\n    # This is for Bessel's correction. tf.nn.moments uses n, instead of n-1, as\n    # the denominator in the formula to calculate variance, while\n    # tf.compat.v1.nn.fused_batch_norm has Bessel's correction built in.\n    sample_size = math_ops.cast(\n        array_ops.size(x) / array_ops.size(scale), scale.dtype)\n    batch_var_corrected = batch_var * sample_size / (\n        math_ops.maximum(sample_size - 1.0, 1.0))\n\n    mean = self._running_mean(old_mean, batch_mean, exponential_avg_factor)\n    var = self._running_mean(old_var, batch_var_corrected,\n                             exponential_avg_factor)\n    return self.evaluate(y), self.evaluate(mean), self.evaluate(var)\n\n  def _test_training(self,\n                     x_shape,\n                     x_dtype,\n                     scale_shape,\n                     scale_dtype,\n                     use_gpu=True,\n                     exponential_avg_factor=1.0,\n                     data_format='NHWC'):\n    np.random.seed(1)\n    x_val = np.random.random_sample(x_shape).astype(x_dtype)\n    scale_val = np.random.random_sample(scale_shape).astype(scale_dtype)\n    offset_val = np.random.random_sample(scale_shape).astype(scale_dtype)\n    if exponential_avg_factor == 1.0:\n      old_mean_val = None\n      old_var_val = None\n    else:\n      old_mean_val = np.random.random_sample(scale_shape).astype(scale_dtype)\n      old_var_val = np.random.random_sample(scale_shape).astype(scale_dtype)\n\n    with self.cached_session(use_gpu=use_gpu) as sess:\n      x = constant_op.constant(x_val, name='x')\n      scale = constant_op.constant(scale_val, name='scale')\n      offset = constant_op.constant(offset_val, name='offset')\n      epsilon = 0.001\n      y, mean, var = nn_impl.fused_batch_norm(\n          x,\n          scale,\n          offset,\n          mean=old_mean_val,\n          variance=old_var_val,\n          epsilon=epsilon,\n          exponential_avg_factor=exponential_avg_factor,\n          data_format=data_format,\n          is_training=True)\n      y_val, mean_val, var_val = self.evaluate([y, mean, var])\n      y_ref, mean_ref, var_ref = self._training_ref(x, scale, offset,\n                                                    old_mean_val, old_var_val,\n                                                    exponential_avg_factor,\n                                                    epsilon, data_format)\n    y_atol = 2e-3 if x_dtype == np.float16 else 1e-3\n    self.assertAllClose(y_ref, y_val, atol=y_atol)\n    self.assertAllClose(mean_ref, mean_val, atol=1e-3)\n    self.assertAllClose(var_ref, var_val, atol=1e-3)\n\n  def _compute_gradient_error_float16(self, x, x32, x_shape, y, y32, y_shape):\n    \"\"\"Computes the gradient error for float16 inputs and/or outputs.\n\n    This returns the same value as gradient_checker.compute_gradient_error. The\n    difference is that gradient_checker.compute_gradient_error does not\n    numerically compute the gradients in a numerically stable way for float16\n    tensors. To fix this, this function requires float32 versions of x and y to\n    numerically compute the gradients, to compare with the float16 symbolically\n    computed gradients.\n\n    Args:\n      x: The input tensor.\n      x32: A float32 version of x.\n      x_shape: The shape of x.\n      y: The output tensor.\n      y32: A float32 version of y. Must be calculated based on x32, not x.\n      y_shape: The shape of y.\n\n    Returns:\n      The maximum error in between the two Jacobians, as in\n      gradient_checker.compute_gradient_error.\n    \"\"\"\n    x_init_val = np.random.random_sample(x_shape).astype(np.float16)\n    x32_init_val = x_init_val.astype(np.float32)\n\n    # TODO(reedwm): Do not perform the unnecessary computations in\n    # compute_gradient, since they double the computation time of this function.\n    theoretical_grad, _ = gradient_checker.compute_gradient(\n        x, x_shape, y, y_shape, delta=1e-3, x_init_value=x_init_val)\n    _, numerical_grad = gradient_checker.compute_gradient(\n        x32, x_shape, y32, y_shape, delta=1e-3, x_init_value=x32_init_val)\n\n    # If grad is empty, no error.\n    if theoretical_grad.size == 0 and numerical_grad.size == 0:\n      return 0\n    return np.fabs(theoretical_grad - numerical_grad).max()\n\n  def _test_gradient(self,\n                     x_shape,\n                     x_dtype,\n                     scale_shape,\n                     scale_dtype,\n                     use_gpu=True,\n                     exponential_avg_factor=1.0,\n                     data_format='NHWC',\n                     is_training=True):\n    np.random.seed(1)\n    x_val = np.random.random_sample(x_shape).astype(x_dtype)\n    scale_val = np.random.random_sample(scale_shape).astype(scale_dtype)\n    offset_val = np.random.random_sample(scale_shape).astype(scale_dtype)\n\n    with self.cached_session(use_gpu=use_gpu):\n      x = constant_op.constant(x_val, name='x')\n      scale = constant_op.constant(scale_val, name='scale')\n      offset = constant_op.constant(offset_val, name='offset')\n      if is_training and exponential_avg_factor == 1.0:\n        pop_mean = None\n        pop_var = None\n      else:\n        pop_mean = np.random.random_sample(scale_shape).astype(scale_dtype)\n        pop_var = np.random.random_sample(scale_shape).astype(scale_dtype)\n      y, _, _ = nn_impl.fused_batch_norm(\n          x,\n          scale,\n          offset,\n          mean=pop_mean,\n          variance=pop_var,\n          exponential_avg_factor=exponential_avg_factor,\n          data_format=data_format,\n          is_training=is_training)\n      if x_dtype != np.float16:\n        err_x = gradient_checker.compute_gradient_error(x, x_shape, y, x_shape)\n        err_scale = gradient_checker.compute_gradient_error(\n            scale, scale_shape, y, x_shape)\n        err_offset = gradient_checker.compute_gradient_error(\n            offset, scale_shape, y, x_shape)\n      else:\n        x32 = constant_op.constant(x_val, name='x32', dtype=dtypes.float32)\n        y32, _, _ = nn_impl.fused_batch_norm(\n            x32,\n            scale,\n            offset,\n            mean=pop_mean,\n            variance=pop_var,\n            data_format=data_format,\n            exponential_avg_factor=exponential_avg_factor,\n            is_training=is_training)\n        err_x = self._compute_gradient_error_float16(x, x32, x_shape, y, y32,\n                                                     x_shape)\n        err_scale = self._compute_gradient_error_float16(\n            scale, scale, scale_shape, y, y32, x_shape)\n        err_offset = self._compute_gradient_error_float16(\n            offset, offset, scale_shape, y, y32, x_shape)\n\n    x_err_tolerance = 2e-3 if x_dtype == np.float16 else 1e-3\n    scale_err_tolerance = 1e-3\n    self.assertLess(err_x, x_err_tolerance)\n    self.assertLess(err_scale, scale_err_tolerance)\n    self.assertLess(err_offset, scale_err_tolerance)\n\n  def _test_grad_grad(self,\n                      x_shape,\n                      x_dtype,\n                      scale_shape,\n                      scale_dtype,\n                      use_gpu=True,\n                      exponential_avg_factor=1.0,\n                      data_format='NHWC',\n                      is_training=True,\n                      err_tolerance=1e-3):\n    np.random.seed(1)\n    x_val = np.random.random_sample(x_shape).astype(x_dtype)\n    grad_y_val = np.random.random_sample(x_shape).astype(x_dtype)\n    scale_val = np.random.random_sample(scale_shape).astype(scale_dtype)\n    offset_val = np.random.random_sample(scale_shape).astype(scale_dtype)\n\n    with self.cached_session(use_gpu=use_gpu) as sess:\n      x = constant_op.constant(x_val, name='x')\n      grad_y = constant_op.constant(grad_y_val, name='grad_y')\n      scale = constant_op.constant(scale_val, name='scale')\n      offset = constant_op.constant(offset_val, name='offset')\n      if is_training and exponential_avg_factor == 1.0:\n        pop_mean = None\n        pop_var = None\n      else:\n        pop_mean = np.random.random_sample(scale_shape).astype(scale_dtype)\n        pop_var = np.random.random_sample(scale_shape).astype(scale_dtype)\n      y, _, _ = nn_impl.fused_batch_norm(\n          x,\n          scale,\n          offset,\n          mean=pop_mean,\n          variance=pop_var,\n          exponential_avg_factor=exponential_avg_factor,\n          data_format=data_format,\n          is_training=is_training)\n      grad_x, grad_scale, grad_offset = gradients_impl.gradients(\n          y, [x, scale, offset], grad_y)\n\n      if is_training:\n        epsilon = y.op.get_attr('epsilon')\n        data_format = y.op.get_attr('data_format')\n        grad_vals = self.evaluate([grad_x, grad_scale, grad_offset])\n        grad_internal = nn_grad._BatchNormGrad(grad_y, x, scale, pop_mean,\n                                               pop_var, epsilon, data_format)\n        grad_internal_vals = self.evaluate(list(grad_internal))\n        for grad_val, grad_internal_val in zip(grad_vals, grad_internal_vals):\n          self.assertAllClose(grad_val, grad_internal_val, atol=err_tolerance)\n\n      if x_dtype != np.float16:\n        err_grad_grad_y_1 = gradient_checker.compute_gradient_error(\n            grad_y, x_shape, grad_x, x_shape)\n        err_grad_grad_y_2 = gradient_checker.compute_gradient_error(\n            grad_y, x_shape, grad_scale, scale_shape)\n        err_grad_grad_y_3 = gradient_checker.compute_gradient_error(\n            grad_y, x_shape, grad_offset, scale_shape)\n        # In freeze mode, grad_x is not a function of x.\n        if is_training:\n          err_grad_x_1 = gradient_checker.compute_gradient_error(\n              x, x_shape, grad_x, x_shape)\n        err_grad_x_2 = gradient_checker.compute_gradient_error(\n            x, x_shape, grad_scale, scale_shape)\n\n        err_grad_scale = gradient_checker.compute_gradient_error(\n            scale, scale_shape, grad_x, x_shape)\n      else:\n        x32 = constant_op.constant(x_val, dtype=dtypes.float32, name='x32')\n        grad_y32 = constant_op.constant(\n            grad_y_val, dtype=dtypes.float32, name='grad_y32')\n        y32, _, _ = nn_impl.fused_batch_norm(\n            x32,\n            scale,\n            offset,\n            mean=pop_mean,\n            variance=pop_var,\n            exponential_avg_factor=exponential_avg_factor,\n            data_format=data_format,\n            is_training=is_training)\n        grad_x32, grad_scale32, grad_offset32 = gradients_impl.gradients(\n            y32, [x32, scale, offset], grad_y32)\n        err_grad_grad_y_1 = self._compute_gradient_error_float16(\n            grad_y, grad_y32, x_shape, grad_x, grad_x32, x_shape)\n        err_grad_grad_y_2 = self._compute_gradient_error_float16(\n            grad_y, grad_y32, x_shape, grad_scale, grad_scale32, scale_shape)\n        err_grad_grad_y_3 = self._compute_gradient_error_float16(\n            grad_y, grad_y32, x_shape, grad_offset, grad_offset32, scale_shape)\n        # In freeze mode, grad_x is not a function of x.\n        if is_training:\n          err_grad_x_1 = self._compute_gradient_error_float16(\n              x, x32, x_shape, grad_x, grad_x32, x_shape)\n        err_grad_x_2 = self._compute_gradient_error_float16(\n            x, x32, x_shape, grad_scale, grad_scale32, scale_shape)\n\n        err_grad_scale = self._compute_gradient_error_float16(\n            scale, scale, scale_shape, grad_x, grad_x32, x_shape)\n\n    self.assertLess(err_grad_grad_y_1, err_tolerance)\n    self.assertLess(err_grad_grad_y_2, err_tolerance)\n    self.assertLess(err_grad_grad_y_3, err_tolerance)\n    if is_training:\n      self.assertLess(err_grad_x_1, err_tolerance)\n    self.assertLess(err_grad_x_2, err_tolerance)\n    self.assertLess(err_grad_scale, err_tolerance)\n\n  def _runtests(self, x_shape, is_training, gradient_test=False,\n                cpu_only=False):\n    if len(x_shape) == 4:\n      data_format_list = ['NHWC', 'NCHW']\n    else:\n      data_format_list = ['NCDHW', 'NDHWC']\n    use_gpu_vals = [False]\n    if test.is_gpu_available(cuda_only=True) and not cpu_only:\n      use_gpu_vals += [True]\n    factors = [1.0, 0.6]\n    for dtype in [np.float16, np.float32]:\n      for use_gpu in use_gpu_vals:\n        for data_format in data_format_list:\n          if data_format == 'NHWC' or data_format == 'NDHWC':\n            scale_shape = x_shape[-1:]\n          else:\n            scale_shape = x_shape[1:2]\n          for exponential_avg_factor in factors:\n            if gradient_test:\n              self._test_gradient(\n                  x_shape,\n                  dtype,\n                  scale_shape,\n                  np.float32,\n                  use_gpu=use_gpu,\n                  data_format=data_format,\n                  is_training=is_training,\n                  exponential_avg_factor=exponential_avg_factor)\n            else:\n              if is_training:\n                self._test_training(\n                    x_shape,\n                    dtype,\n                    scale_shape,\n                    np.float32,\n                    use_gpu=use_gpu,\n                    data_format=data_format,\n                    exponential_avg_factor=exponential_avg_factor)\n              else:\n                self._test_inference(\n                    x_shape,\n                    dtype,\n                    scale_shape,\n                    np.float32,\n                    use_gpu=use_gpu,\n                    data_format=data_format,\n                    exponential_avg_factor=exponential_avg_factor)\n\n  def testInferenceShape1(self):\n    x_shape = [1, 1, 6, 1]\n    self._runtests(x_shape, False)\n\n  def testInferenceShape2(self):\n    x_shape = [1, 1, 6, 2]\n    self._runtests(x_shape, False)\n\n  def testInferenceShape3(self):\n    x_shape = [1, 2, 1, 6]\n    self._runtests(x_shape, False)\n\n  def testInferenceShape4(self):\n    x_shape = [27, 131, 127, 6]\n    self._runtests(x_shape, False)\n\n  def testInferenceShape5(self):\n    x_shape = [0, 131, 127, 6]\n    self._runtests(x_shape, False)\n\n  def testInferenceShape6(self):\n    x_shape = [1, 1, 1, 1]\n    # GPU kernel doesn't properly handle case where non-channel dimensions are 1\n    self._runtests(x_shape, False, cpu_only=True)\n\n  def testInferenceShape7(self):\n    x_shape = [1, 2, 6, 1, 3]\n    self._runtests(x_shape, False)\n\n  def testTrainingShape1(self):\n    x_shape = [1, 1, 6, 1]\n    self._runtests(x_shape, True)\n\n  def testTrainingShape2(self):\n    x_shape = [1, 1, 6, 2]\n    self._runtests(x_shape, True)\n\n  def testTrainingShape3(self):\n    x_shape = [1, 2, 1, 6]\n    self._runtests(x_shape, True)\n\n  def testTrainingShape4(self):\n    x_shape = [27, 131, 127, 6]\n    self._runtests(x_shape, True)\n\n  @test_util.disable_xla('b/141236973: Empty inputs wrong on CPU.')\n  def testTrainingShape5(self):\n    x_shape = [0, 131, 127, 6]\n    self._runtests(x_shape, True)\n\n  @test_util.run_deprecated_v1\n  def testTrainingShape6(self):\n    x_shape = [1, 1, 1, 1]\n    # GPU kernel doesn't properly handle case where non-channel dimensions are 1\n    self._runtests(x_shape, True, cpu_only=True)\n\n  def testTrainingShape7(self):\n    x_shape = [1, 2, 6, 1, 3]\n    self._runtests(x_shape, True)\n\n  @test_util.run_deprecated_v1\n  def testBatchNormGradInferenceShape1(self):\n    x_shape = [1, 1, 6, 1]\n    self._runtests(x_shape, is_training=False, gradient_test=True)\n\n  @test_util.run_deprecated_v1\n  def testBatchNormGradInferenceShape2(self):\n    x_shape = [1, 1, 6, 2]\n    self._runtests(x_shape, is_training=False, gradient_test=True)\n\n  @test_util.run_deprecated_v1\n  def testBatchNormGradInferenceShape3(self):\n    x_shape = [1, 2, 1, 6]\n    self._runtests(x_shape, is_training=False, gradient_test=True)\n\n  @test_util.run_deprecated_v1\n  def testBatchNormGradInferenceShape4(self):\n    x_shape = [5, 7, 11, 4]\n    self._runtests(x_shape, is_training=False, gradient_test=True)\n\n  @test_util.run_deprecated_v1\n  @test_util.disable_xla('This test never passed for XLA')\n  def testBatchNormGradInferenceShape5(self):\n    x_shape = [0, 7, 11, 4]\n    self._runtests(x_shape, is_training=False, gradient_test=True)\n\n  @test_util.run_deprecated_v1\n  def testBatchNormGradInferenceShape6(self):\n    x_shape = [1, 1, 1, 1]\n    # GPU kernel doesn't properly handle case where non-channel dimensions are 1\n    self._runtests(x_shape, is_training=False, gradient_test=True,\n                   cpu_only=True)\n\n  @test_util.run_deprecated_v1\n  def testBatchNormGradInferenceShape7(self):\n    x_shape = [1, 2, 6, 1, 3]\n    self._runtests(x_shape, is_training=False, gradient_test=True)\n\n  @test_util.run_deprecated_v1\n  def testBatchNormGradTrainingShape1(self):\n    x_shape = [1, 1, 6, 1]\n    self._runtests(x_shape, is_training=True, gradient_test=True)\n\n  @test_util.run_deprecated_v1\n  def testBatchNormGradTrainingShape2(self):\n    x_shape = [1, 1, 6, 2]\n    self._runtests(x_shape, is_training=True, gradient_test=True)\n\n  @test_util.run_deprecated_v1\n  def testBatchNormGradTrainingShape3(self):\n    x_shape = [1, 2, 1, 6]\n    self._runtests(x_shape, is_training=True, gradient_test=True)\n\n  @test_util.run_deprecated_v1\n  def testBatchNormGradTrainingShape4(self):\n    x_shape = [5, 7, 11, 4]\n    self._runtests(x_shape, is_training=True, gradient_test=True)\n\n  @test_util.run_deprecated_v1\n  @test_util.disable_xla('This test never passed for XLA')\n  def testBatchNormGradTrainingShape5(self):\n    x_shape = [0, 7, 11, 4]\n    self._runtests(x_shape, is_training=True, gradient_test=True)\n\n  @test_util.run_deprecated_v1\n  def testBatchNormGradTrainingShape6(self):\n    x_shape = [1, 1, 1, 1]\n    # GPU kernel doesn't properly handle case where non-channel dimensions are 1\n    self._runtests(x_shape, is_training=True, gradient_test=True, cpu_only=True)\n\n  @test_util.run_deprecated_v1\n  def testBatchNormGradTrainingShape7(self):\n    x_shape = [1, 2, 6, 1, 3]\n    self._runtests(x_shape, is_training=True, gradient_test=True)\n\n  def _testBatchNormGradGrad(self, config):\n    shape = config['shape']\n    err_tolerance = config['err_tolerance']\n    dtype = config['dtype']\n    rank = len(shape)\n    if rank == 4:\n      data_format_nhwc, features_nhwc = 'NHWC', shape[3]\n      data_format_nchw, features_nchw = 'NCHW', shape[1]\n    else:\n      data_format_nhwc, features_nhwc = 'NDHWC', shape[4]\n      data_format_nchw, features_nchw = 'NCDHW', shape[1]\n    for is_training in [True, False]:\n      if test.is_gpu_available(cuda_only=True):\n        self._test_grad_grad(\n            shape,\n            dtype, [features_nhwc],\n            np.float32,\n            use_gpu=True,\n            data_format=data_format_nhwc,\n            is_training=is_training,\n            err_tolerance=err_tolerance)\n        self._test_grad_grad(\n            shape,\n            dtype, [features_nchw],\n            np.float32,\n            use_gpu=True,\n            data_format=data_format_nchw,\n            is_training=is_training,\n            err_tolerance=err_tolerance)\n      self._test_grad_grad(\n          shape,\n          dtype, [features_nhwc],\n          np.float32,\n          use_gpu=False,\n          data_format=data_format_nhwc,\n          is_training=is_training,\n          err_tolerance=err_tolerance)\n      self._test_grad_grad(\n          shape,\n          dtype, [features_nchw],\n          np.float32,\n          use_gpu=False,\n          data_format=data_format_nchw,\n          is_training=is_training,\n          err_tolerance=err_tolerance)\n\n  @test_util.run_deprecated_v1\n  def testBatchNormGradGradConfig1(self):\n    config = {\n        'shape': [2, 3, 4, 5],\n        'err_tolerance': 1e-2,\n        'dtype': np.float32,\n    }\n    self._testBatchNormGradGrad(config)\n\n  @test_util.run_deprecated_v1\n  def testBatchNormGradGradConfig2(self):\n    config = {\n        'shape': [2, 3, 2, 2],\n        'err_tolerance': 1e-3,\n        'dtype': np.float32,\n    }\n    self._testBatchNormGradGrad(config)\n\n  @test_util.run_deprecated_v1\n  def testBatchNormGradGradConfig3(self):\n    config = {\n        'shape': [2, 3, 4, 5],\n        'err_tolerance': 2e-2,\n        'dtype': np.float16,\n    }\n    self._testBatchNormGradGrad(config)\n\n  @test_util.run_deprecated_v1\n  def testBatchNormGradGradConfig4(self):\n    config = {\n        'shape': [2, 3, 2, 2],\n        'err_tolerance': 2e-3,\n        'dtype': np.float16,\n    }\n    self._testBatchNormGradGrad(config)\n\n  @test_util.run_deprecated_v1\n  def testBatchNormGradGradConfig5(self):\n    config = {\n        'shape': [2, 3, 2, 2, 2],\n        'err_tolerance': 2e-3,\n        'dtype': np.float32,\n    }\n    self._testBatchNormGradGrad(config)\n\n  @test_util.run_deprecated_v1\n  def testBatchNormGradGradConfig6(self):\n    config = {\n        'shape': [2, 3, 2, 2, 2],\n        'err_tolerance': 3e-3,\n        'dtype': np.float16,\n    }\n    self._testBatchNormGradGrad(config)\n\n  def test5dBatchNormFollowedByRelu(self):\n    # The remapper grappler pass previously did not properly handle a 5D\n    # inference FusedBatchNorm followed by Relu. This asserts that this case is\n    # correctly handled.\n    np.random.seed(1)\n    x = np.random.random_sample((2, 3, 2, 2, 3)).astype(np.float32)\n    scale = np.random.random_sample((3,)).astype(np.float32)\n    offset = np.random.random_sample((3,)).astype(np.float32)\n    mean = np.random.random_sample((3,)).astype(np.float32)\n    var = np.random.random_sample((3,)).astype(np.float32)\n\n    epsilon = 0.001\n    y, _, _ = nn_impl.fused_batch_norm(\n        x,\n        scale,\n        offset,\n        mean=mean,\n        variance=var,\n        epsilon=epsilon,\n        data_format='NCDHW',\n        is_training=False)\n    y = nn_ops.relu(y)\n    y_val = self.evaluate(y)\n    y_ref = self._inference_ref(x, scale, offset, mean, var, epsilon,\n                                'NCDHW')\n    y_ref = np.maximum(y_ref, 0.)\n    self.assertAllClose(y_ref, y_val, atol=1e-3)\n\n  def testEagerShapeErrors(self):\n    with context.eager_mode():\n      x = array_ops.ones((2, 2, 2, 2))\n      scale = array_ops.ones((3,))\n      offset = array_ops.ones((2,))\n      with self.assertRaisesRegex(\n          errors_impl.InvalidArgumentError,\n          'scale must have the same number of elements'):\n        nn_impl.fused_batch_norm(x, scale, offset)\n\n      x = array_ops.ones((2, 2, 2, 2))\n      scale = array_ops.ones((2,))\n      offset = array_ops.ones((3,))\n      with self.assertRaisesRegex(\n          errors_impl.InvalidArgumentError,\n          'offset must have the same number of elements'):\n        nn_impl.fused_batch_norm(x, scale, offset)\n\n      x = array_ops.ones((2, 2, 2, 2))\n      scale = array_ops.ones((2,))\n      offset = array_ops.ones((2,))\n      mean = array_ops.ones((0,))\n      variance = array_ops.ones((2,))\n      with self.assertRaisesRegex(\n          errors_impl.InvalidArgumentError,\n          'When is_training=false, mean must have the same number of elements'):\n        nn_impl.fused_batch_norm(\n            x, scale, offset, mean=mean, variance=variance, is_training=False)\n\n      x = array_ops.ones((2, 2, 2, 2))\n      scale = array_ops.ones((2,))\n      offset = array_ops.ones((2,))\n      mean = array_ops.ones((2,))\n      variance = array_ops.ones((0,))\n      with self.assertRaisesRegex(\n          errors_impl.InvalidArgumentError,\n          'When is_training=false, variance must have the same number of '\n          'elements'):\n        nn_impl.fused_batch_norm(\n            x, scale, offset, mean=mean, variance=variance, is_training=False)\n\n      x = array_ops.ones((2, 2, 2, 2))\n      scale = array_ops.ones((2,))\n      offset = array_ops.ones((2,))\n      mean = array_ops.ones((0,))\n      variance = array_ops.ones((2,))\n      with self.assertRaisesRegex(\n          errors_impl.InvalidArgumentError,\n          'When exponential_avg_factor != 1, mean must have the same number of '\n          'elements'):\n        nn_impl.fused_batch_norm(\n            x,\n            scale,\n            offset,\n            mean=mean,\n            variance=variance,\n            exponential_avg_factor=0.5)\n\n      x = array_ops.ones((2, 2, 2, 2))\n      scale = array_ops.ones((2,))\n      offset = array_ops.ones((2,))\n      mean = array_ops.ones((2,))\n      variance = array_ops.ones((0,))\n      with self.assertRaisesRegex(\n          errors_impl.InvalidArgumentError,\n          'When exponential_avg_factor != 1, variance must have the same '\n          'number of elements'):\n        nn_impl.fused_batch_norm(\n            x,\n            scale,\n            offset,\n            mean=mean,\n            variance=variance,\n            exponential_avg_factor=0.5)\n\n  def testEagerShapeGradErrors(self):\n    with context.eager_mode():\n      y_backprop = array_ops.ones((2, 2, 2, 3))\n      x = array_ops.ones((2, 2, 2, 2))\n      scale = array_ops.ones((2,))\n      reserve_space_1 = array_ops.ones((2,))\n      reserve_space_2 = array_ops.ones((2,))\n      with self.assertRaisesRegex(errors_impl.InvalidArgumentError,\n                                  'x and y_backprop must have same shape,'):\n        gen_nn_ops.fused_batch_norm_grad_v2(y_backprop, x, scale,\n                                            reserve_space_1, reserve_space_2)\n\n      y_backprop = array_ops.ones((2, 2, 2, 2))\n      x = array_ops.ones((2, 2, 2, 2))\n      scale = array_ops.ones((3,))\n      reserve_space_1 = array_ops.ones((2,))\n      reserve_space_2 = array_ops.ones((2,))\n      with self.assertRaisesRegex(\n          errors_impl.InvalidArgumentError,\n          'scale must have the same number of elements'):\n        gen_nn_ops.fused_batch_norm_grad_v2(y_backprop, x, scale,\n                                            reserve_space_1, reserve_space_2)\n\n      y_backprop = array_ops.ones((2, 2, 2, 2))\n      x = array_ops.ones((2, 2, 2, 2))\n      scale = array_ops.ones((2,))\n      reserve_space_1 = array_ops.ones((3,))\n      reserve_space_2 = array_ops.ones((2,))\n      with self.assertRaisesRegex(\n          errors_impl.InvalidArgumentError,\n          'reserve_space_1 must have the same number of elements'):\n        gen_nn_ops.fused_batch_norm_grad_v2(y_backprop, x, scale,\n                                            reserve_space_1, reserve_space_2)\n\n      y_backprop = array_ops.ones((2, 2, 2, 2))\n      x = array_ops.ones((2, 2, 2, 2))\n      scale = array_ops.ones((2,))\n      reserve_space_1 = array_ops.ones((2,))\n      reserve_space_2 = array_ops.ones((3,))\n      with self.assertRaisesRegex(\n          errors_impl.InvalidArgumentError,\n          'reserve_space_2 must have the same number of elements'):\n        gen_nn_ops.fused_batch_norm_grad_v2(y_backprop, x, scale,\n                                            reserve_space_1, reserve_space_2)\n\n\nif __name__ == '__main__':\n  test.main()\n"], "filenames": ["tensorflow/core/kernels/fused_batch_norm_op.cc", "tensorflow/python/ops/nn_fused_batchnorm_test.py"], "buggy_code_start_loc": [1343, 18], "buggy_code_end_loc": [1570, 696], "fixing_code_start_loc": [1343, 19], "fixing_code_end_loc": [1595, 820], "type": "CWE-125", "message": "TensorFlow is an open source platform for machine learning. In affected versions the implementation of `FusedBatchNorm` kernels is vulnerable to a heap OOB access. The fix will be included in TensorFlow 2.7.0. We will also cherrypick this commit on TensorFlow 2.6.1, TensorFlow 2.5.2, and TensorFlow 2.4.4, as these are also affected and still in supported range.", "other": {"cve": {"id": "CVE-2021-41223", "sourceIdentifier": "security-advisories@github.com", "published": "2021-11-05T21:15:09.203", "lastModified": "2021-11-09T15:27:14.150", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "TensorFlow is an open source platform for machine learning. In affected versions the implementation of `FusedBatchNorm` kernels is vulnerable to a heap OOB access. The fix will be included in TensorFlow 2.7.0. We will also cherrypick this commit on TensorFlow 2.6.1, TensorFlow 2.5.2, and TensorFlow 2.4.4, as these are also affected and still in supported range."}, {"lang": "es", "value": "TensorFlow es una plataforma de c\u00f3digo abierto para el aprendizaje autom\u00e1tico. En las versiones afectadas, la implementaci\u00f3n de los kernels \"FusedBatchNorm\" es vulnerable a un acceso OOB a la pila. La correcci\u00f3n ser\u00e1 incluida en TensorFlow versi\u00f3n 2.7.0. Tambi\u00e9n ser\u00e1 incluida este commit en TensorFlow versi\u00f3n 2.6.1, TensorFlow versi\u00f3n 2.5.2, y TensorFlow versi\u00f3n 2.4.4, ya que estos tambi\u00e9n est\u00e1n afectados y todav\u00eda est\u00e1n en el rango admitido"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 7.1, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.2}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 7.1, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.2}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:P/I:N/A:P", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "NONE", "availabilityImpact": "PARTIAL", "baseScore": 3.6}, "baseSeverity": "LOW", "exploitabilityScore": 3.9, "impactScore": 4.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-125"}]}, {"source": "security-advisories@github.com", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-125"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionEndExcluding": "2.4.4", "matchCriteriaId": "455FB550-4C9C-4BD6-9F76-A627B62AB332"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.5.0", "versionEndExcluding": "2.5.2", "matchCriteriaId": "035CDF63-1548-4FB4-B8A9-B8D328FAF910"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.6.0", "versionEndExcluding": "2.6.1", "matchCriteriaId": "5D68D8D1-DB27-4395-9D3D-2BED901B852C"}]}]}], "references": [{"url": "https://github.com/tensorflow/tensorflow/commit/aab9998916c2ffbd8f0592059fad352622f89cda", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/security/advisories/GHSA-f54p-f6jp-4rhr", "source": "security-advisories@github.com", "tags": ["Exploit", "Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/tensorflow/tensorflow/commit/aab9998916c2ffbd8f0592059fad352622f89cda"}}