{"buggy_code": ["/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n#include <cstdint>\n#include <utility>\n#include <vector>\n\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/register_types.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/tensor_shape.h\"\n#include \"tensorflow/core/framework/variant.h\"\n#include \"tensorflow/core/framework/variant_encode_decode.h\"\n#include \"tensorflow/core/framework/variant_op_registry.h\"\n#include \"tensorflow/core/kernels/concat_lib.h\"\n#include \"tensorflow/core/kernels/ragged_tensor_variant.h\"\n#include \"tensorflow/core/lib/core/errors.h\"\n#include \"tensorflow/core/lib/core/status.h\"\n#include \"tensorflow/core/platform/errors.h\"\n#include \"tensorflow/core/util/tensor_ops_util.h\"\n\nnamespace tensorflow {\nnamespace {\n\ntemplate <typename VALUE_TYPE>\nStatus UnbatchDenseZerothDim(\n    const RaggedTensorVariant& batched_ragged,\n    std::vector<RaggedTensorVariant>* ragged_components) {\n  Tensor batched_values = batched_ragged.values();\n  TensorShape values_shape = batched_values.shape();\n  if (values_shape.dims() < 1) {\n    return errors::InvalidArgument(\"Can't unbatch rank-0 tensor.\");\n  }\n  auto num_components = values_shape.dim_size(0);\n  values_shape.RemoveDim(0);\n  auto num_values = values_shape.num_elements();\n\n  ragged_components->resize(num_components);\n  const auto& batched_flat = batched_values.flat<VALUE_TYPE>();\n\n  for (auto i = decltype(num_components){}; i < num_components; i++) {\n    (*ragged_components)[i].set_values(\n        Tensor(DataTypeToEnum<VALUE_TYPE>::value, values_shape));\n    auto ragged_component_values_flat =\n        (*ragged_components)[i].mutable_values()->flat<VALUE_TYPE>();\n    for (auto j = decltype(num_values){}; j < num_values; j++) {\n      ragged_component_values_flat(j) = batched_flat(j + i * num_values);\n    }\n  }\n\n  return OkStatus();\n}\n\ntemplate <typename VALUE_TYPE, typename SPLIT_TYPE>\nStatus UnbatchRaggedZerothDim(\n    const RaggedTensorVariant& batched_ragged,\n    std::vector<RaggedTensorVariant>* ragged_components) {\n  // Set up the component Ragged Tensors.\n  int ragged_rank = batched_ragged.ragged_rank();\n  if (ragged_rank == 0) {\n    return UnbatchDenseZerothDim<VALUE_TYPE>(batched_ragged, ragged_components);\n  }\n\n  auto batched_splits_top_vec = batched_ragged.splits(0).vec<SPLIT_TYPE>();\n  auto num_components = batched_splits_top_vec.size() - 1;\n\n  if (num_components < 0) {\n    return errors::Internal(\"Invalid split argument.\");\n  }\n\n  int num_splits = ragged_rank - 1;\n  ragged_components->resize(num_components);\n  for (RaggedTensorVariant& ragged_component : *ragged_components) {\n    ragged_component.mutable_nested_splits()->reserve(num_splits);\n  }\n  const auto& batched_flat = batched_ragged.values().flat<VALUE_TYPE>();\n  auto num_inner_elems = batched_ragged.values().NumElements();\n  if (batched_ragged.values().dim_size(0) > 1) {\n    num_inner_elems /= batched_ragged.values().dim_size(0);\n  }\n  TensorShape values_shape = batched_ragged.values().shape();\n\n  // Corner case: ragged_rank == 1, e.g. [[1, 2, 3], [4, 5]]\n  if (num_splits == 0) {\n    for (auto i = decltype(num_components){}; i < num_components; i++) {\n      auto start = batched_splits_top_vec(i);\n      auto limit = batched_splits_top_vec(i + 1);\n      auto num_values = limit - start;\n      values_shape.set_dim(0, num_values);\n      (*ragged_components)[i].set_values(\n          Tensor(DataTypeToEnum<VALUE_TYPE>::value, values_shape));\n      auto ragged_component_values_flat =\n          (*ragged_components)[i].mutable_values()->template flat<VALUE_TYPE>();\n      for (auto j = decltype(num_values * num_inner_elems){};\n           j < num_values * num_inner_elems; j++) {\n        ragged_component_values_flat(j) =\n            batched_flat(j + start * num_inner_elems);\n      }\n    }\n    return OkStatus();\n  }\n\n  // Unbatch nested splits.\n  std::vector<typename TTypes<SPLIT_TYPE>::ConstVec> batched_splits_vec;\n  batched_splits_vec.reserve(ragged_rank);\n  for (int i = 0; i < ragged_rank; i++) {\n    batched_splits_vec.push_back(batched_ragged.splits(i).vec<SPLIT_TYPE>());\n  }\n  std::vector<SPLIT_TYPE> index(num_splits, 1);\n  std::vector<SPLIT_TYPE> ragged_component_values_size(num_components, 0);\n  for (auto i = decltype(num_components){}; i < num_components; i++) {\n    std::vector<typename TTypes<SPLIT_TYPE>::Vec> ragged_component_splits_vec;\n    ragged_component_splits_vec.reserve(num_splits);\n    SPLIT_TYPE split_size = -1;\n    for (int j = 0; j < num_splits; j++) {\n      if (j == 0) {\n        split_size =\n            batched_splits_top_vec(i + 1) - batched_splits_top_vec(i) + 1;\n      } else {\n        // Update split size based on previous split.\n        SPLIT_TYPE last_index = ragged_component_splits_vec[j - 1].size() - 1;\n        split_size = ragged_component_splits_vec[j - 1](last_index) + 1;\n      }\n      (*ragged_components)[i].append_splits(\n          Tensor(DataTypeToEnum<SPLIT_TYPE>::value, TensorShape({split_size})));\n      ragged_component_splits_vec.push_back((*ragged_components)[i]\n                                                .mutable_splits(j)\n                                                ->template vec<SPLIT_TYPE>());\n      SPLIT_TYPE last_split_value = batched_splits_vec[j + 1](index[j] - 1);\n      ragged_component_splits_vec[j](0) = 0;\n      for (SPLIT_TYPE k = 1; k < split_size; k++, index[j]++) {\n        ragged_component_splits_vec[j](k) =\n            batched_splits_vec[j + 1](index[j]) - last_split_value;\n      }\n    }\n    SPLIT_TYPE last_split_size =\n        ragged_component_splits_vec[num_splits - 1].size();\n    ragged_component_values_size[i] =\n        ragged_component_splits_vec[num_splits - 1](last_split_size - 1);\n  }\n\n  // Unbatch values.\n  int64_t value_index = 0;\n  for (auto i = decltype(num_components){}; i < num_components; i++) {\n    SPLIT_TYPE num_values = ragged_component_values_size[i];\n    values_shape.set_dim(0, num_values);\n    (*ragged_components)[i].set_values(\n        Tensor(DataTypeToEnum<VALUE_TYPE>::value, values_shape));\n    auto ragged_component_values_flat =\n        (*ragged_components)[i].mutable_values()->template flat<VALUE_TYPE>();\n    for (int64_t j = 0; j < num_values * num_inner_elems; j++, value_index++) {\n      ragged_component_values_flat(j) = batched_flat(value_index);\n    }\n  }\n\n  return OkStatus();\n}\n}  // namespace\n\ntemplate <typename VALUE_TYPE, typename SPLIT_TYPE>\nclass RaggedTensorToVariantOp : public OpKernel {\n public:\n  explicit RaggedTensorToVariantOp(OpKernelConstruction* context)\n      : OpKernel(context) {\n    OP_REQUIRES_OK(context, context->GetAttr(\"batched_input\", &batched_input_));\n  }\n\n  void Compute(OpKernelContext* context) override {\n    // Read ragged_splits inputs.\n    OpInputList ragged_nested_splits_in;\n    OP_REQUIRES_OK(context, context->input_list(\"rt_nested_splits\",\n                                                &ragged_nested_splits_in));\n    const int ragged_nested_splits_len = ragged_nested_splits_in.size();\n    RaggedTensorVariant batched_ragged_input;\n    // Read ragged_values input.\n    batched_ragged_input.set_values(context->input(ragged_nested_splits_len));\n    batched_ragged_input.mutable_nested_splits()->reserve(\n        ragged_nested_splits_len);\n    for (int i = 0; i < ragged_nested_splits_len; i++) {\n      batched_ragged_input.append_splits(ragged_nested_splits_in[i]);\n    }\n\n    if (!batched_input_) {\n      // Encode as a Scalar Variant Tensor.\n      Tensor* encoded_scalar;\n      OP_REQUIRES_OK(context, context->allocate_output(0, TensorShape({}),\n                                                       &encoded_scalar));\n      encoded_scalar->scalar<Variant>()() = std::move(batched_ragged_input);\n      return;\n    }\n\n    // Unbatch the Ragged Tensor and encode the components.\n    std::vector<RaggedTensorVariant> unbatched_ragged_input;\n    OP_REQUIRES_OK(context, UnbatchRaggedZerothDim<VALUE_TYPE, SPLIT_TYPE>(\n                                batched_ragged_input, &unbatched_ragged_input));\n\n    // Bundle the encoded scalar Variant Tensors into a rank-1 Variant Tensor.\n    Tensor* encoded_vector;\n\n    // output_size will be used for calling TensorShape(int64_t ...). We\n    // cannot use `auto` type here, or there will be a narrowing error.\n    int64_t output_size = unbatched_ragged_input.size();\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(0, TensorShape({output_size}),\n                                            &encoded_vector));\n    auto encoded_vector_t = encoded_vector->vec<Variant>();\n    for (auto i = decltype(output_size){}; i < output_size; i++) {\n      encoded_vector_t(i) = unbatched_ragged_input[i];\n    }\n  }\n\n private:\n  bool batched_input_;\n};\n\ntemplate <typename VALUE_TYPE, typename SPLIT_TYPE>\nclass RaggedTensorToVariantGradientOp : public OpKernel {\n public:\n  using OpKernel::OpKernel;\n\n  void Compute(OpKernelContext* context) override {\n    // Read inputs.\n    Tensor encoded_variant = context->input(0);\n    Tensor row_splits = context->input(1);\n    auto flat_row_splits = row_splits.flat<SPLIT_TYPE>();\n    TensorShape dense_values_shape;\n    OP_REQUIRES_OK(context,\n                   TensorShapeUtils::MakeShape(context->input(2).vec<int32>(),\n                                               &dense_values_shape));\n\n    const auto& flat_variants = encoded_variant.flat<Variant>();\n\n    // Get a Tensor containing the flat_values for each variant.\n    std::vector<Tensor> values;\n    for (int i = 0; i < flat_variants.size(); ++i) {\n      if (const auto* encoded = flat_variants(i).get<RaggedTensorVariant>()) {\n        values.push_back(encoded->values());\n      } else {\n        // Missing value: this happens if only some of the variant values\n        // generated by ragged_tensor_to_variant impacted the value that we're\n        // calculating the gradient for.  In this case, we will see a\n        // default-constructed variant; so treat it as a zero tensor with the\n        // appropriate shape.\n        const auto value_dtype = DataTypeToEnum<VALUE_TYPE>::v();\n        auto piece_size = flat_row_splits(i + 1) - flat_row_splits(i);\n        TensorShape zeros_shape = dense_values_shape;\n        zeros_shape.set_dim(0, piece_size);\n        Tensor zero(value_dtype, zeros_shape);\n        zero.flat<VALUE_TYPE>().setZero();\n        values.push_back(zero);\n      }\n    }\n\n    if (values.size() == 1) {\n      // Just one flat_value tensor: return as-is.\n      context->set_output(0, values[0]);\n    } else {\n      Tensor* out = nullptr;\n      OP_REQUIRES_OK(context,\n                     context->allocate_output(0, dense_values_shape, &out));\n      // ConcatCPU assumes non-empty output.\n      if (dense_values_shape.num_elements() == 0) return;\n      // Multiple flat_values tensors: concatenate them together.\n      using Piece = typename TTypes<VALUE_TYPE, 2>::Matrix;\n      using ConstPiece = typename TTypes<VALUE_TYPE, 2>::ConstMatrix;\n      std::vector<std::unique_ptr<ConstPiece>> pieces;\n      pieces.reserve(values.size());\n      for (const Tensor& t : values) {\n        // ConcatCPU assumes non-empty inputs.\n        if (t.NumElements() == 0) continue;\n        pieces.emplace_back(\n            new ConstPiece(t.shaped<VALUE_TYPE, 2>({1, t.NumElements()})));\n      }\n      Piece out_flat =\n          out->shaped<VALUE_TYPE, 2>({1, dense_values_shape.num_elements()});\n      ConcatCPU<VALUE_TYPE>(context->device(), pieces, &out_flat);\n    }\n  }\n};\n\n#define REGISTER_KERNELS_WITH_SPLIT_TYPE(value_type, split_type)            \\\n  REGISTER_KERNEL_BUILDER(Name(\"RaggedTensorToVariant\")                     \\\n                              .Device(DEVICE_CPU)                           \\\n                              .TypeConstraint<value_type>(\"Tvalues\")        \\\n                              .TypeConstraint<split_type>(\"Tsplits\"),       \\\n                          RaggedTensorToVariantOp<value_type, split_type>); \\\n  REGISTER_KERNEL_BUILDER(                                                  \\\n      Name(\"RaggedTensorToVariantGradient\")                                 \\\n          .Device(DEVICE_CPU)                                               \\\n          .TypeConstraint<value_type>(\"Tvalues\")                            \\\n          .TypeConstraint<split_type>(\"Tsplits\"),                           \\\n      RaggedTensorToVariantGradientOp<value_type, split_type>);\n\n#define REGISTER_KERNELS(value_type)                  \\\n  REGISTER_KERNELS_WITH_SPLIT_TYPE(value_type, int32) \\\n  REGISTER_KERNELS_WITH_SPLIT_TYPE(value_type, int64_t)\nTF_CALL_POD_TYPES(REGISTER_KERNELS);\nTF_CALL_tstring(REGISTER_KERNELS);\nTF_CALL_QUANTIZED_TYPES(REGISTER_KERNELS);\nTF_CALL_quint16(REGISTER_KERNELS);\nTF_CALL_qint16(REGISTER_KERNELS);\n#undef REGISTER_KERNELS\n#undef REGISTER_KERNELS_WITH_SPLIT_TYPE\n}  // namespace tensorflow\n", "# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for third_party.tensorflow.python.ops.ragged_tensor.\"\"\"\n\nimport functools\nfrom absl.testing import parameterized\nimport numpy as np\n\nfrom tensorflow.core.framework import full_type_pb2\nfrom tensorflow.python.data.ops import dataset_ops\nfrom tensorflow.python.eager import backprop\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.eager import def_function\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.framework import tensor_spec\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.framework import type_spec\nfrom tensorflow.python.framework.type_utils import fulltypes_for_flat_tensors\nfrom tensorflow.python.ops import array_grad  # pylint: disable=unused-import\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import gen_ragged_conversion_ops\nfrom tensorflow.python.ops import gradients_impl\nfrom tensorflow.python.ops import map_fn\nfrom tensorflow.python.ops import math_grad  # pylint: disable=unused-import\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import tensor_array_grad  # pylint: disable=unused-import\nfrom tensorflow.python.ops.ragged import ragged_concat_ops\nfrom tensorflow.python.ops.ragged import ragged_factory_ops\nfrom tensorflow.python.ops.ragged import ragged_gather_ops\nfrom tensorflow.python.ops.ragged import ragged_math_ops\nfrom tensorflow.python.ops.ragged import ragged_tensor\nfrom tensorflow.python.ops.ragged import ragged_tensor_value\nfrom tensorflow.python.ops.ragged.ragged_tensor import RaggedTensor\nfrom tensorflow.python.ops.ragged.ragged_tensor import RaggedTensorSpec\nfrom tensorflow.python.ops.ragged.row_partition import RowPartition\n\nfrom tensorflow.python.platform import googletest\nfrom tensorflow.python.util import nest\n\n\ndef int32array(values):\n  return np.array(values, dtype=np.int32)\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass RaggedTensorTest(test_util.TensorFlowTestCase, parameterized.TestCase):\n  longMessage = True  # Property in unittest.Testcase. pylint: disable=invalid-name\n\n  #=============================================================================\n  # RaggedTensor class docstring examples\n  #=============================================================================\n\n  def testClassDocStringExamples(self):\n    # From section: \"Component Tensors\"\n    rt = RaggedTensor.from_row_splits(\n        values=[3, 1, 4, 1, 5, 9, 2, 6], row_splits=[0, 4, 4, 7, 8, 8])\n    self.assertAllEqual(rt, [[3, 1, 4, 1], [], [5, 9, 2], [6], []])\n    del rt\n\n    # From section: \"Alternative Row-Partitioning Schemes\"\n    values = [3, 1, 4, 1, 5, 9, 2, 6]\n    rt1 = RaggedTensor.from_row_splits(values, row_splits=[0, 4, 4, 7, 8, 8])\n    rt2 = RaggedTensor.from_row_lengths(values, row_lengths=[4, 0, 3, 1, 0])\n    rt3 = RaggedTensor.from_value_rowids(\n        values, value_rowids=[0, 0, 0, 0, 2, 2, 2, 3], nrows=5)\n    rt4 = RaggedTensor.from_row_starts(values, row_starts=[0, 4, 4, 7, 8])\n    rt5 = RaggedTensor.from_row_limits(values, row_limits=[4, 4, 7, 8, 8])\n    for rt in (rt1, rt2, rt3, rt4, rt5):\n      self.assertAllEqual(rt, [[3, 1, 4, 1], [], [5, 9, 2], [6], []])\n    del rt1, rt2, rt3, rt4, rt5\n\n    # From section: \"Multiple Ragged Dimensions\"\n    inner_rt = RaggedTensor.from_row_splits(\n        values=[3, 1, 4, 1, 5, 9, 2, 6], row_splits=[0, 4, 4, 7, 8, 8])\n    outer_rt = RaggedTensor.from_row_splits(\n        values=inner_rt, row_splits=[0, 3, 3, 5])\n    self.assertEqual(outer_rt.ragged_rank, 2)\n    self.assertAllEqual(outer_rt,\n                        [[[3, 1, 4, 1], [], [5, 9, 2]], [], [[6], []]])\n    del inner_rt, outer_rt\n\n    # From section: \"Multiple Ragged Dimensions\"\n    rt = RaggedTensor.from_nested_row_splits(\n        flat_values=[3, 1, 4, 1, 5, 9, 2, 6],\n        nested_row_splits=([0, 3, 3, 5], [0, 4, 4, 7, 8, 8]))\n    self.assertAllEqual(rt, [[[3, 1, 4, 1], [], [5, 9, 2]], [], [[6], []]])\n    del rt\n\n    # From section: \"Uniform Inner Dimensions\"\n    rt = RaggedTensor.from_row_splits(\n        values=array_ops.ones([5, 3]), row_splits=[0, 2, 5])\n    self.assertAllEqual(\n        rt, [[[1, 1, 1], [1, 1, 1]], [[1, 1, 1], [1, 1, 1], [1, 1, 1]]])\n    self.assertEqual(rt.shape.as_list(), [2, None, 3])\n    del rt\n\n  #=============================================================================\n  # RaggedTensorValue Constructor\n  #=============================================================================\n\n  def testRaggedTensorValueConstruction(self):\n    values = np.array(b'a b c d e f g'.split())\n    splits = np.array([0, 2, 5, 6, 6, 7], dtype=np.int64)\n    splits2 = np.array([0, 3, 5], dtype=np.int64)\n\n    # Test construction of a RaggedTensorValue with ragged_rank=1.\n    rt_value = ragged_tensor_value.RaggedTensorValue(values, splits)\n    self.assertEqual(rt_value.row_splits.dtype, np.int64)\n    self.assertEqual(rt_value.shape, (5, None))\n    self.assertLen(rt_value.nested_row_splits, 1)\n    self.assertAllEqual(splits, rt_value.row_splits)\n    self.assertAllEqual(values, rt_value.values)\n    self.assertAllEqual(splits, rt_value.nested_row_splits[0])\n    self.assertAllEqual(values, rt_value.flat_values)\n\n    # Test construction of a RaggedTensorValue with ragged_rank=2.\n    rt_value = ragged_tensor_value.RaggedTensorValue(\n        values=ragged_tensor_value.RaggedTensorValue(values, splits),\n        row_splits=splits2)\n    self.assertEqual(rt_value.row_splits.dtype, np.int64)\n    self.assertEqual(rt_value.shape, (2, None, None))\n    self.assertLen(rt_value.nested_row_splits, 2)\n    self.assertAllEqual(splits2, rt_value.row_splits)\n    self.assertAllEqual(splits, rt_value.values.row_splits)\n    self.assertAllEqual(splits2, rt_value.nested_row_splits[0])\n    self.assertAllEqual(splits, rt_value.nested_row_splits[1])\n    self.assertAllEqual(values, rt_value.values.values)\n    self.assertAllEqual(values, rt_value.flat_values)\n\n  #=============================================================================\n  # RaggedTensor Constructor (private)\n  #=============================================================================\n\n  def testRaggedTensorConstruction(self):\n    values = constant_op.constant(['a', 'b', 'c', 'd', 'e', 'f', 'g'])\n    row_splits = constant_op.constant([0, 2, 2, 5, 6, 7], dtypes.int64)\n    rp = RowPartition.from_row_splits(row_splits)\n    rt = RaggedTensor(values=values, row_partition=rp, internal=True)\n\n    self.assertAllEqual(rt,\n                        [[b'a', b'b'], [], [b'c', b'd', b'e'], [b'f'], [b'g']])\n\n  def testRaggedTensorConstructionErrors(self):\n    values = constant_op.constant(['a', 'b', 'c', 'd', 'e', 'f', 'g'])\n    row_splits = constant_op.constant([0, 2, 2, 5, 6, 7], dtypes.int64)\n    rp = RowPartition.from_row_splits(row_splits)\n\n    with self.assertRaisesRegex(ValueError,\n                                'RaggedTensor constructor is private'):\n      RaggedTensor(values=values, row_partition=rp)\n\n    with self.assertRaisesRegex(\n        TypeError, r'type\\(values\\) must be one of: Tensor, RaggedTensor'):\n      RaggedTensor(values=range(7), row_partition=rp, internal=True)\n\n    with self.assertRaisesRegex(\n        TypeError, 'Argument `row_partition` must be a RowPartition'):\n      RaggedTensor(\n          values=values, row_partition=[0, 2, 2, 5, 6, 7], internal=True)\n\n  #=============================================================================\n  # RaggedTensor Factory Ops\n  #=============================================================================\n\n  def testFromValueRowIdsWithDerivedNRows(self):\n    # nrows is known at graph creation time.\n    values = constant_op.constant(['a', 'b', 'c', 'd', 'e', 'f', 'g'])\n    value_rowids = constant_op.constant([0, 0, 2, 2, 2, 3, 4], dtypes.int64)\n\n    rt = RaggedTensor.from_value_rowids(values, value_rowids, validate=False)\n    self.assertEqual(rt.dtype, dtypes.string)\n    self.assertEqual(rt.shape.as_list(), [5, None])\n    self.assertEqual(rt.ragged_rank, 1)\n\n    rt_values = rt.values\n    rt_value_rowids = rt.value_rowids()\n    rt_nrows = rt.nrows()\n\n    self.assertIs(rt_values, values)\n    self.assertIs(rt_value_rowids, value_rowids)  # cached_value_rowids\n    self.assertAllEqual(rt_value_rowids, value_rowids)\n    self.assertAllEqual(rt_nrows, 5)\n    self.assertAllEqual(rt,\n                        [[b'a', b'b'], [], [b'c', b'd', b'e'], [b'f'], [b'g']])\n\n  def testFromValueRowIdsWithDerivedNRowsDynamic(self):\n    # nrows is not known at graph creation time.\n    values = constant_op.constant(['a', 'b', 'c', 'd', 'e', 'f', 'g'])\n    value_rowids = constant_op.constant([0, 0, 2, 2, 2, 3, 4], dtypes.int64)\n    value_rowids = array_ops.placeholder_with_default(value_rowids, shape=None)\n\n    rt = RaggedTensor.from_value_rowids(values, value_rowids, validate=False)\n    self.assertEqual(rt.dtype, dtypes.string)\n    if context.executing_eagerly():\n      self.assertEqual(rt.shape.as_list(), [5, None])\n    else:\n      self.assertEqual(rt.shape.as_list(), [None, None])\n    self.assertEqual(rt.ragged_rank, 1)\n\n    rt_values = rt.values\n    rt_value_rowids = rt.value_rowids()\n    rt_nrows = rt.nrows()\n\n    self.assertIs(rt_values, values)\n    self.assertIs(rt_value_rowids, value_rowids)  # cached_value_rowids\n    self.assertAllEqual(rt_value_rowids, value_rowids)\n    self.assertAllEqual(rt_nrows, 5)\n    self.assertAllEqual(rt,\n                        [[b'a', b'b'], [], [b'c', b'd', b'e'], [b'f'], [b'g']])\n\n  def testFromValueRowIdsWithExplicitNRows(self):\n    values = constant_op.constant(['a', 'b', 'c', 'd', 'e', 'f', 'g'])\n    value_rowids = constant_op.constant([0, 0, 2, 2, 2, 3, 4], dtypes.int64)\n    nrows = constant_op.constant(7, dtypes.int64)\n\n    rt = RaggedTensor.from_value_rowids(\n        values, value_rowids, nrows, validate=False)\n    self.assertEqual(rt.dtype, dtypes.string)\n    self.assertEqual(rt.shape.as_list(), [7, None])\n    self.assertEqual(rt.ragged_rank, 1)\n\n    rt_values = rt.values\n    rt_value_rowids = rt.value_rowids()\n    rt_nrows = rt.nrows()\n\n    self.assertIs(rt_values, values)\n    self.assertIs(rt_value_rowids, value_rowids)  # cached_value_rowids\n    self.assertIs(rt_nrows, nrows)  # cached_nrows\n    self.assertAllEqual(\n        rt, [[b'a', b'b'], [], [b'c', b'd', b'e'], [b'f'], [b'g'], [], []])\n\n  def testFromValueRowIdsWithExplicitNRowsEqualToDefault(self):\n    values = constant_op.constant(['a', 'b', 'c', 'd', 'e', 'f', 'g'])\n    value_rowids = constant_op.constant([0, 0, 2, 2, 2, 3, 4], dtypes.int64)\n    nrows = constant_op.constant(5, dtypes.int64)\n\n    rt = RaggedTensor.from_value_rowids(\n        values, value_rowids, nrows, validate=False)\n    self.assertEqual(rt.dtype, dtypes.string)\n    self.assertEqual(rt.shape.as_list(), [5, None])\n    self.assertEqual(rt.ragged_rank, 1)\n\n    rt_values = rt.values\n    rt_value_rowids = rt.value_rowids()\n    rt_nrows = rt.nrows()\n\n    self.assertIs(rt_values, values)\n    self.assertIs(rt_value_rowids, value_rowids)  # cached_value_rowids\n    self.assertIs(rt_nrows, nrows)  # cached_nrows\n    self.assertAllEqual(rt_value_rowids, value_rowids)\n    self.assertAllEqual(rt_nrows, nrows)\n    self.assertAllEqual(rt,\n                        [[b'a', b'b'], [], [b'c', b'd', b'e'], [b'f'], [b'g']])\n\n  def testFromValueRowIdsWithEmptyValues(self):\n    rt = RaggedTensor.from_value_rowids([], [])\n    rt_nrows = rt.nrows()\n    self.assertEqual(rt.dtype, dtypes.float32)\n    self.assertEqual(rt.shape.as_list(), [0, None])\n    self.assertEqual(rt.ragged_rank, 1)\n    self.assertEqual(rt.values.shape.as_list(), [0])\n    self.assertEqual(rt.value_rowids().shape.as_list(), [0])\n    self.assertAllEqual(rt_nrows, 0)\n    self.assertAllEqual(rt, [])\n\n  def testFromRowSplits(self):\n    values = constant_op.constant(['a', 'b', 'c', 'd', 'e', 'f', 'g'])\n    row_splits = constant_op.constant([0, 2, 2, 5, 6, 7], dtypes.int64)\n\n    rt = RaggedTensor.from_row_splits(values, row_splits, validate=False)\n    self.assertEqual(rt.dtype, dtypes.string)\n    self.assertEqual(rt.shape.as_list(), [5, None])\n    self.assertEqual(rt.ragged_rank, 1)\n\n    rt_values = rt.values\n    rt_row_splits = rt.row_splits\n    rt_nrows = rt.nrows()\n\n    self.assertIs(rt_values, values)\n    self.assertIs(rt_row_splits, row_splits)\n    self.assertAllEqual(rt_nrows, 5)\n    self.assertAllEqual(rt,\n                        [[b'a', b'b'], [], [b'c', b'd', b'e'], [b'f'], [b'g']])\n\n  def testFromRowSplitsWithDifferentSplitTypes(self):\n    values = constant_op.constant(['a', 'b', 'c', 'd', 'e', 'f', 'g'])\n    splits1 = [0, 2, 2, 5, 6, 7]\n    splits2 = np.array([0, 2, 2, 5, 6, 7], np.int64)\n    splits3 = np.array([0, 2, 2, 5, 6, 7], np.int32)\n    splits4 = constant_op.constant([0, 2, 2, 5, 6, 7], dtypes.int64)\n    splits5 = constant_op.constant([0, 2, 2, 5, 6, 7], dtypes.int32)\n    rt1 = RaggedTensor.from_row_splits(values, splits1)\n    rt2 = RaggedTensor.from_row_splits(values, splits2)\n    rt3 = RaggedTensor.from_row_splits(values, splits3)\n    rt4 = RaggedTensor.from_row_splits(values, splits4)\n    rt5 = RaggedTensor.from_row_splits(values, splits5)\n    self.assertEqual(rt1.row_splits.dtype, dtypes.int64)\n    self.assertEqual(rt2.row_splits.dtype, dtypes.int64)\n    self.assertEqual(rt3.row_splits.dtype, dtypes.int32)\n    self.assertEqual(rt4.row_splits.dtype, dtypes.int64)\n    self.assertEqual(rt5.row_splits.dtype, dtypes.int32)\n\n  def testFromRowSplitsWithEmptySplits(self):\n    err_msg = 'row_splits tensor may not be empty'\n    with self.assertRaisesRegex(ValueError, err_msg):\n      RaggedTensor.from_row_splits([], [])\n\n  def testFromRowStarts(self):\n    values = constant_op.constant(['a', 'b', 'c', 'd', 'e', 'f', 'g'])\n    row_starts = constant_op.constant([0, 2, 2, 5, 6], dtypes.int64)\n\n    rt = RaggedTensor.from_row_starts(values, row_starts, validate=False)\n    self.assertEqual(rt.dtype, dtypes.string)\n    self.assertEqual(rt.shape.as_list(), [5, None])\n    self.assertEqual(rt.ragged_rank, 1)\n\n    rt_values = rt.values\n    rt_row_starts = rt.row_starts()\n    rt_nrows = rt.nrows()\n\n    self.assertIs(rt_values, values)\n    self.assertAllEqual(rt_nrows, 5)\n    self.assertAllEqual(rt_row_starts, row_starts)\n    self.assertAllEqual(rt,\n                        [[b'a', b'b'], [], [b'c', b'd', b'e'], [b'f'], [b'g']])\n\n  def testFromRowLimits(self):\n    values = constant_op.constant(['a', 'b', 'c', 'd', 'e', 'f', 'g'])\n    row_limits = constant_op.constant([2, 2, 5, 6, 7], dtypes.int64)\n\n    rt = RaggedTensor.from_row_limits(values, row_limits, validate=False)\n    self.assertEqual(rt.dtype, dtypes.string)\n    self.assertEqual(rt.shape.as_list(), [5, None])\n    self.assertEqual(rt.ragged_rank, 1)\n\n    rt_values = rt.values\n    rt_row_limits = rt.row_limits()\n    rt_nrows = rt.nrows()\n\n    self.assertIs(rt_values, values)\n    self.assertAllEqual(rt_nrows, 5)\n    self.assertAllEqual(rt_row_limits, row_limits)\n    self.assertAllEqual(rt,\n                        [[b'a', b'b'], [], [b'c', b'd', b'e'], [b'f'], [b'g']])\n\n  def testFromRowLengths(self):\n    values = constant_op.constant(['a', 'b', 'c', 'd', 'e', 'f', 'g'])\n    row_lengths = constant_op.constant([2, 0, 3, 1, 1], dtypes.int64)\n\n    rt = RaggedTensor.from_row_lengths(values, row_lengths, validate=False)\n    self.assertEqual(rt.dtype, dtypes.string)\n    self.assertEqual(rt.shape.as_list(), [5, None])\n    self.assertEqual(rt.ragged_rank, 1)\n\n    rt_values = rt.values\n    rt_row_lengths = rt.row_lengths()\n    rt_nrows = rt.nrows()\n\n    self.assertIs(rt_values, values)\n    self.assertIs(rt_row_lengths, row_lengths)  # cached_nrows\n    self.assertAllEqual(rt_nrows, 5)\n    self.assertAllEqual(rt_row_lengths, row_lengths)\n    self.assertAllEqual(rt,\n                        [[b'a', b'b'], [], [b'c', b'd', b'e'], [b'f'], [b'g']])\n\n  def testFromRowLengthsInt32(self):\n    rt = RaggedTensor.from_row_lengths([1, 2, 3, 4],\n                                       constant_op.constant([1, 0, 3],\n                                                            dtype=dtypes.int32))\n    rt2 = RaggedTensor.from_row_lengths(rt, [2, 1, 0])\n    self.assertAllEqual([2, 1, 0], rt2.row_lengths())\n\n  def testFromUniformRowLength(self):\n    values = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n\n    a1 = RaggedTensor.from_uniform_row_length(values, 2)\n    a2 = RaggedTensor.from_uniform_row_length(values, 2, 8)\n    self.assertAllEqual(\n        a1,\n        [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12], [13, 14], [15, 16]])\n    self.assertAllEqual(a1, a2)\n    self.assertEqual(a1.shape.as_list(), [8, 2])\n    self.assertEqual(a2.shape.as_list(), [8, 2])\n\n    b1 = RaggedTensor.from_uniform_row_length(a1, 2)\n    b2 = RaggedTensor.from_uniform_row_length(a1, 2, 4)\n    self.assertAllEqual(b1, [[[1, 2], [3, 4]], [[5, 6], [7, 8]],\n                             [[9, 10], [11, 12]], [[13, 14], [15, 16]]])\n    self.assertAllEqual(b1, b2)\n    self.assertEqual(b1.shape.as_list(), [4, 2, 2])\n    self.assertEqual(b2.shape.as_list(), [4, 2, 2])\n\n    c1 = RaggedTensor.from_uniform_row_length(b1, 2)\n    c2 = RaggedTensor.from_uniform_row_length(b1, 2, 2)\n    self.assertAllEqual(c1, [[[[1, 2], [3, 4]], [[5, 6], [7, 8]]],\n                             [[[9, 10], [11, 12]], [[13, 14], [15, 16]]]])\n    self.assertAllEqual(c1, c2)\n    self.assertEqual(c1.shape.as_list(), [2, 2, 2, 2])\n    self.assertEqual(c2.shape.as_list(), [2, 2, 2, 2])\n\n  def testFromUniformRowLengthWithEmptyValues(self):\n    empty_values = []\n    a = RaggedTensor.from_uniform_row_length(empty_values, 0, nrows=10)\n    self.assertEqual(a.shape.as_list(), [10, 0])\n\n    b = RaggedTensor.from_uniform_row_length(a, 2)\n    self.assertEqual(b.shape.as_list(), [5, 2, 0])\n\n    # Make sure we avoid divide-by-zero when finding nrows for nvals=rowlen=0.\n    c = RaggedTensor.from_uniform_row_length(empty_values, 0)\n    self.assertEqual(c.shape.as_list(), [0, 0])\n    d = RaggedTensor.from_uniform_row_length(empty_values, 0, nrows=0)\n    self.assertEqual(d.shape.as_list(), [0, 0])\n\n  def testFromUniformRowLengthWithPlaceholders(self):\n    ph_values = array_ops.placeholder_with_default([1, 2, 3, 4, 5, 6], [None])\n    ph_rowlen = array_ops.placeholder_with_default(3, None)\n    rt1 = RaggedTensor.from_uniform_row_length(ph_values, 3)\n    rt2 = RaggedTensor.from_uniform_row_length(ph_values, ph_rowlen)\n    rt3 = RaggedTensor.from_uniform_row_length([1, 2, 3, 4, 5, 6], ph_rowlen)\n    self.assertAllEqual(rt1, [[1, 2, 3], [4, 5, 6]])\n    self.assertAllEqual(rt2, [[1, 2, 3], [4, 5, 6]])\n    self.assertAllEqual(rt3, [[1, 2, 3], [4, 5, 6]])\n    if context.executing_eagerly():\n      self.assertEqual(rt1.shape.as_list(), [2, 3])\n      self.assertEqual(rt2.shape.as_list(), [2, 3])\n      self.assertEqual(rt3.shape.as_list(), [2, 3])\n    else:\n      self.assertEqual(rt1.shape.as_list(), [None, 3])\n      self.assertEqual(rt2.shape.as_list(), [None, None])\n      self.assertEqual(rt3.shape.as_list(), [None, None])\n\n    b = RaggedTensor.from_uniform_row_length(rt1, 2)\n    self.assertAllEqual(b, [[[1, 2, 3], [4, 5, 6]]])\n\n    # Make sure we avoid divide-by-zero when finding nrows for nvals=rowlen=0.\n    ph_empty_values = array_ops.placeholder_with_default(\n        array_ops.zeros([0], dtypes.int64), [None])\n    ph_zero = array_ops.placeholder_with_default(0, [])\n    c = RaggedTensor.from_uniform_row_length(ph_empty_values, ph_zero)\n    if context.executing_eagerly():\n      self.assertEqual(c.shape.as_list(), [0, 0])\n    else:\n      self.assertEqual(c.shape.as_list(), [None, None])\n\n  def testFromNestedValueRowIdsWithDerivedNRows(self):\n    values = constant_op.constant(['a', 'b', 'c', 'd', 'e', 'f', 'g'])\n    nested_value_rowids = [\n        constant_op.constant([0, 0, 1, 3, 3], dtypes.int64),\n        constant_op.constant([0, 0, 2, 2, 2, 3, 4], dtypes.int64)\n    ]\n\n    rt = RaggedTensor.from_nested_value_rowids(values, nested_value_rowids)\n    self.assertEqual(rt.dtype, dtypes.string)\n    self.assertEqual(rt.shape.as_list(), [4, None, None])\n    self.assertEqual(rt.ragged_rank, 2)\n\n    rt_values = rt.values\n    rt_value_rowids = rt.value_rowids()\n    rt_values_values = rt_values.values\n    rt_values_value_rowids = rt_values.value_rowids()\n\n    self.assertIs(rt_values_values, values)\n    self.assertAllEqual(rt_value_rowids, nested_value_rowids[0])\n    self.assertAllEqual(rt_values_value_rowids, nested_value_rowids[1])\n    self.assertAllEqual(\n        rt, [[[b'a', b'b'], []], [[b'c', b'd', b'e']], [], [[b'f'], [b'g']]])\n\n  def testFromNestedRowPartitions(self):\n    flat_values = constant_op.constant(['a', 'b', 'c', 'd', 'e', 'f', 'g'])\n    nested_row_splits = [[0, 2, 3, 3, 5], [0, 2, 2, 5, 6, 7]]\n    nested_row_partition = [\n        RowPartition.from_row_splits(constant_op.constant(x, dtypes.int64))\n        for x in nested_row_splits\n    ]\n\n    rt = RaggedTensor._from_nested_row_partitions(\n        flat_values, nested_row_partition, validate=False)\n    self.assertEqual(rt.dtype, dtypes.string)\n    self.assertEqual(rt.shape.as_list(), [4, None, None])\n    self.assertEqual(rt.ragged_rank, 2)\n    self.assertAllEqual(\n        rt, [[[b'a', b'b'], []], [[b'c', b'd', b'e']], [], [[b'f'], [b'g']]])\n\n  def testFromNestedValueRowIdsWithExplicitNRows(self):\n    values = constant_op.constant(['a', 'b', 'c', 'd', 'e', 'f', 'g'])\n    nested_value_rowids = [\n        constant_op.constant([0, 0, 1, 3, 3, 3], dtypes.int64),\n        constant_op.constant([0, 0, 2, 2, 2, 3, 4], dtypes.int64)\n    ]\n    nrows = [\n        constant_op.constant(6, dtypes.int64),\n        constant_op.constant(6, dtypes.int64)\n    ]\n\n    rt = RaggedTensor.from_nested_value_rowids(values, nested_value_rowids,\n                                               nrows)\n    self.assertEqual(rt.dtype, dtypes.string)\n    self.assertEqual(rt.shape.as_list(), [6, None, None])\n    self.assertEqual(rt.ragged_rank, 2)\n\n    rt_values = rt.values\n    rt_value_rowids = rt.value_rowids()\n    rt_nrows = rt.nrows()\n    rt_values_values = rt_values.values\n    rt_values_value_rowids = rt_values.value_rowids()\n    rt_values_nrows = rt_values.nrows()\n\n    self.assertIs(rt_values_values, values)\n    self.assertAllEqual(rt_value_rowids, nested_value_rowids[0])\n    self.assertAllEqual(rt_values_value_rowids, nested_value_rowids[1])\n    self.assertAllEqual(rt_nrows, nrows[0])\n    self.assertAllEqual(rt_values_nrows, nrows[1])\n    self.assertAllEqual(rt, [[[b'a', b'b'], []], [[b'c', b'd', b'e']], [],\n                             [[b'f'], [b'g'], []], [], []])\n\n  def testFromNestedValueRowIdsWithExplicitNRowsMismatch(self):\n    values = constant_op.constant(['a', 'b', 'c', 'd', 'e', 'f', 'g'])\n    nested_value_rowids = [\n        constant_op.constant([0, 0, 1, 3, 3, 3], dtypes.int64),\n        constant_op.constant([0, 0, 2, 2, 2, 3, 4], dtypes.int64)\n    ]\n    nrows = [constant_op.constant(6, dtypes.int64)]\n    with self.assertRaisesRegex(\n        ValueError, 'Argument `nested_nrows` must have the same length as '\n        'argument `nested_value_rowids`'):\n      RaggedTensor.from_nested_value_rowids(values, nested_value_rowids, nrows)\n\n  def testFromNestedValueRowIdsWithNonListInput(self):\n    with self.assertRaisesRegex(\n        TypeError, 'Argument `nested_value_rowids` must be a list of Tensors'):\n      RaggedTensor.from_nested_value_rowids(\n          [1, 2, 3], constant_op.constant([[0, 1, 2], [0, 1, 2]], dtypes.int64))\n    with self.assertRaisesRegex(\n        TypeError, 'Argument `nested_nrows` must be a list of Tensors'):\n      RaggedTensor.from_nested_value_rowids([1, 2, 3], [[0, 1, 2], [0, 1, 2]],\n                                            constant_op.constant([3, 3]))\n\n  def testFromNestedRowSplits(self):\n    flat_values = constant_op.constant(['a', 'b', 'c', 'd', 'e', 'f', 'g'])\n    nested_row_splits = [\n        constant_op.constant([0, 2, 3, 3, 5], dtypes.int64),\n        constant_op.constant([0, 2, 2, 5, 6, 7], dtypes.int64)\n    ]\n\n    rt = RaggedTensor.from_nested_row_splits(\n        flat_values, nested_row_splits, validate=False)\n    self.assertEqual(rt.dtype, dtypes.string)\n    self.assertEqual(rt.shape.as_list(), [4, None, None])\n    self.assertEqual(rt.ragged_rank, 2)\n\n    rt_values = rt.values\n    rt_row_splits = rt.row_splits\n    rt_values_values = rt_values.values\n    rt_values_row_splits = rt_values.row_splits\n\n    self.assertIs(rt_values_values, flat_values)\n    self.assertIs(rt_row_splits, nested_row_splits[0])\n    self.assertIs(rt_values_row_splits, nested_row_splits[1])\n    self.assertAllEqual(\n        rt, [[[b'a', b'b'], []], [[b'c', b'd', b'e']], [], [[b'f'], [b'g']]])\n\n  def testWithRowSplits(self):\n    flat_values = constant_op.constant(['a', 'b', 'c', 'd', 'e', 'f', 'g'])\n    nested_row_splits = [\n        constant_op.constant([0, 2, 3, 3, 5], dtypes.int64),\n        constant_op.constant([0, 2, 2, 5, 6, 7], dtypes.int64)\n    ]\n\n    rt = RaggedTensor.from_nested_row_splits(\n        flat_values, nested_row_splits, validate=False)\n\n    rt = rt.with_row_splits_dtype(dtypes.int32)\n\n    self.assertEqual(rt.dtype, dtypes.string)\n    self.assertEqual(rt.shape.as_list(), [4, None, None])\n    self.assertEqual(rt.ragged_rank, 2)\n\n    rt_values = rt.values\n    rt_row_splits = rt.row_splits\n    rt_values_values = rt_values.values\n    rt_values_row_splits = rt_values.row_splits\n\n    self.assertAllEqual(rt_values_values, flat_values)\n    self.assertAllEqual(rt_row_splits, nested_row_splits[0])\n    self.assertAllEqual(rt_values_row_splits, nested_row_splits[1])\n    self.assertAllEqual(\n        rt, [[[b'a', b'b'], []], [[b'c', b'd', b'e']], [], [[b'f'], [b'g']]])\n\n  def testFromNestedRowSplitsWithNonListInput(self):\n    with self.assertRaisesRegex(\n        TypeError, '`nested_row_splits` must be a list of Tensors'):\n      RaggedTensor.from_nested_row_splits(\n          [1, 2], constant_op.constant([[0, 1, 2], [0, 1, 2]], dtypes.int64))\n\n  def testFromValueRowIdsWithBadNRows(self):\n    values = constant_op.constant(['a', 'b', 'c', 'd', 'e', 'f', 'g'])\n    value_rowids = constant_op.constant([0, 0, 2, 2, 2, 3, 4], dtypes.int64)\n    nrows = constant_op.constant(5, dtypes.int64)\n\n    with self.assertRaisesRegex(ValueError, r'Expected nrows >= 0; got -2'):\n      RaggedTensor.from_value_rowids(\n          values=values,\n          value_rowids=array_ops.placeholder_with_default(value_rowids, None),\n          nrows=-2)\n\n    with self.assertRaisesRegex(\n        ValueError, r'Expected nrows >= value_rowids\\[-1\\] \\+ 1; got nrows=2, '\n        r'value_rowids\\[-1\\]=4'):\n      RaggedTensor.from_value_rowids(\n          values=values, value_rowids=value_rowids, nrows=2)\n\n    with self.assertRaisesRegex(\n        ValueError, r'Expected nrows >= value_rowids\\[-1\\] \\+ 1; got nrows=4, '\n        r'value_rowids\\[-1\\]=4'):\n      RaggedTensor.from_value_rowids(\n          values=values, value_rowids=value_rowids, nrows=4)\n\n    with self.assertRaisesRegex(ValueError, r'Shape \\(7, 1\\) must have rank 1'):\n      RaggedTensor.from_value_rowids(\n          values=values,\n          value_rowids=array_ops.expand_dims(value_rowids, 1),\n          nrows=nrows)\n\n    with self.assertRaisesRegex(ValueError, r'Shape \\(1,\\) must have rank 0'):\n      RaggedTensor.from_value_rowids(\n          values=values,\n          value_rowids=value_rowids,\n          nrows=array_ops.expand_dims(nrows, 0))\n\n  def testCondWithTensorsFromValueIds(self):\n    # b/141166460\n    rt = RaggedTensor.from_value_rowids([1, 2, 3], [0, 0, 2])\n    c = array_ops.placeholder_with_default(True, None)\n    result = control_flow_ops.cond(c, lambda: rt, lambda: rt)\n    self.assertAllEqual(rt, result)\n\n  def testGraphMismatch(self):\n    if not context.executing_eagerly():\n      with ops.Graph().as_default():\n        values = constant_op.constant([1, 2, 3], dtypes.int64)\n      with ops.Graph().as_default():\n        splits = constant_op.constant([0, 2, 3], dtypes.int64)\n      with self.assertRaisesRegex(ValueError,\n                                  '.* must be from the same graph as .*'):\n        RaggedTensor.from_row_splits(values, splits)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name='Rank0',\n          tensor='a'),\n      dict(\n          testcase_name='Rank1',\n          tensor=['a', 'b']),\n  ])\n  def testFromTensorRankError(self, tensor):\n    with self.assertRaisesRegex(ValueError, 'must be greater than 1'):\n      RaggedTensor.from_tensor(tensor)\n\n  #=============================================================================\n  # Ragged Value & Row-Partitioning Tensor Accessors\n  #=============================================================================\n\n  def testRaggedTensorAccessors_2d(self):\n    values = constant_op.constant(['a', 'b', 'c', 'd', 'e', 'f', 'g'])\n    row_splits = constant_op.constant([0, 2, 2, 5, 6, 7], dtypes.int64)\n    value_rowids = constant_op.constant([0, 0, 2, 2, 2, 3, 4], dtypes.int64)\n    rt1 = RaggedTensor.from_row_splits(values, row_splits)\n    rt2 = RaggedTensor.from_value_rowids(values, value_rowids)\n\n    for rt in [rt1, rt2]:\n      self.assertAllEqual(\n          rt, [[b'a', b'b'], [], [b'c', b'd', b'e'], [b'f'], [b'g']])\n      self.assertAllEqual(rt.values, [b'a', b'b', b'c', b'd', b'e', b'f', b'g'])\n      self.assertEqual(rt.values.shape.dims[0].value, 7)\n      self.assertAllEqual(rt.value_rowids(), [0, 0, 2, 2, 2, 3, 4])\n      self.assertAllEqual(rt.nrows(), 5)\n      self.assertAllEqual(rt.row_splits, [0, 2, 2, 5, 6, 7])\n      self.assertAllEqual(rt.row_starts(), [0, 2, 2, 5, 6])\n      self.assertAllEqual(rt.row_limits(), [2, 2, 5, 6, 7])\n      self.assertAllEqual(rt.row_lengths(), [2, 0, 3, 1, 1])\n      self.assertAllEqual(rt.flat_values,\n                          [b'a', b'b', b'c', b'd', b'e', b'f', b'g'])\n      self.assertLen(rt.nested_row_splits, 1)\n      self.assertAllEqual(rt.nested_row_splits[0], [0, 2, 2, 5, 6, 7])\n\n  def testRaggedTensorAccessors_3d_with_ragged_rank_1(self):\n    values = [[0, 1], [2, 3], [4, 5], [6, 7], [8, 9], [10, 11], [12, 13]]\n    row_splits = constant_op.constant([0, 2, 2, 5, 6, 7], dtypes.int64)\n    value_rowids = constant_op.constant([0, 0, 2, 2, 2, 3, 4], dtypes.int64)\n    row_lengths = constant_op.constant([2, 0, 3, 1, 1])\n    rt1 = RaggedTensor.from_row_splits(values, row_splits)\n    rt2 = RaggedTensor.from_value_rowids(values, value_rowids)\n    rt3 = RaggedTensor.from_row_lengths(values, row_lengths)\n\n    for rt in [rt1, rt2, rt3]:\n      self.assertAllEqual(rt, [[[0, 1], [2, 3]], [], [[4, 5], [6, 7], [8, 9]],\n                               [[10, 11]], [[12, 13]]])\n      self.assertAllEqual(\n          rt.values,\n          [[0, 1], [2, 3], [4, 5], [6, 7], [8, 9], [10, 11], [12, 13]])\n      self.assertEqual(rt.values.shape.dims[0].value, 7)\n      self.assertAllEqual(rt.value_rowids(), [0, 0, 2, 2, 2, 3, 4])\n      self.assertAllEqual(rt.nrows(), 5)\n      self.assertAllEqual(rt.row_splits, [0, 2, 2, 5, 6, 7])\n      self.assertAllEqual(rt.row_starts(), [0, 2, 2, 5, 6])\n      self.assertAllEqual(rt.row_limits(), [2, 2, 5, 6, 7])\n      self.assertAllEqual(rt.row_lengths(), [2, 0, 3, 1, 1])\n      self.assertAllEqual(\n          rt.row_lengths(axis=2), [[2, 2], [], [2, 2, 2], [2], [2]])\n      self.assertAllEqual(\n          rt.flat_values,\n          [[0, 1], [2, 3], [4, 5], [6, 7], [8, 9], [10, 11], [12, 13]])\n      self.assertLen(rt.nested_row_splits, 1)\n      self.assertAllEqual(rt.nested_row_splits[0], [0, 2, 2, 5, 6, 7])\n      self.assertLen(rt.nested_value_rowids(), 1)\n\n      self.assertAllEqual(rt.nested_value_rowids()[0], [0, 0, 2, 2, 2, 3, 4])\n\n  def testRaggedTensorAccessors_3d_with_ragged_rank_2(self):\n    values = constant_op.constant(['a', 'b', 'c', 'd', 'e', 'f', 'g'])\n    nested_row_splits = [\n        constant_op.constant([0, 2, 3, 3, 5], dtypes.int64),\n        constant_op.constant([0, 2, 2, 5, 6, 7], dtypes.int64)\n    ]\n    nested_value_rowids = [\n        constant_op.constant([0, 0, 1, 3, 3], dtypes.int64),\n        constant_op.constant([0, 0, 2, 2, 2, 3, 4], dtypes.int64)\n    ]\n    rt1 = RaggedTensor.from_nested_row_splits(values, nested_row_splits)\n    rt2 = RaggedTensor.from_nested_value_rowids(values, nested_value_rowids)\n\n    for rt in [rt1, rt2]:\n      self.assertAllEqual(\n          rt, [[[b'a', b'b'], []], [[b'c', b'd', b'e']], [], [[b'f'], [b'g']]])\n      self.assertAllEqual(\n          rt.values, [[b'a', b'b'], [], [b'c', b'd', b'e'], [b'f'], [b'g']])\n      self.assertEqual(rt.values.shape.dims[0].value, 5)\n      self.assertAllEqual(rt.value_rowids(), [0, 0, 1, 3, 3])\n      self.assertAllEqual(rt.nrows(), 4)\n      self.assertAllEqual(rt.row_splits, [0, 2, 3, 3, 5])\n      self.assertAllEqual(rt.row_starts(), [0, 2, 3, 3])\n      self.assertAllEqual(rt.row_limits(), [2, 3, 3, 5])\n      self.assertAllEqual(rt.row_lengths(), [2, 1, 0, 2])\n      self.assertAllEqual(rt.flat_values,\n                          [b'a', b'b', b'c', b'd', b'e', b'f', b'g'])\n      self.assertLen(rt.nested_row_splits, 2)\n      self.assertAllEqual(rt.nested_row_splits[0], [0, 2, 3, 3, 5])\n      self.assertAllEqual(rt.nested_row_splits[1], [0, 2, 2, 5, 6, 7])\n      self.assertLen(rt.nested_value_rowids(), 2)\n      self.assertAllEqual(rt.nested_value_rowids()[0], [0, 0, 1, 3, 3])\n      self.assertAllEqual(rt.nested_value_rowids()[1], [0, 0, 2, 2, 2, 3, 4])\n\n  #=============================================================================\n  # RaggedTensor.shape\n  #=============================================================================\n\n  def testShape(self):\n    \"\"\"Tests for RaggedTensor.shape.\"\"\"\n    rt1 = RaggedTensor.from_row_splits(b'a b c d e f g'.split(),\n                                       [0, 2, 5, 6, 6, 7])\n    self.assertEqual(rt1.shape.as_list(), [5, None])\n\n    rt2 = RaggedTensor.from_row_splits(\n        [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12], [13, 14]],\n        [0, 2, 5, 6, 6, 7])\n    self.assertEqual(rt2.shape.as_list(), [5, None, 2])\n\n    rt3 = RaggedTensor.from_row_splits(\n        [[[1, 2], [3, 4]], [[5, 6], [7, 8]], [[9, 10], [11, 12]]], [0, 2, 2, 3])\n    self.assertEqual(rt3.shape.as_list(), [3, None, 2, 2])\n\n    rt4 = RaggedTensor.from_row_splits(rt3, [0, 1, 3, 3])\n    self.assertEqual(rt4.shape.as_list(), [3, None, None, 2, 2])\n\n    if not context.executing_eagerly():\n      rt5 = RaggedTensor.from_row_splits(\n          array_ops.placeholder(dtype=dtypes.string), [0, 2, 3, 5])\n      self.assertIsNone(rt5.shape.ndims)\n\n      rt6 = RaggedTensor.from_row_splits(\n          [1, 2, 3], array_ops.placeholder(dtype=dtypes.int64))\n      self.assertEqual(rt6.shape.as_list(), [None, None])\n\n  def testGetShape(self):\n    rt = RaggedTensor.from_row_splits(b'a b c d e f g'.split(),\n                                      [0, 2, 5, 6, 6, 7])\n    self.assertEqual(rt.shape.as_list(), rt.get_shape().as_list())\n\n  #=============================================================================\n  # RaggedTensor.__str__\n  #=============================================================================\n  def testRaggedTensorStr(self):\n    values = [b'a', b'b', b'c', b'd', b'e', b'f', b'g']\n    row_splits = [0, 2, 5, 6, 6, 7]\n    rt = RaggedTensor.from_row_splits(values, row_splits, validate=False)\n    splits_type = 'int64'\n    if context.executing_eagerly():\n      expected_repr = '<tf.RaggedTensor {}>'.format([[b'a', b'b'],\n                                                     [b'c', b'd', b'e'], [b'f'],\n                                                     [], [b'g']])\n    else:\n      expected_repr = (\n          'tf.RaggedTensor(values=Tensor(\"RaggedFromRowSplits/values:0\", '\n          'shape=(7,), dtype=string), '\n          'row_splits=Tensor('\n          '\"RaggedFromRowSplits/RowPartitionFromRowSplits/row_splits:0\",'\n          ' shape=(6,), dtype={}))').format(splits_type)\n    self.assertEqual(repr(rt), expected_repr)\n    self.assertEqual(str(rt), expected_repr)\n\n  def testRaggedTensorValueStr(self):\n    values = [b'a', b'b', b'c', b'd', b'e', b'f', b'g']\n    row_splits = [0, 2, 5, 6, 6, 7]\n    rt = ragged_tensor_value.RaggedTensorValue(\n        np.array(values), np.array(row_splits, dtype=np.int64))\n    expected_str = '<tf.RaggedTensorValue {}>'.format([[b'a', b'b'],\n                                                       [b'c', b'd', b'e'],\n                                                       [b'f'], [], [b'g']])\n    expected_repr = (\"tf.RaggedTensorValue(values=array({}, dtype='|S1'), \"\n                     'row_splits=array({}))'.format(values, row_splits))\n    self.assertEqual(' '.join(str(rt).split()), expected_str)\n    self.assertEqual(' '.join(repr(rt).split()), expected_repr)\n\n  def testRaggedTensorStrWithZeroSizeInnerShape(self):\n    # Tests that b/226112826 is fixed.\n    if context.executing_eagerly():\n      rt = RaggedTensor.from_row_lengths(array_ops.zeros([9, 0]), [4, 3, 2])\n      expected_repr = (\n          '<tf.RaggedTensor [[[], [], [], []], [[], [], []], [[], []]]>')\n      self.assertEqual(' '.join(repr(rt).split()), expected_repr)\n\n  #=============================================================================\n  # RaggedTensor.with_values() and RaggedTensor.with_flat_values().\n  #=============================================================================\n\n  def testWithValues(self):\n    rt1 = ragged_factory_ops.constant([[1, 2], [3, 4, 5], [6], [], [7]])\n    rt2 = ragged_factory_ops.constant([[[1, 2], [3, 4, 5]], [[6]], [], [[],\n                                                                        [7]]])\n\n    rt1_plus_10 = rt1.with_values(rt1.values + 10)\n    rt2_times_10 = rt2.with_flat_values(rt2.flat_values * 10)\n    rt1_expanded = rt1.with_values(array_ops.expand_dims(rt1.values, axis=1))\n\n    self.assertAllEqual(rt1_plus_10, [[11, 12], [13, 14, 15], [16], [], [17]])\n    self.assertAllEqual(rt2_times_10,\n                        [[[10, 20], [30, 40, 50]], [[60]], [], [[], [70]]])\n    self.assertAllEqual(rt1_expanded,\n                        [[[1], [2]], [[3], [4], [5]], [[6]], [], [[7]]])\n\n  #=============================================================================\n  # Session.run\n  #=============================================================================\n  def testSessionRun(self):\n    if context.executing_eagerly():\n      return\n\n    rt1 = ragged_factory_ops.constant([[1, 2, 3], [4]])\n    rt2 = ragged_factory_ops.constant([[[], [1, 2]], [[3]]])\n    with self.test_session() as session:\n      result = session.run({'rt1': rt1, 'rt2': rt2})\n      self.assertCountEqual(result.keys(), ['rt1', 'rt2'])\n      self.assertEqual(result['rt1'].to_list(), [[1, 2, 3], [4]])\n      self.assertEqual(result['rt2'].to_list(), [[[], [1, 2]], [[3]]])\n\n  def testSessionRunFeed(self):\n    if context.executing_eagerly():\n      return\n\n    rt1 = RaggedTensor.from_row_splits(\n        array_ops.placeholder(dtypes.int32),\n        array_ops.placeholder(dtypes.int64))\n    rt2 = RaggedTensor.from_nested_row_splits(\n        array_ops.placeholder(dtypes.int32), [\n            array_ops.placeholder(dtypes.int64),\n            array_ops.placeholder(dtypes.int64)\n        ])\n\n    rt1_feed_val = ragged_factory_ops.constant_value([[1, 2, 3], [4]])\n    rt2_feed_val = ragged_factory_ops.constant_value([[[], [1, 2]], [[3]]])\n\n    with self.test_session() as session:\n      fetches = {'rt1': rt1, 'rt2': rt2}\n      feeds = {rt1: rt1_feed_val, rt2: rt2_feed_val}\n      result = session.run(fetches, feed_dict=feeds)\n      self.assertCountEqual(result.keys(), ['rt1', 'rt2'])\n      self.assertEqual(result['rt1'].to_list(), [[1, 2, 3], [4]])\n      self.assertEqual(result['rt2'].to_list(), [[[], [1, 2]], [[3]]])\n\n  def testSessionPartialRunFeed(self):\n    if context.executing_eagerly():\n      return\n\n    # Placeholder inputs.\n    a = RaggedTensor.from_row_splits(\n        array_ops.placeholder(dtypes.int32, shape=[None], name='a.values'),\n        array_ops.placeholder(dtypes.int64, name='a.row_splits'))\n    b = RaggedTensor.from_row_splits(\n        array_ops.placeholder(dtypes.int32, shape=[None], name='b.values'),\n        array_ops.placeholder(dtypes.int64, name='b.row_splits'))\n    c = array_ops.placeholder(dtypes.int32, shape=[], name='c')\n\n    # Feed values for placeholder inputs.\n    a_val = ragged_factory_ops.constant_value([[1, 2, 3], [4]])\n    b_val = ragged_factory_ops.constant_value([[5, 4, 3], [2]])\n    c_val = 3\n\n    # Compute some values.\n    r1 = ragged_math_ops.reduce_sum(a * b, axis=1)\n    r2 = ragged_math_ops.reduce_sum(a + c, axis=1)\n\n    with self.test_session() as session:\n      handle = session.partial_run_setup([r1, r2], [a, b, c])\n\n      res1 = session.partial_run(handle, r1, feed_dict={a: a_val, b: b_val})\n      self.assertAllEqual(res1, [22, 8])\n\n      res2 = session.partial_run(handle, r2, feed_dict={c: c_val})\n      self.assertAllEqual(res2, [15, 7])\n\n  # Test case for GitHub issue 24679.\n  def testEagerForLoop(self):\n    if not context.executing_eagerly():\n      return\n\n    values = [[1., 2.], [3., 4., 5.], [6.]]\n    r = ragged_factory_ops.constant(values)\n    i = 0\n    for elem in r:\n      self.assertAllEqual(elem, values[i])\n      i += 1\n\n  def testConsumers(self):\n    if context.executing_eagerly():\n      return\n\n    a = RaggedTensor.from_row_splits(\n        array_ops.placeholder(dtypes.int32, shape=[None], name='a.values'),\n        array_ops.placeholder(dtypes.int64, name='a.row_splits'),\n        validate=False)\n    ragged_math_ops.reduce_sum(a)\n    self.assertLen(a.consumers(), 1)\n\n  @parameterized.parameters([\n      {\n          'descr': 'from_value_rowids',\n          'factory': RaggedTensor.from_value_rowids,\n          'test': RaggedTensor.value_rowids,\n          'values': {\n              'values': [1, 2, 3, 4, 5, 6],\n              'value_rowids': [0, 0, 1, 1, 2, 2],\n          },\n          'tensor_field': 'value_rowids',\n          'value_rowids': [0, 1, 2],\n          'nrows': 10\n      },\n      {\n          'descr': 'from_row_splits',\n          'factory': RaggedTensor.from_row_splits,\n          # row_splits is a property, not a function.\n          'test': (lambda rt: rt.row_splits),\n          'values': {\n              'values': [1, 2, 3, 4, 5, 6],\n              'row_splits': [0, 2, 4, 6],\n          },\n          'tensor_field': 'row_splits',\n          'row_splits': [0, 1, 2, 3]\n      },\n      {\n          'descr': 'from_row_lengths',\n          'factory': RaggedTensor.from_row_lengths,\n          'test': RaggedTensor.row_lengths,\n          'values': {\n              'values': [1, 2, 3, 4, 5, 6],\n              'row_lengths': [2, 2, 2],\n          },\n          'tensor_field': 'row_lengths',\n          'row_lengths': [1, 1, 1],\n      },\n      # from_row_starts\n      {\n          'descr': 'from_row_starts',\n          'factory': RaggedTensor.from_row_starts,\n          'test': RaggedTensor.row_starts,\n          'values': {\n              'values': [1, 2, 3, 4, 5, 6],\n              'row_starts': [0, 2, 4]\n          },\n          'tensor_field': 'row_starts',\n          'row_starts': [0, 1, 2]\n      },\n      # from_row_limits\n      {\n          'descr': 'from_row_limits',\n          'factory': RaggedTensor.from_row_limits,\n          'test': RaggedTensor.row_limits,\n          'values': {\n              'values': [1, 2, 3, 4, 5, 6],\n              'row_limits': [2, 4, 6]\n          },\n          'tensor_field': 'row_limits',\n          'row_limits': [3]\n      },\n      # from_uniform_row_length\n      {\n          'descr': 'from_uniform_row_length',\n          'factory': RaggedTensor.from_uniform_row_length,\n          # One cannot extract uniform_row_length or nvals, so we return\n          # nvals//nrows = uniform_row_length, where nvals = 3\n          'test': (lambda rt: 3 // (rt.shape[0])),\n          'values': {\n              'values': [1, 2, 3, 4, 5, 6],\n              'uniform_row_length': 2\n          },\n          'tensor_field': 'uniform_row_length',\n          'uniform_row_length': 3\n      },\n  ])\n  def testFactoryTypePreference(self, descr, test, factory, values,\n                                tensor_field, **kwargs):\n    # When input tensors have shape information, some of these errors will be\n    # detected statically.\n    def op_cast(k, v):\n      if k == tensor_field:\n        return constant_op.constant(v, dtype=dtypes.int32)\n      else:\n        return v\n\n    value_copy = {k: op_cast(k, v) for k, v in values.items()}\n    rt = factory(**value_copy)\n\n    kw_copy = {k: v for k, v in kwargs.items()}\n    kw_copy['values'] = rt\n    rt2 = factory(**kw_copy)\n    self.assertAllEqual(kwargs[tensor_field], test(rt2))\n\n  @parameterized.parameters([\n      # from_value_rowids\n      {\n          'descr': 'bad rank for value_rowids',\n          'factory': RaggedTensor.from_value_rowids,\n          'values': [[1, 2], [3, 4]],\n          'value_rowids': [[1, 2], [3, 4]],\n          'nrows': 10\n      },\n      {\n          'descr': 'bad rank for nrows',\n          'factory': RaggedTensor.from_value_rowids,\n          'values': [1, 2, 3, 4],\n          'value_rowids': [1, 2, 3, 4],\n          'nrows': [10]\n      },\n      {\n          'descr': 'len(values) != len(value_rowids)',\n          'factory': RaggedTensor.from_value_rowids,\n          'values': [1, 2, 3, 4],\n          'value_rowids': [1, 2, 3, 4, 5],\n          'nrows': 10\n      },\n      {\n          'descr': 'negative value_rowid',\n          'factory': RaggedTensor.from_value_rowids,\n          'values': [1, 2, 3, 4],\n          'value_rowids': [-5, 2, 3, 4],\n          'nrows': 10\n      },\n      {\n          'descr': 'non-monotonic-increasing value_rowid',\n          'factory': RaggedTensor.from_value_rowids,\n          'values': [1, 2, 3, 4],\n          'value_rowids': [4, 3, 2, 1],\n          'nrows': 10\n      },\n      {\n          'descr': 'value_rowid > nrows',\n          'factory': RaggedTensor.from_value_rowids,\n          'values': [1, 2, 3, 4],\n          'value_rowids': [1, 2, 3, 4],\n          'nrows': 2\n      },\n      {\n          'descr': 'bad rank for values',\n          'factory': RaggedTensor.from_value_rowids,\n          'values': 10,\n          'value_rowids': [1, 2, 3, 4],\n          'nrows': 10\n      },\n\n      # from_row_splits\n      {\n          'descr': 'bad rank for row_splits',\n          'factory': RaggedTensor.from_row_splits,\n          'values': [[1, 2], [3, 4]],\n          'row_splits': [[1, 2], [3, 4]]\n      },\n      {\n          'descr': 'row_splits[0] != 0',\n          'factory': RaggedTensor.from_row_splits,\n          'values': [1, 2, 3, 4],\n          'row_splits': [2, 3, 4]\n      },\n      {\n          'descr': 'non-monotonic-increasing row_splits',\n          'factory': RaggedTensor.from_row_splits,\n          'values': [1, 2, 3, 4],\n          'row_splits': [0, 3, 2, 4]\n      },\n      {\n          'descr': 'row_splits[0] != nvals',\n          'factory': RaggedTensor.from_row_splits,\n          'values': [1, 2, 3, 4],\n          'row_splits': [0, 2, 3, 5]\n      },\n      {\n          'descr': 'bad rank for values',\n          'factory': RaggedTensor.from_row_splits,\n          'values': 10,\n          'row_splits': [0, 1]\n      },\n\n      # from_row_lengths\n      {\n          'descr': 'bad rank for row_lengths',\n          'factory': RaggedTensor.from_row_lengths,\n          'values': [1, 2, 3, 4],\n          'row_lengths': [[1, 2], [1, 0]]\n      },\n      {\n          'descr': 'negatve row_lengths',\n          'factory': RaggedTensor.from_row_lengths,\n          'values': [1, 2, 3, 4],\n          'row_lengths': [3, -1, 2]\n      },\n      {\n          'descr': 'sum(row_lengths) != nvals',\n          'factory': RaggedTensor.from_row_lengths,\n          'values': [1, 2, 3, 4],\n          'row_lengths': [2, 4, 2, 8]\n      },\n      {\n          'descr': 'bad rank for values',\n          'factory': RaggedTensor.from_row_lengths,\n          'values': 10,\n          'row_lengths': [0, 1]\n      },\n\n      # from_row_starts\n      {\n          'descr': 'bad rank for row_starts',\n          'factory': RaggedTensor.from_row_starts,\n          'values': [[1, 2], [3, 4]],\n          'row_starts': [[1, 2], [3, 4]]\n      },\n      {\n          'descr': 'row_starts[0] != 0',\n          'factory': RaggedTensor.from_row_starts,\n          'values': [1, 2, 3, 4],\n          'row_starts': [2, 3, 4]\n      },\n      {\n          'descr': 'non-monotonic-increasing row_starts',\n          'factory': RaggedTensor.from_row_starts,\n          'values': [1, 2, 3, 4],\n          'row_starts': [0, 3, 2, 4]\n      },\n      {\n          'descr': 'row_starts[0] > nvals',\n          'factory': RaggedTensor.from_row_starts,\n          'values': [1, 2, 3, 4],\n          'row_starts': [0, 2, 3, 5]\n      },\n      {\n          'descr': 'bad rank for values',\n          'factory': RaggedTensor.from_row_starts,\n          'values': 10,\n          'row_starts': [0, 1]\n      },\n\n      # from_row_limits\n      {\n          'descr': 'bad rank for row_limits',\n          'factory': RaggedTensor.from_row_limits,\n          'values': [[1, 2], [3, 4]],\n          'row_limits': [[1, 2], [3, 4]]\n      },\n      {\n          'descr': 'row_limits[0] < 0',\n          'factory': RaggedTensor.from_row_limits,\n          'values': [1, 2, 3, 4],\n          'row_limits': [-1, 3, 4]\n      },\n      {\n          'descr': 'non-monotonic-increasing row_limits',\n          'factory': RaggedTensor.from_row_limits,\n          'values': [1, 2, 3, 4],\n          'row_limits': [0, 3, 2, 4]\n      },\n      {\n          'descr': 'row_limits[0] != nvals',\n          'factory': RaggedTensor.from_row_limits,\n          'values': [1, 2, 3, 4],\n          'row_limits': [0, 2, 3, 5]\n      },\n      {\n          'descr': 'bad rank for values',\n          'factory': RaggedTensor.from_row_limits,\n          'values': 10,\n          'row_limits': [0, 1]\n      },\n\n      # from_uniform_row_length\n      {\n          'descr': 'rowlen * nrows != nvals (1)',\n          'factory': RaggedTensor.from_uniform_row_length,\n          'values': [1, 2, 3, 4, 5],\n          'uniform_row_length': 3\n      },\n      {\n          'descr': 'rowlen * nrows != nvals (2)',\n          'factory': RaggedTensor.from_uniform_row_length,\n          'values': [1, 2, 3, 4, 5],\n          'uniform_row_length': 6\n      },\n      {\n          'descr': 'rowlen * nrows != nvals (3)',\n          'factory': RaggedTensor.from_uniform_row_length,\n          'values': [1, 2, 3, 4, 5, 6],\n          'uniform_row_length': 3,\n          'nrows': 3\n      },\n      {\n          'descr': 'rowlen must be a scalar',\n          'factory': RaggedTensor.from_uniform_row_length,\n          'values': [1, 2, 3, 4],\n          'uniform_row_length': [2]\n      },\n      {\n          'descr': 'rowlen must be nonnegative',\n          'factory': RaggedTensor.from_uniform_row_length,\n          'values': [1, 2, 3, 4],\n          'uniform_row_length': -1\n      },\n  ])\n  def testFactoryValidation(self, descr, factory, **kwargs):\n    # When input tensors have shape information, some of these errors will be\n    # detected statically.\n    with self.assertRaises((errors.InvalidArgumentError, ValueError)):\n      self.evaluate(factory(**kwargs))\n\n    # Remove shape information (by wrapping tensors in placeholders), and check\n    # that we detect the errors when the graph is run.\n    if not context.executing_eagerly():\n\n      def wrap_arg(v):\n        return array_ops.placeholder_with_default(\n            constant_op.constant(v, dtype=dtypes.int64),\n            tensor_shape.TensorShape(None))\n\n      kwargs = dict((k, wrap_arg(v)) for (k, v) in kwargs.items())\n\n      with self.assertRaises(errors.InvalidArgumentError):\n        self.evaluate(factory(**kwargs))\n\n  #=============================================================================\n  # RaggedTensor Variant conversion\n  #=============================================================================\n\n  @parameterized.named_parameters(\n      {\n          'testcase_name': 'Shape_5_none',\n          'ragged_constant': [[1, 2], [3, 4, 5], [6], [], [7]],\n          'ragged_rank': 1\n      }, {\n          'testcase_name': 'Shape_4_none_2',\n          'ragged_constant': [[[1, 2]], [], [[3, 4]], []],\n          'ragged_rank': 1\n      }, {\n          'testcase_name': 'Shape_1_none_none',\n          'ragged_constant': [[[1], [2, 3, 4, 5, 6, 7]], [[]]],\n          'ragged_rank': 2\n      })\n  def testRaggedToVariant(self, ragged_constant, ragged_rank):\n    rt = ragged_factory_ops.constant(ragged_constant, ragged_rank=ragged_rank)\n    et = rt._to_variant()\n    self.assertEqual(et.shape.as_list(), [])\n    self.assertEqual(et.dtype, dtypes.variant)\n\n  @parameterized.parameters(\n      {\n          'ragged_constant': [[1, 2], [3, 4, 5], [6], [], [7]],\n          'ragged_rank': 1,\n          'num_batched_elems': 5\n      }, {\n          'ragged_constant': [[[1, 2]], [], [[3, 4]], []],\n          'ragged_rank': 1,\n          'num_batched_elems': 4\n      }, {\n          'ragged_constant': [[[1], [2, 3, 4, 5, 6, 7]], [[]]],\n          'ragged_rank': 2,\n          'num_batched_elems': 2\n      })\n  def testRaggedToBatchedVariant(self, ragged_constant, ragged_rank,\n                                 num_batched_elems):\n    rt = ragged_factory_ops.constant(ragged_constant, ragged_rank=ragged_rank)\n    et = rt._to_variant(batched_input=True)\n    self.assertEqual(et.shape.as_list(), [num_batched_elems])\n    self.assertEqual(et.dtype, dtypes.variant)\n\n  @parameterized.parameters(\n      # 2D test cases.\n      {\n          'ragged_constant': [[]],\n          'ragged_rank': 1,\n      },\n      {\n          'ragged_constant': [[1]],\n          'ragged_rank': 1,\n      },\n      {\n          'ragged_constant': [[1, 2]],\n          'ragged_rank': 1,\n      },\n      {\n          'ragged_constant': [[1], [2], [3]],\n          'ragged_rank': 1,\n      },\n      {\n          'ragged_constant': [[1, 2, 3], [4, 5, 6], [7, 8, 9]],\n          'ragged_rank': 1,\n      },\n      {\n          'ragged_constant': [[1, 2], [3, 4, 5], [6], [], [7]],\n          'ragged_rank': 1,\n      },\n      # 3D test cases.\n      {\n          'ragged_constant': [[[]]],\n          'ragged_rank': 2,\n      },\n      {\n          'ragged_constant': [[[1]]],\n          'ragged_rank': 2,\n      },\n      {\n          'ragged_constant': [[[1, 2]]],\n          'ragged_rank': 2,\n      },\n      {\n          'ragged_constant': [[[1, 2], [3, 4]]],\n          'ragged_rank': 2,\n      },\n      {\n          'ragged_constant': [[[1, 2]], [[3, 4]], [[5, 6]], [[7, 8]]],\n          'ragged_rank': 2,\n      },\n      {\n          'ragged_constant': [[[1], [2]], [[3], [4]], [[5], [6]], [[7], [8]]],\n          'ragged_rank': 2,\n      },\n      {\n          'ragged_constant': [[[1, 2]], [], [[3, 4]], []],\n          'ragged_rank': 2,\n      },\n      # 4D test cases.\n      {\n          'ragged_constant': [[[[1, 2], [3, 4]]],\n                              [[[0, 0], [0, 0]], [[5, 6], [7, 8]]], []],\n          'ragged_rank': 3,\n      },\n      # dtype `string`.\n      {\n          'ragged_constant': [['a'], ['b'], ['c']],\n          'ragged_rank': 1,\n          'dtype': dtypes.string,\n      },\n      {\n          'ragged_constant': [[['a', 'b'], ['c', 'd']]],\n          'ragged_rank': 2,\n          'dtype': dtypes.string,\n      },\n      {\n          'ragged_constant': [[[['a', 'b'], ['c', 'd']]],\n                              [[['e', 'f'], ['g', 'h']], [['i', 'j'],\n                                                          ['k', 'l']]], []],\n          'ragged_rank': 3,\n          'dtype': dtypes.string,\n      })\n  def testVariantRoundTrip(self,\n                           ragged_constant,\n                           ragged_rank,\n                           dtype=dtypes.int32):\n    rt = ragged_factory_ops.constant(\n        ragged_constant, ragged_rank=ragged_rank, dtype=dtype)\n    et = rt._to_variant()\n    round_trip_rt = RaggedTensor._from_variant(\n        et, dtype, output_ragged_rank=ragged_rank)\n    self.assertAllEqual(rt, round_trip_rt)\n\n  def testBatchedVariantRoundTripInputRaggedRankInferred(self):\n    ragged_rank = 1\n    rt = ragged_factory_ops.constant(\n        [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9]],\n        ragged_rank=ragged_rank)\n    batched_variant = rt._to_variant(batched_input=True)\n    nested_batched_variant = array_ops.reshape(batched_variant, [5, 2])\n    decoded_rt = RaggedTensor._from_variant(\n        nested_batched_variant,\n        dtype=dtypes.int32,\n        output_ragged_rank=ragged_rank + 1)\n    expected_rt = ragged_factory_ops.constant([[[0], [1]], [[2], [3]], [[4],\n                                                                        [5]],\n                                               [[6], [7]], [[8], [9]]])\n    self.assertAllEqual(decoded_rt, expected_rt)\n\n  def testBatchedVariantRoundTripWithInputRaggedRank(self):\n    ragged_rank = 1\n    rt = ragged_factory_ops.constant(\n        [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9]],\n        ragged_rank=ragged_rank)\n    batched_variant = rt._to_variant(batched_input=True)\n    nested_batched_variant = array_ops.reshape(batched_variant, [5, 2])\n    decoded_rt = RaggedTensor._from_variant(\n        nested_batched_variant,\n        dtype=dtypes.int32,\n        output_ragged_rank=ragged_rank + 1,\n        input_ragged_rank=ragged_rank - 1)\n    expected_rt = ragged_factory_ops.constant([[[0], [1]], [[2], [3]], [[4],\n                                                                        [5]],\n                                               [[6], [7]], [[8], [9]]])\n    self.assertAllEqual(decoded_rt, expected_rt)\n\n  def testUnbatchVariant(self):  # b/141789000\n    rt = ragged_factory_ops.constant([[1, 2, 3], [4, 5], [], [6, 7, 8, 9]])\n    batched = rt._to_variant(batched_input=True)\n    for i in range(4):\n      row = RaggedTensor._from_variant(\n          batched[i], dtype=dtypes.int32, output_ragged_rank=0)\n      self.assertAllEqual(rt[i], row)\n\n  def testUnbatchVariantInDataset(self):\n    rt = ragged_factory_ops.constant([[1, 2, 3], [4, 5], [], [6, 7, 8, 9]])\n    ds = dataset_ops.Dataset.from_tensor_slices(rt)\n    if context.executing_eagerly():\n      for i, value in enumerate(ds):\n        self.assertAllEqual(rt[i], value)\n    else:\n      it = dataset_ops.make_one_shot_iterator(ds)\n      out = it.get_next()\n      with self.cached_session() as sess:\n        for i in range(3):\n          self.assertAllEqual(sess.run(rt[i]), out)\n\n  def testFromVariantInvalidParams(self):\n    rt = ragged_factory_ops.constant([[0], [1], [2], [3]])\n    batched_variant = rt._to_variant(batched_input=True)\n    nested_batched_variant = array_ops.reshape(batched_variant, [2, 2])\n    with self.assertRaisesRegex(ValueError,\n                                r'`output_ragged_rank` \\(1\\) must be equal to'):\n      RaggedTensor._from_variant(\n          nested_batched_variant,\n          dtype=dtypes.int32,\n          output_ragged_rank=1,\n          input_ragged_rank=1)\n\n  def testUnbatchToTensor(self):\n    batched = ragged_factory_ops.constant([[0], [1], [2], [3]])\n    unbatched = [constant_op.constant(x) for x in [[0], [1], [2], [3]]]\n    batched_spec = type_spec.type_spec_from_value(batched)\n\n    # Note that the unbatched_spec is derived from the batched spec, so it can\n    # add back a ragged instead of a dense tensor.\n    unbatched_spec = batched_spec._unbatch()\n    batched_tensor_list = batched_spec._to_batched_tensor_list(batched)\n    unbatched_tensor_lists = zip(\n        *[array_ops.unstack(tensor) for tensor in batched_tensor_list])\n    actual_unbatched = [\n        batched_spec._unbatch()._from_tensor_list(tensor_list)\n        for tensor_list in unbatched_tensor_lists]\n    self.assertLen(actual_unbatched, len(unbatched))\n    for x in actual_unbatched:\n      self.assertTrue(unbatched_spec.is_compatible_with(x))\n\n    for (actual, expected) in zip(actual_unbatched, unbatched):\n      self.assertAllEqual(actual, expected)\n\n  def testDatasetUnbatchTwice(self):\n    batched = ragged_factory_ops.constant([[[0], [1], [5]], [[2], [3]]])\n    ds = dataset_ops.Dataset.from_tensors(batched)\n    ds2 = ds.unbatch()\n    ds3 = ds2.unbatch()\n    if context.executing_eagerly():\n      value = next(iter(ds3))\n      self.assertAllEqual([0], value)\n\n  def testDatasetUnbatchToScalar(self):\n    batched = ragged_factory_ops.constant([[0], [1], [2], [3]])\n    ds = dataset_ops.Dataset.from_tensors(batched)\n    ds2 = ds.unbatch()\n    ds3 = ds2.unbatch()\n    if context.executing_eagerly():\n      value = next(iter(ds3))\n      self.assertAllEqual(0, value)\n\n  def testBatchToTensor(self):\n    batched = ragged_factory_ops.constant([[0], [1], [2], [3]])\n    unbatched = [constant_op.constant(x) for x in [[0], [1], [2], [3]]]\n    batched_spec = type_spec.type_spec_from_value(batched)\n\n    # Note that the unbatched_spec is derived from the batched spec, so it can\n    # add back a ragged instead of a dense tensor.\n    unbatched_spec = batched_spec._unbatch()\n    unbatched_tensor_lists = [unbatched_spec._to_tensor_list(x)\n                              for x in unbatched]\n    batched_tensor_list = [array_ops.stack(tensors)\n                           for tensors in zip(*unbatched_tensor_lists)]\n    actual_batched = unbatched_spec._batch(4)._from_tensor_list(\n        batched_tensor_list)\n    self.assertAllEqual(actual_batched, batched)\n\n  def _testGradient(self, func, x, expected_grad, grad_y=None):\n    x = ragged_factory_ops.constant(x)\n    if grad_y is not None:\n      grad_y = ragged_factory_ops.constant(grad_y)\n    if context.executing_eagerly():\n      with backprop.GradientTape() as t:\n        t.watch(x)\n        y = func(x)\n      g = t.gradient(y, x, grad_y)\n    else:\n      y = func(x)\n      g = gradients_impl.gradients(ys=y, xs=x, grad_ys=grad_y)[0]\n    if expected_grad is None:\n      self.assertIsNone(g)\n    else:\n      g = ragged_tensor.convert_to_tensor_or_ragged_tensor(g)\n      self.assertAllClose(g, expected_grad)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name='RaggedInput',\n          func=lambda x: math_ops.reduce_prod(x, axis=1),\n          x=[[1., 2.], [3.]],\n          expected=[[2., 1.], [1.]]),\n      dict(\n          testcase_name='RaggedOutput',\n          func=lambda x: ragged_concat_ops.stack([x, x[:1]]),\n          x=[3., 2.],\n          expected=[2., 1.]),\n      dict(\n          testcase_name='RaggedInputAndOutput',\n          func=lambda x: array_ops.stack([x, x * x]),\n          x=[[1., 2.], [3.]],\n          expected=[[3., 5.], [7.]]),\n      dict(\n          testcase_name='RaggedOutputWithGradYs',\n          func=lambda x: ragged_concat_ops.stack([x, x[:1]]),\n          x=[3., 2.],\n          grad_ys=[[1., 1.], [1.]],\n          expected=[2., 1.]),\n      dict(\n          testcase_name='RaggedInputAndOutputWithGradYs',\n          func=lambda x: array_ops.stack([x, x * x]),\n          x=[[1., 2.], [3.]],\n          grad_ys=[[[1., 1.], [1.]], [[1., 1.], [1.]]],\n          expected=[[3., 5.], [7.]]),\n      dict(\n          testcase_name='RaggedRank3',\n          func=lambda x: ragged_concat_ops.stack([x, (x * x)[:, 1:]]),\n          x=[[[1., 2.], [3., 4., 5.]], [[6.]]],\n          expected=[[[1.0, 1.0], [7.0, 9.0, 11.0]], [[1.0]]]),\n      dict(\n          testcase_name='RaggedIndexedSlices',\n          func=lambda x: ragged_gather_ops.gather(x, [0, 2]),\n          x=[[1., 2.], [3.], [4., 5., 6.]],\n          expected=[[1., 1.], [0.], [1., 1., 1.]]),\n  ])\n  def testGradient(self, func, x, expected, grad_ys=None):\n    self._testGradient(func, x, expected, grad_ys)\n\n  def testHigherOrderGradient(self):\n    x = ragged_factory_ops.constant([[1.0, 2.0], [3.0]])\n\n    with backprop.GradientTape() as t2:\n      t2.watch(x)\n      with backprop.GradientTape() as t1:\n        t1.watch(x)\n        y = x * x * x\n      dy_dx = t1.gradient(y, x)\n    d2y_dx2 = t2.gradient(dy_dx, x)\n\n    self.assertAllEqual(dy_dx, [[3.0, 12.0], [27.0]])\n    self.assertAllEqual(d2y_dx2, [[6.0, 12.0], [18.0]])\n\n  def testUnconnectedGradient(self):\n    x = ragged_factory_ops.constant([[1.0, 2.0], [3.0]])\n\n    with backprop.GradientTape() as t:\n      t.watch(x)\n      y = ragged_factory_ops.constant([[2.0, 4.0], [6.0]])\n    self.assertIsNone(t.gradient(y, x))\n\n  def testStopGradient(self):\n\n    def func(x):\n      y = x * constant_op.constant([[1.], [3.]])\n      y = y.with_values(array_ops.stop_gradient(y.values))\n      z = x * y\n      return math_ops.reduce_sum(z)\n\n    self._testGradient(func, [[1., 2.], [3., 4., 5.]],\n                       [[1., 2.], [9., 12., 15.]])\n\n  def testStopGradientNoneComponent(self):\n\n    def func(x):\n      y = x * constant_op.constant([[1.], [3.]])\n      y = y.with_values(array_ops.stop_gradient(y.values))\n      return y\n\n    self._testGradient(func, [[1., 2], [3, 4, 5]], None)\n\n  def testRaggedVariantGradients(self):\n\n    def func(x):\n      rt1 = RaggedTensor.from_row_splits(values=x, row_splits=[0, 4, 7, 8])\n      rt2 = rt1 * [[10], [100], [1000]]\n      v = rt2._to_variant(batched_input=False)\n      rt3 = RaggedTensor._from_variant(v, dtype=rt2.dtype, output_ragged_rank=1)\n      return rt3.flat_values\n\n    self._testGradient(func, [3.0, 1.0, 4.0, 1.0, 1.0, 0.0, 2.0, 1.0],\n                       [10., 10., 10., 10., 100., 100., 100., 1000.])\n\n  def testRaggedVariantGradientsEmptyRows(self):\n\n    def func(x):\n      rt1 = RaggedTensor.from_row_splits(\n          values=x, row_splits=[0, 2, 2, 4, 7, 7, 8])\n      rt2 = rt1 * [[10], [20], [30], [40], [50], [60]]\n      v = rt2._to_variant(batched_input=False)\n      rt3 = RaggedTensor._from_variant(v, dtype=rt2.dtype, output_ragged_rank=1)\n      return rt3.flat_values\n\n    self._testGradient(func, [3.0, 1.0, 4.0, 1.0, 1.0, 0.0, 2.0, 1.0],\n                       [10., 10., 30., 30., 40., 40., 40., 60.])\n\n  def testRaggedVariantSteps(self):\n    x = [3.0, 1.0, 4.0, 1.0, 1.0, 0.0, 2.0, 1.0]\n    rt1 = RaggedTensor.from_row_splits(values=x, row_splits=[0, 4, 7, 8])\n    rt2 = rt1 * [[10], [100], [1000]]\n    v = rt2._to_variant(batched_input=False)\n    rt3 = RaggedTensor._from_variant(v, dtype=rt2.dtype, output_ragged_rank=1)\n    self.assertAllClose([30., 10., 40., 10., 100., 0., 200., 1000.],\n                        rt3.flat_values)\n\n  def testRaggedVariantGradientsBatched(self):\n\n    def func(x):\n      rt1 = RaggedTensor.from_row_splits(values=x, row_splits=[0, 4, 7, 8])\n      rt2 = rt1 * [[10], [100], [1000]]\n      v = rt2._to_variant(batched_input=True)\n      rt3 = RaggedTensor._from_variant(v, dtype=rt2.dtype, output_ragged_rank=1)\n      return rt3.flat_values\n\n    self._testGradient(func, [3.0, 1.0, 4.0, 1.0, 1.0, 0.0, 2.0, 1.0],\n                       [10., 10., 10., 10., 100., 100., 100., 1000.])\n\n  def testRaggedVariantGradientsEmptyRowsBatched(self):\n\n    def func(x):\n      rt1 = RaggedTensor.from_row_splits(\n          values=x, row_splits=[0, 2, 2, 4, 7, 7, 8])\n      rt2 = rt1 * [[10], [20], [30], [40], [50], [60]]\n      v = rt2._to_variant(batched_input=True)\n      rt3 = RaggedTensor._from_variant(v, dtype=rt2.dtype, output_ragged_rank=1)\n      return rt3.flat_values\n\n    self._testGradient(func, [3.0, 1.0, 4.0, 1.0, 1.0, 0.0, 2.0, 1.0],\n                       [10., 10., 30., 30., 40., 40., 40., 60.])\n\n  def testRaggedVariantGradientsEmptyOutputBatched(self):\n\n    def func(x):\n      rt1 = RaggedTensor.from_row_splits(\n          values=x, row_splits=[0, 0, 0, 0, 0, 0, 0])\n      rt2 = rt1 * [[10], [20], [30], [40], [50], [60]]\n      v = rt2._to_variant(batched_input=True)\n      rt3 = RaggedTensor._from_variant(v, dtype=rt2.dtype, output_ragged_rank=1)\n      return rt3.flat_values\n\n    self._testGradient(func, [], [])\n\n  def testRaggedVariantGradientsBatchedAndSliced(self):\n\n    def func(x, i):\n      rt1 = RaggedTensor.from_row_splits(values=x, row_splits=[0, 4, 7, 8])\n      rt2 = rt1 * [[10], [100], [1000]]\n      v_slice = rt2._to_variant(batched_input=True)[i]\n      return RaggedTensor._from_variant(\n          v_slice, dtype=rt2.dtype, output_ragged_rank=0)\n\n    self._testGradient(\n        functools.partial(func, i=0), [3.0, 1.0, 4.0, 1.0, 1.0, 0.0, 2.0, 1.0],\n        [10., 10., 10., 10., 0., 0., 0., 0.])\n    self._testGradient(\n        functools.partial(func, i=1), [3.0, 1.0, 4.0, 1.0, 1.0, 0.0, 2.0, 1.0],\n        [0., 0., 0., 0., 100., 100., 100., 0.])\n    self._testGradient(\n        functools.partial(func, i=2), [3.0, 1.0, 4.0, 1.0, 1.0, 0.0, 2.0, 1.0],\n        [0., 0., 0., 0., 0., 0., 0., 1000.])\n\n  def testRaggedVariantGradientsEmptyRowsBatchedAndSliced(self):\n\n    def func(x, i):\n      rt1 = RaggedTensor.from_row_splits(\n          values=x, row_splits=[0, 2, 2, 4, 7, 7, 8])\n      rt2 = rt1 * [[10], [20], [30], [40], [50], [60]]\n      v_slice = rt2._to_variant(batched_input=True)[i]\n      return RaggedTensor._from_variant(\n          v_slice, dtype=rt2.dtype, output_ragged_rank=0)\n\n    self._testGradient(\n        functools.partial(func, i=0), [3.0, 1.0, 4.0, 1.0, 1.0, 0.0, 2.0, 1.0],\n        [10., 10., 0., 0., 0., 0., 0., 0.])\n    self._testGradient(\n        functools.partial(func, i=1), [3.0, 1.0, 4.0, 1.0, 1.0, 0.0, 2.0, 1.0],\n        [0., 0., 0., 0., 0., 0., 0., 0.])\n    self._testGradient(\n        functools.partial(func, i=2), [3.0, 1.0, 4.0, 1.0, 1.0, 0.0, 2.0, 1.0],\n        [0., 0., 30., 30., 0., 0., 0., 0.])\n    self._testGradient(\n        functools.partial(func, i=3), [3.0, 1.0, 4.0, 1.0, 1.0, 0.0, 2.0, 1.0],\n        [0., 0., 0., 0., 40., 40., 40., 0.])\n    self._testGradient(\n        functools.partial(func, i=4), [3.0, 1.0, 4.0, 1.0, 1.0, 0.0, 2.0, 1.0],\n        [0., 0., 0., 0., 0., 0., 0., 0.])\n    self._testGradient(\n        functools.partial(func, i=5), [3.0, 1.0, 4.0, 1.0, 1.0, 0.0, 2.0, 1.0],\n        [0., 0., 0., 0., 0., 0., 0., 60.])\n\n  def testRaggedVariantGradientsRaggedRank0(self):\n\n    def func(x):\n      x2 = x * 2\n      v = gen_ragged_conversion_ops.ragged_tensor_to_variant(\n          [], x2, batched_input=False)\n      return RaggedTensor._from_variant(v, dtype=x2.dtype, output_ragged_rank=0)\n\n    self._testGradient(func, [3.0, 1.0, 4.0, 1.0, 1.0, 0.0, 2.0, 1.0],\n                       [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0])\n\n  def testRaggedVariantGradientsRaggedRank3(self):\n\n    def func(x):\n      x2 = x * 2\n      rt1 = RaggedTensor.from_nested_row_splits(\n          x2, ([0, 0, 3], [0, 2, 2, 3], [0, 4, 7, 8]))\n      v = rt1._to_variant(batched_input=False)\n      rt3 = RaggedTensor._from_variant(v, dtype=x2.dtype, output_ragged_rank=3)\n      return rt3.flat_values\n\n    self._testGradient(func, [3.0, 1.0, 4.0, 1.0, 1.0, 0.0, 2.0, 1.0],\n                       [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0])\n\n  def testRaggedVariantGradientsViaMapFn(self):\n    rt = RaggedTensor.from_row_splits(\n        values=[3, 1.0, 4, 1, 5, 9, 2, 6], row_splits=[0, 4, 7, 8])\n\n    def func(x):\n\n      def transform_row(row):\n        return math_ops.sqrt(\n            math_ops.reduce_mean(math_ops.square(row * x), keepdims=True))\n\n      return math_ops.reduce_sum(map_fn.map_fn(transform_row, rt))\n\n    self._testGradient(func, 3.0, 14.653377)\n\n  def testRaggedVariantGradientsEmptyRowsViaMapFn(self):\n    rt = RaggedTensor.from_row_splits(\n        values=[3, 1.0, 4, 1, 5, 9, 2, 6], row_splits=[0, 2, 2, 4, 7, 7, 8])\n\n    def func(x):\n\n      def transform_row(row):\n        return math_ops.sqrt(\n            math_ops.reduce_mean(math_ops.square(row * x), keepdims=True))\n\n      return math_ops.reduce_sum(map_fn.map_fn(transform_row, rt))\n\n    self._testGradient(func, 3.0, 17.206844)\n\n  def testRaggedVariantGradientsEmptyOutputViaMapFn(self):\n    rt = RaggedTensor.from_row_splits(\n        values=[], row_splits=[0, 0, 0, 0])\n\n    def func(x):\n\n      def transform_row(row):\n        return math_ops.sqrt(\n            math_ops.reduce_mean(math_ops.square(row * x), keepdims=True))\n\n      return math_ops.reduce_sum(map_fn.map_fn(transform_row, rt))\n\n    self._testGradient(func, 3.0, 0.0)\n\n  def testRaggedVariantGradientsViaMapFnReduce(self):\n\n    def func(x):\n      rt1 = RaggedTensor.from_row_splits(values=x, row_splits=[0, 4, 7, 8])\n      return map_fn.map_fn(\n          math_ops.reduce_max,\n          rt1,\n          fn_output_signature=tensor_spec.TensorSpec((), x.dtype))\n\n    self._testGradient(func, [3.0, 1.0, 4.0, 1.0, 1.0, 0.0, 2.0, 1.0],\n                       [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0])\n\n  def testRaggedVariantGradientsEmptyRowsViaMapFnReduce(self):\n\n    def func(x):\n      rt1 = RaggedTensor.from_row_splits(\n          values=x, row_splits=[0, 2, 2, 4, 7, 7, 8])\n      return map_fn.map_fn(\n          math_ops.reduce_max,\n          rt1,\n          fn_output_signature=tensor_spec.TensorSpec((), x.dtype))\n\n    self._testGradient(func, [3.0, 1.0, 4.0, 1.0, 1.0, 0.0, 2.0, 1.0],\n                       [1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0])\n\n  def testRaggedVariantGradientsEmptyOutputViaMapFnReduce(self):\n\n    def func(x):\n      rt1 = RaggedTensor.from_row_splits(\n          values=x, row_splits=[0, 0, 0, 0])\n      return map_fn.map_fn(\n          math_ops.reduce_max,\n          rt1,\n          fn_output_signature=tensor_spec.TensorSpec((), x.dtype))\n\n    self._testGradient(func, [], [])\n\n  def testRaggedVariantGradientsErrors(self):\n    if context.executing_eagerly():\n      return\n\n    rt = RaggedTensor.from_row_splits([1.0, 2.0], row_splits=[0, 2, 2])\n    v1 = rt._to_variant()\n    v2 = array_ops.stack([array_ops.stack([v1])])\n    y = RaggedTensor._from_variant(v2, rt.dtype, output_ragged_rank=3)\n\n    with self.assertRaisesRegex(\n        ValueError, 'Unable to compute gradient: RaggedTensorToVariant '\n        'can currently only generate 0D or 1D output.'):\n      gradients_impl.gradients(ys=y.flat_values, xs=rt.flat_values)\n\n  def assertNumpyObjectTensorsRecursivelyEqual(self, a, b, msg):\n    \"\"\"Check that two numpy arrays are equal.\n\n    For arrays with dtype=object, check values recursively to see if a and b\n    are equal.  (c.f. `np.array_equal`, which checks dtype=object values using\n    object identity.)\n\n    Args:\n      a: A numpy array.\n      b: A numpy array.\n      msg: Message to display if a != b.\n    \"\"\"\n    if isinstance(a, np.ndarray) and a.dtype == object:\n      self.assertEqual(a.dtype, b.dtype, msg)\n      self.assertEqual(a.shape, b.shape, msg)\n      self.assertLen(a, len(b), msg)\n      for a_val, b_val in zip(a, b):\n        self.assertNumpyObjectTensorsRecursivelyEqual(a_val, b_val, msg)\n    else:\n      self.assertAllEqual(a, b, msg)\n\n  @parameterized.named_parameters([\n      ('Shape_2_R',\n       [[1, 2], [3, 4, 5]],\n       np.array([int32array([1, 2]), int32array([3, 4, 5])])),\n      ('Shape_2_2',\n       [[1, 2], [3, 4]],\n       np.array([[1, 2], [3, 4]])),\n      ('Shape_2_R_2',\n       [[[1, 2], [3, 4]], [[5, 6]]],\n       np.array([int32array([[1, 2], [3, 4]]), int32array([[5, 6]])])),\n      ('Shape_3_2_R',\n       [[[1], []], [[2, 3], [4]], [[], [5, 6, 7]]],\n       np.array([[int32array([1]), int32array([])],\n                 [int32array([2, 3]), int32array([4])],\n                 [int32array([]), int32array([5, 6, 7])]])),\n      ('Shape_0_R',\n       ragged_factory_ops.constant_value([], ragged_rank=1, dtype=np.int32),\n       np.zeros([0, 0], dtype=np.int32)),\n      ('Shape_0_R_2',\n       ragged_factory_ops.constant_value([], ragged_rank=1,\n                                         inner_shape=(2,), dtype=np.int32),\n       np.zeros([0, 0, 2], dtype=np.int32)),\n  ])  # pyformat: disable\n  def testRaggedTensorNumpy(self, rt, expected):\n    if isinstance(rt, list):\n      rt = ragged_factory_ops.constant(rt, dtype=dtypes.int32)\n    else:\n      rt = ragged_tensor.convert_to_tensor_or_ragged_tensor(rt)\n    if context.executing_eagerly():\n      actual = rt.numpy()\n      self.assertNumpyObjectTensorsRecursivelyEqual(\n          expected, actual, 'Expected %r, got %r' % (expected, actual))\n    else:\n      with self.assertRaisesRegex(ValueError, 'only supported in eager mode'):\n        rt.numpy()\n\n  @parameterized.parameters([\n      ([[[1, 2], [3, 4, 5]], [[6]]], 2, None),\n      ([[[1, 2], [3, 4, 5]], [[6]]], 2, [None, None, None]),\n      ([[[1, 2], [3, 4, 5]], [[6]]], 2, [2, None, None]),\n      ([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9]]], 1, None),\n      ([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9]]], 1, [None, None, None]),\n      ([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9]]], 1, [2, None, None]),\n      ([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9]]], 1, [2, None, 3]),\n      ([[[1, 2, 3]]], 1, [1, 1, None]),\n      ([[[1, 2, 3]]], 1, [1, 1, 3]),\n  ])\n  def testRaggedTensorSetShape(self, rt, rt_ragged_rank, shape):\n    rt1 = ragged_factory_ops.constant(rt, ragged_rank=rt_ragged_rank)\n    rt1._set_shape(shape)\n    rt1.shape.assert_is_compatible_with(shape)\n    if shape is not None:\n      self.assertIsNot(rt1.shape.rank, None)\n      for a, b in zip(rt1.shape, shape):\n        if b is not None:\n          self.assertEqual(a, b)\n\n  @parameterized.parameters([\n      ([[[1, 2], [3, 4, 5]], [[6]]], 2, None),\n      ([[[1, 2], [3, 4, 5]], [[6]]], 2, [None, None, None]),\n      ([[[1, 2], [3, 4, 5]], [[6]]], 2, [2, None, None]),\n      ([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9]]], 1, None),\n      ([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9]]], 1, [None, None, None]),\n      ([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9]]], 1, [2, None, None]),\n      ([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9]]], 1, [2, None, 3]),\n      ([[[1, 2, 3]]], 1, [1, 1, None]),\n      ([[[1, 2, 3]]], 1, [1, 1, 3]),\n  ])\n  def testRaggedTensorSetShapeWithPlaceholders(self, rt, rt_ragged_rank, shape):\n    rt2 = nest.map_structure(\n        lambda x: array_ops.placeholder_with_default(x, None),\n        ragged_factory_ops.constant(rt, ragged_rank=rt_ragged_rank),\n        expand_composites=True)\n    rt2._set_shape(shape)\n    rt2.shape.assert_is_compatible_with(shape)\n    if shape is not None:\n      self.assertIsNot(rt2.shape.rank, None)\n      for a, b in zip(rt2.shape, shape):\n        if b is not None:\n          self.assertEqual(a, b)\n\n  def testRaggedTensorSetShapeUniformRowLength(self):\n    rt = [[[1], [2], [3]], [[4], [5], [6]]]\n\n    rt1 = RaggedTensor.from_tensor(rt, ragged_rank=1)\n    rt1._set_shape([2, 3, 1])\n\n    rt2 = nest.map_structure(\n        lambda x: array_ops.placeholder_with_default(x, None),\n        rt1,\n        expand_composites=True)\n    rt2._set_shape([2, 3, 1])\n\n  def testRaggedTensorSetShapeInconsistentShapeError(self):\n    rt = RaggedTensor.from_tensor([[[1], [2], [3]], [[4], [5], [6]]],\n                                  ragged_rank=1)\n    self.assertEqual(rt.shape.as_list(), [2, 3, 1])\n    with self.assertRaises(ValueError):\n      rt._set_shape([None, None, 5])\n    with self.assertRaisesRegex(ValueError, 'Inconsistent size'):\n      rt._set_shape([None, 5, None])\n    with self.assertRaises(ValueError):\n      rt._set_shape([5, None, None])\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass RaggedTensorSpecTest(test_util.TensorFlowTestCase,\n                           parameterized.TestCase):\n\n  def assertAllTensorsEqual(self, list1, list2):\n    self.assertLen(list1, len(list2))\n    for (t1, t2) in zip(list1, list2):\n      self.assertAllEqual(t1, t2)\n\n  def testConstruction(self):\n    spec1 = RaggedTensorSpec(ragged_rank=1)\n    self.assertIsNone(spec1._shape.rank)\n    self.assertEqual(spec1._dtype, dtypes.float32)\n    self.assertEqual(spec1._row_splits_dtype, dtypes.int64)\n    self.assertEqual(spec1._ragged_rank, 1)\n\n    self.assertIsNone(spec1.shape.rank)\n    self.assertEqual(spec1.dtype, dtypes.float32)\n    self.assertEqual(spec1.row_splits_dtype, dtypes.int64)\n    self.assertEqual(spec1.ragged_rank, 1)\n\n    spec2 = RaggedTensorSpec(shape=[None, None, None])\n    self.assertEqual(spec2._shape.as_list(), [None, None, None])\n    self.assertEqual(spec2._dtype, dtypes.float32)\n    self.assertEqual(spec2._row_splits_dtype, dtypes.int64)\n    self.assertEqual(spec2._ragged_rank, 2)\n\n    with self.assertRaisesRegex(ValueError, 'Must specify ragged_rank'):\n      RaggedTensorSpec()\n    with self.assertRaisesRegex(TypeError, '`ragged_rank` must be an int'):\n      RaggedTensorSpec(ragged_rank=constant_op.constant(1))\n    with self.assertRaisesRegex(\n        ValueError,\n        r'Argument `ragged_rank` \\(2\\) must be less than rank \\(2\\).'):\n      RaggedTensorSpec(ragged_rank=2, shape=[None, None])\n\n  def testValueType(self):\n    spec1 = RaggedTensorSpec(ragged_rank=1)\n    self.assertEqual(spec1.value_type, RaggedTensor)\n    spec2 = RaggedTensorSpec(ragged_rank=0)\n    self.assertEqual(spec2.value_type, ops.Tensor)\n\n  @parameterized.parameters([\n      (RaggedTensorSpec(ragged_rank=1),\n       (tensor_shape.TensorShape(None), dtypes.float32, 1, dtypes.int64)),\n      (RaggedTensorSpec(shape=[5, None, None]),\n       (tensor_shape.TensorShape([5, None, None]), dtypes.float32,\n        2, dtypes.int64)),\n      (RaggedTensorSpec(shape=[5, None, None], dtype=dtypes.int32),\n       (tensor_shape.TensorShape([5, None, None]), dtypes.int32, 2,\n        dtypes.int64)),\n      (RaggedTensorSpec(ragged_rank=1, row_splits_dtype=dtypes.int32),\n       (tensor_shape.TensorShape(None), dtypes.float32, 1, dtypes.int32)),\n  ])  # pyformat: disable\n  def testSerialize(self, rt_spec, expected):\n    serialization = rt_spec._serialize()\n    # TensorShape has an unconventional definition of equality, so we can't use\n    # assertEqual directly here.  But repr() is deterministic and lossless for\n    # the expected values, so we can use that instead.\n    self.assertEqual(repr(serialization), repr(expected))\n\n  @parameterized.parameters([\n      (RaggedTensorSpec(ragged_rank=0, shape=[5, 3]), [\n          tensor_spec.TensorSpec([5, 3], dtypes.float32),\n      ]),\n      (RaggedTensorSpec(ragged_rank=1), [\n          tensor_spec.TensorSpec(None, dtypes.float32),\n          tensor_spec.TensorSpec([None], dtypes.int64)\n      ]),\n      (RaggedTensorSpec(ragged_rank=1, row_splits_dtype=dtypes.int32), [\n          tensor_spec.TensorSpec(None, dtypes.float32),\n          tensor_spec.TensorSpec([None], dtypes.int32),\n      ]),\n      (RaggedTensorSpec(ragged_rank=2), [\n          tensor_spec.TensorSpec(None, dtypes.float32),\n          tensor_spec.TensorSpec([None], dtypes.int64),\n          tensor_spec.TensorSpec([None], dtypes.int64),\n      ]),\n      (RaggedTensorSpec(shape=[5, None, None], dtype=dtypes.string), [\n          tensor_spec.TensorSpec([None], dtypes.string),\n          tensor_spec.TensorSpec([6], dtypes.int64),\n          tensor_spec.TensorSpec([None], dtypes.int64),\n      ]),\n  ])\n  def testComponentSpecs(self, rt_spec, expected):\n    self.assertEqual(rt_spec._component_specs, expected)\n\n  @parameterized.parameters([\n      {\n          'rt_spec': RaggedTensorSpec(ragged_rank=0),\n          'rt': [1.0, 2.0, 3.0],\n          'components': [[1.0, 2.0, 3.0]]\n      },\n      {\n          'rt_spec': RaggedTensorSpec(ragged_rank=1),\n          'rt': [[1.0, 2.0], [3.0]],\n          'components': [[1.0, 2.0, 3.0], [0, 2, 3]]\n      },\n      {\n          'rt_spec': RaggedTensorSpec(shape=[2, None, None]),\n          'rt': [[[1.0, 2.0], [3.0]], [[], [4.0]]],\n          'components': [[1.0, 2.0, 3.0, 4.0], [0, 2, 4], [0, 2, 3, 3, 4]]\n      },\n  ])\n  def testToFromComponents(self, rt_spec, rt, components):\n    rt = ragged_factory_ops.constant(rt)\n    actual_components = rt_spec._to_components(rt)\n    self.assertAllTensorsEqual(actual_components, components)\n    rt_reconstructed = rt_spec._from_components(actual_components)\n    self.assertAllEqual(rt, rt_reconstructed)\n\n  @parameterized.parameters([\n      {\n          'flat_value_spec': tensor_spec.TensorSpec(None, dtypes.float32),\n          'row_splits_spec': tensor_spec.TensorSpec(None, dtypes.int64),\n      },\n      {\n          'flat_value_spec': tensor_spec.TensorSpec([None,], dtypes.float32),\n          'row_splits_spec': tensor_spec.TensorSpec(None, dtypes.int64),\n      },\n      {\n          'flat_value_spec': tensor_spec.TensorSpec(None, dtypes.float32),\n          'row_splits_spec': tensor_spec.TensorSpec([None,], dtypes.int64),\n      },\n      {\n          'flat_value_spec': tensor_spec.TensorSpec([None,], dtypes.float32),\n          'row_splits_spec': tensor_spec.TensorSpec([None,], dtypes.int64),\n      },\n      {\n          'flat_value_spec': tensor_spec.TensorSpec([4,], dtypes.float32),\n          'row_splits_spec': tensor_spec.TensorSpec(None, dtypes.int64),\n      },\n      {\n          'flat_value_spec': tensor_spec.TensorSpec(None, dtypes.float32),\n          'row_splits_spec': tensor_spec.TensorSpec([3,], dtypes.int64),\n      },\n  ])\n  def testToFromComponentsStaticUnknownShape(self, flat_value_spec,\n                                             row_splits_spec):\n    rt_spec = RaggedTensorSpec(shape=[2, None], ragged_rank=1)\n    tester = self\n\n    @def_function.function(input_signature=[flat_value_spec, row_splits_spec])\n    def test_fn(flat_value, row_splits):\n      # Apply static shape information saved in rt_spec to rt.\n      rt = rt_spec._from_components([flat_value, row_splits])\n      tester.assertEqual(rt.shape.as_list(), [2, None])\n      return rt + ragged_factory_ops.constant([[1.0, 1.0, 1.0], [1.0]])\n\n    result = test_fn([1.0, 2.0, 3.0, 4.0], [0, 3, 4])\n    expected_result = ragged_factory_ops.constant([[2.0, 3.0, 4.0], [5.0]])\n    self.assertAllEqual(result, expected_result)\n\n  @test_util.run_v1_only('RaggedTensorValue is deprecated in v2')\n  def testFromNumpyComponents(self):\n    spec1 = RaggedTensorSpec(ragged_rank=1, dtype=dtypes.int32)\n    rt1 = spec1._from_components([np.array([1, 2, 3]), np.array([0, 2, 3])])\n    self.assertIsInstance(rt1, ragged_tensor_value.RaggedTensorValue)\n    self.assertAllEqual(rt1, [[1, 2], [3]])\n\n    spec2 = RaggedTensorSpec(ragged_rank=2, dtype=dtypes.int32)\n    rt2 = spec2._from_components(\n        [np.array([1, 2, 3]),\n         np.array([0, 2, 3]),\n         np.array([0, 0, 2, 3])])\n    self.assertIsInstance(rt2, ragged_tensor_value.RaggedTensorValue)\n    self.assertAllEqual(rt2, [[[], [1, 2]], [[3]]])\n\n    spec3 = RaggedTensorSpec(ragged_rank=0, dtype=dtypes.int32)\n    rt3 = spec3._from_components([np.array([1, 2, 3])])\n    self.assertIsInstance(rt3, np.ndarray)\n    self.assertAllEqual(rt3, [1, 2, 3])\n\n  @parameterized.parameters([\n      RaggedTensorSpec(ragged_rank=0, shape=[5, 3]),\n      RaggedTensorSpec(ragged_rank=1),\n      RaggedTensorSpec(ragged_rank=1, row_splits_dtype=dtypes.int32),\n      RaggedTensorSpec(ragged_rank=2, dtype=dtypes.string),\n      RaggedTensorSpec(shape=[5, None, None]),\n  ])\n  def testFlatTensorSpecs(self, rt_spec):\n    self.assertEqual(rt_spec._flat_tensor_specs,\n                     [tensor_spec.TensorSpec(None, dtypes.variant)])\n\n  @parameterized.parameters([\n      (dtypes.float32, full_type_pb2.TFT_FLOAT),\n      (dtypes.string, full_type_pb2.TFT_STRING),\n  ])\n  def testFullTypesForFlatTensors(self, dt, ft):\n    rt_spec = RaggedTensorSpec(ragged_rank=2, dtype=dt)\n    full_type_list = fulltypes_for_flat_tensors(rt_spec)\n    expect = [\n        full_type_pb2.FullTypeDef(\n            type_id=full_type_pb2.TFT_RAGGED,\n            args=[full_type_pb2.FullTypeDef(type_id=ft)])\n    ]\n    self.assertEqual(len(rt_spec._flat_tensor_specs), len(full_type_list))\n    self.assertEqual(expect, full_type_list)\n\n  @parameterized.named_parameters([\n      {\n          'testcase_name': 'RaggedRank0',\n          'rt_spec': RaggedTensorSpec(ragged_rank=0),\n          'rt': [1.0, 2.0, 3.0],\n      },\n      {\n          'testcase_name': 'RaggedRank1',\n          'rt_spec': RaggedTensorSpec(ragged_rank=1),\n          'rt': [[1.0, 2.0], [3.0]]\n      },\n      {\n          'testcase_name': 'RaggedRank2',\n          'rt_spec': RaggedTensorSpec(shape=[2, None, None]),\n          'rt': [[[1.0, 2.0], [3.0]], [[], [4.0]]]\n      },\n  ])\n  def testToFromTensorList(self, rt_spec, rt):\n    rt = ragged_factory_ops.constant(rt)\n    tensor_list = rt_spec._to_tensor_list(rt)\n    rt_reconstructed = rt_spec._from_tensor_list(tensor_list)\n    self.assertAllEqual(rt, rt_reconstructed)\n\n  @parameterized.named_parameters([\n      # TODO(b/141789000) Test ragged_rank=0 when support is added.\n      {\n          'testcase_name': 'RaggedRank1',\n          'rt_spec': RaggedTensorSpec(ragged_rank=1),\n          'rt': [[1.0, 2.0], [3.0]]\n      },\n      {\n          'testcase_name': 'RaggedRank2',\n          'rt_spec': RaggedTensorSpec(shape=[2, None, None]),\n          'rt': [[[1.0, 2.0], [3.0]], [[], [4.0]]]\n      },\n  ])\n  def testToFromBatchedTensorList(self, rt_spec, rt):\n    rt = ragged_factory_ops.constant(rt)\n    tensor_list = rt_spec._to_batched_tensor_list(rt)\n    rt_reconstructed = rt_spec._from_tensor_list(tensor_list)\n    self.assertAllEqual(rt, rt_reconstructed)\n    first_row = rt_spec._unbatch()._from_tensor_list(\n        [t[0] for t in tensor_list])\n    self.assertAllEqual(rt[0], first_row)\n\n  def testToFromBatchedTensorListPreservesUniformRowLengths(self):\n    rt = RaggedTensor.from_tensor(array_ops.zeros([3, 4, 5]), ragged_rank=2)\n    rt_spec = rt._type_spec\n    tensor_list = rt_spec._to_batched_tensor_list(rt)\n    rt_reconstructed = rt_spec._from_tensor_list(tensor_list)\n    self.assertAllEqual(rt, rt_reconstructed)\n    self.assertTrue(rt.shape.is_fully_defined())\n    self.assertTrue(rt_reconstructed.shape.is_fully_defined())\n    self.assertEqual(rt.shape.as_list(), rt_reconstructed.shape.as_list())\n\n  @parameterized.parameters([\n      (RaggedTensorSpec([2, None], dtypes.float32, 1), 32,\n       RaggedTensorSpec([32, 2, None], dtypes.float32, 2)),\n      (RaggedTensorSpec([4, None], dtypes.float32, 1), None,\n       RaggedTensorSpec([None, 4, None], dtypes.float32, 2)),\n      (RaggedTensorSpec([2], dtypes.float32,\n                        -1), 32, RaggedTensorSpec([32, 2], dtypes.float32, 0)),\n  ])\n  def testBatch(self, spec, batch_size, expected):\n    self.assertEqual(spec._batch(batch_size), expected)\n\n  @parameterized.parameters([\n      (RaggedTensorSpec([32, None, None], dtypes.float32, 2),\n       RaggedTensorSpec([None, None], dtypes.float32, 1)),\n      (RaggedTensorSpec([None, None, None], dtypes.float32, 2),\n       RaggedTensorSpec([None, None], dtypes.float32, 1)),\n      (RaggedTensorSpec([32, 2], dtypes.float32, 0),\n       RaggedTensorSpec([2], dtypes.float32, -1)),\n      (RaggedTensorSpec([32, None, 4], dtypes.float32, 1, dtypes.int32),\n       RaggedTensorSpec([None, 4], dtypes.float32, 0, dtypes.int32)),\n  ])  # pyformat: disable\n  def testUnbatch(self, spec, expected):\n    self.assertEqual(spec._unbatch(), expected)\n\n  def testIsCompatibleWith(self):\n    spec1 = RaggedTensorSpec([32, None, None], dtypes.float32, 2)\n    spec2 = RaggedTensorSpec(None, dtypes.float32, 2)\n    spec3 = RaggedTensorSpec(None, dtypes.int32, 1)\n    spec4 = RaggedTensorSpec([None], dtypes.int32, 0)\n\n    self.assertTrue(spec1.is_compatible_with(spec2))\n    self.assertFalse(spec1.is_compatible_with(spec3))\n    self.assertFalse(spec1.is_compatible_with(spec4))\n    self.assertFalse(spec2.is_compatible_with(spec3))\n    self.assertFalse(spec2.is_compatible_with(spec4))\n    self.assertFalse(spec3.is_compatible_with(spec4))\n    self.assertTrue(spec4.is_compatible_with(constant_op.constant([1, 2, 3])))\n\n\nif __name__ == '__main__':\n  googletest.main()\n"], "fixing_code": ["/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n#include <cstdint>\n#include <utility>\n#include <vector>\n\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/register_types.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/tensor_shape.h\"\n#include \"tensorflow/core/framework/variant.h\"\n#include \"tensorflow/core/framework/variant_encode_decode.h\"\n#include \"tensorflow/core/framework/variant_op_registry.h\"\n#include \"tensorflow/core/kernels/concat_lib.h\"\n#include \"tensorflow/core/kernels/ragged_tensor_variant.h\"\n#include \"tensorflow/core/lib/core/errors.h\"\n#include \"tensorflow/core/lib/core/status.h\"\n#include \"tensorflow/core/platform/errors.h\"\n#include \"tensorflow/core/util/tensor_ops_util.h\"\n\nnamespace tensorflow {\nnamespace {\n\ntemplate <typename VALUE_TYPE>\nStatus UnbatchDenseZerothDim(\n    const RaggedTensorVariant& batched_ragged,\n    std::vector<RaggedTensorVariant>* ragged_components) {\n  Tensor batched_values = batched_ragged.values();\n  TensorShape values_shape = batched_values.shape();\n  if (values_shape.dims() < 1) {\n    return errors::InvalidArgument(\"Can't unbatch rank-0 tensor.\");\n  }\n  auto num_components = values_shape.dim_size(0);\n  values_shape.RemoveDim(0);\n  auto num_values = values_shape.num_elements();\n\n  ragged_components->resize(num_components);\n  const auto& batched_flat = batched_values.flat<VALUE_TYPE>();\n\n  for (auto i = decltype(num_components){}; i < num_components; i++) {\n    (*ragged_components)[i].set_values(\n        Tensor(DataTypeToEnum<VALUE_TYPE>::value, values_shape));\n    auto ragged_component_values_flat =\n        (*ragged_components)[i].mutable_values()->flat<VALUE_TYPE>();\n    for (auto j = decltype(num_values){}; j < num_values; j++) {\n      ragged_component_values_flat(j) = batched_flat(j + i * num_values);\n    }\n  }\n\n  return OkStatus();\n}\n\ntemplate <typename VALUE_TYPE, typename SPLIT_TYPE>\nStatus UnbatchRaggedZerothDim(\n    const RaggedTensorVariant& batched_ragged,\n    std::vector<RaggedTensorVariant>* ragged_components) {\n  // Set up the component Ragged Tensors.\n  int ragged_rank = batched_ragged.ragged_rank();\n  if (ragged_rank == 0) {\n    return UnbatchDenseZerothDim<VALUE_TYPE>(batched_ragged, ragged_components);\n  }\n\n  auto batched_splits_top_vec = batched_ragged.splits(0).vec<SPLIT_TYPE>();\n  auto num_components = batched_splits_top_vec.size() - 1;\n\n  if (num_components < 0) {\n    return errors::Internal(\"Invalid split argument.\");\n  }\n\n  int num_splits = ragged_rank - 1;\n  ragged_components->resize(num_components);\n  for (RaggedTensorVariant& ragged_component : *ragged_components) {\n    ragged_component.mutable_nested_splits()->reserve(num_splits);\n  }\n  const auto& batched_flat = batched_ragged.values().flat<VALUE_TYPE>();\n  auto num_inner_elems = batched_ragged.values().NumElements();\n  if (batched_ragged.values().dim_size(0) > 1) {\n    num_inner_elems /= batched_ragged.values().dim_size(0);\n  }\n  TensorShape values_shape = batched_ragged.values().shape();\n\n  // Corner case: ragged_rank == 1, e.g. [[1, 2, 3], [4, 5]]\n  if (num_splits == 0) {\n    for (auto i = decltype(num_components){}; i < num_components; i++) {\n      auto start = batched_splits_top_vec(i);\n      auto limit = batched_splits_top_vec(i + 1);\n      auto num_values = limit - start;\n      values_shape.set_dim(0, num_values);\n      (*ragged_components)[i].set_values(\n          Tensor(DataTypeToEnum<VALUE_TYPE>::value, values_shape));\n      auto ragged_component_values_flat =\n          (*ragged_components)[i].mutable_values()->template flat<VALUE_TYPE>();\n      for (auto j = decltype(num_values * num_inner_elems){};\n           j < num_values * num_inner_elems; j++) {\n        ragged_component_values_flat(j) =\n            batched_flat(j + start * num_inner_elems);\n      }\n    }\n    return OkStatus();\n  }\n\n  // Unbatch nested splits.\n  std::vector<typename TTypes<SPLIT_TYPE>::ConstVec> batched_splits_vec;\n  batched_splits_vec.reserve(ragged_rank);\n  for (int i = 0; i < ragged_rank; i++) {\n    batched_splits_vec.push_back(batched_ragged.splits(i).vec<SPLIT_TYPE>());\n  }\n  std::vector<SPLIT_TYPE> index(num_splits, 1);\n  std::vector<SPLIT_TYPE> ragged_component_values_size(num_components, 0);\n  for (auto i = decltype(num_components){}; i < num_components; i++) {\n    std::vector<typename TTypes<SPLIT_TYPE>::Vec> ragged_component_splits_vec;\n    ragged_component_splits_vec.reserve(num_splits);\n    SPLIT_TYPE split_size = -1;\n    for (int j = 0; j < num_splits; j++) {\n      if (j == 0) {\n        split_size =\n            batched_splits_top_vec(i + 1) - batched_splits_top_vec(i) + 1;\n      } else {\n        // Update split size based on previous split.\n        SPLIT_TYPE last_index = ragged_component_splits_vec[j - 1].size() - 1;\n        split_size = ragged_component_splits_vec[j - 1](last_index) + 1;\n      }\n      (*ragged_components)[i].append_splits(\n          Tensor(DataTypeToEnum<SPLIT_TYPE>::value, TensorShape({split_size})));\n      ragged_component_splits_vec.push_back((*ragged_components)[i]\n                                                .mutable_splits(j)\n                                                ->template vec<SPLIT_TYPE>());\n      SPLIT_TYPE last_split_value = batched_splits_vec[j + 1](index[j] - 1);\n      ragged_component_splits_vec[j](0) = 0;\n      for (SPLIT_TYPE k = 1; k < split_size; k++, index[j]++) {\n        ragged_component_splits_vec[j](k) =\n            batched_splits_vec[j + 1](index[j]) - last_split_value;\n      }\n    }\n    SPLIT_TYPE last_split_size =\n        ragged_component_splits_vec[num_splits - 1].size();\n    ragged_component_values_size[i] =\n        ragged_component_splits_vec[num_splits - 1](last_split_size - 1);\n  }\n\n  // Unbatch values.\n  int64_t value_index = 0;\n  for (auto i = decltype(num_components){}; i < num_components; i++) {\n    SPLIT_TYPE num_values = ragged_component_values_size[i];\n    values_shape.set_dim(0, num_values);\n    (*ragged_components)[i].set_values(\n        Tensor(DataTypeToEnum<VALUE_TYPE>::value, values_shape));\n    auto ragged_component_values_flat =\n        (*ragged_components)[i].mutable_values()->template flat<VALUE_TYPE>();\n    for (int64_t j = 0; j < num_values * num_inner_elems; j++, value_index++) {\n      ragged_component_values_flat(j) = batched_flat(value_index);\n    }\n  }\n\n  return OkStatus();\n}\n}  // namespace\n\ntemplate <typename VALUE_TYPE, typename SPLIT_TYPE>\nclass RaggedTensorToVariantOp : public OpKernel {\n public:\n  explicit RaggedTensorToVariantOp(OpKernelConstruction* context)\n      : OpKernel(context) {\n    OP_REQUIRES_OK(context, context->GetAttr(\"batched_input\", &batched_input_));\n  }\n\n  void Compute(OpKernelContext* context) override {\n    // Read ragged_splits inputs.\n    OpInputList ragged_nested_splits_in;\n    OP_REQUIRES_OK(context, context->input_list(\"rt_nested_splits\",\n                                                &ragged_nested_splits_in));\n    const int ragged_nested_splits_len = ragged_nested_splits_in.size();\n    RaggedTensorVariant batched_ragged_input;\n    // Read ragged_values input.\n    batched_ragged_input.set_values(context->input(ragged_nested_splits_len));\n    batched_ragged_input.mutable_nested_splits()->reserve(\n        ragged_nested_splits_len);\n    for (int i = 0; i < ragged_nested_splits_len; i++) {\n      OP_REQUIRES(context, ragged_nested_splits_in[i].dims() == 1,\n                  errors::InvalidArgument(\"Requires nested_row_splits[\", i, \"]\",\n                                          \" to be rank 1 but is rank \",\n                                          ragged_nested_splits_in[i].dims()));\n      batched_ragged_input.append_splits(ragged_nested_splits_in[i]);\n    }\n\n    if (!batched_input_) {\n      // Encode as a Scalar Variant Tensor.\n      Tensor* encoded_scalar;\n      OP_REQUIRES_OK(context, context->allocate_output(0, TensorShape({}),\n                                                       &encoded_scalar));\n      encoded_scalar->scalar<Variant>()() = std::move(batched_ragged_input);\n      return;\n    }\n\n    // Unbatch the Ragged Tensor and encode the components.\n    std::vector<RaggedTensorVariant> unbatched_ragged_input;\n    OP_REQUIRES_OK(context, UnbatchRaggedZerothDim<VALUE_TYPE, SPLIT_TYPE>(\n                                batched_ragged_input, &unbatched_ragged_input));\n\n    // Bundle the encoded scalar Variant Tensors into a rank-1 Variant Tensor.\n    Tensor* encoded_vector;\n\n    // output_size will be used for calling TensorShape(int64_t ...). We\n    // cannot use `auto` type here, or there will be a narrowing error.\n    int64_t output_size = unbatched_ragged_input.size();\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(0, TensorShape({output_size}),\n                                            &encoded_vector));\n    auto encoded_vector_t = encoded_vector->vec<Variant>();\n    for (auto i = decltype(output_size){}; i < output_size; i++) {\n      encoded_vector_t(i) = unbatched_ragged_input[i];\n    }\n  }\n\n private:\n  bool batched_input_;\n};\n\ntemplate <typename VALUE_TYPE, typename SPLIT_TYPE>\nclass RaggedTensorToVariantGradientOp : public OpKernel {\n public:\n  using OpKernel::OpKernel;\n\n  void Compute(OpKernelContext* context) override {\n    // Read inputs.\n    Tensor encoded_variant = context->input(0);\n    Tensor row_splits = context->input(1);\n    auto flat_row_splits = row_splits.flat<SPLIT_TYPE>();\n    TensorShape dense_values_shape;\n    OP_REQUIRES_OK(context,\n                   TensorShapeUtils::MakeShape(context->input(2).vec<int32>(),\n                                               &dense_values_shape));\n\n    const auto& flat_variants = encoded_variant.flat<Variant>();\n\n    // Get a Tensor containing the flat_values for each variant.\n    std::vector<Tensor> values;\n    for (int i = 0; i < flat_variants.size(); ++i) {\n      if (const auto* encoded = flat_variants(i).get<RaggedTensorVariant>()) {\n        values.push_back(encoded->values());\n      } else {\n        // Missing value: this happens if only some of the variant values\n        // generated by ragged_tensor_to_variant impacted the value that we're\n        // calculating the gradient for.  In this case, we will see a\n        // default-constructed variant; so treat it as a zero tensor with the\n        // appropriate shape.\n        const auto value_dtype = DataTypeToEnum<VALUE_TYPE>::v();\n        auto piece_size = flat_row_splits(i + 1) - flat_row_splits(i);\n        TensorShape zeros_shape = dense_values_shape;\n        zeros_shape.set_dim(0, piece_size);\n        Tensor zero(value_dtype, zeros_shape);\n        zero.flat<VALUE_TYPE>().setZero();\n        values.push_back(zero);\n      }\n    }\n\n    if (values.size() == 1) {\n      // Just one flat_value tensor: return as-is.\n      context->set_output(0, values[0]);\n    } else {\n      Tensor* out = nullptr;\n      OP_REQUIRES_OK(context,\n                     context->allocate_output(0, dense_values_shape, &out));\n      // ConcatCPU assumes non-empty output.\n      if (dense_values_shape.num_elements() == 0) return;\n      // Multiple flat_values tensors: concatenate them together.\n      using Piece = typename TTypes<VALUE_TYPE, 2>::Matrix;\n      using ConstPiece = typename TTypes<VALUE_TYPE, 2>::ConstMatrix;\n      std::vector<std::unique_ptr<ConstPiece>> pieces;\n      pieces.reserve(values.size());\n      for (const Tensor& t : values) {\n        // ConcatCPU assumes non-empty inputs.\n        if (t.NumElements() == 0) continue;\n        pieces.emplace_back(\n            new ConstPiece(t.shaped<VALUE_TYPE, 2>({1, t.NumElements()})));\n      }\n      Piece out_flat =\n          out->shaped<VALUE_TYPE, 2>({1, dense_values_shape.num_elements()});\n      ConcatCPU<VALUE_TYPE>(context->device(), pieces, &out_flat);\n    }\n  }\n};\n\n#define REGISTER_KERNELS_WITH_SPLIT_TYPE(value_type, split_type)            \\\n  REGISTER_KERNEL_BUILDER(Name(\"RaggedTensorToVariant\")                     \\\n                              .Device(DEVICE_CPU)                           \\\n                              .TypeConstraint<value_type>(\"Tvalues\")        \\\n                              .TypeConstraint<split_type>(\"Tsplits\"),       \\\n                          RaggedTensorToVariantOp<value_type, split_type>); \\\n  REGISTER_KERNEL_BUILDER(                                                  \\\n      Name(\"RaggedTensorToVariantGradient\")                                 \\\n          .Device(DEVICE_CPU)                                               \\\n          .TypeConstraint<value_type>(\"Tvalues\")                            \\\n          .TypeConstraint<split_type>(\"Tsplits\"),                           \\\n      RaggedTensorToVariantGradientOp<value_type, split_type>);\n\n#define REGISTER_KERNELS(value_type)                  \\\n  REGISTER_KERNELS_WITH_SPLIT_TYPE(value_type, int32) \\\n  REGISTER_KERNELS_WITH_SPLIT_TYPE(value_type, int64_t)\nTF_CALL_POD_TYPES(REGISTER_KERNELS);\nTF_CALL_tstring(REGISTER_KERNELS);\nTF_CALL_QUANTIZED_TYPES(REGISTER_KERNELS);\nTF_CALL_quint16(REGISTER_KERNELS);\nTF_CALL_qint16(REGISTER_KERNELS);\n#undef REGISTER_KERNELS\n#undef REGISTER_KERNELS_WITH_SPLIT_TYPE\n}  // namespace tensorflow\n", "# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for third_party.tensorflow.python.ops.ragged_tensor.\"\"\"\n\nimport functools\nfrom absl.testing import parameterized\nimport numpy as np\n\nfrom tensorflow.core.framework import full_type_pb2\nfrom tensorflow.python.data.ops import dataset_ops\nfrom tensorflow.python.eager import backprop\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.eager import def_function\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.framework import tensor_spec\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.framework import type_spec\nfrom tensorflow.python.framework.type_utils import fulltypes_for_flat_tensors\nfrom tensorflow.python.ops import array_grad  # pylint: disable=unused-import\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import gen_ragged_conversion_ops\nfrom tensorflow.python.ops import gradients_impl\nfrom tensorflow.python.ops import map_fn\nfrom tensorflow.python.ops import math_grad  # pylint: disable=unused-import\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import tensor_array_grad  # pylint: disable=unused-import\nfrom tensorflow.python.ops.ragged import ragged_concat_ops\nfrom tensorflow.python.ops.ragged import ragged_factory_ops\nfrom tensorflow.python.ops.ragged import ragged_gather_ops\nfrom tensorflow.python.ops.ragged import ragged_math_ops\nfrom tensorflow.python.ops.ragged import ragged_tensor\nfrom tensorflow.python.ops.ragged import ragged_tensor_value\nfrom tensorflow.python.ops.ragged.ragged_tensor import RaggedTensor\nfrom tensorflow.python.ops.ragged.ragged_tensor import RaggedTensorSpec\nfrom tensorflow.python.ops.ragged.row_partition import RowPartition\n\nfrom tensorflow.python.platform import googletest\nfrom tensorflow.python.util import nest\n\n\ndef int32array(values):\n  return np.array(values, dtype=np.int32)\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass RaggedTensorTest(test_util.TensorFlowTestCase, parameterized.TestCase):\n  longMessage = True  # Property in unittest.Testcase. pylint: disable=invalid-name\n\n  #=============================================================================\n  # RaggedTensor class docstring examples\n  #=============================================================================\n\n  def testClassDocStringExamples(self):\n    # From section: \"Component Tensors\"\n    rt = RaggedTensor.from_row_splits(\n        values=[3, 1, 4, 1, 5, 9, 2, 6], row_splits=[0, 4, 4, 7, 8, 8])\n    self.assertAllEqual(rt, [[3, 1, 4, 1], [], [5, 9, 2], [6], []])\n    del rt\n\n    # From section: \"Alternative Row-Partitioning Schemes\"\n    values = [3, 1, 4, 1, 5, 9, 2, 6]\n    rt1 = RaggedTensor.from_row_splits(values, row_splits=[0, 4, 4, 7, 8, 8])\n    rt2 = RaggedTensor.from_row_lengths(values, row_lengths=[4, 0, 3, 1, 0])\n    rt3 = RaggedTensor.from_value_rowids(\n        values, value_rowids=[0, 0, 0, 0, 2, 2, 2, 3], nrows=5)\n    rt4 = RaggedTensor.from_row_starts(values, row_starts=[0, 4, 4, 7, 8])\n    rt5 = RaggedTensor.from_row_limits(values, row_limits=[4, 4, 7, 8, 8])\n    for rt in (rt1, rt2, rt3, rt4, rt5):\n      self.assertAllEqual(rt, [[3, 1, 4, 1], [], [5, 9, 2], [6], []])\n    del rt1, rt2, rt3, rt4, rt5\n\n    # From section: \"Multiple Ragged Dimensions\"\n    inner_rt = RaggedTensor.from_row_splits(\n        values=[3, 1, 4, 1, 5, 9, 2, 6], row_splits=[0, 4, 4, 7, 8, 8])\n    outer_rt = RaggedTensor.from_row_splits(\n        values=inner_rt, row_splits=[0, 3, 3, 5])\n    self.assertEqual(outer_rt.ragged_rank, 2)\n    self.assertAllEqual(outer_rt,\n                        [[[3, 1, 4, 1], [], [5, 9, 2]], [], [[6], []]])\n    del inner_rt, outer_rt\n\n    # From section: \"Multiple Ragged Dimensions\"\n    rt = RaggedTensor.from_nested_row_splits(\n        flat_values=[3, 1, 4, 1, 5, 9, 2, 6],\n        nested_row_splits=([0, 3, 3, 5], [0, 4, 4, 7, 8, 8]))\n    self.assertAllEqual(rt, [[[3, 1, 4, 1], [], [5, 9, 2]], [], [[6], []]])\n    del rt\n\n    # From section: \"Uniform Inner Dimensions\"\n    rt = RaggedTensor.from_row_splits(\n        values=array_ops.ones([5, 3]), row_splits=[0, 2, 5])\n    self.assertAllEqual(\n        rt, [[[1, 1, 1], [1, 1, 1]], [[1, 1, 1], [1, 1, 1], [1, 1, 1]]])\n    self.assertEqual(rt.shape.as_list(), [2, None, 3])\n    del rt\n\n  #=============================================================================\n  # RaggedTensorValue Constructor\n  #=============================================================================\n\n  def testRaggedTensorValueConstruction(self):\n    values = np.array(b'a b c d e f g'.split())\n    splits = np.array([0, 2, 5, 6, 6, 7], dtype=np.int64)\n    splits2 = np.array([0, 3, 5], dtype=np.int64)\n\n    # Test construction of a RaggedTensorValue with ragged_rank=1.\n    rt_value = ragged_tensor_value.RaggedTensorValue(values, splits)\n    self.assertEqual(rt_value.row_splits.dtype, np.int64)\n    self.assertEqual(rt_value.shape, (5, None))\n    self.assertLen(rt_value.nested_row_splits, 1)\n    self.assertAllEqual(splits, rt_value.row_splits)\n    self.assertAllEqual(values, rt_value.values)\n    self.assertAllEqual(splits, rt_value.nested_row_splits[0])\n    self.assertAllEqual(values, rt_value.flat_values)\n\n    # Test construction of a RaggedTensorValue with ragged_rank=2.\n    rt_value = ragged_tensor_value.RaggedTensorValue(\n        values=ragged_tensor_value.RaggedTensorValue(values, splits),\n        row_splits=splits2)\n    self.assertEqual(rt_value.row_splits.dtype, np.int64)\n    self.assertEqual(rt_value.shape, (2, None, None))\n    self.assertLen(rt_value.nested_row_splits, 2)\n    self.assertAllEqual(splits2, rt_value.row_splits)\n    self.assertAllEqual(splits, rt_value.values.row_splits)\n    self.assertAllEqual(splits2, rt_value.nested_row_splits[0])\n    self.assertAllEqual(splits, rt_value.nested_row_splits[1])\n    self.assertAllEqual(values, rt_value.values.values)\n    self.assertAllEqual(values, rt_value.flat_values)\n\n  #=============================================================================\n  # RaggedTensor Constructor (private)\n  #=============================================================================\n\n  def testRaggedTensorConstruction(self):\n    values = constant_op.constant(['a', 'b', 'c', 'd', 'e', 'f', 'g'])\n    row_splits = constant_op.constant([0, 2, 2, 5, 6, 7], dtypes.int64)\n    rp = RowPartition.from_row_splits(row_splits)\n    rt = RaggedTensor(values=values, row_partition=rp, internal=True)\n\n    self.assertAllEqual(rt,\n                        [[b'a', b'b'], [], [b'c', b'd', b'e'], [b'f'], [b'g']])\n\n  def testRaggedTensorConstructionErrors(self):\n    values = constant_op.constant(['a', 'b', 'c', 'd', 'e', 'f', 'g'])\n    row_splits = constant_op.constant([0, 2, 2, 5, 6, 7], dtypes.int64)\n    rp = RowPartition.from_row_splits(row_splits)\n\n    with self.assertRaisesRegex(ValueError,\n                                'RaggedTensor constructor is private'):\n      RaggedTensor(values=values, row_partition=rp)\n\n    with self.assertRaisesRegex(\n        TypeError, r'type\\(values\\) must be one of: Tensor, RaggedTensor'):\n      RaggedTensor(values=range(7), row_partition=rp, internal=True)\n\n    with self.assertRaisesRegex(\n        TypeError, 'Argument `row_partition` must be a RowPartition'):\n      RaggedTensor(\n          values=values, row_partition=[0, 2, 2, 5, 6, 7], internal=True)\n\n  #=============================================================================\n  # RaggedTensor Factory Ops\n  #=============================================================================\n\n  def testFromValueRowIdsWithDerivedNRows(self):\n    # nrows is known at graph creation time.\n    values = constant_op.constant(['a', 'b', 'c', 'd', 'e', 'f', 'g'])\n    value_rowids = constant_op.constant([0, 0, 2, 2, 2, 3, 4], dtypes.int64)\n\n    rt = RaggedTensor.from_value_rowids(values, value_rowids, validate=False)\n    self.assertEqual(rt.dtype, dtypes.string)\n    self.assertEqual(rt.shape.as_list(), [5, None])\n    self.assertEqual(rt.ragged_rank, 1)\n\n    rt_values = rt.values\n    rt_value_rowids = rt.value_rowids()\n    rt_nrows = rt.nrows()\n\n    self.assertIs(rt_values, values)\n    self.assertIs(rt_value_rowids, value_rowids)  # cached_value_rowids\n    self.assertAllEqual(rt_value_rowids, value_rowids)\n    self.assertAllEqual(rt_nrows, 5)\n    self.assertAllEqual(rt,\n                        [[b'a', b'b'], [], [b'c', b'd', b'e'], [b'f'], [b'g']])\n\n  def testFromValueRowIdsWithDerivedNRowsDynamic(self):\n    # nrows is not known at graph creation time.\n    values = constant_op.constant(['a', 'b', 'c', 'd', 'e', 'f', 'g'])\n    value_rowids = constant_op.constant([0, 0, 2, 2, 2, 3, 4], dtypes.int64)\n    value_rowids = array_ops.placeholder_with_default(value_rowids, shape=None)\n\n    rt = RaggedTensor.from_value_rowids(values, value_rowids, validate=False)\n    self.assertEqual(rt.dtype, dtypes.string)\n    if context.executing_eagerly():\n      self.assertEqual(rt.shape.as_list(), [5, None])\n    else:\n      self.assertEqual(rt.shape.as_list(), [None, None])\n    self.assertEqual(rt.ragged_rank, 1)\n\n    rt_values = rt.values\n    rt_value_rowids = rt.value_rowids()\n    rt_nrows = rt.nrows()\n\n    self.assertIs(rt_values, values)\n    self.assertIs(rt_value_rowids, value_rowids)  # cached_value_rowids\n    self.assertAllEqual(rt_value_rowids, value_rowids)\n    self.assertAllEqual(rt_nrows, 5)\n    self.assertAllEqual(rt,\n                        [[b'a', b'b'], [], [b'c', b'd', b'e'], [b'f'], [b'g']])\n\n  def testFromValueRowIdsWithExplicitNRows(self):\n    values = constant_op.constant(['a', 'b', 'c', 'd', 'e', 'f', 'g'])\n    value_rowids = constant_op.constant([0, 0, 2, 2, 2, 3, 4], dtypes.int64)\n    nrows = constant_op.constant(7, dtypes.int64)\n\n    rt = RaggedTensor.from_value_rowids(\n        values, value_rowids, nrows, validate=False)\n    self.assertEqual(rt.dtype, dtypes.string)\n    self.assertEqual(rt.shape.as_list(), [7, None])\n    self.assertEqual(rt.ragged_rank, 1)\n\n    rt_values = rt.values\n    rt_value_rowids = rt.value_rowids()\n    rt_nrows = rt.nrows()\n\n    self.assertIs(rt_values, values)\n    self.assertIs(rt_value_rowids, value_rowids)  # cached_value_rowids\n    self.assertIs(rt_nrows, nrows)  # cached_nrows\n    self.assertAllEqual(\n        rt, [[b'a', b'b'], [], [b'c', b'd', b'e'], [b'f'], [b'g'], [], []])\n\n  def testFromValueRowIdsWithExplicitNRowsEqualToDefault(self):\n    values = constant_op.constant(['a', 'b', 'c', 'd', 'e', 'f', 'g'])\n    value_rowids = constant_op.constant([0, 0, 2, 2, 2, 3, 4], dtypes.int64)\n    nrows = constant_op.constant(5, dtypes.int64)\n\n    rt = RaggedTensor.from_value_rowids(\n        values, value_rowids, nrows, validate=False)\n    self.assertEqual(rt.dtype, dtypes.string)\n    self.assertEqual(rt.shape.as_list(), [5, None])\n    self.assertEqual(rt.ragged_rank, 1)\n\n    rt_values = rt.values\n    rt_value_rowids = rt.value_rowids()\n    rt_nrows = rt.nrows()\n\n    self.assertIs(rt_values, values)\n    self.assertIs(rt_value_rowids, value_rowids)  # cached_value_rowids\n    self.assertIs(rt_nrows, nrows)  # cached_nrows\n    self.assertAllEqual(rt_value_rowids, value_rowids)\n    self.assertAllEqual(rt_nrows, nrows)\n    self.assertAllEqual(rt,\n                        [[b'a', b'b'], [], [b'c', b'd', b'e'], [b'f'], [b'g']])\n\n  def testFromValueRowIdsWithEmptyValues(self):\n    rt = RaggedTensor.from_value_rowids([], [])\n    rt_nrows = rt.nrows()\n    self.assertEqual(rt.dtype, dtypes.float32)\n    self.assertEqual(rt.shape.as_list(), [0, None])\n    self.assertEqual(rt.ragged_rank, 1)\n    self.assertEqual(rt.values.shape.as_list(), [0])\n    self.assertEqual(rt.value_rowids().shape.as_list(), [0])\n    self.assertAllEqual(rt_nrows, 0)\n    self.assertAllEqual(rt, [])\n\n  def testFromRowSplits(self):\n    values = constant_op.constant(['a', 'b', 'c', 'd', 'e', 'f', 'g'])\n    row_splits = constant_op.constant([0, 2, 2, 5, 6, 7], dtypes.int64)\n\n    rt = RaggedTensor.from_row_splits(values, row_splits, validate=False)\n    self.assertEqual(rt.dtype, dtypes.string)\n    self.assertEqual(rt.shape.as_list(), [5, None])\n    self.assertEqual(rt.ragged_rank, 1)\n\n    rt_values = rt.values\n    rt_row_splits = rt.row_splits\n    rt_nrows = rt.nrows()\n\n    self.assertIs(rt_values, values)\n    self.assertIs(rt_row_splits, row_splits)\n    self.assertAllEqual(rt_nrows, 5)\n    self.assertAllEqual(rt,\n                        [[b'a', b'b'], [], [b'c', b'd', b'e'], [b'f'], [b'g']])\n\n  def testFromRowSplitsWithDifferentSplitTypes(self):\n    values = constant_op.constant(['a', 'b', 'c', 'd', 'e', 'f', 'g'])\n    splits1 = [0, 2, 2, 5, 6, 7]\n    splits2 = np.array([0, 2, 2, 5, 6, 7], np.int64)\n    splits3 = np.array([0, 2, 2, 5, 6, 7], np.int32)\n    splits4 = constant_op.constant([0, 2, 2, 5, 6, 7], dtypes.int64)\n    splits5 = constant_op.constant([0, 2, 2, 5, 6, 7], dtypes.int32)\n    rt1 = RaggedTensor.from_row_splits(values, splits1)\n    rt2 = RaggedTensor.from_row_splits(values, splits2)\n    rt3 = RaggedTensor.from_row_splits(values, splits3)\n    rt4 = RaggedTensor.from_row_splits(values, splits4)\n    rt5 = RaggedTensor.from_row_splits(values, splits5)\n    self.assertEqual(rt1.row_splits.dtype, dtypes.int64)\n    self.assertEqual(rt2.row_splits.dtype, dtypes.int64)\n    self.assertEqual(rt3.row_splits.dtype, dtypes.int32)\n    self.assertEqual(rt4.row_splits.dtype, dtypes.int64)\n    self.assertEqual(rt5.row_splits.dtype, dtypes.int32)\n\n  def testFromRowSplitsWithEmptySplits(self):\n    err_msg = 'row_splits tensor may not be empty'\n    with self.assertRaisesRegex(ValueError, err_msg):\n      RaggedTensor.from_row_splits([], [])\n\n  def testFromRowStarts(self):\n    values = constant_op.constant(['a', 'b', 'c', 'd', 'e', 'f', 'g'])\n    row_starts = constant_op.constant([0, 2, 2, 5, 6], dtypes.int64)\n\n    rt = RaggedTensor.from_row_starts(values, row_starts, validate=False)\n    self.assertEqual(rt.dtype, dtypes.string)\n    self.assertEqual(rt.shape.as_list(), [5, None])\n    self.assertEqual(rt.ragged_rank, 1)\n\n    rt_values = rt.values\n    rt_row_starts = rt.row_starts()\n    rt_nrows = rt.nrows()\n\n    self.assertIs(rt_values, values)\n    self.assertAllEqual(rt_nrows, 5)\n    self.assertAllEqual(rt_row_starts, row_starts)\n    self.assertAllEqual(rt,\n                        [[b'a', b'b'], [], [b'c', b'd', b'e'], [b'f'], [b'g']])\n\n  def testFromRowLimits(self):\n    values = constant_op.constant(['a', 'b', 'c', 'd', 'e', 'f', 'g'])\n    row_limits = constant_op.constant([2, 2, 5, 6, 7], dtypes.int64)\n\n    rt = RaggedTensor.from_row_limits(values, row_limits, validate=False)\n    self.assertEqual(rt.dtype, dtypes.string)\n    self.assertEqual(rt.shape.as_list(), [5, None])\n    self.assertEqual(rt.ragged_rank, 1)\n\n    rt_values = rt.values\n    rt_row_limits = rt.row_limits()\n    rt_nrows = rt.nrows()\n\n    self.assertIs(rt_values, values)\n    self.assertAllEqual(rt_nrows, 5)\n    self.assertAllEqual(rt_row_limits, row_limits)\n    self.assertAllEqual(rt,\n                        [[b'a', b'b'], [], [b'c', b'd', b'e'], [b'f'], [b'g']])\n\n  def testFromRowLengths(self):\n    values = constant_op.constant(['a', 'b', 'c', 'd', 'e', 'f', 'g'])\n    row_lengths = constant_op.constant([2, 0, 3, 1, 1], dtypes.int64)\n\n    rt = RaggedTensor.from_row_lengths(values, row_lengths, validate=False)\n    self.assertEqual(rt.dtype, dtypes.string)\n    self.assertEqual(rt.shape.as_list(), [5, None])\n    self.assertEqual(rt.ragged_rank, 1)\n\n    rt_values = rt.values\n    rt_row_lengths = rt.row_lengths()\n    rt_nrows = rt.nrows()\n\n    self.assertIs(rt_values, values)\n    self.assertIs(rt_row_lengths, row_lengths)  # cached_nrows\n    self.assertAllEqual(rt_nrows, 5)\n    self.assertAllEqual(rt_row_lengths, row_lengths)\n    self.assertAllEqual(rt,\n                        [[b'a', b'b'], [], [b'c', b'd', b'e'], [b'f'], [b'g']])\n\n  def testFromRowLengthsInt32(self):\n    rt = RaggedTensor.from_row_lengths([1, 2, 3, 4],\n                                       constant_op.constant([1, 0, 3],\n                                                            dtype=dtypes.int32))\n    rt2 = RaggedTensor.from_row_lengths(rt, [2, 1, 0])\n    self.assertAllEqual([2, 1, 0], rt2.row_lengths())\n\n  def testFromUniformRowLength(self):\n    values = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n\n    a1 = RaggedTensor.from_uniform_row_length(values, 2)\n    a2 = RaggedTensor.from_uniform_row_length(values, 2, 8)\n    self.assertAllEqual(\n        a1,\n        [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12], [13, 14], [15, 16]])\n    self.assertAllEqual(a1, a2)\n    self.assertEqual(a1.shape.as_list(), [8, 2])\n    self.assertEqual(a2.shape.as_list(), [8, 2])\n\n    b1 = RaggedTensor.from_uniform_row_length(a1, 2)\n    b2 = RaggedTensor.from_uniform_row_length(a1, 2, 4)\n    self.assertAllEqual(b1, [[[1, 2], [3, 4]], [[5, 6], [7, 8]],\n                             [[9, 10], [11, 12]], [[13, 14], [15, 16]]])\n    self.assertAllEqual(b1, b2)\n    self.assertEqual(b1.shape.as_list(), [4, 2, 2])\n    self.assertEqual(b2.shape.as_list(), [4, 2, 2])\n\n    c1 = RaggedTensor.from_uniform_row_length(b1, 2)\n    c2 = RaggedTensor.from_uniform_row_length(b1, 2, 2)\n    self.assertAllEqual(c1, [[[[1, 2], [3, 4]], [[5, 6], [7, 8]]],\n                             [[[9, 10], [11, 12]], [[13, 14], [15, 16]]]])\n    self.assertAllEqual(c1, c2)\n    self.assertEqual(c1.shape.as_list(), [2, 2, 2, 2])\n    self.assertEqual(c2.shape.as_list(), [2, 2, 2, 2])\n\n  def testFromUniformRowLengthWithEmptyValues(self):\n    empty_values = []\n    a = RaggedTensor.from_uniform_row_length(empty_values, 0, nrows=10)\n    self.assertEqual(a.shape.as_list(), [10, 0])\n\n    b = RaggedTensor.from_uniform_row_length(a, 2)\n    self.assertEqual(b.shape.as_list(), [5, 2, 0])\n\n    # Make sure we avoid divide-by-zero when finding nrows for nvals=rowlen=0.\n    c = RaggedTensor.from_uniform_row_length(empty_values, 0)\n    self.assertEqual(c.shape.as_list(), [0, 0])\n    d = RaggedTensor.from_uniform_row_length(empty_values, 0, nrows=0)\n    self.assertEqual(d.shape.as_list(), [0, 0])\n\n  def testFromUniformRowLengthWithPlaceholders(self):\n    ph_values = array_ops.placeholder_with_default([1, 2, 3, 4, 5, 6], [None])\n    ph_rowlen = array_ops.placeholder_with_default(3, None)\n    rt1 = RaggedTensor.from_uniform_row_length(ph_values, 3)\n    rt2 = RaggedTensor.from_uniform_row_length(ph_values, ph_rowlen)\n    rt3 = RaggedTensor.from_uniform_row_length([1, 2, 3, 4, 5, 6], ph_rowlen)\n    self.assertAllEqual(rt1, [[1, 2, 3], [4, 5, 6]])\n    self.assertAllEqual(rt2, [[1, 2, 3], [4, 5, 6]])\n    self.assertAllEqual(rt3, [[1, 2, 3], [4, 5, 6]])\n    if context.executing_eagerly():\n      self.assertEqual(rt1.shape.as_list(), [2, 3])\n      self.assertEqual(rt2.shape.as_list(), [2, 3])\n      self.assertEqual(rt3.shape.as_list(), [2, 3])\n    else:\n      self.assertEqual(rt1.shape.as_list(), [None, 3])\n      self.assertEqual(rt2.shape.as_list(), [None, None])\n      self.assertEqual(rt3.shape.as_list(), [None, None])\n\n    b = RaggedTensor.from_uniform_row_length(rt1, 2)\n    self.assertAllEqual(b, [[[1, 2, 3], [4, 5, 6]]])\n\n    # Make sure we avoid divide-by-zero when finding nrows for nvals=rowlen=0.\n    ph_empty_values = array_ops.placeholder_with_default(\n        array_ops.zeros([0], dtypes.int64), [None])\n    ph_zero = array_ops.placeholder_with_default(0, [])\n    c = RaggedTensor.from_uniform_row_length(ph_empty_values, ph_zero)\n    if context.executing_eagerly():\n      self.assertEqual(c.shape.as_list(), [0, 0])\n    else:\n      self.assertEqual(c.shape.as_list(), [None, None])\n\n  def testFromNestedValueRowIdsWithDerivedNRows(self):\n    values = constant_op.constant(['a', 'b', 'c', 'd', 'e', 'f', 'g'])\n    nested_value_rowids = [\n        constant_op.constant([0, 0, 1, 3, 3], dtypes.int64),\n        constant_op.constant([0, 0, 2, 2, 2, 3, 4], dtypes.int64)\n    ]\n\n    rt = RaggedTensor.from_nested_value_rowids(values, nested_value_rowids)\n    self.assertEqual(rt.dtype, dtypes.string)\n    self.assertEqual(rt.shape.as_list(), [4, None, None])\n    self.assertEqual(rt.ragged_rank, 2)\n\n    rt_values = rt.values\n    rt_value_rowids = rt.value_rowids()\n    rt_values_values = rt_values.values\n    rt_values_value_rowids = rt_values.value_rowids()\n\n    self.assertIs(rt_values_values, values)\n    self.assertAllEqual(rt_value_rowids, nested_value_rowids[0])\n    self.assertAllEqual(rt_values_value_rowids, nested_value_rowids[1])\n    self.assertAllEqual(\n        rt, [[[b'a', b'b'], []], [[b'c', b'd', b'e']], [], [[b'f'], [b'g']]])\n\n  def testFromNestedRowPartitions(self):\n    flat_values = constant_op.constant(['a', 'b', 'c', 'd', 'e', 'f', 'g'])\n    nested_row_splits = [[0, 2, 3, 3, 5], [0, 2, 2, 5, 6, 7]]\n    nested_row_partition = [\n        RowPartition.from_row_splits(constant_op.constant(x, dtypes.int64))\n        for x in nested_row_splits\n    ]\n\n    rt = RaggedTensor._from_nested_row_partitions(\n        flat_values, nested_row_partition, validate=False)\n    self.assertEqual(rt.dtype, dtypes.string)\n    self.assertEqual(rt.shape.as_list(), [4, None, None])\n    self.assertEqual(rt.ragged_rank, 2)\n    self.assertAllEqual(\n        rt, [[[b'a', b'b'], []], [[b'c', b'd', b'e']], [], [[b'f'], [b'g']]])\n\n  def testFromNestedValueRowIdsWithExplicitNRows(self):\n    values = constant_op.constant(['a', 'b', 'c', 'd', 'e', 'f', 'g'])\n    nested_value_rowids = [\n        constant_op.constant([0, 0, 1, 3, 3, 3], dtypes.int64),\n        constant_op.constant([0, 0, 2, 2, 2, 3, 4], dtypes.int64)\n    ]\n    nrows = [\n        constant_op.constant(6, dtypes.int64),\n        constant_op.constant(6, dtypes.int64)\n    ]\n\n    rt = RaggedTensor.from_nested_value_rowids(values, nested_value_rowids,\n                                               nrows)\n    self.assertEqual(rt.dtype, dtypes.string)\n    self.assertEqual(rt.shape.as_list(), [6, None, None])\n    self.assertEqual(rt.ragged_rank, 2)\n\n    rt_values = rt.values\n    rt_value_rowids = rt.value_rowids()\n    rt_nrows = rt.nrows()\n    rt_values_values = rt_values.values\n    rt_values_value_rowids = rt_values.value_rowids()\n    rt_values_nrows = rt_values.nrows()\n\n    self.assertIs(rt_values_values, values)\n    self.assertAllEqual(rt_value_rowids, nested_value_rowids[0])\n    self.assertAllEqual(rt_values_value_rowids, nested_value_rowids[1])\n    self.assertAllEqual(rt_nrows, nrows[0])\n    self.assertAllEqual(rt_values_nrows, nrows[1])\n    self.assertAllEqual(rt, [[[b'a', b'b'], []], [[b'c', b'd', b'e']], [],\n                             [[b'f'], [b'g'], []], [], []])\n\n  def testFromNestedValueRowIdsWithExplicitNRowsMismatch(self):\n    values = constant_op.constant(['a', 'b', 'c', 'd', 'e', 'f', 'g'])\n    nested_value_rowids = [\n        constant_op.constant([0, 0, 1, 3, 3, 3], dtypes.int64),\n        constant_op.constant([0, 0, 2, 2, 2, 3, 4], dtypes.int64)\n    ]\n    nrows = [constant_op.constant(6, dtypes.int64)]\n    with self.assertRaisesRegex(\n        ValueError, 'Argument `nested_nrows` must have the same length as '\n        'argument `nested_value_rowids`'):\n      RaggedTensor.from_nested_value_rowids(values, nested_value_rowids, nrows)\n\n  def testFromNestedValueRowIdsWithNonListInput(self):\n    with self.assertRaisesRegex(\n        TypeError, 'Argument `nested_value_rowids` must be a list of Tensors'):\n      RaggedTensor.from_nested_value_rowids(\n          [1, 2, 3], constant_op.constant([[0, 1, 2], [0, 1, 2]], dtypes.int64))\n    with self.assertRaisesRegex(\n        TypeError, 'Argument `nested_nrows` must be a list of Tensors'):\n      RaggedTensor.from_nested_value_rowids([1, 2, 3], [[0, 1, 2], [0, 1, 2]],\n                                            constant_op.constant([3, 3]))\n\n  def testFromNestedRowSplits(self):\n    flat_values = constant_op.constant(['a', 'b', 'c', 'd', 'e', 'f', 'g'])\n    nested_row_splits = [\n        constant_op.constant([0, 2, 3, 3, 5], dtypes.int64),\n        constant_op.constant([0, 2, 2, 5, 6, 7], dtypes.int64)\n    ]\n\n    rt = RaggedTensor.from_nested_row_splits(\n        flat_values, nested_row_splits, validate=False)\n    self.assertEqual(rt.dtype, dtypes.string)\n    self.assertEqual(rt.shape.as_list(), [4, None, None])\n    self.assertEqual(rt.ragged_rank, 2)\n\n    rt_values = rt.values\n    rt_row_splits = rt.row_splits\n    rt_values_values = rt_values.values\n    rt_values_row_splits = rt_values.row_splits\n\n    self.assertIs(rt_values_values, flat_values)\n    self.assertIs(rt_row_splits, nested_row_splits[0])\n    self.assertIs(rt_values_row_splits, nested_row_splits[1])\n    self.assertAllEqual(\n        rt, [[[b'a', b'b'], []], [[b'c', b'd', b'e']], [], [[b'f'], [b'g']]])\n\n  def testWithRowSplits(self):\n    flat_values = constant_op.constant(['a', 'b', 'c', 'd', 'e', 'f', 'g'])\n    nested_row_splits = [\n        constant_op.constant([0, 2, 3, 3, 5], dtypes.int64),\n        constant_op.constant([0, 2, 2, 5, 6, 7], dtypes.int64)\n    ]\n\n    rt = RaggedTensor.from_nested_row_splits(\n        flat_values, nested_row_splits, validate=False)\n\n    rt = rt.with_row_splits_dtype(dtypes.int32)\n\n    self.assertEqual(rt.dtype, dtypes.string)\n    self.assertEqual(rt.shape.as_list(), [4, None, None])\n    self.assertEqual(rt.ragged_rank, 2)\n\n    rt_values = rt.values\n    rt_row_splits = rt.row_splits\n    rt_values_values = rt_values.values\n    rt_values_row_splits = rt_values.row_splits\n\n    self.assertAllEqual(rt_values_values, flat_values)\n    self.assertAllEqual(rt_row_splits, nested_row_splits[0])\n    self.assertAllEqual(rt_values_row_splits, nested_row_splits[1])\n    self.assertAllEqual(\n        rt, [[[b'a', b'b'], []], [[b'c', b'd', b'e']], [], [[b'f'], [b'g']]])\n\n  def testFromNestedRowSplitsWithNonListInput(self):\n    with self.assertRaisesRegex(\n        TypeError, '`nested_row_splits` must be a list of Tensors'):\n      RaggedTensor.from_nested_row_splits(\n          [1, 2], constant_op.constant([[0, 1, 2], [0, 1, 2]], dtypes.int64))\n\n  def testFromValueRowIdsWithBadNRows(self):\n    values = constant_op.constant(['a', 'b', 'c', 'd', 'e', 'f', 'g'])\n    value_rowids = constant_op.constant([0, 0, 2, 2, 2, 3, 4], dtypes.int64)\n    nrows = constant_op.constant(5, dtypes.int64)\n\n    with self.assertRaisesRegex(ValueError, r'Expected nrows >= 0; got -2'):\n      RaggedTensor.from_value_rowids(\n          values=values,\n          value_rowids=array_ops.placeholder_with_default(value_rowids, None),\n          nrows=-2)\n\n    with self.assertRaisesRegex(\n        ValueError, r'Expected nrows >= value_rowids\\[-1\\] \\+ 1; got nrows=2, '\n        r'value_rowids\\[-1\\]=4'):\n      RaggedTensor.from_value_rowids(\n          values=values, value_rowids=value_rowids, nrows=2)\n\n    with self.assertRaisesRegex(\n        ValueError, r'Expected nrows >= value_rowids\\[-1\\] \\+ 1; got nrows=4, '\n        r'value_rowids\\[-1\\]=4'):\n      RaggedTensor.from_value_rowids(\n          values=values, value_rowids=value_rowids, nrows=4)\n\n    with self.assertRaisesRegex(ValueError, r'Shape \\(7, 1\\) must have rank 1'):\n      RaggedTensor.from_value_rowids(\n          values=values,\n          value_rowids=array_ops.expand_dims(value_rowids, 1),\n          nrows=nrows)\n\n    with self.assertRaisesRegex(ValueError, r'Shape \\(1,\\) must have rank 0'):\n      RaggedTensor.from_value_rowids(\n          values=values,\n          value_rowids=value_rowids,\n          nrows=array_ops.expand_dims(nrows, 0))\n\n  def testCondWithTensorsFromValueIds(self):\n    # b/141166460\n    rt = RaggedTensor.from_value_rowids([1, 2, 3], [0, 0, 2])\n    c = array_ops.placeholder_with_default(True, None)\n    result = control_flow_ops.cond(c, lambda: rt, lambda: rt)\n    self.assertAllEqual(rt, result)\n\n  def testGraphMismatch(self):\n    if not context.executing_eagerly():\n      with ops.Graph().as_default():\n        values = constant_op.constant([1, 2, 3], dtypes.int64)\n      with ops.Graph().as_default():\n        splits = constant_op.constant([0, 2, 3], dtypes.int64)\n      with self.assertRaisesRegex(ValueError,\n                                  '.* must be from the same graph as .*'):\n        RaggedTensor.from_row_splits(values, splits)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name='Rank0',\n          tensor='a'),\n      dict(\n          testcase_name='Rank1',\n          tensor=['a', 'b']),\n  ])\n  def testFromTensorRankError(self, tensor):\n    with self.assertRaisesRegex(ValueError, 'must be greater than 1'):\n      RaggedTensor.from_tensor(tensor)\n\n  #=============================================================================\n  # Ragged Value & Row-Partitioning Tensor Accessors\n  #=============================================================================\n\n  def testRaggedTensorAccessors_2d(self):\n    values = constant_op.constant(['a', 'b', 'c', 'd', 'e', 'f', 'g'])\n    row_splits = constant_op.constant([0, 2, 2, 5, 6, 7], dtypes.int64)\n    value_rowids = constant_op.constant([0, 0, 2, 2, 2, 3, 4], dtypes.int64)\n    rt1 = RaggedTensor.from_row_splits(values, row_splits)\n    rt2 = RaggedTensor.from_value_rowids(values, value_rowids)\n\n    for rt in [rt1, rt2]:\n      self.assertAllEqual(\n          rt, [[b'a', b'b'], [], [b'c', b'd', b'e'], [b'f'], [b'g']])\n      self.assertAllEqual(rt.values, [b'a', b'b', b'c', b'd', b'e', b'f', b'g'])\n      self.assertEqual(rt.values.shape.dims[0].value, 7)\n      self.assertAllEqual(rt.value_rowids(), [0, 0, 2, 2, 2, 3, 4])\n      self.assertAllEqual(rt.nrows(), 5)\n      self.assertAllEqual(rt.row_splits, [0, 2, 2, 5, 6, 7])\n      self.assertAllEqual(rt.row_starts(), [0, 2, 2, 5, 6])\n      self.assertAllEqual(rt.row_limits(), [2, 2, 5, 6, 7])\n      self.assertAllEqual(rt.row_lengths(), [2, 0, 3, 1, 1])\n      self.assertAllEqual(rt.flat_values,\n                          [b'a', b'b', b'c', b'd', b'e', b'f', b'g'])\n      self.assertLen(rt.nested_row_splits, 1)\n      self.assertAllEqual(rt.nested_row_splits[0], [0, 2, 2, 5, 6, 7])\n\n  def testRaggedTensorAccessors_3d_with_ragged_rank_1(self):\n    values = [[0, 1], [2, 3], [4, 5], [6, 7], [8, 9], [10, 11], [12, 13]]\n    row_splits = constant_op.constant([0, 2, 2, 5, 6, 7], dtypes.int64)\n    value_rowids = constant_op.constant([0, 0, 2, 2, 2, 3, 4], dtypes.int64)\n    row_lengths = constant_op.constant([2, 0, 3, 1, 1])\n    rt1 = RaggedTensor.from_row_splits(values, row_splits)\n    rt2 = RaggedTensor.from_value_rowids(values, value_rowids)\n    rt3 = RaggedTensor.from_row_lengths(values, row_lengths)\n\n    for rt in [rt1, rt2, rt3]:\n      self.assertAllEqual(rt, [[[0, 1], [2, 3]], [], [[4, 5], [6, 7], [8, 9]],\n                               [[10, 11]], [[12, 13]]])\n      self.assertAllEqual(\n          rt.values,\n          [[0, 1], [2, 3], [4, 5], [6, 7], [8, 9], [10, 11], [12, 13]])\n      self.assertEqual(rt.values.shape.dims[0].value, 7)\n      self.assertAllEqual(rt.value_rowids(), [0, 0, 2, 2, 2, 3, 4])\n      self.assertAllEqual(rt.nrows(), 5)\n      self.assertAllEqual(rt.row_splits, [0, 2, 2, 5, 6, 7])\n      self.assertAllEqual(rt.row_starts(), [0, 2, 2, 5, 6])\n      self.assertAllEqual(rt.row_limits(), [2, 2, 5, 6, 7])\n      self.assertAllEqual(rt.row_lengths(), [2, 0, 3, 1, 1])\n      self.assertAllEqual(\n          rt.row_lengths(axis=2), [[2, 2], [], [2, 2, 2], [2], [2]])\n      self.assertAllEqual(\n          rt.flat_values,\n          [[0, 1], [2, 3], [4, 5], [6, 7], [8, 9], [10, 11], [12, 13]])\n      self.assertLen(rt.nested_row_splits, 1)\n      self.assertAllEqual(rt.nested_row_splits[0], [0, 2, 2, 5, 6, 7])\n      self.assertLen(rt.nested_value_rowids(), 1)\n\n      self.assertAllEqual(rt.nested_value_rowids()[0], [0, 0, 2, 2, 2, 3, 4])\n\n  def testRaggedTensorAccessors_3d_with_ragged_rank_2(self):\n    values = constant_op.constant(['a', 'b', 'c', 'd', 'e', 'f', 'g'])\n    nested_row_splits = [\n        constant_op.constant([0, 2, 3, 3, 5], dtypes.int64),\n        constant_op.constant([0, 2, 2, 5, 6, 7], dtypes.int64)\n    ]\n    nested_value_rowids = [\n        constant_op.constant([0, 0, 1, 3, 3], dtypes.int64),\n        constant_op.constant([0, 0, 2, 2, 2, 3, 4], dtypes.int64)\n    ]\n    rt1 = RaggedTensor.from_nested_row_splits(values, nested_row_splits)\n    rt2 = RaggedTensor.from_nested_value_rowids(values, nested_value_rowids)\n\n    for rt in [rt1, rt2]:\n      self.assertAllEqual(\n          rt, [[[b'a', b'b'], []], [[b'c', b'd', b'e']], [], [[b'f'], [b'g']]])\n      self.assertAllEqual(\n          rt.values, [[b'a', b'b'], [], [b'c', b'd', b'e'], [b'f'], [b'g']])\n      self.assertEqual(rt.values.shape.dims[0].value, 5)\n      self.assertAllEqual(rt.value_rowids(), [0, 0, 1, 3, 3])\n      self.assertAllEqual(rt.nrows(), 4)\n      self.assertAllEqual(rt.row_splits, [0, 2, 3, 3, 5])\n      self.assertAllEqual(rt.row_starts(), [0, 2, 3, 3])\n      self.assertAllEqual(rt.row_limits(), [2, 3, 3, 5])\n      self.assertAllEqual(rt.row_lengths(), [2, 1, 0, 2])\n      self.assertAllEqual(rt.flat_values,\n                          [b'a', b'b', b'c', b'd', b'e', b'f', b'g'])\n      self.assertLen(rt.nested_row_splits, 2)\n      self.assertAllEqual(rt.nested_row_splits[0], [0, 2, 3, 3, 5])\n      self.assertAllEqual(rt.nested_row_splits[1], [0, 2, 2, 5, 6, 7])\n      self.assertLen(rt.nested_value_rowids(), 2)\n      self.assertAllEqual(rt.nested_value_rowids()[0], [0, 0, 1, 3, 3])\n      self.assertAllEqual(rt.nested_value_rowids()[1], [0, 0, 2, 2, 2, 3, 4])\n\n  #=============================================================================\n  # RaggedTensor.shape\n  #=============================================================================\n\n  def testShape(self):\n    \"\"\"Tests for RaggedTensor.shape.\"\"\"\n    rt1 = RaggedTensor.from_row_splits(b'a b c d e f g'.split(),\n                                       [0, 2, 5, 6, 6, 7])\n    self.assertEqual(rt1.shape.as_list(), [5, None])\n\n    rt2 = RaggedTensor.from_row_splits(\n        [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12], [13, 14]],\n        [0, 2, 5, 6, 6, 7])\n    self.assertEqual(rt2.shape.as_list(), [5, None, 2])\n\n    rt3 = RaggedTensor.from_row_splits(\n        [[[1, 2], [3, 4]], [[5, 6], [7, 8]], [[9, 10], [11, 12]]], [0, 2, 2, 3])\n    self.assertEqual(rt3.shape.as_list(), [3, None, 2, 2])\n\n    rt4 = RaggedTensor.from_row_splits(rt3, [0, 1, 3, 3])\n    self.assertEqual(rt4.shape.as_list(), [3, None, None, 2, 2])\n\n    if not context.executing_eagerly():\n      rt5 = RaggedTensor.from_row_splits(\n          array_ops.placeholder(dtype=dtypes.string), [0, 2, 3, 5])\n      self.assertIsNone(rt5.shape.ndims)\n\n      rt6 = RaggedTensor.from_row_splits(\n          [1, 2, 3], array_ops.placeholder(dtype=dtypes.int64))\n      self.assertEqual(rt6.shape.as_list(), [None, None])\n\n  def testGetShape(self):\n    rt = RaggedTensor.from_row_splits(b'a b c d e f g'.split(),\n                                      [0, 2, 5, 6, 6, 7])\n    self.assertEqual(rt.shape.as_list(), rt.get_shape().as_list())\n\n  #=============================================================================\n  # RaggedTensor.__str__\n  #=============================================================================\n  def testRaggedTensorStr(self):\n    values = [b'a', b'b', b'c', b'd', b'e', b'f', b'g']\n    row_splits = [0, 2, 5, 6, 6, 7]\n    rt = RaggedTensor.from_row_splits(values, row_splits, validate=False)\n    splits_type = 'int64'\n    if context.executing_eagerly():\n      expected_repr = '<tf.RaggedTensor {}>'.format([[b'a', b'b'],\n                                                     [b'c', b'd', b'e'], [b'f'],\n                                                     [], [b'g']])\n    else:\n      expected_repr = (\n          'tf.RaggedTensor(values=Tensor(\"RaggedFromRowSplits/values:0\", '\n          'shape=(7,), dtype=string), '\n          'row_splits=Tensor('\n          '\"RaggedFromRowSplits/RowPartitionFromRowSplits/row_splits:0\",'\n          ' shape=(6,), dtype={}))').format(splits_type)\n    self.assertEqual(repr(rt), expected_repr)\n    self.assertEqual(str(rt), expected_repr)\n\n  def testRaggedTensorValueStr(self):\n    values = [b'a', b'b', b'c', b'd', b'e', b'f', b'g']\n    row_splits = [0, 2, 5, 6, 6, 7]\n    rt = ragged_tensor_value.RaggedTensorValue(\n        np.array(values), np.array(row_splits, dtype=np.int64))\n    expected_str = '<tf.RaggedTensorValue {}>'.format([[b'a', b'b'],\n                                                       [b'c', b'd', b'e'],\n                                                       [b'f'], [], [b'g']])\n    expected_repr = (\"tf.RaggedTensorValue(values=array({}, dtype='|S1'), \"\n                     'row_splits=array({}))'.format(values, row_splits))\n    self.assertEqual(' '.join(str(rt).split()), expected_str)\n    self.assertEqual(' '.join(repr(rt).split()), expected_repr)\n\n  def testRaggedTensorStrWithZeroSizeInnerShape(self):\n    # Tests that b/226112826 is fixed.\n    if context.executing_eagerly():\n      rt = RaggedTensor.from_row_lengths(array_ops.zeros([9, 0]), [4, 3, 2])\n      expected_repr = (\n          '<tf.RaggedTensor [[[], [], [], []], [[], [], []], [[], []]]>')\n      self.assertEqual(' '.join(repr(rt).split()), expected_repr)\n\n  #=============================================================================\n  # RaggedTensor.with_values() and RaggedTensor.with_flat_values().\n  #=============================================================================\n\n  def testWithValues(self):\n    rt1 = ragged_factory_ops.constant([[1, 2], [3, 4, 5], [6], [], [7]])\n    rt2 = ragged_factory_ops.constant([[[1, 2], [3, 4, 5]], [[6]], [], [[],\n                                                                        [7]]])\n\n    rt1_plus_10 = rt1.with_values(rt1.values + 10)\n    rt2_times_10 = rt2.with_flat_values(rt2.flat_values * 10)\n    rt1_expanded = rt1.with_values(array_ops.expand_dims(rt1.values, axis=1))\n\n    self.assertAllEqual(rt1_plus_10, [[11, 12], [13, 14, 15], [16], [], [17]])\n    self.assertAllEqual(rt2_times_10,\n                        [[[10, 20], [30, 40, 50]], [[60]], [], [[], [70]]])\n    self.assertAllEqual(rt1_expanded,\n                        [[[1], [2]], [[3], [4], [5]], [[6]], [], [[7]]])\n\n  #=============================================================================\n  # Session.run\n  #=============================================================================\n  def testSessionRun(self):\n    if context.executing_eagerly():\n      return\n\n    rt1 = ragged_factory_ops.constant([[1, 2, 3], [4]])\n    rt2 = ragged_factory_ops.constant([[[], [1, 2]], [[3]]])\n    with self.test_session() as session:\n      result = session.run({'rt1': rt1, 'rt2': rt2})\n      self.assertCountEqual(result.keys(), ['rt1', 'rt2'])\n      self.assertEqual(result['rt1'].to_list(), [[1, 2, 3], [4]])\n      self.assertEqual(result['rt2'].to_list(), [[[], [1, 2]], [[3]]])\n\n  def testSessionRunFeed(self):\n    if context.executing_eagerly():\n      return\n\n    rt1 = RaggedTensor.from_row_splits(\n        array_ops.placeholder(dtypes.int32),\n        array_ops.placeholder(dtypes.int64))\n    rt2 = RaggedTensor.from_nested_row_splits(\n        array_ops.placeholder(dtypes.int32), [\n            array_ops.placeholder(dtypes.int64),\n            array_ops.placeholder(dtypes.int64)\n        ])\n\n    rt1_feed_val = ragged_factory_ops.constant_value([[1, 2, 3], [4]])\n    rt2_feed_val = ragged_factory_ops.constant_value([[[], [1, 2]], [[3]]])\n\n    with self.test_session() as session:\n      fetches = {'rt1': rt1, 'rt2': rt2}\n      feeds = {rt1: rt1_feed_val, rt2: rt2_feed_val}\n      result = session.run(fetches, feed_dict=feeds)\n      self.assertCountEqual(result.keys(), ['rt1', 'rt2'])\n      self.assertEqual(result['rt1'].to_list(), [[1, 2, 3], [4]])\n      self.assertEqual(result['rt2'].to_list(), [[[], [1, 2]], [[3]]])\n\n  def testSessionPartialRunFeed(self):\n    if context.executing_eagerly():\n      return\n\n    # Placeholder inputs.\n    a = RaggedTensor.from_row_splits(\n        array_ops.placeholder(dtypes.int32, shape=[None], name='a.values'),\n        array_ops.placeholder(dtypes.int64, name='a.row_splits'))\n    b = RaggedTensor.from_row_splits(\n        array_ops.placeholder(dtypes.int32, shape=[None], name='b.values'),\n        array_ops.placeholder(dtypes.int64, name='b.row_splits'))\n    c = array_ops.placeholder(dtypes.int32, shape=[], name='c')\n\n    # Feed values for placeholder inputs.\n    a_val = ragged_factory_ops.constant_value([[1, 2, 3], [4]])\n    b_val = ragged_factory_ops.constant_value([[5, 4, 3], [2]])\n    c_val = 3\n\n    # Compute some values.\n    r1 = ragged_math_ops.reduce_sum(a * b, axis=1)\n    r2 = ragged_math_ops.reduce_sum(a + c, axis=1)\n\n    with self.test_session() as session:\n      handle = session.partial_run_setup([r1, r2], [a, b, c])\n\n      res1 = session.partial_run(handle, r1, feed_dict={a: a_val, b: b_val})\n      self.assertAllEqual(res1, [22, 8])\n\n      res2 = session.partial_run(handle, r2, feed_dict={c: c_val})\n      self.assertAllEqual(res2, [15, 7])\n\n  # Test case for GitHub issue 24679.\n  def testEagerForLoop(self):\n    if not context.executing_eagerly():\n      return\n\n    values = [[1., 2.], [3., 4., 5.], [6.]]\n    r = ragged_factory_ops.constant(values)\n    i = 0\n    for elem in r:\n      self.assertAllEqual(elem, values[i])\n      i += 1\n\n  def testConsumers(self):\n    if context.executing_eagerly():\n      return\n\n    a = RaggedTensor.from_row_splits(\n        array_ops.placeholder(dtypes.int32, shape=[None], name='a.values'),\n        array_ops.placeholder(dtypes.int64, name='a.row_splits'),\n        validate=False)\n    ragged_math_ops.reduce_sum(a)\n    self.assertLen(a.consumers(), 1)\n\n  @parameterized.parameters([\n      {\n          'descr': 'from_value_rowids',\n          'factory': RaggedTensor.from_value_rowids,\n          'test': RaggedTensor.value_rowids,\n          'values': {\n              'values': [1, 2, 3, 4, 5, 6],\n              'value_rowids': [0, 0, 1, 1, 2, 2],\n          },\n          'tensor_field': 'value_rowids',\n          'value_rowids': [0, 1, 2],\n          'nrows': 10\n      },\n      {\n          'descr': 'from_row_splits',\n          'factory': RaggedTensor.from_row_splits,\n          # row_splits is a property, not a function.\n          'test': (lambda rt: rt.row_splits),\n          'values': {\n              'values': [1, 2, 3, 4, 5, 6],\n              'row_splits': [0, 2, 4, 6],\n          },\n          'tensor_field': 'row_splits',\n          'row_splits': [0, 1, 2, 3]\n      },\n      {\n          'descr': 'from_row_lengths',\n          'factory': RaggedTensor.from_row_lengths,\n          'test': RaggedTensor.row_lengths,\n          'values': {\n              'values': [1, 2, 3, 4, 5, 6],\n              'row_lengths': [2, 2, 2],\n          },\n          'tensor_field': 'row_lengths',\n          'row_lengths': [1, 1, 1],\n      },\n      # from_row_starts\n      {\n          'descr': 'from_row_starts',\n          'factory': RaggedTensor.from_row_starts,\n          'test': RaggedTensor.row_starts,\n          'values': {\n              'values': [1, 2, 3, 4, 5, 6],\n              'row_starts': [0, 2, 4]\n          },\n          'tensor_field': 'row_starts',\n          'row_starts': [0, 1, 2]\n      },\n      # from_row_limits\n      {\n          'descr': 'from_row_limits',\n          'factory': RaggedTensor.from_row_limits,\n          'test': RaggedTensor.row_limits,\n          'values': {\n              'values': [1, 2, 3, 4, 5, 6],\n              'row_limits': [2, 4, 6]\n          },\n          'tensor_field': 'row_limits',\n          'row_limits': [3]\n      },\n      # from_uniform_row_length\n      {\n          'descr': 'from_uniform_row_length',\n          'factory': RaggedTensor.from_uniform_row_length,\n          # One cannot extract uniform_row_length or nvals, so we return\n          # nvals//nrows = uniform_row_length, where nvals = 3\n          'test': (lambda rt: 3 // (rt.shape[0])),\n          'values': {\n              'values': [1, 2, 3, 4, 5, 6],\n              'uniform_row_length': 2\n          },\n          'tensor_field': 'uniform_row_length',\n          'uniform_row_length': 3\n      },\n  ])\n  def testFactoryTypePreference(self, descr, test, factory, values,\n                                tensor_field, **kwargs):\n    # When input tensors have shape information, some of these errors will be\n    # detected statically.\n    def op_cast(k, v):\n      if k == tensor_field:\n        return constant_op.constant(v, dtype=dtypes.int32)\n      else:\n        return v\n\n    value_copy = {k: op_cast(k, v) for k, v in values.items()}\n    rt = factory(**value_copy)\n\n    kw_copy = {k: v for k, v in kwargs.items()}\n    kw_copy['values'] = rt\n    rt2 = factory(**kw_copy)\n    self.assertAllEqual(kwargs[tensor_field], test(rt2))\n\n  @parameterized.parameters([\n      # from_value_rowids\n      {\n          'descr': 'bad rank for value_rowids',\n          'factory': RaggedTensor.from_value_rowids,\n          'values': [[1, 2], [3, 4]],\n          'value_rowids': [[1, 2], [3, 4]],\n          'nrows': 10\n      },\n      {\n          'descr': 'bad rank for nrows',\n          'factory': RaggedTensor.from_value_rowids,\n          'values': [1, 2, 3, 4],\n          'value_rowids': [1, 2, 3, 4],\n          'nrows': [10]\n      },\n      {\n          'descr': 'len(values) != len(value_rowids)',\n          'factory': RaggedTensor.from_value_rowids,\n          'values': [1, 2, 3, 4],\n          'value_rowids': [1, 2, 3, 4, 5],\n          'nrows': 10\n      },\n      {\n          'descr': 'negative value_rowid',\n          'factory': RaggedTensor.from_value_rowids,\n          'values': [1, 2, 3, 4],\n          'value_rowids': [-5, 2, 3, 4],\n          'nrows': 10\n      },\n      {\n          'descr': 'non-monotonic-increasing value_rowid',\n          'factory': RaggedTensor.from_value_rowids,\n          'values': [1, 2, 3, 4],\n          'value_rowids': [4, 3, 2, 1],\n          'nrows': 10\n      },\n      {\n          'descr': 'value_rowid > nrows',\n          'factory': RaggedTensor.from_value_rowids,\n          'values': [1, 2, 3, 4],\n          'value_rowids': [1, 2, 3, 4],\n          'nrows': 2\n      },\n      {\n          'descr': 'bad rank for values',\n          'factory': RaggedTensor.from_value_rowids,\n          'values': 10,\n          'value_rowids': [1, 2, 3, 4],\n          'nrows': 10\n      },\n\n      # from_row_splits\n      {\n          'descr': 'bad rank for row_splits',\n          'factory': RaggedTensor.from_row_splits,\n          'values': [[1, 2], [3, 4]],\n          'row_splits': [[1, 2], [3, 4]]\n      },\n      {\n          'descr': 'row_splits[0] != 0',\n          'factory': RaggedTensor.from_row_splits,\n          'values': [1, 2, 3, 4],\n          'row_splits': [2, 3, 4]\n      },\n      {\n          'descr': 'non-monotonic-increasing row_splits',\n          'factory': RaggedTensor.from_row_splits,\n          'values': [1, 2, 3, 4],\n          'row_splits': [0, 3, 2, 4]\n      },\n      {\n          'descr': 'row_splits[0] != nvals',\n          'factory': RaggedTensor.from_row_splits,\n          'values': [1, 2, 3, 4],\n          'row_splits': [0, 2, 3, 5]\n      },\n      {\n          'descr': 'bad rank for values',\n          'factory': RaggedTensor.from_row_splits,\n          'values': 10,\n          'row_splits': [0, 1]\n      },\n\n      # from_row_lengths\n      {\n          'descr': 'bad rank for row_lengths',\n          'factory': RaggedTensor.from_row_lengths,\n          'values': [1, 2, 3, 4],\n          'row_lengths': [[1, 2], [1, 0]]\n      },\n      {\n          'descr': 'negatve row_lengths',\n          'factory': RaggedTensor.from_row_lengths,\n          'values': [1, 2, 3, 4],\n          'row_lengths': [3, -1, 2]\n      },\n      {\n          'descr': 'sum(row_lengths) != nvals',\n          'factory': RaggedTensor.from_row_lengths,\n          'values': [1, 2, 3, 4],\n          'row_lengths': [2, 4, 2, 8]\n      },\n      {\n          'descr': 'bad rank for values',\n          'factory': RaggedTensor.from_row_lengths,\n          'values': 10,\n          'row_lengths': [0, 1]\n      },\n\n      # from_row_starts\n      {\n          'descr': 'bad rank for row_starts',\n          'factory': RaggedTensor.from_row_starts,\n          'values': [[1, 2], [3, 4]],\n          'row_starts': [[1, 2], [3, 4]]\n      },\n      {\n          'descr': 'row_starts[0] != 0',\n          'factory': RaggedTensor.from_row_starts,\n          'values': [1, 2, 3, 4],\n          'row_starts': [2, 3, 4]\n      },\n      {\n          'descr': 'non-monotonic-increasing row_starts',\n          'factory': RaggedTensor.from_row_starts,\n          'values': [1, 2, 3, 4],\n          'row_starts': [0, 3, 2, 4]\n      },\n      {\n          'descr': 'row_starts[0] > nvals',\n          'factory': RaggedTensor.from_row_starts,\n          'values': [1, 2, 3, 4],\n          'row_starts': [0, 2, 3, 5]\n      },\n      {\n          'descr': 'bad rank for values',\n          'factory': RaggedTensor.from_row_starts,\n          'values': 10,\n          'row_starts': [0, 1]\n      },\n\n      # from_row_limits\n      {\n          'descr': 'bad rank for row_limits',\n          'factory': RaggedTensor.from_row_limits,\n          'values': [[1, 2], [3, 4]],\n          'row_limits': [[1, 2], [3, 4]]\n      },\n      {\n          'descr': 'row_limits[0] < 0',\n          'factory': RaggedTensor.from_row_limits,\n          'values': [1, 2, 3, 4],\n          'row_limits': [-1, 3, 4]\n      },\n      {\n          'descr': 'non-monotonic-increasing row_limits',\n          'factory': RaggedTensor.from_row_limits,\n          'values': [1, 2, 3, 4],\n          'row_limits': [0, 3, 2, 4]\n      },\n      {\n          'descr': 'row_limits[0] != nvals',\n          'factory': RaggedTensor.from_row_limits,\n          'values': [1, 2, 3, 4],\n          'row_limits': [0, 2, 3, 5]\n      },\n      {\n          'descr': 'bad rank for values',\n          'factory': RaggedTensor.from_row_limits,\n          'values': 10,\n          'row_limits': [0, 1]\n      },\n\n      # from_uniform_row_length\n      {\n          'descr': 'rowlen * nrows != nvals (1)',\n          'factory': RaggedTensor.from_uniform_row_length,\n          'values': [1, 2, 3, 4, 5],\n          'uniform_row_length': 3\n      },\n      {\n          'descr': 'rowlen * nrows != nvals (2)',\n          'factory': RaggedTensor.from_uniform_row_length,\n          'values': [1, 2, 3, 4, 5],\n          'uniform_row_length': 6\n      },\n      {\n          'descr': 'rowlen * nrows != nvals (3)',\n          'factory': RaggedTensor.from_uniform_row_length,\n          'values': [1, 2, 3, 4, 5, 6],\n          'uniform_row_length': 3,\n          'nrows': 3\n      },\n      {\n          'descr': 'rowlen must be a scalar',\n          'factory': RaggedTensor.from_uniform_row_length,\n          'values': [1, 2, 3, 4],\n          'uniform_row_length': [2]\n      },\n      {\n          'descr': 'rowlen must be nonnegative',\n          'factory': RaggedTensor.from_uniform_row_length,\n          'values': [1, 2, 3, 4],\n          'uniform_row_length': -1\n      },\n  ])\n  def testFactoryValidation(self, descr, factory, **kwargs):\n    # When input tensors have shape information, some of these errors will be\n    # detected statically.\n    with self.assertRaises((errors.InvalidArgumentError, ValueError)):\n      self.evaluate(factory(**kwargs))\n\n    # Remove shape information (by wrapping tensors in placeholders), and check\n    # that we detect the errors when the graph is run.\n    if not context.executing_eagerly():\n\n      def wrap_arg(v):\n        return array_ops.placeholder_with_default(\n            constant_op.constant(v, dtype=dtypes.int64),\n            tensor_shape.TensorShape(None))\n\n      kwargs = dict((k, wrap_arg(v)) for (k, v) in kwargs.items())\n\n      with self.assertRaises(errors.InvalidArgumentError):\n        self.evaluate(factory(**kwargs))\n\n  #=============================================================================\n  # RaggedTensor Variant conversion\n  #=============================================================================\n\n  @parameterized.named_parameters(\n      {\n          'testcase_name': 'Shape_5_none',\n          'ragged_constant': [[1, 2], [3, 4, 5], [6], [], [7]],\n          'ragged_rank': 1\n      }, {\n          'testcase_name': 'Shape_4_none_2',\n          'ragged_constant': [[[1, 2]], [], [[3, 4]], []],\n          'ragged_rank': 1\n      }, {\n          'testcase_name': 'Shape_1_none_none',\n          'ragged_constant': [[[1], [2, 3, 4, 5, 6, 7]], [[]]],\n          'ragged_rank': 2\n      })\n  def testRaggedToVariant(self, ragged_constant, ragged_rank):\n    rt = ragged_factory_ops.constant(ragged_constant, ragged_rank=ragged_rank)\n    et = rt._to_variant()\n    self.assertEqual(et.shape.as_list(), [])\n    self.assertEqual(et.dtype, dtypes.variant)\n\n  @parameterized.parameters(\n      {\n          'ragged_constant': [[1, 2], [3, 4, 5], [6], [], [7]],\n          'ragged_rank': 1,\n          'num_batched_elems': 5\n      }, {\n          'ragged_constant': [[[1, 2]], [], [[3, 4]], []],\n          'ragged_rank': 1,\n          'num_batched_elems': 4\n      }, {\n          'ragged_constant': [[[1], [2, 3, 4, 5, 6, 7]], [[]]],\n          'ragged_rank': 2,\n          'num_batched_elems': 2\n      })\n  def testRaggedToBatchedVariant(self, ragged_constant, ragged_rank,\n                                 num_batched_elems):\n    rt = ragged_factory_ops.constant(ragged_constant, ragged_rank=ragged_rank)\n    et = rt._to_variant(batched_input=True)\n    self.assertEqual(et.shape.as_list(), [num_batched_elems])\n    self.assertEqual(et.dtype, dtypes.variant)\n\n  @parameterized.parameters(\n      # 2D test cases.\n      {\n          'ragged_constant': [[]],\n          'ragged_rank': 1,\n      },\n      {\n          'ragged_constant': [[1]],\n          'ragged_rank': 1,\n      },\n      {\n          'ragged_constant': [[1, 2]],\n          'ragged_rank': 1,\n      },\n      {\n          'ragged_constant': [[1], [2], [3]],\n          'ragged_rank': 1,\n      },\n      {\n          'ragged_constant': [[1, 2, 3], [4, 5, 6], [7, 8, 9]],\n          'ragged_rank': 1,\n      },\n      {\n          'ragged_constant': [[1, 2], [3, 4, 5], [6], [], [7]],\n          'ragged_rank': 1,\n      },\n      # 3D test cases.\n      {\n          'ragged_constant': [[[]]],\n          'ragged_rank': 2,\n      },\n      {\n          'ragged_constant': [[[1]]],\n          'ragged_rank': 2,\n      },\n      {\n          'ragged_constant': [[[1, 2]]],\n          'ragged_rank': 2,\n      },\n      {\n          'ragged_constant': [[[1, 2], [3, 4]]],\n          'ragged_rank': 2,\n      },\n      {\n          'ragged_constant': [[[1, 2]], [[3, 4]], [[5, 6]], [[7, 8]]],\n          'ragged_rank': 2,\n      },\n      {\n          'ragged_constant': [[[1], [2]], [[3], [4]], [[5], [6]], [[7], [8]]],\n          'ragged_rank': 2,\n      },\n      {\n          'ragged_constant': [[[1, 2]], [], [[3, 4]], []],\n          'ragged_rank': 2,\n      },\n      # 4D test cases.\n      {\n          'ragged_constant': [[[[1, 2], [3, 4]]],\n                              [[[0, 0], [0, 0]], [[5, 6], [7, 8]]], []],\n          'ragged_rank': 3,\n      },\n      # dtype `string`.\n      {\n          'ragged_constant': [['a'], ['b'], ['c']],\n          'ragged_rank': 1,\n          'dtype': dtypes.string,\n      },\n      {\n          'ragged_constant': [[['a', 'b'], ['c', 'd']]],\n          'ragged_rank': 2,\n          'dtype': dtypes.string,\n      },\n      {\n          'ragged_constant': [[[['a', 'b'], ['c', 'd']]],\n                              [[['e', 'f'], ['g', 'h']], [['i', 'j'],\n                                                          ['k', 'l']]], []],\n          'ragged_rank': 3,\n          'dtype': dtypes.string,\n      })\n  def testVariantRoundTrip(self,\n                           ragged_constant,\n                           ragged_rank,\n                           dtype=dtypes.int32):\n    rt = ragged_factory_ops.constant(\n        ragged_constant, ragged_rank=ragged_rank, dtype=dtype)\n    et = rt._to_variant()\n    round_trip_rt = RaggedTensor._from_variant(\n        et, dtype, output_ragged_rank=ragged_rank)\n    self.assertAllEqual(rt, round_trip_rt)\n\n  def testBatchedVariantRoundTripInputRaggedRankInferred(self):\n    ragged_rank = 1\n    rt = ragged_factory_ops.constant(\n        [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9]],\n        ragged_rank=ragged_rank)\n    batched_variant = rt._to_variant(batched_input=True)\n    nested_batched_variant = array_ops.reshape(batched_variant, [5, 2])\n    decoded_rt = RaggedTensor._from_variant(\n        nested_batched_variant,\n        dtype=dtypes.int32,\n        output_ragged_rank=ragged_rank + 1)\n    expected_rt = ragged_factory_ops.constant([[[0], [1]], [[2], [3]], [[4],\n                                                                        [5]],\n                                               [[6], [7]], [[8], [9]]])\n    self.assertAllEqual(decoded_rt, expected_rt)\n\n  def testBatchedVariantRoundTripWithInputRaggedRank(self):\n    ragged_rank = 1\n    rt = ragged_factory_ops.constant(\n        [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9]],\n        ragged_rank=ragged_rank)\n    batched_variant = rt._to_variant(batched_input=True)\n    nested_batched_variant = array_ops.reshape(batched_variant, [5, 2])\n    decoded_rt = RaggedTensor._from_variant(\n        nested_batched_variant,\n        dtype=dtypes.int32,\n        output_ragged_rank=ragged_rank + 1,\n        input_ragged_rank=ragged_rank - 1)\n    expected_rt = ragged_factory_ops.constant([[[0], [1]], [[2], [3]], [[4],\n                                                                        [5]],\n                                               [[6], [7]], [[8], [9]]])\n    self.assertAllEqual(decoded_rt, expected_rt)\n\n  def testUnbatchVariant(self):  # b/141789000\n    rt = ragged_factory_ops.constant([[1, 2, 3], [4, 5], [], [6, 7, 8, 9]])\n    batched = rt._to_variant(batched_input=True)\n    for i in range(4):\n      row = RaggedTensor._from_variant(\n          batched[i], dtype=dtypes.int32, output_ragged_rank=0)\n      self.assertAllEqual(rt[i], row)\n\n  def testUnbatchVariantInDataset(self):\n    rt = ragged_factory_ops.constant([[1, 2, 3], [4, 5], [], [6, 7, 8, 9]])\n    ds = dataset_ops.Dataset.from_tensor_slices(rt)\n    if context.executing_eagerly():\n      for i, value in enumerate(ds):\n        self.assertAllEqual(rt[i], value)\n    else:\n      it = dataset_ops.make_one_shot_iterator(ds)\n      out = it.get_next()\n      with self.cached_session() as sess:\n        for i in range(3):\n          self.assertAllEqual(sess.run(rt[i]), out)\n\n  def testToVariantInvalidParams(self):\n    self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                           r'be rank 1 but is rank 0',\n                           gen_ragged_conversion_ops.ragged_tensor_to_variant,\n                           rt_nested_splits=[0, 1, 2],\n                           rt_dense_values=[0, 1, 2],\n                           batched_input=True)\n\n    self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                           r'be rank 1 but is rank 2',\n                           gen_ragged_conversion_ops.ragged_tensor_to_variant,\n                           rt_nested_splits=[[[0]], [[1]], [[2]]],\n                           rt_dense_values=[0, 1, 2],\n                           batched_input=True)\n\n  def testFromVariantInvalidParams(self):\n    rt = ragged_factory_ops.constant([[0], [1], [2], [3]])\n    batched_variant = rt._to_variant(batched_input=True)\n    nested_batched_variant = array_ops.reshape(batched_variant, [2, 2])\n    with self.assertRaisesRegex(ValueError,\n                                r'`output_ragged_rank` \\(1\\) must be equal to'):\n      RaggedTensor._from_variant(\n          nested_batched_variant,\n          dtype=dtypes.int32,\n          output_ragged_rank=1,\n          input_ragged_rank=1)\n\n  def testUnbatchToTensor(self):\n    batched = ragged_factory_ops.constant([[0], [1], [2], [3]])\n    unbatched = [constant_op.constant(x) for x in [[0], [1], [2], [3]]]\n    batched_spec = type_spec.type_spec_from_value(batched)\n\n    # Note that the unbatched_spec is derived from the batched spec, so it can\n    # add back a ragged instead of a dense tensor.\n    unbatched_spec = batched_spec._unbatch()\n    batched_tensor_list = batched_spec._to_batched_tensor_list(batched)\n    unbatched_tensor_lists = zip(\n        *[array_ops.unstack(tensor) for tensor in batched_tensor_list])\n    actual_unbatched = [\n        batched_spec._unbatch()._from_tensor_list(tensor_list)\n        for tensor_list in unbatched_tensor_lists]\n    self.assertLen(actual_unbatched, len(unbatched))\n    for x in actual_unbatched:\n      self.assertTrue(unbatched_spec.is_compatible_with(x))\n\n    for (actual, expected) in zip(actual_unbatched, unbatched):\n      self.assertAllEqual(actual, expected)\n\n  def testDatasetUnbatchTwice(self):\n    batched = ragged_factory_ops.constant([[[0], [1], [5]], [[2], [3]]])\n    ds = dataset_ops.Dataset.from_tensors(batched)\n    ds2 = ds.unbatch()\n    ds3 = ds2.unbatch()\n    if context.executing_eagerly():\n      value = next(iter(ds3))\n      self.assertAllEqual([0], value)\n\n  def testDatasetUnbatchToScalar(self):\n    batched = ragged_factory_ops.constant([[0], [1], [2], [3]])\n    ds = dataset_ops.Dataset.from_tensors(batched)\n    ds2 = ds.unbatch()\n    ds3 = ds2.unbatch()\n    if context.executing_eagerly():\n      value = next(iter(ds3))\n      self.assertAllEqual(0, value)\n\n  def testBatchToTensor(self):\n    batched = ragged_factory_ops.constant([[0], [1], [2], [3]])\n    unbatched = [constant_op.constant(x) for x in [[0], [1], [2], [3]]]\n    batched_spec = type_spec.type_spec_from_value(batched)\n\n    # Note that the unbatched_spec is derived from the batched spec, so it can\n    # add back a ragged instead of a dense tensor.\n    unbatched_spec = batched_spec._unbatch()\n    unbatched_tensor_lists = [unbatched_spec._to_tensor_list(x)\n                              for x in unbatched]\n    batched_tensor_list = [array_ops.stack(tensors)\n                           for tensors in zip(*unbatched_tensor_lists)]\n    actual_batched = unbatched_spec._batch(4)._from_tensor_list(\n        batched_tensor_list)\n    self.assertAllEqual(actual_batched, batched)\n\n  def _testGradient(self, func, x, expected_grad, grad_y=None):\n    x = ragged_factory_ops.constant(x)\n    if grad_y is not None:\n      grad_y = ragged_factory_ops.constant(grad_y)\n    if context.executing_eagerly():\n      with backprop.GradientTape() as t:\n        t.watch(x)\n        y = func(x)\n      g = t.gradient(y, x, grad_y)\n    else:\n      y = func(x)\n      g = gradients_impl.gradients(ys=y, xs=x, grad_ys=grad_y)[0]\n    if expected_grad is None:\n      self.assertIsNone(g)\n    else:\n      g = ragged_tensor.convert_to_tensor_or_ragged_tensor(g)\n      self.assertAllClose(g, expected_grad)\n\n  @parameterized.named_parameters([\n      dict(\n          testcase_name='RaggedInput',\n          func=lambda x: math_ops.reduce_prod(x, axis=1),\n          x=[[1., 2.], [3.]],\n          expected=[[2., 1.], [1.]]),\n      dict(\n          testcase_name='RaggedOutput',\n          func=lambda x: ragged_concat_ops.stack([x, x[:1]]),\n          x=[3., 2.],\n          expected=[2., 1.]),\n      dict(\n          testcase_name='RaggedInputAndOutput',\n          func=lambda x: array_ops.stack([x, x * x]),\n          x=[[1., 2.], [3.]],\n          expected=[[3., 5.], [7.]]),\n      dict(\n          testcase_name='RaggedOutputWithGradYs',\n          func=lambda x: ragged_concat_ops.stack([x, x[:1]]),\n          x=[3., 2.],\n          grad_ys=[[1., 1.], [1.]],\n          expected=[2., 1.]),\n      dict(\n          testcase_name='RaggedInputAndOutputWithGradYs',\n          func=lambda x: array_ops.stack([x, x * x]),\n          x=[[1., 2.], [3.]],\n          grad_ys=[[[1., 1.], [1.]], [[1., 1.], [1.]]],\n          expected=[[3., 5.], [7.]]),\n      dict(\n          testcase_name='RaggedRank3',\n          func=lambda x: ragged_concat_ops.stack([x, (x * x)[:, 1:]]),\n          x=[[[1., 2.], [3., 4., 5.]], [[6.]]],\n          expected=[[[1.0, 1.0], [7.0, 9.0, 11.0]], [[1.0]]]),\n      dict(\n          testcase_name='RaggedIndexedSlices',\n          func=lambda x: ragged_gather_ops.gather(x, [0, 2]),\n          x=[[1., 2.], [3.], [4., 5., 6.]],\n          expected=[[1., 1.], [0.], [1., 1., 1.]]),\n  ])\n  def testGradient(self, func, x, expected, grad_ys=None):\n    self._testGradient(func, x, expected, grad_ys)\n\n  def testHigherOrderGradient(self):\n    x = ragged_factory_ops.constant([[1.0, 2.0], [3.0]])\n\n    with backprop.GradientTape() as t2:\n      t2.watch(x)\n      with backprop.GradientTape() as t1:\n        t1.watch(x)\n        y = x * x * x\n      dy_dx = t1.gradient(y, x)\n    d2y_dx2 = t2.gradient(dy_dx, x)\n\n    self.assertAllEqual(dy_dx, [[3.0, 12.0], [27.0]])\n    self.assertAllEqual(d2y_dx2, [[6.0, 12.0], [18.0]])\n\n  def testUnconnectedGradient(self):\n    x = ragged_factory_ops.constant([[1.0, 2.0], [3.0]])\n\n    with backprop.GradientTape() as t:\n      t.watch(x)\n      y = ragged_factory_ops.constant([[2.0, 4.0], [6.0]])\n    self.assertIsNone(t.gradient(y, x))\n\n  def testStopGradient(self):\n\n    def func(x):\n      y = x * constant_op.constant([[1.], [3.]])\n      y = y.with_values(array_ops.stop_gradient(y.values))\n      z = x * y\n      return math_ops.reduce_sum(z)\n\n    self._testGradient(func, [[1., 2.], [3., 4., 5.]],\n                       [[1., 2.], [9., 12., 15.]])\n\n  def testStopGradientNoneComponent(self):\n\n    def func(x):\n      y = x * constant_op.constant([[1.], [3.]])\n      y = y.with_values(array_ops.stop_gradient(y.values))\n      return y\n\n    self._testGradient(func, [[1., 2], [3, 4, 5]], None)\n\n  def testRaggedVariantGradients(self):\n\n    def func(x):\n      rt1 = RaggedTensor.from_row_splits(values=x, row_splits=[0, 4, 7, 8])\n      rt2 = rt1 * [[10], [100], [1000]]\n      v = rt2._to_variant(batched_input=False)\n      rt3 = RaggedTensor._from_variant(v, dtype=rt2.dtype, output_ragged_rank=1)\n      return rt3.flat_values\n\n    self._testGradient(func, [3.0, 1.0, 4.0, 1.0, 1.0, 0.0, 2.0, 1.0],\n                       [10., 10., 10., 10., 100., 100., 100., 1000.])\n\n  def testRaggedVariantGradientsEmptyRows(self):\n\n    def func(x):\n      rt1 = RaggedTensor.from_row_splits(\n          values=x, row_splits=[0, 2, 2, 4, 7, 7, 8])\n      rt2 = rt1 * [[10], [20], [30], [40], [50], [60]]\n      v = rt2._to_variant(batched_input=False)\n      rt3 = RaggedTensor._from_variant(v, dtype=rt2.dtype, output_ragged_rank=1)\n      return rt3.flat_values\n\n    self._testGradient(func, [3.0, 1.0, 4.0, 1.0, 1.0, 0.0, 2.0, 1.0],\n                       [10., 10., 30., 30., 40., 40., 40., 60.])\n\n  def testRaggedVariantSteps(self):\n    x = [3.0, 1.0, 4.0, 1.0, 1.0, 0.0, 2.0, 1.0]\n    rt1 = RaggedTensor.from_row_splits(values=x, row_splits=[0, 4, 7, 8])\n    rt2 = rt1 * [[10], [100], [1000]]\n    v = rt2._to_variant(batched_input=False)\n    rt3 = RaggedTensor._from_variant(v, dtype=rt2.dtype, output_ragged_rank=1)\n    self.assertAllClose([30., 10., 40., 10., 100., 0., 200., 1000.],\n                        rt3.flat_values)\n\n  def testRaggedVariantGradientsBatched(self):\n\n    def func(x):\n      rt1 = RaggedTensor.from_row_splits(values=x, row_splits=[0, 4, 7, 8])\n      rt2 = rt1 * [[10], [100], [1000]]\n      v = rt2._to_variant(batched_input=True)\n      rt3 = RaggedTensor._from_variant(v, dtype=rt2.dtype, output_ragged_rank=1)\n      return rt3.flat_values\n\n    self._testGradient(func, [3.0, 1.0, 4.0, 1.0, 1.0, 0.0, 2.0, 1.0],\n                       [10., 10., 10., 10., 100., 100., 100., 1000.])\n\n  def testRaggedVariantGradientsEmptyRowsBatched(self):\n\n    def func(x):\n      rt1 = RaggedTensor.from_row_splits(\n          values=x, row_splits=[0, 2, 2, 4, 7, 7, 8])\n      rt2 = rt1 * [[10], [20], [30], [40], [50], [60]]\n      v = rt2._to_variant(batched_input=True)\n      rt3 = RaggedTensor._from_variant(v, dtype=rt2.dtype, output_ragged_rank=1)\n      return rt3.flat_values\n\n    self._testGradient(func, [3.0, 1.0, 4.0, 1.0, 1.0, 0.0, 2.0, 1.0],\n                       [10., 10., 30., 30., 40., 40., 40., 60.])\n\n  def testRaggedVariantGradientsEmptyOutputBatched(self):\n\n    def func(x):\n      rt1 = RaggedTensor.from_row_splits(\n          values=x, row_splits=[0, 0, 0, 0, 0, 0, 0])\n      rt2 = rt1 * [[10], [20], [30], [40], [50], [60]]\n      v = rt2._to_variant(batched_input=True)\n      rt3 = RaggedTensor._from_variant(v, dtype=rt2.dtype, output_ragged_rank=1)\n      return rt3.flat_values\n\n    self._testGradient(func, [], [])\n\n  def testRaggedVariantGradientsBatchedAndSliced(self):\n\n    def func(x, i):\n      rt1 = RaggedTensor.from_row_splits(values=x, row_splits=[0, 4, 7, 8])\n      rt2 = rt1 * [[10], [100], [1000]]\n      v_slice = rt2._to_variant(batched_input=True)[i]\n      return RaggedTensor._from_variant(\n          v_slice, dtype=rt2.dtype, output_ragged_rank=0)\n\n    self._testGradient(\n        functools.partial(func, i=0), [3.0, 1.0, 4.0, 1.0, 1.0, 0.0, 2.0, 1.0],\n        [10., 10., 10., 10., 0., 0., 0., 0.])\n    self._testGradient(\n        functools.partial(func, i=1), [3.0, 1.0, 4.0, 1.0, 1.0, 0.0, 2.0, 1.0],\n        [0., 0., 0., 0., 100., 100., 100., 0.])\n    self._testGradient(\n        functools.partial(func, i=2), [3.0, 1.0, 4.0, 1.0, 1.0, 0.0, 2.0, 1.0],\n        [0., 0., 0., 0., 0., 0., 0., 1000.])\n\n  def testRaggedVariantGradientsEmptyRowsBatchedAndSliced(self):\n\n    def func(x, i):\n      rt1 = RaggedTensor.from_row_splits(\n          values=x, row_splits=[0, 2, 2, 4, 7, 7, 8])\n      rt2 = rt1 * [[10], [20], [30], [40], [50], [60]]\n      v_slice = rt2._to_variant(batched_input=True)[i]\n      return RaggedTensor._from_variant(\n          v_slice, dtype=rt2.dtype, output_ragged_rank=0)\n\n    self._testGradient(\n        functools.partial(func, i=0), [3.0, 1.0, 4.0, 1.0, 1.0, 0.0, 2.0, 1.0],\n        [10., 10., 0., 0., 0., 0., 0., 0.])\n    self._testGradient(\n        functools.partial(func, i=1), [3.0, 1.0, 4.0, 1.0, 1.0, 0.0, 2.0, 1.0],\n        [0., 0., 0., 0., 0., 0., 0., 0.])\n    self._testGradient(\n        functools.partial(func, i=2), [3.0, 1.0, 4.0, 1.0, 1.0, 0.0, 2.0, 1.0],\n        [0., 0., 30., 30., 0., 0., 0., 0.])\n    self._testGradient(\n        functools.partial(func, i=3), [3.0, 1.0, 4.0, 1.0, 1.0, 0.0, 2.0, 1.0],\n        [0., 0., 0., 0., 40., 40., 40., 0.])\n    self._testGradient(\n        functools.partial(func, i=4), [3.0, 1.0, 4.0, 1.0, 1.0, 0.0, 2.0, 1.0],\n        [0., 0., 0., 0., 0., 0., 0., 0.])\n    self._testGradient(\n        functools.partial(func, i=5), [3.0, 1.0, 4.0, 1.0, 1.0, 0.0, 2.0, 1.0],\n        [0., 0., 0., 0., 0., 0., 0., 60.])\n\n  def testRaggedVariantGradientsRaggedRank0(self):\n\n    def func(x):\n      x2 = x * 2\n      v = gen_ragged_conversion_ops.ragged_tensor_to_variant(\n          [], x2, batched_input=False)\n      return RaggedTensor._from_variant(v, dtype=x2.dtype, output_ragged_rank=0)\n\n    self._testGradient(func, [3.0, 1.0, 4.0, 1.0, 1.0, 0.0, 2.0, 1.0],\n                       [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0])\n\n  def testRaggedVariantGradientsRaggedRank3(self):\n\n    def func(x):\n      x2 = x * 2\n      rt1 = RaggedTensor.from_nested_row_splits(\n          x2, ([0, 0, 3], [0, 2, 2, 3], [0, 4, 7, 8]))\n      v = rt1._to_variant(batched_input=False)\n      rt3 = RaggedTensor._from_variant(v, dtype=x2.dtype, output_ragged_rank=3)\n      return rt3.flat_values\n\n    self._testGradient(func, [3.0, 1.0, 4.0, 1.0, 1.0, 0.0, 2.0, 1.0],\n                       [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0])\n\n  def testRaggedVariantGradientsViaMapFn(self):\n    rt = RaggedTensor.from_row_splits(\n        values=[3, 1.0, 4, 1, 5, 9, 2, 6], row_splits=[0, 4, 7, 8])\n\n    def func(x):\n\n      def transform_row(row):\n        return math_ops.sqrt(\n            math_ops.reduce_mean(math_ops.square(row * x), keepdims=True))\n\n      return math_ops.reduce_sum(map_fn.map_fn(transform_row, rt))\n\n    self._testGradient(func, 3.0, 14.653377)\n\n  def testRaggedVariantGradientsEmptyRowsViaMapFn(self):\n    rt = RaggedTensor.from_row_splits(\n        values=[3, 1.0, 4, 1, 5, 9, 2, 6], row_splits=[0, 2, 2, 4, 7, 7, 8])\n\n    def func(x):\n\n      def transform_row(row):\n        return math_ops.sqrt(\n            math_ops.reduce_mean(math_ops.square(row * x), keepdims=True))\n\n      return math_ops.reduce_sum(map_fn.map_fn(transform_row, rt))\n\n    self._testGradient(func, 3.0, 17.206844)\n\n  def testRaggedVariantGradientsEmptyOutputViaMapFn(self):\n    rt = RaggedTensor.from_row_splits(\n        values=[], row_splits=[0, 0, 0, 0])\n\n    def func(x):\n\n      def transform_row(row):\n        return math_ops.sqrt(\n            math_ops.reduce_mean(math_ops.square(row * x), keepdims=True))\n\n      return math_ops.reduce_sum(map_fn.map_fn(transform_row, rt))\n\n    self._testGradient(func, 3.0, 0.0)\n\n  def testRaggedVariantGradientsViaMapFnReduce(self):\n\n    def func(x):\n      rt1 = RaggedTensor.from_row_splits(values=x, row_splits=[0, 4, 7, 8])\n      return map_fn.map_fn(\n          math_ops.reduce_max,\n          rt1,\n          fn_output_signature=tensor_spec.TensorSpec((), x.dtype))\n\n    self._testGradient(func, [3.0, 1.0, 4.0, 1.0, 1.0, 0.0, 2.0, 1.0],\n                       [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0])\n\n  def testRaggedVariantGradientsEmptyRowsViaMapFnReduce(self):\n\n    def func(x):\n      rt1 = RaggedTensor.from_row_splits(\n          values=x, row_splits=[0, 2, 2, 4, 7, 7, 8])\n      return map_fn.map_fn(\n          math_ops.reduce_max,\n          rt1,\n          fn_output_signature=tensor_spec.TensorSpec((), x.dtype))\n\n    self._testGradient(func, [3.0, 1.0, 4.0, 1.0, 1.0, 0.0, 2.0, 1.0],\n                       [1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0])\n\n  def testRaggedVariantGradientsEmptyOutputViaMapFnReduce(self):\n\n    def func(x):\n      rt1 = RaggedTensor.from_row_splits(\n          values=x, row_splits=[0, 0, 0, 0])\n      return map_fn.map_fn(\n          math_ops.reduce_max,\n          rt1,\n          fn_output_signature=tensor_spec.TensorSpec((), x.dtype))\n\n    self._testGradient(func, [], [])\n\n  def testRaggedVariantGradientsErrors(self):\n    if context.executing_eagerly():\n      return\n\n    rt = RaggedTensor.from_row_splits([1.0, 2.0], row_splits=[0, 2, 2])\n    v1 = rt._to_variant()\n    v2 = array_ops.stack([array_ops.stack([v1])])\n    y = RaggedTensor._from_variant(v2, rt.dtype, output_ragged_rank=3)\n\n    with self.assertRaisesRegex(\n        ValueError, 'Unable to compute gradient: RaggedTensorToVariant '\n        'can currently only generate 0D or 1D output.'):\n      gradients_impl.gradients(ys=y.flat_values, xs=rt.flat_values)\n\n  def assertNumpyObjectTensorsRecursivelyEqual(self, a, b, msg):\n    \"\"\"Check that two numpy arrays are equal.\n\n    For arrays with dtype=object, check values recursively to see if a and b\n    are equal.  (c.f. `np.array_equal`, which checks dtype=object values using\n    object identity.)\n\n    Args:\n      a: A numpy array.\n      b: A numpy array.\n      msg: Message to display if a != b.\n    \"\"\"\n    if isinstance(a, np.ndarray) and a.dtype == object:\n      self.assertEqual(a.dtype, b.dtype, msg)\n      self.assertEqual(a.shape, b.shape, msg)\n      self.assertLen(a, len(b), msg)\n      for a_val, b_val in zip(a, b):\n        self.assertNumpyObjectTensorsRecursivelyEqual(a_val, b_val, msg)\n    else:\n      self.assertAllEqual(a, b, msg)\n\n  @parameterized.named_parameters([\n      ('Shape_2_R',\n       [[1, 2], [3, 4, 5]],\n       np.array([int32array([1, 2]), int32array([3, 4, 5])])),\n      ('Shape_2_2',\n       [[1, 2], [3, 4]],\n       np.array([[1, 2], [3, 4]])),\n      ('Shape_2_R_2',\n       [[[1, 2], [3, 4]], [[5, 6]]],\n       np.array([int32array([[1, 2], [3, 4]]), int32array([[5, 6]])])),\n      ('Shape_3_2_R',\n       [[[1], []], [[2, 3], [4]], [[], [5, 6, 7]]],\n       np.array([[int32array([1]), int32array([])],\n                 [int32array([2, 3]), int32array([4])],\n                 [int32array([]), int32array([5, 6, 7])]])),\n      ('Shape_0_R',\n       ragged_factory_ops.constant_value([], ragged_rank=1, dtype=np.int32),\n       np.zeros([0, 0], dtype=np.int32)),\n      ('Shape_0_R_2',\n       ragged_factory_ops.constant_value([], ragged_rank=1,\n                                         inner_shape=(2,), dtype=np.int32),\n       np.zeros([0, 0, 2], dtype=np.int32)),\n  ])  # pyformat: disable\n  def testRaggedTensorNumpy(self, rt, expected):\n    if isinstance(rt, list):\n      rt = ragged_factory_ops.constant(rt, dtype=dtypes.int32)\n    else:\n      rt = ragged_tensor.convert_to_tensor_or_ragged_tensor(rt)\n    if context.executing_eagerly():\n      actual = rt.numpy()\n      self.assertNumpyObjectTensorsRecursivelyEqual(\n          expected, actual, 'Expected %r, got %r' % (expected, actual))\n    else:\n      with self.assertRaisesRegex(ValueError, 'only supported in eager mode'):\n        rt.numpy()\n\n  @parameterized.parameters([\n      ([[[1, 2], [3, 4, 5]], [[6]]], 2, None),\n      ([[[1, 2], [3, 4, 5]], [[6]]], 2, [None, None, None]),\n      ([[[1, 2], [3, 4, 5]], [[6]]], 2, [2, None, None]),\n      ([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9]]], 1, None),\n      ([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9]]], 1, [None, None, None]),\n      ([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9]]], 1, [2, None, None]),\n      ([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9]]], 1, [2, None, 3]),\n      ([[[1, 2, 3]]], 1, [1, 1, None]),\n      ([[[1, 2, 3]]], 1, [1, 1, 3]),\n  ])\n  def testRaggedTensorSetShape(self, rt, rt_ragged_rank, shape):\n    rt1 = ragged_factory_ops.constant(rt, ragged_rank=rt_ragged_rank)\n    rt1._set_shape(shape)\n    rt1.shape.assert_is_compatible_with(shape)\n    if shape is not None:\n      self.assertIsNot(rt1.shape.rank, None)\n      for a, b in zip(rt1.shape, shape):\n        if b is not None:\n          self.assertEqual(a, b)\n\n  @parameterized.parameters([\n      ([[[1, 2], [3, 4, 5]], [[6]]], 2, None),\n      ([[[1, 2], [3, 4, 5]], [[6]]], 2, [None, None, None]),\n      ([[[1, 2], [3, 4, 5]], [[6]]], 2, [2, None, None]),\n      ([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9]]], 1, None),\n      ([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9]]], 1, [None, None, None]),\n      ([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9]]], 1, [2, None, None]),\n      ([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9]]], 1, [2, None, 3]),\n      ([[[1, 2, 3]]], 1, [1, 1, None]),\n      ([[[1, 2, 3]]], 1, [1, 1, 3]),\n  ])\n  def testRaggedTensorSetShapeWithPlaceholders(self, rt, rt_ragged_rank, shape):\n    rt2 = nest.map_structure(\n        lambda x: array_ops.placeholder_with_default(x, None),\n        ragged_factory_ops.constant(rt, ragged_rank=rt_ragged_rank),\n        expand_composites=True)\n    rt2._set_shape(shape)\n    rt2.shape.assert_is_compatible_with(shape)\n    if shape is not None:\n      self.assertIsNot(rt2.shape.rank, None)\n      for a, b in zip(rt2.shape, shape):\n        if b is not None:\n          self.assertEqual(a, b)\n\n  def testRaggedTensorSetShapeUniformRowLength(self):\n    rt = [[[1], [2], [3]], [[4], [5], [6]]]\n\n    rt1 = RaggedTensor.from_tensor(rt, ragged_rank=1)\n    rt1._set_shape([2, 3, 1])\n\n    rt2 = nest.map_structure(\n        lambda x: array_ops.placeholder_with_default(x, None),\n        rt1,\n        expand_composites=True)\n    rt2._set_shape([2, 3, 1])\n\n  def testRaggedTensorSetShapeInconsistentShapeError(self):\n    rt = RaggedTensor.from_tensor([[[1], [2], [3]], [[4], [5], [6]]],\n                                  ragged_rank=1)\n    self.assertEqual(rt.shape.as_list(), [2, 3, 1])\n    with self.assertRaises(ValueError):\n      rt._set_shape([None, None, 5])\n    with self.assertRaisesRegex(ValueError, 'Inconsistent size'):\n      rt._set_shape([None, 5, None])\n    with self.assertRaises(ValueError):\n      rt._set_shape([5, None, None])\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass RaggedTensorSpecTest(test_util.TensorFlowTestCase,\n                           parameterized.TestCase):\n\n  def assertAllTensorsEqual(self, list1, list2):\n    self.assertLen(list1, len(list2))\n    for (t1, t2) in zip(list1, list2):\n      self.assertAllEqual(t1, t2)\n\n  def testConstruction(self):\n    spec1 = RaggedTensorSpec(ragged_rank=1)\n    self.assertIsNone(spec1._shape.rank)\n    self.assertEqual(spec1._dtype, dtypes.float32)\n    self.assertEqual(spec1._row_splits_dtype, dtypes.int64)\n    self.assertEqual(spec1._ragged_rank, 1)\n\n    self.assertIsNone(spec1.shape.rank)\n    self.assertEqual(spec1.dtype, dtypes.float32)\n    self.assertEqual(spec1.row_splits_dtype, dtypes.int64)\n    self.assertEqual(spec1.ragged_rank, 1)\n\n    spec2 = RaggedTensorSpec(shape=[None, None, None])\n    self.assertEqual(spec2._shape.as_list(), [None, None, None])\n    self.assertEqual(spec2._dtype, dtypes.float32)\n    self.assertEqual(spec2._row_splits_dtype, dtypes.int64)\n    self.assertEqual(spec2._ragged_rank, 2)\n\n    with self.assertRaisesRegex(ValueError, 'Must specify ragged_rank'):\n      RaggedTensorSpec()\n    with self.assertRaisesRegex(TypeError, '`ragged_rank` must be an int'):\n      RaggedTensorSpec(ragged_rank=constant_op.constant(1))\n    with self.assertRaisesRegex(\n        ValueError,\n        r'Argument `ragged_rank` \\(2\\) must be less than rank \\(2\\).'):\n      RaggedTensorSpec(ragged_rank=2, shape=[None, None])\n\n  def testValueType(self):\n    spec1 = RaggedTensorSpec(ragged_rank=1)\n    self.assertEqual(spec1.value_type, RaggedTensor)\n    spec2 = RaggedTensorSpec(ragged_rank=0)\n    self.assertEqual(spec2.value_type, ops.Tensor)\n\n  @parameterized.parameters([\n      (RaggedTensorSpec(ragged_rank=1),\n       (tensor_shape.TensorShape(None), dtypes.float32, 1, dtypes.int64)),\n      (RaggedTensorSpec(shape=[5, None, None]),\n       (tensor_shape.TensorShape([5, None, None]), dtypes.float32,\n        2, dtypes.int64)),\n      (RaggedTensorSpec(shape=[5, None, None], dtype=dtypes.int32),\n       (tensor_shape.TensorShape([5, None, None]), dtypes.int32, 2,\n        dtypes.int64)),\n      (RaggedTensorSpec(ragged_rank=1, row_splits_dtype=dtypes.int32),\n       (tensor_shape.TensorShape(None), dtypes.float32, 1, dtypes.int32)),\n  ])  # pyformat: disable\n  def testSerialize(self, rt_spec, expected):\n    serialization = rt_spec._serialize()\n    # TensorShape has an unconventional definition of equality, so we can't use\n    # assertEqual directly here.  But repr() is deterministic and lossless for\n    # the expected values, so we can use that instead.\n    self.assertEqual(repr(serialization), repr(expected))\n\n  @parameterized.parameters([\n      (RaggedTensorSpec(ragged_rank=0, shape=[5, 3]), [\n          tensor_spec.TensorSpec([5, 3], dtypes.float32),\n      ]),\n      (RaggedTensorSpec(ragged_rank=1), [\n          tensor_spec.TensorSpec(None, dtypes.float32),\n          tensor_spec.TensorSpec([None], dtypes.int64)\n      ]),\n      (RaggedTensorSpec(ragged_rank=1, row_splits_dtype=dtypes.int32), [\n          tensor_spec.TensorSpec(None, dtypes.float32),\n          tensor_spec.TensorSpec([None], dtypes.int32),\n      ]),\n      (RaggedTensorSpec(ragged_rank=2), [\n          tensor_spec.TensorSpec(None, dtypes.float32),\n          tensor_spec.TensorSpec([None], dtypes.int64),\n          tensor_spec.TensorSpec([None], dtypes.int64),\n      ]),\n      (RaggedTensorSpec(shape=[5, None, None], dtype=dtypes.string), [\n          tensor_spec.TensorSpec([None], dtypes.string),\n          tensor_spec.TensorSpec([6], dtypes.int64),\n          tensor_spec.TensorSpec([None], dtypes.int64),\n      ]),\n  ])\n  def testComponentSpecs(self, rt_spec, expected):\n    self.assertEqual(rt_spec._component_specs, expected)\n\n  @parameterized.parameters([\n      {\n          'rt_spec': RaggedTensorSpec(ragged_rank=0),\n          'rt': [1.0, 2.0, 3.0],\n          'components': [[1.0, 2.0, 3.0]]\n      },\n      {\n          'rt_spec': RaggedTensorSpec(ragged_rank=1),\n          'rt': [[1.0, 2.0], [3.0]],\n          'components': [[1.0, 2.0, 3.0], [0, 2, 3]]\n      },\n      {\n          'rt_spec': RaggedTensorSpec(shape=[2, None, None]),\n          'rt': [[[1.0, 2.0], [3.0]], [[], [4.0]]],\n          'components': [[1.0, 2.0, 3.0, 4.0], [0, 2, 4], [0, 2, 3, 3, 4]]\n      },\n  ])\n  def testToFromComponents(self, rt_spec, rt, components):\n    rt = ragged_factory_ops.constant(rt)\n    actual_components = rt_spec._to_components(rt)\n    self.assertAllTensorsEqual(actual_components, components)\n    rt_reconstructed = rt_spec._from_components(actual_components)\n    self.assertAllEqual(rt, rt_reconstructed)\n\n  @parameterized.parameters([\n      {\n          'flat_value_spec': tensor_spec.TensorSpec(None, dtypes.float32),\n          'row_splits_spec': tensor_spec.TensorSpec(None, dtypes.int64),\n      },\n      {\n          'flat_value_spec': tensor_spec.TensorSpec([None,], dtypes.float32),\n          'row_splits_spec': tensor_spec.TensorSpec(None, dtypes.int64),\n      },\n      {\n          'flat_value_spec': tensor_spec.TensorSpec(None, dtypes.float32),\n          'row_splits_spec': tensor_spec.TensorSpec([None,], dtypes.int64),\n      },\n      {\n          'flat_value_spec': tensor_spec.TensorSpec([None,], dtypes.float32),\n          'row_splits_spec': tensor_spec.TensorSpec([None,], dtypes.int64),\n      },\n      {\n          'flat_value_spec': tensor_spec.TensorSpec([4,], dtypes.float32),\n          'row_splits_spec': tensor_spec.TensorSpec(None, dtypes.int64),\n      },\n      {\n          'flat_value_spec': tensor_spec.TensorSpec(None, dtypes.float32),\n          'row_splits_spec': tensor_spec.TensorSpec([3,], dtypes.int64),\n      },\n  ])\n  def testToFromComponentsStaticUnknownShape(self, flat_value_spec,\n                                             row_splits_spec):\n    rt_spec = RaggedTensorSpec(shape=[2, None], ragged_rank=1)\n    tester = self\n\n    @def_function.function(input_signature=[flat_value_spec, row_splits_spec])\n    def test_fn(flat_value, row_splits):\n      # Apply static shape information saved in rt_spec to rt.\n      rt = rt_spec._from_components([flat_value, row_splits])\n      tester.assertEqual(rt.shape.as_list(), [2, None])\n      return rt + ragged_factory_ops.constant([[1.0, 1.0, 1.0], [1.0]])\n\n    result = test_fn([1.0, 2.0, 3.0, 4.0], [0, 3, 4])\n    expected_result = ragged_factory_ops.constant([[2.0, 3.0, 4.0], [5.0]])\n    self.assertAllEqual(result, expected_result)\n\n  @test_util.run_v1_only('RaggedTensorValue is deprecated in v2')\n  def testFromNumpyComponents(self):\n    spec1 = RaggedTensorSpec(ragged_rank=1, dtype=dtypes.int32)\n    rt1 = spec1._from_components([np.array([1, 2, 3]), np.array([0, 2, 3])])\n    self.assertIsInstance(rt1, ragged_tensor_value.RaggedTensorValue)\n    self.assertAllEqual(rt1, [[1, 2], [3]])\n\n    spec2 = RaggedTensorSpec(ragged_rank=2, dtype=dtypes.int32)\n    rt2 = spec2._from_components(\n        [np.array([1, 2, 3]),\n         np.array([0, 2, 3]),\n         np.array([0, 0, 2, 3])])\n    self.assertIsInstance(rt2, ragged_tensor_value.RaggedTensorValue)\n    self.assertAllEqual(rt2, [[[], [1, 2]], [[3]]])\n\n    spec3 = RaggedTensorSpec(ragged_rank=0, dtype=dtypes.int32)\n    rt3 = spec3._from_components([np.array([1, 2, 3])])\n    self.assertIsInstance(rt3, np.ndarray)\n    self.assertAllEqual(rt3, [1, 2, 3])\n\n  @parameterized.parameters([\n      RaggedTensorSpec(ragged_rank=0, shape=[5, 3]),\n      RaggedTensorSpec(ragged_rank=1),\n      RaggedTensorSpec(ragged_rank=1, row_splits_dtype=dtypes.int32),\n      RaggedTensorSpec(ragged_rank=2, dtype=dtypes.string),\n      RaggedTensorSpec(shape=[5, None, None]),\n  ])\n  def testFlatTensorSpecs(self, rt_spec):\n    self.assertEqual(rt_spec._flat_tensor_specs,\n                     [tensor_spec.TensorSpec(None, dtypes.variant)])\n\n  @parameterized.parameters([\n      (dtypes.float32, full_type_pb2.TFT_FLOAT),\n      (dtypes.string, full_type_pb2.TFT_STRING),\n  ])\n  def testFullTypesForFlatTensors(self, dt, ft):\n    rt_spec = RaggedTensorSpec(ragged_rank=2, dtype=dt)\n    full_type_list = fulltypes_for_flat_tensors(rt_spec)\n    expect = [\n        full_type_pb2.FullTypeDef(\n            type_id=full_type_pb2.TFT_RAGGED,\n            args=[full_type_pb2.FullTypeDef(type_id=ft)])\n    ]\n    self.assertEqual(len(rt_spec._flat_tensor_specs), len(full_type_list))\n    self.assertEqual(expect, full_type_list)\n\n  @parameterized.named_parameters([\n      {\n          'testcase_name': 'RaggedRank0',\n          'rt_spec': RaggedTensorSpec(ragged_rank=0),\n          'rt': [1.0, 2.0, 3.0],\n      },\n      {\n          'testcase_name': 'RaggedRank1',\n          'rt_spec': RaggedTensorSpec(ragged_rank=1),\n          'rt': [[1.0, 2.0], [3.0]]\n      },\n      {\n          'testcase_name': 'RaggedRank2',\n          'rt_spec': RaggedTensorSpec(shape=[2, None, None]),\n          'rt': [[[1.0, 2.0], [3.0]], [[], [4.0]]]\n      },\n  ])\n  def testToFromTensorList(self, rt_spec, rt):\n    rt = ragged_factory_ops.constant(rt)\n    tensor_list = rt_spec._to_tensor_list(rt)\n    rt_reconstructed = rt_spec._from_tensor_list(tensor_list)\n    self.assertAllEqual(rt, rt_reconstructed)\n\n  @parameterized.named_parameters([\n      # TODO(b/141789000) Test ragged_rank=0 when support is added.\n      {\n          'testcase_name': 'RaggedRank1',\n          'rt_spec': RaggedTensorSpec(ragged_rank=1),\n          'rt': [[1.0, 2.0], [3.0]]\n      },\n      {\n          'testcase_name': 'RaggedRank2',\n          'rt_spec': RaggedTensorSpec(shape=[2, None, None]),\n          'rt': [[[1.0, 2.0], [3.0]], [[], [4.0]]]\n      },\n  ])\n  def testToFromBatchedTensorList(self, rt_spec, rt):\n    rt = ragged_factory_ops.constant(rt)\n    tensor_list = rt_spec._to_batched_tensor_list(rt)\n    rt_reconstructed = rt_spec._from_tensor_list(tensor_list)\n    self.assertAllEqual(rt, rt_reconstructed)\n    first_row = rt_spec._unbatch()._from_tensor_list(\n        [t[0] for t in tensor_list])\n    self.assertAllEqual(rt[0], first_row)\n\n  def testToFromBatchedTensorListPreservesUniformRowLengths(self):\n    rt = RaggedTensor.from_tensor(array_ops.zeros([3, 4, 5]), ragged_rank=2)\n    rt_spec = rt._type_spec\n    tensor_list = rt_spec._to_batched_tensor_list(rt)\n    rt_reconstructed = rt_spec._from_tensor_list(tensor_list)\n    self.assertAllEqual(rt, rt_reconstructed)\n    self.assertTrue(rt.shape.is_fully_defined())\n    self.assertTrue(rt_reconstructed.shape.is_fully_defined())\n    self.assertEqual(rt.shape.as_list(), rt_reconstructed.shape.as_list())\n\n  @parameterized.parameters([\n      (RaggedTensorSpec([2, None], dtypes.float32, 1), 32,\n       RaggedTensorSpec([32, 2, None], dtypes.float32, 2)),\n      (RaggedTensorSpec([4, None], dtypes.float32, 1), None,\n       RaggedTensorSpec([None, 4, None], dtypes.float32, 2)),\n      (RaggedTensorSpec([2], dtypes.float32,\n                        -1), 32, RaggedTensorSpec([32, 2], dtypes.float32, 0)),\n  ])\n  def testBatch(self, spec, batch_size, expected):\n    self.assertEqual(spec._batch(batch_size), expected)\n\n  @parameterized.parameters([\n      (RaggedTensorSpec([32, None, None], dtypes.float32, 2),\n       RaggedTensorSpec([None, None], dtypes.float32, 1)),\n      (RaggedTensorSpec([None, None, None], dtypes.float32, 2),\n       RaggedTensorSpec([None, None], dtypes.float32, 1)),\n      (RaggedTensorSpec([32, 2], dtypes.float32, 0),\n       RaggedTensorSpec([2], dtypes.float32, -1)),\n      (RaggedTensorSpec([32, None, 4], dtypes.float32, 1, dtypes.int32),\n       RaggedTensorSpec([None, 4], dtypes.float32, 0, dtypes.int32)),\n  ])  # pyformat: disable\n  def testUnbatch(self, spec, expected):\n    self.assertEqual(spec._unbatch(), expected)\n\n  def testIsCompatibleWith(self):\n    spec1 = RaggedTensorSpec([32, None, None], dtypes.float32, 2)\n    spec2 = RaggedTensorSpec(None, dtypes.float32, 2)\n    spec3 = RaggedTensorSpec(None, dtypes.int32, 1)\n    spec4 = RaggedTensorSpec([None], dtypes.int32, 0)\n\n    self.assertTrue(spec1.is_compatible_with(spec2))\n    self.assertFalse(spec1.is_compatible_with(spec3))\n    self.assertFalse(spec1.is_compatible_with(spec4))\n    self.assertFalse(spec2.is_compatible_with(spec3))\n    self.assertFalse(spec2.is_compatible_with(spec4))\n    self.assertFalse(spec3.is_compatible_with(spec4))\n    self.assertTrue(spec4.is_compatible_with(constant_op.constant([1, 2, 3])))\n\n\nif __name__ == '__main__':\n  googletest.main()\n"], "filenames": ["tensorflow/core/kernels/ragged_tensor_to_variant_op.cc", "tensorflow/python/ops/ragged/ragged_tensor_test.py"], "buggy_code_start_loc": [190, 1469], "buggy_code_end_loc": [190, 1469], "fixing_code_start_loc": [191, 1470], "fixing_code_end_loc": [195, 1485], "type": "CWE-617", "message": "TensorFlow is an open source platform for machine learning. If `RaggedTensorToVariant` is given a `rt_nested_splits` list that contains tensors of ranks other than one, it results in a `CHECK` fail that can be used to trigger a denial of service attack. We have patched the issue in GitHub commit 88f93dfe691563baa4ae1e80ccde2d5c7a143821. The fix will be included in TensorFlow 2.10.0. We will also cherrypick this commit on TensorFlow 2.9.1, TensorFlow 2.8.1, and TensorFlow 2.7.2, as these are also affected and still in supported range. There are no known workarounds for this issue.", "other": {"cve": {"id": "CVE-2022-36018", "sourceIdentifier": "security-advisories@github.com", "published": "2022-09-16T22:15:11.827", "lastModified": "2022-09-20T14:55:12.353", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "TensorFlow is an open source platform for machine learning. If `RaggedTensorToVariant` is given a `rt_nested_splits` list that contains tensors of ranks other than one, it results in a `CHECK` fail that can be used to trigger a denial of service attack. We have patched the issue in GitHub commit 88f93dfe691563baa4ae1e80ccde2d5c7a143821. The fix will be included in TensorFlow 2.10.0. We will also cherrypick this commit on TensorFlow 2.9.1, TensorFlow 2.8.1, and TensorFlow 2.7.2, as these are also affected and still in supported range. There are no known workarounds for this issue."}, {"lang": "es", "value": "TensorFlow es una plataforma de c\u00f3digo abierto para el aprendizaje autom\u00e1tico. Si a \"RaggedTensorToVariant\" le es dada una lista \"rt_nested_splits\" que contiene tensores de rangos diferentes a uno, es producido un fallo de \"CHECK\" que puede ser usado para desencadenar un ataque de denegaci\u00f3n de servicio. Hemos parcheado el problema en el commit 88f93dfe691563baa4ae1e80ccde2d5c7a143821 de GitHub. La correcci\u00f3n ser\u00e1 incluida en TensorFlow versi\u00f3n 2.10.0. Tambi\u00e9n seleccionaremos este compromiso en TensorFlow versi\u00f3n 2.9.1, TensorFlow versi\u00f3n 2.8.1, y TensorFlow versi\u00f3n 2.7.2, ya que estos tambi\u00e9n est\u00e1n afectados y todav\u00eda est\u00e1n en el rango admitido. No se presentan mitigaciones conocidas para este problema"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 7.5, "baseSeverity": "HIGH"}, "exploitabilityScore": 3.9, "impactScore": 3.6}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:H/PR:N/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "HIGH", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.9, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 2.2, "impactScore": 3.6}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-617"}]}, {"source": "security-advisories@github.com", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-617"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionEndExcluding": "2.7.2", "matchCriteriaId": "C6622D95-1C86-45C5-AB55-E6EEEA0996DF"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.8.0", "versionEndExcluding": "2.8.1", "matchCriteriaId": "0F9D273D-02DC-441E-AA91-EAC8DEAA4B44"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.9.0", "versionEndExcluding": "2.9.1", "matchCriteriaId": "FE4F8A81-6CC2-4F7F-9602-C170FDD926E7"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.10:rc0:*:*:*:*:*:*", "matchCriteriaId": "1DBFBCE2-0A01-4575-BE45-6775ABFB8B28"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.10:rc1:*:*:*:*:*:*", "matchCriteriaId": "89806CF9-E423-4CA6-A01A-8175C260CB24"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.10:rc2:*:*:*:*:*:*", "matchCriteriaId": "F2B80690-A257-4E16-BD27-9AE045BC56ED"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.10:rc3:*:*:*:*:*:*", "matchCriteriaId": "F335F9A4-5AB8-4E53-BC18-E01F7C653E5E"}]}]}], "references": [{"url": "https://github.com/tensorflow/tensorflow/commit/88f93dfe691563baa4ae1e80ccde2d5c7a143821", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/security/advisories/GHSA-m6cv-4fmf-66xf", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/tensorflow/tensorflow/commit/88f93dfe691563baa4ae1e80ccde2d5c7a143821"}}