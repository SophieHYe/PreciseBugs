{"buggy_code": ["#!/bin/bash\n\n# Copyright 2014,2015,2016,2017,2018,2019,2020 Security Onion Solutions, LLC\n\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU General Public License as published by\n#    the Free Software Foundation, either version 3 of the License, or\n#    (at your option) any later version.\n#\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU General Public License for more details.\n#\n#    You should have received a copy of the GNU General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\n. /usr/sbin/so-common\n\nUPDATE_DIR=/tmp/sogh/securityonion\nINSTALLEDVERSION=$(cat /etc/soversion)\nINSTALLEDSALTVERSION=$(salt --versions-report | grep Salt: | awk {'print $2'})\nDEFAULT_SALT_DIR=/opt/so/saltstack/default\nBATCHSIZE=5\nSOUP_LOG=/root/soup.log\n\nexec 3>&1 1>${SOUP_LOG} 2>&1\n\nadd_common() {\n  cp $UPDATE_DIR/salt/common/tools/sbin/so-common $DEFAULT_SALT_DIR/salt/common/tools/sbin/\n  cp $UPDATE_DIR/salt/common/tools/sbin/so-image-common $DEFAULT_SALT_DIR/salt/common/tools/sbin/\n  salt-call state.apply common queue=True\n  echo \"Run soup one more time\"\n  exit 0\n}\n\nairgap_mounted() {\n  # Let's see if the ISO is already mounted. \n  if [ -f /tmp/soagupdate/SecurityOnion/VERSION ]; then\n    echo \"The ISO is already mounted\"\n  else\n    echo \"\"\n    echo \"Looks like we need access to the upgrade content\"\n    echo \"\" \n    echo \"If you just copied the .iso file over you can specify the path.\"\n    echo \"If you burned the ISO to a disk the standard way you can specify the device.\"\n    echo \"Example: /home/user/securityonion-2.X.0.iso\"\n    echo \"Example: /dev/sdx1\"\n    echo \"\"\n    read -p 'Enter the location of the iso: ' ISOLOC\n    if [ -f $ISOLOC ]; then\n      # Mounting the ISO image\n      mkdir -p /tmp/soagupdate\n      mount -t iso9660 -o loop $ISOLOC /tmp/soagupdate\n      # Make sure mounting was successful\n      if [ ! -f /tmp/soagupdate/SecurityOnion/VERSION ]; then\n        echo \"Something went wrong trying to mount the ISO.\"\n        echo \"Ensure you verify the ISO that you downloaded.\"\n        exit 0\n      else\n        echo \"ISO has been mounted!\"\n      fi  \n    elif [ -f $ISOLOC/SecurityOnion/VERSION ]; then\n      ln -s $ISOLOC /tmp/soagupdate\n      echo \"Found the update content\"\n    else \n      mkdir -p /tmp/soagupdate\n      mount $ISOLOC /tmp/soagupdate\n      if [ ! -f /tmp/soagupdate/SecurityOnion/VERSION ]; then\n        echo \"Something went wrong trying to mount the device.\"\n        echo \"Ensure you verify the ISO that you downloaded.\"\n        exit 0\n      else\n        echo \"Device has been mounted!\"\n      fi        \n    fi\n  fi\n}\n\nairgap_update_dockers() {\n  if [ $is_airgap -eq 0 ]; then\n    # Let's copy the tarball\n    if [ ! -f $AGDOCKER/registry.tar ]; then\n      echo \"Unable to locate registry. Exiting\"\n      exit 1\n    else\n      echo \"Stopping the registry docker\"\n      docker stop so-dockerregistry\n      docker rm so-dockerregistry\n      echo \"Copying the new dockers over\"\n      tar xvf $AGDOCKER/registry.tar -C /nsm/docker-registry/docker\n      echo \"Add Registry back\"\n      docker load -i $AGDOCKER/registry_image.tar\n    fi\n  fi\n}\n\nupdate_registry() {\n  docker stop so-dockerregistry\n  docker rm so-dockerregistry\n  salt-call state.apply registry queue=True\n}\n\ncheck_airgap() {\n  # See if this is an airgap install\n  AIRGAP=$(cat /opt/so/saltstack/local/pillar/global.sls | grep airgap | awk '{print $2}')\n  if [[ \"$AIRGAP\" == \"True\" ]]; then\n      is_airgap=0\n      UPDATE_DIR=/tmp/soagupdate/SecurityOnion\n      AGDOCKER=/tmp/soagupdate/docker\n      AGREPO=/tmp/soagupdate/Packages\n  else \n      is_airgap=1\n  fi\n}\n\nclean_dockers() {\n  # Place Holder for cleaning up old docker images\n  echo \"Trying to clean up old dockers.\"\n  docker system prune -a -f\n\n}\n\nclone_to_tmp() {\n  # Clean old files\n  rm -rf /tmp/sogh\n  # Make a temp location for the files\n  mkdir -p /tmp/sogh\n  cd /tmp/sogh\n  SOUP_BRANCH=\"\"\n  if [ -n \"$BRANCH\" ]; then\n    SOUP_BRANCH=\"-b $BRANCH\"\n  fi\n  git clone $SOUP_BRANCH https://github.com/Security-Onion-Solutions/securityonion.git\n  cd /tmp\n  if [ ! -f $UPDATE_DIR/VERSION ]; then\n    echo \"Update was unable to pull from github. Please check your internet.\"\n    exit 0\n  fi\n}\n\ncopy_new_files() {\n  # Copy new files over to the salt dir\n  cd $UPDATE_DIR\n  rsync -a salt $DEFAULT_SALT_DIR/\n  rsync -a pillar $DEFAULT_SALT_DIR/\n  chown -R socore:socore $DEFAULT_SALT_DIR/\n  chmod 755 $DEFAULT_SALT_DIR/pillar/firewall/addfirewall.sh\n  cd /tmp\n}\n\nhighstate() {\n  # Run a highstate.\n  salt-call state.highstate -l info queue=True\n}\n\nmasterlock() {\n  echo \"Locking Salt Master\"\n  if [[ \"$INSTALLEDVERSION\" =~ rc.1 ]]; then\n    TOPFILE=/opt/so/saltstack/default/salt/top.sls\n    BACKUPTOPFILE=/opt/so/saltstack/default/salt/top.sls.backup\n    mv -v $TOPFILE $BACKUPTOPFILE\n    echo \"base:\" > $TOPFILE\n    echo \"  $MINIONID:\" >> $TOPFILE\n    echo \"    - ca\" >> $TOPFILE\n    echo \"    - ssl\" >> $TOPFILE\n    echo \"    - elasticsearch\" >> $TOPFILE\n  fi\n}\n\nmasterunlock() {\n  echo \"Unlocking Salt Master\"\n  if [[ \"$INSTALLEDVERSION\" =~ rc.1 ]]; then\n    mv -v $BACKUPTOPFILE $TOPFILE\n  fi\n}\n\nplaybook() {\n  echo \"Applying playbook settings\"\n  if [[ \"$INSTALLEDVERSION\" =~ rc.1 ]]; then\n    salt-call state.apply playbook.OLD_db_init\n    rm -f /opt/so/rules/elastalert/playbook/*.yaml\n    so-playbook-ruleupdate >> /root/soup_playbook_rule_update.log 2>&1 &\n  fi\n}\n\npillar_changes() {\n    # This function is to add any new pillar items if needed.\n    echo \"Checking to see if pillar changes are needed.\"\n    \n    [[ \"$INSTALLEDVERSION\" =~ rc.1 ]] && rc1_to_rc2\n    [[ \"$INSTALLEDVERSION\" =~ rc.2 ]] && rc2_to_rc3\n    [[ \"$INSTALLEDVERSION\" =~ rc.3 ]] && rc3_to_2.3.0\n\n}\n\nrc1_to_rc2() {\n\n  # Move the static file to global.sls\n  echo \"Migrating static.sls to global.sls\"\n  mv -v /opt/so/saltstack/local/pillar/static.sls /opt/so/saltstack/local/pillar/global.sls >> \"$SOUP_LOG\" 2>&1\n  sed -i '1c\\global:' /opt/so/saltstack/local/pillar/global.sls >> \"$SOUP_LOG\" 2>&1\n\n  # Moving baseurl from minion sls file to inside global.sls\n  local line=$(grep '^  url_base:' /opt/so/saltstack/local/pillar/minions/$MINIONID.sls)\n  sed -i '/^  url_base:/d' /opt/so/saltstack/local/pillar/minions/$MINIONID.sls;\n  sed -i \"/^global:/a \\\\$line\" /opt/so/saltstack/local/pillar/global.sls;\n\n  # Adding play values to the global.sls\n  local HIVEPLAYSECRET=$(tr -dc 'a-zA-Z0-9' < /dev/urandom  | fold -w 20 | head -n 1)\n  local CORTEXPLAYSECRET=$(tr -dc 'a-zA-Z0-9' < /dev/urandom  | fold -w 20 | head -n 1)\n  sed -i \"/^global:/a \\\\  hiveplaysecret: $HIVEPLAYSECRET\" /opt/so/saltstack/local/pillar/global.sls;\n  sed -i \"/^global:/a \\\\  cortexplaysecret: $CORTEXPLAYSECRET\" /opt/so/saltstack/local/pillar/global.sls;\n\n  # Move storage nodes to hostname for SSL\n  # Get a list we can use:\n  grep -A1 searchnode /opt/so/saltstack/local/pillar/data/nodestab.sls | grep -v '\\-\\-' | sed '$!N;s/\\n/ /' | awk '{print $1,$3}' | awk '/_searchnode:/{gsub(/\\_searchnode:/, \"_searchnode\"); print}' >/tmp/nodes.txt\n  # Remove the nodes from cluster settings\n  while read p; do\n  local NAME=$(echo $p | awk '{print $1}')\n  local IP=$(echo $p | awk '{print $2}')\n  echo \"Removing the old cross cluster config for $NAME\"\n  curl -XPUT -H 'Content-Type: application/json' http://localhost:9200/_cluster/settings -d '{\"persistent\":{\"cluster\":{\"remote\":{\"'$NAME'\":{\"skip_unavailable\":null,\"seeds\":null}}}}}'\n  done </tmp/nodes.txt\n  # Add the nodes back using hostname\n  while read p; do\n      local NAME=$(echo $p | awk '{print $1}')\n      local EHOSTNAME=$(echo $p | awk -F\"_\" '{print $1}')\n\t    local IP=$(echo $p | awk '{print $2}')\n      echo \"Adding the new cross cluster config for $NAME\"\n      curl -XPUT http://localhost:9200/_cluster/settings -H'Content-Type: application/json' -d '{\"persistent\": {\"search\": {\"remote\": {\"'$NAME'\": {\"skip_unavailable\": \"true\", \"seeds\": [\"'$EHOSTNAME':9300\"]}}}}}'\n  done </tmp/nodes.txt\n\n  INSTALLEDVERSION=rc.2\n\n}\n\nrc2_to_rc3() {\n\n  # move location of local.rules\n  cp /opt/so/saltstack/default/salt/idstools/localrules/local.rules /opt/so/saltstack/local/salt/idstools/local.rules\n  \n  if [ -f /opt/so/saltstack/local/salt/idstools/localrules/local.rules ]; then\n    cat /opt/so/saltstack/local/salt/idstools/localrules/local.rules >> /opt/so/saltstack/local/salt/idstools/local.rules\n  fi\n  rm -rf /opt/so/saltstack/local/salt/idstools/localrules\n  rm -rf /opt/so/saltstack/default/salt/idstools/localrules\n\n  # Rename mdengine to MDENGINE\n  sed -i \"s/  zeekversion/  mdengine/g\" /opt/so/saltstack/local/pillar/global.sls\n  # Enable Strelka Rules\n  sed -i \"/  rules:/c\\  rules: 1\" /opt/so/saltstack/local/pillar/global.sls\n\n  INSTALLEDVERSION=rc.3\n\n}\n\nrc3_to_2.3.0() {\n  # Fix Tab Complete\n  if [ ! -f /etc/profile.d/securityonion.sh ]; then\n    echo \"complete -cf sudo\" > /etc/profile.d/securityonion.sh\n  fi\n\n  {\n      echo \"redis_settings:\"\n      echo \"  redis_maxmemory: 827\"\n      echo \"playbook:\"\n      echo \"  api_key: de6639318502476f2fa5aa06f43f51fb389a3d7f\" \n  } >> /opt/so/saltstack/local/pillar/global.sls\n\n  sed -i 's/playbook:/playbook_db:/' /opt/so/saltstack/local/pillar/secrets.sls\n  {\n    echo \"playbook_admin: $(tr -dc 'a-zA-Z0-9' < /dev/urandom | fold -w 20 | head -n 1)\"\n    echo \"playbook_automation: $(tr -dc 'a-zA-Z0-9' < /dev/urandom | fold -w 20 | head -n 1)\"\n  } >> /opt/so/saltstack/local/pillar/secrets.sls\n}\n\nspace_check() {\n  # Check to see if there is enough space\n  CURRENTSPACE=$(df -BG / | grep -v Avail | awk '{print $4}' | sed 's/.$//')\n  if [ \"$CURRENTSPACE\" -lt \"10\" ]; then\n      echo \"You are low on disk space. Upgrade will try and clean up space.\";\n      clean_dockers\n  else\n      echo \"Plenty of space for upgrading\"\n  fi\n  \n}\n\nunmount_update() {\n  cd /tmp\n  umount /tmp/soagupdate\n}\n\nupdate_centos_repo() {\n  # Update the files in the repo\n  echo \"Syncing new updates to /nsm/repo\"\n  rsync -av $AGREPO/* /nsm/repo/\n  echo \"Creating repo\"\n  createrepo /nsm/repo\n}\n\nupdate_version() {\n  # Update the version to the latest\n  echo \"Updating the Security Onion version file.\"\n  echo $NEWVERSION > /etc/soversion\n  sed -i \"/  soversion:/c\\  soversion: $NEWVERSION\" /opt/so/saltstack/local/pillar/global.sls\n}\n\nupgrade_check() {\n    # Let's make sure we actually need to update.\n    NEWVERSION=$(cat $UPDATE_DIR/VERSION)\n    if [ \"$INSTALLEDVERSION\" == \"$NEWVERSION\" ]; then\n      echo \"You are already running the latest version of Security Onion.\"\n      exit 0\n    fi \n}\n\nupgrade_check_salt() {\n    NEWSALTVERSION=$(grep version: $UPDATE_DIR/salt/salt/master.defaults.yaml | awk {'print $2'})\n    if [ \"$INSTALLEDSALTVERSION\" == \"$NEWSALTVERSION\" ]; then\n      echo \"You are already running the correct version of Salt for Security Onion.\"\n    else\n      UPGRADESALT=1\n    fi\n}   \nupgrade_salt() {\n      SALTUPGRADED=True\n      echo \"Performing upgrade of Salt from $INSTALLEDSALTVERSION to $NEWSALTVERSION.\"\n      echo \"\"\n      # If CentOS\n      if [ \"$OS\" == \"centos\" ]; then\n        echo \"Removing yum versionlock for Salt.\"\n        echo \"\"\n        yum versionlock delete \"salt-*\"\n        echo \"Updating Salt packages and restarting services.\"\n        echo \"\"\n\tif [ $is_airgap -eq 0 ]; then\n          sh $UPDATE_DIR/salt/salt/scripts/bootstrap-salt.sh -r -F -M -x python3 stable \"$NEWSALTVERSION\"\n\telse \n          sh $UPDATE_DIR/salt/salt/scripts/bootstrap-salt.sh -F -M -x python3 stable \"$NEWSALTVERSION\"\n\tfi\n        echo \"Applying yum versionlock for Salt.\"\n        echo \"\"\n        yum versionlock add \"salt-*\"\n      # Else do Ubuntu things\n      elif [ \"$OS\" == \"ubuntu\" ]; then\n        echo \"Removing apt hold for Salt.\"\n        echo \"\"\n        apt-mark unhold \"salt-common\"\n        apt-mark unhold \"salt-master\"\n        apt-mark unhold \"salt-minion\"\n        echo \"Updating Salt packages and restarting services.\"\n        echo \"\"\n        sh $UPDATE_DIR/salt/salt/scripts/bootstrap-salt.sh -F -M -x python3 stable \"$NEWSALTVERSION\"\n        echo \"Applying apt hold for Salt.\"\n        echo \"\"\n        apt-mark hold \"salt-common\"\n        apt-mark hold \"salt-master\"\n        apt-mark hold \"salt-minion\"\n      fi\n}\n\nverify_latest_update_script() {\n    # Check to see if the update scripts match. If not run the new one.\n    CURRENTSOUP=$(md5sum /opt/so/saltstack/default/salt/common/tools/sbin/soup | awk '{print $1}')\n    GITSOUP=$(md5sum $UPDATE_DIR/salt/common/tools/sbin/soup | awk '{print $1}')\n    if [[ \"$CURRENTSOUP\" == \"$GITSOUP\" ]]; then\n      echo \"This version of the soup script is up to date. Proceeding.\"\n    else\n      echo \"You are not running the latest soup version. Updating soup.\"\n      cp $UPDATE_DIR/salt/common/tools/sbin/soup $DEFAULT_SALT_DIR/salt/common/tools/sbin/\n      salt-call state.apply common queue=True\n      echo \"\"\n      echo \"soup has been updated. Please run soup again.\"\n      exit 0\n    fi\n}\n\nmain () {\nwhile getopts \":b\" opt; do\n  case \"$opt\" in\n    b ) # process option b\n       shift\n       BATCHSIZE=$1\n       if ! [[ \"$BATCHSIZE\" =~ ^[0-9]+$ ]]; then\n         echo \"Batch size must be a number greater than 0.\"\n         exit 1\n       fi\n      ;;\n    \\? ) echo \"Usage: cmd [-b]\"\n      ;;\n  esac\ndone\n\necho \"Checking to see if this is a manager.\"\necho \"\"\nrequire_manager\nset_minionid\necho \"Checking to see if this is an airgap install\"\necho \"\"\ncheck_airgap\necho \"Found that Security Onion $INSTALLEDVERSION is currently installed.\"\necho \"\"\nset_os\necho \"\"\nif [ $is_airgap -eq 0 ]; then\n  # Let's mount the ISO since this is airgap\n  airgap_mounted\nelse\n  echo \"Cloning Security Onion github repo into $UPDATE_DIR.\"\n  clone_to_tmp\nfi\nif [ -f /usr/sbin/so-image-common ]; then\n  . /usr/sbin/so-image-common\nelse \nadd_common\nfi\n\necho \"\"\necho \"Verifying we have the latest soup script.\"\nverify_latest_update_script\necho \"\"\n\necho \"Let's see if we need to update Security Onion.\"\nupgrade_check\nspace_check\n\necho \"Checking for Salt Master and Minion updates.\"\nupgrade_check_salt\n\necho \"\"\necho \"Performing upgrade from Security Onion $INSTALLEDVERSION to Security Onion $NEWVERSION.\"\necho \"\"\necho \"Updating dockers to $NEWVERSION.\"\nif [ $is_airgap -eq 0 ]; then\n  airgap_update_dockers\nelse\n  update_registry\n  update_docker_containers \"soup\"\nfi\necho \"\"\necho \"Stopping Salt Minion service.\"\nsystemctl stop salt-minion\necho \"\"\necho \"Stopping Salt Master service.\"\nsystemctl stop salt-master\necho \"\"\n\n# Does salt need upgraded. If so update it.\nif [ \"$UPGRADESALT\" == \"1\" ]; then\n  echo \"Upgrading Salt\"\n  # Update the repo files so it can actually upgrade\n  if [ $is_airgap -eq 0 ]; then\n    update_centos_repo\n    yum clean all\n  fi\n  upgrade_salt\nfi\n\necho \"Checking if Salt was upgraded.\"\necho \"\"\n# Check that Salt was upgraded, should be 3 'salt' packages on a manager node. salt-minion, salt-master and salt or salt-common depending on Ubuntu or CentOS. we could add salt-syndic in the future so checking that there are at least 3 packages\nif [[ `rpm -qa | grep salt | grep $NEWSALTVERSION | wc -l` < 3 ]]; then\n  echo \"Salt upgrade failed. Check of indicators of failure in $SOUP_LOG.\"\n  echo \"Once the issue is resolved, run soup again.\"\n  echo \"Exiting.\"\n  echo \"\"\n  exit 1\nelse\n  echo \"Salt upgrade success.\"\n  echo \"\"\nfi\n\necho \"Making pillar changes.\"\npillar_changes\necho \"\"\n\n# Only update the repo if its airgap\nif [[ $is_airgap -eq 0 ]] && [[ \"$UPGRADESALT\" != \"1\" ]]; then\nupdate_centos_repo\nfi\n\necho \"\"\necho \"Copying new Security Onion code from $UPDATE_DIR to $DEFAULT_SALT_DIR.\"\ncopy_new_files\necho \"\"\nupdate_version\n\necho \"\"\necho \"Locking down Salt Master for upgrade\"\nmasterlock\n\necho \"\"\necho \"Starting Salt Master service.\"\nsystemctl start salt-master\n\necho \"\"\necho \"Running a highstate to complete the Security Onion upgrade on this manager. This could take several minutes.\"\nhighstate\necho \"\"\necho \"Upgrade from $INSTALLEDVERSION to $NEWVERSION complete.\"\n\necho \"\"\necho \"Stopping Salt Master to remove ACL\"\nsystemctl stop salt-master\n\nmasterunlock\n\necho \"\"\necho \"Starting Salt Master service.\"\nsystemctl start salt-master\nhighstate\nplaybook\nunmount_update\n\nif [ \"$UPGRADESALT\" == \"1\" ]; then\n  echo \"\"\n  echo \"Upgrading Salt on the remaining Security Onion nodes from $INSTALLEDSALTVERSION to $NEWSALTVERSION.\"\n  if [ $is_airgap -eq 0 ]; then\n    salt -C 'not *_eval and not *_helix and not *_manager and not *_managersearch and not *_standalone' cmd.run \"yum clean all\"\n  fi\n  salt -C 'not *_eval and not *_helix and not *_manager and not *_managersearch and not *_standalone' -b $BATCHSIZE state.apply salt.minion \n  echo \"\"\nfi\n\n}\n\nmain \"$@\" | tee /dev/fd/3\n", "#!/bin/bash\n\n# Copyright 2014,2015,2016,2017,2018,2019,2020 Security Onion Solutions, LLC\n\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU General Public License as published by\n#    the Free Software Foundation, either version 3 of the License, or\n#    (at your option) any later version.\n#\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU General Public License for more details.\n#\n#    You should have received a copy of the GNU General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\nsource ./so-whiptail\nsource ./so-variables\nsource ./so-common-functions\n\nCONTAINER_REGISTRY=quay.io\n\nSOVERSION=$(cat ../VERSION)\n\nlog() {\n\tmsg=$1\n\tlevel=${2:-I}\n\tnow=$(TZ=GMT date +\"%Y-%m-%dT%H:%M:%SZ\")\n\techo -e \"$now | $level | $msg\" >> \"$setup_log\" 2>&1\n}\n\nerror() {\n\tlog \"$1\" \"E\"\n}\n\ninfo() {\n\tlog \"$1\" \"I\"\n}\n\ntitle() {\n\techo -e \"\\n-----------------------------\\n $1\\n-----------------------------\\n\" >> \"$setup_log\" 2>&1\n}\n\nlogCmd() {\n\tcmd=$1\n\tinfo \"Executing command: $cmd\"\n\t$cmd >> \"$setup_log\" 2>&1\n}\n\nairgap_rules() {\n\t# Copy the rules for suricata if using Airgap\n\tmkdir -p /nsm/repo/rules\n\tcp -v /root/SecurityOnion/agrules/emerging-all.rules /nsm/repo/rules/\n\t\n\t# Copy over sigma rules\n\tcp -Rv /root/SecurityOnion/agrules/sigma /nsm/repo/rules/\n\n\t# Don't leave Strelka out\n\tcp -Rv /root/SecurityOnion/agrules/strelka /nsm/repo/rules/\n\n\n}\n\nanalyze_system() {\n\ttitle \"System Characteristics\"\n\tlogCmd \"uptime\"\n\tlogCmd \"uname -a\"\n\tlogCmd \"free -h\"\n\tlogCmd \"lscpu\"\n\tlogCmd \"df -h\"\n\tlogCmd \"ip a\"\n}\n\naccept_salt_key_remote() {\n\tsystemctl restart salt-minion\n\t\n\techo \"Accept the key remotely on the manager\" >> \"$setup_log\" 2>&1\n\t# Delete the key just in case.\n\tssh -i /root/.ssh/so.key soremote@\"$MSRV\" sudo salt-key -d \"$MINION_ID\" -y\n\tsalt-call state.apply ca >> /dev/null 2>&1\n\tssh -i /root/.ssh/so.key soremote@\"$MSRV\" sudo salt-key -a \"$MINION_ID\" -y\n\n}\n\n\nadd_admin_user() {\n\t# Add an admin user with full sudo rights if this is an ISO install. \n\t{\n\t\tuseradd \"$ADMINUSER\";\n\t\techo \"$ADMINUSER\":\"$ADMINPASS1\" | chpasswd --crypt-method=SHA512;\n\t\tusermod -aG wheel \"$ADMINUSER\";\n\t} >> \"$setup_log\" 2>&1\n\t\n}\n\nadd_manager_hostfile() {\n\n\t[ -n \"$TESTING\" ] && return\n\n\techo \"Checking if I can resolve manager. If not add to hosts file\" >> \"$setup_log\" 2>&1\n\t# Pop up an input to get the IP address\n\tMSRVIP=$(whiptail --title \"Security Onion Setup\" --inputbox \\\n\t\"Enter your Manager Server IP Address:\" 10 60 X.X.X.X 3>&1 1>&2 2>&3)\n\n\tlocal exitstatus=$?\n\twhiptail_check_exitstatus $exitstatus\n}\n\nadd_mngr_ip_to_hosts() {\n\techo \"$MSRVIP   $MSRV\" >> /etc/hosts\n}\n\naddtotab_generate_templates() {\n\n\tlocal addtotab_path=$local_salt_dir/pillar/data\n\n\tfor i in evaltab managersearchtab managertab nodestab sensorstab standalonetab; do\n\t\tprintf '%s\\n'\\\n\t\t\"$i:\"\\\n\t\t\"\" > \"$addtotab_path\"/$i.sls\n\t\techo \"Added $i Template\"\n\tdone\n\n}\n\n# $5 => (optional) password variable\nso_add_user() {\n\tlocal username=$1\n\tlocal uid=$2\n\tlocal gid=$3\n\tlocal home_dir=$4\n\tif [ \"$5\" ]; then local pass=$5; fi\n\n\techo \"Add $username user\" >> \"$setup_log\" 2>&1\n\tgroupadd --gid \"$gid\" \"$username\"\n\tuseradd -m --uid \"$uid\" --gid \"$gid\" --home-dir \"$home_dir\" \"$username\"\n\n\t# If a password has been passed in, set the password\n\tif [ \"$pass\" ]; then\n\t\techo \"$username\":\"$pass\" | chpasswd --crypt-method=SHA512\n\tfi\n}\n\nadd_socore_user_manager() {\n\tso_add_user \"socore\" \"939\" \"939\" \"/opt/so\" >> \"$setup_log\" 2>&1\n}\n\nadd_soremote_user_manager() {\n\tso_add_user \"soremote\" \"947\" \"947\" \"/home/soremote\" \"$SOREMOTEPASS1\" >> \"$setup_log\" 2>&1\n}\n\nwait_for_file() {\n\tlocal filename=$1\n\tlocal max_attempts=$2 # this is multiplied by the wait interval, so make sure it isn't too large\n\tlocal cur_attempts=0\n\tlocal wait_interval=$3\n\tlocal total_time=$(( max_attempts * wait_interval ))\n\tlocal date\n\tdate=$(date)\n\n\twhile [[ $cur_attempts -lt $max_attempts ]]; do\n\t\tif [ -f \"$filename\" ]; then\n\t\t\techo \"File $filename found at $date\" >> \"$setup_log\" 2>&1\n\t\t\treturn 0\n\t\telse\n\t\t\t((cur_attempts++))\n\t\t\techo \"File $filename does not exist; waiting ${wait_interval}s then checking again ($cur_attempts/$max_attempts)...\" >> \"$setup_log\" 2>&1\n\t\t\tsleep \"$wait_interval\"\n\t\tfi\n\tdone\n\techo \"Could not find $filename after waiting ${total_time}s\" >> \"$setup_log\" 2>&1\n\treturn 1\n}\n\nadd_web_user() {\n\twait_for_file /opt/so/conf/kratos/db/db.sqlite 30 5\n\t{\n\t\techo \"Attempting to add administrator user for web interface...\";\n\t\techo \"$WEBPASSWD1\" | /usr/sbin/so-user add \"$WEBUSER\";\n\t\techo \"Add user result: $?\";\n\t} >> \"/root/so-user-add.log\" 2>&1\n}\n\n# Create an secrets pillar so that passwords survive re-install\nsecrets_pillar(){\n  if [ ! -f $local_salt_dir/pillar/secrets.sls ]; then\n\techo \"Creating Secrets Pillar\" >> \"$setup_log\" 2>&1\n\tmkdir -p $local_salt_dir/pillar\n\tprintf '%s\\n'\\\n\t\t\"secrets:\"\\\n\t\t\"  mysql: $MYSQLPASS\"\\\n\t\t\"  playbook_db: $PLAYBOOKDBPASS\"\\\n\t\t\"  playbook_admin: $PLAYBOOKADMINPASS\"\\\n\t\t\"  playbook_automation: $PLAYBOOKAUTOMATIONPASS\"\\\n\t\t\"  grafana_admin: $GRAFANAPASS\"\\\n\t\t\"  fleet: $FLEETPASS\"\\\n\t\t\"  fleet_jwt: $FLEETJWT\"\\\n\t\t\"  fleet_enroll-secret: False\" > $local_salt_dir/pillar/secrets.sls\n  fi\n}\n\ncheck_admin_pass() {\n\tcheck_pass_match \"$ADMINPASS1\" \"$ADMINPASS2\" \"APMATCH\"\n}\n\ncheck_hive_init() {\n\n\twait_for_file /opt/so/state/thehive.txt 20 5\n\tlocal return_val=$?\n\tif [[ $return_val -ne 0 ]]; then\n\t\treturn $return_val\n\tfi\n\n\tdocker stop so-thehive\n\tdocker rm so-thehive\n}\n\ncheck_network_manager_conf() {\n\tlocal gmdconf=\"/usr/lib/NetworkManager/conf.d/10-globally-managed-devices.conf\"\n\tlocal nmconf=\"/etc/NetworkManager/NetworkManager.conf\"\n\tlocal preupdir=\"/etc/NetworkManager/dispatcher.d/pre-up.d\"\n\n\tif test -f \"$gmdconf\" && ! test -f \"${gmdconf}.bak\"; then\n\t\t{\n\t\t\tmv \"$gmdconf\" \"${gmdconf}.bak\"\n\t\t\ttouch \"$gmdconf\"\n\t\t\tsystemctl restart NetworkManager\n\t\t} >> \"$setup_log\" 2>&1\n\tfi\n\t\n\tif test -f \"$nmconf\"; then\n\t\tsed -i 's/managed=false/managed=true/g' \"$nmconf\" >> \"$setup_log\" 2>&1\n\t\tsystemctl restart NetworkManager >> \"$setup_log\" 2>&1\n\tfi\n\n\tif [[ ! -d \"$preupdir\" ]]; then\n\t\tmkdir \"$preupdir\" >> \"$setup_log\" 2>&1\n\tfi\n}\n\ncheck_pass_match() {\n\tlocal pass=$1\n\tlocal confirm_pass=$2\n\tlocal var=$3\n\n\tif [ \"$pass\" = \"$confirm_pass\" ]; then\n\t\texport \"$var=yes\"\n\telse\n\t\twhiptail_passwords_dont_match\n\tfi\n}\n\ncheck_service_status() {\n\n\tlocal service_name=$1\n\techo \"Checking service $service_name status\" >> \"$setup_log\" 2>&1\n\tsystemctl status $service_name > /dev/null 2>&1\n\tlocal status=$?\n\t#true if there is an issue with the service false if it is running properly\n\tif [ $status -gt 0 ]; then\n\t\techo \"$service_name is not running\" >> \"$setup_log\" 2>&1\n\t\techo 1;\n\telse\n\t\techo \"$service_name is running\" >> \"$setup_log\" 2>&1\n\t\techo 0;\n\tfi\n\n}\n\ncheck_salt_master_status() {\n\techo \"Checking if we can talk to the salt master\" >> \"$setup_log\" 2>&1\n\tsalt-call saltutil.kill_all_jobs > /dev/null 2>&1\n\tsalt-call state.show_top > /dev/null 2>&1\n\tlocal status=$?\n\t#true if there is an issue talking to salt master\n\tif [ $status -gt 0 ]; then\n\t\techo 1;\n\telse\n\t\techo \"Can talk to salt master\" >> \"$setup_log\" 2>&1\n\t\techo 0;\n\tfi\n\n}\n\ncheck_salt_minion_status() {\n\techo \"Checking if the salt minion will respond to jobs\" >> \"$setup_log\" 2>&1\n\tsalt \"$MINION_ID\" test.ping >> \"$setup_log\" 2>&1\n\tlocal status=$?\n\t#true if there is an issue getting a job response from the minion\n\tif [ $status -gt 0 ]; then\n\t\techo 1;\n\telse\n\t\techo \"Received job response from salt minion\" >> \"$setup_log\" 2>&1\n\t\techo 0;\n\tfi\n\n}\n\ncheck_soremote_pass() {\n\tcheck_pass_match \"$SOREMOTEPASS1\" \"$SOREMOTEPASS2\" \"SCMATCH\"\n}\n\ncheck_fleet_node_pass() {\n\tcheck_pass_match \"$FLEETNODEPASSWD1\" \"$FLEETNODEPASSWD2\" \"FPMATCH\"\n}\n\ncheck_web_pass() {\n\tcheck_pass_match \"$WEBPASSWD1\" \"$WEBPASSWD2\" \"WPMATCH\"\n}\n\nclear_manager() {\n\t# Clear out the old manager public key in case this is a re-install.\n\t# This only happens if you re-install the manager.\n\tif [ -f /etc/salt/pki/minion/minion_master.pub ]; then\n\t\t{\n\t\t\techo \"Clearing old Salt master key\";\n\t\t\trm -f /etc/salt/pki/minion/minion_master.pub;\n\t\t\tsystemctl -q restart salt-minion;\n\t\t} >> \"$setup_log\" 2>&1\n\tfi\n\n}\n\ncollect_soremote_inputs() {\n\twhiptail_create_soremote_user\n\tSCMATCH=no\n\twhile [[ $SCMATCH != yes ]]; do\n\t\twhiptail_create_soremote_user_password1\n\t\twhiptail_create_soremote_user_password2\n\t\tcheck_soremote_pass\n\tdone\n}\n\ncollect_adminuser_inputs() {\n\twhiptail_create_admin_user\n\tAPMATCH=no\n\twhile [[ $APMATCH != yes ]]; do\n\t\twhiptail_create_admin_user_password1\n\t\twhiptail_create_admin_user_password2\n\t\tcheck_admin_pass\n\tdone\n}\n\ncollect_fleet_custom_hostname_inputs() {\n\twhiptail_fleet_custom_hostname\n}\n\ncollect_fleetuser_inputs() {\n\t# Get a username & password for the Fleet admin user\n\tlocal valid_user=no\n\twhile [[ $valid_user != yes ]]; do\n\t\twhiptail_create_fleet_node_user\t\t\n\t\tif so-user valemail \"$FLEETNODEUSER\" >> \"$setup_log\" 2>&1; then\n\t\t\tvalid_user=yes\n\t\telse\n\t\t\twhiptail_invalid_user_warning\n\t\tfi\n\tdone\n\n\tFPMATCH=no\n\twhile [[ $FPMATCH != yes ]]; do\n\t\twhiptail_create_fleet_node_user_password1\n\t\twhile ! check_password \"$FLEETNODEPASSWD1\"; do\n\t\t\twhiptail_invalid_pass_characters_warning\n\t\t\twhiptail_create_fleet_node_user_password1\n\t\tdone\n\t\twhiptail_create_fleet_node_user_password2\n\t\tcheck_fleet_node_pass\n\tdone\n}\n\n\ncollect_webuser_inputs() {\n\t# Get a password for the web admin user\n\tlocal valid_user=no\n\twhile [[ $valid_user != yes ]]; do\n\t\twhiptail_create_web_user\t\t\n\t\tif so-user valemail \"$WEBUSER\" >> \"$setup_log\" 2>&1; then\n\t\t\tvalid_user=yes\n\t\telse\n\t\t\twhiptail_invalid_user_warning\n\t\tfi\n\tdone\n\n\tWPMATCH=no\n\twhile [[ $WPMATCH != yes ]]; do\n\t    whiptail_create_web_user_password1\n\t    while ! check_password \"$WEBPASSWD1\"; do\n\t\t\twhiptail_invalid_pass_characters_warning\n\t\t\twhiptail_create_web_user_password1\n\t\tdone\n\t\tif echo \"$WEBPASSWD1\" | so-user valpass >> \"$setup_log\" 2>&1; then\n\t\t\twhiptail_create_web_user_password2\n\t\t\tcheck_web_pass\n\t\telse\n\t\t\twhiptail_invalid_pass_warning\n\t\tfi\n\tdone\n}\n\nconfigure_minion() {\n\tlocal minion_type=$1\n\techo \"Configuring minion type as $minion_type\" >> \"$setup_log\" 2>&1\n\techo \"role: so-$minion_type\" > /etc/salt/grains\n\n\tlocal minion_config=/etc/salt/minion\n\n\techo \"id: '$MINION_ID'\" > \"$minion_config\"\n\n\tcase \"$minion_type\" in\n\t\t'helix')\n\t\t\techo \"master: '$HOSTNAME'\" >> \"$minion_config\"\n\t\t\t;;\n\t\t'manager' | 'eval' | 'managersearch' | 'standalone' | 'import')\n\t\t\tprintf '%s\\n'\\\n\t\t\t\t\"master: '$HOSTNAME'\"\\\n\t\t\t\t\"mysql.host: '$MAINIP'\"\\\n\t\t\t\t\"mysql.port: '3306'\"\\\n\t\t\t\t\"mysql.user: 'root'\" >> \"$minion_config\"\n\t\t\tif [ ! -f $local_salt_dir/pillar/secrets.sls ]; then\n\t\t\t\techo \"mysql.pass: '$MYSQLPASS'\" >> \"$minion_config\"\n\t\t\telse\n\t\t\t\tOLDPASS=$(grep \"mysql\" $local_salt_dir/pillar/secrets.sls | awk '{print $2}')\n\t\t\t\techo \"mysql.pass: '$OLDPASS'\" >> \"$minion_config\"\n\t\t\tfi\n\t\t\t;;\n\t\t*)\n\t\t\techo \"master: '$MSRV'\" >> \"$minion_config\"\n\t\t\t;;\n\tesac\n\n\tprintf '%s\\n'\\\n\t\t\"use_superseded:\"\\\n\t\t\"  - module.run\"\\\n\t\t\"log_file: /opt/so/log/salt/minion\" >> \"$minion_config\"\n\n\t{\n\t\tsystemctl restart salt-minion;\n\t} >> \"$setup_log\" 2>&1\n}\n\ncheckin_at_boot() {\n\tlocal minion_config=/etc/salt/minion\n\n\techo \"Enabling checkin at boot\" >> \"$setup_log\" 2>&1\n\techo \"startup_states: highstate\" >> \"$minion_config\"\n}\n\ncheck_requirements() {\n\tlocal standalone_or_dist=$1\n\tlocal node_type=$2 # optional\n\tlocal req_mem\n\tlocal req_cores\n\tlocal req_storage\n\tlocal nic_list\n\treadarray -t nic_list <<< \"$(ip link| awk -F: '$0 !~ \"lo|vir|veth|br|docker|wl|^[^0-9]\"{print $2}' | grep -vwe \"bond0\"  | sed 's/ //g')\"\n\tlocal num_nics=${#nic_list[@]}\n\t\n\tif [[ \"$standalone_or_dist\" == 'standalone' ]]; then\n\t\treq_mem=12\n\t\treq_cores=4\n\t\treq_nics=2\n\telif [[ \"$standalone_or_dist\" == 'dist' ]]; then\n\t\treq_mem=8\n\t\treq_cores=4\n\t\tif [[ \"$node_type\" == 'sensor' ]]; then req_nics=2; else req_nics=1; fi\n\t\tif [[ \"$node_type\" == 'fleet' ]]; then req_mem=4; fi\n\telif [[ \"$standalone_or_dist\" == 'import' ]]; then\n\t\treq_mem=4\n\t\treq_cores=2\n\t\treq_nics=1\n\tfi\n\n\tif [[ $setup_type == 'network' ]] ; then\n\t\tif [[ -n $nsm_mount ]]; then\n\t\t\tif [[ \"$standalone_or_dist\" == 'import' ]]; then\n\t\t\t\treq_storage=50\n\t\t\telse\n\t\t\t\treq_storage=100\n\t\t\tfi\n\t\t\tif (( $(echo \"$free_space_root < $req_storage\" | bc -l) )); then\n\t\t\t\twhiptail_storage_requirements \"/\" \"${free_space_root} GB\" \"${req_storage} GB\"\n\t\t\tfi\n\t\t\tif (( $(echo \"$free_space_nsm < $req_storage\" | bc -l) )); then\n\t\t\t\twhiptail_storage_requirements \"/nsm\" \"${free_space_nsm} GB\" \"${req_storage} GB\"\n\t\t\tfi\n\t\telse\n\t\t\tif [[ \"$standalone_or_dist\" == 'import' ]]; then\n\t\t\t\treq_storage=50\n\t\t\telse\n\t\t\t\treq_storage=200\n\t\t\tfi\n\t\t\tif (( $(echo \"$free_space_root < $req_storage\" | bc -l) )); then\n\t\t\t\twhiptail_storage_requirements \"/\" \"${free_space_root} GB\" \"${req_storage} GB\"\n\t\t\tfi\n\t\tfi\t\n\tfi\n\n\tif [[ $num_nics -lt $req_nics ]]; then\n\t\tif [[ $num_nics -eq 1 ]]; then\n\t\t\twhiptail_requirements_error \"NIC\" \"$num_nics\" \"$req_nics\"\n\t\telse\n\t\t\twhiptail_requirements_error \"NICs\" \"$num_nics\" \"$req_nics\"\n\t\tfi\n\tfi\n\n\tif [[ $num_cpu_cores -lt $req_cores ]]; then\n\t\tif [[ $num_cpu_cores -eq 1 ]]; then\n\t\t\twhiptail_requirements_error \"core\" \"$num_cpu_cores\" \"$req_cores\"\n\t\telse\n\t\t\twhiptail_requirements_error \"cores\" \"$num_cpu_cores\" \"$req_cores\"\n\t\tfi\n\t\t\n\tfi\n\t\n\tif [[ $total_mem_hr -lt $req_mem ]]; then\n\t\twhiptail_requirements_error \"memory\" \"${total_mem_hr} GB\" \"${req_mem} GB\"\n\tfi\n}\n\nconfigure_network_sensor() {\n\techo \"Setting up sensor interface\" >> \"$setup_log\" 2>&1\n\tlocal nic_error=0\n\n\t# Set the MTU\n\tif [[ $NSMSETUP != 'ADVANCED' ]]; then\n\t\tif [[ $is_cloud ]]; then MTU=1575; else MTU=1500; fi\n\tfi\n\n\tif [[ $is_cloud ]]; then \n\t\tINTERFACE=${BNICS[0]}\n\t\tlocal nmcli_con_arg=\"type ethernet\"\n\telse \n\t\tINTERFACE='bond0'\n\t\tlocal nmcli_con_arg=\"type bond mode 0\"\n\tfi\n\n\t# Create the bond interface only if it doesn't already exist\n\t\n\tnmcli -f name,uuid -p con | grep -q \"$INTERFACE\" >> \"$setup_log\" 2>&1\n\tlocal found_int=$?\n\n\tif [[ $found_int != 0 ]]; then\n\t\tnmcli con add ifname \"$INTERFACE\" con-name \"$INTERFACE\" $nmcli_con_arg -- \\\n\t\t\tipv4.method disabled \\\n\t\t\tipv6.method ignore \\\n\t\t\tethernet.mtu $MTU \\\n\t\t\tconnection.autoconnect \"yes\" >> \"$setup_log\" 2>&1\n\telse\n\t\tlocal int_uuid\n\t\tint_uuid=$(nmcli -f name,uuid -p con | sed -n \"s/$INTERFACE //p\" | tr -d ' ')\n\n\t\tnmcli con mod \"$int_uuid\" \\\n\t\t\tipv4.method disabled \\\n\t\t\tipv6.method ignore \\\n\t\t\tethernet.mtu $MTU \\\n\t\t\tconnection.autoconnect \"yes\" >> \"$setup_log\" 2>&1\n\tfi\n\n\tfor BNIC in \"${BNICS[@]}\"; do\n\t\t# Check if specific offload features are able to be disabled\n\t\tfor string in \"generic-segmentation-offload\" \"generic-receive-offload\" \"tcp-segmentation-offload\"; do\n\t\t\tif ethtool -k \"$BNIC\" | grep $string | grep -q \"on [fixed]\"; then\n\t\t\t\techo \"The hardware or driver for interface ${BNIC} is not supported, packet capture may not work as expected.\" >> \"$setup_log\" 2>&1\n\t\t\t\tnic_error=1\n\t\t\t\tbreak\n\t\t\tfi\n\t\tdone\n\n\t\t# Turn off various offloading settings for the interface\n\t\tfor i in rx tx sg tso ufo gso gro lro; do\n\t\t\tethtool -K \"$BNIC\" $i off >> \"$setup_log\" 2>&1\n\t\tdone\n\t\t\n\t\tif [[ $is_cloud ]]; then\n\t\t\tnmcli con up \"$BNIC\" >> \"$setup_log\" 2>&1\n\t\telse\n\t\t\t# Check if the bond slave connection has already been created\n\t\t\tnmcli -f name,uuid -p con | grep -q \"bond0-slave-$BNIC\" >> \"$setup_log\" 2>&1\n\t\t\tlocal found_int=$?\n\t\t\t\n\t\t\tif [[ $found_int != 0 ]]; then\n\t\t\t\t# Create the slave interface and assign it to the bond\n\t\t\t\tnmcli con add type ethernet ifname \"$BNIC\" con-name \"bond0-slave-$BNIC\" master bond0 -- \\\n\t\t\t\t\tethernet.mtu $MTU \\\n\t\t\t\t\tconnection.autoconnect \"yes\" >> \"$setup_log\" 2>&1\n\t\t\telse\n\t\t\t\tlocal int_uuid\n\t\t\t\tint_uuid=$(nmcli -f name,uuid -p con | sed -n \"s/bond0-slave-$BNIC //p\" | tr -d ' ')\n\n\t\t\t\tnmcli con mod \"$int_uuid\" \\\n\t\t\t\t\tethernet.mtu $MTU \\\n\t\t\t\t\tconnection.autoconnect \"yes\" >> \"$setup_log\" 2>&1\n\t\t\tfi\n\t\t\t\n\t\t\tnmcli con up \"bond0-slave-$BNIC\" >> \"$setup_log\" 2>&1 # Bring the slave interface up\n\t\tfi\n\tdone\n\n\tif [ $nic_error != 0 ]; then\n\t\treturn 1\n\tfi\n}\n\ncopy_salt_master_config() {\n\n\t# Copy the Salt master config template to the proper directory\n\tif [ \"$setup_type\" = 'iso' ]; then\n\t\tcp /root/SecurityOnion/files/salt/master/master /etc/salt/master >> \"$setup_log\" 2>&1\n\t\tcp /root/SecurityOnion/files/salt/master/salt-master.service /usr/lib/systemd/system/salt-master.service >> \"$setup_log\" 2>&1\n\telse\n\t\tcp ../files/salt/master/master /etc/salt/master >> \"$setup_log\" 2>&1\n\t\tcp ../files/salt/master/salt-master.service /usr/lib/systemd/system/salt-master.service >> \"$setup_log\" 2>&1\n\tfi\n\n\t# Restart the service so it picks up the changes\n\tsystemctl daemon-reload >> \"$setup_log\" 2>&1\n\tsystemctl restart salt-master >> \"$setup_log\" 2>&1\n}\n\ncopy_minion_tmp_files() {\n\tcase \"$install_type\" in\n\t\t'MANAGER' | 'EVAL' | 'HELIXSENSOR' | 'MANAGERSEARCH' | 'STANDALONE' | 'IMPORT')\n\t\t\techo \"Copying pillar and salt files in $temp_install_dir to $local_salt_dir\"\n\t\t\tcp -Rv \"$temp_install_dir\"/pillar/ $local_salt_dir/ >> \"$setup_log\" 2>&1\n\t\t\tif [ -d \"$temp_install_dir\"/salt ] ; then\n\t\t\t\tcp -Rv \"$temp_install_dir\"/salt/ $local_salt_dir/ >> \"$setup_log\" 2>&1\n\t\t\tfi\n\t\t\t;;\n\t\t*)\n\t\t\t{\n\t\t\t\techo \"scp pillar and salt files in $temp_install_dir to manager $local_salt_dir\";\n\t\t\t\tssh -i /root/.ssh/so.key soremote@\"$MSRV\" mkdir -p /tmp/\"$MINION_ID\"/pillar;\n\t\t\t\tssh -i /root/.ssh/so.key soremote@\"$MSRV\" mkdir -p /tmp/\"$MINION_ID\"/schedules;\n\t\t\t\tscp -prv -i /root/.ssh/so.key \"$temp_install_dir\"/pillar/minions/* soremote@\"$MSRV\":/tmp/\"$MINION_ID\"/pillar/;\n\t\t\t\tif [ -d $temp_install_dir/salt/patch/os/schedules/ ]; then\n\t\t\t\t\tif [ \"$(ls -A $temp_install_dir/salt/patch/os/schedules/)\" ]; then\n\t\t\t\t\t\tscp -prv -i /root/.ssh/so.key $temp_install_dir/salt/patch/os/schedules/* soremote@$MSRV:/tmp/$MINION_ID/schedules;\n\t\t\t\t\tfi\n\t\t\t\tfi\n\t\t\t\tssh -i /root/.ssh/so.key soremote@\"$MSRV\" sudo $default_salt_dir/salt/manager/files/add_minion.sh \"$MINION_ID\";\n\t\t\t} >> \"$setup_log\" 2>&1\n\t\t\t;;\n\tesac\n}\n\ncopy_ssh_key() {\n\n\techo \"Generating SSH key\"\n\t# Generate SSH key\n\tmkdir -p /root/.ssh\n\tssh-keygen -f /root/.ssh/so.key -t rsa -q -N \"\" < /dev/zero\n\tchown -R \"$SUDO_USER\":\"$SUDO_USER\" /root/.ssh\n\n\techo \"Removing old entry for manager from known_hosts if it exists\"\n\tsed -i \"/${MSRV}/d\" /root/.ssh/known_hosts\n\n\techo \"Copying the SSH key to the manager\"\n\t#Copy the key over to the manager\n\tssh-copy-id -f -i /root/.ssh/so.key soremote@\"$MSRV\"\n}\n\ncreate_local_directories() {\n\techo \"Creating local pillar and salt directories\"\n\tPILLARSALTDIR=${SCRIPTDIR::-5}\n\tfor i in \"pillar\" \"salt\"; do\n\t\tfor d in $(find $PILLARSALTDIR/$i -type d); do\n\t\t\tsuffixdir=${d//$PILLARSALTDIR/}\n\t\t\tif [ ! -d \"$local_salt_dir/$suffixdir\" ]; then\n\t\t\t\tmkdir -v \"$local_salt_dir$suffixdir\" >> \"$setup_log\" 2>&1\n\t\t\tfi\n\t\tdone\n\t\tchown -R socore:socore \"$local_salt_dir/$i\"\n\tdone\n\n}\n\ncreate_local_nids_rules() {\n\t# Create a local.rules file so it doesn't get blasted on updates\n\tmkdir -p /opt/so/saltstack/local/salt/idstools\n\techo \"# Custom Suricata rules go in this file\" > /opt/so/saltstack/local/salt/idstools/local.rules\n}\n\ncreate_repo() {\n\t# Create the repo for airgap\n\tcreaterepo /nsm/repo\n}\n\ndetect_cloud() {\n  echo \"Testing if setup is running on a cloud instance...\" >> \"$setup_log\" 2>&1\n  if ( curl --fail -s -m 5 http://169.254.169.254/latest/meta-data/instance-id > /dev/null ) || ( dmidecode -s bios-vendor | grep -q Google > /dev/null); then export is_cloud=\"true\"; fi\n}\n\ndetect_os() {\n\n\t# Detect Base OS\n\techo \"Detecting Base OS\" >> \"$setup_log\" 2>&1\n\tif [ -f /etc/redhat-release ]; then\n\t\tOS=centos\n\t\tif grep -q \"CentOS Linux release 7\" /etc/redhat-release; then\n\t\t\tOSVER=7\n\t\telif grep -q \"CentOS Linux release 8\" /etc/redhat-release; then\n\t\t\tOSVER=8\n\t\t\techo \"We currently do not support CentOS $OSVER but we are working on it!\"\n\t\t\texit 1\n\t\telse\n\t\t\techo \"We do not support the version of CentOS you are trying to use.\"\n\t\t\texit 1\n\t\tfi\n\n\t\techo \"Installing required packages to run installer...\" >> \"$setup_log\" 2>&1\n\t\t# Install bind-utils so the host command exists\n\t\tif [[ ! $is_iso ]]; then\n\t\t  if ! command -v host > /dev/null 2>&1; then\n\t\t\tyum -y install bind-utils >> \"$setup_log\" 2>&1\n\t\t  fi\n\t\t  if ! command -v nmcli > /dev/null 2>&1; then\n\t\t\t{\n\t\t\t\tyum -y install NetworkManager;\n\t\t\t\tsystemctl enable NetworkManager;\n\t\t\t\tsystemctl start NetworkManager;\n\t\t\t} >> \"$setup_log\" 2<&1\n\t\t  fi\n\t\t  if ! command -v bc > /dev/null 2>&1; then\n\t\t\tyum -y install bc >> \"$setup_log\" 2>&1\n\t\t  fi\n\t\t  if ! yum versionlock > /dev/null 2>&1; then\n\t\t\tyum -y install yum-plugin-versionlock >> \"$setup_log\" 2>&1\n\t\t  fi\n        else\n\t\t  logCmd \"systemctl enable NetworkManager\"\n\t\t  logCmd \"systemctl start NetworkManager\"\n        fi\n\telif [ -f /etc/os-release ]; then\n\t\tOS=ubuntu\n\t\tif grep -q \"UBUNTU_CODENAME=bionic\" /etc/os-release; then\n\t\t\tOSVER=bionic\n\t\telif grep -q \"UBUNTU_CODENAME=xenial\" /etc/os-release; then\n\t\t\tOSVER=xenial\n\t\telse\n\t\t\techo \"We do not support your current version of Ubuntu.\"\n\t\t\texit 1\n\t\tfi\n\n\t\techo \"Installing required packages to run installer...\"\n\t\t# Install network manager so we can do interface stuff\n\t\tif ! command -v nmcli > /dev/null 2>&1; then\n\t\t\t{\n\t\t\t\tapt-get install -y network-manager;\n\t\t\t\tsystemctl enable NetworkManager;\n\t\t\t\tsystemctl start NetworkManager;\n\t\t\t} >> \"$setup_log\" 2<&1\n\t\tfi\n\t\tapt-get install -y bc curl >> \"$setup_log\" 2>&1\n\n\telse\n\t\techo \"We were unable to determine if you are using a supported OS.\"\n\t\texit 1\n\tfi\n\n\techo \"Found OS: $OS $OSVER\" >> \"$setup_log\" 2>&1\n\n}\n\ndisable_auto_start() {\n\t\n\tif crontab -l -u $INSTALLUSERNAME 2>&1 | grep so-setup > /dev/null 2>&1; then\n\t\t# Remove the automated setup script from crontab, if it exists\n\t\tlogCmd \"crontab -u $INSTALLUSERNAME -r\"\n\tfi\n\n\tif grep so-setup /home/$INSTALLUSERNAME/.bash_profile > /dev/null 2>&1; then\n\t\t# Truncate last line of the bash profile\n\t\tinfo \"Removing auto-run of setup from bash profile\"\n\t\tsed -i '$ d' /home/$INSTALLUSERNAME/.bash_profile >> \"$setup_log\" 2>&1\n\tfi\n}\n\ndisable_ipv6() {\n\t{\n\t\tinfo \"Disabling ipv6\"\n\t\tsysctl -w net.ipv6.conf.all.disable_ipv6=1\n\t\tsysctl -w net.ipv6.conf.default.disable_ipv6=1\n\t}  >> \"$setup_log\" 2>&1\n}\n\ndisable_misc_network_features() {\n\tfilter_unused_nics\n\tif [ ${#filtered_nics[@]} -ne 0 ]; then\n\t\tfor unused_nic in \"${filtered_nics[@]}\"; do\n\t\t\tif [ -n \"$unused_nic\" ]; then\n\t\t\t\techo \"Disabling unused NIC: $unused_nic\" >> \"$setup_log\" 2>&1\n\n\t\t\t\t# Disable DHCPv4/v6 and autoconnect\n\t\t\t\tnmcli con mod \"$unused_nic\" \\\n\t\t\t\t\tipv4.method disabled \\\n\t\t\t\t\tipv6.method ignore \\\n\t\t\t\t\tconnection.autoconnect \"no\" >> \"$setup_log\" 2>&1\n\n\t\t\t\t# Flush any existing IPs\n\t\t\t\tip addr flush \"$unused_nic\" >> \"$setup_log\" 2>&1\n\t\t\tfi\n\t\tdone\n\tfi\n\t# Disable IPv6\n\t{ \n\t\techo \"net.ipv6.conf.all.disable_ipv6 = 1\"\n\t\techo \"net.ipv6.conf.default.disable_ipv6 = 1\"\n\t\techo \"net.ipv6.conf.lo.disable_ipv6 = 1\" \n\t} >> /etc/sysctl.conf\n}\n\ndocker_install() {\n\n\tif [ $OS = 'centos' ]; then\n\t\t{\n\t\t\tyum clean expire-cache;\n\t\t\tif [[ ! $is_airgap ]]; then\n\t\t\t  yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo;\n\t\t\tfi\n\t\t\tif [[ ! $is_iso ]]; then\n\t\t\t  yum -y install docker-ce-19.03.12-3.el7 containerd.io-1.2.13-3.2.el7;\n\t\t\tfi\n\t\t\tyum versionlock docker-ce-19.03.12-3.el7;\n\t\t\tyum versionlock containerd.io-1.2.13-3.2.el7\n\t\t} >> \"$setup_log\" 2>&1\n\t\t\n\telse\n\t\tcase \"$install_type\" in\n\t\t\t'MANAGER' | 'EVAL' | 'STANDALONE' | 'MANAGERSEARCH' | 'IMPORT')\n\t\t\t\tapt-get update >> \"$setup_log\" 2>&1\n\t\t\t\t;;\n\t\t\t*)\n\t\t\t\t{\n\t\t\t\t\tapt-key add \"$temp_install_dir\"/gpg/docker.pub;\n\t\t\t\t\tadd-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\";\n\t\t\t\t\tapt-get update;\n\t\t\t\t} >> \"$setup_log\" 2>&1\n\t\t\t\t;;\n\t\tesac \n\t\t\n\t\tif [ $OSVER != \"xenial\" ]; then\n\t\t\tapt-get -y install docker-ce python3-docker >> \"$setup_log\" 2>&1\n\t\telse\n\t\t\tapt-get -y install docker-ce python-docker >> \"$setup_log\" 2>&1\n\t\tfi\n\tfi\n\tdocker_registry\n\t{\n\t\techo \"Restarting Docker\";\n\t\tsystemctl restart docker;\n\t\tsystemctl enable docker;\n\t} >> \"$setup_log\" 2>&1\n}\n\ndocker_registry() {\n\n\techo \"Setting up Docker Registry\" >> \"$setup_log\" 2>&1\n\tmkdir -p /etc/docker >> \"$setup_log\" 2>&1\n\tif [ -z \"$DOCKERNET\" ]; then\n            DOCKERNET=172.17.0.0\n\tfi\n\t# Make the host use the manager docker registry\n\tDNETBIP=$(echo $DOCKERNET | awk -F'.' '{print $1,$2,$3,1}' OFS='.')/24\n\tif [ -n \"$TURBO\" ]; then local proxy=\"$TURBO\"; else local proxy=\"https://$MSRV\"; fi\n\tprintf '%s\\n'\\\n\t\t\"{\"\\\n\t\t\"  \\\"registry-mirrors\\\": [ \\\"$proxy:5000\\\" ],\"\\\n\t\t\"  \\\"bip\\\": \\\"$DNETBIP\\\",\"\\\n\t\t\"  \\\"default-address-pools\\\": [\"\\\n\t\t\"    {\"\\\n\t\t\"      \\\"base\\\" : \\\"$DOCKERNET\\\",\"\\\n\t\t\"      \\\"size\\\" : 24\"\\\n\t\t\"    }\"\\\n\t\t\"  ]\"\\\n\t\t\"}\" > /etc/docker/daemon.json\n\techo \"Docker Registry Setup - Complete\" >> \"$setup_log\" 2>&1\n\n}\n\ndocker_seed_update() {\n\tlocal name=$1\n\tlocal percent_delta=1\n\tif [ \"$install_type\" == 'HELIXSENSOR' ]; then \n\t\tpercent_delta=6\n\tfi\n\t((docker_seed_update_percent=docker_seed_update_percent+percent_delta))\n\n\tset_progress_str \"$docker_seed_update_percent\" \"Downloading $name\"\n}\n\ndocker_seed_registry() {\n\tlocal VERSION=\"$SOVERSION\"\n\n\tif ! [ -f /nsm/docker-registry/docker/registry.tar ]; then\n\t\tif [ \"$install_type\" == 'IMPORT' ]; then\n\t\t\tcontainer_list 'so-import'\n\t\telif [ \"$install_type\" == 'HELIXSENSOR' ]; then\n\t\t\tcontainer_list 'so-helix'\n\t\telse\n\t\t\tcontainer_list\n\t\tfi\n\n\t\tdocker_seed_update_percent=25\n\n\t\tupdate_docker_containers 'netinstall' '' 'docker_seed_update' \"$setup_log\"\n\telse\n\t\ttar xvf /nsm/docker-registry/docker/registry.tar -C /nsm/docker-registry/docker >> \"$setup_log\" 2>&1\n\t\trm /nsm/docker-registry/docker/registry.tar >> \"$setup_log\" 2>&1\n\tfi\n\n}\n\nfireeye_pillar() {\n\n\tlocal fireeye_pillar_path=$local_salt_dir/pillar/fireeye\n\tmkdir -p \"$fireeye_pillar_path\"\n\n\tprintf '%s\\n'\\\n\t\t\"fireeye:\"\\\n\t\t\"  helix:\"\\\n\t\t\"    api_key: '$HELIXAPIKEY'\" \n\t\t\"\" > \"$fireeye_pillar_path\"/init.sls\n\n}\n\n# Generate Firewall Templates\nfirewall_generate_templates() {\n\n\tlocal firewall_pillar_path=$local_salt_dir/salt/firewall\n\tmkdir -p \"$firewall_pillar_path\"\n   \n\tcp ../files/firewall/* /opt/so/saltstack/local/salt/firewall/ >> \"$setup_log\" 2>&1\n\n\tfor i in analyst beats_endpoint sensor manager minion osquery_endpoint search_node wazuh_endpoint; do\n\t\t$default_salt_dir/salt/common/tools/sbin/so-firewall includehost \"$i\" 127.0.0.1\n\tdone\n\n}\n\nfleet_pillar() {\n\n\tlocal pillar_file=\"$temp_install_dir\"/pillar/minions/\"$MINION_ID\".sls\n\n\t# Create the fleet pillar\n\tprintf '%s\\n'\\\n\t\t\"fleet:\"\\\n\t\t\"  mainip: '$MAINIP'\"\\\n\t\t\"  manager: '$MSRV'\"\\\n\t\t\"\" > \"$pillar_file\"\n}\n\ngenerate_passwords(){\n  # Generate Random Passwords for Things\n  MYSQLPASS=$(tr -dc 'a-zA-Z0-9' < /dev/urandom | fold -w 20 | head -n 1)\n  PLAYBOOKDBPASS=$(tr -dc 'a-zA-Z0-9' < /dev/urandom | fold -w 20 | head -n 1)\n  PLAYBOOKADMINPASS=$(tr -dc 'a-zA-Z0-9' < /dev/urandom | fold -w 20 | head -n 1)\n  PLAYBOOKAUTOMATIONPASS=$(tr -dc 'a-zA-Z0-9' < /dev/urandom | fold -w 20 | head -n 1)\n  FLEETPASS=$(tr -dc 'a-zA-Z0-9' < /dev/urandom  | fold -w 20 | head -n 1)\n  FLEETJWT=$(tr -dc 'a-zA-Z0-9' < /dev/urandom  | fold -w 20 | head -n 1)\n  GRAFANAPASS=$(tr -dc 'a-zA-Z0-9' < /dev/urandom  | fold -w 20 | head -n 1)\n  if [[ \"$THEHIVE\" == \"1\" ]]; then\n  \tHIVEKEY=$(tr -dc 'a-zA-Z0-9' < /dev/urandom  | fold -w 20 | head -n 1)\n  \tHIVEPLAYSECRET=$(tr -dc 'a-zA-Z0-9' < /dev/urandom  | fold -w 20 | head -n 1)\n  \tCORTEXKEY=$(tr -dc 'a-zA-Z0-9' < /dev/urandom  | fold -w 20 | head -n 1)\n  \tCORTEXORGUSERKEY=$(tr -dc 'a-zA-Z0-9' < /dev/urandom  | fold -w 20 | head -n 1)\n  \tCORTEXPLAYSECRET=$(tr -dc 'a-zA-Z0-9' < /dev/urandom  | fold -w 20 | head -n 1)\n  fi\n  SENSORONIKEY=$(tr -dc 'a-zA-Z0-9' < /dev/urandom  | fold -w 20 | head -n 1)\n  KRATOSKEY=$(tr -dc 'a-zA-Z0-9' < /dev/urandom  | fold -w 20 | head -n 1)\n}\n\nget_redirect() {\n\twhiptail_set_redirect\n\tif [ \"$REDIRECTINFO\" = \"OTHER\" ]; then\n\t\twhiptail_set_redirect_host\n\tfi\n}\n\nget_minion_type() {\n\tlocal minion_type\n\tcase \"$install_type\" in\n\t\t'EVAL' | 'MANAGERSEARCH' | 'MANAGER' | 'SENSOR' | 'HEAVYNODE' | 'FLEET' | 'STANDALONE' | 'IMPORT')\n\t\t\tminion_type=$(echo \"$install_type\" | tr '[:upper:]' '[:lower:]')\n\t\t\t;;\n\t\t'HELIXSENSOR')\n\t\t\tminion_type='helix'\n\t\t\t;;\n\t\t*'NODE')\n\t\t\tminion_type='node'\n\t\t\t;;\n\tesac\n\techo \"$minion_type\"\n}\n\nhost_pillar() {\n\n\tlocal pillar_file=\"$temp_install_dir\"/pillar/minions/\"$MINION_ID\".sls\n\n\t# Create the host pillar\n\tprintf '%s\\n'\\\n\t\t\"host:\"\\\n\t\t\"  mainint: '$MNIC'\"\\\n\t\t\"\" > \"$pillar_file\"\n}\n\ninstall_cleanup() {\n\techo \"Installer removing the following files:\"\n\tls -lR \"$temp_install_dir\"\n\n\t# Clean up after ourselves\n\trm -rf \"$temp_install_dir\"\n\t\n\t# All cleanup prior to this statement must be compatible with automated testing. Cleanup\n\t# that will disrupt automated tests should be placed beneath this statement.\n\t[ -n \"$TESTING\" ] && return\n\n\t# If Mysql is running stop it\n\t/usr/sbin/so-mysql-stop\n\n}\n\nimport_registry_docker() {\n\tif [ -f /nsm/docker-registry/docker/registry_image.tar ]; then\n\t  logCmd \"service docker start\"\n\t  logCmd \"docker load -i /nsm/docker-registry/docker/registry_image.tar\" \n\telse\n\t  info \"Need to download registry\"\n\tfi\n}\n\nmanager_pillar() {\n\n\tlocal pillar_file=$temp_install_dir/pillar/minions/$MINION_ID.sls\n\n\t# Create the manager pillar\n\tprintf '%s\\n'\\\n\t\t\"manager:\"\\\n\t\t\"  mainip: '$MAINIP'\"\\\n\t\t\"  mainint: '$MNIC'\"\\\n\t\t\"  esheap: '$ES_HEAP_SIZE'\"\\\n\t\t\"  esclustername: {{ grains.host }}\"\\\n\t\t\"  freq: 0\"\\\n\t\t\"  domainstats: 0\" >> \"$pillar_file\"\n\t\t\n\n\tif [ \"$install_type\" = 'EVAL' ] || [ \"$install_type\" = 'HELIXSENSOR' ] || [ \"$install_type\" = 'MANAGERSEARCH' ] || [ \"$install_type\" = 'STANDALONE' ]; then\n\t\tprintf '%s\\n'\\\n\t\t\t\"  mtu: $MTU\" >> \"$pillar_file\"\n\tfi\n\n\tprintf '%s\\n'\\\n\t\t\"  elastalert: 1\"\\\n\t\t\"  es_port: $node_es_port\"\\\n\t\t\"  log_size_limit: $log_size_limit\"\\\n\t\t\"  cur_close_days: $CURCLOSEDAYS\"\\\n\t\t\"  grafana: $GRAFANA\"\\\n\t\t\"  osquery: $OSQUERY\"\\\n\t\t\"  thehive: $THEHIVE\"\\\n\t\t\"  playbook: $PLAYBOOK\"\\\n\t\t\"\"\\\n\t\t\"elasticsearch:\"\\\n\t\t\"  mainip: '$MAINIP'\"\\\n\t\t\"  mainint: '$MNIC'\"\\\n\t\t\"  esheap: '$NODE_ES_HEAP_SIZE'\"\\\n\t\t\"  esclustername: {{ grains.host }}\"\\\n\t\t\"  node_type: '$NODETYPE'\"\\\n\t\t\"  es_port: $node_es_port\"\\\n\t\t\"  log_size_limit: $log_size_limit\"\\\n\t\t\"  node_route_type: 'hot'\"\\\n\t\t\"\"\\\n\t\t\"logstash_settings:\"\\\n\t\t\"  ls_pipeline_batch_size: 125\"\\\n\t\t\"  ls_input_threads: 1\"\\\n\t\t\"  lsheap: $LS_HEAP_SIZE\"\\\n\t\t\"  ls_pipeline_workers: $num_cpu_cores\"\\\n\t\t\"\"\\\n\t\t\"idstools:\"\\\n\t\t\"  config:\"\\\n\t\t\"    ruleset: '$RULESETUP'\"\\\n\t\t\"    oinkcode: '$OINKCODE'\"\\\n\t\t\"    urls:\"\\\n\t\t\"  sids:\"\\\n\t\t\"    enabled:\"\\\n\t\t\"    disabled:\"\\\n\t\t\"    modify:\"\\\n\t\t\"\"\\\n\t\t\"kratos:\" >> \"$pillar_file\"\n\t\t\n\n\tprintf '%s\\n'\\\n\t\t\"  kratoskey: '$KRATOSKEY'\"\\\n\t\t\"\" >> \"$pillar_file\"\n\n  }\n\nmanager_global() {\n\tlocal global_pillar=\"$local_salt_dir/pillar/global.sls\"\n\n\tif [ -z \"$SENSOR_CHECKIN_INTERVAL_MS\" ]; then\n\t\tSENSOR_CHECKIN_INTERVAL_MS=10000\n\t\tif [ \"$install_type\" = 'EVAL' ] || [ \"$install_type\" = 'STANDALONE' ] || [ \"$install_type\" = 'IMPORT' ]; then\n\t\t\tSENSOR_CHECKIN_INTERVAL_MS=1000\n\t\tfi\n\tfi\n\n\tif [ -z \"$DOCKERNET\" ]; then\n        DOCKERNET=172.17.0.0\n\tfi\n\n\t# Create a global file for global values\n        printf '%s\\n'\\\n                \"global:\"\\\n                \"  soversion: '$SOVERSION'\"\\\n                \"  hnmanager: '$HNMANAGER'\"\\\n                \"  ntpserver: '$NTPSERVER'\"\\\n\t\t\t\t\"  dockernet: '$DOCKERNET'\"\\\n                \"  proxy: '$PROXY'\"\\\n                \"  mdengine: '$ZEEKVERSION'\"\\\n                \"  ids: '$NIDS'\"\\\n\t\t\t\t\"  url_base: '$REDIRECTIT'\"\\\n                \"  managerip: '$MAINIP'\" > \"$global_pillar\"\n\t\t\n        if [[ $is_airgap ]]; then\n\t\t        printf '%s\\n'\\\n                        \"  airgap: True\"\\ >> \"$global_pillar\"\n\t\telse \n\t\t        printf '%s\\n'\\\n                        \"  airgap: False\"\\ >> \"$global_pillar\"\n\t\tfi\n\n        # Check if TheHive is enabled.  If so, add creds and other details\n        if [[ \"$THEHIVE\" == \"1\" ]]; then\n                printf '%s\\n'\\\n                        \"  hiveuser: '$WEBUSER'\"\\\n                        \"  hivepassword: '$WEBPASSWD1'\"\\\n                        \"  hivekey: '$HIVEKEY'\"\\\n                        \"  hiveplaysecret: '$HIVEPLAYSECRET'\"\\\n                        \"  cortexuser: '$WEBUSER'\"\\\n                        \"  cortexpassword: '$WEBPASSWD1'\"\\\n                        \"  cortexkey: '$CORTEXKEY'\"\\\n                        \"  cortexorgname: 'SecurityOnion'\"\\\n                        \"  cortexorguser: 'soadmin'\"\\\n                        \"  cortexorguserkey: '$CORTEXORGUSERKEY'\"\\\n                        \"  cortexplaysecret: '$CORTEXPLAYSECRET'\" >> \"$global_pillar\"\n        fi\n\n        # Continue adding other details\t\n\tprintf '%s\\n'\\\n\t\t\"  fleet_custom_hostname: \"\\\n\t\t\"  fleet_manager: False\"\\\n\t\t\"  fleet_node: False\"\\\n\t\t\"  fleet_packages-timestamp: 'N/A'\"\\\n\t\t\"  fleet_packages-version: 1\"\\\n\t\t\"  fleet_hostname: 'N/A'\"\\\n\t\t\"  fleet_ip: 'N/A'\"\\\n\t\t\"  sensoronikey: '$SENSORONIKEY'\"\\\n\t\t\"  wazuh: $WAZUH\"\\\n                \"  managerupdate: $MANAGERUPDATES\"\\\n\t\t\"  imagerepo: '$IMAGEREPO'\"\\\n\t\t\"  pipeline: 'redis'\"\\\n\t\t\"pcap:\"\\\n\t\t\"  sensor_checkin_interval_ms: $SENSOR_CHECKIN_INTERVAL_MS\"\\\n\t\t\"strelka:\"\\\n\t\t\"  enabled: $STRELKA\"\\\n\t\t\"  rules: 1\"\\\n\t\t\"curator:\"\\\n\t\t\"  hot_warm: False\"\\\n\t\t\"elastic:\"\\\n\t\t\"  features: False\"\\\n\t\t\"elasticsearch:\"\\\n\t\t\"  replicas: 0\"\\\n\t\t\"  true_cluster: False\"\\\n\t\t\"  true_cluster_name: 'so'\"\\\n\t\t\"  discovery_nodes: 1\"\\\n\t\t\"  hot_warm_enabled: False\"\\\n\t\t\"  cluster_routing_allocation_disk.threshold_enabled: true\"\\\n                \"  cluster_routing_allocation_disk_watermark_low: '95%'\"\\\n                \"  cluster_routing_allocation_disk_watermark_high: '98%'\"\\\n                \"  cluster_routing_allocation_disk_watermark_flood_stage: '98%'\"\\\n\t\t\"  index_settings:\"\\\n\t\t\"    so-beats:\"\\\n\t\t\"      shards: 1\"\\\n\t\t\"      warm: 7\"\\\n\t\t\"      close: 30\"\\\n\t\t\"      delete: 365\"\\\n\t\t\"    so-firewall:\"\\\n\t\t\"      shards: 1\"\\\n\t\t\"      warm: 7\"\\\n\t\t\"      close: 30\"\\\n\t\t\"      delete: 365\"\\\n\t\t\"    so-flow:\"\\\n\t\t\"      shards: 1\"\\\n\t\t\"      warm: 7\"\\\n\t\t\"      close: 30\"\\\n\t\t\"      delete: 365\"\\\n\t\t\"    so-ids:\"\\\n\t\t\"      shards: 1\"\\\n\t\t\"      warm: 7\"\\\n\t\t\"      close: 30\"\\\n\t\t\"      delete: 365\"\\\n\t\t\"    so-import:\"\\\n\t\t\"      shards: 1\"\\\n\t\t\"      warm: 7\"\\\n\t\t\"      close: 73000\"\\\n\t\t\"      delete: 73001\"\\\n\t\t\"    so-osquery:\"\\\n\t\t\"      shards: 1\"\\\n\t\t\"      warm: 7\"\\\n\t\t\"      close: 30\"\\\n\t\t\"      delete: 365\"\\\n\t\t\"    so-ossec:\"\\\n\t\t\"      shards: 1\"\\\n\t\t\"      warm: 7\"\\\n\t\t\"      close: 30\"\\\n\t\t\"      delete: 365\"\\\n\t\t\"    so-strelka:\"\\\n\t\t\"      shards: 1\"\\\n\t\t\"      warm: 7\"\\\n\t\t\"      close: 30\"\\\n\t\t\"      delete: 365\"\\\n\t\t\"    so-syslog:\"\\\n\t\t\"      shards: 1\"\\\n\t\t\"      warm: 7\"\\\n\t\t\"      close: 30\"\\\n\t\t\"      delete: 365\"\\\n\t\t\"    so-zeek:\"\\\n\t\t\"      shards: 5\"\\\n\t\t\"      warm: 7\"\\\n\t\t\"      close: 365\"\\\n\t\t\"      delete: 45\"\\\n\t\t\"minio:\"\\\n\t\t\"  access_key: '$ACCESS_KEY'\"\\\n\t\t\"  access_secret: '$ACCESS_SECRET'\"\\\n\t\t\"s3_settings:\"\\\n\t\t\"  size_file: 2048\"\\\n\t\t\"  time_file: 1\"\\\n\t\t\"  upload_queue_size: 4\"\\\n\t\t\"  encoding: 'gzip'\"\\\n\t\t\"  interval: 5\"\\\n\t        \"backup:\"\\\n                \"  locations:\"\\\n                \"    - /opt/so/saltstack/local\"\\\n\t        \"soctopus:\"\\\n\t\t\"  playbook:\"\\\n\t\t\"    rulesets:\"\\\n\t\t\"      - windows\"\\\n\t\t\"redis_settings:\"\\\n\t\t\"  redis_maxmemory: 812\" >> \"$global_pillar\"\n\n\t\t\n\tprintf '%s\\n'  '----' >> \"$setup_log\" 2>&1\n}\n\nminio_generate_keys() {\n\n\tlocal charSet=\"[:graph:]\"\n\n\tACCESS_KEY=$(tr -dc 'a-zA-Z0-9' < /dev/urandom | fold -w 20 | head -n 1)\n\tACCESS_SECRET=$(tr -dc 'a-zA-Z0-9' < /dev/urandom | fold -w 40 | head -n 1)\n\n}\n\nnetwork_setup() {\n\t{\n\t\techo \"Finishing up network setup\";\n\n\t\techo \"... Verifying all network devices are managed by Network Manager\";\n\t\tcheck_network_manager_conf;\n\n\t\techo \"... Disabling unused NICs\";\n\t\tdisable_misc_network_features;\n\n\t\techo \"... Setting ONBOOT for management interface\";\n\t\tif ! netplan > /dev/null 2>&1; then\n\t\t\tnmcli con mod \"$MNIC\" connection.autoconnect \"yes\";\n\t\tfi\n\n        echo \"... Copying 99-so-checksum-offload-disable\";\n        cp ./install_scripts/99-so-checksum-offload-disable /etc/NetworkManager/dispatcher.d/pre-up.d/99-so-checksum-offload-disable ;\n\n\t\techo \"... Modifying 99-so-checksum-offload-disable\";\n\t\tsed -i \"s/\\$MNIC/${MNIC}/g\" /etc/NetworkManager/dispatcher.d/pre-up.d/99-so-checksum-offload-disable;\n\t} >> \"$setup_log\" 2>&1\n}\n\nelasticsearch_pillar() {\n\n\tlocal pillar_file=$temp_install_dir/pillar/minions/$MINION_ID.sls\n\n\t# Create the node pillar\n\tprintf '%s\\n'\\\n\t\t\"elasticsearch:\"\\\n\t\t\"  mainip: '$MAINIP'\"\\\n\t\t\"  mainint: '$MNIC'\"\\\n\t\t\"  esheap: '$NODE_ES_HEAP_SIZE'\"\\\n\t\t\"  esclustername: {{ grains.host }}\"\\\n\t\t\"  node_type: '$NODETYPE'\"\\\n\t\t\"  es_port: $node_es_port\"\\\n\t\t\"  log_size_limit: $log_size_limit\"\\\n\t\t\"  node_route_type: 'hot'\"\\\n\t\t\"\" >> \"$pillar_file\"\n\t\n\tprintf '%s\\n'\\\n\t\t\"logstash_settings:\"\\\n\t\t\"  ls_pipeline_batch_size: $LSPIPELINEBATCH\"\\\n\t\t\"  ls_input_threads: $LSINPUTTHREADS\"\\\n\t\t\"  lsheap: $NODE_LS_HEAP_SIZE\"\\\n\t\t\"  ls_pipeline_workers: $num_cpu_cores\"\\\n\t\t\"\" >> \"$pillar_file\"\n\n}\n\nparse_install_username() {\n\t# parse out the install username so things copy correctly\n    INSTALLUSERNAME=$(pwd | sed -E 's/\\// /g'  | awk '{ print $2 }')\n}\n\npatch_pillar() {\n\n\tlocal pillar_file=$temp_install_dir/pillar/minions/$MINION_ID.sls\n\n\tprintf '%s\\n'\\\n\t\t\"patch:\"\\\n\t\t\"  os:\"\\\n\t\t\"    schedule_name: '$PATCHSCHEDULENAME'\"\\\n\t\t\"    enabled: True\"\\\n\t\t\"    splay: 300\"\\\n\t\t\"\" >> \"$pillar_file\"\n\n}\n\npatch_schedule_os_new() {\n\tlocal OSPATCHSCHEDULEDIR=\"$temp_install_dir/salt/patch/os/schedules\"\n\tlocal OSPATCHSCHEDULE=\"$OSPATCHSCHEDULEDIR/$PATCHSCHEDULENAME.yml\"\n\n\tmkdir -p $OSPATCHSCHEDULEDIR\n\n\tprintf '%s\\n'\\\n\t\t\"patch:\"\\\n\t\t\"  os:\"\\\n\t\t\"    schedule:\"> \"$OSPATCHSCHEDULE\"\n\tfor psd in \"${PATCHSCHEDULEDAYS[@]}\";do\n\t\tpsd=\"${psd//\\\"/}\"\n\t\techo \"      - $psd:\" >> \"$OSPATCHSCHEDULE\"\n\t\tfor psh in \"${PATCHSCHEDULEHOURS[@]}\"\n\t\tdo\n\t\t\tpsh=\"${psh//\\\"/}\"\n\t\t\techo \"        - '$psh'\" >> \"$OSPATCHSCHEDULE\"\n\t\tdone\n\tdone\n\n}\n\nprint_salt_state_apply() {\n\tlocal state=$1\n\n\techo \"Applying $state Salt state\"\n}\n\nreserve_group_ids() {\n\t# This is a hack to fix CentOS from taking group IDs that we need\n\tgroupadd -g 928 kratos\n\tgroupadd -g 930 elasticsearch\n\tgroupadd -g 931 logstash\n\tgroupadd -g 932 kibana\n\tgroupadd -g 933 elastalert\n\tgroupadd -g 934 curator\n\tgroupadd -g 937 zeek\n\tgroupadd -g 940 suricata\n\tgroupadd -g 941 stenographer\n\tgroupadd -g 945 ossec\n\tgroupadd -g 946 cyberchef\n}\n\nreinstall_init() {\n\tinfo \"Putting system in state to run setup again\"\n\n\t{\n\t\tlocal minion_config=/etc/salt/minion\n\t\t\n\t\t# Remove startup_states from minion config so we don't immediately highstate when salt starts back up\n\t\tif [[ -f $minion_config ]] && grep -q \"startup_states\" $minion_config; then\n\t\t\tsed -i '/startup_states/d' $minion_config\n\t\tfi\n\n\t\tif command -v salt-call &> /dev/null; then\n\t\t\t# Disable schedule so highstate doesn't start running during the install\n\t\t\tsalt-call -l info schedule.disable\n\n\t\t\t# Kill any currently running salt jobs, also to prevent issues with highstate.\n\t\t\tsalt-call -l info saltutil.kill_all_jobs\n\t\tfi\n\n\t\tif command -v docker &> /dev/null; then\n\t\t\t# Stop and remove all so-* containers so files can be changed with more safety\n\t\t\tdocker stop $(docker ps -a -q --filter \"name=so-\")\n\t\t\tdocker rm -f $(docker ps -a -q --filter \"name=so-\")\n\t\tfi\n\n\t\tlocal date_string\n\t\tdate_string=$(date +%s)\n\n\t\t# Backup /opt/so since we'll be rebuilding this directory during setup\n\t\tif [[ -d /opt/so ]]; then\n\t\t\tmv /opt/so \"/opt/so_old_${date_string}\"\n\t\tfi\n\n\t\t# Backup /nsm for the same reason\n\t\twhile IFS= read -r -d '' dir; do\n\t\t\tmv \"$dir\" \"${dir}_old_${date_string}\"\n\t\tdone < <(find /nsm -maxdepth 1 -mindepth 1 -type d -print0)\n\n\t\t# Remove the old launcher package in case the config changes\n\t\tremove_package launcher-final\n\n\t} >> $setup_log 2>&1\n}\n\nremove_package() {\n\tlocal package_name=$1\n\tif [ $OS = 'centos' ]; then\n\t\tif rpm -qa | grep -q \"$package_name\"; then\n\t\t\tyum remove -y \"$package_name\"\n\t\tfi\n\telse\n\t\tif dpkg -l | grep -q \"$package_name\"; then\n\t\t\tapt purge -y \"$package_name\"\n\t\tfi\n\tfi\n}\n\n# When updating the salt version, also update the version in securityonion-builds/images/iso-task/Dockerfile and salt/salt/master.defaults.yaml and salt/salt/minion.defaults.yaml\n# CAUTION! SALT VERSION UDDATES - READ BELOW\n# When updating the salt version, also update the version in:\n# - securityonion-builds/iso-resources/build.sh\n# - securityonion-builds/iso-resources/packages.lst\n# - securityonion/salt/salt/master.defaults.yaml\n# - securityonion/salt/salt/minion.defaults.yaml\nsaltify() {\n\n\t# Install updates and Salt\n\tif [ $OS = 'centos' ]; then\n\t\tset_progress_str 5 'Installing Salt repo'\n\t\t{\n\t\t\tsudo rpm --import https://repo.saltstack.com/py3/redhat/7/x86_64/archive/3002.1/SALTSTACK-GPG-KEY.pub;\n\t\t\tcp ./yum_repos/saltstack.repo /etc/yum.repos.d/saltstack.repo;\n\t\t} >> \"$setup_log\" 2>&1\n\t\tset_progress_str 6 'Installing various dependencies'\n\t\tif [[ ! $is_iso ]]; then\n\t\t  logCmd \"yum -y install wget nmap-ncat\"\n\t\tfi \n\t\tcase \"$install_type\" in\n\t\t\t'MANAGER' | 'EVAL' | 'MANAGERSEARCH' | 'FLEET' | 'HELIXSENSOR' | 'STANDALONE'| 'IMPORT')\n\t\t\t\treserve_group_ids >> \"$setup_log\" 2>&1\n\t\t\t\tif [[ ! $is_iso ]]; then\n\t\t\t\t  logCmd \"yum -y install epel-release\"\n\t\t\t\t  logCmd \"yum -y install sqlite argon2 curl mariadb-devel\"\n\t\t\t\tfi \n\t\t\t\t# Download Ubuntu Keys in case manager updates = 1\n\t\t\t\tmkdir -p /opt/so/gpg >> \"$setup_log\" 2>&1\n\t\t\t\tif [[ ! $is_airgap ]]; then\n\t\t\t\t  logCmd \"wget -q --inet4-only -O /opt/so/gpg/SALTSTACK-GPG-KEY.pub https://repo.saltstack.com/py3/ubuntu/18.04/amd64/archive/3002.1/SALTSTACK-GPG-KEY.pub\"\n\t\t\t\t  logCmd \"wget -q --inet4-only -O /opt/so/gpg/docker.pub https://download.docker.com/linux/ubuntu/gpg\"\n\t\t\t\t  logCmd \"wget -q --inet4-only -O /opt/so/gpg/GPG-KEY-WAZUH https://packages.wazuh.com/key/GPG-KEY-WAZUH\"\n\t\t\t\t  logCmd \"cp ./yum_repos/wazuh.repo /etc/yum.repos.d/wazuh.repo\"\n\t\t\t\tfi\n\t\t\t\tset_progress_str 7 'Installing salt-master'\n\t\t\t\tif [[ ! $is_iso ]]; then\n\t\t\t\t  logCmd \"yum -y install salt-master-3002.1\"\n\t\t\t\tfi\n\t\t\t\tsystemctl enable salt-master >> \"$setup_log\" 2>&1\n\t\t\t\t;;\n\t\t\t*)\n\t\t\t\tif [ \"$MANAGERUPDATES\" = '1' ]; then\n\t\t\t\t\t{\n\t\t\t\t\t\tif [[ ! $is_airgap ]]; then\n\t\t\t\t\t\t  # Create the GPG Public Key for the Salt Repo\n\t\t\t\t\t\t  cp ./public_keys/salt.pem /etc/pki/rpm-gpg/saltstack-signing-key;\n\n\t\t\t\t\t\t  # Copy repo files over\n\t\t\t\t\t\t  cp ./yum_repos/saltstack.repo /etc/yum.repos.d/saltstack.repo;\n\t\t\t\t\t\telse\n\t\t\t\t\t\t  info \"This is airgap\"\n\t\t\t\t\t\tfi\n\t\t\t\t\t} >> \"$setup_log\" 2>&1\n\t\t\t\tfi\n\t\t\t\t;;\n\t\tesac\n\t\tif [[ ! $is_airgap ]]; then\n\t\t  cp ./yum_repos/wazuh.repo /etc/yum.repos.d/wazuh.repo >> \"$setup_log\" 2>&1\n\t\t  yum clean expire-cache >> \"$setup_log\" 2>&1\n\t\tfi\n\t\tset_progress_str 8 'Installing salt-minion & python modules'\n\t\t{\n\t\t\tif [[ ! $is_iso ]]; then\n\t\t\t  yum -y install epel-release\n\t\t\t  yum -y install salt-minion-3002.1\\\n\t\t\t   \tpython3\\\n\t\t\t\tpython36-docker\\\n\t\t\t\tpython36-dateutil\\\n\t\t\t\tpython36-m2crypto\\\n\t\t\t\tpython36-mysql\\\n\t\t\t\tyum-utils\\\n\t\t\t\tdevice-mapper-persistent-data\\\n\t\t\t\tlvm2\\\n\t\t\t\topenssl\\\n\t\t\t\tjq;\n\t\t\t  yum -y update --exclude=salt*;\n\t\t\tfi\n\t\t\tsystemctl enable salt-minion;\n\t\t} >> \"$setup_log\" 2>&1\n\t\tyum versionlock salt*\n\telse\n\t\tDEBIAN_FRONTEND=noninteractive apt-get -y -o Dpkg::Options::=\"--force-confdef\" -o Dpkg::Options::=\"--force-confold\" upgrade >> \"$setup_log\" 2>&1\n\n\t\tif [ $OSVER != \"xenial\" ]; then\n\t\t\t# Switch to Python 3 as default if this is not xenial\n\t\t\tupdate-alternatives --install /usr/bin/python python /usr/bin/python3.6 10 >> \"$setup_log\" 2>&1\n\t\tfi\n\t\t# Add the pre-requisites for installing docker-ce\n\t\tapt-get -y install ca-certificates\\\n\t\t\tcurl\\\n\t\t\tsoftware-properties-common\\\n\t\t\tapt-transport-https\\\n\t\t\topenssl\\\n\t\t\tnetcat\\\n\t\t\tjq >> \"$setup_log\" 2>&1\n\n\t\t# Grab the version from the os-release file\n\t\tlocal ubuntu_version\n\t\tubuntu_version=$(grep VERSION_ID /etc/os-release | awk -F '[ \"]' '{print $2}')\n\t\tif [ \"$OSVER\" != \"xenial\" ]; then local py_ver_url_path=\"/py3\"; else local py_ver_url_path=\"/apt\"; fi\n\n\t\tcase \"$install_type\" in\n\t\t\t'FLEET')\n\t\t\t\tif [ \"$OSVER\" != 'xenial' ]; then apt-get -y install python3-mysqldb >> \"$setup_log\" 2>&1; else apt-get -y install python-mysqldb >> \"$setup_log\" 2>&1; fi\n\t\t\t\t;;\n\t\t\t'MANAGER' | 'EVAL' | 'MANAGERSEARCH' | 'STANDALONE' | 'IMPORT') # TODO: should this also be HELIXSENSOR?\n\n\t\t\t\t# Add saltstack repo(s)\n\t\t\t\twget -q --inet4-only -O - https://repo.saltstack.com\"$py_ver_url_path\"/ubuntu/\"$ubuntu_version\"/amd64/archive/3002.1/SALTSTACK-GPG-KEY.pub | apt-key add - >> \"$setup_log\" 2>&1\n\t\t\t\techo \"deb http://repo.saltstack.com$py_ver_url_path/ubuntu/$ubuntu_version/amd64/archive/3002.1 $OSVER main\" > /etc/apt/sources.list.d/saltstack.list 2>> \"$setup_log\"\n\n\t\t\t\t# Add Docker repo\n\t\t\t\tcurl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add - >> \"$setup_log\" 2>&1\n\t\t\t\tadd-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" >> \"$setup_log\" 2>&1\n\n\t\t\t\t# Get gpg keys\n\t\t\t\tmkdir -p /opt/so/gpg >> \"$setup_log\" 2>&1\n\t\t\t\twget -q --inet4-only -O /opt/so/gpg/SALTSTACK-GPG-KEY.pub https://repo.saltstack.com$py_ver_url_path/ubuntu/\"$ubuntu_version\"/amd64/archive/3002.1/SALTSTACK-GPG-KEY.pub >> \"$setup_log\" 2>&1\n\t\t\t\twget -q --inet4-only -O /opt/so/gpg/docker.pub https://download.docker.com/linux/ubuntu/gpg >> \"$setup_log\" 2>&1\n\t\t\t\twget -q --inet4-only -O /opt/so/gpg/GPG-KEY-WAZUH https://packages.wazuh.com/key/GPG-KEY-WAZUH >> \"$setup_log\" 2>&1\n\n\t\t\t\t# Get key and install wazuh\n\t\t\t\tcurl -s https://packages.wazuh.com/key/GPG-KEY-WAZUH | apt-key add - >> \"$setup_log\" 2>&1\n\t\t\t\t# Add repo\n\t\t\t\techo \"deb https://packages.wazuh.com/3.x/apt/ stable main\" > /etc/apt/sources.list.d/wazuh.list 2>> \"$setup_log\"\n\t\t\t\t# Initialize the new repos\n\t\t\t\tapt-get update >> \"$setup_log\" 2>&1\n\t\t\t\tset_progress_str 6 'Installing various dependencies'\n\t\t\t\tapt-get -y install sqlite3 argon2 libssl-dev >> \"$setup_log\" 2>&1\n\t\t\t\tset_progress_str 7 'Installing salt-master'\n\t\t\t\tapt-get -y install salt-master=3002.1+ds-1 >> \"$setup_log\" 2>&1\n\t\t\t\tapt-mark hold salt-master >> \"$setup_log\" 2>&1\n\t\t\t\t;;\n\t\t\t*)\n\t\t\t\t# Copy down the gpg keys and install them from the manager\n\t\t\t\tmkdir \"$temp_install_dir\"/gpg >> \"$setup_log\" 2>&1\n\t\t\t\techo \"scp the gpg keys and install them from the manager\" >> \"$setup_log\" 2>&1\n\t\t\t\tscp -v -i /root/.ssh/so.key soremote@\"$MSRV\":/opt/so/gpg/* \"$temp_install_dir\"/gpg >> \"$setup_log\" 2>&1\n\t\t\t\techo \"Using apt-key add to add SALTSTACK-GPG-KEY.pub and GPG-KEY-WAZUH\" >> \"$setup_log\" 2>&1\n\t\t\t\tapt-key add \"$temp_install_dir\"/gpg/SALTSTACK-GPG-KEY.pub >> \"$setup_log\" 2>&1\n\t\t\t\tapt-key add \"$temp_install_dir\"/gpg/GPG-KEY-WAZUH >> \"$setup_log\" 2>&1\n\t\t\t\techo \"deb http://repo.saltstack.com$py_ver_url_path/ubuntu/$ubuntu_version/amd64/archive/3002.1/ $OSVER main\" > /etc/apt/sources.list.d/saltstack.list 2>> \"$setup_log\"\n\t\t\t\techo \"deb https://packages.wazuh.com/3.x/apt/ stable main\" > /etc/apt/sources.list.d/wazuh.list 2>> \"$setup_log\"\n                ;;\n\t\tesac\n\t\tapt-get update >> \"$setup_log\" 2>&1\n\t\tset_progress_str 8 'Installing salt-minion & python modules'\n\t\tapt-get -y install salt-minion=3002.1+ds-1\\\n\t\t\t\t\tsalt-common=3002.1+ds-1 >> \"$setup_log\" 2>&1\n\t\tapt-mark hold salt-minion salt-common >> \"$setup_log\" 2>&1\n\t\tif [ \"$OSVER\" != 'xenial' ]; then \n\t\t\tapt-get -y install python3-pip python3-dateutil python3-m2crypto python3-mysqldb >> \"$setup_log\" 2>&1\n\t\telse \n\t\t\tapt-get -y install python-pip python-dateutil python-m2crypto python-mysqldb  >> \"$setup_log\" 2>&1\n\t\tfi\n\tfi\n\n}\n\nsalt_checkin() {\n\tcase \"$install_type\" in\n\t\t'MANAGER' | 'EVAL' | 'HELIXSENSOR' | 'MANAGERSEARCH' | 'STANDALONE' | 'IMPORT') # Fix Mine usage\n\t\t\t{\n\t\t\t\techo \"Building Certificate Authority\";\n\t\t\t\tsalt-call state.apply ca;\n\t\t\t\techo \" *** Restarting Salt to fix any SSL errors. ***\";\n\n\t\t\t\tlocal SALT_SERVICES=(\\\n\t\t\t\t\"salt-master\" \\\n\t\t\t\t\"salt-minion\"\n\t\t\t\t)\n\t\t\t\tlocal LOOP_COUNT=0\n\t\t\t\tfor service in \"${SALT_SERVICES[@]}\"; do\n\t\t\t\t\techo \"Stopping service $service\" >> \"$setup_log\" 2>&1\n\t\t\t\t\tsystemctl stop \"$service\" >> \"$setup_log\" 2>&1\n\t\t\t\t\tLOOP_COUNT=0\n\t\t\t\t\twhile ! (( $(check_service_status $service) )); do\n\t\t\t\t\t\techo \"$service still running\" >> \"$setup_log\" 2>&1\n\t\t\t\t\t\tif [ $LOOP_COUNT -gt 60 ]; then\n\t\t\t\t\t\t\techo \"$service could not be stopped in 60 seconds, exiting\" >> \"$setup_log\" 2>&1\n\t\t\t\t\t\t\texit 1\n\t\t\t\t\t\tfi\n\t\t\t\t\t\tsleep 1;\n\t\t\t\t\t\t((LOOP_COUNT+=1))\n\t\t\t\t\tdone\n\t\t\t\tdone\n\n\t\t\t\tsleep 5;\n\n\t\t\t\tfor service in \"${SALT_SERVICES[@]}\"; do\n\t\t\t\t\techo \"Starting service $service\" >> \"$setup_log\" 2>&1\n\t\t\t\t\tsystemctl start \"$service\" >> \"$setup_log\" 2>&1\n\t\t\t\t\tLOOP_COUNT=0\n\t\t\t\t\twhile (( $(check_service_status $service) )); do\n\t\t\t\t\t\techo \"$service still not running\" >> \"$setup_log\" 2>&1\n\t\t\t\t\t\tif [ $LOOP_COUNT -gt 60 ]; then\n\t\t\t\t\t\t\techo \"$service could not be started in 60 seconds, exiting\" >> \"$setup_log\" 2>&1\n\t\t\t\t\t\t\texit 1\n\t\t\t\t\t\tfi\n\t\t\t\t\t\tsleep 1;\n\t\t\t\t\t\t((LOOP_COUNT+=1))\n\t\t\t\t\tdone\n\t\t\t\tdone\n\n\t\t\t\tsleep 5;\n\n\t\t\t\tLOOP_COUNT=0\n\t\t\t\twhile (( $(check_salt_master_status) )); do\n\t\t\t\t\techo \"salt minion cannot talk to salt master\" >> \"$setup_log\" 2>&1\n\t\t\t\t\tif [ $LOOP_COUNT -gt 30 ]; then\n\t\t\t\t\t\techo \"salt minion could not talk to salt master after 30 attempts, exiting\" >> \"$setup_log\" 2>&1\n\t\t\t\t\t\texit 1\n\t\t\t\t\tfi\n\t\t\t\t\tsleep 1;\n\t\t\t\t\t((LOOP_COUNT+=1))\n\t\t\t\tdone\n\n\t\t\t\tLOOP_COUNT=0\n\t\t\t\twhile (( $(check_salt_minion_status) )); do\n\t\t\t\t\techo \"salt master did not get a job response from salt minion\" >> \"$setup_log\" 2>&1\n\t\t\t\t\tif [ $LOOP_COUNT -gt 30 ]; then\n\t\t\t\t\t\techo \"salt master did not get a job response from salt minion after 30 attempts, exiting\" >> \"$setup_log\" 2>&1\n\t\t\t\t\t\texit 1\n\t\t\t\t\tfi\n\t\t\t\t\tsleep 1;\n\t\t\t\t\t((LOOP_COUNT+=1))\n\t\t\t\tdone\n\n\t\t\t\techo \" Confirming existence of the CA certificate\"\n\t\t\t\topenssl x509 -in /etc/pki/ca.crt -noout -subject -issuer -dates\n\t\t\t\techo \" Applyng a mine hack\";\n\t\t\t\tsalt \"$MINION_ID\" mine.send x509.get_pem_entries glob_path=/etc/pki/ca.crt;\n\t\t\t\tsalt \"$MINION_ID\" mine.update;\n\t\t\t\techo \"Confirming salt mine now contains the certificate\";\n\t\t\t\tsalt \"$MINION_ID\" mine.get '*' x509.get_pem_entries | grep -E 'BEGIN CERTIFICATE|END CERTIFICATE';\n\t\t\t\tif [ $? -eq 0 ]; then\n\t\t\t\t\techo \"CA in mine\"\n\t\t\t\telse\n\t\t\t\t\techo \"CA not in mine\"\n\t\t\t\tfi\n\t\t\t\techo \" Applying SSL state\";\n\t\t\t\tsalt-call state.apply ssl;\n\t\t\t} >> \"$setup_log\" 2>&1\n\t\t\t;;\n\t\t*)\n\t\t\t{\n\t\t\t\tsalt-call state.apply ca;\n\t\t\t\tsalt-call state.apply ssl;\n\t\t\t} >> \"$setup_log\" 2>&1\n\t\t\t;;\n\tesac\n\t{\n\t\tsalt-call state.apply ca;\n\t\tsalt-call state.apply ssl;\n\t\tsalt-call saltutil.sync_modules;\n\t} >> \"$setup_log\" 2>&1\n}\n\n# Run a salt command to generate the minion key\nsalt_firstcheckin() {\n\tsalt-call state.show_top >> /dev/null 2>&1 # send output to /dev/null because we don't actually care about the ouput\n}\n\nset_base_heapsizes() {\n\tes_heapsize\n\tls_heapsize\n}\n\nset_network_dev_status_list() {\n\treadarray -t nmcli_dev_status_list <<< \"$(nmcli -t -f DEVICE,STATE -c no dev status)\"\n\texport nmcli_dev_status_list\n}\n\nset_main_ip() {\n\tMAINIP=$(ip route get 1 | awk '{print $7;exit}')\n}\n\n# Add /usr/sbin to everyone's path\nset_path() {\n\techo \"complete -cf sudo\" > /etc/profile.d/securityonion.sh\n}\n\nsetup_salt_master_dirs() {\n\t# Create salt master directories\n\tmkdir -p $default_salt_dir/pillar\n\tmkdir -p $default_salt_dir/salt\n\tmkdir -p $local_salt_dir/pillar\n\tmkdir -p $local_salt_dir/salt\n\n\t# Copy over the salt code and templates\n\tif [ \"$setup_type\" = 'iso' ]; then\n\t\trsync -avh --exclude 'TRANS.TBL' /home/$INSTALLUSERNAME/SecurityOnion/pillar/* $default_salt_dir/pillar/ >> \"$setup_log\" 2>&1\n\t\trsync -avh --exclude 'TRANS.TBL' /home/$INSTALLUSERNAME/SecurityOnion/salt/* $default_salt_dir/salt/ >> \"$setup_log\" 2>&1\n\t\tmkdir -p $local_salt_dir/salt/zeek/policy/intel >> \"$setup_log\" 2>&1\n\t\tcp -Rv /home/$INSTALLUSERNAME/SecurityOnion/files/intel.dat $local_salt_dir/salt/zeek/policy/intel/ >> \"$setup_log\" 2>&1\n\telse\n\t\tcp -Rv ../pillar/* $default_salt_dir/pillar/ >> \"$setup_log\" 2>&1\n\t\tcp -Rv ../salt/* $default_salt_dir/salt/ >> \"$setup_log\" 2>&1\n\t\tmkdir -p $local_salt_dir/salt/zeek/policy/intel >> \"$setup_log\" 2>&1\n\t\tcp -Rv files/intel.dat $local_salt_dir/salt/zeek/policy/intel/ >> \"$setup_log\" 2>&1\n\tfi\n\n\techo \"Chown the salt dirs on the manager for socore\" >> \"$setup_log\" 2>&1\n\tchown -R socore:socore /opt/so\n}\n\nset_progress_str() {\n\tlocal percentage_input=$1\n\tlocal progress_bar_text=$2\n\n\tif (( \"$percentage_input\" >= \"$percentage\" )); then\n\t\tpercentage=\"$percentage_input\"\n\tfi\n\n\tpercentage_str=\"XXX\\n${percentage}\\n${progress_bar_text}\\nXXX\"\n\n\techo -e \"$percentage_str\"\n\n\tprintf '%s\\n' \\\n\t'----'\\\n\t\"$percentage% - ${progress_bar_text^^}\"\\\n\t\"----\" >> \"$setup_log\" 2>&1\n}\n\nsensor_pillar() {\n\n\tlocal pillar_file=$temp_install_dir/pillar/minions/$MINION_ID.sls\n\n\t# Create the sensor pillar\n\tprintf '%s\\n'\\\n\t\t\"sensor:\"\\\n\t\t\"  interface: '$INTERFACE'\"\\\n\t\t\"  mainip: '$MAINIP'\"\\\n\t\t\"  mainint: '$MNIC'\" >> \"$pillar_file\"\n\t\n\tif [ \"$NSMSETUP\" = 'ADVANCED' ]; then\n\t\techo \"  zeek_pins:\" >> \"$pillar_file\"\n\t\tfor PIN in \"${ZEEKPINS[@]}\"; do\n\t\t\tPIN=$(echo \"$PIN\" |  cut -d\\\" -f2)\n\t\techo \"    - $PIN\" >> \"$pillar_file\"\n\t\tdone\n\t\techo \"  suripins:\" >> \"$pillar_file\"\n\t\tfor SPIN in \"${SURIPINS[@]}\"; do\n\t\t\tSPIN=$(echo \"$SPIN\" |  cut -d\\\" -f2)\n\t\techo \"    - $SPIN\" >> \"$pillar_file\"\n\t\tdone\n\telif [ \"$install_type\" = 'HELIXSENSOR' ]; then\n\t\techo \"  zeek_lbprocs: $lb_procs\" >> \"$pillar_file\"\n\t\techo \"  suriprocs: $lb_procs\" >> \"$pillar_file\"\n\telse\n\t\techo \"  zeek_lbprocs: $BASICZEEK\" >> \"$pillar_file\"\n\t\techo \"  suriprocs: $BASICSURI\" >> \"$pillar_file\"\n\tfi\n\tprintf '%s\\n'\\\n\t\t\"  manager: '$MSRV'\"\\\n\t\t\"  mtu: $MTU\"\\\n\t\t\"  uniqueid: $(date '+%s')\" >> \"$pillar_file\"\n\tif [ \"$HNSENSOR\" != 'inherit' ]; then\n\t\techo \"  hnsensor: $HNSENSOR\" >> \"$pillar_file\"\n\tfi\n\n}\n\nset_default_log_size() {\n    local percentage\n\n\tcase $INSTALLTYPE in\n\t\tSTANDALONE | EVAL | HEAVYNODE)\n\t\t\tpercentage=50\n\t\t\t;;\n\t\t*)\n            percentage=80\n\t\t\t;;\n\t\tesac\n\n\tlocal disk_dir=\"/\"\n\tif [ -d /nsm ]; then\n\t\tdisk_dir=\"/nsm\"\n\tfi\n\tlocal disk_size_1k\n\tdisk_size_1k=$(df $disk_dir | grep -v \"^Filesystem\" | awk '{print $2}')\n\n    local ratio=\"1048576\"\n\n    local disk_size_gb\n    disk_size_gb=$( echo \"$disk_size_1k\" \"$ratio\" | awk '{print($1/$2)}' )\n\n\tlog_size_limit=$( echo \"$disk_size_gb\" \"$percentage\" | awk '{printf(\"%.0f\", $1 * ($2/100))}')\n}\n\nset_hostname() {\n\n\thostnamectl set-hostname --static \"$HOSTNAME\"\n\techo \"127.0.0.1   $HOSTNAME $HOSTNAME.localdomain localhost localhost.localdomain localhost4 localhost4.localdomain\" > /etc/hosts\n\techo \"::1   $HOSTNAME $HOSTNAME.localdomain localhost localhost.localdomain localhost6 localhost6.localdomain6\" >> /etc/hosts\n\techo \"$HOSTNAME\" > /etc/hostname\n\n\thostname -F /etc/hostname\n}\n\nset_initial_firewall_policy() {\n\n  set_main_ip\n\n  if [ -f $default_salt_dir/pillar/data/addtotab.sh ]; then chmod +x $default_salt_dir/pillar/data/addtotab.sh; fi\n  if [ -f $default_salt_dir/salt/common/tools/sbin/so-firewall ]; then chmod +x $default_salt_dir/salt/common/tools/sbin/so-firewall; fi\n\n\tcase \"$install_type\" in\n\t\t'MANAGER')\n\t\t\t$default_salt_dir/salt/common/tools/sbin/so-firewall includehost manager \"$MAINIP\"\n\t\t\t$default_salt_dir/salt/common/tools/sbin/so-firewall --apply includehost minion \"$MAINIP\"\n\t\t\t$default_salt_dir/pillar/data/addtotab.sh managertab \"$MINION_ID\" \"$MAINIP\" \"$num_cpu_cores\" \"$random_uid\" \"$MNIC\" \"$filesystem_root\" \"$filesystem_nsm\"\n\t\t\t;;\n\t\t'EVAL' | 'MANAGERSEARCH' | 'STANDALONE' | 'IMPORT')\n\t\t\t$default_salt_dir/salt/common/tools/sbin/so-firewall includehost manager \"$MAINIP\"\n\t\t\t$default_salt_dir/salt/common/tools/sbin/so-firewall includehost minion \"$MAINIP\"\n            $default_salt_dir/salt/common/tools/sbin/so-firewall includehost sensor \"$MAINIP\"\n            $default_salt_dir/salt/common/tools/sbin/so-firewall --apply includehost search_node \"$MAINIP\"\n\t\t\tcase \"$install_type\" in \n\t\t\t\t'EVAL')\n\t\t\t\t\t$default_salt_dir/pillar/data/addtotab.sh evaltab \"$MINION_ID\" \"$MAINIP\" \"$num_cpu_cores\" \"$random_uid\" \"$MNIC\" \"$filesystem_root\" \"$filesystem_nsm\" \"$INTERFACE\" True\n\t\t\t\t\t;;\n\t\t\t\t'MANAGERSEARCH')\n\t\t\t\t\t$default_salt_dir/pillar/data/addtotab.sh managersearchtab \"$MINION_ID\" \"$MAINIP\" \"$num_cpu_cores\" \"$random_uid\" \"$MNIC\" \"$filesystem_root\" \"$filesystem_nsm\"\n\t\t\t\t\t;;\n\t\t\t\t'STANDALONE')\n\t\t\t\t\t$default_salt_dir/pillar/data/addtotab.sh standalonetab \"$MINION_ID\" \"$MAINIP\" \"$num_cpu_cores\" \"$random_uid\" \"$MNIC\" \"$filesystem_root\" \"$filesystem_nsm\" \"$INTERFACE\"\n\t\t\t\t\t;;\n\t\t\tesac\n\t\t\t;;\n\t\t'HELIXSENSOR')\n\t\t\t$default_salt_dir/salt/common/tools/sbin/so-firewall includehost manager \"$MAINIP\"\n\t\t\t$default_salt_dir/salt/common/tools/sbin/so-firewall includehost minion \"$MAINIP\"\n            $default_salt_dir/salt/common/tools/sbin/so-firewall --apply includehost sensor \"$MAINIP\"\n\t\t\t;;\n\t\t'SENSOR' | 'SEARCHNODE' | 'HEAVYNODE' | 'FLEET')\n\t\t\tssh -i /root/.ssh/so.key soremote@\"$MSRV\" sudo $default_salt_dir/salt/common/tools/sbin/so-firewall includehost minion \"$MAINIP\"\n\t\t\tcase \"$install_type\" in\n\t\t\t\t'SENSOR')\n\t\t\t\t\tssh -i /root/.ssh/so.key soremote@\"$MSRV\" sudo $default_salt_dir/salt/common/tools/sbin/so-firewall --apply includehost sensor \"$MAINIP\"\n\t\t\t\t\tssh -i /root/.ssh/so.key soremote@\"$MSRV\" sudo $default_salt_dir/pillar/data/addtotab.sh sensorstab \"$MINION_ID\" \"$MAINIP\" \"$num_cpu_cores\" \"$random_uid\" \"$MNIC\" \"$filesystem_root\" \"$filesystem_nsm\" \"$INTERFACE\"\n\t\t\t\t\t;;\n\t\t\t\t'SEARCHNODE')\n\t\t\t\t\tssh -i /root/.ssh/so.key soremote@\"$MSRV\" sudo $default_salt_dir/salt/common/tools/sbin/so-firewall --apply includehost search_node \"$MAINIP\"\n\t\t\t\t\tssh -i /root/.ssh/so.key soremote@\"$MSRV\" sudo $default_salt_dir/pillar/data/addtotab.sh nodestab \"$MINION_ID\" \"$MAINIP\" \"$num_cpu_cores\" \"$random_uid\" \"$MNIC\" \"$filesystem_root\" \"$filesystem_nsm\"\n\t\t\t\t\t;;\n\t\t\t\t'HEAVYNODE')\n\t\t\t\t\tssh -i /root/.ssh/so.key soremote@\"$MSRV\" sudo $default_salt_dir/salt/common/tools/sbin/so-firewall includehost sensor \"$MAINIP\"\n\t\t\t\t\tssh -i /root/.ssh/so.key soremote@\"$MSRV\" sudo $default_salt_dir/salt/common/tools/sbin/so-firewall --apply includehost search_node \"$MAINIP\"\n\t\t\t\t\tssh -i /root/.ssh/so.key soremote@\"$MSRV\" sudo $default_salt_dir/pillar/data/addtotab.sh sensorstab \"$MINION_ID\" \"$MAINIP\" \"$num_cpu_cores\" \"$random_uid\" \"$MNIC\" \"$filesystem_root\" \"$filesystem_nsm\" \"$INTERFACE\"\n\t\t\t\t\tssh -i /root/.ssh/so.key soremote@\"$MSRV\" sudo $default_salt_dir/pillar/data/addtotab.sh nodestab \"$MINION_ID\" \"$MAINIP\" \"$num_cpu_cores\" \"$random_uid\" \"$MNIC\" \"$filesystem_root\" \"$filesystem_nsm\"\n\t\t\t\t\t;;\n\t\t\t\t'FLEET')\n\t\t\t\t    ssh -i /root/.ssh/so.key soremote@\"$MSRV\" sudo $default_salt_dir/salt/common/tools/sbin/so-firewall --apply includehost beats_endpoint_ssl \"$MAINIP\"\n\t\t\t\t\t;;\n\t\t\tesac\n\t\t\t;;\n\t\t'PARSINGNODE')\n\t\t\t# TODO: implement\n\t\t\t;;\n\t\t'HOTNODE')\n\t\t\t# TODO: implement\n\t\t\t;;\n\t\t'WARMNODE')\n\t\t\t# TODO: implement\n\t\t\t;;\n\tesac\n}\n\n# Set up the management interface on the ISO\nset_management_interface() {\n\n\tif [ \"$address_type\" = 'DHCP' ]; then\n\t\tnmcli con mod \"$MNIC\" connection.autoconnect yes >> \"$setup_log\" 2>&1\n\t\tnmcli con up \"$MNIC\" >> \"$setup_log\" 2>&1\n\telse\n\t\t# Set Static IP\n\t\tnmcli con mod \"$MNIC\" ipv4.addresses \"$MIP\"/\"$MMASK\"\\\n\t\t\tipv4.gateway \"$MGATEWAY\" \\\n\t\t\tipv4.dns \"$MDNS\"\\\n\t\t\tipv4.dns-search \"$MSEARCH\"\\\n\t\t\tconnection.autoconnect yes\\\n\t\t\tipv4.method manual >> \"$setup_log\" 2>&1\n\t\tnmcli con up \"$MNIC\" >> \"$setup_log\" 2>&1\n\tfi\n}\n\nset_node_type() {\n\n\tcase \"$install_type\" in\n\t\t'SEARCHNODE' | 'EVAL' | 'MANAGERSEARCH' | 'HEAVYNODE' | 'STANDALONE')\n\t\t\tNODETYPE='search'\n\t\t\t;;\n\t\t'HOTNODE')\n\t\t\tNODETYPE='hot'\n\t\t\t;;\n\t\t'WARMNODE')\n\t\t\tNODETYPE='warm'\n\t\t\t;;\n\tesac\n}\n\nset_redirect() {\n\tcase $REDIRECTINFO in\n\t\t'IP')\n\t\t\tREDIRECTIT=\"$MAINIP\"\n\t\t\t;;\n\t\t'HOSTNAME')\n\t\t\tREDIRECTIT=\"$HOSTNAME\"\n\t\t\t;;\n\t\t*)\n\t\t\tREDIRECTIT=\"$REDIRECTHOST\"\n\t\t\t;;\n\tesac\n}\n\nset_updates() {\n\tif [ \"$MANAGERUPDATES\" = '1' ]; then\n\t\tif [ \"$OS\" = 'centos' ]; then\n\t\t    if [[ ! $is_airgap ]]; then\n\t\t\t    if ! grep -q \"$MSRV\" /etc/yum.conf; then\n\t\t\t\t    echo \"proxy=http://$MSRV:3142\" >> /etc/yum.conf\n\t\t\t    fi\n\t\t\tfi\n\t\telse\n\t\t\t# Set it up so the updates roll through the manager\n\t\t\tprintf '%s\\n'\\\n\t\t\t\t\"Acquire::http::Proxy \\\"http://$MSRV:3142\\\";\"\\\n\t\t\t\t\"Acquire::https::Proxy \\\"http://$MSRV:3142\\\";\" > /etc/apt/apt.conf.d/00Proxy\n\t\tfi\n\tfi\n}\n\nmark_version() {\n\t# Drop a file with the current version\n\techo \"$SOVERSION\" > /etc/soversion\n}\n\nupdate_sudoers() {\n\n\tif ! grep -qE '^soremote\\ ALL=\\(ALL\\)\\ NOPASSWD:(\\/usr\\/bin\\/salt\\-key|\\/opt\\/so\\/saltstack)' /etc/sudoers; then\n\t\t# Update Sudoers so that soremote can accept keys without a password\n\t\techo \"soremote ALL=(ALL) NOPASSWD:/usr/bin/salt-key\" | tee -a /etc/sudoers\n\t\techo \"soremote ALL=(ALL) NOPASSWD:$default_salt_dir/salt/common/tools/sbin/so-firewall\" | tee -a /etc/sudoers\n\t\techo \"soremote ALL=(ALL) NOPASSWD:$default_salt_dir/pillar/data/addtotab.sh\" | tee -a /etc/sudoers\n\t\techo \"soremote ALL=(ALL) NOPASSWD:$default_salt_dir/salt/manager/files/add_minion.sh\" | tee -a /etc/sudoers\n\telse\n\t\techo \"User soremote already granted sudo privileges\" >> \"$setup_log\" 2>&1\n\tfi\n}\n\nupdate_packages() {\n\tif [ \"$OS\" = 'centos' ]; then\n\t\tyum -y update >> \"$setup_log\"\n\telse\n\t\tapt-get -y update >> \"$setup_log\"\n\t\tapt-get -y upgrade >> \"$setup_log\"\n\tfi\n}\n\n# This is used for development to speed up network install tests.\nuse_turbo_proxy() {\n\tif [[ ! $install_type =~ ^(MANAGER|EVAL|HELIXSENSOR|MANAGERSEARCH|STANDALONE)$ ]]; then\n\t\techo \"turbo is not supported on this install type\" >> $setup_log 2>&1\n\t\treturn\n\tfi\n\n\tif [[ $OS == 'centos' ]]; then\n\t\tprintf '%s\\n' \"proxy=${TURBO}:3142\" >> /etc/yum.conf\n\telse\n\t\tprintf '%s\\n'\\\n\t\t\t\"Acquire {\"\\\n\t\t\t\"  HTTP::proxy \\\"${TURBO}:3142\\\";\"\\\n\t\t\t\"  HTTPS::proxy \\\"${TURBO}:3142\\\";\"\\\n\t\t\t\"}\" > /etc/apt/apt.conf.d/proxy.conf\n\tfi\n}\n\n# Set Logstash heap size based on total memory\nls_heapsize() {\n\n\tif [ \"$total_mem\" -ge 32000 ]; then\n\t\tLS_HEAP_SIZE='1000m'\n\t\treturn\n\tfi\n\n\tcase \"$install_type\" in\n\t\t'MANAGERSEARCH' | 'HEAVYNODE' | 'HELIXSENSOR' | 'STANDALONE')\n\t\t\tLS_HEAP_SIZE='1000m'\n\t\t\t;;\n\t\t'EVAL')\n\t\t\tLS_HEAP_SIZE='700m'\n\t\t\t;;\n\t\t*)\n\t\t\tLS_HEAP_SIZE='500m'\n\t\t\t;;\n\tesac\n\texport LS_HEAP_SIZE\n\n\tif [[ \"$install_type\" =~ ^(EVAL|MANAGERSEARCH|STANDALONE)$ ]]; then\n\t\tNODE_LS_HEAP_SIZE=LS_HEAP_SIZE\n\t\texport NODE_LS_HEAP_SIZE\n\tfi\n}\n\n\nes_heapsize() {\n\n\t# Determine ES Heap Size\n\tif [ \"$total_mem\" -lt 8000 ] ; then\n\t\tES_HEAP_SIZE=\"600m\"\n\telif [ \"$total_mem\" -ge 100000 ]; then\n\t\t# Set a max of 25GB for heap size\n\t\t# https://www.elastic.co/guide/en/elasticsearch/guide/current/heap-sizing.html\n\t\tES_HEAP_SIZE=\"25000m\"\n\telse\n\t\t# Set heap size to 25% of available memory\n\t\tES_HEAP_SIZE=$(( total_mem / 4 ))\"m\"\n\tfi\n\texport ES_HEAP_SIZE\n\n\tif [[ \"$install_type\" =~ ^(EVAL|MANAGERSEARCH|STANDALONE|IMPORT)$ ]]; then\n\t\tNODE_ES_HEAP_SIZE=ES_HEAP_SIZE\n\t\texport NODE_ES_HEAP_SIZE\n\tfi\n}\n\n# Enable Zeek Logs\nzeek_logs_enabled() {\n\techo \"Enabling Zeek Logs\" >> \"$setup_log\" 2>&1\n\n\tlocal zeeklogs_pillar=$local_salt_dir/pillar/zeeklogs.sls\n\n\tprintf '%s\\n'\\\n\t\t\"zeeklogs:\"\\\n\t\t\"  enabled:\" > \"$zeeklogs_pillar\"\n\n\tif [ \"$MANAGERADV\" = 'ADVANCED' ]; then\n\t\tfor BLOG in \"${BLOGS[@]}\"; do\n\t\t\techo \"    - $BLOG\" | tr -d '\"' >> \"$zeeklogs_pillar\"\n\t\tdone\n\telif [ \"$install_type\" == \"EVAL\" ]  || [ \"$install_type\" == \"IMPORT\" ]; then\n\t\tprintf '%s\\n'\\\n\t\t\t\"    - conn\"\\\n\t\t\t\"    - dce_rpc\"\\\n\t\t\t\"    - dhcp\"\\\n\t\t\t\"    - dhcpv6\"\\\n\t\t\t\"    - dnp3\"\\\n\t\t\t\"    - dns\"\\\n\t\t\t\"    - dpd\"\\\n\t\t\t\"    - files\"\\\n\t\t\t\"    - ftp\"\\\n\t\t\t\"    - http\"\\\n\t\t\t\"    - intel\"\\\n\t\t\t\"    - irc\"\\\n\t\t\t\"    - kerberos\"\\\n\t\t\t\"    - modbus\"\\\n\t\t\t\"    - mqtt\"\\\n\t\t\t\"    - notice\"\\\n\t\t\t\"    - ntlm\"\\\n\t\t\t\"    - openvpn\"\\\n\t\t\t\"    - pe\"\\\n\t\t\t\"    - radius\"\\\n\t\t\t\"    - rfb\"\\\n\t\t\t\"    - rdp\"\\\n\t\t\t\"    - signatures\"\\\n\t\t\t\"    - sip\"\\\n\t\t\t\"    - smb_files\"\\\n\t\t\t\"    - smb_mapping\"\\\n\t\t\t\"    - smtp\"\\\n\t\t\t\"    - snmp\"\\\n\t\t\t\"    - software\"\\\n\t\t\t\"    - ssh\"\\\n\t\t\t\"    - ssl\"\\\n\t\t\t\"    - syslog\"\\\n\t\t\t\"    - telnet\"\\\n\t\t\t\"    - tunnel\"\\\n\t\t\t\"    - weird\"\\\n\t\t\t\"    - mysql\"\\\n\t\t\t\"    - socks\"\\\n\t\t\t\"    - x509\" >> \"$zeeklogs_pillar\"\n\t# Disable syslog log by default\n\telse\n\t\tprintf '%s\\n'\\\n\t\t\t\"    - conn\"\\\n\t\t\t\"    - dce_rpc\"\\\n\t\t\t\"    - dhcp\"\\\n\t\t\t\"    - dhcpv6\"\\\n\t\t\t\"    - dnp3\"\\\n\t\t\t\"    - dns\"\\\n\t\t\t\"    - dpd\"\\\n\t\t\t\"    - files\"\\\n\t\t\t\"    - ftp\"\\\n\t\t\t\"    - http\"\\\n\t\t\t\"    - intel\"\\\n\t\t\t\"    - irc\"\\\n\t\t\t\"    - kerberos\"\\\n\t\t\t\"    - modbus\"\\\n\t\t\t\"    - mqtt\"\\\n\t\t\t\"    - notice\"\\\n\t\t\t\"    - ntlm\"\\\n\t\t\t\"    - openvpn\"\\\n\t\t\t\"    - pe\"\\\n\t\t\t\"    - radius\"\\\n\t\t\t\"    - rfb\"\\\n\t\t\t\"    - rdp\"\\\n\t\t\t\"    - signatures\"\\\n\t\t\t\"    - sip\"\\\n\t\t\t\"    - smb_files\"\\\n\t\t\t\"    - smb_mapping\"\\\n\t\t\t\"    - smtp\"\\\n\t\t\t\"    - snmp\"\\\n\t\t\t\"    - software\"\\\n\t\t\t\"    - ssh\"\\\n\t\t\t\"    - ssl\"\\\n\t\t\t\"    - telnet\"\\\n\t\t\t\"    - tunnel\"\\\n\t\t\t\"    - weird\"\\\n\t\t\t\"    - mysql\"\\\n\t\t\t\"    - socks\"\\\n\t\t\t\"    - x509\" >> \"$zeeklogs_pillar\"\n\tfi\n}\n"], "fixing_code": ["#!/bin/bash\n\n# Copyright 2014,2015,2016,2017,2018,2019,2020 Security Onion Solutions, LLC\n\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU General Public License as published by\n#    the Free Software Foundation, either version 3 of the License, or\n#    (at your option) any later version.\n#\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU General Public License for more details.\n#\n#    You should have received a copy of the GNU General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\n. /usr/sbin/so-common\n\nUPDATE_DIR=/tmp/sogh/securityonion\nINSTALLEDVERSION=$(cat /etc/soversion)\nINSTALLEDSALTVERSION=$(salt --versions-report | grep Salt: | awk {'print $2'})\nDEFAULT_SALT_DIR=/opt/so/saltstack/default\nBATCHSIZE=5\nSOUP_LOG=/root/soup.log\n\nexec 3>&1 1>${SOUP_LOG} 2>&1\n\nadd_common() {\n  cp $UPDATE_DIR/salt/common/tools/sbin/so-common $DEFAULT_SALT_DIR/salt/common/tools/sbin/\n  cp $UPDATE_DIR/salt/common/tools/sbin/so-image-common $DEFAULT_SALT_DIR/salt/common/tools/sbin/\n  salt-call state.apply common queue=True\n  echo \"Run soup one more time\"\n  exit 0\n}\n\nairgap_mounted() {\n  # Let's see if the ISO is already mounted. \n  if [ -f /tmp/soagupdate/SecurityOnion/VERSION ]; then\n    echo \"The ISO is already mounted\"\n  else\n    echo \"\"\n    echo \"Looks like we need access to the upgrade content\"\n    echo \"\" \n    echo \"If you just copied the .iso file over you can specify the path.\"\n    echo \"If you burned the ISO to a disk the standard way you can specify the device.\"\n    echo \"Example: /home/user/securityonion-2.X.0.iso\"\n    echo \"Example: /dev/sdx1\"\n    echo \"\"\n    read -p 'Enter the location of the iso: ' ISOLOC\n    if [ -f $ISOLOC ]; then\n      # Mounting the ISO image\n      mkdir -p /tmp/soagupdate\n      mount -t iso9660 -o loop $ISOLOC /tmp/soagupdate\n      # Make sure mounting was successful\n      if [ ! -f /tmp/soagupdate/SecurityOnion/VERSION ]; then\n        echo \"Something went wrong trying to mount the ISO.\"\n        echo \"Ensure you verify the ISO that you downloaded.\"\n        exit 0\n      else\n        echo \"ISO has been mounted!\"\n      fi  \n    elif [ -f $ISOLOC/SecurityOnion/VERSION ]; then\n      ln -s $ISOLOC /tmp/soagupdate\n      echo \"Found the update content\"\n    else \n      mkdir -p /tmp/soagupdate\n      mount $ISOLOC /tmp/soagupdate\n      if [ ! -f /tmp/soagupdate/SecurityOnion/VERSION ]; then\n        echo \"Something went wrong trying to mount the device.\"\n        echo \"Ensure you verify the ISO that you downloaded.\"\n        exit 0\n      else\n        echo \"Device has been mounted!\"\n      fi        \n    fi\n  fi\n}\n\nairgap_update_dockers() {\n  if [ $is_airgap -eq 0 ]; then\n    # Let's copy the tarball\n    if [ ! -f $AGDOCKER/registry.tar ]; then\n      echo \"Unable to locate registry. Exiting\"\n      exit 1\n    else\n      echo \"Stopping the registry docker\"\n      docker stop so-dockerregistry\n      docker rm so-dockerregistry\n      echo \"Copying the new dockers over\"\n      tar xvf $AGDOCKER/registry.tar -C /nsm/docker-registry/docker\n      echo \"Add Registry back\"\n      docker load -i $AGDOCKER/registry_image.tar\n    fi\n  fi\n}\n\nupdate_registry() {\n  docker stop so-dockerregistry\n  docker rm so-dockerregistry\n  salt-call state.apply registry queue=True\n}\n\ncheck_airgap() {\n  # See if this is an airgap install\n  AIRGAP=$(cat /opt/so/saltstack/local/pillar/global.sls | grep airgap | awk '{print $2}')\n  if [[ \"$AIRGAP\" == \"True\" ]]; then\n      is_airgap=0\n      UPDATE_DIR=/tmp/soagupdate/SecurityOnion\n      AGDOCKER=/tmp/soagupdate/docker\n      AGREPO=/tmp/soagupdate/Packages\n  else \n      is_airgap=1\n  fi\n}\n\nclean_dockers() {\n  # Place Holder for cleaning up old docker images\n  echo \"Trying to clean up old dockers.\"\n  docker system prune -a -f\n\n}\n\nclone_to_tmp() {\n  # Clean old files\n  rm -rf /tmp/sogh\n  # Make a temp location for the files\n  mkdir -p /tmp/sogh\n  cd /tmp/sogh\n  SOUP_BRANCH=\"\"\n  if [ -n \"$BRANCH\" ]; then\n    SOUP_BRANCH=\"-b $BRANCH\"\n  fi\n  git clone $SOUP_BRANCH https://github.com/Security-Onion-Solutions/securityonion.git\n  cd /tmp\n  if [ ! -f $UPDATE_DIR/VERSION ]; then\n    echo \"Update was unable to pull from github. Please check your internet.\"\n    exit 0\n  fi\n}\n\ncopy_new_files() {\n  # Copy new files over to the salt dir\n  cd $UPDATE_DIR\n  rsync -a salt $DEFAULT_SALT_DIR/\n  rsync -a pillar $DEFAULT_SALT_DIR/\n  chown -R socore:socore $DEFAULT_SALT_DIR/\n  chmod 755 $DEFAULT_SALT_DIR/pillar/firewall/addfirewall.sh\n  cd /tmp\n}\n\nhighstate() {\n  # Run a highstate.\n  salt-call state.highstate -l info queue=True\n}\n\nmasterlock() {\n  echo \"Locking Salt Master\"\n  if [[ \"$INSTALLEDVERSION\" =~ rc.1 ]]; then\n    TOPFILE=/opt/so/saltstack/default/salt/top.sls\n    BACKUPTOPFILE=/opt/so/saltstack/default/salt/top.sls.backup\n    mv -v $TOPFILE $BACKUPTOPFILE\n    echo \"base:\" > $TOPFILE\n    echo \"  $MINIONID:\" >> $TOPFILE\n    echo \"    - ca\" >> $TOPFILE\n    echo \"    - ssl\" >> $TOPFILE\n    echo \"    - elasticsearch\" >> $TOPFILE\n  fi\n}\n\nmasterunlock() {\n  echo \"Unlocking Salt Master\"\n  if [[ \"$INSTALLEDVERSION\" =~ rc.1 ]]; then\n    mv -v $BACKUPTOPFILE $TOPFILE\n  fi\n}\n\nplaybook() {\n  echo \"Applying playbook settings\"\n  if [[ \"$INSTALLEDVERSION\" =~ rc.1 ]]; then\n    salt-call state.apply playbook.OLD_db_init\n    rm -f /opt/so/rules/elastalert/playbook/*.yaml\n    so-playbook-ruleupdate >> /root/soup_playbook_rule_update.log 2>&1 &\n  fi\n}\n\npillar_changes() {\n    # This function is to add any new pillar items if needed.\n    echo \"Checking to see if pillar changes are needed.\"\n    \n    [[ \"$INSTALLEDVERSION\" =~ rc.1 ]] && rc1_to_rc2\n    [[ \"$INSTALLEDVERSION\" =~ rc.2 ]] && rc2_to_rc3\n    [[ \"$INSTALLEDVERSION\" =~ rc.3 ]] && rc3_to_2.3.0\n\t[[ \"$INSTALLEDVERSION\" =~ 2.3.2 ]] && up_2.3.2_to_2.3.10\n\n}\n\nrc1_to_rc2() {\n\n  # Move the static file to global.sls\n  echo \"Migrating static.sls to global.sls\"\n  mv -v /opt/so/saltstack/local/pillar/static.sls /opt/so/saltstack/local/pillar/global.sls >> \"$SOUP_LOG\" 2>&1\n  sed -i '1c\\global:' /opt/so/saltstack/local/pillar/global.sls >> \"$SOUP_LOG\" 2>&1\n\n  # Moving baseurl from minion sls file to inside global.sls\n  local line=$(grep '^  url_base:' /opt/so/saltstack/local/pillar/minions/$MINIONID.sls)\n  sed -i '/^  url_base:/d' /opt/so/saltstack/local/pillar/minions/$MINIONID.sls;\n  sed -i \"/^global:/a \\\\$line\" /opt/so/saltstack/local/pillar/global.sls;\n\n  # Adding play values to the global.sls\n  local HIVEPLAYSECRET=$(tr -dc 'a-zA-Z0-9' < /dev/urandom  | fold -w 20 | head -n 1)\n  local CORTEXPLAYSECRET=$(tr -dc 'a-zA-Z0-9' < /dev/urandom  | fold -w 20 | head -n 1)\n  sed -i \"/^global:/a \\\\  hiveplaysecret: $HIVEPLAYSECRET\" /opt/so/saltstack/local/pillar/global.sls;\n  sed -i \"/^global:/a \\\\  cortexplaysecret: $CORTEXPLAYSECRET\" /opt/so/saltstack/local/pillar/global.sls;\n\n  # Move storage nodes to hostname for SSL\n  # Get a list we can use:\n  grep -A1 searchnode /opt/so/saltstack/local/pillar/data/nodestab.sls | grep -v '\\-\\-' | sed '$!N;s/\\n/ /' | awk '{print $1,$3}' | awk '/_searchnode:/{gsub(/\\_searchnode:/, \"_searchnode\"); print}' >/tmp/nodes.txt\n  # Remove the nodes from cluster settings\n  while read p; do\n  local NAME=$(echo $p | awk '{print $1}')\n  local IP=$(echo $p | awk '{print $2}')\n  echo \"Removing the old cross cluster config for $NAME\"\n  curl -XPUT -H 'Content-Type: application/json' http://localhost:9200/_cluster/settings -d '{\"persistent\":{\"cluster\":{\"remote\":{\"'$NAME'\":{\"skip_unavailable\":null,\"seeds\":null}}}}}'\n  done </tmp/nodes.txt\n  # Add the nodes back using hostname\n  while read p; do\n      local NAME=$(echo $p | awk '{print $1}')\n      local EHOSTNAME=$(echo $p | awk -F\"_\" '{print $1}')\n\t    local IP=$(echo $p | awk '{print $2}')\n      echo \"Adding the new cross cluster config for $NAME\"\n      curl -XPUT http://localhost:9200/_cluster/settings -H'Content-Type: application/json' -d '{\"persistent\": {\"search\": {\"remote\": {\"'$NAME'\": {\"skip_unavailable\": \"true\", \"seeds\": [\"'$EHOSTNAME':9300\"]}}}}}'\n  done </tmp/nodes.txt\n\n  INSTALLEDVERSION=rc.2\n\n}\n\nrc2_to_rc3() {\n\n  # move location of local.rules\n  cp /opt/so/saltstack/default/salt/idstools/localrules/local.rules /opt/so/saltstack/local/salt/idstools/local.rules\n  \n  if [ -f /opt/so/saltstack/local/salt/idstools/localrules/local.rules ]; then\n    cat /opt/so/saltstack/local/salt/idstools/localrules/local.rules >> /opt/so/saltstack/local/salt/idstools/local.rules\n  fi\n  rm -rf /opt/so/saltstack/local/salt/idstools/localrules\n  rm -rf /opt/so/saltstack/default/salt/idstools/localrules\n\n  # Rename mdengine to MDENGINE\n  sed -i \"s/  zeekversion/  mdengine/g\" /opt/so/saltstack/local/pillar/global.sls\n  # Enable Strelka Rules\n  sed -i \"/  rules:/c\\  rules: 1\" /opt/so/saltstack/local/pillar/global.sls\n\n  INSTALLEDVERSION=rc.3\n\n}\n\nrc3_to_2.3.0() {\n  # Fix Tab Complete\n  if [ ! -f /etc/profile.d/securityonion.sh ]; then\n    echo \"complete -cf sudo\" > /etc/profile.d/securityonion.sh\n  fi\n\n  {\n      echo \"redis_settings:\"\n      echo \"  redis_maxmemory: 827\"\n      echo \"playbook:\"\n      echo \"  api_key: de6639318502476f2fa5aa06f43f51fb389a3d7f\" \n  } >> /opt/so/saltstack/local/pillar/global.sls\n\n  sed -i 's/playbook:/playbook_db:/' /opt/so/saltstack/local/pillar/secrets.sls\n  {\n    echo \"playbook_admin: $(tr -dc 'a-zA-Z0-9' < /dev/urandom | fold -w 20 | head -n 1)\"\n    echo \"playbook_automation: $(tr -dc 'a-zA-Z0-9' < /dev/urandom | fold -w 20 | head -n 1)\"\n  } >> /opt/so/saltstack/local/pillar/secrets.sls\n}\n\nspace_check() {\n  # Check to see if there is enough space\n  CURRENTSPACE=$(df -BG / | grep -v Avail | awk '{print $4}' | sed 's/.$//')\n  if [ \"$CURRENTSPACE\" -lt \"10\" ]; then\n      echo \"You are low on disk space. Upgrade will try and clean up space.\";\n      clean_dockers\n  else\n      echo \"Plenty of space for upgrading\"\n  fi\n  \n}\n\nunmount_update() {\n  cd /tmp\n  umount /tmp/soagupdate\n}\n\nup_2.3.2_to_2.3.10() {\n\tif grep -q \"so-setup\" /etc/sudoers; then\n\t\techo \"[ INFO ] There is an entry for so-setup in the sudoers file, this can be safely deleted using \\\"visudo\\\".\"\n\tfi\n}\n\nupdate_centos_repo() {\n  # Update the files in the repo\n  echo \"Syncing new updates to /nsm/repo\"\n  rsync -av $AGREPO/* /nsm/repo/\n  echo \"Creating repo\"\n  createrepo /nsm/repo\n}\n\nupdate_version() {\n  # Update the version to the latest\n  echo \"Updating the Security Onion version file.\"\n  echo $NEWVERSION > /etc/soversion\n  sed -i \"/  soversion:/c\\  soversion: $NEWVERSION\" /opt/so/saltstack/local/pillar/global.sls\n}\n\nupgrade_check() {\n    # Let's make sure we actually need to update.\n    NEWVERSION=$(cat $UPDATE_DIR/VERSION)\n    if [ \"$INSTALLEDVERSION\" == \"$NEWVERSION\" ]; then\n      echo \"You are already running the latest version of Security Onion.\"\n      exit 0\n    fi \n}\n\nupgrade_check_salt() {\n    NEWSALTVERSION=$(grep version: $UPDATE_DIR/salt/salt/master.defaults.yaml | awk {'print $2'})\n    if [ \"$INSTALLEDSALTVERSION\" == \"$NEWSALTVERSION\" ]; then\n      echo \"You are already running the correct version of Salt for Security Onion.\"\n    else\n      UPGRADESALT=1\n    fi\n}   \nupgrade_salt() {\n      SALTUPGRADED=True\n      echo \"Performing upgrade of Salt from $INSTALLEDSALTVERSION to $NEWSALTVERSION.\"\n      echo \"\"\n      # If CentOS\n      if [ \"$OS\" == \"centos\" ]; then\n        echo \"Removing yum versionlock for Salt.\"\n        echo \"\"\n        yum versionlock delete \"salt-*\"\n        echo \"Updating Salt packages and restarting services.\"\n        echo \"\"\n\tif [ $is_airgap -eq 0 ]; then\n          sh $UPDATE_DIR/salt/salt/scripts/bootstrap-salt.sh -r -F -M -x python3 stable \"$NEWSALTVERSION\"\n\telse \n          sh $UPDATE_DIR/salt/salt/scripts/bootstrap-salt.sh -F -M -x python3 stable \"$NEWSALTVERSION\"\n\tfi\n        echo \"Applying yum versionlock for Salt.\"\n        echo \"\"\n        yum versionlock add \"salt-*\"\n      # Else do Ubuntu things\n      elif [ \"$OS\" == \"ubuntu\" ]; then\n        echo \"Removing apt hold for Salt.\"\n        echo \"\"\n        apt-mark unhold \"salt-common\"\n        apt-mark unhold \"salt-master\"\n        apt-mark unhold \"salt-minion\"\n        echo \"Updating Salt packages and restarting services.\"\n        echo \"\"\n        sh $UPDATE_DIR/salt/salt/scripts/bootstrap-salt.sh -F -M -x python3 stable \"$NEWSALTVERSION\"\n        echo \"Applying apt hold for Salt.\"\n        echo \"\"\n        apt-mark hold \"salt-common\"\n        apt-mark hold \"salt-master\"\n        apt-mark hold \"salt-minion\"\n      fi\n}\n\nverify_latest_update_script() {\n    # Check to see if the update scripts match. If not run the new one.\n    CURRENTSOUP=$(md5sum /opt/so/saltstack/default/salt/common/tools/sbin/soup | awk '{print $1}')\n    GITSOUP=$(md5sum $UPDATE_DIR/salt/common/tools/sbin/soup | awk '{print $1}')\n    if [[ \"$CURRENTSOUP\" == \"$GITSOUP\" ]]; then\n      echo \"This version of the soup script is up to date. Proceeding.\"\n    else\n      echo \"You are not running the latest soup version. Updating soup.\"\n      cp $UPDATE_DIR/salt/common/tools/sbin/soup $DEFAULT_SALT_DIR/salt/common/tools/sbin/\n      salt-call state.apply common queue=True\n      echo \"\"\n      echo \"soup has been updated. Please run soup again.\"\n      exit 0\n    fi\n}\n\nmain () {\nwhile getopts \":b\" opt; do\n  case \"$opt\" in\n    b ) # process option b\n       shift\n       BATCHSIZE=$1\n       if ! [[ \"$BATCHSIZE\" =~ ^[0-9]+$ ]]; then\n         echo \"Batch size must be a number greater than 0.\"\n         exit 1\n       fi\n      ;;\n    \\? ) echo \"Usage: cmd [-b]\"\n      ;;\n  esac\ndone\n\necho \"Checking to see if this is a manager.\"\necho \"\"\nrequire_manager\nset_minionid\necho \"Checking to see if this is an airgap install\"\necho \"\"\ncheck_airgap\necho \"Found that Security Onion $INSTALLEDVERSION is currently installed.\"\necho \"\"\nset_os\necho \"\"\nif [ $is_airgap -eq 0 ]; then\n  # Let's mount the ISO since this is airgap\n  airgap_mounted\nelse\n  echo \"Cloning Security Onion github repo into $UPDATE_DIR.\"\n  clone_to_tmp\nfi\nif [ -f /usr/sbin/so-image-common ]; then\n  . /usr/sbin/so-image-common\nelse \nadd_common\nfi\n\necho \"\"\necho \"Verifying we have the latest soup script.\"\nverify_latest_update_script\necho \"\"\n\necho \"Let's see if we need to update Security Onion.\"\nupgrade_check\nspace_check\n\necho \"Checking for Salt Master and Minion updates.\"\nupgrade_check_salt\n\necho \"\"\necho \"Performing upgrade from Security Onion $INSTALLEDVERSION to Security Onion $NEWVERSION.\"\necho \"\"\necho \"Updating dockers to $NEWVERSION.\"\nif [ $is_airgap -eq 0 ]; then\n  airgap_update_dockers\nelse\n  update_registry\n  update_docker_containers \"soup\"\nfi\necho \"\"\necho \"Stopping Salt Minion service.\"\nsystemctl stop salt-minion\necho \"\"\necho \"Stopping Salt Master service.\"\nsystemctl stop salt-master\necho \"\"\n\n# Does salt need upgraded. If so update it.\nif [ \"$UPGRADESALT\" == \"1\" ]; then\n  echo \"Upgrading Salt\"\n  # Update the repo files so it can actually upgrade\n  if [ $is_airgap -eq 0 ]; then\n    update_centos_repo\n    yum clean all\n  fi\n  upgrade_salt\nfi\n\necho \"Checking if Salt was upgraded.\"\necho \"\"\n# Check that Salt was upgraded, should be 3 'salt' packages on a manager node. salt-minion, salt-master and salt or salt-common depending on Ubuntu or CentOS. we could add salt-syndic in the future so checking that there are at least 3 packages\nif [[ `rpm -qa | grep salt | grep $NEWSALTVERSION | wc -l` < 3 ]]; then\n  echo \"Salt upgrade failed. Check of indicators of failure in $SOUP_LOG.\"\n  echo \"Once the issue is resolved, run soup again.\"\n  echo \"Exiting.\"\n  echo \"\"\n  exit 1\nelse\n  echo \"Salt upgrade success.\"\n  echo \"\"\nfi\n\necho \"Making pillar changes.\"\npillar_changes\necho \"\"\n\n# Only update the repo if its airgap\nif [[ $is_airgap -eq 0 ]] && [[ \"$UPGRADESALT\" != \"1\" ]]; then\nupdate_centos_repo\nfi\n\necho \"\"\necho \"Copying new Security Onion code from $UPDATE_DIR to $DEFAULT_SALT_DIR.\"\ncopy_new_files\necho \"\"\nupdate_version\n\necho \"\"\necho \"Locking down Salt Master for upgrade\"\nmasterlock\n\necho \"\"\necho \"Starting Salt Master service.\"\nsystemctl start salt-master\n\necho \"\"\necho \"Running a highstate to complete the Security Onion upgrade on this manager. This could take several minutes.\"\nhighstate\necho \"\"\necho \"Upgrade from $INSTALLEDVERSION to $NEWVERSION complete.\"\n\necho \"\"\necho \"Stopping Salt Master to remove ACL\"\nsystemctl stop salt-master\n\nmasterunlock\n\necho \"\"\necho \"Starting Salt Master service.\"\nsystemctl start salt-master\nhighstate\nplaybook\nunmount_update\n\nif [ \"$UPGRADESALT\" == \"1\" ]; then\n  echo \"\"\n  echo \"Upgrading Salt on the remaining Security Onion nodes from $INSTALLEDSALTVERSION to $NEWSALTVERSION.\"\n  if [ $is_airgap -eq 0 ]; then\n    salt -C 'not *_eval and not *_helix and not *_manager and not *_managersearch and not *_standalone' cmd.run \"yum clean all\"\n  fi\n  salt -C 'not *_eval and not *_helix and not *_manager and not *_managersearch and not *_standalone' -b $BATCHSIZE state.apply salt.minion \n  echo \"\"\nfi\n\n}\n\nmain \"$@\" | tee /dev/fd/3\n", "#!/bin/bash\n\n# Copyright 2014,2015,2016,2017,2018,2019,2020 Security Onion Solutions, LLC\n\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU General Public License as published by\n#    the Free Software Foundation, either version 3 of the License, or\n#    (at your option) any later version.\n#\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU General Public License for more details.\n#\n#    You should have received a copy of the GNU General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\nsource ./so-whiptail\nsource ./so-variables\nsource ./so-common-functions\n\nCONTAINER_REGISTRY=quay.io\n\nSOVERSION=$(cat ../VERSION)\n\nlog() {\n\tmsg=$1\n\tlevel=${2:-I}\n\tnow=$(TZ=GMT date +\"%Y-%m-%dT%H:%M:%SZ\")\n\techo -e \"$now | $level | $msg\" >> \"$setup_log\" 2>&1\n}\n\nerror() {\n\tlog \"$1\" \"E\"\n}\n\ninfo() {\n\tlog \"$1\" \"I\"\n}\n\ntitle() {\n\techo -e \"\\n-----------------------------\\n $1\\n-----------------------------\\n\" >> \"$setup_log\" 2>&1\n}\n\nlogCmd() {\n\tcmd=$1\n\tinfo \"Executing command: $cmd\"\n\t$cmd >> \"$setup_log\" 2>&1\n}\n\nairgap_rules() {\n\t# Copy the rules for suricata if using Airgap\n\tmkdir -p /nsm/repo/rules\n\tcp -v /root/SecurityOnion/agrules/emerging-all.rules /nsm/repo/rules/\n\t\n\t# Copy over sigma rules\n\tcp -Rv /root/SecurityOnion/agrules/sigma /nsm/repo/rules/\n\n\t# Don't leave Strelka out\n\tcp -Rv /root/SecurityOnion/agrules/strelka /nsm/repo/rules/\n\n\n}\n\nanalyze_system() {\n\ttitle \"System Characteristics\"\n\tlogCmd \"uptime\"\n\tlogCmd \"uname -a\"\n\tlogCmd \"free -h\"\n\tlogCmd \"lscpu\"\n\tlogCmd \"df -h\"\n\tlogCmd \"ip a\"\n}\n\naccept_salt_key_remote() {\n\tsystemctl restart salt-minion\n\t\n\techo \"Accept the key remotely on the manager\" >> \"$setup_log\" 2>&1\n\t# Delete the key just in case.\n\tssh -i /root/.ssh/so.key soremote@\"$MSRV\" sudo salt-key -d \"$MINION_ID\" -y\n\tsalt-call state.apply ca >> /dev/null 2>&1\n\tssh -i /root/.ssh/so.key soremote@\"$MSRV\" sudo salt-key -a \"$MINION_ID\" -y\n\n}\n\n\nadd_admin_user() {\n\t# Add an admin user with full sudo rights if this is an ISO install. \n\t{\n\t\tuseradd \"$ADMINUSER\";\n\t\techo \"$ADMINUSER\":\"$ADMINPASS1\" | chpasswd --crypt-method=SHA512;\n\t\tusermod -aG wheel \"$ADMINUSER\";\n\t} >> \"$setup_log\" 2>&1\n\t\n}\n\nadd_manager_hostfile() {\n\n\t[ -n \"$TESTING\" ] && return\n\n\techo \"Checking if I can resolve manager. If not add to hosts file\" >> \"$setup_log\" 2>&1\n\t# Pop up an input to get the IP address\n\tMSRVIP=$(whiptail --title \"Security Onion Setup\" --inputbox \\\n\t\"Enter your Manager Server IP Address:\" 10 60 X.X.X.X 3>&1 1>&2 2>&3)\n\n\tlocal exitstatus=$?\n\twhiptail_check_exitstatus $exitstatus\n}\n\nadd_mngr_ip_to_hosts() {\n\techo \"$MSRVIP   $MSRV\" >> /etc/hosts\n}\n\naddtotab_generate_templates() {\n\n\tlocal addtotab_path=$local_salt_dir/pillar/data\n\n\tfor i in evaltab managersearchtab managertab nodestab sensorstab standalonetab; do\n\t\tprintf '%s\\n'\\\n\t\t\"$i:\"\\\n\t\t\"\" > \"$addtotab_path\"/$i.sls\n\t\techo \"Added $i Template\"\n\tdone\n\n}\n\n# $5 => (optional) password variable\nso_add_user() {\n\tlocal username=$1\n\tlocal uid=$2\n\tlocal gid=$3\n\tlocal home_dir=$4\n\tif [ \"$5\" ]; then local pass=$5; fi\n\n\techo \"Add $username user\" >> \"$setup_log\" 2>&1\n\tgroupadd --gid \"$gid\" \"$username\"\n\tuseradd -m --uid \"$uid\" --gid \"$gid\" --home-dir \"$home_dir\" \"$username\"\n\n\t# If a password has been passed in, set the password\n\tif [ \"$pass\" ]; then\n\t\techo \"$username\":\"$pass\" | chpasswd --crypt-method=SHA512\n\tfi\n}\n\nadd_socore_user_manager() {\n\tso_add_user \"socore\" \"939\" \"939\" \"/opt/so\" >> \"$setup_log\" 2>&1\n}\n\nadd_soremote_user_manager() {\n\tso_add_user \"soremote\" \"947\" \"947\" \"/home/soremote\" \"$SOREMOTEPASS1\" >> \"$setup_log\" 2>&1\n}\n\nwait_for_file() {\n\tlocal filename=$1\n\tlocal max_attempts=$2 # this is multiplied by the wait interval, so make sure it isn't too large\n\tlocal cur_attempts=0\n\tlocal wait_interval=$3\n\tlocal total_time=$(( max_attempts * wait_interval ))\n\tlocal date\n\tdate=$(date)\n\n\twhile [[ $cur_attempts -lt $max_attempts ]]; do\n\t\tif [ -f \"$filename\" ]; then\n\t\t\techo \"File $filename found at $date\" >> \"$setup_log\" 2>&1\n\t\t\treturn 0\n\t\telse\n\t\t\t((cur_attempts++))\n\t\t\techo \"File $filename does not exist; waiting ${wait_interval}s then checking again ($cur_attempts/$max_attempts)...\" >> \"$setup_log\" 2>&1\n\t\t\tsleep \"$wait_interval\"\n\t\tfi\n\tdone\n\techo \"Could not find $filename after waiting ${total_time}s\" >> \"$setup_log\" 2>&1\n\treturn 1\n}\n\nadd_web_user() {\n\twait_for_file /opt/so/conf/kratos/db/db.sqlite 30 5\n\t{\n\t\techo \"Attempting to add administrator user for web interface...\";\n\t\techo \"$WEBPASSWD1\" | /usr/sbin/so-user add \"$WEBUSER\";\n\t\techo \"Add user result: $?\";\n\t} >> \"/root/so-user-add.log\" 2>&1\n}\n\n# Create an secrets pillar so that passwords survive re-install\nsecrets_pillar(){\n  if [ ! -f $local_salt_dir/pillar/secrets.sls ]; then\n\techo \"Creating Secrets Pillar\" >> \"$setup_log\" 2>&1\n\tmkdir -p $local_salt_dir/pillar\n\tprintf '%s\\n'\\\n\t\t\"secrets:\"\\\n\t\t\"  mysql: $MYSQLPASS\"\\\n\t\t\"  playbook_db: $PLAYBOOKDBPASS\"\\\n\t\t\"  playbook_admin: $PLAYBOOKADMINPASS\"\\\n\t\t\"  playbook_automation: $PLAYBOOKAUTOMATIONPASS\"\\\n\t\t\"  grafana_admin: $GRAFANAPASS\"\\\n\t\t\"  fleet: $FLEETPASS\"\\\n\t\t\"  fleet_jwt: $FLEETJWT\"\\\n\t\t\"  fleet_enroll-secret: False\" > $local_salt_dir/pillar/secrets.sls\n  fi\n}\n\ncheck_admin_pass() {\n\tcheck_pass_match \"$ADMINPASS1\" \"$ADMINPASS2\" \"APMATCH\"\n}\n\ncheck_hive_init() {\n\n\twait_for_file /opt/so/state/thehive.txt 20 5\n\tlocal return_val=$?\n\tif [[ $return_val -ne 0 ]]; then\n\t\treturn $return_val\n\tfi\n\n\tdocker stop so-thehive\n\tdocker rm so-thehive\n}\n\ncheck_network_manager_conf() {\n\tlocal gmdconf=\"/usr/lib/NetworkManager/conf.d/10-globally-managed-devices.conf\"\n\tlocal nmconf=\"/etc/NetworkManager/NetworkManager.conf\"\n\tlocal preupdir=\"/etc/NetworkManager/dispatcher.d/pre-up.d\"\n\n\tif test -f \"$gmdconf\" && ! test -f \"${gmdconf}.bak\"; then\n\t\t{\n\t\t\tmv \"$gmdconf\" \"${gmdconf}.bak\"\n\t\t\ttouch \"$gmdconf\"\n\t\t\tsystemctl restart NetworkManager\n\t\t} >> \"$setup_log\" 2>&1\n\tfi\n\t\n\tif test -f \"$nmconf\"; then\n\t\tsed -i 's/managed=false/managed=true/g' \"$nmconf\" >> \"$setup_log\" 2>&1\n\t\tsystemctl restart NetworkManager >> \"$setup_log\" 2>&1\n\tfi\n\n\tif [[ ! -d \"$preupdir\" ]]; then\n\t\tmkdir \"$preupdir\" >> \"$setup_log\" 2>&1\n\tfi\n}\n\ncheck_pass_match() {\n\tlocal pass=$1\n\tlocal confirm_pass=$2\n\tlocal var=$3\n\n\tif [ \"$pass\" = \"$confirm_pass\" ]; then\n\t\texport \"$var=yes\"\n\telse\n\t\twhiptail_passwords_dont_match\n\tfi\n}\n\ncheck_service_status() {\n\n\tlocal service_name=$1\n\techo \"Checking service $service_name status\" >> \"$setup_log\" 2>&1\n\tsystemctl status $service_name > /dev/null 2>&1\n\tlocal status=$?\n\t#true if there is an issue with the service false if it is running properly\n\tif [ $status -gt 0 ]; then\n\t\techo \"$service_name is not running\" >> \"$setup_log\" 2>&1\n\t\techo 1;\n\telse\n\t\techo \"$service_name is running\" >> \"$setup_log\" 2>&1\n\t\techo 0;\n\tfi\n\n}\n\ncheck_salt_master_status() {\n\techo \"Checking if we can talk to the salt master\" >> \"$setup_log\" 2>&1\n\tsalt-call saltutil.kill_all_jobs > /dev/null 2>&1\n\tsalt-call state.show_top > /dev/null 2>&1\n\tlocal status=$?\n\t#true if there is an issue talking to salt master\n\tif [ $status -gt 0 ]; then\n\t\techo 1;\n\telse\n\t\techo \"Can talk to salt master\" >> \"$setup_log\" 2>&1\n\t\techo 0;\n\tfi\n\n}\n\ncheck_salt_minion_status() {\n\techo \"Checking if the salt minion will respond to jobs\" >> \"$setup_log\" 2>&1\n\tsalt \"$MINION_ID\" test.ping >> \"$setup_log\" 2>&1\n\tlocal status=$?\n\t#true if there is an issue getting a job response from the minion\n\tif [ $status -gt 0 ]; then\n\t\techo 1;\n\telse\n\t\techo \"Received job response from salt minion\" >> \"$setup_log\" 2>&1\n\t\techo 0;\n\tfi\n\n}\n\ncheck_soremote_pass() {\n\tcheck_pass_match \"$SOREMOTEPASS1\" \"$SOREMOTEPASS2\" \"SCMATCH\"\n}\n\ncheck_fleet_node_pass() {\n\tcheck_pass_match \"$FLEETNODEPASSWD1\" \"$FLEETNODEPASSWD2\" \"FPMATCH\"\n}\n\ncheck_web_pass() {\n\tcheck_pass_match \"$WEBPASSWD1\" \"$WEBPASSWD2\" \"WPMATCH\"\n}\n\nclear_manager() {\n\t# Clear out the old manager public key in case this is a re-install.\n\t# This only happens if you re-install the manager.\n\tif [ -f /etc/salt/pki/minion/minion_master.pub ]; then\n\t\t{\n\t\t\techo \"Clearing old Salt master key\";\n\t\t\trm -f /etc/salt/pki/minion/minion_master.pub;\n\t\t\tsystemctl -q restart salt-minion;\n\t\t} >> \"$setup_log\" 2>&1\n\tfi\n\n}\n\ncollect_soremote_inputs() {\n\twhiptail_create_soremote_user\n\tSCMATCH=no\n\twhile [[ $SCMATCH != yes ]]; do\n\t\twhiptail_create_soremote_user_password1\n\t\twhiptail_create_soremote_user_password2\n\t\tcheck_soremote_pass\n\tdone\n}\n\ncollect_adminuser_inputs() {\n\twhiptail_create_admin_user\n\tAPMATCH=no\n\twhile [[ $APMATCH != yes ]]; do\n\t\twhiptail_create_admin_user_password1\n\t\twhiptail_create_admin_user_password2\n\t\tcheck_admin_pass\n\tdone\n}\n\ncollect_fleet_custom_hostname_inputs() {\n\twhiptail_fleet_custom_hostname\n}\n\ncollect_fleetuser_inputs() {\n\t# Get a username & password for the Fleet admin user\n\tlocal valid_user=no\n\twhile [[ $valid_user != yes ]]; do\n\t\twhiptail_create_fleet_node_user\t\t\n\t\tif so-user valemail \"$FLEETNODEUSER\" >> \"$setup_log\" 2>&1; then\n\t\t\tvalid_user=yes\n\t\telse\n\t\t\twhiptail_invalid_user_warning\n\t\tfi\n\tdone\n\n\tFPMATCH=no\n\twhile [[ $FPMATCH != yes ]]; do\n\t\twhiptail_create_fleet_node_user_password1\n\t\twhile ! check_password \"$FLEETNODEPASSWD1\"; do\n\t\t\twhiptail_invalid_pass_characters_warning\n\t\t\twhiptail_create_fleet_node_user_password1\n\t\tdone\n\t\twhiptail_create_fleet_node_user_password2\n\t\tcheck_fleet_node_pass\n\tdone\n}\n\n\ncollect_webuser_inputs() {\n\t# Get a password for the web admin user\n\tlocal valid_user=no\n\twhile [[ $valid_user != yes ]]; do\n\t\twhiptail_create_web_user\t\t\n\t\tif so-user valemail \"$WEBUSER\" >> \"$setup_log\" 2>&1; then\n\t\t\tvalid_user=yes\n\t\telse\n\t\t\twhiptail_invalid_user_warning\n\t\tfi\n\tdone\n\n\tWPMATCH=no\n\twhile [[ $WPMATCH != yes ]]; do\n\t    whiptail_create_web_user_password1\n\t    while ! check_password \"$WEBPASSWD1\"; do\n\t\t\twhiptail_invalid_pass_characters_warning\n\t\t\twhiptail_create_web_user_password1\n\t\tdone\n\t\tif echo \"$WEBPASSWD1\" | so-user valpass >> \"$setup_log\" 2>&1; then\n\t\t\twhiptail_create_web_user_password2\n\t\t\tcheck_web_pass\n\t\telse\n\t\t\twhiptail_invalid_pass_warning\n\t\tfi\n\tdone\n}\n\nconfigure_minion() {\n\tlocal minion_type=$1\n\techo \"Configuring minion type as $minion_type\" >> \"$setup_log\" 2>&1\n\techo \"role: so-$minion_type\" > /etc/salt/grains\n\n\tlocal minion_config=/etc/salt/minion\n\n\techo \"id: '$MINION_ID'\" > \"$minion_config\"\n\n\tcase \"$minion_type\" in\n\t\t'helix')\n\t\t\techo \"master: '$HOSTNAME'\" >> \"$minion_config\"\n\t\t\t;;\n\t\t'manager' | 'eval' | 'managersearch' | 'standalone' | 'import')\n\t\t\tprintf '%s\\n'\\\n\t\t\t\t\"master: '$HOSTNAME'\"\\\n\t\t\t\t\"mysql.host: '$MAINIP'\"\\\n\t\t\t\t\"mysql.port: '3306'\"\\\n\t\t\t\t\"mysql.user: 'root'\" >> \"$minion_config\"\n\t\t\tif [ ! -f $local_salt_dir/pillar/secrets.sls ]; then\n\t\t\t\techo \"mysql.pass: '$MYSQLPASS'\" >> \"$minion_config\"\n\t\t\telse\n\t\t\t\tOLDPASS=$(grep \"mysql\" $local_salt_dir/pillar/secrets.sls | awk '{print $2}')\n\t\t\t\techo \"mysql.pass: '$OLDPASS'\" >> \"$minion_config\"\n\t\t\tfi\n\t\t\t;;\n\t\t*)\n\t\t\techo \"master: '$MSRV'\" >> \"$minion_config\"\n\t\t\t;;\n\tesac\n\n\tprintf '%s\\n'\\\n\t\t\"use_superseded:\"\\\n\t\t\"  - module.run\"\\\n\t\t\"log_file: /opt/so/log/salt/minion\" >> \"$minion_config\"\n\n\t{\n\t\tsystemctl restart salt-minion;\n\t} >> \"$setup_log\" 2>&1\n}\n\ncheckin_at_boot() {\n\tlocal minion_config=/etc/salt/minion\n\n\techo \"Enabling checkin at boot\" >> \"$setup_log\" 2>&1\n\techo \"startup_states: highstate\" >> \"$minion_config\"\n}\n\ncheck_requirements() {\n\tlocal standalone_or_dist=$1\n\tlocal node_type=$2 # optional\n\tlocal req_mem\n\tlocal req_cores\n\tlocal req_storage\n\tlocal nic_list\n\treadarray -t nic_list <<< \"$(ip link| awk -F: '$0 !~ \"lo|vir|veth|br|docker|wl|^[^0-9]\"{print $2}' | grep -vwe \"bond0\"  | sed 's/ //g')\"\n\tlocal num_nics=${#nic_list[@]}\n\t\n\tif [[ \"$standalone_or_dist\" == 'standalone' ]]; then\n\t\treq_mem=12\n\t\treq_cores=4\n\t\treq_nics=2\n\telif [[ \"$standalone_or_dist\" == 'dist' ]]; then\n\t\treq_mem=8\n\t\treq_cores=4\n\t\tif [[ \"$node_type\" == 'sensor' ]]; then req_nics=2; else req_nics=1; fi\n\t\tif [[ \"$node_type\" == 'fleet' ]]; then req_mem=4; fi\n\telif [[ \"$standalone_or_dist\" == 'import' ]]; then\n\t\treq_mem=4\n\t\treq_cores=2\n\t\treq_nics=1\n\tfi\n\n\tif [[ $setup_type == 'network' ]] ; then\n\t\tif [[ -n $nsm_mount ]]; then\n\t\t\tif [[ \"$standalone_or_dist\" == 'import' ]]; then\n\t\t\t\treq_storage=50\n\t\t\telse\n\t\t\t\treq_storage=100\n\t\t\tfi\n\t\t\tif (( $(echo \"$free_space_root < $req_storage\" | bc -l) )); then\n\t\t\t\twhiptail_storage_requirements \"/\" \"${free_space_root} GB\" \"${req_storage} GB\"\n\t\t\tfi\n\t\t\tif (( $(echo \"$free_space_nsm < $req_storage\" | bc -l) )); then\n\t\t\t\twhiptail_storage_requirements \"/nsm\" \"${free_space_nsm} GB\" \"${req_storage} GB\"\n\t\t\tfi\n\t\telse\n\t\t\tif [[ \"$standalone_or_dist\" == 'import' ]]; then\n\t\t\t\treq_storage=50\n\t\t\telse\n\t\t\t\treq_storage=200\n\t\t\tfi\n\t\t\tif (( $(echo \"$free_space_root < $req_storage\" | bc -l) )); then\n\t\t\t\twhiptail_storage_requirements \"/\" \"${free_space_root} GB\" \"${req_storage} GB\"\n\t\t\tfi\n\t\tfi\t\n\tfi\n\n\tif [[ $num_nics -lt $req_nics ]]; then\n\t\tif [[ $num_nics -eq 1 ]]; then\n\t\t\twhiptail_requirements_error \"NIC\" \"$num_nics\" \"$req_nics\"\n\t\telse\n\t\t\twhiptail_requirements_error \"NICs\" \"$num_nics\" \"$req_nics\"\n\t\tfi\n\tfi\n\n\tif [[ $num_cpu_cores -lt $req_cores ]]; then\n\t\tif [[ $num_cpu_cores -eq 1 ]]; then\n\t\t\twhiptail_requirements_error \"core\" \"$num_cpu_cores\" \"$req_cores\"\n\t\telse\n\t\t\twhiptail_requirements_error \"cores\" \"$num_cpu_cores\" \"$req_cores\"\n\t\tfi\n\t\t\n\tfi\n\t\n\tif [[ $total_mem_hr -lt $req_mem ]]; then\n\t\twhiptail_requirements_error \"memory\" \"${total_mem_hr} GB\" \"${req_mem} GB\"\n\tfi\n}\n\nconfigure_network_sensor() {\n\techo \"Setting up sensor interface\" >> \"$setup_log\" 2>&1\n\tlocal nic_error=0\n\n\t# Set the MTU\n\tif [[ $NSMSETUP != 'ADVANCED' ]]; then\n\t\tif [[ $is_cloud ]]; then MTU=1575; else MTU=1500; fi\n\tfi\n\n\tif [[ $is_cloud ]]; then \n\t\tINTERFACE=${BNICS[0]}\n\t\tlocal nmcli_con_arg=\"type ethernet\"\n\telse \n\t\tINTERFACE='bond0'\n\t\tlocal nmcli_con_arg=\"type bond mode 0\"\n\tfi\n\n\t# Create the bond interface only if it doesn't already exist\n\t\n\tnmcli -f name,uuid -p con | grep -q \"$INTERFACE\" >> \"$setup_log\" 2>&1\n\tlocal found_int=$?\n\n\tif [[ $found_int != 0 ]]; then\n\t\tnmcli con add ifname \"$INTERFACE\" con-name \"$INTERFACE\" $nmcli_con_arg -- \\\n\t\t\tipv4.method disabled \\\n\t\t\tipv6.method ignore \\\n\t\t\tethernet.mtu $MTU \\\n\t\t\tconnection.autoconnect \"yes\" >> \"$setup_log\" 2>&1\n\telse\n\t\tlocal int_uuid\n\t\tint_uuid=$(nmcli -f name,uuid -p con | sed -n \"s/$INTERFACE //p\" | tr -d ' ')\n\n\t\tnmcli con mod \"$int_uuid\" \\\n\t\t\tipv4.method disabled \\\n\t\t\tipv6.method ignore \\\n\t\t\tethernet.mtu $MTU \\\n\t\t\tconnection.autoconnect \"yes\" >> \"$setup_log\" 2>&1\n\tfi\n\n\tfor BNIC in \"${BNICS[@]}\"; do\n\t\t# Check if specific offload features are able to be disabled\n\t\tfor string in \"generic-segmentation-offload\" \"generic-receive-offload\" \"tcp-segmentation-offload\"; do\n\t\t\tif ethtool -k \"$BNIC\" | grep $string | grep -q \"on [fixed]\"; then\n\t\t\t\techo \"The hardware or driver for interface ${BNIC} is not supported, packet capture may not work as expected.\" >> \"$setup_log\" 2>&1\n\t\t\t\tnic_error=1\n\t\t\t\tbreak\n\t\t\tfi\n\t\tdone\n\n\t\t# Turn off various offloading settings for the interface\n\t\tfor i in rx tx sg tso ufo gso gro lro; do\n\t\t\tethtool -K \"$BNIC\" $i off >> \"$setup_log\" 2>&1\n\t\tdone\n\t\t\n\t\tif [[ $is_cloud ]]; then\n\t\t\tnmcli con up \"$BNIC\" >> \"$setup_log\" 2>&1\n\t\telse\n\t\t\t# Check if the bond slave connection has already been created\n\t\t\tnmcli -f name,uuid -p con | grep -q \"bond0-slave-$BNIC\" >> \"$setup_log\" 2>&1\n\t\t\tlocal found_int=$?\n\t\t\t\n\t\t\tif [[ $found_int != 0 ]]; then\n\t\t\t\t# Create the slave interface and assign it to the bond\n\t\t\t\tnmcli con add type ethernet ifname \"$BNIC\" con-name \"bond0-slave-$BNIC\" master bond0 -- \\\n\t\t\t\t\tethernet.mtu $MTU \\\n\t\t\t\t\tconnection.autoconnect \"yes\" >> \"$setup_log\" 2>&1\n\t\t\telse\n\t\t\t\tlocal int_uuid\n\t\t\t\tint_uuid=$(nmcli -f name,uuid -p con | sed -n \"s/bond0-slave-$BNIC //p\" | tr -d ' ')\n\n\t\t\t\tnmcli con mod \"$int_uuid\" \\\n\t\t\t\t\tethernet.mtu $MTU \\\n\t\t\t\t\tconnection.autoconnect \"yes\" >> \"$setup_log\" 2>&1\n\t\t\tfi\n\t\t\t\n\t\t\tnmcli con up \"bond0-slave-$BNIC\" >> \"$setup_log\" 2>&1 # Bring the slave interface up\n\t\tfi\n\tdone\n\n\tif [ $nic_error != 0 ]; then\n\t\treturn 1\n\tfi\n}\n\ncopy_salt_master_config() {\n\n\t# Copy the Salt master config template to the proper directory\n\tif [ \"$setup_type\" = 'iso' ]; then\n\t\tcp /root/SecurityOnion/files/salt/master/master /etc/salt/master >> \"$setup_log\" 2>&1\n\t\tcp /root/SecurityOnion/files/salt/master/salt-master.service /usr/lib/systemd/system/salt-master.service >> \"$setup_log\" 2>&1\n\telse\n\t\tcp ../files/salt/master/master /etc/salt/master >> \"$setup_log\" 2>&1\n\t\tcp ../files/salt/master/salt-master.service /usr/lib/systemd/system/salt-master.service >> \"$setup_log\" 2>&1\n\tfi\n\n\t# Restart the service so it picks up the changes\n\tsystemctl daemon-reload >> \"$setup_log\" 2>&1\n\tsystemctl restart salt-master >> \"$setup_log\" 2>&1\n}\n\ncopy_minion_tmp_files() {\n\tcase \"$install_type\" in\n\t\t'MANAGER' | 'EVAL' | 'HELIXSENSOR' | 'MANAGERSEARCH' | 'STANDALONE' | 'IMPORT')\n\t\t\techo \"Copying pillar and salt files in $temp_install_dir to $local_salt_dir\"\n\t\t\tcp -Rv \"$temp_install_dir\"/pillar/ $local_salt_dir/ >> \"$setup_log\" 2>&1\n\t\t\tif [ -d \"$temp_install_dir\"/salt ] ; then\n\t\t\t\tcp -Rv \"$temp_install_dir\"/salt/ $local_salt_dir/ >> \"$setup_log\" 2>&1\n\t\t\tfi\n\t\t\t;;\n\t\t*)\n\t\t\t{\n\t\t\t\techo \"scp pillar and salt files in $temp_install_dir to manager $local_salt_dir\";\n\t\t\t\tssh -i /root/.ssh/so.key soremote@\"$MSRV\" mkdir -p /tmp/\"$MINION_ID\"/pillar;\n\t\t\t\tssh -i /root/.ssh/so.key soremote@\"$MSRV\" mkdir -p /tmp/\"$MINION_ID\"/schedules;\n\t\t\t\tscp -prv -i /root/.ssh/so.key \"$temp_install_dir\"/pillar/minions/* soremote@\"$MSRV\":/tmp/\"$MINION_ID\"/pillar/;\n\t\t\t\tif [ -d $temp_install_dir/salt/patch/os/schedules/ ]; then\n\t\t\t\t\tif [ \"$(ls -A $temp_install_dir/salt/patch/os/schedules/)\" ]; then\n\t\t\t\t\t\tscp -prv -i /root/.ssh/so.key $temp_install_dir/salt/patch/os/schedules/* soremote@$MSRV:/tmp/$MINION_ID/schedules;\n\t\t\t\t\tfi\n\t\t\t\tfi\n\t\t\t\tssh -i /root/.ssh/so.key soremote@\"$MSRV\" sudo $default_salt_dir/salt/manager/files/add_minion.sh \"$MINION_ID\";\n\t\t\t} >> \"$setup_log\" 2>&1\n\t\t\t;;\n\tesac\n}\n\ncopy_ssh_key() {\n\n\techo \"Generating SSH key\"\n\t# Generate SSH key\n\tmkdir -p /root/.ssh\n\tssh-keygen -f /root/.ssh/so.key -t rsa -q -N \"\" < /dev/zero\n\tchown -R \"$SUDO_USER\":\"$SUDO_USER\" /root/.ssh\n\n\techo \"Removing old entry for manager from known_hosts if it exists\"\n\tsed -i \"/${MSRV}/d\" /root/.ssh/known_hosts\n\n\techo \"Copying the SSH key to the manager\"\n\t#Copy the key over to the manager\n\tssh-copy-id -f -i /root/.ssh/so.key soremote@\"$MSRV\"\n}\n\ncreate_local_directories() {\n\techo \"Creating local pillar and salt directories\"\n\tPILLARSALTDIR=${SCRIPTDIR::-5}\n\tfor i in \"pillar\" \"salt\"; do\n\t\tfor d in $(find $PILLARSALTDIR/$i -type d); do\n\t\t\tsuffixdir=${d//$PILLARSALTDIR/}\n\t\t\tif [ ! -d \"$local_salt_dir/$suffixdir\" ]; then\n\t\t\t\tmkdir -v \"$local_salt_dir$suffixdir\" >> \"$setup_log\" 2>&1\n\t\t\tfi\n\t\tdone\n\t\tchown -R socore:socore \"$local_salt_dir/$i\"\n\tdone\n\n}\n\ncreate_local_nids_rules() {\n\t# Create a local.rules file so it doesn't get blasted on updates\n\tmkdir -p /opt/so/saltstack/local/salt/idstools\n\techo \"# Custom Suricata rules go in this file\" > /opt/so/saltstack/local/salt/idstools/local.rules\n}\n\ncreate_repo() {\n\t# Create the repo for airgap\n\tcreaterepo /nsm/repo\n}\n\ndetect_cloud() {\n  echo \"Testing if setup is running on a cloud instance...\" >> \"$setup_log\" 2>&1\n  if ( curl --fail -s -m 5 http://169.254.169.254/latest/meta-data/instance-id > /dev/null ) || ( dmidecode -s bios-vendor | grep -q Google > /dev/null); then export is_cloud=\"true\"; fi\n}\n\ndetect_os() {\n\n\t# Detect Base OS\n\techo \"Detecting Base OS\" >> \"$setup_log\" 2>&1\n\tif [ -f /etc/redhat-release ]; then\n\t\tOS=centos\n\t\tif grep -q \"CentOS Linux release 7\" /etc/redhat-release; then\n\t\t\tOSVER=7\n\t\telif grep -q \"CentOS Linux release 8\" /etc/redhat-release; then\n\t\t\tOSVER=8\n\t\t\techo \"We currently do not support CentOS $OSVER but we are working on it!\"\n\t\t\texit 1\n\t\telse\n\t\t\techo \"We do not support the version of CentOS you are trying to use.\"\n\t\t\texit 1\n\t\tfi\n\n\t\techo \"Installing required packages to run installer...\" >> \"$setup_log\" 2>&1\n\t\t# Install bind-utils so the host command exists\n\t\tif [[ ! $is_iso ]]; then\n\t\t  if ! command -v host > /dev/null 2>&1; then\n\t\t\tyum -y install bind-utils >> \"$setup_log\" 2>&1\n\t\t  fi\n\t\t  if ! command -v nmcli > /dev/null 2>&1; then\n\t\t\t{\n\t\t\t\tyum -y install NetworkManager;\n\t\t\t\tsystemctl enable NetworkManager;\n\t\t\t\tsystemctl start NetworkManager;\n\t\t\t} >> \"$setup_log\" 2<&1\n\t\t  fi\n\t\t  if ! command -v bc > /dev/null 2>&1; then\n\t\t\tyum -y install bc >> \"$setup_log\" 2>&1\n\t\t  fi\n\t\t  if ! yum versionlock > /dev/null 2>&1; then\n\t\t\tyum -y install yum-plugin-versionlock >> \"$setup_log\" 2>&1\n\t\t  fi\n        else\n\t\t  logCmd \"systemctl enable NetworkManager\"\n\t\t  logCmd \"systemctl start NetworkManager\"\n        fi\n\telif [ -f /etc/os-release ]; then\n\t\tOS=ubuntu\n\t\tif grep -q \"UBUNTU_CODENAME=bionic\" /etc/os-release; then\n\t\t\tOSVER=bionic\n\t\telif grep -q \"UBUNTU_CODENAME=xenial\" /etc/os-release; then\n\t\t\tOSVER=xenial\n\t\telse\n\t\t\techo \"We do not support your current version of Ubuntu.\"\n\t\t\texit 1\n\t\tfi\n\n\t\techo \"Installing required packages to run installer...\"\n\t\t# Install network manager so we can do interface stuff\n\t\tif ! command -v nmcli > /dev/null 2>&1; then\n\t\t\t{\n\t\t\t\tapt-get install -y network-manager;\n\t\t\t\tsystemctl enable NetworkManager;\n\t\t\t\tsystemctl start NetworkManager;\n\t\t\t} >> \"$setup_log\" 2<&1\n\t\tfi\n\t\tapt-get install -y bc curl >> \"$setup_log\" 2>&1\n\n\telse\n\t\techo \"We were unable to determine if you are using a supported OS.\"\n\t\texit 1\n\tfi\n\n\techo \"Found OS: $OS $OSVER\" >> \"$setup_log\" 2>&1\n\n}\n\ndisable_auto_start() {\n\t\n\tif crontab -l -u $INSTALLUSERNAME 2>&1 | grep so-setup > /dev/null 2>&1; then\n\t\t# Remove the automated setup script from crontab, if it exists\n\t\tlogCmd \"crontab -u $INSTALLUSERNAME -r\"\n\tfi\n\n\tif grep so-setup /home/$INSTALLUSERNAME/.bash_profile > /dev/null 2>&1; then\n\t\t# Truncate last line of the bash profile\n\t\tinfo \"Removing auto-run of setup from bash profile\"\n\t\tsed -i '$ d' /home/$INSTALLUSERNAME/.bash_profile >> \"$setup_log\" 2>&1\n\tfi\n}\n\ndisable_ipv6() {\n\t{\n\t\tinfo \"Disabling ipv6\"\n\t\tsysctl -w net.ipv6.conf.all.disable_ipv6=1\n\t\tsysctl -w net.ipv6.conf.default.disable_ipv6=1\n\t}  >> \"$setup_log\" 2>&1\n}\n\ndisable_misc_network_features() {\n\tfilter_unused_nics\n\tif [ ${#filtered_nics[@]} -ne 0 ]; then\n\t\tfor unused_nic in \"${filtered_nics[@]}\"; do\n\t\t\tif [ -n \"$unused_nic\" ]; then\n\t\t\t\techo \"Disabling unused NIC: $unused_nic\" >> \"$setup_log\" 2>&1\n\n\t\t\t\t# Disable DHCPv4/v6 and autoconnect\n\t\t\t\tnmcli con mod \"$unused_nic\" \\\n\t\t\t\t\tipv4.method disabled \\\n\t\t\t\t\tipv6.method ignore \\\n\t\t\t\t\tconnection.autoconnect \"no\" >> \"$setup_log\" 2>&1\n\n\t\t\t\t# Flush any existing IPs\n\t\t\t\tip addr flush \"$unused_nic\" >> \"$setup_log\" 2>&1\n\t\t\tfi\n\t\tdone\n\tfi\n\t# Disable IPv6\n\t{ \n\t\techo \"net.ipv6.conf.all.disable_ipv6 = 1\"\n\t\techo \"net.ipv6.conf.default.disable_ipv6 = 1\"\n\t\techo \"net.ipv6.conf.lo.disable_ipv6 = 1\" \n\t} >> /etc/sysctl.conf\n}\n\ndocker_install() {\n\n\tif [ $OS = 'centos' ]; then\n\t\t{\n\t\t\tyum clean expire-cache;\n\t\t\tif [[ ! $is_airgap ]]; then\n\t\t\t  yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo;\n\t\t\tfi\n\t\t\tif [[ ! $is_iso ]]; then\n\t\t\t  yum -y install docker-ce-19.03.12-3.el7 containerd.io-1.2.13-3.2.el7;\n\t\t\tfi\n\t\t\tyum versionlock docker-ce-19.03.12-3.el7;\n\t\t\tyum versionlock containerd.io-1.2.13-3.2.el7\n\t\t} >> \"$setup_log\" 2>&1\n\t\t\n\telse\n\t\tcase \"$install_type\" in\n\t\t\t'MANAGER' | 'EVAL' | 'STANDALONE' | 'MANAGERSEARCH' | 'IMPORT')\n\t\t\t\tapt-get update >> \"$setup_log\" 2>&1\n\t\t\t\t;;\n\t\t\t*)\n\t\t\t\t{\n\t\t\t\t\tapt-key add \"$temp_install_dir\"/gpg/docker.pub;\n\t\t\t\t\tadd-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\";\n\t\t\t\t\tapt-get update;\n\t\t\t\t} >> \"$setup_log\" 2>&1\n\t\t\t\t;;\n\t\tesac \n\t\t\n\t\tif [ $OSVER != \"xenial\" ]; then\n\t\t\tapt-get -y install docker-ce python3-docker >> \"$setup_log\" 2>&1\n\t\telse\n\t\t\tapt-get -y install docker-ce python-docker >> \"$setup_log\" 2>&1\n\t\tfi\n\tfi\n\tdocker_registry\n\t{\n\t\techo \"Restarting Docker\";\n\t\tsystemctl restart docker;\n\t\tsystemctl enable docker;\n\t} >> \"$setup_log\" 2>&1\n}\n\ndocker_registry() {\n\n\techo \"Setting up Docker Registry\" >> \"$setup_log\" 2>&1\n\tmkdir -p /etc/docker >> \"$setup_log\" 2>&1\n\tif [ -z \"$DOCKERNET\" ]; then\n            DOCKERNET=172.17.0.0\n\tfi\n\t# Make the host use the manager docker registry\n\tDNETBIP=$(echo $DOCKERNET | awk -F'.' '{print $1,$2,$3,1}' OFS='.')/24\n\tif [ -n \"$TURBO\" ]; then local proxy=\"$TURBO\"; else local proxy=\"https://$MSRV\"; fi\n\tprintf '%s\\n'\\\n\t\t\"{\"\\\n\t\t\"  \\\"registry-mirrors\\\": [ \\\"$proxy:5000\\\" ],\"\\\n\t\t\"  \\\"bip\\\": \\\"$DNETBIP\\\",\"\\\n\t\t\"  \\\"default-address-pools\\\": [\"\\\n\t\t\"    {\"\\\n\t\t\"      \\\"base\\\" : \\\"$DOCKERNET\\\",\"\\\n\t\t\"      \\\"size\\\" : 24\"\\\n\t\t\"    }\"\\\n\t\t\"  ]\"\\\n\t\t\"}\" > /etc/docker/daemon.json\n\techo \"Docker Registry Setup - Complete\" >> \"$setup_log\" 2>&1\n\n}\n\ndocker_seed_update() {\n\tlocal name=$1\n\tlocal percent_delta=1\n\tif [ \"$install_type\" == 'HELIXSENSOR' ]; then \n\t\tpercent_delta=6\n\tfi\n\t((docker_seed_update_percent=docker_seed_update_percent+percent_delta))\n\n\tset_progress_str \"$docker_seed_update_percent\" \"Downloading $name\"\n}\n\ndocker_seed_registry() {\n\tlocal VERSION=\"$SOVERSION\"\n\n\tif ! [ -f /nsm/docker-registry/docker/registry.tar ]; then\n\t\tif [ \"$install_type\" == 'IMPORT' ]; then\n\t\t\tcontainer_list 'so-import'\n\t\telif [ \"$install_type\" == 'HELIXSENSOR' ]; then\n\t\t\tcontainer_list 'so-helix'\n\t\telse\n\t\t\tcontainer_list\n\t\tfi\n\n\t\tdocker_seed_update_percent=25\n\n\t\tupdate_docker_containers 'netinstall' '' 'docker_seed_update' \"$setup_log\"\n\telse\n\t\ttar xvf /nsm/docker-registry/docker/registry.tar -C /nsm/docker-registry/docker >> \"$setup_log\" 2>&1\n\t\trm /nsm/docker-registry/docker/registry.tar >> \"$setup_log\" 2>&1\n\tfi\n\n}\n\nfireeye_pillar() {\n\n\tlocal fireeye_pillar_path=$local_salt_dir/pillar/fireeye\n\tmkdir -p \"$fireeye_pillar_path\"\n\n\tprintf '%s\\n'\\\n\t\t\"fireeye:\"\\\n\t\t\"  helix:\"\\\n\t\t\"    api_key: '$HELIXAPIKEY'\" \n\t\t\"\" > \"$fireeye_pillar_path\"/init.sls\n\n}\n\n# Generate Firewall Templates\nfirewall_generate_templates() {\n\n\tlocal firewall_pillar_path=$local_salt_dir/salt/firewall\n\tmkdir -p \"$firewall_pillar_path\"\n   \n\tcp ../files/firewall/* /opt/so/saltstack/local/salt/firewall/ >> \"$setup_log\" 2>&1\n\n\tfor i in analyst beats_endpoint sensor manager minion osquery_endpoint search_node wazuh_endpoint; do\n\t\t$default_salt_dir/salt/common/tools/sbin/so-firewall includehost \"$i\" 127.0.0.1\n\tdone\n\n}\n\nfleet_pillar() {\n\n\tlocal pillar_file=\"$temp_install_dir\"/pillar/minions/\"$MINION_ID\".sls\n\n\t# Create the fleet pillar\n\tprintf '%s\\n'\\\n\t\t\"fleet:\"\\\n\t\t\"  mainip: '$MAINIP'\"\\\n\t\t\"  manager: '$MSRV'\"\\\n\t\t\"\" > \"$pillar_file\"\n}\n\ngenerate_passwords(){\n  # Generate Random Passwords for Things\n  MYSQLPASS=$(tr -dc 'a-zA-Z0-9' < /dev/urandom | fold -w 20 | head -n 1)\n  PLAYBOOKDBPASS=$(tr -dc 'a-zA-Z0-9' < /dev/urandom | fold -w 20 | head -n 1)\n  PLAYBOOKADMINPASS=$(tr -dc 'a-zA-Z0-9' < /dev/urandom | fold -w 20 | head -n 1)\n  PLAYBOOKAUTOMATIONPASS=$(tr -dc 'a-zA-Z0-9' < /dev/urandom | fold -w 20 | head -n 1)\n  FLEETPASS=$(tr -dc 'a-zA-Z0-9' < /dev/urandom  | fold -w 20 | head -n 1)\n  FLEETJWT=$(tr -dc 'a-zA-Z0-9' < /dev/urandom  | fold -w 20 | head -n 1)\n  GRAFANAPASS=$(tr -dc 'a-zA-Z0-9' < /dev/urandom  | fold -w 20 | head -n 1)\n  if [[ \"$THEHIVE\" == \"1\" ]]; then\n  \tHIVEKEY=$(tr -dc 'a-zA-Z0-9' < /dev/urandom  | fold -w 20 | head -n 1)\n  \tHIVEPLAYSECRET=$(tr -dc 'a-zA-Z0-9' < /dev/urandom  | fold -w 20 | head -n 1)\n  \tCORTEXKEY=$(tr -dc 'a-zA-Z0-9' < /dev/urandom  | fold -w 20 | head -n 1)\n  \tCORTEXORGUSERKEY=$(tr -dc 'a-zA-Z0-9' < /dev/urandom  | fold -w 20 | head -n 1)\n  \tCORTEXPLAYSECRET=$(tr -dc 'a-zA-Z0-9' < /dev/urandom  | fold -w 20 | head -n 1)\n  fi\n  SENSORONIKEY=$(tr -dc 'a-zA-Z0-9' < /dev/urandom  | fold -w 20 | head -n 1)\n  KRATOSKEY=$(tr -dc 'a-zA-Z0-9' < /dev/urandom  | fold -w 20 | head -n 1)\n}\n\nget_redirect() {\n\twhiptail_set_redirect\n\tif [ \"$REDIRECTINFO\" = \"OTHER\" ]; then\n\t\twhiptail_set_redirect_host\n\tfi\n}\n\nget_minion_type() {\n\tlocal minion_type\n\tcase \"$install_type\" in\n\t\t'EVAL' | 'MANAGERSEARCH' | 'MANAGER' | 'SENSOR' | 'HEAVYNODE' | 'FLEET' | 'STANDALONE' | 'IMPORT')\n\t\t\tminion_type=$(echo \"$install_type\" | tr '[:upper:]' '[:lower:]')\n\t\t\t;;\n\t\t'HELIXSENSOR')\n\t\t\tminion_type='helix'\n\t\t\t;;\n\t\t*'NODE')\n\t\t\tminion_type='node'\n\t\t\t;;\n\tesac\n\techo \"$minion_type\"\n}\n\nhost_pillar() {\n\n\tlocal pillar_file=\"$temp_install_dir\"/pillar/minions/\"$MINION_ID\".sls\n\n\t# Create the host pillar\n\tprintf '%s\\n'\\\n\t\t\"host:\"\\\n\t\t\"  mainint: '$MNIC'\"\\\n\t\t\"\" > \"$pillar_file\"\n}\n\ninstall_cleanup() {\n\techo \"Installer removing the following files:\"\n\tls -lR \"$temp_install_dir\"\n\n\t# Clean up after ourselves\n\trm -rf \"$temp_install_dir\"\n\t\n\t# All cleanup prior to this statement must be compatible with automated testing. Cleanup\n\t# that will disrupt automated tests should be placed beneath this statement.\n\t[ -n \"$TESTING\" ] && return\n\n\t# If Mysql is running stop it\n\t/usr/sbin/so-mysql-stop\n\n\tif [[ $install_type == 'iso' ]]; then\n\t\tinfo \"Removing so-setup permission entry from sudoers file\"\n\t\tsed -i '/so-setup/d' /etc/sudoers\n\tfi\n}\n\nimport_registry_docker() {\n\tif [ -f /nsm/docker-registry/docker/registry_image.tar ]; then\n\t  logCmd \"service docker start\"\n\t  logCmd \"docker load -i /nsm/docker-registry/docker/registry_image.tar\" \n\telse\n\t  info \"Need to download registry\"\n\tfi\n}\n\nmanager_pillar() {\n\n\tlocal pillar_file=$temp_install_dir/pillar/minions/$MINION_ID.sls\n\n\t# Create the manager pillar\n\tprintf '%s\\n'\\\n\t\t\"manager:\"\\\n\t\t\"  mainip: '$MAINIP'\"\\\n\t\t\"  mainint: '$MNIC'\"\\\n\t\t\"  esheap: '$ES_HEAP_SIZE'\"\\\n\t\t\"  esclustername: {{ grains.host }}\"\\\n\t\t\"  freq: 0\"\\\n\t\t\"  domainstats: 0\" >> \"$pillar_file\"\n\t\t\n\n\tif [ \"$install_type\" = 'EVAL' ] || [ \"$install_type\" = 'HELIXSENSOR' ] || [ \"$install_type\" = 'MANAGERSEARCH' ] || [ \"$install_type\" = 'STANDALONE' ]; then\n\t\tprintf '%s\\n'\\\n\t\t\t\"  mtu: $MTU\" >> \"$pillar_file\"\n\tfi\n\n\tprintf '%s\\n'\\\n\t\t\"  elastalert: 1\"\\\n\t\t\"  es_port: $node_es_port\"\\\n\t\t\"  log_size_limit: $log_size_limit\"\\\n\t\t\"  cur_close_days: $CURCLOSEDAYS\"\\\n\t\t\"  grafana: $GRAFANA\"\\\n\t\t\"  osquery: $OSQUERY\"\\\n\t\t\"  thehive: $THEHIVE\"\\\n\t\t\"  playbook: $PLAYBOOK\"\\\n\t\t\"\"\\\n\t\t\"elasticsearch:\"\\\n\t\t\"  mainip: '$MAINIP'\"\\\n\t\t\"  mainint: '$MNIC'\"\\\n\t\t\"  esheap: '$NODE_ES_HEAP_SIZE'\"\\\n\t\t\"  esclustername: {{ grains.host }}\"\\\n\t\t\"  node_type: '$NODETYPE'\"\\\n\t\t\"  es_port: $node_es_port\"\\\n\t\t\"  log_size_limit: $log_size_limit\"\\\n\t\t\"  node_route_type: 'hot'\"\\\n\t\t\"\"\\\n\t\t\"logstash_settings:\"\\\n\t\t\"  ls_pipeline_batch_size: 125\"\\\n\t\t\"  ls_input_threads: 1\"\\\n\t\t\"  lsheap: $LS_HEAP_SIZE\"\\\n\t\t\"  ls_pipeline_workers: $num_cpu_cores\"\\\n\t\t\"\"\\\n\t\t\"idstools:\"\\\n\t\t\"  config:\"\\\n\t\t\"    ruleset: '$RULESETUP'\"\\\n\t\t\"    oinkcode: '$OINKCODE'\"\\\n\t\t\"    urls:\"\\\n\t\t\"  sids:\"\\\n\t\t\"    enabled:\"\\\n\t\t\"    disabled:\"\\\n\t\t\"    modify:\"\\\n\t\t\"\"\\\n\t\t\"kratos:\" >> \"$pillar_file\"\n\t\t\n\n\tprintf '%s\\n'\\\n\t\t\"  kratoskey: '$KRATOSKEY'\"\\\n\t\t\"\" >> \"$pillar_file\"\n\n  }\n\nmanager_global() {\n\tlocal global_pillar=\"$local_salt_dir/pillar/global.sls\"\n\n\tif [ -z \"$SENSOR_CHECKIN_INTERVAL_MS\" ]; then\n\t\tSENSOR_CHECKIN_INTERVAL_MS=10000\n\t\tif [ \"$install_type\" = 'EVAL' ] || [ \"$install_type\" = 'STANDALONE' ] || [ \"$install_type\" = 'IMPORT' ]; then\n\t\t\tSENSOR_CHECKIN_INTERVAL_MS=1000\n\t\tfi\n\tfi\n\n\tif [ -z \"$DOCKERNET\" ]; then\n        DOCKERNET=172.17.0.0\n\tfi\n\n\t# Create a global file for global values\n        printf '%s\\n'\\\n                \"global:\"\\\n                \"  soversion: '$SOVERSION'\"\\\n                \"  hnmanager: '$HNMANAGER'\"\\\n                \"  ntpserver: '$NTPSERVER'\"\\\n\t\t\t\t\"  dockernet: '$DOCKERNET'\"\\\n                \"  proxy: '$PROXY'\"\\\n                \"  mdengine: '$ZEEKVERSION'\"\\\n                \"  ids: '$NIDS'\"\\\n\t\t\t\t\"  url_base: '$REDIRECTIT'\"\\\n                \"  managerip: '$MAINIP'\" > \"$global_pillar\"\n\t\t\n        if [[ $is_airgap ]]; then\n\t\t        printf '%s\\n'\\\n                        \"  airgap: True\"\\ >> \"$global_pillar\"\n\t\telse \n\t\t        printf '%s\\n'\\\n                        \"  airgap: False\"\\ >> \"$global_pillar\"\n\t\tfi\n\n        # Check if TheHive is enabled.  If so, add creds and other details\n        if [[ \"$THEHIVE\" == \"1\" ]]; then\n                printf '%s\\n'\\\n                        \"  hiveuser: '$WEBUSER'\"\\\n                        \"  hivepassword: '$WEBPASSWD1'\"\\\n                        \"  hivekey: '$HIVEKEY'\"\\\n                        \"  hiveplaysecret: '$HIVEPLAYSECRET'\"\\\n                        \"  cortexuser: '$WEBUSER'\"\\\n                        \"  cortexpassword: '$WEBPASSWD1'\"\\\n                        \"  cortexkey: '$CORTEXKEY'\"\\\n                        \"  cortexorgname: 'SecurityOnion'\"\\\n                        \"  cortexorguser: 'soadmin'\"\\\n                        \"  cortexorguserkey: '$CORTEXORGUSERKEY'\"\\\n                        \"  cortexplaysecret: '$CORTEXPLAYSECRET'\" >> \"$global_pillar\"\n        fi\n\n        # Continue adding other details\t\n\tprintf '%s\\n'\\\n\t\t\"  fleet_custom_hostname: \"\\\n\t\t\"  fleet_manager: False\"\\\n\t\t\"  fleet_node: False\"\\\n\t\t\"  fleet_packages-timestamp: 'N/A'\"\\\n\t\t\"  fleet_packages-version: 1\"\\\n\t\t\"  fleet_hostname: 'N/A'\"\\\n\t\t\"  fleet_ip: 'N/A'\"\\\n\t\t\"  sensoronikey: '$SENSORONIKEY'\"\\\n\t\t\"  wazuh: $WAZUH\"\\\n                \"  managerupdate: $MANAGERUPDATES\"\\\n\t\t\"  imagerepo: '$IMAGEREPO'\"\\\n\t\t\"  pipeline: 'redis'\"\\\n\t\t\"pcap:\"\\\n\t\t\"  sensor_checkin_interval_ms: $SENSOR_CHECKIN_INTERVAL_MS\"\\\n\t\t\"strelka:\"\\\n\t\t\"  enabled: $STRELKA\"\\\n\t\t\"  rules: 1\"\\\n\t\t\"curator:\"\\\n\t\t\"  hot_warm: False\"\\\n\t\t\"elastic:\"\\\n\t\t\"  features: False\"\\\n\t\t\"elasticsearch:\"\\\n\t\t\"  replicas: 0\"\\\n\t\t\"  true_cluster: False\"\\\n\t\t\"  true_cluster_name: 'so'\"\\\n\t\t\"  discovery_nodes: 1\"\\\n\t\t\"  hot_warm_enabled: False\"\\\n\t\t\"  cluster_routing_allocation_disk.threshold_enabled: true\"\\\n                \"  cluster_routing_allocation_disk_watermark_low: '95%'\"\\\n                \"  cluster_routing_allocation_disk_watermark_high: '98%'\"\\\n                \"  cluster_routing_allocation_disk_watermark_flood_stage: '98%'\"\\\n\t\t\"  index_settings:\"\\\n\t\t\"    so-beats:\"\\\n\t\t\"      shards: 1\"\\\n\t\t\"      warm: 7\"\\\n\t\t\"      close: 30\"\\\n\t\t\"      delete: 365\"\\\n\t\t\"    so-firewall:\"\\\n\t\t\"      shards: 1\"\\\n\t\t\"      warm: 7\"\\\n\t\t\"      close: 30\"\\\n\t\t\"      delete: 365\"\\\n\t\t\"    so-flow:\"\\\n\t\t\"      shards: 1\"\\\n\t\t\"      warm: 7\"\\\n\t\t\"      close: 30\"\\\n\t\t\"      delete: 365\"\\\n\t\t\"    so-ids:\"\\\n\t\t\"      shards: 1\"\\\n\t\t\"      warm: 7\"\\\n\t\t\"      close: 30\"\\\n\t\t\"      delete: 365\"\\\n\t\t\"    so-import:\"\\\n\t\t\"      shards: 1\"\\\n\t\t\"      warm: 7\"\\\n\t\t\"      close: 73000\"\\\n\t\t\"      delete: 73001\"\\\n\t\t\"    so-osquery:\"\\\n\t\t\"      shards: 1\"\\\n\t\t\"      warm: 7\"\\\n\t\t\"      close: 30\"\\\n\t\t\"      delete: 365\"\\\n\t\t\"    so-ossec:\"\\\n\t\t\"      shards: 1\"\\\n\t\t\"      warm: 7\"\\\n\t\t\"      close: 30\"\\\n\t\t\"      delete: 365\"\\\n\t\t\"    so-strelka:\"\\\n\t\t\"      shards: 1\"\\\n\t\t\"      warm: 7\"\\\n\t\t\"      close: 30\"\\\n\t\t\"      delete: 365\"\\\n\t\t\"    so-syslog:\"\\\n\t\t\"      shards: 1\"\\\n\t\t\"      warm: 7\"\\\n\t\t\"      close: 30\"\\\n\t\t\"      delete: 365\"\\\n\t\t\"    so-zeek:\"\\\n\t\t\"      shards: 5\"\\\n\t\t\"      warm: 7\"\\\n\t\t\"      close: 365\"\\\n\t\t\"      delete: 45\"\\\n\t\t\"minio:\"\\\n\t\t\"  access_key: '$ACCESS_KEY'\"\\\n\t\t\"  access_secret: '$ACCESS_SECRET'\"\\\n\t\t\"s3_settings:\"\\\n\t\t\"  size_file: 2048\"\\\n\t\t\"  time_file: 1\"\\\n\t\t\"  upload_queue_size: 4\"\\\n\t\t\"  encoding: 'gzip'\"\\\n\t\t\"  interval: 5\"\\\n\t        \"backup:\"\\\n                \"  locations:\"\\\n                \"    - /opt/so/saltstack/local\"\\\n\t        \"soctopus:\"\\\n\t\t\"  playbook:\"\\\n\t\t\"    rulesets:\"\\\n\t\t\"      - windows\"\\\n\t\t\"redis_settings:\"\\\n\t\t\"  redis_maxmemory: 812\" >> \"$global_pillar\"\n\n\t\t\n\tprintf '%s\\n'  '----' >> \"$setup_log\" 2>&1\n}\n\nminio_generate_keys() {\n\n\tlocal charSet=\"[:graph:]\"\n\n\tACCESS_KEY=$(tr -dc 'a-zA-Z0-9' < /dev/urandom | fold -w 20 | head -n 1)\n\tACCESS_SECRET=$(tr -dc 'a-zA-Z0-9' < /dev/urandom | fold -w 40 | head -n 1)\n\n}\n\nnetwork_setup() {\n\t{\n\t\techo \"Finishing up network setup\";\n\n\t\techo \"... Verifying all network devices are managed by Network Manager\";\n\t\tcheck_network_manager_conf;\n\n\t\techo \"... Disabling unused NICs\";\n\t\tdisable_misc_network_features;\n\n\t\techo \"... Setting ONBOOT for management interface\";\n\t\tif ! netplan > /dev/null 2>&1; then\n\t\t\tnmcli con mod \"$MNIC\" connection.autoconnect \"yes\";\n\t\tfi\n\n        echo \"... Copying 99-so-checksum-offload-disable\";\n        cp ./install_scripts/99-so-checksum-offload-disable /etc/NetworkManager/dispatcher.d/pre-up.d/99-so-checksum-offload-disable ;\n\n\t\techo \"... Modifying 99-so-checksum-offload-disable\";\n\t\tsed -i \"s/\\$MNIC/${MNIC}/g\" /etc/NetworkManager/dispatcher.d/pre-up.d/99-so-checksum-offload-disable;\n\t} >> \"$setup_log\" 2>&1\n}\n\nelasticsearch_pillar() {\n\n\tlocal pillar_file=$temp_install_dir/pillar/minions/$MINION_ID.sls\n\n\t# Create the node pillar\n\tprintf '%s\\n'\\\n\t\t\"elasticsearch:\"\\\n\t\t\"  mainip: '$MAINIP'\"\\\n\t\t\"  mainint: '$MNIC'\"\\\n\t\t\"  esheap: '$NODE_ES_HEAP_SIZE'\"\\\n\t\t\"  esclustername: {{ grains.host }}\"\\\n\t\t\"  node_type: '$NODETYPE'\"\\\n\t\t\"  es_port: $node_es_port\"\\\n\t\t\"  log_size_limit: $log_size_limit\"\\\n\t\t\"  node_route_type: 'hot'\"\\\n\t\t\"\" >> \"$pillar_file\"\n\t\n\tprintf '%s\\n'\\\n\t\t\"logstash_settings:\"\\\n\t\t\"  ls_pipeline_batch_size: $LSPIPELINEBATCH\"\\\n\t\t\"  ls_input_threads: $LSINPUTTHREADS\"\\\n\t\t\"  lsheap: $NODE_LS_HEAP_SIZE\"\\\n\t\t\"  ls_pipeline_workers: $num_cpu_cores\"\\\n\t\t\"\" >> \"$pillar_file\"\n\n}\n\nparse_install_username() {\n\t# parse out the install username so things copy correctly\n    INSTALLUSERNAME=$(pwd | sed -E 's/\\// /g'  | awk '{ print $2 }')\n}\n\npatch_pillar() {\n\n\tlocal pillar_file=$temp_install_dir/pillar/minions/$MINION_ID.sls\n\n\tprintf '%s\\n'\\\n\t\t\"patch:\"\\\n\t\t\"  os:\"\\\n\t\t\"    schedule_name: '$PATCHSCHEDULENAME'\"\\\n\t\t\"    enabled: True\"\\\n\t\t\"    splay: 300\"\\\n\t\t\"\" >> \"$pillar_file\"\n\n}\n\npatch_schedule_os_new() {\n\tlocal OSPATCHSCHEDULEDIR=\"$temp_install_dir/salt/patch/os/schedules\"\n\tlocal OSPATCHSCHEDULE=\"$OSPATCHSCHEDULEDIR/$PATCHSCHEDULENAME.yml\"\n\n\tmkdir -p $OSPATCHSCHEDULEDIR\n\n\tprintf '%s\\n'\\\n\t\t\"patch:\"\\\n\t\t\"  os:\"\\\n\t\t\"    schedule:\"> \"$OSPATCHSCHEDULE\"\n\tfor psd in \"${PATCHSCHEDULEDAYS[@]}\";do\n\t\tpsd=\"${psd//\\\"/}\"\n\t\techo \"      - $psd:\" >> \"$OSPATCHSCHEDULE\"\n\t\tfor psh in \"${PATCHSCHEDULEHOURS[@]}\"\n\t\tdo\n\t\t\tpsh=\"${psh//\\\"/}\"\n\t\t\techo \"        - '$psh'\" >> \"$OSPATCHSCHEDULE\"\n\t\tdone\n\tdone\n\n}\n\nprint_salt_state_apply() {\n\tlocal state=$1\n\n\techo \"Applying $state Salt state\"\n}\n\nreserve_group_ids() {\n\t# This is a hack to fix CentOS from taking group IDs that we need\n\tgroupadd -g 928 kratos\n\tgroupadd -g 930 elasticsearch\n\tgroupadd -g 931 logstash\n\tgroupadd -g 932 kibana\n\tgroupadd -g 933 elastalert\n\tgroupadd -g 934 curator\n\tgroupadd -g 937 zeek\n\tgroupadd -g 940 suricata\n\tgroupadd -g 941 stenographer\n\tgroupadd -g 945 ossec\n\tgroupadd -g 946 cyberchef\n}\n\nreinstall_init() {\n\tinfo \"Putting system in state to run setup again\"\n\n\t{\n\t\tlocal minion_config=/etc/salt/minion\n\t\t\n\t\t# Remove startup_states from minion config so we don't immediately highstate when salt starts back up\n\t\tif [[ -f $minion_config ]] && grep -q \"startup_states\" $minion_config; then\n\t\t\tsed -i '/startup_states/d' $minion_config\n\t\tfi\n\n\t\tif command -v salt-call &> /dev/null; then\n\t\t\t# Disable schedule so highstate doesn't start running during the install\n\t\t\tsalt-call -l info schedule.disable\n\n\t\t\t# Kill any currently running salt jobs, also to prevent issues with highstate.\n\t\t\tsalt-call -l info saltutil.kill_all_jobs\n\t\tfi\n\n\t\tif command -v docker &> /dev/null; then\n\t\t\t# Stop and remove all so-* containers so files can be changed with more safety\n\t\t\tdocker stop $(docker ps -a -q --filter \"name=so-\")\n\t\t\tdocker rm -f $(docker ps -a -q --filter \"name=so-\")\n\t\tfi\n\n\t\tlocal date_string\n\t\tdate_string=$(date +%s)\n\n\t\t# Backup /opt/so since we'll be rebuilding this directory during setup\n\t\tif [[ -d /opt/so ]]; then\n\t\t\tmv /opt/so \"/opt/so_old_${date_string}\"\n\t\tfi\n\n\t\t# Backup /nsm for the same reason\n\t\twhile IFS= read -r -d '' dir; do\n\t\t\tmv \"$dir\" \"${dir}_old_${date_string}\"\n\t\tdone < <(find /nsm -maxdepth 1 -mindepth 1 -type d -print0)\n\n\t\t# Remove the old launcher package in case the config changes\n\t\tremove_package launcher-final\n\n\t} >> $setup_log 2>&1\n}\n\nremove_package() {\n\tlocal package_name=$1\n\tif [ $OS = 'centos' ]; then\n\t\tif rpm -qa | grep -q \"$package_name\"; then\n\t\t\tyum remove -y \"$package_name\"\n\t\tfi\n\telse\n\t\tif dpkg -l | grep -q \"$package_name\"; then\n\t\t\tapt purge -y \"$package_name\"\n\t\tfi\n\tfi\n}\n\n# When updating the salt version, also update the version in securityonion-builds/images/iso-task/Dockerfile and salt/salt/master.defaults.yaml and salt/salt/minion.defaults.yaml\n# CAUTION! SALT VERSION UDDATES - READ BELOW\n# When updating the salt version, also update the version in:\n# - securityonion-builds/iso-resources/build.sh\n# - securityonion-builds/iso-resources/packages.lst\n# - securityonion/salt/salt/master.defaults.yaml\n# - securityonion/salt/salt/minion.defaults.yaml\nsaltify() {\n\n\t# Install updates and Salt\n\tif [ $OS = 'centos' ]; then\n\t\tset_progress_str 5 'Installing Salt repo'\n\t\t{\n\t\t\tsudo rpm --import https://repo.saltstack.com/py3/redhat/7/x86_64/archive/3002.1/SALTSTACK-GPG-KEY.pub;\n\t\t\tcp ./yum_repos/saltstack.repo /etc/yum.repos.d/saltstack.repo;\n\t\t} >> \"$setup_log\" 2>&1\n\t\tset_progress_str 6 'Installing various dependencies'\n\t\tif [[ ! $is_iso ]]; then\n\t\t  logCmd \"yum -y install wget nmap-ncat\"\n\t\tfi \n\t\tcase \"$install_type\" in\n\t\t\t'MANAGER' | 'EVAL' | 'MANAGERSEARCH' | 'FLEET' | 'HELIXSENSOR' | 'STANDALONE'| 'IMPORT')\n\t\t\t\treserve_group_ids >> \"$setup_log\" 2>&1\n\t\t\t\tif [[ ! $is_iso ]]; then\n\t\t\t\t  logCmd \"yum -y install epel-release\"\n\t\t\t\t  logCmd \"yum -y install sqlite argon2 curl mariadb-devel\"\n\t\t\t\tfi \n\t\t\t\t# Download Ubuntu Keys in case manager updates = 1\n\t\t\t\tmkdir -p /opt/so/gpg >> \"$setup_log\" 2>&1\n\t\t\t\tif [[ ! $is_airgap ]]; then\n\t\t\t\t  logCmd \"wget -q --inet4-only -O /opt/so/gpg/SALTSTACK-GPG-KEY.pub https://repo.saltstack.com/py3/ubuntu/18.04/amd64/archive/3002.1/SALTSTACK-GPG-KEY.pub\"\n\t\t\t\t  logCmd \"wget -q --inet4-only -O /opt/so/gpg/docker.pub https://download.docker.com/linux/ubuntu/gpg\"\n\t\t\t\t  logCmd \"wget -q --inet4-only -O /opt/so/gpg/GPG-KEY-WAZUH https://packages.wazuh.com/key/GPG-KEY-WAZUH\"\n\t\t\t\t  logCmd \"cp ./yum_repos/wazuh.repo /etc/yum.repos.d/wazuh.repo\"\n\t\t\t\tfi\n\t\t\t\tset_progress_str 7 'Installing salt-master'\n\t\t\t\tif [[ ! $is_iso ]]; then\n\t\t\t\t  logCmd \"yum -y install salt-master-3002.1\"\n\t\t\t\tfi\n\t\t\t\tsystemctl enable salt-master >> \"$setup_log\" 2>&1\n\t\t\t\t;;\n\t\t\t*)\n\t\t\t\tif [ \"$MANAGERUPDATES\" = '1' ]; then\n\t\t\t\t\t{\n\t\t\t\t\t\tif [[ ! $is_airgap ]]; then\n\t\t\t\t\t\t  # Create the GPG Public Key for the Salt Repo\n\t\t\t\t\t\t  cp ./public_keys/salt.pem /etc/pki/rpm-gpg/saltstack-signing-key;\n\n\t\t\t\t\t\t  # Copy repo files over\n\t\t\t\t\t\t  cp ./yum_repos/saltstack.repo /etc/yum.repos.d/saltstack.repo;\n\t\t\t\t\t\telse\n\t\t\t\t\t\t  info \"This is airgap\"\n\t\t\t\t\t\tfi\n\t\t\t\t\t} >> \"$setup_log\" 2>&1\n\t\t\t\tfi\n\t\t\t\t;;\n\t\tesac\n\t\tif [[ ! $is_airgap ]]; then\n\t\t  cp ./yum_repos/wazuh.repo /etc/yum.repos.d/wazuh.repo >> \"$setup_log\" 2>&1\n\t\t  yum clean expire-cache >> \"$setup_log\" 2>&1\n\t\tfi\n\t\tset_progress_str 8 'Installing salt-minion & python modules'\n\t\t{\n\t\t\tif [[ ! $is_iso ]]; then\n\t\t\t  yum -y install epel-release\n\t\t\t  yum -y install salt-minion-3002.1\\\n\t\t\t   \tpython3\\\n\t\t\t\tpython36-docker\\\n\t\t\t\tpython36-dateutil\\\n\t\t\t\tpython36-m2crypto\\\n\t\t\t\tpython36-mysql\\\n\t\t\t\tyum-utils\\\n\t\t\t\tdevice-mapper-persistent-data\\\n\t\t\t\tlvm2\\\n\t\t\t\topenssl\\\n\t\t\t\tjq;\n\t\t\t  yum -y update --exclude=salt*;\n\t\t\tfi\n\t\t\tsystemctl enable salt-minion;\n\t\t} >> \"$setup_log\" 2>&1\n\t\tyum versionlock salt*\n\telse\n\t\tDEBIAN_FRONTEND=noninteractive apt-get -y -o Dpkg::Options::=\"--force-confdef\" -o Dpkg::Options::=\"--force-confold\" upgrade >> \"$setup_log\" 2>&1\n\n\t\tif [ $OSVER != \"xenial\" ]; then\n\t\t\t# Switch to Python 3 as default if this is not xenial\n\t\t\tupdate-alternatives --install /usr/bin/python python /usr/bin/python3.6 10 >> \"$setup_log\" 2>&1\n\t\tfi\n\t\t# Add the pre-requisites for installing docker-ce\n\t\tapt-get -y install ca-certificates\\\n\t\t\tcurl\\\n\t\t\tsoftware-properties-common\\\n\t\t\tapt-transport-https\\\n\t\t\topenssl\\\n\t\t\tnetcat\\\n\t\t\tjq >> \"$setup_log\" 2>&1\n\n\t\t# Grab the version from the os-release file\n\t\tlocal ubuntu_version\n\t\tubuntu_version=$(grep VERSION_ID /etc/os-release | awk -F '[ \"]' '{print $2}')\n\t\tif [ \"$OSVER\" != \"xenial\" ]; then local py_ver_url_path=\"/py3\"; else local py_ver_url_path=\"/apt\"; fi\n\n\t\tcase \"$install_type\" in\n\t\t\t'FLEET')\n\t\t\t\tif [ \"$OSVER\" != 'xenial' ]; then apt-get -y install python3-mysqldb >> \"$setup_log\" 2>&1; else apt-get -y install python-mysqldb >> \"$setup_log\" 2>&1; fi\n\t\t\t\t;;\n\t\t\t'MANAGER' | 'EVAL' | 'MANAGERSEARCH' | 'STANDALONE' | 'IMPORT') # TODO: should this also be HELIXSENSOR?\n\n\t\t\t\t# Add saltstack repo(s)\n\t\t\t\twget -q --inet4-only -O - https://repo.saltstack.com\"$py_ver_url_path\"/ubuntu/\"$ubuntu_version\"/amd64/archive/3002.1/SALTSTACK-GPG-KEY.pub | apt-key add - >> \"$setup_log\" 2>&1\n\t\t\t\techo \"deb http://repo.saltstack.com$py_ver_url_path/ubuntu/$ubuntu_version/amd64/archive/3002.1 $OSVER main\" > /etc/apt/sources.list.d/saltstack.list 2>> \"$setup_log\"\n\n\t\t\t\t# Add Docker repo\n\t\t\t\tcurl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add - >> \"$setup_log\" 2>&1\n\t\t\t\tadd-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" >> \"$setup_log\" 2>&1\n\n\t\t\t\t# Get gpg keys\n\t\t\t\tmkdir -p /opt/so/gpg >> \"$setup_log\" 2>&1\n\t\t\t\twget -q --inet4-only -O /opt/so/gpg/SALTSTACK-GPG-KEY.pub https://repo.saltstack.com$py_ver_url_path/ubuntu/\"$ubuntu_version\"/amd64/archive/3002.1/SALTSTACK-GPG-KEY.pub >> \"$setup_log\" 2>&1\n\t\t\t\twget -q --inet4-only -O /opt/so/gpg/docker.pub https://download.docker.com/linux/ubuntu/gpg >> \"$setup_log\" 2>&1\n\t\t\t\twget -q --inet4-only -O /opt/so/gpg/GPG-KEY-WAZUH https://packages.wazuh.com/key/GPG-KEY-WAZUH >> \"$setup_log\" 2>&1\n\n\t\t\t\t# Get key and install wazuh\n\t\t\t\tcurl -s https://packages.wazuh.com/key/GPG-KEY-WAZUH | apt-key add - >> \"$setup_log\" 2>&1\n\t\t\t\t# Add repo\n\t\t\t\techo \"deb https://packages.wazuh.com/3.x/apt/ stable main\" > /etc/apt/sources.list.d/wazuh.list 2>> \"$setup_log\"\n\t\t\t\t# Initialize the new repos\n\t\t\t\tapt-get update >> \"$setup_log\" 2>&1\n\t\t\t\tset_progress_str 6 'Installing various dependencies'\n\t\t\t\tapt-get -y install sqlite3 argon2 libssl-dev >> \"$setup_log\" 2>&1\n\t\t\t\tset_progress_str 7 'Installing salt-master'\n\t\t\t\tapt-get -y install salt-master=3002.1+ds-1 >> \"$setup_log\" 2>&1\n\t\t\t\tapt-mark hold salt-master >> \"$setup_log\" 2>&1\n\t\t\t\t;;\n\t\t\t*)\n\t\t\t\t# Copy down the gpg keys and install them from the manager\n\t\t\t\tmkdir \"$temp_install_dir\"/gpg >> \"$setup_log\" 2>&1\n\t\t\t\techo \"scp the gpg keys and install them from the manager\" >> \"$setup_log\" 2>&1\n\t\t\t\tscp -v -i /root/.ssh/so.key soremote@\"$MSRV\":/opt/so/gpg/* \"$temp_install_dir\"/gpg >> \"$setup_log\" 2>&1\n\t\t\t\techo \"Using apt-key add to add SALTSTACK-GPG-KEY.pub and GPG-KEY-WAZUH\" >> \"$setup_log\" 2>&1\n\t\t\t\tapt-key add \"$temp_install_dir\"/gpg/SALTSTACK-GPG-KEY.pub >> \"$setup_log\" 2>&1\n\t\t\t\tapt-key add \"$temp_install_dir\"/gpg/GPG-KEY-WAZUH >> \"$setup_log\" 2>&1\n\t\t\t\techo \"deb http://repo.saltstack.com$py_ver_url_path/ubuntu/$ubuntu_version/amd64/archive/3002.1/ $OSVER main\" > /etc/apt/sources.list.d/saltstack.list 2>> \"$setup_log\"\n\t\t\t\techo \"deb https://packages.wazuh.com/3.x/apt/ stable main\" > /etc/apt/sources.list.d/wazuh.list 2>> \"$setup_log\"\n                ;;\n\t\tesac\n\t\tapt-get update >> \"$setup_log\" 2>&1\n\t\tset_progress_str 8 'Installing salt-minion & python modules'\n\t\tapt-get -y install salt-minion=3002.1+ds-1\\\n\t\t\t\t\tsalt-common=3002.1+ds-1 >> \"$setup_log\" 2>&1\n\t\tapt-mark hold salt-minion salt-common >> \"$setup_log\" 2>&1\n\t\tif [ \"$OSVER\" != 'xenial' ]; then \n\t\t\tapt-get -y install python3-pip python3-dateutil python3-m2crypto python3-mysqldb >> \"$setup_log\" 2>&1\n\t\telse \n\t\t\tapt-get -y install python-pip python-dateutil python-m2crypto python-mysqldb  >> \"$setup_log\" 2>&1\n\t\tfi\n\tfi\n\n}\n\nsalt_checkin() {\n\tcase \"$install_type\" in\n\t\t'MANAGER' | 'EVAL' | 'HELIXSENSOR' | 'MANAGERSEARCH' | 'STANDALONE' | 'IMPORT') # Fix Mine usage\n\t\t\t{\n\t\t\t\techo \"Building Certificate Authority\";\n\t\t\t\tsalt-call state.apply ca;\n\t\t\t\techo \" *** Restarting Salt to fix any SSL errors. ***\";\n\n\t\t\t\tlocal SALT_SERVICES=(\\\n\t\t\t\t\"salt-master\" \\\n\t\t\t\t\"salt-minion\"\n\t\t\t\t)\n\t\t\t\tlocal LOOP_COUNT=0\n\t\t\t\tfor service in \"${SALT_SERVICES[@]}\"; do\n\t\t\t\t\techo \"Stopping service $service\" >> \"$setup_log\" 2>&1\n\t\t\t\t\tsystemctl stop \"$service\" >> \"$setup_log\" 2>&1\n\t\t\t\t\tLOOP_COUNT=0\n\t\t\t\t\twhile ! (( $(check_service_status $service) )); do\n\t\t\t\t\t\techo \"$service still running\" >> \"$setup_log\" 2>&1\n\t\t\t\t\t\tif [ $LOOP_COUNT -gt 60 ]; then\n\t\t\t\t\t\t\techo \"$service could not be stopped in 60 seconds, exiting\" >> \"$setup_log\" 2>&1\n\t\t\t\t\t\t\texit 1\n\t\t\t\t\t\tfi\n\t\t\t\t\t\tsleep 1;\n\t\t\t\t\t\t((LOOP_COUNT+=1))\n\t\t\t\t\tdone\n\t\t\t\tdone\n\n\t\t\t\tsleep 5;\n\n\t\t\t\tfor service in \"${SALT_SERVICES[@]}\"; do\n\t\t\t\t\techo \"Starting service $service\" >> \"$setup_log\" 2>&1\n\t\t\t\t\tsystemctl start \"$service\" >> \"$setup_log\" 2>&1\n\t\t\t\t\tLOOP_COUNT=0\n\t\t\t\t\twhile (( $(check_service_status $service) )); do\n\t\t\t\t\t\techo \"$service still not running\" >> \"$setup_log\" 2>&1\n\t\t\t\t\t\tif [ $LOOP_COUNT -gt 60 ]; then\n\t\t\t\t\t\t\techo \"$service could not be started in 60 seconds, exiting\" >> \"$setup_log\" 2>&1\n\t\t\t\t\t\t\texit 1\n\t\t\t\t\t\tfi\n\t\t\t\t\t\tsleep 1;\n\t\t\t\t\t\t((LOOP_COUNT+=1))\n\t\t\t\t\tdone\n\t\t\t\tdone\n\n\t\t\t\tsleep 5;\n\n\t\t\t\tLOOP_COUNT=0\n\t\t\t\twhile (( $(check_salt_master_status) )); do\n\t\t\t\t\techo \"salt minion cannot talk to salt master\" >> \"$setup_log\" 2>&1\n\t\t\t\t\tif [ $LOOP_COUNT -gt 30 ]; then\n\t\t\t\t\t\techo \"salt minion could not talk to salt master after 30 attempts, exiting\" >> \"$setup_log\" 2>&1\n\t\t\t\t\t\texit 1\n\t\t\t\t\tfi\n\t\t\t\t\tsleep 1;\n\t\t\t\t\t((LOOP_COUNT+=1))\n\t\t\t\tdone\n\n\t\t\t\tLOOP_COUNT=0\n\t\t\t\twhile (( $(check_salt_minion_status) )); do\n\t\t\t\t\techo \"salt master did not get a job response from salt minion\" >> \"$setup_log\" 2>&1\n\t\t\t\t\tif [ $LOOP_COUNT -gt 30 ]; then\n\t\t\t\t\t\techo \"salt master did not get a job response from salt minion after 30 attempts, exiting\" >> \"$setup_log\" 2>&1\n\t\t\t\t\t\texit 1\n\t\t\t\t\tfi\n\t\t\t\t\tsleep 1;\n\t\t\t\t\t((LOOP_COUNT+=1))\n\t\t\t\tdone\n\n\t\t\t\techo \" Confirming existence of the CA certificate\"\n\t\t\t\topenssl x509 -in /etc/pki/ca.crt -noout -subject -issuer -dates\n\t\t\t\techo \" Applyng a mine hack\";\n\t\t\t\tsalt \"$MINION_ID\" mine.send x509.get_pem_entries glob_path=/etc/pki/ca.crt;\n\t\t\t\tsalt \"$MINION_ID\" mine.update;\n\t\t\t\techo \"Confirming salt mine now contains the certificate\";\n\t\t\t\tsalt \"$MINION_ID\" mine.get '*' x509.get_pem_entries | grep -E 'BEGIN CERTIFICATE|END CERTIFICATE';\n\t\t\t\tif [ $? -eq 0 ]; then\n\t\t\t\t\techo \"CA in mine\"\n\t\t\t\telse\n\t\t\t\t\techo \"CA not in mine\"\n\t\t\t\tfi\n\t\t\t\techo \" Applying SSL state\";\n\t\t\t\tsalt-call state.apply ssl;\n\t\t\t} >> \"$setup_log\" 2>&1\n\t\t\t;;\n\t\t*)\n\t\t\t{\n\t\t\t\tsalt-call state.apply ca;\n\t\t\t\tsalt-call state.apply ssl;\n\t\t\t} >> \"$setup_log\" 2>&1\n\t\t\t;;\n\tesac\n\t{\n\t\tsalt-call state.apply ca;\n\t\tsalt-call state.apply ssl;\n\t\tsalt-call saltutil.sync_modules;\n\t} >> \"$setup_log\" 2>&1\n}\n\n# Run a salt command to generate the minion key\nsalt_firstcheckin() {\n\tsalt-call state.show_top >> /dev/null 2>&1 # send output to /dev/null because we don't actually care about the ouput\n}\n\nset_base_heapsizes() {\n\tes_heapsize\n\tls_heapsize\n}\n\nset_network_dev_status_list() {\n\treadarray -t nmcli_dev_status_list <<< \"$(nmcli -t -f DEVICE,STATE -c no dev status)\"\n\texport nmcli_dev_status_list\n}\n\nset_main_ip() {\n\tMAINIP=$(ip route get 1 | awk '{print $7;exit}')\n}\n\n# Add /usr/sbin to everyone's path\nset_path() {\n\techo \"complete -cf sudo\" > /etc/profile.d/securityonion.sh\n}\n\nsetup_salt_master_dirs() {\n\t# Create salt master directories\n\tmkdir -p $default_salt_dir/pillar\n\tmkdir -p $default_salt_dir/salt\n\tmkdir -p $local_salt_dir/pillar\n\tmkdir -p $local_salt_dir/salt\n\n\t# Copy over the salt code and templates\n\tif [ \"$setup_type\" = 'iso' ]; then\n\t\trsync -avh --exclude 'TRANS.TBL' /home/$INSTALLUSERNAME/SecurityOnion/pillar/* $default_salt_dir/pillar/ >> \"$setup_log\" 2>&1\n\t\trsync -avh --exclude 'TRANS.TBL' /home/$INSTALLUSERNAME/SecurityOnion/salt/* $default_salt_dir/salt/ >> \"$setup_log\" 2>&1\n\t\tmkdir -p $local_salt_dir/salt/zeek/policy/intel >> \"$setup_log\" 2>&1\n\t\tcp -Rv /home/$INSTALLUSERNAME/SecurityOnion/files/intel.dat $local_salt_dir/salt/zeek/policy/intel/ >> \"$setup_log\" 2>&1\n\telse\n\t\tcp -Rv ../pillar/* $default_salt_dir/pillar/ >> \"$setup_log\" 2>&1\n\t\tcp -Rv ../salt/* $default_salt_dir/salt/ >> \"$setup_log\" 2>&1\n\t\tmkdir -p $local_salt_dir/salt/zeek/policy/intel >> \"$setup_log\" 2>&1\n\t\tcp -Rv files/intel.dat $local_salt_dir/salt/zeek/policy/intel/ >> \"$setup_log\" 2>&1\n\tfi\n\n\techo \"Chown the salt dirs on the manager for socore\" >> \"$setup_log\" 2>&1\n\tchown -R socore:socore /opt/so\n}\n\nset_progress_str() {\n\tlocal percentage_input=$1\n\tlocal progress_bar_text=$2\n\n\tif (( \"$percentage_input\" >= \"$percentage\" )); then\n\t\tpercentage=\"$percentage_input\"\n\tfi\n\n\tpercentage_str=\"XXX\\n${percentage}\\n${progress_bar_text}\\nXXX\"\n\n\techo -e \"$percentage_str\"\n\n\tprintf '%s\\n' \\\n\t'----'\\\n\t\"$percentage% - ${progress_bar_text^^}\"\\\n\t\"----\" >> \"$setup_log\" 2>&1\n}\n\nsensor_pillar() {\n\n\tlocal pillar_file=$temp_install_dir/pillar/minions/$MINION_ID.sls\n\n\t# Create the sensor pillar\n\tprintf '%s\\n'\\\n\t\t\"sensor:\"\\\n\t\t\"  interface: '$INTERFACE'\"\\\n\t\t\"  mainip: '$MAINIP'\"\\\n\t\t\"  mainint: '$MNIC'\" >> \"$pillar_file\"\n\t\n\tif [ \"$NSMSETUP\" = 'ADVANCED' ]; then\n\t\techo \"  zeek_pins:\" >> \"$pillar_file\"\n\t\tfor PIN in \"${ZEEKPINS[@]}\"; do\n\t\t\tPIN=$(echo \"$PIN\" |  cut -d\\\" -f2)\n\t\techo \"    - $PIN\" >> \"$pillar_file\"\n\t\tdone\n\t\techo \"  suripins:\" >> \"$pillar_file\"\n\t\tfor SPIN in \"${SURIPINS[@]}\"; do\n\t\t\tSPIN=$(echo \"$SPIN\" |  cut -d\\\" -f2)\n\t\techo \"    - $SPIN\" >> \"$pillar_file\"\n\t\tdone\n\telif [ \"$install_type\" = 'HELIXSENSOR' ]; then\n\t\techo \"  zeek_lbprocs: $lb_procs\" >> \"$pillar_file\"\n\t\techo \"  suriprocs: $lb_procs\" >> \"$pillar_file\"\n\telse\n\t\techo \"  zeek_lbprocs: $BASICZEEK\" >> \"$pillar_file\"\n\t\techo \"  suriprocs: $BASICSURI\" >> \"$pillar_file\"\n\tfi\n\tprintf '%s\\n'\\\n\t\t\"  manager: '$MSRV'\"\\\n\t\t\"  mtu: $MTU\"\\\n\t\t\"  uniqueid: $(date '+%s')\" >> \"$pillar_file\"\n\tif [ \"$HNSENSOR\" != 'inherit' ]; then\n\t\techo \"  hnsensor: $HNSENSOR\" >> \"$pillar_file\"\n\tfi\n\n}\n\nset_default_log_size() {\n    local percentage\n\n\tcase $INSTALLTYPE in\n\t\tSTANDALONE | EVAL | HEAVYNODE)\n\t\t\tpercentage=50\n\t\t\t;;\n\t\t*)\n            percentage=80\n\t\t\t;;\n\t\tesac\n\n\tlocal disk_dir=\"/\"\n\tif [ -d /nsm ]; then\n\t\tdisk_dir=\"/nsm\"\n\tfi\n\tlocal disk_size_1k\n\tdisk_size_1k=$(df $disk_dir | grep -v \"^Filesystem\" | awk '{print $2}')\n\n    local ratio=\"1048576\"\n\n    local disk_size_gb\n    disk_size_gb=$( echo \"$disk_size_1k\" \"$ratio\" | awk '{print($1/$2)}' )\n\n\tlog_size_limit=$( echo \"$disk_size_gb\" \"$percentage\" | awk '{printf(\"%.0f\", $1 * ($2/100))}')\n}\n\nset_hostname() {\n\n\thostnamectl set-hostname --static \"$HOSTNAME\"\n\techo \"127.0.0.1   $HOSTNAME $HOSTNAME.localdomain localhost localhost.localdomain localhost4 localhost4.localdomain\" > /etc/hosts\n\techo \"::1   $HOSTNAME $HOSTNAME.localdomain localhost localhost.localdomain localhost6 localhost6.localdomain6\" >> /etc/hosts\n\techo \"$HOSTNAME\" > /etc/hostname\n\n\thostname -F /etc/hostname\n}\n\nset_initial_firewall_policy() {\n\n  set_main_ip\n\n  if [ -f $default_salt_dir/pillar/data/addtotab.sh ]; then chmod +x $default_salt_dir/pillar/data/addtotab.sh; fi\n  if [ -f $default_salt_dir/salt/common/tools/sbin/so-firewall ]; then chmod +x $default_salt_dir/salt/common/tools/sbin/so-firewall; fi\n\n\tcase \"$install_type\" in\n\t\t'MANAGER')\n\t\t\t$default_salt_dir/salt/common/tools/sbin/so-firewall includehost manager \"$MAINIP\"\n\t\t\t$default_salt_dir/salt/common/tools/sbin/so-firewall --apply includehost minion \"$MAINIP\"\n\t\t\t$default_salt_dir/pillar/data/addtotab.sh managertab \"$MINION_ID\" \"$MAINIP\" \"$num_cpu_cores\" \"$random_uid\" \"$MNIC\" \"$filesystem_root\" \"$filesystem_nsm\"\n\t\t\t;;\n\t\t'EVAL' | 'MANAGERSEARCH' | 'STANDALONE' | 'IMPORT')\n\t\t\t$default_salt_dir/salt/common/tools/sbin/so-firewall includehost manager \"$MAINIP\"\n\t\t\t$default_salt_dir/salt/common/tools/sbin/so-firewall includehost minion \"$MAINIP\"\n            $default_salt_dir/salt/common/tools/sbin/so-firewall includehost sensor \"$MAINIP\"\n            $default_salt_dir/salt/common/tools/sbin/so-firewall --apply includehost search_node \"$MAINIP\"\n\t\t\tcase \"$install_type\" in \n\t\t\t\t'EVAL')\n\t\t\t\t\t$default_salt_dir/pillar/data/addtotab.sh evaltab \"$MINION_ID\" \"$MAINIP\" \"$num_cpu_cores\" \"$random_uid\" \"$MNIC\" \"$filesystem_root\" \"$filesystem_nsm\" \"$INTERFACE\" True\n\t\t\t\t\t;;\n\t\t\t\t'MANAGERSEARCH')\n\t\t\t\t\t$default_salt_dir/pillar/data/addtotab.sh managersearchtab \"$MINION_ID\" \"$MAINIP\" \"$num_cpu_cores\" \"$random_uid\" \"$MNIC\" \"$filesystem_root\" \"$filesystem_nsm\"\n\t\t\t\t\t;;\n\t\t\t\t'STANDALONE')\n\t\t\t\t\t$default_salt_dir/pillar/data/addtotab.sh standalonetab \"$MINION_ID\" \"$MAINIP\" \"$num_cpu_cores\" \"$random_uid\" \"$MNIC\" \"$filesystem_root\" \"$filesystem_nsm\" \"$INTERFACE\"\n\t\t\t\t\t;;\n\t\t\tesac\n\t\t\t;;\n\t\t'HELIXSENSOR')\n\t\t\t$default_salt_dir/salt/common/tools/sbin/so-firewall includehost manager \"$MAINIP\"\n\t\t\t$default_salt_dir/salt/common/tools/sbin/so-firewall includehost minion \"$MAINIP\"\n            $default_salt_dir/salt/common/tools/sbin/so-firewall --apply includehost sensor \"$MAINIP\"\n\t\t\t;;\n\t\t'SENSOR' | 'SEARCHNODE' | 'HEAVYNODE' | 'FLEET')\n\t\t\tssh -i /root/.ssh/so.key soremote@\"$MSRV\" sudo $default_salt_dir/salt/common/tools/sbin/so-firewall includehost minion \"$MAINIP\"\n\t\t\tcase \"$install_type\" in\n\t\t\t\t'SENSOR')\n\t\t\t\t\tssh -i /root/.ssh/so.key soremote@\"$MSRV\" sudo $default_salt_dir/salt/common/tools/sbin/so-firewall --apply includehost sensor \"$MAINIP\"\n\t\t\t\t\tssh -i /root/.ssh/so.key soremote@\"$MSRV\" sudo $default_salt_dir/pillar/data/addtotab.sh sensorstab \"$MINION_ID\" \"$MAINIP\" \"$num_cpu_cores\" \"$random_uid\" \"$MNIC\" \"$filesystem_root\" \"$filesystem_nsm\" \"$INTERFACE\"\n\t\t\t\t\t;;\n\t\t\t\t'SEARCHNODE')\n\t\t\t\t\tssh -i /root/.ssh/so.key soremote@\"$MSRV\" sudo $default_salt_dir/salt/common/tools/sbin/so-firewall --apply includehost search_node \"$MAINIP\"\n\t\t\t\t\tssh -i /root/.ssh/so.key soremote@\"$MSRV\" sudo $default_salt_dir/pillar/data/addtotab.sh nodestab \"$MINION_ID\" \"$MAINIP\" \"$num_cpu_cores\" \"$random_uid\" \"$MNIC\" \"$filesystem_root\" \"$filesystem_nsm\"\n\t\t\t\t\t;;\n\t\t\t\t'HEAVYNODE')\n\t\t\t\t\tssh -i /root/.ssh/so.key soremote@\"$MSRV\" sudo $default_salt_dir/salt/common/tools/sbin/so-firewall includehost sensor \"$MAINIP\"\n\t\t\t\t\tssh -i /root/.ssh/so.key soremote@\"$MSRV\" sudo $default_salt_dir/salt/common/tools/sbin/so-firewall --apply includehost search_node \"$MAINIP\"\n\t\t\t\t\tssh -i /root/.ssh/so.key soremote@\"$MSRV\" sudo $default_salt_dir/pillar/data/addtotab.sh sensorstab \"$MINION_ID\" \"$MAINIP\" \"$num_cpu_cores\" \"$random_uid\" \"$MNIC\" \"$filesystem_root\" \"$filesystem_nsm\" \"$INTERFACE\"\n\t\t\t\t\tssh -i /root/.ssh/so.key soremote@\"$MSRV\" sudo $default_salt_dir/pillar/data/addtotab.sh nodestab \"$MINION_ID\" \"$MAINIP\" \"$num_cpu_cores\" \"$random_uid\" \"$MNIC\" \"$filesystem_root\" \"$filesystem_nsm\"\n\t\t\t\t\t;;\n\t\t\t\t'FLEET')\n\t\t\t\t    ssh -i /root/.ssh/so.key soremote@\"$MSRV\" sudo $default_salt_dir/salt/common/tools/sbin/so-firewall --apply includehost beats_endpoint_ssl \"$MAINIP\"\n\t\t\t\t\t;;\n\t\t\tesac\n\t\t\t;;\n\t\t'PARSINGNODE')\n\t\t\t# TODO: implement\n\t\t\t;;\n\t\t'HOTNODE')\n\t\t\t# TODO: implement\n\t\t\t;;\n\t\t'WARMNODE')\n\t\t\t# TODO: implement\n\t\t\t;;\n\tesac\n}\n\n# Set up the management interface on the ISO\nset_management_interface() {\n\n\tif [ \"$address_type\" = 'DHCP' ]; then\n\t\tnmcli con mod \"$MNIC\" connection.autoconnect yes >> \"$setup_log\" 2>&1\n\t\tnmcli con up \"$MNIC\" >> \"$setup_log\" 2>&1\n\telse\n\t\t# Set Static IP\n\t\tnmcli con mod \"$MNIC\" ipv4.addresses \"$MIP\"/\"$MMASK\"\\\n\t\t\tipv4.gateway \"$MGATEWAY\" \\\n\t\t\tipv4.dns \"$MDNS\"\\\n\t\t\tipv4.dns-search \"$MSEARCH\"\\\n\t\t\tconnection.autoconnect yes\\\n\t\t\tipv4.method manual >> \"$setup_log\" 2>&1\n\t\tnmcli con up \"$MNIC\" >> \"$setup_log\" 2>&1\n\tfi\n}\n\nset_node_type() {\n\n\tcase \"$install_type\" in\n\t\t'SEARCHNODE' | 'EVAL' | 'MANAGERSEARCH' | 'HEAVYNODE' | 'STANDALONE')\n\t\t\tNODETYPE='search'\n\t\t\t;;\n\t\t'HOTNODE')\n\t\t\tNODETYPE='hot'\n\t\t\t;;\n\t\t'WARMNODE')\n\t\t\tNODETYPE='warm'\n\t\t\t;;\n\tesac\n}\n\nset_redirect() {\n\tcase $REDIRECTINFO in\n\t\t'IP')\n\t\t\tREDIRECTIT=\"$MAINIP\"\n\t\t\t;;\n\t\t'HOSTNAME')\n\t\t\tREDIRECTIT=\"$HOSTNAME\"\n\t\t\t;;\n\t\t*)\n\t\t\tREDIRECTIT=\"$REDIRECTHOST\"\n\t\t\t;;\n\tesac\n}\n\nset_updates() {\n\tif [ \"$MANAGERUPDATES\" = '1' ]; then\n\t\tif [ \"$OS\" = 'centos' ]; then\n\t\t    if [[ ! $is_airgap ]]; then\n\t\t\t    if ! grep -q \"$MSRV\" /etc/yum.conf; then\n\t\t\t\t    echo \"proxy=http://$MSRV:3142\" >> /etc/yum.conf\n\t\t\t    fi\n\t\t\tfi\n\t\telse\n\t\t\t# Set it up so the updates roll through the manager\n\t\t\tprintf '%s\\n'\\\n\t\t\t\t\"Acquire::http::Proxy \\\"http://$MSRV:3142\\\";\"\\\n\t\t\t\t\"Acquire::https::Proxy \\\"http://$MSRV:3142\\\";\" > /etc/apt/apt.conf.d/00Proxy\n\t\tfi\n\tfi\n}\n\nmark_version() {\n\t# Drop a file with the current version\n\techo \"$SOVERSION\" > /etc/soversion\n}\n\nupdate_sudoers() {\n\n\tif ! grep -qE '^soremote\\ ALL=\\(ALL\\)\\ NOPASSWD:(\\/usr\\/bin\\/salt\\-key|\\/opt\\/so\\/saltstack)' /etc/sudoers; then\n\t\t# Update Sudoers so that soremote can accept keys without a password\n\t\techo \"soremote ALL=(ALL) NOPASSWD:/usr/bin/salt-key\" | tee -a /etc/sudoers\n\t\techo \"soremote ALL=(ALL) NOPASSWD:$default_salt_dir/salt/common/tools/sbin/so-firewall\" | tee -a /etc/sudoers\n\t\techo \"soremote ALL=(ALL) NOPASSWD:$default_salt_dir/pillar/data/addtotab.sh\" | tee -a /etc/sudoers\n\t\techo \"soremote ALL=(ALL) NOPASSWD:$default_salt_dir/salt/manager/files/add_minion.sh\" | tee -a /etc/sudoers\n\telse\n\t\techo \"User soremote already granted sudo privileges\" >> \"$setup_log\" 2>&1\n\tfi\n}\n\nupdate_packages() {\n\tif [ \"$OS\" = 'centos' ]; then\n\t\tyum -y update >> \"$setup_log\"\n\telse\n\t\tapt-get -y update >> \"$setup_log\"\n\t\tapt-get -y upgrade >> \"$setup_log\"\n\tfi\n}\n\n# This is used for development to speed up network install tests.\nuse_turbo_proxy() {\n\tif [[ ! $install_type =~ ^(MANAGER|EVAL|HELIXSENSOR|MANAGERSEARCH|STANDALONE)$ ]]; then\n\t\techo \"turbo is not supported on this install type\" >> $setup_log 2>&1\n\t\treturn\n\tfi\n\n\tif [[ $OS == 'centos' ]]; then\n\t\tprintf '%s\\n' \"proxy=${TURBO}:3142\" >> /etc/yum.conf\n\telse\n\t\tprintf '%s\\n'\\\n\t\t\t\"Acquire {\"\\\n\t\t\t\"  HTTP::proxy \\\"${TURBO}:3142\\\";\"\\\n\t\t\t\"  HTTPS::proxy \\\"${TURBO}:3142\\\";\"\\\n\t\t\t\"}\" > /etc/apt/apt.conf.d/proxy.conf\n\tfi\n}\n\n# Set Logstash heap size based on total memory\nls_heapsize() {\n\n\tif [ \"$total_mem\" -ge 32000 ]; then\n\t\tLS_HEAP_SIZE='1000m'\n\t\treturn\n\tfi\n\n\tcase \"$install_type\" in\n\t\t'MANAGERSEARCH' | 'HEAVYNODE' | 'HELIXSENSOR' | 'STANDALONE')\n\t\t\tLS_HEAP_SIZE='1000m'\n\t\t\t;;\n\t\t'EVAL')\n\t\t\tLS_HEAP_SIZE='700m'\n\t\t\t;;\n\t\t*)\n\t\t\tLS_HEAP_SIZE='500m'\n\t\t\t;;\n\tesac\n\texport LS_HEAP_SIZE\n\n\tif [[ \"$install_type\" =~ ^(EVAL|MANAGERSEARCH|STANDALONE)$ ]]; then\n\t\tNODE_LS_HEAP_SIZE=LS_HEAP_SIZE\n\t\texport NODE_LS_HEAP_SIZE\n\tfi\n}\n\n\nes_heapsize() {\n\n\t# Determine ES Heap Size\n\tif [ \"$total_mem\" -lt 8000 ] ; then\n\t\tES_HEAP_SIZE=\"600m\"\n\telif [ \"$total_mem\" -ge 100000 ]; then\n\t\t# Set a max of 25GB for heap size\n\t\t# https://www.elastic.co/guide/en/elasticsearch/guide/current/heap-sizing.html\n\t\tES_HEAP_SIZE=\"25000m\"\n\telse\n\t\t# Set heap size to 25% of available memory\n\t\tES_HEAP_SIZE=$(( total_mem / 4 ))\"m\"\n\tfi\n\texport ES_HEAP_SIZE\n\n\tif [[ \"$install_type\" =~ ^(EVAL|MANAGERSEARCH|STANDALONE|IMPORT)$ ]]; then\n\t\tNODE_ES_HEAP_SIZE=ES_HEAP_SIZE\n\t\texport NODE_ES_HEAP_SIZE\n\tfi\n}\n\n# Enable Zeek Logs\nzeek_logs_enabled() {\n\techo \"Enabling Zeek Logs\" >> \"$setup_log\" 2>&1\n\n\tlocal zeeklogs_pillar=$local_salt_dir/pillar/zeeklogs.sls\n\n\tprintf '%s\\n'\\\n\t\t\"zeeklogs:\"\\\n\t\t\"  enabled:\" > \"$zeeklogs_pillar\"\n\n\tif [ \"$MANAGERADV\" = 'ADVANCED' ]; then\n\t\tfor BLOG in \"${BLOGS[@]}\"; do\n\t\t\techo \"    - $BLOG\" | tr -d '\"' >> \"$zeeklogs_pillar\"\n\t\tdone\n\telif [ \"$install_type\" == \"EVAL\" ]  || [ \"$install_type\" == \"IMPORT\" ]; then\n\t\tprintf '%s\\n'\\\n\t\t\t\"    - conn\"\\\n\t\t\t\"    - dce_rpc\"\\\n\t\t\t\"    - dhcp\"\\\n\t\t\t\"    - dhcpv6\"\\\n\t\t\t\"    - dnp3\"\\\n\t\t\t\"    - dns\"\\\n\t\t\t\"    - dpd\"\\\n\t\t\t\"    - files\"\\\n\t\t\t\"    - ftp\"\\\n\t\t\t\"    - http\"\\\n\t\t\t\"    - intel\"\\\n\t\t\t\"    - irc\"\\\n\t\t\t\"    - kerberos\"\\\n\t\t\t\"    - modbus\"\\\n\t\t\t\"    - mqtt\"\\\n\t\t\t\"    - notice\"\\\n\t\t\t\"    - ntlm\"\\\n\t\t\t\"    - openvpn\"\\\n\t\t\t\"    - pe\"\\\n\t\t\t\"    - radius\"\\\n\t\t\t\"    - rfb\"\\\n\t\t\t\"    - rdp\"\\\n\t\t\t\"    - signatures\"\\\n\t\t\t\"    - sip\"\\\n\t\t\t\"    - smb_files\"\\\n\t\t\t\"    - smb_mapping\"\\\n\t\t\t\"    - smtp\"\\\n\t\t\t\"    - snmp\"\\\n\t\t\t\"    - software\"\\\n\t\t\t\"    - ssh\"\\\n\t\t\t\"    - ssl\"\\\n\t\t\t\"    - syslog\"\\\n\t\t\t\"    - telnet\"\\\n\t\t\t\"    - tunnel\"\\\n\t\t\t\"    - weird\"\\\n\t\t\t\"    - mysql\"\\\n\t\t\t\"    - socks\"\\\n\t\t\t\"    - x509\" >> \"$zeeklogs_pillar\"\n\t# Disable syslog log by default\n\telse\n\t\tprintf '%s\\n'\\\n\t\t\t\"    - conn\"\\\n\t\t\t\"    - dce_rpc\"\\\n\t\t\t\"    - dhcp\"\\\n\t\t\t\"    - dhcpv6\"\\\n\t\t\t\"    - dnp3\"\\\n\t\t\t\"    - dns\"\\\n\t\t\t\"    - dpd\"\\\n\t\t\t\"    - files\"\\\n\t\t\t\"    - ftp\"\\\n\t\t\t\"    - http\"\\\n\t\t\t\"    - intel\"\\\n\t\t\t\"    - irc\"\\\n\t\t\t\"    - kerberos\"\\\n\t\t\t\"    - modbus\"\\\n\t\t\t\"    - mqtt\"\\\n\t\t\t\"    - notice\"\\\n\t\t\t\"    - ntlm\"\\\n\t\t\t\"    - openvpn\"\\\n\t\t\t\"    - pe\"\\\n\t\t\t\"    - radius\"\\\n\t\t\t\"    - rfb\"\\\n\t\t\t\"    - rdp\"\\\n\t\t\t\"    - signatures\"\\\n\t\t\t\"    - sip\"\\\n\t\t\t\"    - smb_files\"\\\n\t\t\t\"    - smb_mapping\"\\\n\t\t\t\"    - smtp\"\\\n\t\t\t\"    - snmp\"\\\n\t\t\t\"    - software\"\\\n\t\t\t\"    - ssh\"\\\n\t\t\t\"    - ssl\"\\\n\t\t\t\"    - telnet\"\\\n\t\t\t\"    - tunnel\"\\\n\t\t\t\"    - weird\"\\\n\t\t\t\"    - mysql\"\\\n\t\t\t\"    - socks\"\\\n\t\t\t\"    - x509\" >> \"$zeeklogs_pillar\"\n\tfi\n}\n"], "buggy_code_start_loc": [193, 1021], "buggy_code_end_loc": [292, 1021], "fixing_code_start_loc": [194, 1022], "fixing_code_end_loc": [300, 1026], "type": "CWE-306", "message": "Security Onion v2 prior to 2.3.10 has an incorrect sudo configuration, which allows the administrative user to obtain root access without using the sudo password by editing and executing /home/<user>/SecurityOnion/setup/so-setup.", "other": {"cve": {"id": "CVE-2020-27985", "sourceIdentifier": "cve@mitre.org", "published": "2020-11-23T14:15:12.187", "lastModified": "2021-07-21T11:39:23.747", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "Security Onion v2 prior to 2.3.10 has an incorrect sudo configuration, which allows the administrative user to obtain root access without using the sudo password by editing and executing /home/<user>/SecurityOnion/setup/so-setup."}, {"lang": "es", "value": "Security Onion versiones v2 anteriores a 2.3.10, presenta una configuraci\u00f3n de sudo incorrecta, que permite al usuario administrador obtener acceso de root sin utilizar la contrase\u00f1a de sudo editando y ejecutando  /home/(user)/ SecurityOnion/setup/so-setup"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:C/I:C/A:C", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "COMPLETE", "integrityImpact": "COMPLETE", "availabilityImpact": "COMPLETE", "baseScore": 7.2}, "baseSeverity": "HIGH", "exploitabilityScore": 3.9, "impactScore": 10.0, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-306"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:securityonionsolutions:security_onion:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.0.0", "versionEndExcluding": "2.3.10", "matchCriteriaId": "8A201DB2-6351-4BBC-A5D5-4A447235DFC7"}]}]}], "references": [{"url": "https://github.com/Security-Onion-Solutions/securityonion/commit/b14670030349a2747a00ace665568ab5f51ac47b", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/Security-Onion-Solutions/securityonion/releases", "source": "cve@mitre.org", "tags": ["Release Notes", "Third Party Advisory"]}, {"url": "https://s1gh.sh/cve-2020-27985-security-onion-local-privilege-escalation/", "source": "cve@mitre.org", "tags": ["Exploit", "Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/Security-Onion-Solutions/securityonion/commit/b14670030349a2747a00ace665568ab5f51ac47b"}}