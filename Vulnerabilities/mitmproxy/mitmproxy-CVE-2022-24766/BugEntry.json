{"buggy_code": ["import asyncio\nfrom typing import Dict, Optional, Tuple\n\nfrom mitmproxy import command, ctx, exceptions, flow, http, log, master, options, platform, tcp, websocket\nfrom mitmproxy.flow import Flow\nfrom mitmproxy.proxy import commands, events, server_hooks\nfrom mitmproxy.proxy import server\nfrom mitmproxy.proxy.layers.tcp import TcpMessageInjected\nfrom mitmproxy.proxy.layers.websocket import WebSocketMessageInjected\nfrom mitmproxy.utils import asyncio_utils, human\nfrom wsproto.frame_protocol import Opcode\n\n\nclass ProxyConnectionHandler(server.StreamConnectionHandler):\n    master: master.Master\n\n    def __init__(self, master, r, w, options):\n        self.master = master\n        super().__init__(r, w, options)\n        self.log_prefix = f\"{human.format_address(self.client.peername)}: \"\n\n    async def handle_hook(self, hook: commands.StartHook) -> None:\n        with self.timeout_watchdog.disarm():\n            # We currently only support single-argument hooks.\n            data, = hook.args()\n            await self.master.addons.handle_lifecycle(hook)\n            if isinstance(data, flow.Flow):\n                await data.wait_for_resume()\n\n    def log(self, message: str, level: str = \"info\") -> None:\n        x = log.LogEntry(self.log_prefix + message, level)\n        asyncio_utils.create_task(\n            self.master.addons.handle_lifecycle(log.AddLogHook(x)),\n            name=\"ProxyConnectionHandler.log\"\n        )\n\n\nclass Proxyserver:\n    \"\"\"\n    This addon runs the actual proxy server.\n    \"\"\"\n    server: Optional[asyncio.AbstractServer]\n    listen_port: int\n    master: master.Master\n    options: options.Options\n    is_running: bool\n    _connections: Dict[Tuple, ProxyConnectionHandler]\n\n    def __init__(self):\n        self._lock = asyncio.Lock()\n        self.server = None\n        self.is_running = False\n        self._connections = {}\n\n    def __repr__(self):\n        return f\"ProxyServer({'running' if self.server else 'stopped'}, {len(self._connections)} active conns)\"\n\n    def load(self, loader):\n        loader.add_option(\n            \"connection_strategy\", str, \"eager\",\n            \"Determine when server connections should be established. When set to lazy, mitmproxy \"\n            \"tries to defer establishing an upstream connection as long as possible. This makes it possible to \"\n            \"use server replay while being offline. When set to eager, mitmproxy can detect protocols with \"\n            \"server-side greetings, as well as accurately mirror TLS ALPN negotiation.\",\n            choices=(\"eager\", \"lazy\")\n        )\n        loader.add_option(\n            \"stream_large_bodies\", Optional[str], None,\n            \"\"\"\n            Stream data to the client if response body exceeds the given\n            threshold. If streamed, the body will not be stored in any way.\n            Understands k/m/g suffixes, i.e. 3m for 3 megabytes.\n            \"\"\"\n        )\n        loader.add_option(\n            \"body_size_limit\", Optional[str], None,\n            \"\"\"\n            Byte size limit of HTTP request and response bodies. Understands\n            k/m/g suffixes, i.e. 3m for 3 megabytes.\n            \"\"\"\n        )\n        loader.add_option(\n            \"keep_host_header\", bool, False,\n            \"\"\"\n            Reverse Proxy: Keep the original host header instead of rewriting it\n            to the reverse proxy target.\n            \"\"\"\n        )\n        loader.add_option(\n            \"proxy_debug\", bool, False,\n            \"Enable debug logs in the proxy core.\",\n        )\n        loader.add_option(\n            \"normalize_outbound_headers\", bool, True,\n            \"\"\"\n            Normalize outgoing HTTP/2 header names, but emit a warning when doing so.\n            HTTP/2 does not allow uppercase header names. This option makes sure that HTTP/2 headers set\n            in custom scripts are lowercased before they are sent.\n            \"\"\",\n        )\n\n    async def running(self):\n        self.master = ctx.master\n        self.options = ctx.options\n        self.is_running = True\n        await self.refresh_server()\n\n    def configure(self, updated):\n        if \"stream_large_bodies\" in updated:\n            try:\n                human.parse_size(ctx.options.stream_large_bodies)\n            except ValueError:\n                raise exceptions.OptionsError(f\"Invalid stream_large_bodies specification: \"\n                                              f\"{ctx.options.stream_large_bodies}\")\n        if \"body_size_limit\" in updated:\n            try:\n                human.parse_size(ctx.options.body_size_limit)\n            except ValueError:\n                raise exceptions.OptionsError(f\"Invalid body_size_limit specification: \"\n                                              f\"{ctx.options.body_size_limit}\")\n        if \"mode\" in updated and ctx.options.mode == \"transparent\":  # pragma: no cover\n            platform.init_transparent_mode()\n        if self.is_running and any(x in updated for x in [\"server\", \"listen_host\", \"listen_port\"]):\n            asyncio.create_task(self.refresh_server())\n\n    async def refresh_server(self):\n        async with self._lock:\n            if self.server:\n                await self.shutdown_server()\n                self.server = None\n            if ctx.options.server:\n                if not ctx.master.addons.get(\"nextlayer\"):\n                    ctx.log.warn(\"Warning: Running proxyserver without nextlayer addon!\")\n                try:\n                    self.server = await asyncio.start_server(\n                        self.handle_connection,\n                        self.options.listen_host,\n                        self.options.listen_port,\n                    )\n                except OSError as e:\n                    ctx.log.error(str(e))\n                    return\n                # TODO: This is a bit confusing currently for `-p 0`.\n                addrs = {f\"http://{human.format_address(s.getsockname())}\" for s in self.server.sockets}\n                ctx.log.info(f\"Proxy server listening at {' and '.join(addrs)}\")\n\n    async def shutdown_server(self):\n        ctx.log.info(\"Stopping server...\")\n        self.server.close()\n        await self.server.wait_closed()\n        self.server = None\n\n    async def handle_connection(self, r, w):\n        peername = w.get_extra_info('peername')\n        asyncio_utils.set_task_debug_info(\n            asyncio.current_task(),\n            name=f\"Proxyserver.handle_connection\",\n            client=peername,\n        )\n        handler = ProxyConnectionHandler(\n            self.master,\n            r,\n            w,\n            self.options\n        )\n        self._connections[peername] = handler\n        try:\n            await handler.handle_client()\n        finally:\n            del self._connections[peername]\n\n    def inject_event(self, event: events.MessageInjected):\n        if event.flow.client_conn.peername not in self._connections:\n            raise ValueError(\"Flow is not from a live connection.\")\n        self._connections[event.flow.client_conn.peername].server_event(event)\n\n    @command.command(\"inject.websocket\")\n    def inject_websocket(self, flow: Flow, to_client: bool, message: bytes, is_text: bool = True):\n        if not isinstance(flow, http.HTTPFlow) or not flow.websocket:\n            ctx.log.warn(\"Cannot inject WebSocket messages into non-WebSocket flows.\")\n\n        msg = websocket.WebSocketMessage(\n            Opcode.TEXT if is_text else Opcode.BINARY,\n            not to_client,\n            message\n        )\n        event = WebSocketMessageInjected(flow, msg)\n        try:\n            self.inject_event(event)\n        except ValueError as e:\n            ctx.log.warn(str(e))\n\n    @command.command(\"inject.tcp\")\n    def inject_tcp(self, flow: Flow, to_client: bool, message: bytes):\n        if not isinstance(flow, tcp.TCPFlow):\n            ctx.log.warn(\"Cannot inject TCP messages into non-TCP flows.\")\n\n        event = TcpMessageInjected(flow, tcp.TCPMessage(not to_client, message))\n        try:\n            self.inject_event(event)\n        except ValueError as e:\n            ctx.log.warn(str(e))\n\n    def server_connect(self, ctx: server_hooks.ServerConnectionHookData):\n        assert ctx.server.address\n        self_connect = (\n            ctx.server.address[1] == self.options.listen_port\n            and\n            ctx.server.address[0] in (\"localhost\", \"127.0.0.1\", \"::1\", self.options.listen_host)\n        )\n        if self_connect:\n            ctx.server.error = (\n                \"Request destination unknown. \"\n                \"Unable to figure out where this request should be forwarded to.\"\n            )\n", "from .read import (\n    read_request_head,\n    read_response_head,\n    connection_close,\n    expected_http_body_size,\n)\nfrom .assemble import (\n    assemble_request, assemble_request_head,\n    assemble_response, assemble_response_head,\n    assemble_body,\n)\n\n\n__all__ = [\n    \"read_request_head\",\n    \"read_response_head\",\n    \"connection_close\",\n    \"expected_http_body_size\",\n    \"assemble_request\", \"assemble_request_head\",\n    \"assemble_response\", \"assemble_response_head\",\n    \"assemble_body\",\n]\n", "import re\nimport time\nfrom typing import List, Tuple, Iterable, Optional\n\nfrom mitmproxy.http import Request, Headers, Response\nfrom mitmproxy.net.http import url\n\n\ndef get_header_tokens(headers, key):\n    \"\"\"\n        Retrieve all tokens for a header key. A number of different headers\n        follow a pattern where each header line can containe comma-separated\n        tokens, and headers can be set multiple times.\n    \"\"\"\n    if key not in headers:\n        return []\n    tokens = headers[key].split(\",\")\n    return [token.strip() for token in tokens]\n\n\ndef connection_close(http_version, headers):\n    \"\"\"\n        Checks the message to see if the client connection should be closed\n        according to RFC 2616 Section 8.1.\n        If we don't have a Connection header, HTTP 1.1 connections are assumed\n        to be persistent.\n    \"\"\"\n    if \"connection\" in headers:\n        tokens = get_header_tokens(headers, \"connection\")\n        if \"close\" in tokens:\n            return True\n        elif \"keep-alive\" in tokens:\n            return False\n\n    return http_version not in (\n        \"HTTP/1.1\", b\"HTTP/1.1\",\n        \"HTTP/2.0\", b\"HTTP/2.0\",\n    )\n\n\ndef expected_http_body_size(\n        request: Request,\n        response: Optional[Response] = None\n) -> Optional[int]:\n    \"\"\"\n        Returns:\n            The expected body length:\n            - a positive integer, if the size is known in advance\n            - None, if the size in unknown in advance (chunked encoding)\n            - -1, if all data should be read until end of stream.\n\n        Raises:\n            ValueError, if the content length header is invalid\n    \"\"\"\n    # Determine response size according to http://tools.ietf.org/html/rfc7230#section-3.3, which is inlined below.\n    if not response:\n        headers = request.headers\n    else:\n        headers = response.headers\n\n        #    1.  Any response to a HEAD request and any response with a 1xx\n        #        (Informational), 204 (No Content), or 304 (Not Modified) status\n        #        code is always terminated by the first empty line after the\n        #        header fields, regardless of the header fields present in the\n        #        message, and thus cannot contain a message body.\n        if request.method.upper() == \"HEAD\":\n            return 0\n        if 100 <= response.status_code <= 199:\n            return 0\n        if response.status_code in (204, 304):\n            return 0\n\n        #    2.  Any 2xx (Successful) response to a CONNECT request implies that\n        #        the connection will become a tunnel immediately after the empty\n        #        line that concludes the header fields.  A client MUST ignore any\n        #        Content-Length or Transfer-Encoding header fields received in\n        #        such a message.\n        if 200 <= response.status_code <= 299 and request.method.upper() == \"CONNECT\":\n            return 0\n\n    #    3.  If a Transfer-Encoding header field is present and the chunked\n    #        transfer coding (Section 4.1) is the final encoding, the message\n    #        body length is determined by reading and decoding the chunked\n    #        data until the transfer coding indicates the data is complete.\n    #\n    #        If a Transfer-Encoding header field is present in a response and\n    #        the chunked transfer coding is not the final encoding, the\n    #        message body length is determined by reading the connection until\n    #        it is closed by the server.  If a Transfer-Encoding header field\n    #        is present in a request and the chunked transfer coding is not\n    #        the final encoding, the message body length cannot be determined\n    #        reliably; the server MUST respond with the 400 (Bad Request)\n    #        status code and then close the connection.\n    #\n    #        If a message is received with both a Transfer-Encoding and a\n    #        Content-Length header field, the Transfer-Encoding overrides the\n    #        Content-Length.  Such a message might indicate an attempt to\n    #        perform request smuggling (Section 9.5) or response splitting\n    #        (Section 9.4) and ought to be handled as an error.  A sender MUST\n    #        remove the received Content-Length field prior to forwarding such\n    #        a message downstream.\n    #\n    if \"transfer-encoding\" in headers:\n        if \"content-length\" in headers:\n            raise ValueError(\"Received both a Transfer-Encoding and a Content-Length header, \"\n                             \"refusing as recommended in RFC 7230 Section 3.3.3. \"\n                             \"See https://github.com/mitmproxy/mitmproxy/issues/4799 for details.\")\n\n        te: str = headers[\"transfer-encoding\"]\n        if not te.isascii():\n            # guard against .lower() transforming non-ascii to ascii\n            raise ValueError(f\"Invalid transfer encoding: {te!r}\")\n        te = te.lower().strip(\"\\t \")\n        te = re.sub(r\"[\\t ]*,[\\t ]*\", \",\", te)\n        if te in (\n            \"chunked\",\n            \"compress,chunked\",\n            \"deflate,chunked\",\n            \"gzip,chunked\",\n        ):\n            return None\n        elif te in (\n            \"compress\",\n            \"deflate\",\n            \"gzip\",\n            \"identity\",\n        ):\n            if response:\n                return -1\n            else:\n                raise ValueError(f\"Invalid request transfer encoding, message body cannot be determined reliably.\")\n        else:\n            raise ValueError(f\"Unknown transfer encoding: {headers['transfer-encoding']!r}\")\n\n    #    4.  If a message is received without Transfer-Encoding and with\n    #        either multiple Content-Length header fields having differing\n    #        field-values or a single Content-Length header field having an\n    #        invalid value, then the message framing is invalid and the\n    #        recipient MUST treat it as an unrecoverable error.  If this is a\n    #        request message, the server MUST respond with a 400 (Bad Request)\n    #        status code and then close the connection.  If this is a response\n    #        message received by a proxy, the proxy MUST close the connection\n    #        to the server, discard the received response, and send a 502 (Bad\n    #        Gateway) response to the client.  If this is a response message\n    #        received by a user agent, the user agent MUST close the\n    #        connection to the server and discard the received response.\n    #\n    #    5.  If a valid Content-Length header field is present without\n    #        Transfer-Encoding, its decimal value defines the expected message\n    #        body length in octets.  If the sender closes the connection or\n    #        the recipient times out before the indicated number of octets are\n    #        received, the recipient MUST consider the message to be\n    #        incomplete and close the connection.\n    if \"content-length\" in headers:\n        sizes = headers.get_all(\"content-length\")\n        different_content_length_headers = any(x != sizes[0] for x in sizes)\n        if different_content_length_headers:\n            raise ValueError(f\"Conflicting Content-Length headers: {sizes!r}\")\n        try:\n            size = int(sizes[0])\n        except ValueError:\n            raise ValueError(f\"Invalid Content-Length header: {sizes[0]!r}\")\n        if size < 0:\n            raise ValueError(f\"Negative Content-Length header: {sizes[0]!r}\")\n        return size\n\n    #    6.  If this is a request message and none of the above are true, then\n    #        the message body length is zero (no message body is present).\n    if not response:\n        return 0\n\n    #    7.  Otherwise, this is a response message without a declared message\n    #        body length, so the message body length is determined by the\n    #        number of octets received prior to the server closing the\n    #        connection.\n    return -1\n\n\ndef raise_if_http_version_unknown(http_version: bytes) -> None:\n    if not re.match(br\"^HTTP/\\d\\.\\d$\", http_version):\n        raise ValueError(f\"Unknown HTTP version: {http_version!r}\")\n\n\ndef _read_request_line(line: bytes) -> Tuple[str, int, bytes, bytes, bytes, bytes, bytes]:\n    try:\n        method, target, http_version = line.split()\n        port: Optional[int]\n\n        if target == b\"*\" or target.startswith(b\"/\"):\n            scheme, authority, path = b\"\", b\"\", target\n            host, port = \"\", 0\n        elif method == b\"CONNECT\":\n            scheme, authority, path = b\"\", target, b\"\"\n            host, port = url.parse_authority(authority, check=True)\n            if not port:\n                raise ValueError\n        else:\n            scheme, rest = target.split(b\"://\", maxsplit=1)\n            authority, _, path_ = rest.partition(b\"/\")\n            path = b\"/\" + path_\n            host, port = url.parse_authority(authority, check=True)\n            port = port or url.default_port(scheme)\n            if not port:\n                raise ValueError\n            # TODO: we can probably get rid of this check?\n            url.parse(target)\n\n        raise_if_http_version_unknown(http_version)\n    except ValueError as e:\n        raise ValueError(f\"Bad HTTP request line: {line!r}\") from e\n\n    return host, port, method, scheme, authority, path, http_version\n\n\ndef _read_response_line(line: bytes) -> Tuple[bytes, int, bytes]:\n    try:\n        parts = line.split(None, 2)\n        if len(parts) == 2:  # handle missing message gracefully\n            parts.append(b\"\")\n\n        http_version, status_code_str, reason = parts\n        status_code = int(status_code_str)\n        raise_if_http_version_unknown(http_version)\n    except ValueError as e:\n        raise ValueError(f\"Bad HTTP response line: {line!r}\") from e\n\n    return http_version, status_code, reason\n\n\ndef _read_headers(lines: Iterable[bytes]) -> Headers:\n    \"\"\"\n        Read a set of headers.\n        Stop once a blank line is reached.\n\n        Returns:\n            A headers object\n\n        Raises:\n            exceptions.HttpSyntaxException\n    \"\"\"\n    ret: List[Tuple[bytes, bytes]] = []\n    for line in lines:\n        if line[0] in b\" \\t\":\n            if not ret:\n                raise ValueError(\"Invalid headers\")\n            # continued header\n            ret[-1] = (ret[-1][0], ret[-1][1] + b'\\r\\n ' + line.strip())\n        else:\n            try:\n                name, value = line.split(b\":\", 1)\n                value = value.strip()\n                if not name:\n                    raise ValueError()\n                ret.append((name, value))\n            except ValueError:\n                raise ValueError(f\"Invalid header line: {line!r}\")\n    return Headers(ret)\n\n\ndef read_request_head(lines: List[bytes]) -> Request:\n    \"\"\"\n    Parse an HTTP request head (request line + headers) from an iterable of lines\n\n    Args:\n        lines: The input lines\n\n    Returns:\n        The HTTP request object (without body)\n\n    Raises:\n        ValueError: The input is malformed.\n    \"\"\"\n    host, port, method, scheme, authority, path, http_version = _read_request_line(lines[0])\n    headers = _read_headers(lines[1:])\n\n    return Request(\n        host=host,\n        port=port,\n        method=method,\n        scheme=scheme,\n        authority=authority,\n        path=path,\n        http_version=http_version,\n        headers=headers,\n        content=None,\n        trailers=None,\n        timestamp_start=time.time(),\n        timestamp_end=None\n    )\n\n\ndef read_response_head(lines: List[bytes]) -> Response:\n    \"\"\"\n    Parse an HTTP response head (response line + headers) from an iterable of lines\n\n    Args:\n        lines: The input lines\n\n    Returns:\n        The HTTP response object (without body)\n\n    Raises:\n        ValueError: The input is malformed.\n    \"\"\"\n    http_version, status_code, reason = _read_response_line(lines[0])\n    headers = _read_headers(lines[1:])\n\n    return Response(\n        http_version=http_version,\n        status_code=status_code,\n        reason=reason,\n        headers=headers,\n        content=None,\n        trailers=None,\n        timestamp_start=time.time(),\n        timestamp_end=None,\n    )\n", "import abc\nfrom typing import Callable, Optional, Type, Union\n\nimport h11\nfrom h11._readers import ChunkedReader, ContentLengthReader, Http10Reader\nfrom h11._receivebuffer import ReceiveBuffer\n\nfrom mitmproxy import http, version\nfrom mitmproxy.connection import Connection, ConnectionState\nfrom mitmproxy.net.http import http1, status_codes\nfrom mitmproxy.proxy import commands, events, layer\nfrom mitmproxy.proxy.layers.http._base import ReceiveHttp, StreamId\nfrom mitmproxy.proxy.utils import expect\nfrom mitmproxy.utils import human\nfrom ._base import HttpConnection, format_error\nfrom ._events import HttpEvent, RequestData, RequestEndOfMessage, RequestHeaders, RequestProtocolError, ResponseData, \\\n    ResponseEndOfMessage, ResponseHeaders, ResponseProtocolError\nfrom ...context import Context\n\nTBodyReader = Union[ChunkedReader, Http10Reader, ContentLengthReader]\n\n\nclass Http1Connection(HttpConnection, metaclass=abc.ABCMeta):\n    stream_id: Optional[StreamId] = None\n    request: Optional[http.Request] = None\n    response: Optional[http.Response] = None\n    request_done: bool = False\n    response_done: bool = False\n    # this is a bit of a hack to make both mypy and PyCharm happy.\n    state: Union[Callable[[events.Event], layer.CommandGenerator[None]], Callable]\n    body_reader: TBodyReader\n    buf: ReceiveBuffer\n\n    ReceiveProtocolError: Type[Union[RequestProtocolError, ResponseProtocolError]]\n    ReceiveData: Type[Union[RequestData, ResponseData]]\n    ReceiveEndOfMessage: Type[Union[RequestEndOfMessage, ResponseEndOfMessage]]\n\n    def __init__(self, context: Context, conn: Connection):\n        super().__init__(context, conn)\n        self.buf = ReceiveBuffer()\n\n    @abc.abstractmethod\n    def send(self, event: HttpEvent) -> layer.CommandGenerator[None]:\n        yield from ()  # pragma: no cover\n\n    @abc.abstractmethod\n    def read_headers(self, event: events.ConnectionEvent) -> layer.CommandGenerator[None]:\n        yield from ()  # pragma: no cover\n\n    def _handle_event(self, event: events.Event) -> layer.CommandGenerator[None]:\n        if isinstance(event, HttpEvent):\n            yield from self.send(event)\n        else:\n            if isinstance(event, events.DataReceived) and self.state != self.passthrough:\n                self.buf += event.data\n            yield from self.state(event)\n\n    @expect(events.Start)\n    def start(self, _) -> layer.CommandGenerator[None]:\n        self.state = self.read_headers\n        yield from ()\n\n    state = start\n\n    def read_body(self, event: events.Event) -> layer.CommandGenerator[None]:\n        assert self.stream_id\n        while True:\n            try:\n                if isinstance(event, events.DataReceived):\n                    h11_event = self.body_reader(self.buf)\n                elif isinstance(event, events.ConnectionClosed):\n                    h11_event = self.body_reader.read_eof()\n                else:\n                    raise AssertionError(f\"Unexpected event: {event}\")\n            except h11.ProtocolError as e:\n                yield commands.CloseConnection(self.conn)\n                yield ReceiveHttp(self.ReceiveProtocolError(self.stream_id, f\"HTTP/1 protocol error: {e}\"))\n                return\n\n            if h11_event is None:\n                return\n            elif isinstance(h11_event, h11.Data):\n                data: bytes = bytes(h11_event.data)\n                if data:\n                    yield ReceiveHttp(self.ReceiveData(self.stream_id, data))\n            elif isinstance(h11_event, h11.EndOfMessage):\n                assert self.request\n                if h11_event.headers:\n                    raise NotImplementedError(f\"HTTP trailers are not implemented yet.\")\n                if self.request.data.method.upper() != b\"CONNECT\":\n                    yield ReceiveHttp(self.ReceiveEndOfMessage(self.stream_id))\n                is_request = isinstance(self, Http1Server)\n                yield from self.mark_done(\n                    request=is_request,\n                    response=not is_request\n                )\n                return\n\n    def wait(self, event: events.Event) -> layer.CommandGenerator[None]:\n        \"\"\"\n        We wait for the current flow to be finished before parsing the next message,\n        as we may want to upgrade to WebSocket or plain TCP before that.\n        \"\"\"\n        assert self.stream_id\n        if isinstance(event, events.DataReceived):\n            return\n        elif isinstance(event, events.ConnectionClosed):\n            # for practical purposes, we assume that a peer which sent at least a FIN\n            # is not interested in any more data from us, see\n            # see https://github.com/httpwg/http-core/issues/22\n            if event.connection.state is not ConnectionState.CLOSED:\n                yield commands.CloseConnection(event.connection)\n            yield ReceiveHttp(self.ReceiveProtocolError(self.stream_id, f\"Client disconnected.\",\n                                                        code=status_codes.CLIENT_CLOSED_REQUEST))\n        else:  # pragma: no cover\n            raise AssertionError(f\"Unexpected event: {event}\")\n\n    def done(self, event: events.ConnectionEvent) -> layer.CommandGenerator[None]:\n        yield from ()  # pragma: no cover\n\n    def make_pipe(self) -> layer.CommandGenerator[None]:\n        self.state = self.passthrough\n        if self.buf:\n            already_received = self.buf.maybe_extract_at_most(len(self.buf))\n            # Some clients send superfluous newlines after CONNECT, we want to eat those.\n            already_received = already_received.lstrip(b\"\\r\\n\")\n            if already_received:\n                yield from self.state(events.DataReceived(self.conn, already_received))\n\n    def passthrough(self, event: events.Event) -> layer.CommandGenerator[None]:\n        assert self.stream_id\n        if isinstance(event, events.DataReceived):\n            yield ReceiveHttp(self.ReceiveData(self.stream_id, event.data))\n        elif isinstance(event, events.ConnectionClosed):\n            if isinstance(self, Http1Server):\n                yield ReceiveHttp(RequestEndOfMessage(self.stream_id))\n            else:\n                yield ReceiveHttp(ResponseEndOfMessage(self.stream_id))\n\n    def mark_done(self, *, request: bool = False, response: bool = False) -> layer.CommandGenerator[None]:\n        if request:\n            self.request_done = True\n        if response:\n            self.response_done = True\n        if self.request_done and self.response_done:\n            assert self.request\n            assert self.response\n            if should_make_pipe(self.request, self.response):\n                yield from self.make_pipe()\n                return\n            try:\n                read_until_eof_semantics = http1.expected_http_body_size(self.request, self.response) == -1\n            except ValueError:\n                # this may raise only now (and not earlier) because an addon set invalid headers,\n                # in which case it's not really clear what we are supposed to do.\n                read_until_eof_semantics = False\n            connection_done = (\n                read_until_eof_semantics\n                or http1.connection_close(self.request.http_version, self.request.headers)\n                or http1.connection_close(self.response.http_version, self.response.headers)\n                # If we proxy HTTP/2 to HTTP/1, we only use upstream connections for one request.\n                # This simplifies our connection management quite a bit as we can rely on\n                # the proxyserver's max-connection-per-server throttling.\n                or (self.request.is_http2 and isinstance(self, Http1Client))\n            )\n            if connection_done:\n                yield commands.CloseConnection(self.conn)\n                self.state = self.done\n                return\n            self.request_done = self.response_done = False\n            self.request = self.response = None\n            if isinstance(self, Http1Server):\n                self.stream_id += 2\n            else:\n                self.stream_id = None\n            self.state = self.read_headers\n            if self.buf:\n                yield from self.state(events.DataReceived(self.conn, b\"\"))\n\n\nclass Http1Server(Http1Connection):\n    \"\"\"A simple HTTP/1 server with no pipelining support.\"\"\"\n\n    ReceiveProtocolError = RequestProtocolError\n    ReceiveData = RequestData\n    ReceiveEndOfMessage = RequestEndOfMessage\n    stream_id: int\n\n    def __init__(self, context: Context):\n        super().__init__(context, context.client)\n        self.stream_id = 1\n\n    def send(self, event: HttpEvent) -> layer.CommandGenerator[None]:\n        assert event.stream_id == self.stream_id\n        if isinstance(event, ResponseHeaders):\n            self.response = response = event.response\n\n            if response.is_http2:\n                response = response.copy()\n                # Convert to an HTTP/1 response.\n                response.http_version = \"HTTP/1.1\"\n                # not everyone supports empty reason phrases, so we better make up one.\n                response.reason = status_codes.RESPONSES.get(response.status_code, \"\")\n                # Shall we set a Content-Length header here if there is none?\n                # For now, let's try to modify as little as possible.\n\n            raw = http1.assemble_response_head(response)\n            yield commands.SendData(self.conn, raw)\n        elif isinstance(event, ResponseData):\n            assert self.response\n            if \"chunked\" in self.response.headers.get(\"transfer-encoding\", \"\").lower():\n                raw = b\"%x\\r\\n%s\\r\\n\" % (len(event.data), event.data)\n            else:\n                raw = event.data\n            if raw:\n                yield commands.SendData(self.conn, raw)\n        elif isinstance(event, ResponseEndOfMessage):\n            assert self.response\n            if \"chunked\" in self.response.headers.get(\"transfer-encoding\", \"\").lower():\n                yield commands.SendData(self.conn, b\"0\\r\\n\\r\\n\")\n            yield from self.mark_done(response=True)\n        elif isinstance(event, ResponseProtocolError):\n            if not self.response and event.code != status_codes.NO_RESPONSE:\n                yield commands.SendData(self.conn, make_error_response(event.code, event.message))\n            if self.conn.state & ConnectionState.CAN_WRITE:\n                yield commands.CloseConnection(self.conn)\n        else:\n            raise AssertionError(f\"Unexpected event: {event}\")\n\n    def read_headers(self, event: events.ConnectionEvent) -> layer.CommandGenerator[None]:\n        if isinstance(event, events.DataReceived):\n            request_head = self.buf.maybe_extract_lines()\n            if request_head:\n                request_head = [bytes(x) for x in request_head]  # TODO: Make url.parse compatible with bytearrays\n                try:\n                    self.request = http1.read_request_head(request_head)\n                    expected_body_size = http1.expected_http_body_size(self.request)\n                except ValueError as e:\n                    yield commands.SendData(self.conn, make_error_response(400, str(e)))\n                    yield commands.CloseConnection(self.conn)\n                    if self.request:\n                        # we have headers that we can show in the ui\n                        yield ReceiveHttp(RequestHeaders(self.stream_id, self.request, False))\n                        yield ReceiveHttp(RequestProtocolError(self.stream_id, str(e), 400))\n                    else:\n                        yield commands.Log(f\"{human.format_address(self.conn.peername)}: {e}\")\n                    self.state = self.done\n                    return\n                yield ReceiveHttp(RequestHeaders(self.stream_id, self.request, expected_body_size == 0))\n                self.body_reader = make_body_reader(expected_body_size)\n                self.state = self.read_body\n                yield from self.state(event)\n            else:\n                pass  # FIXME: protect against header size DoS\n        elif isinstance(event, events.ConnectionClosed):\n            buf = bytes(self.buf)\n            if buf.strip():\n                yield commands.Log(f\"Client closed connection before completing request headers: {buf!r}\")\n            yield commands.CloseConnection(self.conn)\n        else:\n            raise AssertionError(f\"Unexpected event: {event}\")\n\n    def mark_done(self, *, request: bool = False, response: bool = False) -> layer.CommandGenerator[None]:\n        yield from super().mark_done(request=request, response=response)\n        if self.request_done and not self.response_done:\n            self.state = self.wait\n\n\nclass Http1Client(Http1Connection):\n    \"\"\"A simple HTTP/1 client with no pipelining support.\"\"\"\n\n    ReceiveProtocolError = ResponseProtocolError\n    ReceiveData = ResponseData\n    ReceiveEndOfMessage = ResponseEndOfMessage\n\n    def __init__(self, context: Context):\n        super().__init__(context, context.server)\n\n    def send(self, event: HttpEvent) -> layer.CommandGenerator[None]:\n        if isinstance(event, RequestProtocolError):\n            yield commands.CloseConnection(self.conn)\n            return\n\n        if not self.stream_id:\n            assert isinstance(event, RequestHeaders)\n            self.stream_id = event.stream_id\n            self.request = event.request\n        assert self.stream_id == event.stream_id\n\n        if isinstance(event, RequestHeaders):\n            request = event.request\n            if request.is_http2:\n                # Convert to an HTTP/1 request.\n                request = request.copy()  # (we could probably be a bit more efficient here.)\n                request.http_version = \"HTTP/1.1\"\n                if \"Host\" not in request.headers and request.authority:\n                    request.headers.insert(0, \"Host\", request.authority)\n                request.authority = \"\"\n            raw = http1.assemble_request_head(request)\n            yield commands.SendData(self.conn, raw)\n        elif isinstance(event, RequestData):\n            assert self.request\n            if \"chunked\" in self.request.headers.get(\"transfer-encoding\", \"\").lower():\n                raw = b\"%x\\r\\n%s\\r\\n\" % (len(event.data), event.data)\n            else:\n                raw = event.data\n            if raw:\n                yield commands.SendData(self.conn, raw)\n        elif isinstance(event, RequestEndOfMessage):\n            assert self.request\n            if \"chunked\" in self.request.headers.get(\"transfer-encoding\", \"\").lower():\n                yield commands.SendData(self.conn, b\"0\\r\\n\\r\\n\")\n            elif http1.expected_http_body_size(self.request, self.response) == -1:\n                yield commands.CloseConnection(self.conn, half_close=True)\n            yield from self.mark_done(request=True)\n        else:\n            raise AssertionError(f\"Unexpected event: {event}\")\n\n    def read_headers(self, event: events.ConnectionEvent) -> layer.CommandGenerator[None]:\n        if isinstance(event, events.DataReceived):\n            if not self.request:\n                # we just received some data for an unknown request.\n                yield commands.Log(f\"Unexpected data from server: {bytes(self.buf)!r}\")\n                yield commands.CloseConnection(self.conn)\n                return\n            assert self.stream_id\n\n            response_head = self.buf.maybe_extract_lines()\n            if response_head:\n                response_head = [bytes(x) for x in response_head]  # TODO: Make url.parse compatible with bytearrays\n                try:\n                    self.response = http1.read_response_head(response_head)\n                    expected_size = http1.expected_http_body_size(self.request, self.response)\n                except ValueError as e:\n                    yield commands.CloseConnection(self.conn)\n                    yield ReceiveHttp(ResponseProtocolError(self.stream_id, f\"Cannot parse HTTP response: {e}\"))\n                    return\n                yield ReceiveHttp(ResponseHeaders(self.stream_id, self.response, expected_size == 0))\n                self.body_reader = make_body_reader(expected_size)\n\n                self.state = self.read_body\n                yield from self.state(event)\n            else:\n                pass  # FIXME: protect against header size DoS\n        elif isinstance(event, events.ConnectionClosed):\n            if self.conn.state & ConnectionState.CAN_WRITE:\n                yield commands.CloseConnection(self.conn)\n            if self.stream_id:\n                if self.buf:\n                    yield ReceiveHttp(ResponseProtocolError(self.stream_id,\n                                                            f\"unexpected server response: {bytes(self.buf)!r}\"))\n                else:\n                    # The server has closed the connection to prevent us from continuing.\n                    # We need to signal that to the stream.\n                    # https://tools.ietf.org/html/rfc7231#section-6.5.11\n                    yield ReceiveHttp(ResponseProtocolError(self.stream_id, \"server closed connection\"))\n            else:\n                return\n        else:\n            raise AssertionError(f\"Unexpected event: {event}\")\n\n\ndef should_make_pipe(request: http.Request, response: http.Response) -> bool:\n    if response.status_code == 101:\n        return True\n    elif response.status_code == 200 and request.method.upper() == \"CONNECT\":\n        return True\n    else:\n        return False\n\n\ndef make_body_reader(expected_size: Optional[int]) -> TBodyReader:\n    if expected_size is None:\n        return ChunkedReader()\n    elif expected_size == -1:\n        return Http10Reader()\n    else:\n        return ContentLengthReader(expected_size)\n\n\ndef make_error_response(\n    status_code: int,\n    message: str = \"\",\n) -> bytes:\n    resp = http.Response.make(\n        status_code,\n        format_error(status_code, message),\n        http.Headers(\n            Server=version.MITMPROXY,\n            Connection=\"close\",\n            Content_Type=\"text/html\",\n        )\n    )\n    return http1.assemble_response(resp)\n\n\n__all__ = [\n    \"Http1Client\",\n    \"Http1Server\",\n]\n", "import collections\nimport time\nfrom enum import Enum\nfrom typing import ClassVar, DefaultDict, Dict, List, Optional, Sequence, Tuple, Type, Union\n\nimport h2.config\nimport h2.connection\nimport h2.errors\nimport h2.events\nimport h2.exceptions\nimport h2.settings\nimport h2.stream\nimport h2.utilities\n\nfrom mitmproxy import http, version\nfrom mitmproxy.connection import Connection\nfrom mitmproxy.net.http import status_codes, url\nfrom mitmproxy.utils import human\nfrom . import RequestData, RequestEndOfMessage, RequestHeaders, RequestProtocolError, ResponseData, \\\n    ResponseEndOfMessage, ResponseHeaders, RequestTrailers, ResponseTrailers, ResponseProtocolError\nfrom ._base import HttpConnection, HttpEvent, ReceiveHttp, format_error\nfrom ._http_h2 import BufferedH2Connection, H2ConnectionLogger\nfrom ...commands import CloseConnection, Log, SendData\nfrom ...context import Context\nfrom ...events import ConnectionClosed, DataReceived, Event, Start\nfrom ...layer import CommandGenerator\nfrom ...utils import expect\n\n\nclass StreamState(Enum):\n    EXPECTING_HEADERS = 1\n    HEADERS_RECEIVED = 2\n\n\nCATCH_HYPER_H2_ERRORS = (ValueError, IndexError)\n\n\nclass Http2Connection(HttpConnection):\n    h2_conf: ClassVar[h2.config.H2Configuration]\n    h2_conf_defaults = dict(\n        header_encoding=False,\n        validate_outbound_headers=False,\n        validate_inbound_headers=True,\n        normalize_inbound_headers=False,  # changing this to True is required to pass h2spec\n        normalize_outbound_headers=False,\n    )\n    h2_conn: BufferedH2Connection\n    streams: Dict[int, StreamState]\n    \"\"\"keep track of all active stream ids to send protocol errors on teardown\"\"\"\n\n    ReceiveProtocolError: Type[Union[RequestProtocolError, ResponseProtocolError]]\n    ReceiveData: Type[Union[RequestData, ResponseData]]\n    ReceiveTrailers: Type[Union[RequestTrailers, ResponseTrailers]]\n    ReceiveEndOfMessage: Type[Union[RequestEndOfMessage, ResponseEndOfMessage]]\n\n    def __init__(self, context: Context, conn: Connection):\n        super().__init__(context, conn)\n        if self.debug:\n            self.h2_conf.logger = H2ConnectionLogger(f\"{human.format_address(self.context.client.peername)}: \"\n                                                     f\"{self.__class__.__name__}\")\n        self.h2_conn = BufferedH2Connection(self.h2_conf)\n        self.streams = {}\n\n    def is_closed(self, stream_id: int) -> bool:\n        \"\"\"Check if a non-idle stream is closed\"\"\"\n        stream = self.h2_conn.streams.get(stream_id, None)\n        if (\n            stream is not None\n            and\n            stream.state_machine.state is not h2.stream.StreamState.CLOSED\n            and\n            self.h2_conn.state_machine.state is not h2.connection.ConnectionState.CLOSED\n        ):\n            return False\n        else:\n            return True\n\n    def is_open_for_us(self, stream_id: int) -> bool:\n        \"\"\"Check if we can write to a non-idle stream.\"\"\"\n        stream = self.h2_conn.streams.get(stream_id, None)\n        if (\n            stream is not None\n            and\n            stream.state_machine.state is not h2.stream.StreamState.HALF_CLOSED_LOCAL\n            and\n            stream.state_machine.state is not h2.stream.StreamState.CLOSED\n            and\n            self.h2_conn.state_machine.state is not h2.connection.ConnectionState.CLOSED\n        ):\n            return True\n        else:\n            return False\n\n    def _handle_event(self, event: Event) -> CommandGenerator[None]:\n        if isinstance(event, Start):\n            self.h2_conn.initiate_connection()\n            yield SendData(self.conn, self.h2_conn.data_to_send())\n\n        elif isinstance(event, HttpEvent):\n            if isinstance(event, (RequestData, ResponseData)):\n                if self.is_open_for_us(event.stream_id):\n                    self.h2_conn.send_data(event.stream_id, event.data)\n            elif isinstance(event, (RequestTrailers, ResponseTrailers)):\n                if self.is_open_for_us(event.stream_id):\n                    trailers = [*event.trailers.fields]\n                    self.h2_conn.send_headers(event.stream_id, trailers, end_stream=True)\n            elif isinstance(event, (RequestEndOfMessage, ResponseEndOfMessage)):\n                if self.is_open_for_us(event.stream_id):\n                    self.h2_conn.end_stream(event.stream_id)\n            elif isinstance(event, (RequestProtocolError, ResponseProtocolError)):\n                if not self.is_closed(event.stream_id):\n                    code = {\n                        status_codes.CLIENT_CLOSED_REQUEST: h2.errors.ErrorCodes.CANCEL,\n                    }.get(event.code, h2.errors.ErrorCodes.INTERNAL_ERROR)\n                    stream: h2.stream.H2Stream = self.h2_conn.streams[event.stream_id]\n                    send_error_message = (\n                        isinstance(event, ResponseProtocolError)\n                        and self.is_open_for_us(event.stream_id)\n                        and not stream.state_machine.headers_sent\n                        and event.code != status_codes.NO_RESPONSE\n                    )\n                    if send_error_message:\n                        self.h2_conn.send_headers(event.stream_id, [\n                            (b\":status\", b\"%d\" % event.code),\n                            (b\"server\", version.MITMPROXY.encode()),\n                            (b\"content-type\", b\"text/html\"),\n                        ])\n                        self.h2_conn.send_data(\n                            event.stream_id,\n                            format_error(event.code, event.message),\n                            end_stream=True\n                        )\n                    else:\n                        self.h2_conn.reset_stream(event.stream_id, code)\n            else:\n                raise AssertionError(f\"Unexpected event: {event}\")\n            data_to_send = self.h2_conn.data_to_send()\n            if data_to_send:\n                yield SendData(self.conn, data_to_send)\n\n        elif isinstance(event, DataReceived):\n            try:\n                try:\n                    events = self.h2_conn.receive_data(event.data)\n                except CATCH_HYPER_H2_ERRORS as e:  # pragma: no cover\n                    # this should never raise a ValueError, but we triggered one while fuzzing:\n                    # https://github.com/python-hyper/hyper-h2/issues/1231\n                    # this stays here as defense-in-depth.\n                    raise h2.exceptions.ProtocolError(f\"uncaught hyper-h2 error: {e}\") from e\n            except h2.exceptions.ProtocolError as e:\n                events = [e]\n\n            for h2_event in events:\n                if self.debug:\n                    yield Log(f\"{self.debug}[h2] {h2_event}\", \"debug\")\n                if (yield from self.handle_h2_event(h2_event)):\n                    if self.debug:\n                        yield Log(f\"{self.debug}[h2] done\", \"debug\")\n                    return\n\n            data_to_send = self.h2_conn.data_to_send()\n            if data_to_send:\n                yield SendData(self.conn, data_to_send)\n\n        elif isinstance(event, ConnectionClosed):\n            yield from self.close_connection(\"peer closed connection\")\n        else:\n            raise AssertionError(f\"Unexpected event: {event!r}\")\n\n    def handle_h2_event(self, event: h2.events.Event) -> CommandGenerator[bool]:\n        \"\"\"returns true if further processing should be stopped.\"\"\"\n        if isinstance(event, h2.events.DataReceived):\n            state = self.streams.get(event.stream_id, None)\n            if state is StreamState.HEADERS_RECEIVED:\n                yield ReceiveHttp(self.ReceiveData(event.stream_id, event.data))\n            elif state is StreamState.EXPECTING_HEADERS:\n                yield from self.protocol_error(f\"Received HTTP/2 data frame, expected headers.\")\n                return True\n            self.h2_conn.acknowledge_received_data(event.flow_controlled_length, event.stream_id)\n        elif isinstance(event, h2.events.TrailersReceived):\n            trailers = http.Headers(event.headers)\n            yield ReceiveHttp(self.ReceiveTrailers(event.stream_id, trailers))\n        elif isinstance(event, h2.events.StreamEnded):\n            state = self.streams.get(event.stream_id, None)\n            if state is StreamState.HEADERS_RECEIVED:\n                yield ReceiveHttp(self.ReceiveEndOfMessage(event.stream_id))\n            elif state is StreamState.EXPECTING_HEADERS:\n                raise AssertionError(\"unreachable\")\n            if self.is_closed(event.stream_id):\n                self.streams.pop(event.stream_id, None)\n        elif isinstance(event, h2.events.StreamReset):\n            if event.stream_id in self.streams:\n                try:\n                    err_str = h2.errors.ErrorCodes(event.error_code).name\n                except ValueError:\n                    err_str = str(event.error_code)\n                err_code = {\n                    h2.errors.ErrorCodes.CANCEL: status_codes.CLIENT_CLOSED_REQUEST,\n                }.get(event.error_code, self.ReceiveProtocolError.code)\n                yield ReceiveHttp(self.ReceiveProtocolError(event.stream_id, f\"stream reset by client ({err_str})\",\n                                                            code=err_code))\n                self.streams.pop(event.stream_id)\n            else:\n                pass  # We don't track priority frames which could be followed by a stream reset here.\n        elif isinstance(event, h2.exceptions.ProtocolError):\n            yield from self.protocol_error(f\"HTTP/2 protocol error: {event}\")\n            return True\n        elif isinstance(event, h2.events.ConnectionTerminated):\n            yield from self.close_connection(f\"HTTP/2 connection closed: {event!r}\")\n            return True\n            # The implementation above isn't really ideal, we should probably only terminate streams > last_stream_id?\n            # We currently lack a mechanism to signal that connections are still active but cannot be reused.\n            # for stream_id in self.streams:\n            #    if stream_id > event.last_stream_id:\n            #        yield ReceiveHttp(self.ReceiveProtocolError(stream_id, f\"HTTP/2 connection closed: {event!r}\"))\n            #        self.streams.pop(stream_id)\n        elif isinstance(event, h2.events.RemoteSettingsChanged):\n            pass\n        elif isinstance(event, h2.events.SettingsAcknowledged):\n            pass\n        elif isinstance(event, h2.events.PriorityUpdated):\n            pass\n        elif isinstance(event, h2.events.PingReceived):\n            pass\n        elif isinstance(event, h2.events.PingAckReceived):\n            pass\n        elif isinstance(event, h2.events.PushedStreamReceived):\n            yield Log(\"Received HTTP/2 push promise, even though we signalled no support.\", \"error\")\n        elif isinstance(event, h2.events.UnknownFrameReceived):\n            # https://http2.github.io/http2-spec/#rfc.section.4.1\n            # Implementations MUST ignore and discard any frame that has a type that is unknown.\n            yield Log(f\"Ignoring unknown HTTP/2 frame type: {event.frame.type}\")\n        else:\n            raise AssertionError(f\"Unexpected event: {event!r}\")\n        return False\n\n    def protocol_error(\n        self,\n        message: str,\n        error_code: int = h2.errors.ErrorCodes.PROTOCOL_ERROR,\n    ) -> CommandGenerator[None]:\n        yield Log(f\"{human.format_address(self.conn.peername)}: {message}\")\n        self.h2_conn.close_connection(error_code, message.encode())\n        yield SendData(self.conn, self.h2_conn.data_to_send())\n        yield from self.close_connection(message)\n\n    def close_connection(self, msg: str) -> CommandGenerator[None]:\n        yield CloseConnection(self.conn)\n        for stream_id in self.streams:\n            yield ReceiveHttp(self.ReceiveProtocolError(stream_id, msg))\n        self.streams.clear()\n        self._handle_event = self.done  # type: ignore\n\n    @expect(DataReceived, HttpEvent, ConnectionClosed)\n    def done(self, _) -> CommandGenerator[None]:\n        yield from ()\n\n\ndef normalize_h1_headers(headers: List[Tuple[bytes, bytes]], is_client: bool) -> List[Tuple[bytes, bytes]]:\n    # HTTP/1 servers commonly send capitalized headers (Content-Length vs content-length),\n    # which isn't valid HTTP/2. As such we normalize.\n    headers = h2.utilities.normalize_outbound_headers(\n        headers,\n        h2.utilities.HeaderValidationFlags(is_client, False, not is_client, False)\n    )\n    # make sure that this is not just an iterator but an iterable,\n    # otherwise hyper-h2 will silently drop headers.\n    headers = list(headers)\n    return headers\n\n\ndef normalize_h2_headers(headers: List[Tuple[bytes, bytes]]) -> CommandGenerator[None]:\n    for i in range(len(headers)):\n        if not headers[i][0].islower():\n            yield Log(f\"Lowercased {repr(headers[i][0]).lstrip('b')} header as uppercase is not allowed with HTTP/2.\")\n            headers[i] = (headers[i][0].lower(), headers[i][1])\n\n\nclass Http2Server(Http2Connection):\n    h2_conf = h2.config.H2Configuration(\n        **Http2Connection.h2_conf_defaults,\n        client_side=False,\n    )\n\n    ReceiveProtocolError = RequestProtocolError\n    ReceiveData = RequestData\n    ReceiveTrailers = RequestTrailers\n    ReceiveEndOfMessage = RequestEndOfMessage\n\n    def __init__(self, context: Context):\n        super().__init__(context, context.client)\n\n    def _handle_event(self, event: Event) -> CommandGenerator[None]:\n        if isinstance(event, ResponseHeaders):\n            if self.is_open_for_us(event.stream_id):\n                headers = [\n                    (b\":status\", b\"%d\" % event.response.status_code),\n                    *event.response.headers.fields\n                ]\n                if event.response.is_http2:\n                    if self.context.options.normalize_outbound_headers:\n                        yield from normalize_h2_headers(headers)\n                else:\n                    headers = normalize_h1_headers(headers, False)\n\n                self.h2_conn.send_headers(\n                    event.stream_id,\n                    headers,\n                    end_stream=event.end_stream,\n                )\n                yield SendData(self.conn, self.h2_conn.data_to_send())\n        else:\n            yield from super()._handle_event(event)\n\n    def handle_h2_event(self, event: h2.events.Event) -> CommandGenerator[bool]:\n        if isinstance(event, h2.events.RequestReceived):\n            try:\n                host, port, method, scheme, authority, path, headers = parse_h2_request_headers(event.headers)\n            except ValueError as e:\n                yield from self.protocol_error(f\"Invalid HTTP/2 request headers: {e}\")\n                return True\n            request = http.Request(\n                host=host,\n                port=port,\n                method=method,\n                scheme=scheme,\n                authority=authority,\n                path=path,\n                http_version=b\"HTTP/2.0\",\n                headers=headers,\n                content=None,\n                trailers=None,\n                timestamp_start=time.time(),\n                timestamp_end=None,\n            )\n            self.streams[event.stream_id] = StreamState.HEADERS_RECEIVED\n            yield ReceiveHttp(RequestHeaders(event.stream_id, request, end_stream=bool(event.stream_ended)))\n            return False\n        else:\n            return (yield from super().handle_h2_event(event))\n\n\nclass Http2Client(Http2Connection):\n    h2_conf = h2.config.H2Configuration(\n        **Http2Connection.h2_conf_defaults,\n        client_side=True,\n    )\n\n    ReceiveProtocolError = ResponseProtocolError\n    ReceiveData = ResponseData\n    ReceiveTrailers = ResponseTrailers\n    ReceiveEndOfMessage = ResponseEndOfMessage\n\n    our_stream_id: Dict[int, int]\n    their_stream_id: Dict[int, int]\n    stream_queue: DefaultDict[int, List[Event]]\n    \"\"\"Queue of streams that we haven't sent yet because we have reached MAX_CONCURRENT_STREAMS\"\"\"\n    provisional_max_concurrency: Optional[int] = 10\n    \"\"\"A provisional currency limit before we get the server's first settings frame.\"\"\"\n\n    def __init__(self, context: Context):\n        super().__init__(context, context.server)\n        # Disable HTTP/2 push for now to keep things simple.\n        # don't send here, that is done as part of initiate_connection().\n        self.h2_conn.local_settings.enable_push = 0\n        # hyper-h2 pitfall: we need to acknowledge here, otherwise its sends out the old settings.\n        self.h2_conn.local_settings.acknowledge()\n        self.our_stream_id = {}\n        self.their_stream_id = {}\n        self.stream_queue = collections.defaultdict(list)\n\n    def _handle_event(self, event: Event) -> CommandGenerator[None]:\n        # We can't reuse stream ids from the client because they may arrived reordered here\n        # and HTTP/2 forbids opening a stream on a lower id than what was previously sent (see test_stream_concurrency).\n        # To mitigate this, we transparently map the outside's stream id to our stream id.\n        if isinstance(event, HttpEvent):\n            ours = self.our_stream_id.get(event.stream_id, None)\n            if ours is None:\n                no_free_streams = (\n                    self.h2_conn.open_outbound_streams >=\n                    (self.provisional_max_concurrency or self.h2_conn.remote_settings.max_concurrent_streams)\n                )\n                if no_free_streams:\n                    self.stream_queue[event.stream_id].append(event)\n                    return\n                ours = self.h2_conn.get_next_available_stream_id()\n                self.our_stream_id[event.stream_id] = ours\n                self.their_stream_id[ours] = event.stream_id\n            event.stream_id = ours\n\n        for cmd in self._handle_event2(event):\n            if isinstance(cmd, ReceiveHttp):\n                cmd.event.stream_id = self.their_stream_id[cmd.event.stream_id]\n            yield cmd\n\n        can_resume_queue = (\n            self.stream_queue and\n            self.h2_conn.open_outbound_streams < (\n                self.provisional_max_concurrency or self.h2_conn.remote_settings.max_concurrent_streams\n            )\n        )\n        if can_resume_queue:\n            # popitem would be LIFO, but we want FIFO.\n            events = self.stream_queue.pop(next(iter(self.stream_queue)))\n            for event in events:\n                yield from self._handle_event(event)\n\n    def _handle_event2(self, event: Event) -> CommandGenerator[None]:\n        if isinstance(event, RequestHeaders):\n            pseudo_headers = [\n                (b':method', event.request.data.method),\n                (b':scheme', event.request.data.scheme),\n                (b':path', event.request.data.path),\n            ]\n            if event.request.authority:\n                pseudo_headers.append((b\":authority\", event.request.data.authority))\n\n            if event.request.is_http2:\n                hdrs = list(event.request.headers.fields)\n                if self.context.options.normalize_outbound_headers:\n                    yield from normalize_h2_headers(hdrs)\n            else:\n                headers = event.request.headers\n                if not event.request.authority and \"host\" in headers:\n                    headers = headers.copy()\n                    pseudo_headers.append((b\":authority\", headers.pop(b\"host\")))\n                hdrs = normalize_h1_headers(list(headers.fields), True)\n\n            self.h2_conn.send_headers(\n                event.stream_id,\n                pseudo_headers + hdrs,\n                end_stream=event.end_stream,\n            )\n            self.streams[event.stream_id] = StreamState.EXPECTING_HEADERS\n            yield SendData(self.conn, self.h2_conn.data_to_send())\n        else:\n            yield from super()._handle_event(event)\n\n    def handle_h2_event(self, event: h2.events.Event) -> CommandGenerator[bool]:\n        if isinstance(event, h2.events.ResponseReceived):\n            if self.streams.get(event.stream_id, None) is not StreamState.EXPECTING_HEADERS:\n                yield from self.protocol_error(f\"Received unexpected HTTP/2 response.\")\n                return True\n\n            try:\n                status_code, headers = parse_h2_response_headers(event.headers)\n            except ValueError as e:\n                yield from self.protocol_error(f\"Invalid HTTP/2 response headers: {e}\")\n                return True\n\n            response = http.Response(\n                http_version=b\"HTTP/2.0\",\n                status_code=status_code,\n                reason=b\"\",\n                headers=headers,\n                content=None,\n                trailers=None,\n                timestamp_start=time.time(),\n                timestamp_end=None,\n            )\n            self.streams[event.stream_id] = StreamState.HEADERS_RECEIVED\n            yield ReceiveHttp(ResponseHeaders(event.stream_id, response, bool(event.stream_ended)))\n            return False\n        elif isinstance(event, h2.events.RequestReceived):\n            yield from self.protocol_error(f\"HTTP/2 protocol error: received request from server\")\n            return True\n        elif isinstance(event, h2.events.RemoteSettingsChanged):\n            # We have received at least one settings from now,\n            # which means we can rely on the max concurrency in remote_settings\n            self.provisional_max_concurrency = None\n            return (yield from super().handle_h2_event(event))\n        else:\n            return (yield from super().handle_h2_event(event))\n\n\ndef split_pseudo_headers(h2_headers: Sequence[Tuple[bytes, bytes]]) -> Tuple[Dict[bytes, bytes], http.Headers]:\n    pseudo_headers: Dict[bytes, bytes] = {}\n    i = 0\n    for (header, value) in h2_headers:\n        if header.startswith(b\":\"):\n            if header in pseudo_headers:\n                raise ValueError(f\"Duplicate HTTP/2 pseudo header: {header!r}\")\n            pseudo_headers[header] = value\n            i += 1\n        else:\n            # Pseudo-headers must be at the start, we are done here.\n            break\n\n    headers = http.Headers(h2_headers[i:])\n\n    return pseudo_headers, headers\n\n\ndef parse_h2_request_headers(\n    h2_headers: Sequence[Tuple[bytes, bytes]]\n) -> Tuple[str, int, bytes, bytes, bytes, bytes, http.Headers]:\n    \"\"\"Split HTTP/2 pseudo-headers from the actual headers and parse them.\"\"\"\n    pseudo_headers, headers = split_pseudo_headers(h2_headers)\n\n    try:\n        method: bytes = pseudo_headers.pop(b\":method\")\n        scheme: bytes = pseudo_headers.pop(b\":scheme\")  # this raises for HTTP/2 CONNECT requests\n        path: bytes = pseudo_headers.pop(b\":path\")\n        authority: bytes = pseudo_headers.pop(b\":authority\", b\"\")\n    except KeyError as e:\n        raise ValueError(f\"Required pseudo header is missing: {e}\")\n\n    if pseudo_headers:\n        raise ValueError(f\"Unknown pseudo headers: {pseudo_headers}\")\n\n    if authority:\n        host, port = url.parse_authority(authority, check=True)\n        if port is None:\n            port = 80 if scheme == b'http' else 443\n    else:\n        host = \"\"\n        port = 0\n\n    return host, port, method, scheme, authority, path, headers\n\n\ndef parse_h2_response_headers(h2_headers: Sequence[Tuple[bytes, bytes]]) -> Tuple[int, http.Headers]:\n    \"\"\"Split HTTP/2 pseudo-headers from the actual headers and parse them.\"\"\"\n    pseudo_headers, headers = split_pseudo_headers(h2_headers)\n\n    try:\n        status_code: int = int(pseudo_headers.pop(b\":status\"))\n    except KeyError as e:\n        raise ValueError(f\"Required pseudo header is missing: {e}\")\n\n    if pseudo_headers:\n        raise ValueError(f\"Unknown pseudo headers: {pseudo_headers}\")\n\n    return status_code, headers\n\n\n__all__ = [\n    \"Http2Client\",\n    \"Http2Server\",\n]\n", "import pytest\n\nfrom mitmproxy.http import Headers\nfrom mitmproxy.net.http.http1.read import (\n    read_request_head,\n    read_response_head, connection_close, expected_http_body_size,\n    _read_request_line, _read_response_line, _read_headers, get_header_tokens\n)\nfrom mitmproxy.test.tutils import treq, tresp\n\n\ndef test_get_header_tokens():\n    headers = Headers()\n    assert get_header_tokens(headers, \"foo\") == []\n    headers[\"foo\"] = \"bar\"\n    assert get_header_tokens(headers, \"foo\") == [\"bar\"]\n    headers[\"foo\"] = \"bar, voing\"\n    assert get_header_tokens(headers, \"foo\") == [\"bar\", \"voing\"]\n    headers.set_all(\"foo\", [\"bar, voing\", \"oink\"])\n    assert get_header_tokens(headers, \"foo\") == [\"bar\", \"voing\", \"oink\"]\n\n\ndef test_connection_close():\n    headers = Headers()\n    assert connection_close(b\"HTTP/1.0\", headers)\n    assert not connection_close(b\"HTTP/1.1\", headers)\n    assert not connection_close(b\"HTTP/2.0\", headers)\n\n    headers[\"connection\"] = \"keep-alive\"\n    assert not connection_close(b\"HTTP/1.1\", headers)\n\n    headers[\"connection\"] = \"close\"\n    assert connection_close(b\"HTTP/1.1\", headers)\n\n    headers[\"connection\"] = \"foobar\"\n    assert connection_close(b\"HTTP/1.0\", headers)\n    assert not connection_close(b\"HTTP/1.1\", headers)\n\n\ndef test_read_request_head():\n    rfile = [\n        b\"GET / HTTP/1.1\\r\\n\",\n        b\"Content-Length: 4\\r\\n\",\n    ]\n    r = read_request_head(rfile)\n    assert r.method == \"GET\"\n    assert r.headers[\"Content-Length\"] == \"4\"\n    assert r.content is None\n\n\ndef test_read_response_head():\n    rfile = [\n        b\"HTTP/1.1 418 I'm a teapot\\r\\n\",\n        b\"Content-Length: 4\\r\\n\",\n    ]\n    r = read_response_head(rfile)\n    assert r.status_code == 418\n    assert r.headers[\"Content-Length\"] == \"4\"\n    assert r.content is None\n\n\ndef test_expected_http_body_size():\n    # Expect: 100-continue\n    assert expected_http_body_size(\n        treq(headers=Headers(expect=\"100-continue\", content_length=\"42\")),\n    ) == 42\n\n    # http://tools.ietf.org/html/rfc7230#section-3.3\n    assert expected_http_body_size(\n        treq(method=b\"HEAD\"),\n        tresp(headers=Headers(content_length=\"42\"))\n    ) == 0\n    assert expected_http_body_size(\n        treq(method=b\"CONNECT\", headers=Headers()),\n        None,\n    ) == 0\n    assert expected_http_body_size(\n        treq(method=b\"CONNECT\"),\n        tresp()\n    ) == 0\n    for code in (100, 204, 304):\n        assert expected_http_body_size(\n            treq(),\n            tresp(status_code=code)\n        ) == 0\n\n    # chunked\n    assert expected_http_body_size(\n        treq(headers=Headers(transfer_encoding=\"chunked\")),\n    ) is None\n    assert expected_http_body_size(\n        treq(headers=Headers(transfer_encoding=\"gzip,\\tchunked\")),\n    ) is None\n    # both content-length and chunked (possible request smuggling)\n    with pytest.raises(ValueError, match=\"Received both a Transfer-Encoding and a Content-Length header\"):\n        expected_http_body_size(\n            treq(headers=Headers(transfer_encoding=\"chunked\", content_length=\"42\")),\n        )\n    with pytest.raises(ValueError, match=\"Invalid transfer encoding\"):\n        expected_http_body_size(\n            treq(headers=Headers(transfer_encoding=\"chun\\u212Aed\")),  # \"chun\u212aed\".lower() == \"chunked\"\n        )\n    with pytest.raises(ValueError, match=\"Unknown transfer encoding\"):\n        expected_http_body_size(\n            treq(headers=Headers(transfer_encoding=\"chun ked\")),  # \"chun\u212aed\".lower() == \"chunked\"\n        )\n    with pytest.raises(ValueError, match=\"Unknown transfer encoding\"):\n        expected_http_body_size(\n            treq(headers=Headers(transfer_encoding=\"qux\")),\n        )\n    # transfer-encoding: gzip\n    with pytest.raises(ValueError, match=\"Invalid request transfer encoding\"):\n        expected_http_body_size(\n            treq(headers=Headers(transfer_encoding=\"gzip\")),\n        )\n    assert expected_http_body_size(\n        treq(),\n        tresp(headers=Headers(transfer_encoding=\"gzip\")),\n    ) == -1\n\n    # explicit length\n    for val in (b\"foo\", b\"-7\"):\n        with pytest.raises(ValueError):\n            expected_http_body_size(\n                treq(headers=Headers(content_length=val))\n            )\n    assert expected_http_body_size(\n        treq(headers=Headers(content_length=\"42\"))\n    ) == 42\n    # multiple content-length headers with same value\n    assert expected_http_body_size(\n        treq(headers=Headers([(b'content-length', b'42'), (b'content-length', b'42')]))\n    ) == 42\n    # multiple content-length headers with conflicting value\n    with pytest.raises(ValueError, match=\"Conflicting Content-Length headers\"):\n        expected_http_body_size(\n            treq(headers=Headers([(b'content-length', b'42'), (b'content-length', b'45')]))\n        )\n\n    # non-int content-length\n    with pytest.raises(ValueError, match=\"Invalid Content-Length header\"):\n        expected_http_body_size(\n            treq(headers=Headers([(b'content-length', b'NaN')]))\n        )\n    # negative content-length\n    with pytest.raises(ValueError, match=\"Negative Content-Length header\"):\n        expected_http_body_size(\n            treq(headers=Headers([(b'content-length', b'-1')]))\n        )\n\n    # no length\n    assert expected_http_body_size(\n        treq(headers=Headers())\n    ) == 0\n    assert expected_http_body_size(\n        treq(headers=Headers()), tresp(headers=Headers())\n    ) == -1\n\n\ndef test_read_request_line():\n    def t(b):\n        return _read_request_line(b)\n\n    assert (t(b\"GET / HTTP/1.1\") ==\n            (\"\", 0, b\"GET\", b\"\", b\"\", b\"/\", b\"HTTP/1.1\"))\n    assert (t(b\"OPTIONS * HTTP/1.1\") ==\n            (\"\", 0, b\"OPTIONS\", b\"\", b\"\", b\"*\", b\"HTTP/1.1\"))\n    assert (t(b\"CONNECT foo:42 HTTP/1.1\") ==\n            (\"foo\", 42, b\"CONNECT\", b\"\", b\"foo:42\", b\"\", b\"HTTP/1.1\"))\n    assert (t(b\"GET http://foo:42/bar HTTP/1.1\") ==\n            (\"foo\", 42, b\"GET\", b\"http\", b\"foo:42\", b\"/bar\", b\"HTTP/1.1\"))\n    assert (t(b\"GET http://foo:42 HTTP/1.1\") ==\n            (\"foo\", 42, b\"GET\", b\"http\", b\"foo:42\", b\"/\", b\"HTTP/1.1\"))\n\n    with pytest.raises(ValueError):\n        t(b\"GET / WTF/1.1\")\n    with pytest.raises(ValueError):\n        t(b\"CONNECT example.com HTTP/1.1\")  # port missing\n    with pytest.raises(ValueError):\n        t(b\"GET ws://example.com/ HTTP/1.1\")  # port missing\n    with pytest.raises(ValueError):\n        t(b\"this is not http\")\n    with pytest.raises(ValueError):\n        t(b\"\")\n\n\ndef test_read_response_line():\n    def t(b):\n        return _read_response_line(b)\n\n    assert t(b\"HTTP/1.1 200 OK\") == (b\"HTTP/1.1\", 200, b\"OK\")\n    assert t(b\"HTTP/1.1 200\") == (b\"HTTP/1.1\", 200, b\"\")\n\n    # https://github.com/mitmproxy/mitmproxy/issues/784\n    assert t(b\"HTTP/1.1 200 Non-Autoris\\xc3\\xa9\") == (b\"HTTP/1.1\", 200, b\"Non-Autoris\\xc3\\xa9\")\n\n    with pytest.raises(ValueError):\n        assert t(b\"HTTP/1.1\")\n\n    with pytest.raises(ValueError):\n        t(b\"HTTP/1.1 OK OK\")\n    with pytest.raises(ValueError):\n        t(b\"WTF/1.1 200 OK\")\n    with pytest.raises(ValueError):\n        t(b\"\")\n\n\nclass TestReadHeaders:\n    @staticmethod\n    def _read(data):\n        return _read_headers(data.splitlines(keepends=True))\n\n    def test_read_simple(self):\n        data = (\n            b\"Header: one\\r\\n\"\n            b\"Header2: two\\r\\n\"\n        )\n        headers = self._read(data)\n        assert headers.fields == ((b\"Header\", b\"one\"), (b\"Header2\", b\"two\"))\n\n    def test_read_multi(self):\n        data = (\n            b\"Header: one\\r\\n\"\n            b\"Header: two\\r\\n\"\n        )\n        headers = self._read(data)\n        assert headers.fields == ((b\"Header\", b\"one\"), (b\"Header\", b\"two\"))\n\n    def test_read_continued(self):\n        data = (\n            b\"Header: one\\r\\n\"\n            b\"\\ttwo\\r\\n\"\n            b\"Header2: three\\r\\n\"\n        )\n        headers = self._read(data)\n        assert headers.fields == ((b\"Header\", b\"one\\r\\n two\"), (b\"Header2\", b\"three\"))\n\n    def test_read_continued_err(self):\n        data = b\"\\tfoo: bar\\r\\n\"\n        with pytest.raises(ValueError):\n            self._read(data)\n\n    def test_read_err(self):\n        data = b\"foo\"\n        with pytest.raises(ValueError):\n            self._read(data)\n\n    def test_read_empty_name(self):\n        data = b\":foo\"\n        with pytest.raises(ValueError):\n            self._read(data)\n\n    def test_read_empty_value(self):\n        data = b\"bar:\"\n        headers = self._read(data)\n        assert headers.fields == ((b\"bar\", b\"\"),)\n", "import pytest\n\nfrom mitmproxy.connection import ConnectionState, Server\nfrom mitmproxy.flow import Error\nfrom mitmproxy.http import HTTPFlow, Response\nfrom mitmproxy.net.server_spec import ServerSpec\nfrom mitmproxy.proxy import layer\nfrom mitmproxy.proxy.commands import CloseConnection, Log, OpenConnection, SendData\nfrom mitmproxy.proxy.events import ConnectionClosed, DataReceived\nfrom mitmproxy.proxy.layers import TCPLayer, http, tls\nfrom mitmproxy.proxy.layers.http import HTTPMode\nfrom mitmproxy.proxy.layers.tcp import TcpMessageInjected, TcpStartHook\nfrom mitmproxy.proxy.layers.websocket import WebsocketStartHook\nfrom mitmproxy.tcp import TCPFlow, TCPMessage\nfrom test.mitmproxy.proxy.tutils import Placeholder, Playbook, reply, reply_next_layer\n\n\ndef test_http_proxy(tctx):\n    \"\"\"Test a simple HTTP GET / request\"\"\"\n    server = Placeholder(Server)\n    flow = Placeholder(HTTPFlow)\n    assert (\n            Playbook(http.HttpLayer(tctx, HTTPMode.regular))\n            >> DataReceived(tctx.client, b\"GET http://example.com/foo?hello=1 HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\")\n            << http.HttpRequestHeadersHook(flow)\n            >> reply()\n            << http.HttpRequestHook(flow)\n            >> reply()\n            << OpenConnection(server)\n            >> reply(None)\n            << SendData(server, b\"GET /foo?hello=1 HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\")\n            >> DataReceived(server, b\"HTTP/1.1 200 OK\\r\\nContent-Length: 12\\r\\n\\r\\nHello World\")\n            << http.HttpResponseHeadersHook(flow)\n            >> reply()\n            >> DataReceived(server, b\"!\")\n            << http.HttpResponseHook(flow)\n            >> reply()\n            << SendData(tctx.client, b\"HTTP/1.1 200 OK\\r\\nContent-Length: 12\\r\\n\\r\\nHello World!\")\n    )\n    assert server().address == (\"example.com\", 80)\n\n\n@pytest.mark.parametrize(\"strategy\", [\"lazy\", \"eager\"])\ndef test_https_proxy(strategy, tctx):\n    \"\"\"Test a CONNECT request, followed by a HTTP GET /\"\"\"\n    server = Placeholder(Server)\n    flow = Placeholder(HTTPFlow)\n    playbook = Playbook(http.HttpLayer(tctx, HTTPMode.regular))\n    tctx.options.connection_strategy = strategy\n\n    (playbook\n     >> DataReceived(tctx.client, b\"CONNECT example.proxy:80 HTTP/1.1\\r\\n\\r\\n\")\n     << http.HttpConnectHook(Placeholder())\n     >> reply())\n    if strategy == \"eager\":\n        (playbook\n         << OpenConnection(server)\n         >> reply(None))\n    (playbook\n     << SendData(tctx.client, b'HTTP/1.1 200 Connection established\\r\\n\\r\\n')\n     >> DataReceived(tctx.client, b\"GET /foo?hello=1 HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\")\n     << layer.NextLayerHook(Placeholder())\n     >> reply_next_layer(lambda ctx: http.HttpLayer(ctx, HTTPMode.transparent))\n     << http.HttpRequestHeadersHook(flow)\n     >> reply()\n     << http.HttpRequestHook(flow)\n     >> reply())\n    if strategy == \"lazy\":\n        (playbook\n         << OpenConnection(server)\n         >> reply(None))\n    (playbook\n     << SendData(server, b\"GET /foo?hello=1 HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\")\n     >> DataReceived(server, b\"HTTP/1.1 200 OK\\r\\nContent-Length: 12\\r\\n\\r\\nHello World!\")\n     << http.HttpResponseHeadersHook(flow)\n     >> reply()\n     << http.HttpResponseHook(flow)\n     >> reply()\n     << SendData(tctx.client, b\"HTTP/1.1 200 OK\\r\\nContent-Length: 12\\r\\n\\r\\nHello World!\"))\n    assert playbook\n\n\n@pytest.mark.parametrize(\"https_client\", [False, True])\n@pytest.mark.parametrize(\"https_server\", [False, True])\n@pytest.mark.parametrize(\"strategy\", [\"lazy\", \"eager\"])\ndef test_redirect(strategy, https_server, https_client, tctx, monkeypatch):\n    \"\"\"Test redirects between http:// and https:// in regular proxy mode.\"\"\"\n    server = Placeholder(Server)\n    flow = Placeholder(HTTPFlow)\n    tctx.options.connection_strategy = strategy\n    p = Playbook(http.HttpLayer(tctx, HTTPMode.regular), hooks=False)\n\n    if https_server:\n        monkeypatch.setattr(tls, \"ServerTLSLayer\", tls.MockTLSLayer)\n\n    def redirect(flow: HTTPFlow):\n        if https_server:\n            flow.request.url = \"https://redirected.site/\"\n        else:\n            flow.request.url = \"http://redirected.site/\"\n\n    if https_client:\n        p >> DataReceived(tctx.client, b\"CONNECT example.com:80 HTTP/1.1\\r\\n\\r\\n\")\n        if strategy == \"eager\":\n            p << OpenConnection(Placeholder())\n            p >> reply(None)\n        p << SendData(tctx.client, b'HTTP/1.1 200 Connection established\\r\\n\\r\\n')\n        p >> DataReceived(tctx.client, b\"GET / HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\")\n        p << layer.NextLayerHook(Placeholder())\n        p >> reply_next_layer(lambda ctx: http.HttpLayer(ctx, HTTPMode.transparent))\n    else:\n        p >> DataReceived(tctx.client, b\"GET http://example.com/ HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\")\n    p << http.HttpRequestHook(flow)\n    p >> reply(side_effect=redirect)\n    p << OpenConnection(server)\n    p >> reply(None)\n    p << SendData(server, b\"GET / HTTP/1.1\\r\\nHost: redirected.site\\r\\n\\r\\n\")\n    p >> DataReceived(server, b\"HTTP/1.1 200 OK\\r\\nContent-Length: 12\\r\\n\\r\\nHello World!\")\n    p << SendData(tctx.client, b\"HTTP/1.1 200 OK\\r\\nContent-Length: 12\\r\\n\\r\\nHello World!\")\n\n    assert p\n    if https_server:\n        assert server().address == (\"redirected.site\", 443)\n    else:\n        assert server().address == (\"redirected.site\", 80)\n\n\ndef test_multiple_server_connections(tctx):\n    \"\"\"Test multiple requests being rewritten to different targets.\"\"\"\n    server1 = Placeholder(Server)\n    server2 = Placeholder(Server)\n    playbook = Playbook(http.HttpLayer(tctx, HTTPMode.regular), hooks=False)\n\n    def redirect(to: str):\n        def side_effect(flow: HTTPFlow):\n            flow.request.url = to\n\n        return side_effect\n\n    assert (\n            playbook\n            >> DataReceived(tctx.client, b\"GET http://example.com/ HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\")\n            << http.HttpRequestHook(Placeholder())\n            >> reply(side_effect=redirect(\"http://one.redirect/\"))\n            << OpenConnection(server1)\n            >> reply(None)\n            << SendData(server1, b\"GET / HTTP/1.1\\r\\nHost: one.redirect\\r\\n\\r\\n\")\n            >> DataReceived(server1, b\"HTTP/1.1 200 OK\\r\\nContent-Length: 0\\r\\n\\r\\n\")\n            << SendData(tctx.client, b\"HTTP/1.1 200 OK\\r\\nContent-Length: 0\\r\\n\\r\\n\")\n    )\n    assert (\n            playbook\n            >> DataReceived(tctx.client, b\"GET http://example.com/ HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\")\n            << http.HttpRequestHook(Placeholder())\n            >> reply(side_effect=redirect(\"http://two.redirect/\"))\n            << OpenConnection(server2)\n            >> reply(None)\n            << SendData(server2, b\"GET / HTTP/1.1\\r\\nHost: two.redirect\\r\\n\\r\\n\")\n            >> DataReceived(server2, b\"HTTP/1.1 200 OK\\r\\nContent-Length: 0\\r\\n\\r\\n\")\n            << SendData(tctx.client, b\"HTTP/1.1 200 OK\\r\\nContent-Length: 0\\r\\n\\r\\n\")\n    )\n    assert server1().address == (\"one.redirect\", 80)\n    assert server2().address == (\"two.redirect\", 80)\n\n\n@pytest.mark.parametrize(\"transfer_encoding\", [\"identity\", \"chunked\"])\ndef test_pipelining(tctx, transfer_encoding):\n    \"\"\"Test that multiple requests can be processed over the same connection\"\"\"\n\n    tctx.server.address = (\"example.com\", 80)\n    tctx.server.state = ConnectionState.OPEN\n\n    req = b\"GET / HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\"\n    if transfer_encoding == \"identity\":\n        resp = (b\"HTTP/1.1 200 OK\\r\\n\"\n                b\"Content-Length: 12\\r\\n\"\n                b\"\\r\\n\"\n                b\"Hello World!\")\n    else:\n        resp = (b\"HTTP/1.1 200 OK\\r\\n\"\n                b\"Transfer-Encoding: chunked\\r\\n\"\n                b\"\\r\\n\"\n                b\"c\\r\\n\"\n                b\"Hello World!\\r\\n\"\n                b\"0\\r\\n\"\n                b\"\\r\\n\")\n\n    assert (\n            Playbook(http.HttpLayer(tctx, HTTPMode.transparent), hooks=False)\n            # Roundtrip 1\n            >> DataReceived(tctx.client, req)\n            << SendData(tctx.server, req)\n            >> DataReceived(tctx.server, resp)\n            << SendData(tctx.client, resp)\n            # Roundtrip 2\n            >> DataReceived(tctx.client, req)\n            << SendData(tctx.server, req)\n            >> DataReceived(tctx.server, resp)\n            << SendData(tctx.client, resp)\n    )\n\n\ndef test_http_reply_from_proxy(tctx):\n    \"\"\"Test a response served by mitmproxy itself.\"\"\"\n\n    def reply_from_proxy(flow: HTTPFlow):\n        flow.response = Response.make(418)\n\n    assert (\n            Playbook(http.HttpLayer(tctx, HTTPMode.regular), hooks=False)\n            >> DataReceived(tctx.client, b\"GET http://example.com/ HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\")\n            << http.HttpRequestHook(Placeholder())\n            >> reply(side_effect=reply_from_proxy)\n            << SendData(tctx.client, b\"HTTP/1.1 418 I'm a teapot\\r\\ncontent-length: 0\\r\\n\\r\\n\")\n    )\n\n\ndef test_response_until_eof(tctx):\n    \"\"\"Test scenario where the server response body is terminated by EOF.\"\"\"\n    server = Placeholder(Server)\n    assert (\n            Playbook(http.HttpLayer(tctx, HTTPMode.regular), hooks=False)\n            >> DataReceived(tctx.client, b\"GET http://example.com/ HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\")\n            << OpenConnection(server)\n            >> reply(None)\n            << SendData(server, b\"GET / HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\")\n            >> DataReceived(server, b\"HTTP/1.1 200 OK\\r\\n\\r\\nfoo\")\n            >> ConnectionClosed(server)\n            << CloseConnection(server)\n            << SendData(tctx.client, b\"HTTP/1.1 200 OK\\r\\n\\r\\nfoo\")\n            << CloseConnection(tctx.client)\n    )\n\n\ndef test_disconnect_while_intercept(tctx):\n    \"\"\"Test a server disconnect while a request is intercepted.\"\"\"\n    tctx.options.connection_strategy = \"eager\"\n\n    server1 = Placeholder(Server)\n    server2 = Placeholder(Server)\n    flow = Placeholder(HTTPFlow)\n\n    assert (\n            Playbook(http.HttpLayer(tctx, HTTPMode.regular), hooks=False)\n            >> DataReceived(tctx.client, b\"CONNECT example.com:80 HTTP/1.1\\r\\n\\r\\n\")\n            << http.HttpConnectHook(Placeholder(HTTPFlow))\n            >> reply()\n            << OpenConnection(server1)\n            >> reply(None)\n            << SendData(tctx.client, b'HTTP/1.1 200 Connection established\\r\\n\\r\\n')\n            >> DataReceived(tctx.client, b\"GET / HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\")\n            << layer.NextLayerHook(Placeholder())\n            >> reply_next_layer(lambda ctx: http.HttpLayer(ctx, HTTPMode.transparent))\n            << http.HttpRequestHook(flow)\n            >> ConnectionClosed(server1)\n            << CloseConnection(server1)\n            >> reply(to=-3)\n            << OpenConnection(server2)\n            >> reply(None)\n            << SendData(server2, b\"GET / HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\")\n            >> DataReceived(server2, b\"HTTP/1.1 200 OK\\r\\nContent-Length: 0\\r\\n\\r\\n\")\n            << SendData(tctx.client, b\"HTTP/1.1 200 OK\\r\\nContent-Length: 0\\r\\n\\r\\n\")\n    )\n    assert server1() != server2()\n    assert flow().server_conn == server2()\n    assert not flow().live\n\n\n@pytest.mark.parametrize(\"why\", [\"body_size=0\", \"body_size=3\", \"addon\"])\n@pytest.mark.parametrize(\"transfer_encoding\", [\"identity\", \"chunked\"])\ndef test_response_streaming(tctx, why, transfer_encoding):\n    \"\"\"Test HTTP response streaming\"\"\"\n    server = Placeholder(Server)\n    flow = Placeholder(HTTPFlow)\n    playbook = Playbook(http.HttpLayer(tctx, HTTPMode.regular))\n\n    if why.startswith(\"body_size\"):\n        tctx.options.stream_large_bodies = why.replace(\"body_size=\", \"\")\n\n    def enable_streaming(flow: HTTPFlow):\n        if why == \"addon\":\n            flow.response.stream = True\n\n    assert (\n        playbook\n        >> DataReceived(tctx.client, b\"GET http://example.com/largefile HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\")\n        << http.HttpRequestHeadersHook(flow)\n        >> reply()\n        << http.HttpRequestHook(flow)\n        >> reply()\n        << OpenConnection(server)\n        >> reply(None)\n        << SendData(server, b\"GET /largefile HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\")\n        >> DataReceived(server, b\"HTTP/1.1 200 OK\\r\\n\")\n    )\n    assert flow().live\n    if transfer_encoding == \"identity\":\n        playbook >> DataReceived(server, b\"Content-Length: 6\\r\\n\\r\\n\"\n                                         b\"abc\")\n    else:\n        playbook >> DataReceived(server, b\"Transfer-Encoding: chunked\\r\\n\\r\\n\"\n                                         b\"3\\r\\nabc\\r\\n\")\n\n    playbook << http.HttpResponseHeadersHook(flow)\n    playbook >> reply(side_effect=enable_streaming)\n\n    if transfer_encoding == \"identity\":\n        playbook << SendData(tctx.client, b\"HTTP/1.1 200 OK\\r\\n\"\n                                          b\"Content-Length: 6\\r\\n\\r\\n\"\n                                          b\"abc\")\n        playbook >> DataReceived(server, b\"def\")\n        playbook << SendData(tctx.client, b\"def\")\n    else:\n        if why == \"body_size=3\":\n            playbook >> DataReceived(server, b\"3\\r\\ndef\\r\\n\")\n            playbook << SendData(tctx.client, b\"HTTP/1.1 200 OK\\r\\n\"\n                                              b\"Transfer-Encoding: chunked\\r\\n\\r\\n\"\n                                              b\"6\\r\\nabcdef\\r\\n\")\n        else:\n            playbook << SendData(tctx.client, b\"HTTP/1.1 200 OK\\r\\n\"\n                                              b\"Transfer-Encoding: chunked\\r\\n\\r\\n\"\n                                              b\"3\\r\\nabc\\r\\n\")\n            playbook >> DataReceived(server, b\"3\\r\\ndef\\r\\n\")\n            playbook << SendData(tctx.client, b\"3\\r\\ndef\\r\\n\")\n        playbook >> DataReceived(server, b\"0\\r\\n\\r\\n\")\n\n    playbook << http.HttpResponseHook(flow)\n    playbook >> reply()\n\n    if transfer_encoding == \"chunked\":\n        playbook << SendData(tctx.client, b\"0\\r\\n\\r\\n\")\n\n    assert playbook\n    assert not flow().live\n\n\ndef test_stream_modify(tctx):\n    \"\"\"Test HTTP stream modification\"\"\"\n    server = Placeholder(Server)\n    flow = Placeholder(HTTPFlow)\n\n    def enable_streaming(flow: HTTPFlow):\n        if flow.response is None:\n            flow.request.stream = lambda x: b\"[\" + x + b\"]\"\n        else:\n            flow.response.stream = lambda x: b\"[\" + x + b\"]\"\n\n    assert (\n        Playbook(http.HttpLayer(tctx, HTTPMode.regular))\n        >> DataReceived(tctx.client, b\"POST http://example.com/ HTTP/1.1\\r\\n\"\n                                     b\"Host: example.com\\r\\n\"\n                                     b\"Transfer-Encoding: chunked\\r\\n\\r\\n\"\n                                     b\"3\\r\\nabc\\r\\n\"\n                                     b\"0\\r\\n\\r\\n\")\n        << http.HttpRequestHeadersHook(flow)\n        >> reply(side_effect=enable_streaming)\n        << OpenConnection(server)\n        >> reply(None)\n        << SendData(server, b\"POST / HTTP/1.1\\r\\n\"\n                            b\"Host: example.com\\r\\n\"\n                            b\"Transfer-Encoding: chunked\\r\\n\\r\\n\"\n                            b\"5\\r\\n[abc]\\r\\n\"\n                            b\"2\\r\\n[]\\r\\n\")\n        << http.HttpRequestHook(flow)\n        >> reply()\n        << SendData(server, b\"0\\r\\n\\r\\n\")\n        >> DataReceived(server, b\"HTTP/1.1 200 OK\\r\\n\"\n                                b\"Transfer-Encoding: chunked\\r\\n\\r\\n\"\n                                b\"3\\r\\ndef\\r\\n\"\n                                b\"0\\r\\n\\r\\n\")\n        << http.HttpResponseHeadersHook(flow)\n        >> reply(side_effect=enable_streaming)\n        << SendData(tctx.client, b\"HTTP/1.1 200 OK\\r\\n\"\n                                 b\"Transfer-Encoding: chunked\\r\\n\\r\\n\"\n                                 b\"5\\r\\n[def]\\r\\n\"\n                                 b\"2\\r\\n[]\\r\\n\")\n        << http.HttpResponseHook(flow)\n        >> reply()\n        << SendData(tctx.client, b\"0\\r\\n\\r\\n\")\n    )\n\n\n@pytest.mark.parametrize(\"why\", [\"body_size=0\", \"body_size=3\", \"addon\"])\n@pytest.mark.parametrize(\"transfer_encoding\", [\"identity\", \"chunked\"])\n@pytest.mark.parametrize(\"response\", [\"normal response\", \"early response\", \"early close\", \"early kill\"])\ndef test_request_streaming(tctx, why, transfer_encoding, response):\n    \"\"\"\n    Test HTTP request streaming\n\n    This is a bit more contrived as we may receive server data while we are still sending the request.\n    \"\"\"\n    server = Placeholder(Server)\n    flow = Placeholder(HTTPFlow)\n    playbook = Playbook(http.HttpLayer(tctx, HTTPMode.regular))\n\n    if why.startswith(\"body_size\"):\n        tctx.options.stream_large_bodies = why.replace(\"body_size=\", \"\")\n\n    def enable_streaming(flow: HTTPFlow):\n        if why == \"addon\":\n            flow.request.stream = True\n\n    playbook >> DataReceived(tctx.client, b\"POST http://example.com/ HTTP/1.1\\r\\n\"\n                                          b\"Host: example.com\\r\\n\")\n    if transfer_encoding == \"identity\":\n        playbook >> DataReceived(tctx.client, b\"Content-Length: 9\\r\\n\\r\\n\"\n                                              b\"abc\")\n    else:\n        playbook >> DataReceived(tctx.client, b\"Transfer-Encoding: chunked\\r\\n\\r\\n\"\n                                              b\"3\\r\\nabc\\r\\n\")\n\n    playbook << http.HttpRequestHeadersHook(flow)\n    playbook >> reply(side_effect=enable_streaming)\n\n    needs_more_data_before_open = (why == \"body_size=3\" and transfer_encoding == \"chunked\")\n    if needs_more_data_before_open:\n        playbook >> DataReceived(tctx.client, b\"3\\r\\ndef\\r\\n\")\n\n    playbook << OpenConnection(server)\n    playbook >> reply(None)\n    playbook << SendData(server, b\"POST / HTTP/1.1\\r\\n\"\n                                 b\"Host: example.com\\r\\n\")\n\n    if transfer_encoding == \"identity\":\n        playbook << SendData(server, b\"Content-Length: 9\\r\\n\\r\\n\"\n                                     b\"abc\")\n        playbook >> DataReceived(tctx.client, b\"def\")\n        playbook << SendData(server, b\"def\")\n    else:\n        if needs_more_data_before_open:\n            playbook << SendData(server, b\"Transfer-Encoding: chunked\\r\\n\\r\\n\"\n                                         b\"6\\r\\nabcdef\\r\\n\")\n        else:\n            playbook << SendData(server, b\"Transfer-Encoding: chunked\\r\\n\\r\\n\"\n                                         b\"3\\r\\nabc\\r\\n\")\n            playbook >> DataReceived(tctx.client, b\"3\\r\\ndef\\r\\n\")\n            playbook << SendData(server, b\"3\\r\\ndef\\r\\n\")\n\n    if response == \"normal response\":\n        if transfer_encoding == \"identity\":\n            playbook >> DataReceived(tctx.client, b\"ghi\")\n            playbook << SendData(server, b\"ghi\")\n        else:\n            playbook >> DataReceived(tctx.client, b\"3\\r\\nghi\\r\\n0\\r\\n\\r\\n\")\n            playbook << SendData(server, b\"3\\r\\nghi\\r\\n\")\n\n        playbook << http.HttpRequestHook(flow)\n        playbook >> reply()\n        if transfer_encoding == \"chunked\":\n            playbook << SendData(server, b\"0\\r\\n\\r\\n\")\n        assert (\n            playbook\n            >> DataReceived(server, b\"HTTP/1.1 200 OK\\r\\nContent-Length: 0\\r\\n\\r\\n\")\n            << http.HttpResponseHeadersHook(flow)\n            >> reply()\n            << http.HttpResponseHook(flow)\n            >> reply()\n            << SendData(tctx.client, b\"HTTP/1.1 200 OK\\r\\nContent-Length: 0\\r\\n\\r\\n\")\n        )\n    elif response == \"early response\":\n        # We may receive a response before we have finished sending our request.\n        # We continue sending unless the server closes the connection.\n        # https://tools.ietf.org/html/rfc7231#section-6.5.11\n        assert (\n            playbook\n            >> DataReceived(server, b\"HTTP/1.1 413 Request Entity Too Large\\r\\nContent-Length: 0\\r\\n\\r\\n\")\n            << http.HttpResponseHeadersHook(flow)\n            >> reply()\n            << http.HttpResponseHook(flow)\n            >> reply()\n            << SendData(tctx.client, b\"HTTP/1.1 413 Request Entity Too Large\\r\\nContent-Length: 0\\r\\n\\r\\n\")\n        )\n        if transfer_encoding == \"identity\":\n            playbook >> DataReceived(tctx.client, b\"ghi\")\n            playbook << SendData(server, b\"ghi\")\n        else:\n            playbook >> DataReceived(tctx.client, b\"3\\r\\nghi\\r\\n0\\r\\n\\r\\n\")\n            playbook << SendData(server, b\"3\\r\\nghi\\r\\n\")\n        playbook << http.HttpRequestHook(flow)\n        playbook >> reply()\n        if transfer_encoding == \"chunked\":\n            playbook << SendData(server, b\"0\\r\\n\\r\\n\")\n        assert playbook\n    elif response == \"early close\":\n        assert (\n                playbook\n                >> DataReceived(server, b\"HTTP/1.1 413 Request Entity Too Large\\r\\nContent-Length: 0\\r\\n\\r\\n\")\n                << http.HttpResponseHeadersHook(flow)\n                >> reply()\n                << http.HttpResponseHook(flow)\n                >> reply()\n                << SendData(tctx.client, b\"HTTP/1.1 413 Request Entity Too Large\\r\\nContent-Length: 0\\r\\n\\r\\n\")\n                >> ConnectionClosed(server)\n                << CloseConnection(server)\n                << CloseConnection(tctx.client)\n        )\n    elif response == \"early kill\":\n        err = Placeholder(bytes)\n        assert (\n                playbook\n                >> ConnectionClosed(server)\n                << CloseConnection(server)\n                << http.HttpErrorHook(flow)\n                >> reply()\n                << SendData(tctx.client, err)\n                << CloseConnection(tctx.client)\n        )\n        assert b\"502 Bad Gateway\" in err()\n    else:  # pragma: no cover\n        assert False\n\n\n@pytest.mark.parametrize(\"where\", [\"request\", \"response\"])\n@pytest.mark.parametrize(\"transfer_encoding\", [\"identity\", \"chunked\"])\ndef test_body_size_limit(tctx, where, transfer_encoding):\n    \"\"\"Test HTTP request body_size_limit\"\"\"\n    tctx.options.body_size_limit = \"3\"\n    err = Placeholder(bytes)\n    flow = Placeholder(HTTPFlow)\n\n    if transfer_encoding == \"identity\":\n        body = b\"Content-Length: 6\\r\\n\\r\\nabcdef\"\n    else:\n        body = b\"Transfer-Encoding: chunked\\r\\n\\r\\n6\\r\\nabcdef\"\n\n    if where == \"request\":\n        assert (\n            Playbook(http.HttpLayer(tctx, HTTPMode.regular))\n            >> DataReceived(tctx.client, b\"POST http://example.com/ HTTP/1.1\\r\\n\"\n                                         b\"Host: example.com\\r\\n\" + body)\n            << http.HttpRequestHeadersHook(flow)\n            >> reply()\n            << http.HttpErrorHook(flow)\n            >> reply()\n            << SendData(tctx.client, err)\n            << CloseConnection(tctx.client)\n        )\n        assert b\"413 Payload Too Large\" in err()\n        assert b\"body_size_limit\" in err()\n        assert not flow().live\n    else:\n        server = Placeholder(Server)\n        assert (\n            Playbook(http.HttpLayer(tctx, HTTPMode.regular))\n            >> DataReceived(tctx.client, b\"GET http://example.com/ HTTP/1.1\\r\\n\"\n                                         b\"Host: example.com\\r\\n\\r\\n\")\n            << http.HttpRequestHeadersHook(flow)\n            >> reply()\n            << http.HttpRequestHook(flow)\n            >> reply()\n            << OpenConnection(server)\n            >> reply(None)\n            << SendData(server, b\"GET / HTTP/1.1\\r\\n\"\n                                b\"Host: example.com\\r\\n\\r\\n\")\n            >> DataReceived(server, b\"HTTP/1.1 200 OK\\r\\n\" + body)\n            << http.HttpResponseHeadersHook(flow)\n            >> reply()\n            << http.HttpErrorHook(flow)\n            >> reply()\n            << SendData(tctx.client, err)\n            << CloseConnection(tctx.client)\n            << CloseConnection(server)\n        )\n        assert b\"502 Bad Gateway\" in err()\n        assert b\"body_size_limit\" in err()\n        assert not flow().live\n\n\n@pytest.mark.parametrize(\"connect\", [True, False])\ndef test_server_unreachable(tctx, connect):\n    \"\"\"Test the scenario where the target server is unreachable.\"\"\"\n    tctx.options.connection_strategy = \"eager\"\n    server = Placeholder(Server)\n    flow = Placeholder(HTTPFlow)\n    err = Placeholder(bytes)\n    playbook = Playbook(http.HttpLayer(tctx, HTTPMode.regular), hooks=False)\n    if connect:\n        playbook >> DataReceived(tctx.client, b\"CONNECT example.com:443 HTTP/1.1\\r\\n\\r\\n\")\n    else:\n        playbook >> DataReceived(tctx.client, b\"GET http://example.com/ HTTP/1.1\\r\\n\\r\\n\")\n\n    playbook << OpenConnection(server)\n    playbook >> reply(\"Connection failed\")\n    if not connect:\n        # Our API isn't ideal here, there is no error hook for CONNECT requests currently.\n        # We could fix this either by having CONNECT request go through all our regular hooks,\n        # or by adding dedicated ok/error hooks.\n        playbook << http.HttpErrorHook(flow)\n        playbook >> reply()\n    playbook << SendData(tctx.client, err)\n    if not connect:\n        playbook << CloseConnection(tctx.client)\n\n    assert playbook\n    if not connect:\n        assert flow().error\n        assert not flow().live\n    assert b\"502 Bad Gateway\" in err()\n    assert b\"Connection failed\" in err()\n\n\n@pytest.mark.parametrize(\"data\", [\n    None,\n    b\"I don't speak HTTP.\",\n    b\"HTTP/1.1 200 OK\\r\\nContent-Length: 10\\r\\n\\r\\nweee\"\n])\ndef test_server_aborts(tctx, data):\n    \"\"\"Test the scenario where the server doesn't serve a response\"\"\"\n    server = Placeholder(Server)\n    flow = Placeholder(HTTPFlow)\n    err = Placeholder(bytes)\n    playbook = Playbook(http.HttpLayer(tctx, HTTPMode.regular), hooks=False)\n    assert (\n            playbook\n            >> DataReceived(tctx.client, b\"GET http://example.com/ HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\")\n            << OpenConnection(server)\n            >> reply(None)\n            << SendData(server, b\"GET / HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\")\n    )\n    if data:\n        playbook >> DataReceived(server, data)\n    assert (\n            playbook\n            >> ConnectionClosed(server)\n            << CloseConnection(server)\n            << http.HttpErrorHook(flow)\n            >> reply()\n            << SendData(tctx.client, err)\n            << CloseConnection(tctx.client)\n    )\n    assert flow().error\n    assert b\"502 Bad Gateway\" in err()\n    assert not flow().live\n\n\n@pytest.mark.parametrize(\"redirect\", [\"\", \"change-destination\", \"change-proxy\"])\n@pytest.mark.parametrize(\"scheme\", [\"http\", \"https\"])\ndef test_upstream_proxy(tctx, redirect, scheme):\n    \"\"\"Test that an upstream HTTP proxy is used.\"\"\"\n    server = Placeholder(Server)\n    server2 = Placeholder(Server)\n    flow = Placeholder(HTTPFlow)\n    tctx.options.mode = \"upstream:http://proxy:8080\"\n    playbook = Playbook(http.HttpLayer(tctx, HTTPMode.upstream), hooks=False)\n\n    if scheme == \"http\":\n        assert (\n                playbook\n                >> DataReceived(tctx.client, b\"GET http://example.com/ HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\")\n                << OpenConnection(server)\n                >> reply(None)\n                << SendData(server, b\"GET http://example.com/ HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\")\n        )\n\n    else:\n        assert (\n                playbook\n                >> DataReceived(tctx.client, b\"CONNECT example.com:443 HTTP/1.1\\r\\n\\r\\n\")\n                << SendData(tctx.client, b\"HTTP/1.1 200 Connection established\\r\\n\\r\\n\")\n                >> DataReceived(tctx.client, b\"GET / HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\")\n                << layer.NextLayerHook(Placeholder())\n                >> reply_next_layer(lambda ctx: http.HttpLayer(ctx, HTTPMode.transparent))\n                << OpenConnection(server)\n                >> reply(None)\n                << SendData(server, b\"CONNECT example.com:443 HTTP/1.1\\r\\n\\r\\n\")\n                >> DataReceived(server, b\"HTTP/1.1 200 Connection established\\r\\n\\r\\n\")\n                << SendData(server, b\"GET / HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\")\n        )\n\n    playbook >> DataReceived(server, b\"HTTP/1.1 418 OK\\r\\nContent-Length: 0\\r\\n\\r\\n\")\n    playbook << SendData(tctx.client, b\"HTTP/1.1 418 OK\\r\\nContent-Length: 0\\r\\n\\r\\n\")\n\n    assert playbook\n    assert server().address == (\"proxy\", 8080)\n\n    if scheme == \"http\":\n        playbook >> DataReceived(tctx.client, b\"GET http://example.com/two HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\")\n    else:\n        playbook >> DataReceived(tctx.client, b\"GET /two HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\")\n\n    assert (playbook << http.HttpRequestHook(flow))\n    if redirect == \"change-destination\":\n        flow().request.host = \"other-server\"\n        flow().request.host_header = \"example.com\"\n    elif redirect == \"change-proxy\":\n        flow().server_conn.via = ServerSpec(\"http\", address=(\"other-proxy\", 1234))\n    playbook >> reply()\n\n    if redirect:\n        # Protocol-wise we wouldn't need to open a new connection for plain http host redirects,\n        # but we disregard this edge case to simplify implementation.\n        playbook << OpenConnection(server2)\n        playbook >> reply(None)\n    else:\n        server2 = server\n\n    if scheme == \"http\":\n        if redirect == \"change-destination\":\n            playbook << SendData(server2, b\"GET http://other-server/two HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\")\n        else:\n            playbook << SendData(server2, b\"GET http://example.com/two HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\")\n    else:\n        if redirect == \"change-destination\":\n            playbook << SendData(server2, b\"CONNECT other-server:443 HTTP/1.1\\r\\n\\r\\n\")\n            playbook >> DataReceived(server2, b\"HTTP/1.1 200 Connection established\\r\\n\\r\\n\")\n        elif redirect == \"change-proxy\":\n            playbook << SendData(server2, b\"CONNECT example.com:443 HTTP/1.1\\r\\n\\r\\n\")\n            playbook >> DataReceived(server2, b\"HTTP/1.1 200 Connection established\\r\\n\\r\\n\")\n        playbook << SendData(server2, b\"GET /two HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\")\n\n    playbook >> DataReceived(server2, b\"HTTP/1.1 418 OK\\r\\nContent-Length: 0\\r\\n\\r\\n\")\n    playbook << SendData(tctx.client, b\"HTTP/1.1 418 OK\\r\\nContent-Length: 0\\r\\n\\r\\n\")\n\n    assert playbook\n\n    if redirect == \"change-destination\":\n        assert flow().server_conn.address[0] == \"other-server\"\n    else:\n        assert flow().server_conn.address[0] == \"example.com\"\n\n    if redirect == \"change-proxy\":\n        assert server2().address == flow().server_conn.via.address == (\"other-proxy\", 1234)\n    else:\n        assert server2().address == flow().server_conn.via.address == (\"proxy\", 8080)\n\n    assert (\n            playbook\n            >> ConnectionClosed(tctx.client)\n            << CloseConnection(tctx.client)\n    )\n\n\n@pytest.mark.parametrize(\"mode\", [\"regular\", \"upstream\"])\n@pytest.mark.parametrize(\"close_first\", [\"client\", \"server\"])\ndef test_http_proxy_tcp(tctx, mode, close_first):\n    \"\"\"Test TCP over HTTP CONNECT.\"\"\"\n    server = Placeholder(Server)\n    f = Placeholder(TCPFlow)\n    tctx.options.connection_strategy = \"lazy\"\n\n    if mode == \"upstream\":\n        tctx.options.mode = \"upstream:http://proxy:8080\"\n        toplayer = http.HttpLayer(tctx, HTTPMode.upstream)\n    else:\n        tctx.options.mode = \"regular\"\n        toplayer = http.HttpLayer(tctx, HTTPMode.regular)\n\n    playbook = Playbook(toplayer, hooks=False)\n    assert (\n            playbook\n            >> DataReceived(tctx.client, b\"CONNECT example:443 HTTP/1.1\\r\\n\\r\\n\")\n            << SendData(tctx.client, b\"HTTP/1.1 200 Connection established\\r\\n\\r\\n\")\n            >> DataReceived(tctx.client, b\"this is not http\")\n            << layer.NextLayerHook(Placeholder())\n            >> reply_next_layer(lambda ctx: TCPLayer(ctx, ignore=False))\n            << TcpStartHook(f)\n            >> reply()\n            << OpenConnection(server)\n    )\n\n    playbook >> reply(None)\n    if mode == \"upstream\":\n        playbook << SendData(server, b\"CONNECT example:443 HTTP/1.1\\r\\n\\r\\n\")\n        playbook >> DataReceived(server, b\"HTTP/1.1 200 Connection established\\r\\n\\r\\n\")\n\n    assert (\n            playbook\n            << SendData(server, b\"this is not http\")\n            >> DataReceived(server, b\"true that\")\n            << SendData(tctx.client, b\"true that\")\n    )\n\n    if mode == \"regular\":\n        assert server().address == (\"example\", 443)\n    else:\n        assert server().address == (\"proxy\", 8080)\n\n    assert (\n        playbook\n        >> TcpMessageInjected(f, TCPMessage(False, b\"fake news from your friendly man-in-the-middle\"))\n        << SendData(tctx.client, b\"fake news from your friendly man-in-the-middle\")\n    )\n\n    if close_first == \"client\":\n        a, b = tctx.client, server\n    else:\n        a, b = server, tctx.client\n    assert (\n            playbook\n            >> ConnectionClosed(a)\n            << CloseConnection(b)\n            >> ConnectionClosed(b)\n            << CloseConnection(a)\n    )\n\n\n@pytest.mark.parametrize(\"strategy\", [\"eager\", \"lazy\"])\ndef test_proxy_chain(tctx, strategy):\n    server = Placeholder(Server)\n    tctx.options.connection_strategy = strategy\n    playbook = Playbook(http.HttpLayer(tctx, HTTPMode.regular), hooks=False)\n\n    playbook >> DataReceived(tctx.client, b\"CONNECT proxy:8080 HTTP/1.1\\r\\n\\r\\n\")\n    if strategy == \"eager\":\n        playbook << OpenConnection(server)\n        playbook >> reply(None)\n    playbook << SendData(tctx.client, b\"HTTP/1.1 200 Connection established\\r\\n\\r\\n\")\n\n    playbook >> DataReceived(tctx.client, b\"CONNECT second-proxy:8080 HTTP/1.1\\r\\n\\r\\n\")\n    playbook << layer.NextLayerHook(Placeholder())\n    playbook >> reply_next_layer(lambda ctx: http.HttpLayer(ctx, HTTPMode.transparent))\n    playbook << SendData(tctx.client,\n                         b\"HTTP/1.1 502 Bad Gateway\\r\\n\"\n                         b\"content-length: 198\\r\\n\"\n                         b\"\\r\\n\"\n                         b\"mitmproxy received an HTTP CONNECT request even though it is not running in regular/upstream mode. \"\n                         b\"This usually indicates a misconfiguration, please see the mitmproxy mode documentation for details.\")\n\n    assert playbook\n\n\ndef test_no_headers(tctx):\n    \"\"\"Test that we can correctly reassemble requests/responses with no headers.\"\"\"\n    server = Placeholder(Server)\n    assert (\n            Playbook(http.HttpLayer(tctx, HTTPMode.regular), hooks=False)\n            >> DataReceived(tctx.client, b\"GET http://example.com/ HTTP/1.1\\r\\n\\r\\n\")\n            << OpenConnection(server)\n            >> reply(None)\n            << SendData(server, b\"GET / HTTP/1.1\\r\\n\\r\\n\")\n            >> DataReceived(server, b\"HTTP/1.1 204 No Content\\r\\n\\r\\n\")\n            << SendData(tctx.client, b\"HTTP/1.1 204 No Content\\r\\n\\r\\n\")\n    )\n    assert server().address == (\"example.com\", 80)\n\n\ndef test_http_proxy_relative_request(tctx):\n    \"\"\"Test handling of a relative-form \"GET /\" in regular proxy mode.\"\"\"\n    server = Placeholder(Server)\n    assert (\n            Playbook(http.HttpLayer(tctx, HTTPMode.regular), hooks=False)\n            >> DataReceived(tctx.client, b\"GET / HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\")\n            << OpenConnection(server)\n            >> reply(None)\n            << SendData(server, b\"GET / HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\")\n            >> DataReceived(server, b\"HTTP/1.1 204 No Content\\r\\n\\r\\n\")\n            << SendData(tctx.client, b\"HTTP/1.1 204 No Content\\r\\n\\r\\n\")\n    )\n    assert server().address == (\"example.com\", 80)\n\n\ndef test_http_proxy_relative_request_no_host_header(tctx):\n    \"\"\"Test handling of a relative-form \"GET /\" in regular proxy mode, but without a host header.\"\"\"\n    err = Placeholder(bytes)\n    assert (\n            Playbook(http.HttpLayer(tctx, HTTPMode.regular), hooks=False)\n            >> DataReceived(tctx.client, b\"GET / HTTP/1.1\\r\\n\\r\\n\")\n            << SendData(tctx.client, err)\n            << CloseConnection(tctx.client)\n    )\n    assert b\"400 Bad Request\" in err()\n    assert b\"HTTP request has no host header, destination unknown.\" in err()\n\n\ndef test_http_expect(tctx):\n    \"\"\"Test handling of a 'Expect: 100-continue' header.\"\"\"\n    server = Placeholder(Server)\n    assert (\n            Playbook(http.HttpLayer(tctx, HTTPMode.regular), hooks=False)\n            >> DataReceived(tctx.client, b\"PUT http://example.com/large-file HTTP/1.1\\r\\n\"\n                                         b\"Host: example.com\\r\\n\"\n                                         b\"Content-Length: 15\\r\\n\"\n                                         b\"Expect: 100-continue\\r\\n\\r\\n\")\n            << SendData(tctx.client, b\"HTTP/1.1 100 Continue\\r\\n\\r\\n\")\n            >> DataReceived(tctx.client, b\"lots of content\")\n            << OpenConnection(server)\n            >> reply(None)\n            << SendData(server, b\"PUT /large-file HTTP/1.1\\r\\n\"\n                                b\"Host: example.com\\r\\n\"\n                                b\"Content-Length: 15\\r\\n\\r\\n\"\n                                b\"lots of content\")\n            >> DataReceived(server, b\"HTTP/1.1 201 Created\\r\\nContent-Length: 0\\r\\n\\r\\n\")\n            << SendData(tctx.client, b\"HTTP/1.1 201 Created\\r\\nContent-Length: 0\\r\\n\\r\\n\")\n    )\n    assert server().address == (\"example.com\", 80)\n\n\n@pytest.mark.parametrize(\"stream\", [True, False])\ndef test_http_client_aborts(tctx, stream):\n    \"\"\"Test handling of the case where a client aborts during request transmission.\"\"\"\n    server = Placeholder(Server)\n    flow = Placeholder(HTTPFlow)\n    playbook = Playbook(http.HttpLayer(tctx, HTTPMode.regular), hooks=True)\n\n    def enable_streaming(flow: HTTPFlow):\n        flow.request.stream = True\n\n    assert (\n            playbook\n            >> DataReceived(tctx.client, b\"POST http://example.com/ HTTP/1.1\\r\\n\"\n                                         b\"Host: example.com\\r\\n\"\n                                         b\"Content-Length: 6\\r\\n\"\n                                         b\"\\r\\n\"\n                                         b\"abc\")\n            << http.HttpRequestHeadersHook(flow)\n    )\n    if stream:\n        assert (\n                playbook\n                >> reply(side_effect=enable_streaming)\n                << OpenConnection(server)\n                >> reply(None)\n                << SendData(server, b\"POST / HTTP/1.1\\r\\n\"\n                                    b\"Host: example.com\\r\\n\"\n                                    b\"Content-Length: 6\\r\\n\"\n                                    b\"\\r\\n\"\n                                    b\"abc\")\n        )\n    else:\n        assert playbook >> reply()\n    (\n            playbook\n            >> ConnectionClosed(tctx.client)\n            << CloseConnection(tctx.client)\n    )\n    if stream:\n        playbook << CloseConnection(server)\n    assert (\n            playbook\n            << http.HttpErrorHook(flow)\n            >> reply()\n            << None\n    )\n\n    assert \"peer closed connection\" in flow().error.msg\n    assert not flow().live\n\n\n@pytest.mark.parametrize(\"stream\", [True, False])\ndef test_http_server_aborts(tctx, stream):\n    \"\"\"Test handling of the case where a server aborts during response transmission.\"\"\"\n    server = Placeholder(Server)\n    flow = Placeholder(HTTPFlow)\n    playbook = Playbook(http.HttpLayer(tctx, HTTPMode.regular))\n\n    def enable_streaming(flow: HTTPFlow):\n        flow.response.stream = True\n\n    assert (\n            playbook\n            >> DataReceived(tctx.client, b\"GET http://example.com/ HTTP/1.1\\r\\n\"\n                                         b\"Host: example.com\\r\\n\\r\\n\")\n            << http.HttpRequestHeadersHook(flow)\n            >> reply()\n            << http.HttpRequestHook(flow)\n            >> reply()\n            << OpenConnection(server)\n            >> reply(None)\n            << SendData(server, b\"GET / HTTP/1.1\\r\\n\"\n                                b\"Host: example.com\\r\\n\\r\\n\")\n            >> DataReceived(server, b\"HTTP/1.1 200 OK\\r\\n\"\n                                    b\"Content-Length: 6\\r\\n\"\n                                    b\"\\r\\n\"\n                                    b\"abc\")\n            << http.HttpResponseHeadersHook(flow)\n    )\n    if stream:\n        assert (\n                playbook\n                >> reply(side_effect=enable_streaming)\n                << SendData(tctx.client, b\"HTTP/1.1 200 OK\\r\\n\"\n                                         b\"Content-Length: 6\\r\\n\"\n                                         b\"\\r\\n\"\n                                         b\"abc\")\n        )\n    else:\n        assert playbook >> reply()\n    assert (\n            playbook\n            >> ConnectionClosed(server)\n            << CloseConnection(server)\n            << http.HttpErrorHook(flow)\n    )\n    if stream:\n        assert (\n                playbook\n                >> reply()\n                << CloseConnection(tctx.client)\n        )\n    else:\n        error_html = Placeholder(bytes)\n        assert (\n                playbook\n                >> reply()\n                << SendData(tctx.client, error_html)\n                << CloseConnection(tctx.client)\n        )\n        assert b\"502 Bad Gateway\" in error_html()\n        assert b\"peer closed connection\" in error_html()\n\n    assert \"peer closed connection\" in flow().error.msg\n    assert not flow().live\n\n\n@pytest.mark.parametrize(\"when\", [\"http_connect\", \"requestheaders\", \"request\", \"script-response-responseheaders\",\n                                  \"responseheaders\",\n                                  \"response\", \"error\"])\ndef test_kill_flow(tctx, when):\n    \"\"\"Test that we properly kill flows if instructed to do so\"\"\"\n    tctx.options.connection_strategy = \"lazy\"\n    server = Placeholder(Server)\n    connect_flow = Placeholder(HTTPFlow)\n    flow = Placeholder(HTTPFlow)\n\n    def kill(flow: HTTPFlow):\n        # Can't use flow.kill() here because that currently still depends on a reply object.\n        flow.error = Error(Error.KILLED_MESSAGE)\n\n    def assert_kill(err_hook: bool = True):\n        playbook >> reply(side_effect=kill)\n        if err_hook:\n            playbook << http.HttpErrorHook(flow)\n            playbook >> reply()\n        playbook << CloseConnection(tctx.client)\n        assert playbook\n        if flow():\n            assert not flow().live\n\n    playbook = Playbook(http.HttpLayer(tctx, HTTPMode.regular))\n    assert (playbook\n            >> DataReceived(tctx.client, b\"CONNECT example.com:80 HTTP/1.1\\r\\n\\r\\n\")\n            << http.HttpConnectHook(connect_flow))\n    if when == \"http_connect\":\n        return assert_kill(False)\n    assert (playbook\n            >> reply()\n            << SendData(tctx.client, b'HTTP/1.1 200 Connection established\\r\\n\\r\\n')\n            >> DataReceived(tctx.client, b\"GET /foo?hello=1 HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\")\n            << layer.NextLayerHook(Placeholder())\n            >> reply_next_layer(lambda ctx: http.HttpLayer(ctx, HTTPMode.transparent))\n            << http.HttpRequestHeadersHook(flow))\n    if when == \"requestheaders\":\n        return assert_kill()\n    assert (playbook\n            >> reply()\n            << http.HttpRequestHook(flow))\n    if when == \"request\":\n        return assert_kill()\n    if when == \"script-response-responseheaders\":\n        assert (playbook\n                >> reply(side_effect=lambda f: setattr(f, \"response\", Response.make()))\n                << http.HttpResponseHeadersHook(flow))\n        return assert_kill()\n    assert (playbook\n            >> reply()\n            << OpenConnection(server)\n            >> reply(None)\n            << SendData(server, b\"GET /foo?hello=1 HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\")\n            >> DataReceived(server, b\"HTTP/1.1 200 OK\\r\\nContent-Length: 12\\r\\n\\r\\nHello World\")\n            << http.HttpResponseHeadersHook(flow))\n    if when == \"responseheaders\":\n        return assert_kill()\n\n    if when == \"response\":\n        assert (playbook\n                >> reply()\n                >> DataReceived(server, b\"!\")\n                << http.HttpResponseHook(flow))\n        return assert_kill(False)\n    elif when == \"error\":\n        assert (playbook\n                >> reply()\n                >> ConnectionClosed(server)\n                << CloseConnection(server)\n                << http.HttpErrorHook(flow))\n        return assert_kill(False)\n    else:\n        raise AssertionError\n\n\ndef test_close_during_connect_hook(tctx):\n    flow = Placeholder(HTTPFlow)\n    assert (\n            Playbook(http.HttpLayer(tctx, HTTPMode.regular))\n            >> DataReceived(tctx.client,\n                            b'CONNECT hi.ls:443 HTTP/1.1\\r\\n'\n                            b'Proxy-Connection: keep-alive\\r\\n'\n                            b'Connection: keep-alive\\r\\n'\n                            b'Host: hi.ls:443\\r\\n\\r\\n')\n            << http.HttpConnectHook(flow)\n            >> ConnectionClosed(tctx.client)\n            << CloseConnection(tctx.client)\n            >> reply(to=-3)\n    )\n\n\n@pytest.mark.parametrize(\"client_close\", [b\"\", b\"Connection: close\\r\\n\"])\n@pytest.mark.parametrize(\"server_close\", [b\"\", b\"Connection: close\\r\\n\"])\ndef test_connection_close_header(tctx, client_close, server_close):\n    \"\"\"Test that we correctly close connections if we have a `Connection: close` header.\"\"\"\n    if not client_close and not server_close:\n        return\n    server = Placeholder(Server)\n    assert (\n            Playbook(http.HttpLayer(tctx, HTTPMode.regular), hooks=False)\n            >> DataReceived(tctx.client, b\"GET http://example/ HTTP/1.1\\r\\n\"\n                                         b\"Host: example\\r\\n\" + client_close +\n                            b\"\\r\\n\")\n            << OpenConnection(server)\n            >> reply(None)\n            << SendData(server, b\"GET / HTTP/1.1\\r\\n\"\n                                b\"Host: example\\r\\n\" + client_close +\n                        b\"\\r\\n\")\n            >> DataReceived(server, b\"HTTP/1.1 200 OK\\r\\n\"\n                                    b\"Content-Length: 0\\r\\n\" + server_close +\n                            b\"\\r\\n\")\n            << CloseConnection(server)\n            << SendData(tctx.client, b\"HTTP/1.1 200 OK\\r\\n\"\n                                     b\"Content-Length: 0\\r\\n\" + server_close +\n                        b\"\\r\\n\")\n            << CloseConnection(tctx.client)\n    )\n\n\n@pytest.mark.parametrize(\"proto\", [\"websocket\", \"tcp\", \"none\"])\ndef test_upgrade(tctx, proto):\n    \"\"\"Test a HTTP -> WebSocket upgrade with different protocols enabled\"\"\"\n    if proto != \"websocket\":\n        tctx.options.websocket = False\n    if proto != \"tcp\":\n        tctx.options.rawtcp = False\n\n    tctx.server.address = (\"example.com\", 80)\n    tctx.server.state = ConnectionState.OPEN\n    http_flow = Placeholder(HTTPFlow)\n    playbook = Playbook(http.HttpLayer(tctx, HTTPMode.transparent))\n    (\n            playbook\n            >> DataReceived(tctx.client,\n                            b\"GET / HTTP/1.1\\r\\n\"\n                            b\"Connection: upgrade\\r\\n\"\n                            b\"Upgrade: websocket\\r\\n\"\n                            b\"Sec-WebSocket-Version: 13\\r\\n\"\n                            b\"\\r\\n\")\n            << http.HttpRequestHeadersHook(http_flow)\n            >> reply()\n            << http.HttpRequestHook(http_flow)\n            >> reply()\n            << SendData(tctx.server, b\"GET / HTTP/1.1\\r\\n\"\n                                     b\"Connection: upgrade\\r\\n\"\n                                     b\"Upgrade: websocket\\r\\n\"\n                                     b\"Sec-WebSocket-Version: 13\\r\\n\"\n                                     b\"\\r\\n\")\n            >> DataReceived(tctx.server, b\"HTTP/1.1 101 Switching Protocols\\r\\n\"\n                                         b\"Upgrade: websocket\\r\\n\"\n                                         b\"Connection: Upgrade\\r\\n\"\n                                         b\"\\r\\n\")\n            << http.HttpResponseHeadersHook(http_flow)\n            >> reply()\n            << http.HttpResponseHook(http_flow)\n            >> reply()\n            << SendData(tctx.client, b\"HTTP/1.1 101 Switching Protocols\\r\\n\"\n                                     b\"Upgrade: websocket\\r\\n\"\n                                     b\"Connection: Upgrade\\r\\n\"\n                                     b\"\\r\\n\")\n    )\n    if proto == \"websocket\":\n        assert playbook << WebsocketStartHook(http_flow)\n    elif proto == \"tcp\":\n        assert playbook << TcpStartHook(Placeholder(TCPFlow))\n    else:\n        assert (\n            playbook\n            << Log(\"Sent HTTP 101 response, but no protocol is enabled to upgrade to.\", \"warn\")\n            << CloseConnection(tctx.client)\n        )\n\n\ndef test_dont_reuse_closed(tctx):\n    \"\"\"Test that a closed connection is not reused.\"\"\"\n    server = Placeholder(Server)\n    server2 = Placeholder(Server)\n    assert (\n            Playbook(http.HttpLayer(tctx, HTTPMode.regular), hooks=False)\n            >> DataReceived(tctx.client, b\"GET http://example.com/ HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\")\n            << OpenConnection(server)\n            >> reply(None)\n            << SendData(server, b\"GET / HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\")\n            >> DataReceived(server, b\"HTTP/1.1 200 OK\\r\\nContent-Length: 0\\r\\n\\r\\n\")\n            << SendData(tctx.client, b\"HTTP/1.1 200 OK\\r\\nContent-Length: 0\\r\\n\\r\\n\")\n            >> ConnectionClosed(server)\n            << CloseConnection(server)\n            >> DataReceived(tctx.client, b\"GET http://example.com/two HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\")\n            << OpenConnection(server2)\n            >> reply(None)\n            << SendData(server2, b\"GET /two HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\")\n            >> DataReceived(server2, b\"HTTP/1.1 200 OK\\r\\nContent-Length: 0\\r\\n\\r\\n\")\n            << SendData(tctx.client, b\"HTTP/1.1 200 OK\\r\\nContent-Length: 0\\r\\n\\r\\n\")\n    )\n\n\ndef test_reuse_error(tctx):\n    \"\"\"Test that an errored connection is reused.\"\"\"\n    tctx.server.address = (\"example.com\", 443)\n    tctx.server.error = \"tls verify failed\"\n    error_html = Placeholder(bytes)\n    assert (\n            Playbook(http.HttpLayer(tctx, HTTPMode.transparent), hooks=False)\n            >> DataReceived(tctx.client, b\"GET / HTTP/1.1\\r\\n\\r\\n\")\n            << SendData(tctx.client, error_html)\n            << CloseConnection(tctx.client)\n    )\n    assert b\"502 Bad Gateway\" in error_html()\n    assert b\"tls verify failed\" in error_html()\n\n\ndef test_transparent_sni(tctx):\n    \"\"\"Test that we keep the SNI in lazy transparent mode.\"\"\"\n    tctx.client.sni = \"example.com\"\n    tctx.server.address = (\"192.0.2.42\", 443)\n    tctx.server.tls = True\n\n    flow = Placeholder(HTTPFlow)\n\n    server = Placeholder(Server)\n    assert (\n            Playbook(http.HttpLayer(tctx, HTTPMode.transparent))\n            >> DataReceived(tctx.client, b\"GET / HTTP/1.1\\r\\n\\r\\n\")\n            << http.HttpRequestHeadersHook(flow)\n            >> reply()\n            << http.HttpRequestHook(flow)\n            >> reply()\n            << OpenConnection(server)\n    )\n    assert server().address == (\"192.0.2.42\", 443)\n    assert server().sni == \"example.com\"\n\n\ndef test_original_server_disconnects(tctx):\n    \"\"\"Test that we correctly handle the case where the initial server conn is just closed.\"\"\"\n    tctx.server.state = ConnectionState.OPEN\n    assert (\n            Playbook(http.HttpLayer(tctx, HTTPMode.transparent))\n            >> ConnectionClosed(tctx.server)\n            << CloseConnection(tctx.server)\n    )\n\n\ndef test_request_smuggling(tctx):\n    \"\"\"Test that we reject request smuggling\"\"\"\n    err = Placeholder(bytes)\n    assert (\n        Playbook(http.HttpLayer(tctx, HTTPMode.regular), hooks=False)\n        >> DataReceived(tctx.client, b\"GET http://example.com/ HTTP/1.1\\r\\n\"\n                                     b\"Host: example.com\\r\\n\"\n                                     b\"Content-Length: 42\\r\\n\"\n                                     b\"Transfer-Encoding: chunked\\r\\n\\r\\n\")\n        << SendData(tctx.client, err)\n        << CloseConnection(tctx.client)\n    )\n    assert b\"Received both a Transfer-Encoding and a Content-Length header\" in err()\n\n\ndef test_request_smuggling_te_te(tctx):\n    \"\"\"Test that we reject transfer-encoding headers that are weird in some way\"\"\"\n    err = Placeholder(bytes)\n    assert (\n        Playbook(http.HttpLayer(tctx, HTTPMode.regular), hooks=False)\n        >> DataReceived(tctx.client, (\"GET http://example.com/ HTTP/1.1\\r\\n\"\n                                      \"Host: example.com\\r\\n\"\n                                      \"Transfer-Encoding: chun\u212aed\\r\\n\\r\\n\").encode())  # note the non-standard \"\u212a\"\n        << SendData(tctx.client, err)\n        << CloseConnection(tctx.client)\n    )\n    assert b\"Invalid transfer encoding\" in err()\n\n\ndef test_invalid_content_length(tctx):\n    \"\"\"Test that we still trigger flow hooks for requests with semantic errors\"\"\"\n    err = Placeholder(bytes)\n    flow = Placeholder(HTTPFlow)\n    assert (\n        Playbook(http.HttpLayer(tctx, HTTPMode.regular))\n        >> DataReceived(tctx.client, (\"GET http://example.com/ HTTP/1.1\\r\\n\"\n                                      \"Host: example.com\\r\\n\"\n                                      \"Content-Length: NaN\\r\\n\\r\\n\").encode())\n        << SendData(tctx.client, err)\n        << CloseConnection(tctx.client)\n        << http.HttpRequestHeadersHook(flow)\n        >> reply()\n        << http.HttpErrorHook(flow)\n        >> reply()\n    )\n    assert b\"Invalid Content-Length header\" in err()\n\n\ndef test_chunked_and_content_length_set_by_addon(tctx):\n    \"\"\"Test that we don't crash when an addon sets a transfer-encoding header\n\n    We reject a request with both transfer-encoding and content-length header to\n    thwart request smuggling, but if a user explicitly sets it we should not crash.\n    \"\"\"\n    server = Placeholder(Server)\n    flow = Placeholder(HTTPFlow)\n\n    def make_chunked(flow: HTTPFlow):\n        if flow.response:\n            flow.response.headers[\"Transfer-Encoding\"] = \"chunked\"\n        else:\n            flow.request.headers[\"Transfer-Encoding\"] = \"chunked\"\n\n    assert (\n            Playbook(http.HttpLayer(tctx, HTTPMode.regular))\n            >> DataReceived(tctx.client, b\"POST http://example.com/ HTTP/1.1\\r\\n\"\n                                         b\"Host: example.com\\r\\n\"\n                                         b\"Content-Length: 0\\r\\n\\r\\n\")\n            << http.HttpRequestHeadersHook(flow)\n            >> reply(side_effect=make_chunked)\n            << http.HttpRequestHook(flow)\n            >> reply()\n            << OpenConnection(server)\n            >> reply(None)\n            << SendData(server, b\"POST / HTTP/1.1\\r\\n\"\n                                b\"Host: example.com\\r\\n\"\n                                b\"Content-Length: 0\\r\\n\"\n                                b\"Transfer-Encoding: chunked\\r\\n\\r\\n\"\n                                b\"0\\r\\n\\r\\n\")\n            >> DataReceived(server, b\"HTTP/1.1 200 OK\\r\\nContent-Length: 0\\r\\n\\r\\n\")\n            << http.HttpResponseHeadersHook(flow)\n            >> reply()\n            << http.HttpResponseHook(flow)\n            >> reply(side_effect=make_chunked)\n            << SendData(tctx.client, b\"HTTP/1.1 200 OK\\r\\n\"\n                                     b\"Content-Length: 0\\r\\n\"\n                                     b\"Transfer-Encoding: chunked\\r\\n\\r\\n\"\n                                     b\"0\\r\\n\\r\\n\")\n    )\n\n\ndef test_connect_more_newlines(tctx):\n    \"\"\"Ignore superfluous \\r\\n in CONNECT request, https://github.com/mitmproxy/mitmproxy/issues/4870\"\"\"\n    server = Placeholder(Server)\n    playbook = Playbook(http.HttpLayer(tctx, HTTPMode.regular))\n    nl = Placeholder(layer.NextLayer)\n\n    assert (\n        playbook\n        >> DataReceived(tctx.client, b\"CONNECT example.com:80 HTTP/1.1\\r\\n\\r\\n\\r\\n\")\n        << http.HttpConnectHook(Placeholder())\n        >> reply()\n        << OpenConnection(server)\n        >> reply(None)\n        << SendData(tctx.client, b'HTTP/1.1 200 Connection established\\r\\n\\r\\n')\n        >> DataReceived(tctx.client, b\"\\x16\\x03\\x03\\x00\\xb3\\x01\\x00\\x00\\xaf\\x03\\x03\")\n        << layer.NextLayerHook(nl)\n    )\n    assert nl().data_client() == b\"\\x16\\x03\\x03\\x00\\xb3\\x01\\x00\\x00\\xaf\\x03\\x03\"\n", "from typing import List, Tuple\n\nimport h2.settings\nimport hpack\nimport hyperframe.frame\nimport pytest\nfrom h2.errors import ErrorCodes\n\nfrom mitmproxy.connection import ConnectionState, Server\nfrom mitmproxy.flow import Error\nfrom mitmproxy.http import HTTPFlow, Headers, Request\nfrom mitmproxy.net.http import status_codes\nfrom mitmproxy.proxy.commands import CloseConnection, Log, OpenConnection, SendData\nfrom mitmproxy.proxy.context import Context\nfrom mitmproxy.proxy.events import ConnectionClosed, DataReceived\nfrom mitmproxy.proxy.layers import http\nfrom mitmproxy.proxy.layers.http import HTTPMode\nfrom mitmproxy.proxy.layers.http._http2 import Http2Client, split_pseudo_headers\nfrom test.mitmproxy.proxy.layers.http.hyper_h2_test_helpers import FrameFactory\nfrom test.mitmproxy.proxy.tutils import Placeholder, Playbook, reply\n\nexample_request_headers = (\n    (b':method', b'GET'),\n    (b':scheme', b'http'),\n    (b':path', b'/'),\n    (b':authority', b'example.com'),\n)\n\nexample_response_headers = (\n    (b':status', b'200'),\n)\n\nexample_request_trailers = (\n    (b'req-trailer-a', b'a'),\n    (b'req-trailer-b', b'b')\n)\n\nexample_response_trailers = (\n    (b'resp-trailer-a', b'a'),\n    (b'resp-trailer-b', b'b')\n)\n\n\n@pytest.fixture\ndef open_h2_server_conn():\n    # this is a bit fake here (port 80, with alpn, but no tls - c'mon),\n    # but we don't want to pollute our tests with TLS handshakes.\n    s = Server((\"example.com\", 80))\n    s.state = ConnectionState.OPEN\n    s.alpn = b\"h2\"\n    return s\n\n\ndef decode_frames(data: bytes) -> List[hyperframe.frame.Frame]:\n    # swallow preamble\n    if data.startswith(b\"PRI * HTTP/2.0\"):\n        data = data[24:]\n    frames = []\n    while data:\n        f, length = hyperframe.frame.Frame.parse_frame_header(data[:9])\n        f.parse_body(memoryview(data[9:9 + length]))\n        frames.append(f)\n        data = data[9 + length:]\n    return frames\n\n\ndef start_h2_client(tctx: Context) -> Tuple[Playbook, FrameFactory]:\n    tctx.client.alpn = b\"h2\"\n    frame_factory = FrameFactory()\n\n    playbook = Playbook(http.HttpLayer(tctx, HTTPMode.regular))\n    assert (\n            playbook\n            << SendData(tctx.client, Placeholder())  # initial settings frame\n            >> DataReceived(tctx.client, frame_factory.preamble())\n            >> DataReceived(tctx.client, frame_factory.build_settings_frame({}, ack=True).serialize())\n    )\n    return playbook, frame_factory\n\n\ndef make_h2(open_connection: OpenConnection) -> None:\n    open_connection.connection.alpn = b\"h2\"\n\n\ndef test_simple(tctx):\n    playbook, cff = start_h2_client(tctx)\n    flow = Placeholder(HTTPFlow)\n    server = Placeholder(Server)\n    initial = Placeholder(bytes)\n    assert (\n            playbook\n            >> DataReceived(tctx.client,\n                            cff.build_headers_frame(example_request_headers, flags=[\"END_STREAM\"]).serialize())\n            << http.HttpRequestHeadersHook(flow)\n            >> reply()\n            << http.HttpRequestHook(flow)\n            >> reply()\n            << OpenConnection(server)\n            >> reply(None, side_effect=make_h2)\n            << SendData(server, initial)\n    )\n    frames = decode_frames(initial())\n    assert [type(x) for x in frames] == [\n        hyperframe.frame.SettingsFrame,\n        hyperframe.frame.HeadersFrame,\n    ]\n    sff = FrameFactory()\n    assert (\n            playbook\n            # a conforming h2 server would send settings first, we disregard this for now.\n            >> DataReceived(server, sff.build_headers_frame(example_response_headers).serialize())\n            << http.HttpResponseHeadersHook(flow)\n            >> reply()\n            >> DataReceived(server, sff.build_data_frame(b\"Hello, World!\", flags=[\"END_STREAM\"]).serialize())\n            << http.HttpResponseHook(flow)\n            >> reply()\n            << SendData(tctx.client,\n                        cff.build_headers_frame(example_response_headers).serialize() +\n                        cff.build_data_frame(b\"Hello, World!\").serialize() +\n                        cff.build_data_frame(b\"\", flags=[\"END_STREAM\"]).serialize())\n    )\n    assert flow().request.url == \"http://example.com/\"\n    assert flow().response.text == \"Hello, World!\"\n\n\n@pytest.mark.parametrize(\"stream\", [\"stream\", \"\"])\ndef test_response_trailers(tctx: Context, open_h2_server_conn: Server, stream):\n    playbook, cff = start_h2_client(tctx)\n    tctx.server = open_h2_server_conn\n    sff = FrameFactory()\n\n    def enable_streaming(flow: HTTPFlow):\n        flow.response.stream = bool(stream)\n\n    flow = Placeholder(HTTPFlow)\n    (\n        playbook\n        >> DataReceived(tctx.client,\n                        cff.build_headers_frame(example_request_headers, flags=[\"END_STREAM\"]).serialize())\n        << http.HttpRequestHeadersHook(flow)\n        >> reply()\n        << http.HttpRequestHook(flow)\n        >> reply()\n        << SendData(tctx.server, Placeholder(bytes))\n        # a conforming h2 server would send settings first, we disregard this for now.\n        >> DataReceived(tctx.server, sff.build_headers_frame(example_response_headers).serialize() +\n                        sff.build_data_frame(b\"Hello, World!\").serialize())\n        << http.HttpResponseHeadersHook(flow)\n        >> reply(side_effect=enable_streaming)\n    )\n    if stream:\n        playbook << SendData(\n            tctx.client,\n            cff.build_headers_frame(example_response_headers).serialize() +\n            cff.build_data_frame(b\"Hello, World!\").serialize()\n        )\n    assert (\n        playbook\n        >> DataReceived(tctx.server, sff.build_headers_frame(example_response_trailers, flags=[\"END_STREAM\"]).serialize())\n        << http.HttpResponseHook(flow)\n    )\n    assert flow().response.trailers\n    del flow().response.trailers[\"resp-trailer-a\"]\n    if stream:\n        assert (\n            playbook\n            >> reply()\n            << SendData(tctx.client,\n                        cff.build_headers_frame(example_response_trailers[1:], flags=[\"END_STREAM\"]).serialize())\n        )\n    else:\n        assert (\n            playbook\n            >> reply()\n            << SendData(tctx.client,\n                        cff.build_headers_frame(example_response_headers).serialize() +\n                        cff.build_data_frame(b\"Hello, World!\").serialize() +\n                        cff.build_headers_frame(example_response_trailers[1:], flags=[\"END_STREAM\"]).serialize()))\n\n\n@pytest.mark.parametrize(\"stream\", [\"stream\", \"\"])\ndef test_request_trailers(tctx: Context, open_h2_server_conn: Server, stream):\n    playbook, cff = start_h2_client(tctx)\n    tctx.server = open_h2_server_conn\n\n    def enable_streaming(flow: HTTPFlow):\n        flow.request.stream = bool(stream)\n\n    flow = Placeholder(HTTPFlow)\n    server_data1 = Placeholder(bytes)\n    server_data2 = Placeholder(bytes)\n    (\n        playbook\n        >> DataReceived(tctx.client,\n                        cff.build_headers_frame(example_request_headers).serialize() +\n                        cff.build_data_frame(b\"Hello, World!\").serialize()\n                        )\n        << http.HttpRequestHeadersHook(flow)\n        >> reply(side_effect=enable_streaming)\n    )\n    if stream:\n        playbook << SendData(tctx.server, server_data1)\n    assert (\n        playbook\n        >> DataReceived(tctx.client,\n                        cff.build_headers_frame(example_request_trailers, flags=[\"END_STREAM\"]).serialize())\n        << http.HttpRequestHook(flow)\n        >> reply()\n        << SendData(tctx.server, server_data2)\n    )\n    frames = decode_frames(server_data1.setdefault(b\"\") + server_data2())\n    assert [type(x) for x in frames] == [\n        hyperframe.frame.SettingsFrame,\n        hyperframe.frame.HeadersFrame,\n        hyperframe.frame.DataFrame,\n        hyperframe.frame.HeadersFrame,\n    ]\n\n\ndef test_upstream_error(tctx):\n    playbook, cff = start_h2_client(tctx)\n    flow = Placeholder(HTTPFlow)\n    server = Placeholder(Server)\n    err = Placeholder(bytes)\n    assert (\n            playbook\n            >> DataReceived(tctx.client,\n                            cff.build_headers_frame(example_request_headers, flags=[\"END_STREAM\"]).serialize())\n            << http.HttpRequestHeadersHook(flow)\n            >> reply()\n            << http.HttpRequestHook(flow)\n            >> reply()\n            << OpenConnection(server)\n            >> reply(\"oops server <> error\")\n            << http.HttpErrorHook(flow)\n            >> reply()\n            << SendData(tctx.client, err)\n    )\n    frames = decode_frames(err())\n    assert [type(x) for x in frames] == [\n        hyperframe.frame.HeadersFrame,\n        hyperframe.frame.DataFrame,\n    ]\n    d = frames[1]\n    assert isinstance(d, hyperframe.frame.DataFrame)\n    assert b\"502 Bad Gateway\" in d.data\n    assert b\"server &lt;&gt; error\" in d.data\n\n\n@pytest.mark.parametrize(\"stream\", [\"stream\", \"\"])\n@pytest.mark.parametrize(\"when\", [\"request\", \"response\"])\n@pytest.mark.parametrize(\"how\", [\"RST\", \"disconnect\", \"RST+disconnect\"])\ndef test_http2_client_aborts(tctx, stream, when, how):\n    \"\"\"\n    Test handling of the case where a client aborts during request or response transmission.\n\n    If the client aborts the request transmission, we must trigger an error hook,\n    if the client disconnects during response transmission, no error hook is triggered.\n    \"\"\"\n    server = Placeholder(Server)\n    flow = Placeholder(HTTPFlow)\n    playbook, cff = start_h2_client(tctx)\n    resp = Placeholder(bytes)\n\n    def enable_request_streaming(flow: HTTPFlow):\n        flow.request.stream = True\n\n    def enable_response_streaming(flow: HTTPFlow):\n        flow.response.stream = True\n\n    assert (\n            playbook\n            >> DataReceived(tctx.client, cff.build_headers_frame(example_request_headers).serialize())\n            << http.HttpRequestHeadersHook(flow)\n    )\n    if stream and when == \"request\":\n        assert (\n                playbook\n                >> reply(side_effect=enable_request_streaming)\n                << OpenConnection(server)\n                >> reply(None)\n                << SendData(server, b\"GET / HTTP/1.1\\r\\n\"\n                                    b\"Host: example.com\\r\\n\\r\\n\")\n        )\n    else:\n        assert playbook >> reply()\n\n    if when == \"request\":\n        if \"RST\" in how:\n            playbook >> DataReceived(tctx.client, cff.build_rst_stream_frame(1, ErrorCodes.CANCEL).serialize())\n        else:\n            playbook >> ConnectionClosed(tctx.client)\n            playbook << CloseConnection(tctx.client)\n\n        if stream:\n            playbook << CloseConnection(server)\n        playbook << http.HttpErrorHook(flow)\n        playbook >> reply()\n\n        if how == \"RST+disconnect\":\n            playbook >> ConnectionClosed(tctx.client)\n            playbook << CloseConnection(tctx.client)\n\n        assert playbook\n        assert \"stream reset\" in flow().error.msg or \"peer closed connection\" in flow().error.msg\n        return\n\n    assert (\n            playbook\n            >> DataReceived(tctx.client, cff.build_data_frame(b\"\", flags=[\"END_STREAM\"]).serialize())\n            << http.HttpRequestHook(flow)\n            >> reply()\n            << OpenConnection(server)\n            >> reply(None)\n            << SendData(server, b\"GET / HTTP/1.1\\r\\n\"\n                                b\"Host: example.com\\r\\n\\r\\n\")\n            >> DataReceived(server, b\"HTTP/1.1 200 OK\\r\\nContent-Length: 6\\r\\n\\r\\n123\")\n            << http.HttpResponseHeadersHook(flow)\n    )\n    if stream:\n        assert (\n                playbook\n                >> reply(side_effect=enable_response_streaming)\n                << SendData(tctx.client, resp)\n        )\n    else:\n        assert playbook >> reply()\n\n    if \"RST\" in how:\n        playbook >> DataReceived(tctx.client, cff.build_rst_stream_frame(1, ErrorCodes.CANCEL).serialize())\n    else:\n        playbook >> ConnectionClosed(tctx.client)\n        playbook << CloseConnection(tctx.client)\n\n    assert (\n            playbook\n            << CloseConnection(server)\n            << http.HttpErrorHook(flow)\n            >> reply()\n    )\n\n    if how == \"RST+disconnect\":\n        assert (\n                playbook\n                >> ConnectionClosed(tctx.client)\n                << CloseConnection(tctx.client)\n        )\n\n    if \"RST\" in how:\n        assert \"stream reset\" in flow().error.msg\n    else:\n        assert \"peer closed connection\" in flow().error.msg\n\n\n@pytest.mark.xfail(reason=\"inbound validation turned on to protect against request smuggling\")\n@pytest.mark.parametrize(\"normalize\", [True, False])\ndef test_no_normalization(tctx, normalize):\n    \"\"\"Test that we don't normalize headers when we just pass them through.\"\"\"\n    tctx.options.normalize_outbound_headers = normalize\n\n    server = Placeholder(Server)\n    flow = Placeholder(HTTPFlow)\n    playbook, cff = start_h2_client(tctx)\n\n    request_headers = list(example_request_headers) + [(b\"Should-Not-Be-Capitalized! \", b\" :) \")]\n    request_headers_lower = [(k.lower(), v) for (k, v) in request_headers]\n    response_headers = list(example_response_headers) + [(b\"Same\", b\"Here\")]\n    response_headers_lower = [(k.lower(), v) for (k, v) in response_headers]\n\n    initial = Placeholder(bytes)\n    assert (\n            playbook\n            >> DataReceived(tctx.client,\n                            cff.build_headers_frame(request_headers, flags=[\"END_STREAM\"]).serialize())\n            << http.HttpRequestHeadersHook(flow)\n            >> reply()\n            << http.HttpRequestHook(flow)\n            >> reply()\n            << OpenConnection(server)\n            >> reply(None, side_effect=make_h2)\n            << SendData(server, initial)\n    )\n    frames = decode_frames(initial())\n    assert [type(x) for x in frames] == [\n        hyperframe.frame.SettingsFrame,\n        hyperframe.frame.HeadersFrame,\n    ]\n    assert hpack.hpack.Decoder().decode(frames[1].data, True) == request_headers_lower if normalize else request_headers\n\n    sff = FrameFactory()\n    (\n            playbook\n            >> DataReceived(server, sff.build_headers_frame(response_headers, flags=[\"END_STREAM\"]).serialize())\n            << http.HttpResponseHeadersHook(flow)\n            >> reply()\n            << http.HttpResponseHook(flow)\n            >> reply()\n    )\n    if normalize:\n        playbook << Log(\"Lowercased 'Same' header as uppercase is not allowed with HTTP/2.\")\n    hdrs = response_headers_lower if normalize else response_headers\n    assert playbook << SendData(tctx.client, cff.build_headers_frame(hdrs, flags=[\"END_STREAM\"]).serialize())\n\n    assert flow().request.headers.fields == ((b\"Should-Not-Be-Capitalized! \", b\" :) \"),)\n    assert flow().response.headers.fields == ((b\"Same\", b\"Here\"),)\n\n\n@pytest.mark.parametrize(\"input,pseudo,headers\", [\n    ([(b\"foo\", b\"bar\")], {}, {\"foo\": \"bar\"}),\n    ([(b\":status\", b\"418\")], {b\":status\": b\"418\"}, {}),\n    ([(b\":status\", b\"418\"), (b\"foo\", b\"bar\")], {b\":status\": b\"418\"}, {\"foo\": \"bar\"}),\n])\ndef test_split_pseudo_headers(input, pseudo, headers):\n    actual_pseudo, actual_headers = split_pseudo_headers(input)\n    assert pseudo == actual_pseudo\n    assert Headers(**headers) == actual_headers\n\n\ndef test_split_pseudo_headers_err():\n    with pytest.raises(ValueError, match=\"Duplicate HTTP/2 pseudo header\"):\n        split_pseudo_headers([(b\":status\", b\"418\"), (b\":status\", b\"418\")])\n\n\ndef test_rst_then_close(tctx):\n    \"\"\"\n    Test that we properly handle the case of a client that first causes protocol errors and then disconnects.\n\n    Adapted from h2spec http2/5.1/5.\n    \"\"\"\n    playbook, cff = start_h2_client(tctx)\n    flow = Placeholder(HTTPFlow)\n    server = Placeholder(Server)\n\n    assert (\n            playbook\n            >> DataReceived(tctx.client,\n                            cff.build_headers_frame(example_request_headers, flags=[\"END_STREAM\"]).serialize())\n            << http.HttpRequestHeadersHook(flow)\n            >> reply()\n            << http.HttpRequestHook(flow)\n            >> reply()\n            << OpenConnection(server)\n            >> DataReceived(tctx.client, cff.build_data_frame(b\"unexpected data frame\").serialize())\n            << SendData(tctx.client, cff.build_rst_stream_frame(1, ErrorCodes.STREAM_CLOSED).serialize())\n            >> ConnectionClosed(tctx.client)\n            << CloseConnection(tctx.client)\n            >> reply(\"connection cancelled\", to=-5)\n            << http.HttpErrorHook(flow)\n            >> reply()\n    )\n    assert flow().error.msg == \"connection cancelled\"\n\n\ndef test_cancel_then_server_disconnect(tctx):\n    \"\"\"\n    Test that we properly handle the case of the following event sequence:\n        - client cancels a stream\n        - we start an error hook\n        - server disconnects\n        - error hook completes.\n    \"\"\"\n    playbook, cff = start_h2_client(tctx)\n    flow = Placeholder(HTTPFlow)\n    server = Placeholder(Server)\n\n    assert (\n            playbook\n            >> DataReceived(tctx.client,\n                            cff.build_headers_frame(example_request_headers, flags=[\"END_STREAM\"]).serialize())\n            << http.HttpRequestHeadersHook(flow)\n            >> reply()\n            << http.HttpRequestHook(flow)\n            >> reply()\n            << OpenConnection(server)\n            >> reply(None)\n            << SendData(server, b'GET / HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n')\n            >> DataReceived(tctx.client, cff.build_rst_stream_frame(1, ErrorCodes.CANCEL).serialize())\n            << CloseConnection(server)\n            << http.HttpErrorHook(flow)\n            >> reply()\n            >> ConnectionClosed(server)\n            << None\n    )\n\n\ndef test_cancel_during_response_hook(tctx):\n    \"\"\"\n    Test that we properly handle the case of the following event sequence:\n        - we receive a server response\n        - we trigger the response hook\n        - the client cancels the stream\n        - the response hook completes\n\n    Given that we have already triggered the response hook, we don't want to trigger the error hook.\n    \"\"\"\n    playbook, cff = start_h2_client(tctx)\n    flow = Placeholder(HTTPFlow)\n    server = Placeholder(Server)\n\n    assert (\n            playbook\n            >> DataReceived(tctx.client,\n                            cff.build_headers_frame(example_request_headers, flags=[\"END_STREAM\"]).serialize())\n            << http.HttpRequestHeadersHook(flow)\n            >> reply()\n            << http.HttpRequestHook(flow)\n            >> reply()\n            << OpenConnection(server)\n            >> reply(None)\n            << SendData(server, b'GET / HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n')\n            >> DataReceived(server, b\"HTTP/1.1 204 No Content\\r\\n\\r\\n\")\n            << http.HttpResponseHeadersHook(flow)\n            << CloseConnection(server)\n            >> reply(to=-2)\n            << http.HttpResponseHook(flow)\n            >> DataReceived(tctx.client, cff.build_rst_stream_frame(1, ErrorCodes.CANCEL).serialize())\n            >> reply(to=-2)\n    )\n\n\ndef test_stream_concurrency(tctx):\n    \"\"\"Test that we can send an intercepted request with a lower stream id than one that has already been sent.\"\"\"\n    playbook, cff = start_h2_client(tctx)\n    flow1 = Placeholder(HTTPFlow)\n    flow2 = Placeholder(HTTPFlow)\n\n    reqheadershook1 = http.HttpRequestHeadersHook(flow1)\n    reqheadershook2 = http.HttpRequestHeadersHook(flow2)\n    reqhook1 = http.HttpRequestHook(flow1)\n    reqhook2 = http.HttpRequestHook(flow2)\n\n    server = Placeholder(Server)\n    data_req1 = Placeholder(bytes)\n    data_req2 = Placeholder(bytes)\n\n    assert (playbook\n            >> DataReceived(\n                tctx.client,\n                cff.build_headers_frame(example_request_headers, flags=[\"END_STREAM\"], stream_id=1).serialize() +\n                cff.build_headers_frame(example_request_headers, flags=[\"END_STREAM\"], stream_id=3).serialize())\n            << reqheadershook1\n            << reqheadershook2\n            >> reply(to=reqheadershook1)\n            << reqhook1\n            >> reply(to=reqheadershook2)\n            << reqhook2\n            # req 2 overtakes 1 and we already have a reply:\n            >> reply(to=reqhook2)\n            << OpenConnection(server)\n            >> reply(None, side_effect=make_h2)\n            << SendData(server, data_req2)\n            >> reply(to=reqhook1)\n            << SendData(server, data_req1)\n            )\n    frames = decode_frames(data_req2())\n    assert [type(x) for x in frames] == [\n        hyperframe.frame.SettingsFrame,\n        hyperframe.frame.HeadersFrame,\n    ]\n    frames = decode_frames(data_req1())\n    assert [type(x) for x in frames] == [\n        hyperframe.frame.HeadersFrame,\n    ]\n\n\ndef test_max_concurrency(tctx):\n    playbook, cff = start_h2_client(tctx)\n    server = Placeholder(Server)\n    req1_bytes = Placeholder(bytes)\n    settings_ack_bytes = Placeholder(bytes)\n    req2_bytes = Placeholder(bytes)\n    playbook.hooks = False\n    sff = FrameFactory()\n\n    assert (\n            playbook\n            >> DataReceived(tctx.client,\n                            cff.build_headers_frame(example_request_headers, flags=[\"END_STREAM\"],\n                                                    stream_id=1).serialize())\n            << OpenConnection(server)\n            >> reply(None, side_effect=make_h2)\n            << SendData(server, req1_bytes)\n            >> DataReceived(server,\n                            sff.build_settings_frame(\n                                {h2.settings.SettingCodes.MAX_CONCURRENT_STREAMS: 1}).serialize())\n            << SendData(server, settings_ack_bytes)\n            >> DataReceived(tctx.client,\n                            cff.build_headers_frame(example_request_headers,\n                                                    flags=[\"END_STREAM\"],\n                                                    stream_id=3).serialize())\n            # Can't send it upstream yet, all streams in use!\n            >> DataReceived(server, sff.build_headers_frame(example_response_headers,\n                                                            flags=[\"END_STREAM\"],\n                                                            stream_id=1).serialize())\n            # But now we can!\n            << SendData(server, req2_bytes)\n            << SendData(tctx.client, Placeholder(bytes))\n            >> DataReceived(server, sff.build_headers_frame(example_response_headers,\n                                                            flags=[\"END_STREAM\"],\n                                                            stream_id=3).serialize())\n            << SendData(tctx.client, Placeholder(bytes))\n    )\n    settings, req1 = decode_frames(req1_bytes())\n    settings_ack, = decode_frames(settings_ack_bytes())\n    req2, = decode_frames(req2_bytes())\n\n    assert type(settings) == hyperframe.frame.SettingsFrame\n    assert type(req1) == hyperframe.frame.HeadersFrame\n    assert type(settings_ack) == hyperframe.frame.SettingsFrame\n    assert type(req2) == hyperframe.frame.HeadersFrame\n    assert req1.stream_id == 1\n    assert req2.stream_id == 3\n\n\ndef test_stream_concurrent_get_connection(tctx):\n    \"\"\"Test that an immediate second request for the same domain does not trigger a second connection attempt.\"\"\"\n    playbook, cff = start_h2_client(tctx)\n    playbook.hooks = False\n\n    server = Placeholder(Server)\n    data = Placeholder(bytes)\n\n    assert (playbook\n            >> DataReceived(tctx.client, cff.build_headers_frame(example_request_headers, flags=[\"END_STREAM\"],\n                                                                 stream_id=1).serialize())\n            << (o := OpenConnection(server))\n            >> DataReceived(tctx.client, cff.build_headers_frame(example_request_headers, flags=[\"END_STREAM\"],\n                                                                 stream_id=3).serialize())\n            >> reply(None, to=o, side_effect=make_h2)\n            << SendData(server, data)\n            )\n    frames = decode_frames(data())\n    assert [type(x) for x in frames] == [\n        hyperframe.frame.SettingsFrame,\n        hyperframe.frame.HeadersFrame,\n        hyperframe.frame.HeadersFrame,\n    ]\n\n\ndef test_kill_stream(tctx):\n    \"\"\"Test that we can kill individual streams.\"\"\"\n    playbook, cff = start_h2_client(tctx)\n    flow1 = Placeholder(HTTPFlow)\n    flow2 = Placeholder(HTTPFlow)\n\n    req_headers_hook_1 = http.HttpRequestHeadersHook(flow1)\n\n    def kill(flow: HTTPFlow):\n        # Can't use flow.kill() here because that currently still depends on a reply object.\n        flow.error = Error(Error.KILLED_MESSAGE)\n\n    server = Placeholder(Server)\n    data_req1 = Placeholder(bytes)\n\n    assert (playbook\n            >> DataReceived(\n                tctx.client,\n                cff.build_headers_frame(example_request_headers, flags=[\"END_STREAM\"], stream_id=1).serialize() +\n                cff.build_headers_frame(example_request_headers, flags=[\"END_STREAM\"], stream_id=3).serialize())\n            << req_headers_hook_1\n            << http.HttpRequestHeadersHook(flow2)\n            >> reply(side_effect=kill)\n            << http.HttpErrorHook(flow2)\n            >> reply()\n            << SendData(tctx.client, cff.build_rst_stream_frame(3, error_code=ErrorCodes.INTERNAL_ERROR).serialize())\n            >> reply(to=req_headers_hook_1)\n            << http.HttpRequestHook(flow1)\n            >> reply()\n            << OpenConnection(server)\n            >> reply(None, side_effect=make_h2)\n            << SendData(server, data_req1)\n            )\n    frames = decode_frames(data_req1())\n    assert [type(x) for x in frames] == [\n        hyperframe.frame.SettingsFrame,\n        hyperframe.frame.HeadersFrame,\n    ]\n\n\nclass TestClient:\n    def test_no_data_on_closed_stream(self, tctx):\n        frame_factory = FrameFactory()\n        req = Request.make(\"GET\", \"http://example.com/\")\n        resp = {\n            \":status\": 200\n        }\n        assert (\n                Playbook(Http2Client(tctx))\n                << SendData(tctx.server, Placeholder(bytes))  # preamble + initial settings frame\n                >> DataReceived(tctx.server, frame_factory.build_settings_frame({}, ack=True).serialize())\n                >> http.RequestHeaders(1, req, end_stream=True)\n                << SendData(tctx.server, b\"\\x00\\x00\\x06\\x01\\x05\\x00\\x00\\x00\\x01\\x82\\x86\\x84\\\\\\x81\\x07\")\n                >> http.RequestEndOfMessage(1)\n                >> DataReceived(tctx.server, frame_factory.build_headers_frame(resp).serialize())\n                << http.ReceiveHttp(Placeholder(http.ResponseHeaders))\n                >> http.RequestProtocolError(1, \"cancelled\", code=status_codes.CLIENT_CLOSED_REQUEST)\n                << SendData(tctx.server, frame_factory.build_rst_stream_frame(1, ErrorCodes.CANCEL).serialize())\n                >> DataReceived(tctx.server, frame_factory.build_data_frame(b\"foo\").serialize())\n                << SendData(tctx.server, frame_factory.build_rst_stream_frame(1, ErrorCodes.STREAM_CLOSED).serialize())\n        )  # important: no ResponseData event here!\n\n\ndef test_early_server_data(tctx):\n    playbook, cff = start_h2_client(tctx)\n    sff = FrameFactory()\n\n    tctx.server.address = (\"example.com\", 80)\n    tctx.server.state = ConnectionState.OPEN\n    tctx.server.alpn = b\"h2\"\n\n    flow = Placeholder(HTTPFlow)\n    server1 = Placeholder(bytes)\n    server2 = Placeholder(bytes)\n    assert (\n            playbook\n            >> DataReceived(tctx.client,\n                            cff.build_headers_frame(example_request_headers, flags=[\"END_STREAM\"]).serialize())\n            << http.HttpRequestHeadersHook(flow)\n            >> reply()\n            << (h := http.HttpRequestHook(flow))\n            # Surprise! We get data from the server before the request hook finishes.\n            >> DataReceived(tctx.server, sff.build_settings_frame({}).serialize())\n            << SendData(tctx.server, server1)\n            # Request hook finishes...\n            >> reply(to=h)\n            << SendData(tctx.server, server2)\n    )\n    assert [type(x) for x in decode_frames(server1())] == [\n        hyperframe.frame.SettingsFrame,\n        hyperframe.frame.SettingsFrame,\n    ]\n    assert [type(x) for x in decode_frames(server2())] == [\n        hyperframe.frame.HeadersFrame,\n    ]\n\n\ndef test_request_smuggling_cl(tctx):\n    playbook, cff = start_h2_client(tctx)\n    playbook.hooks = False\n    err = Placeholder(bytes)\n\n    headers = (\n        (b':method', b'POST'),\n        (b':scheme', b'http'),\n        (b':path', b'/'),\n        (b':authority', b'example.com'),\n        (b'content-length', b'3')\n    )\n\n    assert (\n            playbook\n            >> DataReceived(tctx.client,\n                            cff.build_headers_frame(headers).serialize())\n            >> DataReceived(tctx.client,\n                            cff.build_data_frame(b\"abcPOST / HTTP/1.1 ...\", flags=[\"END_STREAM\"]).serialize())\n            << SendData(tctx.client, err)\n            << CloseConnection(tctx.client)\n    )\n    assert b\"InvalidBodyLengthError\" in err()\n\n\ndef test_request_smuggling_te(tctx):\n    playbook, cff = start_h2_client(tctx)\n    playbook.hooks = False\n    err = Placeholder(bytes)\n\n    headers = (\n        (b':method', b'POST'),\n        (b':scheme', b'http'),\n        (b':path', b'/'),\n        (b':authority', b'example.com'),\n        (b'transfer-encoding', b'chunked')\n    )\n\n    assert (\n            playbook\n            >> DataReceived(tctx.client,\n                            cff.build_headers_frame(headers, flags=[\"END_STREAM\"]).serialize())\n            << SendData(tctx.client, err)\n            << CloseConnection(tctx.client)\n    )\n    assert b\"Connection-specific header field present\" in err()\n"], "fixing_code": ["import asyncio\nfrom typing import Dict, Optional, Tuple\n\nfrom mitmproxy import command, ctx, exceptions, flow, http, log, master, options, platform, tcp, websocket\nfrom mitmproxy.flow import Flow\nfrom mitmproxy.proxy import commands, events, server_hooks\nfrom mitmproxy.proxy import server\nfrom mitmproxy.proxy.layers.tcp import TcpMessageInjected\nfrom mitmproxy.proxy.layers.websocket import WebSocketMessageInjected\nfrom mitmproxy.utils import asyncio_utils, human\nfrom wsproto.frame_protocol import Opcode\n\n\nclass ProxyConnectionHandler(server.StreamConnectionHandler):\n    master: master.Master\n\n    def __init__(self, master, r, w, options):\n        self.master = master\n        super().__init__(r, w, options)\n        self.log_prefix = f\"{human.format_address(self.client.peername)}: \"\n\n    async def handle_hook(self, hook: commands.StartHook) -> None:\n        with self.timeout_watchdog.disarm():\n            # We currently only support single-argument hooks.\n            data, = hook.args()\n            await self.master.addons.handle_lifecycle(hook)\n            if isinstance(data, flow.Flow):\n                await data.wait_for_resume()\n\n    def log(self, message: str, level: str = \"info\") -> None:\n        x = log.LogEntry(self.log_prefix + message, level)\n        asyncio_utils.create_task(\n            self.master.addons.handle_lifecycle(log.AddLogHook(x)),\n            name=\"ProxyConnectionHandler.log\"\n        )\n\n\nclass Proxyserver:\n    \"\"\"\n    This addon runs the actual proxy server.\n    \"\"\"\n    server: Optional[asyncio.AbstractServer]\n    listen_port: int\n    master: master.Master\n    options: options.Options\n    is_running: bool\n    _connections: Dict[Tuple, ProxyConnectionHandler]\n\n    def __init__(self):\n        self._lock = asyncio.Lock()\n        self.server = None\n        self.is_running = False\n        self._connections = {}\n\n    def __repr__(self):\n        return f\"ProxyServer({'running' if self.server else 'stopped'}, {len(self._connections)} active conns)\"\n\n    def load(self, loader):\n        loader.add_option(\n            \"connection_strategy\", str, \"eager\",\n            \"Determine when server connections should be established. When set to lazy, mitmproxy \"\n            \"tries to defer establishing an upstream connection as long as possible. This makes it possible to \"\n            \"use server replay while being offline. When set to eager, mitmproxy can detect protocols with \"\n            \"server-side greetings, as well as accurately mirror TLS ALPN negotiation.\",\n            choices=(\"eager\", \"lazy\")\n        )\n        loader.add_option(\n            \"stream_large_bodies\", Optional[str], None,\n            \"\"\"\n            Stream data to the client if response body exceeds the given\n            threshold. If streamed, the body will not be stored in any way.\n            Understands k/m/g suffixes, i.e. 3m for 3 megabytes.\n            \"\"\"\n        )\n        loader.add_option(\n            \"body_size_limit\", Optional[str], None,\n            \"\"\"\n            Byte size limit of HTTP request and response bodies. Understands\n            k/m/g suffixes, i.e. 3m for 3 megabytes.\n            \"\"\"\n        )\n        loader.add_option(\n            \"keep_host_header\", bool, False,\n            \"\"\"\n            Reverse Proxy: Keep the original host header instead of rewriting it\n            to the reverse proxy target.\n            \"\"\"\n        )\n        loader.add_option(\n            \"proxy_debug\", bool, False,\n            \"Enable debug logs in the proxy core.\",\n        )\n        loader.add_option(\n            \"normalize_outbound_headers\", bool, True,\n            \"\"\"\n            Normalize outgoing HTTP/2 header names, but emit a warning when doing so.\n            HTTP/2 does not allow uppercase header names. This option makes sure that HTTP/2 headers set\n            in custom scripts are lowercased before they are sent.\n            \"\"\",\n        )\n        loader.add_option(\n            \"validate_inbound_headers\", bool, True,\n            \"\"\"\n            Make sure that incoming HTTP requests are not malformed.\n            Disabling this option makes mitmproxy vulnerable to HTTP smuggling attacks.\n            \"\"\",\n        )\n\n    async def running(self):\n        self.master = ctx.master\n        self.options = ctx.options\n        self.is_running = True\n        await self.refresh_server()\n\n    def configure(self, updated):\n        if \"stream_large_bodies\" in updated:\n            try:\n                human.parse_size(ctx.options.stream_large_bodies)\n            except ValueError:\n                raise exceptions.OptionsError(f\"Invalid stream_large_bodies specification: \"\n                                              f\"{ctx.options.stream_large_bodies}\")\n        if \"body_size_limit\" in updated:\n            try:\n                human.parse_size(ctx.options.body_size_limit)\n            except ValueError:\n                raise exceptions.OptionsError(f\"Invalid body_size_limit specification: \"\n                                              f\"{ctx.options.body_size_limit}\")\n        if \"mode\" in updated and ctx.options.mode == \"transparent\":  # pragma: no cover\n            platform.init_transparent_mode()\n        if self.is_running and any(x in updated for x in [\"server\", \"listen_host\", \"listen_port\"]):\n            asyncio.create_task(self.refresh_server())\n\n    async def refresh_server(self):\n        async with self._lock:\n            if self.server:\n                await self.shutdown_server()\n                self.server = None\n            if ctx.options.server:\n                if not ctx.master.addons.get(\"nextlayer\"):\n                    ctx.log.warn(\"Warning: Running proxyserver without nextlayer addon!\")\n                try:\n                    self.server = await asyncio.start_server(\n                        self.handle_connection,\n                        self.options.listen_host,\n                        self.options.listen_port,\n                    )\n                except OSError as e:\n                    ctx.log.error(str(e))\n                    return\n                # TODO: This is a bit confusing currently for `-p 0`.\n                addrs = {f\"http://{human.format_address(s.getsockname())}\" for s in self.server.sockets}\n                ctx.log.info(f\"Proxy server listening at {' and '.join(addrs)}\")\n\n    async def shutdown_server(self):\n        ctx.log.info(\"Stopping server...\")\n        self.server.close()\n        await self.server.wait_closed()\n        self.server = None\n\n    async def handle_connection(self, r, w):\n        peername = w.get_extra_info('peername')\n        asyncio_utils.set_task_debug_info(\n            asyncio.current_task(),\n            name=f\"Proxyserver.handle_connection\",\n            client=peername,\n        )\n        handler = ProxyConnectionHandler(\n            self.master,\n            r,\n            w,\n            self.options\n        )\n        self._connections[peername] = handler\n        try:\n            await handler.handle_client()\n        finally:\n            del self._connections[peername]\n\n    def inject_event(self, event: events.MessageInjected):\n        if event.flow.client_conn.peername not in self._connections:\n            raise ValueError(\"Flow is not from a live connection.\")\n        self._connections[event.flow.client_conn.peername].server_event(event)\n\n    @command.command(\"inject.websocket\")\n    def inject_websocket(self, flow: Flow, to_client: bool, message: bytes, is_text: bool = True):\n        if not isinstance(flow, http.HTTPFlow) or not flow.websocket:\n            ctx.log.warn(\"Cannot inject WebSocket messages into non-WebSocket flows.\")\n\n        msg = websocket.WebSocketMessage(\n            Opcode.TEXT if is_text else Opcode.BINARY,\n            not to_client,\n            message\n        )\n        event = WebSocketMessageInjected(flow, msg)\n        try:\n            self.inject_event(event)\n        except ValueError as e:\n            ctx.log.warn(str(e))\n\n    @command.command(\"inject.tcp\")\n    def inject_tcp(self, flow: Flow, to_client: bool, message: bytes):\n        if not isinstance(flow, tcp.TCPFlow):\n            ctx.log.warn(\"Cannot inject TCP messages into non-TCP flows.\")\n\n        event = TcpMessageInjected(flow, tcp.TCPMessage(not to_client, message))\n        try:\n            self.inject_event(event)\n        except ValueError as e:\n            ctx.log.warn(str(e))\n\n    def server_connect(self, ctx: server_hooks.ServerConnectionHookData):\n        assert ctx.server.address\n        self_connect = (\n            ctx.server.address[1] == self.options.listen_port\n            and\n            ctx.server.address[0] in (\"localhost\", \"127.0.0.1\", \"::1\", self.options.listen_host)\n        )\n        if self_connect:\n            ctx.server.error = (\n                \"Request destination unknown. \"\n                \"Unable to figure out where this request should be forwarded to.\"\n            )\n", "from .read import (\n    read_request_head,\n    read_response_head,\n    connection_close,\n    expected_http_body_size,\n    validate_headers,\n)\nfrom .assemble import (\n    assemble_request, assemble_request_head,\n    assemble_response, assemble_response_head,\n    assemble_body,\n)\n\n\n__all__ = [\n    \"read_request_head\",\n    \"read_response_head\",\n    \"connection_close\",\n    \"expected_http_body_size\",\n    \"validate_headers\",\n    \"assemble_request\", \"assemble_request_head\",\n    \"assemble_response\", \"assemble_response_head\",\n    \"assemble_body\",\n]\n", "import re\nimport time\nfrom typing import List, Tuple, Iterable, Optional\n\nfrom mitmproxy.http import Request, Headers, Response\nfrom mitmproxy.net.http import url\n\n\ndef get_header_tokens(headers, key):\n    \"\"\"\n        Retrieve all tokens for a header key. A number of different headers\n        follow a pattern where each header line can containe comma-separated\n        tokens, and headers can be set multiple times.\n    \"\"\"\n    if key not in headers:\n        return []\n    tokens = headers[key].split(\",\")\n    return [token.strip() for token in tokens]\n\n\ndef connection_close(http_version, headers):\n    \"\"\"\n        Checks the message to see if the client connection should be closed\n        according to RFC 2616 Section 8.1.\n        If we don't have a Connection header, HTTP 1.1 connections are assumed\n        to be persistent.\n    \"\"\"\n    if \"connection\" in headers:\n        tokens = get_header_tokens(headers, \"connection\")\n        if \"close\" in tokens:\n            return True\n        elif \"keep-alive\" in tokens:\n            return False\n\n    return http_version not in (\n        \"HTTP/1.1\", b\"HTTP/1.1\",\n        \"HTTP/2.0\", b\"HTTP/2.0\",\n    )\n\n\n# https://datatracker.ietf.org/doc/html/rfc7230#section-3.2: Header fields are tokens.\n# \"!\" / \"#\" / \"$\" / \"%\" / \"&\" / \"'\" / \"*\" / \"+\" / \"-\" / \".\" /  \"^\" / \"_\" / \"`\" / \"|\" / \"~\" / DIGIT / ALPHA\n_valid_header_name = re.compile(rb\"^[!#$%&'*+\\-.^_`|~0-9a-zA-Z]+$\")\n\n\ndef validate_headers(\n    headers: Headers\n) -> None:\n    \"\"\"\n    Validate headers to avoid request smuggling attacks. Raises a ValueError if they are malformed.\n    \"\"\"\n\n    te_found = False\n    cl_found = False\n\n    for (name, value) in headers.fields:\n        if not _valid_header_name.match(name):\n            raise ValueError(f\"Received an invalid header name: {name!r}. Invalid header names may introduce \"\n                             f\"request smuggling vulnerabilities. Disable the validate_inbound_headers option \"\n                             f\"to skip this security check.\")\n\n        name_lower = name.lower()\n        te_found = te_found or name_lower == b\"transfer-encoding\"\n        cl_found = cl_found or name_lower == b\"content-length\"\n\n    if te_found and cl_found:\n        raise ValueError(\"Received both a Transfer-Encoding and a Content-Length header, \"\n                         \"refusing as recommended in RFC 7230 Section 3.3.3. \"\n                         \"See https://github.com/mitmproxy/mitmproxy/issues/4799 for details. \"\n                         \"Disable the validate_inbound_headers option to skip this security check.\")\n\n\ndef expected_http_body_size(\n        request: Request,\n        response: Optional[Response] = None\n) -> Optional[int]:\n    \"\"\"\n        Returns:\n            The expected body length:\n            - a positive integer, if the size is known in advance\n            - None, if the size in unknown in advance (chunked encoding)\n            - -1, if all data should be read until end of stream.\n\n        Raises:\n            ValueError, if the content length header is invalid\n    \"\"\"\n    # Determine response size according to http://tools.ietf.org/html/rfc7230#section-3.3, which is inlined below.\n    if not response:\n        headers = request.headers\n    else:\n        headers = response.headers\n\n        #    1.  Any response to a HEAD request and any response with a 1xx\n        #        (Informational), 204 (No Content), or 304 (Not Modified) status\n        #        code is always terminated by the first empty line after the\n        #        header fields, regardless of the header fields present in the\n        #        message, and thus cannot contain a message body.\n        if request.method.upper() == \"HEAD\":\n            return 0\n        if 100 <= response.status_code <= 199:\n            return 0\n        if response.status_code in (204, 304):\n            return 0\n\n        #    2.  Any 2xx (Successful) response to a CONNECT request implies that\n        #        the connection will become a tunnel immediately after the empty\n        #        line that concludes the header fields.  A client MUST ignore any\n        #        Content-Length or Transfer-Encoding header fields received in\n        #        such a message.\n        if 200 <= response.status_code <= 299 and request.method.upper() == \"CONNECT\":\n            return 0\n\n    #    3.  If a Transfer-Encoding header field is present and the chunked\n    #        transfer coding (Section 4.1) is the final encoding, the message\n    #        body length is determined by reading and decoding the chunked\n    #        data until the transfer coding indicates the data is complete.\n    #\n    #        If a Transfer-Encoding header field is present in a response and\n    #        the chunked transfer coding is not the final encoding, the\n    #        message body length is determined by reading the connection until\n    #        it is closed by the server.  If a Transfer-Encoding header field\n    #        is present in a request and the chunked transfer coding is not\n    #        the final encoding, the message body length cannot be determined\n    #        reliably; the server MUST respond with the 400 (Bad Request)\n    #        status code and then close the connection.\n    #\n    #        If a message is received with both a Transfer-Encoding and a\n    #        Content-Length header field, the Transfer-Encoding overrides the\n    #        Content-Length.  Such a message might indicate an attempt to\n    #        perform request smuggling (Section 9.5) or response splitting\n    #        (Section 9.4) and ought to be handled as an error.  A sender MUST\n    #        remove the received Content-Length field prior to forwarding such\n    #        a message downstream.\n    #\n    if \"transfer-encoding\" in headers:\n        # we should make sure that there isn't also a content-length header.\n        # this is already handled in validate_headers.\n\n        te: str = headers[\"transfer-encoding\"]\n        if not te.isascii():\n            # guard against .lower() transforming non-ascii to ascii\n            raise ValueError(f\"Invalid transfer encoding: {te!r}\")\n        te = te.lower().strip(\"\\t \")\n        te = re.sub(r\"[\\t ]*,[\\t ]*\", \",\", te)\n        if te in (\n            \"chunked\",\n            \"compress,chunked\",\n            \"deflate,chunked\",\n            \"gzip,chunked\",\n        ):\n            return None\n        elif te in (\n            \"compress\",\n            \"deflate\",\n            \"gzip\",\n            \"identity\",\n        ):\n            if response:\n                return -1\n            else:\n                raise ValueError(f\"Invalid request transfer encoding, message body cannot be determined reliably.\")\n        else:\n            raise ValueError(f\"Unknown transfer encoding: {headers['transfer-encoding']!r}\")\n\n    #    4.  If a message is received without Transfer-Encoding and with\n    #        either multiple Content-Length header fields having differing\n    #        field-values or a single Content-Length header field having an\n    #        invalid value, then the message framing is invalid and the\n    #        recipient MUST treat it as an unrecoverable error.  If this is a\n    #        request message, the server MUST respond with a 400 (Bad Request)\n    #        status code and then close the connection.  If this is a response\n    #        message received by a proxy, the proxy MUST close the connection\n    #        to the server, discard the received response, and send a 502 (Bad\n    #        Gateway) response to the client.  If this is a response message\n    #        received by a user agent, the user agent MUST close the\n    #        connection to the server and discard the received response.\n    #\n    #    5.  If a valid Content-Length header field is present without\n    #        Transfer-Encoding, its decimal value defines the expected message\n    #        body length in octets.  If the sender closes the connection or\n    #        the recipient times out before the indicated number of octets are\n    #        received, the recipient MUST consider the message to be\n    #        incomplete and close the connection.\n    if \"content-length\" in headers:\n        sizes = headers.get_all(\"content-length\")\n        different_content_length_headers = any(x != sizes[0] for x in sizes)\n        if different_content_length_headers:\n            raise ValueError(f\"Conflicting Content-Length headers: {sizes!r}\")\n        try:\n            size = int(sizes[0])\n        except ValueError:\n            raise ValueError(f\"Invalid Content-Length header: {sizes[0]!r}\")\n        if size < 0:\n            raise ValueError(f\"Negative Content-Length header: {sizes[0]!r}\")\n        return size\n\n    #    6.  If this is a request message and none of the above are true, then\n    #        the message body length is zero (no message body is present).\n    if not response:\n        return 0\n\n    #    7.  Otherwise, this is a response message without a declared message\n    #        body length, so the message body length is determined by the\n    #        number of octets received prior to the server closing the\n    #        connection.\n    return -1\n\n\ndef raise_if_http_version_unknown(http_version: bytes) -> None:\n    if not re.match(br\"^HTTP/\\d\\.\\d$\", http_version):\n        raise ValueError(f\"Unknown HTTP version: {http_version!r}\")\n\n\ndef _read_request_line(line: bytes) -> Tuple[str, int, bytes, bytes, bytes, bytes, bytes]:\n    try:\n        method, target, http_version = line.split()\n        port: Optional[int]\n\n        if target == b\"*\" or target.startswith(b\"/\"):\n            scheme, authority, path = b\"\", b\"\", target\n            host, port = \"\", 0\n        elif method == b\"CONNECT\":\n            scheme, authority, path = b\"\", target, b\"\"\n            host, port = url.parse_authority(authority, check=True)\n            if not port:\n                raise ValueError\n        else:\n            scheme, rest = target.split(b\"://\", maxsplit=1)\n            authority, _, path_ = rest.partition(b\"/\")\n            path = b\"/\" + path_\n            host, port = url.parse_authority(authority, check=True)\n            port = port or url.default_port(scheme)\n            if not port:\n                raise ValueError\n            # TODO: we can probably get rid of this check?\n            url.parse(target)\n\n        raise_if_http_version_unknown(http_version)\n    except ValueError as e:\n        raise ValueError(f\"Bad HTTP request line: {line!r}\") from e\n\n    return host, port, method, scheme, authority, path, http_version\n\n\ndef _read_response_line(line: bytes) -> Tuple[bytes, int, bytes]:\n    try:\n        parts = line.split(None, 2)\n        if len(parts) == 2:  # handle missing message gracefully\n            parts.append(b\"\")\n\n        http_version, status_code_str, reason = parts\n        status_code = int(status_code_str)\n        raise_if_http_version_unknown(http_version)\n    except ValueError as e:\n        raise ValueError(f\"Bad HTTP response line: {line!r}\") from e\n\n    return http_version, status_code, reason\n\n\ndef _read_headers(lines: Iterable[bytes]) -> Headers:\n    \"\"\"\n        Read a set of headers.\n        Stop once a blank line is reached.\n\n        Returns:\n            A headers object\n\n        Raises:\n            exceptions.HttpSyntaxException\n    \"\"\"\n    ret: List[Tuple[bytes, bytes]] = []\n    for line in lines:\n        if line[0] in b\" \\t\":\n            if not ret:\n                raise ValueError(\"Invalid headers\")\n            # continued header\n            ret[-1] = (ret[-1][0], ret[-1][1] + b'\\r\\n ' + line.strip())\n        else:\n            try:\n                name, value = line.split(b\":\", 1)\n                value = value.strip()\n                if not name:\n                    raise ValueError()\n                ret.append((name, value))\n            except ValueError:\n                raise ValueError(f\"Invalid header line: {line!r}\")\n    return Headers(ret)\n\n\ndef read_request_head(lines: List[bytes]) -> Request:\n    \"\"\"\n    Parse an HTTP request head (request line + headers) from an iterable of lines\n\n    Args:\n        lines: The input lines\n\n    Returns:\n        The HTTP request object (without body)\n\n    Raises:\n        ValueError: The input is malformed.\n    \"\"\"\n    host, port, method, scheme, authority, path, http_version = _read_request_line(lines[0])\n    headers = _read_headers(lines[1:])\n\n    return Request(\n        host=host,\n        port=port,\n        method=method,\n        scheme=scheme,\n        authority=authority,\n        path=path,\n        http_version=http_version,\n        headers=headers,\n        content=None,\n        trailers=None,\n        timestamp_start=time.time(),\n        timestamp_end=None\n    )\n\n\ndef read_response_head(lines: List[bytes]) -> Response:\n    \"\"\"\n    Parse an HTTP response head (response line + headers) from an iterable of lines\n\n    Args:\n        lines: The input lines\n\n    Returns:\n        The HTTP response object (without body)\n\n    Raises:\n        ValueError: The input is malformed.\n    \"\"\"\n    http_version, status_code, reason = _read_response_line(lines[0])\n    headers = _read_headers(lines[1:])\n\n    return Response(\n        http_version=http_version,\n        status_code=status_code,\n        reason=reason,\n        headers=headers,\n        content=None,\n        trailers=None,\n        timestamp_start=time.time(),\n        timestamp_end=None,\n    )\n", "import abc\nfrom typing import Callable, Optional, Type, Union\n\nimport h11\nfrom h11._readers import ChunkedReader, ContentLengthReader, Http10Reader\nfrom h11._receivebuffer import ReceiveBuffer\n\nfrom mitmproxy import http, version\nfrom mitmproxy.connection import Connection, ConnectionState\nfrom mitmproxy.net.http import http1, status_codes\nfrom mitmproxy.proxy import commands, events, layer\nfrom mitmproxy.proxy.layers.http._base import ReceiveHttp, StreamId\nfrom mitmproxy.proxy.utils import expect\nfrom mitmproxy.utils import human\nfrom ._base import HttpConnection, format_error\nfrom ._events import HttpEvent, RequestData, RequestEndOfMessage, RequestHeaders, RequestProtocolError, ResponseData, \\\n    ResponseEndOfMessage, ResponseHeaders, ResponseProtocolError\nfrom ...context import Context\n\nTBodyReader = Union[ChunkedReader, Http10Reader, ContentLengthReader]\n\n\nclass Http1Connection(HttpConnection, metaclass=abc.ABCMeta):\n    stream_id: Optional[StreamId] = None\n    request: Optional[http.Request] = None\n    response: Optional[http.Response] = None\n    request_done: bool = False\n    response_done: bool = False\n    # this is a bit of a hack to make both mypy and PyCharm happy.\n    state: Union[Callable[[events.Event], layer.CommandGenerator[None]], Callable]\n    body_reader: TBodyReader\n    buf: ReceiveBuffer\n\n    ReceiveProtocolError: Type[Union[RequestProtocolError, ResponseProtocolError]]\n    ReceiveData: Type[Union[RequestData, ResponseData]]\n    ReceiveEndOfMessage: Type[Union[RequestEndOfMessage, ResponseEndOfMessage]]\n\n    def __init__(self, context: Context, conn: Connection):\n        super().__init__(context, conn)\n        self.buf = ReceiveBuffer()\n\n    @abc.abstractmethod\n    def send(self, event: HttpEvent) -> layer.CommandGenerator[None]:\n        yield from ()  # pragma: no cover\n\n    @abc.abstractmethod\n    def read_headers(self, event: events.ConnectionEvent) -> layer.CommandGenerator[None]:\n        yield from ()  # pragma: no cover\n\n    def _handle_event(self, event: events.Event) -> layer.CommandGenerator[None]:\n        if isinstance(event, HttpEvent):\n            yield from self.send(event)\n        else:\n            if isinstance(event, events.DataReceived) and self.state != self.passthrough:\n                self.buf += event.data\n            yield from self.state(event)\n\n    @expect(events.Start)\n    def start(self, _) -> layer.CommandGenerator[None]:\n        self.state = self.read_headers\n        yield from ()\n\n    state = start\n\n    def read_body(self, event: events.Event) -> layer.CommandGenerator[None]:\n        assert self.stream_id\n        while True:\n            try:\n                if isinstance(event, events.DataReceived):\n                    h11_event = self.body_reader(self.buf)\n                elif isinstance(event, events.ConnectionClosed):\n                    h11_event = self.body_reader.read_eof()\n                else:\n                    raise AssertionError(f\"Unexpected event: {event}\")\n            except h11.ProtocolError as e:\n                yield commands.CloseConnection(self.conn)\n                yield ReceiveHttp(self.ReceiveProtocolError(self.stream_id, f\"HTTP/1 protocol error: {e}\"))\n                return\n\n            if h11_event is None:\n                return\n            elif isinstance(h11_event, h11.Data):\n                data: bytes = bytes(h11_event.data)\n                if data:\n                    yield ReceiveHttp(self.ReceiveData(self.stream_id, data))\n            elif isinstance(h11_event, h11.EndOfMessage):\n                assert self.request\n                if h11_event.headers:\n                    raise NotImplementedError(f\"HTTP trailers are not implemented yet.\")\n                if self.request.data.method.upper() != b\"CONNECT\":\n                    yield ReceiveHttp(self.ReceiveEndOfMessage(self.stream_id))\n                is_request = isinstance(self, Http1Server)\n                yield from self.mark_done(\n                    request=is_request,\n                    response=not is_request\n                )\n                return\n\n    def wait(self, event: events.Event) -> layer.CommandGenerator[None]:\n        \"\"\"\n        We wait for the current flow to be finished before parsing the next message,\n        as we may want to upgrade to WebSocket or plain TCP before that.\n        \"\"\"\n        assert self.stream_id\n        if isinstance(event, events.DataReceived):\n            return\n        elif isinstance(event, events.ConnectionClosed):\n            # for practical purposes, we assume that a peer which sent at least a FIN\n            # is not interested in any more data from us, see\n            # see https://github.com/httpwg/http-core/issues/22\n            if event.connection.state is not ConnectionState.CLOSED:\n                yield commands.CloseConnection(event.connection)\n            yield ReceiveHttp(self.ReceiveProtocolError(self.stream_id, f\"Client disconnected.\",\n                                                        code=status_codes.CLIENT_CLOSED_REQUEST))\n        else:  # pragma: no cover\n            raise AssertionError(f\"Unexpected event: {event}\")\n\n    def done(self, event: events.ConnectionEvent) -> layer.CommandGenerator[None]:\n        yield from ()  # pragma: no cover\n\n    def make_pipe(self) -> layer.CommandGenerator[None]:\n        self.state = self.passthrough\n        if self.buf:\n            already_received = self.buf.maybe_extract_at_most(len(self.buf))\n            # Some clients send superfluous newlines after CONNECT, we want to eat those.\n            already_received = already_received.lstrip(b\"\\r\\n\")\n            if already_received:\n                yield from self.state(events.DataReceived(self.conn, already_received))\n\n    def passthrough(self, event: events.Event) -> layer.CommandGenerator[None]:\n        assert self.stream_id\n        if isinstance(event, events.DataReceived):\n            yield ReceiveHttp(self.ReceiveData(self.stream_id, event.data))\n        elif isinstance(event, events.ConnectionClosed):\n            if isinstance(self, Http1Server):\n                yield ReceiveHttp(RequestEndOfMessage(self.stream_id))\n            else:\n                yield ReceiveHttp(ResponseEndOfMessage(self.stream_id))\n\n    def mark_done(self, *, request: bool = False, response: bool = False) -> layer.CommandGenerator[None]:\n        if request:\n            self.request_done = True\n        if response:\n            self.response_done = True\n        if self.request_done and self.response_done:\n            assert self.request\n            assert self.response\n            if should_make_pipe(self.request, self.response):\n                yield from self.make_pipe()\n                return\n            try:\n                read_until_eof_semantics = http1.expected_http_body_size(self.request, self.response) == -1\n            except ValueError:\n                # this may raise only now (and not earlier) because an addon set invalid headers,\n                # in which case it's not really clear what we are supposed to do.\n                read_until_eof_semantics = False\n            connection_done = (\n                read_until_eof_semantics\n                or http1.connection_close(self.request.http_version, self.request.headers)\n                or http1.connection_close(self.response.http_version, self.response.headers)\n                # If we proxy HTTP/2 to HTTP/1, we only use upstream connections for one request.\n                # This simplifies our connection management quite a bit as we can rely on\n                # the proxyserver's max-connection-per-server throttling.\n                or (self.request.is_http2 and isinstance(self, Http1Client))\n            )\n            if connection_done:\n                yield commands.CloseConnection(self.conn)\n                self.state = self.done\n                return\n            self.request_done = self.response_done = False\n            self.request = self.response = None\n            if isinstance(self, Http1Server):\n                self.stream_id += 2\n            else:\n                self.stream_id = None\n            self.state = self.read_headers\n            if self.buf:\n                yield from self.state(events.DataReceived(self.conn, b\"\"))\n\n\nclass Http1Server(Http1Connection):\n    \"\"\"A simple HTTP/1 server with no pipelining support.\"\"\"\n\n    ReceiveProtocolError = RequestProtocolError\n    ReceiveData = RequestData\n    ReceiveEndOfMessage = RequestEndOfMessage\n    stream_id: int\n\n    def __init__(self, context: Context):\n        super().__init__(context, context.client)\n        self.stream_id = 1\n\n    def send(self, event: HttpEvent) -> layer.CommandGenerator[None]:\n        assert event.stream_id == self.stream_id\n        if isinstance(event, ResponseHeaders):\n            self.response = response = event.response\n\n            if response.is_http2:\n                response = response.copy()\n                # Convert to an HTTP/1 response.\n                response.http_version = \"HTTP/1.1\"\n                # not everyone supports empty reason phrases, so we better make up one.\n                response.reason = status_codes.RESPONSES.get(response.status_code, \"\")\n                # Shall we set a Content-Length header here if there is none?\n                # For now, let's try to modify as little as possible.\n\n            raw = http1.assemble_response_head(response)\n            yield commands.SendData(self.conn, raw)\n        elif isinstance(event, ResponseData):\n            assert self.response\n            if \"chunked\" in self.response.headers.get(\"transfer-encoding\", \"\").lower():\n                raw = b\"%x\\r\\n%s\\r\\n\" % (len(event.data), event.data)\n            else:\n                raw = event.data\n            if raw:\n                yield commands.SendData(self.conn, raw)\n        elif isinstance(event, ResponseEndOfMessage):\n            assert self.response\n            if \"chunked\" in self.response.headers.get(\"transfer-encoding\", \"\").lower():\n                yield commands.SendData(self.conn, b\"0\\r\\n\\r\\n\")\n            yield from self.mark_done(response=True)\n        elif isinstance(event, ResponseProtocolError):\n            if not self.response and event.code != status_codes.NO_RESPONSE:\n                yield commands.SendData(self.conn, make_error_response(event.code, event.message))\n            if self.conn.state & ConnectionState.CAN_WRITE:\n                yield commands.CloseConnection(self.conn)\n        else:\n            raise AssertionError(f\"Unexpected event: {event}\")\n\n    def read_headers(self, event: events.ConnectionEvent) -> layer.CommandGenerator[None]:\n        if isinstance(event, events.DataReceived):\n            request_head = self.buf.maybe_extract_lines()\n            if request_head:\n                request_head = [bytes(x) for x in request_head]  # TODO: Make url.parse compatible with bytearrays\n                try:\n                    self.request = http1.read_request_head(request_head)\n                    if self.context.options.validate_inbound_headers:\n                        http1.validate_headers(self.request.headers)\n                    expected_body_size = http1.expected_http_body_size(self.request)\n                except ValueError as e:\n                    yield commands.SendData(self.conn, make_error_response(400, str(e)))\n                    yield commands.CloseConnection(self.conn)\n                    if self.request:\n                        # we have headers that we can show in the ui\n                        yield ReceiveHttp(RequestHeaders(self.stream_id, self.request, False))\n                        yield ReceiveHttp(RequestProtocolError(self.stream_id, str(e), 400))\n                    else:\n                        yield commands.Log(f\"{human.format_address(self.conn.peername)}: {e}\")\n                    self.state = self.done\n                    return\n                yield ReceiveHttp(RequestHeaders(self.stream_id, self.request, expected_body_size == 0))\n                self.body_reader = make_body_reader(expected_body_size)\n                self.state = self.read_body\n                yield from self.state(event)\n            else:\n                pass  # FIXME: protect against header size DoS\n        elif isinstance(event, events.ConnectionClosed):\n            buf = bytes(self.buf)\n            if buf.strip():\n                yield commands.Log(f\"Client closed connection before completing request headers: {buf!r}\")\n            yield commands.CloseConnection(self.conn)\n        else:\n            raise AssertionError(f\"Unexpected event: {event}\")\n\n    def mark_done(self, *, request: bool = False, response: bool = False) -> layer.CommandGenerator[None]:\n        yield from super().mark_done(request=request, response=response)\n        if self.request_done and not self.response_done:\n            self.state = self.wait\n\n\nclass Http1Client(Http1Connection):\n    \"\"\"A simple HTTP/1 client with no pipelining support.\"\"\"\n\n    ReceiveProtocolError = ResponseProtocolError\n    ReceiveData = ResponseData\n    ReceiveEndOfMessage = ResponseEndOfMessage\n\n    def __init__(self, context: Context):\n        super().__init__(context, context.server)\n\n    def send(self, event: HttpEvent) -> layer.CommandGenerator[None]:\n        if isinstance(event, RequestProtocolError):\n            yield commands.CloseConnection(self.conn)\n            return\n\n        if not self.stream_id:\n            assert isinstance(event, RequestHeaders)\n            self.stream_id = event.stream_id\n            self.request = event.request\n        assert self.stream_id == event.stream_id\n\n        if isinstance(event, RequestHeaders):\n            request = event.request\n            if request.is_http2:\n                # Convert to an HTTP/1 request.\n                request = request.copy()  # (we could probably be a bit more efficient here.)\n                request.http_version = \"HTTP/1.1\"\n                if \"Host\" not in request.headers and request.authority:\n                    request.headers.insert(0, \"Host\", request.authority)\n                request.authority = \"\"\n            raw = http1.assemble_request_head(request)\n            yield commands.SendData(self.conn, raw)\n        elif isinstance(event, RequestData):\n            assert self.request\n            if \"chunked\" in self.request.headers.get(\"transfer-encoding\", \"\").lower():\n                raw = b\"%x\\r\\n%s\\r\\n\" % (len(event.data), event.data)\n            else:\n                raw = event.data\n            if raw:\n                yield commands.SendData(self.conn, raw)\n        elif isinstance(event, RequestEndOfMessage):\n            assert self.request\n            if \"chunked\" in self.request.headers.get(\"transfer-encoding\", \"\").lower():\n                yield commands.SendData(self.conn, b\"0\\r\\n\\r\\n\")\n            elif http1.expected_http_body_size(self.request, self.response) == -1:\n                yield commands.CloseConnection(self.conn, half_close=True)\n            yield from self.mark_done(request=True)\n        else:\n            raise AssertionError(f\"Unexpected event: {event}\")\n\n    def read_headers(self, event: events.ConnectionEvent) -> layer.CommandGenerator[None]:\n        if isinstance(event, events.DataReceived):\n            if not self.request:\n                # we just received some data for an unknown request.\n                yield commands.Log(f\"Unexpected data from server: {bytes(self.buf)!r}\")\n                yield commands.CloseConnection(self.conn)\n                return\n            assert self.stream_id\n\n            response_head = self.buf.maybe_extract_lines()\n            if response_head:\n                response_head = [bytes(x) for x in response_head]  # TODO: Make url.parse compatible with bytearrays\n                try:\n                    self.response = http1.read_response_head(response_head)\n                    if self.context.options.validate_inbound_headers:\n                        http1.validate_headers(self.response.headers)\n                    expected_size = http1.expected_http_body_size(self.request, self.response)\n                except ValueError as e:\n                    yield commands.CloseConnection(self.conn)\n                    yield ReceiveHttp(ResponseProtocolError(self.stream_id, f\"Cannot parse HTTP response: {e}\"))\n                    return\n                yield ReceiveHttp(ResponseHeaders(self.stream_id, self.response, expected_size == 0))\n                self.body_reader = make_body_reader(expected_size)\n\n                self.state = self.read_body\n                yield from self.state(event)\n            else:\n                pass  # FIXME: protect against header size DoS\n        elif isinstance(event, events.ConnectionClosed):\n            if self.conn.state & ConnectionState.CAN_WRITE:\n                yield commands.CloseConnection(self.conn)\n            if self.stream_id:\n                if self.buf:\n                    yield ReceiveHttp(ResponseProtocolError(self.stream_id,\n                                                            f\"unexpected server response: {bytes(self.buf)!r}\"))\n                else:\n                    # The server has closed the connection to prevent us from continuing.\n                    # We need to signal that to the stream.\n                    # https://tools.ietf.org/html/rfc7231#section-6.5.11\n                    yield ReceiveHttp(ResponseProtocolError(self.stream_id, \"server closed connection\"))\n            else:\n                return\n        else:\n            raise AssertionError(f\"Unexpected event: {event}\")\n\n\ndef should_make_pipe(request: http.Request, response: http.Response) -> bool:\n    if response.status_code == 101:\n        return True\n    elif response.status_code == 200 and request.method.upper() == \"CONNECT\":\n        return True\n    else:\n        return False\n\n\ndef make_body_reader(expected_size: Optional[int]) -> TBodyReader:\n    if expected_size is None:\n        return ChunkedReader()\n    elif expected_size == -1:\n        return Http10Reader()\n    else:\n        return ContentLengthReader(expected_size)\n\n\ndef make_error_response(\n    status_code: int,\n    message: str = \"\",\n) -> bytes:\n    resp = http.Response.make(\n        status_code,\n        format_error(status_code, message),\n        http.Headers(\n            Server=version.MITMPROXY,\n            Connection=\"close\",\n            Content_Type=\"text/html\",\n        )\n    )\n    return http1.assemble_response(resp)\n\n\n__all__ = [\n    \"Http1Client\",\n    \"Http1Server\",\n]\n", "import collections\nimport time\nfrom enum import Enum\nfrom typing import ClassVar, DefaultDict, Dict, List, Optional, Sequence, Tuple, Type, Union\n\nimport h2.config\nimport h2.connection\nimport h2.errors\nimport h2.events\nimport h2.exceptions\nimport h2.settings\nimport h2.stream\nimport h2.utilities\n\nfrom mitmproxy import http, version\nfrom mitmproxy.connection import Connection\nfrom mitmproxy.net.http import status_codes, url\nfrom mitmproxy.utils import human\nfrom . import RequestData, RequestEndOfMessage, RequestHeaders, RequestProtocolError, ResponseData, \\\n    ResponseEndOfMessage, ResponseHeaders, RequestTrailers, ResponseTrailers, ResponseProtocolError\nfrom ._base import HttpConnection, HttpEvent, ReceiveHttp, format_error\nfrom ._http_h2 import BufferedH2Connection, H2ConnectionLogger\nfrom ...commands import CloseConnection, Log, SendData\nfrom ...context import Context\nfrom ...events import ConnectionClosed, DataReceived, Event, Start\nfrom ...layer import CommandGenerator\nfrom ...utils import expect\n\n\nclass StreamState(Enum):\n    EXPECTING_HEADERS = 1\n    HEADERS_RECEIVED = 2\n\n\nCATCH_HYPER_H2_ERRORS = (ValueError, IndexError)\n\n\nclass Http2Connection(HttpConnection):\n    h2_conf: ClassVar[h2.config.H2Configuration]\n    h2_conf_defaults = dict(\n        header_encoding=False,\n        validate_outbound_headers=False,\n        # validate_inbound_headers is controlled by the validate_inbound_headers option.\n        normalize_inbound_headers=False,  # changing this to True is required to pass h2spec\n        normalize_outbound_headers=False,\n    )\n    h2_conn: BufferedH2Connection\n    streams: Dict[int, StreamState]\n    \"\"\"keep track of all active stream ids to send protocol errors on teardown\"\"\"\n\n    ReceiveProtocolError: Type[Union[RequestProtocolError, ResponseProtocolError]]\n    ReceiveData: Type[Union[RequestData, ResponseData]]\n    ReceiveTrailers: Type[Union[RequestTrailers, ResponseTrailers]]\n    ReceiveEndOfMessage: Type[Union[RequestEndOfMessage, ResponseEndOfMessage]]\n\n    def __init__(self, context: Context, conn: Connection):\n        super().__init__(context, conn)\n        if self.debug:\n            self.h2_conf.logger = H2ConnectionLogger(f\"{human.format_address(self.context.client.peername)}: \"\n                                                     f\"{self.__class__.__name__}\")\n        self.h2_conf.validate_inbound_headers = self.context.options.validate_inbound_headers\n        self.h2_conn = BufferedH2Connection(self.h2_conf)\n        self.streams = {}\n\n    def is_closed(self, stream_id: int) -> bool:\n        \"\"\"Check if a non-idle stream is closed\"\"\"\n        stream = self.h2_conn.streams.get(stream_id, None)\n        if (\n            stream is not None\n            and\n            stream.state_machine.state is not h2.stream.StreamState.CLOSED\n            and\n            self.h2_conn.state_machine.state is not h2.connection.ConnectionState.CLOSED\n        ):\n            return False\n        else:\n            return True\n\n    def is_open_for_us(self, stream_id: int) -> bool:\n        \"\"\"Check if we can write to a non-idle stream.\"\"\"\n        stream = self.h2_conn.streams.get(stream_id, None)\n        if (\n            stream is not None\n            and\n            stream.state_machine.state is not h2.stream.StreamState.HALF_CLOSED_LOCAL\n            and\n            stream.state_machine.state is not h2.stream.StreamState.CLOSED\n            and\n            self.h2_conn.state_machine.state is not h2.connection.ConnectionState.CLOSED\n        ):\n            return True\n        else:\n            return False\n\n    def _handle_event(self, event: Event) -> CommandGenerator[None]:\n        if isinstance(event, Start):\n            self.h2_conn.initiate_connection()\n            yield SendData(self.conn, self.h2_conn.data_to_send())\n\n        elif isinstance(event, HttpEvent):\n            if isinstance(event, (RequestData, ResponseData)):\n                if self.is_open_for_us(event.stream_id):\n                    self.h2_conn.send_data(event.stream_id, event.data)\n            elif isinstance(event, (RequestTrailers, ResponseTrailers)):\n                if self.is_open_for_us(event.stream_id):\n                    trailers = [*event.trailers.fields]\n                    self.h2_conn.send_headers(event.stream_id, trailers, end_stream=True)\n            elif isinstance(event, (RequestEndOfMessage, ResponseEndOfMessage)):\n                if self.is_open_for_us(event.stream_id):\n                    self.h2_conn.end_stream(event.stream_id)\n            elif isinstance(event, (RequestProtocolError, ResponseProtocolError)):\n                if not self.is_closed(event.stream_id):\n                    code = {\n                        status_codes.CLIENT_CLOSED_REQUEST: h2.errors.ErrorCodes.CANCEL,\n                    }.get(event.code, h2.errors.ErrorCodes.INTERNAL_ERROR)\n                    stream: h2.stream.H2Stream = self.h2_conn.streams[event.stream_id]\n                    send_error_message = (\n                        isinstance(event, ResponseProtocolError)\n                        and self.is_open_for_us(event.stream_id)\n                        and not stream.state_machine.headers_sent\n                        and event.code != status_codes.NO_RESPONSE\n                    )\n                    if send_error_message:\n                        self.h2_conn.send_headers(event.stream_id, [\n                            (b\":status\", b\"%d\" % event.code),\n                            (b\"server\", version.MITMPROXY.encode()),\n                            (b\"content-type\", b\"text/html\"),\n                        ])\n                        self.h2_conn.send_data(\n                            event.stream_id,\n                            format_error(event.code, event.message),\n                            end_stream=True\n                        )\n                    else:\n                        self.h2_conn.reset_stream(event.stream_id, code)\n            else:\n                raise AssertionError(f\"Unexpected event: {event}\")\n            data_to_send = self.h2_conn.data_to_send()\n            if data_to_send:\n                yield SendData(self.conn, data_to_send)\n\n        elif isinstance(event, DataReceived):\n            try:\n                try:\n                    events = self.h2_conn.receive_data(event.data)\n                except CATCH_HYPER_H2_ERRORS as e:  # pragma: no cover\n                    # this should never raise a ValueError, but we triggered one while fuzzing:\n                    # https://github.com/python-hyper/hyper-h2/issues/1231\n                    # this stays here as defense-in-depth.\n                    raise h2.exceptions.ProtocolError(f\"uncaught hyper-h2 error: {e}\") from e\n            except h2.exceptions.ProtocolError as e:\n                events = [e]\n\n            for h2_event in events:\n                if self.debug:\n                    yield Log(f\"{self.debug}[h2] {h2_event}\", \"debug\")\n                if (yield from self.handle_h2_event(h2_event)):\n                    if self.debug:\n                        yield Log(f\"{self.debug}[h2] done\", \"debug\")\n                    return\n\n            data_to_send = self.h2_conn.data_to_send()\n            if data_to_send:\n                yield SendData(self.conn, data_to_send)\n\n        elif isinstance(event, ConnectionClosed):\n            yield from self.close_connection(\"peer closed connection\")\n        else:\n            raise AssertionError(f\"Unexpected event: {event!r}\")\n\n    def handle_h2_event(self, event: h2.events.Event) -> CommandGenerator[bool]:\n        \"\"\"returns true if further processing should be stopped.\"\"\"\n        if isinstance(event, h2.events.DataReceived):\n            state = self.streams.get(event.stream_id, None)\n            if state is StreamState.HEADERS_RECEIVED:\n                yield ReceiveHttp(self.ReceiveData(event.stream_id, event.data))\n            elif state is StreamState.EXPECTING_HEADERS:\n                yield from self.protocol_error(f\"Received HTTP/2 data frame, expected headers.\")\n                return True\n            self.h2_conn.acknowledge_received_data(event.flow_controlled_length, event.stream_id)\n        elif isinstance(event, h2.events.TrailersReceived):\n            trailers = http.Headers(event.headers)\n            yield ReceiveHttp(self.ReceiveTrailers(event.stream_id, trailers))\n        elif isinstance(event, h2.events.StreamEnded):\n            state = self.streams.get(event.stream_id, None)\n            if state is StreamState.HEADERS_RECEIVED:\n                yield ReceiveHttp(self.ReceiveEndOfMessage(event.stream_id))\n            elif state is StreamState.EXPECTING_HEADERS:\n                raise AssertionError(\"unreachable\")\n            if self.is_closed(event.stream_id):\n                self.streams.pop(event.stream_id, None)\n        elif isinstance(event, h2.events.StreamReset):\n            if event.stream_id in self.streams:\n                try:\n                    err_str = h2.errors.ErrorCodes(event.error_code).name\n                except ValueError:\n                    err_str = str(event.error_code)\n                err_code = {\n                    h2.errors.ErrorCodes.CANCEL: status_codes.CLIENT_CLOSED_REQUEST,\n                }.get(event.error_code, self.ReceiveProtocolError.code)\n                yield ReceiveHttp(self.ReceiveProtocolError(event.stream_id, f\"stream reset by client ({err_str})\",\n                                                            code=err_code))\n                self.streams.pop(event.stream_id)\n            else:\n                pass  # We don't track priority frames which could be followed by a stream reset here.\n        elif isinstance(event, h2.exceptions.ProtocolError):\n            yield from self.protocol_error(f\"HTTP/2 protocol error: {event}\")\n            return True\n        elif isinstance(event, h2.events.ConnectionTerminated):\n            yield from self.close_connection(f\"HTTP/2 connection closed: {event!r}\")\n            return True\n            # The implementation above isn't really ideal, we should probably only terminate streams > last_stream_id?\n            # We currently lack a mechanism to signal that connections are still active but cannot be reused.\n            # for stream_id in self.streams:\n            #    if stream_id > event.last_stream_id:\n            #        yield ReceiveHttp(self.ReceiveProtocolError(stream_id, f\"HTTP/2 connection closed: {event!r}\"))\n            #        self.streams.pop(stream_id)\n        elif isinstance(event, h2.events.RemoteSettingsChanged):\n            pass\n        elif isinstance(event, h2.events.SettingsAcknowledged):\n            pass\n        elif isinstance(event, h2.events.PriorityUpdated):\n            pass\n        elif isinstance(event, h2.events.PingReceived):\n            pass\n        elif isinstance(event, h2.events.PingAckReceived):\n            pass\n        elif isinstance(event, h2.events.PushedStreamReceived):\n            yield Log(\"Received HTTP/2 push promise, even though we signalled no support.\", \"error\")\n        elif isinstance(event, h2.events.UnknownFrameReceived):\n            # https://http2.github.io/http2-spec/#rfc.section.4.1\n            # Implementations MUST ignore and discard any frame that has a type that is unknown.\n            yield Log(f\"Ignoring unknown HTTP/2 frame type: {event.frame.type}\")\n        else:\n            raise AssertionError(f\"Unexpected event: {event!r}\")\n        return False\n\n    def protocol_error(\n        self,\n        message: str,\n        error_code: int = h2.errors.ErrorCodes.PROTOCOL_ERROR,\n    ) -> CommandGenerator[None]:\n        yield Log(f\"{human.format_address(self.conn.peername)}: {message}\")\n        self.h2_conn.close_connection(error_code, message.encode())\n        yield SendData(self.conn, self.h2_conn.data_to_send())\n        yield from self.close_connection(message)\n\n    def close_connection(self, msg: str) -> CommandGenerator[None]:\n        yield CloseConnection(self.conn)\n        for stream_id in self.streams:\n            yield ReceiveHttp(self.ReceiveProtocolError(stream_id, msg))\n        self.streams.clear()\n        self._handle_event = self.done  # type: ignore\n\n    @expect(DataReceived, HttpEvent, ConnectionClosed)\n    def done(self, _) -> CommandGenerator[None]:\n        yield from ()\n\n\ndef normalize_h1_headers(headers: List[Tuple[bytes, bytes]], is_client: bool) -> List[Tuple[bytes, bytes]]:\n    # HTTP/1 servers commonly send capitalized headers (Content-Length vs content-length),\n    # which isn't valid HTTP/2. As such we normalize.\n    headers = h2.utilities.normalize_outbound_headers(\n        headers,\n        h2.utilities.HeaderValidationFlags(is_client, False, not is_client, False)\n    )\n    # make sure that this is not just an iterator but an iterable,\n    # otherwise hyper-h2 will silently drop headers.\n    headers = list(headers)\n    return headers\n\n\ndef normalize_h2_headers(headers: List[Tuple[bytes, bytes]]) -> CommandGenerator[None]:\n    for i in range(len(headers)):\n        if not headers[i][0].islower():\n            yield Log(f\"Lowercased {repr(headers[i][0]).lstrip('b')} header as uppercase is not allowed with HTTP/2.\")\n            headers[i] = (headers[i][0].lower(), headers[i][1])\n\n\nclass Http2Server(Http2Connection):\n    h2_conf = h2.config.H2Configuration(\n        **Http2Connection.h2_conf_defaults,\n        client_side=False,\n    )\n\n    ReceiveProtocolError = RequestProtocolError\n    ReceiveData = RequestData\n    ReceiveTrailers = RequestTrailers\n    ReceiveEndOfMessage = RequestEndOfMessage\n\n    def __init__(self, context: Context):\n        super().__init__(context, context.client)\n\n    def _handle_event(self, event: Event) -> CommandGenerator[None]:\n        if isinstance(event, ResponseHeaders):\n            if self.is_open_for_us(event.stream_id):\n                headers = [\n                    (b\":status\", b\"%d\" % event.response.status_code),\n                    *event.response.headers.fields\n                ]\n                if event.response.is_http2:\n                    if self.context.options.normalize_outbound_headers:\n                        yield from normalize_h2_headers(headers)\n                else:\n                    headers = normalize_h1_headers(headers, False)\n\n                self.h2_conn.send_headers(\n                    event.stream_id,\n                    headers,\n                    end_stream=event.end_stream,\n                )\n                yield SendData(self.conn, self.h2_conn.data_to_send())\n        else:\n            yield from super()._handle_event(event)\n\n    def handle_h2_event(self, event: h2.events.Event) -> CommandGenerator[bool]:\n        if isinstance(event, h2.events.RequestReceived):\n            try:\n                host, port, method, scheme, authority, path, headers = parse_h2_request_headers(event.headers)\n            except ValueError as e:\n                yield from self.protocol_error(f\"Invalid HTTP/2 request headers: {e}\")\n                return True\n            request = http.Request(\n                host=host,\n                port=port,\n                method=method,\n                scheme=scheme,\n                authority=authority,\n                path=path,\n                http_version=b\"HTTP/2.0\",\n                headers=headers,\n                content=None,\n                trailers=None,\n                timestamp_start=time.time(),\n                timestamp_end=None,\n            )\n            self.streams[event.stream_id] = StreamState.HEADERS_RECEIVED\n            yield ReceiveHttp(RequestHeaders(event.stream_id, request, end_stream=bool(event.stream_ended)))\n            return False\n        else:\n            return (yield from super().handle_h2_event(event))\n\n\nclass Http2Client(Http2Connection):\n    h2_conf = h2.config.H2Configuration(\n        **Http2Connection.h2_conf_defaults,\n        client_side=True,\n    )\n\n    ReceiveProtocolError = ResponseProtocolError\n    ReceiveData = ResponseData\n    ReceiveTrailers = ResponseTrailers\n    ReceiveEndOfMessage = ResponseEndOfMessage\n\n    our_stream_id: Dict[int, int]\n    their_stream_id: Dict[int, int]\n    stream_queue: DefaultDict[int, List[Event]]\n    \"\"\"Queue of streams that we haven't sent yet because we have reached MAX_CONCURRENT_STREAMS\"\"\"\n    provisional_max_concurrency: Optional[int] = 10\n    \"\"\"A provisional currency limit before we get the server's first settings frame.\"\"\"\n\n    def __init__(self, context: Context):\n        super().__init__(context, context.server)\n        # Disable HTTP/2 push for now to keep things simple.\n        # don't send here, that is done as part of initiate_connection().\n        self.h2_conn.local_settings.enable_push = 0\n        # hyper-h2 pitfall: we need to acknowledge here, otherwise its sends out the old settings.\n        self.h2_conn.local_settings.acknowledge()\n        self.our_stream_id = {}\n        self.their_stream_id = {}\n        self.stream_queue = collections.defaultdict(list)\n\n    def _handle_event(self, event: Event) -> CommandGenerator[None]:\n        # We can't reuse stream ids from the client because they may arrived reordered here\n        # and HTTP/2 forbids opening a stream on a lower id than what was previously sent (see test_stream_concurrency).\n        # To mitigate this, we transparently map the outside's stream id to our stream id.\n        if isinstance(event, HttpEvent):\n            ours = self.our_stream_id.get(event.stream_id, None)\n            if ours is None:\n                no_free_streams = (\n                    self.h2_conn.open_outbound_streams >=\n                    (self.provisional_max_concurrency or self.h2_conn.remote_settings.max_concurrent_streams)\n                )\n                if no_free_streams:\n                    self.stream_queue[event.stream_id].append(event)\n                    return\n                ours = self.h2_conn.get_next_available_stream_id()\n                self.our_stream_id[event.stream_id] = ours\n                self.their_stream_id[ours] = event.stream_id\n            event.stream_id = ours\n\n        for cmd in self._handle_event2(event):\n            if isinstance(cmd, ReceiveHttp):\n                cmd.event.stream_id = self.their_stream_id[cmd.event.stream_id]\n            yield cmd\n\n        can_resume_queue = (\n            self.stream_queue and\n            self.h2_conn.open_outbound_streams < (\n                self.provisional_max_concurrency or self.h2_conn.remote_settings.max_concurrent_streams\n            )\n        )\n        if can_resume_queue:\n            # popitem would be LIFO, but we want FIFO.\n            events = self.stream_queue.pop(next(iter(self.stream_queue)))\n            for event in events:\n                yield from self._handle_event(event)\n\n    def _handle_event2(self, event: Event) -> CommandGenerator[None]:\n        if isinstance(event, RequestHeaders):\n            pseudo_headers = [\n                (b':method', event.request.data.method),\n                (b':scheme', event.request.data.scheme),\n                (b':path', event.request.data.path),\n            ]\n            if event.request.authority:\n                pseudo_headers.append((b\":authority\", event.request.data.authority))\n\n            if event.request.is_http2:\n                hdrs = list(event.request.headers.fields)\n                if self.context.options.normalize_outbound_headers:\n                    yield from normalize_h2_headers(hdrs)\n            else:\n                headers = event.request.headers\n                if not event.request.authority and \"host\" in headers:\n                    headers = headers.copy()\n                    pseudo_headers.append((b\":authority\", headers.pop(b\"host\")))\n                hdrs = normalize_h1_headers(list(headers.fields), True)\n\n            self.h2_conn.send_headers(\n                event.stream_id,\n                pseudo_headers + hdrs,\n                end_stream=event.end_stream,\n            )\n            self.streams[event.stream_id] = StreamState.EXPECTING_HEADERS\n            yield SendData(self.conn, self.h2_conn.data_to_send())\n        else:\n            yield from super()._handle_event(event)\n\n    def handle_h2_event(self, event: h2.events.Event) -> CommandGenerator[bool]:\n        if isinstance(event, h2.events.ResponseReceived):\n            if self.streams.get(event.stream_id, None) is not StreamState.EXPECTING_HEADERS:\n                yield from self.protocol_error(f\"Received unexpected HTTP/2 response.\")\n                return True\n\n            try:\n                status_code, headers = parse_h2_response_headers(event.headers)\n            except ValueError as e:\n                yield from self.protocol_error(f\"Invalid HTTP/2 response headers: {e}\")\n                return True\n\n            response = http.Response(\n                http_version=b\"HTTP/2.0\",\n                status_code=status_code,\n                reason=b\"\",\n                headers=headers,\n                content=None,\n                trailers=None,\n                timestamp_start=time.time(),\n                timestamp_end=None,\n            )\n            self.streams[event.stream_id] = StreamState.HEADERS_RECEIVED\n            yield ReceiveHttp(ResponseHeaders(event.stream_id, response, bool(event.stream_ended)))\n            return False\n        elif isinstance(event, h2.events.RequestReceived):\n            yield from self.protocol_error(f\"HTTP/2 protocol error: received request from server\")\n            return True\n        elif isinstance(event, h2.events.RemoteSettingsChanged):\n            # We have received at least one settings from now,\n            # which means we can rely on the max concurrency in remote_settings\n            self.provisional_max_concurrency = None\n            return (yield from super().handle_h2_event(event))\n        else:\n            return (yield from super().handle_h2_event(event))\n\n\ndef split_pseudo_headers(h2_headers: Sequence[Tuple[bytes, bytes]]) -> Tuple[Dict[bytes, bytes], http.Headers]:\n    pseudo_headers: Dict[bytes, bytes] = {}\n    i = 0\n    for (header, value) in h2_headers:\n        if header.startswith(b\":\"):\n            if header in pseudo_headers:\n                raise ValueError(f\"Duplicate HTTP/2 pseudo header: {header!r}\")\n            pseudo_headers[header] = value\n            i += 1\n        else:\n            # Pseudo-headers must be at the start, we are done here.\n            break\n\n    headers = http.Headers(h2_headers[i:])\n\n    return pseudo_headers, headers\n\n\ndef parse_h2_request_headers(\n    h2_headers: Sequence[Tuple[bytes, bytes]]\n) -> Tuple[str, int, bytes, bytes, bytes, bytes, http.Headers]:\n    \"\"\"Split HTTP/2 pseudo-headers from the actual headers and parse them.\"\"\"\n    pseudo_headers, headers = split_pseudo_headers(h2_headers)\n\n    try:\n        method: bytes = pseudo_headers.pop(b\":method\")\n        scheme: bytes = pseudo_headers.pop(b\":scheme\")  # this raises for HTTP/2 CONNECT requests\n        path: bytes = pseudo_headers.pop(b\":path\")\n        authority: bytes = pseudo_headers.pop(b\":authority\", b\"\")\n    except KeyError as e:\n        raise ValueError(f\"Required pseudo header is missing: {e}\")\n\n    if pseudo_headers:\n        raise ValueError(f\"Unknown pseudo headers: {pseudo_headers}\")\n\n    if authority:\n        host, port = url.parse_authority(authority, check=True)\n        if port is None:\n            port = 80 if scheme == b'http' else 443\n    else:\n        host = \"\"\n        port = 0\n\n    return host, port, method, scheme, authority, path, headers\n\n\ndef parse_h2_response_headers(h2_headers: Sequence[Tuple[bytes, bytes]]) -> Tuple[int, http.Headers]:\n    \"\"\"Split HTTP/2 pseudo-headers from the actual headers and parse them.\"\"\"\n    pseudo_headers, headers = split_pseudo_headers(h2_headers)\n\n    try:\n        status_code: int = int(pseudo_headers.pop(b\":status\"))\n    except KeyError as e:\n        raise ValueError(f\"Required pseudo header is missing: {e}\")\n\n    if pseudo_headers:\n        raise ValueError(f\"Unknown pseudo headers: {pseudo_headers}\")\n\n    return status_code, headers\n\n\n__all__ = [\n    \"Http2Client\",\n    \"Http2Server\",\n]\n", "import pytest\n\nfrom mitmproxy.http import Headers\nfrom mitmproxy.net.http.http1.read import (\n    read_request_head,\n    read_response_head, connection_close, expected_http_body_size,\n    _read_request_line, _read_response_line, _read_headers, get_header_tokens, validate_headers\n)\nfrom mitmproxy.test.tutils import treq, tresp\n\n\ndef test_get_header_tokens():\n    headers = Headers()\n    assert get_header_tokens(headers, \"foo\") == []\n    headers[\"foo\"] = \"bar\"\n    assert get_header_tokens(headers, \"foo\") == [\"bar\"]\n    headers[\"foo\"] = \"bar, voing\"\n    assert get_header_tokens(headers, \"foo\") == [\"bar\", \"voing\"]\n    headers.set_all(\"foo\", [\"bar, voing\", \"oink\"])\n    assert get_header_tokens(headers, \"foo\") == [\"bar\", \"voing\", \"oink\"]\n\n\ndef test_connection_close():\n    headers = Headers()\n    assert connection_close(b\"HTTP/1.0\", headers)\n    assert not connection_close(b\"HTTP/1.1\", headers)\n    assert not connection_close(b\"HTTP/2.0\", headers)\n\n    headers[\"connection\"] = \"keep-alive\"\n    assert not connection_close(b\"HTTP/1.1\", headers)\n\n    headers[\"connection\"] = \"close\"\n    assert connection_close(b\"HTTP/1.1\", headers)\n\n    headers[\"connection\"] = \"foobar\"\n    assert connection_close(b\"HTTP/1.0\", headers)\n    assert not connection_close(b\"HTTP/1.1\", headers)\n\n\ndef test_read_request_head():\n    rfile = [\n        b\"GET / HTTP/1.1\\r\\n\",\n        b\"Content-Length: 4\\r\\n\",\n    ]\n    r = read_request_head(rfile)\n    assert r.method == \"GET\"\n    assert r.headers[\"Content-Length\"] == \"4\"\n    assert r.content is None\n\n\ndef test_read_response_head():\n    rfile = [\n        b\"HTTP/1.1 418 I'm a teapot\\r\\n\",\n        b\"Content-Length: 4\\r\\n\",\n    ]\n    r = read_response_head(rfile)\n    assert r.status_code == 418\n    assert r.headers[\"Content-Length\"] == \"4\"\n    assert r.content is None\n\n\ndef test_validate_headers():\n    # both content-length and chunked (possible request smuggling)\n    with pytest.raises(ValueError, match=\"Received both a Transfer-Encoding and a Content-Length header\"):\n        validate_headers(\n            Headers(transfer_encoding=\"chunked\", content_length=\"42\"),\n        )\n\n    with pytest.raises(ValueError, match=\"Received an invalid header name\"):\n        validate_headers(\n            Headers([(b\"content-length \", b\"42\")]),\n        )\n\n\ndef test_expected_http_body_size():\n    # Expect: 100-continue\n    assert expected_http_body_size(\n        treq(headers=Headers(expect=\"100-continue\", content_length=\"42\")),\n    ) == 42\n\n    # http://tools.ietf.org/html/rfc7230#section-3.3\n    assert expected_http_body_size(\n        treq(method=b\"HEAD\"),\n        tresp(headers=Headers(content_length=\"42\"))\n    ) == 0\n    assert expected_http_body_size(\n        treq(method=b\"CONNECT\", headers=Headers()),\n        None,\n    ) == 0\n    assert expected_http_body_size(\n        treq(method=b\"CONNECT\"),\n        tresp()\n    ) == 0\n    for code in (100, 204, 304):\n        assert expected_http_body_size(\n            treq(),\n            tresp(status_code=code)\n        ) == 0\n\n    # chunked\n    assert expected_http_body_size(\n        treq(headers=Headers(transfer_encoding=\"chunked\")),\n    ) is None\n    assert expected_http_body_size(\n        treq(headers=Headers(transfer_encoding=\"gzip,\\tchunked\")),\n    ) is None\n    with pytest.raises(ValueError, match=\"Invalid transfer encoding\"):\n        expected_http_body_size(\n            treq(headers=Headers(transfer_encoding=\"chun\\u212Aed\")),  # \"chun\u212aed\".lower() == \"chunked\"\n        )\n    with pytest.raises(ValueError, match=\"Unknown transfer encoding\"):\n        expected_http_body_size(\n            treq(headers=Headers(transfer_encoding=\"chun ked\")),  # \"chun\u212aed\".lower() == \"chunked\"\n        )\n    with pytest.raises(ValueError, match=\"Unknown transfer encoding\"):\n        expected_http_body_size(\n            treq(headers=Headers(transfer_encoding=\"qux\")),\n        )\n    # transfer-encoding: gzip\n    with pytest.raises(ValueError, match=\"Invalid request transfer encoding\"):\n        expected_http_body_size(\n            treq(headers=Headers(transfer_encoding=\"gzip\")),\n        )\n    assert expected_http_body_size(\n        treq(),\n        tresp(headers=Headers(transfer_encoding=\"gzip\")),\n    ) == -1\n\n    # explicit length\n    for val in (b\"foo\", b\"-7\"):\n        with pytest.raises(ValueError):\n            expected_http_body_size(\n                treq(headers=Headers(content_length=val))\n            )\n    assert expected_http_body_size(\n        treq(headers=Headers(content_length=\"42\"))\n    ) == 42\n    # multiple content-length headers with same value\n    assert expected_http_body_size(\n        treq(headers=Headers([(b'content-length', b'42'), (b'content-length', b'42')]))\n    ) == 42\n    # multiple content-length headers with conflicting value\n    with pytest.raises(ValueError, match=\"Conflicting Content-Length headers\"):\n        expected_http_body_size(\n            treq(headers=Headers([(b'content-length', b'42'), (b'content-length', b'45')]))\n        )\n\n    # non-int content-length\n    with pytest.raises(ValueError, match=\"Invalid Content-Length header\"):\n        expected_http_body_size(\n            treq(headers=Headers([(b'content-length', b'NaN')]))\n        )\n    # negative content-length\n    with pytest.raises(ValueError, match=\"Negative Content-Length header\"):\n        expected_http_body_size(\n            treq(headers=Headers([(b'content-length', b'-1')]))\n        )\n\n    # no length\n    assert expected_http_body_size(\n        treq(headers=Headers())\n    ) == 0\n    assert expected_http_body_size(\n        treq(headers=Headers()), tresp(headers=Headers())\n    ) == -1\n\n\ndef test_read_request_line():\n    def t(b):\n        return _read_request_line(b)\n\n    assert (t(b\"GET / HTTP/1.1\") ==\n            (\"\", 0, b\"GET\", b\"\", b\"\", b\"/\", b\"HTTP/1.1\"))\n    assert (t(b\"OPTIONS * HTTP/1.1\") ==\n            (\"\", 0, b\"OPTIONS\", b\"\", b\"\", b\"*\", b\"HTTP/1.1\"))\n    assert (t(b\"CONNECT foo:42 HTTP/1.1\") ==\n            (\"foo\", 42, b\"CONNECT\", b\"\", b\"foo:42\", b\"\", b\"HTTP/1.1\"))\n    assert (t(b\"GET http://foo:42/bar HTTP/1.1\") ==\n            (\"foo\", 42, b\"GET\", b\"http\", b\"foo:42\", b\"/bar\", b\"HTTP/1.1\"))\n    assert (t(b\"GET http://foo:42 HTTP/1.1\") ==\n            (\"foo\", 42, b\"GET\", b\"http\", b\"foo:42\", b\"/\", b\"HTTP/1.1\"))\n\n    with pytest.raises(ValueError):\n        t(b\"GET / WTF/1.1\")\n    with pytest.raises(ValueError):\n        t(b\"CONNECT example.com HTTP/1.1\")  # port missing\n    with pytest.raises(ValueError):\n        t(b\"GET ws://example.com/ HTTP/1.1\")  # port missing\n    with pytest.raises(ValueError):\n        t(b\"this is not http\")\n    with pytest.raises(ValueError):\n        t(b\"\")\n\n\ndef test_read_response_line():\n    def t(b):\n        return _read_response_line(b)\n\n    assert t(b\"HTTP/1.1 200 OK\") == (b\"HTTP/1.1\", 200, b\"OK\")\n    assert t(b\"HTTP/1.1 200\") == (b\"HTTP/1.1\", 200, b\"\")\n\n    # https://github.com/mitmproxy/mitmproxy/issues/784\n    assert t(b\"HTTP/1.1 200 Non-Autoris\\xc3\\xa9\") == (b\"HTTP/1.1\", 200, b\"Non-Autoris\\xc3\\xa9\")\n\n    with pytest.raises(ValueError):\n        assert t(b\"HTTP/1.1\")\n\n    with pytest.raises(ValueError):\n        t(b\"HTTP/1.1 OK OK\")\n    with pytest.raises(ValueError):\n        t(b\"WTF/1.1 200 OK\")\n    with pytest.raises(ValueError):\n        t(b\"\")\n\n\nclass TestReadHeaders:\n    @staticmethod\n    def _read(data):\n        return _read_headers(data.splitlines(keepends=True))\n\n    def test_read_simple(self):\n        data = (\n            b\"Header: one\\r\\n\"\n            b\"Header2: two\\r\\n\"\n        )\n        headers = self._read(data)\n        assert headers.fields == ((b\"Header\", b\"one\"), (b\"Header2\", b\"two\"))\n\n    def test_read_multi(self):\n        data = (\n            b\"Header: one\\r\\n\"\n            b\"Header: two\\r\\n\"\n        )\n        headers = self._read(data)\n        assert headers.fields == ((b\"Header\", b\"one\"), (b\"Header\", b\"two\"))\n\n    def test_read_continued(self):\n        data = (\n            b\"Header: one\\r\\n\"\n            b\"\\ttwo\\r\\n\"\n            b\"Header2: three\\r\\n\"\n        )\n        headers = self._read(data)\n        assert headers.fields == ((b\"Header\", b\"one\\r\\n two\"), (b\"Header2\", b\"three\"))\n\n    def test_read_continued_err(self):\n        data = b\"\\tfoo: bar\\r\\n\"\n        with pytest.raises(ValueError):\n            self._read(data)\n\n    def test_read_err(self):\n        data = b\"foo\"\n        with pytest.raises(ValueError):\n            self._read(data)\n\n    def test_read_empty_name(self):\n        data = b\":foo\"\n        with pytest.raises(ValueError):\n            self._read(data)\n\n    def test_read_empty_value(self):\n        data = b\"bar:\"\n        headers = self._read(data)\n        assert headers.fields == ((b\"bar\", b\"\"),)\n", "import pytest\n\nfrom mitmproxy.connection import ConnectionState, Server\nfrom mitmproxy.flow import Error\nfrom mitmproxy.http import HTTPFlow, Response\nfrom mitmproxy.net.server_spec import ServerSpec\nfrom mitmproxy.proxy import layer\nfrom mitmproxy.proxy.commands import CloseConnection, Log, OpenConnection, SendData\nfrom mitmproxy.proxy.events import ConnectionClosed, DataReceived\nfrom mitmproxy.proxy.layers import TCPLayer, http, tls\nfrom mitmproxy.proxy.layers.http import HTTPMode\nfrom mitmproxy.proxy.layers.tcp import TcpMessageInjected, TcpStartHook\nfrom mitmproxy.proxy.layers.websocket import WebsocketStartHook\nfrom mitmproxy.tcp import TCPFlow, TCPMessage\nfrom test.mitmproxy.proxy.tutils import Placeholder, Playbook, reply, reply_next_layer\n\n\ndef test_http_proxy(tctx):\n    \"\"\"Test a simple HTTP GET / request\"\"\"\n    server = Placeholder(Server)\n    flow = Placeholder(HTTPFlow)\n    assert (\n            Playbook(http.HttpLayer(tctx, HTTPMode.regular))\n            >> DataReceived(tctx.client, b\"GET http://example.com/foo?hello=1 HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\")\n            << http.HttpRequestHeadersHook(flow)\n            >> reply()\n            << http.HttpRequestHook(flow)\n            >> reply()\n            << OpenConnection(server)\n            >> reply(None)\n            << SendData(server, b\"GET /foo?hello=1 HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\")\n            >> DataReceived(server, b\"HTTP/1.1 200 OK\\r\\nContent-Length: 12\\r\\n\\r\\nHello World\")\n            << http.HttpResponseHeadersHook(flow)\n            >> reply()\n            >> DataReceived(server, b\"!\")\n            << http.HttpResponseHook(flow)\n            >> reply()\n            << SendData(tctx.client, b\"HTTP/1.1 200 OK\\r\\nContent-Length: 12\\r\\n\\r\\nHello World!\")\n    )\n    assert server().address == (\"example.com\", 80)\n\n\n@pytest.mark.parametrize(\"strategy\", [\"lazy\", \"eager\"])\ndef test_https_proxy(strategy, tctx):\n    \"\"\"Test a CONNECT request, followed by a HTTP GET /\"\"\"\n    server = Placeholder(Server)\n    flow = Placeholder(HTTPFlow)\n    playbook = Playbook(http.HttpLayer(tctx, HTTPMode.regular))\n    tctx.options.connection_strategy = strategy\n\n    (playbook\n     >> DataReceived(tctx.client, b\"CONNECT example.proxy:80 HTTP/1.1\\r\\n\\r\\n\")\n     << http.HttpConnectHook(Placeholder())\n     >> reply())\n    if strategy == \"eager\":\n        (playbook\n         << OpenConnection(server)\n         >> reply(None))\n    (playbook\n     << SendData(tctx.client, b'HTTP/1.1 200 Connection established\\r\\n\\r\\n')\n     >> DataReceived(tctx.client, b\"GET /foo?hello=1 HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\")\n     << layer.NextLayerHook(Placeholder())\n     >> reply_next_layer(lambda ctx: http.HttpLayer(ctx, HTTPMode.transparent))\n     << http.HttpRequestHeadersHook(flow)\n     >> reply()\n     << http.HttpRequestHook(flow)\n     >> reply())\n    if strategy == \"lazy\":\n        (playbook\n         << OpenConnection(server)\n         >> reply(None))\n    (playbook\n     << SendData(server, b\"GET /foo?hello=1 HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\")\n     >> DataReceived(server, b\"HTTP/1.1 200 OK\\r\\nContent-Length: 12\\r\\n\\r\\nHello World!\")\n     << http.HttpResponseHeadersHook(flow)\n     >> reply()\n     << http.HttpResponseHook(flow)\n     >> reply()\n     << SendData(tctx.client, b\"HTTP/1.1 200 OK\\r\\nContent-Length: 12\\r\\n\\r\\nHello World!\"))\n    assert playbook\n\n\n@pytest.mark.parametrize(\"https_client\", [False, True])\n@pytest.mark.parametrize(\"https_server\", [False, True])\n@pytest.mark.parametrize(\"strategy\", [\"lazy\", \"eager\"])\ndef test_redirect(strategy, https_server, https_client, tctx, monkeypatch):\n    \"\"\"Test redirects between http:// and https:// in regular proxy mode.\"\"\"\n    server = Placeholder(Server)\n    flow = Placeholder(HTTPFlow)\n    tctx.options.connection_strategy = strategy\n    p = Playbook(http.HttpLayer(tctx, HTTPMode.regular), hooks=False)\n\n    if https_server:\n        monkeypatch.setattr(tls, \"ServerTLSLayer\", tls.MockTLSLayer)\n\n    def redirect(flow: HTTPFlow):\n        if https_server:\n            flow.request.url = \"https://redirected.site/\"\n        else:\n            flow.request.url = \"http://redirected.site/\"\n\n    if https_client:\n        p >> DataReceived(tctx.client, b\"CONNECT example.com:80 HTTP/1.1\\r\\n\\r\\n\")\n        if strategy == \"eager\":\n            p << OpenConnection(Placeholder())\n            p >> reply(None)\n        p << SendData(tctx.client, b'HTTP/1.1 200 Connection established\\r\\n\\r\\n')\n        p >> DataReceived(tctx.client, b\"GET / HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\")\n        p << layer.NextLayerHook(Placeholder())\n        p >> reply_next_layer(lambda ctx: http.HttpLayer(ctx, HTTPMode.transparent))\n    else:\n        p >> DataReceived(tctx.client, b\"GET http://example.com/ HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\")\n    p << http.HttpRequestHook(flow)\n    p >> reply(side_effect=redirect)\n    p << OpenConnection(server)\n    p >> reply(None)\n    p << SendData(server, b\"GET / HTTP/1.1\\r\\nHost: redirected.site\\r\\n\\r\\n\")\n    p >> DataReceived(server, b\"HTTP/1.1 200 OK\\r\\nContent-Length: 12\\r\\n\\r\\nHello World!\")\n    p << SendData(tctx.client, b\"HTTP/1.1 200 OK\\r\\nContent-Length: 12\\r\\n\\r\\nHello World!\")\n\n    assert p\n    if https_server:\n        assert server().address == (\"redirected.site\", 443)\n    else:\n        assert server().address == (\"redirected.site\", 80)\n\n\ndef test_multiple_server_connections(tctx):\n    \"\"\"Test multiple requests being rewritten to different targets.\"\"\"\n    server1 = Placeholder(Server)\n    server2 = Placeholder(Server)\n    playbook = Playbook(http.HttpLayer(tctx, HTTPMode.regular), hooks=False)\n\n    def redirect(to: str):\n        def side_effect(flow: HTTPFlow):\n            flow.request.url = to\n\n        return side_effect\n\n    assert (\n            playbook\n            >> DataReceived(tctx.client, b\"GET http://example.com/ HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\")\n            << http.HttpRequestHook(Placeholder())\n            >> reply(side_effect=redirect(\"http://one.redirect/\"))\n            << OpenConnection(server1)\n            >> reply(None)\n            << SendData(server1, b\"GET / HTTP/1.1\\r\\nHost: one.redirect\\r\\n\\r\\n\")\n            >> DataReceived(server1, b\"HTTP/1.1 200 OK\\r\\nContent-Length: 0\\r\\n\\r\\n\")\n            << SendData(tctx.client, b\"HTTP/1.1 200 OK\\r\\nContent-Length: 0\\r\\n\\r\\n\")\n    )\n    assert (\n            playbook\n            >> DataReceived(tctx.client, b\"GET http://example.com/ HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\")\n            << http.HttpRequestHook(Placeholder())\n            >> reply(side_effect=redirect(\"http://two.redirect/\"))\n            << OpenConnection(server2)\n            >> reply(None)\n            << SendData(server2, b\"GET / HTTP/1.1\\r\\nHost: two.redirect\\r\\n\\r\\n\")\n            >> DataReceived(server2, b\"HTTP/1.1 200 OK\\r\\nContent-Length: 0\\r\\n\\r\\n\")\n            << SendData(tctx.client, b\"HTTP/1.1 200 OK\\r\\nContent-Length: 0\\r\\n\\r\\n\")\n    )\n    assert server1().address == (\"one.redirect\", 80)\n    assert server2().address == (\"two.redirect\", 80)\n\n\n@pytest.mark.parametrize(\"transfer_encoding\", [\"identity\", \"chunked\"])\ndef test_pipelining(tctx, transfer_encoding):\n    \"\"\"Test that multiple requests can be processed over the same connection\"\"\"\n\n    tctx.server.address = (\"example.com\", 80)\n    tctx.server.state = ConnectionState.OPEN\n\n    req = b\"GET / HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\"\n    if transfer_encoding == \"identity\":\n        resp = (b\"HTTP/1.1 200 OK\\r\\n\"\n                b\"Content-Length: 12\\r\\n\"\n                b\"\\r\\n\"\n                b\"Hello World!\")\n    else:\n        resp = (b\"HTTP/1.1 200 OK\\r\\n\"\n                b\"Transfer-Encoding: chunked\\r\\n\"\n                b\"\\r\\n\"\n                b\"c\\r\\n\"\n                b\"Hello World!\\r\\n\"\n                b\"0\\r\\n\"\n                b\"\\r\\n\")\n\n    assert (\n            Playbook(http.HttpLayer(tctx, HTTPMode.transparent), hooks=False)\n            # Roundtrip 1\n            >> DataReceived(tctx.client, req)\n            << SendData(tctx.server, req)\n            >> DataReceived(tctx.server, resp)\n            << SendData(tctx.client, resp)\n            # Roundtrip 2\n            >> DataReceived(tctx.client, req)\n            << SendData(tctx.server, req)\n            >> DataReceived(tctx.server, resp)\n            << SendData(tctx.client, resp)\n    )\n\n\ndef test_http_reply_from_proxy(tctx):\n    \"\"\"Test a response served by mitmproxy itself.\"\"\"\n\n    def reply_from_proxy(flow: HTTPFlow):\n        flow.response = Response.make(418)\n\n    assert (\n            Playbook(http.HttpLayer(tctx, HTTPMode.regular), hooks=False)\n            >> DataReceived(tctx.client, b\"GET http://example.com/ HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\")\n            << http.HttpRequestHook(Placeholder())\n            >> reply(side_effect=reply_from_proxy)\n            << SendData(tctx.client, b\"HTTP/1.1 418 I'm a teapot\\r\\ncontent-length: 0\\r\\n\\r\\n\")\n    )\n\n\ndef test_response_until_eof(tctx):\n    \"\"\"Test scenario where the server response body is terminated by EOF.\"\"\"\n    server = Placeholder(Server)\n    assert (\n            Playbook(http.HttpLayer(tctx, HTTPMode.regular), hooks=False)\n            >> DataReceived(tctx.client, b\"GET http://example.com/ HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\")\n            << OpenConnection(server)\n            >> reply(None)\n            << SendData(server, b\"GET / HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\")\n            >> DataReceived(server, b\"HTTP/1.1 200 OK\\r\\n\\r\\nfoo\")\n            >> ConnectionClosed(server)\n            << CloseConnection(server)\n            << SendData(tctx.client, b\"HTTP/1.1 200 OK\\r\\n\\r\\nfoo\")\n            << CloseConnection(tctx.client)\n    )\n\n\ndef test_disconnect_while_intercept(tctx):\n    \"\"\"Test a server disconnect while a request is intercepted.\"\"\"\n    tctx.options.connection_strategy = \"eager\"\n\n    server1 = Placeholder(Server)\n    server2 = Placeholder(Server)\n    flow = Placeholder(HTTPFlow)\n\n    assert (\n            Playbook(http.HttpLayer(tctx, HTTPMode.regular), hooks=False)\n            >> DataReceived(tctx.client, b\"CONNECT example.com:80 HTTP/1.1\\r\\n\\r\\n\")\n            << http.HttpConnectHook(Placeholder(HTTPFlow))\n            >> reply()\n            << OpenConnection(server1)\n            >> reply(None)\n            << SendData(tctx.client, b'HTTP/1.1 200 Connection established\\r\\n\\r\\n')\n            >> DataReceived(tctx.client, b\"GET / HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\")\n            << layer.NextLayerHook(Placeholder())\n            >> reply_next_layer(lambda ctx: http.HttpLayer(ctx, HTTPMode.transparent))\n            << http.HttpRequestHook(flow)\n            >> ConnectionClosed(server1)\n            << CloseConnection(server1)\n            >> reply(to=-3)\n            << OpenConnection(server2)\n            >> reply(None)\n            << SendData(server2, b\"GET / HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\")\n            >> DataReceived(server2, b\"HTTP/1.1 200 OK\\r\\nContent-Length: 0\\r\\n\\r\\n\")\n            << SendData(tctx.client, b\"HTTP/1.1 200 OK\\r\\nContent-Length: 0\\r\\n\\r\\n\")\n    )\n    assert server1() != server2()\n    assert flow().server_conn == server2()\n    assert not flow().live\n\n\n@pytest.mark.parametrize(\"why\", [\"body_size=0\", \"body_size=3\", \"addon\"])\n@pytest.mark.parametrize(\"transfer_encoding\", [\"identity\", \"chunked\"])\ndef test_response_streaming(tctx, why, transfer_encoding):\n    \"\"\"Test HTTP response streaming\"\"\"\n    server = Placeholder(Server)\n    flow = Placeholder(HTTPFlow)\n    playbook = Playbook(http.HttpLayer(tctx, HTTPMode.regular))\n\n    if why.startswith(\"body_size\"):\n        tctx.options.stream_large_bodies = why.replace(\"body_size=\", \"\")\n\n    def enable_streaming(flow: HTTPFlow):\n        if why == \"addon\":\n            flow.response.stream = True\n\n    assert (\n        playbook\n        >> DataReceived(tctx.client, b\"GET http://example.com/largefile HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\")\n        << http.HttpRequestHeadersHook(flow)\n        >> reply()\n        << http.HttpRequestHook(flow)\n        >> reply()\n        << OpenConnection(server)\n        >> reply(None)\n        << SendData(server, b\"GET /largefile HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\")\n        >> DataReceived(server, b\"HTTP/1.1 200 OK\\r\\n\")\n    )\n    assert flow().live\n    if transfer_encoding == \"identity\":\n        playbook >> DataReceived(server, b\"Content-Length: 6\\r\\n\\r\\n\"\n                                         b\"abc\")\n    else:\n        playbook >> DataReceived(server, b\"Transfer-Encoding: chunked\\r\\n\\r\\n\"\n                                         b\"3\\r\\nabc\\r\\n\")\n\n    playbook << http.HttpResponseHeadersHook(flow)\n    playbook >> reply(side_effect=enable_streaming)\n\n    if transfer_encoding == \"identity\":\n        playbook << SendData(tctx.client, b\"HTTP/1.1 200 OK\\r\\n\"\n                                          b\"Content-Length: 6\\r\\n\\r\\n\"\n                                          b\"abc\")\n        playbook >> DataReceived(server, b\"def\")\n        playbook << SendData(tctx.client, b\"def\")\n    else:\n        if why == \"body_size=3\":\n            playbook >> DataReceived(server, b\"3\\r\\ndef\\r\\n\")\n            playbook << SendData(tctx.client, b\"HTTP/1.1 200 OK\\r\\n\"\n                                              b\"Transfer-Encoding: chunked\\r\\n\\r\\n\"\n                                              b\"6\\r\\nabcdef\\r\\n\")\n        else:\n            playbook << SendData(tctx.client, b\"HTTP/1.1 200 OK\\r\\n\"\n                                              b\"Transfer-Encoding: chunked\\r\\n\\r\\n\"\n                                              b\"3\\r\\nabc\\r\\n\")\n            playbook >> DataReceived(server, b\"3\\r\\ndef\\r\\n\")\n            playbook << SendData(tctx.client, b\"3\\r\\ndef\\r\\n\")\n        playbook >> DataReceived(server, b\"0\\r\\n\\r\\n\")\n\n    playbook << http.HttpResponseHook(flow)\n    playbook >> reply()\n\n    if transfer_encoding == \"chunked\":\n        playbook << SendData(tctx.client, b\"0\\r\\n\\r\\n\")\n\n    assert playbook\n    assert not flow().live\n\n\ndef test_stream_modify(tctx):\n    \"\"\"Test HTTP stream modification\"\"\"\n    server = Placeholder(Server)\n    flow = Placeholder(HTTPFlow)\n\n    def enable_streaming(flow: HTTPFlow):\n        if flow.response is None:\n            flow.request.stream = lambda x: b\"[\" + x + b\"]\"\n        else:\n            flow.response.stream = lambda x: b\"[\" + x + b\"]\"\n\n    assert (\n        Playbook(http.HttpLayer(tctx, HTTPMode.regular))\n        >> DataReceived(tctx.client, b\"POST http://example.com/ HTTP/1.1\\r\\n\"\n                                     b\"Host: example.com\\r\\n\"\n                                     b\"Transfer-Encoding: chunked\\r\\n\\r\\n\"\n                                     b\"3\\r\\nabc\\r\\n\"\n                                     b\"0\\r\\n\\r\\n\")\n        << http.HttpRequestHeadersHook(flow)\n        >> reply(side_effect=enable_streaming)\n        << OpenConnection(server)\n        >> reply(None)\n        << SendData(server, b\"POST / HTTP/1.1\\r\\n\"\n                            b\"Host: example.com\\r\\n\"\n                            b\"Transfer-Encoding: chunked\\r\\n\\r\\n\"\n                            b\"5\\r\\n[abc]\\r\\n\"\n                            b\"2\\r\\n[]\\r\\n\")\n        << http.HttpRequestHook(flow)\n        >> reply()\n        << SendData(server, b\"0\\r\\n\\r\\n\")\n        >> DataReceived(server, b\"HTTP/1.1 200 OK\\r\\n\"\n                                b\"Transfer-Encoding: chunked\\r\\n\\r\\n\"\n                                b\"3\\r\\ndef\\r\\n\"\n                                b\"0\\r\\n\\r\\n\")\n        << http.HttpResponseHeadersHook(flow)\n        >> reply(side_effect=enable_streaming)\n        << SendData(tctx.client, b\"HTTP/1.1 200 OK\\r\\n\"\n                                 b\"Transfer-Encoding: chunked\\r\\n\\r\\n\"\n                                 b\"5\\r\\n[def]\\r\\n\"\n                                 b\"2\\r\\n[]\\r\\n\")\n        << http.HttpResponseHook(flow)\n        >> reply()\n        << SendData(tctx.client, b\"0\\r\\n\\r\\n\")\n    )\n\n\n@pytest.mark.parametrize(\"why\", [\"body_size=0\", \"body_size=3\", \"addon\"])\n@pytest.mark.parametrize(\"transfer_encoding\", [\"identity\", \"chunked\"])\n@pytest.mark.parametrize(\"response\", [\"normal response\", \"early response\", \"early close\", \"early kill\"])\ndef test_request_streaming(tctx, why, transfer_encoding, response):\n    \"\"\"\n    Test HTTP request streaming\n\n    This is a bit more contrived as we may receive server data while we are still sending the request.\n    \"\"\"\n    server = Placeholder(Server)\n    flow = Placeholder(HTTPFlow)\n    playbook = Playbook(http.HttpLayer(tctx, HTTPMode.regular))\n\n    if why.startswith(\"body_size\"):\n        tctx.options.stream_large_bodies = why.replace(\"body_size=\", \"\")\n\n    def enable_streaming(flow: HTTPFlow):\n        if why == \"addon\":\n            flow.request.stream = True\n\n    playbook >> DataReceived(tctx.client, b\"POST http://example.com/ HTTP/1.1\\r\\n\"\n                                          b\"Host: example.com\\r\\n\")\n    if transfer_encoding == \"identity\":\n        playbook >> DataReceived(tctx.client, b\"Content-Length: 9\\r\\n\\r\\n\"\n                                              b\"abc\")\n    else:\n        playbook >> DataReceived(tctx.client, b\"Transfer-Encoding: chunked\\r\\n\\r\\n\"\n                                              b\"3\\r\\nabc\\r\\n\")\n\n    playbook << http.HttpRequestHeadersHook(flow)\n    playbook >> reply(side_effect=enable_streaming)\n\n    needs_more_data_before_open = (why == \"body_size=3\" and transfer_encoding == \"chunked\")\n    if needs_more_data_before_open:\n        playbook >> DataReceived(tctx.client, b\"3\\r\\ndef\\r\\n\")\n\n    playbook << OpenConnection(server)\n    playbook >> reply(None)\n    playbook << SendData(server, b\"POST / HTTP/1.1\\r\\n\"\n                                 b\"Host: example.com\\r\\n\")\n\n    if transfer_encoding == \"identity\":\n        playbook << SendData(server, b\"Content-Length: 9\\r\\n\\r\\n\"\n                                     b\"abc\")\n        playbook >> DataReceived(tctx.client, b\"def\")\n        playbook << SendData(server, b\"def\")\n    else:\n        if needs_more_data_before_open:\n            playbook << SendData(server, b\"Transfer-Encoding: chunked\\r\\n\\r\\n\"\n                                         b\"6\\r\\nabcdef\\r\\n\")\n        else:\n            playbook << SendData(server, b\"Transfer-Encoding: chunked\\r\\n\\r\\n\"\n                                         b\"3\\r\\nabc\\r\\n\")\n            playbook >> DataReceived(tctx.client, b\"3\\r\\ndef\\r\\n\")\n            playbook << SendData(server, b\"3\\r\\ndef\\r\\n\")\n\n    if response == \"normal response\":\n        if transfer_encoding == \"identity\":\n            playbook >> DataReceived(tctx.client, b\"ghi\")\n            playbook << SendData(server, b\"ghi\")\n        else:\n            playbook >> DataReceived(tctx.client, b\"3\\r\\nghi\\r\\n0\\r\\n\\r\\n\")\n            playbook << SendData(server, b\"3\\r\\nghi\\r\\n\")\n\n        playbook << http.HttpRequestHook(flow)\n        playbook >> reply()\n        if transfer_encoding == \"chunked\":\n            playbook << SendData(server, b\"0\\r\\n\\r\\n\")\n        assert (\n            playbook\n            >> DataReceived(server, b\"HTTP/1.1 200 OK\\r\\nContent-Length: 0\\r\\n\\r\\n\")\n            << http.HttpResponseHeadersHook(flow)\n            >> reply()\n            << http.HttpResponseHook(flow)\n            >> reply()\n            << SendData(tctx.client, b\"HTTP/1.1 200 OK\\r\\nContent-Length: 0\\r\\n\\r\\n\")\n        )\n    elif response == \"early response\":\n        # We may receive a response before we have finished sending our request.\n        # We continue sending unless the server closes the connection.\n        # https://tools.ietf.org/html/rfc7231#section-6.5.11\n        assert (\n            playbook\n            >> DataReceived(server, b\"HTTP/1.1 413 Request Entity Too Large\\r\\nContent-Length: 0\\r\\n\\r\\n\")\n            << http.HttpResponseHeadersHook(flow)\n            >> reply()\n            << http.HttpResponseHook(flow)\n            >> reply()\n            << SendData(tctx.client, b\"HTTP/1.1 413 Request Entity Too Large\\r\\nContent-Length: 0\\r\\n\\r\\n\")\n        )\n        if transfer_encoding == \"identity\":\n            playbook >> DataReceived(tctx.client, b\"ghi\")\n            playbook << SendData(server, b\"ghi\")\n        else:\n            playbook >> DataReceived(tctx.client, b\"3\\r\\nghi\\r\\n0\\r\\n\\r\\n\")\n            playbook << SendData(server, b\"3\\r\\nghi\\r\\n\")\n        playbook << http.HttpRequestHook(flow)\n        playbook >> reply()\n        if transfer_encoding == \"chunked\":\n            playbook << SendData(server, b\"0\\r\\n\\r\\n\")\n        assert playbook\n    elif response == \"early close\":\n        assert (\n                playbook\n                >> DataReceived(server, b\"HTTP/1.1 413 Request Entity Too Large\\r\\nContent-Length: 0\\r\\n\\r\\n\")\n                << http.HttpResponseHeadersHook(flow)\n                >> reply()\n                << http.HttpResponseHook(flow)\n                >> reply()\n                << SendData(tctx.client, b\"HTTP/1.1 413 Request Entity Too Large\\r\\nContent-Length: 0\\r\\n\\r\\n\")\n                >> ConnectionClosed(server)\n                << CloseConnection(server)\n                << CloseConnection(tctx.client)\n        )\n    elif response == \"early kill\":\n        err = Placeholder(bytes)\n        assert (\n                playbook\n                >> ConnectionClosed(server)\n                << CloseConnection(server)\n                << http.HttpErrorHook(flow)\n                >> reply()\n                << SendData(tctx.client, err)\n                << CloseConnection(tctx.client)\n        )\n        assert b\"502 Bad Gateway\" in err()\n    else:  # pragma: no cover\n        assert False\n\n\n@pytest.mark.parametrize(\"where\", [\"request\", \"response\"])\n@pytest.mark.parametrize(\"transfer_encoding\", [\"identity\", \"chunked\"])\ndef test_body_size_limit(tctx, where, transfer_encoding):\n    \"\"\"Test HTTP request body_size_limit\"\"\"\n    tctx.options.body_size_limit = \"3\"\n    err = Placeholder(bytes)\n    flow = Placeholder(HTTPFlow)\n\n    if transfer_encoding == \"identity\":\n        body = b\"Content-Length: 6\\r\\n\\r\\nabcdef\"\n    else:\n        body = b\"Transfer-Encoding: chunked\\r\\n\\r\\n6\\r\\nabcdef\"\n\n    if where == \"request\":\n        assert (\n            Playbook(http.HttpLayer(tctx, HTTPMode.regular))\n            >> DataReceived(tctx.client, b\"POST http://example.com/ HTTP/1.1\\r\\n\"\n                                         b\"Host: example.com\\r\\n\" + body)\n            << http.HttpRequestHeadersHook(flow)\n            >> reply()\n            << http.HttpErrorHook(flow)\n            >> reply()\n            << SendData(tctx.client, err)\n            << CloseConnection(tctx.client)\n        )\n        assert b\"413 Payload Too Large\" in err()\n        assert b\"body_size_limit\" in err()\n        assert not flow().live\n    else:\n        server = Placeholder(Server)\n        assert (\n            Playbook(http.HttpLayer(tctx, HTTPMode.regular))\n            >> DataReceived(tctx.client, b\"GET http://example.com/ HTTP/1.1\\r\\n\"\n                                         b\"Host: example.com\\r\\n\\r\\n\")\n            << http.HttpRequestHeadersHook(flow)\n            >> reply()\n            << http.HttpRequestHook(flow)\n            >> reply()\n            << OpenConnection(server)\n            >> reply(None)\n            << SendData(server, b\"GET / HTTP/1.1\\r\\n\"\n                                b\"Host: example.com\\r\\n\\r\\n\")\n            >> DataReceived(server, b\"HTTP/1.1 200 OK\\r\\n\" + body)\n            << http.HttpResponseHeadersHook(flow)\n            >> reply()\n            << http.HttpErrorHook(flow)\n            >> reply()\n            << SendData(tctx.client, err)\n            << CloseConnection(tctx.client)\n            << CloseConnection(server)\n        )\n        assert b\"502 Bad Gateway\" in err()\n        assert b\"body_size_limit\" in err()\n        assert not flow().live\n\n\n@pytest.mark.parametrize(\"connect\", [True, False])\ndef test_server_unreachable(tctx, connect):\n    \"\"\"Test the scenario where the target server is unreachable.\"\"\"\n    tctx.options.connection_strategy = \"eager\"\n    server = Placeholder(Server)\n    flow = Placeholder(HTTPFlow)\n    err = Placeholder(bytes)\n    playbook = Playbook(http.HttpLayer(tctx, HTTPMode.regular), hooks=False)\n    if connect:\n        playbook >> DataReceived(tctx.client, b\"CONNECT example.com:443 HTTP/1.1\\r\\n\\r\\n\")\n    else:\n        playbook >> DataReceived(tctx.client, b\"GET http://example.com/ HTTP/1.1\\r\\n\\r\\n\")\n\n    playbook << OpenConnection(server)\n    playbook >> reply(\"Connection failed\")\n    if not connect:\n        # Our API isn't ideal here, there is no error hook for CONNECT requests currently.\n        # We could fix this either by having CONNECT request go through all our regular hooks,\n        # or by adding dedicated ok/error hooks.\n        playbook << http.HttpErrorHook(flow)\n        playbook >> reply()\n    playbook << SendData(tctx.client, err)\n    if not connect:\n        playbook << CloseConnection(tctx.client)\n\n    assert playbook\n    if not connect:\n        assert flow().error\n        assert not flow().live\n    assert b\"502 Bad Gateway\" in err()\n    assert b\"Connection failed\" in err()\n\n\n@pytest.mark.parametrize(\"data\", [\n    None,\n    b\"I don't speak HTTP.\",\n    b\"HTTP/1.1 200 OK\\r\\nContent-Length: 10\\r\\n\\r\\nweee\"\n])\ndef test_server_aborts(tctx, data):\n    \"\"\"Test the scenario where the server doesn't serve a response\"\"\"\n    server = Placeholder(Server)\n    flow = Placeholder(HTTPFlow)\n    err = Placeholder(bytes)\n    playbook = Playbook(http.HttpLayer(tctx, HTTPMode.regular), hooks=False)\n    assert (\n            playbook\n            >> DataReceived(tctx.client, b\"GET http://example.com/ HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\")\n            << OpenConnection(server)\n            >> reply(None)\n            << SendData(server, b\"GET / HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\")\n    )\n    if data:\n        playbook >> DataReceived(server, data)\n    assert (\n            playbook\n            >> ConnectionClosed(server)\n            << CloseConnection(server)\n            << http.HttpErrorHook(flow)\n            >> reply()\n            << SendData(tctx.client, err)\n            << CloseConnection(tctx.client)\n    )\n    assert flow().error\n    assert b\"502 Bad Gateway\" in err()\n    assert not flow().live\n\n\n@pytest.mark.parametrize(\"redirect\", [\"\", \"change-destination\", \"change-proxy\"])\n@pytest.mark.parametrize(\"scheme\", [\"http\", \"https\"])\ndef test_upstream_proxy(tctx, redirect, scheme):\n    \"\"\"Test that an upstream HTTP proxy is used.\"\"\"\n    server = Placeholder(Server)\n    server2 = Placeholder(Server)\n    flow = Placeholder(HTTPFlow)\n    tctx.options.mode = \"upstream:http://proxy:8080\"\n    playbook = Playbook(http.HttpLayer(tctx, HTTPMode.upstream), hooks=False)\n\n    if scheme == \"http\":\n        assert (\n                playbook\n                >> DataReceived(tctx.client, b\"GET http://example.com/ HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\")\n                << OpenConnection(server)\n                >> reply(None)\n                << SendData(server, b\"GET http://example.com/ HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\")\n        )\n\n    else:\n        assert (\n                playbook\n                >> DataReceived(tctx.client, b\"CONNECT example.com:443 HTTP/1.1\\r\\n\\r\\n\")\n                << SendData(tctx.client, b\"HTTP/1.1 200 Connection established\\r\\n\\r\\n\")\n                >> DataReceived(tctx.client, b\"GET / HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\")\n                << layer.NextLayerHook(Placeholder())\n                >> reply_next_layer(lambda ctx: http.HttpLayer(ctx, HTTPMode.transparent))\n                << OpenConnection(server)\n                >> reply(None)\n                << SendData(server, b\"CONNECT example.com:443 HTTP/1.1\\r\\n\\r\\n\")\n                >> DataReceived(server, b\"HTTP/1.1 200 Connection established\\r\\n\\r\\n\")\n                << SendData(server, b\"GET / HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\")\n        )\n\n    playbook >> DataReceived(server, b\"HTTP/1.1 418 OK\\r\\nContent-Length: 0\\r\\n\\r\\n\")\n    playbook << SendData(tctx.client, b\"HTTP/1.1 418 OK\\r\\nContent-Length: 0\\r\\n\\r\\n\")\n\n    assert playbook\n    assert server().address == (\"proxy\", 8080)\n\n    if scheme == \"http\":\n        playbook >> DataReceived(tctx.client, b\"GET http://example.com/two HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\")\n    else:\n        playbook >> DataReceived(tctx.client, b\"GET /two HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\")\n\n    assert (playbook << http.HttpRequestHook(flow))\n    if redirect == \"change-destination\":\n        flow().request.host = \"other-server\"\n        flow().request.host_header = \"example.com\"\n    elif redirect == \"change-proxy\":\n        flow().server_conn.via = ServerSpec(\"http\", address=(\"other-proxy\", 1234))\n    playbook >> reply()\n\n    if redirect:\n        # Protocol-wise we wouldn't need to open a new connection for plain http host redirects,\n        # but we disregard this edge case to simplify implementation.\n        playbook << OpenConnection(server2)\n        playbook >> reply(None)\n    else:\n        server2 = server\n\n    if scheme == \"http\":\n        if redirect == \"change-destination\":\n            playbook << SendData(server2, b\"GET http://other-server/two HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\")\n        else:\n            playbook << SendData(server2, b\"GET http://example.com/two HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\")\n    else:\n        if redirect == \"change-destination\":\n            playbook << SendData(server2, b\"CONNECT other-server:443 HTTP/1.1\\r\\n\\r\\n\")\n            playbook >> DataReceived(server2, b\"HTTP/1.1 200 Connection established\\r\\n\\r\\n\")\n        elif redirect == \"change-proxy\":\n            playbook << SendData(server2, b\"CONNECT example.com:443 HTTP/1.1\\r\\n\\r\\n\")\n            playbook >> DataReceived(server2, b\"HTTP/1.1 200 Connection established\\r\\n\\r\\n\")\n        playbook << SendData(server2, b\"GET /two HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\")\n\n    playbook >> DataReceived(server2, b\"HTTP/1.1 418 OK\\r\\nContent-Length: 0\\r\\n\\r\\n\")\n    playbook << SendData(tctx.client, b\"HTTP/1.1 418 OK\\r\\nContent-Length: 0\\r\\n\\r\\n\")\n\n    assert playbook\n\n    if redirect == \"change-destination\":\n        assert flow().server_conn.address[0] == \"other-server\"\n    else:\n        assert flow().server_conn.address[0] == \"example.com\"\n\n    if redirect == \"change-proxy\":\n        assert server2().address == flow().server_conn.via.address == (\"other-proxy\", 1234)\n    else:\n        assert server2().address == flow().server_conn.via.address == (\"proxy\", 8080)\n\n    assert (\n            playbook\n            >> ConnectionClosed(tctx.client)\n            << CloseConnection(tctx.client)\n    )\n\n\n@pytest.mark.parametrize(\"mode\", [\"regular\", \"upstream\"])\n@pytest.mark.parametrize(\"close_first\", [\"client\", \"server\"])\ndef test_http_proxy_tcp(tctx, mode, close_first):\n    \"\"\"Test TCP over HTTP CONNECT.\"\"\"\n    server = Placeholder(Server)\n    f = Placeholder(TCPFlow)\n    tctx.options.connection_strategy = \"lazy\"\n\n    if mode == \"upstream\":\n        tctx.options.mode = \"upstream:http://proxy:8080\"\n        toplayer = http.HttpLayer(tctx, HTTPMode.upstream)\n    else:\n        tctx.options.mode = \"regular\"\n        toplayer = http.HttpLayer(tctx, HTTPMode.regular)\n\n    playbook = Playbook(toplayer, hooks=False)\n    assert (\n            playbook\n            >> DataReceived(tctx.client, b\"CONNECT example:443 HTTP/1.1\\r\\n\\r\\n\")\n            << SendData(tctx.client, b\"HTTP/1.1 200 Connection established\\r\\n\\r\\n\")\n            >> DataReceived(tctx.client, b\"this is not http\")\n            << layer.NextLayerHook(Placeholder())\n            >> reply_next_layer(lambda ctx: TCPLayer(ctx, ignore=False))\n            << TcpStartHook(f)\n            >> reply()\n            << OpenConnection(server)\n    )\n\n    playbook >> reply(None)\n    if mode == \"upstream\":\n        playbook << SendData(server, b\"CONNECT example:443 HTTP/1.1\\r\\n\\r\\n\")\n        playbook >> DataReceived(server, b\"HTTP/1.1 200 Connection established\\r\\n\\r\\n\")\n\n    assert (\n            playbook\n            << SendData(server, b\"this is not http\")\n            >> DataReceived(server, b\"true that\")\n            << SendData(tctx.client, b\"true that\")\n    )\n\n    if mode == \"regular\":\n        assert server().address == (\"example\", 443)\n    else:\n        assert server().address == (\"proxy\", 8080)\n\n    assert (\n        playbook\n        >> TcpMessageInjected(f, TCPMessage(False, b\"fake news from your friendly man-in-the-middle\"))\n        << SendData(tctx.client, b\"fake news from your friendly man-in-the-middle\")\n    )\n\n    if close_first == \"client\":\n        a, b = tctx.client, server\n    else:\n        a, b = server, tctx.client\n    assert (\n            playbook\n            >> ConnectionClosed(a)\n            << CloseConnection(b)\n            >> ConnectionClosed(b)\n            << CloseConnection(a)\n    )\n\n\n@pytest.mark.parametrize(\"strategy\", [\"eager\", \"lazy\"])\ndef test_proxy_chain(tctx, strategy):\n    server = Placeholder(Server)\n    tctx.options.connection_strategy = strategy\n    playbook = Playbook(http.HttpLayer(tctx, HTTPMode.regular), hooks=False)\n\n    playbook >> DataReceived(tctx.client, b\"CONNECT proxy:8080 HTTP/1.1\\r\\n\\r\\n\")\n    if strategy == \"eager\":\n        playbook << OpenConnection(server)\n        playbook >> reply(None)\n    playbook << SendData(tctx.client, b\"HTTP/1.1 200 Connection established\\r\\n\\r\\n\")\n\n    playbook >> DataReceived(tctx.client, b\"CONNECT second-proxy:8080 HTTP/1.1\\r\\n\\r\\n\")\n    playbook << layer.NextLayerHook(Placeholder())\n    playbook >> reply_next_layer(lambda ctx: http.HttpLayer(ctx, HTTPMode.transparent))\n    playbook << SendData(tctx.client,\n                         b\"HTTP/1.1 502 Bad Gateway\\r\\n\"\n                         b\"content-length: 198\\r\\n\"\n                         b\"\\r\\n\"\n                         b\"mitmproxy received an HTTP CONNECT request even though it is not running in regular/upstream mode. \"\n                         b\"This usually indicates a misconfiguration, please see the mitmproxy mode documentation for details.\")\n\n    assert playbook\n\n\ndef test_no_headers(tctx):\n    \"\"\"Test that we can correctly reassemble requests/responses with no headers.\"\"\"\n    server = Placeholder(Server)\n    assert (\n            Playbook(http.HttpLayer(tctx, HTTPMode.regular), hooks=False)\n            >> DataReceived(tctx.client, b\"GET http://example.com/ HTTP/1.1\\r\\n\\r\\n\")\n            << OpenConnection(server)\n            >> reply(None)\n            << SendData(server, b\"GET / HTTP/1.1\\r\\n\\r\\n\")\n            >> DataReceived(server, b\"HTTP/1.1 204 No Content\\r\\n\\r\\n\")\n            << SendData(tctx.client, b\"HTTP/1.1 204 No Content\\r\\n\\r\\n\")\n    )\n    assert server().address == (\"example.com\", 80)\n\n\ndef test_http_proxy_relative_request(tctx):\n    \"\"\"Test handling of a relative-form \"GET /\" in regular proxy mode.\"\"\"\n    server = Placeholder(Server)\n    assert (\n            Playbook(http.HttpLayer(tctx, HTTPMode.regular), hooks=False)\n            >> DataReceived(tctx.client, b\"GET / HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\")\n            << OpenConnection(server)\n            >> reply(None)\n            << SendData(server, b\"GET / HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\")\n            >> DataReceived(server, b\"HTTP/1.1 204 No Content\\r\\n\\r\\n\")\n            << SendData(tctx.client, b\"HTTP/1.1 204 No Content\\r\\n\\r\\n\")\n    )\n    assert server().address == (\"example.com\", 80)\n\n\ndef test_http_proxy_relative_request_no_host_header(tctx):\n    \"\"\"Test handling of a relative-form \"GET /\" in regular proxy mode, but without a host header.\"\"\"\n    err = Placeholder(bytes)\n    assert (\n            Playbook(http.HttpLayer(tctx, HTTPMode.regular), hooks=False)\n            >> DataReceived(tctx.client, b\"GET / HTTP/1.1\\r\\n\\r\\n\")\n            << SendData(tctx.client, err)\n            << CloseConnection(tctx.client)\n    )\n    assert b\"400 Bad Request\" in err()\n    assert b\"HTTP request has no host header, destination unknown.\" in err()\n\n\ndef test_http_expect(tctx):\n    \"\"\"Test handling of a 'Expect: 100-continue' header.\"\"\"\n    server = Placeholder(Server)\n    assert (\n            Playbook(http.HttpLayer(tctx, HTTPMode.regular), hooks=False)\n            >> DataReceived(tctx.client, b\"PUT http://example.com/large-file HTTP/1.1\\r\\n\"\n                                         b\"Host: example.com\\r\\n\"\n                                         b\"Content-Length: 15\\r\\n\"\n                                         b\"Expect: 100-continue\\r\\n\\r\\n\")\n            << SendData(tctx.client, b\"HTTP/1.1 100 Continue\\r\\n\\r\\n\")\n            >> DataReceived(tctx.client, b\"lots of content\")\n            << OpenConnection(server)\n            >> reply(None)\n            << SendData(server, b\"PUT /large-file HTTP/1.1\\r\\n\"\n                                b\"Host: example.com\\r\\n\"\n                                b\"Content-Length: 15\\r\\n\\r\\n\"\n                                b\"lots of content\")\n            >> DataReceived(server, b\"HTTP/1.1 201 Created\\r\\nContent-Length: 0\\r\\n\\r\\n\")\n            << SendData(tctx.client, b\"HTTP/1.1 201 Created\\r\\nContent-Length: 0\\r\\n\\r\\n\")\n    )\n    assert server().address == (\"example.com\", 80)\n\n\n@pytest.mark.parametrize(\"stream\", [True, False])\ndef test_http_client_aborts(tctx, stream):\n    \"\"\"Test handling of the case where a client aborts during request transmission.\"\"\"\n    server = Placeholder(Server)\n    flow = Placeholder(HTTPFlow)\n    playbook = Playbook(http.HttpLayer(tctx, HTTPMode.regular), hooks=True)\n\n    def enable_streaming(flow: HTTPFlow):\n        flow.request.stream = True\n\n    assert (\n            playbook\n            >> DataReceived(tctx.client, b\"POST http://example.com/ HTTP/1.1\\r\\n\"\n                                         b\"Host: example.com\\r\\n\"\n                                         b\"Content-Length: 6\\r\\n\"\n                                         b\"\\r\\n\"\n                                         b\"abc\")\n            << http.HttpRequestHeadersHook(flow)\n    )\n    if stream:\n        assert (\n                playbook\n                >> reply(side_effect=enable_streaming)\n                << OpenConnection(server)\n                >> reply(None)\n                << SendData(server, b\"POST / HTTP/1.1\\r\\n\"\n                                    b\"Host: example.com\\r\\n\"\n                                    b\"Content-Length: 6\\r\\n\"\n                                    b\"\\r\\n\"\n                                    b\"abc\")\n        )\n    else:\n        assert playbook >> reply()\n    (\n            playbook\n            >> ConnectionClosed(tctx.client)\n            << CloseConnection(tctx.client)\n    )\n    if stream:\n        playbook << CloseConnection(server)\n    assert (\n            playbook\n            << http.HttpErrorHook(flow)\n            >> reply()\n            << None\n    )\n\n    assert \"peer closed connection\" in flow().error.msg\n    assert not flow().live\n\n\n@pytest.mark.parametrize(\"stream\", [True, False])\ndef test_http_server_aborts(tctx, stream):\n    \"\"\"Test handling of the case where a server aborts during response transmission.\"\"\"\n    server = Placeholder(Server)\n    flow = Placeholder(HTTPFlow)\n    playbook = Playbook(http.HttpLayer(tctx, HTTPMode.regular))\n\n    def enable_streaming(flow: HTTPFlow):\n        flow.response.stream = True\n\n    assert (\n            playbook\n            >> DataReceived(tctx.client, b\"GET http://example.com/ HTTP/1.1\\r\\n\"\n                                         b\"Host: example.com\\r\\n\\r\\n\")\n            << http.HttpRequestHeadersHook(flow)\n            >> reply()\n            << http.HttpRequestHook(flow)\n            >> reply()\n            << OpenConnection(server)\n            >> reply(None)\n            << SendData(server, b\"GET / HTTP/1.1\\r\\n\"\n                                b\"Host: example.com\\r\\n\\r\\n\")\n            >> DataReceived(server, b\"HTTP/1.1 200 OK\\r\\n\"\n                                    b\"Content-Length: 6\\r\\n\"\n                                    b\"\\r\\n\"\n                                    b\"abc\")\n            << http.HttpResponseHeadersHook(flow)\n    )\n    if stream:\n        assert (\n                playbook\n                >> reply(side_effect=enable_streaming)\n                << SendData(tctx.client, b\"HTTP/1.1 200 OK\\r\\n\"\n                                         b\"Content-Length: 6\\r\\n\"\n                                         b\"\\r\\n\"\n                                         b\"abc\")\n        )\n    else:\n        assert playbook >> reply()\n    assert (\n            playbook\n            >> ConnectionClosed(server)\n            << CloseConnection(server)\n            << http.HttpErrorHook(flow)\n    )\n    if stream:\n        assert (\n                playbook\n                >> reply()\n                << CloseConnection(tctx.client)\n        )\n    else:\n        error_html = Placeholder(bytes)\n        assert (\n                playbook\n                >> reply()\n                << SendData(tctx.client, error_html)\n                << CloseConnection(tctx.client)\n        )\n        assert b\"502 Bad Gateway\" in error_html()\n        assert b\"peer closed connection\" in error_html()\n\n    assert \"peer closed connection\" in flow().error.msg\n    assert not flow().live\n\n\n@pytest.mark.parametrize(\"when\", [\"http_connect\", \"requestheaders\", \"request\", \"script-response-responseheaders\",\n                                  \"responseheaders\",\n                                  \"response\", \"error\"])\ndef test_kill_flow(tctx, when):\n    \"\"\"Test that we properly kill flows if instructed to do so\"\"\"\n    tctx.options.connection_strategy = \"lazy\"\n    server = Placeholder(Server)\n    connect_flow = Placeholder(HTTPFlow)\n    flow = Placeholder(HTTPFlow)\n\n    def kill(flow: HTTPFlow):\n        # Can't use flow.kill() here because that currently still depends on a reply object.\n        flow.error = Error(Error.KILLED_MESSAGE)\n\n    def assert_kill(err_hook: bool = True):\n        playbook >> reply(side_effect=kill)\n        if err_hook:\n            playbook << http.HttpErrorHook(flow)\n            playbook >> reply()\n        playbook << CloseConnection(tctx.client)\n        assert playbook\n        if flow():\n            assert not flow().live\n\n    playbook = Playbook(http.HttpLayer(tctx, HTTPMode.regular))\n    assert (playbook\n            >> DataReceived(tctx.client, b\"CONNECT example.com:80 HTTP/1.1\\r\\n\\r\\n\")\n            << http.HttpConnectHook(connect_flow))\n    if when == \"http_connect\":\n        return assert_kill(False)\n    assert (playbook\n            >> reply()\n            << SendData(tctx.client, b'HTTP/1.1 200 Connection established\\r\\n\\r\\n')\n            >> DataReceived(tctx.client, b\"GET /foo?hello=1 HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\")\n            << layer.NextLayerHook(Placeholder())\n            >> reply_next_layer(lambda ctx: http.HttpLayer(ctx, HTTPMode.transparent))\n            << http.HttpRequestHeadersHook(flow))\n    if when == \"requestheaders\":\n        return assert_kill()\n    assert (playbook\n            >> reply()\n            << http.HttpRequestHook(flow))\n    if when == \"request\":\n        return assert_kill()\n    if when == \"script-response-responseheaders\":\n        assert (playbook\n                >> reply(side_effect=lambda f: setattr(f, \"response\", Response.make()))\n                << http.HttpResponseHeadersHook(flow))\n        return assert_kill()\n    assert (playbook\n            >> reply()\n            << OpenConnection(server)\n            >> reply(None)\n            << SendData(server, b\"GET /foo?hello=1 HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\")\n            >> DataReceived(server, b\"HTTP/1.1 200 OK\\r\\nContent-Length: 12\\r\\n\\r\\nHello World\")\n            << http.HttpResponseHeadersHook(flow))\n    if when == \"responseheaders\":\n        return assert_kill()\n\n    if when == \"response\":\n        assert (playbook\n                >> reply()\n                >> DataReceived(server, b\"!\")\n                << http.HttpResponseHook(flow))\n        return assert_kill(False)\n    elif when == \"error\":\n        assert (playbook\n                >> reply()\n                >> ConnectionClosed(server)\n                << CloseConnection(server)\n                << http.HttpErrorHook(flow))\n        return assert_kill(False)\n    else:\n        raise AssertionError\n\n\ndef test_close_during_connect_hook(tctx):\n    flow = Placeholder(HTTPFlow)\n    assert (\n            Playbook(http.HttpLayer(tctx, HTTPMode.regular))\n            >> DataReceived(tctx.client,\n                            b'CONNECT hi.ls:443 HTTP/1.1\\r\\n'\n                            b'Proxy-Connection: keep-alive\\r\\n'\n                            b'Connection: keep-alive\\r\\n'\n                            b'Host: hi.ls:443\\r\\n\\r\\n')\n            << http.HttpConnectHook(flow)\n            >> ConnectionClosed(tctx.client)\n            << CloseConnection(tctx.client)\n            >> reply(to=-3)\n    )\n\n\n@pytest.mark.parametrize(\"client_close\", [b\"\", b\"Connection: close\\r\\n\"])\n@pytest.mark.parametrize(\"server_close\", [b\"\", b\"Connection: close\\r\\n\"])\ndef test_connection_close_header(tctx, client_close, server_close):\n    \"\"\"Test that we correctly close connections if we have a `Connection: close` header.\"\"\"\n    if not client_close and not server_close:\n        return\n    server = Placeholder(Server)\n    assert (\n            Playbook(http.HttpLayer(tctx, HTTPMode.regular), hooks=False)\n            >> DataReceived(tctx.client, b\"GET http://example/ HTTP/1.1\\r\\n\"\n                                         b\"Host: example\\r\\n\" + client_close +\n                            b\"\\r\\n\")\n            << OpenConnection(server)\n            >> reply(None)\n            << SendData(server, b\"GET / HTTP/1.1\\r\\n\"\n                                b\"Host: example\\r\\n\" + client_close +\n                        b\"\\r\\n\")\n            >> DataReceived(server, b\"HTTP/1.1 200 OK\\r\\n\"\n                                    b\"Content-Length: 0\\r\\n\" + server_close +\n                            b\"\\r\\n\")\n            << CloseConnection(server)\n            << SendData(tctx.client, b\"HTTP/1.1 200 OK\\r\\n\"\n                                     b\"Content-Length: 0\\r\\n\" + server_close +\n                        b\"\\r\\n\")\n            << CloseConnection(tctx.client)\n    )\n\n\n@pytest.mark.parametrize(\"proto\", [\"websocket\", \"tcp\", \"none\"])\ndef test_upgrade(tctx, proto):\n    \"\"\"Test a HTTP -> WebSocket upgrade with different protocols enabled\"\"\"\n    if proto != \"websocket\":\n        tctx.options.websocket = False\n    if proto != \"tcp\":\n        tctx.options.rawtcp = False\n\n    tctx.server.address = (\"example.com\", 80)\n    tctx.server.state = ConnectionState.OPEN\n    http_flow = Placeholder(HTTPFlow)\n    playbook = Playbook(http.HttpLayer(tctx, HTTPMode.transparent))\n    (\n            playbook\n            >> DataReceived(tctx.client,\n                            b\"GET / HTTP/1.1\\r\\n\"\n                            b\"Connection: upgrade\\r\\n\"\n                            b\"Upgrade: websocket\\r\\n\"\n                            b\"Sec-WebSocket-Version: 13\\r\\n\"\n                            b\"\\r\\n\")\n            << http.HttpRequestHeadersHook(http_flow)\n            >> reply()\n            << http.HttpRequestHook(http_flow)\n            >> reply()\n            << SendData(tctx.server, b\"GET / HTTP/1.1\\r\\n\"\n                                     b\"Connection: upgrade\\r\\n\"\n                                     b\"Upgrade: websocket\\r\\n\"\n                                     b\"Sec-WebSocket-Version: 13\\r\\n\"\n                                     b\"\\r\\n\")\n            >> DataReceived(tctx.server, b\"HTTP/1.1 101 Switching Protocols\\r\\n\"\n                                         b\"Upgrade: websocket\\r\\n\"\n                                         b\"Connection: Upgrade\\r\\n\"\n                                         b\"\\r\\n\")\n            << http.HttpResponseHeadersHook(http_flow)\n            >> reply()\n            << http.HttpResponseHook(http_flow)\n            >> reply()\n            << SendData(tctx.client, b\"HTTP/1.1 101 Switching Protocols\\r\\n\"\n                                     b\"Upgrade: websocket\\r\\n\"\n                                     b\"Connection: Upgrade\\r\\n\"\n                                     b\"\\r\\n\")\n    )\n    if proto == \"websocket\":\n        assert playbook << WebsocketStartHook(http_flow)\n    elif proto == \"tcp\":\n        assert playbook << TcpStartHook(Placeholder(TCPFlow))\n    else:\n        assert (\n            playbook\n            << Log(\"Sent HTTP 101 response, but no protocol is enabled to upgrade to.\", \"warn\")\n            << CloseConnection(tctx.client)\n        )\n\n\ndef test_dont_reuse_closed(tctx):\n    \"\"\"Test that a closed connection is not reused.\"\"\"\n    server = Placeholder(Server)\n    server2 = Placeholder(Server)\n    assert (\n            Playbook(http.HttpLayer(tctx, HTTPMode.regular), hooks=False)\n            >> DataReceived(tctx.client, b\"GET http://example.com/ HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\")\n            << OpenConnection(server)\n            >> reply(None)\n            << SendData(server, b\"GET / HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\")\n            >> DataReceived(server, b\"HTTP/1.1 200 OK\\r\\nContent-Length: 0\\r\\n\\r\\n\")\n            << SendData(tctx.client, b\"HTTP/1.1 200 OK\\r\\nContent-Length: 0\\r\\n\\r\\n\")\n            >> ConnectionClosed(server)\n            << CloseConnection(server)\n            >> DataReceived(tctx.client, b\"GET http://example.com/two HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\")\n            << OpenConnection(server2)\n            >> reply(None)\n            << SendData(server2, b\"GET /two HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\")\n            >> DataReceived(server2, b\"HTTP/1.1 200 OK\\r\\nContent-Length: 0\\r\\n\\r\\n\")\n            << SendData(tctx.client, b\"HTTP/1.1 200 OK\\r\\nContent-Length: 0\\r\\n\\r\\n\")\n    )\n\n\ndef test_reuse_error(tctx):\n    \"\"\"Test that an errored connection is reused.\"\"\"\n    tctx.server.address = (\"example.com\", 443)\n    tctx.server.error = \"tls verify failed\"\n    error_html = Placeholder(bytes)\n    assert (\n            Playbook(http.HttpLayer(tctx, HTTPMode.transparent), hooks=False)\n            >> DataReceived(tctx.client, b\"GET / HTTP/1.1\\r\\n\\r\\n\")\n            << SendData(tctx.client, error_html)\n            << CloseConnection(tctx.client)\n    )\n    assert b\"502 Bad Gateway\" in error_html()\n    assert b\"tls verify failed\" in error_html()\n\n\ndef test_transparent_sni(tctx):\n    \"\"\"Test that we keep the SNI in lazy transparent mode.\"\"\"\n    tctx.client.sni = \"example.com\"\n    tctx.server.address = (\"192.0.2.42\", 443)\n    tctx.server.tls = True\n\n    flow = Placeholder(HTTPFlow)\n\n    server = Placeholder(Server)\n    assert (\n            Playbook(http.HttpLayer(tctx, HTTPMode.transparent))\n            >> DataReceived(tctx.client, b\"GET / HTTP/1.1\\r\\n\\r\\n\")\n            << http.HttpRequestHeadersHook(flow)\n            >> reply()\n            << http.HttpRequestHook(flow)\n            >> reply()\n            << OpenConnection(server)\n    )\n    assert server().address == (\"192.0.2.42\", 443)\n    assert server().sni == \"example.com\"\n\n\ndef test_original_server_disconnects(tctx):\n    \"\"\"Test that we correctly handle the case where the initial server conn is just closed.\"\"\"\n    tctx.server.state = ConnectionState.OPEN\n    assert (\n            Playbook(http.HttpLayer(tctx, HTTPMode.transparent))\n            >> ConnectionClosed(tctx.server)\n            << CloseConnection(tctx.server)\n    )\n\n\ndef test_request_smuggling(tctx):\n    \"\"\"Test that we reject request smuggling\"\"\"\n    err = Placeholder(bytes)\n    assert (\n        Playbook(http.HttpLayer(tctx, HTTPMode.regular), hooks=False)\n        >> DataReceived(tctx.client, b\"GET http://example.com/ HTTP/1.1\\r\\n\"\n                                     b\"Host: example.com\\r\\n\"\n                                     b\"Content-Length: 42\\r\\n\"\n                                     b\"Transfer-Encoding: chunked\\r\\n\\r\\n\")\n        << SendData(tctx.client, err)\n        << CloseConnection(tctx.client)\n    )\n    assert b\"Received both a Transfer-Encoding and a Content-Length header\" in err()\n\n\ndef test_request_smuggling_whitespace(tctx):\n    \"\"\"Test that we reject header names with whitespace\"\"\"\n    err = Placeholder(bytes)\n    assert (\n        Playbook(http.HttpLayer(tctx, HTTPMode.regular), hooks=False)\n        >> DataReceived(tctx.client, b\"GET http://example.com/ HTTP/1.1\\r\\n\"\n                                     b\"Host: example.com\\r\\n\"\n                                     b\"Content-Length : 42\\r\\n\\r\\n\")\n        << SendData(tctx.client, err)\n        << CloseConnection(tctx.client)\n    )\n    assert b\"Received an invalid header name\" in err()\n\n\ndef test_request_smuggling_validation_disabled(tctx):\n    \"\"\"Test that we don't reject request smuggling when validation is disabled.\"\"\"\n    tctx.options.validate_inbound_headers = False\n    assert (\n        Playbook(http.HttpLayer(tctx, HTTPMode.regular), hooks=False)\n        >> DataReceived(tctx.client, b\"GET http://example.com/ HTTP/1.1\\r\\n\"\n                                     b\"Host: example.com\\r\\n\"\n                                     b\"Content-Length: 4\\r\\n\"\n                                     b\"Transfer-Encoding: chunked\\r\\n\\r\\n\"\n                                     b\"4\\r\\n\"\n                                     b\"abcd\\r\\n\"\n                                     b\"0\\r\\n\"\n                                     b\"\\r\\n\")\n        << OpenConnection(Placeholder(Server))\n    )\n\n\ndef test_request_smuggling_te_te(tctx):\n    \"\"\"Test that we reject transfer-encoding headers that are weird in some way\"\"\"\n    err = Placeholder(bytes)\n    assert (\n        Playbook(http.HttpLayer(tctx, HTTPMode.regular), hooks=False)\n        >> DataReceived(tctx.client, (\"GET http://example.com/ HTTP/1.1\\r\\n\"\n                                      \"Host: example.com\\r\\n\"\n                                      \"Transfer-Encoding: chun\u212aed\\r\\n\\r\\n\").encode())  # note the non-standard \"\u212a\"\n        << SendData(tctx.client, err)\n        << CloseConnection(tctx.client)\n    )\n    assert b\"Invalid transfer encoding\" in err()\n\n\ndef test_invalid_content_length(tctx):\n    \"\"\"Test that we still trigger flow hooks for requests with semantic errors\"\"\"\n    err = Placeholder(bytes)\n    flow = Placeholder(HTTPFlow)\n    assert (\n        Playbook(http.HttpLayer(tctx, HTTPMode.regular))\n        >> DataReceived(tctx.client, (\"GET http://example.com/ HTTP/1.1\\r\\n\"\n                                      \"Host: example.com\\r\\n\"\n                                      \"Content-Length: NaN\\r\\n\\r\\n\").encode())\n        << SendData(tctx.client, err)\n        << CloseConnection(tctx.client)\n        << http.HttpRequestHeadersHook(flow)\n        >> reply()\n        << http.HttpErrorHook(flow)\n        >> reply()\n    )\n    assert b\"Invalid Content-Length header\" in err()\n\n\ndef test_chunked_and_content_length_set_by_addon(tctx):\n    \"\"\"Test that we don't crash when an addon sets a transfer-encoding header\n\n    We reject a request with both transfer-encoding and content-length header to\n    thwart request smuggling, but if a user explicitly sets it we should not crash.\n    \"\"\"\n    server = Placeholder(Server)\n    flow = Placeholder(HTTPFlow)\n\n    def make_chunked(flow: HTTPFlow):\n        if flow.response:\n            flow.response.headers[\"Transfer-Encoding\"] = \"chunked\"\n        else:\n            flow.request.headers[\"Transfer-Encoding\"] = \"chunked\"\n\n    assert (\n            Playbook(http.HttpLayer(tctx, HTTPMode.regular))\n            >> DataReceived(tctx.client, b\"POST http://example.com/ HTTP/1.1\\r\\n\"\n                                         b\"Host: example.com\\r\\n\"\n                                         b\"Content-Length: 0\\r\\n\\r\\n\")\n            << http.HttpRequestHeadersHook(flow)\n            >> reply(side_effect=make_chunked)\n            << http.HttpRequestHook(flow)\n            >> reply()\n            << OpenConnection(server)\n            >> reply(None)\n            << SendData(server, b\"POST / HTTP/1.1\\r\\n\"\n                                b\"Host: example.com\\r\\n\"\n                                b\"Content-Length: 0\\r\\n\"\n                                b\"Transfer-Encoding: chunked\\r\\n\\r\\n\"\n                                b\"0\\r\\n\\r\\n\")\n            >> DataReceived(server, b\"HTTP/1.1 200 OK\\r\\nContent-Length: 0\\r\\n\\r\\n\")\n            << http.HttpResponseHeadersHook(flow)\n            >> reply()\n            << http.HttpResponseHook(flow)\n            >> reply(side_effect=make_chunked)\n            << SendData(tctx.client, b\"HTTP/1.1 200 OK\\r\\n\"\n                                     b\"Content-Length: 0\\r\\n\"\n                                     b\"Transfer-Encoding: chunked\\r\\n\\r\\n\"\n                                     b\"0\\r\\n\\r\\n\")\n    )\n\n\ndef test_connect_more_newlines(tctx):\n    \"\"\"Ignore superfluous \\r\\n in CONNECT request, https://github.com/mitmproxy/mitmproxy/issues/4870\"\"\"\n    server = Placeholder(Server)\n    playbook = Playbook(http.HttpLayer(tctx, HTTPMode.regular))\n    nl = Placeholder(layer.NextLayer)\n\n    assert (\n        playbook\n        >> DataReceived(tctx.client, b\"CONNECT example.com:80 HTTP/1.1\\r\\n\\r\\n\\r\\n\")\n        << http.HttpConnectHook(Placeholder())\n        >> reply()\n        << OpenConnection(server)\n        >> reply(None)\n        << SendData(tctx.client, b'HTTP/1.1 200 Connection established\\r\\n\\r\\n')\n        >> DataReceived(tctx.client, b\"\\x16\\x03\\x03\\x00\\xb3\\x01\\x00\\x00\\xaf\\x03\\x03\")\n        << layer.NextLayerHook(nl)\n    )\n    assert nl().data_client() == b\"\\x16\\x03\\x03\\x00\\xb3\\x01\\x00\\x00\\xaf\\x03\\x03\"\n", "from typing import List, Tuple\n\nimport h2.settings\nimport hpack\nimport hyperframe.frame\nimport pytest\nfrom h2.errors import ErrorCodes\n\nfrom mitmproxy.connection import ConnectionState, Server\nfrom mitmproxy.flow import Error\nfrom mitmproxy.http import HTTPFlow, Headers, Request\nfrom mitmproxy.net.http import status_codes\nfrom mitmproxy.proxy.commands import CloseConnection, Log, OpenConnection, SendData\nfrom mitmproxy.proxy.context import Context\nfrom mitmproxy.proxy.events import ConnectionClosed, DataReceived\nfrom mitmproxy.proxy.layers import http\nfrom mitmproxy.proxy.layers.http import HTTPMode\nfrom mitmproxy.proxy.layers.http._http2 import Http2Client, split_pseudo_headers\nfrom test.mitmproxy.proxy.layers.http.hyper_h2_test_helpers import FrameFactory\nfrom test.mitmproxy.proxy.tutils import Placeholder, Playbook, reply\n\nexample_request_headers = (\n    (b':method', b'GET'),\n    (b':scheme', b'http'),\n    (b':path', b'/'),\n    (b':authority', b'example.com'),\n)\n\nexample_response_headers = (\n    (b':status', b'200'),\n)\n\nexample_request_trailers = (\n    (b'req-trailer-a', b'a'),\n    (b'req-trailer-b', b'b')\n)\n\nexample_response_trailers = (\n    (b'resp-trailer-a', b'a'),\n    (b'resp-trailer-b', b'b')\n)\n\n\n@pytest.fixture\ndef open_h2_server_conn():\n    # this is a bit fake here (port 80, with alpn, but no tls - c'mon),\n    # but we don't want to pollute our tests with TLS handshakes.\n    s = Server((\"example.com\", 80))\n    s.state = ConnectionState.OPEN\n    s.alpn = b\"h2\"\n    return s\n\n\ndef decode_frames(data: bytes) -> List[hyperframe.frame.Frame]:\n    # swallow preamble\n    if data.startswith(b\"PRI * HTTP/2.0\"):\n        data = data[24:]\n    frames = []\n    while data:\n        f, length = hyperframe.frame.Frame.parse_frame_header(data[:9])\n        f.parse_body(memoryview(data[9:9 + length]))\n        frames.append(f)\n        data = data[9 + length:]\n    return frames\n\n\ndef start_h2_client(tctx: Context) -> Tuple[Playbook, FrameFactory]:\n    tctx.client.alpn = b\"h2\"\n    frame_factory = FrameFactory()\n\n    playbook = Playbook(http.HttpLayer(tctx, HTTPMode.regular))\n    assert (\n            playbook\n            << SendData(tctx.client, Placeholder())  # initial settings frame\n            >> DataReceived(tctx.client, frame_factory.preamble())\n            >> DataReceived(tctx.client, frame_factory.build_settings_frame({}, ack=True).serialize())\n    )\n    return playbook, frame_factory\n\n\ndef make_h2(open_connection: OpenConnection) -> None:\n    open_connection.connection.alpn = b\"h2\"\n\n\ndef test_simple(tctx):\n    playbook, cff = start_h2_client(tctx)\n    flow = Placeholder(HTTPFlow)\n    server = Placeholder(Server)\n    initial = Placeholder(bytes)\n    assert (\n            playbook\n            >> DataReceived(tctx.client,\n                            cff.build_headers_frame(example_request_headers, flags=[\"END_STREAM\"]).serialize())\n            << http.HttpRequestHeadersHook(flow)\n            >> reply()\n            << http.HttpRequestHook(flow)\n            >> reply()\n            << OpenConnection(server)\n            >> reply(None, side_effect=make_h2)\n            << SendData(server, initial)\n    )\n    frames = decode_frames(initial())\n    assert [type(x) for x in frames] == [\n        hyperframe.frame.SettingsFrame,\n        hyperframe.frame.HeadersFrame,\n    ]\n    sff = FrameFactory()\n    assert (\n            playbook\n            # a conforming h2 server would send settings first, we disregard this for now.\n            >> DataReceived(server, sff.build_headers_frame(example_response_headers).serialize())\n            << http.HttpResponseHeadersHook(flow)\n            >> reply()\n            >> DataReceived(server, sff.build_data_frame(b\"Hello, World!\", flags=[\"END_STREAM\"]).serialize())\n            << http.HttpResponseHook(flow)\n            >> reply()\n            << SendData(tctx.client,\n                        cff.build_headers_frame(example_response_headers).serialize() +\n                        cff.build_data_frame(b\"Hello, World!\").serialize() +\n                        cff.build_data_frame(b\"\", flags=[\"END_STREAM\"]).serialize())\n    )\n    assert flow().request.url == \"http://example.com/\"\n    assert flow().response.text == \"Hello, World!\"\n\n\n@pytest.mark.parametrize(\"stream\", [\"stream\", \"\"])\ndef test_response_trailers(tctx: Context, open_h2_server_conn: Server, stream):\n    playbook, cff = start_h2_client(tctx)\n    tctx.server = open_h2_server_conn\n    sff = FrameFactory()\n\n    def enable_streaming(flow: HTTPFlow):\n        flow.response.stream = bool(stream)\n\n    flow = Placeholder(HTTPFlow)\n    (\n        playbook\n        >> DataReceived(tctx.client,\n                        cff.build_headers_frame(example_request_headers, flags=[\"END_STREAM\"]).serialize())\n        << http.HttpRequestHeadersHook(flow)\n        >> reply()\n        << http.HttpRequestHook(flow)\n        >> reply()\n        << SendData(tctx.server, Placeholder(bytes))\n        # a conforming h2 server would send settings first, we disregard this for now.\n        >> DataReceived(tctx.server, sff.build_headers_frame(example_response_headers).serialize() +\n                        sff.build_data_frame(b\"Hello, World!\").serialize())\n        << http.HttpResponseHeadersHook(flow)\n        >> reply(side_effect=enable_streaming)\n    )\n    if stream:\n        playbook << SendData(\n            tctx.client,\n            cff.build_headers_frame(example_response_headers).serialize() +\n            cff.build_data_frame(b\"Hello, World!\").serialize()\n        )\n    assert (\n        playbook\n        >> DataReceived(tctx.server, sff.build_headers_frame(example_response_trailers, flags=[\"END_STREAM\"]).serialize())\n        << http.HttpResponseHook(flow)\n    )\n    assert flow().response.trailers\n    del flow().response.trailers[\"resp-trailer-a\"]\n    if stream:\n        assert (\n            playbook\n            >> reply()\n            << SendData(tctx.client,\n                        cff.build_headers_frame(example_response_trailers[1:], flags=[\"END_STREAM\"]).serialize())\n        )\n    else:\n        assert (\n            playbook\n            >> reply()\n            << SendData(tctx.client,\n                        cff.build_headers_frame(example_response_headers).serialize() +\n                        cff.build_data_frame(b\"Hello, World!\").serialize() +\n                        cff.build_headers_frame(example_response_trailers[1:], flags=[\"END_STREAM\"]).serialize()))\n\n\n@pytest.mark.parametrize(\"stream\", [\"stream\", \"\"])\ndef test_request_trailers(tctx: Context, open_h2_server_conn: Server, stream):\n    playbook, cff = start_h2_client(tctx)\n    tctx.server = open_h2_server_conn\n\n    def enable_streaming(flow: HTTPFlow):\n        flow.request.stream = bool(stream)\n\n    flow = Placeholder(HTTPFlow)\n    server_data1 = Placeholder(bytes)\n    server_data2 = Placeholder(bytes)\n    (\n        playbook\n        >> DataReceived(tctx.client,\n                        cff.build_headers_frame(example_request_headers).serialize() +\n                        cff.build_data_frame(b\"Hello, World!\").serialize()\n                        )\n        << http.HttpRequestHeadersHook(flow)\n        >> reply(side_effect=enable_streaming)\n    )\n    if stream:\n        playbook << SendData(tctx.server, server_data1)\n    assert (\n        playbook\n        >> DataReceived(tctx.client,\n                        cff.build_headers_frame(example_request_trailers, flags=[\"END_STREAM\"]).serialize())\n        << http.HttpRequestHook(flow)\n        >> reply()\n        << SendData(tctx.server, server_data2)\n    )\n    frames = decode_frames(server_data1.setdefault(b\"\") + server_data2())\n    assert [type(x) for x in frames] == [\n        hyperframe.frame.SettingsFrame,\n        hyperframe.frame.HeadersFrame,\n        hyperframe.frame.DataFrame,\n        hyperframe.frame.HeadersFrame,\n    ]\n\n\ndef test_upstream_error(tctx):\n    playbook, cff = start_h2_client(tctx)\n    flow = Placeholder(HTTPFlow)\n    server = Placeholder(Server)\n    err = Placeholder(bytes)\n    assert (\n            playbook\n            >> DataReceived(tctx.client,\n                            cff.build_headers_frame(example_request_headers, flags=[\"END_STREAM\"]).serialize())\n            << http.HttpRequestHeadersHook(flow)\n            >> reply()\n            << http.HttpRequestHook(flow)\n            >> reply()\n            << OpenConnection(server)\n            >> reply(\"oops server <> error\")\n            << http.HttpErrorHook(flow)\n            >> reply()\n            << SendData(tctx.client, err)\n    )\n    frames = decode_frames(err())\n    assert [type(x) for x in frames] == [\n        hyperframe.frame.HeadersFrame,\n        hyperframe.frame.DataFrame,\n    ]\n    d = frames[1]\n    assert isinstance(d, hyperframe.frame.DataFrame)\n    assert b\"502 Bad Gateway\" in d.data\n    assert b\"server &lt;&gt; error\" in d.data\n\n\n@pytest.mark.parametrize(\"stream\", [\"stream\", \"\"])\n@pytest.mark.parametrize(\"when\", [\"request\", \"response\"])\n@pytest.mark.parametrize(\"how\", [\"RST\", \"disconnect\", \"RST+disconnect\"])\ndef test_http2_client_aborts(tctx, stream, when, how):\n    \"\"\"\n    Test handling of the case where a client aborts during request or response transmission.\n\n    If the client aborts the request transmission, we must trigger an error hook,\n    if the client disconnects during response transmission, no error hook is triggered.\n    \"\"\"\n    server = Placeholder(Server)\n    flow = Placeholder(HTTPFlow)\n    playbook, cff = start_h2_client(tctx)\n    resp = Placeholder(bytes)\n\n    def enable_request_streaming(flow: HTTPFlow):\n        flow.request.stream = True\n\n    def enable_response_streaming(flow: HTTPFlow):\n        flow.response.stream = True\n\n    assert (\n            playbook\n            >> DataReceived(tctx.client, cff.build_headers_frame(example_request_headers).serialize())\n            << http.HttpRequestHeadersHook(flow)\n    )\n    if stream and when == \"request\":\n        assert (\n                playbook\n                >> reply(side_effect=enable_request_streaming)\n                << OpenConnection(server)\n                >> reply(None)\n                << SendData(server, b\"GET / HTTP/1.1\\r\\n\"\n                                    b\"Host: example.com\\r\\n\\r\\n\")\n        )\n    else:\n        assert playbook >> reply()\n\n    if when == \"request\":\n        if \"RST\" in how:\n            playbook >> DataReceived(tctx.client, cff.build_rst_stream_frame(1, ErrorCodes.CANCEL).serialize())\n        else:\n            playbook >> ConnectionClosed(tctx.client)\n            playbook << CloseConnection(tctx.client)\n\n        if stream:\n            playbook << CloseConnection(server)\n        playbook << http.HttpErrorHook(flow)\n        playbook >> reply()\n\n        if how == \"RST+disconnect\":\n            playbook >> ConnectionClosed(tctx.client)\n            playbook << CloseConnection(tctx.client)\n\n        assert playbook\n        assert \"stream reset\" in flow().error.msg or \"peer closed connection\" in flow().error.msg\n        return\n\n    assert (\n            playbook\n            >> DataReceived(tctx.client, cff.build_data_frame(b\"\", flags=[\"END_STREAM\"]).serialize())\n            << http.HttpRequestHook(flow)\n            >> reply()\n            << OpenConnection(server)\n            >> reply(None)\n            << SendData(server, b\"GET / HTTP/1.1\\r\\n\"\n                                b\"Host: example.com\\r\\n\\r\\n\")\n            >> DataReceived(server, b\"HTTP/1.1 200 OK\\r\\nContent-Length: 6\\r\\n\\r\\n123\")\n            << http.HttpResponseHeadersHook(flow)\n    )\n    if stream:\n        assert (\n                playbook\n                >> reply(side_effect=enable_response_streaming)\n                << SendData(tctx.client, resp)\n        )\n    else:\n        assert playbook >> reply()\n\n    if \"RST\" in how:\n        playbook >> DataReceived(tctx.client, cff.build_rst_stream_frame(1, ErrorCodes.CANCEL).serialize())\n    else:\n        playbook >> ConnectionClosed(tctx.client)\n        playbook << CloseConnection(tctx.client)\n\n    assert (\n            playbook\n            << CloseConnection(server)\n            << http.HttpErrorHook(flow)\n            >> reply()\n    )\n\n    if how == \"RST+disconnect\":\n        assert (\n                playbook\n                >> ConnectionClosed(tctx.client)\n                << CloseConnection(tctx.client)\n        )\n\n    if \"RST\" in how:\n        assert \"stream reset\" in flow().error.msg\n    else:\n        assert \"peer closed connection\" in flow().error.msg\n\n\n@pytest.mark.parametrize(\"normalize\", [True, False])\ndef test_no_normalization(tctx, normalize):\n    \"\"\"Test that we don't normalize headers when we just pass them through.\"\"\"\n    tctx.options.normalize_outbound_headers = normalize\n    tctx.options.validate_inbound_headers = False\n\n    server = Placeholder(Server)\n    flow = Placeholder(HTTPFlow)\n    playbook, cff = start_h2_client(tctx)\n\n    request_headers = list(example_request_headers) + [(b\"Should-Not-Be-Capitalized! \", b\" :) \")]\n    request_headers_lower = [(k.lower(), v) for (k, v) in request_headers]\n    response_headers = list(example_response_headers) + [(b\"Same\", b\"Here\")]\n    response_headers_lower = [(k.lower(), v) for (k, v) in response_headers]\n\n    initial = Placeholder(bytes)\n    assert (\n            playbook\n            >> DataReceived(tctx.client,\n                            cff.build_headers_frame(request_headers, flags=[\"END_STREAM\"]).serialize())\n            << http.HttpRequestHeadersHook(flow)\n            >> reply()\n            << http.HttpRequestHook(flow)\n            >> reply()\n            << OpenConnection(server)\n            >> reply(None, side_effect=make_h2)\n            << SendData(server, initial)\n    )\n    frames = decode_frames(initial())\n    assert [type(x) for x in frames] == [\n        hyperframe.frame.SettingsFrame,\n        hyperframe.frame.HeadersFrame,\n    ]\n    assert hpack.hpack.Decoder().decode(frames[1].data, True) == request_headers_lower if normalize else request_headers\n\n    sff = FrameFactory()\n    (\n            playbook\n            >> DataReceived(server, sff.build_headers_frame(response_headers, flags=[\"END_STREAM\"]).serialize())\n            << http.HttpResponseHeadersHook(flow)\n            >> reply()\n            << http.HttpResponseHook(flow)\n            >> reply()\n    )\n    if normalize:\n        playbook << Log(\"Lowercased 'Same' header as uppercase is not allowed with HTTP/2.\")\n    hdrs = response_headers_lower if normalize else response_headers\n    assert playbook << SendData(tctx.client, cff.build_headers_frame(hdrs, flags=[\"END_STREAM\"]).serialize())\n\n    assert flow().request.headers.fields == ((b\"Should-Not-Be-Capitalized! \", b\" :) \"),)\n    assert flow().response.headers.fields == ((b\"Same\", b\"Here\"),)\n\n\n@pytest.mark.parametrize(\"input,pseudo,headers\", [\n    ([(b\"foo\", b\"bar\")], {}, {\"foo\": \"bar\"}),\n    ([(b\":status\", b\"418\")], {b\":status\": b\"418\"}, {}),\n    ([(b\":status\", b\"418\"), (b\"foo\", b\"bar\")], {b\":status\": b\"418\"}, {\"foo\": \"bar\"}),\n])\ndef test_split_pseudo_headers(input, pseudo, headers):\n    actual_pseudo, actual_headers = split_pseudo_headers(input)\n    assert pseudo == actual_pseudo\n    assert Headers(**headers) == actual_headers\n\n\ndef test_split_pseudo_headers_err():\n    with pytest.raises(ValueError, match=\"Duplicate HTTP/2 pseudo header\"):\n        split_pseudo_headers([(b\":status\", b\"418\"), (b\":status\", b\"418\")])\n\n\ndef test_rst_then_close(tctx):\n    \"\"\"\n    Test that we properly handle the case of a client that first causes protocol errors and then disconnects.\n\n    Adapted from h2spec http2/5.1/5.\n    \"\"\"\n    playbook, cff = start_h2_client(tctx)\n    flow = Placeholder(HTTPFlow)\n    server = Placeholder(Server)\n\n    assert (\n            playbook\n            >> DataReceived(tctx.client,\n                            cff.build_headers_frame(example_request_headers, flags=[\"END_STREAM\"]).serialize())\n            << http.HttpRequestHeadersHook(flow)\n            >> reply()\n            << http.HttpRequestHook(flow)\n            >> reply()\n            << OpenConnection(server)\n            >> DataReceived(tctx.client, cff.build_data_frame(b\"unexpected data frame\").serialize())\n            << SendData(tctx.client, cff.build_rst_stream_frame(1, ErrorCodes.STREAM_CLOSED).serialize())\n            >> ConnectionClosed(tctx.client)\n            << CloseConnection(tctx.client)\n            >> reply(\"connection cancelled\", to=-5)\n            << http.HttpErrorHook(flow)\n            >> reply()\n    )\n    assert flow().error.msg == \"connection cancelled\"\n\n\ndef test_cancel_then_server_disconnect(tctx):\n    \"\"\"\n    Test that we properly handle the case of the following event sequence:\n        - client cancels a stream\n        - we start an error hook\n        - server disconnects\n        - error hook completes.\n    \"\"\"\n    playbook, cff = start_h2_client(tctx)\n    flow = Placeholder(HTTPFlow)\n    server = Placeholder(Server)\n\n    assert (\n            playbook\n            >> DataReceived(tctx.client,\n                            cff.build_headers_frame(example_request_headers, flags=[\"END_STREAM\"]).serialize())\n            << http.HttpRequestHeadersHook(flow)\n            >> reply()\n            << http.HttpRequestHook(flow)\n            >> reply()\n            << OpenConnection(server)\n            >> reply(None)\n            << SendData(server, b'GET / HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n')\n            >> DataReceived(tctx.client, cff.build_rst_stream_frame(1, ErrorCodes.CANCEL).serialize())\n            << CloseConnection(server)\n            << http.HttpErrorHook(flow)\n            >> reply()\n            >> ConnectionClosed(server)\n            << None\n    )\n\n\ndef test_cancel_during_response_hook(tctx):\n    \"\"\"\n    Test that we properly handle the case of the following event sequence:\n        - we receive a server response\n        - we trigger the response hook\n        - the client cancels the stream\n        - the response hook completes\n\n    Given that we have already triggered the response hook, we don't want to trigger the error hook.\n    \"\"\"\n    playbook, cff = start_h2_client(tctx)\n    flow = Placeholder(HTTPFlow)\n    server = Placeholder(Server)\n\n    assert (\n            playbook\n            >> DataReceived(tctx.client,\n                            cff.build_headers_frame(example_request_headers, flags=[\"END_STREAM\"]).serialize())\n            << http.HttpRequestHeadersHook(flow)\n            >> reply()\n            << http.HttpRequestHook(flow)\n            >> reply()\n            << OpenConnection(server)\n            >> reply(None)\n            << SendData(server, b'GET / HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n')\n            >> DataReceived(server, b\"HTTP/1.1 204 No Content\\r\\n\\r\\n\")\n            << http.HttpResponseHeadersHook(flow)\n            << CloseConnection(server)\n            >> reply(to=-2)\n            << http.HttpResponseHook(flow)\n            >> DataReceived(tctx.client, cff.build_rst_stream_frame(1, ErrorCodes.CANCEL).serialize())\n            >> reply(to=-2)\n    )\n\n\ndef test_stream_concurrency(tctx):\n    \"\"\"Test that we can send an intercepted request with a lower stream id than one that has already been sent.\"\"\"\n    playbook, cff = start_h2_client(tctx)\n    flow1 = Placeholder(HTTPFlow)\n    flow2 = Placeholder(HTTPFlow)\n\n    reqheadershook1 = http.HttpRequestHeadersHook(flow1)\n    reqheadershook2 = http.HttpRequestHeadersHook(flow2)\n    reqhook1 = http.HttpRequestHook(flow1)\n    reqhook2 = http.HttpRequestHook(flow2)\n\n    server = Placeholder(Server)\n    data_req1 = Placeholder(bytes)\n    data_req2 = Placeholder(bytes)\n\n    assert (playbook\n            >> DataReceived(\n                tctx.client,\n                cff.build_headers_frame(example_request_headers, flags=[\"END_STREAM\"], stream_id=1).serialize() +\n                cff.build_headers_frame(example_request_headers, flags=[\"END_STREAM\"], stream_id=3).serialize())\n            << reqheadershook1\n            << reqheadershook2\n            >> reply(to=reqheadershook1)\n            << reqhook1\n            >> reply(to=reqheadershook2)\n            << reqhook2\n            # req 2 overtakes 1 and we already have a reply:\n            >> reply(to=reqhook2)\n            << OpenConnection(server)\n            >> reply(None, side_effect=make_h2)\n            << SendData(server, data_req2)\n            >> reply(to=reqhook1)\n            << SendData(server, data_req1)\n            )\n    frames = decode_frames(data_req2())\n    assert [type(x) for x in frames] == [\n        hyperframe.frame.SettingsFrame,\n        hyperframe.frame.HeadersFrame,\n    ]\n    frames = decode_frames(data_req1())\n    assert [type(x) for x in frames] == [\n        hyperframe.frame.HeadersFrame,\n    ]\n\n\ndef test_max_concurrency(tctx):\n    playbook, cff = start_h2_client(tctx)\n    server = Placeholder(Server)\n    req1_bytes = Placeholder(bytes)\n    settings_ack_bytes = Placeholder(bytes)\n    req2_bytes = Placeholder(bytes)\n    playbook.hooks = False\n    sff = FrameFactory()\n\n    assert (\n            playbook\n            >> DataReceived(tctx.client,\n                            cff.build_headers_frame(example_request_headers, flags=[\"END_STREAM\"],\n                                                    stream_id=1).serialize())\n            << OpenConnection(server)\n            >> reply(None, side_effect=make_h2)\n            << SendData(server, req1_bytes)\n            >> DataReceived(server,\n                            sff.build_settings_frame(\n                                {h2.settings.SettingCodes.MAX_CONCURRENT_STREAMS: 1}).serialize())\n            << SendData(server, settings_ack_bytes)\n            >> DataReceived(tctx.client,\n                            cff.build_headers_frame(example_request_headers,\n                                                    flags=[\"END_STREAM\"],\n                                                    stream_id=3).serialize())\n            # Can't send it upstream yet, all streams in use!\n            >> DataReceived(server, sff.build_headers_frame(example_response_headers,\n                                                            flags=[\"END_STREAM\"],\n                                                            stream_id=1).serialize())\n            # But now we can!\n            << SendData(server, req2_bytes)\n            << SendData(tctx.client, Placeholder(bytes))\n            >> DataReceived(server, sff.build_headers_frame(example_response_headers,\n                                                            flags=[\"END_STREAM\"],\n                                                            stream_id=3).serialize())\n            << SendData(tctx.client, Placeholder(bytes))\n    )\n    settings, req1 = decode_frames(req1_bytes())\n    settings_ack, = decode_frames(settings_ack_bytes())\n    req2, = decode_frames(req2_bytes())\n\n    assert type(settings) == hyperframe.frame.SettingsFrame\n    assert type(req1) == hyperframe.frame.HeadersFrame\n    assert type(settings_ack) == hyperframe.frame.SettingsFrame\n    assert type(req2) == hyperframe.frame.HeadersFrame\n    assert req1.stream_id == 1\n    assert req2.stream_id == 3\n\n\ndef test_stream_concurrent_get_connection(tctx):\n    \"\"\"Test that an immediate second request for the same domain does not trigger a second connection attempt.\"\"\"\n    playbook, cff = start_h2_client(tctx)\n    playbook.hooks = False\n\n    server = Placeholder(Server)\n    data = Placeholder(bytes)\n\n    assert (playbook\n            >> DataReceived(tctx.client, cff.build_headers_frame(example_request_headers, flags=[\"END_STREAM\"],\n                                                                 stream_id=1).serialize())\n            << (o := OpenConnection(server))\n            >> DataReceived(tctx.client, cff.build_headers_frame(example_request_headers, flags=[\"END_STREAM\"],\n                                                                 stream_id=3).serialize())\n            >> reply(None, to=o, side_effect=make_h2)\n            << SendData(server, data)\n            )\n    frames = decode_frames(data())\n    assert [type(x) for x in frames] == [\n        hyperframe.frame.SettingsFrame,\n        hyperframe.frame.HeadersFrame,\n        hyperframe.frame.HeadersFrame,\n    ]\n\n\ndef test_kill_stream(tctx):\n    \"\"\"Test that we can kill individual streams.\"\"\"\n    playbook, cff = start_h2_client(tctx)\n    flow1 = Placeholder(HTTPFlow)\n    flow2 = Placeholder(HTTPFlow)\n\n    req_headers_hook_1 = http.HttpRequestHeadersHook(flow1)\n\n    def kill(flow: HTTPFlow):\n        # Can't use flow.kill() here because that currently still depends on a reply object.\n        flow.error = Error(Error.KILLED_MESSAGE)\n\n    server = Placeholder(Server)\n    data_req1 = Placeholder(bytes)\n\n    assert (playbook\n            >> DataReceived(\n                tctx.client,\n                cff.build_headers_frame(example_request_headers, flags=[\"END_STREAM\"], stream_id=1).serialize() +\n                cff.build_headers_frame(example_request_headers, flags=[\"END_STREAM\"], stream_id=3).serialize())\n            << req_headers_hook_1\n            << http.HttpRequestHeadersHook(flow2)\n            >> reply(side_effect=kill)\n            << http.HttpErrorHook(flow2)\n            >> reply()\n            << SendData(tctx.client, cff.build_rst_stream_frame(3, error_code=ErrorCodes.INTERNAL_ERROR).serialize())\n            >> reply(to=req_headers_hook_1)\n            << http.HttpRequestHook(flow1)\n            >> reply()\n            << OpenConnection(server)\n            >> reply(None, side_effect=make_h2)\n            << SendData(server, data_req1)\n            )\n    frames = decode_frames(data_req1())\n    assert [type(x) for x in frames] == [\n        hyperframe.frame.SettingsFrame,\n        hyperframe.frame.HeadersFrame,\n    ]\n\n\nclass TestClient:\n    def test_no_data_on_closed_stream(self, tctx):\n        frame_factory = FrameFactory()\n        req = Request.make(\"GET\", \"http://example.com/\")\n        resp = {\n            \":status\": 200\n        }\n        assert (\n                Playbook(Http2Client(tctx))\n                << SendData(tctx.server, Placeholder(bytes))  # preamble + initial settings frame\n                >> DataReceived(tctx.server, frame_factory.build_settings_frame({}, ack=True).serialize())\n                >> http.RequestHeaders(1, req, end_stream=True)\n                << SendData(tctx.server, b\"\\x00\\x00\\x06\\x01\\x05\\x00\\x00\\x00\\x01\\x82\\x86\\x84\\\\\\x81\\x07\")\n                >> http.RequestEndOfMessage(1)\n                >> DataReceived(tctx.server, frame_factory.build_headers_frame(resp).serialize())\n                << http.ReceiveHttp(Placeholder(http.ResponseHeaders))\n                >> http.RequestProtocolError(1, \"cancelled\", code=status_codes.CLIENT_CLOSED_REQUEST)\n                << SendData(tctx.server, frame_factory.build_rst_stream_frame(1, ErrorCodes.CANCEL).serialize())\n                >> DataReceived(tctx.server, frame_factory.build_data_frame(b\"foo\").serialize())\n                << SendData(tctx.server, frame_factory.build_rst_stream_frame(1, ErrorCodes.STREAM_CLOSED).serialize())\n        )  # important: no ResponseData event here!\n\n\ndef test_early_server_data(tctx):\n    playbook, cff = start_h2_client(tctx)\n    sff = FrameFactory()\n\n    tctx.server.address = (\"example.com\", 80)\n    tctx.server.state = ConnectionState.OPEN\n    tctx.server.alpn = b\"h2\"\n\n    flow = Placeholder(HTTPFlow)\n    server1 = Placeholder(bytes)\n    server2 = Placeholder(bytes)\n    assert (\n            playbook\n            >> DataReceived(tctx.client,\n                            cff.build_headers_frame(example_request_headers, flags=[\"END_STREAM\"]).serialize())\n            << http.HttpRequestHeadersHook(flow)\n            >> reply()\n            << (h := http.HttpRequestHook(flow))\n            # Surprise! We get data from the server before the request hook finishes.\n            >> DataReceived(tctx.server, sff.build_settings_frame({}).serialize())\n            << SendData(tctx.server, server1)\n            # Request hook finishes...\n            >> reply(to=h)\n            << SendData(tctx.server, server2)\n    )\n    assert [type(x) for x in decode_frames(server1())] == [\n        hyperframe.frame.SettingsFrame,\n        hyperframe.frame.SettingsFrame,\n    ]\n    assert [type(x) for x in decode_frames(server2())] == [\n        hyperframe.frame.HeadersFrame,\n    ]\n\n\ndef test_request_smuggling_cl(tctx):\n    playbook, cff = start_h2_client(tctx)\n    playbook.hooks = False\n    err = Placeholder(bytes)\n\n    headers = (\n        (b':method', b'POST'),\n        (b':scheme', b'http'),\n        (b':path', b'/'),\n        (b':authority', b'example.com'),\n        (b'content-length', b'3')\n    )\n\n    assert (\n            playbook\n            >> DataReceived(tctx.client,\n                            cff.build_headers_frame(headers).serialize())\n            >> DataReceived(tctx.client,\n                            cff.build_data_frame(b\"abcPOST / HTTP/1.1 ...\", flags=[\"END_STREAM\"]).serialize())\n            << SendData(tctx.client, err)\n            << CloseConnection(tctx.client)\n    )\n    assert b\"InvalidBodyLengthError\" in err()\n\n\ndef test_request_smuggling_te(tctx):\n    playbook, cff = start_h2_client(tctx)\n    playbook.hooks = False\n    err = Placeholder(bytes)\n\n    headers = (\n        (b':method', b'POST'),\n        (b':scheme', b'http'),\n        (b':path', b'/'),\n        (b':authority', b'example.com'),\n        (b'transfer-encoding', b'chunked')\n    )\n\n    assert (\n            playbook\n            >> DataReceived(tctx.client,\n                            cff.build_headers_frame(headers, flags=[\"END_STREAM\"]).serialize())\n            << SendData(tctx.client, err)\n            << CloseConnection(tctx.client)\n    )\n    assert b\"Connection-specific header field present\" in err()\n"], "buggy_code_start_loc": [98, 5, 38, 236, 43, 7, 1263, 355], "buggy_code_end_loc": [98, 18, 108, 332, 60, 99, 1263, 359], "fixing_code_start_loc": [99, 6, 39, 237, 43, 7, 1264, 354], "fixing_code_end_loc": [106, 21, 138, 337, 62, 106, 1295, 360], "type": "CWE-444", "message": "mitmproxy is an interactive, SSL/TLS-capable intercepting proxy. In mitmproxy 7.0.4 and below, a malicious client or server is able to perform HTTP request smuggling attacks through mitmproxy. This means that a malicious client/server could smuggle a request/response through mitmproxy as part of another request/response's HTTP message body. While mitmproxy would only see one request, the target server would see multiple requests. A smuggled request is still captured as part of another request's body, but it does not appear in the request list and does not go through the usual mitmproxy event hooks, where users may have implemented custom access control checks or input sanitization. Unless mitmproxy is used to protect an HTTP/1 service, no action is required. The vulnerability has been fixed in mitmproxy 8.0.0 and above. There are currently no known workarounds.", "other": {"cve": {"id": "CVE-2022-24766", "sourceIdentifier": "security-advisories@github.com", "published": "2022-03-21T19:15:11.613", "lastModified": "2022-03-29T16:49:43.347", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "mitmproxy is an interactive, SSL/TLS-capable intercepting proxy. In mitmproxy 7.0.4 and below, a malicious client or server is able to perform HTTP request smuggling attacks through mitmproxy. This means that a malicious client/server could smuggle a request/response through mitmproxy as part of another request/response's HTTP message body. While mitmproxy would only see one request, the target server would see multiple requests. A smuggled request is still captured as part of another request's body, but it does not appear in the request list and does not go through the usual mitmproxy event hooks, where users may have implemented custom access control checks or input sanitization. Unless mitmproxy is used to protect an HTTP/1 service, no action is required. The vulnerability has been fixed in mitmproxy 8.0.0 and above. There are currently no known workarounds."}, {"lang": "es", "value": "mitmproxy es un proxy interactivo con capacidad de interceptaci\u00f3n SSL/TLS. En mitmproxy versiones 7.0.4 y anteriores, un cliente o servidor malicioso puede llevar a cabo ataques de contrabando de peticiones HTTP mediante mitmproxy. Esto significa que un cliente/servidor malicioso podr\u00eda pasar de contrabando una petici\u00f3n/respuesta mediante mitmproxy como parte del cuerpo del mensaje HTTP de otra petici\u00f3n/respuesta. Mientras que mitmproxy s\u00f3lo ver\u00eda una petici\u00f3n, el servidor de destino ver\u00eda m\u00faltiples peticiones. Una petici\u00f3n contrabandeada sigue siendo capturada como parte del cuerpo de otra petici\u00f3n, pero no aparece en la lista de peticiones y no pasa por los ganchos de eventos habituales de mitmproxy, donde los usuarios pueden haber implementado comprobaciones de control de acceso personalizadas o saneo de entradas. A menos que mitmproxy sea usado para proteger un servicio HTTP/1, no es requerida ninguna acci\u00f3n. La vulnerabilidad ha sido corregida en mitmproxy versiones 8.0.0 y superiores. Actualmente no se presentan medidas de mitigaci\u00f3n conocidas"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 9.8, "baseSeverity": "CRITICAL"}, "exploitabilityScore": 3.9, "impactScore": 5.9}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 9.8, "baseSeverity": "CRITICAL"}, "exploitabilityScore": 3.9, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:L/Au:N/C:P/I:P/A:P", "accessVector": "NETWORK", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "PARTIAL", "availabilityImpact": "PARTIAL", "baseScore": 7.5}, "baseSeverity": "HIGH", "exploitabilityScore": 10.0, "impactScore": 6.4, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-444"}]}, {"source": "security-advisories@github.com", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-444"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:mitmproxy:mitmproxy:*:*:*:*:*:*:*:*", "versionEndIncluding": "7.0.4", "matchCriteriaId": "41ABE70A-F0A7-4F97-A8F1-8B8D1BDD5663"}]}]}], "references": [{"url": "https://github.com/mitmproxy/mitmproxy/commit/b06fb6d157087d526bd02e7aadbe37c56865c71b", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/mitmproxy/mitmproxy/security/advisories/GHSA-gcx2-gvj7-pxv3", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}, {"url": "https://mitmproxy.org/posts/releases/mitmproxy8/", "source": "security-advisories@github.com", "tags": ["Release Notes", "Vendor Advisory"]}]}, "github_commit_url": "https://github.com/mitmproxy/mitmproxy/commit/b06fb6d157087d526bd02e7aadbe37c56865c71b"}}