{"buggy_code": ["/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#define EIGEN_USE_THREADS\n\n#include <algorithm>\n#include <numeric>\n#include <unordered_map>\n#include <utility>\n#include <vector>\n\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/register_types.h\"\n#include \"tensorflow/core/framework/resource_mgr.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/tensor_util.h\"\n#include \"tensorflow/core/framework/types.h\"\n#include \"tensorflow/core/lib/gtl/inlined_vector.h\"\n#include \"tensorflow/core/util/overflow.h\"\n#include \"tensorflow/core/util/sparse/sparse_tensor.h\"\n\nnamespace tensorflow {\n\ntypedef Eigen::ThreadPoolDevice CPUDevice;\n\nusing sparse::SparseTensor;\n\nclass SparseTensorsMap : public ResourceBase {\n public:\n  explicit SparseTensorsMap(const string& name) : name_(name), counter_(0) {}\n\n  string DebugString() const override { return \"A SparseTensorsMap\"; }\n\n  typedef struct {\n    Tensor indices;\n    Tensor values;\n    gtl::InlinedVector<int64_t, 8> shape;\n  } PersistentSparseTensor;\n\n  Status AddSparseTensor(OpKernelContext* ctx, const SparseTensor& sp,\n                         int64_t* handle) {\n    Tensor ix;\n    TF_RETURN_IF_ERROR(\n        ctx->allocate_temp(sp.indices().dtype(), sp.indices().shape(), &ix));\n    ix = sp.indices();\n\n    Tensor values;\n    TF_RETURN_IF_ERROR(ctx->allocate_temp(sp.indices().dtype(),\n                                          sp.indices().shape(), &values));\n    values = sp.values();\n    {\n      mutex_lock l(mu_);\n      int64_t unique_st_handle = counter_++;  // increment is guarded on purpose\n      sp_tensors_[unique_st_handle] = PersistentSparseTensor{\n          ix, values,\n          gtl::InlinedVector<int64_t, 8>(sp.shape().begin(), sp.shape().end())};\n      *handle = unique_st_handle;\n    }\n    return Status::OK();\n  }\n\n  Status RetrieveAndClearSparseTensors(\n      OpKernelContext* ctx, const TTypes<int64_t>::ConstVec& handles,\n      std::vector<SparseTensor>* sparse_tensors) {\n    sparse_tensors->clear();\n    sparse_tensors->reserve(handles.size());\n    {\n      mutex_lock l(mu_);\n      for (size_t i = 0; i < handles.size(); ++i) {\n        const int64_t handle = handles(i);\n        auto sp_iter = sp_tensors_.find(handle);\n        if (sp_iter == sp_tensors_.end()) {\n          return errors::InvalidArgument(\n              \"Unable to find SparseTensor: \", handle, \" in map: \", name_);\n        }\n        const Tensor* ix = &sp_iter->second.indices;\n        const Tensor* values = &sp_iter->second.values;\n        const auto& shape = sp_iter->second.shape;\n        SparseTensor tensor;\n        TF_RETURN_IF_ERROR(SparseTensor::Create(*ix, *values, shape, &tensor));\n        sparse_tensors->push_back(std::move(tensor));\n        sp_tensors_.erase(sp_iter);\n      }\n    }\n\n    return Status::OK();\n  }\n\n protected:\n  ~SparseTensorsMap() override {}\n\n private:\n  string name_;\n\n  mutex mu_;\n  int64_t counter_ TF_GUARDED_BY(mu_);\n  std::unordered_map<int64_t, PersistentSparseTensor> sp_tensors_\n      TF_GUARDED_BY(mu_);\n};\n\nclass SparseTensorAccessingOp : public OpKernel {\n public:\n  typedef std::function<Status(SparseTensorsMap**)> CreatorCallback;\n\n  explicit SparseTensorAccessingOp(OpKernelConstruction* context)\n      : OpKernel(context), sparse_tensors_map_(nullptr) {}\n\n protected:\n  ~SparseTensorAccessingOp() override {\n    if (sparse_tensors_map_) sparse_tensors_map_->Unref();\n  }\n\n  Status GetMap(OpKernelContext* ctx, bool is_writing,\n                SparseTensorsMap** sparse_tensors_map) {\n    mutex_lock l(mu_);\n\n    if (sparse_tensors_map_) {\n      *sparse_tensors_map = sparse_tensors_map_;\n      return Status::OK();\n    }\n\n    TF_RETURN_IF_ERROR(cinfo_.Init(ctx->resource_manager(), def(),\n                                   is_writing /* use_node_name_as_default */));\n\n    CreatorCallback sparse_tensors_map_creator = [this](SparseTensorsMap** c) {\n      SparseTensorsMap* map = new SparseTensorsMap(cinfo_.name());\n      *c = map;\n      return Status::OK();\n    };\n\n    TF_RETURN_IF_ERROR(\n        cinfo_.resource_manager()->LookupOrCreate<SparseTensorsMap>(\n            cinfo_.container(), cinfo_.name(), &sparse_tensors_map_,\n            sparse_tensors_map_creator));\n\n    *sparse_tensors_map = sparse_tensors_map_;\n    return Status::OK();\n  }\n\n private:\n  ContainerInfo cinfo_;\n\n  mutex mu_;\n  SparseTensorsMap* sparse_tensors_map_ TF_PT_GUARDED_BY(mu_);\n};\n\nclass AddSparseToTensorsMapOp : public SparseTensorAccessingOp {\n public:\n  explicit AddSparseToTensorsMapOp(OpKernelConstruction* context)\n      : SparseTensorAccessingOp(context) {}\n\n  void Compute(OpKernelContext* context) override {\n    const Tensor* input_indices;\n    const Tensor* input_values;\n    const Tensor* input_shape;\n    SparseTensorsMap* map;\n\n    OP_REQUIRES_OK(context, context->input(\"sparse_indices\", &input_indices));\n    OP_REQUIRES_OK(context, context->input(\"sparse_values\", &input_values));\n    OP_REQUIRES_OK(context, context->input(\"sparse_shape\", &input_shape));\n    OP_REQUIRES_OK(context, GetMap(context, true /* is_writing */, &map));\n\n    OP_REQUIRES(context, TensorShapeUtils::IsMatrix(input_indices->shape()),\n                errors::InvalidArgument(\n                    \"Input indices should be a matrix but received shape \",\n                    input_indices->shape().DebugString()));\n\n    OP_REQUIRES(context, TensorShapeUtils::IsVector(input_values->shape()),\n                errors::InvalidArgument(\n                    \"Input values should be a vector but received shape \",\n                    input_values->shape().DebugString()));\n\n    OP_REQUIRES(context, TensorShapeUtils::IsVector(input_shape->shape()),\n                errors::InvalidArgument(\n                    \"Input shape should be a vector but received shape \",\n                    input_shape->shape().DebugString()));\n\n    TensorShape input_shape_object;\n    OP_REQUIRES_OK(\n        context, TensorShapeUtils::MakeShape(input_shape->vec<int64_t>().data(),\n                                             input_shape->NumElements(),\n                                             &input_shape_object));\n    SparseTensor st;\n    OP_REQUIRES_OK(context, SparseTensor::Create(*input_indices, *input_values,\n                                                 input_shape_object, &st));\n    int64_t handle;\n    OP_REQUIRES_OK(context, map->AddSparseTensor(context, st, &handle));\n\n    Tensor sparse_handle(DT_INT64, TensorShape({}));\n    auto sparse_handle_t = sparse_handle.scalar<int64_t>();\n\n    sparse_handle_t() = handle;\n\n    context->set_output(0, sparse_handle);\n  }\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"AddSparseToTensorsMap\").Device(DEVICE_CPU),\n                        AddSparseToTensorsMapOp);\n\ntemplate <typename T>\nclass AddManySparseToTensorsMapOp : public SparseTensorAccessingOp {\n public:\n  explicit AddManySparseToTensorsMapOp(OpKernelConstruction* context)\n      : SparseTensorAccessingOp(context) {}\n\n  void Compute(OpKernelContext* context) override {\n    const Tensor* input_indices;\n    const Tensor* input_values;\n    const Tensor* input_shape;\n    SparseTensorsMap* map;\n\n    OP_REQUIRES_OK(context, context->input(\"sparse_indices\", &input_indices));\n    OP_REQUIRES_OK(context, context->input(\"sparse_values\", &input_values));\n    OP_REQUIRES_OK(context, context->input(\"sparse_shape\", &input_shape));\n    OP_REQUIRES_OK(context, GetMap(context, true /* is_writing */, &map));\n\n    OP_REQUIRES(context, TensorShapeUtils::IsMatrix(input_indices->shape()),\n                errors::InvalidArgument(\n                    \"Input indices should be a matrix but received shape \",\n                    input_indices->shape().DebugString()));\n    OP_REQUIRES(context, TensorShapeUtils::IsVector(input_values->shape()),\n                errors::InvalidArgument(\n                    \"Input values should be a vector but received shape \",\n                    input_values->shape().DebugString()));\n    OP_REQUIRES(context, TensorShapeUtils::IsVector(input_shape->shape()),\n                errors::InvalidArgument(\n                    \"Input shape should be a vector but received shape \",\n                    input_shape->shape().DebugString()));\n    OP_REQUIRES(\n        context,\n        input_values->shape().dim_size(0) == input_indices->shape().dim_size(0),\n        errors::InvalidArgument(\n            \"Number of values must match first dimension of indices. \", \"Got \",\n            input_values->shape().dim_size(0),\n            \" values, indices shape: \", input_indices->shape().DebugString()));\n    OP_REQUIRES(\n        context,\n        input_shape->shape().dim_size(0) == input_indices->shape().dim_size(1),\n        errors::InvalidArgument(\n            \"Number of dimensions must match second dimension of indices. \",\n            \"Got \", input_shape->shape().dim_size(0),\n            \" dimensions, indices shape: \",\n            input_indices->shape().DebugString()));\n\n    int rank = input_shape->NumElements();\n\n    OP_REQUIRES(\n        context, rank > 1,\n        errors::InvalidArgument(\n            \"Rank of input SparseTensor should be > 1, but saw rank: \", rank));\n\n    auto input_shape_vec = input_shape->vec<int64_t>();\n    int new_num_elements = 1;\n    bool overflow_ocurred = false;\n    for (int i = 0; i < input_shape_vec.size(); i++) {\n      new_num_elements =\n          MultiplyWithoutOverflow(new_num_elements, input_shape_vec(i));\n      if (new_num_elements < 0) {\n        overflow_ocurred = true;\n        break;\n      }\n    }\n\n    OP_REQUIRES(\n        context, !overflow_ocurred,\n        errors::Internal(\"Encountered overflow from large input shape.\"));\n\n    TensorShape tensor_input_shape(input_shape_vec);\n    gtl::InlinedVector<int64_t, 8> std_order(rank);\n    std::iota(std_order.begin(), std_order.end(), 0);\n    SparseTensor input_st;\n    OP_REQUIRES_OK(context, SparseTensor::Create(*input_indices, *input_values,\n                                                 tensor_input_shape, std_order,\n                                                 &input_st));\n\n    const int64_t N = input_shape_vec(0);\n\n    Tensor sparse_handles(DT_INT64, TensorShape({N}));\n    auto sparse_handles_t = sparse_handles.vec<int64_t>();\n\n    OP_REQUIRES_OK(context, input_st.IndicesValid());\n\n    // We can generate the output shape proto string now, for all\n    // minibatch entries.\n    TensorShape output_shape;\n    OP_REQUIRES_OK(context, TensorShapeUtils::MakeShape(\n                                input_shape_vec.data() + 1,\n                                input_shape->NumElements() - 1, &output_shape));\n\n    // Get groups by minibatch dimension\n    std::unordered_set<int64_t> visited;\n    sparse::GroupIterable minibatch = input_st.group({0});\n    for (const auto& subset : minibatch) {\n      const int64_t b = subset.group()[0];\n      visited.insert(b);\n      OP_REQUIRES(\n          context, b > -1 && b < N,\n          errors::InvalidArgument(\n              \"Received unexpected column 0 value in input SparseTensor: \", b,\n              \" < 0 or >= N (= \", N, \")\"));\n\n      const auto indices = subset.indices();\n      const auto values = subset.values<T>();\n      const int64_t num_entries = values.size();\n\n      Tensor output_indices = Tensor(DT_INT64, {num_entries, rank - 1});\n      Tensor output_values = Tensor(DataTypeToEnum<T>::value, {num_entries});\n\n      auto output_indices_t = output_indices.matrix<int64_t>();\n      auto output_values_t = output_values.vec<T>();\n\n      for (int i = 0; i < num_entries; ++i) {\n        for (int d = 1; d < rank; ++d) {\n          output_indices_t(i, d - 1) = indices(i, d);\n        }\n        output_values_t(i) = values(i);\n      }\n\n      SparseTensor st_i;\n      OP_REQUIRES_OK(context,\n                     SparseTensor::Create(output_indices, output_values,\n                                          output_shape, &st_i));\n      int64_t handle;\n      OP_REQUIRES_OK(context, map->AddSparseTensor(context, st_i, &handle));\n      sparse_handles_t(b) = handle;\n    }\n\n    // Fill in any gaps; we must provide an empty ST for batch entries\n    // the grouper didn't find.\n    if (visited.size() < N) {\n      Tensor empty_indices(DT_INT64, {0, rank - 1});\n      Tensor empty_values(DataTypeToEnum<T>::value, {0});\n      SparseTensor empty_st;\n      OP_REQUIRES_OK(context, SparseTensor::Create(empty_indices, empty_values,\n                                                   output_shape, &empty_st));\n\n      for (int64_t b = 0; b < N; ++b) {\n        // We skipped this batch entry.\n        if (visited.find(b) == visited.end()) {\n          int64_t handle;\n          OP_REQUIRES_OK(context,\n                         map->AddSparseTensor(context, empty_st, &handle));\n          sparse_handles_t(b) = handle;\n        }\n      }\n    }\n\n    context->set_output(0, sparse_handles);\n  }\n};\n\n#define REGISTER_KERNELS(type)                              \\\n  REGISTER_KERNEL_BUILDER(Name(\"AddManySparseToTensorsMap\") \\\n                              .Device(DEVICE_CPU)           \\\n                              .TypeConstraint<type>(\"T\"),   \\\n                          AddManySparseToTensorsMapOp<type>)\n\nTF_CALL_ALL_TYPES(REGISTER_KERNELS);\n#undef REGISTER_KERNELS\n\ntemplate <typename T>\nclass TakeManySparseFromTensorsMapOp : public SparseTensorAccessingOp {\n public:\n  explicit TakeManySparseFromTensorsMapOp(OpKernelConstruction* context)\n      : SparseTensorAccessingOp(context) {}\n\n  void Compute(OpKernelContext* context) override {\n    SparseTensorsMap* map = nullptr;\n    OP_REQUIRES_OK(context, GetMap(context, false /* is_writing */, &map));\n\n    const Tensor& sparse_handles = context->input(0);\n\n    OP_REQUIRES(context, TensorShapeUtils::IsVector(sparse_handles.shape()),\n                errors::InvalidArgument(\n                    \"sparse_handles should be a vector but received shape \",\n                    sparse_handles.shape().DebugString()));\n\n    int64_t N = sparse_handles.shape().dim_size(0);\n\n    OP_REQUIRES(\n        context, N > 0,\n        errors::InvalidArgument(\"Must have at least 1 serialized SparseTensor, \"\n                                \"but input matrix has 0 rows\"));\n\n    std::vector<Tensor> indices_to_concat;\n    std::vector<Tensor> values_to_concat;\n    std::vector<TensorShape> shapes_to_concat;\n\n    const auto& sparse_handles_t = sparse_handles.vec<int64_t>();\n\n    std::vector<SparseTensor> sparse_tensors;\n\n    OP_REQUIRES_OK(context, map->RetrieveAndClearSparseTensors(\n                                context, sparse_handles_t, &sparse_tensors));\n\n    for (int64_t i = 0; i < N; ++i) {\n      const SparseTensor& st = sparse_tensors[i];\n      const Tensor& output_indices = st.indices();\n      const Tensor& output_values = st.values();\n      const auto output_shape = st.shape();\n\n      OP_REQUIRES(context, TensorShapeUtils::IsMatrix(output_indices.shape()),\n                  errors::InvalidArgument(\n                      \"Expected sparse_handles[\", i,\n                      \"] to represent an index matrix but received shape \",\n                      output_indices.shape().DebugString()));\n      OP_REQUIRES(context, TensorShapeUtils::IsVector(output_values.shape()),\n                  errors::InvalidArgument(\n                      \"Expected sparse_handles[\", i,\n                      \"] to represent a values vector but received shape \",\n                      output_values.shape().DebugString()));\n      OP_REQUIRES(\n          context, DataTypeToEnum<T>::value == output_values.dtype(),\n          errors::InvalidArgument(\n              \"Requested SparseTensor of type \",\n              DataTypeString(DataTypeToEnum<T>::value), \" but SparseTensor[\", i,\n              \"].values.dtype() == \", DataTypeString(output_values.dtype())));\n\n      int64_t num_entries = output_indices.dim_size(0);\n      OP_REQUIRES(context, num_entries == output_values.dim_size(0),\n                  errors::InvalidArgument(\n                      \"Expected row counts of SparseTensor[\", i,\n                      \"].indices and SparseTensor[\", i,\n                      \"].values to match but they do not: \", num_entries,\n                      \" vs. \", output_values.dim_size(0)));\n      int rank = output_indices.dim_size(1);\n      OP_REQUIRES(\n          context, rank == output_shape.size(),\n          errors::InvalidArgument(\"Expected column counts of SparseTensor[\", i,\n                                  \"].indices to match size of SparseTensor[\", i,\n                                  \"].shape \"\n                                  \"but they do not: \",\n                                  rank, \" vs. \", output_shape.size()));\n\n      // Now we expand each SparseTensors' indices and shape by\n      // prefixing a dimension\n      Tensor expanded_indices(\n          DT_INT64, TensorShape({num_entries, 1 + output_indices.dim_size(1)}));\n      Tensor expanded_shape(DT_INT64, TensorShape({1 + rank}));\n      const auto& output_indices_t = output_indices.matrix<int64_t>();\n      auto expanded_indices_t = expanded_indices.matrix<int64_t>();\n      auto expanded_shape_t = expanded_shape.vec<int64_t>();\n      expanded_indices_t.chip<1>(0).setZero();\n      Eigen::DSizes<Eigen::DenseIndex, 2> indices_start(0, 1);\n      Eigen::DSizes<Eigen::DenseIndex, 2> indices_sizes(num_entries, rank);\n      expanded_indices_t.slice(indices_start, indices_sizes) = output_indices_t;\n      expanded_shape_t(0) = 1;\n      // TODO: copy shape from TensorShape to &expanded_shape_t(1)\n      // std::copy_n(&output_shape_t(0), rank, &expanded_shape_t(1));\n      for (int i = 0; i < rank; ++i) {\n        expanded_shape_t(i + 1) = output_shape[i];\n      }\n      TensorShape expanded_tensor_shape(expanded_shape_t);\n\n      indices_to_concat.push_back(std::move(expanded_indices));\n      values_to_concat.push_back(output_values);\n      shapes_to_concat.push_back(std::move(expanded_tensor_shape));\n    }\n\n    int rank = -1;\n    for (int i = 0; i < N; ++i) {\n      if (rank < 0) rank = shapes_to_concat[i].dims();\n      OP_REQUIRES(context, rank == shapes_to_concat[i].dims(),\n                  errors::InvalidArgument(\n                      \"Inconsistent rank across SparseTensors: rank prior to \"\n                      \"SparseTensor[\",\n                      i, \"] was: \", rank, \" but rank of SparseTensor[\", i,\n                      \"] is: \", shapes_to_concat[i].dims()));\n    }\n\n    // SparseTensor::Concat requires consistent shape for all but the\n    // primary order dimension (dimension 0 in this case).  So we get\n    // the maximum value across all the input SparseTensors for each\n    // dimension and use that.\n    TensorShape preconcat_shape(shapes_to_concat[0]);\n    for (int i = 0; i < N; ++i) {\n      for (int d = 0; d < rank; ++d) {\n        preconcat_shape.set_dim(d, std::max(preconcat_shape.dim_size(d),\n                                            shapes_to_concat[i].dim_size(d)));\n      }\n    }\n\n    // Dimension 0 is the primary dimension.\n    gtl::InlinedVector<int64_t, 8> std_order(rank);\n    std::iota(std_order.begin(), std_order.end(), 0);\n\n    std::vector<SparseTensor> tensors_to_concat;\n    tensors_to_concat.reserve(N);\n    for (int i = 0; i < N; ++i) {\n      SparseTensor tensor;\n      OP_REQUIRES_OK(context,\n                     SparseTensor::Create(std::move(indices_to_concat[i]),\n                                          std::move(values_to_concat[i]),\n                                          preconcat_shape, std_order, &tensor));\n      tensors_to_concat.push_back(std::move(tensor));\n    }\n\n    auto output = SparseTensor::Concat<T>(tensors_to_concat);\n    Tensor final_output_shape(DT_INT64, TensorShape({output.dims()}));\n\n    std::copy_n(output.shape().data(), output.dims(),\n                final_output_shape.vec<int64_t>().data());\n\n    context->set_output(0, output.indices());\n    context->set_output(1, output.values());\n    context->set_output(2, final_output_shape);\n  }\n};\n\n#define REGISTER_KERNELS(type)                                 \\\n  REGISTER_KERNEL_BUILDER(Name(\"TakeManySparseFromTensorsMap\") \\\n                              .Device(DEVICE_CPU)              \\\n                              .TypeConstraint<type>(\"dtype\"),  \\\n                          TakeManySparseFromTensorsMapOp<type>)\n\nTF_CALL_ALL_TYPES(REGISTER_KERNELS);\n#undef REGISTER_KERNELS\n\n}  // namespace tensorflow\n"], "fixing_code": ["/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#define EIGEN_USE_THREADS\n\n#include <algorithm>\n#include <numeric>\n#include <unordered_map>\n#include <utility>\n#include <vector>\n\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/register_types.h\"\n#include \"tensorflow/core/framework/resource_mgr.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/tensor_util.h\"\n#include \"tensorflow/core/framework/types.h\"\n#include \"tensorflow/core/lib/gtl/inlined_vector.h\"\n#include \"tensorflow/core/util/overflow.h\"\n#include \"tensorflow/core/util/sparse/sparse_tensor.h\"\n\nnamespace tensorflow {\n\ntypedef Eigen::ThreadPoolDevice CPUDevice;\n\nusing sparse::SparseTensor;\n\nclass SparseTensorsMap : public ResourceBase {\n public:\n  explicit SparseTensorsMap(const string& name) : name_(name), counter_(0) {}\n\n  string DebugString() const override { return \"A SparseTensorsMap\"; }\n\n  typedef struct {\n    Tensor indices;\n    Tensor values;\n    gtl::InlinedVector<int64_t, 8> shape;\n  } PersistentSparseTensor;\n\n  Status AddSparseTensor(OpKernelContext* ctx, const SparseTensor& sp,\n                         int64_t* handle) {\n    Tensor ix;\n    TF_RETURN_IF_ERROR(\n        ctx->allocate_temp(sp.indices().dtype(), sp.indices().shape(), &ix));\n    ix = sp.indices();\n\n    Tensor values;\n    TF_RETURN_IF_ERROR(ctx->allocate_temp(sp.indices().dtype(),\n                                          sp.indices().shape(), &values));\n    values = sp.values();\n    {\n      mutex_lock l(mu_);\n      int64_t unique_st_handle = counter_++;  // increment is guarded on purpose\n      sp_tensors_[unique_st_handle] = PersistentSparseTensor{\n          ix, values,\n          gtl::InlinedVector<int64_t, 8>(sp.shape().begin(), sp.shape().end())};\n      *handle = unique_st_handle;\n    }\n    return Status::OK();\n  }\n\n  Status RetrieveAndClearSparseTensors(\n      OpKernelContext* ctx, const TTypes<int64_t>::ConstVec& handles,\n      std::vector<SparseTensor>* sparse_tensors) {\n    sparse_tensors->clear();\n    sparse_tensors->reserve(handles.size());\n    {\n      mutex_lock l(mu_);\n      for (size_t i = 0; i < handles.size(); ++i) {\n        const int64_t handle = handles(i);\n        auto sp_iter = sp_tensors_.find(handle);\n        if (sp_iter == sp_tensors_.end()) {\n          return errors::InvalidArgument(\n              \"Unable to find SparseTensor: \", handle, \" in map: \", name_);\n        }\n        const Tensor* ix = &sp_iter->second.indices;\n        const Tensor* values = &sp_iter->second.values;\n        const auto& shape = sp_iter->second.shape;\n        SparseTensor tensor;\n        TF_RETURN_IF_ERROR(SparseTensor::Create(*ix, *values, shape, &tensor));\n        sparse_tensors->push_back(std::move(tensor));\n        sp_tensors_.erase(sp_iter);\n      }\n    }\n\n    return Status::OK();\n  }\n\n protected:\n  ~SparseTensorsMap() override {}\n\n private:\n  string name_;\n\n  mutex mu_;\n  int64_t counter_ TF_GUARDED_BY(mu_);\n  std::unordered_map<int64_t, PersistentSparseTensor> sp_tensors_\n      TF_GUARDED_BY(mu_);\n};\n\nclass SparseTensorAccessingOp : public OpKernel {\n public:\n  typedef std::function<Status(SparseTensorsMap**)> CreatorCallback;\n\n  explicit SparseTensorAccessingOp(OpKernelConstruction* context)\n      : OpKernel(context), sparse_tensors_map_(nullptr) {}\n\n protected:\n  ~SparseTensorAccessingOp() override {\n    if (sparse_tensors_map_) sparse_tensors_map_->Unref();\n  }\n\n  Status GetMap(OpKernelContext* ctx, bool is_writing,\n                SparseTensorsMap** sparse_tensors_map) {\n    mutex_lock l(mu_);\n\n    if (sparse_tensors_map_) {\n      *sparse_tensors_map = sparse_tensors_map_;\n      return Status::OK();\n    }\n\n    TF_RETURN_IF_ERROR(cinfo_.Init(ctx->resource_manager(), def(),\n                                   is_writing /* use_node_name_as_default */));\n\n    CreatorCallback sparse_tensors_map_creator = [this](SparseTensorsMap** c) {\n      SparseTensorsMap* map = new SparseTensorsMap(cinfo_.name());\n      *c = map;\n      return Status::OK();\n    };\n\n    TF_RETURN_IF_ERROR(\n        cinfo_.resource_manager()->LookupOrCreate<SparseTensorsMap>(\n            cinfo_.container(), cinfo_.name(), &sparse_tensors_map_,\n            sparse_tensors_map_creator));\n\n    *sparse_tensors_map = sparse_tensors_map_;\n    return Status::OK();\n  }\n\n private:\n  ContainerInfo cinfo_;\n\n  mutex mu_;\n  SparseTensorsMap* sparse_tensors_map_ TF_PT_GUARDED_BY(mu_);\n};\n\nclass AddSparseToTensorsMapOp : public SparseTensorAccessingOp {\n public:\n  explicit AddSparseToTensorsMapOp(OpKernelConstruction* context)\n      : SparseTensorAccessingOp(context) {}\n\n  void Compute(OpKernelContext* context) override {\n    const Tensor* input_indices;\n    const Tensor* input_values;\n    const Tensor* input_shape;\n    SparseTensorsMap* map;\n\n    OP_REQUIRES_OK(context, context->input(\"sparse_indices\", &input_indices));\n    OP_REQUIRES_OK(context, context->input(\"sparse_values\", &input_values));\n    OP_REQUIRES_OK(context, context->input(\"sparse_shape\", &input_shape));\n    OP_REQUIRES_OK(context, GetMap(context, true /* is_writing */, &map));\n\n    OP_REQUIRES(context, TensorShapeUtils::IsMatrix(input_indices->shape()),\n                errors::InvalidArgument(\n                    \"Input indices should be a matrix but received shape \",\n                    input_indices->shape().DebugString()));\n\n    OP_REQUIRES(context, TensorShapeUtils::IsVector(input_values->shape()),\n                errors::InvalidArgument(\n                    \"Input values should be a vector but received shape \",\n                    input_values->shape().DebugString()));\n\n    OP_REQUIRES(context, TensorShapeUtils::IsVector(input_shape->shape()),\n                errors::InvalidArgument(\n                    \"Input shape should be a vector but received shape \",\n                    input_shape->shape().DebugString()));\n\n    TensorShape input_shape_object;\n    OP_REQUIRES_OK(\n        context, TensorShapeUtils::MakeShape(input_shape->vec<int64_t>().data(),\n                                             input_shape->NumElements(),\n                                             &input_shape_object));\n    SparseTensor st;\n    OP_REQUIRES_OK(context, SparseTensor::Create(*input_indices, *input_values,\n                                                 input_shape_object, &st));\n    int64_t handle;\n    OP_REQUIRES_OK(context, map->AddSparseTensor(context, st, &handle));\n\n    Tensor sparse_handle(DT_INT64, TensorShape({}));\n    auto sparse_handle_t = sparse_handle.scalar<int64_t>();\n\n    sparse_handle_t() = handle;\n\n    context->set_output(0, sparse_handle);\n  }\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"AddSparseToTensorsMap\").Device(DEVICE_CPU),\n                        AddSparseToTensorsMapOp);\n\ntemplate <typename T>\nclass AddManySparseToTensorsMapOp : public SparseTensorAccessingOp {\n public:\n  explicit AddManySparseToTensorsMapOp(OpKernelConstruction* context)\n      : SparseTensorAccessingOp(context) {}\n\n  void Compute(OpKernelContext* context) override {\n    const Tensor* input_indices;\n    const Tensor* input_values;\n    const Tensor* input_shape;\n    SparseTensorsMap* map;\n\n    OP_REQUIRES_OK(context, context->input(\"sparse_indices\", &input_indices));\n    OP_REQUIRES_OK(context, context->input(\"sparse_values\", &input_values));\n    OP_REQUIRES_OK(context, context->input(\"sparse_shape\", &input_shape));\n    OP_REQUIRES_OK(context, GetMap(context, true /* is_writing */, &map));\n\n    OP_REQUIRES(context, TensorShapeUtils::IsMatrix(input_indices->shape()),\n                errors::InvalidArgument(\n                    \"Input indices should be a matrix but received shape \",\n                    input_indices->shape().DebugString()));\n    OP_REQUIRES(context, TensorShapeUtils::IsVector(input_values->shape()),\n                errors::InvalidArgument(\n                    \"Input values should be a vector but received shape \",\n                    input_values->shape().DebugString()));\n    OP_REQUIRES(context, TensorShapeUtils::IsVector(input_shape->shape()),\n                errors::InvalidArgument(\n                    \"Input shape should be a vector but received shape \",\n                    input_shape->shape().DebugString()));\n    OP_REQUIRES(\n        context,\n        input_values->shape().dim_size(0) == input_indices->shape().dim_size(0),\n        errors::InvalidArgument(\n            \"Number of values must match first dimension of indices. \", \"Got \",\n            input_values->shape().dim_size(0),\n            \" values, indices shape: \", input_indices->shape().DebugString()));\n    OP_REQUIRES(\n        context,\n        input_shape->shape().dim_size(0) == input_indices->shape().dim_size(1),\n        errors::InvalidArgument(\n            \"Number of dimensions must match second dimension of indices. \",\n            \"Got \", input_shape->shape().dim_size(0),\n            \" dimensions, indices shape: \",\n            input_indices->shape().DebugString()));\n\n    int rank = input_shape->NumElements();\n\n    OP_REQUIRES(\n        context, rank > 1,\n        errors::InvalidArgument(\n            \"Rank of input SparseTensor should be > 1, but saw rank: \", rank));\n\n    auto input_shape_vec = input_shape->vec<int64_t>();\n\n    TensorShape tensor_input_shape;\n    OP_REQUIRES_OK(context, TensorShape::BuildTensorShape(input_shape_vec,\n                                                          &tensor_input_shape));\n    gtl::InlinedVector<int64_t, 8> std_order(rank);\n    std::iota(std_order.begin(), std_order.end(), 0);\n    SparseTensor input_st;\n    OP_REQUIRES_OK(context, SparseTensor::Create(*input_indices, *input_values,\n                                                 tensor_input_shape, std_order,\n                                                 &input_st));\n\n    const int64_t N = input_shape_vec(0);\n\n    Tensor sparse_handles(DT_INT64, TensorShape({N}));\n    auto sparse_handles_t = sparse_handles.vec<int64_t>();\n\n    OP_REQUIRES_OK(context, input_st.IndicesValid());\n\n    // We can generate the output shape proto string now, for all\n    // minibatch entries.\n    TensorShape output_shape;\n    OP_REQUIRES_OK(context, TensorShapeUtils::MakeShape(\n                                input_shape_vec.data() + 1,\n                                input_shape->NumElements() - 1, &output_shape));\n\n    // Get groups by minibatch dimension\n    std::unordered_set<int64_t> visited;\n    sparse::GroupIterable minibatch = input_st.group({0});\n    for (const auto& subset : minibatch) {\n      const int64_t b = subset.group()[0];\n      visited.insert(b);\n      OP_REQUIRES(\n          context, b > -1 && b < N,\n          errors::InvalidArgument(\n              \"Received unexpected column 0 value in input SparseTensor: \", b,\n              \" < 0 or >= N (= \", N, \")\"));\n\n      const auto indices = subset.indices();\n      const auto values = subset.values<T>();\n      const int64_t num_entries = values.size();\n\n      Tensor output_indices = Tensor(DT_INT64, {num_entries, rank - 1});\n      Tensor output_values = Tensor(DataTypeToEnum<T>::value, {num_entries});\n\n      auto output_indices_t = output_indices.matrix<int64_t>();\n      auto output_values_t = output_values.vec<T>();\n\n      for (int i = 0; i < num_entries; ++i) {\n        for (int d = 1; d < rank; ++d) {\n          output_indices_t(i, d - 1) = indices(i, d);\n        }\n        output_values_t(i) = values(i);\n      }\n\n      SparseTensor st_i;\n      OP_REQUIRES_OK(context,\n                     SparseTensor::Create(output_indices, output_values,\n                                          output_shape, &st_i));\n      int64_t handle;\n      OP_REQUIRES_OK(context, map->AddSparseTensor(context, st_i, &handle));\n      sparse_handles_t(b) = handle;\n    }\n\n    // Fill in any gaps; we must provide an empty ST for batch entries\n    // the grouper didn't find.\n    if (visited.size() < N) {\n      Tensor empty_indices(DT_INT64, {0, rank - 1});\n      Tensor empty_values(DataTypeToEnum<T>::value, {0});\n      SparseTensor empty_st;\n      OP_REQUIRES_OK(context, SparseTensor::Create(empty_indices, empty_values,\n                                                   output_shape, &empty_st));\n\n      for (int64_t b = 0; b < N; ++b) {\n        // We skipped this batch entry.\n        if (visited.find(b) == visited.end()) {\n          int64_t handle;\n          OP_REQUIRES_OK(context,\n                         map->AddSparseTensor(context, empty_st, &handle));\n          sparse_handles_t(b) = handle;\n        }\n      }\n    }\n\n    context->set_output(0, sparse_handles);\n  }\n};\n\n#define REGISTER_KERNELS(type)                              \\\n  REGISTER_KERNEL_BUILDER(Name(\"AddManySparseToTensorsMap\") \\\n                              .Device(DEVICE_CPU)           \\\n                              .TypeConstraint<type>(\"T\"),   \\\n                          AddManySparseToTensorsMapOp<type>)\n\nTF_CALL_ALL_TYPES(REGISTER_KERNELS);\n#undef REGISTER_KERNELS\n\ntemplate <typename T>\nclass TakeManySparseFromTensorsMapOp : public SparseTensorAccessingOp {\n public:\n  explicit TakeManySparseFromTensorsMapOp(OpKernelConstruction* context)\n      : SparseTensorAccessingOp(context) {}\n\n  void Compute(OpKernelContext* context) override {\n    SparseTensorsMap* map = nullptr;\n    OP_REQUIRES_OK(context, GetMap(context, false /* is_writing */, &map));\n\n    const Tensor& sparse_handles = context->input(0);\n\n    OP_REQUIRES(context, TensorShapeUtils::IsVector(sparse_handles.shape()),\n                errors::InvalidArgument(\n                    \"sparse_handles should be a vector but received shape \",\n                    sparse_handles.shape().DebugString()));\n\n    int64_t N = sparse_handles.shape().dim_size(0);\n\n    OP_REQUIRES(\n        context, N > 0,\n        errors::InvalidArgument(\"Must have at least 1 serialized SparseTensor, \"\n                                \"but input matrix has 0 rows\"));\n\n    std::vector<Tensor> indices_to_concat;\n    std::vector<Tensor> values_to_concat;\n    std::vector<TensorShape> shapes_to_concat;\n\n    const auto& sparse_handles_t = sparse_handles.vec<int64_t>();\n\n    std::vector<SparseTensor> sparse_tensors;\n\n    OP_REQUIRES_OK(context, map->RetrieveAndClearSparseTensors(\n                                context, sparse_handles_t, &sparse_tensors));\n\n    for (int64_t i = 0; i < N; ++i) {\n      const SparseTensor& st = sparse_tensors[i];\n      const Tensor& output_indices = st.indices();\n      const Tensor& output_values = st.values();\n      const auto output_shape = st.shape();\n\n      OP_REQUIRES(context, TensorShapeUtils::IsMatrix(output_indices.shape()),\n                  errors::InvalidArgument(\n                      \"Expected sparse_handles[\", i,\n                      \"] to represent an index matrix but received shape \",\n                      output_indices.shape().DebugString()));\n      OP_REQUIRES(context, TensorShapeUtils::IsVector(output_values.shape()),\n                  errors::InvalidArgument(\n                      \"Expected sparse_handles[\", i,\n                      \"] to represent a values vector but received shape \",\n                      output_values.shape().DebugString()));\n      OP_REQUIRES(\n          context, DataTypeToEnum<T>::value == output_values.dtype(),\n          errors::InvalidArgument(\n              \"Requested SparseTensor of type \",\n              DataTypeString(DataTypeToEnum<T>::value), \" but SparseTensor[\", i,\n              \"].values.dtype() == \", DataTypeString(output_values.dtype())));\n\n      int64_t num_entries = output_indices.dim_size(0);\n      OP_REQUIRES(context, num_entries == output_values.dim_size(0),\n                  errors::InvalidArgument(\n                      \"Expected row counts of SparseTensor[\", i,\n                      \"].indices and SparseTensor[\", i,\n                      \"].values to match but they do not: \", num_entries,\n                      \" vs. \", output_values.dim_size(0)));\n      int rank = output_indices.dim_size(1);\n      OP_REQUIRES(\n          context, rank == output_shape.size(),\n          errors::InvalidArgument(\"Expected column counts of SparseTensor[\", i,\n                                  \"].indices to match size of SparseTensor[\", i,\n                                  \"].shape \"\n                                  \"but they do not: \",\n                                  rank, \" vs. \", output_shape.size()));\n\n      // Now we expand each SparseTensors' indices and shape by\n      // prefixing a dimension\n      Tensor expanded_indices(\n          DT_INT64, TensorShape({num_entries, 1 + output_indices.dim_size(1)}));\n      Tensor expanded_shape(DT_INT64, TensorShape({1 + rank}));\n      const auto& output_indices_t = output_indices.matrix<int64_t>();\n      auto expanded_indices_t = expanded_indices.matrix<int64_t>();\n      auto expanded_shape_t = expanded_shape.vec<int64_t>();\n      expanded_indices_t.chip<1>(0).setZero();\n      Eigen::DSizes<Eigen::DenseIndex, 2> indices_start(0, 1);\n      Eigen::DSizes<Eigen::DenseIndex, 2> indices_sizes(num_entries, rank);\n      expanded_indices_t.slice(indices_start, indices_sizes) = output_indices_t;\n      expanded_shape_t(0) = 1;\n      // TODO: copy shape from TensorShape to &expanded_shape_t(1)\n      // std::copy_n(&output_shape_t(0), rank, &expanded_shape_t(1));\n      for (int i = 0; i < rank; ++i) {\n        expanded_shape_t(i + 1) = output_shape[i];\n      }\n      TensorShape expanded_tensor_shape(expanded_shape_t);\n\n      indices_to_concat.push_back(std::move(expanded_indices));\n      values_to_concat.push_back(output_values);\n      shapes_to_concat.push_back(std::move(expanded_tensor_shape));\n    }\n\n    int rank = -1;\n    for (int i = 0; i < N; ++i) {\n      if (rank < 0) rank = shapes_to_concat[i].dims();\n      OP_REQUIRES(context, rank == shapes_to_concat[i].dims(),\n                  errors::InvalidArgument(\n                      \"Inconsistent rank across SparseTensors: rank prior to \"\n                      \"SparseTensor[\",\n                      i, \"] was: \", rank, \" but rank of SparseTensor[\", i,\n                      \"] is: \", shapes_to_concat[i].dims()));\n    }\n\n    // SparseTensor::Concat requires consistent shape for all but the\n    // primary order dimension (dimension 0 in this case).  So we get\n    // the maximum value across all the input SparseTensors for each\n    // dimension and use that.\n    TensorShape preconcat_shape(shapes_to_concat[0]);\n    for (int i = 0; i < N; ++i) {\n      for (int d = 0; d < rank; ++d) {\n        preconcat_shape.set_dim(d, std::max(preconcat_shape.dim_size(d),\n                                            shapes_to_concat[i].dim_size(d)));\n      }\n    }\n\n    // Dimension 0 is the primary dimension.\n    gtl::InlinedVector<int64_t, 8> std_order(rank);\n    std::iota(std_order.begin(), std_order.end(), 0);\n\n    std::vector<SparseTensor> tensors_to_concat;\n    tensors_to_concat.reserve(N);\n    for (int i = 0; i < N; ++i) {\n      SparseTensor tensor;\n      OP_REQUIRES_OK(context,\n                     SparseTensor::Create(std::move(indices_to_concat[i]),\n                                          std::move(values_to_concat[i]),\n                                          preconcat_shape, std_order, &tensor));\n      tensors_to_concat.push_back(std::move(tensor));\n    }\n\n    auto output = SparseTensor::Concat<T>(tensors_to_concat);\n    Tensor final_output_shape(DT_INT64, TensorShape({output.dims()}));\n\n    std::copy_n(output.shape().data(), output.dims(),\n                final_output_shape.vec<int64_t>().data());\n\n    context->set_output(0, output.indices());\n    context->set_output(1, output.values());\n    context->set_output(2, final_output_shape);\n  }\n};\n\n#define REGISTER_KERNELS(type)                                 \\\n  REGISTER_KERNEL_BUILDER(Name(\"TakeManySparseFromTensorsMap\") \\\n                              .Device(DEVICE_CPU)              \\\n                              .TypeConstraint<type>(\"dtype\"),  \\\n                          TakeManySparseFromTensorsMapOp<type>)\n\nTF_CALL_ALL_TYPES(REGISTER_KERNELS);\n#undef REGISTER_KERNELS\n\n}  // namespace tensorflow\n"], "filenames": ["tensorflow/core/kernels/sparse_tensors_map_ops.cc"], "buggy_code_start_loc": [266], "buggy_code_end_loc": [282], "fixing_code_start_loc": [266], "fixing_code_end_loc": [270], "type": "CWE-190", "message": "Tensorflow is an Open Source Machine Learning Framework. The implementation of `AddManySparseToTensorsMap` is vulnerable to an integer overflow which results in a `CHECK`-fail when building new `TensorShape` objects (so, an assert failure based denial of service). We are missing some validation on the shapes of the input tensors as well as directly constructing a large `TensorShape` with user-provided dimensions. The fix will be included in TensorFlow 2.8.0. We will also cherrypick this commit on TensorFlow 2.7.1, TensorFlow 2.6.3, and TensorFlow 2.5.3, as these are also affected and still in supported range.", "other": {"cve": {"id": "CVE-2022-23568", "sourceIdentifier": "security-advisories@github.com", "published": "2022-02-03T12:15:08.177", "lastModified": "2022-02-09T04:55:54.037", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "Tensorflow is an Open Source Machine Learning Framework. The implementation of `AddManySparseToTensorsMap` is vulnerable to an integer overflow which results in a `CHECK`-fail when building new `TensorShape` objects (so, an assert failure based denial of service). We are missing some validation on the shapes of the input tensors as well as directly constructing a large `TensorShape` with user-provided dimensions. The fix will be included in TensorFlow 2.8.0. We will also cherrypick this commit on TensorFlow 2.7.1, TensorFlow 2.6.3, and TensorFlow 2.5.3, as these are also affected and still in supported range."}, {"lang": "es", "value": "Tensorflow es un marco de aprendizaje autom\u00e1tico de c\u00f3digo abierto. La implementaci\u00f3n de \"AddManySparseToTensorsMap\" es vulnerable a un desbordamiento de enteros que resulta en un fallo de \"CHECK\" cuando son construidos nuevos objetos \"TensorShape\" (por tanto, una denegaci\u00f3n de servicio basada en un fallo de assert). Nos falta algo de comprobaci\u00f3n en las formas de los tensores de entrada, as\u00ed como construir directamente un \"TensorShape\" grande con las dimensiones proporcionadas por el usuario. La correcci\u00f3n ser\u00e1 incluida en TensorFlow versi\u00f3n 2.8.0. Tambi\u00e9n seleccionaremos este commit en TensorFlow versi\u00f3n 2.7.1, TensorFlow versi\u00f3n 2.6.3, y TensorFlow versi\u00f3n 2.5.3, ya que estos tambi\u00e9n est\u00e1n afectados y a\u00fan est\u00e1n en el rango admitido"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 6.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 2.8, "impactScore": 3.6}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 6.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 2.8, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:L/Au:S/C:N/I:N/A:P", "accessVector": "NETWORK", "accessComplexity": "LOW", "authentication": "SINGLE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "PARTIAL", "baseScore": 4.0}, "baseSeverity": "MEDIUM", "exploitabilityScore": 8.0, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-190"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionEndIncluding": "2.5.2", "matchCriteriaId": "688150BF-477C-48FC-9AEF-A79AC57A6DDC"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.6.0", "versionEndIncluding": "2.6.2", "matchCriteriaId": "C9E69B60-8C97-47E2-9027-9598B8392E5D"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.7.0:*:*:*:*:*:*:*", "matchCriteriaId": "2EDFAAB8-799C-4259-9102-944D4760DA2C"}]}]}], "references": [{"url": "https://github.com/tensorflow/tensorflow/blob/5100e359aef5c8021f2e71c7b986420b85ce7b3d/tensorflow/core/kernels/sparse_tensors_map_ops.cc", "source": "security-advisories@github.com", "tags": ["Exploit", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/commit/a68f68061e263a88321c104a6c911fe5598050a8", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/commit/b51b82fe65ebace4475e3c54eb089c18a4403f1c", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/security/advisories/GHSA-6445-fm66-fvq2", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/tensorflow/tensorflow/commit/a68f68061e263a88321c104a6c911fe5598050a8"}}