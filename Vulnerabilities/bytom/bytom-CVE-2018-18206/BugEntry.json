{"buggy_code": ["// Copyright 2016 The go-ethereum Authors\n// This file is part of the go-ethereum library.\n//\n// The go-ethereum library is free software: you can redistribute it and/or modify\n// it under the terms of the GNU Lesser General Public License as published by\n// the Free Software Foundation, either version 3 of the License, or\n// (at your option) any later version.\n//\n// The go-ethereum library is distributed in the hope that it will be useful,\n// but WITHOUT ANY WARRANTY; without even the implied warranty of\n// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n// GNU Lesser General Public License for more details.\n//\n// You should have received a copy of the GNU Lesser General Public License\n// along with the go-ethereum library. If not, see <http://www.gnu.org/licenses/>.\n\npackage discover\n\nimport (\n\t\"bytes\"\n\t\"errors\"\n\t\"fmt\"\n\t\"net\"\n\t\"time\"\n\n\tlog \"github.com/sirupsen/logrus\"\n\t\"github.com/tendermint/go-crypto\"\n\t\"github.com/tendermint/go-wire\"\n\t\"golang.org/x/crypto/sha3\"\n\n\t\"github.com/bytom/common\"\n\t\"github.com/bytom/p2p/netutil\"\n)\n\nvar (\n\terrInvalidEvent = errors.New(\"invalid in current state\")\n\terrNoQuery      = errors.New(\"no pending query\")\n\terrWrongAddress = errors.New(\"unknown sender address\")\n)\n\nconst (\n\tautoRefreshInterval   = 1 * time.Hour\n\tbucketRefreshInterval = 1 * time.Minute\n\tseedCount             = 30\n\tseedMaxAge            = 5 * 24 * time.Hour\n\tlowPort               = 1024\n)\n\nconst testTopic = \"foo\"\n\nconst (\n\tprintTestImgLogs = false\n)\n\n// Network manages the table and all protocol interaction.\ntype Network struct {\n\tdb          *nodeDB // database of known nodes\n\tconn        transport\n\tnetrestrict *netutil.Netlist\n\n\tclosed           chan struct{}          // closed when loop is done\n\tcloseReq         chan struct{}          // 'request to close'\n\trefreshReq       chan []*Node           // lookups ask for refresh on this channel\n\trefreshResp      chan (<-chan struct{}) // ...and get the channel to block on from this one\n\tread             chan ingressPacket     // ingress packets arrive here\n\ttimeout          chan timeoutEvent\n\tqueryReq         chan *findnodeQuery // lookups submit findnode queries on this channel\n\ttableOpReq       chan func()\n\ttableOpResp      chan struct{}\n\ttopicRegisterReq chan topicRegisterReq\n\ttopicSearchReq   chan topicSearchReq\n\n\t// State of the main loop.\n\ttab           *Table\n\ttopictab      *topicTable\n\tticketStore   *ticketStore\n\tnursery       []*Node\n\tnodes         map[NodeID]*Node // tracks active nodes with state != known\n\ttimeoutTimers map[timeoutEvent]*time.Timer\n\n\t// Revalidation queues.\n\t// Nodes put on these queues will be pinged eventually.\n\tslowRevalidateQueue []*Node\n\tfastRevalidateQueue []*Node\n\n\t// Buffers for state transition.\n\tsendBuf []*ingressPacket\n}\n\n// transport is implemented by the UDP transport.\n// it is an interface so we can test without opening lots of UDP\n// sockets and without generating a private key.\ntype transport interface {\n\tsendPing(remote *Node, remoteAddr *net.UDPAddr, topics []Topic) (hash []byte)\n\tsendNeighbours(remote *Node, nodes []*Node)\n\tsendFindnodeHash(remote *Node, target common.Hash)\n\tsendTopicRegister(remote *Node, topics []Topic, topicIdx int, pong []byte)\n\tsendTopicNodes(remote *Node, queryHash common.Hash, nodes []*Node)\n\n\tsend(remote *Node, ptype nodeEvent, p interface{}) (hash []byte)\n\n\tlocalAddr() *net.UDPAddr\n\tClose()\n}\n\ntype findnodeQuery struct {\n\tremote   *Node\n\ttarget   common.Hash\n\treply    chan<- []*Node\n\tnresults int // counter for received nodes\n}\n\ntype topicRegisterReq struct {\n\tadd   bool\n\ttopic Topic\n}\n\ntype topicSearchReq struct {\n\ttopic  Topic\n\tfound  chan<- *Node\n\tlookup chan<- bool\n\tdelay  time.Duration\n}\n\ntype topicSearchResult struct {\n\ttarget lookupInfo\n\tnodes  []*Node\n}\n\ntype timeoutEvent struct {\n\tev   nodeEvent\n\tnode *Node\n}\n\nfunc newNetwork(conn transport, ourPubkey crypto.PubKeyEd25519, dbPath string, netrestrict *netutil.Netlist) (*Network, error) {\n\tourID := NodeID(ourPubkey)\n\n\tvar db *nodeDB\n\tif dbPath != \"<no database>\" {\n\t\tvar err error\n\t\tif db, err = newNodeDB(dbPath, Version, ourID); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\ttab := newTable(ourID, conn.localAddr())\n\tnet := &Network{\n\t\tdb:               db,\n\t\tconn:             conn,\n\t\tnetrestrict:      netrestrict,\n\t\ttab:              tab,\n\t\ttopictab:         newTopicTable(db, tab.self),\n\t\tticketStore:      newTicketStore(),\n\t\trefreshReq:       make(chan []*Node),\n\t\trefreshResp:      make(chan (<-chan struct{})),\n\t\tclosed:           make(chan struct{}),\n\t\tcloseReq:         make(chan struct{}),\n\t\tread:             make(chan ingressPacket, 100),\n\t\ttimeout:          make(chan timeoutEvent),\n\t\ttimeoutTimers:    make(map[timeoutEvent]*time.Timer),\n\t\ttableOpReq:       make(chan func()),\n\t\ttableOpResp:      make(chan struct{}),\n\t\tqueryReq:         make(chan *findnodeQuery),\n\t\ttopicRegisterReq: make(chan topicRegisterReq),\n\t\ttopicSearchReq:   make(chan topicSearchReq),\n\t\tnodes:            make(map[NodeID]*Node),\n\t}\n\tgo net.loop()\n\treturn net, nil\n}\n\n// Close terminates the network listener and flushes the node database.\nfunc (net *Network) Close() {\n\tnet.conn.Close()\n\tselect {\n\tcase <-net.closed:\n\tcase net.closeReq <- struct{}{}:\n\t\t<-net.closed\n\t}\n}\n\n// Self returns the local node.\n// The returned node should not be modified by the caller.\nfunc (net *Network) Self() *Node {\n\treturn net.tab.self\n}\n\n// ReadRandomNodes fills the given slice with random nodes from the\n// table. It will not write the same node more than once. The nodes in\n// the slice are copies and can be modified by the caller.\nfunc (net *Network) ReadRandomNodes(buf []*Node) (n int) {\n\tnet.reqTableOp(func() { n = net.tab.readRandomNodes(buf) })\n\treturn n\n}\n\n// SetFallbackNodes sets the initial points of contact. These nodes\n// are used to connect to the network if the table is empty and there\n// are no known nodes in the database.\nfunc (net *Network) SetFallbackNodes(nodes []*Node) error {\n\tnursery := make([]*Node, 0, len(nodes))\n\tfor _, n := range nodes {\n\t\tif err := n.validateComplete(); err != nil {\n\t\t\treturn fmt.Errorf(\"bad bootstrap/fallback node %q (%v)\", n, err)\n\t\t}\n\t\t// Recompute cpy.sha because the node might not have been\n\t\t// created by NewNode or ParseNode.\n\t\tcpy := *n\n\t\tcpy.sha = common.BytesToHash(n.ID[:])\n\t\tnursery = append(nursery, &cpy)\n\t}\n\tnet.reqRefresh(nursery)\n\treturn nil\n}\n\n// Resolve searches for a specific node with the given ID.\n// It returns nil if the node could not be found.\nfunc (net *Network) Resolve(targetID NodeID) *Node {\n\tresult := net.lookup(common.BytesToHash(targetID[:]), true)\n\tfor _, n := range result {\n\t\tif n.ID == targetID {\n\t\t\treturn n\n\t\t}\n\t}\n\treturn nil\n}\n\n// Lookup performs a network search for nodes close\n// to the given target. It approaches the target by querying\n// nodes that are closer to it on each iteration.\n// The given target does not need to be an actual node\n// identifier.\n//\n// The local node may be included in the result.\nfunc (net *Network) Lookup(targetID NodeID) []*Node {\n\treturn net.lookup(common.BytesToHash(targetID[:]), false)\n}\n\nfunc (net *Network) lookup(target common.Hash, stopOnMatch bool) []*Node {\n\tvar (\n\t\tasked          = make(map[NodeID]bool)\n\t\tseen           = make(map[NodeID]bool)\n\t\treply          = make(chan []*Node, alpha)\n\t\tresult         = nodesByDistance{target: target}\n\t\tpendingQueries = 0\n\t)\n\t// Get initial answers from the local node.\n\tresult.push(net.tab.self, bucketSize)\n\tfor {\n\t\t// Ask the \u03b1 closest nodes that we haven't asked yet.\n\t\tfor i := 0; i < len(result.entries) && pendingQueries < alpha; i++ {\n\t\t\tn := result.entries[i]\n\t\t\tif !asked[n.ID] {\n\t\t\t\tasked[n.ID] = true\n\t\t\t\tpendingQueries++\n\t\t\t\tnet.reqQueryFindnode(n, target, reply)\n\t\t\t}\n\t\t}\n\t\tif pendingQueries == 0 {\n\t\t\t// We have asked all closest nodes, stop the search.\n\t\t\tbreak\n\t\t}\n\t\t// Wait for the next reply.\n\t\tselect {\n\t\tcase nodes := <-reply:\n\t\t\tfor _, n := range nodes {\n\t\t\t\tif n != nil && !seen[n.ID] {\n\t\t\t\t\tseen[n.ID] = true\n\t\t\t\t\tresult.push(n, bucketSize)\n\t\t\t\t\tif stopOnMatch && n.sha == target {\n\t\t\t\t\t\treturn result.entries\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tpendingQueries--\n\t\tcase <-time.After(respTimeout):\n\t\t\t// forget all pending requests, start new ones\n\t\t\tpendingQueries = 0\n\t\t\treply = make(chan []*Node, alpha)\n\t\t}\n\t}\n\treturn result.entries\n}\n\nfunc (net *Network) RegisterTopic(topic Topic, stop <-chan struct{}) {\n\tselect {\n\tcase net.topicRegisterReq <- topicRegisterReq{true, topic}:\n\tcase <-net.closed:\n\t\treturn\n\t}\n\tselect {\n\tcase <-net.closed:\n\tcase <-stop:\n\t\tselect {\n\t\tcase net.topicRegisterReq <- topicRegisterReq{false, topic}:\n\t\tcase <-net.closed:\n\t\t}\n\t}\n}\n\nfunc (net *Network) SearchTopic(topic Topic, setPeriod <-chan time.Duration, found chan<- *Node, lookup chan<- bool) {\n\tfor {\n\t\tselect {\n\t\tcase <-net.closed:\n\t\t\treturn\n\t\tcase delay, ok := <-setPeriod:\n\t\t\tselect {\n\t\t\tcase net.topicSearchReq <- topicSearchReq{topic: topic, found: found, lookup: lookup, delay: delay}:\n\t\t\tcase <-net.closed:\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif !ok {\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc (net *Network) reqRefresh(nursery []*Node) <-chan struct{} {\n\tselect {\n\tcase net.refreshReq <- nursery:\n\t\treturn <-net.refreshResp\n\tcase <-net.closed:\n\t\treturn net.closed\n\t}\n}\n\nfunc (net *Network) reqQueryFindnode(n *Node, target common.Hash, reply chan []*Node) bool {\n\tq := &findnodeQuery{remote: n, target: target, reply: reply}\n\tselect {\n\tcase net.queryReq <- q:\n\t\treturn true\n\tcase <-net.closed:\n\t\treturn false\n\t}\n}\n\nfunc (net *Network) reqReadPacket(pkt ingressPacket) {\n\tselect {\n\tcase net.read <- pkt:\n\tcase <-net.closed:\n\t}\n}\n\nfunc (net *Network) reqTableOp(f func()) (called bool) {\n\tselect {\n\tcase net.tableOpReq <- f:\n\t\t<-net.tableOpResp\n\t\treturn true\n\tcase <-net.closed:\n\t\treturn false\n\t}\n}\n\n// TODO: external address handling.\n\ntype topicSearchInfo struct {\n\tlookupChn chan<- bool\n\tperiod    time.Duration\n}\n\nconst maxSearchCount = 5\n\nfunc (net *Network) loop() {\n\tvar (\n\t\trefreshTimer       = time.NewTicker(autoRefreshInterval)\n\t\tbucketRefreshTimer = time.NewTimer(bucketRefreshInterval)\n\t\trefreshDone        chan struct{} // closed when the 'refresh' lookup has ended\n\t)\n\n\t// Tracking the next ticket to register.\n\tvar (\n\t\tnextTicket        *ticketRef\n\t\tnextRegisterTimer *time.Timer\n\t\tnextRegisterTime  <-chan time.Time\n\t)\n\tdefer func() {\n\t\tif nextRegisterTimer != nil {\n\t\t\tnextRegisterTimer.Stop()\n\t\t}\n\t}()\n\tresetNextTicket := func() {\n\t\tticket, timeout := net.ticketStore.nextFilteredTicket()\n\t\tif nextTicket != ticket {\n\t\t\tnextTicket = ticket\n\t\t\tif nextRegisterTimer != nil {\n\t\t\t\tnextRegisterTimer.Stop()\n\t\t\t\tnextRegisterTime = nil\n\t\t\t}\n\t\t\tif ticket != nil {\n\t\t\t\tnextRegisterTimer = time.NewTimer(timeout)\n\t\t\t\tnextRegisterTime = nextRegisterTimer.C\n\t\t\t}\n\t\t}\n\t}\n\n\t// Tracking registration and search lookups.\n\tvar (\n\t\ttopicRegisterLookupTarget lookupInfo\n\t\ttopicRegisterLookupDone   chan []*Node\n\t\ttopicRegisterLookupTick   = time.NewTimer(0)\n\t\tsearchReqWhenRefreshDone  []topicSearchReq\n\t\tsearchInfo                = make(map[Topic]topicSearchInfo)\n\t\tactiveSearchCount         int\n\t)\n\ttopicSearchLookupDone := make(chan topicSearchResult, 100)\n\ttopicSearch := make(chan Topic, 100)\n\t<-topicRegisterLookupTick.C\n\n\tstatsDump := time.NewTicker(10 * time.Second)\n\nloop:\n\tfor {\n\t\tresetNextTicket()\n\n\t\tselect {\n\t\tcase <-net.closeReq:\n\t\t\tlog.Debug(\"<-net.closeReq\")\n\t\t\tbreak loop\n\n\t\t// Ingress packet handling.\n\t\tcase pkt := <-net.read:\n\t\t\t//fmt.Println(\"read\", pkt.ev)\n\t\t\tlog.Debug(\"<-net.read\")\n\t\t\tn := net.internNode(&pkt)\n\t\t\tprestate := n.state\n\t\t\tstatus := \"ok\"\n\t\t\tif err := net.handle(n, pkt.ev, &pkt); err != nil {\n\t\t\t\tstatus = err.Error()\n\t\t\t}\n\t\t\tlog.Debug(\"\", \"msg\", net.tab.count, pkt.ev, pkt.remoteID[:8], pkt.remoteAddr, prestate, n.state, status)\n\n\t\t\t// TODO: persist state if n.state goes >= known, delete if it goes <= known\n\n\t\t// State transition timeouts.\n\t\tcase timeout := <-net.timeout:\n\t\t\tlog.Debug(\"<-net.timeout\")\n\t\t\tif net.timeoutTimers[timeout] == nil {\n\t\t\t\t// Stale timer (was aborted).\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tdelete(net.timeoutTimers, timeout)\n\t\t\tprestate := timeout.node.state\n\t\t\tstatus := \"ok\"\n\t\t\tif err := net.handle(timeout.node, timeout.ev, nil); err != nil {\n\t\t\t\tstatus = err.Error()\n\t\t\t}\n\t\t\tlog.Debug(\"\", \"msg\", net.tab.count, timeout.ev, timeout.node.ID[:8], timeout.node.addr(), prestate, timeout.node.state, status)\n\n\t\t// Querying.\n\t\tcase q := <-net.queryReq:\n\t\t\tlog.Debug(\"<-net.queryReq\")\n\t\t\tif !q.start(net) {\n\t\t\t\tq.remote.deferQuery(q)\n\t\t\t}\n\n\t\t// Interacting with the table.\n\t\tcase f := <-net.tableOpReq:\n\t\t\tlog.Debug(\"<-net.tableOpReq\")\n\t\t\tf()\n\t\t\tnet.tableOpResp <- struct{}{}\n\n\t\t// Topic registration stuff.\n\t\tcase req := <-net.topicRegisterReq:\n\t\t\tlog.Debug(\"<-net.topicRegisterReq\")\n\t\t\tif !req.add {\n\t\t\t\tnet.ticketStore.removeRegisterTopic(req.topic)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tnet.ticketStore.addTopic(req.topic, true)\n\t\t\t// If we're currently waiting idle (nothing to look up), give the ticket store a\n\t\t\t// chance to start it sooner. This should speed up convergence of the radius\n\t\t\t// determination for new topics.\n\t\t\t// if topicRegisterLookupDone == nil {\n\t\t\tif topicRegisterLookupTarget.target == (common.Hash{}) {\n\t\t\t\tlog.Debug(\"topicRegisterLookupTarget == null\")\n\t\t\t\tif topicRegisterLookupTick.Stop() {\n\t\t\t\t\t<-topicRegisterLookupTick.C\n\t\t\t\t}\n\t\t\t\ttarget, delay := net.ticketStore.nextRegisterLookup()\n\t\t\t\ttopicRegisterLookupTarget = target\n\t\t\t\ttopicRegisterLookupTick.Reset(delay)\n\t\t\t}\n\n\t\tcase nodes := <-topicRegisterLookupDone:\n\t\t\tlog.Debug(\"<-topicRegisterLookupDone\")\n\t\t\tnet.ticketStore.registerLookupDone(topicRegisterLookupTarget, nodes, func(n *Node) []byte {\n\t\t\t\tnet.ping(n, n.addr())\n\t\t\t\treturn n.pingEcho\n\t\t\t})\n\t\t\ttarget, delay := net.ticketStore.nextRegisterLookup()\n\t\t\ttopicRegisterLookupTarget = target\n\t\t\ttopicRegisterLookupTick.Reset(delay)\n\t\t\ttopicRegisterLookupDone = nil\n\n\t\tcase <-topicRegisterLookupTick.C:\n\t\t\tlog.Debug(\"<-topicRegisterLookupTick\")\n\t\t\tif (topicRegisterLookupTarget.target == common.Hash{}) {\n\t\t\t\ttarget, delay := net.ticketStore.nextRegisterLookup()\n\t\t\t\ttopicRegisterLookupTarget = target\n\t\t\t\ttopicRegisterLookupTick.Reset(delay)\n\t\t\t\ttopicRegisterLookupDone = nil\n\t\t\t} else {\n\t\t\t\ttopicRegisterLookupDone = make(chan []*Node)\n\t\t\t\ttarget := topicRegisterLookupTarget.target\n\t\t\t\tgo func() { topicRegisterLookupDone <- net.lookup(target, false) }()\n\t\t\t}\n\n\t\tcase <-nextRegisterTime:\n\t\t\tlog.Debug(\"<-nextRegisterTime\")\n\t\t\tnet.ticketStore.ticketRegistered(*nextTicket)\n\t\t\t//fmt.Println(\"sendTopicRegister\", nextTicket.t.node.addr().String(), nextTicket.t.topics, nextTicket.idx, nextTicket.t.pong)\n\t\t\tnet.conn.sendTopicRegister(nextTicket.t.node, nextTicket.t.topics, nextTicket.idx, nextTicket.t.pong)\n\n\t\tcase req := <-net.topicSearchReq:\n\t\t\tif refreshDone == nil {\n\t\t\t\tlog.Debug(\"<-net.topicSearchReq\")\n\t\t\t\tinfo, ok := searchInfo[req.topic]\n\t\t\t\tif ok {\n\t\t\t\t\tif req.delay == time.Duration(0) {\n\t\t\t\t\t\tdelete(searchInfo, req.topic)\n\t\t\t\t\t\tnet.ticketStore.removeSearchTopic(req.topic)\n\t\t\t\t\t} else {\n\t\t\t\t\t\tinfo.period = req.delay\n\t\t\t\t\t\tsearchInfo[req.topic] = info\n\t\t\t\t\t}\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tif req.delay != time.Duration(0) {\n\t\t\t\t\tvar info topicSearchInfo\n\t\t\t\t\tinfo.period = req.delay\n\t\t\t\t\tinfo.lookupChn = req.lookup\n\t\t\t\t\tsearchInfo[req.topic] = info\n\t\t\t\t\tnet.ticketStore.addSearchTopic(req.topic, req.found)\n\t\t\t\t\ttopicSearch <- req.topic\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tsearchReqWhenRefreshDone = append(searchReqWhenRefreshDone, req)\n\t\t\t}\n\n\t\tcase topic := <-topicSearch:\n\t\t\tif activeSearchCount < maxSearchCount {\n\t\t\t\tactiveSearchCount++\n\t\t\t\ttarget := net.ticketStore.nextSearchLookup(topic)\n\t\t\t\tgo func() {\n\t\t\t\t\tnodes := net.lookup(target.target, false)\n\t\t\t\t\ttopicSearchLookupDone <- topicSearchResult{target: target, nodes: nodes}\n\t\t\t\t}()\n\t\t\t}\n\t\t\tperiod := searchInfo[topic].period\n\t\t\tif period != time.Duration(0) {\n\t\t\t\tgo func() {\n\t\t\t\t\ttime.Sleep(period)\n\t\t\t\t\ttopicSearch <- topic\n\t\t\t\t}()\n\t\t\t}\n\n\t\tcase res := <-topicSearchLookupDone:\n\t\t\tactiveSearchCount--\n\t\t\tif lookupChn := searchInfo[res.target.topic].lookupChn; lookupChn != nil {\n\t\t\t\tlookupChn <- net.ticketStore.radius[res.target.topic].converged\n\t\t\t}\n\t\t\tnet.ticketStore.searchLookupDone(res.target, res.nodes, func(n *Node, topic Topic) []byte {\n\t\t\t\tif n.state != nil && n.state.canQuery {\n\t\t\t\t\treturn net.conn.send(n, topicQueryPacket, topicQuery{Topic: topic}) // TODO: set expiration\n\t\t\t\t} else {\n\t\t\t\t\tif n.state == unknown {\n\t\t\t\t\t\tnet.ping(n, n.addr())\n\t\t\t\t\t}\n\t\t\t\t\treturn nil\n\t\t\t\t}\n\t\t\t})\n\n\t\tcase <-statsDump.C:\n\t\t\tlog.Debug(\"<-statsDump.C\")\n\t\t\t/*r, ok := net.ticketStore.radius[testTopic]\n\t\t\tif !ok {\n\t\t\t\tfmt.Printf(\"(%x) no radius @ %v\\n\", net.tab.self.ID[:8], time.Now())\n\t\t\t} else {\n\t\t\t\ttopics := len(net.ticketStore.tickets)\n\t\t\t\ttickets := len(net.ticketStore.nodes)\n\t\t\t\trad := r.radius / (maxRadius/10000+1)\n\t\t\t\tfmt.Printf(\"(%x) topics:%d radius:%d tickets:%d @ %v\\n\", net.tab.self.ID[:8], topics, rad, tickets, time.Now())\n\t\t\t}*/\n\n\t\t\ttm := Now()\n\t\t\tfor topic, r := range net.ticketStore.radius {\n\t\t\t\tif printTestImgLogs {\n\t\t\t\t\trad := r.radius / (maxRadius/1000000 + 1)\n\t\t\t\t\tminrad := r.minRadius / (maxRadius/1000000 + 1)\n\t\t\t\t\tfmt.Printf(\"*R %d %v %016x %v\\n\", tm/1000000, topic, net.tab.self.sha[:8], rad)\n\t\t\t\t\tfmt.Printf(\"*MR %d %v %016x %v\\n\", tm/1000000, topic, net.tab.self.sha[:8], minrad)\n\t\t\t\t}\n\t\t\t}\n\t\t\tfor topic, t := range net.topictab.topics {\n\t\t\t\twp := t.wcl.nextWaitPeriod(tm)\n\t\t\t\tif printTestImgLogs {\n\t\t\t\t\tfmt.Printf(\"*W %d %v %016x %d\\n\", tm/1000000, topic, net.tab.self.sha[:8], wp/1000000)\n\t\t\t\t}\n\t\t\t}\n\n\t\t// Periodic / lookup-initiated bucket refresh.\n\t\tcase <-refreshTimer.C:\n\t\t\tlog.Debug(\"<-refreshTimer.C\")\n\t\t\t// TODO: ideally we would start the refresh timer after\n\t\t\t// fallback nodes have been set for the first time.\n\t\t\tif refreshDone == nil {\n\t\t\t\trefreshDone = make(chan struct{})\n\t\t\t\tnet.refresh(refreshDone)\n\t\t\t}\n\t\tcase <-bucketRefreshTimer.C:\n\t\t\ttarget := net.tab.chooseBucketRefreshTarget()\n\t\t\tgo func() {\n\t\t\t\tnet.lookup(target, false)\n\t\t\t\tbucketRefreshTimer.Reset(bucketRefreshInterval)\n\t\t\t}()\n\t\tcase newNursery := <-net.refreshReq:\n\t\t\tlog.Debug(\"<-net.refreshReq\")\n\t\t\tif newNursery != nil {\n\t\t\t\tnet.nursery = newNursery\n\t\t\t}\n\t\t\tif refreshDone == nil {\n\t\t\t\trefreshDone = make(chan struct{})\n\t\t\t\tnet.refresh(refreshDone)\n\t\t\t}\n\t\t\tnet.refreshResp <- refreshDone\n\t\tcase <-refreshDone:\n\t\t\tlog.Debug(\"<-net.refreshDone\", \"table size\", net.tab.count)\n\t\t\tif net.tab.count != 0 {\n\t\t\t\trefreshDone = nil\n\t\t\t\tlist := searchReqWhenRefreshDone\n\t\t\t\tsearchReqWhenRefreshDone = nil\n\t\t\t\tgo func() {\n\t\t\t\t\tfor _, req := range list {\n\t\t\t\t\t\tnet.topicSearchReq <- req\n\t\t\t\t\t}\n\t\t\t\t}()\n\t\t\t} else {\n\t\t\t\trefreshDone = make(chan struct{})\n\t\t\t\tnet.refresh(refreshDone)\n\t\t\t}\n\t\t}\n\t}\n\tlog.Debug(\"loop stopped\")\n\n\tlog.Debug(fmt.Sprintf(\"shutting down\"))\n\tif net.conn != nil {\n\t\tnet.conn.Close()\n\t}\n\tif refreshDone != nil {\n\t\t// TODO: wait for pending refresh.\n\t\t//<-refreshResults\n\t}\n\t// Cancel all pending timeouts.\n\tfor _, timer := range net.timeoutTimers {\n\t\ttimer.Stop()\n\t}\n\tif net.db != nil {\n\t\tnet.db.close()\n\t}\n\tclose(net.closed)\n}\n\n// Everything below runs on the Network.loop goroutine\n// and can modify Node, Table and Network at any time without locking.\n\nfunc (net *Network) refresh(done chan<- struct{}) {\n\tvar seeds []*Node\n\tif net.db != nil {\n\t\tseeds = net.db.querySeeds(seedCount, seedMaxAge)\n\t}\n\tif len(seeds) == 0 {\n\t\tseeds = net.nursery\n\t}\n\tif len(seeds) == 0 {\n\t\tlog.Debug(\"no seed nodes found\")\n\t\tclose(done)\n\t\treturn\n\t}\n\tfor _, n := range seeds {\n\t\tn = net.internNodeFromDB(n)\n\t\tif n.state == unknown {\n\t\t\tnet.transition(n, verifyinit)\n\t\t}\n\t\t// Force-add the seed node so Lookup does something.\n\t\t// It will be deleted again if verification fails.\n\t\tnet.tab.add(n)\n\t}\n\t// Start self lookup to fill up the buckets.\n\tgo func() {\n\t\tnet.Lookup(net.tab.self.ID)\n\t\tclose(done)\n\t}()\n}\n\n// Node Interning.\n\nfunc (net *Network) internNode(pkt *ingressPacket) *Node {\n\tif n := net.nodes[pkt.remoteID]; n != nil {\n\t\tn.IP = pkt.remoteAddr.IP\n\t\tn.UDP = uint16(pkt.remoteAddr.Port)\n\t\tn.TCP = uint16(pkt.remoteAddr.Port)\n\t\treturn n\n\t}\n\tn := NewNode(pkt.remoteID, pkt.remoteAddr.IP, uint16(pkt.remoteAddr.Port), uint16(pkt.remoteAddr.Port))\n\tn.state = unknown\n\tnet.nodes[pkt.remoteID] = n\n\treturn n\n}\n\nfunc (net *Network) internNodeFromDB(dbn *Node) *Node {\n\tif n := net.nodes[dbn.ID]; n != nil {\n\t\treturn n\n\t}\n\tn := NewNode(dbn.ID, dbn.IP, dbn.UDP, dbn.TCP)\n\tn.state = unknown\n\tnet.nodes[n.ID] = n\n\treturn n\n}\n\nfunc (net *Network) internNodeFromNeighbours(sender *net.UDPAddr, rn rpcNode) (n *Node, err error) {\n\tif rn.ID == net.tab.self.ID {\n\t\treturn nil, errors.New(\"is self\")\n\t}\n\tif rn.UDP <= lowPort {\n\t\treturn nil, errors.New(\"low port\")\n\t}\n\tn = net.nodes[rn.ID]\n\tif n == nil {\n\t\t// We haven't seen this node before.\n\t\tn, err = nodeFromRPC(sender, rn)\n\t\tif net.netrestrict != nil && !net.netrestrict.Contains(n.IP) {\n\t\t\treturn n, errors.New(\"not contained in netrestrict whitelist\")\n\t\t}\n\t\tif err == nil {\n\t\t\tn.state = unknown\n\t\t\tnet.nodes[n.ID] = n\n\t\t}\n\t\treturn n, err\n\t}\n\tif !n.IP.Equal(rn.IP) || n.UDP != rn.UDP || n.TCP != rn.TCP {\n\t\tif n.state == known {\n\t\t\t// reject address change if node is known by us\n\t\t\terr = fmt.Errorf(\"metadata mismatch: got %v, want %v\", rn, n)\n\t\t} else {\n\t\t\t// accept otherwise; this will be handled nicer with signed ENRs\n\t\t\tn.IP = rn.IP\n\t\t\tn.UDP = rn.UDP\n\t\t\tn.TCP = rn.TCP\n\t\t}\n\t}\n\treturn n, err\n}\n\n// nodeNetGuts is embedded in Node and contains fields.\ntype nodeNetGuts struct {\n\t// This is a cached copy of sha3(ID) which is used for node\n\t// distance calculations. This is part of Node in order to make it\n\t// possible to write tests that need a node at a certain distance.\n\t// In those tests, the content of sha will not actually correspond\n\t// with ID.\n\tsha common.Hash\n\n\t// State machine fields. Access to these fields\n\t// is restricted to the Network.loop goroutine.\n\tstate             *nodeState\n\tpingEcho          []byte           // hash of last ping sent by us\n\tpingTopics        []Topic          // topic set sent by us in last ping\n\tdeferredQueries   []*findnodeQuery // queries that can't be sent yet\n\tpendingNeighbours *findnodeQuery   // current query, waiting for reply\n\tqueryTimeouts     int\n}\n\nfunc (n *nodeNetGuts) deferQuery(q *findnodeQuery) {\n\tn.deferredQueries = append(n.deferredQueries, q)\n}\n\nfunc (n *nodeNetGuts) startNextQuery(net *Network) {\n\tif len(n.deferredQueries) == 0 {\n\t\treturn\n\t}\n\tnextq := n.deferredQueries[0]\n\tif nextq.start(net) {\n\t\tn.deferredQueries = append(n.deferredQueries[:0], n.deferredQueries[1:]...)\n\t}\n}\n\nfunc (q *findnodeQuery) start(net *Network) bool {\n\t// Satisfy queries against the local node directly.\n\tif q.remote == net.tab.self {\n\t\tlog.Debug(\"findnodeQuery self\")\n\t\tclosest := net.tab.closest(common.BytesToHash(q.target[:]), bucketSize)\n\n\t\tq.reply <- closest.entries\n\t\treturn true\n\t}\n\tif q.remote.state.canQuery && q.remote.pendingNeighbours == nil {\n\t\tlog.Debug(\"findnodeQuery\", \"remote peer:\", q.remote.ID, \"targetID:\", q.target)\n\t\tnet.conn.sendFindnodeHash(q.remote, q.target)\n\t\tnet.timedEvent(respTimeout, q.remote, neighboursTimeout)\n\t\tq.remote.pendingNeighbours = q\n\t\treturn true\n\t}\n\t// If the node is not known yet, it won't accept queries.\n\t// Initiate the transition to known.\n\t// The request will be sent later when the node reaches known state.\n\tif q.remote.state == unknown {\n\t\tlog.Debug(\"findnodeQuery\", \"id:\", q.remote.ID, \"status:\", \"unknown->verifyinit\")\n\t\tnet.transition(q.remote, verifyinit)\n\t}\n\treturn false\n}\n\n// Node Events (the input to the state machine).\n\ntype nodeEvent uint\n\n//go:generate stringer -type=nodeEvent\n\nconst (\n\tinvalidEvent nodeEvent = iota // zero is reserved\n\n\t// Packet type events.\n\t// These correspond to packet types in the UDP protocol.\n\tpingPacket\n\tpongPacket\n\tfindnodePacket\n\tneighborsPacket\n\tfindnodeHashPacket\n\ttopicRegisterPacket\n\ttopicQueryPacket\n\ttopicNodesPacket\n\n\t// Non-packet events.\n\t// Event values in this category are allocated outside\n\t// the packet type range (packet types are encoded as a single byte).\n\tpongTimeout nodeEvent = iota + 256\n\tpingTimeout\n\tneighboursTimeout\n)\n\n// Node State Machine.\n\ntype nodeState struct {\n\tname     string\n\thandle   func(*Network, *Node, nodeEvent, *ingressPacket) (next *nodeState, err error)\n\tenter    func(*Network, *Node)\n\tcanQuery bool\n}\n\nfunc (s *nodeState) String() string {\n\treturn s.name\n}\n\nvar (\n\tunknown          *nodeState\n\tverifyinit       *nodeState\n\tverifywait       *nodeState\n\tremoteverifywait *nodeState\n\tknown            *nodeState\n\tcontested        *nodeState\n\tunresponsive     *nodeState\n)\n\nfunc init() {\n\tunknown = &nodeState{\n\t\tname: \"unknown\",\n\t\tenter: func(net *Network, n *Node) {\n\t\t\tnet.tab.delete(n)\n\t\t\tn.pingEcho = nil\n\t\t\t// Abort active queries.\n\t\t\tfor _, q := range n.deferredQueries {\n\t\t\t\tq.reply <- nil\n\t\t\t}\n\t\t\tn.deferredQueries = nil\n\t\t\tif n.pendingNeighbours != nil {\n\t\t\t\tn.pendingNeighbours.reply <- nil\n\t\t\t\tn.pendingNeighbours = nil\n\t\t\t}\n\t\t\tn.queryTimeouts = 0\n\t\t},\n\t\thandle: func(net *Network, n *Node, ev nodeEvent, pkt *ingressPacket) (*nodeState, error) {\n\t\t\tswitch ev {\n\t\t\tcase pingPacket:\n\t\t\t\tnet.handlePing(n, pkt)\n\t\t\t\tnet.ping(n, pkt.remoteAddr)\n\t\t\t\treturn verifywait, nil\n\t\t\tdefault:\n\t\t\t\treturn unknown, errInvalidEvent\n\t\t\t}\n\t\t},\n\t}\n\n\tverifyinit = &nodeState{\n\t\tname: \"verifyinit\",\n\t\tenter: func(net *Network, n *Node) {\n\t\t\tnet.ping(n, n.addr())\n\t\t},\n\t\thandle: func(net *Network, n *Node, ev nodeEvent, pkt *ingressPacket) (*nodeState, error) {\n\t\t\tswitch ev {\n\t\t\tcase pingPacket:\n\t\t\t\tnet.handlePing(n, pkt)\n\t\t\t\treturn verifywait, nil\n\t\t\tcase pongPacket:\n\t\t\t\terr := net.handleKnownPong(n, pkt)\n\t\t\t\treturn remoteverifywait, err\n\t\t\tcase pongTimeout:\n\t\t\t\treturn unknown, nil\n\t\t\tdefault:\n\t\t\t\treturn verifyinit, errInvalidEvent\n\t\t\t}\n\t\t},\n\t}\n\n\tverifywait = &nodeState{\n\t\tname: \"verifywait\",\n\t\thandle: func(net *Network, n *Node, ev nodeEvent, pkt *ingressPacket) (*nodeState, error) {\n\t\t\tswitch ev {\n\t\t\tcase pingPacket:\n\t\t\t\tnet.handlePing(n, pkt)\n\t\t\t\treturn verifywait, nil\n\t\t\tcase pongPacket:\n\t\t\t\terr := net.handleKnownPong(n, pkt)\n\t\t\t\treturn known, err\n\t\t\tcase pongTimeout:\n\t\t\t\treturn unknown, nil\n\t\t\tdefault:\n\t\t\t\treturn verifywait, errInvalidEvent\n\t\t\t}\n\t\t},\n\t}\n\n\tremoteverifywait = &nodeState{\n\t\tname: \"remoteverifywait\",\n\t\tenter: func(net *Network, n *Node) {\n\t\t\tnet.timedEvent(respTimeout, n, pingTimeout)\n\t\t},\n\t\thandle: func(net *Network, n *Node, ev nodeEvent, pkt *ingressPacket) (*nodeState, error) {\n\t\t\tswitch ev {\n\t\t\tcase pingPacket:\n\t\t\t\tnet.handlePing(n, pkt)\n\t\t\t\treturn remoteverifywait, nil\n\t\t\tcase pingTimeout:\n\t\t\t\treturn known, nil\n\t\t\tdefault:\n\t\t\t\treturn remoteverifywait, errInvalidEvent\n\t\t\t}\n\t\t},\n\t}\n\n\tknown = &nodeState{\n\t\tname:     \"known\",\n\t\tcanQuery: true,\n\t\tenter: func(net *Network, n *Node) {\n\t\t\tn.queryTimeouts = 0\n\t\t\tn.startNextQuery(net)\n\t\t\t// Insert into the table and start revalidation of the last node\n\t\t\t// in the bucket if it is full.\n\t\t\tlast := net.tab.add(n)\n\t\t\tif last != nil && last.state == known {\n\t\t\t\t// TODO: do this asynchronously\n\t\t\t\tnet.transition(last, contested)\n\t\t\t}\n\t\t},\n\t\thandle: func(net *Network, n *Node, ev nodeEvent, pkt *ingressPacket) (*nodeState, error) {\n\t\t\tswitch ev {\n\t\t\tcase pingPacket:\n\t\t\t\tnet.handlePing(n, pkt)\n\t\t\t\treturn known, nil\n\t\t\tcase pongPacket:\n\t\t\t\terr := net.handleKnownPong(n, pkt)\n\t\t\t\treturn known, err\n\t\t\tdefault:\n\t\t\t\treturn net.handleQueryEvent(n, ev, pkt)\n\t\t\t}\n\t\t},\n\t}\n\n\tcontested = &nodeState{\n\t\tname:     \"contested\",\n\t\tcanQuery: true,\n\t\tenter: func(net *Network, n *Node) {\n\t\t\tn.pingEcho = nil\n\t\t\tnet.ping(n, n.addr())\n\t\t},\n\t\thandle: func(net *Network, n *Node, ev nodeEvent, pkt *ingressPacket) (*nodeState, error) {\n\t\t\tswitch ev {\n\t\t\tcase pongPacket:\n\t\t\t\t// Node is still alive.\n\t\t\t\terr := net.handleKnownPong(n, pkt)\n\t\t\t\treturn known, err\n\t\t\tcase pongTimeout:\n\t\t\t\tnet.tab.deleteReplace(n)\n\t\t\t\treturn unresponsive, nil\n\t\t\tcase pingPacket:\n\t\t\t\tnet.handlePing(n, pkt)\n\t\t\t\treturn contested, nil\n\t\t\tdefault:\n\t\t\t\treturn net.handleQueryEvent(n, ev, pkt)\n\t\t\t}\n\t\t},\n\t}\n\n\tunresponsive = &nodeState{\n\t\tname:     \"unresponsive\",\n\t\tcanQuery: true,\n\t\thandle: func(net *Network, n *Node, ev nodeEvent, pkt *ingressPacket) (*nodeState, error) {\n\t\t\tswitch ev {\n\t\t\tcase pingPacket:\n\t\t\t\tnet.handlePing(n, pkt)\n\t\t\t\treturn known, nil\n\t\t\tcase pongPacket:\n\t\t\t\terr := net.handleKnownPong(n, pkt)\n\t\t\t\treturn known, err\n\t\t\tdefault:\n\t\t\t\treturn net.handleQueryEvent(n, ev, pkt)\n\t\t\t}\n\t\t},\n\t}\n}\n\n// handle processes packets sent by n and events related to n.\nfunc (net *Network) handle(n *Node, ev nodeEvent, pkt *ingressPacket) error {\n\t//fmt.Println(\"handle\", n.addr().String(), n.state, ev)\n\tif pkt != nil {\n\t\tif err := net.checkPacket(n, ev, pkt); err != nil {\n\t\t\t//fmt.Println(\"check err:\", err)\n\t\t\treturn err\n\t\t}\n\t\t// Start the background expiration goroutine after the first\n\t\t// successful communication. Subsequent calls have no effect if it\n\t\t// is already running. We do this here instead of somewhere else\n\t\t// so that the search for seed nodes also considers older nodes\n\t\t// that would otherwise be removed by the expirer.\n\t\tif net.db != nil {\n\t\t\tnet.db.ensureExpirer()\n\t\t}\n\t}\n\tif n.state == nil {\n\t\tn.state = unknown //???\n\t}\n\tnext, err := n.state.handle(net, n, ev, pkt)\n\tnet.transition(n, next)\n\t//fmt.Println(\"new state:\", n.state)\n\treturn err\n}\n\nfunc (net *Network) checkPacket(n *Node, ev nodeEvent, pkt *ingressPacket) error {\n\t// Replay prevention checks.\n\tswitch ev {\n\tcase pingPacket, findnodeHashPacket, neighborsPacket:\n\t\t// TODO: check date is > last date seen\n\t\t// TODO: check ping version\n\tcase pongPacket:\n\t\tif !bytes.Equal(pkt.data.(*pong).ReplyTok, n.pingEcho) {\n\t\t\t// fmt.Println(\"pong reply token mismatch\")\n\t\t\treturn fmt.Errorf(\"pong reply token mismatch\")\n\t\t}\n\t\tn.pingEcho = nil\n\t}\n\t// Address validation.\n\t// TODO: Ideally we would do the following:\n\t//  - reject all packets with wrong address except ping.\n\t//  - for ping with new address, transition to verifywait but keep the\n\t//    previous node (with old address) around. if the new one reaches known,\n\t//    swap it out.\n\treturn nil\n}\n\nfunc (net *Network) transition(n *Node, next *nodeState) {\n\tif n.state != next {\n\t\tn.state = next\n\t\tif next.enter != nil {\n\t\t\tnext.enter(net, n)\n\t\t}\n\t}\n\n\t// TODO: persist/unpersist node\n}\n\nfunc (net *Network) timedEvent(d time.Duration, n *Node, ev nodeEvent) {\n\ttimeout := timeoutEvent{ev, n}\n\tnet.timeoutTimers[timeout] = time.AfterFunc(d, func() {\n\t\tselect {\n\t\tcase net.timeout <- timeout:\n\t\tcase <-net.closed:\n\t\t}\n\t})\n}\n\nfunc (net *Network) abortTimedEvent(n *Node, ev nodeEvent) {\n\ttimer := net.timeoutTimers[timeoutEvent{ev, n}]\n\tif timer != nil {\n\t\ttimer.Stop()\n\t\tdelete(net.timeoutTimers, timeoutEvent{ev, n})\n\t}\n}\n\nfunc (net *Network) ping(n *Node, addr *net.UDPAddr) {\n\t//fmt.Println(\"ping\", n.addr().String(), n.ID.String(), n.sha.Hex())\n\tif n.pingEcho != nil || n.ID == net.tab.self.ID {\n\t\t//fmt.Println(\" not sent\")\n\t\treturn\n\t}\n\tlog.Debug(\"Pinging remote node\", \"node\", n.ID)\n\tn.pingTopics = net.ticketStore.regTopicSet()\n\tn.pingEcho = net.conn.sendPing(n, addr, n.pingTopics)\n\tnet.timedEvent(respTimeout, n, pongTimeout)\n}\n\nfunc (net *Network) handlePing(n *Node, pkt *ingressPacket) {\n\tlog.Debug(\"Handling remote ping\", \"node\", n.ID)\n\tping := pkt.data.(*ping)\n\tn.TCP = ping.From.TCP\n\tt := net.topictab.getTicket(n, ping.Topics)\n\n\tpong := &pong{\n\t\tTo:         makeEndpoint(n.addr(), n.TCP), // TODO: maybe use known TCP port from DB\n\t\tReplyTok:   pkt.hash,\n\t\tExpiration: uint64(time.Now().Add(expiration).Unix()),\n\t}\n\tticketToPong(t, pong)\n\tnet.conn.send(n, pongPacket, pong)\n}\n\nfunc (net *Network) handleKnownPong(n *Node, pkt *ingressPacket) error {\n\tlog.Debug(\"Handling known pong\", \"node\", n.ID)\n\tnet.abortTimedEvent(n, pongTimeout)\n\tnow := Now()\n\tticket, err := pongToTicket(now, n.pingTopics, n, pkt)\n\tif err == nil {\n\t\t// fmt.Printf(\"(%x) ticket: %+v\\n\", net.tab.self.ID[:8], pkt.data)\n\t\tnet.ticketStore.addTicket(now, pkt.data.(*pong).ReplyTok, ticket)\n\t} else {\n\t\tlog.Debug(\"Failed to convert pong to ticket\", \"err\", err)\n\t}\n\tn.pingEcho = nil\n\tn.pingTopics = nil\n\treturn err\n}\n\nfunc (net *Network) handleQueryEvent(n *Node, ev nodeEvent, pkt *ingressPacket) (*nodeState, error) {\n\tswitch ev {\n\tcase findnodePacket:\n\t\ttarget := common.BytesToHash(pkt.data.(*findnode).Target[:])\n\t\tresults := net.tab.closest(target, bucketSize).entries\n\t\tnet.conn.sendNeighbours(n, results)\n\t\treturn n.state, nil\n\tcase neighborsPacket:\n\t\terr := net.handleNeighboursPacket(n, pkt)\n\t\treturn n.state, err\n\tcase neighboursTimeout:\n\t\tif n.pendingNeighbours != nil {\n\t\t\tn.pendingNeighbours.reply <- nil\n\t\t\tn.pendingNeighbours = nil\n\t\t}\n\t\tn.queryTimeouts++\n\t\tif n.queryTimeouts > maxFindnodeFailures && n.state == known {\n\t\t\treturn contested, errors.New(\"too many timeouts\")\n\t\t}\n\t\treturn n.state, nil\n\n\t// v5\n\n\tcase findnodeHashPacket:\n\t\tresults := net.tab.closest(pkt.data.(*findnodeHash).Target, bucketSize).entries\n\t\tnet.conn.sendNeighbours(n, results)\n\t\treturn n.state, nil\n\tcase topicRegisterPacket:\n\t\t//fmt.Println(\"got topicRegisterPacket\")\n\t\tregdata := pkt.data.(*topicRegister)\n\t\tpong, err := net.checkTopicRegister(regdata)\n\t\tif err != nil {\n\t\t\t//fmt.Println(err)\n\t\t\treturn n.state, fmt.Errorf(\"bad waiting ticket: %v\", err)\n\t\t}\n\t\tnet.topictab.useTicket(n, pong.TicketSerial, regdata.Topics, int(regdata.Idx), pong.Expiration, pong.WaitPeriods)\n\t\treturn n.state, nil\n\tcase topicQueryPacket:\n\t\t// TODO: handle expiration\n\t\ttopic := pkt.data.(*topicQuery).Topic\n\t\tresults := net.topictab.getEntries(topic)\n\t\tif _, ok := net.ticketStore.tickets[topic]; ok {\n\t\t\tresults = append(results, net.tab.self) // we're not registering in our own table but if we're advertising, return ourselves too\n\t\t}\n\t\tif len(results) > 10 {\n\t\t\tresults = results[:10]\n\t\t}\n\t\tvar hash common.Hash\n\t\tcopy(hash[:], pkt.hash)\n\t\tnet.conn.sendTopicNodes(n, hash, results)\n\t\treturn n.state, nil\n\tcase topicNodesPacket:\n\t\tp := pkt.data.(*topicNodes)\n\t\tif net.ticketStore.gotTopicNodes(n, p.Echo, p.Nodes) {\n\t\t\tn.queryTimeouts++\n\t\t\tif n.queryTimeouts > maxFindnodeFailures && n.state == known {\n\t\t\t\treturn contested, errors.New(\"too many timeouts\")\n\t\t\t}\n\t\t}\n\t\treturn n.state, nil\n\n\tdefault:\n\t\treturn n.state, errInvalidEvent\n\t}\n}\n\nfunc (net *Network) checkTopicRegister(data *topicRegister) (*pong, error) {\n\tvar pongpkt ingressPacket\n\tif err := decodePacket(data.Pong, &pongpkt); err != nil {\n\t\treturn nil, err\n\t}\n\tif pongpkt.ev != pongPacket {\n\t\treturn nil, errors.New(\"is not pong packet\")\n\t}\n\tif pongpkt.remoteID != net.tab.self.ID {\n\t\treturn nil, errors.New(\"not signed by us\")\n\t}\n\t// check that we previously authorised all topics\n\t// that the other side is trying to register.\n\thash, _, _ := wireHash(data.Topics)\n\tif hash != pongpkt.data.(*pong).TopicHash {\n\t\treturn nil, errors.New(\"topic hash mismatch\")\n\t}\n\tif data.Idx < 0 || int(data.Idx) >= len(data.Topics) {\n\t\treturn nil, errors.New(\"topic index out of range\")\n\t}\n\treturn pongpkt.data.(*pong), nil\n}\n\nfunc wireHash(x interface{}) (h common.Hash, n int, err error) {\n\thw := sha3.New256()\n\twire.WriteBinary(x, hw, &n, &err)\n\thw.Sum(h[:0])\n\treturn h, n, err\n}\n\nfunc (net *Network) handleNeighboursPacket(n *Node, pkt *ingressPacket) error {\n\tif n.pendingNeighbours == nil {\n\t\treturn errNoQuery\n\t}\n\tnet.abortTimedEvent(n, neighboursTimeout)\n\n\treq := pkt.data.(*neighbors)\n\tnodes := make([]*Node, len(req.Nodes))\n\tfor i, rn := range req.Nodes {\n\t\tnn, err := net.internNodeFromNeighbours(pkt.remoteAddr, rn)\n\t\tif err != nil {\n\t\t\tlog.Debug(fmt.Sprintf(\"invalid neighbour (%v) from %x@%v: %v\", rn.IP, n.ID[:8], pkt.remoteAddr, err))\n\t\t\tcontinue\n\t\t}\n\t\tnodes[i] = nn\n\t\t// Start validation of query results immediately.\n\t\t// This fills the table quickly.\n\t\t// TODO: generates way too many packets, maybe do it via queue.\n\t\tif nn.state == unknown {\n\t\t\tnet.transition(nn, verifyinit)\n\t\t}\n\t}\n\t// TODO: don't ignore second packet\n\tn.pendingNeighbours.reply <- nodes\n\tn.pendingNeighbours = nil\n\t// Now that this query is done, start the next one.\n\tn.startNextQuery(net)\n\treturn nil\n}\n"], "fixing_code": ["// Copyright 2016 The go-ethereum Authors\n// This file is part of the go-ethereum library.\n//\n// The go-ethereum library is free software: you can redistribute it and/or modify\n// it under the terms of the GNU Lesser General Public License as published by\n// the Free Software Foundation, either version 3 of the License, or\n// (at your option) any later version.\n//\n// The go-ethereum library is distributed in the hope that it will be useful,\n// but WITHOUT ANY WARRANTY; without even the implied warranty of\n// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n// GNU Lesser General Public License for more details.\n//\n// You should have received a copy of the GNU Lesser General Public License\n// along with the go-ethereum library. If not, see <http://www.gnu.org/licenses/>.\n\npackage discover\n\nimport (\n\t\"bytes\"\n\t\"errors\"\n\t\"fmt\"\n\t\"net\"\n\t\"time\"\n\n\tlog \"github.com/sirupsen/logrus\"\n\t\"github.com/tendermint/go-crypto\"\n\t\"github.com/tendermint/go-wire\"\n\t\"golang.org/x/crypto/sha3\"\n\n\t\"github.com/bytom/common\"\n\t\"github.com/bytom/p2p/netutil\"\n)\n\nvar (\n\terrInvalidEvent = errors.New(\"invalid in current state\")\n\terrNoQuery      = errors.New(\"no pending query\")\n\terrWrongAddress = errors.New(\"unknown sender address\")\n)\n\nconst (\n\tautoRefreshInterval   = 1 * time.Hour\n\tbucketRefreshInterval = 1 * time.Minute\n\tseedCount             = 30\n\tseedMaxAge            = 5 * 24 * time.Hour\n\tlowPort               = 1024\n)\n\nconst testTopic = \"foo\"\n\nconst (\n\tprintTestImgLogs = false\n)\n\n// Network manages the table and all protocol interaction.\ntype Network struct {\n\tdb          *nodeDB // database of known nodes\n\tconn        transport\n\tnetrestrict *netutil.Netlist\n\n\tclosed           chan struct{}          // closed when loop is done\n\tcloseReq         chan struct{}          // 'request to close'\n\trefreshReq       chan []*Node           // lookups ask for refresh on this channel\n\trefreshResp      chan (<-chan struct{}) // ...and get the channel to block on from this one\n\tread             chan ingressPacket     // ingress packets arrive here\n\ttimeout          chan timeoutEvent\n\tqueryReq         chan *findnodeQuery // lookups submit findnode queries on this channel\n\ttableOpReq       chan func()\n\ttableOpResp      chan struct{}\n\ttopicRegisterReq chan topicRegisterReq\n\ttopicSearchReq   chan topicSearchReq\n\n\t// State of the main loop.\n\ttab           *Table\n\ttopictab      *topicTable\n\tticketStore   *ticketStore\n\tnursery       []*Node\n\tnodes         map[NodeID]*Node // tracks active nodes with state != known\n\ttimeoutTimers map[timeoutEvent]*time.Timer\n\n\t// Revalidation queues.\n\t// Nodes put on these queues will be pinged eventually.\n\tslowRevalidateQueue []*Node\n\tfastRevalidateQueue []*Node\n\n\t// Buffers for state transition.\n\tsendBuf []*ingressPacket\n}\n\n// transport is implemented by the UDP transport.\n// it is an interface so we can test without opening lots of UDP\n// sockets and without generating a private key.\ntype transport interface {\n\tsendPing(remote *Node, remoteAddr *net.UDPAddr, topics []Topic) (hash []byte)\n\tsendNeighbours(remote *Node, nodes []*Node)\n\tsendFindnodeHash(remote *Node, target common.Hash)\n\tsendTopicRegister(remote *Node, topics []Topic, topicIdx int, pong []byte)\n\tsendTopicNodes(remote *Node, queryHash common.Hash, nodes []*Node)\n\n\tsend(remote *Node, ptype nodeEvent, p interface{}) (hash []byte)\n\n\tlocalAddr() *net.UDPAddr\n\tClose()\n}\n\ntype findnodeQuery struct {\n\tremote   *Node\n\ttarget   common.Hash\n\treply    chan<- []*Node\n\tnresults int // counter for received nodes\n}\n\ntype topicRegisterReq struct {\n\tadd   bool\n\ttopic Topic\n}\n\ntype topicSearchReq struct {\n\ttopic  Topic\n\tfound  chan<- *Node\n\tlookup chan<- bool\n\tdelay  time.Duration\n}\n\ntype topicSearchResult struct {\n\ttarget lookupInfo\n\tnodes  []*Node\n}\n\ntype timeoutEvent struct {\n\tev   nodeEvent\n\tnode *Node\n}\n\nfunc newNetwork(conn transport, ourPubkey crypto.PubKeyEd25519, dbPath string, netrestrict *netutil.Netlist) (*Network, error) {\n\tourID := NodeID(ourPubkey)\n\n\tvar db *nodeDB\n\tif dbPath != \"<no database>\" {\n\t\tvar err error\n\t\tif db, err = newNodeDB(dbPath, Version, ourID); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\ttab := newTable(ourID, conn.localAddr())\n\tnet := &Network{\n\t\tdb:               db,\n\t\tconn:             conn,\n\t\tnetrestrict:      netrestrict,\n\t\ttab:              tab,\n\t\ttopictab:         newTopicTable(db, tab.self),\n\t\tticketStore:      newTicketStore(),\n\t\trefreshReq:       make(chan []*Node),\n\t\trefreshResp:      make(chan (<-chan struct{})),\n\t\tclosed:           make(chan struct{}),\n\t\tcloseReq:         make(chan struct{}),\n\t\tread:             make(chan ingressPacket, 100),\n\t\ttimeout:          make(chan timeoutEvent),\n\t\ttimeoutTimers:    make(map[timeoutEvent]*time.Timer),\n\t\ttableOpReq:       make(chan func()),\n\t\ttableOpResp:      make(chan struct{}),\n\t\tqueryReq:         make(chan *findnodeQuery),\n\t\ttopicRegisterReq: make(chan topicRegisterReq),\n\t\ttopicSearchReq:   make(chan topicSearchReq),\n\t\tnodes:            make(map[NodeID]*Node),\n\t}\n\tgo net.loop()\n\treturn net, nil\n}\n\n// Close terminates the network listener and flushes the node database.\nfunc (net *Network) Close() {\n\tnet.conn.Close()\n\tselect {\n\tcase <-net.closed:\n\tcase net.closeReq <- struct{}{}:\n\t\t<-net.closed\n\t}\n}\n\n// Self returns the local node.\n// The returned node should not be modified by the caller.\nfunc (net *Network) Self() *Node {\n\treturn net.tab.self\n}\n\n// ReadRandomNodes fills the given slice with random nodes from the\n// table. It will not write the same node more than once. The nodes in\n// the slice are copies and can be modified by the caller.\nfunc (net *Network) ReadRandomNodes(buf []*Node) (n int) {\n\tnet.reqTableOp(func() { n = net.tab.readRandomNodes(buf) })\n\treturn n\n}\n\n// SetFallbackNodes sets the initial points of contact. These nodes\n// are used to connect to the network if the table is empty and there\n// are no known nodes in the database.\nfunc (net *Network) SetFallbackNodes(nodes []*Node) error {\n\tnursery := make([]*Node, 0, len(nodes))\n\tfor _, n := range nodes {\n\t\tif err := n.validateComplete(); err != nil {\n\t\t\treturn fmt.Errorf(\"bad bootstrap/fallback node %q (%v)\", n, err)\n\t\t}\n\t\t// Recompute cpy.sha because the node might not have been\n\t\t// created by NewNode or ParseNode.\n\t\tcpy := *n\n\t\tcpy.sha = common.BytesToHash(n.ID[:])\n\t\tnursery = append(nursery, &cpy)\n\t}\n\tnet.reqRefresh(nursery)\n\treturn nil\n}\n\n// Resolve searches for a specific node with the given ID.\n// It returns nil if the node could not be found.\nfunc (net *Network) Resolve(targetID NodeID) *Node {\n\tresult := net.lookup(common.BytesToHash(targetID[:]), true)\n\tfor _, n := range result {\n\t\tif n.ID == targetID {\n\t\t\treturn n\n\t\t}\n\t}\n\treturn nil\n}\n\n// Lookup performs a network search for nodes close\n// to the given target. It approaches the target by querying\n// nodes that are closer to it on each iteration.\n// The given target does not need to be an actual node\n// identifier.\n//\n// The local node may be included in the result.\nfunc (net *Network) Lookup(targetID NodeID) []*Node {\n\treturn net.lookup(common.BytesToHash(targetID[:]), false)\n}\n\nfunc (net *Network) lookup(target common.Hash, stopOnMatch bool) []*Node {\n\tvar (\n\t\tasked          = make(map[NodeID]bool)\n\t\tseen           = make(map[NodeID]bool)\n\t\treply          = make(chan []*Node, alpha)\n\t\tresult         = nodesByDistance{target: target}\n\t\tpendingQueries = 0\n\t)\n\t// Get initial answers from the local node.\n\tresult.push(net.tab.self, bucketSize)\n\tfor {\n\t\t// Ask the \u03b1 closest nodes that we haven't asked yet.\n\t\tfor i := 0; i < len(result.entries) && pendingQueries < alpha; i++ {\n\t\t\tn := result.entries[i]\n\t\t\tif !asked[n.ID] {\n\t\t\t\tasked[n.ID] = true\n\t\t\t\tpendingQueries++\n\t\t\t\tnet.reqQueryFindnode(n, target, reply)\n\t\t\t}\n\t\t}\n\t\tif pendingQueries == 0 {\n\t\t\t// We have asked all closest nodes, stop the search.\n\t\t\tbreak\n\t\t}\n\t\t// Wait for the next reply.\n\t\tselect {\n\t\tcase nodes := <-reply:\n\t\t\tfor _, n := range nodes {\n\t\t\t\tif n != nil && !seen[n.ID] {\n\t\t\t\t\tseen[n.ID] = true\n\t\t\t\t\tresult.push(n, bucketSize)\n\t\t\t\t\tif stopOnMatch && n.sha == target {\n\t\t\t\t\t\treturn result.entries\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tpendingQueries--\n\t\tcase <-time.After(respTimeout):\n\t\t\t// forget all pending requests, start new ones\n\t\t\tpendingQueries = 0\n\t\t\treply = make(chan []*Node, alpha)\n\t\t}\n\t}\n\treturn result.entries\n}\n\nfunc (net *Network) RegisterTopic(topic Topic, stop <-chan struct{}) {\n\tselect {\n\tcase net.topicRegisterReq <- topicRegisterReq{true, topic}:\n\tcase <-net.closed:\n\t\treturn\n\t}\n\tselect {\n\tcase <-net.closed:\n\tcase <-stop:\n\t\tselect {\n\t\tcase net.topicRegisterReq <- topicRegisterReq{false, topic}:\n\t\tcase <-net.closed:\n\t\t}\n\t}\n}\n\nfunc (net *Network) SearchTopic(topic Topic, setPeriod <-chan time.Duration, found chan<- *Node, lookup chan<- bool) {\n\tfor {\n\t\tselect {\n\t\tcase <-net.closed:\n\t\t\treturn\n\t\tcase delay, ok := <-setPeriod:\n\t\t\tselect {\n\t\t\tcase net.topicSearchReq <- topicSearchReq{topic: topic, found: found, lookup: lookup, delay: delay}:\n\t\t\tcase <-net.closed:\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif !ok {\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc (net *Network) reqRefresh(nursery []*Node) <-chan struct{} {\n\tselect {\n\tcase net.refreshReq <- nursery:\n\t\treturn <-net.refreshResp\n\tcase <-net.closed:\n\t\treturn net.closed\n\t}\n}\n\nfunc (net *Network) reqQueryFindnode(n *Node, target common.Hash, reply chan []*Node) bool {\n\tq := &findnodeQuery{remote: n, target: target, reply: reply}\n\tselect {\n\tcase net.queryReq <- q:\n\t\treturn true\n\tcase <-net.closed:\n\t\treturn false\n\t}\n}\n\nfunc (net *Network) reqReadPacket(pkt ingressPacket) {\n\tselect {\n\tcase net.read <- pkt:\n\tcase <-net.closed:\n\t}\n}\n\nfunc (net *Network) reqTableOp(f func()) (called bool) {\n\tselect {\n\tcase net.tableOpReq <- f:\n\t\t<-net.tableOpResp\n\t\treturn true\n\tcase <-net.closed:\n\t\treturn false\n\t}\n}\n\n// TODO: external address handling.\n\ntype topicSearchInfo struct {\n\tlookupChn chan<- bool\n\tperiod    time.Duration\n}\n\nconst maxSearchCount = 5\n\nfunc (net *Network) loop() {\n\tvar (\n\t\trefreshTimer       = time.NewTicker(autoRefreshInterval)\n\t\tbucketRefreshTimer = time.NewTimer(bucketRefreshInterval)\n\t\trefreshDone        chan struct{} // closed when the 'refresh' lookup has ended\n\t)\n\n\t// Tracking the next ticket to register.\n\tvar (\n\t\tnextTicket        *ticketRef\n\t\tnextRegisterTimer *time.Timer\n\t\tnextRegisterTime  <-chan time.Time\n\t)\n\tdefer func() {\n\t\tif nextRegisterTimer != nil {\n\t\t\tnextRegisterTimer.Stop()\n\t\t}\n\t}()\n\tresetNextTicket := func() {\n\t\tticket, timeout := net.ticketStore.nextFilteredTicket()\n\t\tif nextTicket != ticket {\n\t\t\tnextTicket = ticket\n\t\t\tif nextRegisterTimer != nil {\n\t\t\t\tnextRegisterTimer.Stop()\n\t\t\t\tnextRegisterTime = nil\n\t\t\t}\n\t\t\tif ticket != nil {\n\t\t\t\tnextRegisterTimer = time.NewTimer(timeout)\n\t\t\t\tnextRegisterTime = nextRegisterTimer.C\n\t\t\t}\n\t\t}\n\t}\n\n\t// Tracking registration and search lookups.\n\tvar (\n\t\ttopicRegisterLookupTarget lookupInfo\n\t\ttopicRegisterLookupDone   chan []*Node\n\t\ttopicRegisterLookupTick   = time.NewTimer(0)\n\t\tsearchReqWhenRefreshDone  []topicSearchReq\n\t\tsearchInfo                = make(map[Topic]topicSearchInfo)\n\t\tactiveSearchCount         int\n\t)\n\ttopicSearchLookupDone := make(chan topicSearchResult, 100)\n\ttopicSearch := make(chan Topic, 100)\n\t<-topicRegisterLookupTick.C\n\n\tstatsDump := time.NewTicker(10 * time.Second)\n\nloop:\n\tfor {\n\t\tresetNextTicket()\n\n\t\tselect {\n\t\tcase <-net.closeReq:\n\t\t\tlog.Debug(\"<-net.closeReq\")\n\t\t\tbreak loop\n\n\t\t// Ingress packet handling.\n\t\tcase pkt := <-net.read:\n\t\t\t//fmt.Println(\"read\", pkt.ev)\n\t\t\tlog.Debug(\"<-net.read\")\n\t\t\tn := net.internNode(&pkt)\n\t\t\tprestate := n.state\n\t\t\tstatus := \"ok\"\n\t\t\tif err := net.handle(n, pkt.ev, &pkt); err != nil {\n\t\t\t\tstatus = err.Error()\n\t\t\t}\n\t\t\tlog.Debug(\"\", \"msg\", net.tab.count, pkt.ev, pkt.remoteID[:8], pkt.remoteAddr, prestate, n.state, status)\n\n\t\t\t// TODO: persist state if n.state goes >= known, delete if it goes <= known\n\n\t\t// State transition timeouts.\n\t\tcase timeout := <-net.timeout:\n\t\t\tlog.Debug(\"<-net.timeout\")\n\t\t\tif net.timeoutTimers[timeout] == nil {\n\t\t\t\t// Stale timer (was aborted).\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tdelete(net.timeoutTimers, timeout)\n\t\t\tprestate := timeout.node.state\n\t\t\tstatus := \"ok\"\n\t\t\tif err := net.handle(timeout.node, timeout.ev, nil); err != nil {\n\t\t\t\tstatus = err.Error()\n\t\t\t}\n\t\t\tlog.Debug(\"\", \"msg\", net.tab.count, timeout.ev, timeout.node.ID[:8], timeout.node.addr(), prestate, timeout.node.state, status)\n\n\t\t// Querying.\n\t\tcase q := <-net.queryReq:\n\t\t\tlog.Debug(\"<-net.queryReq\")\n\t\t\tif !q.start(net) {\n\t\t\t\tq.remote.deferQuery(q)\n\t\t\t}\n\n\t\t// Interacting with the table.\n\t\tcase f := <-net.tableOpReq:\n\t\t\tlog.Debug(\"<-net.tableOpReq\")\n\t\t\tf()\n\t\t\tnet.tableOpResp <- struct{}{}\n\n\t\t// Topic registration stuff.\n\t\tcase req := <-net.topicRegisterReq:\n\t\t\tlog.Debug(\"<-net.topicRegisterReq\")\n\t\t\tif !req.add {\n\t\t\t\tnet.ticketStore.removeRegisterTopic(req.topic)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tnet.ticketStore.addTopic(req.topic, true)\n\t\t\t// If we're currently waiting idle (nothing to look up), give the ticket store a\n\t\t\t// chance to start it sooner. This should speed up convergence of the radius\n\t\t\t// determination for new topics.\n\t\t\t// if topicRegisterLookupDone == nil {\n\t\t\tif topicRegisterLookupTarget.target == (common.Hash{}) {\n\t\t\t\tlog.Debug(\"topicRegisterLookupTarget == null\")\n\t\t\t\tif topicRegisterLookupTick.Stop() {\n\t\t\t\t\t<-topicRegisterLookupTick.C\n\t\t\t\t}\n\t\t\t\ttarget, delay := net.ticketStore.nextRegisterLookup()\n\t\t\t\ttopicRegisterLookupTarget = target\n\t\t\t\ttopicRegisterLookupTick.Reset(delay)\n\t\t\t}\n\n\t\tcase nodes := <-topicRegisterLookupDone:\n\t\t\tlog.Debug(\"<-topicRegisterLookupDone\")\n\t\t\tnet.ticketStore.registerLookupDone(topicRegisterLookupTarget, nodes, func(n *Node) []byte {\n\t\t\t\tnet.ping(n, n.addr())\n\t\t\t\treturn n.pingEcho\n\t\t\t})\n\t\t\ttarget, delay := net.ticketStore.nextRegisterLookup()\n\t\t\ttopicRegisterLookupTarget = target\n\t\t\ttopicRegisterLookupTick.Reset(delay)\n\t\t\ttopicRegisterLookupDone = nil\n\n\t\tcase <-topicRegisterLookupTick.C:\n\t\t\tlog.Debug(\"<-topicRegisterLookupTick\")\n\t\t\tif (topicRegisterLookupTarget.target == common.Hash{}) {\n\t\t\t\ttarget, delay := net.ticketStore.nextRegisterLookup()\n\t\t\t\ttopicRegisterLookupTarget = target\n\t\t\t\ttopicRegisterLookupTick.Reset(delay)\n\t\t\t\ttopicRegisterLookupDone = nil\n\t\t\t} else {\n\t\t\t\ttopicRegisterLookupDone = make(chan []*Node)\n\t\t\t\ttarget := topicRegisterLookupTarget.target\n\t\t\t\tgo func() { topicRegisterLookupDone <- net.lookup(target, false) }()\n\t\t\t}\n\n\t\tcase <-nextRegisterTime:\n\t\t\tlog.Debug(\"<-nextRegisterTime\")\n\t\t\tnet.ticketStore.ticketRegistered(*nextTicket)\n\t\t\t//fmt.Println(\"sendTopicRegister\", nextTicket.t.node.addr().String(), nextTicket.t.topics, nextTicket.idx, nextTicket.t.pong)\n\t\t\tnet.conn.sendTopicRegister(nextTicket.t.node, nextTicket.t.topics, nextTicket.idx, nextTicket.t.pong)\n\n\t\tcase req := <-net.topicSearchReq:\n\t\t\tif refreshDone == nil {\n\t\t\t\tlog.Debug(\"<-net.topicSearchReq\")\n\t\t\t\tinfo, ok := searchInfo[req.topic]\n\t\t\t\tif ok {\n\t\t\t\t\tif req.delay == time.Duration(0) {\n\t\t\t\t\t\tdelete(searchInfo, req.topic)\n\t\t\t\t\t\tnet.ticketStore.removeSearchTopic(req.topic)\n\t\t\t\t\t} else {\n\t\t\t\t\t\tinfo.period = req.delay\n\t\t\t\t\t\tsearchInfo[req.topic] = info\n\t\t\t\t\t}\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tif req.delay != time.Duration(0) {\n\t\t\t\t\tvar info topicSearchInfo\n\t\t\t\t\tinfo.period = req.delay\n\t\t\t\t\tinfo.lookupChn = req.lookup\n\t\t\t\t\tsearchInfo[req.topic] = info\n\t\t\t\t\tnet.ticketStore.addSearchTopic(req.topic, req.found)\n\t\t\t\t\ttopicSearch <- req.topic\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tsearchReqWhenRefreshDone = append(searchReqWhenRefreshDone, req)\n\t\t\t}\n\n\t\tcase topic := <-topicSearch:\n\t\t\tif activeSearchCount < maxSearchCount {\n\t\t\t\tactiveSearchCount++\n\t\t\t\ttarget := net.ticketStore.nextSearchLookup(topic)\n\t\t\t\tgo func() {\n\t\t\t\t\tnodes := net.lookup(target.target, false)\n\t\t\t\t\ttopicSearchLookupDone <- topicSearchResult{target: target, nodes: nodes}\n\t\t\t\t}()\n\t\t\t}\n\t\t\tperiod := searchInfo[topic].period\n\t\t\tif period != time.Duration(0) {\n\t\t\t\tgo func() {\n\t\t\t\t\ttime.Sleep(period)\n\t\t\t\t\ttopicSearch <- topic\n\t\t\t\t}()\n\t\t\t}\n\n\t\tcase res := <-topicSearchLookupDone:\n\t\t\tactiveSearchCount--\n\t\t\tif lookupChn := searchInfo[res.target.topic].lookupChn; lookupChn != nil {\n\t\t\t\tlookupChn <- net.ticketStore.radius[res.target.topic].converged\n\t\t\t}\n\t\t\tnet.ticketStore.searchLookupDone(res.target, res.nodes, func(n *Node, topic Topic) []byte {\n\t\t\t\tif n.state != nil && n.state.canQuery {\n\t\t\t\t\treturn net.conn.send(n, topicQueryPacket, topicQuery{Topic: topic}) // TODO: set expiration\n\t\t\t\t} else {\n\t\t\t\t\tif n.state == unknown {\n\t\t\t\t\t\tnet.ping(n, n.addr())\n\t\t\t\t\t}\n\t\t\t\t\treturn nil\n\t\t\t\t}\n\t\t\t})\n\n\t\tcase <-statsDump.C:\n\t\t\tlog.Debug(\"<-statsDump.C\")\n\t\t\t/*r, ok := net.ticketStore.radius[testTopic]\n\t\t\tif !ok {\n\t\t\t\tfmt.Printf(\"(%x) no radius @ %v\\n\", net.tab.self.ID[:8], time.Now())\n\t\t\t} else {\n\t\t\t\ttopics := len(net.ticketStore.tickets)\n\t\t\t\ttickets := len(net.ticketStore.nodes)\n\t\t\t\trad := r.radius / (maxRadius/10000+1)\n\t\t\t\tfmt.Printf(\"(%x) topics:%d radius:%d tickets:%d @ %v\\n\", net.tab.self.ID[:8], topics, rad, tickets, time.Now())\n\t\t\t}*/\n\n\t\t\ttm := Now()\n\t\t\tfor topic, r := range net.ticketStore.radius {\n\t\t\t\tif printTestImgLogs {\n\t\t\t\t\trad := r.radius / (maxRadius/1000000 + 1)\n\t\t\t\t\tminrad := r.minRadius / (maxRadius/1000000 + 1)\n\t\t\t\t\tfmt.Printf(\"*R %d %v %016x %v\\n\", tm/1000000, topic, net.tab.self.sha[:8], rad)\n\t\t\t\t\tfmt.Printf(\"*MR %d %v %016x %v\\n\", tm/1000000, topic, net.tab.self.sha[:8], minrad)\n\t\t\t\t}\n\t\t\t}\n\t\t\tfor topic, t := range net.topictab.topics {\n\t\t\t\twp := t.wcl.nextWaitPeriod(tm)\n\t\t\t\tif printTestImgLogs {\n\t\t\t\t\tfmt.Printf(\"*W %d %v %016x %d\\n\", tm/1000000, topic, net.tab.self.sha[:8], wp/1000000)\n\t\t\t\t}\n\t\t\t}\n\n\t\t// Periodic / lookup-initiated bucket refresh.\n\t\tcase <-refreshTimer.C:\n\t\t\tlog.Debug(\"<-refreshTimer.C\")\n\t\t\t// TODO: ideally we would start the refresh timer after\n\t\t\t// fallback nodes have been set for the first time.\n\t\t\tif refreshDone == nil {\n\t\t\t\trefreshDone = make(chan struct{})\n\t\t\t\tnet.refresh(refreshDone)\n\t\t\t}\n\t\tcase <-bucketRefreshTimer.C:\n\t\t\ttarget := net.tab.chooseBucketRefreshTarget()\n\t\t\tgo func() {\n\t\t\t\tnet.lookup(target, false)\n\t\t\t\tbucketRefreshTimer.Reset(bucketRefreshInterval)\n\t\t\t}()\n\t\tcase newNursery := <-net.refreshReq:\n\t\t\tlog.Debug(\"<-net.refreshReq\")\n\t\t\tif newNursery != nil {\n\t\t\t\tnet.nursery = newNursery\n\t\t\t}\n\t\t\tif refreshDone == nil {\n\t\t\t\trefreshDone = make(chan struct{})\n\t\t\t\tnet.refresh(refreshDone)\n\t\t\t}\n\t\t\tnet.refreshResp <- refreshDone\n\t\tcase <-refreshDone:\n\t\t\tlog.Debug(\"<-net.refreshDone\", \"table size\", net.tab.count)\n\t\t\tif net.tab.count != 0 {\n\t\t\t\trefreshDone = nil\n\t\t\t\tlist := searchReqWhenRefreshDone\n\t\t\t\tsearchReqWhenRefreshDone = nil\n\t\t\t\tgo func() {\n\t\t\t\t\tfor _, req := range list {\n\t\t\t\t\t\tnet.topicSearchReq <- req\n\t\t\t\t\t}\n\t\t\t\t}()\n\t\t\t} else {\n\t\t\t\trefreshDone = make(chan struct{})\n\t\t\t\tnet.refresh(refreshDone)\n\t\t\t}\n\t\t}\n\t}\n\tlog.Debug(\"loop stopped\")\n\n\tlog.Debug(fmt.Sprintf(\"shutting down\"))\n\tif net.conn != nil {\n\t\tnet.conn.Close()\n\t}\n\tif refreshDone != nil {\n\t\t// TODO: wait for pending refresh.\n\t\t//<-refreshResults\n\t}\n\t// Cancel all pending timeouts.\n\tfor _, timer := range net.timeoutTimers {\n\t\ttimer.Stop()\n\t}\n\tif net.db != nil {\n\t\tnet.db.close()\n\t}\n\tclose(net.closed)\n}\n\n// Everything below runs on the Network.loop goroutine\n// and can modify Node, Table and Network at any time without locking.\n\nfunc (net *Network) refresh(done chan<- struct{}) {\n\tvar seeds []*Node\n\tif net.db != nil {\n\t\tseeds = net.db.querySeeds(seedCount, seedMaxAge)\n\t}\n\tif len(seeds) == 0 {\n\t\tseeds = net.nursery\n\t}\n\tif len(seeds) == 0 {\n\t\tlog.Debug(\"no seed nodes found\")\n\t\tclose(done)\n\t\treturn\n\t}\n\tfor _, n := range seeds {\n\t\tn = net.internNodeFromDB(n)\n\t\tif n.state == unknown {\n\t\t\tnet.transition(n, verifyinit)\n\t\t}\n\t\t// Force-add the seed node so Lookup does something.\n\t\t// It will be deleted again if verification fails.\n\t\tnet.tab.add(n)\n\t}\n\t// Start self lookup to fill up the buckets.\n\tgo func() {\n\t\tnet.Lookup(net.tab.self.ID)\n\t\tclose(done)\n\t}()\n}\n\n// Node Interning.\n\nfunc (net *Network) internNode(pkt *ingressPacket) *Node {\n\tif n := net.nodes[pkt.remoteID]; n != nil {\n\t\tn.IP = pkt.remoteAddr.IP\n\t\tn.UDP = uint16(pkt.remoteAddr.Port)\n\t\tn.TCP = uint16(pkt.remoteAddr.Port)\n\t\treturn n\n\t}\n\tn := NewNode(pkt.remoteID, pkt.remoteAddr.IP, uint16(pkt.remoteAddr.Port), uint16(pkt.remoteAddr.Port))\n\tn.state = unknown\n\tnet.nodes[pkt.remoteID] = n\n\treturn n\n}\n\nfunc (net *Network) internNodeFromDB(dbn *Node) *Node {\n\tif n := net.nodes[dbn.ID]; n != nil {\n\t\treturn n\n\t}\n\tn := NewNode(dbn.ID, dbn.IP, dbn.UDP, dbn.TCP)\n\tn.state = unknown\n\tnet.nodes[n.ID] = n\n\treturn n\n}\n\nfunc (net *Network) internNodeFromNeighbours(sender *net.UDPAddr, rn rpcNode) (n *Node, err error) {\n\tif rn.ID == net.tab.self.ID {\n\t\treturn nil, errors.New(\"is self\")\n\t}\n\tif rn.UDP <= lowPort {\n\t\treturn nil, errors.New(\"low port\")\n\t}\n\tn = net.nodes[rn.ID]\n\tif n == nil {\n\t\t// We haven't seen this node before.\n\t\tn, err = nodeFromRPC(sender, rn)\n\t\tif net.netrestrict != nil && !net.netrestrict.Contains(n.IP) {\n\t\t\treturn n, errors.New(\"not contained in netrestrict whitelist\")\n\t\t}\n\t\tif err == nil {\n\t\t\tn.state = unknown\n\t\t\tnet.nodes[n.ID] = n\n\t\t}\n\t\treturn n, err\n\t}\n\tif !n.IP.Equal(rn.IP) || n.UDP != rn.UDP || n.TCP != rn.TCP {\n\t\tif n.state == known {\n\t\t\t// reject address change if node is known by us\n\t\t\terr = fmt.Errorf(\"metadata mismatch: got %v, want %v\", rn, n)\n\t\t} else {\n\t\t\t// accept otherwise; this will be handled nicer with signed ENRs\n\t\t\tn.IP = rn.IP\n\t\t\tn.UDP = rn.UDP\n\t\t\tn.TCP = rn.TCP\n\t\t}\n\t}\n\treturn n, err\n}\n\n// nodeNetGuts is embedded in Node and contains fields.\ntype nodeNetGuts struct {\n\t// This is a cached copy of sha3(ID) which is used for node\n\t// distance calculations. This is part of Node in order to make it\n\t// possible to write tests that need a node at a certain distance.\n\t// In those tests, the content of sha will not actually correspond\n\t// with ID.\n\tsha common.Hash\n\n\t// State machine fields. Access to these fields\n\t// is restricted to the Network.loop goroutine.\n\tstate             *nodeState\n\tpingEcho          []byte           // hash of last ping sent by us\n\tpingTopics        []Topic          // topic set sent by us in last ping\n\tdeferredQueries   []*findnodeQuery // queries that can't be sent yet\n\tpendingNeighbours *findnodeQuery   // current query, waiting for reply\n\tqueryTimeouts     int\n}\n\nfunc (n *nodeNetGuts) deferQuery(q *findnodeQuery) {\n\tn.deferredQueries = append(n.deferredQueries, q)\n}\n\nfunc (n *nodeNetGuts) startNextQuery(net *Network) {\n\tif len(n.deferredQueries) == 0 {\n\t\treturn\n\t}\n\tnextq := n.deferredQueries[0]\n\tif nextq.start(net) {\n\t\tn.deferredQueries = append(n.deferredQueries[:0], n.deferredQueries[1:]...)\n\t}\n}\n\nfunc (q *findnodeQuery) start(net *Network) bool {\n\t// Satisfy queries against the local node directly.\n\tif q.remote == net.tab.self {\n\t\tlog.Debug(\"findnodeQuery self\")\n\t\tclosest := net.tab.closest(common.BytesToHash(q.target[:]), bucketSize)\n\n\t\tq.reply <- closest.entries\n\t\treturn true\n\t}\n\tif q.remote.state.canQuery && q.remote.pendingNeighbours == nil {\n\t\tlog.Debug(\"findnodeQuery\", \"remote peer:\", q.remote.ID, \"targetID:\", q.target)\n\t\tnet.conn.sendFindnodeHash(q.remote, q.target)\n\t\tnet.timedEvent(respTimeout, q.remote, neighboursTimeout)\n\t\tq.remote.pendingNeighbours = q\n\t\treturn true\n\t}\n\t// If the node is not known yet, it won't accept queries.\n\t// Initiate the transition to known.\n\t// The request will be sent later when the node reaches known state.\n\tif q.remote.state == unknown {\n\t\tlog.Debug(\"findnodeQuery\", \"id:\", q.remote.ID, \"status:\", \"unknown->verifyinit\")\n\t\tnet.transition(q.remote, verifyinit)\n\t}\n\treturn false\n}\n\n// Node Events (the input to the state machine).\n\ntype nodeEvent uint\n\n//go:generate stringer -type=nodeEvent\n\nconst (\n\tinvalidEvent nodeEvent = iota // zero is reserved\n\n\t// Packet type events.\n\t// These correspond to packet types in the UDP protocol.\n\tpingPacket\n\tpongPacket\n\tfindnodePacket\n\tneighborsPacket\n\tfindnodeHashPacket\n\ttopicRegisterPacket\n\ttopicQueryPacket\n\ttopicNodesPacket\n\n\t// Non-packet events.\n\t// Event values in this category are allocated outside\n\t// the packet type range (packet types are encoded as a single byte).\n\tpongTimeout nodeEvent = iota + 256\n\tpingTimeout\n\tneighboursTimeout\n)\n\n// Node State Machine.\n\ntype nodeState struct {\n\tname     string\n\thandle   func(*Network, *Node, nodeEvent, *ingressPacket) (next *nodeState, err error)\n\tenter    func(*Network, *Node)\n\tcanQuery bool\n}\n\nfunc (s *nodeState) String() string {\n\treturn s.name\n}\n\nvar (\n\tunknown          *nodeState\n\tverifyinit       *nodeState\n\tverifywait       *nodeState\n\tremoteverifywait *nodeState\n\tknown            *nodeState\n\tcontested        *nodeState\n\tunresponsive     *nodeState\n)\n\nfunc init() {\n\tunknown = &nodeState{\n\t\tname: \"unknown\",\n\t\tenter: func(net *Network, n *Node) {\n\t\t\tnet.tab.delete(n)\n\t\t\tn.pingEcho = nil\n\t\t\t// Abort active queries.\n\t\t\tfor _, q := range n.deferredQueries {\n\t\t\t\tq.reply <- nil\n\t\t\t}\n\t\t\tn.deferredQueries = nil\n\t\t\tif n.pendingNeighbours != nil {\n\t\t\t\tn.pendingNeighbours.reply <- nil\n\t\t\t\tn.pendingNeighbours = nil\n\t\t\t}\n\t\t\tn.queryTimeouts = 0\n\t\t},\n\t\thandle: func(net *Network, n *Node, ev nodeEvent, pkt *ingressPacket) (*nodeState, error) {\n\t\t\tswitch ev {\n\t\t\tcase pingPacket:\n\t\t\t\tnet.handlePing(n, pkt)\n\t\t\t\tnet.ping(n, pkt.remoteAddr)\n\t\t\t\treturn verifywait, nil\n\t\t\tdefault:\n\t\t\t\treturn unknown, errInvalidEvent\n\t\t\t}\n\t\t},\n\t}\n\n\tverifyinit = &nodeState{\n\t\tname: \"verifyinit\",\n\t\tenter: func(net *Network, n *Node) {\n\t\t\tnet.ping(n, n.addr())\n\t\t},\n\t\thandle: func(net *Network, n *Node, ev nodeEvent, pkt *ingressPacket) (*nodeState, error) {\n\t\t\tswitch ev {\n\t\t\tcase pingPacket:\n\t\t\t\tnet.handlePing(n, pkt)\n\t\t\t\treturn verifywait, nil\n\t\t\tcase pongPacket:\n\t\t\t\terr := net.handleKnownPong(n, pkt)\n\t\t\t\treturn remoteverifywait, err\n\t\t\tcase pongTimeout:\n\t\t\t\treturn unknown, nil\n\t\t\tdefault:\n\t\t\t\treturn verifyinit, errInvalidEvent\n\t\t\t}\n\t\t},\n\t}\n\n\tverifywait = &nodeState{\n\t\tname: \"verifywait\",\n\t\thandle: func(net *Network, n *Node, ev nodeEvent, pkt *ingressPacket) (*nodeState, error) {\n\t\t\tswitch ev {\n\t\t\tcase pingPacket:\n\t\t\t\tnet.handlePing(n, pkt)\n\t\t\t\treturn verifywait, nil\n\t\t\tcase pongPacket:\n\t\t\t\terr := net.handleKnownPong(n, pkt)\n\t\t\t\treturn known, err\n\t\t\tcase pongTimeout:\n\t\t\t\treturn unknown, nil\n\t\t\tdefault:\n\t\t\t\treturn verifywait, errInvalidEvent\n\t\t\t}\n\t\t},\n\t}\n\n\tremoteverifywait = &nodeState{\n\t\tname: \"remoteverifywait\",\n\t\tenter: func(net *Network, n *Node) {\n\t\t\tnet.timedEvent(respTimeout, n, pingTimeout)\n\t\t},\n\t\thandle: func(net *Network, n *Node, ev nodeEvent, pkt *ingressPacket) (*nodeState, error) {\n\t\t\tswitch ev {\n\t\t\tcase pingPacket:\n\t\t\t\tnet.handlePing(n, pkt)\n\t\t\t\treturn remoteverifywait, nil\n\t\t\tcase pingTimeout:\n\t\t\t\treturn known, nil\n\t\t\tdefault:\n\t\t\t\treturn remoteverifywait, errInvalidEvent\n\t\t\t}\n\t\t},\n\t}\n\n\tknown = &nodeState{\n\t\tname:     \"known\",\n\t\tcanQuery: true,\n\t\tenter: func(net *Network, n *Node) {\n\t\t\tn.queryTimeouts = 0\n\t\t\tn.startNextQuery(net)\n\t\t\t// Insert into the table and start revalidation of the last node\n\t\t\t// in the bucket if it is full.\n\t\t\tlast := net.tab.add(n)\n\t\t\tif last != nil && last.state == known {\n\t\t\t\t// TODO: do this asynchronously\n\t\t\t\tnet.transition(last, contested)\n\t\t\t}\n\t\t},\n\t\thandle: func(net *Network, n *Node, ev nodeEvent, pkt *ingressPacket) (*nodeState, error) {\n\t\t\tswitch ev {\n\t\t\tcase pingPacket:\n\t\t\t\tnet.handlePing(n, pkt)\n\t\t\t\treturn known, nil\n\t\t\tcase pongPacket:\n\t\t\t\terr := net.handleKnownPong(n, pkt)\n\t\t\t\treturn known, err\n\t\t\tdefault:\n\t\t\t\treturn net.handleQueryEvent(n, ev, pkt)\n\t\t\t}\n\t\t},\n\t}\n\n\tcontested = &nodeState{\n\t\tname:     \"contested\",\n\t\tcanQuery: true,\n\t\tenter: func(net *Network, n *Node) {\n\t\t\tn.pingEcho = nil\n\t\t\tnet.ping(n, n.addr())\n\t\t},\n\t\thandle: func(net *Network, n *Node, ev nodeEvent, pkt *ingressPacket) (*nodeState, error) {\n\t\t\tswitch ev {\n\t\t\tcase pongPacket:\n\t\t\t\t// Node is still alive.\n\t\t\t\terr := net.handleKnownPong(n, pkt)\n\t\t\t\treturn known, err\n\t\t\tcase pongTimeout:\n\t\t\t\tnet.tab.deleteReplace(n)\n\t\t\t\treturn unresponsive, nil\n\t\t\tcase pingPacket:\n\t\t\t\tnet.handlePing(n, pkt)\n\t\t\t\treturn contested, nil\n\t\t\tdefault:\n\t\t\t\treturn net.handleQueryEvent(n, ev, pkt)\n\t\t\t}\n\t\t},\n\t}\n\n\tunresponsive = &nodeState{\n\t\tname:     \"unresponsive\",\n\t\tcanQuery: true,\n\t\thandle: func(net *Network, n *Node, ev nodeEvent, pkt *ingressPacket) (*nodeState, error) {\n\t\t\tswitch ev {\n\t\t\tcase pingPacket:\n\t\t\t\tnet.handlePing(n, pkt)\n\t\t\t\treturn known, nil\n\t\t\tcase pongPacket:\n\t\t\t\terr := net.handleKnownPong(n, pkt)\n\t\t\t\treturn known, err\n\t\t\tdefault:\n\t\t\t\treturn net.handleQueryEvent(n, ev, pkt)\n\t\t\t}\n\t\t},\n\t}\n}\n\n// handle processes packets sent by n and events related to n.\nfunc (net *Network) handle(n *Node, ev nodeEvent, pkt *ingressPacket) error {\n\t//fmt.Println(\"handle\", n.addr().String(), n.state, ev)\n\tif pkt != nil {\n\t\tif err := net.checkPacket(n, ev, pkt); err != nil {\n\t\t\t//fmt.Println(\"check err:\", err)\n\t\t\treturn err\n\t\t}\n\t\t// Start the background expiration goroutine after the first\n\t\t// successful communication. Subsequent calls have no effect if it\n\t\t// is already running. We do this here instead of somewhere else\n\t\t// so that the search for seed nodes also considers older nodes\n\t\t// that would otherwise be removed by the expirer.\n\t\tif net.db != nil {\n\t\t\tnet.db.ensureExpirer()\n\t\t}\n\t}\n\tif n.state == nil {\n\t\tn.state = unknown //???\n\t}\n\tnext, err := n.state.handle(net, n, ev, pkt)\n\tnet.transition(n, next)\n\t//fmt.Println(\"new state:\", n.state)\n\treturn err\n}\n\nfunc (net *Network) checkPacket(n *Node, ev nodeEvent, pkt *ingressPacket) error {\n\t// Replay prevention checks.\n\tswitch ev {\n\tcase pingPacket, findnodeHashPacket, neighborsPacket:\n\t\t// TODO: check date is > last date seen\n\t\t// TODO: check ping version\n\tcase pongPacket:\n\t\tif !bytes.Equal(pkt.data.(*pong).ReplyTok, n.pingEcho) {\n\t\t\t// fmt.Println(\"pong reply token mismatch\")\n\t\t\treturn fmt.Errorf(\"pong reply token mismatch\")\n\t\t}\n\t\tn.pingEcho = nil\n\t}\n\t// Address validation.\n\t// TODO: Ideally we would do the following:\n\t//  - reject all packets with wrong address except ping.\n\t//  - for ping with new address, transition to verifywait but keep the\n\t//    previous node (with old address) around. if the new one reaches known,\n\t//    swap it out.\n\treturn nil\n}\n\nfunc (net *Network) transition(n *Node, next *nodeState) {\n\tif n.state != next {\n\t\tn.state = next\n\t\tif next.enter != nil {\n\t\t\tnext.enter(net, n)\n\t\t}\n\t}\n\n\t// TODO: persist/unpersist node\n}\n\nfunc (net *Network) timedEvent(d time.Duration, n *Node, ev nodeEvent) {\n\ttimeout := timeoutEvent{ev, n}\n\tnet.timeoutTimers[timeout] = time.AfterFunc(d, func() {\n\t\tselect {\n\t\tcase net.timeout <- timeout:\n\t\tcase <-net.closed:\n\t\t}\n\t})\n}\n\nfunc (net *Network) abortTimedEvent(n *Node, ev nodeEvent) {\n\ttimer := net.timeoutTimers[timeoutEvent{ev, n}]\n\tif timer != nil {\n\t\ttimer.Stop()\n\t\tdelete(net.timeoutTimers, timeoutEvent{ev, n})\n\t}\n}\n\nfunc (net *Network) ping(n *Node, addr *net.UDPAddr) {\n\t//fmt.Println(\"ping\", n.addr().String(), n.ID.String(), n.sha.Hex())\n\tif n.pingEcho != nil || n.ID == net.tab.self.ID {\n\t\t//fmt.Println(\" not sent\")\n\t\treturn\n\t}\n\tlog.Debug(\"Pinging remote node\", \"node\", n.ID)\n\tn.pingTopics = net.ticketStore.regTopicSet()\n\tn.pingEcho = net.conn.sendPing(n, addr, n.pingTopics)\n\tnet.timedEvent(respTimeout, n, pongTimeout)\n}\n\nfunc (net *Network) handlePing(n *Node, pkt *ingressPacket) {\n\tlog.Debug(\"Handling remote ping\", \"node\", n.ID)\n\tping := pkt.data.(*ping)\n\tn.TCP = ping.From.TCP\n\tt := net.topictab.getTicket(n, ping.Topics)\n\n\tpong := &pong{\n\t\tTo:         makeEndpoint(n.addr(), n.TCP), // TODO: maybe use known TCP port from DB\n\t\tReplyTok:   pkt.hash,\n\t\tExpiration: uint64(time.Now().Add(expiration).Unix()),\n\t}\n\tticketToPong(t, pong)\n\tnet.conn.send(n, pongPacket, pong)\n}\n\nfunc (net *Network) handleKnownPong(n *Node, pkt *ingressPacket) error {\n\tlog.Debug(\"Handling known pong\", \"node\", n.ID)\n\tnet.abortTimedEvent(n, pongTimeout)\n\tnow := Now()\n\tticket, err := pongToTicket(now, n.pingTopics, n, pkt)\n\tif err == nil {\n\t\t// fmt.Printf(\"(%x) ticket: %+v\\n\", net.tab.self.ID[:8], pkt.data)\n\t\tnet.ticketStore.addTicket(now, pkt.data.(*pong).ReplyTok, ticket)\n\t} else {\n\t\tlog.Debug(\"Failed to convert pong to ticket\", \"err\", err)\n\t}\n\tn.pingEcho = nil\n\tn.pingTopics = nil\n\treturn err\n}\n\nfunc (net *Network) handleQueryEvent(n *Node, ev nodeEvent, pkt *ingressPacket) (*nodeState, error) {\n\tswitch ev {\n\tcase findnodePacket:\n\t\ttarget := common.BytesToHash(pkt.data.(*findnode).Target[:])\n\t\tresults := net.tab.closest(target, bucketSize).entries\n\t\tnet.conn.sendNeighbours(n, results)\n\t\treturn n.state, nil\n\tcase neighborsPacket:\n\t\terr := net.handleNeighboursPacket(n, pkt)\n\t\treturn n.state, err\n\tcase neighboursTimeout:\n\t\tif n.pendingNeighbours != nil {\n\t\t\tn.pendingNeighbours.reply <- nil\n\t\t\tn.pendingNeighbours = nil\n\t\t}\n\t\tn.queryTimeouts++\n\t\tif n.queryTimeouts > maxFindnodeFailures && n.state == known {\n\t\t\treturn contested, errors.New(\"too many timeouts\")\n\t\t}\n\t\treturn n.state, nil\n\n\t// v5\n\n\tcase findnodeHashPacket:\n\t\tresults := net.tab.closest(pkt.data.(*findnodeHash).Target, bucketSize).entries\n\t\tnet.conn.sendNeighbours(n, results)\n\t\treturn n.state, nil\n\tcase topicRegisterPacket:\n\t\t//fmt.Println(\"got topicRegisterPacket\")\n\t\tregdata := pkt.data.(*topicRegister)\n\t\tpong, err := net.checkTopicRegister(regdata)\n\t\tif err != nil {\n\t\t\t//fmt.Println(err)\n\t\t\treturn n.state, fmt.Errorf(\"bad waiting ticket: %v\", err)\n\t\t}\n\t\tnet.topictab.useTicket(n, pong.TicketSerial, regdata.Topics, int(regdata.Idx), pong.Expiration, pong.WaitPeriods)\n\t\treturn n.state, nil\n\tcase topicQueryPacket:\n\t\t// TODO: handle expiration\n\t\ttopic := pkt.data.(*topicQuery).Topic\n\t\tresults := net.topictab.getEntries(topic)\n\t\tif _, ok := net.ticketStore.tickets[topic]; ok {\n\t\t\tresults = append(results, net.tab.self) // we're not registering in our own table but if we're advertising, return ourselves too\n\t\t}\n\t\tif len(results) > 10 {\n\t\t\tresults = results[:10]\n\t\t}\n\t\tvar hash common.Hash\n\t\tcopy(hash[:], pkt.hash)\n\t\tnet.conn.sendTopicNodes(n, hash, results)\n\t\treturn n.state, nil\n\tcase topicNodesPacket:\n\t\tp := pkt.data.(*topicNodes)\n\t\tif net.ticketStore.gotTopicNodes(n, p.Echo, p.Nodes) {\n\t\t\tn.queryTimeouts++\n\t\t\tif n.queryTimeouts > maxFindnodeFailures && n.state == known {\n\t\t\t\treturn contested, errors.New(\"too many timeouts\")\n\t\t\t}\n\t\t}\n\t\treturn n.state, nil\n\n\tdefault:\n\t\treturn n.state, errInvalidEvent\n\t}\n}\n\nfunc (net *Network) checkTopicRegister(data *topicRegister) (*pong, error) {\n\tvar pongpkt ingressPacket\n\tif err := decodePacket(data.Pong, &pongpkt); err != nil {\n\t\treturn nil, err\n\t}\n\tif pongpkt.ev != pongPacket {\n\t\treturn nil, errors.New(\"is not pong packet\")\n\t}\n\tif pongpkt.remoteID != net.tab.self.ID {\n\t\treturn nil, errors.New(\"not signed by us\")\n\t}\n\t// check that we previously authorised all topics\n\t// that the other side is trying to register.\n\thash, _, _ := wireHash(data.Topics)\n\tif hash != pongpkt.data.(*pong).TopicHash {\n\t\treturn nil, errors.New(\"topic hash mismatch\")\n\t}\n\tif int(data.Idx) < 0 || int(data.Idx) >= len(data.Topics) {\n\t\treturn nil, errors.New(\"topic index out of range\")\n\t}\n\treturn pongpkt.data.(*pong), nil\n}\n\nfunc wireHash(x interface{}) (h common.Hash, n int, err error) {\n\thw := sha3.New256()\n\twire.WriteBinary(x, hw, &n, &err)\n\thw.Sum(h[:0])\n\treturn h, n, err\n}\n\nfunc (net *Network) handleNeighboursPacket(n *Node, pkt *ingressPacket) error {\n\tif n.pendingNeighbours == nil {\n\t\treturn errNoQuery\n\t}\n\tnet.abortTimedEvent(n, neighboursTimeout)\n\n\treq := pkt.data.(*neighbors)\n\tnodes := make([]*Node, len(req.Nodes))\n\tfor i, rn := range req.Nodes {\n\t\tnn, err := net.internNodeFromNeighbours(pkt.remoteAddr, rn)\n\t\tif err != nil {\n\t\t\tlog.Debug(fmt.Sprintf(\"invalid neighbour (%v) from %x@%v: %v\", rn.IP, n.ID[:8], pkt.remoteAddr, err))\n\t\t\tcontinue\n\t\t}\n\t\tnodes[i] = nn\n\t\t// Start validation of query results immediately.\n\t\t// This fills the table quickly.\n\t\t// TODO: generates way too many packets, maybe do it via queue.\n\t\tif nn.state == unknown {\n\t\t\tnet.transition(nn, verifyinit)\n\t\t}\n\t}\n\t// TODO: don't ignore second packet\n\tn.pendingNeighbours.reply <- nodes\n\tn.pendingNeighbours = nil\n\t// Now that this query is done, start the next one.\n\tn.startNextQuery(net)\n\treturn nil\n}\n"], "buggy_code_start_loc": [1224], "buggy_code_end_loc": [1225], "fixing_code_start_loc": [1224], "fixing_code_end_loc": [1225], "type": "CWE-190", "message": "In the client in Bytom before 1.0.6, checkTopicRegister in p2p/discover/net.go does not prevent negative idx values, leading to a crash.", "other": {"cve": {"id": "CVE-2018-18206", "sourceIdentifier": "cve@mitre.org", "published": "2018-10-10T09:29:00.270", "lastModified": "2018-12-28T13:46:25.700", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "In the client in Bytom before 1.0.6, checkTopicRegister in p2p/discover/net.go does not prevent negative idx values, leading to a crash."}, {"lang": "es", "value": "En el cliente en Bytom en versiones anteriores a la 1.0.6, checkTopicRegister en p2p/discover/net.go no evita los valores idx negativos, lo que conduce a un cierre inesperado."}], "metrics": {"cvssMetricV30": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.0", "vectorString": "CVSS:3.0/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 7.5, "baseSeverity": "HIGH"}, "exploitabilityScore": 3.9, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:L/Au:N/C:N/I:N/A:P", "accessVector": "NETWORK", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "PARTIAL", "baseScore": 5.0}, "baseSeverity": "MEDIUM", "exploitabilityScore": 10.0, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-190"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:bytom:bytom:*:*:*:*:*:*:*:*", "versionEndExcluding": "1.0.6", "matchCriteriaId": "6CF2D50E-A935-44CF-B434-6BB4DE0BEB27"}]}]}], "references": [{"url": "https://github.com/Bytom/bytom/commit/1ac3c8ac4f2b1e1df9675228290bda6b9586ba42", "source": "cve@mitre.org", "tags": ["Patch", "Vendor Advisory"]}]}, "github_commit_url": "https://github.com/Bytom/bytom/commit/1ac3c8ac4f2b1e1df9675228290bda6b9586ba42"}}