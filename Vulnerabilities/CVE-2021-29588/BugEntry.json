{"buggy_code": ["/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include <stddef.h>\n#include <stdint.h>\n\n#include <vector>\n\n#include \"tensorflow/lite/c/builtin_op_data.h\"\n#include \"tensorflow/lite/c/common.h\"\n#include \"tensorflow/lite/kernels/cpu_backend_context.h\"\n#include \"tensorflow/lite/kernels/internal/compatibility.h\"\n// NOLINTNEXTLINE - This header file shouldn't go to the top.\n#include \"tensorflow/lite/kernels/internal/optimized/integer_ops/transpose_conv.h\"\n#include \"tensorflow/lite/kernels/internal/optimized/optimized_ops.h\"\n// NOLINTNEXTLINE - This header file shouldn't go to the top.\n#include \"tensorflow/lite/kernels/internal/reference/integer_ops/transpose_conv.h\"\n#include \"tensorflow/lite/kernels/internal/reference/reference_ops.h\"\n#include \"tensorflow/lite/kernels/internal/tensor.h\"\n#include \"tensorflow/lite/kernels/internal/tensor_ctypes.h\"\n#include \"tensorflow/lite/kernels/internal/types.h\"\n#include \"tensorflow/lite/kernels/kernel_util.h\"\n#include \"tensorflow/lite/kernels/padding.h\"\n\nnamespace tflite {\nnamespace ops {\nnamespace builtin {\nnamespace transpose_conv {\n\n// This file has 2 implementation of TransposeConv.\nenum KernelType {\n  kReference,\n  kGenericOptimized,  // Neon-free\n};\n\nconstexpr int kOutputShapeTensor = 0;\nconstexpr int kWeightsTensor = 1;\nconstexpr int kDataInputTensor = 2;\nconstexpr int kBiasTensor = 3;\nconstexpr int kOutputTensor = 0;\n\nconst int kTensorNotAllocated = -1;\n\nstruct OpData {\n  // IDs are the arbitrary identifiers used by TF Lite to identify and access\n  // memory buffers.\n  int col2im_id = kTensorNotAllocated;\n  int transposed_weights_id = kTensorNotAllocated;\n  int scratch_tensor_id = kTensorNotAllocated;\n\n  // col2im is the temporary tensor allocated and used in optimized path for\n  // storing col2im data:gemm result for input_matrix x filter_matrix.\n  int32_t col2im_index;\n\n  // TfLiteConverter will transpose weights from HWOI to OHWI order.\n  // In optimized path, we will transpose them back to HWOI, this temporary\n  // tensor is allocated for storing transposed weights.\n  int32_t transposed_weights_index;\n\n  // Scratch tensor is used in the quantized path for storing accumulation\n  // results.\n  int32_t scratch_tensor_index;\n\n  TfLitePaddingValues padding;\n  // The scaling factor from input to output (aka the 'real multiplier') can\n  // be represented as a fixed point multiplier plus a left shift.\n  int32_t output_multiplier;\n  int output_shift;\n\n  // Per channel output multiplier and shift.\n  std::vector<int32_t> per_channel_output_multiplier;\n  std::vector<int32_t> per_channel_output_shift;\n\n  // The range of the fused activation layer. For example for kNone and\n  // uint8_t these would be 0 and 255.\n  int32_t output_activation_min;\n  int32_t output_activation_max;\n\n  bool has_col2im = false;\n  bool weights_are_transposed = false;\n};\n\nvoid* Init(TfLiteContext* context, const char* buffer, size_t length) {\n  return new OpData;\n}\n\nvoid Free(TfLiteContext* context, void* buffer) {\n  delete reinterpret_cast<OpData*>(buffer);\n}\n\nTfLiteStatus ResizeTensor(TfLiteContext* context,\n                          const TfLiteTensor* shape_tensor,\n                          TfLiteTensor* tensor_to_resize) {\n  // Currently only support int32 for output shape.\n  if (shape_tensor->type != kTfLiteInt32) {\n    TF_LITE_KERNEL_LOG(context, \"Output shape is %s, not int32.\",\n                       TfLiteTypeGetName(shape_tensor->type));\n    return kTfLiteError;\n  }\n\n  TfLiteIntArray* shape = TfLiteIntArrayCreate(NumElements(shape_tensor));\n  for (int i = 0; i < shape->size; ++i) {\n    shape->data[i] = GetTensorData<int32_t>(shape_tensor)[i];\n  }\n\n  return context->ResizeTensor(context, tensor_to_resize, shape);\n}\n\n// Allocate temporary tensors if necessary.\ntemplate <KernelType kernel_type>\nstatic TfLiteStatus AllocateTemporaryTensorsIfRequired(TfLiteContext* context,\n                                                       TfLiteType input_type,\n                                                       TfLiteType weights_type,\n                                                       TfLiteNode* node) {\n  OpData* data = reinterpret_cast<OpData*>(node->user_data);\n  int temporaries_count = 0;\n\n  // Allocate col2im tensor. Currently it's only used for optimized kernels.\n  if (kernel_type == kGenericOptimized) {\n    if (data->col2im_id == kTensorNotAllocated) {\n      context->AddTensors(context, 1, &data->col2im_id);\n    }\n    data->col2im_index = temporaries_count;\n    data->has_col2im = true;\n    ++temporaries_count;\n  }\n\n  // Allocate transposed_weights tensor. Currently it's only used for optimized\n  // float kernels.\n  if (kernel_type == kGenericOptimized) {\n    if (data->transposed_weights_id == kTensorNotAllocated) {\n      context->AddTensors(context, 1, &data->transposed_weights_id);\n    }\n    data->transposed_weights_index = temporaries_count;\n    data->weights_are_transposed = true;\n    ++temporaries_count;\n  }\n\n  // Allocate scratch buffer tensor\n  if (input_type == kTfLiteUInt8 || input_type == kTfLiteInt8 ||\n      input_type == kTfLiteInt16) {\n    if (data->scratch_tensor_id == kTensorNotAllocated) {\n      context->AddTensors(context, 1, &data->scratch_tensor_id);\n    }\n    data->scratch_tensor_index = temporaries_count;\n    ++temporaries_count;\n  }\n\n  TfLiteIntArrayFree(node->temporaries);\n  node->temporaries = TfLiteIntArrayCreate(temporaries_count);\n\n  return kTfLiteOk;\n}\n\nTfLiteStatus ResizeCol2ImTensor(TfLiteContext* context,\n                                const TfLiteTensor* output_shape,\n                                const TfLiteTensor* weights,\n                                const TfLiteTensor* input,\n                                TfLiteTensor* col2im) {\n  if (output_shape->type != kTfLiteInt32) {\n    TF_LITE_KERNEL_LOG(context, \"col2im shape is %s, not int32.\",\n                       TfLiteTypeGetName(output_shape->type));\n    return kTfLiteError;\n  }\n  TF_LITE_ENSURE_EQ(context, NumElements(output_shape), 4);\n  TfLiteIntArray* col2im_shape_array = TfLiteIntArrayCreate(2);\n  const RuntimeShape& input_shape = GetTensorShape(input);\n  const RuntimeShape& weights_shape = GetTensorShape(weights);\n  col2im_shape_array->data[0] = input_shape.Dims(1) * input_shape.Dims(2);\n  col2im_shape_array->data[1] =\n      weights_shape.Dims(0) * weights_shape.Dims(1) * weights_shape.Dims(2);\n\n  col2im->type = input->type == kTfLiteFloat32 ? kTfLiteFloat32 : kTfLiteInt32;\n  col2im->allocation_type = kTfLiteDynamic;\n  return context->ResizeTensor(context, col2im, col2im_shape_array);\n}\n\nTfLiteStatus ResizeAndTransposeWeights(TfLiteContext* context,\n                                       const TfLiteTensor* weights,\n                                       TfLiteTensor* transposed_weights) {\n  TfLiteIntArray* transposed_weights_shape_array = TfLiteIntArrayCreate(4);\n  const RuntimeShape& input_shape = GetTensorShape(weights);\n  transposed_weights_shape_array->data[0] = input_shape.Dims(1);\n  transposed_weights_shape_array->data[1] = input_shape.Dims(2);\n  transposed_weights_shape_array->data[2] = input_shape.Dims(0);\n  transposed_weights_shape_array->data[3] = input_shape.Dims(3);\n\n  transposed_weights->type = weights->type;\n  transposed_weights->allocation_type = kTfLiteDynamic;\n  TF_LITE_ENSURE_STATUS(context->ResizeTensor(context, transposed_weights,\n                                              transposed_weights_shape_array));\n\n  // Transpose the weights from OHWI order to HWOI order.\n  TransposeParams transpose_params;\n  transpose_params.perm_count = 4;\n  transpose_params.perm[0] = 1;\n  transpose_params.perm[1] = 2;\n  transpose_params.perm[2] = 0;\n  transpose_params.perm[3] = 3;\n\n  if (weights->type == kTfLiteFloat32) {\n    optimized_ops::Transpose(transpose_params, input_shape,\n                             GetTensorData<float>(weights),\n                             GetTensorShape(transposed_weights),\n                             GetTensorData<float>(transposed_weights));\n  } else if (weights->type == kTfLiteUInt8) {\n    optimized_ops::Transpose(transpose_params, input_shape,\n                             GetTensorData<uint8>(weights),\n                             GetTensorShape(transposed_weights),\n                             GetTensorData<uint8>(transposed_weights));\n  } else if (weights->type == kTfLiteInt8) {\n    // int16 transpose_conv also with int8 weights\n    optimized_ops::Transpose(transpose_params, input_shape,\n                             GetTensorData<int8>(weights),\n                             GetTensorShape(transposed_weights),\n                             GetTensorData<int8>(transposed_weights));\n  } else {\n    TF_LITE_KERNEL_LOG(\n        context,\n        \"Only float32, uint8, int8, int16 is supported currently, got %s.\",\n        TfLiteTypeGetName(weights->type));\n    return kTfLiteError;\n  }\n\n  return kTfLiteOk;\n}\n\ntemplate <KernelType kernel_type>\nTfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n  OpData* data = reinterpret_cast<OpData*>(node->user_data);\n\n  bool has_bias = NumInputs(node) == 4;\n\n  // Sanity checks on op\n  TF_LITE_ENSURE(context, has_bias || NumInputs(node) == 3);\n  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);\n\n  // Retrieve tensors\n  const TfLiteTensor* output_shape;\n  TF_LITE_ENSURE_OK(\n      context, GetInputSafe(context, node, kOutputShapeTensor, &output_shape));\n  const TfLiteTensor* weights;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kWeightsTensor, &weights));\n  const TfLiteTensor* input;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kDataInputTensor, &input));\n  const TfLiteTensor* bias = nullptr;\n\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context,\n                    GetOutputSafe(context, node, kOutputTensor, &output));\n\n  // Tensor sanity checks\n  TF_LITE_ENSURE_EQ(context, NumDimensions(output_shape), 1);\n  TF_LITE_ENSURE_EQ(context, NumDimensions(input), 4);\n  TF_LITE_ENSURE_EQ(context, NumDimensions(weights), 4);\n  TF_LITE_ENSURE(context,\n                 input->type == kTfLiteFloat32 || input->type == kTfLiteUInt8 ||\n                     input->type == kTfLiteInt8 || input->type == kTfLiteInt16);\n\n  if (has_bias) {\n    bias = GetOptionalInputTensor(context, node, kBiasTensor);\n    if (bias) {\n      if (input->type == kTfLiteUInt8 || input->type == kTfLiteInt8) {\n        TF_LITE_ENSURE_TYPES_EQ(context, bias->type, kTfLiteInt32);\n        if (input->type == kTfLiteInt8) {\n          TF_LITE_ENSURE_EQ(context, bias->params.zero_point, 0);\n        }\n      } else if (input->type == kTfLiteInt16) {\n        TF_LITE_ENSURE_EQ(context, bias->type, kTfLiteInt64);\n        TF_LITE_ENSURE_EQ(context, bias->params.zero_point, 0);\n      } else {\n        TF_LITE_ENSURE_TYPES_EQ(context, bias->type, input->type);\n      }\n      TF_LITE_ENSURE_EQ(context, NumElements(bias),\n                        SizeOfDimension(weights, 0));\n    }\n  }\n\n  if (input->type == kTfLiteInt16) {\n    TF_LITE_ENSURE_EQ(context, weights->type, kTfLiteInt8);\n    TF_LITE_ENSURE_EQ(context, input->params.zero_point, 0);\n    TF_LITE_ENSURE_EQ(context, output->params.zero_point, 0);\n  } else {\n    TF_LITE_ENSURE_TYPES_EQ(context, weights->type, input->type);\n  }\n  TF_LITE_ENSURE_TYPES_EQ(context, output->type, input->type);\n  // Ensure that weights and inputs have the same channel dimension.\n  // Note: TOCO will reorder weights in the following format: OHWI.\n  TF_LITE_ENSURE_EQ(context, SizeOfDimension(input, 3),\n                    SizeOfDimension(weights, 3));\n\n  // Allocate col2Im, transposed_weights & scratch Tensor.\n  TF_LITE_ENSURE_STATUS(AllocateTemporaryTensorsIfRequired<kernel_type>(\n      context, input->type, weights->type, node));\n\n  OpData* user_data = reinterpret_cast<OpData*>(node->user_data);\n  TfLiteTensor* col2im = nullptr;\n  if (data->has_col2im) {\n    node->temporaries->data[data->col2im_index] = data->col2im_id;\n    TF_LITE_ENSURE_OK(\n        context,\n        GetTemporarySafe(context, node, user_data->col2im_index, &col2im));\n  }\n\n  if (!IsConstantTensor(output_shape)) {\n    // Defer resizing until Eval().\n    SetTensorToDynamic(output);\n    if (data->has_col2im) {\n      SetTensorToDynamic(col2im);\n    }\n  } else {\n    TF_LITE_ENSURE_STATUS(ResizeTensor(context, output_shape, output));\n    if (data->has_col2im) {\n      TF_LITE_ENSURE_STATUS(\n          ResizeCol2ImTensor(context, output_shape, weights, input, col2im));\n    }\n  }\n\n  if (data->weights_are_transposed) {\n    node->temporaries->data[data->transposed_weights_index] =\n        data->transposed_weights_id;\n    TfLiteTensor* transposed_weights;\n    TF_LITE_ENSURE_OK(\n        context,\n        GetTemporarySafe(context, node, user_data->transposed_weights_index,\n                         &transposed_weights));\n    if (!IsConstantTensor(weights)) {\n      SetTensorToDynamic(transposed_weights);\n    } else {\n      ResizeAndTransposeWeights(context, weights, transposed_weights);\n    }\n  }\n\n  if (input->type == kTfLiteUInt8 || input->type == kTfLiteInt8 ||\n      input->type == kTfLiteInt16) {\n    node->temporaries->data[data->scratch_tensor_index] =\n        data->scratch_tensor_id;\n    TfLiteTensor* scratch_buffer;\n    TF_LITE_ENSURE_OK(\n        context, GetTemporarySafe(context, node, data->scratch_tensor_index,\n                                  &scratch_buffer));\n    if (input->type == kTfLiteInt16) {\n      scratch_buffer->type = kTfLiteInt64;\n    } else {\n      scratch_buffer->type = kTfLiteInt32;\n    }\n\n    scratch_buffer->allocation_type = kTfLiteDynamic;\n    if (!IsConstantTensor(output_shape)) {\n      SetTensorToDynamic(scratch_buffer);\n    } else {\n      TF_LITE_ENSURE_STATUS(\n          ResizeTensor(context, output_shape, scratch_buffer));\n    }\n\n    TF_LITE_ENSURE_EQ(context, weights->quantization.type,\n                      kTfLiteAffineQuantization);\n    const auto* affine_quantization =\n        reinterpret_cast<TfLiteAffineQuantization*>(\n            weights->quantization.params);\n    const int channels_out = weights->dims->data[0];\n    TF_LITE_ENSURE(context, affine_quantization);\n    TF_LITE_ENSURE(context, affine_quantization->scale);\n    TF_LITE_ENSURE(context, (affine_quantization->scale->size == 1 ||\n                             affine_quantization->scale->size == channels_out));\n\n    data->per_channel_output_multiplier.resize(channels_out);\n    data->per_channel_output_shift.resize(channels_out);\n    TF_LITE_ENSURE_STATUS(tflite::PopulateConvolutionQuantizationParams(\n        context, input, weights, bias, output, kTfLiteActNone,\n        &data->output_multiplier, &data->output_shift,\n        &data->output_activation_min, &data->output_activation_max,\n        data->per_channel_output_multiplier.data(),\n        data->per_channel_output_shift.data(), channels_out));\n  }\n\n  return kTfLiteOk;\n}\n\ntemplate <KernelType kernel_type>\nvoid EvalFloat(TfLiteContext* context, const TfLiteTransposeConvParams* params,\n               const OpData* data, const TfLiteTensor* input,\n               const TfLiteTensor* weights, const TfLiteTensor* bias,\n               const TfLiteTensor* transposed_weights, TfLiteTensor* col2im,\n               TfLiteTensor* output) {\n  tflite::ConvParams op_params;\n  op_params.padding_type = PaddingType::kSame;\n  op_params.padding_values.width = data->padding.width;\n  op_params.padding_values.height = data->padding.height;\n  op_params.padding_values.width_offset = data->padding.width_offset;\n  op_params.padding_values.height_offset = data->padding.height_offset;\n  op_params.stride_width = params->stride_width;\n  op_params.stride_height = params->stride_height;\n\n  switch (kernel_type) {\n    case kReference: {\n      reference_ops::TransposeConv(\n          op_params, GetTensorShape(input), GetTensorData<float>(input),\n          GetTensorShape(weights), GetTensorData<float>(weights),\n          GetTensorShape(bias), GetTensorData<float>(bias),\n          GetTensorShape(output), GetTensorData<float>(output),\n          GetTensorShape(col2im), GetTensorData<float>(col2im));\n      break;\n    }\n    case kGenericOptimized: {\n      optimized_ops::TransposeConvV2(\n          op_params, GetTensorShape(input), GetTensorData<float>(input),\n          GetTensorShape(transposed_weights),\n          GetTensorData<float>(transposed_weights), GetTensorShape(bias),\n          GetTensorData<float>(bias), GetTensorShape(output),\n          GetTensorData<float>(output), GetTensorShape(col2im),\n          GetTensorData<float>(col2im),\n          CpuBackendContext::GetFromContext(context));\n      break;\n    }\n  }\n}\n\ntemplate <KernelType kernel_type>\nvoid EvalQuantized(TfLiteContext* context,\n                   const TfLiteTransposeConvParams* params, OpData* data,\n                   const TfLiteTensor* input, const TfLiteTensor* weights,\n                   const TfLiteTensor* transposed_weights,\n                   const TfLiteTensor* bias, TfLiteTensor* col2im,\n                   TfLiteTensor* output, TfLiteTensor* scratch_buffer) {\n  int32_t input_offset = -input->params.zero_point;\n  int32_t filter_offset = -weights->params.zero_point;\n  int32_t output_offset = output->params.zero_point;\n\n  tflite::ConvParams op_params;\n  op_params.padding_type = PaddingType::kSame;\n  op_params.padding_values.width = data->padding.width;\n  op_params.padding_values.height = data->padding.height;\n  op_params.padding_values.width_offset = data->padding.width_offset;\n  op_params.padding_values.height_offset = data->padding.height_offset;\n  op_params.stride_width = params->stride_width;\n  op_params.stride_height = params->stride_height;\n  op_params.input_offset = input_offset;\n  op_params.output_offset = output_offset;\n  op_params.weights_offset = filter_offset;\n  op_params.output_multiplier = data->output_multiplier;\n  op_params.output_shift = -data->output_shift;\n  op_params.quantized_activation_min = data->output_activation_min;\n  op_params.quantized_activation_max = data->output_activation_max;\n\n  switch (kernel_type) {\n    case kReference: {\n      reference_ops::TransposeConv(\n          op_params, GetTensorShape(input), GetTensorData<uint8>(input),\n          GetTensorShape(weights), GetTensorData<uint8>(weights),\n          GetTensorShape(bias), GetTensorData<int32_t>(bias),\n          GetTensorShape(output), GetTensorData<uint8>(output),\n          GetTensorShape(col2im), GetTensorData<uint8>(col2im),\n          GetTensorData<int32_t>(scratch_buffer));\n      break;\n    }\n    case kGenericOptimized: {\n      optimized_ops::TransposeConvV2(\n          op_params, GetTensorShape(input), GetTensorData<uint8>(input),\n          GetTensorShape(transposed_weights),\n          GetTensorData<uint8>(transposed_weights), GetTensorShape(bias),\n          GetTensorData<int32>(bias), GetTensorShape(output),\n          GetTensorData<uint8>(output), GetTensorShape(col2im),\n          GetTensorData<int32>(col2im), GetTensorData<int32>(scratch_buffer),\n          CpuBackendContext::GetFromContext(context));\n      break;\n    }\n  }\n}\n\ntemplate <KernelType kernel_type>\nvoid EvalQuantizedPerChannel(\n    TfLiteContext* context, const TfLiteTransposeConvParams* params,\n    OpData* data, const TfLiteTensor* input, const TfLiteTensor* weights,\n    const TfLiteTensor* transposed_weights, const TfLiteTensor* bias,\n    TfLiteTensor* col2im, TfLiteTensor* output, TfLiteTensor* scratch_buffer) {\n  tflite::ConvParams op_params;\n  op_params.padding_type = PaddingType::kSame;\n  op_params.padding_values.width = data->padding.width;\n  op_params.padding_values.height = data->padding.height;\n  op_params.padding_values.width_offset = data->padding.width_offset;\n  op_params.padding_values.height_offset = data->padding.height_offset;\n  op_params.stride_width = params->stride_width;\n  op_params.stride_height = params->stride_height;\n  // Need to flip the sign of input offset to add it directly to the quantized\n  // buffer.\n  op_params.input_offset = -input->params.zero_point;\n  op_params.output_offset = output->params.zero_point;\n  op_params.quantized_activation_min = data->output_activation_min;\n  op_params.quantized_activation_max = data->output_activation_max;\n\n  switch (kernel_type) {\n    case kReference: {\n      reference_integer_ops::TransposeConv(\n          op_params, data->per_channel_output_multiplier.data(),\n          data->per_channel_output_shift.data(), GetTensorShape(input),\n          GetTensorData<int8>(input), GetTensorShape(weights),\n          GetTensorData<int8>(weights), GetTensorShape(bias),\n          GetTensorData<int32>(bias), GetTensorShape(output),\n          GetTensorData<int8>(output), GetTensorShape(col2im),\n          GetTensorData<int8>(col2im), GetTensorData<int32_t>(scratch_buffer));\n      break;\n    }\n    case kGenericOptimized: {\n      optimized_integer_ops::TransposeConvV2(\n          op_params, data->per_channel_output_multiplier.data(),\n          data->per_channel_output_shift.data(), GetTensorShape(input),\n          GetTensorData<int8>(input), GetTensorShape(transposed_weights),\n          GetTensorData<int8>(transposed_weights), GetTensorShape(bias),\n          GetTensorData<int32>(bias), GetTensorShape(output),\n          GetTensorData<int8>(output), GetTensorShape(col2im),\n          GetTensorData<int32>(col2im), GetTensorData<int32>(scratch_buffer),\n          CpuBackendContext::GetFromContext(context));\n      break;\n    }\n  }\n}\n\nvoid EvalQuantizedPerChannel16x8(\n    TfLiteContext* context, const TfLiteTransposeConvParams* params,\n    OpData* data, const TfLiteTensor* input, const TfLiteTensor* weights,\n    const TfLiteTensor* transposed_weights, const TfLiteTensor* bias,\n    TfLiteTensor* col2im, TfLiteTensor* output, TfLiteTensor* scratch_buffer) {\n  tflite::ConvParams op_params;\n  op_params.padding_type = PaddingType::kSame;\n  op_params.padding_values.width = data->padding.width;\n  op_params.padding_values.height = data->padding.height;\n  op_params.padding_values.width_offset = data->padding.width_offset;\n  op_params.padding_values.height_offset = data->padding.height_offset;\n  op_params.stride_width = params->stride_width;\n  op_params.stride_height = params->stride_height;\n  // Need to flip the sign of input offset to add it directly to the quantized\n  // buffer.\n  op_params.input_offset = -input->params.zero_point;\n  op_params.output_offset = output->params.zero_point;\n  op_params.quantized_activation_min = data->output_activation_min;\n  op_params.quantized_activation_max = data->output_activation_max;\n\n  // Need to add optimized kernel\n  reference_integer_ops::TransposeConv(\n      op_params, data->per_channel_output_multiplier.data(),\n      data->per_channel_output_shift.data(), GetTensorShape(input),\n      GetTensorData<int16>(input), GetTensorShape(weights),\n      GetTensorData<int8>(weights), GetTensorShape(bias),\n      GetTensorData<int64_t>(bias), GetTensorShape(output),\n      GetTensorData<int16>(output), GetTensorShape(col2im),\n      GetTensorData<int8>(col2im), GetTensorData<int64_t>(scratch_buffer));\n}\n\ntemplate <KernelType kernel_type>\nTfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n  // Retrieve tensors (All should be allocated by now)\n  const TfLiteTensor* output_shape;\n  TF_LITE_ENSURE_OK(\n      context, GetInputSafe(context, node, kOutputShapeTensor, &output_shape));\n  const TfLiteTensor* weights;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kWeightsTensor, &weights));\n  const TfLiteTensor* input;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kDataInputTensor, &input));\n  const TfLiteTensor* bias =\n      (NumInputs(node) == 4)\n          ? GetOptionalInputTensor(context, node, kBiasTensor)\n          : nullptr;\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context,\n                    GetOutputSafe(context, node, kOutputTensor, &output));\n  OpData* data = reinterpret_cast<OpData*>(node->user_data);\n  TfLiteTensor* col2im = data->has_col2im\n                             ? GetTemporary(context, node, data->col2im_index)\n                             : nullptr;\n  TfLiteTensor* transposed_weights =\n      data->weights_are_transposed\n          ? GetTemporary(context, node, data->transposed_weights_index)\n          : nullptr;\n  const auto* params =\n      reinterpret_cast<TfLiteTransposeConvParams*>(node->builtin_data);\n\n  // Resize any deferred dynamic tensors\n  if (IsDynamicTensor(output)) {\n    TF_LITE_ENSURE_OK(context, ResizeTensor(context, output_shape, output));\n  }\n  if (data->has_col2im && IsDynamicTensor(col2im)) {\n    TF_LITE_ENSURE_OK(context, ResizeCol2ImTensor(context, output_shape,\n                                                  weights, input, col2im));\n  }\n\n  // Get height and width of the output image.\n  const int width = SizeOfDimension(output, 2);\n  const int height = SizeOfDimension(output, 1);\n  const int filter_width = SizeOfDimension(weights, 2);\n  const int filter_height = SizeOfDimension(weights, 1);\n\n  int unused_output_height, unused_output_width;\n  data->padding = ComputePaddingHeightWidth(\n      params->stride_height, params->stride_width, 1, 1, height, width,\n      filter_height, filter_width, params->padding, &unused_output_height,\n      &unused_output_width);\n\n  // Currently support float32, uint8, int8, int16.\n  switch (input->type) {\n    case kTfLiteFloat32: {\n      // Only for GenericOptimized path, we use transposed weights.\n      if (data->weights_are_transposed) {\n        if (!IsConstantTensor(weights)) {\n          ResizeAndTransposeWeights(context, weights, transposed_weights);\n        }\n      }\n      EvalFloat<kernel_type>(context, params, data, input, weights, bias,\n                             transposed_weights, col2im, output);\n      break;\n    }\n    case kTfLiteUInt8: {\n      TfLiteTensor* scratch_buffer;\n      TF_LITE_ENSURE_OK(\n          context, GetTemporarySafe(context, node, data->scratch_tensor_index,\n                                    &scratch_buffer));\n      if (IsDynamicTensor(scratch_buffer)) {\n        TF_LITE_ENSURE_OK(context,\n                          ResizeTensor(context, output_shape, scratch_buffer));\n      }\n      if (data->weights_are_transposed) {\n        if (!IsConstantTensor(weights)) {\n          ResizeAndTransposeWeights(context, weights, transposed_weights);\n        }\n      }\n      EvalQuantized<kernel_type>(context, params, data, input, weights,\n                                 transposed_weights, bias, col2im, output,\n                                 scratch_buffer);\n      break;\n    }\n    case kTfLiteInt8: {\n      TfLiteTensor* scratch_buffer;\n      TF_LITE_ENSURE_OK(\n          context, GetTemporarySafe(context, node, data->scratch_tensor_index,\n                                    &scratch_buffer));\n      if (IsDynamicTensor(scratch_buffer)) {\n        TF_LITE_ENSURE_OK(context,\n                          ResizeTensor(context, output_shape, scratch_buffer));\n      }\n      if (data->weights_are_transposed && !IsConstantTensor(weights)) {\n        ResizeAndTransposeWeights(context, weights, transposed_weights);\n      }\n      EvalQuantizedPerChannel<kernel_type>(context, params, data, input,\n                                           weights, transposed_weights, bias,\n                                           col2im, output, scratch_buffer);\n      break;\n    }\n    case kTfLiteInt16: {\n      TfLiteTensor* scratch_buffer;\n      TF_LITE_ENSURE_OK(\n          context, GetTemporarySafe(context, node, data->scratch_tensor_index,\n                                    &scratch_buffer));\n      if (IsDynamicTensor(scratch_buffer)) {\n        TF_LITE_ENSURE_OK(context,\n                          ResizeTensor(context, output_shape, scratch_buffer));\n      }\n      if (data->weights_are_transposed && !IsConstantTensor(weights)) {\n        ResizeAndTransposeWeights(context, weights, transposed_weights);\n      }\n      EvalQuantizedPerChannel16x8(context, params, data, input, weights,\n                                  transposed_weights, bias, col2im, output,\n                                  scratch_buffer);\n      break;\n    }\n    default:\n      context->ReportError(context, \"Type '%s' is not currently supported.\",\n                           TfLiteTypeGetName(input->type));\n      return kTfLiteError;\n  }\n  return kTfLiteOk;\n}\n\n}  // namespace transpose_conv\n\nTfLiteRegistration* Register_TRANSPOSECONV_REF() {\n  static TfLiteRegistration r = {\n      transpose_conv::Init, transpose_conv::Free,\n      transpose_conv::Prepare<transpose_conv::kReference>,\n      transpose_conv::Eval<transpose_conv::kReference>};\n  return &r;\n}\n\nTfLiteRegistration* Register_TRANSPOSECONV_GENERIC_OPT() {\n  static TfLiteRegistration r = {\n      transpose_conv::Init, transpose_conv::Free,\n      transpose_conv::Prepare<transpose_conv::kGenericOptimized>,\n      transpose_conv::Eval<transpose_conv::kGenericOptimized>};\n  return &r;\n}\n\nTfLiteRegistration* Register_TRANSPOSE_CONV() {\n  return Register_TRANSPOSECONV_GENERIC_OPT();\n}\n\n}  // namespace builtin\n}  // namespace ops\n}  // namespace tflite\n"], "fixing_code": ["/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include <stddef.h>\n#include <stdint.h>\n\n#include <vector>\n\n#include \"tensorflow/lite/c/builtin_op_data.h\"\n#include \"tensorflow/lite/c/common.h\"\n#include \"tensorflow/lite/kernels/cpu_backend_context.h\"\n#include \"tensorflow/lite/kernels/internal/compatibility.h\"\n// NOLINTNEXTLINE - This header file shouldn't go to the top.\n#include \"tensorflow/lite/kernels/internal/optimized/integer_ops/transpose_conv.h\"\n#include \"tensorflow/lite/kernels/internal/optimized/optimized_ops.h\"\n// NOLINTNEXTLINE - This header file shouldn't go to the top.\n#include \"tensorflow/lite/kernels/internal/reference/integer_ops/transpose_conv.h\"\n#include \"tensorflow/lite/kernels/internal/reference/reference_ops.h\"\n#include \"tensorflow/lite/kernels/internal/tensor.h\"\n#include \"tensorflow/lite/kernels/internal/tensor_ctypes.h\"\n#include \"tensorflow/lite/kernels/internal/types.h\"\n#include \"tensorflow/lite/kernels/kernel_util.h\"\n#include \"tensorflow/lite/kernels/padding.h\"\n\nnamespace tflite {\nnamespace ops {\nnamespace builtin {\nnamespace transpose_conv {\n\n// This file has 2 implementation of TransposeConv.\nenum KernelType {\n  kReference,\n  kGenericOptimized,  // Neon-free\n};\n\nconstexpr int kOutputShapeTensor = 0;\nconstexpr int kWeightsTensor = 1;\nconstexpr int kDataInputTensor = 2;\nconstexpr int kBiasTensor = 3;\nconstexpr int kOutputTensor = 0;\n\nconst int kTensorNotAllocated = -1;\n\nstruct OpData {\n  // IDs are the arbitrary identifiers used by TF Lite to identify and access\n  // memory buffers.\n  int col2im_id = kTensorNotAllocated;\n  int transposed_weights_id = kTensorNotAllocated;\n  int scratch_tensor_id = kTensorNotAllocated;\n\n  // col2im is the temporary tensor allocated and used in optimized path for\n  // storing col2im data:gemm result for input_matrix x filter_matrix.\n  int32_t col2im_index;\n\n  // TfLiteConverter will transpose weights from HWOI to OHWI order.\n  // In optimized path, we will transpose them back to HWOI, this temporary\n  // tensor is allocated for storing transposed weights.\n  int32_t transposed_weights_index;\n\n  // Scratch tensor is used in the quantized path for storing accumulation\n  // results.\n  int32_t scratch_tensor_index;\n\n  TfLitePaddingValues padding;\n  // The scaling factor from input to output (aka the 'real multiplier') can\n  // be represented as a fixed point multiplier plus a left shift.\n  int32_t output_multiplier;\n  int output_shift;\n\n  // Per channel output multiplier and shift.\n  std::vector<int32_t> per_channel_output_multiplier;\n  std::vector<int32_t> per_channel_output_shift;\n\n  // The range of the fused activation layer. For example for kNone and\n  // uint8_t these would be 0 and 255.\n  int32_t output_activation_min;\n  int32_t output_activation_max;\n\n  bool has_col2im = false;\n  bool weights_are_transposed = false;\n};\n\nvoid* Init(TfLiteContext* context, const char* buffer, size_t length) {\n  return new OpData;\n}\n\nvoid Free(TfLiteContext* context, void* buffer) {\n  delete reinterpret_cast<OpData*>(buffer);\n}\n\nTfLiteStatus ResizeTensor(TfLiteContext* context,\n                          const TfLiteTensor* shape_tensor,\n                          TfLiteTensor* tensor_to_resize) {\n  // Currently only support int32 for output shape.\n  if (shape_tensor->type != kTfLiteInt32) {\n    TF_LITE_KERNEL_LOG(context, \"Output shape is %s, not int32.\",\n                       TfLiteTypeGetName(shape_tensor->type));\n    return kTfLiteError;\n  }\n\n  TfLiteIntArray* shape = TfLiteIntArrayCreate(NumElements(shape_tensor));\n  for (int i = 0; i < shape->size; ++i) {\n    shape->data[i] = GetTensorData<int32_t>(shape_tensor)[i];\n  }\n\n  return context->ResizeTensor(context, tensor_to_resize, shape);\n}\n\n// Allocate temporary tensors if necessary.\ntemplate <KernelType kernel_type>\nstatic TfLiteStatus AllocateTemporaryTensorsIfRequired(TfLiteContext* context,\n                                                       TfLiteType input_type,\n                                                       TfLiteType weights_type,\n                                                       TfLiteNode* node) {\n  OpData* data = reinterpret_cast<OpData*>(node->user_data);\n  int temporaries_count = 0;\n\n  // Allocate col2im tensor. Currently it's only used for optimized kernels.\n  if (kernel_type == kGenericOptimized) {\n    if (data->col2im_id == kTensorNotAllocated) {\n      context->AddTensors(context, 1, &data->col2im_id);\n    }\n    data->col2im_index = temporaries_count;\n    data->has_col2im = true;\n    ++temporaries_count;\n  }\n\n  // Allocate transposed_weights tensor. Currently it's only used for optimized\n  // float kernels.\n  if (kernel_type == kGenericOptimized) {\n    if (data->transposed_weights_id == kTensorNotAllocated) {\n      context->AddTensors(context, 1, &data->transposed_weights_id);\n    }\n    data->transposed_weights_index = temporaries_count;\n    data->weights_are_transposed = true;\n    ++temporaries_count;\n  }\n\n  // Allocate scratch buffer tensor\n  if (input_type == kTfLiteUInt8 || input_type == kTfLiteInt8 ||\n      input_type == kTfLiteInt16) {\n    if (data->scratch_tensor_id == kTensorNotAllocated) {\n      context->AddTensors(context, 1, &data->scratch_tensor_id);\n    }\n    data->scratch_tensor_index = temporaries_count;\n    ++temporaries_count;\n  }\n\n  TfLiteIntArrayFree(node->temporaries);\n  node->temporaries = TfLiteIntArrayCreate(temporaries_count);\n\n  return kTfLiteOk;\n}\n\nTfLiteStatus ResizeCol2ImTensor(TfLiteContext* context,\n                                const TfLiteTensor* output_shape,\n                                const TfLiteTensor* weights,\n                                const TfLiteTensor* input,\n                                TfLiteTensor* col2im) {\n  if (output_shape->type != kTfLiteInt32) {\n    TF_LITE_KERNEL_LOG(context, \"col2im shape is %s, not int32.\",\n                       TfLiteTypeGetName(output_shape->type));\n    return kTfLiteError;\n  }\n  TF_LITE_ENSURE_EQ(context, NumElements(output_shape), 4);\n  TfLiteIntArray* col2im_shape_array = TfLiteIntArrayCreate(2);\n  const RuntimeShape& input_shape = GetTensorShape(input);\n  const RuntimeShape& weights_shape = GetTensorShape(weights);\n  col2im_shape_array->data[0] = input_shape.Dims(1) * input_shape.Dims(2);\n  col2im_shape_array->data[1] =\n      weights_shape.Dims(0) * weights_shape.Dims(1) * weights_shape.Dims(2);\n\n  col2im->type = input->type == kTfLiteFloat32 ? kTfLiteFloat32 : kTfLiteInt32;\n  col2im->allocation_type = kTfLiteDynamic;\n  return context->ResizeTensor(context, col2im, col2im_shape_array);\n}\n\nTfLiteStatus ResizeAndTransposeWeights(TfLiteContext* context,\n                                       const TfLiteTensor* weights,\n                                       TfLiteTensor* transposed_weights) {\n  TfLiteIntArray* transposed_weights_shape_array = TfLiteIntArrayCreate(4);\n  const RuntimeShape& input_shape = GetTensorShape(weights);\n  transposed_weights_shape_array->data[0] = input_shape.Dims(1);\n  transposed_weights_shape_array->data[1] = input_shape.Dims(2);\n  transposed_weights_shape_array->data[2] = input_shape.Dims(0);\n  transposed_weights_shape_array->data[3] = input_shape.Dims(3);\n\n  transposed_weights->type = weights->type;\n  transposed_weights->allocation_type = kTfLiteDynamic;\n  TF_LITE_ENSURE_STATUS(context->ResizeTensor(context, transposed_weights,\n                                              transposed_weights_shape_array));\n\n  // Transpose the weights from OHWI order to HWOI order.\n  TransposeParams transpose_params;\n  transpose_params.perm_count = 4;\n  transpose_params.perm[0] = 1;\n  transpose_params.perm[1] = 2;\n  transpose_params.perm[2] = 0;\n  transpose_params.perm[3] = 3;\n\n  if (weights->type == kTfLiteFloat32) {\n    optimized_ops::Transpose(transpose_params, input_shape,\n                             GetTensorData<float>(weights),\n                             GetTensorShape(transposed_weights),\n                             GetTensorData<float>(transposed_weights));\n  } else if (weights->type == kTfLiteUInt8) {\n    optimized_ops::Transpose(transpose_params, input_shape,\n                             GetTensorData<uint8>(weights),\n                             GetTensorShape(transposed_weights),\n                             GetTensorData<uint8>(transposed_weights));\n  } else if (weights->type == kTfLiteInt8) {\n    // int16 transpose_conv also with int8 weights\n    optimized_ops::Transpose(transpose_params, input_shape,\n                             GetTensorData<int8>(weights),\n                             GetTensorShape(transposed_weights),\n                             GetTensorData<int8>(transposed_weights));\n  } else {\n    TF_LITE_KERNEL_LOG(\n        context,\n        \"Only float32, uint8, int8, int16 is supported currently, got %s.\",\n        TfLiteTypeGetName(weights->type));\n    return kTfLiteError;\n  }\n\n  return kTfLiteOk;\n}\n\ntemplate <KernelType kernel_type>\nTfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n  OpData* data = reinterpret_cast<OpData*>(node->user_data);\n\n  bool has_bias = NumInputs(node) == 4;\n\n  // Sanity checks on op\n  TF_LITE_ENSURE(context, has_bias || NumInputs(node) == 3);\n  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);\n\n  // Retrieve tensors\n  const TfLiteTensor* output_shape;\n  TF_LITE_ENSURE_OK(\n      context, GetInputSafe(context, node, kOutputShapeTensor, &output_shape));\n  const TfLiteTensor* weights;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kWeightsTensor, &weights));\n  const TfLiteTensor* input;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kDataInputTensor, &input));\n  const TfLiteTensor* bias = nullptr;\n\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context,\n                    GetOutputSafe(context, node, kOutputTensor, &output));\n\n  // Tensor sanity checks\n  TF_LITE_ENSURE_EQ(context, NumDimensions(output_shape), 1);\n  TF_LITE_ENSURE_EQ(context, NumDimensions(input), 4);\n  TF_LITE_ENSURE_EQ(context, NumDimensions(weights), 4);\n  TF_LITE_ENSURE(context,\n                 input->type == kTfLiteFloat32 || input->type == kTfLiteUInt8 ||\n                     input->type == kTfLiteInt8 || input->type == kTfLiteInt16);\n\n  if (has_bias) {\n    bias = GetOptionalInputTensor(context, node, kBiasTensor);\n    if (bias) {\n      if (input->type == kTfLiteUInt8 || input->type == kTfLiteInt8) {\n        TF_LITE_ENSURE_TYPES_EQ(context, bias->type, kTfLiteInt32);\n        if (input->type == kTfLiteInt8) {\n          TF_LITE_ENSURE_EQ(context, bias->params.zero_point, 0);\n        }\n      } else if (input->type == kTfLiteInt16) {\n        TF_LITE_ENSURE_EQ(context, bias->type, kTfLiteInt64);\n        TF_LITE_ENSURE_EQ(context, bias->params.zero_point, 0);\n      } else {\n        TF_LITE_ENSURE_TYPES_EQ(context, bias->type, input->type);\n      }\n      TF_LITE_ENSURE_EQ(context, NumElements(bias),\n                        SizeOfDimension(weights, 0));\n    }\n  }\n\n  if (input->type == kTfLiteInt16) {\n    TF_LITE_ENSURE_EQ(context, weights->type, kTfLiteInt8);\n    TF_LITE_ENSURE_EQ(context, input->params.zero_point, 0);\n    TF_LITE_ENSURE_EQ(context, output->params.zero_point, 0);\n  } else {\n    TF_LITE_ENSURE_TYPES_EQ(context, weights->type, input->type);\n  }\n  TF_LITE_ENSURE_TYPES_EQ(context, output->type, input->type);\n  // Ensure that weights and inputs have the same channel dimension.\n  // Note: TOCO will reorder weights in the following format: OHWI.\n  TF_LITE_ENSURE_EQ(context, SizeOfDimension(input, 3),\n                    SizeOfDimension(weights, 3));\n\n  // Allocate col2Im, transposed_weights & scratch Tensor.\n  TF_LITE_ENSURE_STATUS(AllocateTemporaryTensorsIfRequired<kernel_type>(\n      context, input->type, weights->type, node));\n\n  OpData* user_data = reinterpret_cast<OpData*>(node->user_data);\n  TfLiteTensor* col2im = nullptr;\n  if (data->has_col2im) {\n    node->temporaries->data[data->col2im_index] = data->col2im_id;\n    TF_LITE_ENSURE_OK(\n        context,\n        GetTemporarySafe(context, node, user_data->col2im_index, &col2im));\n  }\n\n  if (!IsConstantTensor(output_shape)) {\n    // Defer resizing until Eval().\n    SetTensorToDynamic(output);\n    if (data->has_col2im) {\n      SetTensorToDynamic(col2im);\n    }\n  } else {\n    TF_LITE_ENSURE_STATUS(ResizeTensor(context, output_shape, output));\n    if (data->has_col2im) {\n      TF_LITE_ENSURE_STATUS(\n          ResizeCol2ImTensor(context, output_shape, weights, input, col2im));\n    }\n  }\n\n  if (data->weights_are_transposed) {\n    node->temporaries->data[data->transposed_weights_index] =\n        data->transposed_weights_id;\n    TfLiteTensor* transposed_weights;\n    TF_LITE_ENSURE_OK(\n        context,\n        GetTemporarySafe(context, node, user_data->transposed_weights_index,\n                         &transposed_weights));\n    if (!IsConstantTensor(weights)) {\n      SetTensorToDynamic(transposed_weights);\n    } else {\n      ResizeAndTransposeWeights(context, weights, transposed_weights);\n    }\n  }\n\n  if (input->type == kTfLiteUInt8 || input->type == kTfLiteInt8 ||\n      input->type == kTfLiteInt16) {\n    node->temporaries->data[data->scratch_tensor_index] =\n        data->scratch_tensor_id;\n    TfLiteTensor* scratch_buffer;\n    TF_LITE_ENSURE_OK(\n        context, GetTemporarySafe(context, node, data->scratch_tensor_index,\n                                  &scratch_buffer));\n    if (input->type == kTfLiteInt16) {\n      scratch_buffer->type = kTfLiteInt64;\n    } else {\n      scratch_buffer->type = kTfLiteInt32;\n    }\n\n    scratch_buffer->allocation_type = kTfLiteDynamic;\n    if (!IsConstantTensor(output_shape)) {\n      SetTensorToDynamic(scratch_buffer);\n    } else {\n      TF_LITE_ENSURE_STATUS(\n          ResizeTensor(context, output_shape, scratch_buffer));\n    }\n\n    TF_LITE_ENSURE_EQ(context, weights->quantization.type,\n                      kTfLiteAffineQuantization);\n    const auto* affine_quantization =\n        reinterpret_cast<TfLiteAffineQuantization*>(\n            weights->quantization.params);\n    const int channels_out = weights->dims->data[0];\n    TF_LITE_ENSURE(context, affine_quantization);\n    TF_LITE_ENSURE(context, affine_quantization->scale);\n    TF_LITE_ENSURE(context, (affine_quantization->scale->size == 1 ||\n                             affine_quantization->scale->size == channels_out));\n\n    data->per_channel_output_multiplier.resize(channels_out);\n    data->per_channel_output_shift.resize(channels_out);\n    TF_LITE_ENSURE_STATUS(tflite::PopulateConvolutionQuantizationParams(\n        context, input, weights, bias, output, kTfLiteActNone,\n        &data->output_multiplier, &data->output_shift,\n        &data->output_activation_min, &data->output_activation_max,\n        data->per_channel_output_multiplier.data(),\n        data->per_channel_output_shift.data(), channels_out));\n  }\n\n  return kTfLiteOk;\n}\n\ntemplate <KernelType kernel_type>\nvoid EvalFloat(TfLiteContext* context, const TfLiteTransposeConvParams* params,\n               const OpData* data, const TfLiteTensor* input,\n               const TfLiteTensor* weights, const TfLiteTensor* bias,\n               const TfLiteTensor* transposed_weights, TfLiteTensor* col2im,\n               TfLiteTensor* output) {\n  tflite::ConvParams op_params;\n  op_params.padding_type = PaddingType::kSame;\n  op_params.padding_values.width = data->padding.width;\n  op_params.padding_values.height = data->padding.height;\n  op_params.padding_values.width_offset = data->padding.width_offset;\n  op_params.padding_values.height_offset = data->padding.height_offset;\n  op_params.stride_width = params->stride_width;\n  op_params.stride_height = params->stride_height;\n\n  switch (kernel_type) {\n    case kReference: {\n      reference_ops::TransposeConv(\n          op_params, GetTensorShape(input), GetTensorData<float>(input),\n          GetTensorShape(weights), GetTensorData<float>(weights),\n          GetTensorShape(bias), GetTensorData<float>(bias),\n          GetTensorShape(output), GetTensorData<float>(output),\n          GetTensorShape(col2im), GetTensorData<float>(col2im));\n      break;\n    }\n    case kGenericOptimized: {\n      optimized_ops::TransposeConvV2(\n          op_params, GetTensorShape(input), GetTensorData<float>(input),\n          GetTensorShape(transposed_weights),\n          GetTensorData<float>(transposed_weights), GetTensorShape(bias),\n          GetTensorData<float>(bias), GetTensorShape(output),\n          GetTensorData<float>(output), GetTensorShape(col2im),\n          GetTensorData<float>(col2im),\n          CpuBackendContext::GetFromContext(context));\n      break;\n    }\n  }\n}\n\ntemplate <KernelType kernel_type>\nvoid EvalQuantized(TfLiteContext* context,\n                   const TfLiteTransposeConvParams* params, OpData* data,\n                   const TfLiteTensor* input, const TfLiteTensor* weights,\n                   const TfLiteTensor* transposed_weights,\n                   const TfLiteTensor* bias, TfLiteTensor* col2im,\n                   TfLiteTensor* output, TfLiteTensor* scratch_buffer) {\n  int32_t input_offset = -input->params.zero_point;\n  int32_t filter_offset = -weights->params.zero_point;\n  int32_t output_offset = output->params.zero_point;\n\n  tflite::ConvParams op_params;\n  op_params.padding_type = PaddingType::kSame;\n  op_params.padding_values.width = data->padding.width;\n  op_params.padding_values.height = data->padding.height;\n  op_params.padding_values.width_offset = data->padding.width_offset;\n  op_params.padding_values.height_offset = data->padding.height_offset;\n  op_params.stride_width = params->stride_width;\n  op_params.stride_height = params->stride_height;\n  op_params.input_offset = input_offset;\n  op_params.output_offset = output_offset;\n  op_params.weights_offset = filter_offset;\n  op_params.output_multiplier = data->output_multiplier;\n  op_params.output_shift = -data->output_shift;\n  op_params.quantized_activation_min = data->output_activation_min;\n  op_params.quantized_activation_max = data->output_activation_max;\n\n  switch (kernel_type) {\n    case kReference: {\n      reference_ops::TransposeConv(\n          op_params, GetTensorShape(input), GetTensorData<uint8>(input),\n          GetTensorShape(weights), GetTensorData<uint8>(weights),\n          GetTensorShape(bias), GetTensorData<int32_t>(bias),\n          GetTensorShape(output), GetTensorData<uint8>(output),\n          GetTensorShape(col2im), GetTensorData<uint8>(col2im),\n          GetTensorData<int32_t>(scratch_buffer));\n      break;\n    }\n    case kGenericOptimized: {\n      optimized_ops::TransposeConvV2(\n          op_params, GetTensorShape(input), GetTensorData<uint8>(input),\n          GetTensorShape(transposed_weights),\n          GetTensorData<uint8>(transposed_weights), GetTensorShape(bias),\n          GetTensorData<int32>(bias), GetTensorShape(output),\n          GetTensorData<uint8>(output), GetTensorShape(col2im),\n          GetTensorData<int32>(col2im), GetTensorData<int32>(scratch_buffer),\n          CpuBackendContext::GetFromContext(context));\n      break;\n    }\n  }\n}\n\ntemplate <KernelType kernel_type>\nvoid EvalQuantizedPerChannel(\n    TfLiteContext* context, const TfLiteTransposeConvParams* params,\n    OpData* data, const TfLiteTensor* input, const TfLiteTensor* weights,\n    const TfLiteTensor* transposed_weights, const TfLiteTensor* bias,\n    TfLiteTensor* col2im, TfLiteTensor* output, TfLiteTensor* scratch_buffer) {\n  tflite::ConvParams op_params;\n  op_params.padding_type = PaddingType::kSame;\n  op_params.padding_values.width = data->padding.width;\n  op_params.padding_values.height = data->padding.height;\n  op_params.padding_values.width_offset = data->padding.width_offset;\n  op_params.padding_values.height_offset = data->padding.height_offset;\n  op_params.stride_width = params->stride_width;\n  op_params.stride_height = params->stride_height;\n  // Need to flip the sign of input offset to add it directly to the quantized\n  // buffer.\n  op_params.input_offset = -input->params.zero_point;\n  op_params.output_offset = output->params.zero_point;\n  op_params.quantized_activation_min = data->output_activation_min;\n  op_params.quantized_activation_max = data->output_activation_max;\n\n  switch (kernel_type) {\n    case kReference: {\n      reference_integer_ops::TransposeConv(\n          op_params, data->per_channel_output_multiplier.data(),\n          data->per_channel_output_shift.data(), GetTensorShape(input),\n          GetTensorData<int8>(input), GetTensorShape(weights),\n          GetTensorData<int8>(weights), GetTensorShape(bias),\n          GetTensorData<int32>(bias), GetTensorShape(output),\n          GetTensorData<int8>(output), GetTensorShape(col2im),\n          GetTensorData<int8>(col2im), GetTensorData<int32_t>(scratch_buffer));\n      break;\n    }\n    case kGenericOptimized: {\n      optimized_integer_ops::TransposeConvV2(\n          op_params, data->per_channel_output_multiplier.data(),\n          data->per_channel_output_shift.data(), GetTensorShape(input),\n          GetTensorData<int8>(input), GetTensorShape(transposed_weights),\n          GetTensorData<int8>(transposed_weights), GetTensorShape(bias),\n          GetTensorData<int32>(bias), GetTensorShape(output),\n          GetTensorData<int8>(output), GetTensorShape(col2im),\n          GetTensorData<int32>(col2im), GetTensorData<int32>(scratch_buffer),\n          CpuBackendContext::GetFromContext(context));\n      break;\n    }\n  }\n}\n\nvoid EvalQuantizedPerChannel16x8(\n    TfLiteContext* context, const TfLiteTransposeConvParams* params,\n    OpData* data, const TfLiteTensor* input, const TfLiteTensor* weights,\n    const TfLiteTensor* transposed_weights, const TfLiteTensor* bias,\n    TfLiteTensor* col2im, TfLiteTensor* output, TfLiteTensor* scratch_buffer) {\n  tflite::ConvParams op_params;\n  op_params.padding_type = PaddingType::kSame;\n  op_params.padding_values.width = data->padding.width;\n  op_params.padding_values.height = data->padding.height;\n  op_params.padding_values.width_offset = data->padding.width_offset;\n  op_params.padding_values.height_offset = data->padding.height_offset;\n  op_params.stride_width = params->stride_width;\n  op_params.stride_height = params->stride_height;\n  // Need to flip the sign of input offset to add it directly to the quantized\n  // buffer.\n  op_params.input_offset = -input->params.zero_point;\n  op_params.output_offset = output->params.zero_point;\n  op_params.quantized_activation_min = data->output_activation_min;\n  op_params.quantized_activation_max = data->output_activation_max;\n\n  // Need to add optimized kernel\n  reference_integer_ops::TransposeConv(\n      op_params, data->per_channel_output_multiplier.data(),\n      data->per_channel_output_shift.data(), GetTensorShape(input),\n      GetTensorData<int16>(input), GetTensorShape(weights),\n      GetTensorData<int8>(weights), GetTensorShape(bias),\n      GetTensorData<int64_t>(bias), GetTensorShape(output),\n      GetTensorData<int16>(output), GetTensorShape(col2im),\n      GetTensorData<int8>(col2im), GetTensorData<int64_t>(scratch_buffer));\n}\n\ntemplate <KernelType kernel_type>\nTfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n  // Retrieve tensors (All should be allocated by now)\n  const TfLiteTensor* output_shape;\n  TF_LITE_ENSURE_OK(\n      context, GetInputSafe(context, node, kOutputShapeTensor, &output_shape));\n  const TfLiteTensor* weights;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kWeightsTensor, &weights));\n  const TfLiteTensor* input;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kDataInputTensor, &input));\n  const TfLiteTensor* bias =\n      (NumInputs(node) == 4)\n          ? GetOptionalInputTensor(context, node, kBiasTensor)\n          : nullptr;\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context,\n                    GetOutputSafe(context, node, kOutputTensor, &output));\n  OpData* data = reinterpret_cast<OpData*>(node->user_data);\n  TfLiteTensor* col2im = data->has_col2im\n                             ? GetTemporary(context, node, data->col2im_index)\n                             : nullptr;\n  TfLiteTensor* transposed_weights =\n      data->weights_are_transposed\n          ? GetTemporary(context, node, data->transposed_weights_index)\n          : nullptr;\n  const auto* params =\n      reinterpret_cast<TfLiteTransposeConvParams*>(node->builtin_data);\n\n  // Prevent divisions by 0\n  TF_LITE_ENSURE(context, params->stride_height > 0);\n  TF_LITE_ENSURE(context, params->stride_width > 0);\n\n  // Resize any deferred dynamic tensors\n  if (IsDynamicTensor(output)) {\n    TF_LITE_ENSURE_OK(context, ResizeTensor(context, output_shape, output));\n  }\n  if (data->has_col2im && IsDynamicTensor(col2im)) {\n    TF_LITE_ENSURE_OK(context, ResizeCol2ImTensor(context, output_shape,\n                                                  weights, input, col2im));\n  }\n\n  // Get height and width of the output image.\n  const int width = SizeOfDimension(output, 2);\n  const int height = SizeOfDimension(output, 1);\n  const int filter_width = SizeOfDimension(weights, 2);\n  const int filter_height = SizeOfDimension(weights, 1);\n\n  int unused_output_height, unused_output_width;\n  data->padding = ComputePaddingHeightWidth(\n      params->stride_height, params->stride_width, 1, 1, height, width,\n      filter_height, filter_width, params->padding, &unused_output_height,\n      &unused_output_width);\n\n  // Currently support float32, uint8, int8, int16.\n  switch (input->type) {\n    case kTfLiteFloat32: {\n      // Only for GenericOptimized path, we use transposed weights.\n      if (data->weights_are_transposed) {\n        if (!IsConstantTensor(weights)) {\n          ResizeAndTransposeWeights(context, weights, transposed_weights);\n        }\n      }\n      EvalFloat<kernel_type>(context, params, data, input, weights, bias,\n                             transposed_weights, col2im, output);\n      break;\n    }\n    case kTfLiteUInt8: {\n      TfLiteTensor* scratch_buffer;\n      TF_LITE_ENSURE_OK(\n          context, GetTemporarySafe(context, node, data->scratch_tensor_index,\n                                    &scratch_buffer));\n      if (IsDynamicTensor(scratch_buffer)) {\n        TF_LITE_ENSURE_OK(context,\n                          ResizeTensor(context, output_shape, scratch_buffer));\n      }\n      if (data->weights_are_transposed) {\n        if (!IsConstantTensor(weights)) {\n          ResizeAndTransposeWeights(context, weights, transposed_weights);\n        }\n      }\n      EvalQuantized<kernel_type>(context, params, data, input, weights,\n                                 transposed_weights, bias, col2im, output,\n                                 scratch_buffer);\n      break;\n    }\n    case kTfLiteInt8: {\n      TfLiteTensor* scratch_buffer;\n      TF_LITE_ENSURE_OK(\n          context, GetTemporarySafe(context, node, data->scratch_tensor_index,\n                                    &scratch_buffer));\n      if (IsDynamicTensor(scratch_buffer)) {\n        TF_LITE_ENSURE_OK(context,\n                          ResizeTensor(context, output_shape, scratch_buffer));\n      }\n      if (data->weights_are_transposed && !IsConstantTensor(weights)) {\n        ResizeAndTransposeWeights(context, weights, transposed_weights);\n      }\n      EvalQuantizedPerChannel<kernel_type>(context, params, data, input,\n                                           weights, transposed_weights, bias,\n                                           col2im, output, scratch_buffer);\n      break;\n    }\n    case kTfLiteInt16: {\n      TfLiteTensor* scratch_buffer;\n      TF_LITE_ENSURE_OK(\n          context, GetTemporarySafe(context, node, data->scratch_tensor_index,\n                                    &scratch_buffer));\n      if (IsDynamicTensor(scratch_buffer)) {\n        TF_LITE_ENSURE_OK(context,\n                          ResizeTensor(context, output_shape, scratch_buffer));\n      }\n      if (data->weights_are_transposed && !IsConstantTensor(weights)) {\n        ResizeAndTransposeWeights(context, weights, transposed_weights);\n      }\n      EvalQuantizedPerChannel16x8(context, params, data, input, weights,\n                                  transposed_weights, bias, col2im, output,\n                                  scratch_buffer);\n      break;\n    }\n    default:\n      context->ReportError(context, \"Type '%s' is not currently supported.\",\n                           TfLiteTypeGetName(input->type));\n      return kTfLiteError;\n  }\n  return kTfLiteOk;\n}\n\n}  // namespace transpose_conv\n\nTfLiteRegistration* Register_TRANSPOSECONV_REF() {\n  static TfLiteRegistration r = {\n      transpose_conv::Init, transpose_conv::Free,\n      transpose_conv::Prepare<transpose_conv::kReference>,\n      transpose_conv::Eval<transpose_conv::kReference>};\n  return &r;\n}\n\nTfLiteRegistration* Register_TRANSPOSECONV_GENERIC_OPT() {\n  static TfLiteRegistration r = {\n      transpose_conv::Init, transpose_conv::Free,\n      transpose_conv::Prepare<transpose_conv::kGenericOptimized>,\n      transpose_conv::Eval<transpose_conv::kGenericOptimized>};\n  return &r;\n}\n\nTfLiteRegistration* Register_TRANSPOSE_CONV() {\n  return Register_TRANSPOSECONV_GENERIC_OPT();\n}\n\n}  // namespace builtin\n}  // namespace ops\n}  // namespace tflite\n"], "filenames": ["tensorflow/lite/kernels/transpose_conv.cc"], "buggy_code_start_loc": [593], "buggy_code_end_loc": [593], "fixing_code_start_loc": [594], "fixing_code_end_loc": [598], "type": "CWE-369", "message": "TensorFlow is an end-to-end open source platform for machine learning. The optimized implementation of the `TransposeConv` TFLite operator is [vulnerable to a division by zero error](https://github.com/tensorflow/tensorflow/blob/0d45ea1ca641b21b73bcf9c00e0179cda284e7e7/tensorflow/lite/kernels/internal/optimized/optimized_ops.h#L5221-L5222). An attacker can craft a model such that `stride_{h,w}` values are 0. Code calling this function must validate these arguments. The fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range.", "other": {"cve": {"id": "CVE-2021-29588", "sourceIdentifier": "security-advisories@github.com", "published": "2021-05-14T20:15:14.723", "lastModified": "2021-05-19T15:02:10.633", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "TensorFlow is an end-to-end open source platform for machine learning. The optimized implementation of the `TransposeConv` TFLite operator is [vulnerable to a division by zero error](https://github.com/tensorflow/tensorflow/blob/0d45ea1ca641b21b73bcf9c00e0179cda284e7e7/tensorflow/lite/kernels/internal/optimized/optimized_ops.h#L5221-L5222). An attacker can craft a model such that `stride_{h,w}` values are 0. Code calling this function must validate these arguments. The fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range."}, {"lang": "es", "value": "TensorFlow es una plataforma de c\u00f3digo abierto de extremo a extremo para el aprendizaje autom\u00e1tico.&#xa0;La implementaci\u00f3n optimizada del operador TFLite \"TransposeConv\" es [vulnerable a un error de divisi\u00f3n por cero] (https://github.com/tensorflow/tensorflow/blob/0d45ea1ca641b21b73bcf9c00e0179cda284e7e7/tensorflow/lite/kernels/internal/optimized/optimized#L5221-L5222).&#xa0;Un atacante puede dise\u00f1ar un modelo tal que los valores de \"stride_ {h, w}\" sean 0. El c\u00f3digo que llama a esta funci\u00f3n debe comprobar estos argumentos.&#xa0;La correcci\u00f3n ser\u00e1 incluida en TensorFlow versi\u00f3n 2.5.0.&#xa0;Tambi\u00e9n seleccionaremos este commit en TensorFlow versi\u00f3n 2.4.2, TensorFlow versi\u00f3n 2.3.3, TensorFlow versi\u00f3n 2.2.3 y TensorFlow versi\u00f3n 2.1.4, ya que estos tambi\u00e9n est\u00e1n afectados y a\u00fan est\u00e1n en el rango compatible"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.9}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:H/PR:L/UI:N/S:U/C:N/I:N/A:L", "attackVector": "LOCAL", "attackComplexity": "HIGH", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "LOW", "baseScore": 2.5, "baseSeverity": "LOW"}, "exploitabilityScore": 1.0, "impactScore": 1.4}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:P/I:P/A:P", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "PARTIAL", "availabilityImpact": "PARTIAL", "baseScore": 4.6}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.9, "impactScore": 6.4, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-369"}]}, {"source": "security-advisories@github.com", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-369"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionEndExcluding": "2.1.4", "matchCriteriaId": "323ABCCE-24EB-47CC-87F6-48C101477587"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.2.0", "versionEndExcluding": "2.2.3", "matchCriteriaId": "64ABA90C-0649-4BB0-89C9-83C14BBDCC0F"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.3.0", "versionEndExcluding": "2.3.3", "matchCriteriaId": "0F83E0CF-CBF6-4C24-8683-3E7A5DC95BA9"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.4.0", "versionEndExcluding": "2.4.2", "matchCriteriaId": "8259531B-A8AC-4F8B-B60F-B69DE4767C03"}]}]}], "references": [{"url": "https://github.com/tensorflow/tensorflow/commit/801c1c6be5324219689c98e1bd3e0ca365ee834d", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/security/advisories/GHSA-vfr4-x8j2-3rf9", "source": "security-advisories@github.com", "tags": ["Exploit", "Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/tensorflow/tensorflow/commit/801c1c6be5324219689c98e1bd3e0ca365ee834d"}}