{"buggy_code": ["//! This implements the VCode container: a CFG of Insts that have been lowered.\n//!\n//! VCode is virtual-register code. An instruction in VCode is almost a machine\n//! instruction; however, its register slots can refer to virtual registers in\n//! addition to real machine registers.\n//!\n//! VCode is structured with traditional basic blocks, and\n//! each block must be terminated by an unconditional branch (one target), a\n//! conditional branch (two targets), or a return (no targets). Note that this\n//! slightly differs from the machine code of most ISAs: in most ISAs, a\n//! conditional branch has one target (and the not-taken case falls through).\n//! However, we expect that machine backends will elide branches to the following\n//! block (i.e., zero-offset jumps), and will be able to codegen a branch-cond /\n//! branch-uncond pair if *both* targets are not fallthrough. This allows us to\n//! play with layout prior to final binary emission, as well, if we want.\n//!\n//! See the main module comment in `mod.rs` for more details on the VCode-based\n//! backend pipeline.\n\nuse crate::ir::{self, types, Constant, ConstantData, SourceLoc};\nuse crate::machinst::*;\nuse crate::settings;\nuse crate::timing;\nuse regalloc::Function as RegallocFunction;\nuse regalloc::Set as RegallocSet;\nuse regalloc::{\n    BlockIx, InstIx, PrettyPrint, Range, RegAllocResult, RegClass, RegUsageCollector,\n    RegUsageMapper, SpillSlot, StackmapRequestInfo,\n};\n\nuse alloc::boxed::Box;\nuse alloc::{borrow::Cow, vec::Vec};\nuse cranelift_entity::{entity_impl, Keys, PrimaryMap};\nuse std::cell::RefCell;\nuse std::collections::HashMap;\nuse std::fmt;\nuse std::iter;\nuse std::string::String;\n\n/// Index referring to an instruction in VCode.\npub type InsnIndex = u32;\n/// Index referring to a basic block in VCode.\npub type BlockIndex = u32;\n\n/// VCodeInst wraps all requirements for a MachInst to be in VCode: it must be\n/// a `MachInst` and it must be able to emit itself at least to a `SizeCodeSink`.\npub trait VCodeInst: MachInst + MachInstEmit {}\nimpl<I: MachInst + MachInstEmit> VCodeInst for I {}\n\n/// A function in \"VCode\" (virtualized-register code) form, after lowering.\n/// This is essentially a standard CFG of basic blocks, where each basic block\n/// consists of lowered instructions produced by the machine-specific backend.\npub struct VCode<I: VCodeInst> {\n    /// Function liveins.\n    liveins: RegallocSet<RealReg>,\n\n    /// Function liveouts.\n    liveouts: RegallocSet<RealReg>,\n\n    /// VReg IR-level types.\n    vreg_types: Vec<Type>,\n\n    /// Do we have any ref values among our vregs?\n    have_ref_values: bool,\n\n    /// Lowered machine instructions in order corresponding to the original IR.\n    insts: Vec<I>,\n\n    /// Source locations for each instruction. (`SourceLoc` is a `u32`, so it is\n    /// reasonable to keep one of these per instruction.)\n    srclocs: Vec<SourceLoc>,\n\n    /// Entry block.\n    entry: BlockIndex,\n\n    /// Block instruction indices.\n    block_ranges: Vec<(InsnIndex, InsnIndex)>,\n\n    /// Block successors: index range in the successor-list below.\n    block_succ_range: Vec<(usize, usize)>,\n\n    /// Block successor lists, concatenated into one Vec. The `block_succ_range`\n    /// list of tuples above gives (start, end) ranges within this list that\n    /// correspond to each basic block's successors.\n    block_succs: Vec<BlockIx>,\n\n    /// Block-order information.\n    block_order: BlockLoweringOrder,\n\n    /// ABI object.\n    abi: Box<dyn ABICallee<I = I>>,\n\n    /// Constant information used during code emission. This should be\n    /// immutable across function compilations within the same module.\n    emit_info: I::Info,\n\n    /// Safepoint instruction indices. Filled in post-regalloc. (Prior to\n    /// regalloc, the safepoint instructions are listed in the separate\n    /// `StackmapRequestInfo` held separate from the `VCode`.)\n    safepoint_insns: Vec<InsnIndex>,\n\n    /// For each safepoint entry in `safepoint_insns`, a list of `SpillSlot`s.\n    /// These are used to generate actual stack maps at emission. Filled in\n    /// post-regalloc.\n    safepoint_slots: Vec<Vec<SpillSlot>>,\n\n    /// Do we generate debug info?\n    generate_debug_info: bool,\n\n    /// Instruction end offsets, instruction indices at each label,\n    /// total buffer size, and start of cold code.  Only present if\n    /// `generate_debug_info` is set.\n    insts_layout: RefCell<InstsLayoutInfo>,\n\n    /// Constants.\n    constants: VCodeConstants,\n\n    /// Are any debug value-labels present? If not, we can skip the\n    /// post-emission analysis.\n    has_value_labels: bool,\n}\n\n#[derive(Debug, Default)]\npub(crate) struct InstsLayoutInfo {\n    pub(crate) inst_end_offsets: Vec<CodeOffset>,\n    pub(crate) label_inst_indices: Vec<CodeOffset>,\n    pub(crate) start_of_cold_code: Option<CodeOffset>,\n}\n\n/// A builder for a VCode function body. This builder is designed for the\n/// lowering approach that we take: we traverse basic blocks in forward\n/// (original IR) order, but within each basic block, we generate code from\n/// bottom to top; and within each IR instruction that we visit in this reverse\n/// order, we emit machine instructions in *forward* order again.\n///\n/// Hence, to produce the final instructions in proper order, we perform two\n/// swaps.  First, the machine instructions (`I` instances) are produced in\n/// forward order for an individual IR instruction. Then these are *reversed*\n/// and concatenated to `bb_insns` at the end of the IR instruction lowering.\n/// The `bb_insns` vec will thus contain all machine instructions for a basic\n/// block, in reverse order. Finally, when we're done with a basic block, we\n/// reverse the whole block's vec of instructions again, and concatenate onto\n/// the VCode's insts.\npub struct VCodeBuilder<I: VCodeInst> {\n    /// In-progress VCode.\n    vcode: VCode<I>,\n\n    /// In-progress stack map-request info.\n    stack_map_info: StackmapRequestInfo,\n\n    /// Index of the last block-start in the vcode.\n    block_start: InsnIndex,\n\n    /// Start of succs for the current block in the concatenated succs list.\n    succ_start: usize,\n\n    /// Current source location.\n    cur_srcloc: SourceLoc,\n}\n\nimpl<I: VCodeInst> VCodeBuilder<I> {\n    /// Create a new VCodeBuilder.\n    pub fn new(\n        abi: Box<dyn ABICallee<I = I>>,\n        emit_info: I::Info,\n        block_order: BlockLoweringOrder,\n        constants: VCodeConstants,\n    ) -> VCodeBuilder<I> {\n        let reftype_class = I::ref_type_regclass(abi.flags());\n        let vcode = VCode::new(\n            abi,\n            emit_info,\n            block_order,\n            constants,\n            /* generate_debug_info = */ true,\n        );\n        let stack_map_info = StackmapRequestInfo {\n            reftype_class,\n            reftyped_vregs: vec![],\n            safepoint_insns: vec![],\n        };\n\n        VCodeBuilder {\n            vcode,\n            stack_map_info,\n            block_start: 0,\n            succ_start: 0,\n            cur_srcloc: SourceLoc::default(),\n        }\n    }\n\n    /// Access the ABI object.\n    pub fn abi(&mut self) -> &mut dyn ABICallee<I = I> {\n        &mut *self.vcode.abi\n    }\n\n    /// Access to the BlockLoweringOrder object.\n    pub fn block_order(&self) -> &BlockLoweringOrder {\n        &self.vcode.block_order\n    }\n\n    /// Set the type of a VReg.\n    pub fn set_vreg_type(&mut self, vreg: VirtualReg, ty: Type) {\n        if self.vcode.vreg_types.len() <= vreg.get_index() {\n            self.vcode\n                .vreg_types\n                .resize(vreg.get_index() + 1, ir::types::I8);\n        }\n        self.vcode.vreg_types[vreg.get_index()] = ty;\n        if is_reftype(ty) {\n            self.stack_map_info.reftyped_vregs.push(vreg);\n            self.vcode.have_ref_values = true;\n        }\n    }\n\n    /// Set the current block as the entry block.\n    pub fn set_entry(&mut self, block: BlockIndex) {\n        self.vcode.entry = block;\n    }\n\n    /// End the current basic block. Must be called after emitting vcode insts\n    /// for IR insts and prior to ending the function (building the VCode).\n    pub fn end_bb(&mut self) {\n        let start_idx = self.block_start;\n        let end_idx = self.vcode.insts.len() as InsnIndex;\n        self.block_start = end_idx;\n        // Add the instruction index range to the list of blocks.\n        self.vcode.block_ranges.push((start_idx, end_idx));\n        // End the successors list.\n        let succ_end = self.vcode.block_succs.len();\n        self.vcode\n            .block_succ_range\n            .push((self.succ_start, succ_end));\n        self.succ_start = succ_end;\n    }\n\n    /// Push an instruction for the current BB and current IR inst within the BB.\n    pub fn push(&mut self, insn: I, is_safepoint: bool) {\n        match insn.is_term() {\n            MachTerminator::None | MachTerminator::Ret => {}\n            MachTerminator::Uncond(target) => {\n                self.vcode.block_succs.push(BlockIx::new(target.get()));\n            }\n            MachTerminator::Cond(true_branch, false_branch) => {\n                self.vcode.block_succs.push(BlockIx::new(true_branch.get()));\n                self.vcode\n                    .block_succs\n                    .push(BlockIx::new(false_branch.get()));\n            }\n            MachTerminator::Indirect(targets) => {\n                for target in targets {\n                    self.vcode.block_succs.push(BlockIx::new(target.get()));\n                }\n            }\n        }\n        if insn.defines_value_label().is_some() {\n            self.vcode.has_value_labels = true;\n        }\n        self.vcode.insts.push(insn);\n        self.vcode.srclocs.push(self.cur_srcloc);\n        if is_safepoint {\n            self.stack_map_info\n                .safepoint_insns\n                .push(InstIx::new((self.vcode.insts.len() - 1) as u32));\n        }\n    }\n\n    /// Set the current source location.\n    pub fn set_srcloc(&mut self, srcloc: SourceLoc) {\n        self.cur_srcloc = srcloc;\n    }\n\n    /// Access the constants.\n    pub fn constants(&mut self) -> &mut VCodeConstants {\n        &mut self.vcode.constants\n    }\n\n    /// Build the final VCode, returning the vcode itself as well as auxiliary\n    /// information, such as the stack map request information.\n    pub fn build(self) -> (VCode<I>, StackmapRequestInfo) {\n        // TODO: come up with an abstraction for \"vcode and auxiliary data\". The\n        // auxiliary data needs to be separate from the vcode so that it can be\n        // referenced as the vcode is mutated (e.g. by the register allocator).\n        (self.vcode, self.stack_map_info)\n    }\n}\n\nfn is_redundant_move<I: VCodeInst>(insn: &I) -> bool {\n    if let Some((to, from)) = insn.is_move() {\n        to.to_reg() == from\n    } else {\n        false\n    }\n}\n\n/// Is this type a reference type?\nfn is_reftype(ty: Type) -> bool {\n    ty == types::R64 || ty == types::R32\n}\n\nimpl<I: VCodeInst> VCode<I> {\n    /// New empty VCode.\n    fn new(\n        abi: Box<dyn ABICallee<I = I>>,\n        emit_info: I::Info,\n        block_order: BlockLoweringOrder,\n        constants: VCodeConstants,\n        generate_debug_info: bool,\n    ) -> VCode<I> {\n        VCode {\n            liveins: abi.liveins(),\n            liveouts: abi.liveouts(),\n            vreg_types: vec![],\n            have_ref_values: false,\n            insts: vec![],\n            srclocs: vec![],\n            entry: 0,\n            block_ranges: vec![],\n            block_succ_range: vec![],\n            block_succs: vec![],\n            block_order,\n            abi,\n            emit_info,\n            safepoint_insns: vec![],\n            safepoint_slots: vec![],\n            generate_debug_info,\n            insts_layout: RefCell::new(Default::default()),\n            constants,\n            has_value_labels: false,\n        }\n    }\n\n    /// Returns the flags controlling this function's compilation.\n    pub fn flags(&self) -> &settings::Flags {\n        self.abi.flags()\n    }\n\n    /// Get the IR-level type of a VReg.\n    pub fn vreg_type(&self, vreg: VirtualReg) -> Type {\n        self.vreg_types[vreg.get_index()]\n    }\n\n    /// Get the number of blocks. Block indices will be in the range `0 ..\n    /// (self.num_blocks() - 1)`.\n    pub fn num_blocks(&self) -> usize {\n        self.block_ranges.len()\n    }\n\n    /// Stack frame size for the full function's body.\n    pub fn frame_size(&self) -> u32 {\n        self.abi.frame_size()\n    }\n\n    /// Get the successors for a block.\n    pub fn succs(&self, block: BlockIndex) -> &[BlockIx] {\n        let (start, end) = self.block_succ_range[block as usize];\n        &self.block_succs[start..end]\n    }\n\n    /// Take the results of register allocation, with a sequence of\n    /// instructions including spliced fill/reload/move instructions, and replace\n    /// the VCode with them.\n    pub fn replace_insns_from_regalloc(&mut self, result: RegAllocResult<Self>) {\n        // Record the spillslot count and clobbered registers for the ABI/stack\n        // setup code.\n        self.abi.set_num_spillslots(result.num_spill_slots as usize);\n        self.abi\n            .set_clobbered(result.clobbered_registers.map(|r| Writable::from_reg(*r)));\n\n        let mut final_insns = vec![];\n        let mut final_block_ranges = vec![(0, 0); self.num_blocks()];\n        let mut final_srclocs = vec![];\n        let mut final_safepoint_insns = vec![];\n        let mut safept_idx = 0;\n\n        assert!(result.target_map.elems().len() == self.num_blocks());\n        for block in 0..self.num_blocks() {\n            let start = result.target_map.elems()[block].get() as usize;\n            let end = if block == self.num_blocks() - 1 {\n                result.insns.len()\n            } else {\n                result.target_map.elems()[block + 1].get() as usize\n            };\n            let block = block as BlockIndex;\n            let final_start = final_insns.len() as InsnIndex;\n\n            if block == self.entry {\n                // Start with the prologue.\n                let prologue = self.abi.gen_prologue();\n                let len = prologue.len();\n                final_insns.extend(prologue.into_iter());\n                final_srclocs.extend(iter::repeat(SourceLoc::default()).take(len));\n            }\n\n            for i in start..end {\n                let insn = &result.insns[i];\n\n                // Elide redundant moves at this point (we only know what is\n                // redundant once registers are allocated).\n                if is_redundant_move(insn) {\n                    continue;\n                }\n\n                // Is there a srcloc associated with this insn? Look it up based on original\n                // instruction index (if new insn corresponds to some original insn, i.e., is not\n                // an inserted load/spill/move).\n                let orig_iix = result.orig_insn_map[InstIx::new(i as u32)];\n                let srcloc = if orig_iix.is_invalid() {\n                    SourceLoc::default()\n                } else {\n                    self.srclocs[orig_iix.get() as usize]\n                };\n\n                // Whenever encountering a return instruction, replace it\n                // with the epilogue.\n                let is_ret = insn.is_term() == MachTerminator::Ret;\n                if is_ret {\n                    let epilogue = self.abi.gen_epilogue();\n                    let len = epilogue.len();\n                    final_insns.extend(epilogue.into_iter());\n                    final_srclocs.extend(iter::repeat(srcloc).take(len));\n                } else {\n                    final_insns.push(insn.clone());\n                    final_srclocs.push(srcloc);\n                }\n\n                // Was this instruction a safepoint instruction? Add its final\n                // index to the safepoint insn-index list if so.\n                if safept_idx < result.new_safepoint_insns.len()\n                    && (result.new_safepoint_insns[safept_idx].get() as usize) == i\n                {\n                    let idx = final_insns.len() - 1;\n                    final_safepoint_insns.push(idx as InsnIndex);\n                    safept_idx += 1;\n                }\n            }\n\n            let final_end = final_insns.len() as InsnIndex;\n            final_block_ranges[block as usize] = (final_start, final_end);\n        }\n\n        debug_assert!(final_insns.len() == final_srclocs.len());\n\n        self.insts = final_insns;\n        self.srclocs = final_srclocs;\n        self.block_ranges = final_block_ranges;\n        self.safepoint_insns = final_safepoint_insns;\n\n        // Save safepoint slot-lists. These will be passed to the `EmitState`\n        // for the machine backend during emission so that it can do\n        // target-specific translations of slot numbers to stack offsets.\n        self.safepoint_slots = result.stackmaps;\n    }\n\n    /// Emit the instructions to a `MachBuffer`, containing fixed-up code and external\n    /// reloc/trap/etc. records ready for use.\n    pub fn emit(\n        &self,\n    ) -> (\n        MachBuffer<I>,\n        Vec<CodeOffset>,\n        Vec<(CodeOffset, CodeOffset)>,\n    )\n    where\n        I: MachInstEmit,\n    {\n        let _tt = timing::vcode_emit();\n        let mut buffer = MachBuffer::new();\n        let mut state = I::State::new(&*self.abi);\n        let cfg_metadata = self.flags().machine_code_cfg_info();\n        let mut bb_starts: Vec<Option<CodeOffset>> = vec![];\n\n        // The first M MachLabels are reserved for block indices, the next N MachLabels for\n        // constants.\n        buffer.reserve_labels_for_blocks(self.num_blocks() as BlockIndex);\n        buffer.reserve_labels_for_constants(&self.constants);\n\n        let mut inst_end_offsets = vec![0; self.insts.len()];\n        let mut label_inst_indices = vec![0; self.num_blocks()];\n\n        // Construct the final order we emit code in: cold blocks at the end.\n        let mut final_order: SmallVec<[BlockIndex; 16]> = smallvec![];\n        let mut cold_blocks: SmallVec<[BlockIndex; 16]> = smallvec![];\n        for block in 0..self.num_blocks() {\n            let block = block as BlockIndex;\n            if self.block_order.is_cold(block) {\n                cold_blocks.push(block);\n            } else {\n                final_order.push(block);\n            }\n        }\n        let first_cold_block = cold_blocks.first().cloned();\n        final_order.extend(cold_blocks.clone());\n\n        // Emit blocks.\n        let mut safepoint_idx = 0;\n        let mut cur_srcloc = None;\n        let mut last_offset = None;\n        let mut start_of_cold_code = None;\n        for block in final_order {\n            let new_offset = I::align_basic_block(buffer.cur_offset());\n            while new_offset > buffer.cur_offset() {\n                // Pad with NOPs up to the aligned block offset.\n                let nop = I::gen_nop((new_offset - buffer.cur_offset()) as usize);\n                nop.emit(&mut buffer, &self.emit_info, &mut Default::default());\n            }\n            assert_eq!(buffer.cur_offset(), new_offset);\n\n            if Some(block) == first_cold_block {\n                start_of_cold_code = Some(buffer.cur_offset());\n            }\n\n            let (start, end) = self.block_ranges[block as usize];\n            buffer.bind_label(MachLabel::from_block(block));\n            label_inst_indices[block as usize] = start;\n\n            if cfg_metadata {\n                // Track BB starts. If we have backed up due to MachBuffer\n                // branch opts, note that the removed blocks were removed.\n                let cur_offset = buffer.cur_offset();\n                if last_offset.is_some() && cur_offset <= last_offset.unwrap() {\n                    for i in (0..bb_starts.len()).rev() {\n                        if bb_starts[i].is_some() && cur_offset > bb_starts[i].unwrap() {\n                            break;\n                        }\n                        bb_starts[i] = None;\n                    }\n                }\n                bb_starts.push(Some(cur_offset));\n                last_offset = Some(cur_offset);\n            }\n\n            for iix in start..end {\n                let srcloc = self.srclocs[iix as usize];\n                if cur_srcloc != Some(srcloc) {\n                    if cur_srcloc.is_some() {\n                        buffer.end_srcloc();\n                    }\n                    buffer.start_srcloc(srcloc);\n                    cur_srcloc = Some(srcloc);\n                }\n                state.pre_sourceloc(cur_srcloc.unwrap_or(SourceLoc::default()));\n\n                if safepoint_idx < self.safepoint_insns.len()\n                    && self.safepoint_insns[safepoint_idx] == iix\n                {\n                    if self.safepoint_slots[safepoint_idx].len() > 0 {\n                        let stack_map = self.abi.spillslots_to_stack_map(\n                            &self.safepoint_slots[safepoint_idx][..],\n                            &state,\n                        );\n                        state.pre_safepoint(stack_map);\n                    }\n                    safepoint_idx += 1;\n                }\n\n                self.insts[iix as usize].emit(&mut buffer, &self.emit_info, &mut state);\n\n                if self.generate_debug_info {\n                    // Buffer truncation may have happened since last inst append; trim inst-end\n                    // layout info as appropriate.\n                    let l = &mut inst_end_offsets[0..iix as usize];\n                    for end in l.iter_mut().rev() {\n                        if *end > buffer.cur_offset() {\n                            *end = buffer.cur_offset();\n                        } else {\n                            break;\n                        }\n                    }\n                    inst_end_offsets[iix as usize] = buffer.cur_offset();\n                }\n            }\n\n            if cur_srcloc.is_some() {\n                buffer.end_srcloc();\n                cur_srcloc = None;\n            }\n\n            // Do we need an island? Get the worst-case size of the next BB and see if, having\n            // emitted that many bytes, we will be beyond the deadline.\n            if block < (self.num_blocks() - 1) as BlockIndex {\n                let next_block = block + 1;\n                let next_block_range = self.block_ranges[next_block as usize];\n                let next_block_size = next_block_range.1 - next_block_range.0;\n                let worst_case_next_bb = I::worst_case_size() * next_block_size;\n                if buffer.island_needed(worst_case_next_bb) {\n                    buffer.emit_island(worst_case_next_bb);\n                }\n            }\n        }\n\n        // Emit the constants used by the function.\n        for (constant, data) in self.constants.iter() {\n            let label = buffer.get_label_for_constant(constant);\n            buffer.defer_constant(label, data.alignment(), data.as_slice(), u32::max_value());\n        }\n\n        if self.generate_debug_info {\n            for end in inst_end_offsets.iter_mut().rev() {\n                if *end > buffer.cur_offset() {\n                    *end = buffer.cur_offset();\n                } else {\n                    break;\n                }\n            }\n            *self.insts_layout.borrow_mut() = InstsLayoutInfo {\n                inst_end_offsets,\n                label_inst_indices,\n                start_of_cold_code,\n            };\n        }\n\n        // Create `bb_edges` and final (filtered) `bb_starts`.\n        let mut final_bb_starts = vec![];\n        let mut bb_edges = vec![];\n        if cfg_metadata {\n            for block in 0..self.num_blocks() {\n                if bb_starts[block].is_none() {\n                    // Block was deleted by MachBuffer; skip.\n                    continue;\n                }\n                let from = bb_starts[block].unwrap();\n\n                final_bb_starts.push(from);\n                // Resolve each `succ` label and add edges.\n                let succs = self.block_succs(BlockIx::new(block as u32));\n                for succ in succs.iter() {\n                    let to = buffer.resolve_label_offset(MachLabel::from_block(succ.get()));\n                    bb_edges.push((from, to));\n                }\n            }\n        }\n\n        (buffer, final_bb_starts, bb_edges)\n    }\n\n    /// Generates value-label ranges.\n    pub fn value_labels_ranges(&self) -> ValueLabelsRanges {\n        if !self.has_value_labels {\n            return ValueLabelsRanges::default();\n        }\n\n        let layout_info = &self.insts_layout.borrow();\n        debug::compute(&self.insts, &*layout_info)\n    }\n\n    /// Get the offsets of stackslots.\n    pub fn stackslot_offsets(&self) -> &PrimaryMap<StackSlot, u32> {\n        self.abi.stackslot_offsets()\n    }\n\n    /// Get the IR block for a BlockIndex, if one exists.\n    pub fn bindex_to_bb(&self, block: BlockIndex) -> Option<ir::Block> {\n        self.block_order.lowered_order()[block as usize].orig_block()\n    }\n}\n\nimpl<I: VCodeInst> RegallocFunction for VCode<I> {\n    type Inst = I;\n\n    fn insns(&self) -> &[I] {\n        &self.insts[..]\n    }\n\n    fn insns_mut(&mut self) -> &mut [I] {\n        &mut self.insts[..]\n    }\n\n    fn get_insn(&self, insn: InstIx) -> &I {\n        &self.insts[insn.get() as usize]\n    }\n\n    fn get_insn_mut(&mut self, insn: InstIx) -> &mut I {\n        &mut self.insts[insn.get() as usize]\n    }\n\n    fn blocks(&self) -> Range<BlockIx> {\n        Range::new(BlockIx::new(0), self.block_ranges.len())\n    }\n\n    fn entry_block(&self) -> BlockIx {\n        BlockIx::new(self.entry)\n    }\n\n    fn block_insns(&self, block: BlockIx) -> Range<InstIx> {\n        let (start, end) = self.block_ranges[block.get() as usize];\n        Range::new(InstIx::new(start), (end - start) as usize)\n    }\n\n    fn block_succs(&self, block: BlockIx) -> Cow<[BlockIx]> {\n        let (start, end) = self.block_succ_range[block.get() as usize];\n        Cow::Borrowed(&self.block_succs[start..end])\n    }\n\n    fn is_ret(&self, insn: InstIx) -> bool {\n        match self.insts[insn.get() as usize].is_term() {\n            MachTerminator::Ret => true,\n            _ => false,\n        }\n    }\n\n    fn is_included_in_clobbers(&self, insn: &I) -> bool {\n        insn.is_included_in_clobbers()\n    }\n\n    fn get_regs(insn: &I, collector: &mut RegUsageCollector) {\n        insn.get_regs(collector)\n    }\n\n    fn map_regs<RUM: RegUsageMapper>(insn: &mut I, mapper: &RUM) {\n        insn.map_regs(mapper);\n    }\n\n    fn is_move(&self, insn: &I) -> Option<(Writable<Reg>, Reg)> {\n        insn.is_move()\n    }\n\n    fn get_num_vregs(&self) -> usize {\n        self.vreg_types.len()\n    }\n\n    fn get_spillslot_size(&self, regclass: RegClass, _: VirtualReg) -> u32 {\n        self.abi.get_spillslot_size(regclass)\n    }\n\n    fn gen_spill(&self, to_slot: SpillSlot, from_reg: RealReg, _: Option<VirtualReg>) -> I {\n        self.abi.gen_spill(to_slot, from_reg)\n    }\n\n    fn gen_reload(\n        &self,\n        to_reg: Writable<RealReg>,\n        from_slot: SpillSlot,\n        _: Option<VirtualReg>,\n    ) -> I {\n        self.abi.gen_reload(to_reg, from_slot)\n    }\n\n    fn gen_move(&self, to_reg: Writable<RealReg>, from_reg: RealReg, vreg: VirtualReg) -> I {\n        let ty = self.vreg_type(vreg);\n        I::gen_move(to_reg.map(|r| r.to_reg()), from_reg.to_reg(), ty)\n    }\n\n    fn gen_zero_len_nop(&self) -> I {\n        I::gen_nop(0)\n    }\n\n    fn maybe_direct_reload(&self, insn: &I, reg: VirtualReg, slot: SpillSlot) -> Option<I> {\n        insn.maybe_direct_reload(reg, slot)\n    }\n\n    fn func_liveins(&self) -> RegallocSet<RealReg> {\n        self.liveins.clone()\n    }\n\n    fn func_liveouts(&self) -> RegallocSet<RealReg> {\n        self.liveouts.clone()\n    }\n}\n\nimpl<I: VCodeInst> fmt::Debug for VCode<I> {\n    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {\n        writeln!(f, \"VCode_Debug {{\")?;\n        writeln!(f, \"  Entry block: {}\", self.entry)?;\n\n        for block in 0..self.num_blocks() {\n            writeln!(f, \"Block {}:\", block,)?;\n            for succ in self.succs(block as BlockIndex) {\n                writeln!(f, \"  (successor: Block {})\", succ.get())?;\n            }\n            let (start, end) = self.block_ranges[block];\n            writeln!(f, \"  (instruction range: {} .. {})\", start, end)?;\n            for inst in start..end {\n                writeln!(f, \"  Inst {}: {:?}\", inst, self.insts[inst as usize])?;\n            }\n        }\n\n        writeln!(f, \"}}\")?;\n        Ok(())\n    }\n}\n\n/// Pretty-printing with `RealRegUniverse` context.\nimpl<I: VCodeInst> PrettyPrint for VCode<I> {\n    fn show_rru(&self, mb_rru: Option<&RealRegUniverse>) -> String {\n        use std::fmt::Write;\n\n        let mut s = String::new();\n        write!(&mut s, \"VCode_ShowWithRRU {{{{\\n\").unwrap();\n        write!(&mut s, \"  Entry block: {}\\n\", self.entry).unwrap();\n\n        let mut state = Default::default();\n        let mut safepoint_idx = 0;\n        for i in 0..self.num_blocks() {\n            let block = i as BlockIndex;\n\n            write!(&mut s, \"Block {}:\\n\", block).unwrap();\n            if let Some(bb) = self.bindex_to_bb(block) {\n                write!(&mut s, \"  (original IR block: {})\\n\", bb).unwrap();\n            }\n            for succ in self.succs(block) {\n                write!(&mut s, \"  (successor: Block {})\\n\", succ.get()).unwrap();\n            }\n            let (start, end) = self.block_ranges[block as usize];\n            write!(&mut s, \"  (instruction range: {} .. {})\\n\", start, end).unwrap();\n            for inst in start..end {\n                if safepoint_idx < self.safepoint_insns.len()\n                    && self.safepoint_insns[safepoint_idx] == inst\n                {\n                    write!(\n                        &mut s,\n                        \"      (safepoint: slots {:?} with EmitState {:?})\\n\",\n                        self.safepoint_slots[safepoint_idx], state,\n                    )\n                    .unwrap();\n                    safepoint_idx += 1;\n                }\n                write!(\n                    &mut s,\n                    \"  Inst {}:   {}\\n\",\n                    inst,\n                    self.insts[inst as usize].pretty_print(mb_rru, &mut state)\n                )\n                .unwrap();\n            }\n        }\n\n        write!(&mut s, \"}}}}\\n\").unwrap();\n\n        s\n    }\n}\n\n/// This structure tracks the large constants used in VCode that will be emitted separately by the\n/// [MachBuffer].\n///\n/// First, during the lowering phase, constants are inserted using\n/// [VCodeConstants.insert]; an intermediate handle, [VCodeConstant], tracks what constants are\n/// used in this phase. Some deduplication is performed, when possible, as constant\n/// values are inserted.\n///\n/// Secondly, during the emission phase, the [MachBuffer] assigns [MachLabel]s for each of the\n/// constants so that instructions can refer to the value's memory location. The [MachBuffer]\n/// then writes the constant values to the buffer.\n#[derive(Default)]\npub struct VCodeConstants {\n    constants: PrimaryMap<VCodeConstant, VCodeConstantData>,\n    pool_uses: HashMap<Constant, VCodeConstant>,\n    well_known_uses: HashMap<*const [u8], VCodeConstant>,\n}\nimpl VCodeConstants {\n    /// Initialize the structure with the expected number of constants.\n    pub fn with_capacity(expected_num_constants: usize) -> Self {\n        Self {\n            constants: PrimaryMap::with_capacity(expected_num_constants),\n            pool_uses: HashMap::with_capacity(expected_num_constants),\n            well_known_uses: HashMap::new(),\n        }\n    }\n\n    /// Insert a constant; using this method indicates that a constant value will be used and thus\n    /// will be emitted to the `MachBuffer`. The current implementation can deduplicate constants\n    /// that are [VCodeConstantData::Pool] or [VCodeConstantData::WellKnown] but not\n    /// [VCodeConstantData::Generated].\n    pub fn insert(&mut self, data: VCodeConstantData) -> VCodeConstant {\n        match data {\n            VCodeConstantData::Generated(_) => self.constants.push(data),\n            VCodeConstantData::Pool(constant, _) => match self.pool_uses.get(&constant) {\n                None => {\n                    let vcode_constant = self.constants.push(data);\n                    self.pool_uses.insert(constant, vcode_constant);\n                    vcode_constant\n                }\n                Some(&vcode_constant) => vcode_constant,\n            },\n            VCodeConstantData::WellKnown(data_ref) => {\n                match self.well_known_uses.get(&(data_ref as *const [u8])) {\n                    None => {\n                        let vcode_constant = self.constants.push(data);\n                        self.well_known_uses\n                            .insert(data_ref as *const [u8], vcode_constant);\n                        vcode_constant\n                    }\n                    Some(&vcode_constant) => vcode_constant,\n                }\n            }\n        }\n    }\n\n    /// Return the number of constants inserted.\n    pub fn len(&self) -> usize {\n        self.constants.len()\n    }\n\n    /// Iterate over the [VCodeConstant] keys inserted in this structure.\n    pub fn keys(&self) -> Keys<VCodeConstant> {\n        self.constants.keys()\n    }\n\n    /// Iterate over the [VCodeConstant] keys and the data (as a byte slice) inserted in this\n    /// structure.\n    pub fn iter(&self) -> impl Iterator<Item = (VCodeConstant, &VCodeConstantData)> {\n        self.constants.iter()\n    }\n}\n\n/// A use of a constant by one or more VCode instructions; see [VCodeConstants].\n#[derive(Clone, Copy, Debug, PartialEq, Eq)]\npub struct VCodeConstant(u32);\nentity_impl!(VCodeConstant);\n\n/// Identify the different types of constant that can be inserted into [VCodeConstants]. Tracking\n/// these separately instead of as raw byte buffers allows us to avoid some duplication.\npub enum VCodeConstantData {\n    /// A constant already present in the Cranelift IR\n    /// [ConstantPool](crate::ir::constant::ConstantPool).\n    Pool(Constant, ConstantData),\n    /// A reference to a well-known constant value that is statically encoded within the compiler.\n    WellKnown(&'static [u8]),\n    /// A constant value generated during lowering; the value may depend on the instruction context\n    /// which makes it difficult to de-duplicate--if possible, use other variants.\n    Generated(ConstantData),\n}\nimpl VCodeConstantData {\n    /// Retrieve the constant data as a byte slice.\n    pub fn as_slice(&self) -> &[u8] {\n        match self {\n            VCodeConstantData::Pool(_, d) | VCodeConstantData::Generated(d) => d.as_slice(),\n            VCodeConstantData::WellKnown(d) => d,\n        }\n    }\n\n    /// Calculate the alignment of the constant data.\n    pub fn alignment(&self) -> u32 {\n        if self.as_slice().len() <= 8 {\n            8\n        } else {\n            16\n        }\n    }\n}\n\n#[cfg(test)]\nmod test {\n    use super::*;\n    use std::mem::size_of;\n\n    #[test]\n    fn size_of_constant_structs() {\n        assert_eq!(size_of::<Constant>(), 4);\n        assert_eq!(size_of::<VCodeConstant>(), 4);\n        assert_eq!(size_of::<ConstantData>(), 24);\n        assert_eq!(size_of::<VCodeConstantData>(), 32);\n        assert_eq!(\n            size_of::<PrimaryMap<VCodeConstant, VCodeConstantData>>(),\n            24\n        );\n        // TODO The VCodeConstants structure's memory size could be further optimized.\n        // With certain versions of Rust, each `HashMap` in `VCodeConstants` occupied at\n        // least 48 bytes, making an empty `VCodeConstants` cost 120 bytes.\n    }\n}\n", "use super::ref_types_module;\nuse std::sync::atomic::{AtomicBool, Ordering::SeqCst};\nuse std::sync::Arc;\nuse wasmtime::*;\n\n#[test]\nfn pass_funcref_in_and_out_of_wasm() -> anyhow::Result<()> {\n    let (mut store, module) = ref_types_module(\n        r#\"\n            (module\n                (func (export \"func\") (param funcref) (result funcref)\n                    local.get 0\n                )\n            )\n        \"#,\n    )?;\n\n    let instance = Instance::new(&mut store, &module, &[])?;\n    let func = instance.get_func(&mut store, \"func\").unwrap();\n\n    // Pass in a non-null funcref.\n    {\n        let mut results = [Val::I32(0)];\n        func.call(\n            &mut store,\n            &[Val::FuncRef(Some(func.clone()))],\n            &mut results,\n        )?;\n\n        // Can't compare `Func` for equality, so this is the best we can do here.\n        let result_func = results[0].unwrap_funcref().unwrap();\n        assert_eq!(func.ty(&store), result_func.ty(&store));\n    }\n\n    // Pass in a null funcref.\n    {\n        let mut results = [Val::I32(0)];\n        func.call(&mut store, &[Val::FuncRef(None)], &mut results)?;\n        let result_func = results[0].unwrap_funcref();\n        assert!(result_func.is_none());\n    }\n\n    // Pass in a `funcref` from another instance.\n    {\n        let other_instance = Instance::new(&mut store, &module, &[])?;\n        let other_instance_func = other_instance.get_func(&mut store, \"func\").unwrap();\n\n        let mut results = [Val::I32(0)];\n        func.call(\n            &mut store,\n            &[Val::FuncRef(Some(other_instance_func.clone()))],\n            &mut results,\n        )?;\n        assert_eq!(results.len(), 1);\n\n        // Can't compare `Func` for equality, so this is the best we can do here.\n        let result_func = results[0].unwrap_funcref().unwrap();\n        assert_eq!(other_instance_func.ty(&store), result_func.ty(&store));\n    }\n\n    // Passing in a `funcref` from another store fails.\n    {\n        let (mut other_store, other_module) = ref_types_module(r#\"(module (func (export \"f\")))\"#)?;\n        let other_store_instance = Instance::new(&mut other_store, &other_module, &[])?;\n        let f = other_store_instance\n            .get_func(&mut other_store, \"f\")\n            .unwrap();\n\n        assert!(func\n            .call(&mut store, &[Val::FuncRef(Some(f))], &mut [Val::I32(0)])\n            .is_err());\n    }\n\n    Ok(())\n}\n\n#[test]\nfn receive_null_funcref_from_wasm() -> anyhow::Result<()> {\n    let (mut store, module) = ref_types_module(\n        r#\"\n            (module\n                (func (export \"get-null\") (result funcref)\n                    ref.null func\n                )\n            )\n        \"#,\n    )?;\n\n    let instance = Instance::new(&mut store, &module, &[])?;\n    let get_null = instance.get_func(&mut store, \"get-null\").unwrap();\n\n    let mut results = [Val::I32(0)];\n    get_null.call(&mut store, &[], &mut results)?;\n    let result_func = results[0].unwrap_funcref();\n    assert!(result_func.is_none());\n\n    Ok(())\n}\n\n#[test]\nfn wrong_store() -> anyhow::Result<()> {\n    let dropped = Arc::new(AtomicBool::new(false));\n    {\n        let mut store1 = Store::<()>::default();\n        let mut store2 = Store::<()>::default();\n\n        let set = SetOnDrop(dropped.clone());\n        let f1 = Func::wrap(&mut store1, move || drop(&set));\n        let f2 = Func::wrap(&mut store2, move || Some(f1.clone()));\n        assert!(f2.call(&mut store2, &[], &mut []).is_err());\n    }\n    assert!(dropped.load(SeqCst));\n\n    return Ok(());\n\n    struct SetOnDrop(Arc<AtomicBool>);\n\n    impl Drop for SetOnDrop {\n        fn drop(&mut self) {\n            self.0.store(true, SeqCst);\n        }\n    }\n}\n\n#[test]\nfn func_new_returns_wrong_store() -> anyhow::Result<()> {\n    let dropped = Arc::new(AtomicBool::new(false));\n    {\n        let mut store1 = Store::<()>::default();\n        let mut store2 = Store::<()>::default();\n\n        let set = SetOnDrop(dropped.clone());\n        let f1 = Func::wrap(&mut store1, move || drop(&set));\n        let f2 = Func::new(\n            &mut store2,\n            FuncType::new(None, Some(ValType::FuncRef)),\n            move |_, _, results| {\n                results[0] = f1.clone().into();\n                Ok(())\n            },\n        );\n        assert!(f2.call(&mut store2, &[], &mut [Val::I32(0)]).is_err());\n    }\n    assert!(dropped.load(SeqCst));\n\n    return Ok(());\n\n    struct SetOnDrop(Arc<AtomicBool>);\n\n    impl Drop for SetOnDrop {\n        fn drop(&mut self) {\n            self.0.store(true, SeqCst);\n        }\n    }\n}\n", "use super::ref_types_module;\nuse super::skip_pooling_allocator_tests;\nuse std::sync::atomic::{AtomicBool, AtomicUsize, Ordering::SeqCst};\nuse std::sync::Arc;\nuse wasmtime::*;\n\nstruct SetFlagOnDrop(Arc<AtomicBool>);\n\nimpl Drop for SetFlagOnDrop {\n    fn drop(&mut self) {\n        self.0.store(true, SeqCst);\n    }\n}\n\n#[test]\nfn smoke_test_gc() -> anyhow::Result<()> {\n    let (mut store, module) = ref_types_module(\n        r#\"\n            (module\n                (import \"\" \"\" (func $do_gc))\n                (func $recursive (export \"func\") (param i32 externref) (result externref)\n                    local.get 0\n                    i32.eqz\n                    if (result externref)\n                        call $do_gc\n                        local.get 1\n                    else\n                        local.get 0\n                        i32.const 1\n                        i32.sub\n                        local.get 1\n                        call $recursive\n                    end\n                )\n            )\n        \"#,\n    )?;\n\n    let do_gc = Func::wrap(&mut store, |mut caller: Caller<'_, _>| {\n        // Do a GC with `externref`s on the stack in Wasm frames.\n        caller.gc();\n    });\n    let instance = Instance::new(&mut store, &module, &[do_gc.into()])?;\n    let func = instance.get_func(&mut store, \"func\").unwrap();\n\n    let inner_dropped = Arc::new(AtomicBool::new(false));\n    let r = ExternRef::new(SetFlagOnDrop(inner_dropped.clone()));\n    {\n        let args = [Val::I32(5), Val::ExternRef(Some(r.clone()))];\n        func.call(&mut store, &args, &mut [Val::I32(0)])?;\n    }\n\n    // Still held alive by the `VMExternRefActivationsTable` (potentially in\n    // multiple slots within the table) and by this `r` local.\n    assert!(r.strong_count() >= 2);\n\n    // Doing a GC should see that there aren't any `externref`s on the stack in\n    // Wasm frames anymore.\n    store.gc();\n    assert_eq!(r.strong_count(), 1);\n\n    // Dropping `r` should drop the inner `SetFlagOnDrop` value.\n    drop(r);\n    assert!(inner_dropped.load(SeqCst));\n\n    Ok(())\n}\n\n#[test]\nfn wasm_dropping_refs() -> anyhow::Result<()> {\n    let (mut store, module) = ref_types_module(\n        r#\"\n            (module\n                (func (export \"drop_ref\") (param externref)\n                    nop\n                )\n            )\n        \"#,\n    )?;\n\n    let instance = Instance::new(&mut store, &module, &[])?;\n    let drop_ref = instance.get_func(&mut store, \"drop_ref\").unwrap();\n\n    let num_refs_dropped = Arc::new(AtomicUsize::new(0));\n\n    // NB: 4096 is greater than the initial `VMExternRefActivationsTable`\n    // capacity, so this will trigger at least one GC.\n    for _ in 0..4096 {\n        let r = ExternRef::new(CountDrops(num_refs_dropped.clone()));\n        let args = [Val::ExternRef(Some(r))];\n        drop_ref.call(&mut store, &args, &mut [])?;\n    }\n\n    assert!(num_refs_dropped.load(SeqCst) > 0);\n\n    // And after doing a final GC, all the refs should have been dropped.\n    store.gc();\n    assert_eq!(num_refs_dropped.load(SeqCst), 4096);\n\n    return Ok(());\n\n    struct CountDrops(Arc<AtomicUsize>);\n\n    impl Drop for CountDrops {\n        fn drop(&mut self) {\n            self.0.fetch_add(1, SeqCst);\n        }\n    }\n}\n\n#[test]\nfn many_live_refs() -> anyhow::Result<()> {\n    let mut wat = r#\"\n        (module\n            ;; Make new `externref`s.\n            (import \"\" \"make_ref\" (func $make_ref (result externref)))\n\n            ;; Observe an `externref` so it is kept live.\n            (import \"\" \"observe_ref\" (func $observe_ref (param externref)))\n\n            (func (export \"many_live_refs\")\n    \"#\n    .to_string();\n\n    // This is more than the initial `VMExternRefActivationsTable` capacity, so\n    // it will need to allocate additional bump chunks.\n    const NUM_LIVE_REFS: usize = 1024;\n\n    // Push `externref`s onto the stack.\n    for _ in 0..NUM_LIVE_REFS {\n        wat.push_str(\"(call $make_ref)\\n\");\n    }\n\n    // Pop `externref`s from the stack. Because we pass each of them to a\n    // function call here, they are all live references for the duration of\n    // their lifetimes.\n    for _ in 0..NUM_LIVE_REFS {\n        wat.push_str(\"(call $observe_ref)\\n\");\n    }\n\n    wat.push_str(\n        \"\n            ) ;; func\n        ) ;; module\n        \",\n    );\n\n    let (mut store, module) = ref_types_module(&wat)?;\n\n    let live_refs = Arc::new(AtomicUsize::new(0));\n\n    let make_ref = Func::wrap(&mut store, {\n        let live_refs = live_refs.clone();\n        move || Some(ExternRef::new(CountLiveRefs::new(live_refs.clone())))\n    });\n\n    let observe_ref = Func::wrap(&mut store, |r: Option<ExternRef>| {\n        let r = r.unwrap();\n        let r = r.data().downcast_ref::<CountLiveRefs>().unwrap();\n        assert!(r.live_refs.load(SeqCst) > 0);\n    });\n\n    let instance = Instance::new(&mut store, &module, &[make_ref.into(), observe_ref.into()])?;\n    let many_live_refs = instance.get_func(&mut store, \"many_live_refs\").unwrap();\n\n    many_live_refs.call(&mut store, &[], &mut [])?;\n\n    store.gc();\n    assert_eq!(live_refs.load(SeqCst), 0);\n\n    return Ok(());\n\n    struct CountLiveRefs {\n        live_refs: Arc<AtomicUsize>,\n    }\n\n    impl CountLiveRefs {\n        fn new(live_refs: Arc<AtomicUsize>) -> Self {\n            live_refs.fetch_add(1, SeqCst);\n            Self { live_refs }\n        }\n    }\n\n    impl Drop for CountLiveRefs {\n        fn drop(&mut self) {\n            self.live_refs.fetch_sub(1, SeqCst);\n        }\n    }\n}\n\n#[test]\nfn drop_externref_via_table_set() -> anyhow::Result<()> {\n    let (mut store, module) = ref_types_module(\n        r#\"\n            (module\n                (table $t 1 externref)\n\n                (func (export \"table-set\") (param externref)\n                  (table.set $t (i32.const 0) (local.get 0))\n                )\n            )\n        \"#,\n    )?;\n\n    let instance = Instance::new(&mut store, &module, &[])?;\n    let table_set = instance.get_func(&mut store, \"table-set\").unwrap();\n\n    let foo_is_dropped = Arc::new(AtomicBool::new(false));\n    let bar_is_dropped = Arc::new(AtomicBool::new(false));\n\n    let foo = ExternRef::new(SetFlagOnDrop(foo_is_dropped.clone()));\n    let bar = ExternRef::new(SetFlagOnDrop(bar_is_dropped.clone()));\n\n    {\n        let args = vec![Val::ExternRef(Some(foo))];\n        table_set.call(&mut store, &args, &mut [])?;\n    }\n    store.gc();\n    assert!(!foo_is_dropped.load(SeqCst));\n    assert!(!bar_is_dropped.load(SeqCst));\n\n    {\n        let args = vec![Val::ExternRef(Some(bar))];\n        table_set.call(&mut store, &args, &mut [])?;\n    }\n    store.gc();\n    assert!(foo_is_dropped.load(SeqCst));\n    assert!(!bar_is_dropped.load(SeqCst));\n\n    table_set.call(&mut store, &[Val::ExternRef(None)], &mut [])?;\n    assert!(foo_is_dropped.load(SeqCst));\n    assert!(bar_is_dropped.load(SeqCst));\n\n    Ok(())\n}\n\n#[test]\nfn global_drops_externref() -> anyhow::Result<()> {\n    test_engine(&Engine::default())?;\n\n    if !skip_pooling_allocator_tests() {\n        test_engine(&Engine::new(\n            Config::new().allocation_strategy(InstanceAllocationStrategy::pooling()),\n        )?)?;\n    }\n\n    return Ok(());\n\n    fn test_engine(engine: &Engine) -> anyhow::Result<()> {\n        let mut store = Store::new(&engine, ());\n        let flag = Arc::new(AtomicBool::new(false));\n        let externref = ExternRef::new(SetFlagOnDrop(flag.clone()));\n        Global::new(\n            &mut store,\n            GlobalType::new(ValType::ExternRef, Mutability::Const),\n            externref.into(),\n        )?;\n        drop(store);\n        assert!(flag.load(SeqCst));\n\n        let mut store = Store::new(&engine, ());\n        let module = Module::new(\n            &engine,\n            r#\"\n                (module\n                    (global (mut externref) (ref.null extern))\n\n                    (func (export \"run\") (param externref)\n                        local.get 0\n                        global.set 0\n                    )\n                )\n            \"#,\n        )?;\n        let instance = Instance::new(&mut store, &module, &[])?;\n        let run = instance.get_typed_func::<Option<ExternRef>, (), _>(&mut store, \"run\")?;\n        let flag = Arc::new(AtomicBool::new(false));\n        let externref = ExternRef::new(SetFlagOnDrop(flag.clone()));\n        run.call(&mut store, Some(externref))?;\n        drop(store);\n        assert!(flag.load(SeqCst));\n        Ok(())\n    }\n}\n\n#[test]\nfn table_drops_externref() -> anyhow::Result<()> {\n    test_engine(&Engine::default())?;\n\n    if !skip_pooling_allocator_tests() {\n        test_engine(&Engine::new(\n            Config::new().allocation_strategy(InstanceAllocationStrategy::pooling()),\n        )?)?;\n    }\n\n    return Ok(());\n\n    fn test_engine(engine: &Engine) -> anyhow::Result<()> {\n        let mut store = Store::new(&engine, ());\n        let flag = Arc::new(AtomicBool::new(false));\n        let externref = ExternRef::new(SetFlagOnDrop(flag.clone()));\n        Table::new(\n            &mut store,\n            TableType::new(ValType::ExternRef, 1, None),\n            externref.into(),\n        )?;\n        drop(store);\n        assert!(flag.load(SeqCst));\n\n        let mut store = Store::new(&engine, ());\n        let module = Module::new(\n            &engine,\n            r#\"\n            (module\n                (table 1 externref)\n\n                (func (export \"run\") (param externref)\n                    i32.const 0\n                    local.get 0\n                    table.set 0\n                )\n            )\n        \"#,\n        )?;\n        let instance = Instance::new(&mut store, &module, &[])?;\n        let run = instance.get_typed_func::<Option<ExternRef>, (), _>(&mut store, \"run\")?;\n        let flag = Arc::new(AtomicBool::new(false));\n        let externref = ExternRef::new(SetFlagOnDrop(flag.clone()));\n        run.call(&mut store, Some(externref))?;\n        drop(store);\n        assert!(flag.load(SeqCst));\n        Ok(())\n    }\n}\n\n#[test]\nfn gee_i_sure_hope_refcounting_is_atomic() -> anyhow::Result<()> {\n    let mut config = Config::new();\n    config.wasm_reference_types(true);\n    config.epoch_interruption(true);\n    let engine = Engine::new(&config)?;\n    let mut store = Store::new(&engine, ());\n    let module = Module::new(\n        &engine,\n        r#\"\n            (module\n                (global (mut externref) (ref.null extern))\n                (table 1 externref)\n\n                (func (export \"run\") (param externref)\n                    local.get 0\n                    global.set 0\n                    i32.const 0\n                    local.get 0\n                    table.set 0\n                    loop\n                        global.get 0\n                        global.set 0\n\n                        i32.const 0\n                        i32.const 0\n                        table.get\n                        table.set\n\n                        local.get 0\n                        call $f\n\n                        br 0\n                    end\n                )\n\n                (func $f (param externref))\n            )\n        \"#,\n    )?;\n\n    let instance = Instance::new(&mut store, &module, &[])?;\n    let run = instance.get_typed_func::<Option<ExternRef>, (), _>(&mut store, \"run\")?;\n\n    let flag = Arc::new(AtomicBool::new(false));\n    let externref = ExternRef::new(SetFlagOnDrop(flag.clone()));\n    let externref2 = externref.clone();\n\n    let child = std::thread::spawn(move || run.call(&mut store, Some(externref2)));\n\n    for _ in 0..10000 {\n        drop(externref.clone());\n    }\n    engine.increment_epoch();\n\n    assert!(child.join().unwrap().is_err());\n    assert!(!flag.load(SeqCst));\n    assert_eq!(externref.strong_count(), 1);\n    drop(externref);\n    assert!(flag.load(SeqCst));\n\n    Ok(())\n}\n\n#[test]\nfn global_init_no_leak() -> anyhow::Result<()> {\n    let (mut store, module) = ref_types_module(\n        r#\"\n            (module\n                (import \"\" \"\" (global externref))\n                (global externref (global.get 0))\n            )\n        \"#,\n    )?;\n\n    let externref = ExternRef::new(());\n    let global = Global::new(\n        &mut store,\n        GlobalType::new(ValType::ExternRef, Mutability::Const),\n        externref.clone().into(),\n    )?;\n    Instance::new(&mut store, &module, &[global.into()])?;\n    drop(store);\n    assert_eq!(externref.strong_count(), 1);\n\n    Ok(())\n}\n\n#[test]\nfn no_gc_middle_of_args() -> anyhow::Result<()> {\n    let (mut store, module) = ref_types_module(\n        r#\"\n            (module\n                (import \"\" \"return_some\" (func $return (result externref externref externref)))\n                (import \"\" \"take_some\" (func $take (param externref externref externref)))\n                (func (export \"run\")\n                    (local i32)\n                    i32.const 1000\n                    local.set 0\n                    loop\n                        call $return\n                        call $take\n                        local.get 0\n                        i32.const -1\n                        i32.add\n                        local.tee 0\n                        br_if 0\n                    end\n                )\n            )\n        \"#,\n    )?;\n\n    let mut linker = Linker::new(store.engine());\n    linker.func_wrap(\"\", \"return_some\", || {\n        (\n            Some(ExternRef::new(\"a\".to_string())),\n            Some(ExternRef::new(\"b\".to_string())),\n            Some(ExternRef::new(\"c\".to_string())),\n        )\n    })?;\n    linker.func_wrap(\n        \"\",\n        \"take_some\",\n        |a: Option<ExternRef>, b: Option<ExternRef>, c: Option<ExternRef>| {\n            let a = a.unwrap();\n            let b = b.unwrap();\n            let c = c.unwrap();\n            assert_eq!(a.data().downcast_ref::<String>().unwrap(), \"a\");\n            assert_eq!(b.data().downcast_ref::<String>().unwrap(), \"b\");\n            assert_eq!(c.data().downcast_ref::<String>().unwrap(), \"c\");\n        },\n    )?;\n\n    let instance = linker.instantiate(&mut store, &module)?;\n    let func = instance.get_typed_func::<(), (), _>(&mut store, \"run\")?;\n    func.call(&mut store, ())?;\n\n    Ok(())\n}\n", "mod async_functions;\nmod call_hook;\nmod cli_tests;\nmod custom_signal_handler;\nmod debug;\nmod epoch_interruption;\nmod externals;\nmod fuel;\nmod func;\nmod funcref;\nmod gc;\nmod globals;\nmod host_funcs;\nmod iloop;\nmod import_calling_export;\nmod import_indexes;\nmod instance;\nmod invoke_func_via_table;\nmod limits;\nmod linker;\nmod memory;\nmod memory_creator;\nmod module;\nmod module_serialize;\nmod name;\nmod pooling_allocator;\nmod relocs;\nmod stack_overflow;\nmod store;\nmod table;\nmod traps;\nmod wast;\n\n/// A helper to compile a module in a new store with reference types enabled.\npub(crate) fn ref_types_module(\n    source: &str,\n) -> anyhow::Result<(wasmtime::Store<()>, wasmtime::Module)> {\n    use wasmtime::*;\n\n    let _ = env_logger::try_init();\n\n    let mut config = Config::new();\n    config.wasm_reference_types(true);\n\n    let engine = Engine::new(&config)?;\n    let store = Store::new(&engine, ());\n\n    let module = Module::new(&engine, source)?;\n\n    Ok((store, module))\n}\n\n/// A helper determining whether the pooling allocator tests should be skipped.\npub(crate) fn skip_pooling_allocator_tests() -> bool {\n    // There are a couple of issues when running the pooling allocator tests under QEMU:\n    // - high memory usage that may exceed the limits imposed by the environment (e.g. CI)\n    // - https://github.com/bytecodealliance/wasmtime/pull/2518#issuecomment-747280133\n    std::env::var(\"WASMTIME_TEST_NO_HOG_MEMORY\").is_ok()\n}\n"], "fixing_code": ["//! This implements the VCode container: a CFG of Insts that have been lowered.\n//!\n//! VCode is virtual-register code. An instruction in VCode is almost a machine\n//! instruction; however, its register slots can refer to virtual registers in\n//! addition to real machine registers.\n//!\n//! VCode is structured with traditional basic blocks, and\n//! each block must be terminated by an unconditional branch (one target), a\n//! conditional branch (two targets), or a return (no targets). Note that this\n//! slightly differs from the machine code of most ISAs: in most ISAs, a\n//! conditional branch has one target (and the not-taken case falls through).\n//! However, we expect that machine backends will elide branches to the following\n//! block (i.e., zero-offset jumps), and will be able to codegen a branch-cond /\n//! branch-uncond pair if *both* targets are not fallthrough. This allows us to\n//! play with layout prior to final binary emission, as well, if we want.\n//!\n//! See the main module comment in `mod.rs` for more details on the VCode-based\n//! backend pipeline.\n\nuse crate::fx::FxHashMap;\nuse crate::ir::{self, types, Constant, ConstantData, SourceLoc};\nuse crate::machinst::*;\nuse crate::settings;\nuse crate::timing;\nuse regalloc::Function as RegallocFunction;\nuse regalloc::Set as RegallocSet;\nuse regalloc::{\n    BlockIx, InstIx, PrettyPrint, Range, RegAllocResult, RegClass, RegUsageCollector,\n    RegUsageMapper, SpillSlot, StackmapRequestInfo,\n};\n\nuse alloc::boxed::Box;\nuse alloc::{borrow::Cow, vec::Vec};\nuse cranelift_entity::{entity_impl, Keys, PrimaryMap};\nuse std::cell::RefCell;\nuse std::collections::HashMap;\nuse std::fmt;\nuse std::iter;\nuse std::string::String;\n\n/// Index referring to an instruction in VCode.\npub type InsnIndex = u32;\n/// Index referring to a basic block in VCode.\npub type BlockIndex = u32;\n\n/// VCodeInst wraps all requirements for a MachInst to be in VCode: it must be\n/// a `MachInst` and it must be able to emit itself at least to a `SizeCodeSink`.\npub trait VCodeInst: MachInst + MachInstEmit {}\nimpl<I: MachInst + MachInstEmit> VCodeInst for I {}\n\n/// A function in \"VCode\" (virtualized-register code) form, after lowering.\n/// This is essentially a standard CFG of basic blocks, where each basic block\n/// consists of lowered instructions produced by the machine-specific backend.\npub struct VCode<I: VCodeInst> {\n    /// Function liveins.\n    liveins: RegallocSet<RealReg>,\n\n    /// Function liveouts.\n    liveouts: RegallocSet<RealReg>,\n\n    /// VReg IR-level types.\n    vreg_types: Vec<Type>,\n\n    /// Do we have any ref values among our vregs?\n    have_ref_values: bool,\n\n    /// Lowered machine instructions in order corresponding to the original IR.\n    insts: Vec<I>,\n\n    /// Source locations for each instruction. (`SourceLoc` is a `u32`, so it is\n    /// reasonable to keep one of these per instruction.)\n    srclocs: Vec<SourceLoc>,\n\n    /// Entry block.\n    entry: BlockIndex,\n\n    /// Block instruction indices.\n    block_ranges: Vec<(InsnIndex, InsnIndex)>,\n\n    /// Block successors: index range in the successor-list below.\n    block_succ_range: Vec<(usize, usize)>,\n\n    /// Block successor lists, concatenated into one Vec. The `block_succ_range`\n    /// list of tuples above gives (start, end) ranges within this list that\n    /// correspond to each basic block's successors.\n    block_succs: Vec<BlockIx>,\n\n    /// Block-order information.\n    block_order: BlockLoweringOrder,\n\n    /// ABI object.\n    abi: Box<dyn ABICallee<I = I>>,\n\n    /// Constant information used during code emission. This should be\n    /// immutable across function compilations within the same module.\n    emit_info: I::Info,\n\n    /// Safepoint instruction indices. Filled in post-regalloc. (Prior to\n    /// regalloc, the safepoint instructions are listed in the separate\n    /// `StackmapRequestInfo` held separate from the `VCode`.)\n    safepoint_insns: Vec<InsnIndex>,\n\n    /// For each safepoint entry in `safepoint_insns`, a list of `SpillSlot`s.\n    /// These are used to generate actual stack maps at emission. Filled in\n    /// post-regalloc.\n    safepoint_slots: Vec<Vec<SpillSlot>>,\n\n    /// Do we generate debug info?\n    generate_debug_info: bool,\n\n    /// Instruction end offsets, instruction indices at each label,\n    /// total buffer size, and start of cold code.  Only present if\n    /// `generate_debug_info` is set.\n    insts_layout: RefCell<InstsLayoutInfo>,\n\n    /// Constants.\n    constants: VCodeConstants,\n\n    /// Are any debug value-labels present? If not, we can skip the\n    /// post-emission analysis.\n    has_value_labels: bool,\n}\n\n#[derive(Debug, Default)]\npub(crate) struct InstsLayoutInfo {\n    pub(crate) inst_end_offsets: Vec<CodeOffset>,\n    pub(crate) label_inst_indices: Vec<CodeOffset>,\n    pub(crate) start_of_cold_code: Option<CodeOffset>,\n}\n\n/// A builder for a VCode function body. This builder is designed for the\n/// lowering approach that we take: we traverse basic blocks in forward\n/// (original IR) order, but within each basic block, we generate code from\n/// bottom to top; and within each IR instruction that we visit in this reverse\n/// order, we emit machine instructions in *forward* order again.\n///\n/// Hence, to produce the final instructions in proper order, we perform two\n/// swaps.  First, the machine instructions (`I` instances) are produced in\n/// forward order for an individual IR instruction. Then these are *reversed*\n/// and concatenated to `bb_insns` at the end of the IR instruction lowering.\n/// The `bb_insns` vec will thus contain all machine instructions for a basic\n/// block, in reverse order. Finally, when we're done with a basic block, we\n/// reverse the whole block's vec of instructions again, and concatenate onto\n/// the VCode's insts.\npub struct VCodeBuilder<I: VCodeInst> {\n    /// In-progress VCode.\n    vcode: VCode<I>,\n\n    /// In-progress stack map-request info.\n    stack_map_info: StackmapRequestInfo,\n\n    /// Index of the last block-start in the vcode.\n    block_start: InsnIndex,\n\n    /// Start of succs for the current block in the concatenated succs list.\n    succ_start: usize,\n\n    /// Current source location.\n    cur_srcloc: SourceLoc,\n}\n\nimpl<I: VCodeInst> VCodeBuilder<I> {\n    /// Create a new VCodeBuilder.\n    pub fn new(\n        abi: Box<dyn ABICallee<I = I>>,\n        emit_info: I::Info,\n        block_order: BlockLoweringOrder,\n        constants: VCodeConstants,\n    ) -> VCodeBuilder<I> {\n        let reftype_class = I::ref_type_regclass(abi.flags());\n        let vcode = VCode::new(\n            abi,\n            emit_info,\n            block_order,\n            constants,\n            /* generate_debug_info = */ true,\n        );\n        let stack_map_info = StackmapRequestInfo {\n            reftype_class,\n            reftyped_vregs: vec![],\n            safepoint_insns: vec![],\n        };\n\n        VCodeBuilder {\n            vcode,\n            stack_map_info,\n            block_start: 0,\n            succ_start: 0,\n            cur_srcloc: SourceLoc::default(),\n        }\n    }\n\n    /// Access the ABI object.\n    pub fn abi(&mut self) -> &mut dyn ABICallee<I = I> {\n        &mut *self.vcode.abi\n    }\n\n    /// Access to the BlockLoweringOrder object.\n    pub fn block_order(&self) -> &BlockLoweringOrder {\n        &self.vcode.block_order\n    }\n\n    /// Set the type of a VReg.\n    pub fn set_vreg_type(&mut self, vreg: VirtualReg, ty: Type) {\n        if self.vcode.vreg_types.len() <= vreg.get_index() {\n            self.vcode\n                .vreg_types\n                .resize(vreg.get_index() + 1, ir::types::I8);\n        }\n        self.vcode.vreg_types[vreg.get_index()] = ty;\n        if is_reftype(ty) {\n            self.stack_map_info.reftyped_vregs.push(vreg);\n            self.vcode.have_ref_values = true;\n        }\n    }\n\n    /// Set the current block as the entry block.\n    pub fn set_entry(&mut self, block: BlockIndex) {\n        self.vcode.entry = block;\n    }\n\n    /// End the current basic block. Must be called after emitting vcode insts\n    /// for IR insts and prior to ending the function (building the VCode).\n    pub fn end_bb(&mut self) {\n        let start_idx = self.block_start;\n        let end_idx = self.vcode.insts.len() as InsnIndex;\n        self.block_start = end_idx;\n        // Add the instruction index range to the list of blocks.\n        self.vcode.block_ranges.push((start_idx, end_idx));\n        // End the successors list.\n        let succ_end = self.vcode.block_succs.len();\n        self.vcode\n            .block_succ_range\n            .push((self.succ_start, succ_end));\n        self.succ_start = succ_end;\n    }\n\n    /// Push an instruction for the current BB and current IR inst within the BB.\n    pub fn push(&mut self, insn: I, is_safepoint: bool) {\n        match insn.is_term() {\n            MachTerminator::None | MachTerminator::Ret => {}\n            MachTerminator::Uncond(target) => {\n                self.vcode.block_succs.push(BlockIx::new(target.get()));\n            }\n            MachTerminator::Cond(true_branch, false_branch) => {\n                self.vcode.block_succs.push(BlockIx::new(true_branch.get()));\n                self.vcode\n                    .block_succs\n                    .push(BlockIx::new(false_branch.get()));\n            }\n            MachTerminator::Indirect(targets) => {\n                for target in targets {\n                    self.vcode.block_succs.push(BlockIx::new(target.get()));\n                }\n            }\n        }\n        if insn.defines_value_label().is_some() {\n            self.vcode.has_value_labels = true;\n        }\n        self.vcode.insts.push(insn);\n        self.vcode.srclocs.push(self.cur_srcloc);\n        if is_safepoint {\n            self.stack_map_info\n                .safepoint_insns\n                .push(InstIx::new((self.vcode.insts.len() - 1) as u32));\n        }\n    }\n\n    /// Set the current source location.\n    pub fn set_srcloc(&mut self, srcloc: SourceLoc) {\n        self.cur_srcloc = srcloc;\n    }\n\n    /// Access the constants.\n    pub fn constants(&mut self) -> &mut VCodeConstants {\n        &mut self.vcode.constants\n    }\n\n    /// Build the final VCode, returning the vcode itself as well as auxiliary\n    /// information, such as the stack map request information.\n    pub fn build(self) -> (VCode<I>, StackmapRequestInfo) {\n        // TODO: come up with an abstraction for \"vcode and auxiliary data\". The\n        // auxiliary data needs to be separate from the vcode so that it can be\n        // referenced as the vcode is mutated (e.g. by the register allocator).\n        (self.vcode, self.stack_map_info)\n    }\n}\n\nfn is_redundant_move<I: VCodeInst>(insn: &I) -> bool {\n    if let Some((to, from)) = insn.is_move() {\n        to.to_reg() == from\n    } else {\n        false\n    }\n}\n\n/// Is this type a reference type?\nfn is_reftype(ty: Type) -> bool {\n    ty == types::R64 || ty == types::R32\n}\n\nimpl<I: VCodeInst> VCode<I> {\n    /// New empty VCode.\n    fn new(\n        abi: Box<dyn ABICallee<I = I>>,\n        emit_info: I::Info,\n        block_order: BlockLoweringOrder,\n        constants: VCodeConstants,\n        generate_debug_info: bool,\n    ) -> VCode<I> {\n        VCode {\n            liveins: abi.liveins(),\n            liveouts: abi.liveouts(),\n            vreg_types: vec![],\n            have_ref_values: false,\n            insts: vec![],\n            srclocs: vec![],\n            entry: 0,\n            block_ranges: vec![],\n            block_succ_range: vec![],\n            block_succs: vec![],\n            block_order,\n            abi,\n            emit_info,\n            safepoint_insns: vec![],\n            safepoint_slots: vec![],\n            generate_debug_info,\n            insts_layout: RefCell::new(Default::default()),\n            constants,\n            has_value_labels: false,\n        }\n    }\n\n    /// Returns the flags controlling this function's compilation.\n    pub fn flags(&self) -> &settings::Flags {\n        self.abi.flags()\n    }\n\n    /// Get the IR-level type of a VReg.\n    pub fn vreg_type(&self, vreg: VirtualReg) -> Type {\n        self.vreg_types[vreg.get_index()]\n    }\n\n    /// Get the number of blocks. Block indices will be in the range `0 ..\n    /// (self.num_blocks() - 1)`.\n    pub fn num_blocks(&self) -> usize {\n        self.block_ranges.len()\n    }\n\n    /// Stack frame size for the full function's body.\n    pub fn frame_size(&self) -> u32 {\n        self.abi.frame_size()\n    }\n\n    /// Get the successors for a block.\n    pub fn succs(&self, block: BlockIndex) -> &[BlockIx] {\n        let (start, end) = self.block_succ_range[block as usize];\n        &self.block_succs[start..end]\n    }\n\n    /// Take the results of register allocation, with a sequence of\n    /// instructions including spliced fill/reload/move instructions, and replace\n    /// the VCode with them.\n    pub fn replace_insns_from_regalloc(&mut self, result: RegAllocResult<Self>) {\n        // Record the spillslot count and clobbered registers for the ABI/stack\n        // setup code.\n        self.abi.set_num_spillslots(result.num_spill_slots as usize);\n        self.abi\n            .set_clobbered(result.clobbered_registers.map(|r| Writable::from_reg(*r)));\n\n        let mut final_insns = vec![];\n        let mut final_block_ranges = vec![(0, 0); self.num_blocks()];\n        let mut final_srclocs = vec![];\n        let mut final_safepoint_insns = vec![];\n        let mut safept_idx = 0;\n\n        assert!(result.target_map.elems().len() == self.num_blocks());\n        for block in 0..self.num_blocks() {\n            let start = result.target_map.elems()[block].get() as usize;\n            let end = if block == self.num_blocks() - 1 {\n                result.insns.len()\n            } else {\n                result.target_map.elems()[block + 1].get() as usize\n            };\n            let block = block as BlockIndex;\n            let final_start = final_insns.len() as InsnIndex;\n\n            if block == self.entry {\n                // Start with the prologue.\n                let prologue = self.abi.gen_prologue();\n                let len = prologue.len();\n                final_insns.extend(prologue.into_iter());\n                final_srclocs.extend(iter::repeat(SourceLoc::default()).take(len));\n            }\n\n            for i in start..end {\n                let insn = &result.insns[i];\n\n                // Elide redundant moves at this point (we only know what is\n                // redundant once registers are allocated).\n                if is_redundant_move(insn) {\n                    continue;\n                }\n\n                // Is there a srcloc associated with this insn? Look it up based on original\n                // instruction index (if new insn corresponds to some original insn, i.e., is not\n                // an inserted load/spill/move).\n                let orig_iix = result.orig_insn_map[InstIx::new(i as u32)];\n                let srcloc = if orig_iix.is_invalid() {\n                    SourceLoc::default()\n                } else {\n                    self.srclocs[orig_iix.get() as usize]\n                };\n\n                // Whenever encountering a return instruction, replace it\n                // with the epilogue.\n                let is_ret = insn.is_term() == MachTerminator::Ret;\n                if is_ret {\n                    let epilogue = self.abi.gen_epilogue();\n                    let len = epilogue.len();\n                    final_insns.extend(epilogue.into_iter());\n                    final_srclocs.extend(iter::repeat(srcloc).take(len));\n                } else {\n                    final_insns.push(insn.clone());\n                    final_srclocs.push(srcloc);\n                }\n\n                // Was this instruction a safepoint instruction? Add its final\n                // index to the safepoint insn-index list if so.\n                if safept_idx < result.new_safepoint_insns.len()\n                    && (result.new_safepoint_insns[safept_idx].get() as usize) == i\n                {\n                    let idx = final_insns.len() - 1;\n                    final_safepoint_insns.push(idx as InsnIndex);\n                    safept_idx += 1;\n                }\n            }\n\n            let final_end = final_insns.len() as InsnIndex;\n            final_block_ranges[block as usize] = (final_start, final_end);\n        }\n\n        debug_assert!(final_insns.len() == final_srclocs.len());\n\n        self.insts = final_insns;\n        self.srclocs = final_srclocs;\n        self.block_ranges = final_block_ranges;\n        self.safepoint_insns = final_safepoint_insns;\n\n        // Save safepoint slot-lists. These will be passed to the `EmitState`\n        // for the machine backend during emission so that it can do\n        // target-specific translations of slot numbers to stack offsets.\n        self.safepoint_slots = result.stackmaps;\n    }\n\n    /// Emit the instructions to a `MachBuffer`, containing fixed-up code and external\n    /// reloc/trap/etc. records ready for use.\n    pub fn emit(\n        &self,\n    ) -> (\n        MachBuffer<I>,\n        Vec<CodeOffset>,\n        Vec<(CodeOffset, CodeOffset)>,\n    )\n    where\n        I: MachInstEmit,\n    {\n        let _tt = timing::vcode_emit();\n        let mut buffer = MachBuffer::new();\n        let mut state = I::State::new(&*self.abi);\n        let cfg_metadata = self.flags().machine_code_cfg_info();\n        let mut bb_starts: Vec<Option<CodeOffset>> = vec![];\n\n        // The first M MachLabels are reserved for block indices, the next N MachLabels for\n        // constants.\n        buffer.reserve_labels_for_blocks(self.num_blocks() as BlockIndex);\n        buffer.reserve_labels_for_constants(&self.constants);\n\n        let mut inst_end_offsets = vec![0; self.insts.len()];\n        let mut label_inst_indices = vec![0; self.num_blocks()];\n\n        // Map from instruction index to index in\n        // `safepoint_slots`. We need this because we emit\n        // instructions out-of-order, while the safepoint_insns /\n        // safepoint_slots data structures are sorted in instruction\n        // order.\n        let mut safepoint_indices: FxHashMap<u32, usize> = FxHashMap::default();\n        for (safepoint_idx, iix) in self.safepoint_insns.iter().enumerate() {\n            // Disregard safepoints that ended up having no live refs.\n            if self.safepoint_slots[safepoint_idx].len() > 0 {\n                safepoint_indices.insert(*iix, safepoint_idx);\n            }\n        }\n\n        // Construct the final order we emit code in: cold blocks at the end.\n        let mut final_order: SmallVec<[BlockIndex; 16]> = smallvec![];\n        let mut cold_blocks: SmallVec<[BlockIndex; 16]> = smallvec![];\n        for block in 0..self.num_blocks() {\n            let block = block as BlockIndex;\n            if self.block_order.is_cold(block) {\n                cold_blocks.push(block);\n            } else {\n                final_order.push(block);\n            }\n        }\n        let first_cold_block = cold_blocks.first().cloned();\n        final_order.extend(cold_blocks.clone());\n\n        // Emit blocks.\n        let mut cur_srcloc = None;\n        let mut last_offset = None;\n        let mut start_of_cold_code = None;\n        for block in final_order {\n            let new_offset = I::align_basic_block(buffer.cur_offset());\n            while new_offset > buffer.cur_offset() {\n                // Pad with NOPs up to the aligned block offset.\n                let nop = I::gen_nop((new_offset - buffer.cur_offset()) as usize);\n                nop.emit(&mut buffer, &self.emit_info, &mut Default::default());\n            }\n            assert_eq!(buffer.cur_offset(), new_offset);\n\n            if Some(block) == first_cold_block {\n                start_of_cold_code = Some(buffer.cur_offset());\n            }\n\n            let (start, end) = self.block_ranges[block as usize];\n            buffer.bind_label(MachLabel::from_block(block));\n            label_inst_indices[block as usize] = start;\n\n            if cfg_metadata {\n                // Track BB starts. If we have backed up due to MachBuffer\n                // branch opts, note that the removed blocks were removed.\n                let cur_offset = buffer.cur_offset();\n                if last_offset.is_some() && cur_offset <= last_offset.unwrap() {\n                    for i in (0..bb_starts.len()).rev() {\n                        if bb_starts[i].is_some() && cur_offset > bb_starts[i].unwrap() {\n                            break;\n                        }\n                        bb_starts[i] = None;\n                    }\n                }\n                bb_starts.push(Some(cur_offset));\n                last_offset = Some(cur_offset);\n            }\n\n            for iix in start..end {\n                let srcloc = self.srclocs[iix as usize];\n                if cur_srcloc != Some(srcloc) {\n                    if cur_srcloc.is_some() {\n                        buffer.end_srcloc();\n                    }\n                    buffer.start_srcloc(srcloc);\n                    cur_srcloc = Some(srcloc);\n                }\n                state.pre_sourceloc(cur_srcloc.unwrap_or(SourceLoc::default()));\n\n                if let Some(safepoint_idx) = safepoint_indices.get(&iix) {\n                    let stack_map = self\n                        .abi\n                        .spillslots_to_stack_map(&self.safepoint_slots[*safepoint_idx][..], &state);\n                    state.pre_safepoint(stack_map);\n                }\n\n                self.insts[iix as usize].emit(&mut buffer, &self.emit_info, &mut state);\n\n                if self.generate_debug_info {\n                    // Buffer truncation may have happened since last inst append; trim inst-end\n                    // layout info as appropriate.\n                    let l = &mut inst_end_offsets[0..iix as usize];\n                    for end in l.iter_mut().rev() {\n                        if *end > buffer.cur_offset() {\n                            *end = buffer.cur_offset();\n                        } else {\n                            break;\n                        }\n                    }\n                    inst_end_offsets[iix as usize] = buffer.cur_offset();\n                }\n            }\n\n            if cur_srcloc.is_some() {\n                buffer.end_srcloc();\n                cur_srcloc = None;\n            }\n\n            // Do we need an island? Get the worst-case size of the next BB and see if, having\n            // emitted that many bytes, we will be beyond the deadline.\n            if block < (self.num_blocks() - 1) as BlockIndex {\n                let next_block = block + 1;\n                let next_block_range = self.block_ranges[next_block as usize];\n                let next_block_size = next_block_range.1 - next_block_range.0;\n                let worst_case_next_bb = I::worst_case_size() * next_block_size;\n                if buffer.island_needed(worst_case_next_bb) {\n                    buffer.emit_island(worst_case_next_bb);\n                }\n            }\n        }\n\n        // Emit the constants used by the function.\n        for (constant, data) in self.constants.iter() {\n            let label = buffer.get_label_for_constant(constant);\n            buffer.defer_constant(label, data.alignment(), data.as_slice(), u32::max_value());\n        }\n\n        if self.generate_debug_info {\n            for end in inst_end_offsets.iter_mut().rev() {\n                if *end > buffer.cur_offset() {\n                    *end = buffer.cur_offset();\n                } else {\n                    break;\n                }\n            }\n            *self.insts_layout.borrow_mut() = InstsLayoutInfo {\n                inst_end_offsets,\n                label_inst_indices,\n                start_of_cold_code,\n            };\n        }\n\n        // Create `bb_edges` and final (filtered) `bb_starts`.\n        let mut final_bb_starts = vec![];\n        let mut bb_edges = vec![];\n        if cfg_metadata {\n            for block in 0..self.num_blocks() {\n                if bb_starts[block].is_none() {\n                    // Block was deleted by MachBuffer; skip.\n                    continue;\n                }\n                let from = bb_starts[block].unwrap();\n\n                final_bb_starts.push(from);\n                // Resolve each `succ` label and add edges.\n                let succs = self.block_succs(BlockIx::new(block as u32));\n                for succ in succs.iter() {\n                    let to = buffer.resolve_label_offset(MachLabel::from_block(succ.get()));\n                    bb_edges.push((from, to));\n                }\n            }\n        }\n\n        (buffer, final_bb_starts, bb_edges)\n    }\n\n    /// Generates value-label ranges.\n    pub fn value_labels_ranges(&self) -> ValueLabelsRanges {\n        if !self.has_value_labels {\n            return ValueLabelsRanges::default();\n        }\n\n        let layout_info = &self.insts_layout.borrow();\n        debug::compute(&self.insts, &*layout_info)\n    }\n\n    /// Get the offsets of stackslots.\n    pub fn stackslot_offsets(&self) -> &PrimaryMap<StackSlot, u32> {\n        self.abi.stackslot_offsets()\n    }\n\n    /// Get the IR block for a BlockIndex, if one exists.\n    pub fn bindex_to_bb(&self, block: BlockIndex) -> Option<ir::Block> {\n        self.block_order.lowered_order()[block as usize].orig_block()\n    }\n}\n\nimpl<I: VCodeInst> RegallocFunction for VCode<I> {\n    type Inst = I;\n\n    fn insns(&self) -> &[I] {\n        &self.insts[..]\n    }\n\n    fn insns_mut(&mut self) -> &mut [I] {\n        &mut self.insts[..]\n    }\n\n    fn get_insn(&self, insn: InstIx) -> &I {\n        &self.insts[insn.get() as usize]\n    }\n\n    fn get_insn_mut(&mut self, insn: InstIx) -> &mut I {\n        &mut self.insts[insn.get() as usize]\n    }\n\n    fn blocks(&self) -> Range<BlockIx> {\n        Range::new(BlockIx::new(0), self.block_ranges.len())\n    }\n\n    fn entry_block(&self) -> BlockIx {\n        BlockIx::new(self.entry)\n    }\n\n    fn block_insns(&self, block: BlockIx) -> Range<InstIx> {\n        let (start, end) = self.block_ranges[block.get() as usize];\n        Range::new(InstIx::new(start), (end - start) as usize)\n    }\n\n    fn block_succs(&self, block: BlockIx) -> Cow<[BlockIx]> {\n        let (start, end) = self.block_succ_range[block.get() as usize];\n        Cow::Borrowed(&self.block_succs[start..end])\n    }\n\n    fn is_ret(&self, insn: InstIx) -> bool {\n        match self.insts[insn.get() as usize].is_term() {\n            MachTerminator::Ret => true,\n            _ => false,\n        }\n    }\n\n    fn is_included_in_clobbers(&self, insn: &I) -> bool {\n        insn.is_included_in_clobbers()\n    }\n\n    fn get_regs(insn: &I, collector: &mut RegUsageCollector) {\n        insn.get_regs(collector)\n    }\n\n    fn map_regs<RUM: RegUsageMapper>(insn: &mut I, mapper: &RUM) {\n        insn.map_regs(mapper);\n    }\n\n    fn is_move(&self, insn: &I) -> Option<(Writable<Reg>, Reg)> {\n        insn.is_move()\n    }\n\n    fn get_num_vregs(&self) -> usize {\n        self.vreg_types.len()\n    }\n\n    fn get_spillslot_size(&self, regclass: RegClass, _: VirtualReg) -> u32 {\n        self.abi.get_spillslot_size(regclass)\n    }\n\n    fn gen_spill(&self, to_slot: SpillSlot, from_reg: RealReg, _: Option<VirtualReg>) -> I {\n        self.abi.gen_spill(to_slot, from_reg)\n    }\n\n    fn gen_reload(\n        &self,\n        to_reg: Writable<RealReg>,\n        from_slot: SpillSlot,\n        _: Option<VirtualReg>,\n    ) -> I {\n        self.abi.gen_reload(to_reg, from_slot)\n    }\n\n    fn gen_move(&self, to_reg: Writable<RealReg>, from_reg: RealReg, vreg: VirtualReg) -> I {\n        let ty = self.vreg_type(vreg);\n        I::gen_move(to_reg.map(|r| r.to_reg()), from_reg.to_reg(), ty)\n    }\n\n    fn gen_zero_len_nop(&self) -> I {\n        I::gen_nop(0)\n    }\n\n    fn maybe_direct_reload(&self, insn: &I, reg: VirtualReg, slot: SpillSlot) -> Option<I> {\n        insn.maybe_direct_reload(reg, slot)\n    }\n\n    fn func_liveins(&self) -> RegallocSet<RealReg> {\n        self.liveins.clone()\n    }\n\n    fn func_liveouts(&self) -> RegallocSet<RealReg> {\n        self.liveouts.clone()\n    }\n}\n\nimpl<I: VCodeInst> fmt::Debug for VCode<I> {\n    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {\n        writeln!(f, \"VCode_Debug {{\")?;\n        writeln!(f, \"  Entry block: {}\", self.entry)?;\n\n        for block in 0..self.num_blocks() {\n            writeln!(f, \"Block {}:\", block,)?;\n            for succ in self.succs(block as BlockIndex) {\n                writeln!(f, \"  (successor: Block {})\", succ.get())?;\n            }\n            let (start, end) = self.block_ranges[block];\n            writeln!(f, \"  (instruction range: {} .. {})\", start, end)?;\n            for inst in start..end {\n                writeln!(f, \"  Inst {}: {:?}\", inst, self.insts[inst as usize])?;\n            }\n        }\n\n        writeln!(f, \"}}\")?;\n        Ok(())\n    }\n}\n\n/// Pretty-printing with `RealRegUniverse` context.\nimpl<I: VCodeInst> PrettyPrint for VCode<I> {\n    fn show_rru(&self, mb_rru: Option<&RealRegUniverse>) -> String {\n        use std::fmt::Write;\n\n        let mut s = String::new();\n        write!(&mut s, \"VCode_ShowWithRRU {{{{\\n\").unwrap();\n        write!(&mut s, \"  Entry block: {}\\n\", self.entry).unwrap();\n\n        let mut state = Default::default();\n        let mut safepoint_idx = 0;\n        for i in 0..self.num_blocks() {\n            let block = i as BlockIndex;\n\n            write!(&mut s, \"Block {}:\\n\", block).unwrap();\n            if let Some(bb) = self.bindex_to_bb(block) {\n                write!(&mut s, \"  (original IR block: {})\\n\", bb).unwrap();\n            }\n            for succ in self.succs(block) {\n                write!(&mut s, \"  (successor: Block {})\\n\", succ.get()).unwrap();\n            }\n            let (start, end) = self.block_ranges[block as usize];\n            write!(&mut s, \"  (instruction range: {} .. {})\\n\", start, end).unwrap();\n            for inst in start..end {\n                if safepoint_idx < self.safepoint_insns.len()\n                    && self.safepoint_insns[safepoint_idx] == inst\n                {\n                    write!(\n                        &mut s,\n                        \"      (safepoint: slots {:?} with EmitState {:?})\\n\",\n                        self.safepoint_slots[safepoint_idx], state,\n                    )\n                    .unwrap();\n                    safepoint_idx += 1;\n                }\n                write!(\n                    &mut s,\n                    \"  Inst {}:   {}\\n\",\n                    inst,\n                    self.insts[inst as usize].pretty_print(mb_rru, &mut state)\n                )\n                .unwrap();\n            }\n        }\n\n        write!(&mut s, \"}}}}\\n\").unwrap();\n\n        s\n    }\n}\n\n/// This structure tracks the large constants used in VCode that will be emitted separately by the\n/// [MachBuffer].\n///\n/// First, during the lowering phase, constants are inserted using\n/// [VCodeConstants.insert]; an intermediate handle, [VCodeConstant], tracks what constants are\n/// used in this phase. Some deduplication is performed, when possible, as constant\n/// values are inserted.\n///\n/// Secondly, during the emission phase, the [MachBuffer] assigns [MachLabel]s for each of the\n/// constants so that instructions can refer to the value's memory location. The [MachBuffer]\n/// then writes the constant values to the buffer.\n#[derive(Default)]\npub struct VCodeConstants {\n    constants: PrimaryMap<VCodeConstant, VCodeConstantData>,\n    pool_uses: HashMap<Constant, VCodeConstant>,\n    well_known_uses: HashMap<*const [u8], VCodeConstant>,\n}\nimpl VCodeConstants {\n    /// Initialize the structure with the expected number of constants.\n    pub fn with_capacity(expected_num_constants: usize) -> Self {\n        Self {\n            constants: PrimaryMap::with_capacity(expected_num_constants),\n            pool_uses: HashMap::with_capacity(expected_num_constants),\n            well_known_uses: HashMap::new(),\n        }\n    }\n\n    /// Insert a constant; using this method indicates that a constant value will be used and thus\n    /// will be emitted to the `MachBuffer`. The current implementation can deduplicate constants\n    /// that are [VCodeConstantData::Pool] or [VCodeConstantData::WellKnown] but not\n    /// [VCodeConstantData::Generated].\n    pub fn insert(&mut self, data: VCodeConstantData) -> VCodeConstant {\n        match data {\n            VCodeConstantData::Generated(_) => self.constants.push(data),\n            VCodeConstantData::Pool(constant, _) => match self.pool_uses.get(&constant) {\n                None => {\n                    let vcode_constant = self.constants.push(data);\n                    self.pool_uses.insert(constant, vcode_constant);\n                    vcode_constant\n                }\n                Some(&vcode_constant) => vcode_constant,\n            },\n            VCodeConstantData::WellKnown(data_ref) => {\n                match self.well_known_uses.get(&(data_ref as *const [u8])) {\n                    None => {\n                        let vcode_constant = self.constants.push(data);\n                        self.well_known_uses\n                            .insert(data_ref as *const [u8], vcode_constant);\n                        vcode_constant\n                    }\n                    Some(&vcode_constant) => vcode_constant,\n                }\n            }\n        }\n    }\n\n    /// Return the number of constants inserted.\n    pub fn len(&self) -> usize {\n        self.constants.len()\n    }\n\n    /// Iterate over the [VCodeConstant] keys inserted in this structure.\n    pub fn keys(&self) -> Keys<VCodeConstant> {\n        self.constants.keys()\n    }\n\n    /// Iterate over the [VCodeConstant] keys and the data (as a byte slice) inserted in this\n    /// structure.\n    pub fn iter(&self) -> impl Iterator<Item = (VCodeConstant, &VCodeConstantData)> {\n        self.constants.iter()\n    }\n}\n\n/// A use of a constant by one or more VCode instructions; see [VCodeConstants].\n#[derive(Clone, Copy, Debug, PartialEq, Eq)]\npub struct VCodeConstant(u32);\nentity_impl!(VCodeConstant);\n\n/// Identify the different types of constant that can be inserted into [VCodeConstants]. Tracking\n/// these separately instead of as raw byte buffers allows us to avoid some duplication.\npub enum VCodeConstantData {\n    /// A constant already present in the Cranelift IR\n    /// [ConstantPool](crate::ir::constant::ConstantPool).\n    Pool(Constant, ConstantData),\n    /// A reference to a well-known constant value that is statically encoded within the compiler.\n    WellKnown(&'static [u8]),\n    /// A constant value generated during lowering; the value may depend on the instruction context\n    /// which makes it difficult to de-duplicate--if possible, use other variants.\n    Generated(ConstantData),\n}\nimpl VCodeConstantData {\n    /// Retrieve the constant data as a byte slice.\n    pub fn as_slice(&self) -> &[u8] {\n        match self {\n            VCodeConstantData::Pool(_, d) | VCodeConstantData::Generated(d) => d.as_slice(),\n            VCodeConstantData::WellKnown(d) => d,\n        }\n    }\n\n    /// Calculate the alignment of the constant data.\n    pub fn alignment(&self) -> u32 {\n        if self.as_slice().len() <= 8 {\n            8\n        } else {\n            16\n        }\n    }\n}\n\n#[cfg(test)]\nmod test {\n    use super::*;\n    use std::mem::size_of;\n\n    #[test]\n    fn size_of_constant_structs() {\n        assert_eq!(size_of::<Constant>(), 4);\n        assert_eq!(size_of::<VCodeConstant>(), 4);\n        assert_eq!(size_of::<ConstantData>(), 24);\n        assert_eq!(size_of::<VCodeConstantData>(), 32);\n        assert_eq!(\n            size_of::<PrimaryMap<VCodeConstant, VCodeConstantData>>(),\n            24\n        );\n        // TODO The VCodeConstants structure's memory size could be further optimized.\n        // With certain versions of Rust, each `HashMap` in `VCodeConstants` occupied at\n        // least 48 bytes, making an empty `VCodeConstants` cost 120 bytes.\n    }\n}\n", "use super::ref_types_module;\nuse std::sync::atomic::{AtomicBool, Ordering::SeqCst};\nuse std::sync::Arc;\nuse wasmtime::*;\n\n#[test]\nfn pass_funcref_in_and_out_of_wasm() -> anyhow::Result<()> {\n    let (mut store, module) = ref_types_module(\n        false,\n        r#\"\n            (module\n                (func (export \"func\") (param funcref) (result funcref)\n                    local.get 0\n                )\n            )\n        \"#,\n    )?;\n\n    let instance = Instance::new(&mut store, &module, &[])?;\n    let func = instance.get_func(&mut store, \"func\").unwrap();\n\n    // Pass in a non-null funcref.\n    {\n        let mut results = [Val::I32(0)];\n        func.call(\n            &mut store,\n            &[Val::FuncRef(Some(func.clone()))],\n            &mut results,\n        )?;\n\n        // Can't compare `Func` for equality, so this is the best we can do here.\n        let result_func = results[0].unwrap_funcref().unwrap();\n        assert_eq!(func.ty(&store), result_func.ty(&store));\n    }\n\n    // Pass in a null funcref.\n    {\n        let mut results = [Val::I32(0)];\n        func.call(&mut store, &[Val::FuncRef(None)], &mut results)?;\n        let result_func = results[0].unwrap_funcref();\n        assert!(result_func.is_none());\n    }\n\n    // Pass in a `funcref` from another instance.\n    {\n        let other_instance = Instance::new(&mut store, &module, &[])?;\n        let other_instance_func = other_instance.get_func(&mut store, \"func\").unwrap();\n\n        let mut results = [Val::I32(0)];\n        func.call(\n            &mut store,\n            &[Val::FuncRef(Some(other_instance_func.clone()))],\n            &mut results,\n        )?;\n        assert_eq!(results.len(), 1);\n\n        // Can't compare `Func` for equality, so this is the best we can do here.\n        let result_func = results[0].unwrap_funcref().unwrap();\n        assert_eq!(other_instance_func.ty(&store), result_func.ty(&store));\n    }\n\n    // Passing in a `funcref` from another store fails.\n    {\n        let (mut other_store, other_module) =\n            ref_types_module(false, r#\"(module (func (export \"f\")))\"#)?;\n        let other_store_instance = Instance::new(&mut other_store, &other_module, &[])?;\n        let f = other_store_instance\n            .get_func(&mut other_store, \"f\")\n            .unwrap();\n\n        assert!(func\n            .call(&mut store, &[Val::FuncRef(Some(f))], &mut [Val::I32(0)])\n            .is_err());\n    }\n\n    Ok(())\n}\n\n#[test]\nfn receive_null_funcref_from_wasm() -> anyhow::Result<()> {\n    let (mut store, module) = ref_types_module(\n        false,\n        r#\"\n            (module\n                (func (export \"get-null\") (result funcref)\n                    ref.null func\n                )\n            )\n        \"#,\n    )?;\n\n    let instance = Instance::new(&mut store, &module, &[])?;\n    let get_null = instance.get_func(&mut store, \"get-null\").unwrap();\n\n    let mut results = [Val::I32(0)];\n    get_null.call(&mut store, &[], &mut results)?;\n    let result_func = results[0].unwrap_funcref();\n    assert!(result_func.is_none());\n\n    Ok(())\n}\n\n#[test]\nfn wrong_store() -> anyhow::Result<()> {\n    let dropped = Arc::new(AtomicBool::new(false));\n    {\n        let mut store1 = Store::<()>::default();\n        let mut store2 = Store::<()>::default();\n\n        let set = SetOnDrop(dropped.clone());\n        let f1 = Func::wrap(&mut store1, move || drop(&set));\n        let f2 = Func::wrap(&mut store2, move || Some(f1.clone()));\n        assert!(f2.call(&mut store2, &[], &mut []).is_err());\n    }\n    assert!(dropped.load(SeqCst));\n\n    return Ok(());\n\n    struct SetOnDrop(Arc<AtomicBool>);\n\n    impl Drop for SetOnDrop {\n        fn drop(&mut self) {\n            self.0.store(true, SeqCst);\n        }\n    }\n}\n\n#[test]\nfn func_new_returns_wrong_store() -> anyhow::Result<()> {\n    let dropped = Arc::new(AtomicBool::new(false));\n    {\n        let mut store1 = Store::<()>::default();\n        let mut store2 = Store::<()>::default();\n\n        let set = SetOnDrop(dropped.clone());\n        let f1 = Func::wrap(&mut store1, move || drop(&set));\n        let f2 = Func::new(\n            &mut store2,\n            FuncType::new(None, Some(ValType::FuncRef)),\n            move |_, _, results| {\n                results[0] = f1.clone().into();\n                Ok(())\n            },\n        );\n        assert!(f2.call(&mut store2, &[], &mut [Val::I32(0)]).is_err());\n    }\n    assert!(dropped.load(SeqCst));\n\n    return Ok(());\n\n    struct SetOnDrop(Arc<AtomicBool>);\n\n    impl Drop for SetOnDrop {\n        fn drop(&mut self) {\n            self.0.store(true, SeqCst);\n        }\n    }\n}\n", "use super::ref_types_module;\nuse super::skip_pooling_allocator_tests;\nuse std::sync::atomic::{AtomicBool, AtomicUsize, Ordering::SeqCst};\nuse std::sync::Arc;\nuse wasmtime::*;\n\nstruct SetFlagOnDrop(Arc<AtomicBool>);\n\nimpl Drop for SetFlagOnDrop {\n    fn drop(&mut self) {\n        self.0.store(true, SeqCst);\n    }\n}\n\n#[test]\nfn smoke_test_gc() -> anyhow::Result<()> {\n    smoke_test_gc_impl(false)\n}\n\n#[test]\nfn smoke_test_gc_epochs() -> anyhow::Result<()> {\n    smoke_test_gc_impl(true)\n}\n\nfn smoke_test_gc_impl(use_epochs: bool) -> anyhow::Result<()> {\n    let (mut store, module) = ref_types_module(\n        use_epochs,\n        r#\"\n            (module\n                (import \"\" \"\" (func $do_gc))\n                (func $recursive (export \"func\") (param i32 externref) (result externref)\n                    local.get 0\n                    i32.eqz\n                    if (result externref)\n                        call $do_gc\n                        local.get 1\n                    else\n                        local.get 0\n                        i32.const 1\n                        i32.sub\n                        local.get 1\n                        call $recursive\n                    end\n                )\n            )\n        \"#,\n    )?;\n\n    let do_gc = Func::wrap(&mut store, |mut caller: Caller<'_, _>| {\n        // Do a GC with `externref`s on the stack in Wasm frames.\n        caller.gc();\n    });\n    let instance = Instance::new(&mut store, &module, &[do_gc.into()])?;\n    let func = instance.get_func(&mut store, \"func\").unwrap();\n\n    let inner_dropped = Arc::new(AtomicBool::new(false));\n    let r = ExternRef::new(SetFlagOnDrop(inner_dropped.clone()));\n    {\n        let args = [Val::I32(5), Val::ExternRef(Some(r.clone()))];\n        func.call(&mut store, &args, &mut [Val::I32(0)])?;\n    }\n\n    // Still held alive by the `VMExternRefActivationsTable` (potentially in\n    // multiple slots within the table) and by this `r` local.\n    assert!(r.strong_count() >= 2);\n\n    // Doing a GC should see that there aren't any `externref`s on the stack in\n    // Wasm frames anymore.\n    store.gc();\n    assert_eq!(r.strong_count(), 1);\n\n    // Dropping `r` should drop the inner `SetFlagOnDrop` value.\n    drop(r);\n    assert!(inner_dropped.load(SeqCst));\n\n    Ok(())\n}\n\n#[test]\nfn wasm_dropping_refs() -> anyhow::Result<()> {\n    let (mut store, module) = ref_types_module(\n        false,\n        r#\"\n            (module\n                (func (export \"drop_ref\") (param externref)\n                    nop\n                )\n            )\n        \"#,\n    )?;\n\n    let instance = Instance::new(&mut store, &module, &[])?;\n    let drop_ref = instance.get_func(&mut store, \"drop_ref\").unwrap();\n\n    let num_refs_dropped = Arc::new(AtomicUsize::new(0));\n\n    // NB: 4096 is greater than the initial `VMExternRefActivationsTable`\n    // capacity, so this will trigger at least one GC.\n    for _ in 0..4096 {\n        let r = ExternRef::new(CountDrops(num_refs_dropped.clone()));\n        let args = [Val::ExternRef(Some(r))];\n        drop_ref.call(&mut store, &args, &mut [])?;\n    }\n\n    assert!(num_refs_dropped.load(SeqCst) > 0);\n\n    // And after doing a final GC, all the refs should have been dropped.\n    store.gc();\n    assert_eq!(num_refs_dropped.load(SeqCst), 4096);\n\n    return Ok(());\n\n    struct CountDrops(Arc<AtomicUsize>);\n\n    impl Drop for CountDrops {\n        fn drop(&mut self) {\n            self.0.fetch_add(1, SeqCst);\n        }\n    }\n}\n\n#[test]\nfn many_live_refs() -> anyhow::Result<()> {\n    let mut wat = r#\"\n        (module\n            ;; Make new `externref`s.\n            (import \"\" \"make_ref\" (func $make_ref (result externref)))\n\n            ;; Observe an `externref` so it is kept live.\n            (import \"\" \"observe_ref\" (func $observe_ref (param externref)))\n\n            (func (export \"many_live_refs\")\n    \"#\n    .to_string();\n\n    // This is more than the initial `VMExternRefActivationsTable` capacity, so\n    // it will need to allocate additional bump chunks.\n    const NUM_LIVE_REFS: usize = 1024;\n\n    // Push `externref`s onto the stack.\n    for _ in 0..NUM_LIVE_REFS {\n        wat.push_str(\"(call $make_ref)\\n\");\n    }\n\n    // Pop `externref`s from the stack. Because we pass each of them to a\n    // function call here, they are all live references for the duration of\n    // their lifetimes.\n    for _ in 0..NUM_LIVE_REFS {\n        wat.push_str(\"(call $observe_ref)\\n\");\n    }\n\n    wat.push_str(\n        \"\n            ) ;; func\n        ) ;; module\n        \",\n    );\n\n    let (mut store, module) = ref_types_module(false, &wat)?;\n\n    let live_refs = Arc::new(AtomicUsize::new(0));\n\n    let make_ref = Func::wrap(&mut store, {\n        let live_refs = live_refs.clone();\n        move || Some(ExternRef::new(CountLiveRefs::new(live_refs.clone())))\n    });\n\n    let observe_ref = Func::wrap(&mut store, |r: Option<ExternRef>| {\n        let r = r.unwrap();\n        let r = r.data().downcast_ref::<CountLiveRefs>().unwrap();\n        assert!(r.live_refs.load(SeqCst) > 0);\n    });\n\n    let instance = Instance::new(&mut store, &module, &[make_ref.into(), observe_ref.into()])?;\n    let many_live_refs = instance.get_func(&mut store, \"many_live_refs\").unwrap();\n\n    many_live_refs.call(&mut store, &[], &mut [])?;\n\n    store.gc();\n    assert_eq!(live_refs.load(SeqCst), 0);\n\n    return Ok(());\n\n    struct CountLiveRefs {\n        live_refs: Arc<AtomicUsize>,\n    }\n\n    impl CountLiveRefs {\n        fn new(live_refs: Arc<AtomicUsize>) -> Self {\n            live_refs.fetch_add(1, SeqCst);\n            Self { live_refs }\n        }\n    }\n\n    impl Drop for CountLiveRefs {\n        fn drop(&mut self) {\n            self.live_refs.fetch_sub(1, SeqCst);\n        }\n    }\n}\n\n#[test]\nfn drop_externref_via_table_set() -> anyhow::Result<()> {\n    let (mut store, module) = ref_types_module(\n        false,\n        r#\"\n            (module\n                (table $t 1 externref)\n\n                (func (export \"table-set\") (param externref)\n                  (table.set $t (i32.const 0) (local.get 0))\n                )\n            )\n        \"#,\n    )?;\n\n    let instance = Instance::new(&mut store, &module, &[])?;\n    let table_set = instance.get_func(&mut store, \"table-set\").unwrap();\n\n    let foo_is_dropped = Arc::new(AtomicBool::new(false));\n    let bar_is_dropped = Arc::new(AtomicBool::new(false));\n\n    let foo = ExternRef::new(SetFlagOnDrop(foo_is_dropped.clone()));\n    let bar = ExternRef::new(SetFlagOnDrop(bar_is_dropped.clone()));\n\n    {\n        let args = vec![Val::ExternRef(Some(foo))];\n        table_set.call(&mut store, &args, &mut [])?;\n    }\n    store.gc();\n    assert!(!foo_is_dropped.load(SeqCst));\n    assert!(!bar_is_dropped.load(SeqCst));\n\n    {\n        let args = vec![Val::ExternRef(Some(bar))];\n        table_set.call(&mut store, &args, &mut [])?;\n    }\n    store.gc();\n    assert!(foo_is_dropped.load(SeqCst));\n    assert!(!bar_is_dropped.load(SeqCst));\n\n    table_set.call(&mut store, &[Val::ExternRef(None)], &mut [])?;\n    assert!(foo_is_dropped.load(SeqCst));\n    assert!(bar_is_dropped.load(SeqCst));\n\n    Ok(())\n}\n\n#[test]\nfn global_drops_externref() -> anyhow::Result<()> {\n    test_engine(&Engine::default())?;\n\n    if !skip_pooling_allocator_tests() {\n        test_engine(&Engine::new(\n            Config::new().allocation_strategy(InstanceAllocationStrategy::pooling()),\n        )?)?;\n    }\n\n    return Ok(());\n\n    fn test_engine(engine: &Engine) -> anyhow::Result<()> {\n        let mut store = Store::new(&engine, ());\n        let flag = Arc::new(AtomicBool::new(false));\n        let externref = ExternRef::new(SetFlagOnDrop(flag.clone()));\n        Global::new(\n            &mut store,\n            GlobalType::new(ValType::ExternRef, Mutability::Const),\n            externref.into(),\n        )?;\n        drop(store);\n        assert!(flag.load(SeqCst));\n\n        let mut store = Store::new(&engine, ());\n        let module = Module::new(\n            &engine,\n            r#\"\n                (module\n                    (global (mut externref) (ref.null extern))\n\n                    (func (export \"run\") (param externref)\n                        local.get 0\n                        global.set 0\n                    )\n                )\n            \"#,\n        )?;\n        let instance = Instance::new(&mut store, &module, &[])?;\n        let run = instance.get_typed_func::<Option<ExternRef>, (), _>(&mut store, \"run\")?;\n        let flag = Arc::new(AtomicBool::new(false));\n        let externref = ExternRef::new(SetFlagOnDrop(flag.clone()));\n        run.call(&mut store, Some(externref))?;\n        drop(store);\n        assert!(flag.load(SeqCst));\n        Ok(())\n    }\n}\n\n#[test]\nfn table_drops_externref() -> anyhow::Result<()> {\n    test_engine(&Engine::default())?;\n\n    if !skip_pooling_allocator_tests() {\n        test_engine(&Engine::new(\n            Config::new().allocation_strategy(InstanceAllocationStrategy::pooling()),\n        )?)?;\n    }\n\n    return Ok(());\n\n    fn test_engine(engine: &Engine) -> anyhow::Result<()> {\n        let mut store = Store::new(&engine, ());\n        let flag = Arc::new(AtomicBool::new(false));\n        let externref = ExternRef::new(SetFlagOnDrop(flag.clone()));\n        Table::new(\n            &mut store,\n            TableType::new(ValType::ExternRef, 1, None),\n            externref.into(),\n        )?;\n        drop(store);\n        assert!(flag.load(SeqCst));\n\n        let mut store = Store::new(&engine, ());\n        let module = Module::new(\n            &engine,\n            r#\"\n            (module\n                (table 1 externref)\n\n                (func (export \"run\") (param externref)\n                    i32.const 0\n                    local.get 0\n                    table.set 0\n                )\n            )\n        \"#,\n        )?;\n        let instance = Instance::new(&mut store, &module, &[])?;\n        let run = instance.get_typed_func::<Option<ExternRef>, (), _>(&mut store, \"run\")?;\n        let flag = Arc::new(AtomicBool::new(false));\n        let externref = ExternRef::new(SetFlagOnDrop(flag.clone()));\n        run.call(&mut store, Some(externref))?;\n        drop(store);\n        assert!(flag.load(SeqCst));\n        Ok(())\n    }\n}\n\n#[test]\nfn gee_i_sure_hope_refcounting_is_atomic() -> anyhow::Result<()> {\n    let mut config = Config::new();\n    config.wasm_reference_types(true);\n    config.epoch_interruption(true);\n    let engine = Engine::new(&config)?;\n    let mut store = Store::new(&engine, ());\n    let module = Module::new(\n        &engine,\n        r#\"\n            (module\n                (global (mut externref) (ref.null extern))\n                (table 1 externref)\n\n                (func (export \"run\") (param externref)\n                    local.get 0\n                    global.set 0\n                    i32.const 0\n                    local.get 0\n                    table.set 0\n                    loop\n                        global.get 0\n                        global.set 0\n\n                        i32.const 0\n                        i32.const 0\n                        table.get\n                        table.set\n\n                        local.get 0\n                        call $f\n\n                        br 0\n                    end\n                )\n\n                (func $f (param externref))\n            )\n        \"#,\n    )?;\n\n    let instance = Instance::new(&mut store, &module, &[])?;\n    let run = instance.get_typed_func::<Option<ExternRef>, (), _>(&mut store, \"run\")?;\n\n    let flag = Arc::new(AtomicBool::new(false));\n    let externref = ExternRef::new(SetFlagOnDrop(flag.clone()));\n    let externref2 = externref.clone();\n\n    let child = std::thread::spawn(move || run.call(&mut store, Some(externref2)));\n\n    for _ in 0..10000 {\n        drop(externref.clone());\n    }\n    engine.increment_epoch();\n\n    assert!(child.join().unwrap().is_err());\n    assert!(!flag.load(SeqCst));\n    assert_eq!(externref.strong_count(), 1);\n    drop(externref);\n    assert!(flag.load(SeqCst));\n\n    Ok(())\n}\n\n#[test]\nfn global_init_no_leak() -> anyhow::Result<()> {\n    let (mut store, module) = ref_types_module(\n        false,\n        r#\"\n            (module\n                (import \"\" \"\" (global externref))\n                (global externref (global.get 0))\n            )\n        \"#,\n    )?;\n\n    let externref = ExternRef::new(());\n    let global = Global::new(\n        &mut store,\n        GlobalType::new(ValType::ExternRef, Mutability::Const),\n        externref.clone().into(),\n    )?;\n    Instance::new(&mut store, &module, &[global.into()])?;\n    drop(store);\n    assert_eq!(externref.strong_count(), 1);\n\n    Ok(())\n}\n\n#[test]\nfn no_gc_middle_of_args() -> anyhow::Result<()> {\n    let (mut store, module) = ref_types_module(\n        false,\n        r#\"\n            (module\n                (import \"\" \"return_some\" (func $return (result externref externref externref)))\n                (import \"\" \"take_some\" (func $take (param externref externref externref)))\n                (func (export \"run\")\n                    (local i32)\n                    i32.const 1000\n                    local.set 0\n                    loop\n                        call $return\n                        call $take\n                        local.get 0\n                        i32.const -1\n                        i32.add\n                        local.tee 0\n                        br_if 0\n                    end\n                )\n            )\n        \"#,\n    )?;\n\n    let mut linker = Linker::new(store.engine());\n    linker.func_wrap(\"\", \"return_some\", || {\n        (\n            Some(ExternRef::new(\"a\".to_string())),\n            Some(ExternRef::new(\"b\".to_string())),\n            Some(ExternRef::new(\"c\".to_string())),\n        )\n    })?;\n    linker.func_wrap(\n        \"\",\n        \"take_some\",\n        |a: Option<ExternRef>, b: Option<ExternRef>, c: Option<ExternRef>| {\n            let a = a.unwrap();\n            let b = b.unwrap();\n            let c = c.unwrap();\n            assert_eq!(a.data().downcast_ref::<String>().unwrap(), \"a\");\n            assert_eq!(b.data().downcast_ref::<String>().unwrap(), \"b\");\n            assert_eq!(c.data().downcast_ref::<String>().unwrap(), \"c\");\n        },\n    )?;\n\n    let instance = linker.instantiate(&mut store, &module)?;\n    let func = instance.get_typed_func::<(), (), _>(&mut store, \"run\")?;\n    func.call(&mut store, ())?;\n\n    Ok(())\n}\n", "mod async_functions;\nmod call_hook;\nmod cli_tests;\nmod custom_signal_handler;\nmod debug;\nmod epoch_interruption;\nmod externals;\nmod fuel;\nmod func;\nmod funcref;\nmod gc;\nmod globals;\nmod host_funcs;\nmod iloop;\nmod import_calling_export;\nmod import_indexes;\nmod instance;\nmod invoke_func_via_table;\nmod limits;\nmod linker;\nmod memory;\nmod memory_creator;\nmod module;\nmod module_serialize;\nmod name;\nmod pooling_allocator;\nmod relocs;\nmod stack_overflow;\nmod store;\nmod table;\nmod traps;\nmod wast;\n\n/// A helper to compile a module in a new store with reference types enabled.\npub(crate) fn ref_types_module(\n    use_epochs: bool,\n    source: &str,\n) -> anyhow::Result<(wasmtime::Store<()>, wasmtime::Module)> {\n    use wasmtime::*;\n\n    let _ = env_logger::try_init();\n\n    let mut config = Config::new();\n    config.wasm_reference_types(true);\n    if use_epochs {\n        config.epoch_interruption(true);\n    }\n\n    let engine = Engine::new(&config)?;\n    let mut store = Store::new(&engine, ());\n    if use_epochs {\n        store.set_epoch_deadline(1);\n    }\n\n    let module = Module::new(&engine, source)?;\n\n    Ok((store, module))\n}\n\n/// A helper determining whether the pooling allocator tests should be skipped.\npub(crate) fn skip_pooling_allocator_tests() -> bool {\n    // There are a couple of issues when running the pooling allocator tests under QEMU:\n    // - high memory usage that may exceed the limits imposed by the environment (e.g. CI)\n    // - https://github.com/bytecodealliance/wasmtime/pull/2518#issuecomment-747280133\n    std::env::var(\"WASMTIME_TEST_NO_HOG_MEMORY\").is_ok()\n}\n"], "buggy_code_start_loc": [19, 8, 16, 35], "buggy_code_end_loc": [555, 79, 426, 47], "fixing_code_start_loc": [20, 9, 17, 36], "fixing_code_end_loc": [562, 83, 441, 54], "type": "CWE-416", "message": "Wasmtime is a standalone JIT-style runtime for WebAssembly, using Cranelift. There is a use after free vulnerability in Wasmtime when both running Wasm that uses externrefs and enabling epoch interruption in Wasmtime. If you are not explicitly enabling epoch interruption (it is disabled by default) then you are not affected. If you are explicitly disabling the Wasm reference types proposal (it is enabled by default) then you are also not affected. The use after free is caused by Cranelift failing to emit stack maps when there are safepoints inside cold blocks. Cold blocks occur when epoch interruption is enabled. Cold blocks are emitted at the end of compiled functions, and change the order blocks are emitted versus defined. This reordering accidentally caused Cranelift to skip emitting some stack maps because it expected to emit the stack maps in block definition order, rather than block emission order. When Wasmtime would eventually collect garbage, it would fail to find live references on the stack because of the missing stack maps, think that they were unreferenced garbage, and therefore reclaim them. Then after the collection ended, the Wasm code could use the reclaimed-too-early references, which is a use after free. Patches have been released in versions 0.34.2 and 0.35.2, which fix the vulnerability. All Wasmtime users are recommended to upgrade to these patched versions. If upgrading is not an option for you at this time, you can avoid the vulnerability by either: disabling the Wasm reference types proposal, config.wasm_reference_types(false); or by disabling epoch interruption if you were previously enabling it. config.epoch_interruption(false).", "other": {"cve": {"id": "CVE-2022-24791", "sourceIdentifier": "security-advisories@github.com", "published": "2022-03-31T23:15:08.063", "lastModified": "2022-04-08T16:35:28.077", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "Wasmtime is a standalone JIT-style runtime for WebAssembly, using Cranelift. There is a use after free vulnerability in Wasmtime when both running Wasm that uses externrefs and enabling epoch interruption in Wasmtime. If you are not explicitly enabling epoch interruption (it is disabled by default) then you are not affected. If you are explicitly disabling the Wasm reference types proposal (it is enabled by default) then you are also not affected. The use after free is caused by Cranelift failing to emit stack maps when there are safepoints inside cold blocks. Cold blocks occur when epoch interruption is enabled. Cold blocks are emitted at the end of compiled functions, and change the order blocks are emitted versus defined. This reordering accidentally caused Cranelift to skip emitting some stack maps because it expected to emit the stack maps in block definition order, rather than block emission order. When Wasmtime would eventually collect garbage, it would fail to find live references on the stack because of the missing stack maps, think that they were unreferenced garbage, and therefore reclaim them. Then after the collection ended, the Wasm code could use the reclaimed-too-early references, which is a use after free. Patches have been released in versions 0.34.2 and 0.35.2, which fix the vulnerability. All Wasmtime users are recommended to upgrade to these patched versions. If upgrading is not an option for you at this time, you can avoid the vulnerability by either: disabling the Wasm reference types proposal, config.wasm_reference_types(false); or by disabling epoch interruption if you were previously enabling it. config.epoch_interruption(false)."}, {"lang": "es", "value": "Wasmtime es un tiempo de ejecuci\u00f3n independiente de estilo JIT para WebAssembly, usando Cranelift. Se presenta una vulnerabilidad de uso de memoria previamente liberada en Wasmtime cuando es ejecutado Wasm que usa externrefs y es habilitada la interrupci\u00f3n de \u00e9poca en Wasmtime. Si no est\u00e1 habilitando expl\u00edcitamente la interrupci\u00f3n de \u00e9poca (est\u00e1 deshabilitada por defecto) entonces no est\u00e1 afectado. Si est\u00e1 deshabilitando expl\u00edcitamente la propuesta de tipos de referencia de Wasm (est\u00e1 habilitada por defecto) entonces tampoco le afecta. El uso de memoria previamente liberada es causado por Cranelift que no emite mapas de pila cuando se presentan puntos de seguridad dentro de los bloques fr\u00edos. Los bloques fr\u00edos son producidos cuando la interrupci\u00f3n de la \u00e9poca est\u00e1 habilitada. Los bloques fr\u00edos son emitidos al final de las funciones compiladas, y cambian el orden en que son emitidos los bloques frente a los definidos. Esta reordenaci\u00f3n caus\u00f3 accidentalmente que Cranelift omitiera la emisi\u00f3n de algunos mapas de pila porque esperaba emitir los mapas de pila en orden de definici\u00f3n de bloques, en lugar de en orden de emisi\u00f3n de bloques. Cuando Wasmtime recog\u00eda finalmente la basura, no encontraba referencias vivas en la pila debido a los mapas de pila faltantes, pensaba que eran basura no referenciada y, por tanto, los reclamaba. Entonces, una vez terminada la recolecci\u00f3n, el c\u00f3digo Wasm pod\u00eda usar las referencias recuperadas demasiado pronto, lo cual es un uso de memoria previamente liberada. Han sido publicados parches en versiones 0.34.2 y 0.35.2, que corrigen la vulnerabilidad. Es recomendado a todos los usuarios de Wasmtime actualizar a estas versiones parcheadas. Si actualizar no es una opci\u00f3n para ti en este momento, puedes evitar la vulnerabilidad: deshabilitando la propuesta de tipos de referencia de Wasm, config.wasm_reference_types(false); o deshabilitando la interrupci\u00f3n de \u00e9poca si la estabas habilitando previamente. config.epoch_interruption(false)"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 9.8, "baseSeverity": "CRITICAL"}, "exploitabilityScore": 3.9, "impactScore": 5.9}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:H/PR:N/UI:N/S:U/C:H/I:H/A:H", "attackVector": "NETWORK", "attackComplexity": "HIGH", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 8.1, "baseSeverity": "HIGH"}, "exploitabilityScore": 2.2, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:M/Au:N/C:P/I:P/A:P", "accessVector": "NETWORK", "accessComplexity": "MEDIUM", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "PARTIAL", "availabilityImpact": "PARTIAL", "baseScore": 6.8}, "baseSeverity": "MEDIUM", "exploitabilityScore": 8.6, "impactScore": 6.4, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "security-advisories@github.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-416"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:bytecodealliance:wasmtime:*:*:*:*:*:rust:*:*", "versionStartIncluding": "0.34.0", "versionEndExcluding": "0.34.2", "matchCriteriaId": "7ECB8D81-3539-4C1C-AE07-DEF4ABC95699"}, {"vulnerable": true, "criteria": "cpe:2.3:a:bytecodealliance:wasmtime:*:*:*:*:*:rust:*:*", "versionStartIncluding": "0.35.0", "versionEndExcluding": "0.35.2", "matchCriteriaId": "6A238AA9-2742-42B1-BFA7-9BE16087BCE5"}]}]}], "references": [{"url": "https://github.com/bytecodealliance/wasmtime/commit/666c2554ea0e1728c35aa41178cf235920db888a", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/bytecodealliance/wasmtime/security/advisories/GHSA-gwc9-348x-qwv2", "source": "security-advisories@github.com", "tags": ["Mitigation", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/bytecodealliance/wasmtime/commit/666c2554ea0e1728c35aa41178cf235920db888a"}}