{"buggy_code": ["// SPDX-License-Identifier: GPL-2.0-only\n/*\n * linux/kernel/ptrace.c\n *\n * (C) Copyright 1999 Linus Torvalds\n *\n * Common interfaces for \"ptrace()\" which we do not want\n * to continually duplicate across every architecture.\n */\n\n#include <linux/capability.h>\n#include <linux/export.h>\n#include <linux/sched.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/task.h>\n#include <linux/errno.h>\n#include <linux/mm.h>\n#include <linux/highmem.h>\n#include <linux/pagemap.h>\n#include <linux/ptrace.h>\n#include <linux/security.h>\n#include <linux/signal.h>\n#include <linux/uio.h>\n#include <linux/audit.h>\n#include <linux/pid_namespace.h>\n#include <linux/syscalls.h>\n#include <linux/uaccess.h>\n#include <linux/regset.h>\n#include <linux/hw_breakpoint.h>\n#include <linux/cn_proc.h>\n#include <linux/compat.h>\n#include <linux/sched/signal.h>\n#include <linux/minmax.h>\n\n#include <asm/syscall.h>\t/* for syscall_get_* */\n\n/*\n * Access another process' address space via ptrace.\n * Source/target buffer must be kernel space,\n * Do not walk the page table directly, use get_user_pages\n */\nint ptrace_access_vm(struct task_struct *tsk, unsigned long addr,\n\t\t     void *buf, int len, unsigned int gup_flags)\n{\n\tstruct mm_struct *mm;\n\tint ret;\n\n\tmm = get_task_mm(tsk);\n\tif (!mm)\n\t\treturn 0;\n\n\tif (!tsk->ptrace ||\n\t    (current != tsk->parent) ||\n\t    ((get_dumpable(mm) != SUID_DUMP_USER) &&\n\t     !ptracer_capable(tsk, mm->user_ns))) {\n\t\tmmput(mm);\n\t\treturn 0;\n\t}\n\n\tret = __access_remote_vm(mm, addr, buf, len, gup_flags);\n\tmmput(mm);\n\n\treturn ret;\n}\n\n\nvoid __ptrace_link(struct task_struct *child, struct task_struct *new_parent,\n\t\t   const struct cred *ptracer_cred)\n{\n\tBUG_ON(!list_empty(&child->ptrace_entry));\n\tlist_add(&child->ptrace_entry, &new_parent->ptraced);\n\tchild->parent = new_parent;\n\tchild->ptracer_cred = get_cred(ptracer_cred);\n}\n\n/*\n * ptrace a task: make the debugger its new parent and\n * move it to the ptrace list.\n *\n * Must be called with the tasklist lock write-held.\n */\nstatic void ptrace_link(struct task_struct *child, struct task_struct *new_parent)\n{\n\t__ptrace_link(child, new_parent, current_cred());\n}\n\n/**\n * __ptrace_unlink - unlink ptracee and restore its execution state\n * @child: ptracee to be unlinked\n *\n * Remove @child from the ptrace list, move it back to the original parent,\n * and restore the execution state so that it conforms to the group stop\n * state.\n *\n * Unlinking can happen via two paths - explicit PTRACE_DETACH or ptracer\n * exiting.  For PTRACE_DETACH, unless the ptracee has been killed between\n * ptrace_check_attach() and here, it's guaranteed to be in TASK_TRACED.\n * If the ptracer is exiting, the ptracee can be in any state.\n *\n * After detach, the ptracee should be in a state which conforms to the\n * group stop.  If the group is stopped or in the process of stopping, the\n * ptracee should be put into TASK_STOPPED; otherwise, it should be woken\n * up from TASK_TRACED.\n *\n * If the ptracee is in TASK_TRACED and needs to be moved to TASK_STOPPED,\n * it goes through TRACED -> RUNNING -> STOPPED transition which is similar\n * to but in the opposite direction of what happens while attaching to a\n * stopped task.  However, in this direction, the intermediate RUNNING\n * state is not hidden even from the current ptracer and if it immediately\n * re-attaches and performs a WNOHANG wait(2), it may fail.\n *\n * CONTEXT:\n * write_lock_irq(tasklist_lock)\n */\nvoid __ptrace_unlink(struct task_struct *child)\n{\n\tconst struct cred *old_cred;\n\tBUG_ON(!child->ptrace);\n\n\tclear_task_syscall_work(child, SYSCALL_TRACE);\n#if defined(CONFIG_GENERIC_ENTRY) || defined(TIF_SYSCALL_EMU)\n\tclear_task_syscall_work(child, SYSCALL_EMU);\n#endif\n\n\tchild->parent = child->real_parent;\n\tlist_del_init(&child->ptrace_entry);\n\told_cred = child->ptracer_cred;\n\tchild->ptracer_cred = NULL;\n\tput_cred(old_cred);\n\n\tspin_lock(&child->sighand->siglock);\n\tchild->ptrace = 0;\n\t/*\n\t * Clear all pending traps and TRAPPING.  TRAPPING should be\n\t * cleared regardless of JOBCTL_STOP_PENDING.  Do it explicitly.\n\t */\n\ttask_clear_jobctl_pending(child, JOBCTL_TRAP_MASK);\n\ttask_clear_jobctl_trapping(child);\n\n\t/*\n\t * Reinstate JOBCTL_STOP_PENDING if group stop is in effect and\n\t * @child isn't dead.\n\t */\n\tif (!(child->flags & PF_EXITING) &&\n\t    (child->signal->flags & SIGNAL_STOP_STOPPED ||\n\t     child->signal->group_stop_count)) {\n\t\tchild->jobctl |= JOBCTL_STOP_PENDING;\n\n\t\t/*\n\t\t * This is only possible if this thread was cloned by the\n\t\t * traced task running in the stopped group, set the signal\n\t\t * for the future reports.\n\t\t * FIXME: we should change ptrace_init_task() to handle this\n\t\t * case.\n\t\t */\n\t\tif (!(child->jobctl & JOBCTL_STOP_SIGMASK))\n\t\t\tchild->jobctl |= SIGSTOP;\n\t}\n\n\t/*\n\t * If transition to TASK_STOPPED is pending or in TASK_TRACED, kick\n\t * @child in the butt.  Note that @resume should be used iff @child\n\t * is in TASK_TRACED; otherwise, we might unduly disrupt\n\t * TASK_KILLABLE sleeps.\n\t */\n\tif (child->jobctl & JOBCTL_STOP_PENDING || task_is_traced(child))\n\t\tptrace_signal_wake_up(child, true);\n\n\tspin_unlock(&child->sighand->siglock);\n}\n\nstatic bool looks_like_a_spurious_pid(struct task_struct *task)\n{\n\tif (task->exit_code != ((PTRACE_EVENT_EXEC << 8) | SIGTRAP))\n\t\treturn false;\n\n\tif (task_pid_vnr(task) == task->ptrace_message)\n\t\treturn false;\n\t/*\n\t * The tracee changed its pid but the PTRACE_EVENT_EXEC event\n\t * was not wait()'ed, most probably debugger targets the old\n\t * leader which was destroyed in de_thread().\n\t */\n\treturn true;\n}\n\n/* Ensure that nothing can wake it up, even SIGKILL */\nstatic bool ptrace_freeze_traced(struct task_struct *task)\n{\n\tbool ret = false;\n\n\t/* Lockless, nobody but us can set this flag */\n\tif (task->jobctl & JOBCTL_LISTENING)\n\t\treturn ret;\n\n\tspin_lock_irq(&task->sighand->siglock);\n\tif (task_is_traced(task) && !looks_like_a_spurious_pid(task) &&\n\t    !__fatal_signal_pending(task)) {\n\t\tWRITE_ONCE(task->__state, __TASK_TRACED);\n\t\tret = true;\n\t}\n\tspin_unlock_irq(&task->sighand->siglock);\n\n\treturn ret;\n}\n\nstatic void ptrace_unfreeze_traced(struct task_struct *task)\n{\n\tif (READ_ONCE(task->__state) != __TASK_TRACED)\n\t\treturn;\n\n\tWARN_ON(!task->ptrace || task->parent != current);\n\n\t/*\n\t * PTRACE_LISTEN can allow ptrace_trap_notify to wake us up remotely.\n\t * Recheck state under the lock to close this race.\n\t */\n\tspin_lock_irq(&task->sighand->siglock);\n\tif (READ_ONCE(task->__state) == __TASK_TRACED) {\n\t\tif (__fatal_signal_pending(task))\n\t\t\twake_up_state(task, __TASK_TRACED);\n\t\telse\n\t\t\tWRITE_ONCE(task->__state, TASK_TRACED);\n\t}\n\tspin_unlock_irq(&task->sighand->siglock);\n}\n\n/**\n * ptrace_check_attach - check whether ptracee is ready for ptrace operation\n * @child: ptracee to check for\n * @ignore_state: don't check whether @child is currently %TASK_TRACED\n *\n * Check whether @child is being ptraced by %current and ready for further\n * ptrace operations.  If @ignore_state is %false, @child also should be in\n * %TASK_TRACED state and on return the child is guaranteed to be traced\n * and not executing.  If @ignore_state is %true, @child can be in any\n * state.\n *\n * CONTEXT:\n * Grabs and releases tasklist_lock and @child->sighand->siglock.\n *\n * RETURNS:\n * 0 on success, -ESRCH if %child is not ready.\n */\nstatic int ptrace_check_attach(struct task_struct *child, bool ignore_state)\n{\n\tint ret = -ESRCH;\n\n\t/*\n\t * We take the read lock around doing both checks to close a\n\t * possible race where someone else was tracing our child and\n\t * detached between these two checks.  After this locked check,\n\t * we are sure that this is our traced child and that can only\n\t * be changed by us so it's not changing right after this.\n\t */\n\tread_lock(&tasklist_lock);\n\tif (child->ptrace && child->parent == current) {\n\t\tWARN_ON(READ_ONCE(child->__state) == __TASK_TRACED);\n\t\t/*\n\t\t * child->sighand can't be NULL, release_task()\n\t\t * does ptrace_unlink() before __exit_signal().\n\t\t */\n\t\tif (ignore_state || ptrace_freeze_traced(child))\n\t\t\tret = 0;\n\t}\n\tread_unlock(&tasklist_lock);\n\n\tif (!ret && !ignore_state) {\n\t\tif (!wait_task_inactive(child, __TASK_TRACED)) {\n\t\t\t/*\n\t\t\t * This can only happen if may_ptrace_stop() fails and\n\t\t\t * ptrace_stop() changes ->state back to TASK_RUNNING,\n\t\t\t * so we should not worry about leaking __TASK_TRACED.\n\t\t\t */\n\t\t\tWARN_ON(READ_ONCE(child->__state) == __TASK_TRACED);\n\t\t\tret = -ESRCH;\n\t\t}\n\t}\n\n\treturn ret;\n}\n\nstatic bool ptrace_has_cap(struct user_namespace *ns, unsigned int mode)\n{\n\tif (mode & PTRACE_MODE_NOAUDIT)\n\t\treturn ns_capable_noaudit(ns, CAP_SYS_PTRACE);\n\treturn ns_capable(ns, CAP_SYS_PTRACE);\n}\n\n/* Returns 0 on success, -errno on denial. */\nstatic int __ptrace_may_access(struct task_struct *task, unsigned int mode)\n{\n\tconst struct cred *cred = current_cred(), *tcred;\n\tstruct mm_struct *mm;\n\tkuid_t caller_uid;\n\tkgid_t caller_gid;\n\n\tif (!(mode & PTRACE_MODE_FSCREDS) == !(mode & PTRACE_MODE_REALCREDS)) {\n\t\tWARN(1, \"denying ptrace access check without PTRACE_MODE_*CREDS\\n\");\n\t\treturn -EPERM;\n\t}\n\n\t/* May we inspect the given task?\n\t * This check is used both for attaching with ptrace\n\t * and for allowing access to sensitive information in /proc.\n\t *\n\t * ptrace_attach denies several cases that /proc allows\n\t * because setting up the necessary parent/child relationship\n\t * or halting the specified task is impossible.\n\t */\n\n\t/* Don't let security modules deny introspection */\n\tif (same_thread_group(task, current))\n\t\treturn 0;\n\trcu_read_lock();\n\tif (mode & PTRACE_MODE_FSCREDS) {\n\t\tcaller_uid = cred->fsuid;\n\t\tcaller_gid = cred->fsgid;\n\t} else {\n\t\t/*\n\t\t * Using the euid would make more sense here, but something\n\t\t * in userland might rely on the old behavior, and this\n\t\t * shouldn't be a security problem since\n\t\t * PTRACE_MODE_REALCREDS implies that the caller explicitly\n\t\t * used a syscall that requests access to another process\n\t\t * (and not a filesystem syscall to procfs).\n\t\t */\n\t\tcaller_uid = cred->uid;\n\t\tcaller_gid = cred->gid;\n\t}\n\ttcred = __task_cred(task);\n\tif (uid_eq(caller_uid, tcred->euid) &&\n\t    uid_eq(caller_uid, tcred->suid) &&\n\t    uid_eq(caller_uid, tcred->uid)  &&\n\t    gid_eq(caller_gid, tcred->egid) &&\n\t    gid_eq(caller_gid, tcred->sgid) &&\n\t    gid_eq(caller_gid, tcred->gid))\n\t\tgoto ok;\n\tif (ptrace_has_cap(tcred->user_ns, mode))\n\t\tgoto ok;\n\trcu_read_unlock();\n\treturn -EPERM;\nok:\n\trcu_read_unlock();\n\t/*\n\t * If a task drops privileges and becomes nondumpable (through a syscall\n\t * like setresuid()) while we are trying to access it, we must ensure\n\t * that the dumpability is read after the credentials; otherwise,\n\t * we may be able to attach to a task that we shouldn't be able to\n\t * attach to (as if the task had dropped privileges without becoming\n\t * nondumpable).\n\t * Pairs with a write barrier in commit_creds().\n\t */\n\tsmp_rmb();\n\tmm = task->mm;\n\tif (mm &&\n\t    ((get_dumpable(mm) != SUID_DUMP_USER) &&\n\t     !ptrace_has_cap(mm->user_ns, mode)))\n\t    return -EPERM;\n\n\treturn security_ptrace_access_check(task, mode);\n}\n\nbool ptrace_may_access(struct task_struct *task, unsigned int mode)\n{\n\tint err;\n\ttask_lock(task);\n\terr = __ptrace_may_access(task, mode);\n\ttask_unlock(task);\n\treturn !err;\n}\n\nstatic int ptrace_attach(struct task_struct *task, long request,\n\t\t\t unsigned long addr,\n\t\t\t unsigned long flags)\n{\n\tbool seize = (request == PTRACE_SEIZE);\n\tint retval;\n\n\tretval = -EIO;\n\tif (seize) {\n\t\tif (addr != 0)\n\t\t\tgoto out;\n\t\tif (flags & ~(unsigned long)PTRACE_O_MASK)\n\t\t\tgoto out;\n\t\tflags = PT_PTRACED | PT_SEIZED | (flags << PT_OPT_FLAG_SHIFT);\n\t} else {\n\t\tflags = PT_PTRACED;\n\t}\n\n\taudit_ptrace(task);\n\n\tretval = -EPERM;\n\tif (unlikely(task->flags & PF_KTHREAD))\n\t\tgoto out;\n\tif (same_thread_group(task, current))\n\t\tgoto out;\n\n\t/*\n\t * Protect exec's credential calculations against our interference;\n\t * SUID, SGID and LSM creds get determined differently\n\t * under ptrace.\n\t */\n\tretval = -ERESTARTNOINTR;\n\tif (mutex_lock_interruptible(&task->signal->cred_guard_mutex))\n\t\tgoto out;\n\n\ttask_lock(task);\n\tretval = __ptrace_may_access(task, PTRACE_MODE_ATTACH_REALCREDS);\n\ttask_unlock(task);\n\tif (retval)\n\t\tgoto unlock_creds;\n\n\twrite_lock_irq(&tasklist_lock);\n\tretval = -EPERM;\n\tif (unlikely(task->exit_state))\n\t\tgoto unlock_tasklist;\n\tif (task->ptrace)\n\t\tgoto unlock_tasklist;\n\n\ttask->ptrace = flags;\n\n\tptrace_link(task, current);\n\n\t/* SEIZE doesn't trap tracee on attach */\n\tif (!seize)\n\t\tsend_sig_info(SIGSTOP, SEND_SIG_PRIV, task);\n\n\tspin_lock(&task->sighand->siglock);\n\n\t/*\n\t * If the task is already STOPPED, set JOBCTL_TRAP_STOP and\n\t * TRAPPING, and kick it so that it transits to TRACED.  TRAPPING\n\t * will be cleared if the child completes the transition or any\n\t * event which clears the group stop states happens.  We'll wait\n\t * for the transition to complete before returning from this\n\t * function.\n\t *\n\t * This hides STOPPED -> RUNNING -> TRACED transition from the\n\t * attaching thread but a different thread in the same group can\n\t * still observe the transient RUNNING state.  IOW, if another\n\t * thread's WNOHANG wait(2) on the stopped tracee races against\n\t * ATTACH, the wait(2) may fail due to the transient RUNNING.\n\t *\n\t * The following task_is_stopped() test is safe as both transitions\n\t * in and out of STOPPED are protected by siglock.\n\t */\n\tif (task_is_stopped(task) &&\n\t    task_set_jobctl_pending(task, JOBCTL_TRAP_STOP | JOBCTL_TRAPPING))\n\t\tsignal_wake_up_state(task, __TASK_STOPPED);\n\n\tspin_unlock(&task->sighand->siglock);\n\n\tretval = 0;\nunlock_tasklist:\n\twrite_unlock_irq(&tasklist_lock);\nunlock_creds:\n\tmutex_unlock(&task->signal->cred_guard_mutex);\nout:\n\tif (!retval) {\n\t\t/*\n\t\t * We do not bother to change retval or clear JOBCTL_TRAPPING\n\t\t * if wait_on_bit() was interrupted by SIGKILL. The tracer will\n\t\t * not return to user-mode, it will exit and clear this bit in\n\t\t * __ptrace_unlink() if it wasn't already cleared by the tracee;\n\t\t * and until then nobody can ptrace this task.\n\t\t */\n\t\twait_on_bit(&task->jobctl, JOBCTL_TRAPPING_BIT, TASK_KILLABLE);\n\t\tproc_ptrace_connector(task, PTRACE_ATTACH);\n\t}\n\n\treturn retval;\n}\n\n/**\n * ptrace_traceme  --  helper for PTRACE_TRACEME\n *\n * Performs checks and sets PT_PTRACED.\n * Should be used by all ptrace implementations for PTRACE_TRACEME.\n */\nstatic int ptrace_traceme(void)\n{\n\tint ret = -EPERM;\n\n\twrite_lock_irq(&tasklist_lock);\n\t/* Are we already being traced? */\n\tif (!current->ptrace) {\n\t\tret = security_ptrace_traceme(current->parent);\n\t\t/*\n\t\t * Check PF_EXITING to ensure ->real_parent has not passed\n\t\t * exit_ptrace(). Otherwise we don't report the error but\n\t\t * pretend ->real_parent untraces us right after return.\n\t\t */\n\t\tif (!ret && !(current->real_parent->flags & PF_EXITING)) {\n\t\t\tcurrent->ptrace = PT_PTRACED;\n\t\t\tptrace_link(current, current->real_parent);\n\t\t}\n\t}\n\twrite_unlock_irq(&tasklist_lock);\n\n\treturn ret;\n}\n\n/*\n * Called with irqs disabled, returns true if childs should reap themselves.\n */\nstatic int ignoring_children(struct sighand_struct *sigh)\n{\n\tint ret;\n\tspin_lock(&sigh->siglock);\n\tret = (sigh->action[SIGCHLD-1].sa.sa_handler == SIG_IGN) ||\n\t      (sigh->action[SIGCHLD-1].sa.sa_flags & SA_NOCLDWAIT);\n\tspin_unlock(&sigh->siglock);\n\treturn ret;\n}\n\n/*\n * Called with tasklist_lock held for writing.\n * Unlink a traced task, and clean it up if it was a traced zombie.\n * Return true if it needs to be reaped with release_task().\n * (We can't call release_task() here because we already hold tasklist_lock.)\n *\n * If it's a zombie, our attachedness prevented normal parent notification\n * or self-reaping.  Do notification now if it would have happened earlier.\n * If it should reap itself, return true.\n *\n * If it's our own child, there is no notification to do. But if our normal\n * children self-reap, then this child was prevented by ptrace and we must\n * reap it now, in that case we must also wake up sub-threads sleeping in\n * do_wait().\n */\nstatic bool __ptrace_detach(struct task_struct *tracer, struct task_struct *p)\n{\n\tbool dead;\n\n\t__ptrace_unlink(p);\n\n\tif (p->exit_state != EXIT_ZOMBIE)\n\t\treturn false;\n\n\tdead = !thread_group_leader(p);\n\n\tif (!dead && thread_group_empty(p)) {\n\t\tif (!same_thread_group(p->real_parent, tracer))\n\t\t\tdead = do_notify_parent(p, p->exit_signal);\n\t\telse if (ignoring_children(tracer->sighand)) {\n\t\t\t__wake_up_parent(p, tracer);\n\t\t\tdead = true;\n\t\t}\n\t}\n\t/* Mark it as in the process of being reaped. */\n\tif (dead)\n\t\tp->exit_state = EXIT_DEAD;\n\treturn dead;\n}\n\nstatic int ptrace_detach(struct task_struct *child, unsigned int data)\n{\n\tif (!valid_signal(data))\n\t\treturn -EIO;\n\n\t/* Architecture-specific hardware disable .. */\n\tptrace_disable(child);\n\n\twrite_lock_irq(&tasklist_lock);\n\t/*\n\t * We rely on ptrace_freeze_traced(). It can't be killed and\n\t * untraced by another thread, it can't be a zombie.\n\t */\n\tWARN_ON(!child->ptrace || child->exit_state);\n\t/*\n\t * tasklist_lock avoids the race with wait_task_stopped(), see\n\t * the comment in ptrace_resume().\n\t */\n\tchild->exit_code = data;\n\t__ptrace_detach(current, child);\n\twrite_unlock_irq(&tasklist_lock);\n\n\tproc_ptrace_connector(child, PTRACE_DETACH);\n\n\treturn 0;\n}\n\n/*\n * Detach all tasks we were using ptrace on. Called with tasklist held\n * for writing.\n */\nvoid exit_ptrace(struct task_struct *tracer, struct list_head *dead)\n{\n\tstruct task_struct *p, *n;\n\n\tlist_for_each_entry_safe(p, n, &tracer->ptraced, ptrace_entry) {\n\t\tif (unlikely(p->ptrace & PT_EXITKILL))\n\t\t\tsend_sig_info(SIGKILL, SEND_SIG_PRIV, p);\n\n\t\tif (__ptrace_detach(tracer, p))\n\t\t\tlist_add(&p->ptrace_entry, dead);\n\t}\n}\n\nint ptrace_readdata(struct task_struct *tsk, unsigned long src, char __user *dst, int len)\n{\n\tint copied = 0;\n\n\twhile (len > 0) {\n\t\tchar buf[128];\n\t\tint this_len, retval;\n\n\t\tthis_len = (len > sizeof(buf)) ? sizeof(buf) : len;\n\t\tretval = ptrace_access_vm(tsk, src, buf, this_len, FOLL_FORCE);\n\n\t\tif (!retval) {\n\t\t\tif (copied)\n\t\t\t\tbreak;\n\t\t\treturn -EIO;\n\t\t}\n\t\tif (copy_to_user(dst, buf, retval))\n\t\t\treturn -EFAULT;\n\t\tcopied += retval;\n\t\tsrc += retval;\n\t\tdst += retval;\n\t\tlen -= retval;\n\t}\n\treturn copied;\n}\n\nint ptrace_writedata(struct task_struct *tsk, char __user *src, unsigned long dst, int len)\n{\n\tint copied = 0;\n\n\twhile (len > 0) {\n\t\tchar buf[128];\n\t\tint this_len, retval;\n\n\t\tthis_len = (len > sizeof(buf)) ? sizeof(buf) : len;\n\t\tif (copy_from_user(buf, src, this_len))\n\t\t\treturn -EFAULT;\n\t\tretval = ptrace_access_vm(tsk, dst, buf, this_len,\n\t\t\t\tFOLL_FORCE | FOLL_WRITE);\n\t\tif (!retval) {\n\t\t\tif (copied)\n\t\t\t\tbreak;\n\t\t\treturn -EIO;\n\t\t}\n\t\tcopied += retval;\n\t\tsrc += retval;\n\t\tdst += retval;\n\t\tlen -= retval;\n\t}\n\treturn copied;\n}\n\nstatic int ptrace_setoptions(struct task_struct *child, unsigned long data)\n{\n\tunsigned flags;\n\n\tif (data & ~(unsigned long)PTRACE_O_MASK)\n\t\treturn -EINVAL;\n\n\tif (unlikely(data & PTRACE_O_SUSPEND_SECCOMP)) {\n\t\tif (!IS_ENABLED(CONFIG_CHECKPOINT_RESTORE) ||\n\t\t    !IS_ENABLED(CONFIG_SECCOMP))\n\t\t\treturn -EINVAL;\n\n\t\tif (!capable(CAP_SYS_ADMIN))\n\t\t\treturn -EPERM;\n\n\t\tif (seccomp_mode(&current->seccomp) != SECCOMP_MODE_DISABLED ||\n\t\t    current->ptrace & PT_SUSPEND_SECCOMP)\n\t\t\treturn -EPERM;\n\t}\n\n\t/* Avoid intermediate state when all opts are cleared */\n\tflags = child->ptrace;\n\tflags &= ~(PTRACE_O_MASK << PT_OPT_FLAG_SHIFT);\n\tflags |= (data << PT_OPT_FLAG_SHIFT);\n\tchild->ptrace = flags;\n\n\treturn 0;\n}\n\nstatic int ptrace_getsiginfo(struct task_struct *child, kernel_siginfo_t *info)\n{\n\tunsigned long flags;\n\tint error = -ESRCH;\n\n\tif (lock_task_sighand(child, &flags)) {\n\t\terror = -EINVAL;\n\t\tif (likely(child->last_siginfo != NULL)) {\n\t\t\tcopy_siginfo(info, child->last_siginfo);\n\t\t\terror = 0;\n\t\t}\n\t\tunlock_task_sighand(child, &flags);\n\t}\n\treturn error;\n}\n\nstatic int ptrace_setsiginfo(struct task_struct *child, const kernel_siginfo_t *info)\n{\n\tunsigned long flags;\n\tint error = -ESRCH;\n\n\tif (lock_task_sighand(child, &flags)) {\n\t\terror = -EINVAL;\n\t\tif (likely(child->last_siginfo != NULL)) {\n\t\t\tcopy_siginfo(child->last_siginfo, info);\n\t\t\terror = 0;\n\t\t}\n\t\tunlock_task_sighand(child, &flags);\n\t}\n\treturn error;\n}\n\nstatic int ptrace_peek_siginfo(struct task_struct *child,\n\t\t\t\tunsigned long addr,\n\t\t\t\tunsigned long data)\n{\n\tstruct ptrace_peeksiginfo_args arg;\n\tstruct sigpending *pending;\n\tstruct sigqueue *q;\n\tint ret, i;\n\n\tret = copy_from_user(&arg, (void __user *) addr,\n\t\t\t\tsizeof(struct ptrace_peeksiginfo_args));\n\tif (ret)\n\t\treturn -EFAULT;\n\n\tif (arg.flags & ~PTRACE_PEEKSIGINFO_SHARED)\n\t\treturn -EINVAL; /* unknown flags */\n\n\tif (arg.nr < 0)\n\t\treturn -EINVAL;\n\n\t/* Ensure arg.off fits in an unsigned long */\n\tif (arg.off > ULONG_MAX)\n\t\treturn 0;\n\n\tif (arg.flags & PTRACE_PEEKSIGINFO_SHARED)\n\t\tpending = &child->signal->shared_pending;\n\telse\n\t\tpending = &child->pending;\n\n\tfor (i = 0; i < arg.nr; ) {\n\t\tkernel_siginfo_t info;\n\t\tunsigned long off = arg.off + i;\n\t\tbool found = false;\n\n\t\tspin_lock_irq(&child->sighand->siglock);\n\t\tlist_for_each_entry(q, &pending->list, list) {\n\t\t\tif (!off--) {\n\t\t\t\tfound = true;\n\t\t\t\tcopy_siginfo(&info, &q->info);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tspin_unlock_irq(&child->sighand->siglock);\n\n\t\tif (!found) /* beyond the end of the list */\n\t\t\tbreak;\n\n#ifdef CONFIG_COMPAT\n\t\tif (unlikely(in_compat_syscall())) {\n\t\t\tcompat_siginfo_t __user *uinfo = compat_ptr(data);\n\n\t\t\tif (copy_siginfo_to_user32(uinfo, &info)) {\n\t\t\t\tret = -EFAULT;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t} else\n#endif\n\t\t{\n\t\t\tsiginfo_t __user *uinfo = (siginfo_t __user *) data;\n\n\t\t\tif (copy_siginfo_to_user(uinfo, &info)) {\n\t\t\t\tret = -EFAULT;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tdata += sizeof(siginfo_t);\n\t\ti++;\n\n\t\tif (signal_pending(current))\n\t\t\tbreak;\n\n\t\tcond_resched();\n\t}\n\n\tif (i > 0)\n\t\treturn i;\n\n\treturn ret;\n}\n\n#ifdef CONFIG_RSEQ\nstatic long ptrace_get_rseq_configuration(struct task_struct *task,\n\t\t\t\t\t  unsigned long size, void __user *data)\n{\n\tstruct ptrace_rseq_configuration conf = {\n\t\t.rseq_abi_pointer = (u64)(uintptr_t)task->rseq,\n\t\t.rseq_abi_size = sizeof(*task->rseq),\n\t\t.signature = task->rseq_sig,\n\t\t.flags = 0,\n\t};\n\n\tsize = min_t(unsigned long, size, sizeof(conf));\n\tif (copy_to_user(data, &conf, size))\n\t\treturn -EFAULT;\n\treturn sizeof(conf);\n}\n#endif\n\n#ifdef PTRACE_SINGLESTEP\n#define is_singlestep(request)\t\t((request) == PTRACE_SINGLESTEP)\n#else\n#define is_singlestep(request)\t\t0\n#endif\n\n#ifdef PTRACE_SINGLEBLOCK\n#define is_singleblock(request)\t\t((request) == PTRACE_SINGLEBLOCK)\n#else\n#define is_singleblock(request)\t\t0\n#endif\n\n#ifdef PTRACE_SYSEMU\n#define is_sysemu_singlestep(request)\t((request) == PTRACE_SYSEMU_SINGLESTEP)\n#else\n#define is_sysemu_singlestep(request)\t0\n#endif\n\nstatic int ptrace_resume(struct task_struct *child, long request,\n\t\t\t unsigned long data)\n{\n\tbool need_siglock;\n\n\tif (!valid_signal(data))\n\t\treturn -EIO;\n\n\tif (request == PTRACE_SYSCALL)\n\t\tset_task_syscall_work(child, SYSCALL_TRACE);\n\telse\n\t\tclear_task_syscall_work(child, SYSCALL_TRACE);\n\n#if defined(CONFIG_GENERIC_ENTRY) || defined(TIF_SYSCALL_EMU)\n\tif (request == PTRACE_SYSEMU || request == PTRACE_SYSEMU_SINGLESTEP)\n\t\tset_task_syscall_work(child, SYSCALL_EMU);\n\telse\n\t\tclear_task_syscall_work(child, SYSCALL_EMU);\n#endif\n\n\tif (is_singleblock(request)) {\n\t\tif (unlikely(!arch_has_block_step()))\n\t\t\treturn -EIO;\n\t\tuser_enable_block_step(child);\n\t} else if (is_singlestep(request) || is_sysemu_singlestep(request)) {\n\t\tif (unlikely(!arch_has_single_step()))\n\t\t\treturn -EIO;\n\t\tuser_enable_single_step(child);\n\t} else {\n\t\tuser_disable_single_step(child);\n\t}\n\n\t/*\n\t * Change ->exit_code and ->state under siglock to avoid the race\n\t * with wait_task_stopped() in between; a non-zero ->exit_code will\n\t * wrongly look like another report from tracee.\n\t *\n\t * Note that we need siglock even if ->exit_code == data and/or this\n\t * status was not reported yet, the new status must not be cleared by\n\t * wait_task_stopped() after resume.\n\t *\n\t * If data == 0 we do not care if wait_task_stopped() reports the old\n\t * status and clears the code too; this can't race with the tracee, it\n\t * takes siglock after resume.\n\t */\n\tneed_siglock = data && !thread_group_empty(current);\n\tif (need_siglock)\n\t\tspin_lock_irq(&child->sighand->siglock);\n\tchild->exit_code = data;\n\twake_up_state(child, __TASK_TRACED);\n\tif (need_siglock)\n\t\tspin_unlock_irq(&child->sighand->siglock);\n\n\treturn 0;\n}\n\n#ifdef CONFIG_HAVE_ARCH_TRACEHOOK\n\nstatic const struct user_regset *\nfind_regset(const struct user_regset_view *view, unsigned int type)\n{\n\tconst struct user_regset *regset;\n\tint n;\n\n\tfor (n = 0; n < view->n; ++n) {\n\t\tregset = view->regsets + n;\n\t\tif (regset->core_note_type == type)\n\t\t\treturn regset;\n\t}\n\n\treturn NULL;\n}\n\nstatic int ptrace_regset(struct task_struct *task, int req, unsigned int type,\n\t\t\t struct iovec *kiov)\n{\n\tconst struct user_regset_view *view = task_user_regset_view(task);\n\tconst struct user_regset *regset = find_regset(view, type);\n\tint regset_no;\n\n\tif (!regset || (kiov->iov_len % regset->size) != 0)\n\t\treturn -EINVAL;\n\n\tregset_no = regset - view->regsets;\n\tkiov->iov_len = min(kiov->iov_len,\n\t\t\t    (__kernel_size_t) (regset->n * regset->size));\n\n\tif (req == PTRACE_GETREGSET)\n\t\treturn copy_regset_to_user(task, view, regset_no, 0,\n\t\t\t\t\t   kiov->iov_len, kiov->iov_base);\n\telse\n\t\treturn copy_regset_from_user(task, view, regset_no, 0,\n\t\t\t\t\t     kiov->iov_len, kiov->iov_base);\n}\n\n/*\n * This is declared in linux/regset.h and defined in machine-dependent\n * code.  We put the export here, near the primary machine-neutral use,\n * to ensure no machine forgets it.\n */\nEXPORT_SYMBOL_GPL(task_user_regset_view);\n\nstatic unsigned long\nptrace_get_syscall_info_entry(struct task_struct *child, struct pt_regs *regs,\n\t\t\t      struct ptrace_syscall_info *info)\n{\n\tunsigned long args[ARRAY_SIZE(info->entry.args)];\n\tint i;\n\n\tinfo->op = PTRACE_SYSCALL_INFO_ENTRY;\n\tinfo->entry.nr = syscall_get_nr(child, regs);\n\tsyscall_get_arguments(child, regs, args);\n\tfor (i = 0; i < ARRAY_SIZE(args); i++)\n\t\tinfo->entry.args[i] = args[i];\n\n\t/* args is the last field in struct ptrace_syscall_info.entry */\n\treturn offsetofend(struct ptrace_syscall_info, entry.args);\n}\n\nstatic unsigned long\nptrace_get_syscall_info_seccomp(struct task_struct *child, struct pt_regs *regs,\n\t\t\t\tstruct ptrace_syscall_info *info)\n{\n\t/*\n\t * As struct ptrace_syscall_info.entry is currently a subset\n\t * of struct ptrace_syscall_info.seccomp, it makes sense to\n\t * initialize that subset using ptrace_get_syscall_info_entry().\n\t * This can be reconsidered in the future if these structures\n\t * diverge significantly enough.\n\t */\n\tptrace_get_syscall_info_entry(child, regs, info);\n\tinfo->op = PTRACE_SYSCALL_INFO_SECCOMP;\n\tinfo->seccomp.ret_data = child->ptrace_message;\n\n\t/* ret_data is the last field in struct ptrace_syscall_info.seccomp */\n\treturn offsetofend(struct ptrace_syscall_info, seccomp.ret_data);\n}\n\nstatic unsigned long\nptrace_get_syscall_info_exit(struct task_struct *child, struct pt_regs *regs,\n\t\t\t     struct ptrace_syscall_info *info)\n{\n\tinfo->op = PTRACE_SYSCALL_INFO_EXIT;\n\tinfo->exit.rval = syscall_get_error(child, regs);\n\tinfo->exit.is_error = !!info->exit.rval;\n\tif (!info->exit.is_error)\n\t\tinfo->exit.rval = syscall_get_return_value(child, regs);\n\n\t/* is_error is the last field in struct ptrace_syscall_info.exit */\n\treturn offsetofend(struct ptrace_syscall_info, exit.is_error);\n}\n\nstatic int\nptrace_get_syscall_info(struct task_struct *child, unsigned long user_size,\n\t\t\tvoid __user *datavp)\n{\n\tstruct pt_regs *regs = task_pt_regs(child);\n\tstruct ptrace_syscall_info info = {\n\t\t.op = PTRACE_SYSCALL_INFO_NONE,\n\t\t.arch = syscall_get_arch(child),\n\t\t.instruction_pointer = instruction_pointer(regs),\n\t\t.stack_pointer = user_stack_pointer(regs),\n\t};\n\tunsigned long actual_size = offsetof(struct ptrace_syscall_info, entry);\n\tunsigned long write_size;\n\n\t/*\n\t * This does not need lock_task_sighand() to access\n\t * child->last_siginfo because ptrace_freeze_traced()\n\t * called earlier by ptrace_check_attach() ensures that\n\t * the tracee cannot go away and clear its last_siginfo.\n\t */\n\tswitch (child->last_siginfo ? child->last_siginfo->si_code : 0) {\n\tcase SIGTRAP | 0x80:\n\t\tswitch (child->ptrace_message) {\n\t\tcase PTRACE_EVENTMSG_SYSCALL_ENTRY:\n\t\t\tactual_size = ptrace_get_syscall_info_entry(child, regs,\n\t\t\t\t\t\t\t\t    &info);\n\t\t\tbreak;\n\t\tcase PTRACE_EVENTMSG_SYSCALL_EXIT:\n\t\t\tactual_size = ptrace_get_syscall_info_exit(child, regs,\n\t\t\t\t\t\t\t\t   &info);\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase SIGTRAP | (PTRACE_EVENT_SECCOMP << 8):\n\t\tactual_size = ptrace_get_syscall_info_seccomp(child, regs,\n\t\t\t\t\t\t\t      &info);\n\t\tbreak;\n\t}\n\n\twrite_size = min(actual_size, user_size);\n\treturn copy_to_user(datavp, &info, write_size) ? -EFAULT : actual_size;\n}\n#endif /* CONFIG_HAVE_ARCH_TRACEHOOK */\n\nint ptrace_request(struct task_struct *child, long request,\n\t\t   unsigned long addr, unsigned long data)\n{\n\tbool seized = child->ptrace & PT_SEIZED;\n\tint ret = -EIO;\n\tkernel_siginfo_t siginfo, *si;\n\tvoid __user *datavp = (void __user *) data;\n\tunsigned long __user *datalp = datavp;\n\tunsigned long flags;\n\n\tswitch (request) {\n\tcase PTRACE_PEEKTEXT:\n\tcase PTRACE_PEEKDATA:\n\t\treturn generic_ptrace_peekdata(child, addr, data);\n\tcase PTRACE_POKETEXT:\n\tcase PTRACE_POKEDATA:\n\t\treturn generic_ptrace_pokedata(child, addr, data);\n\n#ifdef PTRACE_OLDSETOPTIONS\n\tcase PTRACE_OLDSETOPTIONS:\n#endif\n\tcase PTRACE_SETOPTIONS:\n\t\tret = ptrace_setoptions(child, data);\n\t\tbreak;\n\tcase PTRACE_GETEVENTMSG:\n\t\tret = put_user(child->ptrace_message, datalp);\n\t\tbreak;\n\n\tcase PTRACE_PEEKSIGINFO:\n\t\tret = ptrace_peek_siginfo(child, addr, data);\n\t\tbreak;\n\n\tcase PTRACE_GETSIGINFO:\n\t\tret = ptrace_getsiginfo(child, &siginfo);\n\t\tif (!ret)\n\t\t\tret = copy_siginfo_to_user(datavp, &siginfo);\n\t\tbreak;\n\n\tcase PTRACE_SETSIGINFO:\n\t\tret = copy_siginfo_from_user(&siginfo, datavp);\n\t\tif (!ret)\n\t\t\tret = ptrace_setsiginfo(child, &siginfo);\n\t\tbreak;\n\n\tcase PTRACE_GETSIGMASK: {\n\t\tsigset_t *mask;\n\n\t\tif (addr != sizeof(sigset_t)) {\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (test_tsk_restore_sigmask(child))\n\t\t\tmask = &child->saved_sigmask;\n\t\telse\n\t\t\tmask = &child->blocked;\n\n\t\tif (copy_to_user(datavp, mask, sizeof(sigset_t)))\n\t\t\tret = -EFAULT;\n\t\telse\n\t\t\tret = 0;\n\n\t\tbreak;\n\t}\n\n\tcase PTRACE_SETSIGMASK: {\n\t\tsigset_t new_set;\n\n\t\tif (addr != sizeof(sigset_t)) {\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (copy_from_user(&new_set, datavp, sizeof(sigset_t))) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\n\t\tsigdelsetmask(&new_set, sigmask(SIGKILL)|sigmask(SIGSTOP));\n\n\t\t/*\n\t\t * Every thread does recalc_sigpending() after resume, so\n\t\t * retarget_shared_pending() and recalc_sigpending() are not\n\t\t * called here.\n\t\t */\n\t\tspin_lock_irq(&child->sighand->siglock);\n\t\tchild->blocked = new_set;\n\t\tspin_unlock_irq(&child->sighand->siglock);\n\n\t\tclear_tsk_restore_sigmask(child);\n\n\t\tret = 0;\n\t\tbreak;\n\t}\n\n\tcase PTRACE_INTERRUPT:\n\t\t/*\n\t\t * Stop tracee without any side-effect on signal or job\n\t\t * control.  At least one trap is guaranteed to happen\n\t\t * after this request.  If @child is already trapped, the\n\t\t * current trap is not disturbed and another trap will\n\t\t * happen after the current trap is ended with PTRACE_CONT.\n\t\t *\n\t\t * The actual trap might not be PTRACE_EVENT_STOP trap but\n\t\t * the pending condition is cleared regardless.\n\t\t */\n\t\tif (unlikely(!seized || !lock_task_sighand(child, &flags)))\n\t\t\tbreak;\n\n\t\t/*\n\t\t * INTERRUPT doesn't disturb existing trap sans one\n\t\t * exception.  If ptracer issued LISTEN for the current\n\t\t * STOP, this INTERRUPT should clear LISTEN and re-trap\n\t\t * tracee into STOP.\n\t\t */\n\t\tif (likely(task_set_jobctl_pending(child, JOBCTL_TRAP_STOP)))\n\t\t\tptrace_signal_wake_up(child, child->jobctl & JOBCTL_LISTENING);\n\n\t\tunlock_task_sighand(child, &flags);\n\t\tret = 0;\n\t\tbreak;\n\n\tcase PTRACE_LISTEN:\n\t\t/*\n\t\t * Listen for events.  Tracee must be in STOP.  It's not\n\t\t * resumed per-se but is not considered to be in TRACED by\n\t\t * wait(2) or ptrace(2).  If an async event (e.g. group\n\t\t * stop state change) happens, tracee will enter STOP trap\n\t\t * again.  Alternatively, ptracer can issue INTERRUPT to\n\t\t * finish listening and re-trap tracee into STOP.\n\t\t */\n\t\tif (unlikely(!seized || !lock_task_sighand(child, &flags)))\n\t\t\tbreak;\n\n\t\tsi = child->last_siginfo;\n\t\tif (likely(si && (si->si_code >> 8) == PTRACE_EVENT_STOP)) {\n\t\t\tchild->jobctl |= JOBCTL_LISTENING;\n\t\t\t/*\n\t\t\t * If NOTIFY is set, it means event happened between\n\t\t\t * start of this trap and now.  Trigger re-trap.\n\t\t\t */\n\t\t\tif (child->jobctl & JOBCTL_TRAP_NOTIFY)\n\t\t\t\tptrace_signal_wake_up(child, true);\n\t\t\tret = 0;\n\t\t}\n\t\tunlock_task_sighand(child, &flags);\n\t\tbreak;\n\n\tcase PTRACE_DETACH:\t /* detach a process that was attached. */\n\t\tret = ptrace_detach(child, data);\n\t\tbreak;\n\n#ifdef CONFIG_BINFMT_ELF_FDPIC\n\tcase PTRACE_GETFDPIC: {\n\t\tstruct mm_struct *mm = get_task_mm(child);\n\t\tunsigned long tmp = 0;\n\n\t\tret = -ESRCH;\n\t\tif (!mm)\n\t\t\tbreak;\n\n\t\tswitch (addr) {\n\t\tcase PTRACE_GETFDPIC_EXEC:\n\t\t\ttmp = mm->context.exec_fdpic_loadmap;\n\t\t\tbreak;\n\t\tcase PTRACE_GETFDPIC_INTERP:\n\t\t\ttmp = mm->context.interp_fdpic_loadmap;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\tmmput(mm);\n\n\t\tret = put_user(tmp, datalp);\n\t\tbreak;\n\t}\n#endif\n\n#ifdef PTRACE_SINGLESTEP\n\tcase PTRACE_SINGLESTEP:\n#endif\n#ifdef PTRACE_SINGLEBLOCK\n\tcase PTRACE_SINGLEBLOCK:\n#endif\n#ifdef PTRACE_SYSEMU\n\tcase PTRACE_SYSEMU:\n\tcase PTRACE_SYSEMU_SINGLESTEP:\n#endif\n\tcase PTRACE_SYSCALL:\n\tcase PTRACE_CONT:\n\t\treturn ptrace_resume(child, request, data);\n\n\tcase PTRACE_KILL:\n\t\tif (child->exit_state)\t/* already dead */\n\t\t\treturn 0;\n\t\treturn ptrace_resume(child, request, SIGKILL);\n\n#ifdef CONFIG_HAVE_ARCH_TRACEHOOK\n\tcase PTRACE_GETREGSET:\n\tcase PTRACE_SETREGSET: {\n\t\tstruct iovec kiov;\n\t\tstruct iovec __user *uiov = datavp;\n\n\t\tif (!access_ok(uiov, sizeof(*uiov)))\n\t\t\treturn -EFAULT;\n\n\t\tif (__get_user(kiov.iov_base, &uiov->iov_base) ||\n\t\t    __get_user(kiov.iov_len, &uiov->iov_len))\n\t\t\treturn -EFAULT;\n\n\t\tret = ptrace_regset(child, request, addr, &kiov);\n\t\tif (!ret)\n\t\t\tret = __put_user(kiov.iov_len, &uiov->iov_len);\n\t\tbreak;\n\t}\n\n\tcase PTRACE_GET_SYSCALL_INFO:\n\t\tret = ptrace_get_syscall_info(child, addr, datavp);\n\t\tbreak;\n#endif\n\n\tcase PTRACE_SECCOMP_GET_FILTER:\n\t\tret = seccomp_get_filter(child, addr, datavp);\n\t\tbreak;\n\n\tcase PTRACE_SECCOMP_GET_METADATA:\n\t\tret = seccomp_get_metadata(child, addr, datavp);\n\t\tbreak;\n\n#ifdef CONFIG_RSEQ\n\tcase PTRACE_GET_RSEQ_CONFIGURATION:\n\t\tret = ptrace_get_rseq_configuration(child, addr, datavp);\n\t\tbreak;\n#endif\n\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\n#ifndef arch_ptrace_attach\n#define arch_ptrace_attach(child)\tdo { } while (0)\n#endif\n\nSYSCALL_DEFINE4(ptrace, long, request, long, pid, unsigned long, addr,\n\t\tunsigned long, data)\n{\n\tstruct task_struct *child;\n\tlong ret;\n\n\tif (request == PTRACE_TRACEME) {\n\t\tret = ptrace_traceme();\n\t\tif (!ret)\n\t\t\tarch_ptrace_attach(current);\n\t\tgoto out;\n\t}\n\n\tchild = find_get_task_by_vpid(pid);\n\tif (!child) {\n\t\tret = -ESRCH;\n\t\tgoto out;\n\t}\n\n\tif (request == PTRACE_ATTACH || request == PTRACE_SEIZE) {\n\t\tret = ptrace_attach(child, request, addr, data);\n\t\t/*\n\t\t * Some architectures need to do book-keeping after\n\t\t * a ptrace attach.\n\t\t */\n\t\tif (!ret)\n\t\t\tarch_ptrace_attach(child);\n\t\tgoto out_put_task_struct;\n\t}\n\n\tret = ptrace_check_attach(child, request == PTRACE_KILL ||\n\t\t\t\t  request == PTRACE_INTERRUPT);\n\tif (ret < 0)\n\t\tgoto out_put_task_struct;\n\n\tret = arch_ptrace(child, request, addr, data);\n\tif (ret || request != PTRACE_DETACH)\n\t\tptrace_unfreeze_traced(child);\n\n out_put_task_struct:\n\tput_task_struct(child);\n out:\n\treturn ret;\n}\n\nint generic_ptrace_peekdata(struct task_struct *tsk, unsigned long addr,\n\t\t\t    unsigned long data)\n{\n\tunsigned long tmp;\n\tint copied;\n\n\tcopied = ptrace_access_vm(tsk, addr, &tmp, sizeof(tmp), FOLL_FORCE);\n\tif (copied != sizeof(tmp))\n\t\treturn -EIO;\n\treturn put_user(tmp, (unsigned long __user *)data);\n}\n\nint generic_ptrace_pokedata(struct task_struct *tsk, unsigned long addr,\n\t\t\t    unsigned long data)\n{\n\tint copied;\n\n\tcopied = ptrace_access_vm(tsk, addr, &data, sizeof(data),\n\t\t\tFOLL_FORCE | FOLL_WRITE);\n\treturn (copied == sizeof(data)) ? 0 : -EIO;\n}\n\n#if defined CONFIG_COMPAT\n\nint compat_ptrace_request(struct task_struct *child, compat_long_t request,\n\t\t\t  compat_ulong_t addr, compat_ulong_t data)\n{\n\tcompat_ulong_t __user *datap = compat_ptr(data);\n\tcompat_ulong_t word;\n\tkernel_siginfo_t siginfo;\n\tint ret;\n\n\tswitch (request) {\n\tcase PTRACE_PEEKTEXT:\n\tcase PTRACE_PEEKDATA:\n\t\tret = ptrace_access_vm(child, addr, &word, sizeof(word),\n\t\t\t\tFOLL_FORCE);\n\t\tif (ret != sizeof(word))\n\t\t\tret = -EIO;\n\t\telse\n\t\t\tret = put_user(word, datap);\n\t\tbreak;\n\n\tcase PTRACE_POKETEXT:\n\tcase PTRACE_POKEDATA:\n\t\tret = ptrace_access_vm(child, addr, &data, sizeof(data),\n\t\t\t\tFOLL_FORCE | FOLL_WRITE);\n\t\tret = (ret != sizeof(data) ? -EIO : 0);\n\t\tbreak;\n\n\tcase PTRACE_GETEVENTMSG:\n\t\tret = put_user((compat_ulong_t) child->ptrace_message, datap);\n\t\tbreak;\n\n\tcase PTRACE_GETSIGINFO:\n\t\tret = ptrace_getsiginfo(child, &siginfo);\n\t\tif (!ret)\n\t\t\tret = copy_siginfo_to_user32(\n\t\t\t\t(struct compat_siginfo __user *) datap,\n\t\t\t\t&siginfo);\n\t\tbreak;\n\n\tcase PTRACE_SETSIGINFO:\n\t\tret = copy_siginfo_from_user32(\n\t\t\t&siginfo, (struct compat_siginfo __user *) datap);\n\t\tif (!ret)\n\t\t\tret = ptrace_setsiginfo(child, &siginfo);\n\t\tbreak;\n#ifdef CONFIG_HAVE_ARCH_TRACEHOOK\n\tcase PTRACE_GETREGSET:\n\tcase PTRACE_SETREGSET:\n\t{\n\t\tstruct iovec kiov;\n\t\tstruct compat_iovec __user *uiov =\n\t\t\t(struct compat_iovec __user *) datap;\n\t\tcompat_uptr_t ptr;\n\t\tcompat_size_t len;\n\n\t\tif (!access_ok(uiov, sizeof(*uiov)))\n\t\t\treturn -EFAULT;\n\n\t\tif (__get_user(ptr, &uiov->iov_base) ||\n\t\t    __get_user(len, &uiov->iov_len))\n\t\t\treturn -EFAULT;\n\n\t\tkiov.iov_base = compat_ptr(ptr);\n\t\tkiov.iov_len = len;\n\n\t\tret = ptrace_regset(child, request, addr, &kiov);\n\t\tif (!ret)\n\t\t\tret = __put_user(kiov.iov_len, &uiov->iov_len);\n\t\tbreak;\n\t}\n#endif\n\n\tdefault:\n\t\tret = ptrace_request(child, request, addr, data);\n\t}\n\n\treturn ret;\n}\n\nCOMPAT_SYSCALL_DEFINE4(ptrace, compat_long_t, request, compat_long_t, pid,\n\t\t       compat_long_t, addr, compat_long_t, data)\n{\n\tstruct task_struct *child;\n\tlong ret;\n\n\tif (request == PTRACE_TRACEME) {\n\t\tret = ptrace_traceme();\n\t\tgoto out;\n\t}\n\n\tchild = find_get_task_by_vpid(pid);\n\tif (!child) {\n\t\tret = -ESRCH;\n\t\tgoto out;\n\t}\n\n\tif (request == PTRACE_ATTACH || request == PTRACE_SEIZE) {\n\t\tret = ptrace_attach(child, request, addr, data);\n\t\t/*\n\t\t * Some architectures need to do book-keeping after\n\t\t * a ptrace attach.\n\t\t */\n\t\tif (!ret)\n\t\t\tarch_ptrace_attach(child);\n\t\tgoto out_put_task_struct;\n\t}\n\n\tret = ptrace_check_attach(child, request == PTRACE_KILL ||\n\t\t\t\t  request == PTRACE_INTERRUPT);\n\tif (!ret) {\n\t\tret = compat_arch_ptrace(child, request, addr, data);\n\t\tif (ret || request != PTRACE_DETACH)\n\t\t\tptrace_unfreeze_traced(child);\n\t}\n\n out_put_task_struct:\n\tput_task_struct(child);\n out:\n\treturn ret;\n}\n#endif\t/* CONFIG_COMPAT */\n"], "fixing_code": ["// SPDX-License-Identifier: GPL-2.0-only\n/*\n * linux/kernel/ptrace.c\n *\n * (C) Copyright 1999 Linus Torvalds\n *\n * Common interfaces for \"ptrace()\" which we do not want\n * to continually duplicate across every architecture.\n */\n\n#include <linux/capability.h>\n#include <linux/export.h>\n#include <linux/sched.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/task.h>\n#include <linux/errno.h>\n#include <linux/mm.h>\n#include <linux/highmem.h>\n#include <linux/pagemap.h>\n#include <linux/ptrace.h>\n#include <linux/security.h>\n#include <linux/signal.h>\n#include <linux/uio.h>\n#include <linux/audit.h>\n#include <linux/pid_namespace.h>\n#include <linux/syscalls.h>\n#include <linux/uaccess.h>\n#include <linux/regset.h>\n#include <linux/hw_breakpoint.h>\n#include <linux/cn_proc.h>\n#include <linux/compat.h>\n#include <linux/sched/signal.h>\n#include <linux/minmax.h>\n\n#include <asm/syscall.h>\t/* for syscall_get_* */\n\n/*\n * Access another process' address space via ptrace.\n * Source/target buffer must be kernel space,\n * Do not walk the page table directly, use get_user_pages\n */\nint ptrace_access_vm(struct task_struct *tsk, unsigned long addr,\n\t\t     void *buf, int len, unsigned int gup_flags)\n{\n\tstruct mm_struct *mm;\n\tint ret;\n\n\tmm = get_task_mm(tsk);\n\tif (!mm)\n\t\treturn 0;\n\n\tif (!tsk->ptrace ||\n\t    (current != tsk->parent) ||\n\t    ((get_dumpable(mm) != SUID_DUMP_USER) &&\n\t     !ptracer_capable(tsk, mm->user_ns))) {\n\t\tmmput(mm);\n\t\treturn 0;\n\t}\n\n\tret = __access_remote_vm(mm, addr, buf, len, gup_flags);\n\tmmput(mm);\n\n\treturn ret;\n}\n\n\nvoid __ptrace_link(struct task_struct *child, struct task_struct *new_parent,\n\t\t   const struct cred *ptracer_cred)\n{\n\tBUG_ON(!list_empty(&child->ptrace_entry));\n\tlist_add(&child->ptrace_entry, &new_parent->ptraced);\n\tchild->parent = new_parent;\n\tchild->ptracer_cred = get_cred(ptracer_cred);\n}\n\n/*\n * ptrace a task: make the debugger its new parent and\n * move it to the ptrace list.\n *\n * Must be called with the tasklist lock write-held.\n */\nstatic void ptrace_link(struct task_struct *child, struct task_struct *new_parent)\n{\n\t__ptrace_link(child, new_parent, current_cred());\n}\n\n/**\n * __ptrace_unlink - unlink ptracee and restore its execution state\n * @child: ptracee to be unlinked\n *\n * Remove @child from the ptrace list, move it back to the original parent,\n * and restore the execution state so that it conforms to the group stop\n * state.\n *\n * Unlinking can happen via two paths - explicit PTRACE_DETACH or ptracer\n * exiting.  For PTRACE_DETACH, unless the ptracee has been killed between\n * ptrace_check_attach() and here, it's guaranteed to be in TASK_TRACED.\n * If the ptracer is exiting, the ptracee can be in any state.\n *\n * After detach, the ptracee should be in a state which conforms to the\n * group stop.  If the group is stopped or in the process of stopping, the\n * ptracee should be put into TASK_STOPPED; otherwise, it should be woken\n * up from TASK_TRACED.\n *\n * If the ptracee is in TASK_TRACED and needs to be moved to TASK_STOPPED,\n * it goes through TRACED -> RUNNING -> STOPPED transition which is similar\n * to but in the opposite direction of what happens while attaching to a\n * stopped task.  However, in this direction, the intermediate RUNNING\n * state is not hidden even from the current ptracer and if it immediately\n * re-attaches and performs a WNOHANG wait(2), it may fail.\n *\n * CONTEXT:\n * write_lock_irq(tasklist_lock)\n */\nvoid __ptrace_unlink(struct task_struct *child)\n{\n\tconst struct cred *old_cred;\n\tBUG_ON(!child->ptrace);\n\n\tclear_task_syscall_work(child, SYSCALL_TRACE);\n#if defined(CONFIG_GENERIC_ENTRY) || defined(TIF_SYSCALL_EMU)\n\tclear_task_syscall_work(child, SYSCALL_EMU);\n#endif\n\n\tchild->parent = child->real_parent;\n\tlist_del_init(&child->ptrace_entry);\n\told_cred = child->ptracer_cred;\n\tchild->ptracer_cred = NULL;\n\tput_cred(old_cred);\n\n\tspin_lock(&child->sighand->siglock);\n\tchild->ptrace = 0;\n\t/*\n\t * Clear all pending traps and TRAPPING.  TRAPPING should be\n\t * cleared regardless of JOBCTL_STOP_PENDING.  Do it explicitly.\n\t */\n\ttask_clear_jobctl_pending(child, JOBCTL_TRAP_MASK);\n\ttask_clear_jobctl_trapping(child);\n\n\t/*\n\t * Reinstate JOBCTL_STOP_PENDING if group stop is in effect and\n\t * @child isn't dead.\n\t */\n\tif (!(child->flags & PF_EXITING) &&\n\t    (child->signal->flags & SIGNAL_STOP_STOPPED ||\n\t     child->signal->group_stop_count)) {\n\t\tchild->jobctl |= JOBCTL_STOP_PENDING;\n\n\t\t/*\n\t\t * This is only possible if this thread was cloned by the\n\t\t * traced task running in the stopped group, set the signal\n\t\t * for the future reports.\n\t\t * FIXME: we should change ptrace_init_task() to handle this\n\t\t * case.\n\t\t */\n\t\tif (!(child->jobctl & JOBCTL_STOP_SIGMASK))\n\t\t\tchild->jobctl |= SIGSTOP;\n\t}\n\n\t/*\n\t * If transition to TASK_STOPPED is pending or in TASK_TRACED, kick\n\t * @child in the butt.  Note that @resume should be used iff @child\n\t * is in TASK_TRACED; otherwise, we might unduly disrupt\n\t * TASK_KILLABLE sleeps.\n\t */\n\tif (child->jobctl & JOBCTL_STOP_PENDING || task_is_traced(child))\n\t\tptrace_signal_wake_up(child, true);\n\n\tspin_unlock(&child->sighand->siglock);\n}\n\nstatic bool looks_like_a_spurious_pid(struct task_struct *task)\n{\n\tif (task->exit_code != ((PTRACE_EVENT_EXEC << 8) | SIGTRAP))\n\t\treturn false;\n\n\tif (task_pid_vnr(task) == task->ptrace_message)\n\t\treturn false;\n\t/*\n\t * The tracee changed its pid but the PTRACE_EVENT_EXEC event\n\t * was not wait()'ed, most probably debugger targets the old\n\t * leader which was destroyed in de_thread().\n\t */\n\treturn true;\n}\n\n/* Ensure that nothing can wake it up, even SIGKILL */\nstatic bool ptrace_freeze_traced(struct task_struct *task)\n{\n\tbool ret = false;\n\n\t/* Lockless, nobody but us can set this flag */\n\tif (task->jobctl & JOBCTL_LISTENING)\n\t\treturn ret;\n\n\tspin_lock_irq(&task->sighand->siglock);\n\tif (task_is_traced(task) && !looks_like_a_spurious_pid(task) &&\n\t    !__fatal_signal_pending(task)) {\n\t\tWRITE_ONCE(task->__state, __TASK_TRACED);\n\t\tret = true;\n\t}\n\tspin_unlock_irq(&task->sighand->siglock);\n\n\treturn ret;\n}\n\nstatic void ptrace_unfreeze_traced(struct task_struct *task)\n{\n\tif (READ_ONCE(task->__state) != __TASK_TRACED)\n\t\treturn;\n\n\tWARN_ON(!task->ptrace || task->parent != current);\n\n\t/*\n\t * PTRACE_LISTEN can allow ptrace_trap_notify to wake us up remotely.\n\t * Recheck state under the lock to close this race.\n\t */\n\tspin_lock_irq(&task->sighand->siglock);\n\tif (READ_ONCE(task->__state) == __TASK_TRACED) {\n\t\tif (__fatal_signal_pending(task))\n\t\t\twake_up_state(task, __TASK_TRACED);\n\t\telse\n\t\t\tWRITE_ONCE(task->__state, TASK_TRACED);\n\t}\n\tspin_unlock_irq(&task->sighand->siglock);\n}\n\n/**\n * ptrace_check_attach - check whether ptracee is ready for ptrace operation\n * @child: ptracee to check for\n * @ignore_state: don't check whether @child is currently %TASK_TRACED\n *\n * Check whether @child is being ptraced by %current and ready for further\n * ptrace operations.  If @ignore_state is %false, @child also should be in\n * %TASK_TRACED state and on return the child is guaranteed to be traced\n * and not executing.  If @ignore_state is %true, @child can be in any\n * state.\n *\n * CONTEXT:\n * Grabs and releases tasklist_lock and @child->sighand->siglock.\n *\n * RETURNS:\n * 0 on success, -ESRCH if %child is not ready.\n */\nstatic int ptrace_check_attach(struct task_struct *child, bool ignore_state)\n{\n\tint ret = -ESRCH;\n\n\t/*\n\t * We take the read lock around doing both checks to close a\n\t * possible race where someone else was tracing our child and\n\t * detached between these two checks.  After this locked check,\n\t * we are sure that this is our traced child and that can only\n\t * be changed by us so it's not changing right after this.\n\t */\n\tread_lock(&tasklist_lock);\n\tif (child->ptrace && child->parent == current) {\n\t\tWARN_ON(READ_ONCE(child->__state) == __TASK_TRACED);\n\t\t/*\n\t\t * child->sighand can't be NULL, release_task()\n\t\t * does ptrace_unlink() before __exit_signal().\n\t\t */\n\t\tif (ignore_state || ptrace_freeze_traced(child))\n\t\t\tret = 0;\n\t}\n\tread_unlock(&tasklist_lock);\n\n\tif (!ret && !ignore_state) {\n\t\tif (!wait_task_inactive(child, __TASK_TRACED)) {\n\t\t\t/*\n\t\t\t * This can only happen if may_ptrace_stop() fails and\n\t\t\t * ptrace_stop() changes ->state back to TASK_RUNNING,\n\t\t\t * so we should not worry about leaking __TASK_TRACED.\n\t\t\t */\n\t\t\tWARN_ON(READ_ONCE(child->__state) == __TASK_TRACED);\n\t\t\tret = -ESRCH;\n\t\t}\n\t}\n\n\treturn ret;\n}\n\nstatic bool ptrace_has_cap(struct user_namespace *ns, unsigned int mode)\n{\n\tif (mode & PTRACE_MODE_NOAUDIT)\n\t\treturn ns_capable_noaudit(ns, CAP_SYS_PTRACE);\n\treturn ns_capable(ns, CAP_SYS_PTRACE);\n}\n\n/* Returns 0 on success, -errno on denial. */\nstatic int __ptrace_may_access(struct task_struct *task, unsigned int mode)\n{\n\tconst struct cred *cred = current_cred(), *tcred;\n\tstruct mm_struct *mm;\n\tkuid_t caller_uid;\n\tkgid_t caller_gid;\n\n\tif (!(mode & PTRACE_MODE_FSCREDS) == !(mode & PTRACE_MODE_REALCREDS)) {\n\t\tWARN(1, \"denying ptrace access check without PTRACE_MODE_*CREDS\\n\");\n\t\treturn -EPERM;\n\t}\n\n\t/* May we inspect the given task?\n\t * This check is used both for attaching with ptrace\n\t * and for allowing access to sensitive information in /proc.\n\t *\n\t * ptrace_attach denies several cases that /proc allows\n\t * because setting up the necessary parent/child relationship\n\t * or halting the specified task is impossible.\n\t */\n\n\t/* Don't let security modules deny introspection */\n\tif (same_thread_group(task, current))\n\t\treturn 0;\n\trcu_read_lock();\n\tif (mode & PTRACE_MODE_FSCREDS) {\n\t\tcaller_uid = cred->fsuid;\n\t\tcaller_gid = cred->fsgid;\n\t} else {\n\t\t/*\n\t\t * Using the euid would make more sense here, but something\n\t\t * in userland might rely on the old behavior, and this\n\t\t * shouldn't be a security problem since\n\t\t * PTRACE_MODE_REALCREDS implies that the caller explicitly\n\t\t * used a syscall that requests access to another process\n\t\t * (and not a filesystem syscall to procfs).\n\t\t */\n\t\tcaller_uid = cred->uid;\n\t\tcaller_gid = cred->gid;\n\t}\n\ttcred = __task_cred(task);\n\tif (uid_eq(caller_uid, tcred->euid) &&\n\t    uid_eq(caller_uid, tcred->suid) &&\n\t    uid_eq(caller_uid, tcred->uid)  &&\n\t    gid_eq(caller_gid, tcred->egid) &&\n\t    gid_eq(caller_gid, tcred->sgid) &&\n\t    gid_eq(caller_gid, tcred->gid))\n\t\tgoto ok;\n\tif (ptrace_has_cap(tcred->user_ns, mode))\n\t\tgoto ok;\n\trcu_read_unlock();\n\treturn -EPERM;\nok:\n\trcu_read_unlock();\n\t/*\n\t * If a task drops privileges and becomes nondumpable (through a syscall\n\t * like setresuid()) while we are trying to access it, we must ensure\n\t * that the dumpability is read after the credentials; otherwise,\n\t * we may be able to attach to a task that we shouldn't be able to\n\t * attach to (as if the task had dropped privileges without becoming\n\t * nondumpable).\n\t * Pairs with a write barrier in commit_creds().\n\t */\n\tsmp_rmb();\n\tmm = task->mm;\n\tif (mm &&\n\t    ((get_dumpable(mm) != SUID_DUMP_USER) &&\n\t     !ptrace_has_cap(mm->user_ns, mode)))\n\t    return -EPERM;\n\n\treturn security_ptrace_access_check(task, mode);\n}\n\nbool ptrace_may_access(struct task_struct *task, unsigned int mode)\n{\n\tint err;\n\ttask_lock(task);\n\terr = __ptrace_may_access(task, mode);\n\ttask_unlock(task);\n\treturn !err;\n}\n\nstatic int check_ptrace_options(unsigned long data)\n{\n\tif (data & ~(unsigned long)PTRACE_O_MASK)\n\t\treturn -EINVAL;\n\n\tif (unlikely(data & PTRACE_O_SUSPEND_SECCOMP)) {\n\t\tif (!IS_ENABLED(CONFIG_CHECKPOINT_RESTORE) ||\n\t\t    !IS_ENABLED(CONFIG_SECCOMP))\n\t\t\treturn -EINVAL;\n\n\t\tif (!capable(CAP_SYS_ADMIN))\n\t\t\treturn -EPERM;\n\n\t\tif (seccomp_mode(&current->seccomp) != SECCOMP_MODE_DISABLED ||\n\t\t    current->ptrace & PT_SUSPEND_SECCOMP)\n\t\t\treturn -EPERM;\n\t}\n\treturn 0;\n}\n\nstatic int ptrace_attach(struct task_struct *task, long request,\n\t\t\t unsigned long addr,\n\t\t\t unsigned long flags)\n{\n\tbool seize = (request == PTRACE_SEIZE);\n\tint retval;\n\n\tretval = -EIO;\n\tif (seize) {\n\t\tif (addr != 0)\n\t\t\tgoto out;\n\t\t/*\n\t\t * This duplicates the check in check_ptrace_options() because\n\t\t * ptrace_attach() and ptrace_setoptions() have historically\n\t\t * used different error codes for unknown ptrace options.\n\t\t */\n\t\tif (flags & ~(unsigned long)PTRACE_O_MASK)\n\t\t\tgoto out;\n\t\tretval = check_ptrace_options(flags);\n\t\tif (retval)\n\t\t\treturn retval;\n\t\tflags = PT_PTRACED | PT_SEIZED | (flags << PT_OPT_FLAG_SHIFT);\n\t} else {\n\t\tflags = PT_PTRACED;\n\t}\n\n\taudit_ptrace(task);\n\n\tretval = -EPERM;\n\tif (unlikely(task->flags & PF_KTHREAD))\n\t\tgoto out;\n\tif (same_thread_group(task, current))\n\t\tgoto out;\n\n\t/*\n\t * Protect exec's credential calculations against our interference;\n\t * SUID, SGID and LSM creds get determined differently\n\t * under ptrace.\n\t */\n\tretval = -ERESTARTNOINTR;\n\tif (mutex_lock_interruptible(&task->signal->cred_guard_mutex))\n\t\tgoto out;\n\n\ttask_lock(task);\n\tretval = __ptrace_may_access(task, PTRACE_MODE_ATTACH_REALCREDS);\n\ttask_unlock(task);\n\tif (retval)\n\t\tgoto unlock_creds;\n\n\twrite_lock_irq(&tasklist_lock);\n\tretval = -EPERM;\n\tif (unlikely(task->exit_state))\n\t\tgoto unlock_tasklist;\n\tif (task->ptrace)\n\t\tgoto unlock_tasklist;\n\n\ttask->ptrace = flags;\n\n\tptrace_link(task, current);\n\n\t/* SEIZE doesn't trap tracee on attach */\n\tif (!seize)\n\t\tsend_sig_info(SIGSTOP, SEND_SIG_PRIV, task);\n\n\tspin_lock(&task->sighand->siglock);\n\n\t/*\n\t * If the task is already STOPPED, set JOBCTL_TRAP_STOP and\n\t * TRAPPING, and kick it so that it transits to TRACED.  TRAPPING\n\t * will be cleared if the child completes the transition or any\n\t * event which clears the group stop states happens.  We'll wait\n\t * for the transition to complete before returning from this\n\t * function.\n\t *\n\t * This hides STOPPED -> RUNNING -> TRACED transition from the\n\t * attaching thread but a different thread in the same group can\n\t * still observe the transient RUNNING state.  IOW, if another\n\t * thread's WNOHANG wait(2) on the stopped tracee races against\n\t * ATTACH, the wait(2) may fail due to the transient RUNNING.\n\t *\n\t * The following task_is_stopped() test is safe as both transitions\n\t * in and out of STOPPED are protected by siglock.\n\t */\n\tif (task_is_stopped(task) &&\n\t    task_set_jobctl_pending(task, JOBCTL_TRAP_STOP | JOBCTL_TRAPPING))\n\t\tsignal_wake_up_state(task, __TASK_STOPPED);\n\n\tspin_unlock(&task->sighand->siglock);\n\n\tretval = 0;\nunlock_tasklist:\n\twrite_unlock_irq(&tasklist_lock);\nunlock_creds:\n\tmutex_unlock(&task->signal->cred_guard_mutex);\nout:\n\tif (!retval) {\n\t\t/*\n\t\t * We do not bother to change retval or clear JOBCTL_TRAPPING\n\t\t * if wait_on_bit() was interrupted by SIGKILL. The tracer will\n\t\t * not return to user-mode, it will exit and clear this bit in\n\t\t * __ptrace_unlink() if it wasn't already cleared by the tracee;\n\t\t * and until then nobody can ptrace this task.\n\t\t */\n\t\twait_on_bit(&task->jobctl, JOBCTL_TRAPPING_BIT, TASK_KILLABLE);\n\t\tproc_ptrace_connector(task, PTRACE_ATTACH);\n\t}\n\n\treturn retval;\n}\n\n/**\n * ptrace_traceme  --  helper for PTRACE_TRACEME\n *\n * Performs checks and sets PT_PTRACED.\n * Should be used by all ptrace implementations for PTRACE_TRACEME.\n */\nstatic int ptrace_traceme(void)\n{\n\tint ret = -EPERM;\n\n\twrite_lock_irq(&tasklist_lock);\n\t/* Are we already being traced? */\n\tif (!current->ptrace) {\n\t\tret = security_ptrace_traceme(current->parent);\n\t\t/*\n\t\t * Check PF_EXITING to ensure ->real_parent has not passed\n\t\t * exit_ptrace(). Otherwise we don't report the error but\n\t\t * pretend ->real_parent untraces us right after return.\n\t\t */\n\t\tif (!ret && !(current->real_parent->flags & PF_EXITING)) {\n\t\t\tcurrent->ptrace = PT_PTRACED;\n\t\t\tptrace_link(current, current->real_parent);\n\t\t}\n\t}\n\twrite_unlock_irq(&tasklist_lock);\n\n\treturn ret;\n}\n\n/*\n * Called with irqs disabled, returns true if childs should reap themselves.\n */\nstatic int ignoring_children(struct sighand_struct *sigh)\n{\n\tint ret;\n\tspin_lock(&sigh->siglock);\n\tret = (sigh->action[SIGCHLD-1].sa.sa_handler == SIG_IGN) ||\n\t      (sigh->action[SIGCHLD-1].sa.sa_flags & SA_NOCLDWAIT);\n\tspin_unlock(&sigh->siglock);\n\treturn ret;\n}\n\n/*\n * Called with tasklist_lock held for writing.\n * Unlink a traced task, and clean it up if it was a traced zombie.\n * Return true if it needs to be reaped with release_task().\n * (We can't call release_task() here because we already hold tasklist_lock.)\n *\n * If it's a zombie, our attachedness prevented normal parent notification\n * or self-reaping.  Do notification now if it would have happened earlier.\n * If it should reap itself, return true.\n *\n * If it's our own child, there is no notification to do. But if our normal\n * children self-reap, then this child was prevented by ptrace and we must\n * reap it now, in that case we must also wake up sub-threads sleeping in\n * do_wait().\n */\nstatic bool __ptrace_detach(struct task_struct *tracer, struct task_struct *p)\n{\n\tbool dead;\n\n\t__ptrace_unlink(p);\n\n\tif (p->exit_state != EXIT_ZOMBIE)\n\t\treturn false;\n\n\tdead = !thread_group_leader(p);\n\n\tif (!dead && thread_group_empty(p)) {\n\t\tif (!same_thread_group(p->real_parent, tracer))\n\t\t\tdead = do_notify_parent(p, p->exit_signal);\n\t\telse if (ignoring_children(tracer->sighand)) {\n\t\t\t__wake_up_parent(p, tracer);\n\t\t\tdead = true;\n\t\t}\n\t}\n\t/* Mark it as in the process of being reaped. */\n\tif (dead)\n\t\tp->exit_state = EXIT_DEAD;\n\treturn dead;\n}\n\nstatic int ptrace_detach(struct task_struct *child, unsigned int data)\n{\n\tif (!valid_signal(data))\n\t\treturn -EIO;\n\n\t/* Architecture-specific hardware disable .. */\n\tptrace_disable(child);\n\n\twrite_lock_irq(&tasklist_lock);\n\t/*\n\t * We rely on ptrace_freeze_traced(). It can't be killed and\n\t * untraced by another thread, it can't be a zombie.\n\t */\n\tWARN_ON(!child->ptrace || child->exit_state);\n\t/*\n\t * tasklist_lock avoids the race with wait_task_stopped(), see\n\t * the comment in ptrace_resume().\n\t */\n\tchild->exit_code = data;\n\t__ptrace_detach(current, child);\n\twrite_unlock_irq(&tasklist_lock);\n\n\tproc_ptrace_connector(child, PTRACE_DETACH);\n\n\treturn 0;\n}\n\n/*\n * Detach all tasks we were using ptrace on. Called with tasklist held\n * for writing.\n */\nvoid exit_ptrace(struct task_struct *tracer, struct list_head *dead)\n{\n\tstruct task_struct *p, *n;\n\n\tlist_for_each_entry_safe(p, n, &tracer->ptraced, ptrace_entry) {\n\t\tif (unlikely(p->ptrace & PT_EXITKILL))\n\t\t\tsend_sig_info(SIGKILL, SEND_SIG_PRIV, p);\n\n\t\tif (__ptrace_detach(tracer, p))\n\t\t\tlist_add(&p->ptrace_entry, dead);\n\t}\n}\n\nint ptrace_readdata(struct task_struct *tsk, unsigned long src, char __user *dst, int len)\n{\n\tint copied = 0;\n\n\twhile (len > 0) {\n\t\tchar buf[128];\n\t\tint this_len, retval;\n\n\t\tthis_len = (len > sizeof(buf)) ? sizeof(buf) : len;\n\t\tretval = ptrace_access_vm(tsk, src, buf, this_len, FOLL_FORCE);\n\n\t\tif (!retval) {\n\t\t\tif (copied)\n\t\t\t\tbreak;\n\t\t\treturn -EIO;\n\t\t}\n\t\tif (copy_to_user(dst, buf, retval))\n\t\t\treturn -EFAULT;\n\t\tcopied += retval;\n\t\tsrc += retval;\n\t\tdst += retval;\n\t\tlen -= retval;\n\t}\n\treturn copied;\n}\n\nint ptrace_writedata(struct task_struct *tsk, char __user *src, unsigned long dst, int len)\n{\n\tint copied = 0;\n\n\twhile (len > 0) {\n\t\tchar buf[128];\n\t\tint this_len, retval;\n\n\t\tthis_len = (len > sizeof(buf)) ? sizeof(buf) : len;\n\t\tif (copy_from_user(buf, src, this_len))\n\t\t\treturn -EFAULT;\n\t\tretval = ptrace_access_vm(tsk, dst, buf, this_len,\n\t\t\t\tFOLL_FORCE | FOLL_WRITE);\n\t\tif (!retval) {\n\t\t\tif (copied)\n\t\t\t\tbreak;\n\t\t\treturn -EIO;\n\t\t}\n\t\tcopied += retval;\n\t\tsrc += retval;\n\t\tdst += retval;\n\t\tlen -= retval;\n\t}\n\treturn copied;\n}\n\nstatic int ptrace_setoptions(struct task_struct *child, unsigned long data)\n{\n\tunsigned flags;\n\tint ret;\n\n\tret = check_ptrace_options(data);\n\tif (ret)\n\t\treturn ret;\n\n\t/* Avoid intermediate state when all opts are cleared */\n\tflags = child->ptrace;\n\tflags &= ~(PTRACE_O_MASK << PT_OPT_FLAG_SHIFT);\n\tflags |= (data << PT_OPT_FLAG_SHIFT);\n\tchild->ptrace = flags;\n\n\treturn 0;\n}\n\nstatic int ptrace_getsiginfo(struct task_struct *child, kernel_siginfo_t *info)\n{\n\tunsigned long flags;\n\tint error = -ESRCH;\n\n\tif (lock_task_sighand(child, &flags)) {\n\t\terror = -EINVAL;\n\t\tif (likely(child->last_siginfo != NULL)) {\n\t\t\tcopy_siginfo(info, child->last_siginfo);\n\t\t\terror = 0;\n\t\t}\n\t\tunlock_task_sighand(child, &flags);\n\t}\n\treturn error;\n}\n\nstatic int ptrace_setsiginfo(struct task_struct *child, const kernel_siginfo_t *info)\n{\n\tunsigned long flags;\n\tint error = -ESRCH;\n\n\tif (lock_task_sighand(child, &flags)) {\n\t\terror = -EINVAL;\n\t\tif (likely(child->last_siginfo != NULL)) {\n\t\t\tcopy_siginfo(child->last_siginfo, info);\n\t\t\terror = 0;\n\t\t}\n\t\tunlock_task_sighand(child, &flags);\n\t}\n\treturn error;\n}\n\nstatic int ptrace_peek_siginfo(struct task_struct *child,\n\t\t\t\tunsigned long addr,\n\t\t\t\tunsigned long data)\n{\n\tstruct ptrace_peeksiginfo_args arg;\n\tstruct sigpending *pending;\n\tstruct sigqueue *q;\n\tint ret, i;\n\n\tret = copy_from_user(&arg, (void __user *) addr,\n\t\t\t\tsizeof(struct ptrace_peeksiginfo_args));\n\tif (ret)\n\t\treturn -EFAULT;\n\n\tif (arg.flags & ~PTRACE_PEEKSIGINFO_SHARED)\n\t\treturn -EINVAL; /* unknown flags */\n\n\tif (arg.nr < 0)\n\t\treturn -EINVAL;\n\n\t/* Ensure arg.off fits in an unsigned long */\n\tif (arg.off > ULONG_MAX)\n\t\treturn 0;\n\n\tif (arg.flags & PTRACE_PEEKSIGINFO_SHARED)\n\t\tpending = &child->signal->shared_pending;\n\telse\n\t\tpending = &child->pending;\n\n\tfor (i = 0; i < arg.nr; ) {\n\t\tkernel_siginfo_t info;\n\t\tunsigned long off = arg.off + i;\n\t\tbool found = false;\n\n\t\tspin_lock_irq(&child->sighand->siglock);\n\t\tlist_for_each_entry(q, &pending->list, list) {\n\t\t\tif (!off--) {\n\t\t\t\tfound = true;\n\t\t\t\tcopy_siginfo(&info, &q->info);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tspin_unlock_irq(&child->sighand->siglock);\n\n\t\tif (!found) /* beyond the end of the list */\n\t\t\tbreak;\n\n#ifdef CONFIG_COMPAT\n\t\tif (unlikely(in_compat_syscall())) {\n\t\t\tcompat_siginfo_t __user *uinfo = compat_ptr(data);\n\n\t\t\tif (copy_siginfo_to_user32(uinfo, &info)) {\n\t\t\t\tret = -EFAULT;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t} else\n#endif\n\t\t{\n\t\t\tsiginfo_t __user *uinfo = (siginfo_t __user *) data;\n\n\t\t\tif (copy_siginfo_to_user(uinfo, &info)) {\n\t\t\t\tret = -EFAULT;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tdata += sizeof(siginfo_t);\n\t\ti++;\n\n\t\tif (signal_pending(current))\n\t\t\tbreak;\n\n\t\tcond_resched();\n\t}\n\n\tif (i > 0)\n\t\treturn i;\n\n\treturn ret;\n}\n\n#ifdef CONFIG_RSEQ\nstatic long ptrace_get_rseq_configuration(struct task_struct *task,\n\t\t\t\t\t  unsigned long size, void __user *data)\n{\n\tstruct ptrace_rseq_configuration conf = {\n\t\t.rseq_abi_pointer = (u64)(uintptr_t)task->rseq,\n\t\t.rseq_abi_size = sizeof(*task->rseq),\n\t\t.signature = task->rseq_sig,\n\t\t.flags = 0,\n\t};\n\n\tsize = min_t(unsigned long, size, sizeof(conf));\n\tif (copy_to_user(data, &conf, size))\n\t\treturn -EFAULT;\n\treturn sizeof(conf);\n}\n#endif\n\n#ifdef PTRACE_SINGLESTEP\n#define is_singlestep(request)\t\t((request) == PTRACE_SINGLESTEP)\n#else\n#define is_singlestep(request)\t\t0\n#endif\n\n#ifdef PTRACE_SINGLEBLOCK\n#define is_singleblock(request)\t\t((request) == PTRACE_SINGLEBLOCK)\n#else\n#define is_singleblock(request)\t\t0\n#endif\n\n#ifdef PTRACE_SYSEMU\n#define is_sysemu_singlestep(request)\t((request) == PTRACE_SYSEMU_SINGLESTEP)\n#else\n#define is_sysemu_singlestep(request)\t0\n#endif\n\nstatic int ptrace_resume(struct task_struct *child, long request,\n\t\t\t unsigned long data)\n{\n\tbool need_siglock;\n\n\tif (!valid_signal(data))\n\t\treturn -EIO;\n\n\tif (request == PTRACE_SYSCALL)\n\t\tset_task_syscall_work(child, SYSCALL_TRACE);\n\telse\n\t\tclear_task_syscall_work(child, SYSCALL_TRACE);\n\n#if defined(CONFIG_GENERIC_ENTRY) || defined(TIF_SYSCALL_EMU)\n\tif (request == PTRACE_SYSEMU || request == PTRACE_SYSEMU_SINGLESTEP)\n\t\tset_task_syscall_work(child, SYSCALL_EMU);\n\telse\n\t\tclear_task_syscall_work(child, SYSCALL_EMU);\n#endif\n\n\tif (is_singleblock(request)) {\n\t\tif (unlikely(!arch_has_block_step()))\n\t\t\treturn -EIO;\n\t\tuser_enable_block_step(child);\n\t} else if (is_singlestep(request) || is_sysemu_singlestep(request)) {\n\t\tif (unlikely(!arch_has_single_step()))\n\t\t\treturn -EIO;\n\t\tuser_enable_single_step(child);\n\t} else {\n\t\tuser_disable_single_step(child);\n\t}\n\n\t/*\n\t * Change ->exit_code and ->state under siglock to avoid the race\n\t * with wait_task_stopped() in between; a non-zero ->exit_code will\n\t * wrongly look like another report from tracee.\n\t *\n\t * Note that we need siglock even if ->exit_code == data and/or this\n\t * status was not reported yet, the new status must not be cleared by\n\t * wait_task_stopped() after resume.\n\t *\n\t * If data == 0 we do not care if wait_task_stopped() reports the old\n\t * status and clears the code too; this can't race with the tracee, it\n\t * takes siglock after resume.\n\t */\n\tneed_siglock = data && !thread_group_empty(current);\n\tif (need_siglock)\n\t\tspin_lock_irq(&child->sighand->siglock);\n\tchild->exit_code = data;\n\twake_up_state(child, __TASK_TRACED);\n\tif (need_siglock)\n\t\tspin_unlock_irq(&child->sighand->siglock);\n\n\treturn 0;\n}\n\n#ifdef CONFIG_HAVE_ARCH_TRACEHOOK\n\nstatic const struct user_regset *\nfind_regset(const struct user_regset_view *view, unsigned int type)\n{\n\tconst struct user_regset *regset;\n\tint n;\n\n\tfor (n = 0; n < view->n; ++n) {\n\t\tregset = view->regsets + n;\n\t\tif (regset->core_note_type == type)\n\t\t\treturn regset;\n\t}\n\n\treturn NULL;\n}\n\nstatic int ptrace_regset(struct task_struct *task, int req, unsigned int type,\n\t\t\t struct iovec *kiov)\n{\n\tconst struct user_regset_view *view = task_user_regset_view(task);\n\tconst struct user_regset *regset = find_regset(view, type);\n\tint regset_no;\n\n\tif (!regset || (kiov->iov_len % regset->size) != 0)\n\t\treturn -EINVAL;\n\n\tregset_no = regset - view->regsets;\n\tkiov->iov_len = min(kiov->iov_len,\n\t\t\t    (__kernel_size_t) (regset->n * regset->size));\n\n\tif (req == PTRACE_GETREGSET)\n\t\treturn copy_regset_to_user(task, view, regset_no, 0,\n\t\t\t\t\t   kiov->iov_len, kiov->iov_base);\n\telse\n\t\treturn copy_regset_from_user(task, view, regset_no, 0,\n\t\t\t\t\t     kiov->iov_len, kiov->iov_base);\n}\n\n/*\n * This is declared in linux/regset.h and defined in machine-dependent\n * code.  We put the export here, near the primary machine-neutral use,\n * to ensure no machine forgets it.\n */\nEXPORT_SYMBOL_GPL(task_user_regset_view);\n\nstatic unsigned long\nptrace_get_syscall_info_entry(struct task_struct *child, struct pt_regs *regs,\n\t\t\t      struct ptrace_syscall_info *info)\n{\n\tunsigned long args[ARRAY_SIZE(info->entry.args)];\n\tint i;\n\n\tinfo->op = PTRACE_SYSCALL_INFO_ENTRY;\n\tinfo->entry.nr = syscall_get_nr(child, regs);\n\tsyscall_get_arguments(child, regs, args);\n\tfor (i = 0; i < ARRAY_SIZE(args); i++)\n\t\tinfo->entry.args[i] = args[i];\n\n\t/* args is the last field in struct ptrace_syscall_info.entry */\n\treturn offsetofend(struct ptrace_syscall_info, entry.args);\n}\n\nstatic unsigned long\nptrace_get_syscall_info_seccomp(struct task_struct *child, struct pt_regs *regs,\n\t\t\t\tstruct ptrace_syscall_info *info)\n{\n\t/*\n\t * As struct ptrace_syscall_info.entry is currently a subset\n\t * of struct ptrace_syscall_info.seccomp, it makes sense to\n\t * initialize that subset using ptrace_get_syscall_info_entry().\n\t * This can be reconsidered in the future if these structures\n\t * diverge significantly enough.\n\t */\n\tptrace_get_syscall_info_entry(child, regs, info);\n\tinfo->op = PTRACE_SYSCALL_INFO_SECCOMP;\n\tinfo->seccomp.ret_data = child->ptrace_message;\n\n\t/* ret_data is the last field in struct ptrace_syscall_info.seccomp */\n\treturn offsetofend(struct ptrace_syscall_info, seccomp.ret_data);\n}\n\nstatic unsigned long\nptrace_get_syscall_info_exit(struct task_struct *child, struct pt_regs *regs,\n\t\t\t     struct ptrace_syscall_info *info)\n{\n\tinfo->op = PTRACE_SYSCALL_INFO_EXIT;\n\tinfo->exit.rval = syscall_get_error(child, regs);\n\tinfo->exit.is_error = !!info->exit.rval;\n\tif (!info->exit.is_error)\n\t\tinfo->exit.rval = syscall_get_return_value(child, regs);\n\n\t/* is_error is the last field in struct ptrace_syscall_info.exit */\n\treturn offsetofend(struct ptrace_syscall_info, exit.is_error);\n}\n\nstatic int\nptrace_get_syscall_info(struct task_struct *child, unsigned long user_size,\n\t\t\tvoid __user *datavp)\n{\n\tstruct pt_regs *regs = task_pt_regs(child);\n\tstruct ptrace_syscall_info info = {\n\t\t.op = PTRACE_SYSCALL_INFO_NONE,\n\t\t.arch = syscall_get_arch(child),\n\t\t.instruction_pointer = instruction_pointer(regs),\n\t\t.stack_pointer = user_stack_pointer(regs),\n\t};\n\tunsigned long actual_size = offsetof(struct ptrace_syscall_info, entry);\n\tunsigned long write_size;\n\n\t/*\n\t * This does not need lock_task_sighand() to access\n\t * child->last_siginfo because ptrace_freeze_traced()\n\t * called earlier by ptrace_check_attach() ensures that\n\t * the tracee cannot go away and clear its last_siginfo.\n\t */\n\tswitch (child->last_siginfo ? child->last_siginfo->si_code : 0) {\n\tcase SIGTRAP | 0x80:\n\t\tswitch (child->ptrace_message) {\n\t\tcase PTRACE_EVENTMSG_SYSCALL_ENTRY:\n\t\t\tactual_size = ptrace_get_syscall_info_entry(child, regs,\n\t\t\t\t\t\t\t\t    &info);\n\t\t\tbreak;\n\t\tcase PTRACE_EVENTMSG_SYSCALL_EXIT:\n\t\t\tactual_size = ptrace_get_syscall_info_exit(child, regs,\n\t\t\t\t\t\t\t\t   &info);\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase SIGTRAP | (PTRACE_EVENT_SECCOMP << 8):\n\t\tactual_size = ptrace_get_syscall_info_seccomp(child, regs,\n\t\t\t\t\t\t\t      &info);\n\t\tbreak;\n\t}\n\n\twrite_size = min(actual_size, user_size);\n\treturn copy_to_user(datavp, &info, write_size) ? -EFAULT : actual_size;\n}\n#endif /* CONFIG_HAVE_ARCH_TRACEHOOK */\n\nint ptrace_request(struct task_struct *child, long request,\n\t\t   unsigned long addr, unsigned long data)\n{\n\tbool seized = child->ptrace & PT_SEIZED;\n\tint ret = -EIO;\n\tkernel_siginfo_t siginfo, *si;\n\tvoid __user *datavp = (void __user *) data;\n\tunsigned long __user *datalp = datavp;\n\tunsigned long flags;\n\n\tswitch (request) {\n\tcase PTRACE_PEEKTEXT:\n\tcase PTRACE_PEEKDATA:\n\t\treturn generic_ptrace_peekdata(child, addr, data);\n\tcase PTRACE_POKETEXT:\n\tcase PTRACE_POKEDATA:\n\t\treturn generic_ptrace_pokedata(child, addr, data);\n\n#ifdef PTRACE_OLDSETOPTIONS\n\tcase PTRACE_OLDSETOPTIONS:\n#endif\n\tcase PTRACE_SETOPTIONS:\n\t\tret = ptrace_setoptions(child, data);\n\t\tbreak;\n\tcase PTRACE_GETEVENTMSG:\n\t\tret = put_user(child->ptrace_message, datalp);\n\t\tbreak;\n\n\tcase PTRACE_PEEKSIGINFO:\n\t\tret = ptrace_peek_siginfo(child, addr, data);\n\t\tbreak;\n\n\tcase PTRACE_GETSIGINFO:\n\t\tret = ptrace_getsiginfo(child, &siginfo);\n\t\tif (!ret)\n\t\t\tret = copy_siginfo_to_user(datavp, &siginfo);\n\t\tbreak;\n\n\tcase PTRACE_SETSIGINFO:\n\t\tret = copy_siginfo_from_user(&siginfo, datavp);\n\t\tif (!ret)\n\t\t\tret = ptrace_setsiginfo(child, &siginfo);\n\t\tbreak;\n\n\tcase PTRACE_GETSIGMASK: {\n\t\tsigset_t *mask;\n\n\t\tif (addr != sizeof(sigset_t)) {\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (test_tsk_restore_sigmask(child))\n\t\t\tmask = &child->saved_sigmask;\n\t\telse\n\t\t\tmask = &child->blocked;\n\n\t\tif (copy_to_user(datavp, mask, sizeof(sigset_t)))\n\t\t\tret = -EFAULT;\n\t\telse\n\t\t\tret = 0;\n\n\t\tbreak;\n\t}\n\n\tcase PTRACE_SETSIGMASK: {\n\t\tsigset_t new_set;\n\n\t\tif (addr != sizeof(sigset_t)) {\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (copy_from_user(&new_set, datavp, sizeof(sigset_t))) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\n\t\tsigdelsetmask(&new_set, sigmask(SIGKILL)|sigmask(SIGSTOP));\n\n\t\t/*\n\t\t * Every thread does recalc_sigpending() after resume, so\n\t\t * retarget_shared_pending() and recalc_sigpending() are not\n\t\t * called here.\n\t\t */\n\t\tspin_lock_irq(&child->sighand->siglock);\n\t\tchild->blocked = new_set;\n\t\tspin_unlock_irq(&child->sighand->siglock);\n\n\t\tclear_tsk_restore_sigmask(child);\n\n\t\tret = 0;\n\t\tbreak;\n\t}\n\n\tcase PTRACE_INTERRUPT:\n\t\t/*\n\t\t * Stop tracee without any side-effect on signal or job\n\t\t * control.  At least one trap is guaranteed to happen\n\t\t * after this request.  If @child is already trapped, the\n\t\t * current trap is not disturbed and another trap will\n\t\t * happen after the current trap is ended with PTRACE_CONT.\n\t\t *\n\t\t * The actual trap might not be PTRACE_EVENT_STOP trap but\n\t\t * the pending condition is cleared regardless.\n\t\t */\n\t\tif (unlikely(!seized || !lock_task_sighand(child, &flags)))\n\t\t\tbreak;\n\n\t\t/*\n\t\t * INTERRUPT doesn't disturb existing trap sans one\n\t\t * exception.  If ptracer issued LISTEN for the current\n\t\t * STOP, this INTERRUPT should clear LISTEN and re-trap\n\t\t * tracee into STOP.\n\t\t */\n\t\tif (likely(task_set_jobctl_pending(child, JOBCTL_TRAP_STOP)))\n\t\t\tptrace_signal_wake_up(child, child->jobctl & JOBCTL_LISTENING);\n\n\t\tunlock_task_sighand(child, &flags);\n\t\tret = 0;\n\t\tbreak;\n\n\tcase PTRACE_LISTEN:\n\t\t/*\n\t\t * Listen for events.  Tracee must be in STOP.  It's not\n\t\t * resumed per-se but is not considered to be in TRACED by\n\t\t * wait(2) or ptrace(2).  If an async event (e.g. group\n\t\t * stop state change) happens, tracee will enter STOP trap\n\t\t * again.  Alternatively, ptracer can issue INTERRUPT to\n\t\t * finish listening and re-trap tracee into STOP.\n\t\t */\n\t\tif (unlikely(!seized || !lock_task_sighand(child, &flags)))\n\t\t\tbreak;\n\n\t\tsi = child->last_siginfo;\n\t\tif (likely(si && (si->si_code >> 8) == PTRACE_EVENT_STOP)) {\n\t\t\tchild->jobctl |= JOBCTL_LISTENING;\n\t\t\t/*\n\t\t\t * If NOTIFY is set, it means event happened between\n\t\t\t * start of this trap and now.  Trigger re-trap.\n\t\t\t */\n\t\t\tif (child->jobctl & JOBCTL_TRAP_NOTIFY)\n\t\t\t\tptrace_signal_wake_up(child, true);\n\t\t\tret = 0;\n\t\t}\n\t\tunlock_task_sighand(child, &flags);\n\t\tbreak;\n\n\tcase PTRACE_DETACH:\t /* detach a process that was attached. */\n\t\tret = ptrace_detach(child, data);\n\t\tbreak;\n\n#ifdef CONFIG_BINFMT_ELF_FDPIC\n\tcase PTRACE_GETFDPIC: {\n\t\tstruct mm_struct *mm = get_task_mm(child);\n\t\tunsigned long tmp = 0;\n\n\t\tret = -ESRCH;\n\t\tif (!mm)\n\t\t\tbreak;\n\n\t\tswitch (addr) {\n\t\tcase PTRACE_GETFDPIC_EXEC:\n\t\t\ttmp = mm->context.exec_fdpic_loadmap;\n\t\t\tbreak;\n\t\tcase PTRACE_GETFDPIC_INTERP:\n\t\t\ttmp = mm->context.interp_fdpic_loadmap;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\tmmput(mm);\n\n\t\tret = put_user(tmp, datalp);\n\t\tbreak;\n\t}\n#endif\n\n#ifdef PTRACE_SINGLESTEP\n\tcase PTRACE_SINGLESTEP:\n#endif\n#ifdef PTRACE_SINGLEBLOCK\n\tcase PTRACE_SINGLEBLOCK:\n#endif\n#ifdef PTRACE_SYSEMU\n\tcase PTRACE_SYSEMU:\n\tcase PTRACE_SYSEMU_SINGLESTEP:\n#endif\n\tcase PTRACE_SYSCALL:\n\tcase PTRACE_CONT:\n\t\treturn ptrace_resume(child, request, data);\n\n\tcase PTRACE_KILL:\n\t\tif (child->exit_state)\t/* already dead */\n\t\t\treturn 0;\n\t\treturn ptrace_resume(child, request, SIGKILL);\n\n#ifdef CONFIG_HAVE_ARCH_TRACEHOOK\n\tcase PTRACE_GETREGSET:\n\tcase PTRACE_SETREGSET: {\n\t\tstruct iovec kiov;\n\t\tstruct iovec __user *uiov = datavp;\n\n\t\tif (!access_ok(uiov, sizeof(*uiov)))\n\t\t\treturn -EFAULT;\n\n\t\tif (__get_user(kiov.iov_base, &uiov->iov_base) ||\n\t\t    __get_user(kiov.iov_len, &uiov->iov_len))\n\t\t\treturn -EFAULT;\n\n\t\tret = ptrace_regset(child, request, addr, &kiov);\n\t\tif (!ret)\n\t\t\tret = __put_user(kiov.iov_len, &uiov->iov_len);\n\t\tbreak;\n\t}\n\n\tcase PTRACE_GET_SYSCALL_INFO:\n\t\tret = ptrace_get_syscall_info(child, addr, datavp);\n\t\tbreak;\n#endif\n\n\tcase PTRACE_SECCOMP_GET_FILTER:\n\t\tret = seccomp_get_filter(child, addr, datavp);\n\t\tbreak;\n\n\tcase PTRACE_SECCOMP_GET_METADATA:\n\t\tret = seccomp_get_metadata(child, addr, datavp);\n\t\tbreak;\n\n#ifdef CONFIG_RSEQ\n\tcase PTRACE_GET_RSEQ_CONFIGURATION:\n\t\tret = ptrace_get_rseq_configuration(child, addr, datavp);\n\t\tbreak;\n#endif\n\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\n#ifndef arch_ptrace_attach\n#define arch_ptrace_attach(child)\tdo { } while (0)\n#endif\n\nSYSCALL_DEFINE4(ptrace, long, request, long, pid, unsigned long, addr,\n\t\tunsigned long, data)\n{\n\tstruct task_struct *child;\n\tlong ret;\n\n\tif (request == PTRACE_TRACEME) {\n\t\tret = ptrace_traceme();\n\t\tif (!ret)\n\t\t\tarch_ptrace_attach(current);\n\t\tgoto out;\n\t}\n\n\tchild = find_get_task_by_vpid(pid);\n\tif (!child) {\n\t\tret = -ESRCH;\n\t\tgoto out;\n\t}\n\n\tif (request == PTRACE_ATTACH || request == PTRACE_SEIZE) {\n\t\tret = ptrace_attach(child, request, addr, data);\n\t\t/*\n\t\t * Some architectures need to do book-keeping after\n\t\t * a ptrace attach.\n\t\t */\n\t\tif (!ret)\n\t\t\tarch_ptrace_attach(child);\n\t\tgoto out_put_task_struct;\n\t}\n\n\tret = ptrace_check_attach(child, request == PTRACE_KILL ||\n\t\t\t\t  request == PTRACE_INTERRUPT);\n\tif (ret < 0)\n\t\tgoto out_put_task_struct;\n\n\tret = arch_ptrace(child, request, addr, data);\n\tif (ret || request != PTRACE_DETACH)\n\t\tptrace_unfreeze_traced(child);\n\n out_put_task_struct:\n\tput_task_struct(child);\n out:\n\treturn ret;\n}\n\nint generic_ptrace_peekdata(struct task_struct *tsk, unsigned long addr,\n\t\t\t    unsigned long data)\n{\n\tunsigned long tmp;\n\tint copied;\n\n\tcopied = ptrace_access_vm(tsk, addr, &tmp, sizeof(tmp), FOLL_FORCE);\n\tif (copied != sizeof(tmp))\n\t\treturn -EIO;\n\treturn put_user(tmp, (unsigned long __user *)data);\n}\n\nint generic_ptrace_pokedata(struct task_struct *tsk, unsigned long addr,\n\t\t\t    unsigned long data)\n{\n\tint copied;\n\n\tcopied = ptrace_access_vm(tsk, addr, &data, sizeof(data),\n\t\t\tFOLL_FORCE | FOLL_WRITE);\n\treturn (copied == sizeof(data)) ? 0 : -EIO;\n}\n\n#if defined CONFIG_COMPAT\n\nint compat_ptrace_request(struct task_struct *child, compat_long_t request,\n\t\t\t  compat_ulong_t addr, compat_ulong_t data)\n{\n\tcompat_ulong_t __user *datap = compat_ptr(data);\n\tcompat_ulong_t word;\n\tkernel_siginfo_t siginfo;\n\tint ret;\n\n\tswitch (request) {\n\tcase PTRACE_PEEKTEXT:\n\tcase PTRACE_PEEKDATA:\n\t\tret = ptrace_access_vm(child, addr, &word, sizeof(word),\n\t\t\t\tFOLL_FORCE);\n\t\tif (ret != sizeof(word))\n\t\t\tret = -EIO;\n\t\telse\n\t\t\tret = put_user(word, datap);\n\t\tbreak;\n\n\tcase PTRACE_POKETEXT:\n\tcase PTRACE_POKEDATA:\n\t\tret = ptrace_access_vm(child, addr, &data, sizeof(data),\n\t\t\t\tFOLL_FORCE | FOLL_WRITE);\n\t\tret = (ret != sizeof(data) ? -EIO : 0);\n\t\tbreak;\n\n\tcase PTRACE_GETEVENTMSG:\n\t\tret = put_user((compat_ulong_t) child->ptrace_message, datap);\n\t\tbreak;\n\n\tcase PTRACE_GETSIGINFO:\n\t\tret = ptrace_getsiginfo(child, &siginfo);\n\t\tif (!ret)\n\t\t\tret = copy_siginfo_to_user32(\n\t\t\t\t(struct compat_siginfo __user *) datap,\n\t\t\t\t&siginfo);\n\t\tbreak;\n\n\tcase PTRACE_SETSIGINFO:\n\t\tret = copy_siginfo_from_user32(\n\t\t\t&siginfo, (struct compat_siginfo __user *) datap);\n\t\tif (!ret)\n\t\t\tret = ptrace_setsiginfo(child, &siginfo);\n\t\tbreak;\n#ifdef CONFIG_HAVE_ARCH_TRACEHOOK\n\tcase PTRACE_GETREGSET:\n\tcase PTRACE_SETREGSET:\n\t{\n\t\tstruct iovec kiov;\n\t\tstruct compat_iovec __user *uiov =\n\t\t\t(struct compat_iovec __user *) datap;\n\t\tcompat_uptr_t ptr;\n\t\tcompat_size_t len;\n\n\t\tif (!access_ok(uiov, sizeof(*uiov)))\n\t\t\treturn -EFAULT;\n\n\t\tif (__get_user(ptr, &uiov->iov_base) ||\n\t\t    __get_user(len, &uiov->iov_len))\n\t\t\treturn -EFAULT;\n\n\t\tkiov.iov_base = compat_ptr(ptr);\n\t\tkiov.iov_len = len;\n\n\t\tret = ptrace_regset(child, request, addr, &kiov);\n\t\tif (!ret)\n\t\t\tret = __put_user(kiov.iov_len, &uiov->iov_len);\n\t\tbreak;\n\t}\n#endif\n\n\tdefault:\n\t\tret = ptrace_request(child, request, addr, data);\n\t}\n\n\treturn ret;\n}\n\nCOMPAT_SYSCALL_DEFINE4(ptrace, compat_long_t, request, compat_long_t, pid,\n\t\t       compat_long_t, addr, compat_long_t, data)\n{\n\tstruct task_struct *child;\n\tlong ret;\n\n\tif (request == PTRACE_TRACEME) {\n\t\tret = ptrace_traceme();\n\t\tgoto out;\n\t}\n\n\tchild = find_get_task_by_vpid(pid);\n\tif (!child) {\n\t\tret = -ESRCH;\n\t\tgoto out;\n\t}\n\n\tif (request == PTRACE_ATTACH || request == PTRACE_SEIZE) {\n\t\tret = ptrace_attach(child, request, addr, data);\n\t\t/*\n\t\t * Some architectures need to do book-keeping after\n\t\t * a ptrace attach.\n\t\t */\n\t\tif (!ret)\n\t\t\tarch_ptrace_attach(child);\n\t\tgoto out_put_task_struct;\n\t}\n\n\tret = ptrace_check_attach(child, request == PTRACE_KILL ||\n\t\t\t\t  request == PTRACE_INTERRUPT);\n\tif (!ret) {\n\t\tret = compat_arch_ptrace(child, request, addr, data);\n\t\tif (ret || request != PTRACE_DETACH)\n\t\t\tptrace_unfreeze_traced(child);\n\t}\n\n out_put_task_struct:\n\tput_task_struct(child);\n out:\n\treturn ret;\n}\n#endif\t/* CONFIG_COMPAT */\n"], "buggy_code_start_loc": [373], "buggy_code_end_loc": [673], "fixing_code_start_loc": [374], "fixing_code_end_loc": [690], "type": "CWE-863", "message": "The Linux kernel before 5.17.2 mishandles seccomp permissions. The PTRACE_SEIZE code path allows attackers to bypass intended restrictions on setting the PT_SUSPEND_SECCOMP flag.", "other": {"cve": {"id": "CVE-2022-30594", "sourceIdentifier": "cve@mitre.org", "published": "2022-05-12T05:15:06.657", "lastModified": "2023-02-23T18:42:05.847", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "The Linux kernel before 5.17.2 mishandles seccomp permissions. The PTRACE_SEIZE code path allows attackers to bypass intended restrictions on setting the PT_SUSPEND_SECCOMP flag."}, {"lang": "es", "value": "El kernel de Linux versiones anteriores a 5.17.2, maneja inapropiadamente los permisos de seccomp. La ruta de c\u00f3digo PTRACE_SEIZE permite a atacantes omitir las restricciones previstas al establecer el flag PT_SUSPEND_SECCOMP"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:M/Au:N/C:P/I:P/A:P", "accessVector": "LOCAL", "accessComplexity": "MEDIUM", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "PARTIAL", "availabilityImpact": "PARTIAL", "baseScore": 4.4}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.4, "impactScore": 6.4, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-863"}]}], "configurations": [{"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "4.19.238", "matchCriteriaId": "E0B62342-F287-4C6B-92AA-C1BC61DDAFFA"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.20", "versionEndExcluding": "5.4.189", "matchCriteriaId": "8CB6E8F5-C2B1-46F3-A807-0F6104AC340F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "5.5.0", "versionEndExcluding": "5.10.110", "matchCriteriaId": "6FB6A7BA-92AE-4423-9814-EBEED5876483"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "5.11", "versionEndExcluding": "5.15.33", "matchCriteriaId": "27C42AE8-B387-43E2-938A-E1C8B40BE6D5"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "5.16.0", "versionEndExcluding": "5.16.19", "matchCriteriaId": "D765FECA-B64D-4F49-9CD1-07C9337ADB2C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "5.17", "versionEndExcluding": "5.17.2", "matchCriteriaId": "210C679C-CF84-44A3-8939-E629C87E54BF"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:9.0:*:*:*:*:*:*:*", "matchCriteriaId": "DEECE5FC-CACF-4496-A3E7-164736409252"}, {"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:10.0:*:*:*:*:*:*:*", "matchCriteriaId": "07B237A9-69A3-4A9C-9DA0-4E06BD37AE73"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:netapp:solidfire\\,_enterprise_sds_\\&_hci_storage_node:-:*:*:*:*:*:*:*", "matchCriteriaId": "DAA3919C-B2B1-4CB5-BA76-7A079AAFFC52"}, {"vulnerable": true, "criteria": "cpe:2.3:a:netapp:solidfire_\\&_hci_management_node:-:*:*:*:*:*:*:*", "matchCriteriaId": "D6D700C5-F67F-4FFB-BE69-D524592A3D2E"}, {"vulnerable": true, "criteria": "cpe:2.3:h:netapp:hci_compute_node:-:*:*:*:*:*:*:*", "matchCriteriaId": "AD7447BC-F315-4298-A822-549942FC118B"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:8300_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "4E73901F-666D-4D8B-BDFD-93DD2F70C74B"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:8300:-:*:*:*:*:*:*:*", "matchCriteriaId": "D0FD5AED-42CF-4918-B32C-D675738EF15C"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:8700_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "34B25BEF-8708-4E2C-8BA6-EBCD5267EB04"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:8700:-:*:*:*:*:*:*:*", "matchCriteriaId": "CE0F11D2-B5D9-46B4-BFC5-C86BC87D516A"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:a400_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "04E3BD77-8915-4FFC-8483-5DB5D610F829"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:a400:-:*:*:*:*:*:*:*", "matchCriteriaId": "97E94ECB-BB51-4364-BEDD-8648C193196F"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:h300s_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "6770B6C3-732E-4E22-BF1C-2D2FD610061C"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:h300s:-:*:*:*:*:*:*:*", "matchCriteriaId": "9F9C8C20-42EB-4AB5-BD97-212DEB070C43"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:h500s_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "7FFF7106-ED78-49BA-9EC5-B889E3685D53"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:h500s:-:*:*:*:*:*:*:*", "matchCriteriaId": "E63D8B0F-006E-4801-BF9D-1C001BBFB4F9"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:h700s_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "56409CEC-5A1E-4450-AA42-641E459CC2AF"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:h700s:-:*:*:*:*:*:*:*", "matchCriteriaId": "B06F4839-D16A-4A61-9BB5-55B13F41E47F"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:h410s_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "D0B4AD8A-F172-4558-AEC6-FF424BA2D912"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:h410s:-:*:*:*:*:*:*:*", "matchCriteriaId": "8497A4C9-8474-4A62-8331-3FE862ED4098"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:h410c_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "234DEFE0-5CE5-4B0A-96B8-5D227CB8ED31"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:h410c:-:*:*:*:*:*:*:*", "matchCriteriaId": "CDDF61B7-EC5C-467C-B710-B89F502CD04F"}]}]}], "references": [{"url": "http://packetstormsecurity.com/files/167386/Kernel-Live-Patch-Security-Notice-LSN-0086-1.html", "source": "cve@mitre.org", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "http://packetstormsecurity.com/files/170362/Linux-PT_SUSPEND_SECCOMP-Permission-Bypass-Ptracer-Death-Race.html", "source": "cve@mitre.org", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "https://bugs.chromium.org/p/project-zero/issues/detail?id=2276", "source": "cve@mitre.org", "tags": ["Exploit", "Mailing List", "Patch", "Third Party Advisory"]}, {"url": "https://cdn.kernel.org/pub/linux/kernel/v5.x/ChangeLog-5.17.2", "source": "cve@mitre.org", "tags": ["Mailing List", "Patch", "Vendor Advisory"]}, {"url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=ee1fee900537b5d9560e9f937402de5ddc8412f3", "source": "cve@mitre.org", "tags": ["Mailing List", "Patch", "Vendor Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/ee1fee900537b5d9560e9f937402de5ddc8412f3", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://lists.debian.org/debian-lts-announce/2022/07/msg00000.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://security.netapp.com/advisory/ntap-20220707-0001/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://www.debian.org/security/2022/dsa-5173", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/ee1fee900537b5d9560e9f937402de5ddc8412f3"}}