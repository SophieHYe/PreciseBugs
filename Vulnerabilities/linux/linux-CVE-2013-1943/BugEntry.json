{"buggy_code": ["/*\n * Kernel-based Virtual Machine driver for Linux\n *\n * This module enables machines with Intel VT-x extensions to run virtual\n * machines without emulation or binary translation.\n *\n * MMU support\n *\n * Copyright (C) 2006 Qumranet, Inc.\n * Copyright 2010 Red Hat, Inc. and/or its affiliates.\n *\n * Authors:\n *   Yaniv Kamay  <yaniv@qumranet.com>\n *   Avi Kivity   <avi@qumranet.com>\n *\n * This work is licensed under the terms of the GNU GPL, version 2.  See\n * the COPYING file in the top-level directory.\n *\n */\n\n/*\n * We need the mmu code to access both 32-bit and 64-bit guest ptes,\n * so the code in this file is compiled twice, once per pte size.\n */\n\n#if PTTYPE == 64\n\t#define pt_element_t u64\n\t#define guest_walker guest_walker64\n\t#define FNAME(name) paging##64_##name\n\t#define PT_BASE_ADDR_MASK PT64_BASE_ADDR_MASK\n\t#define PT_LVL_ADDR_MASK(lvl) PT64_LVL_ADDR_MASK(lvl)\n\t#define PT_LVL_OFFSET_MASK(lvl) PT64_LVL_OFFSET_MASK(lvl)\n\t#define PT_INDEX(addr, level) PT64_INDEX(addr, level)\n\t#define PT_LEVEL_BITS PT64_LEVEL_BITS\n\t#ifdef CONFIG_X86_64\n\t#define PT_MAX_FULL_LEVELS 4\n\t#define CMPXCHG cmpxchg\n\t#else\n\t#define CMPXCHG cmpxchg64\n\t#define PT_MAX_FULL_LEVELS 2\n\t#endif\n#elif PTTYPE == 32\n\t#define pt_element_t u32\n\t#define guest_walker guest_walker32\n\t#define FNAME(name) paging##32_##name\n\t#define PT_BASE_ADDR_MASK PT32_BASE_ADDR_MASK\n\t#define PT_LVL_ADDR_MASK(lvl) PT32_LVL_ADDR_MASK(lvl)\n\t#define PT_LVL_OFFSET_MASK(lvl) PT32_LVL_OFFSET_MASK(lvl)\n\t#define PT_INDEX(addr, level) PT32_INDEX(addr, level)\n\t#define PT_LEVEL_BITS PT32_LEVEL_BITS\n\t#define PT_MAX_FULL_LEVELS 2\n\t#define CMPXCHG cmpxchg\n#else\n\t#error Invalid PTTYPE value\n#endif\n\n#define gpte_to_gfn_lvl FNAME(gpte_to_gfn_lvl)\n#define gpte_to_gfn(pte) gpte_to_gfn_lvl((pte), PT_PAGE_TABLE_LEVEL)\n\n/*\n * The guest_walker structure emulates the behavior of the hardware page\n * table walker.\n */\nstruct guest_walker {\n\tint level;\n\tgfn_t table_gfn[PT_MAX_FULL_LEVELS];\n\tpt_element_t ptes[PT_MAX_FULL_LEVELS];\n\tpt_element_t prefetch_ptes[PTE_PREFETCH_NUM];\n\tgpa_t pte_gpa[PT_MAX_FULL_LEVELS];\n\tunsigned pt_access;\n\tunsigned pte_access;\n\tgfn_t gfn;\n\tstruct x86_exception fault;\n};\n\nstatic gfn_t gpte_to_gfn_lvl(pt_element_t gpte, int lvl)\n{\n\treturn (gpte & PT_LVL_ADDR_MASK(lvl)) >> PAGE_SHIFT;\n}\n\nstatic int FNAME(cmpxchg_gpte)(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu,\n\t\t\t gfn_t table_gfn, unsigned index,\n\t\t\t pt_element_t orig_pte, pt_element_t new_pte)\n{\n\tpt_element_t ret;\n\tpt_element_t *table;\n\tstruct page *page;\n\tgpa_t gpa;\n\n\tgpa = mmu->translate_gpa(vcpu, table_gfn << PAGE_SHIFT,\n\t\t\t\t PFERR_USER_MASK|PFERR_WRITE_MASK);\n\tif (gpa == UNMAPPED_GVA)\n\t\treturn -EFAULT;\n\n\tpage = gfn_to_page(vcpu->kvm, gpa_to_gfn(gpa));\n\n\ttable = kmap_atomic(page, KM_USER0);\n\tret = CMPXCHG(&table[index], orig_pte, new_pte);\n\tkunmap_atomic(table, KM_USER0);\n\n\tkvm_release_page_dirty(page);\n\n\treturn (ret != orig_pte);\n}\n\nstatic unsigned FNAME(gpte_access)(struct kvm_vcpu *vcpu, pt_element_t gpte)\n{\n\tunsigned access;\n\n\taccess = (gpte & (PT_WRITABLE_MASK | PT_USER_MASK)) | ACC_EXEC_MASK;\n#if PTTYPE == 64\n\tif (vcpu->arch.mmu.nx)\n\t\taccess &= ~(gpte >> PT64_NX_SHIFT);\n#endif\n\treturn access;\n}\n\n/*\n * Fetch a guest pte for a guest virtual address\n */\nstatic int FNAME(walk_addr_generic)(struct guest_walker *walker,\n\t\t\t\t    struct kvm_vcpu *vcpu, struct kvm_mmu *mmu,\n\t\t\t\t    gva_t addr, u32 access)\n{\n\tpt_element_t pte;\n\tpt_element_t __user *ptep_user;\n\tgfn_t table_gfn;\n\tunsigned index, pt_access, uninitialized_var(pte_access);\n\tgpa_t pte_gpa;\n\tbool eperm, present, rsvd_fault;\n\tint offset, write_fault, user_fault, fetch_fault;\n\n\twrite_fault = access & PFERR_WRITE_MASK;\n\tuser_fault = access & PFERR_USER_MASK;\n\tfetch_fault = access & PFERR_FETCH_MASK;\n\n\ttrace_kvm_mmu_pagetable_walk(addr, write_fault, user_fault,\n\t\t\t\t     fetch_fault);\nwalk:\n\tpresent = true;\n\teperm = rsvd_fault = false;\n\twalker->level = mmu->root_level;\n\tpte           = mmu->get_cr3(vcpu);\n\n#if PTTYPE == 64\n\tif (walker->level == PT32E_ROOT_LEVEL) {\n\t\tpte = kvm_pdptr_read_mmu(vcpu, mmu, (addr >> 30) & 3);\n\t\ttrace_kvm_mmu_paging_element(pte, walker->level);\n\t\tif (!is_present_gpte(pte)) {\n\t\t\tpresent = false;\n\t\t\tgoto error;\n\t\t}\n\t\t--walker->level;\n\t}\n#endif\n\tASSERT((!is_long_mode(vcpu) && is_pae(vcpu)) ||\n\t       (mmu->get_cr3(vcpu) & CR3_NONPAE_RESERVED_BITS) == 0);\n\n\tpt_access = ACC_ALL;\n\n\tfor (;;) {\n\t\tgfn_t real_gfn;\n\t\tunsigned long host_addr;\n\n\t\tindex = PT_INDEX(addr, walker->level);\n\n\t\ttable_gfn = gpte_to_gfn(pte);\n\t\toffset    = index * sizeof(pt_element_t);\n\t\tpte_gpa   = gfn_to_gpa(table_gfn) + offset;\n\t\twalker->table_gfn[walker->level - 1] = table_gfn;\n\t\twalker->pte_gpa[walker->level - 1] = pte_gpa;\n\n\t\treal_gfn = mmu->translate_gpa(vcpu, gfn_to_gpa(table_gfn),\n\t\t\t\t\t      PFERR_USER_MASK|PFERR_WRITE_MASK);\n\t\tif (unlikely(real_gfn == UNMAPPED_GVA)) {\n\t\t\tpresent = false;\n\t\t\tbreak;\n\t\t}\n\t\treal_gfn = gpa_to_gfn(real_gfn);\n\n\t\thost_addr = gfn_to_hva(vcpu->kvm, real_gfn);\n\t\tif (unlikely(kvm_is_error_hva(host_addr))) {\n\t\t\tpresent = false;\n\t\t\tbreak;\n\t\t}\n\n\t\tptep_user = (pt_element_t __user *)((void *)host_addr + offset);\n\t\tif (unlikely(copy_from_user(&pte, ptep_user, sizeof(pte)))) {\n\t\t\tpresent = false;\n\t\t\tbreak;\n\t\t}\n\n\t\ttrace_kvm_mmu_paging_element(pte, walker->level);\n\n\t\tif (unlikely(!is_present_gpte(pte))) {\n\t\t\tpresent = false;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (unlikely(is_rsvd_bits_set(&vcpu->arch.mmu, pte,\n\t\t\t\t\t      walker->level))) {\n\t\t\trsvd_fault = true;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (unlikely(write_fault && !is_writable_pte(pte)\n\t\t\t     && (user_fault || is_write_protection(vcpu))))\n\t\t\teperm = true;\n\n\t\tif (unlikely(user_fault && !(pte & PT_USER_MASK)))\n\t\t\teperm = true;\n\n#if PTTYPE == 64\n\t\tif (unlikely(fetch_fault && (pte & PT64_NX_MASK)))\n\t\t\teperm = true;\n#endif\n\n\t\tif (!eperm && !rsvd_fault\n\t\t    && unlikely(!(pte & PT_ACCESSED_MASK))) {\n\t\t\tint ret;\n\t\t\ttrace_kvm_mmu_set_accessed_bit(table_gfn, index,\n\t\t\t\t\t\t       sizeof(pte));\n\t\t\tret = FNAME(cmpxchg_gpte)(vcpu, mmu, table_gfn,\n\t\t\t\t\tindex, pte, pte|PT_ACCESSED_MASK);\n\t\t\tif (ret < 0) {\n\t\t\t\tpresent = false;\n\t\t\t\tbreak;\n\t\t\t} else if (ret)\n\t\t\t\tgoto walk;\n\n\t\t\tmark_page_dirty(vcpu->kvm, table_gfn);\n\t\t\tpte |= PT_ACCESSED_MASK;\n\t\t}\n\n\t\tpte_access = pt_access & FNAME(gpte_access)(vcpu, pte);\n\n\t\twalker->ptes[walker->level - 1] = pte;\n\n\t\tif ((walker->level == PT_PAGE_TABLE_LEVEL) ||\n\t\t    ((walker->level == PT_DIRECTORY_LEVEL) &&\n\t\t\t\tis_large_pte(pte) &&\n\t\t\t\t(PTTYPE == 64 || is_pse(vcpu))) ||\n\t\t    ((walker->level == PT_PDPE_LEVEL) &&\n\t\t\t\tis_large_pte(pte) &&\n\t\t\t\tmmu->root_level == PT64_ROOT_LEVEL)) {\n\t\t\tint lvl = walker->level;\n\t\t\tgpa_t real_gpa;\n\t\t\tgfn_t gfn;\n\t\t\tu32 ac;\n\n\t\t\tgfn = gpte_to_gfn_lvl(pte, lvl);\n\t\t\tgfn += (addr & PT_LVL_OFFSET_MASK(lvl)) >> PAGE_SHIFT;\n\n\t\t\tif (PTTYPE == 32 &&\n\t\t\t    walker->level == PT_DIRECTORY_LEVEL &&\n\t\t\t    is_cpuid_PSE36())\n\t\t\t\tgfn += pse36_gfn_delta(pte);\n\n\t\t\tac = write_fault | fetch_fault | user_fault;\n\n\t\t\treal_gpa = mmu->translate_gpa(vcpu, gfn_to_gpa(gfn),\n\t\t\t\t\t\t      ac);\n\t\t\tif (real_gpa == UNMAPPED_GVA)\n\t\t\t\treturn 0;\n\n\t\t\twalker->gfn = real_gpa >> PAGE_SHIFT;\n\n\t\t\tbreak;\n\t\t}\n\n\t\tpt_access = pte_access;\n\t\t--walker->level;\n\t}\n\n\tif (unlikely(!present || eperm || rsvd_fault))\n\t\tgoto error;\n\n\tif (write_fault && unlikely(!is_dirty_gpte(pte))) {\n\t\tint ret;\n\n\t\ttrace_kvm_mmu_set_dirty_bit(table_gfn, index, sizeof(pte));\n\t\tret = FNAME(cmpxchg_gpte)(vcpu, mmu, table_gfn, index, pte,\n\t\t\t    pte|PT_DIRTY_MASK);\n\t\tif (ret < 0) {\n\t\t\tpresent = false;\n\t\t\tgoto error;\n\t\t} else if (ret)\n\t\t\tgoto walk;\n\n\t\tmark_page_dirty(vcpu->kvm, table_gfn);\n\t\tpte |= PT_DIRTY_MASK;\n\t\twalker->ptes[walker->level - 1] = pte;\n\t}\n\n\twalker->pt_access = pt_access;\n\twalker->pte_access = pte_access;\n\tpgprintk(\"%s: pte %llx pte_access %x pt_access %x\\n\",\n\t\t __func__, (u64)pte, pte_access, pt_access);\n\treturn 1;\n\nerror:\n\twalker->fault.vector = PF_VECTOR;\n\twalker->fault.error_code_valid = true;\n\twalker->fault.error_code = 0;\n\tif (present)\n\t\twalker->fault.error_code |= PFERR_PRESENT_MASK;\n\n\twalker->fault.error_code |= write_fault | user_fault;\n\n\tif (fetch_fault && mmu->nx)\n\t\twalker->fault.error_code |= PFERR_FETCH_MASK;\n\tif (rsvd_fault)\n\t\twalker->fault.error_code |= PFERR_RSVD_MASK;\n\n\twalker->fault.address = addr;\n\twalker->fault.nested_page_fault = mmu != vcpu->arch.walk_mmu;\n\n\ttrace_kvm_mmu_walker_error(walker->fault.error_code);\n\treturn 0;\n}\n\nstatic int FNAME(walk_addr)(struct guest_walker *walker,\n\t\t\t    struct kvm_vcpu *vcpu, gva_t addr, u32 access)\n{\n\treturn FNAME(walk_addr_generic)(walker, vcpu, &vcpu->arch.mmu, addr,\n\t\t\t\t\taccess);\n}\n\nstatic int FNAME(walk_addr_nested)(struct guest_walker *walker,\n\t\t\t\t   struct kvm_vcpu *vcpu, gva_t addr,\n\t\t\t\t   u32 access)\n{\n\treturn FNAME(walk_addr_generic)(walker, vcpu, &vcpu->arch.nested_mmu,\n\t\t\t\t\taddr, access);\n}\n\nstatic bool FNAME(prefetch_invalid_gpte)(struct kvm_vcpu *vcpu,\n\t\t\t\t    struct kvm_mmu_page *sp, u64 *spte,\n\t\t\t\t    pt_element_t gpte)\n{\n\tu64 nonpresent = shadow_trap_nonpresent_pte;\n\n\tif (is_rsvd_bits_set(&vcpu->arch.mmu, gpte, PT_PAGE_TABLE_LEVEL))\n\t\tgoto no_present;\n\n\tif (!is_present_gpte(gpte)) {\n\t\tif (!sp->unsync)\n\t\t\tnonpresent = shadow_notrap_nonpresent_pte;\n\t\tgoto no_present;\n\t}\n\n\tif (!(gpte & PT_ACCESSED_MASK))\n\t\tgoto no_present;\n\n\treturn false;\n\nno_present:\n\tdrop_spte(vcpu->kvm, spte, nonpresent);\n\treturn true;\n}\n\nstatic void FNAME(update_pte)(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,\n\t\t\t      u64 *spte, const void *pte)\n{\n\tpt_element_t gpte;\n\tunsigned pte_access;\n\tpfn_t pfn;\n\n\tgpte = *(const pt_element_t *)pte;\n\tif (FNAME(prefetch_invalid_gpte)(vcpu, sp, spte, gpte))\n\t\treturn;\n\n\tpgprintk(\"%s: gpte %llx spte %p\\n\", __func__, (u64)gpte, spte);\n\tpte_access = sp->role.access & FNAME(gpte_access)(vcpu, gpte);\n\tpfn = gfn_to_pfn_atomic(vcpu->kvm, gpte_to_gfn(gpte));\n\tif (is_error_pfn(pfn)) {\n\t\tkvm_release_pfn_clean(pfn);\n\t\treturn;\n\t}\n\n\t/*\n\t * we call mmu_set_spte() with host_writable = true because that\n\t * vcpu->arch.update_pte.pfn was fetched from get_user_pages(write = 1).\n\t */\n\tmmu_set_spte(vcpu, spte, sp->role.access, pte_access, 0, 0,\n\t\t     is_dirty_gpte(gpte), NULL, PT_PAGE_TABLE_LEVEL,\n\t\t     gpte_to_gfn(gpte), pfn, true, true);\n}\n\nstatic bool FNAME(gpte_changed)(struct kvm_vcpu *vcpu,\n\t\t\t\tstruct guest_walker *gw, int level)\n{\n\tpt_element_t curr_pte;\n\tgpa_t base_gpa, pte_gpa = gw->pte_gpa[level - 1];\n\tu64 mask;\n\tint r, index;\n\n\tif (level == PT_PAGE_TABLE_LEVEL) {\n\t\tmask = PTE_PREFETCH_NUM * sizeof(pt_element_t) - 1;\n\t\tbase_gpa = pte_gpa & ~mask;\n\t\tindex = (pte_gpa - base_gpa) / sizeof(pt_element_t);\n\n\t\tr = kvm_read_guest_atomic(vcpu->kvm, base_gpa,\n\t\t\t\tgw->prefetch_ptes, sizeof(gw->prefetch_ptes));\n\t\tcurr_pte = gw->prefetch_ptes[index];\n\t} else\n\t\tr = kvm_read_guest_atomic(vcpu->kvm, pte_gpa,\n\t\t\t\t  &curr_pte, sizeof(curr_pte));\n\n\treturn r || curr_pte != gw->ptes[level - 1];\n}\n\nstatic void FNAME(pte_prefetch)(struct kvm_vcpu *vcpu, struct guest_walker *gw,\n\t\t\t\tu64 *sptep)\n{\n\tstruct kvm_mmu_page *sp;\n\tpt_element_t *gptep = gw->prefetch_ptes;\n\tu64 *spte;\n\tint i;\n\n\tsp = page_header(__pa(sptep));\n\n\tif (sp->role.level > PT_PAGE_TABLE_LEVEL)\n\t\treturn;\n\n\tif (sp->role.direct)\n\t\treturn __direct_pte_prefetch(vcpu, sp, sptep);\n\n\ti = (sptep - sp->spt) & ~(PTE_PREFETCH_NUM - 1);\n\tspte = sp->spt + i;\n\n\tfor (i = 0; i < PTE_PREFETCH_NUM; i++, spte++) {\n\t\tpt_element_t gpte;\n\t\tunsigned pte_access;\n\t\tgfn_t gfn;\n\t\tpfn_t pfn;\n\t\tbool dirty;\n\n\t\tif (spte == sptep)\n\t\t\tcontinue;\n\n\t\tif (*spte != shadow_trap_nonpresent_pte)\n\t\t\tcontinue;\n\n\t\tgpte = gptep[i];\n\n\t\tif (FNAME(prefetch_invalid_gpte)(vcpu, sp, spte, gpte))\n\t\t\tcontinue;\n\n\t\tpte_access = sp->role.access & FNAME(gpte_access)(vcpu, gpte);\n\t\tgfn = gpte_to_gfn(gpte);\n\t\tdirty = is_dirty_gpte(gpte);\n\t\tpfn = pte_prefetch_gfn_to_pfn(vcpu, gfn,\n\t\t\t\t      (pte_access & ACC_WRITE_MASK) && dirty);\n\t\tif (is_error_pfn(pfn)) {\n\t\t\tkvm_release_pfn_clean(pfn);\n\t\t\tbreak;\n\t\t}\n\n\t\tmmu_set_spte(vcpu, spte, sp->role.access, pte_access, 0, 0,\n\t\t\t     dirty, NULL, PT_PAGE_TABLE_LEVEL, gfn,\n\t\t\t     pfn, true, true);\n\t}\n}\n\n/*\n * Fetch a shadow pte for a specific level in the paging hierarchy.\n */\nstatic u64 *FNAME(fetch)(struct kvm_vcpu *vcpu, gva_t addr,\n\t\t\t struct guest_walker *gw,\n\t\t\t int user_fault, int write_fault, int hlevel,\n\t\t\t int *ptwrite, pfn_t pfn, bool map_writable,\n\t\t\t bool prefault)\n{\n\tunsigned access = gw->pt_access;\n\tstruct kvm_mmu_page *sp = NULL;\n\tbool dirty = is_dirty_gpte(gw->ptes[gw->level - 1]);\n\tint top_level;\n\tunsigned direct_access;\n\tstruct kvm_shadow_walk_iterator it;\n\n\tif (!is_present_gpte(gw->ptes[gw->level - 1]))\n\t\treturn NULL;\n\n\tdirect_access = gw->pt_access & gw->pte_access;\n\tif (!dirty)\n\t\tdirect_access &= ~ACC_WRITE_MASK;\n\n\ttop_level = vcpu->arch.mmu.root_level;\n\tif (top_level == PT32E_ROOT_LEVEL)\n\t\ttop_level = PT32_ROOT_LEVEL;\n\t/*\n\t * Verify that the top-level gpte is still there.  Since the page\n\t * is a root page, it is either write protected (and cannot be\n\t * changed from now on) or it is invalid (in which case, we don't\n\t * really care if it changes underneath us after this point).\n\t */\n\tif (FNAME(gpte_changed)(vcpu, gw, top_level))\n\t\tgoto out_gpte_changed;\n\n\tfor (shadow_walk_init(&it, vcpu, addr);\n\t     shadow_walk_okay(&it) && it.level > gw->level;\n\t     shadow_walk_next(&it)) {\n\t\tgfn_t table_gfn;\n\n\t\tdrop_large_spte(vcpu, it.sptep);\n\n\t\tsp = NULL;\n\t\tif (!is_shadow_present_pte(*it.sptep)) {\n\t\t\ttable_gfn = gw->table_gfn[it.level - 2];\n\t\t\tsp = kvm_mmu_get_page(vcpu, table_gfn, addr, it.level-1,\n\t\t\t\t\t      false, access, it.sptep);\n\t\t}\n\n\t\t/*\n\t\t * Verify that the gpte in the page we've just write\n\t\t * protected is still there.\n\t\t */\n\t\tif (FNAME(gpte_changed)(vcpu, gw, it.level - 1))\n\t\t\tgoto out_gpte_changed;\n\n\t\tif (sp)\n\t\t\tlink_shadow_page(it.sptep, sp);\n\t}\n\n\tfor (;\n\t     shadow_walk_okay(&it) && it.level > hlevel;\n\t     shadow_walk_next(&it)) {\n\t\tgfn_t direct_gfn;\n\n\t\tvalidate_direct_spte(vcpu, it.sptep, direct_access);\n\n\t\tdrop_large_spte(vcpu, it.sptep);\n\n\t\tif (is_shadow_present_pte(*it.sptep))\n\t\t\tcontinue;\n\n\t\tdirect_gfn = gw->gfn & ~(KVM_PAGES_PER_HPAGE(it.level) - 1);\n\n\t\tsp = kvm_mmu_get_page(vcpu, direct_gfn, addr, it.level-1,\n\t\t\t\t      true, direct_access, it.sptep);\n\t\tlink_shadow_page(it.sptep, sp);\n\t}\n\n\tmmu_set_spte(vcpu, it.sptep, access, gw->pte_access & access,\n\t\t     user_fault, write_fault, dirty, ptwrite, it.level,\n\t\t     gw->gfn, pfn, prefault, map_writable);\n\tFNAME(pte_prefetch)(vcpu, gw, it.sptep);\n\n\treturn it.sptep;\n\nout_gpte_changed:\n\tif (sp)\n\t\tkvm_mmu_put_page(sp, it.sptep);\n\tkvm_release_pfn_clean(pfn);\n\treturn NULL;\n}\n\n/*\n * Page fault handler.  There are several causes for a page fault:\n *   - there is no shadow pte for the guest pte\n *   - write access through a shadow pte marked read only so that we can set\n *     the dirty bit\n *   - write access to a shadow pte marked read only so we can update the page\n *     dirty bitmap, when userspace requests it\n *   - mmio access; in this case we will never install a present shadow pte\n *   - normal guest page fault due to the guest pte marked not present, not\n *     writable, or not executable\n *\n *  Returns: 1 if we need to emulate the instruction, 0 otherwise, or\n *           a negative value on error.\n */\nstatic int FNAME(page_fault)(struct kvm_vcpu *vcpu, gva_t addr, u32 error_code,\n\t\t\t     bool prefault)\n{\n\tint write_fault = error_code & PFERR_WRITE_MASK;\n\tint user_fault = error_code & PFERR_USER_MASK;\n\tstruct guest_walker walker;\n\tu64 *sptep;\n\tint write_pt = 0;\n\tint r;\n\tpfn_t pfn;\n\tint level = PT_PAGE_TABLE_LEVEL;\n\tint force_pt_level;\n\tunsigned long mmu_seq;\n\tbool map_writable;\n\n\tpgprintk(\"%s: addr %lx err %x\\n\", __func__, addr, error_code);\n\n\tr = mmu_topup_memory_caches(vcpu);\n\tif (r)\n\t\treturn r;\n\n\t/*\n\t * Look up the guest pte for the faulting address.\n\t */\n\tr = FNAME(walk_addr)(&walker, vcpu, addr, error_code);\n\n\t/*\n\t * The page is not mapped by the guest.  Let the guest handle it.\n\t */\n\tif (!r) {\n\t\tpgprintk(\"%s: guest page fault\\n\", __func__);\n\t\tif (!prefault) {\n\t\t\tinject_page_fault(vcpu, &walker.fault);\n\t\t\t/* reset fork detector */\n\t\t\tvcpu->arch.last_pt_write_count = 0;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tif (walker.level >= PT_DIRECTORY_LEVEL)\n\t\tforce_pt_level = mapping_level_dirty_bitmap(vcpu, walker.gfn);\n\telse\n\t\tforce_pt_level = 1;\n\tif (!force_pt_level) {\n\t\tlevel = min(walker.level, mapping_level(vcpu, walker.gfn));\n\t\twalker.gfn = walker.gfn & ~(KVM_PAGES_PER_HPAGE(level) - 1);\n\t}\n\n\tmmu_seq = vcpu->kvm->mmu_notifier_seq;\n\tsmp_rmb();\n\n\tif (try_async_pf(vcpu, prefault, walker.gfn, addr, &pfn, write_fault,\n\t\t\t &map_writable))\n\t\treturn 0;\n\n\t/* mmio */\n\tif (is_error_pfn(pfn))\n\t\treturn kvm_handle_bad_page(vcpu->kvm, walker.gfn, pfn);\n\n\tspin_lock(&vcpu->kvm->mmu_lock);\n\tif (mmu_notifier_retry(vcpu, mmu_seq))\n\t\tgoto out_unlock;\n\n\ttrace_kvm_mmu_audit(vcpu, AUDIT_PRE_PAGE_FAULT);\n\tkvm_mmu_free_some_pages(vcpu);\n\tif (!force_pt_level)\n\t\ttransparent_hugepage_adjust(vcpu, &walker.gfn, &pfn, &level);\n\tsptep = FNAME(fetch)(vcpu, addr, &walker, user_fault, write_fault,\n\t\t\t     level, &write_pt, pfn, map_writable, prefault);\n\t(void)sptep;\n\tpgprintk(\"%s: shadow pte %p %llx ptwrite %d\\n\", __func__,\n\t\t sptep, *sptep, write_pt);\n\n\tif (!write_pt)\n\t\tvcpu->arch.last_pt_write_count = 0; /* reset fork detector */\n\n\t++vcpu->stat.pf_fixed;\n\ttrace_kvm_mmu_audit(vcpu, AUDIT_POST_PAGE_FAULT);\n\tspin_unlock(&vcpu->kvm->mmu_lock);\n\n\treturn write_pt;\n\nout_unlock:\n\tspin_unlock(&vcpu->kvm->mmu_lock);\n\tkvm_release_pfn_clean(pfn);\n\treturn 0;\n}\n\nstatic void FNAME(invlpg)(struct kvm_vcpu *vcpu, gva_t gva)\n{\n\tstruct kvm_shadow_walk_iterator iterator;\n\tstruct kvm_mmu_page *sp;\n\tgpa_t pte_gpa = -1;\n\tint level;\n\tu64 *sptep;\n\tint need_flush = 0;\n\n\tspin_lock(&vcpu->kvm->mmu_lock);\n\n\tfor_each_shadow_entry(vcpu, gva, iterator) {\n\t\tlevel = iterator.level;\n\t\tsptep = iterator.sptep;\n\n\t\tsp = page_header(__pa(sptep));\n\t\tif (is_last_spte(*sptep, level)) {\n\t\t\tint offset, shift;\n\n\t\t\tif (!sp->unsync)\n\t\t\t\tbreak;\n\n\t\t\tshift = PAGE_SHIFT -\n\t\t\t\t  (PT_LEVEL_BITS - PT64_LEVEL_BITS) * level;\n\t\t\toffset = sp->role.quadrant << shift;\n\n\t\t\tpte_gpa = (sp->gfn << PAGE_SHIFT) + offset;\n\t\t\tpte_gpa += (sptep - sp->spt) * sizeof(pt_element_t);\n\n\t\t\tif (is_shadow_present_pte(*sptep)) {\n\t\t\t\tif (is_large_pte(*sptep))\n\t\t\t\t\t--vcpu->kvm->stat.lpages;\n\t\t\t\tdrop_spte(vcpu->kvm, sptep,\n\t\t\t\t\t  shadow_trap_nonpresent_pte);\n\t\t\t\tneed_flush = 1;\n\t\t\t} else\n\t\t\t\t__set_spte(sptep, shadow_trap_nonpresent_pte);\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!is_shadow_present_pte(*sptep) || !sp->unsync_children)\n\t\t\tbreak;\n\t}\n\n\tif (need_flush)\n\t\tkvm_flush_remote_tlbs(vcpu->kvm);\n\n\tatomic_inc(&vcpu->kvm->arch.invlpg_counter);\n\n\tspin_unlock(&vcpu->kvm->mmu_lock);\n\n\tif (pte_gpa == -1)\n\t\treturn;\n\n\tif (mmu_topup_memory_caches(vcpu))\n\t\treturn;\n\tkvm_mmu_pte_write(vcpu, pte_gpa, NULL, sizeof(pt_element_t), 0);\n}\n\nstatic gpa_t FNAME(gva_to_gpa)(struct kvm_vcpu *vcpu, gva_t vaddr, u32 access,\n\t\t\t       struct x86_exception *exception)\n{\n\tstruct guest_walker walker;\n\tgpa_t gpa = UNMAPPED_GVA;\n\tint r;\n\n\tr = FNAME(walk_addr)(&walker, vcpu, vaddr, access);\n\n\tif (r) {\n\t\tgpa = gfn_to_gpa(walker.gfn);\n\t\tgpa |= vaddr & ~PAGE_MASK;\n\t} else if (exception)\n\t\t*exception = walker.fault;\n\n\treturn gpa;\n}\n\nstatic gpa_t FNAME(gva_to_gpa_nested)(struct kvm_vcpu *vcpu, gva_t vaddr,\n\t\t\t\t      u32 access,\n\t\t\t\t      struct x86_exception *exception)\n{\n\tstruct guest_walker walker;\n\tgpa_t gpa = UNMAPPED_GVA;\n\tint r;\n\n\tr = FNAME(walk_addr_nested)(&walker, vcpu, vaddr, access);\n\n\tif (r) {\n\t\tgpa = gfn_to_gpa(walker.gfn);\n\t\tgpa |= vaddr & ~PAGE_MASK;\n\t} else if (exception)\n\t\t*exception = walker.fault;\n\n\treturn gpa;\n}\n\nstatic void FNAME(prefetch_page)(struct kvm_vcpu *vcpu,\n\t\t\t\t struct kvm_mmu_page *sp)\n{\n\tint i, j, offset, r;\n\tpt_element_t pt[256 / sizeof(pt_element_t)];\n\tgpa_t pte_gpa;\n\n\tif (sp->role.direct\n\t    || (PTTYPE == 32 && sp->role.level > PT_PAGE_TABLE_LEVEL)) {\n\t\tnonpaging_prefetch_page(vcpu, sp);\n\t\treturn;\n\t}\n\n\tpte_gpa = gfn_to_gpa(sp->gfn);\n\tif (PTTYPE == 32) {\n\t\toffset = sp->role.quadrant << PT64_LEVEL_BITS;\n\t\tpte_gpa += offset * sizeof(pt_element_t);\n\t}\n\n\tfor (i = 0; i < PT64_ENT_PER_PAGE; i += ARRAY_SIZE(pt)) {\n\t\tr = kvm_read_guest_atomic(vcpu->kvm, pte_gpa, pt, sizeof pt);\n\t\tpte_gpa += ARRAY_SIZE(pt) * sizeof(pt_element_t);\n\t\tfor (j = 0; j < ARRAY_SIZE(pt); ++j)\n\t\t\tif (r || is_present_gpte(pt[j]))\n\t\t\t\tsp->spt[i+j] = shadow_trap_nonpresent_pte;\n\t\t\telse\n\t\t\t\tsp->spt[i+j] = shadow_notrap_nonpresent_pte;\n\t}\n}\n\n/*\n * Using the cached information from sp->gfns is safe because:\n * - The spte has a reference to the struct page, so the pfn for a given gfn\n *   can't change unless all sptes pointing to it are nuked first.\n *\n * Note:\n *   We should flush all tlbs if spte is dropped even though guest is\n *   responsible for it. Since if we don't, kvm_mmu_notifier_invalidate_page\n *   and kvm_mmu_notifier_invalidate_range_start detect the mapping page isn't\n *   used by guest then tlbs are not flushed, so guest is allowed to access the\n *   freed pages.\n *   And we increase kvm->tlbs_dirty to delay tlbs flush in this case.\n */\nstatic int FNAME(sync_page)(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp)\n{\n\tint i, offset, nr_present;\n\tbool host_writable;\n\tgpa_t first_pte_gpa;\n\n\toffset = nr_present = 0;\n\n\t/* direct kvm_mmu_page can not be unsync. */\n\tBUG_ON(sp->role.direct);\n\n\tif (PTTYPE == 32)\n\t\toffset = sp->role.quadrant << PT64_LEVEL_BITS;\n\n\tfirst_pte_gpa = gfn_to_gpa(sp->gfn) + offset * sizeof(pt_element_t);\n\n\tfor (i = 0; i < PT64_ENT_PER_PAGE; i++) {\n\t\tunsigned pte_access;\n\t\tpt_element_t gpte;\n\t\tgpa_t pte_gpa;\n\t\tgfn_t gfn;\n\n\t\tif (!is_shadow_present_pte(sp->spt[i]))\n\t\t\tcontinue;\n\n\t\tpte_gpa = first_pte_gpa + i * sizeof(pt_element_t);\n\n\t\tif (kvm_read_guest_atomic(vcpu->kvm, pte_gpa, &gpte,\n\t\t\t\t\t  sizeof(pt_element_t)))\n\t\t\treturn -EINVAL;\n\n\t\tgfn = gpte_to_gfn(gpte);\n\n\t\tif (FNAME(prefetch_invalid_gpte)(vcpu, sp, &sp->spt[i], gpte)) {\n\t\t\tvcpu->kvm->tlbs_dirty++;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (gfn != sp->gfns[i]) {\n\t\t\tdrop_spte(vcpu->kvm, &sp->spt[i],\n\t\t\t\t      shadow_trap_nonpresent_pte);\n\t\t\tvcpu->kvm->tlbs_dirty++;\n\t\t\tcontinue;\n\t\t}\n\n\t\tnr_present++;\n\t\tpte_access = sp->role.access & FNAME(gpte_access)(vcpu, gpte);\n\t\thost_writable = sp->spt[i] & SPTE_HOST_WRITEABLE;\n\n\t\tset_spte(vcpu, &sp->spt[i], pte_access, 0, 0,\n\t\t\t is_dirty_gpte(gpte), PT_PAGE_TABLE_LEVEL, gfn,\n\t\t\t spte_to_pfn(sp->spt[i]), true, false,\n\t\t\t host_writable);\n\t}\n\n\treturn !nr_present;\n}\n\n#undef pt_element_t\n#undef guest_walker\n#undef FNAME\n#undef PT_BASE_ADDR_MASK\n#undef PT_INDEX\n#undef PT_LVL_ADDR_MASK\n#undef PT_LVL_OFFSET_MASK\n#undef PT_LEVEL_BITS\n#undef PT_MAX_FULL_LEVELS\n#undef gpte_to_gfn\n#undef gpte_to_gfn_lvl\n#undef CMPXCHG\n", "/*\n * Kernel-based Virtual Machine driver for Linux\n *\n * This module enables machines with Intel VT-x extensions to run virtual\n * machines without emulation or binary translation.\n *\n * Copyright (C) 2006 Qumranet, Inc.\n * Copyright 2010 Red Hat, Inc. and/or its affiliates.\n *\n * Authors:\n *   Avi Kivity   <avi@qumranet.com>\n *   Yaniv Kamay  <yaniv@qumranet.com>\n *\n * This work is licensed under the terms of the GNU GPL, version 2.  See\n * the COPYING file in the top-level directory.\n *\n */\n\n#include \"iodev.h\"\n\n#include <linux/kvm_host.h>\n#include <linux/kvm.h>\n#include <linux/module.h>\n#include <linux/errno.h>\n#include <linux/percpu.h>\n#include <linux/mm.h>\n#include <linux/miscdevice.h>\n#include <linux/vmalloc.h>\n#include <linux/reboot.h>\n#include <linux/debugfs.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/syscore_ops.h>\n#include <linux/cpu.h>\n#include <linux/sched.h>\n#include <linux/cpumask.h>\n#include <linux/smp.h>\n#include <linux/anon_inodes.h>\n#include <linux/profile.h>\n#include <linux/kvm_para.h>\n#include <linux/pagemap.h>\n#include <linux/mman.h>\n#include <linux/swap.h>\n#include <linux/bitops.h>\n#include <linux/spinlock.h>\n#include <linux/compat.h>\n#include <linux/srcu.h>\n#include <linux/hugetlb.h>\n#include <linux/slab.h>\n\n#include <asm/processor.h>\n#include <asm/io.h>\n#include <asm/uaccess.h>\n#include <asm/pgtable.h>\n\n#include \"coalesced_mmio.h\"\n#include \"async_pf.h\"\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/kvm.h>\n\nMODULE_AUTHOR(\"Qumranet\");\nMODULE_LICENSE(\"GPL\");\n\n/*\n * Ordering of locks:\n *\n * \t\tkvm->lock --> kvm->slots_lock --> kvm->irq_lock\n */\n\nDEFINE_RAW_SPINLOCK(kvm_lock);\nLIST_HEAD(vm_list);\n\nstatic cpumask_var_t cpus_hardware_enabled;\nstatic int kvm_usage_count = 0;\nstatic atomic_t hardware_enable_failed;\n\nstruct kmem_cache *kvm_vcpu_cache;\nEXPORT_SYMBOL_GPL(kvm_vcpu_cache);\n\nstatic __read_mostly struct preempt_ops kvm_preempt_ops;\n\nstruct dentry *kvm_debugfs_dir;\n\nstatic long kvm_vcpu_ioctl(struct file *file, unsigned int ioctl,\n\t\t\t   unsigned long arg);\nstatic int hardware_enable_all(void);\nstatic void hardware_disable_all(void);\n\nstatic void kvm_io_bus_destroy(struct kvm_io_bus *bus);\n\nbool kvm_rebooting;\nEXPORT_SYMBOL_GPL(kvm_rebooting);\n\nstatic bool largepages_enabled = true;\n\nstatic struct page *hwpoison_page;\nstatic pfn_t hwpoison_pfn;\n\nstatic struct page *fault_page;\nstatic pfn_t fault_pfn;\n\ninline int kvm_is_mmio_pfn(pfn_t pfn)\n{\n\tif (pfn_valid(pfn)) {\n\t\tint reserved;\n\t\tstruct page *tail = pfn_to_page(pfn);\n\t\tstruct page *head = compound_trans_head(tail);\n\t\treserved = PageReserved(head);\n\t\tif (head != tail) {\n\t\t\t/*\n\t\t\t * \"head\" is not a dangling pointer\n\t\t\t * (compound_trans_head takes care of that)\n\t\t\t * but the hugepage may have been splitted\n\t\t\t * from under us (and we may not hold a\n\t\t\t * reference count on the head page so it can\n\t\t\t * be reused before we run PageReferenced), so\n\t\t\t * we've to check PageTail before returning\n\t\t\t * what we just read.\n\t\t\t */\n\t\t\tsmp_rmb();\n\t\t\tif (PageTail(tail))\n\t\t\t\treturn reserved;\n\t\t}\n\t\treturn PageReserved(tail);\n\t}\n\n\treturn true;\n}\n\n/*\n * Switches to specified vcpu, until a matching vcpu_put()\n */\nvoid vcpu_load(struct kvm_vcpu *vcpu)\n{\n\tint cpu;\n\n\tmutex_lock(&vcpu->mutex);\n\tif (unlikely(vcpu->pid != current->pids[PIDTYPE_PID].pid)) {\n\t\t/* The thread running this VCPU changed. */\n\t\tstruct pid *oldpid = vcpu->pid;\n\t\tstruct pid *newpid = get_task_pid(current, PIDTYPE_PID);\n\t\trcu_assign_pointer(vcpu->pid, newpid);\n\t\tsynchronize_rcu();\n\t\tput_pid(oldpid);\n\t}\n\tcpu = get_cpu();\n\tpreempt_notifier_register(&vcpu->preempt_notifier);\n\tkvm_arch_vcpu_load(vcpu, cpu);\n\tput_cpu();\n}\n\nvoid vcpu_put(struct kvm_vcpu *vcpu)\n{\n\tpreempt_disable();\n\tkvm_arch_vcpu_put(vcpu);\n\tpreempt_notifier_unregister(&vcpu->preempt_notifier);\n\tpreempt_enable();\n\tmutex_unlock(&vcpu->mutex);\n}\n\nstatic void ack_flush(void *_completed)\n{\n}\n\nstatic bool make_all_cpus_request(struct kvm *kvm, unsigned int req)\n{\n\tint i, cpu, me;\n\tcpumask_var_t cpus;\n\tbool called = true;\n\tstruct kvm_vcpu *vcpu;\n\n\tzalloc_cpumask_var(&cpus, GFP_ATOMIC);\n\n\tme = get_cpu();\n\tkvm_for_each_vcpu(i, vcpu, kvm) {\n\t\tkvm_make_request(req, vcpu);\n\t\tcpu = vcpu->cpu;\n\n\t\t/* Set ->requests bit before we read ->mode */\n\t\tsmp_mb();\n\n\t\tif (cpus != NULL && cpu != -1 && cpu != me &&\n\t\t      kvm_vcpu_exiting_guest_mode(vcpu) != OUTSIDE_GUEST_MODE)\n\t\t\tcpumask_set_cpu(cpu, cpus);\n\t}\n\tif (unlikely(cpus == NULL))\n\t\tsmp_call_function_many(cpu_online_mask, ack_flush, NULL, 1);\n\telse if (!cpumask_empty(cpus))\n\t\tsmp_call_function_many(cpus, ack_flush, NULL, 1);\n\telse\n\t\tcalled = false;\n\tput_cpu();\n\tfree_cpumask_var(cpus);\n\treturn called;\n}\n\nvoid kvm_flush_remote_tlbs(struct kvm *kvm)\n{\n\tint dirty_count = kvm->tlbs_dirty;\n\n\tsmp_mb();\n\tif (make_all_cpus_request(kvm, KVM_REQ_TLB_FLUSH))\n\t\t++kvm->stat.remote_tlb_flush;\n\tcmpxchg(&kvm->tlbs_dirty, dirty_count, 0);\n}\n\nvoid kvm_reload_remote_mmus(struct kvm *kvm)\n{\n\tmake_all_cpus_request(kvm, KVM_REQ_MMU_RELOAD);\n}\n\nint kvm_vcpu_init(struct kvm_vcpu *vcpu, struct kvm *kvm, unsigned id)\n{\n\tstruct page *page;\n\tint r;\n\n\tmutex_init(&vcpu->mutex);\n\tvcpu->cpu = -1;\n\tvcpu->kvm = kvm;\n\tvcpu->vcpu_id = id;\n\tvcpu->pid = NULL;\n\tinit_waitqueue_head(&vcpu->wq);\n\tkvm_async_pf_vcpu_init(vcpu);\n\n\tpage = alloc_page(GFP_KERNEL | __GFP_ZERO);\n\tif (!page) {\n\t\tr = -ENOMEM;\n\t\tgoto fail;\n\t}\n\tvcpu->run = page_address(page);\n\n\tr = kvm_arch_vcpu_init(vcpu);\n\tif (r < 0)\n\t\tgoto fail_free_run;\n\treturn 0;\n\nfail_free_run:\n\tfree_page((unsigned long)vcpu->run);\nfail:\n\treturn r;\n}\nEXPORT_SYMBOL_GPL(kvm_vcpu_init);\n\nvoid kvm_vcpu_uninit(struct kvm_vcpu *vcpu)\n{\n\tput_pid(vcpu->pid);\n\tkvm_arch_vcpu_uninit(vcpu);\n\tfree_page((unsigned long)vcpu->run);\n}\nEXPORT_SYMBOL_GPL(kvm_vcpu_uninit);\n\n#if defined(CONFIG_MMU_NOTIFIER) && defined(KVM_ARCH_WANT_MMU_NOTIFIER)\nstatic inline struct kvm *mmu_notifier_to_kvm(struct mmu_notifier *mn)\n{\n\treturn container_of(mn, struct kvm, mmu_notifier);\n}\n\nstatic void kvm_mmu_notifier_invalidate_page(struct mmu_notifier *mn,\n\t\t\t\t\t     struct mm_struct *mm,\n\t\t\t\t\t     unsigned long address)\n{\n\tstruct kvm *kvm = mmu_notifier_to_kvm(mn);\n\tint need_tlb_flush, idx;\n\n\t/*\n\t * When ->invalidate_page runs, the linux pte has been zapped\n\t * already but the page is still allocated until\n\t * ->invalidate_page returns. So if we increase the sequence\n\t * here the kvm page fault will notice if the spte can't be\n\t * established because the page is going to be freed. If\n\t * instead the kvm page fault establishes the spte before\n\t * ->invalidate_page runs, kvm_unmap_hva will release it\n\t * before returning.\n\t *\n\t * The sequence increase only need to be seen at spin_unlock\n\t * time, and not at spin_lock time.\n\t *\n\t * Increasing the sequence after the spin_unlock would be\n\t * unsafe because the kvm page fault could then establish the\n\t * pte after kvm_unmap_hva returned, without noticing the page\n\t * is going to be freed.\n\t */\n\tidx = srcu_read_lock(&kvm->srcu);\n\tspin_lock(&kvm->mmu_lock);\n\tkvm->mmu_notifier_seq++;\n\tneed_tlb_flush = kvm_unmap_hva(kvm, address) | kvm->tlbs_dirty;\n\tspin_unlock(&kvm->mmu_lock);\n\tsrcu_read_unlock(&kvm->srcu, idx);\n\n\t/* we've to flush the tlb before the pages can be freed */\n\tif (need_tlb_flush)\n\t\tkvm_flush_remote_tlbs(kvm);\n\n}\n\nstatic void kvm_mmu_notifier_change_pte(struct mmu_notifier *mn,\n\t\t\t\t\tstruct mm_struct *mm,\n\t\t\t\t\tunsigned long address,\n\t\t\t\t\tpte_t pte)\n{\n\tstruct kvm *kvm = mmu_notifier_to_kvm(mn);\n\tint idx;\n\n\tidx = srcu_read_lock(&kvm->srcu);\n\tspin_lock(&kvm->mmu_lock);\n\tkvm->mmu_notifier_seq++;\n\tkvm_set_spte_hva(kvm, address, pte);\n\tspin_unlock(&kvm->mmu_lock);\n\tsrcu_read_unlock(&kvm->srcu, idx);\n}\n\nstatic void kvm_mmu_notifier_invalidate_range_start(struct mmu_notifier *mn,\n\t\t\t\t\t\t    struct mm_struct *mm,\n\t\t\t\t\t\t    unsigned long start,\n\t\t\t\t\t\t    unsigned long end)\n{\n\tstruct kvm *kvm = mmu_notifier_to_kvm(mn);\n\tint need_tlb_flush = 0, idx;\n\n\tidx = srcu_read_lock(&kvm->srcu);\n\tspin_lock(&kvm->mmu_lock);\n\t/*\n\t * The count increase must become visible at unlock time as no\n\t * spte can be established without taking the mmu_lock and\n\t * count is also read inside the mmu_lock critical section.\n\t */\n\tkvm->mmu_notifier_count++;\n\tfor (; start < end; start += PAGE_SIZE)\n\t\tneed_tlb_flush |= kvm_unmap_hva(kvm, start);\n\tneed_tlb_flush |= kvm->tlbs_dirty;\n\tspin_unlock(&kvm->mmu_lock);\n\tsrcu_read_unlock(&kvm->srcu, idx);\n\n\t/* we've to flush the tlb before the pages can be freed */\n\tif (need_tlb_flush)\n\t\tkvm_flush_remote_tlbs(kvm);\n}\n\nstatic void kvm_mmu_notifier_invalidate_range_end(struct mmu_notifier *mn,\n\t\t\t\t\t\t  struct mm_struct *mm,\n\t\t\t\t\t\t  unsigned long start,\n\t\t\t\t\t\t  unsigned long end)\n{\n\tstruct kvm *kvm = mmu_notifier_to_kvm(mn);\n\n\tspin_lock(&kvm->mmu_lock);\n\t/*\n\t * This sequence increase will notify the kvm page fault that\n\t * the page that is going to be mapped in the spte could have\n\t * been freed.\n\t */\n\tkvm->mmu_notifier_seq++;\n\t/*\n\t * The above sequence increase must be visible before the\n\t * below count decrease but both values are read by the kvm\n\t * page fault under mmu_lock spinlock so we don't need to add\n\t * a smb_wmb() here in between the two.\n\t */\n\tkvm->mmu_notifier_count--;\n\tspin_unlock(&kvm->mmu_lock);\n\n\tBUG_ON(kvm->mmu_notifier_count < 0);\n}\n\nstatic int kvm_mmu_notifier_clear_flush_young(struct mmu_notifier *mn,\n\t\t\t\t\t      struct mm_struct *mm,\n\t\t\t\t\t      unsigned long address)\n{\n\tstruct kvm *kvm = mmu_notifier_to_kvm(mn);\n\tint young, idx;\n\n\tidx = srcu_read_lock(&kvm->srcu);\n\tspin_lock(&kvm->mmu_lock);\n\tyoung = kvm_age_hva(kvm, address);\n\tspin_unlock(&kvm->mmu_lock);\n\tsrcu_read_unlock(&kvm->srcu, idx);\n\n\tif (young)\n\t\tkvm_flush_remote_tlbs(kvm);\n\n\treturn young;\n}\n\nstatic int kvm_mmu_notifier_test_young(struct mmu_notifier *mn,\n\t\t\t\t       struct mm_struct *mm,\n\t\t\t\t       unsigned long address)\n{\n\tstruct kvm *kvm = mmu_notifier_to_kvm(mn);\n\tint young, idx;\n\n\tidx = srcu_read_lock(&kvm->srcu);\n\tspin_lock(&kvm->mmu_lock);\n\tyoung = kvm_test_age_hva(kvm, address);\n\tspin_unlock(&kvm->mmu_lock);\n\tsrcu_read_unlock(&kvm->srcu, idx);\n\n\treturn young;\n}\n\nstatic void kvm_mmu_notifier_release(struct mmu_notifier *mn,\n\t\t\t\t     struct mm_struct *mm)\n{\n\tstruct kvm *kvm = mmu_notifier_to_kvm(mn);\n\tint idx;\n\n\tidx = srcu_read_lock(&kvm->srcu);\n\tkvm_arch_flush_shadow(kvm);\n\tsrcu_read_unlock(&kvm->srcu, idx);\n}\n\nstatic const struct mmu_notifier_ops kvm_mmu_notifier_ops = {\n\t.invalidate_page\t= kvm_mmu_notifier_invalidate_page,\n\t.invalidate_range_start\t= kvm_mmu_notifier_invalidate_range_start,\n\t.invalidate_range_end\t= kvm_mmu_notifier_invalidate_range_end,\n\t.clear_flush_young\t= kvm_mmu_notifier_clear_flush_young,\n\t.test_young\t\t= kvm_mmu_notifier_test_young,\n\t.change_pte\t\t= kvm_mmu_notifier_change_pte,\n\t.release\t\t= kvm_mmu_notifier_release,\n};\n\nstatic int kvm_init_mmu_notifier(struct kvm *kvm)\n{\n\tkvm->mmu_notifier.ops = &kvm_mmu_notifier_ops;\n\treturn mmu_notifier_register(&kvm->mmu_notifier, current->mm);\n}\n\n#else  /* !(CONFIG_MMU_NOTIFIER && KVM_ARCH_WANT_MMU_NOTIFIER) */\n\nstatic int kvm_init_mmu_notifier(struct kvm *kvm)\n{\n\treturn 0;\n}\n\n#endif /* CONFIG_MMU_NOTIFIER && KVM_ARCH_WANT_MMU_NOTIFIER */\n\nstatic struct kvm *kvm_create_vm(void)\n{\n\tint r, i;\n\tstruct kvm *kvm = kvm_arch_alloc_vm();\n\n\tif (!kvm)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tr = kvm_arch_init_vm(kvm);\n\tif (r)\n\t\tgoto out_err_nodisable;\n\n\tr = hardware_enable_all();\n\tif (r)\n\t\tgoto out_err_nodisable;\n\n#ifdef CONFIG_HAVE_KVM_IRQCHIP\n\tINIT_HLIST_HEAD(&kvm->mask_notifier_list);\n\tINIT_HLIST_HEAD(&kvm->irq_ack_notifier_list);\n#endif\n\n\tr = -ENOMEM;\n\tkvm->memslots = kzalloc(sizeof(struct kvm_memslots), GFP_KERNEL);\n\tif (!kvm->memslots)\n\t\tgoto out_err_nosrcu;\n\tif (init_srcu_struct(&kvm->srcu))\n\t\tgoto out_err_nosrcu;\n\tfor (i = 0; i < KVM_NR_BUSES; i++) {\n\t\tkvm->buses[i] = kzalloc(sizeof(struct kvm_io_bus),\n\t\t\t\t\tGFP_KERNEL);\n\t\tif (!kvm->buses[i])\n\t\t\tgoto out_err;\n\t}\n\n\tr = kvm_init_mmu_notifier(kvm);\n\tif (r)\n\t\tgoto out_err;\n\n\tkvm->mm = current->mm;\n\tatomic_inc(&kvm->mm->mm_count);\n\tspin_lock_init(&kvm->mmu_lock);\n\tkvm_eventfd_init(kvm);\n\tmutex_init(&kvm->lock);\n\tmutex_init(&kvm->irq_lock);\n\tmutex_init(&kvm->slots_lock);\n\tatomic_set(&kvm->users_count, 1);\n\traw_spin_lock(&kvm_lock);\n\tlist_add(&kvm->vm_list, &vm_list);\n\traw_spin_unlock(&kvm_lock);\n\n\treturn kvm;\n\nout_err:\n\tcleanup_srcu_struct(&kvm->srcu);\nout_err_nosrcu:\n\thardware_disable_all();\nout_err_nodisable:\n\tfor (i = 0; i < KVM_NR_BUSES; i++)\n\t\tkfree(kvm->buses[i]);\n\tkfree(kvm->memslots);\n\tkvm_arch_free_vm(kvm);\n\treturn ERR_PTR(r);\n}\n\nstatic void kvm_destroy_dirty_bitmap(struct kvm_memory_slot *memslot)\n{\n\tif (!memslot->dirty_bitmap)\n\t\treturn;\n\n\tif (2 * kvm_dirty_bitmap_bytes(memslot) > PAGE_SIZE)\n\t\tvfree(memslot->dirty_bitmap_head);\n\telse\n\t\tkfree(memslot->dirty_bitmap_head);\n\n\tmemslot->dirty_bitmap = NULL;\n\tmemslot->dirty_bitmap_head = NULL;\n}\n\n/*\n * Free any memory in @free but not in @dont.\n */\nstatic void kvm_free_physmem_slot(struct kvm_memory_slot *free,\n\t\t\t\t  struct kvm_memory_slot *dont)\n{\n\tint i;\n\n\tif (!dont || free->rmap != dont->rmap)\n\t\tvfree(free->rmap);\n\n\tif (!dont || free->dirty_bitmap != dont->dirty_bitmap)\n\t\tkvm_destroy_dirty_bitmap(free);\n\n\n\tfor (i = 0; i < KVM_NR_PAGE_SIZES - 1; ++i) {\n\t\tif (!dont || free->lpage_info[i] != dont->lpage_info[i]) {\n\t\t\tvfree(free->lpage_info[i]);\n\t\t\tfree->lpage_info[i] = NULL;\n\t\t}\n\t}\n\n\tfree->npages = 0;\n\tfree->rmap = NULL;\n}\n\nvoid kvm_free_physmem(struct kvm *kvm)\n{\n\tint i;\n\tstruct kvm_memslots *slots = kvm->memslots;\n\n\tfor (i = 0; i < slots->nmemslots; ++i)\n\t\tkvm_free_physmem_slot(&slots->memslots[i], NULL);\n\n\tkfree(kvm->memslots);\n}\n\nstatic void kvm_destroy_vm(struct kvm *kvm)\n{\n\tint i;\n\tstruct mm_struct *mm = kvm->mm;\n\n\tkvm_arch_sync_events(kvm);\n\traw_spin_lock(&kvm_lock);\n\tlist_del(&kvm->vm_list);\n\traw_spin_unlock(&kvm_lock);\n\tkvm_free_irq_routing(kvm);\n\tfor (i = 0; i < KVM_NR_BUSES; i++)\n\t\tkvm_io_bus_destroy(kvm->buses[i]);\n\tkvm_coalesced_mmio_free(kvm);\n#if defined(CONFIG_MMU_NOTIFIER) && defined(KVM_ARCH_WANT_MMU_NOTIFIER)\n\tmmu_notifier_unregister(&kvm->mmu_notifier, kvm->mm);\n#else\n\tkvm_arch_flush_shadow(kvm);\n#endif\n\tkvm_arch_destroy_vm(kvm);\n\tkvm_free_physmem(kvm);\n\tcleanup_srcu_struct(&kvm->srcu);\n\tkvm_arch_free_vm(kvm);\n\thardware_disable_all();\n\tmmdrop(mm);\n}\n\nvoid kvm_get_kvm(struct kvm *kvm)\n{\n\tatomic_inc(&kvm->users_count);\n}\nEXPORT_SYMBOL_GPL(kvm_get_kvm);\n\nvoid kvm_put_kvm(struct kvm *kvm)\n{\n\tif (atomic_dec_and_test(&kvm->users_count))\n\t\tkvm_destroy_vm(kvm);\n}\nEXPORT_SYMBOL_GPL(kvm_put_kvm);\n\n\nstatic int kvm_vm_release(struct inode *inode, struct file *filp)\n{\n\tstruct kvm *kvm = filp->private_data;\n\n\tkvm_irqfd_release(kvm);\n\n\tkvm_put_kvm(kvm);\n\treturn 0;\n}\n\n#ifndef CONFIG_S390\n/*\n * Allocation size is twice as large as the actual dirty bitmap size.\n * This makes it possible to do double buffering: see x86's\n * kvm_vm_ioctl_get_dirty_log().\n */\nstatic int kvm_create_dirty_bitmap(struct kvm_memory_slot *memslot)\n{\n\tunsigned long dirty_bytes = 2 * kvm_dirty_bitmap_bytes(memslot);\n\n\tif (dirty_bytes > PAGE_SIZE)\n\t\tmemslot->dirty_bitmap = vzalloc(dirty_bytes);\n\telse\n\t\tmemslot->dirty_bitmap = kzalloc(dirty_bytes, GFP_KERNEL);\n\n\tif (!memslot->dirty_bitmap)\n\t\treturn -ENOMEM;\n\n\tmemslot->dirty_bitmap_head = memslot->dirty_bitmap;\n\treturn 0;\n}\n#endif /* !CONFIG_S390 */\n\n/*\n * Allocate some memory and give it an address in the guest physical address\n * space.\n *\n * Discontiguous memory is allowed, mostly for framebuffers.\n *\n * Must be called holding mmap_sem for write.\n */\nint __kvm_set_memory_region(struct kvm *kvm,\n\t\t\t    struct kvm_userspace_memory_region *mem,\n\t\t\t    int user_alloc)\n{\n\tint r;\n\tgfn_t base_gfn;\n\tunsigned long npages;\n\tunsigned long i;\n\tstruct kvm_memory_slot *memslot;\n\tstruct kvm_memory_slot old, new;\n\tstruct kvm_memslots *slots, *old_memslots;\n\n\tr = -EINVAL;\n\t/* General sanity checks */\n\tif (mem->memory_size & (PAGE_SIZE - 1))\n\t\tgoto out;\n\tif (mem->guest_phys_addr & (PAGE_SIZE - 1))\n\t\tgoto out;\n\tif (user_alloc && (mem->userspace_addr & (PAGE_SIZE - 1)))\n\t\tgoto out;\n\tif (mem->slot >= KVM_MEMORY_SLOTS + KVM_PRIVATE_MEM_SLOTS)\n\t\tgoto out;\n\tif (mem->guest_phys_addr + mem->memory_size < mem->guest_phys_addr)\n\t\tgoto out;\n\n\tmemslot = &kvm->memslots->memslots[mem->slot];\n\tbase_gfn = mem->guest_phys_addr >> PAGE_SHIFT;\n\tnpages = mem->memory_size >> PAGE_SHIFT;\n\n\tr = -EINVAL;\n\tif (npages > KVM_MEM_MAX_NR_PAGES)\n\t\tgoto out;\n\n\tif (!npages)\n\t\tmem->flags &= ~KVM_MEM_LOG_DIRTY_PAGES;\n\n\tnew = old = *memslot;\n\n\tnew.id = mem->slot;\n\tnew.base_gfn = base_gfn;\n\tnew.npages = npages;\n\tnew.flags = mem->flags;\n\n\t/* Disallow changing a memory slot's size. */\n\tr = -EINVAL;\n\tif (npages && old.npages && npages != old.npages)\n\t\tgoto out_free;\n\n\t/* Check for overlaps */\n\tr = -EEXIST;\n\tfor (i = 0; i < KVM_MEMORY_SLOTS; ++i) {\n\t\tstruct kvm_memory_slot *s = &kvm->memslots->memslots[i];\n\n\t\tif (s == memslot || !s->npages)\n\t\t\tcontinue;\n\t\tif (!((base_gfn + npages <= s->base_gfn) ||\n\t\t      (base_gfn >= s->base_gfn + s->npages)))\n\t\t\tgoto out_free;\n\t}\n\n\t/* Free page dirty bitmap if unneeded */\n\tif (!(new.flags & KVM_MEM_LOG_DIRTY_PAGES))\n\t\tnew.dirty_bitmap = NULL;\n\n\tr = -ENOMEM;\n\n\t/* Allocate if a slot is being created */\n#ifndef CONFIG_S390\n\tif (npages && !new.rmap) {\n\t\tnew.rmap = vzalloc(npages * sizeof(*new.rmap));\n\n\t\tif (!new.rmap)\n\t\t\tgoto out_free;\n\n\t\tnew.user_alloc = user_alloc;\n\t\tnew.userspace_addr = mem->userspace_addr;\n\t}\n\tif (!npages)\n\t\tgoto skip_lpage;\n\n\tfor (i = 0; i < KVM_NR_PAGE_SIZES - 1; ++i) {\n\t\tunsigned long ugfn;\n\t\tunsigned long j;\n\t\tint lpages;\n\t\tint level = i + 2;\n\n\t\t/* Avoid unused variable warning if no large pages */\n\t\t(void)level;\n\n\t\tif (new.lpage_info[i])\n\t\t\tcontinue;\n\n\t\tlpages = 1 + ((base_gfn + npages - 1)\n\t\t\t     >> KVM_HPAGE_GFN_SHIFT(level));\n\t\tlpages -= base_gfn >> KVM_HPAGE_GFN_SHIFT(level);\n\n\t\tnew.lpage_info[i] = vzalloc(lpages * sizeof(*new.lpage_info[i]));\n\n\t\tif (!new.lpage_info[i])\n\t\t\tgoto out_free;\n\n\t\tif (base_gfn & (KVM_PAGES_PER_HPAGE(level) - 1))\n\t\t\tnew.lpage_info[i][0].write_count = 1;\n\t\tif ((base_gfn+npages) & (KVM_PAGES_PER_HPAGE(level) - 1))\n\t\t\tnew.lpage_info[i][lpages - 1].write_count = 1;\n\t\tugfn = new.userspace_addr >> PAGE_SHIFT;\n\t\t/*\n\t\t * If the gfn and userspace address are not aligned wrt each\n\t\t * other, or if explicitly asked to, disable large page\n\t\t * support for this slot\n\t\t */\n\t\tif ((base_gfn ^ ugfn) & (KVM_PAGES_PER_HPAGE(level) - 1) ||\n\t\t    !largepages_enabled)\n\t\t\tfor (j = 0; j < lpages; ++j)\n\t\t\t\tnew.lpage_info[i][j].write_count = 1;\n\t}\n\nskip_lpage:\n\n\t/* Allocate page dirty bitmap if needed */\n\tif ((new.flags & KVM_MEM_LOG_DIRTY_PAGES) && !new.dirty_bitmap) {\n\t\tif (kvm_create_dirty_bitmap(&new) < 0)\n\t\t\tgoto out_free;\n\t\t/* destroy any largepage mappings for dirty tracking */\n\t}\n#else  /* not defined CONFIG_S390 */\n\tnew.user_alloc = user_alloc;\n\tif (user_alloc)\n\t\tnew.userspace_addr = mem->userspace_addr;\n#endif /* not defined CONFIG_S390 */\n\n\tif (!npages) {\n\t\tr = -ENOMEM;\n\t\tslots = kzalloc(sizeof(struct kvm_memslots), GFP_KERNEL);\n\t\tif (!slots)\n\t\t\tgoto out_free;\n\t\tmemcpy(slots, kvm->memslots, sizeof(struct kvm_memslots));\n\t\tif (mem->slot >= slots->nmemslots)\n\t\t\tslots->nmemslots = mem->slot + 1;\n\t\tslots->generation++;\n\t\tslots->memslots[mem->slot].flags |= KVM_MEMSLOT_INVALID;\n\n\t\told_memslots = kvm->memslots;\n\t\trcu_assign_pointer(kvm->memslots, slots);\n\t\tsynchronize_srcu_expedited(&kvm->srcu);\n\t\t/* From this point no new shadow pages pointing to a deleted\n\t\t * memslot will be created.\n\t\t *\n\t\t * validation of sp->gfn happens in:\n\t\t * \t- gfn_to_hva (kvm_read_guest, gfn_to_pfn)\n\t\t * \t- kvm_is_visible_gfn (mmu_check_roots)\n\t\t */\n\t\tkvm_arch_flush_shadow(kvm);\n\t\tkfree(old_memslots);\n\t}\n\n\tr = kvm_arch_prepare_memory_region(kvm, &new, old, mem, user_alloc);\n\tif (r)\n\t\tgoto out_free;\n\n\t/* map the pages in iommu page table */\n\tif (npages) {\n\t\tr = kvm_iommu_map_pages(kvm, &new);\n\t\tif (r)\n\t\t\tgoto out_free;\n\t}\n\n\tr = -ENOMEM;\n\tslots = kzalloc(sizeof(struct kvm_memslots), GFP_KERNEL);\n\tif (!slots)\n\t\tgoto out_free;\n\tmemcpy(slots, kvm->memslots, sizeof(struct kvm_memslots));\n\tif (mem->slot >= slots->nmemslots)\n\t\tslots->nmemslots = mem->slot + 1;\n\tslots->generation++;\n\n\t/* actual memory is freed via old in kvm_free_physmem_slot below */\n\tif (!npages) {\n\t\tnew.rmap = NULL;\n\t\tnew.dirty_bitmap = NULL;\n\t\tfor (i = 0; i < KVM_NR_PAGE_SIZES - 1; ++i)\n\t\t\tnew.lpage_info[i] = NULL;\n\t}\n\n\tslots->memslots[mem->slot] = new;\n\told_memslots = kvm->memslots;\n\trcu_assign_pointer(kvm->memslots, slots);\n\tsynchronize_srcu_expedited(&kvm->srcu);\n\n\tkvm_arch_commit_memory_region(kvm, mem, old, user_alloc);\n\n\tkvm_free_physmem_slot(&old, &new);\n\tkfree(old_memslots);\n\n\treturn 0;\n\nout_free:\n\tkvm_free_physmem_slot(&new, &old);\nout:\n\treturn r;\n\n}\nEXPORT_SYMBOL_GPL(__kvm_set_memory_region);\n\nint kvm_set_memory_region(struct kvm *kvm,\n\t\t\t  struct kvm_userspace_memory_region *mem,\n\t\t\t  int user_alloc)\n{\n\tint r;\n\n\tmutex_lock(&kvm->slots_lock);\n\tr = __kvm_set_memory_region(kvm, mem, user_alloc);\n\tmutex_unlock(&kvm->slots_lock);\n\treturn r;\n}\nEXPORT_SYMBOL_GPL(kvm_set_memory_region);\n\nint kvm_vm_ioctl_set_memory_region(struct kvm *kvm,\n\t\t\t\t   struct\n\t\t\t\t   kvm_userspace_memory_region *mem,\n\t\t\t\t   int user_alloc)\n{\n\tif (mem->slot >= KVM_MEMORY_SLOTS)\n\t\treturn -EINVAL;\n\treturn kvm_set_memory_region(kvm, mem, user_alloc);\n}\n\nint kvm_get_dirty_log(struct kvm *kvm,\n\t\t\tstruct kvm_dirty_log *log, int *is_dirty)\n{\n\tstruct kvm_memory_slot *memslot;\n\tint r, i;\n\tunsigned long n;\n\tunsigned long any = 0;\n\n\tr = -EINVAL;\n\tif (log->slot >= KVM_MEMORY_SLOTS)\n\t\tgoto out;\n\n\tmemslot = &kvm->memslots->memslots[log->slot];\n\tr = -ENOENT;\n\tif (!memslot->dirty_bitmap)\n\t\tgoto out;\n\n\tn = kvm_dirty_bitmap_bytes(memslot);\n\n\tfor (i = 0; !any && i < n/sizeof(long); ++i)\n\t\tany = memslot->dirty_bitmap[i];\n\n\tr = -EFAULT;\n\tif (copy_to_user(log->dirty_bitmap, memslot->dirty_bitmap, n))\n\t\tgoto out;\n\n\tif (any)\n\t\t*is_dirty = 1;\n\n\tr = 0;\nout:\n\treturn r;\n}\n\nvoid kvm_disable_largepages(void)\n{\n\tlargepages_enabled = false;\n}\nEXPORT_SYMBOL_GPL(kvm_disable_largepages);\n\nint is_error_page(struct page *page)\n{\n\treturn page == bad_page || page == hwpoison_page || page == fault_page;\n}\nEXPORT_SYMBOL_GPL(is_error_page);\n\nint is_error_pfn(pfn_t pfn)\n{\n\treturn pfn == bad_pfn || pfn == hwpoison_pfn || pfn == fault_pfn;\n}\nEXPORT_SYMBOL_GPL(is_error_pfn);\n\nint is_hwpoison_pfn(pfn_t pfn)\n{\n\treturn pfn == hwpoison_pfn;\n}\nEXPORT_SYMBOL_GPL(is_hwpoison_pfn);\n\nint is_fault_pfn(pfn_t pfn)\n{\n\treturn pfn == fault_pfn;\n}\nEXPORT_SYMBOL_GPL(is_fault_pfn);\n\nstatic inline unsigned long bad_hva(void)\n{\n\treturn PAGE_OFFSET;\n}\n\nint kvm_is_error_hva(unsigned long addr)\n{\n\treturn addr == bad_hva();\n}\nEXPORT_SYMBOL_GPL(kvm_is_error_hva);\n\nstatic struct kvm_memory_slot *__gfn_to_memslot(struct kvm_memslots *slots,\n\t\t\t\t\t\tgfn_t gfn)\n{\n\tint i;\n\n\tfor (i = 0; i < slots->nmemslots; ++i) {\n\t\tstruct kvm_memory_slot *memslot = &slots->memslots[i];\n\n\t\tif (gfn >= memslot->base_gfn\n\t\t    && gfn < memslot->base_gfn + memslot->npages)\n\t\t\treturn memslot;\n\t}\n\treturn NULL;\n}\n\nstruct kvm_memory_slot *gfn_to_memslot(struct kvm *kvm, gfn_t gfn)\n{\n\treturn __gfn_to_memslot(kvm_memslots(kvm), gfn);\n}\nEXPORT_SYMBOL_GPL(gfn_to_memslot);\n\nint kvm_is_visible_gfn(struct kvm *kvm, gfn_t gfn)\n{\n\tint i;\n\tstruct kvm_memslots *slots = kvm_memslots(kvm);\n\n\tfor (i = 0; i < KVM_MEMORY_SLOTS; ++i) {\n\t\tstruct kvm_memory_slot *memslot = &slots->memslots[i];\n\n\t\tif (memslot->flags & KVM_MEMSLOT_INVALID)\n\t\t\tcontinue;\n\n\t\tif (gfn >= memslot->base_gfn\n\t\t    && gfn < memslot->base_gfn + memslot->npages)\n\t\t\treturn 1;\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(kvm_is_visible_gfn);\n\nunsigned long kvm_host_page_size(struct kvm *kvm, gfn_t gfn)\n{\n\tstruct vm_area_struct *vma;\n\tunsigned long addr, size;\n\n\tsize = PAGE_SIZE;\n\n\taddr = gfn_to_hva(kvm, gfn);\n\tif (kvm_is_error_hva(addr))\n\t\treturn PAGE_SIZE;\n\n\tdown_read(&current->mm->mmap_sem);\n\tvma = find_vma(current->mm, addr);\n\tif (!vma)\n\t\tgoto out;\n\n\tsize = vma_kernel_pagesize(vma);\n\nout:\n\tup_read(&current->mm->mmap_sem);\n\n\treturn size;\n}\n\nstatic unsigned long gfn_to_hva_many(struct kvm_memory_slot *slot, gfn_t gfn,\n\t\t\t\t     gfn_t *nr_pages)\n{\n\tif (!slot || slot->flags & KVM_MEMSLOT_INVALID)\n\t\treturn bad_hva();\n\n\tif (nr_pages)\n\t\t*nr_pages = slot->npages - (gfn - slot->base_gfn);\n\n\treturn gfn_to_hva_memslot(slot, gfn);\n}\n\nunsigned long gfn_to_hva(struct kvm *kvm, gfn_t gfn)\n{\n\treturn gfn_to_hva_many(gfn_to_memslot(kvm, gfn), gfn, NULL);\n}\nEXPORT_SYMBOL_GPL(gfn_to_hva);\n\nstatic pfn_t get_fault_pfn(void)\n{\n\tget_page(fault_page);\n\treturn fault_pfn;\n}\n\nint get_user_page_nowait(struct task_struct *tsk, struct mm_struct *mm,\n\tunsigned long start, int write, struct page **page)\n{\n\tint flags = FOLL_TOUCH | FOLL_NOWAIT | FOLL_HWPOISON | FOLL_GET;\n\n\tif (write)\n\t\tflags |= FOLL_WRITE;\n\n\treturn __get_user_pages(tsk, mm, start, 1, flags, page, NULL, NULL);\n}\n\nstatic inline int check_user_page_hwpoison(unsigned long addr)\n{\n\tint rc, flags = FOLL_TOUCH | FOLL_HWPOISON | FOLL_WRITE;\n\n\trc = __get_user_pages(current, current->mm, addr, 1,\n\t\t\t      flags, NULL, NULL, NULL);\n\treturn rc == -EHWPOISON;\n}\n\nstatic pfn_t hva_to_pfn(struct kvm *kvm, unsigned long addr, bool atomic,\n\t\t\tbool *async, bool write_fault, bool *writable)\n{\n\tstruct page *page[1];\n\tint npages = 0;\n\tpfn_t pfn;\n\n\t/* we can do it either atomically or asynchronously, not both */\n\tBUG_ON(atomic && async);\n\n\tBUG_ON(!write_fault && !writable);\n\n\tif (writable)\n\t\t*writable = true;\n\n\tif (atomic || async)\n\t\tnpages = __get_user_pages_fast(addr, 1, 1, page);\n\n\tif (unlikely(npages != 1) && !atomic) {\n\t\tmight_sleep();\n\n\t\tif (writable)\n\t\t\t*writable = write_fault;\n\n\t\tif (async) {\n\t\t\tdown_read(&current->mm->mmap_sem);\n\t\t\tnpages = get_user_page_nowait(current, current->mm,\n\t\t\t\t\t\t     addr, write_fault, page);\n\t\t\tup_read(&current->mm->mmap_sem);\n\t\t} else\n\t\t\tnpages = get_user_pages_fast(addr, 1, write_fault,\n\t\t\t\t\t\t     page);\n\n\t\t/* map read fault as writable if possible */\n\t\tif (unlikely(!write_fault) && npages == 1) {\n\t\t\tstruct page *wpage[1];\n\n\t\t\tnpages = __get_user_pages_fast(addr, 1, 1, wpage);\n\t\t\tif (npages == 1) {\n\t\t\t\t*writable = true;\n\t\t\t\tput_page(page[0]);\n\t\t\t\tpage[0] = wpage[0];\n\t\t\t}\n\t\t\tnpages = 1;\n\t\t}\n\t}\n\n\tif (unlikely(npages != 1)) {\n\t\tstruct vm_area_struct *vma;\n\n\t\tif (atomic)\n\t\t\treturn get_fault_pfn();\n\n\t\tdown_read(&current->mm->mmap_sem);\n\t\tif (npages == -EHWPOISON ||\n\t\t\t(!async && check_user_page_hwpoison(addr))) {\n\t\t\tup_read(&current->mm->mmap_sem);\n\t\t\tget_page(hwpoison_page);\n\t\t\treturn page_to_pfn(hwpoison_page);\n\t\t}\n\n\t\tvma = find_vma_intersection(current->mm, addr, addr+1);\n\n\t\tif (vma == NULL)\n\t\t\tpfn = get_fault_pfn();\n\t\telse if ((vma->vm_flags & VM_PFNMAP)) {\n\t\t\tpfn = ((addr - vma->vm_start) >> PAGE_SHIFT) +\n\t\t\t\tvma->vm_pgoff;\n\t\t\tBUG_ON(!kvm_is_mmio_pfn(pfn));\n\t\t} else {\n\t\t\tif (async && (vma->vm_flags & VM_WRITE))\n\t\t\t\t*async = true;\n\t\t\tpfn = get_fault_pfn();\n\t\t}\n\t\tup_read(&current->mm->mmap_sem);\n\t} else\n\t\tpfn = page_to_pfn(page[0]);\n\n\treturn pfn;\n}\n\npfn_t hva_to_pfn_atomic(struct kvm *kvm, unsigned long addr)\n{\n\treturn hva_to_pfn(kvm, addr, true, NULL, true, NULL);\n}\nEXPORT_SYMBOL_GPL(hva_to_pfn_atomic);\n\nstatic pfn_t __gfn_to_pfn(struct kvm *kvm, gfn_t gfn, bool atomic, bool *async,\n\t\t\t  bool write_fault, bool *writable)\n{\n\tunsigned long addr;\n\n\tif (async)\n\t\t*async = false;\n\n\taddr = gfn_to_hva(kvm, gfn);\n\tif (kvm_is_error_hva(addr)) {\n\t\tget_page(bad_page);\n\t\treturn page_to_pfn(bad_page);\n\t}\n\n\treturn hva_to_pfn(kvm, addr, atomic, async, write_fault, writable);\n}\n\npfn_t gfn_to_pfn_atomic(struct kvm *kvm, gfn_t gfn)\n{\n\treturn __gfn_to_pfn(kvm, gfn, true, NULL, true, NULL);\n}\nEXPORT_SYMBOL_GPL(gfn_to_pfn_atomic);\n\npfn_t gfn_to_pfn_async(struct kvm *kvm, gfn_t gfn, bool *async,\n\t\t       bool write_fault, bool *writable)\n{\n\treturn __gfn_to_pfn(kvm, gfn, false, async, write_fault, writable);\n}\nEXPORT_SYMBOL_GPL(gfn_to_pfn_async);\n\npfn_t gfn_to_pfn(struct kvm *kvm, gfn_t gfn)\n{\n\treturn __gfn_to_pfn(kvm, gfn, false, NULL, true, NULL);\n}\nEXPORT_SYMBOL_GPL(gfn_to_pfn);\n\npfn_t gfn_to_pfn_prot(struct kvm *kvm, gfn_t gfn, bool write_fault,\n\t\t      bool *writable)\n{\n\treturn __gfn_to_pfn(kvm, gfn, false, NULL, write_fault, writable);\n}\nEXPORT_SYMBOL_GPL(gfn_to_pfn_prot);\n\npfn_t gfn_to_pfn_memslot(struct kvm *kvm,\n\t\t\t struct kvm_memory_slot *slot, gfn_t gfn)\n{\n\tunsigned long addr = gfn_to_hva_memslot(slot, gfn);\n\treturn hva_to_pfn(kvm, addr, false, NULL, true, NULL);\n}\n\nint gfn_to_page_many_atomic(struct kvm *kvm, gfn_t gfn, struct page **pages,\n\t\t\t\t\t\t\t\t  int nr_pages)\n{\n\tunsigned long addr;\n\tgfn_t entry;\n\n\taddr = gfn_to_hva_many(gfn_to_memslot(kvm, gfn), gfn, &entry);\n\tif (kvm_is_error_hva(addr))\n\t\treturn -1;\n\n\tif (entry < nr_pages)\n\t\treturn 0;\n\n\treturn __get_user_pages_fast(addr, nr_pages, 1, pages);\n}\nEXPORT_SYMBOL_GPL(gfn_to_page_many_atomic);\n\nstruct page *gfn_to_page(struct kvm *kvm, gfn_t gfn)\n{\n\tpfn_t pfn;\n\n\tpfn = gfn_to_pfn(kvm, gfn);\n\tif (!kvm_is_mmio_pfn(pfn))\n\t\treturn pfn_to_page(pfn);\n\n\tWARN_ON(kvm_is_mmio_pfn(pfn));\n\n\tget_page(bad_page);\n\treturn bad_page;\n}\n\nEXPORT_SYMBOL_GPL(gfn_to_page);\n\nvoid kvm_release_page_clean(struct page *page)\n{\n\tkvm_release_pfn_clean(page_to_pfn(page));\n}\nEXPORT_SYMBOL_GPL(kvm_release_page_clean);\n\nvoid kvm_release_pfn_clean(pfn_t pfn)\n{\n\tif (!kvm_is_mmio_pfn(pfn))\n\t\tput_page(pfn_to_page(pfn));\n}\nEXPORT_SYMBOL_GPL(kvm_release_pfn_clean);\n\nvoid kvm_release_page_dirty(struct page *page)\n{\n\tkvm_release_pfn_dirty(page_to_pfn(page));\n}\nEXPORT_SYMBOL_GPL(kvm_release_page_dirty);\n\nvoid kvm_release_pfn_dirty(pfn_t pfn)\n{\n\tkvm_set_pfn_dirty(pfn);\n\tkvm_release_pfn_clean(pfn);\n}\nEXPORT_SYMBOL_GPL(kvm_release_pfn_dirty);\n\nvoid kvm_set_page_dirty(struct page *page)\n{\n\tkvm_set_pfn_dirty(page_to_pfn(page));\n}\nEXPORT_SYMBOL_GPL(kvm_set_page_dirty);\n\nvoid kvm_set_pfn_dirty(pfn_t pfn)\n{\n\tif (!kvm_is_mmio_pfn(pfn)) {\n\t\tstruct page *page = pfn_to_page(pfn);\n\t\tif (!PageReserved(page))\n\t\t\tSetPageDirty(page);\n\t}\n}\nEXPORT_SYMBOL_GPL(kvm_set_pfn_dirty);\n\nvoid kvm_set_pfn_accessed(pfn_t pfn)\n{\n\tif (!kvm_is_mmio_pfn(pfn))\n\t\tmark_page_accessed(pfn_to_page(pfn));\n}\nEXPORT_SYMBOL_GPL(kvm_set_pfn_accessed);\n\nvoid kvm_get_pfn(pfn_t pfn)\n{\n\tif (!kvm_is_mmio_pfn(pfn))\n\t\tget_page(pfn_to_page(pfn));\n}\nEXPORT_SYMBOL_GPL(kvm_get_pfn);\n\nstatic int next_segment(unsigned long len, int offset)\n{\n\tif (len > PAGE_SIZE - offset)\n\t\treturn PAGE_SIZE - offset;\n\telse\n\t\treturn len;\n}\n\nint kvm_read_guest_page(struct kvm *kvm, gfn_t gfn, void *data, int offset,\n\t\t\tint len)\n{\n\tint r;\n\tunsigned long addr;\n\n\taddr = gfn_to_hva(kvm, gfn);\n\tif (kvm_is_error_hva(addr))\n\t\treturn -EFAULT;\n\tr = copy_from_user(data, (void __user *)addr + offset, len);\n\tif (r)\n\t\treturn -EFAULT;\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(kvm_read_guest_page);\n\nint kvm_read_guest(struct kvm *kvm, gpa_t gpa, void *data, unsigned long len)\n{\n\tgfn_t gfn = gpa >> PAGE_SHIFT;\n\tint seg;\n\tint offset = offset_in_page(gpa);\n\tint ret;\n\n\twhile ((seg = next_segment(len, offset)) != 0) {\n\t\tret = kvm_read_guest_page(kvm, gfn, data, offset, seg);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t\toffset = 0;\n\t\tlen -= seg;\n\t\tdata += seg;\n\t\t++gfn;\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(kvm_read_guest);\n\nint kvm_read_guest_atomic(struct kvm *kvm, gpa_t gpa, void *data,\n\t\t\t  unsigned long len)\n{\n\tint r;\n\tunsigned long addr;\n\tgfn_t gfn = gpa >> PAGE_SHIFT;\n\tint offset = offset_in_page(gpa);\n\n\taddr = gfn_to_hva(kvm, gfn);\n\tif (kvm_is_error_hva(addr))\n\t\treturn -EFAULT;\n\tpagefault_disable();\n\tr = __copy_from_user_inatomic(data, (void __user *)addr + offset, len);\n\tpagefault_enable();\n\tif (r)\n\t\treturn -EFAULT;\n\treturn 0;\n}\nEXPORT_SYMBOL(kvm_read_guest_atomic);\n\nint kvm_write_guest_page(struct kvm *kvm, gfn_t gfn, const void *data,\n\t\t\t int offset, int len)\n{\n\tint r;\n\tunsigned long addr;\n\n\taddr = gfn_to_hva(kvm, gfn);\n\tif (kvm_is_error_hva(addr))\n\t\treturn -EFAULT;\n\tr = copy_to_user((void __user *)addr + offset, data, len);\n\tif (r)\n\t\treturn -EFAULT;\n\tmark_page_dirty(kvm, gfn);\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(kvm_write_guest_page);\n\nint kvm_write_guest(struct kvm *kvm, gpa_t gpa, const void *data,\n\t\t    unsigned long len)\n{\n\tgfn_t gfn = gpa >> PAGE_SHIFT;\n\tint seg;\n\tint offset = offset_in_page(gpa);\n\tint ret;\n\n\twhile ((seg = next_segment(len, offset)) != 0) {\n\t\tret = kvm_write_guest_page(kvm, gfn, data, offset, seg);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t\toffset = 0;\n\t\tlen -= seg;\n\t\tdata += seg;\n\t\t++gfn;\n\t}\n\treturn 0;\n}\n\nint kvm_gfn_to_hva_cache_init(struct kvm *kvm, struct gfn_to_hva_cache *ghc,\n\t\t\t      gpa_t gpa)\n{\n\tstruct kvm_memslots *slots = kvm_memslots(kvm);\n\tint offset = offset_in_page(gpa);\n\tgfn_t gfn = gpa >> PAGE_SHIFT;\n\n\tghc->gpa = gpa;\n\tghc->generation = slots->generation;\n\tghc->memslot = __gfn_to_memslot(slots, gfn);\n\tghc->hva = gfn_to_hva_many(ghc->memslot, gfn, NULL);\n\tif (!kvm_is_error_hva(ghc->hva))\n\t\tghc->hva += offset;\n\telse\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(kvm_gfn_to_hva_cache_init);\n\nint kvm_write_guest_cached(struct kvm *kvm, struct gfn_to_hva_cache *ghc,\n\t\t\t   void *data, unsigned long len)\n{\n\tstruct kvm_memslots *slots = kvm_memslots(kvm);\n\tint r;\n\n\tif (slots->generation != ghc->generation)\n\t\tkvm_gfn_to_hva_cache_init(kvm, ghc, ghc->gpa);\n\n\tif (kvm_is_error_hva(ghc->hva))\n\t\treturn -EFAULT;\n\n\tr = copy_to_user((void __user *)ghc->hva, data, len);\n\tif (r)\n\t\treturn -EFAULT;\n\tmark_page_dirty_in_slot(kvm, ghc->memslot, ghc->gpa >> PAGE_SHIFT);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(kvm_write_guest_cached);\n\nint kvm_clear_guest_page(struct kvm *kvm, gfn_t gfn, int offset, int len)\n{\n\treturn kvm_write_guest_page(kvm, gfn, (const void *) empty_zero_page,\n\t\t\t\t    offset, len);\n}\nEXPORT_SYMBOL_GPL(kvm_clear_guest_page);\n\nint kvm_clear_guest(struct kvm *kvm, gpa_t gpa, unsigned long len)\n{\n\tgfn_t gfn = gpa >> PAGE_SHIFT;\n\tint seg;\n\tint offset = offset_in_page(gpa);\n\tint ret;\n\n        while ((seg = next_segment(len, offset)) != 0) {\n\t\tret = kvm_clear_guest_page(kvm, gfn, offset, seg);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t\toffset = 0;\n\t\tlen -= seg;\n\t\t++gfn;\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(kvm_clear_guest);\n\nvoid mark_page_dirty_in_slot(struct kvm *kvm, struct kvm_memory_slot *memslot,\n\t\t\t     gfn_t gfn)\n{\n\tif (memslot && memslot->dirty_bitmap) {\n\t\tunsigned long rel_gfn = gfn - memslot->base_gfn;\n\n\t\t__set_bit_le(rel_gfn, memslot->dirty_bitmap);\n\t}\n}\n\nvoid mark_page_dirty(struct kvm *kvm, gfn_t gfn)\n{\n\tstruct kvm_memory_slot *memslot;\n\n\tmemslot = gfn_to_memslot(kvm, gfn);\n\tmark_page_dirty_in_slot(kvm, memslot, gfn);\n}\n\n/*\n * The vCPU has executed a HLT instruction with in-kernel mode enabled.\n */\nvoid kvm_vcpu_block(struct kvm_vcpu *vcpu)\n{\n\tDEFINE_WAIT(wait);\n\n\tfor (;;) {\n\t\tprepare_to_wait(&vcpu->wq, &wait, TASK_INTERRUPTIBLE);\n\n\t\tif (kvm_arch_vcpu_runnable(vcpu)) {\n\t\t\tkvm_make_request(KVM_REQ_UNHALT, vcpu);\n\t\t\tbreak;\n\t\t}\n\t\tif (kvm_cpu_has_pending_timer(vcpu))\n\t\t\tbreak;\n\t\tif (signal_pending(current))\n\t\t\tbreak;\n\n\t\tschedule();\n\t}\n\n\tfinish_wait(&vcpu->wq, &wait);\n}\n\nvoid kvm_resched(struct kvm_vcpu *vcpu)\n{\n\tif (!need_resched())\n\t\treturn;\n\tcond_resched();\n}\nEXPORT_SYMBOL_GPL(kvm_resched);\n\nvoid kvm_vcpu_on_spin(struct kvm_vcpu *me)\n{\n\tstruct kvm *kvm = me->kvm;\n\tstruct kvm_vcpu *vcpu;\n\tint last_boosted_vcpu = me->kvm->last_boosted_vcpu;\n\tint yielded = 0;\n\tint pass;\n\tint i;\n\n\t/*\n\t * We boost the priority of a VCPU that is runnable but not\n\t * currently running, because it got preempted by something\n\t * else and called schedule in __vcpu_run.  Hopefully that\n\t * VCPU is holding the lock that we need and will release it.\n\t * We approximate round-robin by starting at the last boosted VCPU.\n\t */\n\tfor (pass = 0; pass < 2 && !yielded; pass++) {\n\t\tkvm_for_each_vcpu(i, vcpu, kvm) {\n\t\t\tstruct task_struct *task = NULL;\n\t\t\tstruct pid *pid;\n\t\t\tif (!pass && i < last_boosted_vcpu) {\n\t\t\t\ti = last_boosted_vcpu;\n\t\t\t\tcontinue;\n\t\t\t} else if (pass && i > last_boosted_vcpu)\n\t\t\t\tbreak;\n\t\t\tif (vcpu == me)\n\t\t\t\tcontinue;\n\t\t\tif (waitqueue_active(&vcpu->wq))\n\t\t\t\tcontinue;\n\t\t\trcu_read_lock();\n\t\t\tpid = rcu_dereference(vcpu->pid);\n\t\t\tif (pid)\n\t\t\t\ttask = get_pid_task(vcpu->pid, PIDTYPE_PID);\n\t\t\trcu_read_unlock();\n\t\t\tif (!task)\n\t\t\t\tcontinue;\n\t\t\tif (task->flags & PF_VCPU) {\n\t\t\t\tput_task_struct(task);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (yield_to(task, 1)) {\n\t\t\t\tput_task_struct(task);\n\t\t\t\tkvm->last_boosted_vcpu = i;\n\t\t\t\tyielded = 1;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tput_task_struct(task);\n\t\t}\n\t}\n}\nEXPORT_SYMBOL_GPL(kvm_vcpu_on_spin);\n\nstatic int kvm_vcpu_fault(struct vm_area_struct *vma, struct vm_fault *vmf)\n{\n\tstruct kvm_vcpu *vcpu = vma->vm_file->private_data;\n\tstruct page *page;\n\n\tif (vmf->pgoff == 0)\n\t\tpage = virt_to_page(vcpu->run);\n#ifdef CONFIG_X86\n\telse if (vmf->pgoff == KVM_PIO_PAGE_OFFSET)\n\t\tpage = virt_to_page(vcpu->arch.pio_data);\n#endif\n#ifdef KVM_COALESCED_MMIO_PAGE_OFFSET\n\telse if (vmf->pgoff == KVM_COALESCED_MMIO_PAGE_OFFSET)\n\t\tpage = virt_to_page(vcpu->kvm->coalesced_mmio_ring);\n#endif\n\telse\n\t\treturn VM_FAULT_SIGBUS;\n\tget_page(page);\n\tvmf->page = page;\n\treturn 0;\n}\n\nstatic const struct vm_operations_struct kvm_vcpu_vm_ops = {\n\t.fault = kvm_vcpu_fault,\n};\n\nstatic int kvm_vcpu_mmap(struct file *file, struct vm_area_struct *vma)\n{\n\tvma->vm_ops = &kvm_vcpu_vm_ops;\n\treturn 0;\n}\n\nstatic int kvm_vcpu_release(struct inode *inode, struct file *filp)\n{\n\tstruct kvm_vcpu *vcpu = filp->private_data;\n\n\tkvm_put_kvm(vcpu->kvm);\n\treturn 0;\n}\n\nstatic struct file_operations kvm_vcpu_fops = {\n\t.release        = kvm_vcpu_release,\n\t.unlocked_ioctl = kvm_vcpu_ioctl,\n\t.compat_ioctl   = kvm_vcpu_ioctl,\n\t.mmap           = kvm_vcpu_mmap,\n\t.llseek\t\t= noop_llseek,\n};\n\n/*\n * Allocates an inode for the vcpu.\n */\nstatic int create_vcpu_fd(struct kvm_vcpu *vcpu)\n{\n\treturn anon_inode_getfd(\"kvm-vcpu\", &kvm_vcpu_fops, vcpu, O_RDWR);\n}\n\n/*\n * Creates some virtual cpus.  Good luck creating more than one.\n */\nstatic int kvm_vm_ioctl_create_vcpu(struct kvm *kvm, u32 id)\n{\n\tint r;\n\tstruct kvm_vcpu *vcpu, *v;\n\n\tvcpu = kvm_arch_vcpu_create(kvm, id);\n\tif (IS_ERR(vcpu))\n\t\treturn PTR_ERR(vcpu);\n\n\tpreempt_notifier_init(&vcpu->preempt_notifier, &kvm_preempt_ops);\n\n\tr = kvm_arch_vcpu_setup(vcpu);\n\tif (r)\n\t\treturn r;\n\n\tmutex_lock(&kvm->lock);\n\tif (atomic_read(&kvm->online_vcpus) == KVM_MAX_VCPUS) {\n\t\tr = -EINVAL;\n\t\tgoto vcpu_destroy;\n\t}\n\n\tkvm_for_each_vcpu(r, v, kvm)\n\t\tif (v->vcpu_id == id) {\n\t\t\tr = -EEXIST;\n\t\t\tgoto vcpu_destroy;\n\t\t}\n\n\tBUG_ON(kvm->vcpus[atomic_read(&kvm->online_vcpus)]);\n\n\t/* Now it's all set up, let userspace reach it */\n\tkvm_get_kvm(kvm);\n\tr = create_vcpu_fd(vcpu);\n\tif (r < 0) {\n\t\tkvm_put_kvm(kvm);\n\t\tgoto vcpu_destroy;\n\t}\n\n\tkvm->vcpus[atomic_read(&kvm->online_vcpus)] = vcpu;\n\tsmp_wmb();\n\tatomic_inc(&kvm->online_vcpus);\n\n#ifdef CONFIG_KVM_APIC_ARCHITECTURE\n\tif (kvm->bsp_vcpu_id == id)\n\t\tkvm->bsp_vcpu = vcpu;\n#endif\n\tmutex_unlock(&kvm->lock);\n\treturn r;\n\nvcpu_destroy:\n\tmutex_unlock(&kvm->lock);\n\tkvm_arch_vcpu_destroy(vcpu);\n\treturn r;\n}\n\nstatic int kvm_vcpu_ioctl_set_sigmask(struct kvm_vcpu *vcpu, sigset_t *sigset)\n{\n\tif (sigset) {\n\t\tsigdelsetmask(sigset, sigmask(SIGKILL)|sigmask(SIGSTOP));\n\t\tvcpu->sigset_active = 1;\n\t\tvcpu->sigset = *sigset;\n\t} else\n\t\tvcpu->sigset_active = 0;\n\treturn 0;\n}\n\nstatic long kvm_vcpu_ioctl(struct file *filp,\n\t\t\t   unsigned int ioctl, unsigned long arg)\n{\n\tstruct kvm_vcpu *vcpu = filp->private_data;\n\tvoid __user *argp = (void __user *)arg;\n\tint r;\n\tstruct kvm_fpu *fpu = NULL;\n\tstruct kvm_sregs *kvm_sregs = NULL;\n\n\tif (vcpu->kvm->mm != current->mm)\n\t\treturn -EIO;\n\n#if defined(CONFIG_S390) || defined(CONFIG_PPC)\n\t/*\n\t * Special cases: vcpu ioctls that are asynchronous to vcpu execution,\n\t * so vcpu_load() would break it.\n\t */\n\tif (ioctl == KVM_S390_INTERRUPT || ioctl == KVM_INTERRUPT)\n\t\treturn kvm_arch_vcpu_ioctl(filp, ioctl, arg);\n#endif\n\n\n\tvcpu_load(vcpu);\n\tswitch (ioctl) {\n\tcase KVM_RUN:\n\t\tr = -EINVAL;\n\t\tif (arg)\n\t\t\tgoto out;\n\t\tr = kvm_arch_vcpu_ioctl_run(vcpu, vcpu->run);\n\t\ttrace_kvm_userspace_exit(vcpu->run->exit_reason, r);\n\t\tbreak;\n\tcase KVM_GET_REGS: {\n\t\tstruct kvm_regs *kvm_regs;\n\n\t\tr = -ENOMEM;\n\t\tkvm_regs = kzalloc(sizeof(struct kvm_regs), GFP_KERNEL);\n\t\tif (!kvm_regs)\n\t\t\tgoto out;\n\t\tr = kvm_arch_vcpu_ioctl_get_regs(vcpu, kvm_regs);\n\t\tif (r)\n\t\t\tgoto out_free1;\n\t\tr = -EFAULT;\n\t\tif (copy_to_user(argp, kvm_regs, sizeof(struct kvm_regs)))\n\t\t\tgoto out_free1;\n\t\tr = 0;\nout_free1:\n\t\tkfree(kvm_regs);\n\t\tbreak;\n\t}\n\tcase KVM_SET_REGS: {\n\t\tstruct kvm_regs *kvm_regs;\n\n\t\tr = -ENOMEM;\n\t\tkvm_regs = kzalloc(sizeof(struct kvm_regs), GFP_KERNEL);\n\t\tif (!kvm_regs)\n\t\t\tgoto out;\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(kvm_regs, argp, sizeof(struct kvm_regs)))\n\t\t\tgoto out_free2;\n\t\tr = kvm_arch_vcpu_ioctl_set_regs(vcpu, kvm_regs);\n\t\tif (r)\n\t\t\tgoto out_free2;\n\t\tr = 0;\nout_free2:\n\t\tkfree(kvm_regs);\n\t\tbreak;\n\t}\n\tcase KVM_GET_SREGS: {\n\t\tkvm_sregs = kzalloc(sizeof(struct kvm_sregs), GFP_KERNEL);\n\t\tr = -ENOMEM;\n\t\tif (!kvm_sregs)\n\t\t\tgoto out;\n\t\tr = kvm_arch_vcpu_ioctl_get_sregs(vcpu, kvm_sregs);\n\t\tif (r)\n\t\t\tgoto out;\n\t\tr = -EFAULT;\n\t\tif (copy_to_user(argp, kvm_sregs, sizeof(struct kvm_sregs)))\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_SET_SREGS: {\n\t\tkvm_sregs = kmalloc(sizeof(struct kvm_sregs), GFP_KERNEL);\n\t\tr = -ENOMEM;\n\t\tif (!kvm_sregs)\n\t\t\tgoto out;\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(kvm_sregs, argp, sizeof(struct kvm_sregs)))\n\t\t\tgoto out;\n\t\tr = kvm_arch_vcpu_ioctl_set_sregs(vcpu, kvm_sregs);\n\t\tif (r)\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_GET_MP_STATE: {\n\t\tstruct kvm_mp_state mp_state;\n\n\t\tr = kvm_arch_vcpu_ioctl_get_mpstate(vcpu, &mp_state);\n\t\tif (r)\n\t\t\tgoto out;\n\t\tr = -EFAULT;\n\t\tif (copy_to_user(argp, &mp_state, sizeof mp_state))\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_SET_MP_STATE: {\n\t\tstruct kvm_mp_state mp_state;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&mp_state, argp, sizeof mp_state))\n\t\t\tgoto out;\n\t\tr = kvm_arch_vcpu_ioctl_set_mpstate(vcpu, &mp_state);\n\t\tif (r)\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_TRANSLATE: {\n\t\tstruct kvm_translation tr;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&tr, argp, sizeof tr))\n\t\t\tgoto out;\n\t\tr = kvm_arch_vcpu_ioctl_translate(vcpu, &tr);\n\t\tif (r)\n\t\t\tgoto out;\n\t\tr = -EFAULT;\n\t\tif (copy_to_user(argp, &tr, sizeof tr))\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_SET_GUEST_DEBUG: {\n\t\tstruct kvm_guest_debug dbg;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&dbg, argp, sizeof dbg))\n\t\t\tgoto out;\n\t\tr = kvm_arch_vcpu_ioctl_set_guest_debug(vcpu, &dbg);\n\t\tif (r)\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_SET_SIGNAL_MASK: {\n\t\tstruct kvm_signal_mask __user *sigmask_arg = argp;\n\t\tstruct kvm_signal_mask kvm_sigmask;\n\t\tsigset_t sigset, *p;\n\n\t\tp = NULL;\n\t\tif (argp) {\n\t\t\tr = -EFAULT;\n\t\t\tif (copy_from_user(&kvm_sigmask, argp,\n\t\t\t\t\t   sizeof kvm_sigmask))\n\t\t\t\tgoto out;\n\t\t\tr = -EINVAL;\n\t\t\tif (kvm_sigmask.len != sizeof sigset)\n\t\t\t\tgoto out;\n\t\t\tr = -EFAULT;\n\t\t\tif (copy_from_user(&sigset, sigmask_arg->sigset,\n\t\t\t\t\t   sizeof sigset))\n\t\t\t\tgoto out;\n\t\t\tp = &sigset;\n\t\t}\n\t\tr = kvm_vcpu_ioctl_set_sigmask(vcpu, p);\n\t\tbreak;\n\t}\n\tcase KVM_GET_FPU: {\n\t\tfpu = kzalloc(sizeof(struct kvm_fpu), GFP_KERNEL);\n\t\tr = -ENOMEM;\n\t\tif (!fpu)\n\t\t\tgoto out;\n\t\tr = kvm_arch_vcpu_ioctl_get_fpu(vcpu, fpu);\n\t\tif (r)\n\t\t\tgoto out;\n\t\tr = -EFAULT;\n\t\tif (copy_to_user(argp, fpu, sizeof(struct kvm_fpu)))\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_SET_FPU: {\n\t\tfpu = kmalloc(sizeof(struct kvm_fpu), GFP_KERNEL);\n\t\tr = -ENOMEM;\n\t\tif (!fpu)\n\t\t\tgoto out;\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(fpu, argp, sizeof(struct kvm_fpu)))\n\t\t\tgoto out;\n\t\tr = kvm_arch_vcpu_ioctl_set_fpu(vcpu, fpu);\n\t\tif (r)\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tdefault:\n\t\tr = kvm_arch_vcpu_ioctl(filp, ioctl, arg);\n\t}\nout:\n\tvcpu_put(vcpu);\n\tkfree(fpu);\n\tkfree(kvm_sregs);\n\treturn r;\n}\n\nstatic long kvm_vm_ioctl(struct file *filp,\n\t\t\t   unsigned int ioctl, unsigned long arg)\n{\n\tstruct kvm *kvm = filp->private_data;\n\tvoid __user *argp = (void __user *)arg;\n\tint r;\n\n\tif (kvm->mm != current->mm)\n\t\treturn -EIO;\n\tswitch (ioctl) {\n\tcase KVM_CREATE_VCPU:\n\t\tr = kvm_vm_ioctl_create_vcpu(kvm, arg);\n\t\tif (r < 0)\n\t\t\tgoto out;\n\t\tbreak;\n\tcase KVM_SET_USER_MEMORY_REGION: {\n\t\tstruct kvm_userspace_memory_region kvm_userspace_mem;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&kvm_userspace_mem, argp,\n\t\t\t\t\t\tsizeof kvm_userspace_mem))\n\t\t\tgoto out;\n\n\t\tr = kvm_vm_ioctl_set_memory_region(kvm, &kvm_userspace_mem, 1);\n\t\tif (r)\n\t\t\tgoto out;\n\t\tbreak;\n\t}\n\tcase KVM_GET_DIRTY_LOG: {\n\t\tstruct kvm_dirty_log log;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&log, argp, sizeof log))\n\t\t\tgoto out;\n\t\tr = kvm_vm_ioctl_get_dirty_log(kvm, &log);\n\t\tif (r)\n\t\t\tgoto out;\n\t\tbreak;\n\t}\n#ifdef KVM_COALESCED_MMIO_PAGE_OFFSET\n\tcase KVM_REGISTER_COALESCED_MMIO: {\n\t\tstruct kvm_coalesced_mmio_zone zone;\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&zone, argp, sizeof zone))\n\t\t\tgoto out;\n\t\tr = kvm_vm_ioctl_register_coalesced_mmio(kvm, &zone);\n\t\tif (r)\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_UNREGISTER_COALESCED_MMIO: {\n\t\tstruct kvm_coalesced_mmio_zone zone;\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&zone, argp, sizeof zone))\n\t\t\tgoto out;\n\t\tr = kvm_vm_ioctl_unregister_coalesced_mmio(kvm, &zone);\n\t\tif (r)\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\t}\n#endif\n\tcase KVM_IRQFD: {\n\t\tstruct kvm_irqfd data;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&data, argp, sizeof data))\n\t\t\tgoto out;\n\t\tr = kvm_irqfd(kvm, data.fd, data.gsi, data.flags);\n\t\tbreak;\n\t}\n\tcase KVM_IOEVENTFD: {\n\t\tstruct kvm_ioeventfd data;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&data, argp, sizeof data))\n\t\t\tgoto out;\n\t\tr = kvm_ioeventfd(kvm, &data);\n\t\tbreak;\n\t}\n#ifdef CONFIG_KVM_APIC_ARCHITECTURE\n\tcase KVM_SET_BOOT_CPU_ID:\n\t\tr = 0;\n\t\tmutex_lock(&kvm->lock);\n\t\tif (atomic_read(&kvm->online_vcpus) != 0)\n\t\t\tr = -EBUSY;\n\t\telse\n\t\t\tkvm->bsp_vcpu_id = arg;\n\t\tmutex_unlock(&kvm->lock);\n\t\tbreak;\n#endif\n\tdefault:\n\t\tr = kvm_arch_vm_ioctl(filp, ioctl, arg);\n\t\tif (r == -ENOTTY)\n\t\t\tr = kvm_vm_ioctl_assigned_device(kvm, ioctl, arg);\n\t}\nout:\n\treturn r;\n}\n\n#ifdef CONFIG_COMPAT\nstruct compat_kvm_dirty_log {\n\t__u32 slot;\n\t__u32 padding1;\n\tunion {\n\t\tcompat_uptr_t dirty_bitmap; /* one bit per page */\n\t\t__u64 padding2;\n\t};\n};\n\nstatic long kvm_vm_compat_ioctl(struct file *filp,\n\t\t\t   unsigned int ioctl, unsigned long arg)\n{\n\tstruct kvm *kvm = filp->private_data;\n\tint r;\n\n\tif (kvm->mm != current->mm)\n\t\treturn -EIO;\n\tswitch (ioctl) {\n\tcase KVM_GET_DIRTY_LOG: {\n\t\tstruct compat_kvm_dirty_log compat_log;\n\t\tstruct kvm_dirty_log log;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&compat_log, (void __user *)arg,\n\t\t\t\t   sizeof(compat_log)))\n\t\t\tgoto out;\n\t\tlog.slot\t = compat_log.slot;\n\t\tlog.padding1\t = compat_log.padding1;\n\t\tlog.padding2\t = compat_log.padding2;\n\t\tlog.dirty_bitmap = compat_ptr(compat_log.dirty_bitmap);\n\n\t\tr = kvm_vm_ioctl_get_dirty_log(kvm, &log);\n\t\tif (r)\n\t\t\tgoto out;\n\t\tbreak;\n\t}\n\tdefault:\n\t\tr = kvm_vm_ioctl(filp, ioctl, arg);\n\t}\n\nout:\n\treturn r;\n}\n#endif\n\nstatic int kvm_vm_fault(struct vm_area_struct *vma, struct vm_fault *vmf)\n{\n\tstruct page *page[1];\n\tunsigned long addr;\n\tint npages;\n\tgfn_t gfn = vmf->pgoff;\n\tstruct kvm *kvm = vma->vm_file->private_data;\n\n\taddr = gfn_to_hva(kvm, gfn);\n\tif (kvm_is_error_hva(addr))\n\t\treturn VM_FAULT_SIGBUS;\n\n\tnpages = get_user_pages(current, current->mm, addr, 1, 1, 0, page,\n\t\t\t\tNULL);\n\tif (unlikely(npages != 1))\n\t\treturn VM_FAULT_SIGBUS;\n\n\tvmf->page = page[0];\n\treturn 0;\n}\n\nstatic const struct vm_operations_struct kvm_vm_vm_ops = {\n\t.fault = kvm_vm_fault,\n};\n\nstatic int kvm_vm_mmap(struct file *file, struct vm_area_struct *vma)\n{\n\tvma->vm_ops = &kvm_vm_vm_ops;\n\treturn 0;\n}\n\nstatic struct file_operations kvm_vm_fops = {\n\t.release        = kvm_vm_release,\n\t.unlocked_ioctl = kvm_vm_ioctl,\n#ifdef CONFIG_COMPAT\n\t.compat_ioctl   = kvm_vm_compat_ioctl,\n#endif\n\t.mmap           = kvm_vm_mmap,\n\t.llseek\t\t= noop_llseek,\n};\n\nstatic int kvm_dev_ioctl_create_vm(void)\n{\n\tint r;\n\tstruct kvm *kvm;\n\n\tkvm = kvm_create_vm();\n\tif (IS_ERR(kvm))\n\t\treturn PTR_ERR(kvm);\n#ifdef KVM_COALESCED_MMIO_PAGE_OFFSET\n\tr = kvm_coalesced_mmio_init(kvm);\n\tif (r < 0) {\n\t\tkvm_put_kvm(kvm);\n\t\treturn r;\n\t}\n#endif\n\tr = anon_inode_getfd(\"kvm-vm\", &kvm_vm_fops, kvm, O_RDWR);\n\tif (r < 0)\n\t\tkvm_put_kvm(kvm);\n\n\treturn r;\n}\n\nstatic long kvm_dev_ioctl_check_extension_generic(long arg)\n{\n\tswitch (arg) {\n\tcase KVM_CAP_USER_MEMORY:\n\tcase KVM_CAP_DESTROY_MEMORY_REGION_WORKS:\n\tcase KVM_CAP_JOIN_MEMORY_REGIONS_WORKS:\n#ifdef CONFIG_KVM_APIC_ARCHITECTURE\n\tcase KVM_CAP_SET_BOOT_CPU_ID:\n#endif\n\tcase KVM_CAP_INTERNAL_ERROR_DATA:\n\t\treturn 1;\n#ifdef CONFIG_HAVE_KVM_IRQCHIP\n\tcase KVM_CAP_IRQ_ROUTING:\n\t\treturn KVM_MAX_IRQ_ROUTES;\n#endif\n\tdefault:\n\t\tbreak;\n\t}\n\treturn kvm_dev_ioctl_check_extension(arg);\n}\n\nstatic long kvm_dev_ioctl(struct file *filp,\n\t\t\t  unsigned int ioctl, unsigned long arg)\n{\n\tlong r = -EINVAL;\n\n\tswitch (ioctl) {\n\tcase KVM_GET_API_VERSION:\n\t\tr = -EINVAL;\n\t\tif (arg)\n\t\t\tgoto out;\n\t\tr = KVM_API_VERSION;\n\t\tbreak;\n\tcase KVM_CREATE_VM:\n\t\tr = -EINVAL;\n\t\tif (arg)\n\t\t\tgoto out;\n\t\tr = kvm_dev_ioctl_create_vm();\n\t\tbreak;\n\tcase KVM_CHECK_EXTENSION:\n\t\tr = kvm_dev_ioctl_check_extension_generic(arg);\n\t\tbreak;\n\tcase KVM_GET_VCPU_MMAP_SIZE:\n\t\tr = -EINVAL;\n\t\tif (arg)\n\t\t\tgoto out;\n\t\tr = PAGE_SIZE;     /* struct kvm_run */\n#ifdef CONFIG_X86\n\t\tr += PAGE_SIZE;    /* pio data page */\n#endif\n#ifdef KVM_COALESCED_MMIO_PAGE_OFFSET\n\t\tr += PAGE_SIZE;    /* coalesced mmio ring page */\n#endif\n\t\tbreak;\n\tcase KVM_TRACE_ENABLE:\n\tcase KVM_TRACE_PAUSE:\n\tcase KVM_TRACE_DISABLE:\n\t\tr = -EOPNOTSUPP;\n\t\tbreak;\n\tdefault:\n\t\treturn kvm_arch_dev_ioctl(filp, ioctl, arg);\n\t}\nout:\n\treturn r;\n}\n\nstatic struct file_operations kvm_chardev_ops = {\n\t.unlocked_ioctl = kvm_dev_ioctl,\n\t.compat_ioctl   = kvm_dev_ioctl,\n\t.llseek\t\t= noop_llseek,\n};\n\nstatic struct miscdevice kvm_dev = {\n\tKVM_MINOR,\n\t\"kvm\",\n\t&kvm_chardev_ops,\n};\n\nstatic void hardware_enable_nolock(void *junk)\n{\n\tint cpu = raw_smp_processor_id();\n\tint r;\n\n\tif (cpumask_test_cpu(cpu, cpus_hardware_enabled))\n\t\treturn;\n\n\tcpumask_set_cpu(cpu, cpus_hardware_enabled);\n\n\tr = kvm_arch_hardware_enable(NULL);\n\n\tif (r) {\n\t\tcpumask_clear_cpu(cpu, cpus_hardware_enabled);\n\t\tatomic_inc(&hardware_enable_failed);\n\t\tprintk(KERN_INFO \"kvm: enabling virtualization on \"\n\t\t\t\t \"CPU%d failed\\n\", cpu);\n\t}\n}\n\nstatic void hardware_enable(void *junk)\n{\n\traw_spin_lock(&kvm_lock);\n\thardware_enable_nolock(junk);\n\traw_spin_unlock(&kvm_lock);\n}\n\nstatic void hardware_disable_nolock(void *junk)\n{\n\tint cpu = raw_smp_processor_id();\n\n\tif (!cpumask_test_cpu(cpu, cpus_hardware_enabled))\n\t\treturn;\n\tcpumask_clear_cpu(cpu, cpus_hardware_enabled);\n\tkvm_arch_hardware_disable(NULL);\n}\n\nstatic void hardware_disable(void *junk)\n{\n\traw_spin_lock(&kvm_lock);\n\thardware_disable_nolock(junk);\n\traw_spin_unlock(&kvm_lock);\n}\n\nstatic void hardware_disable_all_nolock(void)\n{\n\tBUG_ON(!kvm_usage_count);\n\n\tkvm_usage_count--;\n\tif (!kvm_usage_count)\n\t\ton_each_cpu(hardware_disable_nolock, NULL, 1);\n}\n\nstatic void hardware_disable_all(void)\n{\n\traw_spin_lock(&kvm_lock);\n\thardware_disable_all_nolock();\n\traw_spin_unlock(&kvm_lock);\n}\n\nstatic int hardware_enable_all(void)\n{\n\tint r = 0;\n\n\traw_spin_lock(&kvm_lock);\n\n\tkvm_usage_count++;\n\tif (kvm_usage_count == 1) {\n\t\tatomic_set(&hardware_enable_failed, 0);\n\t\ton_each_cpu(hardware_enable_nolock, NULL, 1);\n\n\t\tif (atomic_read(&hardware_enable_failed)) {\n\t\t\thardware_disable_all_nolock();\n\t\t\tr = -EBUSY;\n\t\t}\n\t}\n\n\traw_spin_unlock(&kvm_lock);\n\n\treturn r;\n}\n\nstatic int kvm_cpu_hotplug(struct notifier_block *notifier, unsigned long val,\n\t\t\t   void *v)\n{\n\tint cpu = (long)v;\n\n\tif (!kvm_usage_count)\n\t\treturn NOTIFY_OK;\n\n\tval &= ~CPU_TASKS_FROZEN;\n\tswitch (val) {\n\tcase CPU_DYING:\n\t\tprintk(KERN_INFO \"kvm: disabling virtualization on CPU%d\\n\",\n\t\t       cpu);\n\t\thardware_disable(NULL);\n\t\tbreak;\n\tcase CPU_STARTING:\n\t\tprintk(KERN_INFO \"kvm: enabling virtualization on CPU%d\\n\",\n\t\t       cpu);\n\t\thardware_enable(NULL);\n\t\tbreak;\n\t}\n\treturn NOTIFY_OK;\n}\n\n\nasmlinkage void kvm_spurious_fault(void)\n{\n\t/* Fault while not rebooting.  We want the trace. */\n\tBUG();\n}\nEXPORT_SYMBOL_GPL(kvm_spurious_fault);\n\nstatic int kvm_reboot(struct notifier_block *notifier, unsigned long val,\n\t\t      void *v)\n{\n\t/*\n\t * Some (well, at least mine) BIOSes hang on reboot if\n\t * in vmx root mode.\n\t *\n\t * And Intel TXT required VMX off for all cpu when system shutdown.\n\t */\n\tprintk(KERN_INFO \"kvm: exiting hardware virtualization\\n\");\n\tkvm_rebooting = true;\n\ton_each_cpu(hardware_disable_nolock, NULL, 1);\n\treturn NOTIFY_OK;\n}\n\nstatic struct notifier_block kvm_reboot_notifier = {\n\t.notifier_call = kvm_reboot,\n\t.priority = 0,\n};\n\nstatic void kvm_io_bus_destroy(struct kvm_io_bus *bus)\n{\n\tint i;\n\n\tfor (i = 0; i < bus->dev_count; i++) {\n\t\tstruct kvm_io_device *pos = bus->devs[i];\n\n\t\tkvm_iodevice_destructor(pos);\n\t}\n\tkfree(bus);\n}\n\n/* kvm_io_bus_write - called under kvm->slots_lock */\nint kvm_io_bus_write(struct kvm *kvm, enum kvm_bus bus_idx, gpa_t addr,\n\t\t     int len, const void *val)\n{\n\tint i;\n\tstruct kvm_io_bus *bus;\n\n\tbus = srcu_dereference(kvm->buses[bus_idx], &kvm->srcu);\n\tfor (i = 0; i < bus->dev_count; i++)\n\t\tif (!kvm_iodevice_write(bus->devs[i], addr, len, val))\n\t\t\treturn 0;\n\treturn -EOPNOTSUPP;\n}\n\n/* kvm_io_bus_read - called under kvm->slots_lock */\nint kvm_io_bus_read(struct kvm *kvm, enum kvm_bus bus_idx, gpa_t addr,\n\t\t    int len, void *val)\n{\n\tint i;\n\tstruct kvm_io_bus *bus;\n\n\tbus = srcu_dereference(kvm->buses[bus_idx], &kvm->srcu);\n\tfor (i = 0; i < bus->dev_count; i++)\n\t\tif (!kvm_iodevice_read(bus->devs[i], addr, len, val))\n\t\t\treturn 0;\n\treturn -EOPNOTSUPP;\n}\n\n/* Caller must hold slots_lock. */\nint kvm_io_bus_register_dev(struct kvm *kvm, enum kvm_bus bus_idx,\n\t\t\t    struct kvm_io_device *dev)\n{\n\tstruct kvm_io_bus *new_bus, *bus;\n\n\tbus = kvm->buses[bus_idx];\n\tif (bus->dev_count > NR_IOBUS_DEVS-1)\n\t\treturn -ENOSPC;\n\n\tnew_bus = kzalloc(sizeof(struct kvm_io_bus), GFP_KERNEL);\n\tif (!new_bus)\n\t\treturn -ENOMEM;\n\tmemcpy(new_bus, bus, sizeof(struct kvm_io_bus));\n\tnew_bus->devs[new_bus->dev_count++] = dev;\n\trcu_assign_pointer(kvm->buses[bus_idx], new_bus);\n\tsynchronize_srcu_expedited(&kvm->srcu);\n\tkfree(bus);\n\n\treturn 0;\n}\n\n/* Caller must hold slots_lock. */\nint kvm_io_bus_unregister_dev(struct kvm *kvm, enum kvm_bus bus_idx,\n\t\t\t      struct kvm_io_device *dev)\n{\n\tint i, r;\n\tstruct kvm_io_bus *new_bus, *bus;\n\n\tnew_bus = kzalloc(sizeof(struct kvm_io_bus), GFP_KERNEL);\n\tif (!new_bus)\n\t\treturn -ENOMEM;\n\n\tbus = kvm->buses[bus_idx];\n\tmemcpy(new_bus, bus, sizeof(struct kvm_io_bus));\n\n\tr = -ENOENT;\n\tfor (i = 0; i < new_bus->dev_count; i++)\n\t\tif (new_bus->devs[i] == dev) {\n\t\t\tr = 0;\n\t\t\tnew_bus->devs[i] = new_bus->devs[--new_bus->dev_count];\n\t\t\tbreak;\n\t\t}\n\n\tif (r) {\n\t\tkfree(new_bus);\n\t\treturn r;\n\t}\n\n\trcu_assign_pointer(kvm->buses[bus_idx], new_bus);\n\tsynchronize_srcu_expedited(&kvm->srcu);\n\tkfree(bus);\n\treturn r;\n}\n\nstatic struct notifier_block kvm_cpu_notifier = {\n\t.notifier_call = kvm_cpu_hotplug,\n};\n\nstatic int vm_stat_get(void *_offset, u64 *val)\n{\n\tunsigned offset = (long)_offset;\n\tstruct kvm *kvm;\n\n\t*val = 0;\n\traw_spin_lock(&kvm_lock);\n\tlist_for_each_entry(kvm, &vm_list, vm_list)\n\t\t*val += *(u32 *)((void *)kvm + offset);\n\traw_spin_unlock(&kvm_lock);\n\treturn 0;\n}\n\nDEFINE_SIMPLE_ATTRIBUTE(vm_stat_fops, vm_stat_get, NULL, \"%llu\\n\");\n\nstatic int vcpu_stat_get(void *_offset, u64 *val)\n{\n\tunsigned offset = (long)_offset;\n\tstruct kvm *kvm;\n\tstruct kvm_vcpu *vcpu;\n\tint i;\n\n\t*val = 0;\n\traw_spin_lock(&kvm_lock);\n\tlist_for_each_entry(kvm, &vm_list, vm_list)\n\t\tkvm_for_each_vcpu(i, vcpu, kvm)\n\t\t\t*val += *(u32 *)((void *)vcpu + offset);\n\n\traw_spin_unlock(&kvm_lock);\n\treturn 0;\n}\n\nDEFINE_SIMPLE_ATTRIBUTE(vcpu_stat_fops, vcpu_stat_get, NULL, \"%llu\\n\");\n\nstatic const struct file_operations *stat_fops[] = {\n\t[KVM_STAT_VCPU] = &vcpu_stat_fops,\n\t[KVM_STAT_VM]   = &vm_stat_fops,\n};\n\nstatic void kvm_init_debug(void)\n{\n\tstruct kvm_stats_debugfs_item *p;\n\n\tkvm_debugfs_dir = debugfs_create_dir(\"kvm\", NULL);\n\tfor (p = debugfs_entries; p->name; ++p)\n\t\tp->dentry = debugfs_create_file(p->name, 0444, kvm_debugfs_dir,\n\t\t\t\t\t\t(void *)(long)p->offset,\n\t\t\t\t\t\tstat_fops[p->kind]);\n}\n\nstatic void kvm_exit_debug(void)\n{\n\tstruct kvm_stats_debugfs_item *p;\n\n\tfor (p = debugfs_entries; p->name; ++p)\n\t\tdebugfs_remove(p->dentry);\n\tdebugfs_remove(kvm_debugfs_dir);\n}\n\nstatic int kvm_suspend(void)\n{\n\tif (kvm_usage_count)\n\t\thardware_disable_nolock(NULL);\n\treturn 0;\n}\n\nstatic void kvm_resume(void)\n{\n\tif (kvm_usage_count) {\n\t\tWARN_ON(raw_spin_is_locked(&kvm_lock));\n\t\thardware_enable_nolock(NULL);\n\t}\n}\n\nstatic struct syscore_ops kvm_syscore_ops = {\n\t.suspend = kvm_suspend,\n\t.resume = kvm_resume,\n};\n\nstruct page *bad_page;\npfn_t bad_pfn;\n\nstatic inline\nstruct kvm_vcpu *preempt_notifier_to_vcpu(struct preempt_notifier *pn)\n{\n\treturn container_of(pn, struct kvm_vcpu, preempt_notifier);\n}\n\nstatic void kvm_sched_in(struct preempt_notifier *pn, int cpu)\n{\n\tstruct kvm_vcpu *vcpu = preempt_notifier_to_vcpu(pn);\n\n\tkvm_arch_vcpu_load(vcpu, cpu);\n}\n\nstatic void kvm_sched_out(struct preempt_notifier *pn,\n\t\t\t  struct task_struct *next)\n{\n\tstruct kvm_vcpu *vcpu = preempt_notifier_to_vcpu(pn);\n\n\tkvm_arch_vcpu_put(vcpu);\n}\n\nint kvm_init(void *opaque, unsigned vcpu_size, unsigned vcpu_align,\n\t\t  struct module *module)\n{\n\tint r;\n\tint cpu;\n\n\tr = kvm_arch_init(opaque);\n\tif (r)\n\t\tgoto out_fail;\n\n\tbad_page = alloc_page(GFP_KERNEL | __GFP_ZERO);\n\n\tif (bad_page == NULL) {\n\t\tr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tbad_pfn = page_to_pfn(bad_page);\n\n\thwpoison_page = alloc_page(GFP_KERNEL | __GFP_ZERO);\n\n\tif (hwpoison_page == NULL) {\n\t\tr = -ENOMEM;\n\t\tgoto out_free_0;\n\t}\n\n\thwpoison_pfn = page_to_pfn(hwpoison_page);\n\n\tfault_page = alloc_page(GFP_KERNEL | __GFP_ZERO);\n\n\tif (fault_page == NULL) {\n\t\tr = -ENOMEM;\n\t\tgoto out_free_0;\n\t}\n\n\tfault_pfn = page_to_pfn(fault_page);\n\n\tif (!zalloc_cpumask_var(&cpus_hardware_enabled, GFP_KERNEL)) {\n\t\tr = -ENOMEM;\n\t\tgoto out_free_0;\n\t}\n\n\tr = kvm_arch_hardware_setup();\n\tif (r < 0)\n\t\tgoto out_free_0a;\n\n\tfor_each_online_cpu(cpu) {\n\t\tsmp_call_function_single(cpu,\n\t\t\t\tkvm_arch_check_processor_compat,\n\t\t\t\t&r, 1);\n\t\tif (r < 0)\n\t\t\tgoto out_free_1;\n\t}\n\n\tr = register_cpu_notifier(&kvm_cpu_notifier);\n\tif (r)\n\t\tgoto out_free_2;\n\tregister_reboot_notifier(&kvm_reboot_notifier);\n\n\t/* A kmem cache lets us meet the alignment requirements of fx_save. */\n\tif (!vcpu_align)\n\t\tvcpu_align = __alignof__(struct kvm_vcpu);\n\tkvm_vcpu_cache = kmem_cache_create(\"kvm_vcpu\", vcpu_size, vcpu_align,\n\t\t\t\t\t   0, NULL);\n\tif (!kvm_vcpu_cache) {\n\t\tr = -ENOMEM;\n\t\tgoto out_free_3;\n\t}\n\n\tr = kvm_async_pf_init();\n\tif (r)\n\t\tgoto out_free;\n\n\tkvm_chardev_ops.owner = module;\n\tkvm_vm_fops.owner = module;\n\tkvm_vcpu_fops.owner = module;\n\n\tr = misc_register(&kvm_dev);\n\tif (r) {\n\t\tprintk(KERN_ERR \"kvm: misc device register failed\\n\");\n\t\tgoto out_unreg;\n\t}\n\n\tregister_syscore_ops(&kvm_syscore_ops);\n\n\tkvm_preempt_ops.sched_in = kvm_sched_in;\n\tkvm_preempt_ops.sched_out = kvm_sched_out;\n\n\tkvm_init_debug();\n\n\treturn 0;\n\nout_unreg:\n\tkvm_async_pf_deinit();\nout_free:\n\tkmem_cache_destroy(kvm_vcpu_cache);\nout_free_3:\n\tunregister_reboot_notifier(&kvm_reboot_notifier);\n\tunregister_cpu_notifier(&kvm_cpu_notifier);\nout_free_2:\nout_free_1:\n\tkvm_arch_hardware_unsetup();\nout_free_0a:\n\tfree_cpumask_var(cpus_hardware_enabled);\nout_free_0:\n\tif (fault_page)\n\t\t__free_page(fault_page);\n\tif (hwpoison_page)\n\t\t__free_page(hwpoison_page);\n\t__free_page(bad_page);\nout:\n\tkvm_arch_exit();\nout_fail:\n\treturn r;\n}\nEXPORT_SYMBOL_GPL(kvm_init);\n\nvoid kvm_exit(void)\n{\n\tkvm_exit_debug();\n\tmisc_deregister(&kvm_dev);\n\tkmem_cache_destroy(kvm_vcpu_cache);\n\tkvm_async_pf_deinit();\n\tunregister_syscore_ops(&kvm_syscore_ops);\n\tunregister_reboot_notifier(&kvm_reboot_notifier);\n\tunregister_cpu_notifier(&kvm_cpu_notifier);\n\ton_each_cpu(hardware_disable_nolock, NULL, 1);\n\tkvm_arch_hardware_unsetup();\n\tkvm_arch_exit();\n\tfree_cpumask_var(cpus_hardware_enabled);\n\t__free_page(hwpoison_page);\n\t__free_page(bad_page);\n}\nEXPORT_SYMBOL_GPL(kvm_exit);\n"], "fixing_code": ["/*\n * Kernel-based Virtual Machine driver for Linux\n *\n * This module enables machines with Intel VT-x extensions to run virtual\n * machines without emulation or binary translation.\n *\n * MMU support\n *\n * Copyright (C) 2006 Qumranet, Inc.\n * Copyright 2010 Red Hat, Inc. and/or its affiliates.\n *\n * Authors:\n *   Yaniv Kamay  <yaniv@qumranet.com>\n *   Avi Kivity   <avi@qumranet.com>\n *\n * This work is licensed under the terms of the GNU GPL, version 2.  See\n * the COPYING file in the top-level directory.\n *\n */\n\n/*\n * We need the mmu code to access both 32-bit and 64-bit guest ptes,\n * so the code in this file is compiled twice, once per pte size.\n */\n\n#if PTTYPE == 64\n\t#define pt_element_t u64\n\t#define guest_walker guest_walker64\n\t#define FNAME(name) paging##64_##name\n\t#define PT_BASE_ADDR_MASK PT64_BASE_ADDR_MASK\n\t#define PT_LVL_ADDR_MASK(lvl) PT64_LVL_ADDR_MASK(lvl)\n\t#define PT_LVL_OFFSET_MASK(lvl) PT64_LVL_OFFSET_MASK(lvl)\n\t#define PT_INDEX(addr, level) PT64_INDEX(addr, level)\n\t#define PT_LEVEL_BITS PT64_LEVEL_BITS\n\t#ifdef CONFIG_X86_64\n\t#define PT_MAX_FULL_LEVELS 4\n\t#define CMPXCHG cmpxchg\n\t#else\n\t#define CMPXCHG cmpxchg64\n\t#define PT_MAX_FULL_LEVELS 2\n\t#endif\n#elif PTTYPE == 32\n\t#define pt_element_t u32\n\t#define guest_walker guest_walker32\n\t#define FNAME(name) paging##32_##name\n\t#define PT_BASE_ADDR_MASK PT32_BASE_ADDR_MASK\n\t#define PT_LVL_ADDR_MASK(lvl) PT32_LVL_ADDR_MASK(lvl)\n\t#define PT_LVL_OFFSET_MASK(lvl) PT32_LVL_OFFSET_MASK(lvl)\n\t#define PT_INDEX(addr, level) PT32_INDEX(addr, level)\n\t#define PT_LEVEL_BITS PT32_LEVEL_BITS\n\t#define PT_MAX_FULL_LEVELS 2\n\t#define CMPXCHG cmpxchg\n#else\n\t#error Invalid PTTYPE value\n#endif\n\n#define gpte_to_gfn_lvl FNAME(gpte_to_gfn_lvl)\n#define gpte_to_gfn(pte) gpte_to_gfn_lvl((pte), PT_PAGE_TABLE_LEVEL)\n\n/*\n * The guest_walker structure emulates the behavior of the hardware page\n * table walker.\n */\nstruct guest_walker {\n\tint level;\n\tgfn_t table_gfn[PT_MAX_FULL_LEVELS];\n\tpt_element_t ptes[PT_MAX_FULL_LEVELS];\n\tpt_element_t prefetch_ptes[PTE_PREFETCH_NUM];\n\tgpa_t pte_gpa[PT_MAX_FULL_LEVELS];\n\tunsigned pt_access;\n\tunsigned pte_access;\n\tgfn_t gfn;\n\tstruct x86_exception fault;\n};\n\nstatic gfn_t gpte_to_gfn_lvl(pt_element_t gpte, int lvl)\n{\n\treturn (gpte & PT_LVL_ADDR_MASK(lvl)) >> PAGE_SHIFT;\n}\n\nstatic int FNAME(cmpxchg_gpte)(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu,\n\t\t\t gfn_t table_gfn, unsigned index,\n\t\t\t pt_element_t orig_pte, pt_element_t new_pte)\n{\n\tpt_element_t ret;\n\tpt_element_t *table;\n\tstruct page *page;\n\tgpa_t gpa;\n\n\tgpa = mmu->translate_gpa(vcpu, table_gfn << PAGE_SHIFT,\n\t\t\t\t PFERR_USER_MASK|PFERR_WRITE_MASK);\n\tif (gpa == UNMAPPED_GVA)\n\t\treturn -EFAULT;\n\n\tpage = gfn_to_page(vcpu->kvm, gpa_to_gfn(gpa));\n\n\ttable = kmap_atomic(page, KM_USER0);\n\tret = CMPXCHG(&table[index], orig_pte, new_pte);\n\tkunmap_atomic(table, KM_USER0);\n\n\tkvm_release_page_dirty(page);\n\n\treturn (ret != orig_pte);\n}\n\nstatic unsigned FNAME(gpte_access)(struct kvm_vcpu *vcpu, pt_element_t gpte)\n{\n\tunsigned access;\n\n\taccess = (gpte & (PT_WRITABLE_MASK | PT_USER_MASK)) | ACC_EXEC_MASK;\n#if PTTYPE == 64\n\tif (vcpu->arch.mmu.nx)\n\t\taccess &= ~(gpte >> PT64_NX_SHIFT);\n#endif\n\treturn access;\n}\n\n/*\n * Fetch a guest pte for a guest virtual address\n */\nstatic int FNAME(walk_addr_generic)(struct guest_walker *walker,\n\t\t\t\t    struct kvm_vcpu *vcpu, struct kvm_mmu *mmu,\n\t\t\t\t    gva_t addr, u32 access)\n{\n\tpt_element_t pte;\n\tpt_element_t __user *ptep_user;\n\tgfn_t table_gfn;\n\tunsigned index, pt_access, uninitialized_var(pte_access);\n\tgpa_t pte_gpa;\n\tbool eperm, present, rsvd_fault;\n\tint offset, write_fault, user_fault, fetch_fault;\n\n\twrite_fault = access & PFERR_WRITE_MASK;\n\tuser_fault = access & PFERR_USER_MASK;\n\tfetch_fault = access & PFERR_FETCH_MASK;\n\n\ttrace_kvm_mmu_pagetable_walk(addr, write_fault, user_fault,\n\t\t\t\t     fetch_fault);\nwalk:\n\tpresent = true;\n\teperm = rsvd_fault = false;\n\twalker->level = mmu->root_level;\n\tpte           = mmu->get_cr3(vcpu);\n\n#if PTTYPE == 64\n\tif (walker->level == PT32E_ROOT_LEVEL) {\n\t\tpte = kvm_pdptr_read_mmu(vcpu, mmu, (addr >> 30) & 3);\n\t\ttrace_kvm_mmu_paging_element(pte, walker->level);\n\t\tif (!is_present_gpte(pte)) {\n\t\t\tpresent = false;\n\t\t\tgoto error;\n\t\t}\n\t\t--walker->level;\n\t}\n#endif\n\tASSERT((!is_long_mode(vcpu) && is_pae(vcpu)) ||\n\t       (mmu->get_cr3(vcpu) & CR3_NONPAE_RESERVED_BITS) == 0);\n\n\tpt_access = ACC_ALL;\n\n\tfor (;;) {\n\t\tgfn_t real_gfn;\n\t\tunsigned long host_addr;\n\n\t\tindex = PT_INDEX(addr, walker->level);\n\n\t\ttable_gfn = gpte_to_gfn(pte);\n\t\toffset    = index * sizeof(pt_element_t);\n\t\tpte_gpa   = gfn_to_gpa(table_gfn) + offset;\n\t\twalker->table_gfn[walker->level - 1] = table_gfn;\n\t\twalker->pte_gpa[walker->level - 1] = pte_gpa;\n\n\t\treal_gfn = mmu->translate_gpa(vcpu, gfn_to_gpa(table_gfn),\n\t\t\t\t\t      PFERR_USER_MASK|PFERR_WRITE_MASK);\n\t\tif (unlikely(real_gfn == UNMAPPED_GVA)) {\n\t\t\tpresent = false;\n\t\t\tbreak;\n\t\t}\n\t\treal_gfn = gpa_to_gfn(real_gfn);\n\n\t\thost_addr = gfn_to_hva(vcpu->kvm, real_gfn);\n\t\tif (unlikely(kvm_is_error_hva(host_addr))) {\n\t\t\tpresent = false;\n\t\t\tbreak;\n\t\t}\n\n\t\tptep_user = (pt_element_t __user *)((void *)host_addr + offset);\n\t\tif (unlikely(__copy_from_user(&pte, ptep_user, sizeof(pte)))) {\n\t\t\tpresent = false;\n\t\t\tbreak;\n\t\t}\n\n\t\ttrace_kvm_mmu_paging_element(pte, walker->level);\n\n\t\tif (unlikely(!is_present_gpte(pte))) {\n\t\t\tpresent = false;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (unlikely(is_rsvd_bits_set(&vcpu->arch.mmu, pte,\n\t\t\t\t\t      walker->level))) {\n\t\t\trsvd_fault = true;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (unlikely(write_fault && !is_writable_pte(pte)\n\t\t\t     && (user_fault || is_write_protection(vcpu))))\n\t\t\teperm = true;\n\n\t\tif (unlikely(user_fault && !(pte & PT_USER_MASK)))\n\t\t\teperm = true;\n\n#if PTTYPE == 64\n\t\tif (unlikely(fetch_fault && (pte & PT64_NX_MASK)))\n\t\t\teperm = true;\n#endif\n\n\t\tif (!eperm && !rsvd_fault\n\t\t    && unlikely(!(pte & PT_ACCESSED_MASK))) {\n\t\t\tint ret;\n\t\t\ttrace_kvm_mmu_set_accessed_bit(table_gfn, index,\n\t\t\t\t\t\t       sizeof(pte));\n\t\t\tret = FNAME(cmpxchg_gpte)(vcpu, mmu, table_gfn,\n\t\t\t\t\tindex, pte, pte|PT_ACCESSED_MASK);\n\t\t\tif (ret < 0) {\n\t\t\t\tpresent = false;\n\t\t\t\tbreak;\n\t\t\t} else if (ret)\n\t\t\t\tgoto walk;\n\n\t\t\tmark_page_dirty(vcpu->kvm, table_gfn);\n\t\t\tpte |= PT_ACCESSED_MASK;\n\t\t}\n\n\t\tpte_access = pt_access & FNAME(gpte_access)(vcpu, pte);\n\n\t\twalker->ptes[walker->level - 1] = pte;\n\n\t\tif ((walker->level == PT_PAGE_TABLE_LEVEL) ||\n\t\t    ((walker->level == PT_DIRECTORY_LEVEL) &&\n\t\t\t\tis_large_pte(pte) &&\n\t\t\t\t(PTTYPE == 64 || is_pse(vcpu))) ||\n\t\t    ((walker->level == PT_PDPE_LEVEL) &&\n\t\t\t\tis_large_pte(pte) &&\n\t\t\t\tmmu->root_level == PT64_ROOT_LEVEL)) {\n\t\t\tint lvl = walker->level;\n\t\t\tgpa_t real_gpa;\n\t\t\tgfn_t gfn;\n\t\t\tu32 ac;\n\n\t\t\tgfn = gpte_to_gfn_lvl(pte, lvl);\n\t\t\tgfn += (addr & PT_LVL_OFFSET_MASK(lvl)) >> PAGE_SHIFT;\n\n\t\t\tif (PTTYPE == 32 &&\n\t\t\t    walker->level == PT_DIRECTORY_LEVEL &&\n\t\t\t    is_cpuid_PSE36())\n\t\t\t\tgfn += pse36_gfn_delta(pte);\n\n\t\t\tac = write_fault | fetch_fault | user_fault;\n\n\t\t\treal_gpa = mmu->translate_gpa(vcpu, gfn_to_gpa(gfn),\n\t\t\t\t\t\t      ac);\n\t\t\tif (real_gpa == UNMAPPED_GVA)\n\t\t\t\treturn 0;\n\n\t\t\twalker->gfn = real_gpa >> PAGE_SHIFT;\n\n\t\t\tbreak;\n\t\t}\n\n\t\tpt_access = pte_access;\n\t\t--walker->level;\n\t}\n\n\tif (unlikely(!present || eperm || rsvd_fault))\n\t\tgoto error;\n\n\tif (write_fault && unlikely(!is_dirty_gpte(pte))) {\n\t\tint ret;\n\n\t\ttrace_kvm_mmu_set_dirty_bit(table_gfn, index, sizeof(pte));\n\t\tret = FNAME(cmpxchg_gpte)(vcpu, mmu, table_gfn, index, pte,\n\t\t\t    pte|PT_DIRTY_MASK);\n\t\tif (ret < 0) {\n\t\t\tpresent = false;\n\t\t\tgoto error;\n\t\t} else if (ret)\n\t\t\tgoto walk;\n\n\t\tmark_page_dirty(vcpu->kvm, table_gfn);\n\t\tpte |= PT_DIRTY_MASK;\n\t\twalker->ptes[walker->level - 1] = pte;\n\t}\n\n\twalker->pt_access = pt_access;\n\twalker->pte_access = pte_access;\n\tpgprintk(\"%s: pte %llx pte_access %x pt_access %x\\n\",\n\t\t __func__, (u64)pte, pte_access, pt_access);\n\treturn 1;\n\nerror:\n\twalker->fault.vector = PF_VECTOR;\n\twalker->fault.error_code_valid = true;\n\twalker->fault.error_code = 0;\n\tif (present)\n\t\twalker->fault.error_code |= PFERR_PRESENT_MASK;\n\n\twalker->fault.error_code |= write_fault | user_fault;\n\n\tif (fetch_fault && mmu->nx)\n\t\twalker->fault.error_code |= PFERR_FETCH_MASK;\n\tif (rsvd_fault)\n\t\twalker->fault.error_code |= PFERR_RSVD_MASK;\n\n\twalker->fault.address = addr;\n\twalker->fault.nested_page_fault = mmu != vcpu->arch.walk_mmu;\n\n\ttrace_kvm_mmu_walker_error(walker->fault.error_code);\n\treturn 0;\n}\n\nstatic int FNAME(walk_addr)(struct guest_walker *walker,\n\t\t\t    struct kvm_vcpu *vcpu, gva_t addr, u32 access)\n{\n\treturn FNAME(walk_addr_generic)(walker, vcpu, &vcpu->arch.mmu, addr,\n\t\t\t\t\taccess);\n}\n\nstatic int FNAME(walk_addr_nested)(struct guest_walker *walker,\n\t\t\t\t   struct kvm_vcpu *vcpu, gva_t addr,\n\t\t\t\t   u32 access)\n{\n\treturn FNAME(walk_addr_generic)(walker, vcpu, &vcpu->arch.nested_mmu,\n\t\t\t\t\taddr, access);\n}\n\nstatic bool FNAME(prefetch_invalid_gpte)(struct kvm_vcpu *vcpu,\n\t\t\t\t    struct kvm_mmu_page *sp, u64 *spte,\n\t\t\t\t    pt_element_t gpte)\n{\n\tu64 nonpresent = shadow_trap_nonpresent_pte;\n\n\tif (is_rsvd_bits_set(&vcpu->arch.mmu, gpte, PT_PAGE_TABLE_LEVEL))\n\t\tgoto no_present;\n\n\tif (!is_present_gpte(gpte)) {\n\t\tif (!sp->unsync)\n\t\t\tnonpresent = shadow_notrap_nonpresent_pte;\n\t\tgoto no_present;\n\t}\n\n\tif (!(gpte & PT_ACCESSED_MASK))\n\t\tgoto no_present;\n\n\treturn false;\n\nno_present:\n\tdrop_spte(vcpu->kvm, spte, nonpresent);\n\treturn true;\n}\n\nstatic void FNAME(update_pte)(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,\n\t\t\t      u64 *spte, const void *pte)\n{\n\tpt_element_t gpte;\n\tunsigned pte_access;\n\tpfn_t pfn;\n\n\tgpte = *(const pt_element_t *)pte;\n\tif (FNAME(prefetch_invalid_gpte)(vcpu, sp, spte, gpte))\n\t\treturn;\n\n\tpgprintk(\"%s: gpte %llx spte %p\\n\", __func__, (u64)gpte, spte);\n\tpte_access = sp->role.access & FNAME(gpte_access)(vcpu, gpte);\n\tpfn = gfn_to_pfn_atomic(vcpu->kvm, gpte_to_gfn(gpte));\n\tif (is_error_pfn(pfn)) {\n\t\tkvm_release_pfn_clean(pfn);\n\t\treturn;\n\t}\n\n\t/*\n\t * we call mmu_set_spte() with host_writable = true because that\n\t * vcpu->arch.update_pte.pfn was fetched from get_user_pages(write = 1).\n\t */\n\tmmu_set_spte(vcpu, spte, sp->role.access, pte_access, 0, 0,\n\t\t     is_dirty_gpte(gpte), NULL, PT_PAGE_TABLE_LEVEL,\n\t\t     gpte_to_gfn(gpte), pfn, true, true);\n}\n\nstatic bool FNAME(gpte_changed)(struct kvm_vcpu *vcpu,\n\t\t\t\tstruct guest_walker *gw, int level)\n{\n\tpt_element_t curr_pte;\n\tgpa_t base_gpa, pte_gpa = gw->pte_gpa[level - 1];\n\tu64 mask;\n\tint r, index;\n\n\tif (level == PT_PAGE_TABLE_LEVEL) {\n\t\tmask = PTE_PREFETCH_NUM * sizeof(pt_element_t) - 1;\n\t\tbase_gpa = pte_gpa & ~mask;\n\t\tindex = (pte_gpa - base_gpa) / sizeof(pt_element_t);\n\n\t\tr = kvm_read_guest_atomic(vcpu->kvm, base_gpa,\n\t\t\t\tgw->prefetch_ptes, sizeof(gw->prefetch_ptes));\n\t\tcurr_pte = gw->prefetch_ptes[index];\n\t} else\n\t\tr = kvm_read_guest_atomic(vcpu->kvm, pte_gpa,\n\t\t\t\t  &curr_pte, sizeof(curr_pte));\n\n\treturn r || curr_pte != gw->ptes[level - 1];\n}\n\nstatic void FNAME(pte_prefetch)(struct kvm_vcpu *vcpu, struct guest_walker *gw,\n\t\t\t\tu64 *sptep)\n{\n\tstruct kvm_mmu_page *sp;\n\tpt_element_t *gptep = gw->prefetch_ptes;\n\tu64 *spte;\n\tint i;\n\n\tsp = page_header(__pa(sptep));\n\n\tif (sp->role.level > PT_PAGE_TABLE_LEVEL)\n\t\treturn;\n\n\tif (sp->role.direct)\n\t\treturn __direct_pte_prefetch(vcpu, sp, sptep);\n\n\ti = (sptep - sp->spt) & ~(PTE_PREFETCH_NUM - 1);\n\tspte = sp->spt + i;\n\n\tfor (i = 0; i < PTE_PREFETCH_NUM; i++, spte++) {\n\t\tpt_element_t gpte;\n\t\tunsigned pte_access;\n\t\tgfn_t gfn;\n\t\tpfn_t pfn;\n\t\tbool dirty;\n\n\t\tif (spte == sptep)\n\t\t\tcontinue;\n\n\t\tif (*spte != shadow_trap_nonpresent_pte)\n\t\t\tcontinue;\n\n\t\tgpte = gptep[i];\n\n\t\tif (FNAME(prefetch_invalid_gpte)(vcpu, sp, spte, gpte))\n\t\t\tcontinue;\n\n\t\tpte_access = sp->role.access & FNAME(gpte_access)(vcpu, gpte);\n\t\tgfn = gpte_to_gfn(gpte);\n\t\tdirty = is_dirty_gpte(gpte);\n\t\tpfn = pte_prefetch_gfn_to_pfn(vcpu, gfn,\n\t\t\t\t      (pte_access & ACC_WRITE_MASK) && dirty);\n\t\tif (is_error_pfn(pfn)) {\n\t\t\tkvm_release_pfn_clean(pfn);\n\t\t\tbreak;\n\t\t}\n\n\t\tmmu_set_spte(vcpu, spte, sp->role.access, pte_access, 0, 0,\n\t\t\t     dirty, NULL, PT_PAGE_TABLE_LEVEL, gfn,\n\t\t\t     pfn, true, true);\n\t}\n}\n\n/*\n * Fetch a shadow pte for a specific level in the paging hierarchy.\n */\nstatic u64 *FNAME(fetch)(struct kvm_vcpu *vcpu, gva_t addr,\n\t\t\t struct guest_walker *gw,\n\t\t\t int user_fault, int write_fault, int hlevel,\n\t\t\t int *ptwrite, pfn_t pfn, bool map_writable,\n\t\t\t bool prefault)\n{\n\tunsigned access = gw->pt_access;\n\tstruct kvm_mmu_page *sp = NULL;\n\tbool dirty = is_dirty_gpte(gw->ptes[gw->level - 1]);\n\tint top_level;\n\tunsigned direct_access;\n\tstruct kvm_shadow_walk_iterator it;\n\n\tif (!is_present_gpte(gw->ptes[gw->level - 1]))\n\t\treturn NULL;\n\n\tdirect_access = gw->pt_access & gw->pte_access;\n\tif (!dirty)\n\t\tdirect_access &= ~ACC_WRITE_MASK;\n\n\ttop_level = vcpu->arch.mmu.root_level;\n\tif (top_level == PT32E_ROOT_LEVEL)\n\t\ttop_level = PT32_ROOT_LEVEL;\n\t/*\n\t * Verify that the top-level gpte is still there.  Since the page\n\t * is a root page, it is either write protected (and cannot be\n\t * changed from now on) or it is invalid (in which case, we don't\n\t * really care if it changes underneath us after this point).\n\t */\n\tif (FNAME(gpte_changed)(vcpu, gw, top_level))\n\t\tgoto out_gpte_changed;\n\n\tfor (shadow_walk_init(&it, vcpu, addr);\n\t     shadow_walk_okay(&it) && it.level > gw->level;\n\t     shadow_walk_next(&it)) {\n\t\tgfn_t table_gfn;\n\n\t\tdrop_large_spte(vcpu, it.sptep);\n\n\t\tsp = NULL;\n\t\tif (!is_shadow_present_pte(*it.sptep)) {\n\t\t\ttable_gfn = gw->table_gfn[it.level - 2];\n\t\t\tsp = kvm_mmu_get_page(vcpu, table_gfn, addr, it.level-1,\n\t\t\t\t\t      false, access, it.sptep);\n\t\t}\n\n\t\t/*\n\t\t * Verify that the gpte in the page we've just write\n\t\t * protected is still there.\n\t\t */\n\t\tif (FNAME(gpte_changed)(vcpu, gw, it.level - 1))\n\t\t\tgoto out_gpte_changed;\n\n\t\tif (sp)\n\t\t\tlink_shadow_page(it.sptep, sp);\n\t}\n\n\tfor (;\n\t     shadow_walk_okay(&it) && it.level > hlevel;\n\t     shadow_walk_next(&it)) {\n\t\tgfn_t direct_gfn;\n\n\t\tvalidate_direct_spte(vcpu, it.sptep, direct_access);\n\n\t\tdrop_large_spte(vcpu, it.sptep);\n\n\t\tif (is_shadow_present_pte(*it.sptep))\n\t\t\tcontinue;\n\n\t\tdirect_gfn = gw->gfn & ~(KVM_PAGES_PER_HPAGE(it.level) - 1);\n\n\t\tsp = kvm_mmu_get_page(vcpu, direct_gfn, addr, it.level-1,\n\t\t\t\t      true, direct_access, it.sptep);\n\t\tlink_shadow_page(it.sptep, sp);\n\t}\n\n\tmmu_set_spte(vcpu, it.sptep, access, gw->pte_access & access,\n\t\t     user_fault, write_fault, dirty, ptwrite, it.level,\n\t\t     gw->gfn, pfn, prefault, map_writable);\n\tFNAME(pte_prefetch)(vcpu, gw, it.sptep);\n\n\treturn it.sptep;\n\nout_gpte_changed:\n\tif (sp)\n\t\tkvm_mmu_put_page(sp, it.sptep);\n\tkvm_release_pfn_clean(pfn);\n\treturn NULL;\n}\n\n/*\n * Page fault handler.  There are several causes for a page fault:\n *   - there is no shadow pte for the guest pte\n *   - write access through a shadow pte marked read only so that we can set\n *     the dirty bit\n *   - write access to a shadow pte marked read only so we can update the page\n *     dirty bitmap, when userspace requests it\n *   - mmio access; in this case we will never install a present shadow pte\n *   - normal guest page fault due to the guest pte marked not present, not\n *     writable, or not executable\n *\n *  Returns: 1 if we need to emulate the instruction, 0 otherwise, or\n *           a negative value on error.\n */\nstatic int FNAME(page_fault)(struct kvm_vcpu *vcpu, gva_t addr, u32 error_code,\n\t\t\t     bool prefault)\n{\n\tint write_fault = error_code & PFERR_WRITE_MASK;\n\tint user_fault = error_code & PFERR_USER_MASK;\n\tstruct guest_walker walker;\n\tu64 *sptep;\n\tint write_pt = 0;\n\tint r;\n\tpfn_t pfn;\n\tint level = PT_PAGE_TABLE_LEVEL;\n\tint force_pt_level;\n\tunsigned long mmu_seq;\n\tbool map_writable;\n\n\tpgprintk(\"%s: addr %lx err %x\\n\", __func__, addr, error_code);\n\n\tr = mmu_topup_memory_caches(vcpu);\n\tif (r)\n\t\treturn r;\n\n\t/*\n\t * Look up the guest pte for the faulting address.\n\t */\n\tr = FNAME(walk_addr)(&walker, vcpu, addr, error_code);\n\n\t/*\n\t * The page is not mapped by the guest.  Let the guest handle it.\n\t */\n\tif (!r) {\n\t\tpgprintk(\"%s: guest page fault\\n\", __func__);\n\t\tif (!prefault) {\n\t\t\tinject_page_fault(vcpu, &walker.fault);\n\t\t\t/* reset fork detector */\n\t\t\tvcpu->arch.last_pt_write_count = 0;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tif (walker.level >= PT_DIRECTORY_LEVEL)\n\t\tforce_pt_level = mapping_level_dirty_bitmap(vcpu, walker.gfn);\n\telse\n\t\tforce_pt_level = 1;\n\tif (!force_pt_level) {\n\t\tlevel = min(walker.level, mapping_level(vcpu, walker.gfn));\n\t\twalker.gfn = walker.gfn & ~(KVM_PAGES_PER_HPAGE(level) - 1);\n\t}\n\n\tmmu_seq = vcpu->kvm->mmu_notifier_seq;\n\tsmp_rmb();\n\n\tif (try_async_pf(vcpu, prefault, walker.gfn, addr, &pfn, write_fault,\n\t\t\t &map_writable))\n\t\treturn 0;\n\n\t/* mmio */\n\tif (is_error_pfn(pfn))\n\t\treturn kvm_handle_bad_page(vcpu->kvm, walker.gfn, pfn);\n\n\tspin_lock(&vcpu->kvm->mmu_lock);\n\tif (mmu_notifier_retry(vcpu, mmu_seq))\n\t\tgoto out_unlock;\n\n\ttrace_kvm_mmu_audit(vcpu, AUDIT_PRE_PAGE_FAULT);\n\tkvm_mmu_free_some_pages(vcpu);\n\tif (!force_pt_level)\n\t\ttransparent_hugepage_adjust(vcpu, &walker.gfn, &pfn, &level);\n\tsptep = FNAME(fetch)(vcpu, addr, &walker, user_fault, write_fault,\n\t\t\t     level, &write_pt, pfn, map_writable, prefault);\n\t(void)sptep;\n\tpgprintk(\"%s: shadow pte %p %llx ptwrite %d\\n\", __func__,\n\t\t sptep, *sptep, write_pt);\n\n\tif (!write_pt)\n\t\tvcpu->arch.last_pt_write_count = 0; /* reset fork detector */\n\n\t++vcpu->stat.pf_fixed;\n\ttrace_kvm_mmu_audit(vcpu, AUDIT_POST_PAGE_FAULT);\n\tspin_unlock(&vcpu->kvm->mmu_lock);\n\n\treturn write_pt;\n\nout_unlock:\n\tspin_unlock(&vcpu->kvm->mmu_lock);\n\tkvm_release_pfn_clean(pfn);\n\treturn 0;\n}\n\nstatic void FNAME(invlpg)(struct kvm_vcpu *vcpu, gva_t gva)\n{\n\tstruct kvm_shadow_walk_iterator iterator;\n\tstruct kvm_mmu_page *sp;\n\tgpa_t pte_gpa = -1;\n\tint level;\n\tu64 *sptep;\n\tint need_flush = 0;\n\n\tspin_lock(&vcpu->kvm->mmu_lock);\n\n\tfor_each_shadow_entry(vcpu, gva, iterator) {\n\t\tlevel = iterator.level;\n\t\tsptep = iterator.sptep;\n\n\t\tsp = page_header(__pa(sptep));\n\t\tif (is_last_spte(*sptep, level)) {\n\t\t\tint offset, shift;\n\n\t\t\tif (!sp->unsync)\n\t\t\t\tbreak;\n\n\t\t\tshift = PAGE_SHIFT -\n\t\t\t\t  (PT_LEVEL_BITS - PT64_LEVEL_BITS) * level;\n\t\t\toffset = sp->role.quadrant << shift;\n\n\t\t\tpte_gpa = (sp->gfn << PAGE_SHIFT) + offset;\n\t\t\tpte_gpa += (sptep - sp->spt) * sizeof(pt_element_t);\n\n\t\t\tif (is_shadow_present_pte(*sptep)) {\n\t\t\t\tif (is_large_pte(*sptep))\n\t\t\t\t\t--vcpu->kvm->stat.lpages;\n\t\t\t\tdrop_spte(vcpu->kvm, sptep,\n\t\t\t\t\t  shadow_trap_nonpresent_pte);\n\t\t\t\tneed_flush = 1;\n\t\t\t} else\n\t\t\t\t__set_spte(sptep, shadow_trap_nonpresent_pte);\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!is_shadow_present_pte(*sptep) || !sp->unsync_children)\n\t\t\tbreak;\n\t}\n\n\tif (need_flush)\n\t\tkvm_flush_remote_tlbs(vcpu->kvm);\n\n\tatomic_inc(&vcpu->kvm->arch.invlpg_counter);\n\n\tspin_unlock(&vcpu->kvm->mmu_lock);\n\n\tif (pte_gpa == -1)\n\t\treturn;\n\n\tif (mmu_topup_memory_caches(vcpu))\n\t\treturn;\n\tkvm_mmu_pte_write(vcpu, pte_gpa, NULL, sizeof(pt_element_t), 0);\n}\n\nstatic gpa_t FNAME(gva_to_gpa)(struct kvm_vcpu *vcpu, gva_t vaddr, u32 access,\n\t\t\t       struct x86_exception *exception)\n{\n\tstruct guest_walker walker;\n\tgpa_t gpa = UNMAPPED_GVA;\n\tint r;\n\n\tr = FNAME(walk_addr)(&walker, vcpu, vaddr, access);\n\n\tif (r) {\n\t\tgpa = gfn_to_gpa(walker.gfn);\n\t\tgpa |= vaddr & ~PAGE_MASK;\n\t} else if (exception)\n\t\t*exception = walker.fault;\n\n\treturn gpa;\n}\n\nstatic gpa_t FNAME(gva_to_gpa_nested)(struct kvm_vcpu *vcpu, gva_t vaddr,\n\t\t\t\t      u32 access,\n\t\t\t\t      struct x86_exception *exception)\n{\n\tstruct guest_walker walker;\n\tgpa_t gpa = UNMAPPED_GVA;\n\tint r;\n\n\tr = FNAME(walk_addr_nested)(&walker, vcpu, vaddr, access);\n\n\tif (r) {\n\t\tgpa = gfn_to_gpa(walker.gfn);\n\t\tgpa |= vaddr & ~PAGE_MASK;\n\t} else if (exception)\n\t\t*exception = walker.fault;\n\n\treturn gpa;\n}\n\nstatic void FNAME(prefetch_page)(struct kvm_vcpu *vcpu,\n\t\t\t\t struct kvm_mmu_page *sp)\n{\n\tint i, j, offset, r;\n\tpt_element_t pt[256 / sizeof(pt_element_t)];\n\tgpa_t pte_gpa;\n\n\tif (sp->role.direct\n\t    || (PTTYPE == 32 && sp->role.level > PT_PAGE_TABLE_LEVEL)) {\n\t\tnonpaging_prefetch_page(vcpu, sp);\n\t\treturn;\n\t}\n\n\tpte_gpa = gfn_to_gpa(sp->gfn);\n\tif (PTTYPE == 32) {\n\t\toffset = sp->role.quadrant << PT64_LEVEL_BITS;\n\t\tpte_gpa += offset * sizeof(pt_element_t);\n\t}\n\n\tfor (i = 0; i < PT64_ENT_PER_PAGE; i += ARRAY_SIZE(pt)) {\n\t\tr = kvm_read_guest_atomic(vcpu->kvm, pte_gpa, pt, sizeof pt);\n\t\tpte_gpa += ARRAY_SIZE(pt) * sizeof(pt_element_t);\n\t\tfor (j = 0; j < ARRAY_SIZE(pt); ++j)\n\t\t\tif (r || is_present_gpte(pt[j]))\n\t\t\t\tsp->spt[i+j] = shadow_trap_nonpresent_pte;\n\t\t\telse\n\t\t\t\tsp->spt[i+j] = shadow_notrap_nonpresent_pte;\n\t}\n}\n\n/*\n * Using the cached information from sp->gfns is safe because:\n * - The spte has a reference to the struct page, so the pfn for a given gfn\n *   can't change unless all sptes pointing to it are nuked first.\n *\n * Note:\n *   We should flush all tlbs if spte is dropped even though guest is\n *   responsible for it. Since if we don't, kvm_mmu_notifier_invalidate_page\n *   and kvm_mmu_notifier_invalidate_range_start detect the mapping page isn't\n *   used by guest then tlbs are not flushed, so guest is allowed to access the\n *   freed pages.\n *   And we increase kvm->tlbs_dirty to delay tlbs flush in this case.\n */\nstatic int FNAME(sync_page)(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp)\n{\n\tint i, offset, nr_present;\n\tbool host_writable;\n\tgpa_t first_pte_gpa;\n\n\toffset = nr_present = 0;\n\n\t/* direct kvm_mmu_page can not be unsync. */\n\tBUG_ON(sp->role.direct);\n\n\tif (PTTYPE == 32)\n\t\toffset = sp->role.quadrant << PT64_LEVEL_BITS;\n\n\tfirst_pte_gpa = gfn_to_gpa(sp->gfn) + offset * sizeof(pt_element_t);\n\n\tfor (i = 0; i < PT64_ENT_PER_PAGE; i++) {\n\t\tunsigned pte_access;\n\t\tpt_element_t gpte;\n\t\tgpa_t pte_gpa;\n\t\tgfn_t gfn;\n\n\t\tif (!is_shadow_present_pte(sp->spt[i]))\n\t\t\tcontinue;\n\n\t\tpte_gpa = first_pte_gpa + i * sizeof(pt_element_t);\n\n\t\tif (kvm_read_guest_atomic(vcpu->kvm, pte_gpa, &gpte,\n\t\t\t\t\t  sizeof(pt_element_t)))\n\t\t\treturn -EINVAL;\n\n\t\tgfn = gpte_to_gfn(gpte);\n\n\t\tif (FNAME(prefetch_invalid_gpte)(vcpu, sp, &sp->spt[i], gpte)) {\n\t\t\tvcpu->kvm->tlbs_dirty++;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (gfn != sp->gfns[i]) {\n\t\t\tdrop_spte(vcpu->kvm, &sp->spt[i],\n\t\t\t\t      shadow_trap_nonpresent_pte);\n\t\t\tvcpu->kvm->tlbs_dirty++;\n\t\t\tcontinue;\n\t\t}\n\n\t\tnr_present++;\n\t\tpte_access = sp->role.access & FNAME(gpte_access)(vcpu, gpte);\n\t\thost_writable = sp->spt[i] & SPTE_HOST_WRITEABLE;\n\n\t\tset_spte(vcpu, &sp->spt[i], pte_access, 0, 0,\n\t\t\t is_dirty_gpte(gpte), PT_PAGE_TABLE_LEVEL, gfn,\n\t\t\t spte_to_pfn(sp->spt[i]), true, false,\n\t\t\t host_writable);\n\t}\n\n\treturn !nr_present;\n}\n\n#undef pt_element_t\n#undef guest_walker\n#undef FNAME\n#undef PT_BASE_ADDR_MASK\n#undef PT_INDEX\n#undef PT_LVL_ADDR_MASK\n#undef PT_LVL_OFFSET_MASK\n#undef PT_LEVEL_BITS\n#undef PT_MAX_FULL_LEVELS\n#undef gpte_to_gfn\n#undef gpte_to_gfn_lvl\n#undef CMPXCHG\n", "/*\n * Kernel-based Virtual Machine driver for Linux\n *\n * This module enables machines with Intel VT-x extensions to run virtual\n * machines without emulation or binary translation.\n *\n * Copyright (C) 2006 Qumranet, Inc.\n * Copyright 2010 Red Hat, Inc. and/or its affiliates.\n *\n * Authors:\n *   Avi Kivity   <avi@qumranet.com>\n *   Yaniv Kamay  <yaniv@qumranet.com>\n *\n * This work is licensed under the terms of the GNU GPL, version 2.  See\n * the COPYING file in the top-level directory.\n *\n */\n\n#include \"iodev.h\"\n\n#include <linux/kvm_host.h>\n#include <linux/kvm.h>\n#include <linux/module.h>\n#include <linux/errno.h>\n#include <linux/percpu.h>\n#include <linux/mm.h>\n#include <linux/miscdevice.h>\n#include <linux/vmalloc.h>\n#include <linux/reboot.h>\n#include <linux/debugfs.h>\n#include <linux/highmem.h>\n#include <linux/file.h>\n#include <linux/syscore_ops.h>\n#include <linux/cpu.h>\n#include <linux/sched.h>\n#include <linux/cpumask.h>\n#include <linux/smp.h>\n#include <linux/anon_inodes.h>\n#include <linux/profile.h>\n#include <linux/kvm_para.h>\n#include <linux/pagemap.h>\n#include <linux/mman.h>\n#include <linux/swap.h>\n#include <linux/bitops.h>\n#include <linux/spinlock.h>\n#include <linux/compat.h>\n#include <linux/srcu.h>\n#include <linux/hugetlb.h>\n#include <linux/slab.h>\n\n#include <asm/processor.h>\n#include <asm/io.h>\n#include <asm/uaccess.h>\n#include <asm/pgtable.h>\n\n#include \"coalesced_mmio.h\"\n#include \"async_pf.h\"\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/kvm.h>\n\nMODULE_AUTHOR(\"Qumranet\");\nMODULE_LICENSE(\"GPL\");\n\n/*\n * Ordering of locks:\n *\n * \t\tkvm->lock --> kvm->slots_lock --> kvm->irq_lock\n */\n\nDEFINE_RAW_SPINLOCK(kvm_lock);\nLIST_HEAD(vm_list);\n\nstatic cpumask_var_t cpus_hardware_enabled;\nstatic int kvm_usage_count = 0;\nstatic atomic_t hardware_enable_failed;\n\nstruct kmem_cache *kvm_vcpu_cache;\nEXPORT_SYMBOL_GPL(kvm_vcpu_cache);\n\nstatic __read_mostly struct preempt_ops kvm_preempt_ops;\n\nstruct dentry *kvm_debugfs_dir;\n\nstatic long kvm_vcpu_ioctl(struct file *file, unsigned int ioctl,\n\t\t\t   unsigned long arg);\nstatic int hardware_enable_all(void);\nstatic void hardware_disable_all(void);\n\nstatic void kvm_io_bus_destroy(struct kvm_io_bus *bus);\n\nbool kvm_rebooting;\nEXPORT_SYMBOL_GPL(kvm_rebooting);\n\nstatic bool largepages_enabled = true;\n\nstatic struct page *hwpoison_page;\nstatic pfn_t hwpoison_pfn;\n\nstatic struct page *fault_page;\nstatic pfn_t fault_pfn;\n\ninline int kvm_is_mmio_pfn(pfn_t pfn)\n{\n\tif (pfn_valid(pfn)) {\n\t\tint reserved;\n\t\tstruct page *tail = pfn_to_page(pfn);\n\t\tstruct page *head = compound_trans_head(tail);\n\t\treserved = PageReserved(head);\n\t\tif (head != tail) {\n\t\t\t/*\n\t\t\t * \"head\" is not a dangling pointer\n\t\t\t * (compound_trans_head takes care of that)\n\t\t\t * but the hugepage may have been splitted\n\t\t\t * from under us (and we may not hold a\n\t\t\t * reference count on the head page so it can\n\t\t\t * be reused before we run PageReferenced), so\n\t\t\t * we've to check PageTail before returning\n\t\t\t * what we just read.\n\t\t\t */\n\t\t\tsmp_rmb();\n\t\t\tif (PageTail(tail))\n\t\t\t\treturn reserved;\n\t\t}\n\t\treturn PageReserved(tail);\n\t}\n\n\treturn true;\n}\n\n/*\n * Switches to specified vcpu, until a matching vcpu_put()\n */\nvoid vcpu_load(struct kvm_vcpu *vcpu)\n{\n\tint cpu;\n\n\tmutex_lock(&vcpu->mutex);\n\tif (unlikely(vcpu->pid != current->pids[PIDTYPE_PID].pid)) {\n\t\t/* The thread running this VCPU changed. */\n\t\tstruct pid *oldpid = vcpu->pid;\n\t\tstruct pid *newpid = get_task_pid(current, PIDTYPE_PID);\n\t\trcu_assign_pointer(vcpu->pid, newpid);\n\t\tsynchronize_rcu();\n\t\tput_pid(oldpid);\n\t}\n\tcpu = get_cpu();\n\tpreempt_notifier_register(&vcpu->preempt_notifier);\n\tkvm_arch_vcpu_load(vcpu, cpu);\n\tput_cpu();\n}\n\nvoid vcpu_put(struct kvm_vcpu *vcpu)\n{\n\tpreempt_disable();\n\tkvm_arch_vcpu_put(vcpu);\n\tpreempt_notifier_unregister(&vcpu->preempt_notifier);\n\tpreempt_enable();\n\tmutex_unlock(&vcpu->mutex);\n}\n\nstatic void ack_flush(void *_completed)\n{\n}\n\nstatic bool make_all_cpus_request(struct kvm *kvm, unsigned int req)\n{\n\tint i, cpu, me;\n\tcpumask_var_t cpus;\n\tbool called = true;\n\tstruct kvm_vcpu *vcpu;\n\n\tzalloc_cpumask_var(&cpus, GFP_ATOMIC);\n\n\tme = get_cpu();\n\tkvm_for_each_vcpu(i, vcpu, kvm) {\n\t\tkvm_make_request(req, vcpu);\n\t\tcpu = vcpu->cpu;\n\n\t\t/* Set ->requests bit before we read ->mode */\n\t\tsmp_mb();\n\n\t\tif (cpus != NULL && cpu != -1 && cpu != me &&\n\t\t      kvm_vcpu_exiting_guest_mode(vcpu) != OUTSIDE_GUEST_MODE)\n\t\t\tcpumask_set_cpu(cpu, cpus);\n\t}\n\tif (unlikely(cpus == NULL))\n\t\tsmp_call_function_many(cpu_online_mask, ack_flush, NULL, 1);\n\telse if (!cpumask_empty(cpus))\n\t\tsmp_call_function_many(cpus, ack_flush, NULL, 1);\n\telse\n\t\tcalled = false;\n\tput_cpu();\n\tfree_cpumask_var(cpus);\n\treturn called;\n}\n\nvoid kvm_flush_remote_tlbs(struct kvm *kvm)\n{\n\tint dirty_count = kvm->tlbs_dirty;\n\n\tsmp_mb();\n\tif (make_all_cpus_request(kvm, KVM_REQ_TLB_FLUSH))\n\t\t++kvm->stat.remote_tlb_flush;\n\tcmpxchg(&kvm->tlbs_dirty, dirty_count, 0);\n}\n\nvoid kvm_reload_remote_mmus(struct kvm *kvm)\n{\n\tmake_all_cpus_request(kvm, KVM_REQ_MMU_RELOAD);\n}\n\nint kvm_vcpu_init(struct kvm_vcpu *vcpu, struct kvm *kvm, unsigned id)\n{\n\tstruct page *page;\n\tint r;\n\n\tmutex_init(&vcpu->mutex);\n\tvcpu->cpu = -1;\n\tvcpu->kvm = kvm;\n\tvcpu->vcpu_id = id;\n\tvcpu->pid = NULL;\n\tinit_waitqueue_head(&vcpu->wq);\n\tkvm_async_pf_vcpu_init(vcpu);\n\n\tpage = alloc_page(GFP_KERNEL | __GFP_ZERO);\n\tif (!page) {\n\t\tr = -ENOMEM;\n\t\tgoto fail;\n\t}\n\tvcpu->run = page_address(page);\n\n\tr = kvm_arch_vcpu_init(vcpu);\n\tif (r < 0)\n\t\tgoto fail_free_run;\n\treturn 0;\n\nfail_free_run:\n\tfree_page((unsigned long)vcpu->run);\nfail:\n\treturn r;\n}\nEXPORT_SYMBOL_GPL(kvm_vcpu_init);\n\nvoid kvm_vcpu_uninit(struct kvm_vcpu *vcpu)\n{\n\tput_pid(vcpu->pid);\n\tkvm_arch_vcpu_uninit(vcpu);\n\tfree_page((unsigned long)vcpu->run);\n}\nEXPORT_SYMBOL_GPL(kvm_vcpu_uninit);\n\n#if defined(CONFIG_MMU_NOTIFIER) && defined(KVM_ARCH_WANT_MMU_NOTIFIER)\nstatic inline struct kvm *mmu_notifier_to_kvm(struct mmu_notifier *mn)\n{\n\treturn container_of(mn, struct kvm, mmu_notifier);\n}\n\nstatic void kvm_mmu_notifier_invalidate_page(struct mmu_notifier *mn,\n\t\t\t\t\t     struct mm_struct *mm,\n\t\t\t\t\t     unsigned long address)\n{\n\tstruct kvm *kvm = mmu_notifier_to_kvm(mn);\n\tint need_tlb_flush, idx;\n\n\t/*\n\t * When ->invalidate_page runs, the linux pte has been zapped\n\t * already but the page is still allocated until\n\t * ->invalidate_page returns. So if we increase the sequence\n\t * here the kvm page fault will notice if the spte can't be\n\t * established because the page is going to be freed. If\n\t * instead the kvm page fault establishes the spte before\n\t * ->invalidate_page runs, kvm_unmap_hva will release it\n\t * before returning.\n\t *\n\t * The sequence increase only need to be seen at spin_unlock\n\t * time, and not at spin_lock time.\n\t *\n\t * Increasing the sequence after the spin_unlock would be\n\t * unsafe because the kvm page fault could then establish the\n\t * pte after kvm_unmap_hva returned, without noticing the page\n\t * is going to be freed.\n\t */\n\tidx = srcu_read_lock(&kvm->srcu);\n\tspin_lock(&kvm->mmu_lock);\n\tkvm->mmu_notifier_seq++;\n\tneed_tlb_flush = kvm_unmap_hva(kvm, address) | kvm->tlbs_dirty;\n\tspin_unlock(&kvm->mmu_lock);\n\tsrcu_read_unlock(&kvm->srcu, idx);\n\n\t/* we've to flush the tlb before the pages can be freed */\n\tif (need_tlb_flush)\n\t\tkvm_flush_remote_tlbs(kvm);\n\n}\n\nstatic void kvm_mmu_notifier_change_pte(struct mmu_notifier *mn,\n\t\t\t\t\tstruct mm_struct *mm,\n\t\t\t\t\tunsigned long address,\n\t\t\t\t\tpte_t pte)\n{\n\tstruct kvm *kvm = mmu_notifier_to_kvm(mn);\n\tint idx;\n\n\tidx = srcu_read_lock(&kvm->srcu);\n\tspin_lock(&kvm->mmu_lock);\n\tkvm->mmu_notifier_seq++;\n\tkvm_set_spte_hva(kvm, address, pte);\n\tspin_unlock(&kvm->mmu_lock);\n\tsrcu_read_unlock(&kvm->srcu, idx);\n}\n\nstatic void kvm_mmu_notifier_invalidate_range_start(struct mmu_notifier *mn,\n\t\t\t\t\t\t    struct mm_struct *mm,\n\t\t\t\t\t\t    unsigned long start,\n\t\t\t\t\t\t    unsigned long end)\n{\n\tstruct kvm *kvm = mmu_notifier_to_kvm(mn);\n\tint need_tlb_flush = 0, idx;\n\n\tidx = srcu_read_lock(&kvm->srcu);\n\tspin_lock(&kvm->mmu_lock);\n\t/*\n\t * The count increase must become visible at unlock time as no\n\t * spte can be established without taking the mmu_lock and\n\t * count is also read inside the mmu_lock critical section.\n\t */\n\tkvm->mmu_notifier_count++;\n\tfor (; start < end; start += PAGE_SIZE)\n\t\tneed_tlb_flush |= kvm_unmap_hva(kvm, start);\n\tneed_tlb_flush |= kvm->tlbs_dirty;\n\tspin_unlock(&kvm->mmu_lock);\n\tsrcu_read_unlock(&kvm->srcu, idx);\n\n\t/* we've to flush the tlb before the pages can be freed */\n\tif (need_tlb_flush)\n\t\tkvm_flush_remote_tlbs(kvm);\n}\n\nstatic void kvm_mmu_notifier_invalidate_range_end(struct mmu_notifier *mn,\n\t\t\t\t\t\t  struct mm_struct *mm,\n\t\t\t\t\t\t  unsigned long start,\n\t\t\t\t\t\t  unsigned long end)\n{\n\tstruct kvm *kvm = mmu_notifier_to_kvm(mn);\n\n\tspin_lock(&kvm->mmu_lock);\n\t/*\n\t * This sequence increase will notify the kvm page fault that\n\t * the page that is going to be mapped in the spte could have\n\t * been freed.\n\t */\n\tkvm->mmu_notifier_seq++;\n\t/*\n\t * The above sequence increase must be visible before the\n\t * below count decrease but both values are read by the kvm\n\t * page fault under mmu_lock spinlock so we don't need to add\n\t * a smb_wmb() here in between the two.\n\t */\n\tkvm->mmu_notifier_count--;\n\tspin_unlock(&kvm->mmu_lock);\n\n\tBUG_ON(kvm->mmu_notifier_count < 0);\n}\n\nstatic int kvm_mmu_notifier_clear_flush_young(struct mmu_notifier *mn,\n\t\t\t\t\t      struct mm_struct *mm,\n\t\t\t\t\t      unsigned long address)\n{\n\tstruct kvm *kvm = mmu_notifier_to_kvm(mn);\n\tint young, idx;\n\n\tidx = srcu_read_lock(&kvm->srcu);\n\tspin_lock(&kvm->mmu_lock);\n\tyoung = kvm_age_hva(kvm, address);\n\tspin_unlock(&kvm->mmu_lock);\n\tsrcu_read_unlock(&kvm->srcu, idx);\n\n\tif (young)\n\t\tkvm_flush_remote_tlbs(kvm);\n\n\treturn young;\n}\n\nstatic int kvm_mmu_notifier_test_young(struct mmu_notifier *mn,\n\t\t\t\t       struct mm_struct *mm,\n\t\t\t\t       unsigned long address)\n{\n\tstruct kvm *kvm = mmu_notifier_to_kvm(mn);\n\tint young, idx;\n\n\tidx = srcu_read_lock(&kvm->srcu);\n\tspin_lock(&kvm->mmu_lock);\n\tyoung = kvm_test_age_hva(kvm, address);\n\tspin_unlock(&kvm->mmu_lock);\n\tsrcu_read_unlock(&kvm->srcu, idx);\n\n\treturn young;\n}\n\nstatic void kvm_mmu_notifier_release(struct mmu_notifier *mn,\n\t\t\t\t     struct mm_struct *mm)\n{\n\tstruct kvm *kvm = mmu_notifier_to_kvm(mn);\n\tint idx;\n\n\tidx = srcu_read_lock(&kvm->srcu);\n\tkvm_arch_flush_shadow(kvm);\n\tsrcu_read_unlock(&kvm->srcu, idx);\n}\n\nstatic const struct mmu_notifier_ops kvm_mmu_notifier_ops = {\n\t.invalidate_page\t= kvm_mmu_notifier_invalidate_page,\n\t.invalidate_range_start\t= kvm_mmu_notifier_invalidate_range_start,\n\t.invalidate_range_end\t= kvm_mmu_notifier_invalidate_range_end,\n\t.clear_flush_young\t= kvm_mmu_notifier_clear_flush_young,\n\t.test_young\t\t= kvm_mmu_notifier_test_young,\n\t.change_pte\t\t= kvm_mmu_notifier_change_pte,\n\t.release\t\t= kvm_mmu_notifier_release,\n};\n\nstatic int kvm_init_mmu_notifier(struct kvm *kvm)\n{\n\tkvm->mmu_notifier.ops = &kvm_mmu_notifier_ops;\n\treturn mmu_notifier_register(&kvm->mmu_notifier, current->mm);\n}\n\n#else  /* !(CONFIG_MMU_NOTIFIER && KVM_ARCH_WANT_MMU_NOTIFIER) */\n\nstatic int kvm_init_mmu_notifier(struct kvm *kvm)\n{\n\treturn 0;\n}\n\n#endif /* CONFIG_MMU_NOTIFIER && KVM_ARCH_WANT_MMU_NOTIFIER */\n\nstatic struct kvm *kvm_create_vm(void)\n{\n\tint r, i;\n\tstruct kvm *kvm = kvm_arch_alloc_vm();\n\n\tif (!kvm)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tr = kvm_arch_init_vm(kvm);\n\tif (r)\n\t\tgoto out_err_nodisable;\n\n\tr = hardware_enable_all();\n\tif (r)\n\t\tgoto out_err_nodisable;\n\n#ifdef CONFIG_HAVE_KVM_IRQCHIP\n\tINIT_HLIST_HEAD(&kvm->mask_notifier_list);\n\tINIT_HLIST_HEAD(&kvm->irq_ack_notifier_list);\n#endif\n\n\tr = -ENOMEM;\n\tkvm->memslots = kzalloc(sizeof(struct kvm_memslots), GFP_KERNEL);\n\tif (!kvm->memslots)\n\t\tgoto out_err_nosrcu;\n\tif (init_srcu_struct(&kvm->srcu))\n\t\tgoto out_err_nosrcu;\n\tfor (i = 0; i < KVM_NR_BUSES; i++) {\n\t\tkvm->buses[i] = kzalloc(sizeof(struct kvm_io_bus),\n\t\t\t\t\tGFP_KERNEL);\n\t\tif (!kvm->buses[i])\n\t\t\tgoto out_err;\n\t}\n\n\tr = kvm_init_mmu_notifier(kvm);\n\tif (r)\n\t\tgoto out_err;\n\n\tkvm->mm = current->mm;\n\tatomic_inc(&kvm->mm->mm_count);\n\tspin_lock_init(&kvm->mmu_lock);\n\tkvm_eventfd_init(kvm);\n\tmutex_init(&kvm->lock);\n\tmutex_init(&kvm->irq_lock);\n\tmutex_init(&kvm->slots_lock);\n\tatomic_set(&kvm->users_count, 1);\n\traw_spin_lock(&kvm_lock);\n\tlist_add(&kvm->vm_list, &vm_list);\n\traw_spin_unlock(&kvm_lock);\n\n\treturn kvm;\n\nout_err:\n\tcleanup_srcu_struct(&kvm->srcu);\nout_err_nosrcu:\n\thardware_disable_all();\nout_err_nodisable:\n\tfor (i = 0; i < KVM_NR_BUSES; i++)\n\t\tkfree(kvm->buses[i]);\n\tkfree(kvm->memslots);\n\tkvm_arch_free_vm(kvm);\n\treturn ERR_PTR(r);\n}\n\nstatic void kvm_destroy_dirty_bitmap(struct kvm_memory_slot *memslot)\n{\n\tif (!memslot->dirty_bitmap)\n\t\treturn;\n\n\tif (2 * kvm_dirty_bitmap_bytes(memslot) > PAGE_SIZE)\n\t\tvfree(memslot->dirty_bitmap_head);\n\telse\n\t\tkfree(memslot->dirty_bitmap_head);\n\n\tmemslot->dirty_bitmap = NULL;\n\tmemslot->dirty_bitmap_head = NULL;\n}\n\n/*\n * Free any memory in @free but not in @dont.\n */\nstatic void kvm_free_physmem_slot(struct kvm_memory_slot *free,\n\t\t\t\t  struct kvm_memory_slot *dont)\n{\n\tint i;\n\n\tif (!dont || free->rmap != dont->rmap)\n\t\tvfree(free->rmap);\n\n\tif (!dont || free->dirty_bitmap != dont->dirty_bitmap)\n\t\tkvm_destroy_dirty_bitmap(free);\n\n\n\tfor (i = 0; i < KVM_NR_PAGE_SIZES - 1; ++i) {\n\t\tif (!dont || free->lpage_info[i] != dont->lpage_info[i]) {\n\t\t\tvfree(free->lpage_info[i]);\n\t\t\tfree->lpage_info[i] = NULL;\n\t\t}\n\t}\n\n\tfree->npages = 0;\n\tfree->rmap = NULL;\n}\n\nvoid kvm_free_physmem(struct kvm *kvm)\n{\n\tint i;\n\tstruct kvm_memslots *slots = kvm->memslots;\n\n\tfor (i = 0; i < slots->nmemslots; ++i)\n\t\tkvm_free_physmem_slot(&slots->memslots[i], NULL);\n\n\tkfree(kvm->memslots);\n}\n\nstatic void kvm_destroy_vm(struct kvm *kvm)\n{\n\tint i;\n\tstruct mm_struct *mm = kvm->mm;\n\n\tkvm_arch_sync_events(kvm);\n\traw_spin_lock(&kvm_lock);\n\tlist_del(&kvm->vm_list);\n\traw_spin_unlock(&kvm_lock);\n\tkvm_free_irq_routing(kvm);\n\tfor (i = 0; i < KVM_NR_BUSES; i++)\n\t\tkvm_io_bus_destroy(kvm->buses[i]);\n\tkvm_coalesced_mmio_free(kvm);\n#if defined(CONFIG_MMU_NOTIFIER) && defined(KVM_ARCH_WANT_MMU_NOTIFIER)\n\tmmu_notifier_unregister(&kvm->mmu_notifier, kvm->mm);\n#else\n\tkvm_arch_flush_shadow(kvm);\n#endif\n\tkvm_arch_destroy_vm(kvm);\n\tkvm_free_physmem(kvm);\n\tcleanup_srcu_struct(&kvm->srcu);\n\tkvm_arch_free_vm(kvm);\n\thardware_disable_all();\n\tmmdrop(mm);\n}\n\nvoid kvm_get_kvm(struct kvm *kvm)\n{\n\tatomic_inc(&kvm->users_count);\n}\nEXPORT_SYMBOL_GPL(kvm_get_kvm);\n\nvoid kvm_put_kvm(struct kvm *kvm)\n{\n\tif (atomic_dec_and_test(&kvm->users_count))\n\t\tkvm_destroy_vm(kvm);\n}\nEXPORT_SYMBOL_GPL(kvm_put_kvm);\n\n\nstatic int kvm_vm_release(struct inode *inode, struct file *filp)\n{\n\tstruct kvm *kvm = filp->private_data;\n\n\tkvm_irqfd_release(kvm);\n\n\tkvm_put_kvm(kvm);\n\treturn 0;\n}\n\n#ifndef CONFIG_S390\n/*\n * Allocation size is twice as large as the actual dirty bitmap size.\n * This makes it possible to do double buffering: see x86's\n * kvm_vm_ioctl_get_dirty_log().\n */\nstatic int kvm_create_dirty_bitmap(struct kvm_memory_slot *memslot)\n{\n\tunsigned long dirty_bytes = 2 * kvm_dirty_bitmap_bytes(memslot);\n\n\tif (dirty_bytes > PAGE_SIZE)\n\t\tmemslot->dirty_bitmap = vzalloc(dirty_bytes);\n\telse\n\t\tmemslot->dirty_bitmap = kzalloc(dirty_bytes, GFP_KERNEL);\n\n\tif (!memslot->dirty_bitmap)\n\t\treturn -ENOMEM;\n\n\tmemslot->dirty_bitmap_head = memslot->dirty_bitmap;\n\treturn 0;\n}\n#endif /* !CONFIG_S390 */\n\n/*\n * Allocate some memory and give it an address in the guest physical address\n * space.\n *\n * Discontiguous memory is allowed, mostly for framebuffers.\n *\n * Must be called holding mmap_sem for write.\n */\nint __kvm_set_memory_region(struct kvm *kvm,\n\t\t\t    struct kvm_userspace_memory_region *mem,\n\t\t\t    int user_alloc)\n{\n\tint r;\n\tgfn_t base_gfn;\n\tunsigned long npages;\n\tunsigned long i;\n\tstruct kvm_memory_slot *memslot;\n\tstruct kvm_memory_slot old, new;\n\tstruct kvm_memslots *slots, *old_memslots;\n\n\tr = -EINVAL;\n\t/* General sanity checks */\n\tif (mem->memory_size & (PAGE_SIZE - 1))\n\t\tgoto out;\n\tif (mem->guest_phys_addr & (PAGE_SIZE - 1))\n\t\tgoto out;\n\t/* We can read the guest memory with __xxx_user() later on. */\n\tif (user_alloc &&\n\t    ((mem->userspace_addr & (PAGE_SIZE - 1)) ||\n\t     !access_ok(VERIFY_WRITE, mem->userspace_addr, mem->memory_size)))\n\t\tgoto out;\n\tif (mem->slot >= KVM_MEMORY_SLOTS + KVM_PRIVATE_MEM_SLOTS)\n\t\tgoto out;\n\tif (mem->guest_phys_addr + mem->memory_size < mem->guest_phys_addr)\n\t\tgoto out;\n\n\tmemslot = &kvm->memslots->memslots[mem->slot];\n\tbase_gfn = mem->guest_phys_addr >> PAGE_SHIFT;\n\tnpages = mem->memory_size >> PAGE_SHIFT;\n\n\tr = -EINVAL;\n\tif (npages > KVM_MEM_MAX_NR_PAGES)\n\t\tgoto out;\n\n\tif (!npages)\n\t\tmem->flags &= ~KVM_MEM_LOG_DIRTY_PAGES;\n\n\tnew = old = *memslot;\n\n\tnew.id = mem->slot;\n\tnew.base_gfn = base_gfn;\n\tnew.npages = npages;\n\tnew.flags = mem->flags;\n\n\t/* Disallow changing a memory slot's size. */\n\tr = -EINVAL;\n\tif (npages && old.npages && npages != old.npages)\n\t\tgoto out_free;\n\n\t/* Check for overlaps */\n\tr = -EEXIST;\n\tfor (i = 0; i < KVM_MEMORY_SLOTS; ++i) {\n\t\tstruct kvm_memory_slot *s = &kvm->memslots->memslots[i];\n\n\t\tif (s == memslot || !s->npages)\n\t\t\tcontinue;\n\t\tif (!((base_gfn + npages <= s->base_gfn) ||\n\t\t      (base_gfn >= s->base_gfn + s->npages)))\n\t\t\tgoto out_free;\n\t}\n\n\t/* Free page dirty bitmap if unneeded */\n\tif (!(new.flags & KVM_MEM_LOG_DIRTY_PAGES))\n\t\tnew.dirty_bitmap = NULL;\n\n\tr = -ENOMEM;\n\n\t/* Allocate if a slot is being created */\n#ifndef CONFIG_S390\n\tif (npages && !new.rmap) {\n\t\tnew.rmap = vzalloc(npages * sizeof(*new.rmap));\n\n\t\tif (!new.rmap)\n\t\t\tgoto out_free;\n\n\t\tnew.user_alloc = user_alloc;\n\t\tnew.userspace_addr = mem->userspace_addr;\n\t}\n\tif (!npages)\n\t\tgoto skip_lpage;\n\n\tfor (i = 0; i < KVM_NR_PAGE_SIZES - 1; ++i) {\n\t\tunsigned long ugfn;\n\t\tunsigned long j;\n\t\tint lpages;\n\t\tint level = i + 2;\n\n\t\t/* Avoid unused variable warning if no large pages */\n\t\t(void)level;\n\n\t\tif (new.lpage_info[i])\n\t\t\tcontinue;\n\n\t\tlpages = 1 + ((base_gfn + npages - 1)\n\t\t\t     >> KVM_HPAGE_GFN_SHIFT(level));\n\t\tlpages -= base_gfn >> KVM_HPAGE_GFN_SHIFT(level);\n\n\t\tnew.lpage_info[i] = vzalloc(lpages * sizeof(*new.lpage_info[i]));\n\n\t\tif (!new.lpage_info[i])\n\t\t\tgoto out_free;\n\n\t\tif (base_gfn & (KVM_PAGES_PER_HPAGE(level) - 1))\n\t\t\tnew.lpage_info[i][0].write_count = 1;\n\t\tif ((base_gfn+npages) & (KVM_PAGES_PER_HPAGE(level) - 1))\n\t\t\tnew.lpage_info[i][lpages - 1].write_count = 1;\n\t\tugfn = new.userspace_addr >> PAGE_SHIFT;\n\t\t/*\n\t\t * If the gfn and userspace address are not aligned wrt each\n\t\t * other, or if explicitly asked to, disable large page\n\t\t * support for this slot\n\t\t */\n\t\tif ((base_gfn ^ ugfn) & (KVM_PAGES_PER_HPAGE(level) - 1) ||\n\t\t    !largepages_enabled)\n\t\t\tfor (j = 0; j < lpages; ++j)\n\t\t\t\tnew.lpage_info[i][j].write_count = 1;\n\t}\n\nskip_lpage:\n\n\t/* Allocate page dirty bitmap if needed */\n\tif ((new.flags & KVM_MEM_LOG_DIRTY_PAGES) && !new.dirty_bitmap) {\n\t\tif (kvm_create_dirty_bitmap(&new) < 0)\n\t\t\tgoto out_free;\n\t\t/* destroy any largepage mappings for dirty tracking */\n\t}\n#else  /* not defined CONFIG_S390 */\n\tnew.user_alloc = user_alloc;\n\tif (user_alloc)\n\t\tnew.userspace_addr = mem->userspace_addr;\n#endif /* not defined CONFIG_S390 */\n\n\tif (!npages) {\n\t\tr = -ENOMEM;\n\t\tslots = kzalloc(sizeof(struct kvm_memslots), GFP_KERNEL);\n\t\tif (!slots)\n\t\t\tgoto out_free;\n\t\tmemcpy(slots, kvm->memslots, sizeof(struct kvm_memslots));\n\t\tif (mem->slot >= slots->nmemslots)\n\t\t\tslots->nmemslots = mem->slot + 1;\n\t\tslots->generation++;\n\t\tslots->memslots[mem->slot].flags |= KVM_MEMSLOT_INVALID;\n\n\t\told_memslots = kvm->memslots;\n\t\trcu_assign_pointer(kvm->memslots, slots);\n\t\tsynchronize_srcu_expedited(&kvm->srcu);\n\t\t/* From this point no new shadow pages pointing to a deleted\n\t\t * memslot will be created.\n\t\t *\n\t\t * validation of sp->gfn happens in:\n\t\t * \t- gfn_to_hva (kvm_read_guest, gfn_to_pfn)\n\t\t * \t- kvm_is_visible_gfn (mmu_check_roots)\n\t\t */\n\t\tkvm_arch_flush_shadow(kvm);\n\t\tkfree(old_memslots);\n\t}\n\n\tr = kvm_arch_prepare_memory_region(kvm, &new, old, mem, user_alloc);\n\tif (r)\n\t\tgoto out_free;\n\n\t/* map the pages in iommu page table */\n\tif (npages) {\n\t\tr = kvm_iommu_map_pages(kvm, &new);\n\t\tif (r)\n\t\t\tgoto out_free;\n\t}\n\n\tr = -ENOMEM;\n\tslots = kzalloc(sizeof(struct kvm_memslots), GFP_KERNEL);\n\tif (!slots)\n\t\tgoto out_free;\n\tmemcpy(slots, kvm->memslots, sizeof(struct kvm_memslots));\n\tif (mem->slot >= slots->nmemslots)\n\t\tslots->nmemslots = mem->slot + 1;\n\tslots->generation++;\n\n\t/* actual memory is freed via old in kvm_free_physmem_slot below */\n\tif (!npages) {\n\t\tnew.rmap = NULL;\n\t\tnew.dirty_bitmap = NULL;\n\t\tfor (i = 0; i < KVM_NR_PAGE_SIZES - 1; ++i)\n\t\t\tnew.lpage_info[i] = NULL;\n\t}\n\n\tslots->memslots[mem->slot] = new;\n\told_memslots = kvm->memslots;\n\trcu_assign_pointer(kvm->memslots, slots);\n\tsynchronize_srcu_expedited(&kvm->srcu);\n\n\tkvm_arch_commit_memory_region(kvm, mem, old, user_alloc);\n\n\tkvm_free_physmem_slot(&old, &new);\n\tkfree(old_memslots);\n\n\treturn 0;\n\nout_free:\n\tkvm_free_physmem_slot(&new, &old);\nout:\n\treturn r;\n\n}\nEXPORT_SYMBOL_GPL(__kvm_set_memory_region);\n\nint kvm_set_memory_region(struct kvm *kvm,\n\t\t\t  struct kvm_userspace_memory_region *mem,\n\t\t\t  int user_alloc)\n{\n\tint r;\n\n\tmutex_lock(&kvm->slots_lock);\n\tr = __kvm_set_memory_region(kvm, mem, user_alloc);\n\tmutex_unlock(&kvm->slots_lock);\n\treturn r;\n}\nEXPORT_SYMBOL_GPL(kvm_set_memory_region);\n\nint kvm_vm_ioctl_set_memory_region(struct kvm *kvm,\n\t\t\t\t   struct\n\t\t\t\t   kvm_userspace_memory_region *mem,\n\t\t\t\t   int user_alloc)\n{\n\tif (mem->slot >= KVM_MEMORY_SLOTS)\n\t\treturn -EINVAL;\n\treturn kvm_set_memory_region(kvm, mem, user_alloc);\n}\n\nint kvm_get_dirty_log(struct kvm *kvm,\n\t\t\tstruct kvm_dirty_log *log, int *is_dirty)\n{\n\tstruct kvm_memory_slot *memslot;\n\tint r, i;\n\tunsigned long n;\n\tunsigned long any = 0;\n\n\tr = -EINVAL;\n\tif (log->slot >= KVM_MEMORY_SLOTS)\n\t\tgoto out;\n\n\tmemslot = &kvm->memslots->memslots[log->slot];\n\tr = -ENOENT;\n\tif (!memslot->dirty_bitmap)\n\t\tgoto out;\n\n\tn = kvm_dirty_bitmap_bytes(memslot);\n\n\tfor (i = 0; !any && i < n/sizeof(long); ++i)\n\t\tany = memslot->dirty_bitmap[i];\n\n\tr = -EFAULT;\n\tif (copy_to_user(log->dirty_bitmap, memslot->dirty_bitmap, n))\n\t\tgoto out;\n\n\tif (any)\n\t\t*is_dirty = 1;\n\n\tr = 0;\nout:\n\treturn r;\n}\n\nvoid kvm_disable_largepages(void)\n{\n\tlargepages_enabled = false;\n}\nEXPORT_SYMBOL_GPL(kvm_disable_largepages);\n\nint is_error_page(struct page *page)\n{\n\treturn page == bad_page || page == hwpoison_page || page == fault_page;\n}\nEXPORT_SYMBOL_GPL(is_error_page);\n\nint is_error_pfn(pfn_t pfn)\n{\n\treturn pfn == bad_pfn || pfn == hwpoison_pfn || pfn == fault_pfn;\n}\nEXPORT_SYMBOL_GPL(is_error_pfn);\n\nint is_hwpoison_pfn(pfn_t pfn)\n{\n\treturn pfn == hwpoison_pfn;\n}\nEXPORT_SYMBOL_GPL(is_hwpoison_pfn);\n\nint is_fault_pfn(pfn_t pfn)\n{\n\treturn pfn == fault_pfn;\n}\nEXPORT_SYMBOL_GPL(is_fault_pfn);\n\nstatic inline unsigned long bad_hva(void)\n{\n\treturn PAGE_OFFSET;\n}\n\nint kvm_is_error_hva(unsigned long addr)\n{\n\treturn addr == bad_hva();\n}\nEXPORT_SYMBOL_GPL(kvm_is_error_hva);\n\nstatic struct kvm_memory_slot *__gfn_to_memslot(struct kvm_memslots *slots,\n\t\t\t\t\t\tgfn_t gfn)\n{\n\tint i;\n\n\tfor (i = 0; i < slots->nmemslots; ++i) {\n\t\tstruct kvm_memory_slot *memslot = &slots->memslots[i];\n\n\t\tif (gfn >= memslot->base_gfn\n\t\t    && gfn < memslot->base_gfn + memslot->npages)\n\t\t\treturn memslot;\n\t}\n\treturn NULL;\n}\n\nstruct kvm_memory_slot *gfn_to_memslot(struct kvm *kvm, gfn_t gfn)\n{\n\treturn __gfn_to_memslot(kvm_memslots(kvm), gfn);\n}\nEXPORT_SYMBOL_GPL(gfn_to_memslot);\n\nint kvm_is_visible_gfn(struct kvm *kvm, gfn_t gfn)\n{\n\tint i;\n\tstruct kvm_memslots *slots = kvm_memslots(kvm);\n\n\tfor (i = 0; i < KVM_MEMORY_SLOTS; ++i) {\n\t\tstruct kvm_memory_slot *memslot = &slots->memslots[i];\n\n\t\tif (memslot->flags & KVM_MEMSLOT_INVALID)\n\t\t\tcontinue;\n\n\t\tif (gfn >= memslot->base_gfn\n\t\t    && gfn < memslot->base_gfn + memslot->npages)\n\t\t\treturn 1;\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(kvm_is_visible_gfn);\n\nunsigned long kvm_host_page_size(struct kvm *kvm, gfn_t gfn)\n{\n\tstruct vm_area_struct *vma;\n\tunsigned long addr, size;\n\n\tsize = PAGE_SIZE;\n\n\taddr = gfn_to_hva(kvm, gfn);\n\tif (kvm_is_error_hva(addr))\n\t\treturn PAGE_SIZE;\n\n\tdown_read(&current->mm->mmap_sem);\n\tvma = find_vma(current->mm, addr);\n\tif (!vma)\n\t\tgoto out;\n\n\tsize = vma_kernel_pagesize(vma);\n\nout:\n\tup_read(&current->mm->mmap_sem);\n\n\treturn size;\n}\n\nstatic unsigned long gfn_to_hva_many(struct kvm_memory_slot *slot, gfn_t gfn,\n\t\t\t\t     gfn_t *nr_pages)\n{\n\tif (!slot || slot->flags & KVM_MEMSLOT_INVALID)\n\t\treturn bad_hva();\n\n\tif (nr_pages)\n\t\t*nr_pages = slot->npages - (gfn - slot->base_gfn);\n\n\treturn gfn_to_hva_memslot(slot, gfn);\n}\n\nunsigned long gfn_to_hva(struct kvm *kvm, gfn_t gfn)\n{\n\treturn gfn_to_hva_many(gfn_to_memslot(kvm, gfn), gfn, NULL);\n}\nEXPORT_SYMBOL_GPL(gfn_to_hva);\n\nstatic pfn_t get_fault_pfn(void)\n{\n\tget_page(fault_page);\n\treturn fault_pfn;\n}\n\nint get_user_page_nowait(struct task_struct *tsk, struct mm_struct *mm,\n\tunsigned long start, int write, struct page **page)\n{\n\tint flags = FOLL_TOUCH | FOLL_NOWAIT | FOLL_HWPOISON | FOLL_GET;\n\n\tif (write)\n\t\tflags |= FOLL_WRITE;\n\n\treturn __get_user_pages(tsk, mm, start, 1, flags, page, NULL, NULL);\n}\n\nstatic inline int check_user_page_hwpoison(unsigned long addr)\n{\n\tint rc, flags = FOLL_TOUCH | FOLL_HWPOISON | FOLL_WRITE;\n\n\trc = __get_user_pages(current, current->mm, addr, 1,\n\t\t\t      flags, NULL, NULL, NULL);\n\treturn rc == -EHWPOISON;\n}\n\nstatic pfn_t hva_to_pfn(struct kvm *kvm, unsigned long addr, bool atomic,\n\t\t\tbool *async, bool write_fault, bool *writable)\n{\n\tstruct page *page[1];\n\tint npages = 0;\n\tpfn_t pfn;\n\n\t/* we can do it either atomically or asynchronously, not both */\n\tBUG_ON(atomic && async);\n\n\tBUG_ON(!write_fault && !writable);\n\n\tif (writable)\n\t\t*writable = true;\n\n\tif (atomic || async)\n\t\tnpages = __get_user_pages_fast(addr, 1, 1, page);\n\n\tif (unlikely(npages != 1) && !atomic) {\n\t\tmight_sleep();\n\n\t\tif (writable)\n\t\t\t*writable = write_fault;\n\n\t\tif (async) {\n\t\t\tdown_read(&current->mm->mmap_sem);\n\t\t\tnpages = get_user_page_nowait(current, current->mm,\n\t\t\t\t\t\t     addr, write_fault, page);\n\t\t\tup_read(&current->mm->mmap_sem);\n\t\t} else\n\t\t\tnpages = get_user_pages_fast(addr, 1, write_fault,\n\t\t\t\t\t\t     page);\n\n\t\t/* map read fault as writable if possible */\n\t\tif (unlikely(!write_fault) && npages == 1) {\n\t\t\tstruct page *wpage[1];\n\n\t\t\tnpages = __get_user_pages_fast(addr, 1, 1, wpage);\n\t\t\tif (npages == 1) {\n\t\t\t\t*writable = true;\n\t\t\t\tput_page(page[0]);\n\t\t\t\tpage[0] = wpage[0];\n\t\t\t}\n\t\t\tnpages = 1;\n\t\t}\n\t}\n\n\tif (unlikely(npages != 1)) {\n\t\tstruct vm_area_struct *vma;\n\n\t\tif (atomic)\n\t\t\treturn get_fault_pfn();\n\n\t\tdown_read(&current->mm->mmap_sem);\n\t\tif (npages == -EHWPOISON ||\n\t\t\t(!async && check_user_page_hwpoison(addr))) {\n\t\t\tup_read(&current->mm->mmap_sem);\n\t\t\tget_page(hwpoison_page);\n\t\t\treturn page_to_pfn(hwpoison_page);\n\t\t}\n\n\t\tvma = find_vma_intersection(current->mm, addr, addr+1);\n\n\t\tif (vma == NULL)\n\t\t\tpfn = get_fault_pfn();\n\t\telse if ((vma->vm_flags & VM_PFNMAP)) {\n\t\t\tpfn = ((addr - vma->vm_start) >> PAGE_SHIFT) +\n\t\t\t\tvma->vm_pgoff;\n\t\t\tBUG_ON(!kvm_is_mmio_pfn(pfn));\n\t\t} else {\n\t\t\tif (async && (vma->vm_flags & VM_WRITE))\n\t\t\t\t*async = true;\n\t\t\tpfn = get_fault_pfn();\n\t\t}\n\t\tup_read(&current->mm->mmap_sem);\n\t} else\n\t\tpfn = page_to_pfn(page[0]);\n\n\treturn pfn;\n}\n\npfn_t hva_to_pfn_atomic(struct kvm *kvm, unsigned long addr)\n{\n\treturn hva_to_pfn(kvm, addr, true, NULL, true, NULL);\n}\nEXPORT_SYMBOL_GPL(hva_to_pfn_atomic);\n\nstatic pfn_t __gfn_to_pfn(struct kvm *kvm, gfn_t gfn, bool atomic, bool *async,\n\t\t\t  bool write_fault, bool *writable)\n{\n\tunsigned long addr;\n\n\tif (async)\n\t\t*async = false;\n\n\taddr = gfn_to_hva(kvm, gfn);\n\tif (kvm_is_error_hva(addr)) {\n\t\tget_page(bad_page);\n\t\treturn page_to_pfn(bad_page);\n\t}\n\n\treturn hva_to_pfn(kvm, addr, atomic, async, write_fault, writable);\n}\n\npfn_t gfn_to_pfn_atomic(struct kvm *kvm, gfn_t gfn)\n{\n\treturn __gfn_to_pfn(kvm, gfn, true, NULL, true, NULL);\n}\nEXPORT_SYMBOL_GPL(gfn_to_pfn_atomic);\n\npfn_t gfn_to_pfn_async(struct kvm *kvm, gfn_t gfn, bool *async,\n\t\t       bool write_fault, bool *writable)\n{\n\treturn __gfn_to_pfn(kvm, gfn, false, async, write_fault, writable);\n}\nEXPORT_SYMBOL_GPL(gfn_to_pfn_async);\n\npfn_t gfn_to_pfn(struct kvm *kvm, gfn_t gfn)\n{\n\treturn __gfn_to_pfn(kvm, gfn, false, NULL, true, NULL);\n}\nEXPORT_SYMBOL_GPL(gfn_to_pfn);\n\npfn_t gfn_to_pfn_prot(struct kvm *kvm, gfn_t gfn, bool write_fault,\n\t\t      bool *writable)\n{\n\treturn __gfn_to_pfn(kvm, gfn, false, NULL, write_fault, writable);\n}\nEXPORT_SYMBOL_GPL(gfn_to_pfn_prot);\n\npfn_t gfn_to_pfn_memslot(struct kvm *kvm,\n\t\t\t struct kvm_memory_slot *slot, gfn_t gfn)\n{\n\tunsigned long addr = gfn_to_hva_memslot(slot, gfn);\n\treturn hva_to_pfn(kvm, addr, false, NULL, true, NULL);\n}\n\nint gfn_to_page_many_atomic(struct kvm *kvm, gfn_t gfn, struct page **pages,\n\t\t\t\t\t\t\t\t  int nr_pages)\n{\n\tunsigned long addr;\n\tgfn_t entry;\n\n\taddr = gfn_to_hva_many(gfn_to_memslot(kvm, gfn), gfn, &entry);\n\tif (kvm_is_error_hva(addr))\n\t\treturn -1;\n\n\tif (entry < nr_pages)\n\t\treturn 0;\n\n\treturn __get_user_pages_fast(addr, nr_pages, 1, pages);\n}\nEXPORT_SYMBOL_GPL(gfn_to_page_many_atomic);\n\nstruct page *gfn_to_page(struct kvm *kvm, gfn_t gfn)\n{\n\tpfn_t pfn;\n\n\tpfn = gfn_to_pfn(kvm, gfn);\n\tif (!kvm_is_mmio_pfn(pfn))\n\t\treturn pfn_to_page(pfn);\n\n\tWARN_ON(kvm_is_mmio_pfn(pfn));\n\n\tget_page(bad_page);\n\treturn bad_page;\n}\n\nEXPORT_SYMBOL_GPL(gfn_to_page);\n\nvoid kvm_release_page_clean(struct page *page)\n{\n\tkvm_release_pfn_clean(page_to_pfn(page));\n}\nEXPORT_SYMBOL_GPL(kvm_release_page_clean);\n\nvoid kvm_release_pfn_clean(pfn_t pfn)\n{\n\tif (!kvm_is_mmio_pfn(pfn))\n\t\tput_page(pfn_to_page(pfn));\n}\nEXPORT_SYMBOL_GPL(kvm_release_pfn_clean);\n\nvoid kvm_release_page_dirty(struct page *page)\n{\n\tkvm_release_pfn_dirty(page_to_pfn(page));\n}\nEXPORT_SYMBOL_GPL(kvm_release_page_dirty);\n\nvoid kvm_release_pfn_dirty(pfn_t pfn)\n{\n\tkvm_set_pfn_dirty(pfn);\n\tkvm_release_pfn_clean(pfn);\n}\nEXPORT_SYMBOL_GPL(kvm_release_pfn_dirty);\n\nvoid kvm_set_page_dirty(struct page *page)\n{\n\tkvm_set_pfn_dirty(page_to_pfn(page));\n}\nEXPORT_SYMBOL_GPL(kvm_set_page_dirty);\n\nvoid kvm_set_pfn_dirty(pfn_t pfn)\n{\n\tif (!kvm_is_mmio_pfn(pfn)) {\n\t\tstruct page *page = pfn_to_page(pfn);\n\t\tif (!PageReserved(page))\n\t\t\tSetPageDirty(page);\n\t}\n}\nEXPORT_SYMBOL_GPL(kvm_set_pfn_dirty);\n\nvoid kvm_set_pfn_accessed(pfn_t pfn)\n{\n\tif (!kvm_is_mmio_pfn(pfn))\n\t\tmark_page_accessed(pfn_to_page(pfn));\n}\nEXPORT_SYMBOL_GPL(kvm_set_pfn_accessed);\n\nvoid kvm_get_pfn(pfn_t pfn)\n{\n\tif (!kvm_is_mmio_pfn(pfn))\n\t\tget_page(pfn_to_page(pfn));\n}\nEXPORT_SYMBOL_GPL(kvm_get_pfn);\n\nstatic int next_segment(unsigned long len, int offset)\n{\n\tif (len > PAGE_SIZE - offset)\n\t\treturn PAGE_SIZE - offset;\n\telse\n\t\treturn len;\n}\n\nint kvm_read_guest_page(struct kvm *kvm, gfn_t gfn, void *data, int offset,\n\t\t\tint len)\n{\n\tint r;\n\tunsigned long addr;\n\n\taddr = gfn_to_hva(kvm, gfn);\n\tif (kvm_is_error_hva(addr))\n\t\treturn -EFAULT;\n\tr = __copy_from_user(data, (void __user *)addr + offset, len);\n\tif (r)\n\t\treturn -EFAULT;\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(kvm_read_guest_page);\n\nint kvm_read_guest(struct kvm *kvm, gpa_t gpa, void *data, unsigned long len)\n{\n\tgfn_t gfn = gpa >> PAGE_SHIFT;\n\tint seg;\n\tint offset = offset_in_page(gpa);\n\tint ret;\n\n\twhile ((seg = next_segment(len, offset)) != 0) {\n\t\tret = kvm_read_guest_page(kvm, gfn, data, offset, seg);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t\toffset = 0;\n\t\tlen -= seg;\n\t\tdata += seg;\n\t\t++gfn;\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(kvm_read_guest);\n\nint kvm_read_guest_atomic(struct kvm *kvm, gpa_t gpa, void *data,\n\t\t\t  unsigned long len)\n{\n\tint r;\n\tunsigned long addr;\n\tgfn_t gfn = gpa >> PAGE_SHIFT;\n\tint offset = offset_in_page(gpa);\n\n\taddr = gfn_to_hva(kvm, gfn);\n\tif (kvm_is_error_hva(addr))\n\t\treturn -EFAULT;\n\tpagefault_disable();\n\tr = __copy_from_user_inatomic(data, (void __user *)addr + offset, len);\n\tpagefault_enable();\n\tif (r)\n\t\treturn -EFAULT;\n\treturn 0;\n}\nEXPORT_SYMBOL(kvm_read_guest_atomic);\n\nint kvm_write_guest_page(struct kvm *kvm, gfn_t gfn, const void *data,\n\t\t\t int offset, int len)\n{\n\tint r;\n\tunsigned long addr;\n\n\taddr = gfn_to_hva(kvm, gfn);\n\tif (kvm_is_error_hva(addr))\n\t\treturn -EFAULT;\n\tr = copy_to_user((void __user *)addr + offset, data, len);\n\tif (r)\n\t\treturn -EFAULT;\n\tmark_page_dirty(kvm, gfn);\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(kvm_write_guest_page);\n\nint kvm_write_guest(struct kvm *kvm, gpa_t gpa, const void *data,\n\t\t    unsigned long len)\n{\n\tgfn_t gfn = gpa >> PAGE_SHIFT;\n\tint seg;\n\tint offset = offset_in_page(gpa);\n\tint ret;\n\n\twhile ((seg = next_segment(len, offset)) != 0) {\n\t\tret = kvm_write_guest_page(kvm, gfn, data, offset, seg);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t\toffset = 0;\n\t\tlen -= seg;\n\t\tdata += seg;\n\t\t++gfn;\n\t}\n\treturn 0;\n}\n\nint kvm_gfn_to_hva_cache_init(struct kvm *kvm, struct gfn_to_hva_cache *ghc,\n\t\t\t      gpa_t gpa)\n{\n\tstruct kvm_memslots *slots = kvm_memslots(kvm);\n\tint offset = offset_in_page(gpa);\n\tgfn_t gfn = gpa >> PAGE_SHIFT;\n\n\tghc->gpa = gpa;\n\tghc->generation = slots->generation;\n\tghc->memslot = __gfn_to_memslot(slots, gfn);\n\tghc->hva = gfn_to_hva_many(ghc->memslot, gfn, NULL);\n\tif (!kvm_is_error_hva(ghc->hva))\n\t\tghc->hva += offset;\n\telse\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(kvm_gfn_to_hva_cache_init);\n\nint kvm_write_guest_cached(struct kvm *kvm, struct gfn_to_hva_cache *ghc,\n\t\t\t   void *data, unsigned long len)\n{\n\tstruct kvm_memslots *slots = kvm_memslots(kvm);\n\tint r;\n\n\tif (slots->generation != ghc->generation)\n\t\tkvm_gfn_to_hva_cache_init(kvm, ghc, ghc->gpa);\n\n\tif (kvm_is_error_hva(ghc->hva))\n\t\treturn -EFAULT;\n\n\tr = copy_to_user((void __user *)ghc->hva, data, len);\n\tif (r)\n\t\treturn -EFAULT;\n\tmark_page_dirty_in_slot(kvm, ghc->memslot, ghc->gpa >> PAGE_SHIFT);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(kvm_write_guest_cached);\n\nint kvm_clear_guest_page(struct kvm *kvm, gfn_t gfn, int offset, int len)\n{\n\treturn kvm_write_guest_page(kvm, gfn, (const void *) empty_zero_page,\n\t\t\t\t    offset, len);\n}\nEXPORT_SYMBOL_GPL(kvm_clear_guest_page);\n\nint kvm_clear_guest(struct kvm *kvm, gpa_t gpa, unsigned long len)\n{\n\tgfn_t gfn = gpa >> PAGE_SHIFT;\n\tint seg;\n\tint offset = offset_in_page(gpa);\n\tint ret;\n\n        while ((seg = next_segment(len, offset)) != 0) {\n\t\tret = kvm_clear_guest_page(kvm, gfn, offset, seg);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t\toffset = 0;\n\t\tlen -= seg;\n\t\t++gfn;\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(kvm_clear_guest);\n\nvoid mark_page_dirty_in_slot(struct kvm *kvm, struct kvm_memory_slot *memslot,\n\t\t\t     gfn_t gfn)\n{\n\tif (memslot && memslot->dirty_bitmap) {\n\t\tunsigned long rel_gfn = gfn - memslot->base_gfn;\n\n\t\t__set_bit_le(rel_gfn, memslot->dirty_bitmap);\n\t}\n}\n\nvoid mark_page_dirty(struct kvm *kvm, gfn_t gfn)\n{\n\tstruct kvm_memory_slot *memslot;\n\n\tmemslot = gfn_to_memslot(kvm, gfn);\n\tmark_page_dirty_in_slot(kvm, memslot, gfn);\n}\n\n/*\n * The vCPU has executed a HLT instruction with in-kernel mode enabled.\n */\nvoid kvm_vcpu_block(struct kvm_vcpu *vcpu)\n{\n\tDEFINE_WAIT(wait);\n\n\tfor (;;) {\n\t\tprepare_to_wait(&vcpu->wq, &wait, TASK_INTERRUPTIBLE);\n\n\t\tif (kvm_arch_vcpu_runnable(vcpu)) {\n\t\t\tkvm_make_request(KVM_REQ_UNHALT, vcpu);\n\t\t\tbreak;\n\t\t}\n\t\tif (kvm_cpu_has_pending_timer(vcpu))\n\t\t\tbreak;\n\t\tif (signal_pending(current))\n\t\t\tbreak;\n\n\t\tschedule();\n\t}\n\n\tfinish_wait(&vcpu->wq, &wait);\n}\n\nvoid kvm_resched(struct kvm_vcpu *vcpu)\n{\n\tif (!need_resched())\n\t\treturn;\n\tcond_resched();\n}\nEXPORT_SYMBOL_GPL(kvm_resched);\n\nvoid kvm_vcpu_on_spin(struct kvm_vcpu *me)\n{\n\tstruct kvm *kvm = me->kvm;\n\tstruct kvm_vcpu *vcpu;\n\tint last_boosted_vcpu = me->kvm->last_boosted_vcpu;\n\tint yielded = 0;\n\tint pass;\n\tint i;\n\n\t/*\n\t * We boost the priority of a VCPU that is runnable but not\n\t * currently running, because it got preempted by something\n\t * else and called schedule in __vcpu_run.  Hopefully that\n\t * VCPU is holding the lock that we need and will release it.\n\t * We approximate round-robin by starting at the last boosted VCPU.\n\t */\n\tfor (pass = 0; pass < 2 && !yielded; pass++) {\n\t\tkvm_for_each_vcpu(i, vcpu, kvm) {\n\t\t\tstruct task_struct *task = NULL;\n\t\t\tstruct pid *pid;\n\t\t\tif (!pass && i < last_boosted_vcpu) {\n\t\t\t\ti = last_boosted_vcpu;\n\t\t\t\tcontinue;\n\t\t\t} else if (pass && i > last_boosted_vcpu)\n\t\t\t\tbreak;\n\t\t\tif (vcpu == me)\n\t\t\t\tcontinue;\n\t\t\tif (waitqueue_active(&vcpu->wq))\n\t\t\t\tcontinue;\n\t\t\trcu_read_lock();\n\t\t\tpid = rcu_dereference(vcpu->pid);\n\t\t\tif (pid)\n\t\t\t\ttask = get_pid_task(vcpu->pid, PIDTYPE_PID);\n\t\t\trcu_read_unlock();\n\t\t\tif (!task)\n\t\t\t\tcontinue;\n\t\t\tif (task->flags & PF_VCPU) {\n\t\t\t\tput_task_struct(task);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (yield_to(task, 1)) {\n\t\t\t\tput_task_struct(task);\n\t\t\t\tkvm->last_boosted_vcpu = i;\n\t\t\t\tyielded = 1;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tput_task_struct(task);\n\t\t}\n\t}\n}\nEXPORT_SYMBOL_GPL(kvm_vcpu_on_spin);\n\nstatic int kvm_vcpu_fault(struct vm_area_struct *vma, struct vm_fault *vmf)\n{\n\tstruct kvm_vcpu *vcpu = vma->vm_file->private_data;\n\tstruct page *page;\n\n\tif (vmf->pgoff == 0)\n\t\tpage = virt_to_page(vcpu->run);\n#ifdef CONFIG_X86\n\telse if (vmf->pgoff == KVM_PIO_PAGE_OFFSET)\n\t\tpage = virt_to_page(vcpu->arch.pio_data);\n#endif\n#ifdef KVM_COALESCED_MMIO_PAGE_OFFSET\n\telse if (vmf->pgoff == KVM_COALESCED_MMIO_PAGE_OFFSET)\n\t\tpage = virt_to_page(vcpu->kvm->coalesced_mmio_ring);\n#endif\n\telse\n\t\treturn VM_FAULT_SIGBUS;\n\tget_page(page);\n\tvmf->page = page;\n\treturn 0;\n}\n\nstatic const struct vm_operations_struct kvm_vcpu_vm_ops = {\n\t.fault = kvm_vcpu_fault,\n};\n\nstatic int kvm_vcpu_mmap(struct file *file, struct vm_area_struct *vma)\n{\n\tvma->vm_ops = &kvm_vcpu_vm_ops;\n\treturn 0;\n}\n\nstatic int kvm_vcpu_release(struct inode *inode, struct file *filp)\n{\n\tstruct kvm_vcpu *vcpu = filp->private_data;\n\n\tkvm_put_kvm(vcpu->kvm);\n\treturn 0;\n}\n\nstatic struct file_operations kvm_vcpu_fops = {\n\t.release        = kvm_vcpu_release,\n\t.unlocked_ioctl = kvm_vcpu_ioctl,\n\t.compat_ioctl   = kvm_vcpu_ioctl,\n\t.mmap           = kvm_vcpu_mmap,\n\t.llseek\t\t= noop_llseek,\n};\n\n/*\n * Allocates an inode for the vcpu.\n */\nstatic int create_vcpu_fd(struct kvm_vcpu *vcpu)\n{\n\treturn anon_inode_getfd(\"kvm-vcpu\", &kvm_vcpu_fops, vcpu, O_RDWR);\n}\n\n/*\n * Creates some virtual cpus.  Good luck creating more than one.\n */\nstatic int kvm_vm_ioctl_create_vcpu(struct kvm *kvm, u32 id)\n{\n\tint r;\n\tstruct kvm_vcpu *vcpu, *v;\n\n\tvcpu = kvm_arch_vcpu_create(kvm, id);\n\tif (IS_ERR(vcpu))\n\t\treturn PTR_ERR(vcpu);\n\n\tpreempt_notifier_init(&vcpu->preempt_notifier, &kvm_preempt_ops);\n\n\tr = kvm_arch_vcpu_setup(vcpu);\n\tif (r)\n\t\treturn r;\n\n\tmutex_lock(&kvm->lock);\n\tif (atomic_read(&kvm->online_vcpus) == KVM_MAX_VCPUS) {\n\t\tr = -EINVAL;\n\t\tgoto vcpu_destroy;\n\t}\n\n\tkvm_for_each_vcpu(r, v, kvm)\n\t\tif (v->vcpu_id == id) {\n\t\t\tr = -EEXIST;\n\t\t\tgoto vcpu_destroy;\n\t\t}\n\n\tBUG_ON(kvm->vcpus[atomic_read(&kvm->online_vcpus)]);\n\n\t/* Now it's all set up, let userspace reach it */\n\tkvm_get_kvm(kvm);\n\tr = create_vcpu_fd(vcpu);\n\tif (r < 0) {\n\t\tkvm_put_kvm(kvm);\n\t\tgoto vcpu_destroy;\n\t}\n\n\tkvm->vcpus[atomic_read(&kvm->online_vcpus)] = vcpu;\n\tsmp_wmb();\n\tatomic_inc(&kvm->online_vcpus);\n\n#ifdef CONFIG_KVM_APIC_ARCHITECTURE\n\tif (kvm->bsp_vcpu_id == id)\n\t\tkvm->bsp_vcpu = vcpu;\n#endif\n\tmutex_unlock(&kvm->lock);\n\treturn r;\n\nvcpu_destroy:\n\tmutex_unlock(&kvm->lock);\n\tkvm_arch_vcpu_destroy(vcpu);\n\treturn r;\n}\n\nstatic int kvm_vcpu_ioctl_set_sigmask(struct kvm_vcpu *vcpu, sigset_t *sigset)\n{\n\tif (sigset) {\n\t\tsigdelsetmask(sigset, sigmask(SIGKILL)|sigmask(SIGSTOP));\n\t\tvcpu->sigset_active = 1;\n\t\tvcpu->sigset = *sigset;\n\t} else\n\t\tvcpu->sigset_active = 0;\n\treturn 0;\n}\n\nstatic long kvm_vcpu_ioctl(struct file *filp,\n\t\t\t   unsigned int ioctl, unsigned long arg)\n{\n\tstruct kvm_vcpu *vcpu = filp->private_data;\n\tvoid __user *argp = (void __user *)arg;\n\tint r;\n\tstruct kvm_fpu *fpu = NULL;\n\tstruct kvm_sregs *kvm_sregs = NULL;\n\n\tif (vcpu->kvm->mm != current->mm)\n\t\treturn -EIO;\n\n#if defined(CONFIG_S390) || defined(CONFIG_PPC)\n\t/*\n\t * Special cases: vcpu ioctls that are asynchronous to vcpu execution,\n\t * so vcpu_load() would break it.\n\t */\n\tif (ioctl == KVM_S390_INTERRUPT || ioctl == KVM_INTERRUPT)\n\t\treturn kvm_arch_vcpu_ioctl(filp, ioctl, arg);\n#endif\n\n\n\tvcpu_load(vcpu);\n\tswitch (ioctl) {\n\tcase KVM_RUN:\n\t\tr = -EINVAL;\n\t\tif (arg)\n\t\t\tgoto out;\n\t\tr = kvm_arch_vcpu_ioctl_run(vcpu, vcpu->run);\n\t\ttrace_kvm_userspace_exit(vcpu->run->exit_reason, r);\n\t\tbreak;\n\tcase KVM_GET_REGS: {\n\t\tstruct kvm_regs *kvm_regs;\n\n\t\tr = -ENOMEM;\n\t\tkvm_regs = kzalloc(sizeof(struct kvm_regs), GFP_KERNEL);\n\t\tif (!kvm_regs)\n\t\t\tgoto out;\n\t\tr = kvm_arch_vcpu_ioctl_get_regs(vcpu, kvm_regs);\n\t\tif (r)\n\t\t\tgoto out_free1;\n\t\tr = -EFAULT;\n\t\tif (copy_to_user(argp, kvm_regs, sizeof(struct kvm_regs)))\n\t\t\tgoto out_free1;\n\t\tr = 0;\nout_free1:\n\t\tkfree(kvm_regs);\n\t\tbreak;\n\t}\n\tcase KVM_SET_REGS: {\n\t\tstruct kvm_regs *kvm_regs;\n\n\t\tr = -ENOMEM;\n\t\tkvm_regs = kzalloc(sizeof(struct kvm_regs), GFP_KERNEL);\n\t\tif (!kvm_regs)\n\t\t\tgoto out;\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(kvm_regs, argp, sizeof(struct kvm_regs)))\n\t\t\tgoto out_free2;\n\t\tr = kvm_arch_vcpu_ioctl_set_regs(vcpu, kvm_regs);\n\t\tif (r)\n\t\t\tgoto out_free2;\n\t\tr = 0;\nout_free2:\n\t\tkfree(kvm_regs);\n\t\tbreak;\n\t}\n\tcase KVM_GET_SREGS: {\n\t\tkvm_sregs = kzalloc(sizeof(struct kvm_sregs), GFP_KERNEL);\n\t\tr = -ENOMEM;\n\t\tif (!kvm_sregs)\n\t\t\tgoto out;\n\t\tr = kvm_arch_vcpu_ioctl_get_sregs(vcpu, kvm_sregs);\n\t\tif (r)\n\t\t\tgoto out;\n\t\tr = -EFAULT;\n\t\tif (copy_to_user(argp, kvm_sregs, sizeof(struct kvm_sregs)))\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_SET_SREGS: {\n\t\tkvm_sregs = kmalloc(sizeof(struct kvm_sregs), GFP_KERNEL);\n\t\tr = -ENOMEM;\n\t\tif (!kvm_sregs)\n\t\t\tgoto out;\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(kvm_sregs, argp, sizeof(struct kvm_sregs)))\n\t\t\tgoto out;\n\t\tr = kvm_arch_vcpu_ioctl_set_sregs(vcpu, kvm_sregs);\n\t\tif (r)\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_GET_MP_STATE: {\n\t\tstruct kvm_mp_state mp_state;\n\n\t\tr = kvm_arch_vcpu_ioctl_get_mpstate(vcpu, &mp_state);\n\t\tif (r)\n\t\t\tgoto out;\n\t\tr = -EFAULT;\n\t\tif (copy_to_user(argp, &mp_state, sizeof mp_state))\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_SET_MP_STATE: {\n\t\tstruct kvm_mp_state mp_state;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&mp_state, argp, sizeof mp_state))\n\t\t\tgoto out;\n\t\tr = kvm_arch_vcpu_ioctl_set_mpstate(vcpu, &mp_state);\n\t\tif (r)\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_TRANSLATE: {\n\t\tstruct kvm_translation tr;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&tr, argp, sizeof tr))\n\t\t\tgoto out;\n\t\tr = kvm_arch_vcpu_ioctl_translate(vcpu, &tr);\n\t\tif (r)\n\t\t\tgoto out;\n\t\tr = -EFAULT;\n\t\tif (copy_to_user(argp, &tr, sizeof tr))\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_SET_GUEST_DEBUG: {\n\t\tstruct kvm_guest_debug dbg;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&dbg, argp, sizeof dbg))\n\t\t\tgoto out;\n\t\tr = kvm_arch_vcpu_ioctl_set_guest_debug(vcpu, &dbg);\n\t\tif (r)\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_SET_SIGNAL_MASK: {\n\t\tstruct kvm_signal_mask __user *sigmask_arg = argp;\n\t\tstruct kvm_signal_mask kvm_sigmask;\n\t\tsigset_t sigset, *p;\n\n\t\tp = NULL;\n\t\tif (argp) {\n\t\t\tr = -EFAULT;\n\t\t\tif (copy_from_user(&kvm_sigmask, argp,\n\t\t\t\t\t   sizeof kvm_sigmask))\n\t\t\t\tgoto out;\n\t\t\tr = -EINVAL;\n\t\t\tif (kvm_sigmask.len != sizeof sigset)\n\t\t\t\tgoto out;\n\t\t\tr = -EFAULT;\n\t\t\tif (copy_from_user(&sigset, sigmask_arg->sigset,\n\t\t\t\t\t   sizeof sigset))\n\t\t\t\tgoto out;\n\t\t\tp = &sigset;\n\t\t}\n\t\tr = kvm_vcpu_ioctl_set_sigmask(vcpu, p);\n\t\tbreak;\n\t}\n\tcase KVM_GET_FPU: {\n\t\tfpu = kzalloc(sizeof(struct kvm_fpu), GFP_KERNEL);\n\t\tr = -ENOMEM;\n\t\tif (!fpu)\n\t\t\tgoto out;\n\t\tr = kvm_arch_vcpu_ioctl_get_fpu(vcpu, fpu);\n\t\tif (r)\n\t\t\tgoto out;\n\t\tr = -EFAULT;\n\t\tif (copy_to_user(argp, fpu, sizeof(struct kvm_fpu)))\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_SET_FPU: {\n\t\tfpu = kmalloc(sizeof(struct kvm_fpu), GFP_KERNEL);\n\t\tr = -ENOMEM;\n\t\tif (!fpu)\n\t\t\tgoto out;\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(fpu, argp, sizeof(struct kvm_fpu)))\n\t\t\tgoto out;\n\t\tr = kvm_arch_vcpu_ioctl_set_fpu(vcpu, fpu);\n\t\tif (r)\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tdefault:\n\t\tr = kvm_arch_vcpu_ioctl(filp, ioctl, arg);\n\t}\nout:\n\tvcpu_put(vcpu);\n\tkfree(fpu);\n\tkfree(kvm_sregs);\n\treturn r;\n}\n\nstatic long kvm_vm_ioctl(struct file *filp,\n\t\t\t   unsigned int ioctl, unsigned long arg)\n{\n\tstruct kvm *kvm = filp->private_data;\n\tvoid __user *argp = (void __user *)arg;\n\tint r;\n\n\tif (kvm->mm != current->mm)\n\t\treturn -EIO;\n\tswitch (ioctl) {\n\tcase KVM_CREATE_VCPU:\n\t\tr = kvm_vm_ioctl_create_vcpu(kvm, arg);\n\t\tif (r < 0)\n\t\t\tgoto out;\n\t\tbreak;\n\tcase KVM_SET_USER_MEMORY_REGION: {\n\t\tstruct kvm_userspace_memory_region kvm_userspace_mem;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&kvm_userspace_mem, argp,\n\t\t\t\t\t\tsizeof kvm_userspace_mem))\n\t\t\tgoto out;\n\n\t\tr = kvm_vm_ioctl_set_memory_region(kvm, &kvm_userspace_mem, 1);\n\t\tif (r)\n\t\t\tgoto out;\n\t\tbreak;\n\t}\n\tcase KVM_GET_DIRTY_LOG: {\n\t\tstruct kvm_dirty_log log;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&log, argp, sizeof log))\n\t\t\tgoto out;\n\t\tr = kvm_vm_ioctl_get_dirty_log(kvm, &log);\n\t\tif (r)\n\t\t\tgoto out;\n\t\tbreak;\n\t}\n#ifdef KVM_COALESCED_MMIO_PAGE_OFFSET\n\tcase KVM_REGISTER_COALESCED_MMIO: {\n\t\tstruct kvm_coalesced_mmio_zone zone;\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&zone, argp, sizeof zone))\n\t\t\tgoto out;\n\t\tr = kvm_vm_ioctl_register_coalesced_mmio(kvm, &zone);\n\t\tif (r)\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_UNREGISTER_COALESCED_MMIO: {\n\t\tstruct kvm_coalesced_mmio_zone zone;\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&zone, argp, sizeof zone))\n\t\t\tgoto out;\n\t\tr = kvm_vm_ioctl_unregister_coalesced_mmio(kvm, &zone);\n\t\tif (r)\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\t}\n#endif\n\tcase KVM_IRQFD: {\n\t\tstruct kvm_irqfd data;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&data, argp, sizeof data))\n\t\t\tgoto out;\n\t\tr = kvm_irqfd(kvm, data.fd, data.gsi, data.flags);\n\t\tbreak;\n\t}\n\tcase KVM_IOEVENTFD: {\n\t\tstruct kvm_ioeventfd data;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&data, argp, sizeof data))\n\t\t\tgoto out;\n\t\tr = kvm_ioeventfd(kvm, &data);\n\t\tbreak;\n\t}\n#ifdef CONFIG_KVM_APIC_ARCHITECTURE\n\tcase KVM_SET_BOOT_CPU_ID:\n\t\tr = 0;\n\t\tmutex_lock(&kvm->lock);\n\t\tif (atomic_read(&kvm->online_vcpus) != 0)\n\t\t\tr = -EBUSY;\n\t\telse\n\t\t\tkvm->bsp_vcpu_id = arg;\n\t\tmutex_unlock(&kvm->lock);\n\t\tbreak;\n#endif\n\tdefault:\n\t\tr = kvm_arch_vm_ioctl(filp, ioctl, arg);\n\t\tif (r == -ENOTTY)\n\t\t\tr = kvm_vm_ioctl_assigned_device(kvm, ioctl, arg);\n\t}\nout:\n\treturn r;\n}\n\n#ifdef CONFIG_COMPAT\nstruct compat_kvm_dirty_log {\n\t__u32 slot;\n\t__u32 padding1;\n\tunion {\n\t\tcompat_uptr_t dirty_bitmap; /* one bit per page */\n\t\t__u64 padding2;\n\t};\n};\n\nstatic long kvm_vm_compat_ioctl(struct file *filp,\n\t\t\t   unsigned int ioctl, unsigned long arg)\n{\n\tstruct kvm *kvm = filp->private_data;\n\tint r;\n\n\tif (kvm->mm != current->mm)\n\t\treturn -EIO;\n\tswitch (ioctl) {\n\tcase KVM_GET_DIRTY_LOG: {\n\t\tstruct compat_kvm_dirty_log compat_log;\n\t\tstruct kvm_dirty_log log;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&compat_log, (void __user *)arg,\n\t\t\t\t   sizeof(compat_log)))\n\t\t\tgoto out;\n\t\tlog.slot\t = compat_log.slot;\n\t\tlog.padding1\t = compat_log.padding1;\n\t\tlog.padding2\t = compat_log.padding2;\n\t\tlog.dirty_bitmap = compat_ptr(compat_log.dirty_bitmap);\n\n\t\tr = kvm_vm_ioctl_get_dirty_log(kvm, &log);\n\t\tif (r)\n\t\t\tgoto out;\n\t\tbreak;\n\t}\n\tdefault:\n\t\tr = kvm_vm_ioctl(filp, ioctl, arg);\n\t}\n\nout:\n\treturn r;\n}\n#endif\n\nstatic int kvm_vm_fault(struct vm_area_struct *vma, struct vm_fault *vmf)\n{\n\tstruct page *page[1];\n\tunsigned long addr;\n\tint npages;\n\tgfn_t gfn = vmf->pgoff;\n\tstruct kvm *kvm = vma->vm_file->private_data;\n\n\taddr = gfn_to_hva(kvm, gfn);\n\tif (kvm_is_error_hva(addr))\n\t\treturn VM_FAULT_SIGBUS;\n\n\tnpages = get_user_pages(current, current->mm, addr, 1, 1, 0, page,\n\t\t\t\tNULL);\n\tif (unlikely(npages != 1))\n\t\treturn VM_FAULT_SIGBUS;\n\n\tvmf->page = page[0];\n\treturn 0;\n}\n\nstatic const struct vm_operations_struct kvm_vm_vm_ops = {\n\t.fault = kvm_vm_fault,\n};\n\nstatic int kvm_vm_mmap(struct file *file, struct vm_area_struct *vma)\n{\n\tvma->vm_ops = &kvm_vm_vm_ops;\n\treturn 0;\n}\n\nstatic struct file_operations kvm_vm_fops = {\n\t.release        = kvm_vm_release,\n\t.unlocked_ioctl = kvm_vm_ioctl,\n#ifdef CONFIG_COMPAT\n\t.compat_ioctl   = kvm_vm_compat_ioctl,\n#endif\n\t.mmap           = kvm_vm_mmap,\n\t.llseek\t\t= noop_llseek,\n};\n\nstatic int kvm_dev_ioctl_create_vm(void)\n{\n\tint r;\n\tstruct kvm *kvm;\n\n\tkvm = kvm_create_vm();\n\tif (IS_ERR(kvm))\n\t\treturn PTR_ERR(kvm);\n#ifdef KVM_COALESCED_MMIO_PAGE_OFFSET\n\tr = kvm_coalesced_mmio_init(kvm);\n\tif (r < 0) {\n\t\tkvm_put_kvm(kvm);\n\t\treturn r;\n\t}\n#endif\n\tr = anon_inode_getfd(\"kvm-vm\", &kvm_vm_fops, kvm, O_RDWR);\n\tif (r < 0)\n\t\tkvm_put_kvm(kvm);\n\n\treturn r;\n}\n\nstatic long kvm_dev_ioctl_check_extension_generic(long arg)\n{\n\tswitch (arg) {\n\tcase KVM_CAP_USER_MEMORY:\n\tcase KVM_CAP_DESTROY_MEMORY_REGION_WORKS:\n\tcase KVM_CAP_JOIN_MEMORY_REGIONS_WORKS:\n#ifdef CONFIG_KVM_APIC_ARCHITECTURE\n\tcase KVM_CAP_SET_BOOT_CPU_ID:\n#endif\n\tcase KVM_CAP_INTERNAL_ERROR_DATA:\n\t\treturn 1;\n#ifdef CONFIG_HAVE_KVM_IRQCHIP\n\tcase KVM_CAP_IRQ_ROUTING:\n\t\treturn KVM_MAX_IRQ_ROUTES;\n#endif\n\tdefault:\n\t\tbreak;\n\t}\n\treturn kvm_dev_ioctl_check_extension(arg);\n}\n\nstatic long kvm_dev_ioctl(struct file *filp,\n\t\t\t  unsigned int ioctl, unsigned long arg)\n{\n\tlong r = -EINVAL;\n\n\tswitch (ioctl) {\n\tcase KVM_GET_API_VERSION:\n\t\tr = -EINVAL;\n\t\tif (arg)\n\t\t\tgoto out;\n\t\tr = KVM_API_VERSION;\n\t\tbreak;\n\tcase KVM_CREATE_VM:\n\t\tr = -EINVAL;\n\t\tif (arg)\n\t\t\tgoto out;\n\t\tr = kvm_dev_ioctl_create_vm();\n\t\tbreak;\n\tcase KVM_CHECK_EXTENSION:\n\t\tr = kvm_dev_ioctl_check_extension_generic(arg);\n\t\tbreak;\n\tcase KVM_GET_VCPU_MMAP_SIZE:\n\t\tr = -EINVAL;\n\t\tif (arg)\n\t\t\tgoto out;\n\t\tr = PAGE_SIZE;     /* struct kvm_run */\n#ifdef CONFIG_X86\n\t\tr += PAGE_SIZE;    /* pio data page */\n#endif\n#ifdef KVM_COALESCED_MMIO_PAGE_OFFSET\n\t\tr += PAGE_SIZE;    /* coalesced mmio ring page */\n#endif\n\t\tbreak;\n\tcase KVM_TRACE_ENABLE:\n\tcase KVM_TRACE_PAUSE:\n\tcase KVM_TRACE_DISABLE:\n\t\tr = -EOPNOTSUPP;\n\t\tbreak;\n\tdefault:\n\t\treturn kvm_arch_dev_ioctl(filp, ioctl, arg);\n\t}\nout:\n\treturn r;\n}\n\nstatic struct file_operations kvm_chardev_ops = {\n\t.unlocked_ioctl = kvm_dev_ioctl,\n\t.compat_ioctl   = kvm_dev_ioctl,\n\t.llseek\t\t= noop_llseek,\n};\n\nstatic struct miscdevice kvm_dev = {\n\tKVM_MINOR,\n\t\"kvm\",\n\t&kvm_chardev_ops,\n};\n\nstatic void hardware_enable_nolock(void *junk)\n{\n\tint cpu = raw_smp_processor_id();\n\tint r;\n\n\tif (cpumask_test_cpu(cpu, cpus_hardware_enabled))\n\t\treturn;\n\n\tcpumask_set_cpu(cpu, cpus_hardware_enabled);\n\n\tr = kvm_arch_hardware_enable(NULL);\n\n\tif (r) {\n\t\tcpumask_clear_cpu(cpu, cpus_hardware_enabled);\n\t\tatomic_inc(&hardware_enable_failed);\n\t\tprintk(KERN_INFO \"kvm: enabling virtualization on \"\n\t\t\t\t \"CPU%d failed\\n\", cpu);\n\t}\n}\n\nstatic void hardware_enable(void *junk)\n{\n\traw_spin_lock(&kvm_lock);\n\thardware_enable_nolock(junk);\n\traw_spin_unlock(&kvm_lock);\n}\n\nstatic void hardware_disable_nolock(void *junk)\n{\n\tint cpu = raw_smp_processor_id();\n\n\tif (!cpumask_test_cpu(cpu, cpus_hardware_enabled))\n\t\treturn;\n\tcpumask_clear_cpu(cpu, cpus_hardware_enabled);\n\tkvm_arch_hardware_disable(NULL);\n}\n\nstatic void hardware_disable(void *junk)\n{\n\traw_spin_lock(&kvm_lock);\n\thardware_disable_nolock(junk);\n\traw_spin_unlock(&kvm_lock);\n}\n\nstatic void hardware_disable_all_nolock(void)\n{\n\tBUG_ON(!kvm_usage_count);\n\n\tkvm_usage_count--;\n\tif (!kvm_usage_count)\n\t\ton_each_cpu(hardware_disable_nolock, NULL, 1);\n}\n\nstatic void hardware_disable_all(void)\n{\n\traw_spin_lock(&kvm_lock);\n\thardware_disable_all_nolock();\n\traw_spin_unlock(&kvm_lock);\n}\n\nstatic int hardware_enable_all(void)\n{\n\tint r = 0;\n\n\traw_spin_lock(&kvm_lock);\n\n\tkvm_usage_count++;\n\tif (kvm_usage_count == 1) {\n\t\tatomic_set(&hardware_enable_failed, 0);\n\t\ton_each_cpu(hardware_enable_nolock, NULL, 1);\n\n\t\tif (atomic_read(&hardware_enable_failed)) {\n\t\t\thardware_disable_all_nolock();\n\t\t\tr = -EBUSY;\n\t\t}\n\t}\n\n\traw_spin_unlock(&kvm_lock);\n\n\treturn r;\n}\n\nstatic int kvm_cpu_hotplug(struct notifier_block *notifier, unsigned long val,\n\t\t\t   void *v)\n{\n\tint cpu = (long)v;\n\n\tif (!kvm_usage_count)\n\t\treturn NOTIFY_OK;\n\n\tval &= ~CPU_TASKS_FROZEN;\n\tswitch (val) {\n\tcase CPU_DYING:\n\t\tprintk(KERN_INFO \"kvm: disabling virtualization on CPU%d\\n\",\n\t\t       cpu);\n\t\thardware_disable(NULL);\n\t\tbreak;\n\tcase CPU_STARTING:\n\t\tprintk(KERN_INFO \"kvm: enabling virtualization on CPU%d\\n\",\n\t\t       cpu);\n\t\thardware_enable(NULL);\n\t\tbreak;\n\t}\n\treturn NOTIFY_OK;\n}\n\n\nasmlinkage void kvm_spurious_fault(void)\n{\n\t/* Fault while not rebooting.  We want the trace. */\n\tBUG();\n}\nEXPORT_SYMBOL_GPL(kvm_spurious_fault);\n\nstatic int kvm_reboot(struct notifier_block *notifier, unsigned long val,\n\t\t      void *v)\n{\n\t/*\n\t * Some (well, at least mine) BIOSes hang on reboot if\n\t * in vmx root mode.\n\t *\n\t * And Intel TXT required VMX off for all cpu when system shutdown.\n\t */\n\tprintk(KERN_INFO \"kvm: exiting hardware virtualization\\n\");\n\tkvm_rebooting = true;\n\ton_each_cpu(hardware_disable_nolock, NULL, 1);\n\treturn NOTIFY_OK;\n}\n\nstatic struct notifier_block kvm_reboot_notifier = {\n\t.notifier_call = kvm_reboot,\n\t.priority = 0,\n};\n\nstatic void kvm_io_bus_destroy(struct kvm_io_bus *bus)\n{\n\tint i;\n\n\tfor (i = 0; i < bus->dev_count; i++) {\n\t\tstruct kvm_io_device *pos = bus->devs[i];\n\n\t\tkvm_iodevice_destructor(pos);\n\t}\n\tkfree(bus);\n}\n\n/* kvm_io_bus_write - called under kvm->slots_lock */\nint kvm_io_bus_write(struct kvm *kvm, enum kvm_bus bus_idx, gpa_t addr,\n\t\t     int len, const void *val)\n{\n\tint i;\n\tstruct kvm_io_bus *bus;\n\n\tbus = srcu_dereference(kvm->buses[bus_idx], &kvm->srcu);\n\tfor (i = 0; i < bus->dev_count; i++)\n\t\tif (!kvm_iodevice_write(bus->devs[i], addr, len, val))\n\t\t\treturn 0;\n\treturn -EOPNOTSUPP;\n}\n\n/* kvm_io_bus_read - called under kvm->slots_lock */\nint kvm_io_bus_read(struct kvm *kvm, enum kvm_bus bus_idx, gpa_t addr,\n\t\t    int len, void *val)\n{\n\tint i;\n\tstruct kvm_io_bus *bus;\n\n\tbus = srcu_dereference(kvm->buses[bus_idx], &kvm->srcu);\n\tfor (i = 0; i < bus->dev_count; i++)\n\t\tif (!kvm_iodevice_read(bus->devs[i], addr, len, val))\n\t\t\treturn 0;\n\treturn -EOPNOTSUPP;\n}\n\n/* Caller must hold slots_lock. */\nint kvm_io_bus_register_dev(struct kvm *kvm, enum kvm_bus bus_idx,\n\t\t\t    struct kvm_io_device *dev)\n{\n\tstruct kvm_io_bus *new_bus, *bus;\n\n\tbus = kvm->buses[bus_idx];\n\tif (bus->dev_count > NR_IOBUS_DEVS-1)\n\t\treturn -ENOSPC;\n\n\tnew_bus = kzalloc(sizeof(struct kvm_io_bus), GFP_KERNEL);\n\tif (!new_bus)\n\t\treturn -ENOMEM;\n\tmemcpy(new_bus, bus, sizeof(struct kvm_io_bus));\n\tnew_bus->devs[new_bus->dev_count++] = dev;\n\trcu_assign_pointer(kvm->buses[bus_idx], new_bus);\n\tsynchronize_srcu_expedited(&kvm->srcu);\n\tkfree(bus);\n\n\treturn 0;\n}\n\n/* Caller must hold slots_lock. */\nint kvm_io_bus_unregister_dev(struct kvm *kvm, enum kvm_bus bus_idx,\n\t\t\t      struct kvm_io_device *dev)\n{\n\tint i, r;\n\tstruct kvm_io_bus *new_bus, *bus;\n\n\tnew_bus = kzalloc(sizeof(struct kvm_io_bus), GFP_KERNEL);\n\tif (!new_bus)\n\t\treturn -ENOMEM;\n\n\tbus = kvm->buses[bus_idx];\n\tmemcpy(new_bus, bus, sizeof(struct kvm_io_bus));\n\n\tr = -ENOENT;\n\tfor (i = 0; i < new_bus->dev_count; i++)\n\t\tif (new_bus->devs[i] == dev) {\n\t\t\tr = 0;\n\t\t\tnew_bus->devs[i] = new_bus->devs[--new_bus->dev_count];\n\t\t\tbreak;\n\t\t}\n\n\tif (r) {\n\t\tkfree(new_bus);\n\t\treturn r;\n\t}\n\n\trcu_assign_pointer(kvm->buses[bus_idx], new_bus);\n\tsynchronize_srcu_expedited(&kvm->srcu);\n\tkfree(bus);\n\treturn r;\n}\n\nstatic struct notifier_block kvm_cpu_notifier = {\n\t.notifier_call = kvm_cpu_hotplug,\n};\n\nstatic int vm_stat_get(void *_offset, u64 *val)\n{\n\tunsigned offset = (long)_offset;\n\tstruct kvm *kvm;\n\n\t*val = 0;\n\traw_spin_lock(&kvm_lock);\n\tlist_for_each_entry(kvm, &vm_list, vm_list)\n\t\t*val += *(u32 *)((void *)kvm + offset);\n\traw_spin_unlock(&kvm_lock);\n\treturn 0;\n}\n\nDEFINE_SIMPLE_ATTRIBUTE(vm_stat_fops, vm_stat_get, NULL, \"%llu\\n\");\n\nstatic int vcpu_stat_get(void *_offset, u64 *val)\n{\n\tunsigned offset = (long)_offset;\n\tstruct kvm *kvm;\n\tstruct kvm_vcpu *vcpu;\n\tint i;\n\n\t*val = 0;\n\traw_spin_lock(&kvm_lock);\n\tlist_for_each_entry(kvm, &vm_list, vm_list)\n\t\tkvm_for_each_vcpu(i, vcpu, kvm)\n\t\t\t*val += *(u32 *)((void *)vcpu + offset);\n\n\traw_spin_unlock(&kvm_lock);\n\treturn 0;\n}\n\nDEFINE_SIMPLE_ATTRIBUTE(vcpu_stat_fops, vcpu_stat_get, NULL, \"%llu\\n\");\n\nstatic const struct file_operations *stat_fops[] = {\n\t[KVM_STAT_VCPU] = &vcpu_stat_fops,\n\t[KVM_STAT_VM]   = &vm_stat_fops,\n};\n\nstatic void kvm_init_debug(void)\n{\n\tstruct kvm_stats_debugfs_item *p;\n\n\tkvm_debugfs_dir = debugfs_create_dir(\"kvm\", NULL);\n\tfor (p = debugfs_entries; p->name; ++p)\n\t\tp->dentry = debugfs_create_file(p->name, 0444, kvm_debugfs_dir,\n\t\t\t\t\t\t(void *)(long)p->offset,\n\t\t\t\t\t\tstat_fops[p->kind]);\n}\n\nstatic void kvm_exit_debug(void)\n{\n\tstruct kvm_stats_debugfs_item *p;\n\n\tfor (p = debugfs_entries; p->name; ++p)\n\t\tdebugfs_remove(p->dentry);\n\tdebugfs_remove(kvm_debugfs_dir);\n}\n\nstatic int kvm_suspend(void)\n{\n\tif (kvm_usage_count)\n\t\thardware_disable_nolock(NULL);\n\treturn 0;\n}\n\nstatic void kvm_resume(void)\n{\n\tif (kvm_usage_count) {\n\t\tWARN_ON(raw_spin_is_locked(&kvm_lock));\n\t\thardware_enable_nolock(NULL);\n\t}\n}\n\nstatic struct syscore_ops kvm_syscore_ops = {\n\t.suspend = kvm_suspend,\n\t.resume = kvm_resume,\n};\n\nstruct page *bad_page;\npfn_t bad_pfn;\n\nstatic inline\nstruct kvm_vcpu *preempt_notifier_to_vcpu(struct preempt_notifier *pn)\n{\n\treturn container_of(pn, struct kvm_vcpu, preempt_notifier);\n}\n\nstatic void kvm_sched_in(struct preempt_notifier *pn, int cpu)\n{\n\tstruct kvm_vcpu *vcpu = preempt_notifier_to_vcpu(pn);\n\n\tkvm_arch_vcpu_load(vcpu, cpu);\n}\n\nstatic void kvm_sched_out(struct preempt_notifier *pn,\n\t\t\t  struct task_struct *next)\n{\n\tstruct kvm_vcpu *vcpu = preempt_notifier_to_vcpu(pn);\n\n\tkvm_arch_vcpu_put(vcpu);\n}\n\nint kvm_init(void *opaque, unsigned vcpu_size, unsigned vcpu_align,\n\t\t  struct module *module)\n{\n\tint r;\n\tint cpu;\n\n\tr = kvm_arch_init(opaque);\n\tif (r)\n\t\tgoto out_fail;\n\n\tbad_page = alloc_page(GFP_KERNEL | __GFP_ZERO);\n\n\tif (bad_page == NULL) {\n\t\tr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tbad_pfn = page_to_pfn(bad_page);\n\n\thwpoison_page = alloc_page(GFP_KERNEL | __GFP_ZERO);\n\n\tif (hwpoison_page == NULL) {\n\t\tr = -ENOMEM;\n\t\tgoto out_free_0;\n\t}\n\n\thwpoison_pfn = page_to_pfn(hwpoison_page);\n\n\tfault_page = alloc_page(GFP_KERNEL | __GFP_ZERO);\n\n\tif (fault_page == NULL) {\n\t\tr = -ENOMEM;\n\t\tgoto out_free_0;\n\t}\n\n\tfault_pfn = page_to_pfn(fault_page);\n\n\tif (!zalloc_cpumask_var(&cpus_hardware_enabled, GFP_KERNEL)) {\n\t\tr = -ENOMEM;\n\t\tgoto out_free_0;\n\t}\n\n\tr = kvm_arch_hardware_setup();\n\tif (r < 0)\n\t\tgoto out_free_0a;\n\n\tfor_each_online_cpu(cpu) {\n\t\tsmp_call_function_single(cpu,\n\t\t\t\tkvm_arch_check_processor_compat,\n\t\t\t\t&r, 1);\n\t\tif (r < 0)\n\t\t\tgoto out_free_1;\n\t}\n\n\tr = register_cpu_notifier(&kvm_cpu_notifier);\n\tif (r)\n\t\tgoto out_free_2;\n\tregister_reboot_notifier(&kvm_reboot_notifier);\n\n\t/* A kmem cache lets us meet the alignment requirements of fx_save. */\n\tif (!vcpu_align)\n\t\tvcpu_align = __alignof__(struct kvm_vcpu);\n\tkvm_vcpu_cache = kmem_cache_create(\"kvm_vcpu\", vcpu_size, vcpu_align,\n\t\t\t\t\t   0, NULL);\n\tif (!kvm_vcpu_cache) {\n\t\tr = -ENOMEM;\n\t\tgoto out_free_3;\n\t}\n\n\tr = kvm_async_pf_init();\n\tif (r)\n\t\tgoto out_free;\n\n\tkvm_chardev_ops.owner = module;\n\tkvm_vm_fops.owner = module;\n\tkvm_vcpu_fops.owner = module;\n\n\tr = misc_register(&kvm_dev);\n\tif (r) {\n\t\tprintk(KERN_ERR \"kvm: misc device register failed\\n\");\n\t\tgoto out_unreg;\n\t}\n\n\tregister_syscore_ops(&kvm_syscore_ops);\n\n\tkvm_preempt_ops.sched_in = kvm_sched_in;\n\tkvm_preempt_ops.sched_out = kvm_sched_out;\n\n\tkvm_init_debug();\n\n\treturn 0;\n\nout_unreg:\n\tkvm_async_pf_deinit();\nout_free:\n\tkmem_cache_destroy(kvm_vcpu_cache);\nout_free_3:\n\tunregister_reboot_notifier(&kvm_reboot_notifier);\n\tunregister_cpu_notifier(&kvm_cpu_notifier);\nout_free_2:\nout_free_1:\n\tkvm_arch_hardware_unsetup();\nout_free_0a:\n\tfree_cpumask_var(cpus_hardware_enabled);\nout_free_0:\n\tif (fault_page)\n\t\t__free_page(fault_page);\n\tif (hwpoison_page)\n\t\t__free_page(hwpoison_page);\n\t__free_page(bad_page);\nout:\n\tkvm_arch_exit();\nout_fail:\n\treturn r;\n}\nEXPORT_SYMBOL_GPL(kvm_init);\n\nvoid kvm_exit(void)\n{\n\tkvm_exit_debug();\n\tmisc_deregister(&kvm_dev);\n\tkmem_cache_destroy(kvm_vcpu_cache);\n\tkvm_async_pf_deinit();\n\tunregister_syscore_ops(&kvm_syscore_ops);\n\tunregister_reboot_notifier(&kvm_reboot_notifier);\n\tunregister_cpu_notifier(&kvm_cpu_notifier);\n\ton_each_cpu(hardware_disable_nolock, NULL, 1);\n\tkvm_arch_hardware_unsetup();\n\tkvm_arch_exit();\n\tfree_cpumask_var(cpus_hardware_enabled);\n\t__free_page(hwpoison_page);\n\t__free_page(bad_page);\n}\nEXPORT_SYMBOL_GPL(kvm_exit);\n"], "buggy_code_start_loc": [188, 651], "buggy_code_end_loc": [189, 1287], "fixing_code_start_loc": [188, 651], "fixing_code_end_loc": [189, 1290], "type": "CWE-20", "message": "The KVM subsystem in the Linux kernel before 3.0 does not check whether kernel addresses are specified during allocation of memory slots for use in a guest's physical address space, which allows local users to gain privileges or obtain sensitive information from kernel memory via a crafted application, related to arch/x86/kvm/paging_tmpl.h and virt/kvm/kvm_main.c.", "other": {"cve": {"id": "CVE-2013-1943", "sourceIdentifier": "secalert@redhat.com", "published": "2013-07-16T14:08:50.933", "lastModified": "2023-02-13T04:42:18.817", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "The KVM subsystem in the Linux kernel before 3.0 does not check whether kernel addresses are specified during allocation of memory slots for use in a guest's physical address space, which allows local users to gain privileges or obtain sensitive information from kernel memory via a crafted application, related to arch/x86/kvm/paging_tmpl.h and virt/kvm/kvm_main.c."}, {"lang": "es", "value": "El subsistema KVM en el kernel de Linux anterior a v3.0 no comprueba si las direcciones del n\u00facleo se especifican durante la asignaci\u00f3n de slots de memoria para su uso en el espacio de direcciones f\u00edsicas huesped, permitiendo a usuarios locales conseguir privilegios u obtener informaci\u00f3n confidencial de la memoria del n\u00facleo a trav\u00e9s de una aplicaci\u00f3n especialmente dise\u00f1ada, relacionada con arch/x86/kvm/paging_tmpl.h y virt/kvm/kvm_main.c."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:N/UI:R/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "REQUIRED", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:M/Au:N/C:P/I:P/A:P", "accessVector": "LOCAL", "accessComplexity": "MEDIUM", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "PARTIAL", "availabilityImpact": "PARTIAL", "baseScore": 4.4}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.4, "impactScore": 6.4, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-20"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "3.0", "matchCriteriaId": "E0135A6D-9FB7-4E1B-B471-914E37494942"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux:5.0:*:*:*:*:*:*:*", "matchCriteriaId": "1D8B549B-E57B-4DFE-8A13-CAB06B5356B3"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_eus:6.2:*:*:*:*:*:*:*", "matchCriteriaId": "C0554C89-3716-49F3-BFAE-E008D5E4E29C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_eus:6.3:*:*:*:*:*:*:*", "matchCriteriaId": "8382A145-CDD9-437E-9DE7-A349956778B3"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:10.04:*:*:*:-:*:*:*", "matchCriteriaId": "01EDA41C-6B2E-49AF-B503-EB3882265C11"}]}]}], "references": [{"url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux-2.6.git%3Ba=commit%3Bh=fa3d315a4ce2c0891cdde262562e710d95fba19e", "source": "secalert@redhat.com"}, {"url": "http://www.ubuntu.com/usn/USN-1939-1", "source": "secalert@redhat.com", "tags": ["Third Party Advisory"]}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=950490", "source": "secalert@redhat.com", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/fa3d315a4ce2c0891cdde262562e710d95fba19e", "source": "secalert@redhat.com", "tags": ["Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/fa3d315a4ce2c0891cdde262562e710d95fba19e"}}