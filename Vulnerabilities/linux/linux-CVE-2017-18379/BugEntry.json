{"buggy_code": ["/*\n * Copyright (c) 2016 Avago Technologies.  All rights reserved.\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of version 2 of the GNU General Public License as\n * published by the Free Software Foundation.\n *\n * This program is distributed in the hope that it will be useful.\n * ALL EXPRESS OR IMPLIED CONDITIONS, REPRESENTATIONS AND WARRANTIES,\n * INCLUDING ANY IMPLIED WARRANTY OF MERCHANTABILITY, FITNESS FOR A\n * PARTICULAR PURPOSE, OR NON-INFRINGEMENT, ARE DISCLAIMED, EXCEPT TO\n * THE EXTENT THAT SUCH DISCLAIMERS ARE HELD TO BE LEGALLY INVALID.\n * See the GNU General Public License for more details, a copy of which\n * can be found in the file COPYING included with this package\n *\n */\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n#include <linux/module.h>\n#include <linux/slab.h>\n#include <linux/blk-mq.h>\n#include <linux/parser.h>\n#include <linux/random.h>\n#include <uapi/scsi/fc/fc_fs.h>\n#include <uapi/scsi/fc/fc_els.h>\n\n#include \"nvmet.h\"\n#include <linux/nvme-fc-driver.h>\n#include <linux/nvme-fc.h>\n\n\n/* *************************** Data Structures/Defines ****************** */\n\n\n#define NVMET_LS_CTX_COUNT\t\t4\n\n/* for this implementation, assume small single frame rqst/rsp */\n#define NVME_FC_MAX_LS_BUFFER_SIZE\t\t2048\n\nstruct nvmet_fc_tgtport;\nstruct nvmet_fc_tgt_assoc;\n\nstruct nvmet_fc_ls_iod {\n\tstruct nvmefc_tgt_ls_req\t*lsreq;\n\tstruct nvmefc_tgt_fcp_req\t*fcpreq;\t/* only if RS */\n\n\tstruct list_head\t\tls_list;\t/* tgtport->ls_list */\n\n\tstruct nvmet_fc_tgtport\t\t*tgtport;\n\tstruct nvmet_fc_tgt_assoc\t*assoc;\n\n\tu8\t\t\t\t*rqstbuf;\n\tu8\t\t\t\t*rspbuf;\n\tu16\t\t\t\trqstdatalen;\n\tdma_addr_t\t\t\trspdma;\n\n\tstruct scatterlist\t\tsg[2];\n\n\tstruct work_struct\t\twork;\n} __aligned(sizeof(unsigned long long));\n\n#define NVMET_FC_MAX_SEQ_LENGTH\t\t(256 * 1024)\n#define NVMET_FC_MAX_XFR_SGENTS\t\t(NVMET_FC_MAX_SEQ_LENGTH / PAGE_SIZE)\n\nenum nvmet_fcp_datadir {\n\tNVMET_FCP_NODATA,\n\tNVMET_FCP_WRITE,\n\tNVMET_FCP_READ,\n\tNVMET_FCP_ABORTED,\n};\n\nstruct nvmet_fc_fcp_iod {\n\tstruct nvmefc_tgt_fcp_req\t*fcpreq;\n\n\tstruct nvme_fc_cmd_iu\t\tcmdiubuf;\n\tstruct nvme_fc_ersp_iu\t\trspiubuf;\n\tdma_addr_t\t\t\trspdma;\n\tstruct scatterlist\t\t*data_sg;\n\tint\t\t\t\tdata_sg_cnt;\n\tu32\t\t\t\ttotal_length;\n\tu32\t\t\t\toffset;\n\tenum nvmet_fcp_datadir\t\tio_dir;\n\tbool\t\t\t\tactive;\n\tbool\t\t\t\tabort;\n\tbool\t\t\t\taborted;\n\tbool\t\t\t\twritedataactive;\n\tspinlock_t\t\t\tflock;\n\n\tstruct nvmet_req\t\treq;\n\tstruct work_struct\t\twork;\n\tstruct work_struct\t\tdone_work;\n\n\tstruct nvmet_fc_tgtport\t\t*tgtport;\n\tstruct nvmet_fc_tgt_queue\t*queue;\n\n\tstruct list_head\t\tfcp_list;\t/* tgtport->fcp_list */\n};\n\nstruct nvmet_fc_tgtport {\n\n\tstruct nvmet_fc_target_port\tfc_target_port;\n\n\tstruct list_head\t\ttgt_list; /* nvmet_fc_target_list */\n\tstruct device\t\t\t*dev;\t/* dev for dma mapping */\n\tstruct nvmet_fc_target_template\t*ops;\n\n\tstruct nvmet_fc_ls_iod\t\t*iod;\n\tspinlock_t\t\t\tlock;\n\tstruct list_head\t\tls_list;\n\tstruct list_head\t\tls_busylist;\n\tstruct list_head\t\tassoc_list;\n\tstruct ida\t\t\tassoc_cnt;\n\tstruct nvmet_port\t\t*port;\n\tstruct kref\t\t\tref;\n\tu32\t\t\t\tmax_sg_cnt;\n};\n\nstruct nvmet_fc_defer_fcp_req {\n\tstruct list_head\t\treq_list;\n\tstruct nvmefc_tgt_fcp_req\t*fcp_req;\n};\n\nstruct nvmet_fc_tgt_queue {\n\tbool\t\t\t\tninetypercent;\n\tu16\t\t\t\tqid;\n\tu16\t\t\t\tsqsize;\n\tu16\t\t\t\tersp_ratio;\n\t__le16\t\t\t\tsqhd;\n\tint\t\t\t\tcpu;\n\tatomic_t\t\t\tconnected;\n\tatomic_t\t\t\tsqtail;\n\tatomic_t\t\t\tzrspcnt;\n\tatomic_t\t\t\trsn;\n\tspinlock_t\t\t\tqlock;\n\tstruct nvmet_port\t\t*port;\n\tstruct nvmet_cq\t\t\tnvme_cq;\n\tstruct nvmet_sq\t\t\tnvme_sq;\n\tstruct nvmet_fc_tgt_assoc\t*assoc;\n\tstruct nvmet_fc_fcp_iod\t\t*fod;\t\t/* array of fcp_iods */\n\tstruct list_head\t\tfod_list;\n\tstruct list_head\t\tpending_cmd_list;\n\tstruct list_head\t\tavail_defer_list;\n\tstruct workqueue_struct\t\t*work_q;\n\tstruct kref\t\t\tref;\n} __aligned(sizeof(unsigned long long));\n\nstruct nvmet_fc_tgt_assoc {\n\tu64\t\t\t\tassociation_id;\n\tu32\t\t\t\ta_id;\n\tstruct nvmet_fc_tgtport\t\t*tgtport;\n\tstruct list_head\t\ta_list;\n\tstruct nvmet_fc_tgt_queue\t*queues[NVMET_NR_QUEUES + 1];\n\tstruct kref\t\t\tref;\n};\n\n\nstatic inline int\nnvmet_fc_iodnum(struct nvmet_fc_ls_iod *iodptr)\n{\n\treturn (iodptr - iodptr->tgtport->iod);\n}\n\nstatic inline int\nnvmet_fc_fodnum(struct nvmet_fc_fcp_iod *fodptr)\n{\n\treturn (fodptr - fodptr->queue->fod);\n}\n\n\n/*\n * Association and Connection IDs:\n *\n * Association ID will have random number in upper 6 bytes and zero\n *   in lower 2 bytes\n *\n * Connection IDs will be Association ID with QID or'd in lower 2 bytes\n *\n * note: Association ID = Connection ID for queue 0\n */\n#define BYTES_FOR_QID\t\t\tsizeof(u16)\n#define BYTES_FOR_QID_SHIFT\t\t(BYTES_FOR_QID * 8)\n#define NVMET_FC_QUEUEID_MASK\t\t((u64)((1 << BYTES_FOR_QID_SHIFT) - 1))\n\nstatic inline u64\nnvmet_fc_makeconnid(struct nvmet_fc_tgt_assoc *assoc, u16 qid)\n{\n\treturn (assoc->association_id | qid);\n}\n\nstatic inline u64\nnvmet_fc_getassociationid(u64 connectionid)\n{\n\treturn connectionid & ~NVMET_FC_QUEUEID_MASK;\n}\n\nstatic inline u16\nnvmet_fc_getqueueid(u64 connectionid)\n{\n\treturn (u16)(connectionid & NVMET_FC_QUEUEID_MASK);\n}\n\nstatic inline struct nvmet_fc_tgtport *\ntargetport_to_tgtport(struct nvmet_fc_target_port *targetport)\n{\n\treturn container_of(targetport, struct nvmet_fc_tgtport,\n\t\t\t\t fc_target_port);\n}\n\nstatic inline struct nvmet_fc_fcp_iod *\nnvmet_req_to_fod(struct nvmet_req *nvme_req)\n{\n\treturn container_of(nvme_req, struct nvmet_fc_fcp_iod, req);\n}\n\n\n/* *************************** Globals **************************** */\n\n\nstatic DEFINE_SPINLOCK(nvmet_fc_tgtlock);\n\nstatic LIST_HEAD(nvmet_fc_target_list);\nstatic DEFINE_IDA(nvmet_fc_tgtport_cnt);\n\n\nstatic void nvmet_fc_handle_ls_rqst_work(struct work_struct *work);\nstatic void nvmet_fc_handle_fcp_rqst_work(struct work_struct *work);\nstatic void nvmet_fc_fcp_rqst_op_done_work(struct work_struct *work);\nstatic void nvmet_fc_tgt_a_put(struct nvmet_fc_tgt_assoc *assoc);\nstatic int nvmet_fc_tgt_a_get(struct nvmet_fc_tgt_assoc *assoc);\nstatic void nvmet_fc_tgt_q_put(struct nvmet_fc_tgt_queue *queue);\nstatic int nvmet_fc_tgt_q_get(struct nvmet_fc_tgt_queue *queue);\nstatic void nvmet_fc_tgtport_put(struct nvmet_fc_tgtport *tgtport);\nstatic int nvmet_fc_tgtport_get(struct nvmet_fc_tgtport *tgtport);\nstatic void nvmet_fc_handle_fcp_rqst(struct nvmet_fc_tgtport *tgtport,\n\t\t\t\t\tstruct nvmet_fc_fcp_iod *fod);\n\n\n/* *********************** FC-NVME DMA Handling **************************** */\n\n/*\n * The fcloop device passes in a NULL device pointer. Real LLD's will\n * pass in a valid device pointer. If NULL is passed to the dma mapping\n * routines, depending on the platform, it may or may not succeed, and\n * may crash.\n *\n * As such:\n * Wrapper all the dma routines and check the dev pointer.\n *\n * If simple mappings (return just a dma address, we'll noop them,\n * returning a dma address of 0.\n *\n * On more complex mappings (dma_map_sg), a pseudo routine fills\n * in the scatter list, setting all dma addresses to 0.\n */\n\nstatic inline dma_addr_t\nfc_dma_map_single(struct device *dev, void *ptr, size_t size,\n\t\tenum dma_data_direction dir)\n{\n\treturn dev ? dma_map_single(dev, ptr, size, dir) : (dma_addr_t)0L;\n}\n\nstatic inline int\nfc_dma_mapping_error(struct device *dev, dma_addr_t dma_addr)\n{\n\treturn dev ? dma_mapping_error(dev, dma_addr) : 0;\n}\n\nstatic inline void\nfc_dma_unmap_single(struct device *dev, dma_addr_t addr, size_t size,\n\tenum dma_data_direction dir)\n{\n\tif (dev)\n\t\tdma_unmap_single(dev, addr, size, dir);\n}\n\nstatic inline void\nfc_dma_sync_single_for_cpu(struct device *dev, dma_addr_t addr, size_t size,\n\t\tenum dma_data_direction dir)\n{\n\tif (dev)\n\t\tdma_sync_single_for_cpu(dev, addr, size, dir);\n}\n\nstatic inline void\nfc_dma_sync_single_for_device(struct device *dev, dma_addr_t addr, size_t size,\n\t\tenum dma_data_direction dir)\n{\n\tif (dev)\n\t\tdma_sync_single_for_device(dev, addr, size, dir);\n}\n\n/* pseudo dma_map_sg call */\nstatic int\nfc_map_sg(struct scatterlist *sg, int nents)\n{\n\tstruct scatterlist *s;\n\tint i;\n\n\tWARN_ON(nents == 0 || sg[0].length == 0);\n\n\tfor_each_sg(sg, s, nents, i) {\n\t\ts->dma_address = 0L;\n#ifdef CONFIG_NEED_SG_DMA_LENGTH\n\t\ts->dma_length = s->length;\n#endif\n\t}\n\treturn nents;\n}\n\nstatic inline int\nfc_dma_map_sg(struct device *dev, struct scatterlist *sg, int nents,\n\t\tenum dma_data_direction dir)\n{\n\treturn dev ? dma_map_sg(dev, sg, nents, dir) : fc_map_sg(sg, nents);\n}\n\nstatic inline void\nfc_dma_unmap_sg(struct device *dev, struct scatterlist *sg, int nents,\n\t\tenum dma_data_direction dir)\n{\n\tif (dev)\n\t\tdma_unmap_sg(dev, sg, nents, dir);\n}\n\n\n/* *********************** FC-NVME Port Management ************************ */\n\n\nstatic int\nnvmet_fc_alloc_ls_iodlist(struct nvmet_fc_tgtport *tgtport)\n{\n\tstruct nvmet_fc_ls_iod *iod;\n\tint i;\n\n\tiod = kcalloc(NVMET_LS_CTX_COUNT, sizeof(struct nvmet_fc_ls_iod),\n\t\t\tGFP_KERNEL);\n\tif (!iod)\n\t\treturn -ENOMEM;\n\n\ttgtport->iod = iod;\n\n\tfor (i = 0; i < NVMET_LS_CTX_COUNT; iod++, i++) {\n\t\tINIT_WORK(&iod->work, nvmet_fc_handle_ls_rqst_work);\n\t\tiod->tgtport = tgtport;\n\t\tlist_add_tail(&iod->ls_list, &tgtport->ls_list);\n\n\t\tiod->rqstbuf = kcalloc(2, NVME_FC_MAX_LS_BUFFER_SIZE,\n\t\t\tGFP_KERNEL);\n\t\tif (!iod->rqstbuf)\n\t\t\tgoto out_fail;\n\n\t\tiod->rspbuf = iod->rqstbuf + NVME_FC_MAX_LS_BUFFER_SIZE;\n\n\t\tiod->rspdma = fc_dma_map_single(tgtport->dev, iod->rspbuf,\n\t\t\t\t\t\tNVME_FC_MAX_LS_BUFFER_SIZE,\n\t\t\t\t\t\tDMA_TO_DEVICE);\n\t\tif (fc_dma_mapping_error(tgtport->dev, iod->rspdma))\n\t\t\tgoto out_fail;\n\t}\n\n\treturn 0;\n\nout_fail:\n\tkfree(iod->rqstbuf);\n\tlist_del(&iod->ls_list);\n\tfor (iod--, i--; i >= 0; iod--, i--) {\n\t\tfc_dma_unmap_single(tgtport->dev, iod->rspdma,\n\t\t\t\tNVME_FC_MAX_LS_BUFFER_SIZE, DMA_TO_DEVICE);\n\t\tkfree(iod->rqstbuf);\n\t\tlist_del(&iod->ls_list);\n\t}\n\n\tkfree(iod);\n\n\treturn -EFAULT;\n}\n\nstatic void\nnvmet_fc_free_ls_iodlist(struct nvmet_fc_tgtport *tgtport)\n{\n\tstruct nvmet_fc_ls_iod *iod = tgtport->iod;\n\tint i;\n\n\tfor (i = 0; i < NVMET_LS_CTX_COUNT; iod++, i++) {\n\t\tfc_dma_unmap_single(tgtport->dev,\n\t\t\t\tiod->rspdma, NVME_FC_MAX_LS_BUFFER_SIZE,\n\t\t\t\tDMA_TO_DEVICE);\n\t\tkfree(iod->rqstbuf);\n\t\tlist_del(&iod->ls_list);\n\t}\n\tkfree(tgtport->iod);\n}\n\nstatic struct nvmet_fc_ls_iod *\nnvmet_fc_alloc_ls_iod(struct nvmet_fc_tgtport *tgtport)\n{\n\tstruct nvmet_fc_ls_iod *iod;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&tgtport->lock, flags);\n\tiod = list_first_entry_or_null(&tgtport->ls_list,\n\t\t\t\t\tstruct nvmet_fc_ls_iod, ls_list);\n\tif (iod)\n\t\tlist_move_tail(&iod->ls_list, &tgtport->ls_busylist);\n\tspin_unlock_irqrestore(&tgtport->lock, flags);\n\treturn iod;\n}\n\n\nstatic void\nnvmet_fc_free_ls_iod(struct nvmet_fc_tgtport *tgtport,\n\t\t\tstruct nvmet_fc_ls_iod *iod)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&tgtport->lock, flags);\n\tlist_move(&iod->ls_list, &tgtport->ls_list);\n\tspin_unlock_irqrestore(&tgtport->lock, flags);\n}\n\nstatic void\nnvmet_fc_prep_fcp_iodlist(struct nvmet_fc_tgtport *tgtport,\n\t\t\t\tstruct nvmet_fc_tgt_queue *queue)\n{\n\tstruct nvmet_fc_fcp_iod *fod = queue->fod;\n\tint i;\n\n\tfor (i = 0; i < queue->sqsize; fod++, i++) {\n\t\tINIT_WORK(&fod->work, nvmet_fc_handle_fcp_rqst_work);\n\t\tINIT_WORK(&fod->done_work, nvmet_fc_fcp_rqst_op_done_work);\n\t\tfod->tgtport = tgtport;\n\t\tfod->queue = queue;\n\t\tfod->active = false;\n\t\tfod->abort = false;\n\t\tfod->aborted = false;\n\t\tfod->fcpreq = NULL;\n\t\tlist_add_tail(&fod->fcp_list, &queue->fod_list);\n\t\tspin_lock_init(&fod->flock);\n\n\t\tfod->rspdma = fc_dma_map_single(tgtport->dev, &fod->rspiubuf,\n\t\t\t\t\tsizeof(fod->rspiubuf), DMA_TO_DEVICE);\n\t\tif (fc_dma_mapping_error(tgtport->dev, fod->rspdma)) {\n\t\t\tlist_del(&fod->fcp_list);\n\t\t\tfor (fod--, i--; i >= 0; fod--, i--) {\n\t\t\t\tfc_dma_unmap_single(tgtport->dev, fod->rspdma,\n\t\t\t\t\t\tsizeof(fod->rspiubuf),\n\t\t\t\t\t\tDMA_TO_DEVICE);\n\t\t\t\tfod->rspdma = 0L;\n\t\t\t\tlist_del(&fod->fcp_list);\n\t\t\t}\n\n\t\t\treturn;\n\t\t}\n\t}\n}\n\nstatic void\nnvmet_fc_destroy_fcp_iodlist(struct nvmet_fc_tgtport *tgtport,\n\t\t\t\tstruct nvmet_fc_tgt_queue *queue)\n{\n\tstruct nvmet_fc_fcp_iod *fod = queue->fod;\n\tint i;\n\n\tfor (i = 0; i < queue->sqsize; fod++, i++) {\n\t\tif (fod->rspdma)\n\t\t\tfc_dma_unmap_single(tgtport->dev, fod->rspdma,\n\t\t\t\tsizeof(fod->rspiubuf), DMA_TO_DEVICE);\n\t}\n}\n\nstatic struct nvmet_fc_fcp_iod *\nnvmet_fc_alloc_fcp_iod(struct nvmet_fc_tgt_queue *queue)\n{\n\tstruct nvmet_fc_fcp_iod *fod;\n\n\tlockdep_assert_held(&queue->qlock);\n\n\tfod = list_first_entry_or_null(&queue->fod_list,\n\t\t\t\t\tstruct nvmet_fc_fcp_iod, fcp_list);\n\tif (fod) {\n\t\tlist_del(&fod->fcp_list);\n\t\tfod->active = true;\n\t\t/*\n\t\t * no queue reference is taken, as it was taken by the\n\t\t * queue lookup just prior to the allocation. The iod\n\t\t * will \"inherit\" that reference.\n\t\t */\n\t}\n\treturn fod;\n}\n\n\nstatic void\nnvmet_fc_queue_fcp_req(struct nvmet_fc_tgtport *tgtport,\n\t\t       struct nvmet_fc_tgt_queue *queue,\n\t\t       struct nvmefc_tgt_fcp_req *fcpreq)\n{\n\tstruct nvmet_fc_fcp_iod *fod = fcpreq->nvmet_fc_private;\n\n\t/*\n\t * put all admin cmds on hw queue id 0. All io commands go to\n\t * the respective hw queue based on a modulo basis\n\t */\n\tfcpreq->hwqid = queue->qid ?\n\t\t\t((queue->qid - 1) % tgtport->ops->max_hw_queues) : 0;\n\n\tif (tgtport->ops->target_features & NVMET_FCTGTFEAT_CMD_IN_ISR)\n\t\tqueue_work_on(queue->cpu, queue->work_q, &fod->work);\n\telse\n\t\tnvmet_fc_handle_fcp_rqst(tgtport, fod);\n}\n\nstatic void\nnvmet_fc_free_fcp_iod(struct nvmet_fc_tgt_queue *queue,\n\t\t\tstruct nvmet_fc_fcp_iod *fod)\n{\n\tstruct nvmefc_tgt_fcp_req *fcpreq = fod->fcpreq;\n\tstruct nvmet_fc_tgtport *tgtport = fod->tgtport;\n\tstruct nvmet_fc_defer_fcp_req *deferfcp;\n\tunsigned long flags;\n\n\tfc_dma_sync_single_for_cpu(tgtport->dev, fod->rspdma,\n\t\t\t\tsizeof(fod->rspiubuf), DMA_TO_DEVICE);\n\n\tfcpreq->nvmet_fc_private = NULL;\n\n\tfod->active = false;\n\tfod->abort = false;\n\tfod->aborted = false;\n\tfod->writedataactive = false;\n\tfod->fcpreq = NULL;\n\n\ttgtport->ops->fcp_req_release(&tgtport->fc_target_port, fcpreq);\n\n\tspin_lock_irqsave(&queue->qlock, flags);\n\tdeferfcp = list_first_entry_or_null(&queue->pending_cmd_list,\n\t\t\t\tstruct nvmet_fc_defer_fcp_req, req_list);\n\tif (!deferfcp) {\n\t\tlist_add_tail(&fod->fcp_list, &fod->queue->fod_list);\n\t\tspin_unlock_irqrestore(&queue->qlock, flags);\n\n\t\t/* Release reference taken at queue lookup and fod allocation */\n\t\tnvmet_fc_tgt_q_put(queue);\n\t\treturn;\n\t}\n\n\t/* Re-use the fod for the next pending cmd that was deferred */\n\tlist_del(&deferfcp->req_list);\n\n\tfcpreq = deferfcp->fcp_req;\n\n\t/* deferfcp can be reused for another IO at a later date */\n\tlist_add_tail(&deferfcp->req_list, &queue->avail_defer_list);\n\n\tspin_unlock_irqrestore(&queue->qlock, flags);\n\n\t/* Save NVME CMD IO in fod */\n\tmemcpy(&fod->cmdiubuf, fcpreq->rspaddr, fcpreq->rsplen);\n\n\t/* Setup new fcpreq to be processed */\n\tfcpreq->rspaddr = NULL;\n\tfcpreq->rsplen  = 0;\n\tfcpreq->nvmet_fc_private = fod;\n\tfod->fcpreq = fcpreq;\n\tfod->active = true;\n\n\t/* inform LLDD IO is now being processed */\n\ttgtport->ops->defer_rcv(&tgtport->fc_target_port, fcpreq);\n\n\t/* Submit deferred IO for processing */\n\tnvmet_fc_queue_fcp_req(tgtport, queue, fcpreq);\n\n\t/*\n\t * Leave the queue lookup get reference taken when\n\t * fod was originally allocated.\n\t */\n}\n\nstatic int\nnvmet_fc_queue_to_cpu(struct nvmet_fc_tgtport *tgtport, int qid)\n{\n\tint cpu, idx, cnt;\n\n\tif (tgtport->ops->max_hw_queues == 1)\n\t\treturn WORK_CPU_UNBOUND;\n\n\t/* Simple cpu selection based on qid modulo active cpu count */\n\tidx = !qid ? 0 : (qid - 1) % num_active_cpus();\n\n\t/* find the n'th active cpu */\n\tfor (cpu = 0, cnt = 0; ; ) {\n\t\tif (cpu_active(cpu)) {\n\t\t\tif (cnt == idx)\n\t\t\t\tbreak;\n\t\t\tcnt++;\n\t\t}\n\t\tcpu = (cpu + 1) % num_possible_cpus();\n\t}\n\n\treturn cpu;\n}\n\nstatic struct nvmet_fc_tgt_queue *\nnvmet_fc_alloc_target_queue(struct nvmet_fc_tgt_assoc *assoc,\n\t\t\tu16 qid, u16 sqsize)\n{\n\tstruct nvmet_fc_tgt_queue *queue;\n\tunsigned long flags;\n\tint ret;\n\n\tif (qid > NVMET_NR_QUEUES)\n\t\treturn NULL;\n\n\tqueue = kzalloc((sizeof(*queue) +\n\t\t\t\t(sizeof(struct nvmet_fc_fcp_iod) * sqsize)),\n\t\t\t\tGFP_KERNEL);\n\tif (!queue)\n\t\treturn NULL;\n\n\tif (!nvmet_fc_tgt_a_get(assoc))\n\t\tgoto out_free_queue;\n\n\tqueue->work_q = alloc_workqueue(\"ntfc%d.%d.%d\", 0, 0,\n\t\t\t\tassoc->tgtport->fc_target_port.port_num,\n\t\t\t\tassoc->a_id, qid);\n\tif (!queue->work_q)\n\t\tgoto out_a_put;\n\n\tqueue->fod = (struct nvmet_fc_fcp_iod *)&queue[1];\n\tqueue->qid = qid;\n\tqueue->sqsize = sqsize;\n\tqueue->assoc = assoc;\n\tqueue->port = assoc->tgtport->port;\n\tqueue->cpu = nvmet_fc_queue_to_cpu(assoc->tgtport, qid);\n\tINIT_LIST_HEAD(&queue->fod_list);\n\tINIT_LIST_HEAD(&queue->avail_defer_list);\n\tINIT_LIST_HEAD(&queue->pending_cmd_list);\n\tatomic_set(&queue->connected, 0);\n\tatomic_set(&queue->sqtail, 0);\n\tatomic_set(&queue->rsn, 1);\n\tatomic_set(&queue->zrspcnt, 0);\n\tspin_lock_init(&queue->qlock);\n\tkref_init(&queue->ref);\n\n\tnvmet_fc_prep_fcp_iodlist(assoc->tgtport, queue);\n\n\tret = nvmet_sq_init(&queue->nvme_sq);\n\tif (ret)\n\t\tgoto out_fail_iodlist;\n\n\tWARN_ON(assoc->queues[qid]);\n\tspin_lock_irqsave(&assoc->tgtport->lock, flags);\n\tassoc->queues[qid] = queue;\n\tspin_unlock_irqrestore(&assoc->tgtport->lock, flags);\n\n\treturn queue;\n\nout_fail_iodlist:\n\tnvmet_fc_destroy_fcp_iodlist(assoc->tgtport, queue);\n\tdestroy_workqueue(queue->work_q);\nout_a_put:\n\tnvmet_fc_tgt_a_put(assoc);\nout_free_queue:\n\tkfree(queue);\n\treturn NULL;\n}\n\n\nstatic void\nnvmet_fc_tgt_queue_free(struct kref *ref)\n{\n\tstruct nvmet_fc_tgt_queue *queue =\n\t\tcontainer_of(ref, struct nvmet_fc_tgt_queue, ref);\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&queue->assoc->tgtport->lock, flags);\n\tqueue->assoc->queues[queue->qid] = NULL;\n\tspin_unlock_irqrestore(&queue->assoc->tgtport->lock, flags);\n\n\tnvmet_fc_destroy_fcp_iodlist(queue->assoc->tgtport, queue);\n\n\tnvmet_fc_tgt_a_put(queue->assoc);\n\n\tdestroy_workqueue(queue->work_q);\n\n\tkfree(queue);\n}\n\nstatic void\nnvmet_fc_tgt_q_put(struct nvmet_fc_tgt_queue *queue)\n{\n\tkref_put(&queue->ref, nvmet_fc_tgt_queue_free);\n}\n\nstatic int\nnvmet_fc_tgt_q_get(struct nvmet_fc_tgt_queue *queue)\n{\n\treturn kref_get_unless_zero(&queue->ref);\n}\n\n\nstatic void\nnvmet_fc_delete_target_queue(struct nvmet_fc_tgt_queue *queue)\n{\n\tstruct nvmet_fc_tgtport *tgtport = queue->assoc->tgtport;\n\tstruct nvmet_fc_fcp_iod *fod = queue->fod;\n\tstruct nvmet_fc_defer_fcp_req *deferfcp, *tempptr;\n\tunsigned long flags;\n\tint i, writedataactive;\n\tbool disconnect;\n\n\tdisconnect = atomic_xchg(&queue->connected, 0);\n\n\tspin_lock_irqsave(&queue->qlock, flags);\n\t/* about outstanding io's */\n\tfor (i = 0; i < queue->sqsize; fod++, i++) {\n\t\tif (fod->active) {\n\t\t\tspin_lock(&fod->flock);\n\t\t\tfod->abort = true;\n\t\t\twritedataactive = fod->writedataactive;\n\t\t\tspin_unlock(&fod->flock);\n\t\t\t/*\n\t\t\t * only call lldd abort routine if waiting for\n\t\t\t * writedata. other outstanding ops should finish\n\t\t\t * on their own.\n\t\t\t */\n\t\t\tif (writedataactive) {\n\t\t\t\tspin_lock(&fod->flock);\n\t\t\t\tfod->aborted = true;\n\t\t\t\tspin_unlock(&fod->flock);\n\t\t\t\ttgtport->ops->fcp_abort(\n\t\t\t\t\t&tgtport->fc_target_port, fod->fcpreq);\n\t\t\t}\n\t\t}\n\t}\n\n\t/* Cleanup defer'ed IOs in queue */\n\tlist_for_each_entry_safe(deferfcp, tempptr, &queue->avail_defer_list,\n\t\t\t\treq_list) {\n\t\tlist_del(&deferfcp->req_list);\n\t\tkfree(deferfcp);\n\t}\n\n\tfor (;;) {\n\t\tdeferfcp = list_first_entry_or_null(&queue->pending_cmd_list,\n\t\t\t\tstruct nvmet_fc_defer_fcp_req, req_list);\n\t\tif (!deferfcp)\n\t\t\tbreak;\n\n\t\tlist_del(&deferfcp->req_list);\n\t\tspin_unlock_irqrestore(&queue->qlock, flags);\n\n\t\ttgtport->ops->defer_rcv(&tgtport->fc_target_port,\n\t\t\t\tdeferfcp->fcp_req);\n\n\t\ttgtport->ops->fcp_abort(&tgtport->fc_target_port,\n\t\t\t\tdeferfcp->fcp_req);\n\n\t\ttgtport->ops->fcp_req_release(&tgtport->fc_target_port,\n\t\t\t\tdeferfcp->fcp_req);\n\n\t\tkfree(deferfcp);\n\n\t\tspin_lock_irqsave(&queue->qlock, flags);\n\t}\n\tspin_unlock_irqrestore(&queue->qlock, flags);\n\n\tflush_workqueue(queue->work_q);\n\n\tif (disconnect)\n\t\tnvmet_sq_destroy(&queue->nvme_sq);\n\n\tnvmet_fc_tgt_q_put(queue);\n}\n\nstatic struct nvmet_fc_tgt_queue *\nnvmet_fc_find_target_queue(struct nvmet_fc_tgtport *tgtport,\n\t\t\t\tu64 connection_id)\n{\n\tstruct nvmet_fc_tgt_assoc *assoc;\n\tstruct nvmet_fc_tgt_queue *queue;\n\tu64 association_id = nvmet_fc_getassociationid(connection_id);\n\tu16 qid = nvmet_fc_getqueueid(connection_id);\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&tgtport->lock, flags);\n\tlist_for_each_entry(assoc, &tgtport->assoc_list, a_list) {\n\t\tif (association_id == assoc->association_id) {\n\t\t\tqueue = assoc->queues[qid];\n\t\t\tif (queue &&\n\t\t\t    (!atomic_read(&queue->connected) ||\n\t\t\t     !nvmet_fc_tgt_q_get(queue)))\n\t\t\t\tqueue = NULL;\n\t\t\tspin_unlock_irqrestore(&tgtport->lock, flags);\n\t\t\treturn queue;\n\t\t}\n\t}\n\tspin_unlock_irqrestore(&tgtport->lock, flags);\n\treturn NULL;\n}\n\nstatic struct nvmet_fc_tgt_assoc *\nnvmet_fc_alloc_target_assoc(struct nvmet_fc_tgtport *tgtport)\n{\n\tstruct nvmet_fc_tgt_assoc *assoc, *tmpassoc;\n\tunsigned long flags;\n\tu64 ran;\n\tint idx;\n\tbool needrandom = true;\n\n\tassoc = kzalloc(sizeof(*assoc), GFP_KERNEL);\n\tif (!assoc)\n\t\treturn NULL;\n\n\tidx = ida_simple_get(&tgtport->assoc_cnt, 0, 0, GFP_KERNEL);\n\tif (idx < 0)\n\t\tgoto out_free_assoc;\n\n\tif (!nvmet_fc_tgtport_get(tgtport))\n\t\tgoto out_ida_put;\n\n\tassoc->tgtport = tgtport;\n\tassoc->a_id = idx;\n\tINIT_LIST_HEAD(&assoc->a_list);\n\tkref_init(&assoc->ref);\n\n\twhile (needrandom) {\n\t\tget_random_bytes(&ran, sizeof(ran) - BYTES_FOR_QID);\n\t\tran = ran << BYTES_FOR_QID_SHIFT;\n\n\t\tspin_lock_irqsave(&tgtport->lock, flags);\n\t\tneedrandom = false;\n\t\tlist_for_each_entry(tmpassoc, &tgtport->assoc_list, a_list)\n\t\t\tif (ran == tmpassoc->association_id) {\n\t\t\t\tneedrandom = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\tif (!needrandom) {\n\t\t\tassoc->association_id = ran;\n\t\t\tlist_add_tail(&assoc->a_list, &tgtport->assoc_list);\n\t\t}\n\t\tspin_unlock_irqrestore(&tgtport->lock, flags);\n\t}\n\n\treturn assoc;\n\nout_ida_put:\n\tida_simple_remove(&tgtport->assoc_cnt, idx);\nout_free_assoc:\n\tkfree(assoc);\n\treturn NULL;\n}\n\nstatic void\nnvmet_fc_target_assoc_free(struct kref *ref)\n{\n\tstruct nvmet_fc_tgt_assoc *assoc =\n\t\tcontainer_of(ref, struct nvmet_fc_tgt_assoc, ref);\n\tstruct nvmet_fc_tgtport *tgtport = assoc->tgtport;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&tgtport->lock, flags);\n\tlist_del(&assoc->a_list);\n\tspin_unlock_irqrestore(&tgtport->lock, flags);\n\tida_simple_remove(&tgtport->assoc_cnt, assoc->a_id);\n\tkfree(assoc);\n\tnvmet_fc_tgtport_put(tgtport);\n}\n\nstatic void\nnvmet_fc_tgt_a_put(struct nvmet_fc_tgt_assoc *assoc)\n{\n\tkref_put(&assoc->ref, nvmet_fc_target_assoc_free);\n}\n\nstatic int\nnvmet_fc_tgt_a_get(struct nvmet_fc_tgt_assoc *assoc)\n{\n\treturn kref_get_unless_zero(&assoc->ref);\n}\n\nstatic void\nnvmet_fc_delete_target_assoc(struct nvmet_fc_tgt_assoc *assoc)\n{\n\tstruct nvmet_fc_tgtport *tgtport = assoc->tgtport;\n\tstruct nvmet_fc_tgt_queue *queue;\n\tunsigned long flags;\n\tint i;\n\n\tspin_lock_irqsave(&tgtport->lock, flags);\n\tfor (i = NVMET_NR_QUEUES; i >= 0; i--) {\n\t\tqueue = assoc->queues[i];\n\t\tif (queue) {\n\t\t\tif (!nvmet_fc_tgt_q_get(queue))\n\t\t\t\tcontinue;\n\t\t\tspin_unlock_irqrestore(&tgtport->lock, flags);\n\t\t\tnvmet_fc_delete_target_queue(queue);\n\t\t\tnvmet_fc_tgt_q_put(queue);\n\t\t\tspin_lock_irqsave(&tgtport->lock, flags);\n\t\t}\n\t}\n\tspin_unlock_irqrestore(&tgtport->lock, flags);\n\n\tnvmet_fc_tgt_a_put(assoc);\n}\n\nstatic struct nvmet_fc_tgt_assoc *\nnvmet_fc_find_target_assoc(struct nvmet_fc_tgtport *tgtport,\n\t\t\t\tu64 association_id)\n{\n\tstruct nvmet_fc_tgt_assoc *assoc;\n\tstruct nvmet_fc_tgt_assoc *ret = NULL;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&tgtport->lock, flags);\n\tlist_for_each_entry(assoc, &tgtport->assoc_list, a_list) {\n\t\tif (association_id == assoc->association_id) {\n\t\t\tret = assoc;\n\t\t\tnvmet_fc_tgt_a_get(assoc);\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock_irqrestore(&tgtport->lock, flags);\n\n\treturn ret;\n}\n\n\n/**\n * nvme_fc_register_targetport - transport entry point called by an\n *                              LLDD to register the existence of a local\n *                              NVME subystem FC port.\n * @pinfo:     pointer to information about the port to be registered\n * @template:  LLDD entrypoints and operational parameters for the port\n * @dev:       physical hardware device node port corresponds to. Will be\n *             used for DMA mappings\n * @portptr:   pointer to a local port pointer. Upon success, the routine\n *             will allocate a nvme_fc_local_port structure and place its\n *             address in the local port pointer. Upon failure, local port\n *             pointer will be set to NULL.\n *\n * Returns:\n * a completion status. Must be 0 upon success; a negative errno\n * (ex: -ENXIO) upon failure.\n */\nint\nnvmet_fc_register_targetport(struct nvmet_fc_port_info *pinfo,\n\t\t\tstruct nvmet_fc_target_template *template,\n\t\t\tstruct device *dev,\n\t\t\tstruct nvmet_fc_target_port **portptr)\n{\n\tstruct nvmet_fc_tgtport *newrec;\n\tunsigned long flags;\n\tint ret, idx;\n\n\tif (!template->xmt_ls_rsp || !template->fcp_op ||\n\t    !template->fcp_abort ||\n\t    !template->fcp_req_release || !template->targetport_delete ||\n\t    !template->max_hw_queues || !template->max_sgl_segments ||\n\t    !template->max_dif_sgl_segments || !template->dma_boundary) {\n\t\tret = -EINVAL;\n\t\tgoto out_regtgt_failed;\n\t}\n\n\tnewrec = kzalloc((sizeof(*newrec) + template->target_priv_sz),\n\t\t\t GFP_KERNEL);\n\tif (!newrec) {\n\t\tret = -ENOMEM;\n\t\tgoto out_regtgt_failed;\n\t}\n\n\tidx = ida_simple_get(&nvmet_fc_tgtport_cnt, 0, 0, GFP_KERNEL);\n\tif (idx < 0) {\n\t\tret = -ENOSPC;\n\t\tgoto out_fail_kfree;\n\t}\n\n\tif (!get_device(dev) && dev) {\n\t\tret = -ENODEV;\n\t\tgoto out_ida_put;\n\t}\n\n\tnewrec->fc_target_port.node_name = pinfo->node_name;\n\tnewrec->fc_target_port.port_name = pinfo->port_name;\n\tnewrec->fc_target_port.private = &newrec[1];\n\tnewrec->fc_target_port.port_id = pinfo->port_id;\n\tnewrec->fc_target_port.port_num = idx;\n\tINIT_LIST_HEAD(&newrec->tgt_list);\n\tnewrec->dev = dev;\n\tnewrec->ops = template;\n\tspin_lock_init(&newrec->lock);\n\tINIT_LIST_HEAD(&newrec->ls_list);\n\tINIT_LIST_HEAD(&newrec->ls_busylist);\n\tINIT_LIST_HEAD(&newrec->assoc_list);\n\tkref_init(&newrec->ref);\n\tida_init(&newrec->assoc_cnt);\n\tnewrec->max_sg_cnt = min_t(u32, NVMET_FC_MAX_XFR_SGENTS,\n\t\t\t\t\ttemplate->max_sgl_segments);\n\n\tret = nvmet_fc_alloc_ls_iodlist(newrec);\n\tif (ret) {\n\t\tret = -ENOMEM;\n\t\tgoto out_free_newrec;\n\t}\n\n\tspin_lock_irqsave(&nvmet_fc_tgtlock, flags);\n\tlist_add_tail(&newrec->tgt_list, &nvmet_fc_target_list);\n\tspin_unlock_irqrestore(&nvmet_fc_tgtlock, flags);\n\n\t*portptr = &newrec->fc_target_port;\n\treturn 0;\n\nout_free_newrec:\n\tput_device(dev);\nout_ida_put:\n\tida_simple_remove(&nvmet_fc_tgtport_cnt, idx);\nout_fail_kfree:\n\tkfree(newrec);\nout_regtgt_failed:\n\t*portptr = NULL;\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(nvmet_fc_register_targetport);\n\n\nstatic void\nnvmet_fc_free_tgtport(struct kref *ref)\n{\n\tstruct nvmet_fc_tgtport *tgtport =\n\t\tcontainer_of(ref, struct nvmet_fc_tgtport, ref);\n\tstruct device *dev = tgtport->dev;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&nvmet_fc_tgtlock, flags);\n\tlist_del(&tgtport->tgt_list);\n\tspin_unlock_irqrestore(&nvmet_fc_tgtlock, flags);\n\n\tnvmet_fc_free_ls_iodlist(tgtport);\n\n\t/* let the LLDD know we've finished tearing it down */\n\ttgtport->ops->targetport_delete(&tgtport->fc_target_port);\n\n\tida_simple_remove(&nvmet_fc_tgtport_cnt,\n\t\t\ttgtport->fc_target_port.port_num);\n\n\tida_destroy(&tgtport->assoc_cnt);\n\n\tkfree(tgtport);\n\n\tput_device(dev);\n}\n\nstatic void\nnvmet_fc_tgtport_put(struct nvmet_fc_tgtport *tgtport)\n{\n\tkref_put(&tgtport->ref, nvmet_fc_free_tgtport);\n}\n\nstatic int\nnvmet_fc_tgtport_get(struct nvmet_fc_tgtport *tgtport)\n{\n\treturn kref_get_unless_zero(&tgtport->ref);\n}\n\nstatic void\n__nvmet_fc_free_assocs(struct nvmet_fc_tgtport *tgtport)\n{\n\tstruct nvmet_fc_tgt_assoc *assoc, *next;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&tgtport->lock, flags);\n\tlist_for_each_entry_safe(assoc, next,\n\t\t\t\t&tgtport->assoc_list, a_list) {\n\t\tif (!nvmet_fc_tgt_a_get(assoc))\n\t\t\tcontinue;\n\t\tspin_unlock_irqrestore(&tgtport->lock, flags);\n\t\tnvmet_fc_delete_target_assoc(assoc);\n\t\tnvmet_fc_tgt_a_put(assoc);\n\t\tspin_lock_irqsave(&tgtport->lock, flags);\n\t}\n\tspin_unlock_irqrestore(&tgtport->lock, flags);\n}\n\n/*\n * nvmet layer has called to terminate an association\n */\nstatic void\nnvmet_fc_delete_ctrl(struct nvmet_ctrl *ctrl)\n{\n\tstruct nvmet_fc_tgtport *tgtport, *next;\n\tstruct nvmet_fc_tgt_assoc *assoc;\n\tstruct nvmet_fc_tgt_queue *queue;\n\tunsigned long flags;\n\tbool found_ctrl = false;\n\n\t/* this is a bit ugly, but don't want to make locks layered */\n\tspin_lock_irqsave(&nvmet_fc_tgtlock, flags);\n\tlist_for_each_entry_safe(tgtport, next, &nvmet_fc_target_list,\n\t\t\ttgt_list) {\n\t\tif (!nvmet_fc_tgtport_get(tgtport))\n\t\t\tcontinue;\n\t\tspin_unlock_irqrestore(&nvmet_fc_tgtlock, flags);\n\n\t\tspin_lock_irqsave(&tgtport->lock, flags);\n\t\tlist_for_each_entry(assoc, &tgtport->assoc_list, a_list) {\n\t\t\tqueue = assoc->queues[0];\n\t\t\tif (queue && queue->nvme_sq.ctrl == ctrl) {\n\t\t\t\tif (nvmet_fc_tgt_a_get(assoc))\n\t\t\t\t\tfound_ctrl = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tspin_unlock_irqrestore(&tgtport->lock, flags);\n\n\t\tnvmet_fc_tgtport_put(tgtport);\n\n\t\tif (found_ctrl) {\n\t\t\tnvmet_fc_delete_target_assoc(assoc);\n\t\t\tnvmet_fc_tgt_a_put(assoc);\n\t\t\treturn;\n\t\t}\n\n\t\tspin_lock_irqsave(&nvmet_fc_tgtlock, flags);\n\t}\n\tspin_unlock_irqrestore(&nvmet_fc_tgtlock, flags);\n}\n\n/**\n * nvme_fc_unregister_targetport - transport entry point called by an\n *                              LLDD to deregister/remove a previously\n *                              registered a local NVME subsystem FC port.\n * @tgtport: pointer to the (registered) target port that is to be\n *           deregistered.\n *\n * Returns:\n * a completion status. Must be 0 upon success; a negative errno\n * (ex: -ENXIO) upon failure.\n */\nint\nnvmet_fc_unregister_targetport(struct nvmet_fc_target_port *target_port)\n{\n\tstruct nvmet_fc_tgtport *tgtport = targetport_to_tgtport(target_port);\n\n\t/* terminate any outstanding associations */\n\t__nvmet_fc_free_assocs(tgtport);\n\n\tnvmet_fc_tgtport_put(tgtport);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(nvmet_fc_unregister_targetport);\n\n\n/* *********************** FC-NVME LS Handling **************************** */\n\n\nstatic void\nnvmet_fc_format_rsp_hdr(void *buf, u8 ls_cmd, __be32 desc_len, u8 rqst_ls_cmd)\n{\n\tstruct fcnvme_ls_acc_hdr *acc = buf;\n\n\tacc->w0.ls_cmd = ls_cmd;\n\tacc->desc_list_len = desc_len;\n\tacc->rqst.desc_tag = cpu_to_be32(FCNVME_LSDESC_RQST);\n\tacc->rqst.desc_len =\n\t\t\tfcnvme_lsdesc_len(sizeof(struct fcnvme_lsdesc_rqst));\n\tacc->rqst.w0.ls_cmd = rqst_ls_cmd;\n}\n\nstatic int\nnvmet_fc_format_rjt(void *buf, u16 buflen, u8 ls_cmd,\n\t\t\tu8 reason, u8 explanation, u8 vendor)\n{\n\tstruct fcnvme_ls_rjt *rjt = buf;\n\n\tnvmet_fc_format_rsp_hdr(buf, FCNVME_LSDESC_RQST,\n\t\t\tfcnvme_lsdesc_len(sizeof(struct fcnvme_ls_rjt)),\n\t\t\tls_cmd);\n\trjt->rjt.desc_tag = cpu_to_be32(FCNVME_LSDESC_RJT);\n\trjt->rjt.desc_len = fcnvme_lsdesc_len(sizeof(struct fcnvme_lsdesc_rjt));\n\trjt->rjt.reason_code = reason;\n\trjt->rjt.reason_explanation = explanation;\n\trjt->rjt.vendor = vendor;\n\n\treturn sizeof(struct fcnvme_ls_rjt);\n}\n\n/* Validation Error indexes into the string table below */\nenum {\n\tVERR_NO_ERROR\t\t= 0,\n\tVERR_CR_ASSOC_LEN\t= 1,\n\tVERR_CR_ASSOC_RQST_LEN\t= 2,\n\tVERR_CR_ASSOC_CMD\t= 3,\n\tVERR_CR_ASSOC_CMD_LEN\t= 4,\n\tVERR_ERSP_RATIO\t\t= 5,\n\tVERR_ASSOC_ALLOC_FAIL\t= 6,\n\tVERR_QUEUE_ALLOC_FAIL\t= 7,\n\tVERR_CR_CONN_LEN\t= 8,\n\tVERR_CR_CONN_RQST_LEN\t= 9,\n\tVERR_ASSOC_ID\t\t= 10,\n\tVERR_ASSOC_ID_LEN\t= 11,\n\tVERR_NO_ASSOC\t\t= 12,\n\tVERR_CONN_ID\t\t= 13,\n\tVERR_CONN_ID_LEN\t= 14,\n\tVERR_NO_CONN\t\t= 15,\n\tVERR_CR_CONN_CMD\t= 16,\n\tVERR_CR_CONN_CMD_LEN\t= 17,\n\tVERR_DISCONN_LEN\t= 18,\n\tVERR_DISCONN_RQST_LEN\t= 19,\n\tVERR_DISCONN_CMD\t= 20,\n\tVERR_DISCONN_CMD_LEN\t= 21,\n\tVERR_DISCONN_SCOPE\t= 22,\n\tVERR_RS_LEN\t\t= 23,\n\tVERR_RS_RQST_LEN\t= 24,\n\tVERR_RS_CMD\t\t= 25,\n\tVERR_RS_CMD_LEN\t\t= 26,\n\tVERR_RS_RCTL\t\t= 27,\n\tVERR_RS_RO\t\t= 28,\n};\n\nstatic char *validation_errors[] = {\n\t\"OK\",\n\t\"Bad CR_ASSOC Length\",\n\t\"Bad CR_ASSOC Rqst Length\",\n\t\"Not CR_ASSOC Cmd\",\n\t\"Bad CR_ASSOC Cmd Length\",\n\t\"Bad Ersp Ratio\",\n\t\"Association Allocation Failed\",\n\t\"Queue Allocation Failed\",\n\t\"Bad CR_CONN Length\",\n\t\"Bad CR_CONN Rqst Length\",\n\t\"Not Association ID\",\n\t\"Bad Association ID Length\",\n\t\"No Association\",\n\t\"Not Connection ID\",\n\t\"Bad Connection ID Length\",\n\t\"No Connection\",\n\t\"Not CR_CONN Cmd\",\n\t\"Bad CR_CONN Cmd Length\",\n\t\"Bad DISCONN Length\",\n\t\"Bad DISCONN Rqst Length\",\n\t\"Not DISCONN Cmd\",\n\t\"Bad DISCONN Cmd Length\",\n\t\"Bad Disconnect Scope\",\n\t\"Bad RS Length\",\n\t\"Bad RS Rqst Length\",\n\t\"Not RS Cmd\",\n\t\"Bad RS Cmd Length\",\n\t\"Bad RS R_CTL\",\n\t\"Bad RS Relative Offset\",\n};\n\nstatic void\nnvmet_fc_ls_create_association(struct nvmet_fc_tgtport *tgtport,\n\t\t\tstruct nvmet_fc_ls_iod *iod)\n{\n\tstruct fcnvme_ls_cr_assoc_rqst *rqst =\n\t\t\t\t(struct fcnvme_ls_cr_assoc_rqst *)iod->rqstbuf;\n\tstruct fcnvme_ls_cr_assoc_acc *acc =\n\t\t\t\t(struct fcnvme_ls_cr_assoc_acc *)iod->rspbuf;\n\tstruct nvmet_fc_tgt_queue *queue;\n\tint ret = 0;\n\n\tmemset(acc, 0, sizeof(*acc));\n\n\t/*\n\t * FC-NVME spec changes. There are initiators sending different\n\t * lengths as padding sizes for Create Association Cmd descriptor\n\t * was incorrect.\n\t * Accept anything of \"minimum\" length. Assume format per 1.15\n\t * spec (with HOSTID reduced to 16 bytes), ignore how long the\n\t * trailing pad length is.\n\t */\n\tif (iod->rqstdatalen < FCNVME_LSDESC_CRA_RQST_MINLEN)\n\t\tret = VERR_CR_ASSOC_LEN;\n\telse if (be32_to_cpu(rqst->desc_list_len) <\n\t\t\tFCNVME_LSDESC_CRA_RQST_MIN_LISTLEN)\n\t\tret = VERR_CR_ASSOC_RQST_LEN;\n\telse if (rqst->assoc_cmd.desc_tag !=\n\t\t\tcpu_to_be32(FCNVME_LSDESC_CREATE_ASSOC_CMD))\n\t\tret = VERR_CR_ASSOC_CMD;\n\telse if (be32_to_cpu(rqst->assoc_cmd.desc_len) <\n\t\t\tFCNVME_LSDESC_CRA_CMD_DESC_MIN_DESCLEN)\n\t\tret = VERR_CR_ASSOC_CMD_LEN;\n\telse if (!rqst->assoc_cmd.ersp_ratio ||\n\t\t (be16_to_cpu(rqst->assoc_cmd.ersp_ratio) >=\n\t\t\t\tbe16_to_cpu(rqst->assoc_cmd.sqsize)))\n\t\tret = VERR_ERSP_RATIO;\n\n\telse {\n\t\t/* new association w/ admin queue */\n\t\tiod->assoc = nvmet_fc_alloc_target_assoc(tgtport);\n\t\tif (!iod->assoc)\n\t\t\tret = VERR_ASSOC_ALLOC_FAIL;\n\t\telse {\n\t\t\tqueue = nvmet_fc_alloc_target_queue(iod->assoc, 0,\n\t\t\t\t\tbe16_to_cpu(rqst->assoc_cmd.sqsize));\n\t\t\tif (!queue)\n\t\t\t\tret = VERR_QUEUE_ALLOC_FAIL;\n\t\t}\n\t}\n\n\tif (ret) {\n\t\tdev_err(tgtport->dev,\n\t\t\t\"Create Association LS failed: %s\\n\",\n\t\t\tvalidation_errors[ret]);\n\t\tiod->lsreq->rsplen = nvmet_fc_format_rjt(acc,\n\t\t\t\tNVME_FC_MAX_LS_BUFFER_SIZE, rqst->w0.ls_cmd,\n\t\t\t\tFCNVME_RJT_RC_LOGIC,\n\t\t\t\tFCNVME_RJT_EXP_NONE, 0);\n\t\treturn;\n\t}\n\n\tqueue->ersp_ratio = be16_to_cpu(rqst->assoc_cmd.ersp_ratio);\n\tatomic_set(&queue->connected, 1);\n\tqueue->sqhd = 0;\t/* best place to init value */\n\n\t/* format a response */\n\n\tiod->lsreq->rsplen = sizeof(*acc);\n\n\tnvmet_fc_format_rsp_hdr(acc, FCNVME_LS_ACC,\n\t\t\tfcnvme_lsdesc_len(\n\t\t\t\tsizeof(struct fcnvme_ls_cr_assoc_acc)),\n\t\t\tFCNVME_LS_CREATE_ASSOCIATION);\n\tacc->associd.desc_tag = cpu_to_be32(FCNVME_LSDESC_ASSOC_ID);\n\tacc->associd.desc_len =\n\t\t\tfcnvme_lsdesc_len(\n\t\t\t\tsizeof(struct fcnvme_lsdesc_assoc_id));\n\tacc->associd.association_id =\n\t\t\tcpu_to_be64(nvmet_fc_makeconnid(iod->assoc, 0));\n\tacc->connectid.desc_tag = cpu_to_be32(FCNVME_LSDESC_CONN_ID);\n\tacc->connectid.desc_len =\n\t\t\tfcnvme_lsdesc_len(\n\t\t\t\tsizeof(struct fcnvme_lsdesc_conn_id));\n\tacc->connectid.connection_id = acc->associd.association_id;\n}\n\nstatic void\nnvmet_fc_ls_create_connection(struct nvmet_fc_tgtport *tgtport,\n\t\t\tstruct nvmet_fc_ls_iod *iod)\n{\n\tstruct fcnvme_ls_cr_conn_rqst *rqst =\n\t\t\t\t(struct fcnvme_ls_cr_conn_rqst *)iod->rqstbuf;\n\tstruct fcnvme_ls_cr_conn_acc *acc =\n\t\t\t\t(struct fcnvme_ls_cr_conn_acc *)iod->rspbuf;\n\tstruct nvmet_fc_tgt_queue *queue;\n\tint ret = 0;\n\n\tmemset(acc, 0, sizeof(*acc));\n\n\tif (iod->rqstdatalen < sizeof(struct fcnvme_ls_cr_conn_rqst))\n\t\tret = VERR_CR_CONN_LEN;\n\telse if (rqst->desc_list_len !=\n\t\t\tfcnvme_lsdesc_len(\n\t\t\t\tsizeof(struct fcnvme_ls_cr_conn_rqst)))\n\t\tret = VERR_CR_CONN_RQST_LEN;\n\telse if (rqst->associd.desc_tag != cpu_to_be32(FCNVME_LSDESC_ASSOC_ID))\n\t\tret = VERR_ASSOC_ID;\n\telse if (rqst->associd.desc_len !=\n\t\t\tfcnvme_lsdesc_len(\n\t\t\t\tsizeof(struct fcnvme_lsdesc_assoc_id)))\n\t\tret = VERR_ASSOC_ID_LEN;\n\telse if (rqst->connect_cmd.desc_tag !=\n\t\t\tcpu_to_be32(FCNVME_LSDESC_CREATE_CONN_CMD))\n\t\tret = VERR_CR_CONN_CMD;\n\telse if (rqst->connect_cmd.desc_len !=\n\t\t\tfcnvme_lsdesc_len(\n\t\t\t\tsizeof(struct fcnvme_lsdesc_cr_conn_cmd)))\n\t\tret = VERR_CR_CONN_CMD_LEN;\n\telse if (!rqst->connect_cmd.ersp_ratio ||\n\t\t (be16_to_cpu(rqst->connect_cmd.ersp_ratio) >=\n\t\t\t\tbe16_to_cpu(rqst->connect_cmd.sqsize)))\n\t\tret = VERR_ERSP_RATIO;\n\n\telse {\n\t\t/* new io queue */\n\t\tiod->assoc = nvmet_fc_find_target_assoc(tgtport,\n\t\t\t\tbe64_to_cpu(rqst->associd.association_id));\n\t\tif (!iod->assoc)\n\t\t\tret = VERR_NO_ASSOC;\n\t\telse {\n\t\t\tqueue = nvmet_fc_alloc_target_queue(iod->assoc,\n\t\t\t\t\tbe16_to_cpu(rqst->connect_cmd.qid),\n\t\t\t\t\tbe16_to_cpu(rqst->connect_cmd.sqsize));\n\t\t\tif (!queue)\n\t\t\t\tret = VERR_QUEUE_ALLOC_FAIL;\n\n\t\t\t/* release get taken in nvmet_fc_find_target_assoc */\n\t\t\tnvmet_fc_tgt_a_put(iod->assoc);\n\t\t}\n\t}\n\n\tif (ret) {\n\t\tdev_err(tgtport->dev,\n\t\t\t\"Create Connection LS failed: %s\\n\",\n\t\t\tvalidation_errors[ret]);\n\t\tiod->lsreq->rsplen = nvmet_fc_format_rjt(acc,\n\t\t\t\tNVME_FC_MAX_LS_BUFFER_SIZE, rqst->w0.ls_cmd,\n\t\t\t\t(ret == VERR_NO_ASSOC) ?\n\t\t\t\t\tFCNVME_RJT_RC_INV_ASSOC :\n\t\t\t\t\tFCNVME_RJT_RC_LOGIC,\n\t\t\t\tFCNVME_RJT_EXP_NONE, 0);\n\t\treturn;\n\t}\n\n\tqueue->ersp_ratio = be16_to_cpu(rqst->connect_cmd.ersp_ratio);\n\tatomic_set(&queue->connected, 1);\n\tqueue->sqhd = 0;\t/* best place to init value */\n\n\t/* format a response */\n\n\tiod->lsreq->rsplen = sizeof(*acc);\n\n\tnvmet_fc_format_rsp_hdr(acc, FCNVME_LS_ACC,\n\t\t\tfcnvme_lsdesc_len(sizeof(struct fcnvme_ls_cr_conn_acc)),\n\t\t\tFCNVME_LS_CREATE_CONNECTION);\n\tacc->connectid.desc_tag = cpu_to_be32(FCNVME_LSDESC_CONN_ID);\n\tacc->connectid.desc_len =\n\t\t\tfcnvme_lsdesc_len(\n\t\t\t\tsizeof(struct fcnvme_lsdesc_conn_id));\n\tacc->connectid.connection_id =\n\t\t\tcpu_to_be64(nvmet_fc_makeconnid(iod->assoc,\n\t\t\t\tbe16_to_cpu(rqst->connect_cmd.qid)));\n}\n\nstatic void\nnvmet_fc_ls_disconnect(struct nvmet_fc_tgtport *tgtport,\n\t\t\tstruct nvmet_fc_ls_iod *iod)\n{\n\tstruct fcnvme_ls_disconnect_rqst *rqst =\n\t\t\t(struct fcnvme_ls_disconnect_rqst *)iod->rqstbuf;\n\tstruct fcnvme_ls_disconnect_acc *acc =\n\t\t\t(struct fcnvme_ls_disconnect_acc *)iod->rspbuf;\n\tstruct nvmet_fc_tgt_queue *queue = NULL;\n\tstruct nvmet_fc_tgt_assoc *assoc;\n\tint ret = 0;\n\tbool del_assoc = false;\n\n\tmemset(acc, 0, sizeof(*acc));\n\n\tif (iod->rqstdatalen < sizeof(struct fcnvme_ls_disconnect_rqst))\n\t\tret = VERR_DISCONN_LEN;\n\telse if (rqst->desc_list_len !=\n\t\t\tfcnvme_lsdesc_len(\n\t\t\t\tsizeof(struct fcnvme_ls_disconnect_rqst)))\n\t\tret = VERR_DISCONN_RQST_LEN;\n\telse if (rqst->associd.desc_tag != cpu_to_be32(FCNVME_LSDESC_ASSOC_ID))\n\t\tret = VERR_ASSOC_ID;\n\telse if (rqst->associd.desc_len !=\n\t\t\tfcnvme_lsdesc_len(\n\t\t\t\tsizeof(struct fcnvme_lsdesc_assoc_id)))\n\t\tret = VERR_ASSOC_ID_LEN;\n\telse if (rqst->discon_cmd.desc_tag !=\n\t\t\tcpu_to_be32(FCNVME_LSDESC_DISCONN_CMD))\n\t\tret = VERR_DISCONN_CMD;\n\telse if (rqst->discon_cmd.desc_len !=\n\t\t\tfcnvme_lsdesc_len(\n\t\t\t\tsizeof(struct fcnvme_lsdesc_disconn_cmd)))\n\t\tret = VERR_DISCONN_CMD_LEN;\n\telse if ((rqst->discon_cmd.scope != FCNVME_DISCONN_ASSOCIATION) &&\n\t\t\t(rqst->discon_cmd.scope != FCNVME_DISCONN_CONNECTION))\n\t\tret = VERR_DISCONN_SCOPE;\n\telse {\n\t\t/* match an active association */\n\t\tassoc = nvmet_fc_find_target_assoc(tgtport,\n\t\t\t\tbe64_to_cpu(rqst->associd.association_id));\n\t\tiod->assoc = assoc;\n\t\tif (assoc) {\n\t\t\tif (rqst->discon_cmd.scope ==\n\t\t\t\t\tFCNVME_DISCONN_CONNECTION) {\n\t\t\t\tqueue = nvmet_fc_find_target_queue(tgtport,\n\t\t\t\t\t\tbe64_to_cpu(\n\t\t\t\t\t\t\trqst->discon_cmd.id));\n\t\t\t\tif (!queue) {\n\t\t\t\t\tnvmet_fc_tgt_a_put(assoc);\n\t\t\t\t\tret = VERR_NO_CONN;\n\t\t\t\t}\n\t\t\t}\n\t\t} else\n\t\t\tret = VERR_NO_ASSOC;\n\t}\n\n\tif (ret) {\n\t\tdev_err(tgtport->dev,\n\t\t\t\"Disconnect LS failed: %s\\n\",\n\t\t\tvalidation_errors[ret]);\n\t\tiod->lsreq->rsplen = nvmet_fc_format_rjt(acc,\n\t\t\t\tNVME_FC_MAX_LS_BUFFER_SIZE, rqst->w0.ls_cmd,\n\t\t\t\t(ret == VERR_NO_ASSOC) ?\n\t\t\t\t\tFCNVME_RJT_RC_INV_ASSOC :\n\t\t\t\t\t(ret == VERR_NO_CONN) ?\n\t\t\t\t\t\tFCNVME_RJT_RC_INV_CONN :\n\t\t\t\t\t\tFCNVME_RJT_RC_LOGIC,\n\t\t\t\tFCNVME_RJT_EXP_NONE, 0);\n\t\treturn;\n\t}\n\n\t/* format a response */\n\n\tiod->lsreq->rsplen = sizeof(*acc);\n\n\tnvmet_fc_format_rsp_hdr(acc, FCNVME_LS_ACC,\n\t\t\tfcnvme_lsdesc_len(\n\t\t\t\tsizeof(struct fcnvme_ls_disconnect_acc)),\n\t\t\tFCNVME_LS_DISCONNECT);\n\n\n\t/* are we to delete a Connection ID (queue) */\n\tif (queue) {\n\t\tint qid = queue->qid;\n\n\t\tnvmet_fc_delete_target_queue(queue);\n\n\t\t/* release the get taken by find_target_queue */\n\t\tnvmet_fc_tgt_q_put(queue);\n\n\t\t/* tear association down if io queue terminated */\n\t\tif (!qid)\n\t\t\tdel_assoc = true;\n\t}\n\n\t/* release get taken in nvmet_fc_find_target_assoc */\n\tnvmet_fc_tgt_a_put(iod->assoc);\n\n\tif (del_assoc)\n\t\tnvmet_fc_delete_target_assoc(iod->assoc);\n}\n\n\n/* *********************** NVME Ctrl Routines **************************** */\n\n\nstatic void nvmet_fc_fcp_nvme_cmd_done(struct nvmet_req *nvme_req);\n\nstatic struct nvmet_fabrics_ops nvmet_fc_tgt_fcp_ops;\n\nstatic void\nnvmet_fc_xmt_ls_rsp_done(struct nvmefc_tgt_ls_req *lsreq)\n{\n\tstruct nvmet_fc_ls_iod *iod = lsreq->nvmet_fc_private;\n\tstruct nvmet_fc_tgtport *tgtport = iod->tgtport;\n\n\tfc_dma_sync_single_for_cpu(tgtport->dev, iod->rspdma,\n\t\t\t\tNVME_FC_MAX_LS_BUFFER_SIZE, DMA_TO_DEVICE);\n\tnvmet_fc_free_ls_iod(tgtport, iod);\n\tnvmet_fc_tgtport_put(tgtport);\n}\n\nstatic void\nnvmet_fc_xmt_ls_rsp(struct nvmet_fc_tgtport *tgtport,\n\t\t\t\tstruct nvmet_fc_ls_iod *iod)\n{\n\tint ret;\n\n\tfc_dma_sync_single_for_device(tgtport->dev, iod->rspdma,\n\t\t\t\t  NVME_FC_MAX_LS_BUFFER_SIZE, DMA_TO_DEVICE);\n\n\tret = tgtport->ops->xmt_ls_rsp(&tgtport->fc_target_port, iod->lsreq);\n\tif (ret)\n\t\tnvmet_fc_xmt_ls_rsp_done(iod->lsreq);\n}\n\n/*\n * Actual processing routine for received FC-NVME LS Requests from the LLD\n */\nstatic void\nnvmet_fc_handle_ls_rqst(struct nvmet_fc_tgtport *tgtport,\n\t\t\tstruct nvmet_fc_ls_iod *iod)\n{\n\tstruct fcnvme_ls_rqst_w0 *w0 =\n\t\t\t(struct fcnvme_ls_rqst_w0 *)iod->rqstbuf;\n\n\tiod->lsreq->nvmet_fc_private = iod;\n\tiod->lsreq->rspbuf = iod->rspbuf;\n\tiod->lsreq->rspdma = iod->rspdma;\n\tiod->lsreq->done = nvmet_fc_xmt_ls_rsp_done;\n\t/* Be preventative. handlers will later set to valid length */\n\tiod->lsreq->rsplen = 0;\n\n\tiod->assoc = NULL;\n\n\t/*\n\t * handlers:\n\t *   parse request input, execute the request, and format the\n\t *   LS response\n\t */\n\tswitch (w0->ls_cmd) {\n\tcase FCNVME_LS_CREATE_ASSOCIATION:\n\t\t/* Creates Association and initial Admin Queue/Connection */\n\t\tnvmet_fc_ls_create_association(tgtport, iod);\n\t\tbreak;\n\tcase FCNVME_LS_CREATE_CONNECTION:\n\t\t/* Creates an IO Queue/Connection */\n\t\tnvmet_fc_ls_create_connection(tgtport, iod);\n\t\tbreak;\n\tcase FCNVME_LS_DISCONNECT:\n\t\t/* Terminate a Queue/Connection or the Association */\n\t\tnvmet_fc_ls_disconnect(tgtport, iod);\n\t\tbreak;\n\tdefault:\n\t\tiod->lsreq->rsplen = nvmet_fc_format_rjt(iod->rspbuf,\n\t\t\t\tNVME_FC_MAX_LS_BUFFER_SIZE, w0->ls_cmd,\n\t\t\t\tFCNVME_RJT_RC_INVAL, FCNVME_RJT_EXP_NONE, 0);\n\t}\n\n\tnvmet_fc_xmt_ls_rsp(tgtport, iod);\n}\n\n/*\n * Actual processing routine for received FC-NVME LS Requests from the LLD\n */\nstatic void\nnvmet_fc_handle_ls_rqst_work(struct work_struct *work)\n{\n\tstruct nvmet_fc_ls_iod *iod =\n\t\tcontainer_of(work, struct nvmet_fc_ls_iod, work);\n\tstruct nvmet_fc_tgtport *tgtport = iod->tgtport;\n\n\tnvmet_fc_handle_ls_rqst(tgtport, iod);\n}\n\n\n/**\n * nvmet_fc_rcv_ls_req - transport entry point called by an LLDD\n *                       upon the reception of a NVME LS request.\n *\n * The nvmet-fc layer will copy payload to an internal structure for\n * processing.  As such, upon completion of the routine, the LLDD may\n * immediately free/reuse the LS request buffer passed in the call.\n *\n * If this routine returns error, the LLDD should abort the exchange.\n *\n * @tgtport:    pointer to the (registered) target port the LS was\n *              received on.\n * @lsreq:      pointer to a lsreq request structure to be used to reference\n *              the exchange corresponding to the LS.\n * @lsreqbuf:   pointer to the buffer containing the LS Request\n * @lsreqbuf_len: length, in bytes, of the received LS request\n */\nint\nnvmet_fc_rcv_ls_req(struct nvmet_fc_target_port *target_port,\n\t\t\tstruct nvmefc_tgt_ls_req *lsreq,\n\t\t\tvoid *lsreqbuf, u32 lsreqbuf_len)\n{\n\tstruct nvmet_fc_tgtport *tgtport = targetport_to_tgtport(target_port);\n\tstruct nvmet_fc_ls_iod *iod;\n\n\tif (lsreqbuf_len > NVME_FC_MAX_LS_BUFFER_SIZE)\n\t\treturn -E2BIG;\n\n\tif (!nvmet_fc_tgtport_get(tgtport))\n\t\treturn -ESHUTDOWN;\n\n\tiod = nvmet_fc_alloc_ls_iod(tgtport);\n\tif (!iod) {\n\t\tnvmet_fc_tgtport_put(tgtport);\n\t\treturn -ENOENT;\n\t}\n\n\tiod->lsreq = lsreq;\n\tiod->fcpreq = NULL;\n\tmemcpy(iod->rqstbuf, lsreqbuf, lsreqbuf_len);\n\tiod->rqstdatalen = lsreqbuf_len;\n\n\tschedule_work(&iod->work);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(nvmet_fc_rcv_ls_req);\n\n\n/*\n * **********************\n * Start of FCP handling\n * **********************\n */\n\nstatic int\nnvmet_fc_alloc_tgt_pgs(struct nvmet_fc_fcp_iod *fod)\n{\n\tstruct scatterlist *sg;\n\tstruct page *page;\n\tunsigned int nent;\n\tu32 page_len, length;\n\tint i = 0;\n\n\tlength = fod->total_length;\n\tnent = DIV_ROUND_UP(length, PAGE_SIZE);\n\tsg = kmalloc_array(nent, sizeof(struct scatterlist), GFP_KERNEL);\n\tif (!sg)\n\t\tgoto out;\n\n\tsg_init_table(sg, nent);\n\n\twhile (length) {\n\t\tpage_len = min_t(u32, length, PAGE_SIZE);\n\n\t\tpage = alloc_page(GFP_KERNEL);\n\t\tif (!page)\n\t\t\tgoto out_free_pages;\n\n\t\tsg_set_page(&sg[i], page, page_len, 0);\n\t\tlength -= page_len;\n\t\ti++;\n\t}\n\n\tfod->data_sg = sg;\n\tfod->data_sg_cnt = nent;\n\tfod->data_sg_cnt = fc_dma_map_sg(fod->tgtport->dev, sg, nent,\n\t\t\t\t((fod->io_dir == NVMET_FCP_WRITE) ?\n\t\t\t\t\tDMA_FROM_DEVICE : DMA_TO_DEVICE));\n\t\t\t\t/* note: write from initiator perspective */\n\n\treturn 0;\n\nout_free_pages:\n\twhile (i > 0) {\n\t\ti--;\n\t\t__free_page(sg_page(&sg[i]));\n\t}\n\tkfree(sg);\n\tfod->data_sg = NULL;\n\tfod->data_sg_cnt = 0;\nout:\n\treturn NVME_SC_INTERNAL;\n}\n\nstatic void\nnvmet_fc_free_tgt_pgs(struct nvmet_fc_fcp_iod *fod)\n{\n\tstruct scatterlist *sg;\n\tint count;\n\n\tif (!fod->data_sg || !fod->data_sg_cnt)\n\t\treturn;\n\n\tfc_dma_unmap_sg(fod->tgtport->dev, fod->data_sg, fod->data_sg_cnt,\n\t\t\t\t((fod->io_dir == NVMET_FCP_WRITE) ?\n\t\t\t\t\tDMA_FROM_DEVICE : DMA_TO_DEVICE));\n\tfor_each_sg(fod->data_sg, sg, fod->data_sg_cnt, count)\n\t\t__free_page(sg_page(sg));\n\tkfree(fod->data_sg);\n\tfod->data_sg = NULL;\n\tfod->data_sg_cnt = 0;\n}\n\n\nstatic bool\nqueue_90percent_full(struct nvmet_fc_tgt_queue *q, u32 sqhd)\n{\n\tu32 sqtail, used;\n\n\t/* egad, this is ugly. And sqtail is just a best guess */\n\tsqtail = atomic_read(&q->sqtail) % q->sqsize;\n\n\tused = (sqtail < sqhd) ? (sqtail + q->sqsize - sqhd) : (sqtail - sqhd);\n\treturn ((used * 10) >= (((u32)(q->sqsize - 1) * 9)));\n}\n\n/*\n * Prep RSP payload.\n * May be a NVMET_FCOP_RSP or NVMET_FCOP_READDATA_RSP op\n */\nstatic void\nnvmet_fc_prep_fcp_rsp(struct nvmet_fc_tgtport *tgtport,\n\t\t\t\tstruct nvmet_fc_fcp_iod *fod)\n{\n\tstruct nvme_fc_ersp_iu *ersp = &fod->rspiubuf;\n\tstruct nvme_common_command *sqe = &fod->cmdiubuf.sqe.common;\n\tstruct nvme_completion *cqe = &ersp->cqe;\n\tu32 *cqewd = (u32 *)cqe;\n\tbool send_ersp = false;\n\tu32 rsn, rspcnt, xfr_length;\n\n\tif (fod->fcpreq->op == NVMET_FCOP_READDATA_RSP)\n\t\txfr_length = fod->total_length;\n\telse\n\t\txfr_length = fod->offset;\n\n\t/*\n\t * check to see if we can send a 0's rsp.\n\t *   Note: to send a 0's response, the NVME-FC host transport will\n\t *   recreate the CQE. The host transport knows: sq id, SQHD (last\n\t *   seen in an ersp), and command_id. Thus it will create a\n\t *   zero-filled CQE with those known fields filled in. Transport\n\t *   must send an ersp for any condition where the cqe won't match\n\t *   this.\n\t *\n\t * Here are the FC-NVME mandated cases where we must send an ersp:\n\t *  every N responses, where N=ersp_ratio\n\t *  force fabric commands to send ersp's (not in FC-NVME but good\n\t *    practice)\n\t *  normal cmds: any time status is non-zero, or status is zero\n\t *     but words 0 or 1 are non-zero.\n\t *  the SQ is 90% or more full\n\t *  the cmd is a fused command\n\t *  transferred data length not equal to cmd iu length\n\t */\n\trspcnt = atomic_inc_return(&fod->queue->zrspcnt);\n\tif (!(rspcnt % fod->queue->ersp_ratio) ||\n\t    sqe->opcode == nvme_fabrics_command ||\n\t    xfr_length != fod->total_length ||\n\t    (le16_to_cpu(cqe->status) & 0xFFFE) || cqewd[0] || cqewd[1] ||\n\t    (sqe->flags & (NVME_CMD_FUSE_FIRST | NVME_CMD_FUSE_SECOND)) ||\n\t    queue_90percent_full(fod->queue, le16_to_cpu(cqe->sq_head)))\n\t\tsend_ersp = true;\n\n\t/* re-set the fields */\n\tfod->fcpreq->rspaddr = ersp;\n\tfod->fcpreq->rspdma = fod->rspdma;\n\n\tif (!send_ersp) {\n\t\tmemset(ersp, 0, NVME_FC_SIZEOF_ZEROS_RSP);\n\t\tfod->fcpreq->rsplen = NVME_FC_SIZEOF_ZEROS_RSP;\n\t} else {\n\t\tersp->iu_len = cpu_to_be16(sizeof(*ersp)/sizeof(u32));\n\t\trsn = atomic_inc_return(&fod->queue->rsn);\n\t\tersp->rsn = cpu_to_be32(rsn);\n\t\tersp->xfrd_len = cpu_to_be32(xfr_length);\n\t\tfod->fcpreq->rsplen = sizeof(*ersp);\n\t}\n\n\tfc_dma_sync_single_for_device(tgtport->dev, fod->rspdma,\n\t\t\t\t  sizeof(fod->rspiubuf), DMA_TO_DEVICE);\n}\n\nstatic void nvmet_fc_xmt_fcp_op_done(struct nvmefc_tgt_fcp_req *fcpreq);\n\nstatic void\nnvmet_fc_abort_op(struct nvmet_fc_tgtport *tgtport,\n\t\t\t\tstruct nvmet_fc_fcp_iod *fod)\n{\n\tstruct nvmefc_tgt_fcp_req *fcpreq = fod->fcpreq;\n\n\t/* data no longer needed */\n\tnvmet_fc_free_tgt_pgs(fod);\n\n\t/*\n\t * if an ABTS was received or we issued the fcp_abort early\n\t * don't call abort routine again.\n\t */\n\t/* no need to take lock - lock was taken earlier to get here */\n\tif (!fod->aborted)\n\t\ttgtport->ops->fcp_abort(&tgtport->fc_target_port, fcpreq);\n\n\tnvmet_fc_free_fcp_iod(fod->queue, fod);\n}\n\nstatic void\nnvmet_fc_xmt_fcp_rsp(struct nvmet_fc_tgtport *tgtport,\n\t\t\t\tstruct nvmet_fc_fcp_iod *fod)\n{\n\tint ret;\n\n\tfod->fcpreq->op = NVMET_FCOP_RSP;\n\tfod->fcpreq->timeout = 0;\n\n\tnvmet_fc_prep_fcp_rsp(tgtport, fod);\n\n\tret = tgtport->ops->fcp_op(&tgtport->fc_target_port, fod->fcpreq);\n\tif (ret)\n\t\tnvmet_fc_abort_op(tgtport, fod);\n}\n\nstatic void\nnvmet_fc_transfer_fcp_data(struct nvmet_fc_tgtport *tgtport,\n\t\t\t\tstruct nvmet_fc_fcp_iod *fod, u8 op)\n{\n\tstruct nvmefc_tgt_fcp_req *fcpreq = fod->fcpreq;\n\tunsigned long flags;\n\tu32 tlen;\n\tint ret;\n\n\tfcpreq->op = op;\n\tfcpreq->offset = fod->offset;\n\tfcpreq->timeout = NVME_FC_TGTOP_TIMEOUT_SEC;\n\n\ttlen = min_t(u32, tgtport->max_sg_cnt * PAGE_SIZE,\n\t\t\t(fod->total_length - fod->offset));\n\tfcpreq->transfer_length = tlen;\n\tfcpreq->transferred_length = 0;\n\tfcpreq->fcp_error = 0;\n\tfcpreq->rsplen = 0;\n\n\tfcpreq->sg = &fod->data_sg[fod->offset / PAGE_SIZE];\n\tfcpreq->sg_cnt = DIV_ROUND_UP(tlen, PAGE_SIZE);\n\n\t/*\n\t * If the last READDATA request: check if LLDD supports\n\t * combined xfr with response.\n\t */\n\tif ((op == NVMET_FCOP_READDATA) &&\n\t    ((fod->offset + fcpreq->transfer_length) == fod->total_length) &&\n\t    (tgtport->ops->target_features & NVMET_FCTGTFEAT_READDATA_RSP)) {\n\t\tfcpreq->op = NVMET_FCOP_READDATA_RSP;\n\t\tnvmet_fc_prep_fcp_rsp(tgtport, fod);\n\t}\n\n\tret = tgtport->ops->fcp_op(&tgtport->fc_target_port, fod->fcpreq);\n\tif (ret) {\n\t\t/*\n\t\t * should be ok to set w/o lock as its in the thread of\n\t\t * execution (not an async timer routine) and doesn't\n\t\t * contend with any clearing action\n\t\t */\n\t\tfod->abort = true;\n\n\t\tif (op == NVMET_FCOP_WRITEDATA) {\n\t\t\tspin_lock_irqsave(&fod->flock, flags);\n\t\t\tfod->writedataactive = false;\n\t\t\tspin_unlock_irqrestore(&fod->flock, flags);\n\t\t\tnvmet_req_complete(&fod->req, NVME_SC_INTERNAL);\n\t\t} else /* NVMET_FCOP_READDATA or NVMET_FCOP_READDATA_RSP */ {\n\t\t\tfcpreq->fcp_error = ret;\n\t\t\tfcpreq->transferred_length = 0;\n\t\t\tnvmet_fc_xmt_fcp_op_done(fod->fcpreq);\n\t\t}\n\t}\n}\n\nstatic inline bool\n__nvmet_fc_fod_op_abort(struct nvmet_fc_fcp_iod *fod, bool abort)\n{\n\tstruct nvmefc_tgt_fcp_req *fcpreq = fod->fcpreq;\n\tstruct nvmet_fc_tgtport *tgtport = fod->tgtport;\n\n\t/* if in the middle of an io and we need to tear down */\n\tif (abort) {\n\t\tif (fcpreq->op == NVMET_FCOP_WRITEDATA) {\n\t\t\tnvmet_req_complete(&fod->req, NVME_SC_INTERNAL);\n\t\t\treturn true;\n\t\t}\n\n\t\tnvmet_fc_abort_op(tgtport, fod);\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n/*\n * actual done handler for FCP operations when completed by the lldd\n */\nstatic void\nnvmet_fc_fod_op_done(struct nvmet_fc_fcp_iod *fod)\n{\n\tstruct nvmefc_tgt_fcp_req *fcpreq = fod->fcpreq;\n\tstruct nvmet_fc_tgtport *tgtport = fod->tgtport;\n\tunsigned long flags;\n\tbool abort;\n\n\tspin_lock_irqsave(&fod->flock, flags);\n\tabort = fod->abort;\n\tfod->writedataactive = false;\n\tspin_unlock_irqrestore(&fod->flock, flags);\n\n\tswitch (fcpreq->op) {\n\n\tcase NVMET_FCOP_WRITEDATA:\n\t\tif (__nvmet_fc_fod_op_abort(fod, abort))\n\t\t\treturn;\n\t\tif (fcpreq->fcp_error ||\n\t\t    fcpreq->transferred_length != fcpreq->transfer_length) {\n\t\t\tspin_lock(&fod->flock);\n\t\t\tfod->abort = true;\n\t\t\tspin_unlock(&fod->flock);\n\n\t\t\tnvmet_req_complete(&fod->req, NVME_SC_INTERNAL);\n\t\t\treturn;\n\t\t}\n\n\t\tfod->offset += fcpreq->transferred_length;\n\t\tif (fod->offset != fod->total_length) {\n\t\t\tspin_lock_irqsave(&fod->flock, flags);\n\t\t\tfod->writedataactive = true;\n\t\t\tspin_unlock_irqrestore(&fod->flock, flags);\n\n\t\t\t/* transfer the next chunk */\n\t\t\tnvmet_fc_transfer_fcp_data(tgtport, fod,\n\t\t\t\t\t\tNVMET_FCOP_WRITEDATA);\n\t\t\treturn;\n\t\t}\n\n\t\t/* data transfer complete, resume with nvmet layer */\n\n\t\tfod->req.execute(&fod->req);\n\n\t\tbreak;\n\n\tcase NVMET_FCOP_READDATA:\n\tcase NVMET_FCOP_READDATA_RSP:\n\t\tif (__nvmet_fc_fod_op_abort(fod, abort))\n\t\t\treturn;\n\t\tif (fcpreq->fcp_error ||\n\t\t    fcpreq->transferred_length != fcpreq->transfer_length) {\n\t\t\tnvmet_fc_abort_op(tgtport, fod);\n\t\t\treturn;\n\t\t}\n\n\t\t/* success */\n\n\t\tif (fcpreq->op == NVMET_FCOP_READDATA_RSP) {\n\t\t\t/* data no longer needed */\n\t\t\tnvmet_fc_free_tgt_pgs(fod);\n\t\t\tnvmet_fc_free_fcp_iod(fod->queue, fod);\n\t\t\treturn;\n\t\t}\n\n\t\tfod->offset += fcpreq->transferred_length;\n\t\tif (fod->offset != fod->total_length) {\n\t\t\t/* transfer the next chunk */\n\t\t\tnvmet_fc_transfer_fcp_data(tgtport, fod,\n\t\t\t\t\t\tNVMET_FCOP_READDATA);\n\t\t\treturn;\n\t\t}\n\n\t\t/* data transfer complete, send response */\n\n\t\t/* data no longer needed */\n\t\tnvmet_fc_free_tgt_pgs(fod);\n\n\t\tnvmet_fc_xmt_fcp_rsp(tgtport, fod);\n\n\t\tbreak;\n\n\tcase NVMET_FCOP_RSP:\n\t\tif (__nvmet_fc_fod_op_abort(fod, abort))\n\t\t\treturn;\n\t\tnvmet_fc_free_fcp_iod(fod->queue, fod);\n\t\tbreak;\n\n\tdefault:\n\t\tbreak;\n\t}\n}\n\nstatic void\nnvmet_fc_fcp_rqst_op_done_work(struct work_struct *work)\n{\n\tstruct nvmet_fc_fcp_iod *fod =\n\t\tcontainer_of(work, struct nvmet_fc_fcp_iod, done_work);\n\n\tnvmet_fc_fod_op_done(fod);\n}\n\nstatic void\nnvmet_fc_xmt_fcp_op_done(struct nvmefc_tgt_fcp_req *fcpreq)\n{\n\tstruct nvmet_fc_fcp_iod *fod = fcpreq->nvmet_fc_private;\n\tstruct nvmet_fc_tgt_queue *queue = fod->queue;\n\n\tif (fod->tgtport->ops->target_features & NVMET_FCTGTFEAT_OPDONE_IN_ISR)\n\t\t/* context switch so completion is not in ISR context */\n\t\tqueue_work_on(queue->cpu, queue->work_q, &fod->done_work);\n\telse\n\t\tnvmet_fc_fod_op_done(fod);\n}\n\n/*\n * actual completion handler after execution by the nvmet layer\n */\nstatic void\n__nvmet_fc_fcp_nvme_cmd_done(struct nvmet_fc_tgtport *tgtport,\n\t\t\tstruct nvmet_fc_fcp_iod *fod, int status)\n{\n\tstruct nvme_common_command *sqe = &fod->cmdiubuf.sqe.common;\n\tstruct nvme_completion *cqe = &fod->rspiubuf.cqe;\n\tunsigned long flags;\n\tbool abort;\n\n\tspin_lock_irqsave(&fod->flock, flags);\n\tabort = fod->abort;\n\tspin_unlock_irqrestore(&fod->flock, flags);\n\n\t/* if we have a CQE, snoop the last sq_head value */\n\tif (!status)\n\t\tfod->queue->sqhd = cqe->sq_head;\n\n\tif (abort) {\n\t\tnvmet_fc_abort_op(tgtport, fod);\n\t\treturn;\n\t}\n\n\t/* if an error handling the cmd post initial parsing */\n\tif (status) {\n\t\t/* fudge up a failed CQE status for our transport error */\n\t\tmemset(cqe, 0, sizeof(*cqe));\n\t\tcqe->sq_head = fod->queue->sqhd;\t/* echo last cqe sqhd */\n\t\tcqe->sq_id = cpu_to_le16(fod->queue->qid);\n\t\tcqe->command_id = sqe->command_id;\n\t\tcqe->status = cpu_to_le16(status);\n\t} else {\n\n\t\t/*\n\t\t * try to push the data even if the SQE status is non-zero.\n\t\t * There may be a status where data still was intended to\n\t\t * be moved\n\t\t */\n\t\tif ((fod->io_dir == NVMET_FCP_READ) && (fod->data_sg_cnt)) {\n\t\t\t/* push the data over before sending rsp */\n\t\t\tnvmet_fc_transfer_fcp_data(tgtport, fod,\n\t\t\t\t\t\tNVMET_FCOP_READDATA);\n\t\t\treturn;\n\t\t}\n\n\t\t/* writes & no data - fall thru */\n\t}\n\n\t/* data no longer needed */\n\tnvmet_fc_free_tgt_pgs(fod);\n\n\tnvmet_fc_xmt_fcp_rsp(tgtport, fod);\n}\n\n\nstatic void\nnvmet_fc_fcp_nvme_cmd_done(struct nvmet_req *nvme_req)\n{\n\tstruct nvmet_fc_fcp_iod *fod = nvmet_req_to_fod(nvme_req);\n\tstruct nvmet_fc_tgtport *tgtport = fod->tgtport;\n\n\t__nvmet_fc_fcp_nvme_cmd_done(tgtport, fod, 0);\n}\n\n\n/*\n * Actual processing routine for received FC-NVME LS Requests from the LLD\n */\nstatic void\nnvmet_fc_handle_fcp_rqst(struct nvmet_fc_tgtport *tgtport,\n\t\t\tstruct nvmet_fc_fcp_iod *fod)\n{\n\tstruct nvme_fc_cmd_iu *cmdiu = &fod->cmdiubuf;\n\tint ret;\n\n\t/*\n\t * Fused commands are currently not supported in the linux\n\t * implementation.\n\t *\n\t * As such, the implementation of the FC transport does not\n\t * look at the fused commands and order delivery to the upper\n\t * layer until we have both based on csn.\n\t */\n\n\tfod->fcpreq->done = nvmet_fc_xmt_fcp_op_done;\n\n\tfod->total_length = be32_to_cpu(cmdiu->data_len);\n\tif (cmdiu->flags & FCNVME_CMD_FLAGS_WRITE) {\n\t\tfod->io_dir = NVMET_FCP_WRITE;\n\t\tif (!nvme_is_write(&cmdiu->sqe))\n\t\t\tgoto transport_error;\n\t} else if (cmdiu->flags & FCNVME_CMD_FLAGS_READ) {\n\t\tfod->io_dir = NVMET_FCP_READ;\n\t\tif (nvme_is_write(&cmdiu->sqe))\n\t\t\tgoto transport_error;\n\t} else {\n\t\tfod->io_dir = NVMET_FCP_NODATA;\n\t\tif (fod->total_length)\n\t\t\tgoto transport_error;\n\t}\n\n\tfod->req.cmd = &fod->cmdiubuf.sqe;\n\tfod->req.rsp = &fod->rspiubuf.cqe;\n\tfod->req.port = fod->queue->port;\n\n\t/* ensure nvmet handlers will set cmd handler callback */\n\tfod->req.execute = NULL;\n\n\t/* clear any response payload */\n\tmemset(&fod->rspiubuf, 0, sizeof(fod->rspiubuf));\n\n\tfod->data_sg = NULL;\n\tfod->data_sg_cnt = 0;\n\n\tret = nvmet_req_init(&fod->req,\n\t\t\t\t&fod->queue->nvme_cq,\n\t\t\t\t&fod->queue->nvme_sq,\n\t\t\t\t&nvmet_fc_tgt_fcp_ops);\n\tif (!ret) {\n\t\t/* bad SQE content or invalid ctrl state */\n\t\t/* nvmet layer has already called op done to send rsp. */\n\t\treturn;\n\t}\n\n\t/* keep a running counter of tail position */\n\tatomic_inc(&fod->queue->sqtail);\n\n\tif (fod->total_length) {\n\t\tret = nvmet_fc_alloc_tgt_pgs(fod);\n\t\tif (ret) {\n\t\t\tnvmet_req_complete(&fod->req, ret);\n\t\t\treturn;\n\t\t}\n\t}\n\tfod->req.sg = fod->data_sg;\n\tfod->req.sg_cnt = fod->data_sg_cnt;\n\tfod->offset = 0;\n\n\tif (fod->io_dir == NVMET_FCP_WRITE) {\n\t\t/* pull the data over before invoking nvmet layer */\n\t\tnvmet_fc_transfer_fcp_data(tgtport, fod, NVMET_FCOP_WRITEDATA);\n\t\treturn;\n\t}\n\n\t/*\n\t * Reads or no data:\n\t *\n\t * can invoke the nvmet_layer now. If read data, cmd completion will\n\t * push the data\n\t */\n\n\tfod->req.execute(&fod->req);\n\n\treturn;\n\ntransport_error:\n\tnvmet_fc_abort_op(tgtport, fod);\n}\n\n/*\n * Actual processing routine for received FC-NVME LS Requests from the LLD\n */\nstatic void\nnvmet_fc_handle_fcp_rqst_work(struct work_struct *work)\n{\n\tstruct nvmet_fc_fcp_iod *fod =\n\t\tcontainer_of(work, struct nvmet_fc_fcp_iod, work);\n\tstruct nvmet_fc_tgtport *tgtport = fod->tgtport;\n\n\tnvmet_fc_handle_fcp_rqst(tgtport, fod);\n}\n\n/**\n * nvmet_fc_rcv_fcp_req - transport entry point called by an LLDD\n *                       upon the reception of a NVME FCP CMD IU.\n *\n * Pass a FC-NVME FCP CMD IU received from the FC link to the nvmet-fc\n * layer for processing.\n *\n * The nvmet_fc layer allocates a local job structure (struct\n * nvmet_fc_fcp_iod) from the queue for the io and copies the\n * CMD IU buffer to the job structure. As such, on a successful\n * completion (returns 0), the LLDD may immediately free/reuse\n * the CMD IU buffer passed in the call.\n *\n * However, in some circumstances, due to the packetized nature of FC\n * and the api of the FC LLDD which may issue a hw command to send the\n * response, but the LLDD may not get the hw completion for that command\n * and upcall the nvmet_fc layer before a new command may be\n * asynchronously received - its possible for a command to be received\n * before the LLDD and nvmet_fc have recycled the job structure. It gives\n * the appearance of more commands received than fits in the sq.\n * To alleviate this scenario, a temporary queue is maintained in the\n * transport for pending LLDD requests waiting for a queue job structure.\n * In these \"overrun\" cases, a temporary queue element is allocated\n * the LLDD request and CMD iu buffer information remembered, and the\n * routine returns a -EOVERFLOW status. Subsequently, when a queue job\n * structure is freed, it is immediately reallocated for anything on the\n * pending request list. The LLDDs defer_rcv() callback is called,\n * informing the LLDD that it may reuse the CMD IU buffer, and the io\n * is then started normally with the transport.\n *\n * The LLDD, when receiving an -EOVERFLOW completion status, is to treat\n * the completion as successful but must not reuse the CMD IU buffer\n * until the LLDD's defer_rcv() callback has been called for the\n * corresponding struct nvmefc_tgt_fcp_req pointer.\n *\n * If there is any other condition in which an error occurs, the\n * transport will return a non-zero status indicating the error.\n * In all cases other than -EOVERFLOW, the transport has not accepted the\n * request and the LLDD should abort the exchange.\n *\n * @target_port: pointer to the (registered) target port the FCP CMD IU\n *              was received on.\n * @fcpreq:     pointer to a fcpreq request structure to be used to reference\n *              the exchange corresponding to the FCP Exchange.\n * @cmdiubuf:   pointer to the buffer containing the FCP CMD IU\n * @cmdiubuf_len: length, in bytes, of the received FCP CMD IU\n */\nint\nnvmet_fc_rcv_fcp_req(struct nvmet_fc_target_port *target_port,\n\t\t\tstruct nvmefc_tgt_fcp_req *fcpreq,\n\t\t\tvoid *cmdiubuf, u32 cmdiubuf_len)\n{\n\tstruct nvmet_fc_tgtport *tgtport = targetport_to_tgtport(target_port);\n\tstruct nvme_fc_cmd_iu *cmdiu = cmdiubuf;\n\tstruct nvmet_fc_tgt_queue *queue;\n\tstruct nvmet_fc_fcp_iod *fod;\n\tstruct nvmet_fc_defer_fcp_req *deferfcp;\n\tunsigned long flags;\n\n\t/* validate iu, so the connection id can be used to find the queue */\n\tif ((cmdiubuf_len != sizeof(*cmdiu)) ||\n\t\t\t(cmdiu->scsi_id != NVME_CMD_SCSI_ID) ||\n\t\t\t(cmdiu->fc_id != NVME_CMD_FC_ID) ||\n\t\t\t(be16_to_cpu(cmdiu->iu_len) != (sizeof(*cmdiu)/4)))\n\t\treturn -EIO;\n\n\tqueue = nvmet_fc_find_target_queue(tgtport,\n\t\t\t\tbe64_to_cpu(cmdiu->connection_id));\n\tif (!queue)\n\t\treturn -ENOTCONN;\n\n\t/*\n\t * note: reference taken by find_target_queue\n\t * After successful fod allocation, the fod will inherit the\n\t * ownership of that reference and will remove the reference\n\t * when the fod is freed.\n\t */\n\n\tspin_lock_irqsave(&queue->qlock, flags);\n\n\tfod = nvmet_fc_alloc_fcp_iod(queue);\n\tif (fod) {\n\t\tspin_unlock_irqrestore(&queue->qlock, flags);\n\n\t\tfcpreq->nvmet_fc_private = fod;\n\t\tfod->fcpreq = fcpreq;\n\n\t\tmemcpy(&fod->cmdiubuf, cmdiubuf, cmdiubuf_len);\n\n\t\tnvmet_fc_queue_fcp_req(tgtport, queue, fcpreq);\n\n\t\treturn 0;\n\t}\n\n\tif (!tgtport->ops->defer_rcv) {\n\t\tspin_unlock_irqrestore(&queue->qlock, flags);\n\t\t/* release the queue lookup reference */\n\t\tnvmet_fc_tgt_q_put(queue);\n\t\treturn -ENOENT;\n\t}\n\n\tdeferfcp = list_first_entry_or_null(&queue->avail_defer_list,\n\t\t\tstruct nvmet_fc_defer_fcp_req, req_list);\n\tif (deferfcp) {\n\t\t/* Just re-use one that was previously allocated */\n\t\tlist_del(&deferfcp->req_list);\n\t} else {\n\t\tspin_unlock_irqrestore(&queue->qlock, flags);\n\n\t\t/* Now we need to dynamically allocate one */\n\t\tdeferfcp = kmalloc(sizeof(*deferfcp), GFP_KERNEL);\n\t\tif (!deferfcp) {\n\t\t\t/* release the queue lookup reference */\n\t\t\tnvmet_fc_tgt_q_put(queue);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tspin_lock_irqsave(&queue->qlock, flags);\n\t}\n\n\t/* For now, use rspaddr / rsplen to save payload information */\n\tfcpreq->rspaddr = cmdiubuf;\n\tfcpreq->rsplen  = cmdiubuf_len;\n\tdeferfcp->fcp_req = fcpreq;\n\n\t/* defer processing till a fod becomes available */\n\tlist_add_tail(&deferfcp->req_list, &queue->pending_cmd_list);\n\n\t/* NOTE: the queue lookup reference is still valid */\n\n\tspin_unlock_irqrestore(&queue->qlock, flags);\n\n\treturn -EOVERFLOW;\n}\nEXPORT_SYMBOL_GPL(nvmet_fc_rcv_fcp_req);\n\n/**\n * nvmet_fc_rcv_fcp_abort - transport entry point called by an LLDD\n *                       upon the reception of an ABTS for a FCP command\n *\n * Notify the transport that an ABTS has been received for a FCP command\n * that had been given to the transport via nvmet_fc_rcv_fcp_req(). The\n * LLDD believes the command is still being worked on\n * (template_ops->fcp_req_release() has not been called).\n *\n * The transport will wait for any outstanding work (an op to the LLDD,\n * which the lldd should complete with error due to the ABTS; or the\n * completion from the nvmet layer of the nvme command), then will\n * stop processing and call the nvmet_fc_rcv_fcp_req() callback to\n * return the i/o context to the LLDD.  The LLDD may send the BA_ACC\n * to the ABTS either after return from this function (assuming any\n * outstanding op work has been terminated) or upon the callback being\n * called.\n *\n * @target_port: pointer to the (registered) target port the FCP CMD IU\n *              was received on.\n * @fcpreq:     pointer to the fcpreq request structure that corresponds\n *              to the exchange that received the ABTS.\n */\nvoid\nnvmet_fc_rcv_fcp_abort(struct nvmet_fc_target_port *target_port,\n\t\t\tstruct nvmefc_tgt_fcp_req *fcpreq)\n{\n\tstruct nvmet_fc_fcp_iod *fod = fcpreq->nvmet_fc_private;\n\tstruct nvmet_fc_tgt_queue *queue;\n\tunsigned long flags;\n\n\tif (!fod || fod->fcpreq != fcpreq)\n\t\t/* job appears to have already completed, ignore abort */\n\t\treturn;\n\n\tqueue = fod->queue;\n\n\tspin_lock_irqsave(&queue->qlock, flags);\n\tif (fod->active) {\n\t\t/*\n\t\t * mark as abort. The abort handler, invoked upon completion\n\t\t * of any work, will detect the aborted status and do the\n\t\t * callback.\n\t\t */\n\t\tspin_lock(&fod->flock);\n\t\tfod->abort = true;\n\t\tfod->aborted = true;\n\t\tspin_unlock(&fod->flock);\n\t}\n\tspin_unlock_irqrestore(&queue->qlock, flags);\n}\nEXPORT_SYMBOL_GPL(nvmet_fc_rcv_fcp_abort);\n\n\nstruct nvmet_fc_traddr {\n\tu64\tnn;\n\tu64\tpn;\n};\n\nstatic int\n__nvme_fc_parse_u64(substring_t *sstr, u64 *val)\n{\n\tu64 token64;\n\n\tif (match_u64(sstr, &token64))\n\t\treturn -EINVAL;\n\t*val = token64;\n\n\treturn 0;\n}\n\n/*\n * This routine validates and extracts the WWN's from the TRADDR string.\n * As kernel parsers need the 0x to determine number base, universally\n * build string to parse with 0x prefix before parsing name strings.\n */\nstatic int\nnvme_fc_parse_traddr(struct nvmet_fc_traddr *traddr, char *buf, size_t blen)\n{\n\tchar name[2 + NVME_FC_TRADDR_HEXNAMELEN + 1];\n\tsubstring_t wwn = { name, &name[sizeof(name)-1] };\n\tint nnoffset, pnoffset;\n\n\t/* validate it string one of the 2 allowed formats */\n\tif (strnlen(buf, blen) == NVME_FC_TRADDR_MAXLENGTH &&\n\t\t\t!strncmp(buf, \"nn-0x\", NVME_FC_TRADDR_OXNNLEN) &&\n\t\t\t!strncmp(&buf[NVME_FC_TRADDR_MAX_PN_OFFSET],\n\t\t\t\t\"pn-0x\", NVME_FC_TRADDR_OXNNLEN)) {\n\t\tnnoffset = NVME_FC_TRADDR_OXNNLEN;\n\t\tpnoffset = NVME_FC_TRADDR_MAX_PN_OFFSET +\n\t\t\t\t\t\tNVME_FC_TRADDR_OXNNLEN;\n\t} else if ((strnlen(buf, blen) == NVME_FC_TRADDR_MINLENGTH &&\n\t\t\t!strncmp(buf, \"nn-\", NVME_FC_TRADDR_NNLEN) &&\n\t\t\t!strncmp(&buf[NVME_FC_TRADDR_MIN_PN_OFFSET],\n\t\t\t\t\"pn-\", NVME_FC_TRADDR_NNLEN))) {\n\t\tnnoffset = NVME_FC_TRADDR_NNLEN;\n\t\tpnoffset = NVME_FC_TRADDR_MIN_PN_OFFSET + NVME_FC_TRADDR_NNLEN;\n\t} else\n\t\tgoto out_einval;\n\n\tname[0] = '0';\n\tname[1] = 'x';\n\tname[2 + NVME_FC_TRADDR_HEXNAMELEN] = 0;\n\n\tmemcpy(&name[2], &buf[nnoffset], NVME_FC_TRADDR_HEXNAMELEN);\n\tif (__nvme_fc_parse_u64(&wwn, &traddr->nn))\n\t\tgoto out_einval;\n\n\tmemcpy(&name[2], &buf[pnoffset], NVME_FC_TRADDR_HEXNAMELEN);\n\tif (__nvme_fc_parse_u64(&wwn, &traddr->pn))\n\t\tgoto out_einval;\n\n\treturn 0;\n\nout_einval:\n\tpr_warn(\"%s: bad traddr string\\n\", __func__);\n\treturn -EINVAL;\n}\n\nstatic int\nnvmet_fc_add_port(struct nvmet_port *port)\n{\n\tstruct nvmet_fc_tgtport *tgtport;\n\tstruct nvmet_fc_traddr traddr = { 0L, 0L };\n\tunsigned long flags;\n\tint ret;\n\n\t/* validate the address info */\n\tif ((port->disc_addr.trtype != NVMF_TRTYPE_FC) ||\n\t    (port->disc_addr.adrfam != NVMF_ADDR_FAMILY_FC))\n\t\treturn -EINVAL;\n\n\t/* map the traddr address info to a target port */\n\n\tret = nvme_fc_parse_traddr(&traddr, port->disc_addr.traddr,\n\t\t\tsizeof(port->disc_addr.traddr));\n\tif (ret)\n\t\treturn ret;\n\n\tret = -ENXIO;\n\tspin_lock_irqsave(&nvmet_fc_tgtlock, flags);\n\tlist_for_each_entry(tgtport, &nvmet_fc_target_list, tgt_list) {\n\t\tif ((tgtport->fc_target_port.node_name == traddr.nn) &&\n\t\t    (tgtport->fc_target_port.port_name == traddr.pn)) {\n\t\t\t/* a FC port can only be 1 nvmet port id */\n\t\t\tif (!tgtport->port) {\n\t\t\t\ttgtport->port = port;\n\t\t\t\tport->priv = tgtport;\n\t\t\t\tnvmet_fc_tgtport_get(tgtport);\n\t\t\t\tret = 0;\n\t\t\t} else\n\t\t\t\tret = -EALREADY;\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock_irqrestore(&nvmet_fc_tgtlock, flags);\n\treturn ret;\n}\n\nstatic void\nnvmet_fc_remove_port(struct nvmet_port *port)\n{\n\tstruct nvmet_fc_tgtport *tgtport = port->priv;\n\tunsigned long flags;\n\tbool matched = false;\n\n\tspin_lock_irqsave(&nvmet_fc_tgtlock, flags);\n\tif (tgtport->port == port) {\n\t\tmatched = true;\n\t\ttgtport->port = NULL;\n\t}\n\tspin_unlock_irqrestore(&nvmet_fc_tgtlock, flags);\n\n\tif (matched)\n\t\tnvmet_fc_tgtport_put(tgtport);\n}\n\nstatic struct nvmet_fabrics_ops nvmet_fc_tgt_fcp_ops = {\n\t.owner\t\t\t= THIS_MODULE,\n\t.type\t\t\t= NVMF_TRTYPE_FC,\n\t.msdbd\t\t\t= 1,\n\t.add_port\t\t= nvmet_fc_add_port,\n\t.remove_port\t\t= nvmet_fc_remove_port,\n\t.queue_response\t\t= nvmet_fc_fcp_nvme_cmd_done,\n\t.delete_ctrl\t\t= nvmet_fc_delete_ctrl,\n};\n\nstatic int __init nvmet_fc_init_module(void)\n{\n\treturn nvmet_register_transport(&nvmet_fc_tgt_fcp_ops);\n}\n\nstatic void __exit nvmet_fc_exit_module(void)\n{\n\t/* sanity check - all lports should be removed */\n\tif (!list_empty(&nvmet_fc_target_list))\n\t\tpr_warn(\"%s: targetport list not empty\\n\", __func__);\n\n\tnvmet_unregister_transport(&nvmet_fc_tgt_fcp_ops);\n\n\tida_destroy(&nvmet_fc_tgtport_cnt);\n}\n\nmodule_init(nvmet_fc_init_module);\nmodule_exit(nvmet_fc_exit_module);\n\nMODULE_LICENSE(\"GPL v2\");\n"], "fixing_code": ["/*\n * Copyright (c) 2016 Avago Technologies.  All rights reserved.\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of version 2 of the GNU General Public License as\n * published by the Free Software Foundation.\n *\n * This program is distributed in the hope that it will be useful.\n * ALL EXPRESS OR IMPLIED CONDITIONS, REPRESENTATIONS AND WARRANTIES,\n * INCLUDING ANY IMPLIED WARRANTY OF MERCHANTABILITY, FITNESS FOR A\n * PARTICULAR PURPOSE, OR NON-INFRINGEMENT, ARE DISCLAIMED, EXCEPT TO\n * THE EXTENT THAT SUCH DISCLAIMERS ARE HELD TO BE LEGALLY INVALID.\n * See the GNU General Public License for more details, a copy of which\n * can be found in the file COPYING included with this package\n *\n */\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n#include <linux/module.h>\n#include <linux/slab.h>\n#include <linux/blk-mq.h>\n#include <linux/parser.h>\n#include <linux/random.h>\n#include <uapi/scsi/fc/fc_fs.h>\n#include <uapi/scsi/fc/fc_els.h>\n\n#include \"nvmet.h\"\n#include <linux/nvme-fc-driver.h>\n#include <linux/nvme-fc.h>\n\n\n/* *************************** Data Structures/Defines ****************** */\n\n\n#define NVMET_LS_CTX_COUNT\t\t4\n\n/* for this implementation, assume small single frame rqst/rsp */\n#define NVME_FC_MAX_LS_BUFFER_SIZE\t\t2048\n\nstruct nvmet_fc_tgtport;\nstruct nvmet_fc_tgt_assoc;\n\nstruct nvmet_fc_ls_iod {\n\tstruct nvmefc_tgt_ls_req\t*lsreq;\n\tstruct nvmefc_tgt_fcp_req\t*fcpreq;\t/* only if RS */\n\n\tstruct list_head\t\tls_list;\t/* tgtport->ls_list */\n\n\tstruct nvmet_fc_tgtport\t\t*tgtport;\n\tstruct nvmet_fc_tgt_assoc\t*assoc;\n\n\tu8\t\t\t\t*rqstbuf;\n\tu8\t\t\t\t*rspbuf;\n\tu16\t\t\t\trqstdatalen;\n\tdma_addr_t\t\t\trspdma;\n\n\tstruct scatterlist\t\tsg[2];\n\n\tstruct work_struct\t\twork;\n} __aligned(sizeof(unsigned long long));\n\n#define NVMET_FC_MAX_SEQ_LENGTH\t\t(256 * 1024)\n#define NVMET_FC_MAX_XFR_SGENTS\t\t(NVMET_FC_MAX_SEQ_LENGTH / PAGE_SIZE)\n\nenum nvmet_fcp_datadir {\n\tNVMET_FCP_NODATA,\n\tNVMET_FCP_WRITE,\n\tNVMET_FCP_READ,\n\tNVMET_FCP_ABORTED,\n};\n\nstruct nvmet_fc_fcp_iod {\n\tstruct nvmefc_tgt_fcp_req\t*fcpreq;\n\n\tstruct nvme_fc_cmd_iu\t\tcmdiubuf;\n\tstruct nvme_fc_ersp_iu\t\trspiubuf;\n\tdma_addr_t\t\t\trspdma;\n\tstruct scatterlist\t\t*data_sg;\n\tint\t\t\t\tdata_sg_cnt;\n\tu32\t\t\t\ttotal_length;\n\tu32\t\t\t\toffset;\n\tenum nvmet_fcp_datadir\t\tio_dir;\n\tbool\t\t\t\tactive;\n\tbool\t\t\t\tabort;\n\tbool\t\t\t\taborted;\n\tbool\t\t\t\twritedataactive;\n\tspinlock_t\t\t\tflock;\n\n\tstruct nvmet_req\t\treq;\n\tstruct work_struct\t\twork;\n\tstruct work_struct\t\tdone_work;\n\n\tstruct nvmet_fc_tgtport\t\t*tgtport;\n\tstruct nvmet_fc_tgt_queue\t*queue;\n\n\tstruct list_head\t\tfcp_list;\t/* tgtport->fcp_list */\n};\n\nstruct nvmet_fc_tgtport {\n\n\tstruct nvmet_fc_target_port\tfc_target_port;\n\n\tstruct list_head\t\ttgt_list; /* nvmet_fc_target_list */\n\tstruct device\t\t\t*dev;\t/* dev for dma mapping */\n\tstruct nvmet_fc_target_template\t*ops;\n\n\tstruct nvmet_fc_ls_iod\t\t*iod;\n\tspinlock_t\t\t\tlock;\n\tstruct list_head\t\tls_list;\n\tstruct list_head\t\tls_busylist;\n\tstruct list_head\t\tassoc_list;\n\tstruct ida\t\t\tassoc_cnt;\n\tstruct nvmet_port\t\t*port;\n\tstruct kref\t\t\tref;\n\tu32\t\t\t\tmax_sg_cnt;\n};\n\nstruct nvmet_fc_defer_fcp_req {\n\tstruct list_head\t\treq_list;\n\tstruct nvmefc_tgt_fcp_req\t*fcp_req;\n};\n\nstruct nvmet_fc_tgt_queue {\n\tbool\t\t\t\tninetypercent;\n\tu16\t\t\t\tqid;\n\tu16\t\t\t\tsqsize;\n\tu16\t\t\t\tersp_ratio;\n\t__le16\t\t\t\tsqhd;\n\tint\t\t\t\tcpu;\n\tatomic_t\t\t\tconnected;\n\tatomic_t\t\t\tsqtail;\n\tatomic_t\t\t\tzrspcnt;\n\tatomic_t\t\t\trsn;\n\tspinlock_t\t\t\tqlock;\n\tstruct nvmet_port\t\t*port;\n\tstruct nvmet_cq\t\t\tnvme_cq;\n\tstruct nvmet_sq\t\t\tnvme_sq;\n\tstruct nvmet_fc_tgt_assoc\t*assoc;\n\tstruct nvmet_fc_fcp_iod\t\t*fod;\t\t/* array of fcp_iods */\n\tstruct list_head\t\tfod_list;\n\tstruct list_head\t\tpending_cmd_list;\n\tstruct list_head\t\tavail_defer_list;\n\tstruct workqueue_struct\t\t*work_q;\n\tstruct kref\t\t\tref;\n} __aligned(sizeof(unsigned long long));\n\nstruct nvmet_fc_tgt_assoc {\n\tu64\t\t\t\tassociation_id;\n\tu32\t\t\t\ta_id;\n\tstruct nvmet_fc_tgtport\t\t*tgtport;\n\tstruct list_head\t\ta_list;\n\tstruct nvmet_fc_tgt_queue\t*queues[NVMET_NR_QUEUES + 1];\n\tstruct kref\t\t\tref;\n};\n\n\nstatic inline int\nnvmet_fc_iodnum(struct nvmet_fc_ls_iod *iodptr)\n{\n\treturn (iodptr - iodptr->tgtport->iod);\n}\n\nstatic inline int\nnvmet_fc_fodnum(struct nvmet_fc_fcp_iod *fodptr)\n{\n\treturn (fodptr - fodptr->queue->fod);\n}\n\n\n/*\n * Association and Connection IDs:\n *\n * Association ID will have random number in upper 6 bytes and zero\n *   in lower 2 bytes\n *\n * Connection IDs will be Association ID with QID or'd in lower 2 bytes\n *\n * note: Association ID = Connection ID for queue 0\n */\n#define BYTES_FOR_QID\t\t\tsizeof(u16)\n#define BYTES_FOR_QID_SHIFT\t\t(BYTES_FOR_QID * 8)\n#define NVMET_FC_QUEUEID_MASK\t\t((u64)((1 << BYTES_FOR_QID_SHIFT) - 1))\n\nstatic inline u64\nnvmet_fc_makeconnid(struct nvmet_fc_tgt_assoc *assoc, u16 qid)\n{\n\treturn (assoc->association_id | qid);\n}\n\nstatic inline u64\nnvmet_fc_getassociationid(u64 connectionid)\n{\n\treturn connectionid & ~NVMET_FC_QUEUEID_MASK;\n}\n\nstatic inline u16\nnvmet_fc_getqueueid(u64 connectionid)\n{\n\treturn (u16)(connectionid & NVMET_FC_QUEUEID_MASK);\n}\n\nstatic inline struct nvmet_fc_tgtport *\ntargetport_to_tgtport(struct nvmet_fc_target_port *targetport)\n{\n\treturn container_of(targetport, struct nvmet_fc_tgtport,\n\t\t\t\t fc_target_port);\n}\n\nstatic inline struct nvmet_fc_fcp_iod *\nnvmet_req_to_fod(struct nvmet_req *nvme_req)\n{\n\treturn container_of(nvme_req, struct nvmet_fc_fcp_iod, req);\n}\n\n\n/* *************************** Globals **************************** */\n\n\nstatic DEFINE_SPINLOCK(nvmet_fc_tgtlock);\n\nstatic LIST_HEAD(nvmet_fc_target_list);\nstatic DEFINE_IDA(nvmet_fc_tgtport_cnt);\n\n\nstatic void nvmet_fc_handle_ls_rqst_work(struct work_struct *work);\nstatic void nvmet_fc_handle_fcp_rqst_work(struct work_struct *work);\nstatic void nvmet_fc_fcp_rqst_op_done_work(struct work_struct *work);\nstatic void nvmet_fc_tgt_a_put(struct nvmet_fc_tgt_assoc *assoc);\nstatic int nvmet_fc_tgt_a_get(struct nvmet_fc_tgt_assoc *assoc);\nstatic void nvmet_fc_tgt_q_put(struct nvmet_fc_tgt_queue *queue);\nstatic int nvmet_fc_tgt_q_get(struct nvmet_fc_tgt_queue *queue);\nstatic void nvmet_fc_tgtport_put(struct nvmet_fc_tgtport *tgtport);\nstatic int nvmet_fc_tgtport_get(struct nvmet_fc_tgtport *tgtport);\nstatic void nvmet_fc_handle_fcp_rqst(struct nvmet_fc_tgtport *tgtport,\n\t\t\t\t\tstruct nvmet_fc_fcp_iod *fod);\n\n\n/* *********************** FC-NVME DMA Handling **************************** */\n\n/*\n * The fcloop device passes in a NULL device pointer. Real LLD's will\n * pass in a valid device pointer. If NULL is passed to the dma mapping\n * routines, depending on the platform, it may or may not succeed, and\n * may crash.\n *\n * As such:\n * Wrapper all the dma routines and check the dev pointer.\n *\n * If simple mappings (return just a dma address, we'll noop them,\n * returning a dma address of 0.\n *\n * On more complex mappings (dma_map_sg), a pseudo routine fills\n * in the scatter list, setting all dma addresses to 0.\n */\n\nstatic inline dma_addr_t\nfc_dma_map_single(struct device *dev, void *ptr, size_t size,\n\t\tenum dma_data_direction dir)\n{\n\treturn dev ? dma_map_single(dev, ptr, size, dir) : (dma_addr_t)0L;\n}\n\nstatic inline int\nfc_dma_mapping_error(struct device *dev, dma_addr_t dma_addr)\n{\n\treturn dev ? dma_mapping_error(dev, dma_addr) : 0;\n}\n\nstatic inline void\nfc_dma_unmap_single(struct device *dev, dma_addr_t addr, size_t size,\n\tenum dma_data_direction dir)\n{\n\tif (dev)\n\t\tdma_unmap_single(dev, addr, size, dir);\n}\n\nstatic inline void\nfc_dma_sync_single_for_cpu(struct device *dev, dma_addr_t addr, size_t size,\n\t\tenum dma_data_direction dir)\n{\n\tif (dev)\n\t\tdma_sync_single_for_cpu(dev, addr, size, dir);\n}\n\nstatic inline void\nfc_dma_sync_single_for_device(struct device *dev, dma_addr_t addr, size_t size,\n\t\tenum dma_data_direction dir)\n{\n\tif (dev)\n\t\tdma_sync_single_for_device(dev, addr, size, dir);\n}\n\n/* pseudo dma_map_sg call */\nstatic int\nfc_map_sg(struct scatterlist *sg, int nents)\n{\n\tstruct scatterlist *s;\n\tint i;\n\n\tWARN_ON(nents == 0 || sg[0].length == 0);\n\n\tfor_each_sg(sg, s, nents, i) {\n\t\ts->dma_address = 0L;\n#ifdef CONFIG_NEED_SG_DMA_LENGTH\n\t\ts->dma_length = s->length;\n#endif\n\t}\n\treturn nents;\n}\n\nstatic inline int\nfc_dma_map_sg(struct device *dev, struct scatterlist *sg, int nents,\n\t\tenum dma_data_direction dir)\n{\n\treturn dev ? dma_map_sg(dev, sg, nents, dir) : fc_map_sg(sg, nents);\n}\n\nstatic inline void\nfc_dma_unmap_sg(struct device *dev, struct scatterlist *sg, int nents,\n\t\tenum dma_data_direction dir)\n{\n\tif (dev)\n\t\tdma_unmap_sg(dev, sg, nents, dir);\n}\n\n\n/* *********************** FC-NVME Port Management ************************ */\n\n\nstatic int\nnvmet_fc_alloc_ls_iodlist(struct nvmet_fc_tgtport *tgtport)\n{\n\tstruct nvmet_fc_ls_iod *iod;\n\tint i;\n\n\tiod = kcalloc(NVMET_LS_CTX_COUNT, sizeof(struct nvmet_fc_ls_iod),\n\t\t\tGFP_KERNEL);\n\tif (!iod)\n\t\treturn -ENOMEM;\n\n\ttgtport->iod = iod;\n\n\tfor (i = 0; i < NVMET_LS_CTX_COUNT; iod++, i++) {\n\t\tINIT_WORK(&iod->work, nvmet_fc_handle_ls_rqst_work);\n\t\tiod->tgtport = tgtport;\n\t\tlist_add_tail(&iod->ls_list, &tgtport->ls_list);\n\n\t\tiod->rqstbuf = kcalloc(2, NVME_FC_MAX_LS_BUFFER_SIZE,\n\t\t\tGFP_KERNEL);\n\t\tif (!iod->rqstbuf)\n\t\t\tgoto out_fail;\n\n\t\tiod->rspbuf = iod->rqstbuf + NVME_FC_MAX_LS_BUFFER_SIZE;\n\n\t\tiod->rspdma = fc_dma_map_single(tgtport->dev, iod->rspbuf,\n\t\t\t\t\t\tNVME_FC_MAX_LS_BUFFER_SIZE,\n\t\t\t\t\t\tDMA_TO_DEVICE);\n\t\tif (fc_dma_mapping_error(tgtport->dev, iod->rspdma))\n\t\t\tgoto out_fail;\n\t}\n\n\treturn 0;\n\nout_fail:\n\tkfree(iod->rqstbuf);\n\tlist_del(&iod->ls_list);\n\tfor (iod--, i--; i >= 0; iod--, i--) {\n\t\tfc_dma_unmap_single(tgtport->dev, iod->rspdma,\n\t\t\t\tNVME_FC_MAX_LS_BUFFER_SIZE, DMA_TO_DEVICE);\n\t\tkfree(iod->rqstbuf);\n\t\tlist_del(&iod->ls_list);\n\t}\n\n\tkfree(iod);\n\n\treturn -EFAULT;\n}\n\nstatic void\nnvmet_fc_free_ls_iodlist(struct nvmet_fc_tgtport *tgtport)\n{\n\tstruct nvmet_fc_ls_iod *iod = tgtport->iod;\n\tint i;\n\n\tfor (i = 0; i < NVMET_LS_CTX_COUNT; iod++, i++) {\n\t\tfc_dma_unmap_single(tgtport->dev,\n\t\t\t\tiod->rspdma, NVME_FC_MAX_LS_BUFFER_SIZE,\n\t\t\t\tDMA_TO_DEVICE);\n\t\tkfree(iod->rqstbuf);\n\t\tlist_del(&iod->ls_list);\n\t}\n\tkfree(tgtport->iod);\n}\n\nstatic struct nvmet_fc_ls_iod *\nnvmet_fc_alloc_ls_iod(struct nvmet_fc_tgtport *tgtport)\n{\n\tstruct nvmet_fc_ls_iod *iod;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&tgtport->lock, flags);\n\tiod = list_first_entry_or_null(&tgtport->ls_list,\n\t\t\t\t\tstruct nvmet_fc_ls_iod, ls_list);\n\tif (iod)\n\t\tlist_move_tail(&iod->ls_list, &tgtport->ls_busylist);\n\tspin_unlock_irqrestore(&tgtport->lock, flags);\n\treturn iod;\n}\n\n\nstatic void\nnvmet_fc_free_ls_iod(struct nvmet_fc_tgtport *tgtport,\n\t\t\tstruct nvmet_fc_ls_iod *iod)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&tgtport->lock, flags);\n\tlist_move(&iod->ls_list, &tgtport->ls_list);\n\tspin_unlock_irqrestore(&tgtport->lock, flags);\n}\n\nstatic void\nnvmet_fc_prep_fcp_iodlist(struct nvmet_fc_tgtport *tgtport,\n\t\t\t\tstruct nvmet_fc_tgt_queue *queue)\n{\n\tstruct nvmet_fc_fcp_iod *fod = queue->fod;\n\tint i;\n\n\tfor (i = 0; i < queue->sqsize; fod++, i++) {\n\t\tINIT_WORK(&fod->work, nvmet_fc_handle_fcp_rqst_work);\n\t\tINIT_WORK(&fod->done_work, nvmet_fc_fcp_rqst_op_done_work);\n\t\tfod->tgtport = tgtport;\n\t\tfod->queue = queue;\n\t\tfod->active = false;\n\t\tfod->abort = false;\n\t\tfod->aborted = false;\n\t\tfod->fcpreq = NULL;\n\t\tlist_add_tail(&fod->fcp_list, &queue->fod_list);\n\t\tspin_lock_init(&fod->flock);\n\n\t\tfod->rspdma = fc_dma_map_single(tgtport->dev, &fod->rspiubuf,\n\t\t\t\t\tsizeof(fod->rspiubuf), DMA_TO_DEVICE);\n\t\tif (fc_dma_mapping_error(tgtport->dev, fod->rspdma)) {\n\t\t\tlist_del(&fod->fcp_list);\n\t\t\tfor (fod--, i--; i >= 0; fod--, i--) {\n\t\t\t\tfc_dma_unmap_single(tgtport->dev, fod->rspdma,\n\t\t\t\t\t\tsizeof(fod->rspiubuf),\n\t\t\t\t\t\tDMA_TO_DEVICE);\n\t\t\t\tfod->rspdma = 0L;\n\t\t\t\tlist_del(&fod->fcp_list);\n\t\t\t}\n\n\t\t\treturn;\n\t\t}\n\t}\n}\n\nstatic void\nnvmet_fc_destroy_fcp_iodlist(struct nvmet_fc_tgtport *tgtport,\n\t\t\t\tstruct nvmet_fc_tgt_queue *queue)\n{\n\tstruct nvmet_fc_fcp_iod *fod = queue->fod;\n\tint i;\n\n\tfor (i = 0; i < queue->sqsize; fod++, i++) {\n\t\tif (fod->rspdma)\n\t\t\tfc_dma_unmap_single(tgtport->dev, fod->rspdma,\n\t\t\t\tsizeof(fod->rspiubuf), DMA_TO_DEVICE);\n\t}\n}\n\nstatic struct nvmet_fc_fcp_iod *\nnvmet_fc_alloc_fcp_iod(struct nvmet_fc_tgt_queue *queue)\n{\n\tstruct nvmet_fc_fcp_iod *fod;\n\n\tlockdep_assert_held(&queue->qlock);\n\n\tfod = list_first_entry_or_null(&queue->fod_list,\n\t\t\t\t\tstruct nvmet_fc_fcp_iod, fcp_list);\n\tif (fod) {\n\t\tlist_del(&fod->fcp_list);\n\t\tfod->active = true;\n\t\t/*\n\t\t * no queue reference is taken, as it was taken by the\n\t\t * queue lookup just prior to the allocation. The iod\n\t\t * will \"inherit\" that reference.\n\t\t */\n\t}\n\treturn fod;\n}\n\n\nstatic void\nnvmet_fc_queue_fcp_req(struct nvmet_fc_tgtport *tgtport,\n\t\t       struct nvmet_fc_tgt_queue *queue,\n\t\t       struct nvmefc_tgt_fcp_req *fcpreq)\n{\n\tstruct nvmet_fc_fcp_iod *fod = fcpreq->nvmet_fc_private;\n\n\t/*\n\t * put all admin cmds on hw queue id 0. All io commands go to\n\t * the respective hw queue based on a modulo basis\n\t */\n\tfcpreq->hwqid = queue->qid ?\n\t\t\t((queue->qid - 1) % tgtport->ops->max_hw_queues) : 0;\n\n\tif (tgtport->ops->target_features & NVMET_FCTGTFEAT_CMD_IN_ISR)\n\t\tqueue_work_on(queue->cpu, queue->work_q, &fod->work);\n\telse\n\t\tnvmet_fc_handle_fcp_rqst(tgtport, fod);\n}\n\nstatic void\nnvmet_fc_free_fcp_iod(struct nvmet_fc_tgt_queue *queue,\n\t\t\tstruct nvmet_fc_fcp_iod *fod)\n{\n\tstruct nvmefc_tgt_fcp_req *fcpreq = fod->fcpreq;\n\tstruct nvmet_fc_tgtport *tgtport = fod->tgtport;\n\tstruct nvmet_fc_defer_fcp_req *deferfcp;\n\tunsigned long flags;\n\n\tfc_dma_sync_single_for_cpu(tgtport->dev, fod->rspdma,\n\t\t\t\tsizeof(fod->rspiubuf), DMA_TO_DEVICE);\n\n\tfcpreq->nvmet_fc_private = NULL;\n\n\tfod->active = false;\n\tfod->abort = false;\n\tfod->aborted = false;\n\tfod->writedataactive = false;\n\tfod->fcpreq = NULL;\n\n\ttgtport->ops->fcp_req_release(&tgtport->fc_target_port, fcpreq);\n\n\tspin_lock_irqsave(&queue->qlock, flags);\n\tdeferfcp = list_first_entry_or_null(&queue->pending_cmd_list,\n\t\t\t\tstruct nvmet_fc_defer_fcp_req, req_list);\n\tif (!deferfcp) {\n\t\tlist_add_tail(&fod->fcp_list, &fod->queue->fod_list);\n\t\tspin_unlock_irqrestore(&queue->qlock, flags);\n\n\t\t/* Release reference taken at queue lookup and fod allocation */\n\t\tnvmet_fc_tgt_q_put(queue);\n\t\treturn;\n\t}\n\n\t/* Re-use the fod for the next pending cmd that was deferred */\n\tlist_del(&deferfcp->req_list);\n\n\tfcpreq = deferfcp->fcp_req;\n\n\t/* deferfcp can be reused for another IO at a later date */\n\tlist_add_tail(&deferfcp->req_list, &queue->avail_defer_list);\n\n\tspin_unlock_irqrestore(&queue->qlock, flags);\n\n\t/* Save NVME CMD IO in fod */\n\tmemcpy(&fod->cmdiubuf, fcpreq->rspaddr, fcpreq->rsplen);\n\n\t/* Setup new fcpreq to be processed */\n\tfcpreq->rspaddr = NULL;\n\tfcpreq->rsplen  = 0;\n\tfcpreq->nvmet_fc_private = fod;\n\tfod->fcpreq = fcpreq;\n\tfod->active = true;\n\n\t/* inform LLDD IO is now being processed */\n\ttgtport->ops->defer_rcv(&tgtport->fc_target_port, fcpreq);\n\n\t/* Submit deferred IO for processing */\n\tnvmet_fc_queue_fcp_req(tgtport, queue, fcpreq);\n\n\t/*\n\t * Leave the queue lookup get reference taken when\n\t * fod was originally allocated.\n\t */\n}\n\nstatic int\nnvmet_fc_queue_to_cpu(struct nvmet_fc_tgtport *tgtport, int qid)\n{\n\tint cpu, idx, cnt;\n\n\tif (tgtport->ops->max_hw_queues == 1)\n\t\treturn WORK_CPU_UNBOUND;\n\n\t/* Simple cpu selection based on qid modulo active cpu count */\n\tidx = !qid ? 0 : (qid - 1) % num_active_cpus();\n\n\t/* find the n'th active cpu */\n\tfor (cpu = 0, cnt = 0; ; ) {\n\t\tif (cpu_active(cpu)) {\n\t\t\tif (cnt == idx)\n\t\t\t\tbreak;\n\t\t\tcnt++;\n\t\t}\n\t\tcpu = (cpu + 1) % num_possible_cpus();\n\t}\n\n\treturn cpu;\n}\n\nstatic struct nvmet_fc_tgt_queue *\nnvmet_fc_alloc_target_queue(struct nvmet_fc_tgt_assoc *assoc,\n\t\t\tu16 qid, u16 sqsize)\n{\n\tstruct nvmet_fc_tgt_queue *queue;\n\tunsigned long flags;\n\tint ret;\n\n\tif (qid > NVMET_NR_QUEUES)\n\t\treturn NULL;\n\n\tqueue = kzalloc((sizeof(*queue) +\n\t\t\t\t(sizeof(struct nvmet_fc_fcp_iod) * sqsize)),\n\t\t\t\tGFP_KERNEL);\n\tif (!queue)\n\t\treturn NULL;\n\n\tif (!nvmet_fc_tgt_a_get(assoc))\n\t\tgoto out_free_queue;\n\n\tqueue->work_q = alloc_workqueue(\"ntfc%d.%d.%d\", 0, 0,\n\t\t\t\tassoc->tgtport->fc_target_port.port_num,\n\t\t\t\tassoc->a_id, qid);\n\tif (!queue->work_q)\n\t\tgoto out_a_put;\n\n\tqueue->fod = (struct nvmet_fc_fcp_iod *)&queue[1];\n\tqueue->qid = qid;\n\tqueue->sqsize = sqsize;\n\tqueue->assoc = assoc;\n\tqueue->port = assoc->tgtport->port;\n\tqueue->cpu = nvmet_fc_queue_to_cpu(assoc->tgtport, qid);\n\tINIT_LIST_HEAD(&queue->fod_list);\n\tINIT_LIST_HEAD(&queue->avail_defer_list);\n\tINIT_LIST_HEAD(&queue->pending_cmd_list);\n\tatomic_set(&queue->connected, 0);\n\tatomic_set(&queue->sqtail, 0);\n\tatomic_set(&queue->rsn, 1);\n\tatomic_set(&queue->zrspcnt, 0);\n\tspin_lock_init(&queue->qlock);\n\tkref_init(&queue->ref);\n\n\tnvmet_fc_prep_fcp_iodlist(assoc->tgtport, queue);\n\n\tret = nvmet_sq_init(&queue->nvme_sq);\n\tif (ret)\n\t\tgoto out_fail_iodlist;\n\n\tWARN_ON(assoc->queues[qid]);\n\tspin_lock_irqsave(&assoc->tgtport->lock, flags);\n\tassoc->queues[qid] = queue;\n\tspin_unlock_irqrestore(&assoc->tgtport->lock, flags);\n\n\treturn queue;\n\nout_fail_iodlist:\n\tnvmet_fc_destroy_fcp_iodlist(assoc->tgtport, queue);\n\tdestroy_workqueue(queue->work_q);\nout_a_put:\n\tnvmet_fc_tgt_a_put(assoc);\nout_free_queue:\n\tkfree(queue);\n\treturn NULL;\n}\n\n\nstatic void\nnvmet_fc_tgt_queue_free(struct kref *ref)\n{\n\tstruct nvmet_fc_tgt_queue *queue =\n\t\tcontainer_of(ref, struct nvmet_fc_tgt_queue, ref);\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&queue->assoc->tgtport->lock, flags);\n\tqueue->assoc->queues[queue->qid] = NULL;\n\tspin_unlock_irqrestore(&queue->assoc->tgtport->lock, flags);\n\n\tnvmet_fc_destroy_fcp_iodlist(queue->assoc->tgtport, queue);\n\n\tnvmet_fc_tgt_a_put(queue->assoc);\n\n\tdestroy_workqueue(queue->work_q);\n\n\tkfree(queue);\n}\n\nstatic void\nnvmet_fc_tgt_q_put(struct nvmet_fc_tgt_queue *queue)\n{\n\tkref_put(&queue->ref, nvmet_fc_tgt_queue_free);\n}\n\nstatic int\nnvmet_fc_tgt_q_get(struct nvmet_fc_tgt_queue *queue)\n{\n\treturn kref_get_unless_zero(&queue->ref);\n}\n\n\nstatic void\nnvmet_fc_delete_target_queue(struct nvmet_fc_tgt_queue *queue)\n{\n\tstruct nvmet_fc_tgtport *tgtport = queue->assoc->tgtport;\n\tstruct nvmet_fc_fcp_iod *fod = queue->fod;\n\tstruct nvmet_fc_defer_fcp_req *deferfcp, *tempptr;\n\tunsigned long flags;\n\tint i, writedataactive;\n\tbool disconnect;\n\n\tdisconnect = atomic_xchg(&queue->connected, 0);\n\n\tspin_lock_irqsave(&queue->qlock, flags);\n\t/* about outstanding io's */\n\tfor (i = 0; i < queue->sqsize; fod++, i++) {\n\t\tif (fod->active) {\n\t\t\tspin_lock(&fod->flock);\n\t\t\tfod->abort = true;\n\t\t\twritedataactive = fod->writedataactive;\n\t\t\tspin_unlock(&fod->flock);\n\t\t\t/*\n\t\t\t * only call lldd abort routine if waiting for\n\t\t\t * writedata. other outstanding ops should finish\n\t\t\t * on their own.\n\t\t\t */\n\t\t\tif (writedataactive) {\n\t\t\t\tspin_lock(&fod->flock);\n\t\t\t\tfod->aborted = true;\n\t\t\t\tspin_unlock(&fod->flock);\n\t\t\t\ttgtport->ops->fcp_abort(\n\t\t\t\t\t&tgtport->fc_target_port, fod->fcpreq);\n\t\t\t}\n\t\t}\n\t}\n\n\t/* Cleanup defer'ed IOs in queue */\n\tlist_for_each_entry_safe(deferfcp, tempptr, &queue->avail_defer_list,\n\t\t\t\treq_list) {\n\t\tlist_del(&deferfcp->req_list);\n\t\tkfree(deferfcp);\n\t}\n\n\tfor (;;) {\n\t\tdeferfcp = list_first_entry_or_null(&queue->pending_cmd_list,\n\t\t\t\tstruct nvmet_fc_defer_fcp_req, req_list);\n\t\tif (!deferfcp)\n\t\t\tbreak;\n\n\t\tlist_del(&deferfcp->req_list);\n\t\tspin_unlock_irqrestore(&queue->qlock, flags);\n\n\t\ttgtport->ops->defer_rcv(&tgtport->fc_target_port,\n\t\t\t\tdeferfcp->fcp_req);\n\n\t\ttgtport->ops->fcp_abort(&tgtport->fc_target_port,\n\t\t\t\tdeferfcp->fcp_req);\n\n\t\ttgtport->ops->fcp_req_release(&tgtport->fc_target_port,\n\t\t\t\tdeferfcp->fcp_req);\n\n\t\tkfree(deferfcp);\n\n\t\tspin_lock_irqsave(&queue->qlock, flags);\n\t}\n\tspin_unlock_irqrestore(&queue->qlock, flags);\n\n\tflush_workqueue(queue->work_q);\n\n\tif (disconnect)\n\t\tnvmet_sq_destroy(&queue->nvme_sq);\n\n\tnvmet_fc_tgt_q_put(queue);\n}\n\nstatic struct nvmet_fc_tgt_queue *\nnvmet_fc_find_target_queue(struct nvmet_fc_tgtport *tgtport,\n\t\t\t\tu64 connection_id)\n{\n\tstruct nvmet_fc_tgt_assoc *assoc;\n\tstruct nvmet_fc_tgt_queue *queue;\n\tu64 association_id = nvmet_fc_getassociationid(connection_id);\n\tu16 qid = nvmet_fc_getqueueid(connection_id);\n\tunsigned long flags;\n\n\tif (qid > NVMET_NR_QUEUES)\n\t\treturn NULL;\n\n\tspin_lock_irqsave(&tgtport->lock, flags);\n\tlist_for_each_entry(assoc, &tgtport->assoc_list, a_list) {\n\t\tif (association_id == assoc->association_id) {\n\t\t\tqueue = assoc->queues[qid];\n\t\t\tif (queue &&\n\t\t\t    (!atomic_read(&queue->connected) ||\n\t\t\t     !nvmet_fc_tgt_q_get(queue)))\n\t\t\t\tqueue = NULL;\n\t\t\tspin_unlock_irqrestore(&tgtport->lock, flags);\n\t\t\treturn queue;\n\t\t}\n\t}\n\tspin_unlock_irqrestore(&tgtport->lock, flags);\n\treturn NULL;\n}\n\nstatic struct nvmet_fc_tgt_assoc *\nnvmet_fc_alloc_target_assoc(struct nvmet_fc_tgtport *tgtport)\n{\n\tstruct nvmet_fc_tgt_assoc *assoc, *tmpassoc;\n\tunsigned long flags;\n\tu64 ran;\n\tint idx;\n\tbool needrandom = true;\n\n\tassoc = kzalloc(sizeof(*assoc), GFP_KERNEL);\n\tif (!assoc)\n\t\treturn NULL;\n\n\tidx = ida_simple_get(&tgtport->assoc_cnt, 0, 0, GFP_KERNEL);\n\tif (idx < 0)\n\t\tgoto out_free_assoc;\n\n\tif (!nvmet_fc_tgtport_get(tgtport))\n\t\tgoto out_ida_put;\n\n\tassoc->tgtport = tgtport;\n\tassoc->a_id = idx;\n\tINIT_LIST_HEAD(&assoc->a_list);\n\tkref_init(&assoc->ref);\n\n\twhile (needrandom) {\n\t\tget_random_bytes(&ran, sizeof(ran) - BYTES_FOR_QID);\n\t\tran = ran << BYTES_FOR_QID_SHIFT;\n\n\t\tspin_lock_irqsave(&tgtport->lock, flags);\n\t\tneedrandom = false;\n\t\tlist_for_each_entry(tmpassoc, &tgtport->assoc_list, a_list)\n\t\t\tif (ran == tmpassoc->association_id) {\n\t\t\t\tneedrandom = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\tif (!needrandom) {\n\t\t\tassoc->association_id = ran;\n\t\t\tlist_add_tail(&assoc->a_list, &tgtport->assoc_list);\n\t\t}\n\t\tspin_unlock_irqrestore(&tgtport->lock, flags);\n\t}\n\n\treturn assoc;\n\nout_ida_put:\n\tida_simple_remove(&tgtport->assoc_cnt, idx);\nout_free_assoc:\n\tkfree(assoc);\n\treturn NULL;\n}\n\nstatic void\nnvmet_fc_target_assoc_free(struct kref *ref)\n{\n\tstruct nvmet_fc_tgt_assoc *assoc =\n\t\tcontainer_of(ref, struct nvmet_fc_tgt_assoc, ref);\n\tstruct nvmet_fc_tgtport *tgtport = assoc->tgtport;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&tgtport->lock, flags);\n\tlist_del(&assoc->a_list);\n\tspin_unlock_irqrestore(&tgtport->lock, flags);\n\tida_simple_remove(&tgtport->assoc_cnt, assoc->a_id);\n\tkfree(assoc);\n\tnvmet_fc_tgtport_put(tgtport);\n}\n\nstatic void\nnvmet_fc_tgt_a_put(struct nvmet_fc_tgt_assoc *assoc)\n{\n\tkref_put(&assoc->ref, nvmet_fc_target_assoc_free);\n}\n\nstatic int\nnvmet_fc_tgt_a_get(struct nvmet_fc_tgt_assoc *assoc)\n{\n\treturn kref_get_unless_zero(&assoc->ref);\n}\n\nstatic void\nnvmet_fc_delete_target_assoc(struct nvmet_fc_tgt_assoc *assoc)\n{\n\tstruct nvmet_fc_tgtport *tgtport = assoc->tgtport;\n\tstruct nvmet_fc_tgt_queue *queue;\n\tunsigned long flags;\n\tint i;\n\n\tspin_lock_irqsave(&tgtport->lock, flags);\n\tfor (i = NVMET_NR_QUEUES; i >= 0; i--) {\n\t\tqueue = assoc->queues[i];\n\t\tif (queue) {\n\t\t\tif (!nvmet_fc_tgt_q_get(queue))\n\t\t\t\tcontinue;\n\t\t\tspin_unlock_irqrestore(&tgtport->lock, flags);\n\t\t\tnvmet_fc_delete_target_queue(queue);\n\t\t\tnvmet_fc_tgt_q_put(queue);\n\t\t\tspin_lock_irqsave(&tgtport->lock, flags);\n\t\t}\n\t}\n\tspin_unlock_irqrestore(&tgtport->lock, flags);\n\n\tnvmet_fc_tgt_a_put(assoc);\n}\n\nstatic struct nvmet_fc_tgt_assoc *\nnvmet_fc_find_target_assoc(struct nvmet_fc_tgtport *tgtport,\n\t\t\t\tu64 association_id)\n{\n\tstruct nvmet_fc_tgt_assoc *assoc;\n\tstruct nvmet_fc_tgt_assoc *ret = NULL;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&tgtport->lock, flags);\n\tlist_for_each_entry(assoc, &tgtport->assoc_list, a_list) {\n\t\tif (association_id == assoc->association_id) {\n\t\t\tret = assoc;\n\t\t\tnvmet_fc_tgt_a_get(assoc);\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock_irqrestore(&tgtport->lock, flags);\n\n\treturn ret;\n}\n\n\n/**\n * nvme_fc_register_targetport - transport entry point called by an\n *                              LLDD to register the existence of a local\n *                              NVME subystem FC port.\n * @pinfo:     pointer to information about the port to be registered\n * @template:  LLDD entrypoints and operational parameters for the port\n * @dev:       physical hardware device node port corresponds to. Will be\n *             used for DMA mappings\n * @portptr:   pointer to a local port pointer. Upon success, the routine\n *             will allocate a nvme_fc_local_port structure and place its\n *             address in the local port pointer. Upon failure, local port\n *             pointer will be set to NULL.\n *\n * Returns:\n * a completion status. Must be 0 upon success; a negative errno\n * (ex: -ENXIO) upon failure.\n */\nint\nnvmet_fc_register_targetport(struct nvmet_fc_port_info *pinfo,\n\t\t\tstruct nvmet_fc_target_template *template,\n\t\t\tstruct device *dev,\n\t\t\tstruct nvmet_fc_target_port **portptr)\n{\n\tstruct nvmet_fc_tgtport *newrec;\n\tunsigned long flags;\n\tint ret, idx;\n\n\tif (!template->xmt_ls_rsp || !template->fcp_op ||\n\t    !template->fcp_abort ||\n\t    !template->fcp_req_release || !template->targetport_delete ||\n\t    !template->max_hw_queues || !template->max_sgl_segments ||\n\t    !template->max_dif_sgl_segments || !template->dma_boundary) {\n\t\tret = -EINVAL;\n\t\tgoto out_regtgt_failed;\n\t}\n\n\tnewrec = kzalloc((sizeof(*newrec) + template->target_priv_sz),\n\t\t\t GFP_KERNEL);\n\tif (!newrec) {\n\t\tret = -ENOMEM;\n\t\tgoto out_regtgt_failed;\n\t}\n\n\tidx = ida_simple_get(&nvmet_fc_tgtport_cnt, 0, 0, GFP_KERNEL);\n\tif (idx < 0) {\n\t\tret = -ENOSPC;\n\t\tgoto out_fail_kfree;\n\t}\n\n\tif (!get_device(dev) && dev) {\n\t\tret = -ENODEV;\n\t\tgoto out_ida_put;\n\t}\n\n\tnewrec->fc_target_port.node_name = pinfo->node_name;\n\tnewrec->fc_target_port.port_name = pinfo->port_name;\n\tnewrec->fc_target_port.private = &newrec[1];\n\tnewrec->fc_target_port.port_id = pinfo->port_id;\n\tnewrec->fc_target_port.port_num = idx;\n\tINIT_LIST_HEAD(&newrec->tgt_list);\n\tnewrec->dev = dev;\n\tnewrec->ops = template;\n\tspin_lock_init(&newrec->lock);\n\tINIT_LIST_HEAD(&newrec->ls_list);\n\tINIT_LIST_HEAD(&newrec->ls_busylist);\n\tINIT_LIST_HEAD(&newrec->assoc_list);\n\tkref_init(&newrec->ref);\n\tida_init(&newrec->assoc_cnt);\n\tnewrec->max_sg_cnt = min_t(u32, NVMET_FC_MAX_XFR_SGENTS,\n\t\t\t\t\ttemplate->max_sgl_segments);\n\n\tret = nvmet_fc_alloc_ls_iodlist(newrec);\n\tif (ret) {\n\t\tret = -ENOMEM;\n\t\tgoto out_free_newrec;\n\t}\n\n\tspin_lock_irqsave(&nvmet_fc_tgtlock, flags);\n\tlist_add_tail(&newrec->tgt_list, &nvmet_fc_target_list);\n\tspin_unlock_irqrestore(&nvmet_fc_tgtlock, flags);\n\n\t*portptr = &newrec->fc_target_port;\n\treturn 0;\n\nout_free_newrec:\n\tput_device(dev);\nout_ida_put:\n\tida_simple_remove(&nvmet_fc_tgtport_cnt, idx);\nout_fail_kfree:\n\tkfree(newrec);\nout_regtgt_failed:\n\t*portptr = NULL;\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(nvmet_fc_register_targetport);\n\n\nstatic void\nnvmet_fc_free_tgtport(struct kref *ref)\n{\n\tstruct nvmet_fc_tgtport *tgtport =\n\t\tcontainer_of(ref, struct nvmet_fc_tgtport, ref);\n\tstruct device *dev = tgtport->dev;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&nvmet_fc_tgtlock, flags);\n\tlist_del(&tgtport->tgt_list);\n\tspin_unlock_irqrestore(&nvmet_fc_tgtlock, flags);\n\n\tnvmet_fc_free_ls_iodlist(tgtport);\n\n\t/* let the LLDD know we've finished tearing it down */\n\ttgtport->ops->targetport_delete(&tgtport->fc_target_port);\n\n\tida_simple_remove(&nvmet_fc_tgtport_cnt,\n\t\t\ttgtport->fc_target_port.port_num);\n\n\tida_destroy(&tgtport->assoc_cnt);\n\n\tkfree(tgtport);\n\n\tput_device(dev);\n}\n\nstatic void\nnvmet_fc_tgtport_put(struct nvmet_fc_tgtport *tgtport)\n{\n\tkref_put(&tgtport->ref, nvmet_fc_free_tgtport);\n}\n\nstatic int\nnvmet_fc_tgtport_get(struct nvmet_fc_tgtport *tgtport)\n{\n\treturn kref_get_unless_zero(&tgtport->ref);\n}\n\nstatic void\n__nvmet_fc_free_assocs(struct nvmet_fc_tgtport *tgtport)\n{\n\tstruct nvmet_fc_tgt_assoc *assoc, *next;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&tgtport->lock, flags);\n\tlist_for_each_entry_safe(assoc, next,\n\t\t\t\t&tgtport->assoc_list, a_list) {\n\t\tif (!nvmet_fc_tgt_a_get(assoc))\n\t\t\tcontinue;\n\t\tspin_unlock_irqrestore(&tgtport->lock, flags);\n\t\tnvmet_fc_delete_target_assoc(assoc);\n\t\tnvmet_fc_tgt_a_put(assoc);\n\t\tspin_lock_irqsave(&tgtport->lock, flags);\n\t}\n\tspin_unlock_irqrestore(&tgtport->lock, flags);\n}\n\n/*\n * nvmet layer has called to terminate an association\n */\nstatic void\nnvmet_fc_delete_ctrl(struct nvmet_ctrl *ctrl)\n{\n\tstruct nvmet_fc_tgtport *tgtport, *next;\n\tstruct nvmet_fc_tgt_assoc *assoc;\n\tstruct nvmet_fc_tgt_queue *queue;\n\tunsigned long flags;\n\tbool found_ctrl = false;\n\n\t/* this is a bit ugly, but don't want to make locks layered */\n\tspin_lock_irqsave(&nvmet_fc_tgtlock, flags);\n\tlist_for_each_entry_safe(tgtport, next, &nvmet_fc_target_list,\n\t\t\ttgt_list) {\n\t\tif (!nvmet_fc_tgtport_get(tgtport))\n\t\t\tcontinue;\n\t\tspin_unlock_irqrestore(&nvmet_fc_tgtlock, flags);\n\n\t\tspin_lock_irqsave(&tgtport->lock, flags);\n\t\tlist_for_each_entry(assoc, &tgtport->assoc_list, a_list) {\n\t\t\tqueue = assoc->queues[0];\n\t\t\tif (queue && queue->nvme_sq.ctrl == ctrl) {\n\t\t\t\tif (nvmet_fc_tgt_a_get(assoc))\n\t\t\t\t\tfound_ctrl = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tspin_unlock_irqrestore(&tgtport->lock, flags);\n\n\t\tnvmet_fc_tgtport_put(tgtport);\n\n\t\tif (found_ctrl) {\n\t\t\tnvmet_fc_delete_target_assoc(assoc);\n\t\t\tnvmet_fc_tgt_a_put(assoc);\n\t\t\treturn;\n\t\t}\n\n\t\tspin_lock_irqsave(&nvmet_fc_tgtlock, flags);\n\t}\n\tspin_unlock_irqrestore(&nvmet_fc_tgtlock, flags);\n}\n\n/**\n * nvme_fc_unregister_targetport - transport entry point called by an\n *                              LLDD to deregister/remove a previously\n *                              registered a local NVME subsystem FC port.\n * @tgtport: pointer to the (registered) target port that is to be\n *           deregistered.\n *\n * Returns:\n * a completion status. Must be 0 upon success; a negative errno\n * (ex: -ENXIO) upon failure.\n */\nint\nnvmet_fc_unregister_targetport(struct nvmet_fc_target_port *target_port)\n{\n\tstruct nvmet_fc_tgtport *tgtport = targetport_to_tgtport(target_port);\n\n\t/* terminate any outstanding associations */\n\t__nvmet_fc_free_assocs(tgtport);\n\n\tnvmet_fc_tgtport_put(tgtport);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(nvmet_fc_unregister_targetport);\n\n\n/* *********************** FC-NVME LS Handling **************************** */\n\n\nstatic void\nnvmet_fc_format_rsp_hdr(void *buf, u8 ls_cmd, __be32 desc_len, u8 rqst_ls_cmd)\n{\n\tstruct fcnvme_ls_acc_hdr *acc = buf;\n\n\tacc->w0.ls_cmd = ls_cmd;\n\tacc->desc_list_len = desc_len;\n\tacc->rqst.desc_tag = cpu_to_be32(FCNVME_LSDESC_RQST);\n\tacc->rqst.desc_len =\n\t\t\tfcnvme_lsdesc_len(sizeof(struct fcnvme_lsdesc_rqst));\n\tacc->rqst.w0.ls_cmd = rqst_ls_cmd;\n}\n\nstatic int\nnvmet_fc_format_rjt(void *buf, u16 buflen, u8 ls_cmd,\n\t\t\tu8 reason, u8 explanation, u8 vendor)\n{\n\tstruct fcnvme_ls_rjt *rjt = buf;\n\n\tnvmet_fc_format_rsp_hdr(buf, FCNVME_LSDESC_RQST,\n\t\t\tfcnvme_lsdesc_len(sizeof(struct fcnvme_ls_rjt)),\n\t\t\tls_cmd);\n\trjt->rjt.desc_tag = cpu_to_be32(FCNVME_LSDESC_RJT);\n\trjt->rjt.desc_len = fcnvme_lsdesc_len(sizeof(struct fcnvme_lsdesc_rjt));\n\trjt->rjt.reason_code = reason;\n\trjt->rjt.reason_explanation = explanation;\n\trjt->rjt.vendor = vendor;\n\n\treturn sizeof(struct fcnvme_ls_rjt);\n}\n\n/* Validation Error indexes into the string table below */\nenum {\n\tVERR_NO_ERROR\t\t= 0,\n\tVERR_CR_ASSOC_LEN\t= 1,\n\tVERR_CR_ASSOC_RQST_LEN\t= 2,\n\tVERR_CR_ASSOC_CMD\t= 3,\n\tVERR_CR_ASSOC_CMD_LEN\t= 4,\n\tVERR_ERSP_RATIO\t\t= 5,\n\tVERR_ASSOC_ALLOC_FAIL\t= 6,\n\tVERR_QUEUE_ALLOC_FAIL\t= 7,\n\tVERR_CR_CONN_LEN\t= 8,\n\tVERR_CR_CONN_RQST_LEN\t= 9,\n\tVERR_ASSOC_ID\t\t= 10,\n\tVERR_ASSOC_ID_LEN\t= 11,\n\tVERR_NO_ASSOC\t\t= 12,\n\tVERR_CONN_ID\t\t= 13,\n\tVERR_CONN_ID_LEN\t= 14,\n\tVERR_NO_CONN\t\t= 15,\n\tVERR_CR_CONN_CMD\t= 16,\n\tVERR_CR_CONN_CMD_LEN\t= 17,\n\tVERR_DISCONN_LEN\t= 18,\n\tVERR_DISCONN_RQST_LEN\t= 19,\n\tVERR_DISCONN_CMD\t= 20,\n\tVERR_DISCONN_CMD_LEN\t= 21,\n\tVERR_DISCONN_SCOPE\t= 22,\n\tVERR_RS_LEN\t\t= 23,\n\tVERR_RS_RQST_LEN\t= 24,\n\tVERR_RS_CMD\t\t= 25,\n\tVERR_RS_CMD_LEN\t\t= 26,\n\tVERR_RS_RCTL\t\t= 27,\n\tVERR_RS_RO\t\t= 28,\n};\n\nstatic char *validation_errors[] = {\n\t\"OK\",\n\t\"Bad CR_ASSOC Length\",\n\t\"Bad CR_ASSOC Rqst Length\",\n\t\"Not CR_ASSOC Cmd\",\n\t\"Bad CR_ASSOC Cmd Length\",\n\t\"Bad Ersp Ratio\",\n\t\"Association Allocation Failed\",\n\t\"Queue Allocation Failed\",\n\t\"Bad CR_CONN Length\",\n\t\"Bad CR_CONN Rqst Length\",\n\t\"Not Association ID\",\n\t\"Bad Association ID Length\",\n\t\"No Association\",\n\t\"Not Connection ID\",\n\t\"Bad Connection ID Length\",\n\t\"No Connection\",\n\t\"Not CR_CONN Cmd\",\n\t\"Bad CR_CONN Cmd Length\",\n\t\"Bad DISCONN Length\",\n\t\"Bad DISCONN Rqst Length\",\n\t\"Not DISCONN Cmd\",\n\t\"Bad DISCONN Cmd Length\",\n\t\"Bad Disconnect Scope\",\n\t\"Bad RS Length\",\n\t\"Bad RS Rqst Length\",\n\t\"Not RS Cmd\",\n\t\"Bad RS Cmd Length\",\n\t\"Bad RS R_CTL\",\n\t\"Bad RS Relative Offset\",\n};\n\nstatic void\nnvmet_fc_ls_create_association(struct nvmet_fc_tgtport *tgtport,\n\t\t\tstruct nvmet_fc_ls_iod *iod)\n{\n\tstruct fcnvme_ls_cr_assoc_rqst *rqst =\n\t\t\t\t(struct fcnvme_ls_cr_assoc_rqst *)iod->rqstbuf;\n\tstruct fcnvme_ls_cr_assoc_acc *acc =\n\t\t\t\t(struct fcnvme_ls_cr_assoc_acc *)iod->rspbuf;\n\tstruct nvmet_fc_tgt_queue *queue;\n\tint ret = 0;\n\n\tmemset(acc, 0, sizeof(*acc));\n\n\t/*\n\t * FC-NVME spec changes. There are initiators sending different\n\t * lengths as padding sizes for Create Association Cmd descriptor\n\t * was incorrect.\n\t * Accept anything of \"minimum\" length. Assume format per 1.15\n\t * spec (with HOSTID reduced to 16 bytes), ignore how long the\n\t * trailing pad length is.\n\t */\n\tif (iod->rqstdatalen < FCNVME_LSDESC_CRA_RQST_MINLEN)\n\t\tret = VERR_CR_ASSOC_LEN;\n\telse if (be32_to_cpu(rqst->desc_list_len) <\n\t\t\tFCNVME_LSDESC_CRA_RQST_MIN_LISTLEN)\n\t\tret = VERR_CR_ASSOC_RQST_LEN;\n\telse if (rqst->assoc_cmd.desc_tag !=\n\t\t\tcpu_to_be32(FCNVME_LSDESC_CREATE_ASSOC_CMD))\n\t\tret = VERR_CR_ASSOC_CMD;\n\telse if (be32_to_cpu(rqst->assoc_cmd.desc_len) <\n\t\t\tFCNVME_LSDESC_CRA_CMD_DESC_MIN_DESCLEN)\n\t\tret = VERR_CR_ASSOC_CMD_LEN;\n\telse if (!rqst->assoc_cmd.ersp_ratio ||\n\t\t (be16_to_cpu(rqst->assoc_cmd.ersp_ratio) >=\n\t\t\t\tbe16_to_cpu(rqst->assoc_cmd.sqsize)))\n\t\tret = VERR_ERSP_RATIO;\n\n\telse {\n\t\t/* new association w/ admin queue */\n\t\tiod->assoc = nvmet_fc_alloc_target_assoc(tgtport);\n\t\tif (!iod->assoc)\n\t\t\tret = VERR_ASSOC_ALLOC_FAIL;\n\t\telse {\n\t\t\tqueue = nvmet_fc_alloc_target_queue(iod->assoc, 0,\n\t\t\t\t\tbe16_to_cpu(rqst->assoc_cmd.sqsize));\n\t\t\tif (!queue)\n\t\t\t\tret = VERR_QUEUE_ALLOC_FAIL;\n\t\t}\n\t}\n\n\tif (ret) {\n\t\tdev_err(tgtport->dev,\n\t\t\t\"Create Association LS failed: %s\\n\",\n\t\t\tvalidation_errors[ret]);\n\t\tiod->lsreq->rsplen = nvmet_fc_format_rjt(acc,\n\t\t\t\tNVME_FC_MAX_LS_BUFFER_SIZE, rqst->w0.ls_cmd,\n\t\t\t\tFCNVME_RJT_RC_LOGIC,\n\t\t\t\tFCNVME_RJT_EXP_NONE, 0);\n\t\treturn;\n\t}\n\n\tqueue->ersp_ratio = be16_to_cpu(rqst->assoc_cmd.ersp_ratio);\n\tatomic_set(&queue->connected, 1);\n\tqueue->sqhd = 0;\t/* best place to init value */\n\n\t/* format a response */\n\n\tiod->lsreq->rsplen = sizeof(*acc);\n\n\tnvmet_fc_format_rsp_hdr(acc, FCNVME_LS_ACC,\n\t\t\tfcnvme_lsdesc_len(\n\t\t\t\tsizeof(struct fcnvme_ls_cr_assoc_acc)),\n\t\t\tFCNVME_LS_CREATE_ASSOCIATION);\n\tacc->associd.desc_tag = cpu_to_be32(FCNVME_LSDESC_ASSOC_ID);\n\tacc->associd.desc_len =\n\t\t\tfcnvme_lsdesc_len(\n\t\t\t\tsizeof(struct fcnvme_lsdesc_assoc_id));\n\tacc->associd.association_id =\n\t\t\tcpu_to_be64(nvmet_fc_makeconnid(iod->assoc, 0));\n\tacc->connectid.desc_tag = cpu_to_be32(FCNVME_LSDESC_CONN_ID);\n\tacc->connectid.desc_len =\n\t\t\tfcnvme_lsdesc_len(\n\t\t\t\tsizeof(struct fcnvme_lsdesc_conn_id));\n\tacc->connectid.connection_id = acc->associd.association_id;\n}\n\nstatic void\nnvmet_fc_ls_create_connection(struct nvmet_fc_tgtport *tgtport,\n\t\t\tstruct nvmet_fc_ls_iod *iod)\n{\n\tstruct fcnvme_ls_cr_conn_rqst *rqst =\n\t\t\t\t(struct fcnvme_ls_cr_conn_rqst *)iod->rqstbuf;\n\tstruct fcnvme_ls_cr_conn_acc *acc =\n\t\t\t\t(struct fcnvme_ls_cr_conn_acc *)iod->rspbuf;\n\tstruct nvmet_fc_tgt_queue *queue;\n\tint ret = 0;\n\n\tmemset(acc, 0, sizeof(*acc));\n\n\tif (iod->rqstdatalen < sizeof(struct fcnvme_ls_cr_conn_rqst))\n\t\tret = VERR_CR_CONN_LEN;\n\telse if (rqst->desc_list_len !=\n\t\t\tfcnvme_lsdesc_len(\n\t\t\t\tsizeof(struct fcnvme_ls_cr_conn_rqst)))\n\t\tret = VERR_CR_CONN_RQST_LEN;\n\telse if (rqst->associd.desc_tag != cpu_to_be32(FCNVME_LSDESC_ASSOC_ID))\n\t\tret = VERR_ASSOC_ID;\n\telse if (rqst->associd.desc_len !=\n\t\t\tfcnvme_lsdesc_len(\n\t\t\t\tsizeof(struct fcnvme_lsdesc_assoc_id)))\n\t\tret = VERR_ASSOC_ID_LEN;\n\telse if (rqst->connect_cmd.desc_tag !=\n\t\t\tcpu_to_be32(FCNVME_LSDESC_CREATE_CONN_CMD))\n\t\tret = VERR_CR_CONN_CMD;\n\telse if (rqst->connect_cmd.desc_len !=\n\t\t\tfcnvme_lsdesc_len(\n\t\t\t\tsizeof(struct fcnvme_lsdesc_cr_conn_cmd)))\n\t\tret = VERR_CR_CONN_CMD_LEN;\n\telse if (!rqst->connect_cmd.ersp_ratio ||\n\t\t (be16_to_cpu(rqst->connect_cmd.ersp_ratio) >=\n\t\t\t\tbe16_to_cpu(rqst->connect_cmd.sqsize)))\n\t\tret = VERR_ERSP_RATIO;\n\n\telse {\n\t\t/* new io queue */\n\t\tiod->assoc = nvmet_fc_find_target_assoc(tgtport,\n\t\t\t\tbe64_to_cpu(rqst->associd.association_id));\n\t\tif (!iod->assoc)\n\t\t\tret = VERR_NO_ASSOC;\n\t\telse {\n\t\t\tqueue = nvmet_fc_alloc_target_queue(iod->assoc,\n\t\t\t\t\tbe16_to_cpu(rqst->connect_cmd.qid),\n\t\t\t\t\tbe16_to_cpu(rqst->connect_cmd.sqsize));\n\t\t\tif (!queue)\n\t\t\t\tret = VERR_QUEUE_ALLOC_FAIL;\n\n\t\t\t/* release get taken in nvmet_fc_find_target_assoc */\n\t\t\tnvmet_fc_tgt_a_put(iod->assoc);\n\t\t}\n\t}\n\n\tif (ret) {\n\t\tdev_err(tgtport->dev,\n\t\t\t\"Create Connection LS failed: %s\\n\",\n\t\t\tvalidation_errors[ret]);\n\t\tiod->lsreq->rsplen = nvmet_fc_format_rjt(acc,\n\t\t\t\tNVME_FC_MAX_LS_BUFFER_SIZE, rqst->w0.ls_cmd,\n\t\t\t\t(ret == VERR_NO_ASSOC) ?\n\t\t\t\t\tFCNVME_RJT_RC_INV_ASSOC :\n\t\t\t\t\tFCNVME_RJT_RC_LOGIC,\n\t\t\t\tFCNVME_RJT_EXP_NONE, 0);\n\t\treturn;\n\t}\n\n\tqueue->ersp_ratio = be16_to_cpu(rqst->connect_cmd.ersp_ratio);\n\tatomic_set(&queue->connected, 1);\n\tqueue->sqhd = 0;\t/* best place to init value */\n\n\t/* format a response */\n\n\tiod->lsreq->rsplen = sizeof(*acc);\n\n\tnvmet_fc_format_rsp_hdr(acc, FCNVME_LS_ACC,\n\t\t\tfcnvme_lsdesc_len(sizeof(struct fcnvme_ls_cr_conn_acc)),\n\t\t\tFCNVME_LS_CREATE_CONNECTION);\n\tacc->connectid.desc_tag = cpu_to_be32(FCNVME_LSDESC_CONN_ID);\n\tacc->connectid.desc_len =\n\t\t\tfcnvme_lsdesc_len(\n\t\t\t\tsizeof(struct fcnvme_lsdesc_conn_id));\n\tacc->connectid.connection_id =\n\t\t\tcpu_to_be64(nvmet_fc_makeconnid(iod->assoc,\n\t\t\t\tbe16_to_cpu(rqst->connect_cmd.qid)));\n}\n\nstatic void\nnvmet_fc_ls_disconnect(struct nvmet_fc_tgtport *tgtport,\n\t\t\tstruct nvmet_fc_ls_iod *iod)\n{\n\tstruct fcnvme_ls_disconnect_rqst *rqst =\n\t\t\t(struct fcnvme_ls_disconnect_rqst *)iod->rqstbuf;\n\tstruct fcnvme_ls_disconnect_acc *acc =\n\t\t\t(struct fcnvme_ls_disconnect_acc *)iod->rspbuf;\n\tstruct nvmet_fc_tgt_queue *queue = NULL;\n\tstruct nvmet_fc_tgt_assoc *assoc;\n\tint ret = 0;\n\tbool del_assoc = false;\n\n\tmemset(acc, 0, sizeof(*acc));\n\n\tif (iod->rqstdatalen < sizeof(struct fcnvme_ls_disconnect_rqst))\n\t\tret = VERR_DISCONN_LEN;\n\telse if (rqst->desc_list_len !=\n\t\t\tfcnvme_lsdesc_len(\n\t\t\t\tsizeof(struct fcnvme_ls_disconnect_rqst)))\n\t\tret = VERR_DISCONN_RQST_LEN;\n\telse if (rqst->associd.desc_tag != cpu_to_be32(FCNVME_LSDESC_ASSOC_ID))\n\t\tret = VERR_ASSOC_ID;\n\telse if (rqst->associd.desc_len !=\n\t\t\tfcnvme_lsdesc_len(\n\t\t\t\tsizeof(struct fcnvme_lsdesc_assoc_id)))\n\t\tret = VERR_ASSOC_ID_LEN;\n\telse if (rqst->discon_cmd.desc_tag !=\n\t\t\tcpu_to_be32(FCNVME_LSDESC_DISCONN_CMD))\n\t\tret = VERR_DISCONN_CMD;\n\telse if (rqst->discon_cmd.desc_len !=\n\t\t\tfcnvme_lsdesc_len(\n\t\t\t\tsizeof(struct fcnvme_lsdesc_disconn_cmd)))\n\t\tret = VERR_DISCONN_CMD_LEN;\n\telse if ((rqst->discon_cmd.scope != FCNVME_DISCONN_ASSOCIATION) &&\n\t\t\t(rqst->discon_cmd.scope != FCNVME_DISCONN_CONNECTION))\n\t\tret = VERR_DISCONN_SCOPE;\n\telse {\n\t\t/* match an active association */\n\t\tassoc = nvmet_fc_find_target_assoc(tgtport,\n\t\t\t\tbe64_to_cpu(rqst->associd.association_id));\n\t\tiod->assoc = assoc;\n\t\tif (assoc) {\n\t\t\tif (rqst->discon_cmd.scope ==\n\t\t\t\t\tFCNVME_DISCONN_CONNECTION) {\n\t\t\t\tqueue = nvmet_fc_find_target_queue(tgtport,\n\t\t\t\t\t\tbe64_to_cpu(\n\t\t\t\t\t\t\trqst->discon_cmd.id));\n\t\t\t\tif (!queue) {\n\t\t\t\t\tnvmet_fc_tgt_a_put(assoc);\n\t\t\t\t\tret = VERR_NO_CONN;\n\t\t\t\t}\n\t\t\t}\n\t\t} else\n\t\t\tret = VERR_NO_ASSOC;\n\t}\n\n\tif (ret) {\n\t\tdev_err(tgtport->dev,\n\t\t\t\"Disconnect LS failed: %s\\n\",\n\t\t\tvalidation_errors[ret]);\n\t\tiod->lsreq->rsplen = nvmet_fc_format_rjt(acc,\n\t\t\t\tNVME_FC_MAX_LS_BUFFER_SIZE, rqst->w0.ls_cmd,\n\t\t\t\t(ret == VERR_NO_ASSOC) ?\n\t\t\t\t\tFCNVME_RJT_RC_INV_ASSOC :\n\t\t\t\t\t(ret == VERR_NO_CONN) ?\n\t\t\t\t\t\tFCNVME_RJT_RC_INV_CONN :\n\t\t\t\t\t\tFCNVME_RJT_RC_LOGIC,\n\t\t\t\tFCNVME_RJT_EXP_NONE, 0);\n\t\treturn;\n\t}\n\n\t/* format a response */\n\n\tiod->lsreq->rsplen = sizeof(*acc);\n\n\tnvmet_fc_format_rsp_hdr(acc, FCNVME_LS_ACC,\n\t\t\tfcnvme_lsdesc_len(\n\t\t\t\tsizeof(struct fcnvme_ls_disconnect_acc)),\n\t\t\tFCNVME_LS_DISCONNECT);\n\n\n\t/* are we to delete a Connection ID (queue) */\n\tif (queue) {\n\t\tint qid = queue->qid;\n\n\t\tnvmet_fc_delete_target_queue(queue);\n\n\t\t/* release the get taken by find_target_queue */\n\t\tnvmet_fc_tgt_q_put(queue);\n\n\t\t/* tear association down if io queue terminated */\n\t\tif (!qid)\n\t\t\tdel_assoc = true;\n\t}\n\n\t/* release get taken in nvmet_fc_find_target_assoc */\n\tnvmet_fc_tgt_a_put(iod->assoc);\n\n\tif (del_assoc)\n\t\tnvmet_fc_delete_target_assoc(iod->assoc);\n}\n\n\n/* *********************** NVME Ctrl Routines **************************** */\n\n\nstatic void nvmet_fc_fcp_nvme_cmd_done(struct nvmet_req *nvme_req);\n\nstatic struct nvmet_fabrics_ops nvmet_fc_tgt_fcp_ops;\n\nstatic void\nnvmet_fc_xmt_ls_rsp_done(struct nvmefc_tgt_ls_req *lsreq)\n{\n\tstruct nvmet_fc_ls_iod *iod = lsreq->nvmet_fc_private;\n\tstruct nvmet_fc_tgtport *tgtport = iod->tgtport;\n\n\tfc_dma_sync_single_for_cpu(tgtport->dev, iod->rspdma,\n\t\t\t\tNVME_FC_MAX_LS_BUFFER_SIZE, DMA_TO_DEVICE);\n\tnvmet_fc_free_ls_iod(tgtport, iod);\n\tnvmet_fc_tgtport_put(tgtport);\n}\n\nstatic void\nnvmet_fc_xmt_ls_rsp(struct nvmet_fc_tgtport *tgtport,\n\t\t\t\tstruct nvmet_fc_ls_iod *iod)\n{\n\tint ret;\n\n\tfc_dma_sync_single_for_device(tgtport->dev, iod->rspdma,\n\t\t\t\t  NVME_FC_MAX_LS_BUFFER_SIZE, DMA_TO_DEVICE);\n\n\tret = tgtport->ops->xmt_ls_rsp(&tgtport->fc_target_port, iod->lsreq);\n\tif (ret)\n\t\tnvmet_fc_xmt_ls_rsp_done(iod->lsreq);\n}\n\n/*\n * Actual processing routine for received FC-NVME LS Requests from the LLD\n */\nstatic void\nnvmet_fc_handle_ls_rqst(struct nvmet_fc_tgtport *tgtport,\n\t\t\tstruct nvmet_fc_ls_iod *iod)\n{\n\tstruct fcnvme_ls_rqst_w0 *w0 =\n\t\t\t(struct fcnvme_ls_rqst_w0 *)iod->rqstbuf;\n\n\tiod->lsreq->nvmet_fc_private = iod;\n\tiod->lsreq->rspbuf = iod->rspbuf;\n\tiod->lsreq->rspdma = iod->rspdma;\n\tiod->lsreq->done = nvmet_fc_xmt_ls_rsp_done;\n\t/* Be preventative. handlers will later set to valid length */\n\tiod->lsreq->rsplen = 0;\n\n\tiod->assoc = NULL;\n\n\t/*\n\t * handlers:\n\t *   parse request input, execute the request, and format the\n\t *   LS response\n\t */\n\tswitch (w0->ls_cmd) {\n\tcase FCNVME_LS_CREATE_ASSOCIATION:\n\t\t/* Creates Association and initial Admin Queue/Connection */\n\t\tnvmet_fc_ls_create_association(tgtport, iod);\n\t\tbreak;\n\tcase FCNVME_LS_CREATE_CONNECTION:\n\t\t/* Creates an IO Queue/Connection */\n\t\tnvmet_fc_ls_create_connection(tgtport, iod);\n\t\tbreak;\n\tcase FCNVME_LS_DISCONNECT:\n\t\t/* Terminate a Queue/Connection or the Association */\n\t\tnvmet_fc_ls_disconnect(tgtport, iod);\n\t\tbreak;\n\tdefault:\n\t\tiod->lsreq->rsplen = nvmet_fc_format_rjt(iod->rspbuf,\n\t\t\t\tNVME_FC_MAX_LS_BUFFER_SIZE, w0->ls_cmd,\n\t\t\t\tFCNVME_RJT_RC_INVAL, FCNVME_RJT_EXP_NONE, 0);\n\t}\n\n\tnvmet_fc_xmt_ls_rsp(tgtport, iod);\n}\n\n/*\n * Actual processing routine for received FC-NVME LS Requests from the LLD\n */\nstatic void\nnvmet_fc_handle_ls_rqst_work(struct work_struct *work)\n{\n\tstruct nvmet_fc_ls_iod *iod =\n\t\tcontainer_of(work, struct nvmet_fc_ls_iod, work);\n\tstruct nvmet_fc_tgtport *tgtport = iod->tgtport;\n\n\tnvmet_fc_handle_ls_rqst(tgtport, iod);\n}\n\n\n/**\n * nvmet_fc_rcv_ls_req - transport entry point called by an LLDD\n *                       upon the reception of a NVME LS request.\n *\n * The nvmet-fc layer will copy payload to an internal structure for\n * processing.  As such, upon completion of the routine, the LLDD may\n * immediately free/reuse the LS request buffer passed in the call.\n *\n * If this routine returns error, the LLDD should abort the exchange.\n *\n * @tgtport:    pointer to the (registered) target port the LS was\n *              received on.\n * @lsreq:      pointer to a lsreq request structure to be used to reference\n *              the exchange corresponding to the LS.\n * @lsreqbuf:   pointer to the buffer containing the LS Request\n * @lsreqbuf_len: length, in bytes, of the received LS request\n */\nint\nnvmet_fc_rcv_ls_req(struct nvmet_fc_target_port *target_port,\n\t\t\tstruct nvmefc_tgt_ls_req *lsreq,\n\t\t\tvoid *lsreqbuf, u32 lsreqbuf_len)\n{\n\tstruct nvmet_fc_tgtport *tgtport = targetport_to_tgtport(target_port);\n\tstruct nvmet_fc_ls_iod *iod;\n\n\tif (lsreqbuf_len > NVME_FC_MAX_LS_BUFFER_SIZE)\n\t\treturn -E2BIG;\n\n\tif (!nvmet_fc_tgtport_get(tgtport))\n\t\treturn -ESHUTDOWN;\n\n\tiod = nvmet_fc_alloc_ls_iod(tgtport);\n\tif (!iod) {\n\t\tnvmet_fc_tgtport_put(tgtport);\n\t\treturn -ENOENT;\n\t}\n\n\tiod->lsreq = lsreq;\n\tiod->fcpreq = NULL;\n\tmemcpy(iod->rqstbuf, lsreqbuf, lsreqbuf_len);\n\tiod->rqstdatalen = lsreqbuf_len;\n\n\tschedule_work(&iod->work);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(nvmet_fc_rcv_ls_req);\n\n\n/*\n * **********************\n * Start of FCP handling\n * **********************\n */\n\nstatic int\nnvmet_fc_alloc_tgt_pgs(struct nvmet_fc_fcp_iod *fod)\n{\n\tstruct scatterlist *sg;\n\tstruct page *page;\n\tunsigned int nent;\n\tu32 page_len, length;\n\tint i = 0;\n\n\tlength = fod->total_length;\n\tnent = DIV_ROUND_UP(length, PAGE_SIZE);\n\tsg = kmalloc_array(nent, sizeof(struct scatterlist), GFP_KERNEL);\n\tif (!sg)\n\t\tgoto out;\n\n\tsg_init_table(sg, nent);\n\n\twhile (length) {\n\t\tpage_len = min_t(u32, length, PAGE_SIZE);\n\n\t\tpage = alloc_page(GFP_KERNEL);\n\t\tif (!page)\n\t\t\tgoto out_free_pages;\n\n\t\tsg_set_page(&sg[i], page, page_len, 0);\n\t\tlength -= page_len;\n\t\ti++;\n\t}\n\n\tfod->data_sg = sg;\n\tfod->data_sg_cnt = nent;\n\tfod->data_sg_cnt = fc_dma_map_sg(fod->tgtport->dev, sg, nent,\n\t\t\t\t((fod->io_dir == NVMET_FCP_WRITE) ?\n\t\t\t\t\tDMA_FROM_DEVICE : DMA_TO_DEVICE));\n\t\t\t\t/* note: write from initiator perspective */\n\n\treturn 0;\n\nout_free_pages:\n\twhile (i > 0) {\n\t\ti--;\n\t\t__free_page(sg_page(&sg[i]));\n\t}\n\tkfree(sg);\n\tfod->data_sg = NULL;\n\tfod->data_sg_cnt = 0;\nout:\n\treturn NVME_SC_INTERNAL;\n}\n\nstatic void\nnvmet_fc_free_tgt_pgs(struct nvmet_fc_fcp_iod *fod)\n{\n\tstruct scatterlist *sg;\n\tint count;\n\n\tif (!fod->data_sg || !fod->data_sg_cnt)\n\t\treturn;\n\n\tfc_dma_unmap_sg(fod->tgtport->dev, fod->data_sg, fod->data_sg_cnt,\n\t\t\t\t((fod->io_dir == NVMET_FCP_WRITE) ?\n\t\t\t\t\tDMA_FROM_DEVICE : DMA_TO_DEVICE));\n\tfor_each_sg(fod->data_sg, sg, fod->data_sg_cnt, count)\n\t\t__free_page(sg_page(sg));\n\tkfree(fod->data_sg);\n\tfod->data_sg = NULL;\n\tfod->data_sg_cnt = 0;\n}\n\n\nstatic bool\nqueue_90percent_full(struct nvmet_fc_tgt_queue *q, u32 sqhd)\n{\n\tu32 sqtail, used;\n\n\t/* egad, this is ugly. And sqtail is just a best guess */\n\tsqtail = atomic_read(&q->sqtail) % q->sqsize;\n\n\tused = (sqtail < sqhd) ? (sqtail + q->sqsize - sqhd) : (sqtail - sqhd);\n\treturn ((used * 10) >= (((u32)(q->sqsize - 1) * 9)));\n}\n\n/*\n * Prep RSP payload.\n * May be a NVMET_FCOP_RSP or NVMET_FCOP_READDATA_RSP op\n */\nstatic void\nnvmet_fc_prep_fcp_rsp(struct nvmet_fc_tgtport *tgtport,\n\t\t\t\tstruct nvmet_fc_fcp_iod *fod)\n{\n\tstruct nvme_fc_ersp_iu *ersp = &fod->rspiubuf;\n\tstruct nvme_common_command *sqe = &fod->cmdiubuf.sqe.common;\n\tstruct nvme_completion *cqe = &ersp->cqe;\n\tu32 *cqewd = (u32 *)cqe;\n\tbool send_ersp = false;\n\tu32 rsn, rspcnt, xfr_length;\n\n\tif (fod->fcpreq->op == NVMET_FCOP_READDATA_RSP)\n\t\txfr_length = fod->total_length;\n\telse\n\t\txfr_length = fod->offset;\n\n\t/*\n\t * check to see if we can send a 0's rsp.\n\t *   Note: to send a 0's response, the NVME-FC host transport will\n\t *   recreate the CQE. The host transport knows: sq id, SQHD (last\n\t *   seen in an ersp), and command_id. Thus it will create a\n\t *   zero-filled CQE with those known fields filled in. Transport\n\t *   must send an ersp for any condition where the cqe won't match\n\t *   this.\n\t *\n\t * Here are the FC-NVME mandated cases where we must send an ersp:\n\t *  every N responses, where N=ersp_ratio\n\t *  force fabric commands to send ersp's (not in FC-NVME but good\n\t *    practice)\n\t *  normal cmds: any time status is non-zero, or status is zero\n\t *     but words 0 or 1 are non-zero.\n\t *  the SQ is 90% or more full\n\t *  the cmd is a fused command\n\t *  transferred data length not equal to cmd iu length\n\t */\n\trspcnt = atomic_inc_return(&fod->queue->zrspcnt);\n\tif (!(rspcnt % fod->queue->ersp_ratio) ||\n\t    sqe->opcode == nvme_fabrics_command ||\n\t    xfr_length != fod->total_length ||\n\t    (le16_to_cpu(cqe->status) & 0xFFFE) || cqewd[0] || cqewd[1] ||\n\t    (sqe->flags & (NVME_CMD_FUSE_FIRST | NVME_CMD_FUSE_SECOND)) ||\n\t    queue_90percent_full(fod->queue, le16_to_cpu(cqe->sq_head)))\n\t\tsend_ersp = true;\n\n\t/* re-set the fields */\n\tfod->fcpreq->rspaddr = ersp;\n\tfod->fcpreq->rspdma = fod->rspdma;\n\n\tif (!send_ersp) {\n\t\tmemset(ersp, 0, NVME_FC_SIZEOF_ZEROS_RSP);\n\t\tfod->fcpreq->rsplen = NVME_FC_SIZEOF_ZEROS_RSP;\n\t} else {\n\t\tersp->iu_len = cpu_to_be16(sizeof(*ersp)/sizeof(u32));\n\t\trsn = atomic_inc_return(&fod->queue->rsn);\n\t\tersp->rsn = cpu_to_be32(rsn);\n\t\tersp->xfrd_len = cpu_to_be32(xfr_length);\n\t\tfod->fcpreq->rsplen = sizeof(*ersp);\n\t}\n\n\tfc_dma_sync_single_for_device(tgtport->dev, fod->rspdma,\n\t\t\t\t  sizeof(fod->rspiubuf), DMA_TO_DEVICE);\n}\n\nstatic void nvmet_fc_xmt_fcp_op_done(struct nvmefc_tgt_fcp_req *fcpreq);\n\nstatic void\nnvmet_fc_abort_op(struct nvmet_fc_tgtport *tgtport,\n\t\t\t\tstruct nvmet_fc_fcp_iod *fod)\n{\n\tstruct nvmefc_tgt_fcp_req *fcpreq = fod->fcpreq;\n\n\t/* data no longer needed */\n\tnvmet_fc_free_tgt_pgs(fod);\n\n\t/*\n\t * if an ABTS was received or we issued the fcp_abort early\n\t * don't call abort routine again.\n\t */\n\t/* no need to take lock - lock was taken earlier to get here */\n\tif (!fod->aborted)\n\t\ttgtport->ops->fcp_abort(&tgtport->fc_target_port, fcpreq);\n\n\tnvmet_fc_free_fcp_iod(fod->queue, fod);\n}\n\nstatic void\nnvmet_fc_xmt_fcp_rsp(struct nvmet_fc_tgtport *tgtport,\n\t\t\t\tstruct nvmet_fc_fcp_iod *fod)\n{\n\tint ret;\n\n\tfod->fcpreq->op = NVMET_FCOP_RSP;\n\tfod->fcpreq->timeout = 0;\n\n\tnvmet_fc_prep_fcp_rsp(tgtport, fod);\n\n\tret = tgtport->ops->fcp_op(&tgtport->fc_target_port, fod->fcpreq);\n\tif (ret)\n\t\tnvmet_fc_abort_op(tgtport, fod);\n}\n\nstatic void\nnvmet_fc_transfer_fcp_data(struct nvmet_fc_tgtport *tgtport,\n\t\t\t\tstruct nvmet_fc_fcp_iod *fod, u8 op)\n{\n\tstruct nvmefc_tgt_fcp_req *fcpreq = fod->fcpreq;\n\tunsigned long flags;\n\tu32 tlen;\n\tint ret;\n\n\tfcpreq->op = op;\n\tfcpreq->offset = fod->offset;\n\tfcpreq->timeout = NVME_FC_TGTOP_TIMEOUT_SEC;\n\n\ttlen = min_t(u32, tgtport->max_sg_cnt * PAGE_SIZE,\n\t\t\t(fod->total_length - fod->offset));\n\tfcpreq->transfer_length = tlen;\n\tfcpreq->transferred_length = 0;\n\tfcpreq->fcp_error = 0;\n\tfcpreq->rsplen = 0;\n\n\tfcpreq->sg = &fod->data_sg[fod->offset / PAGE_SIZE];\n\tfcpreq->sg_cnt = DIV_ROUND_UP(tlen, PAGE_SIZE);\n\n\t/*\n\t * If the last READDATA request: check if LLDD supports\n\t * combined xfr with response.\n\t */\n\tif ((op == NVMET_FCOP_READDATA) &&\n\t    ((fod->offset + fcpreq->transfer_length) == fod->total_length) &&\n\t    (tgtport->ops->target_features & NVMET_FCTGTFEAT_READDATA_RSP)) {\n\t\tfcpreq->op = NVMET_FCOP_READDATA_RSP;\n\t\tnvmet_fc_prep_fcp_rsp(tgtport, fod);\n\t}\n\n\tret = tgtport->ops->fcp_op(&tgtport->fc_target_port, fod->fcpreq);\n\tif (ret) {\n\t\t/*\n\t\t * should be ok to set w/o lock as its in the thread of\n\t\t * execution (not an async timer routine) and doesn't\n\t\t * contend with any clearing action\n\t\t */\n\t\tfod->abort = true;\n\n\t\tif (op == NVMET_FCOP_WRITEDATA) {\n\t\t\tspin_lock_irqsave(&fod->flock, flags);\n\t\t\tfod->writedataactive = false;\n\t\t\tspin_unlock_irqrestore(&fod->flock, flags);\n\t\t\tnvmet_req_complete(&fod->req, NVME_SC_INTERNAL);\n\t\t} else /* NVMET_FCOP_READDATA or NVMET_FCOP_READDATA_RSP */ {\n\t\t\tfcpreq->fcp_error = ret;\n\t\t\tfcpreq->transferred_length = 0;\n\t\t\tnvmet_fc_xmt_fcp_op_done(fod->fcpreq);\n\t\t}\n\t}\n}\n\nstatic inline bool\n__nvmet_fc_fod_op_abort(struct nvmet_fc_fcp_iod *fod, bool abort)\n{\n\tstruct nvmefc_tgt_fcp_req *fcpreq = fod->fcpreq;\n\tstruct nvmet_fc_tgtport *tgtport = fod->tgtport;\n\n\t/* if in the middle of an io and we need to tear down */\n\tif (abort) {\n\t\tif (fcpreq->op == NVMET_FCOP_WRITEDATA) {\n\t\t\tnvmet_req_complete(&fod->req, NVME_SC_INTERNAL);\n\t\t\treturn true;\n\t\t}\n\n\t\tnvmet_fc_abort_op(tgtport, fod);\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n/*\n * actual done handler for FCP operations when completed by the lldd\n */\nstatic void\nnvmet_fc_fod_op_done(struct nvmet_fc_fcp_iod *fod)\n{\n\tstruct nvmefc_tgt_fcp_req *fcpreq = fod->fcpreq;\n\tstruct nvmet_fc_tgtport *tgtport = fod->tgtport;\n\tunsigned long flags;\n\tbool abort;\n\n\tspin_lock_irqsave(&fod->flock, flags);\n\tabort = fod->abort;\n\tfod->writedataactive = false;\n\tspin_unlock_irqrestore(&fod->flock, flags);\n\n\tswitch (fcpreq->op) {\n\n\tcase NVMET_FCOP_WRITEDATA:\n\t\tif (__nvmet_fc_fod_op_abort(fod, abort))\n\t\t\treturn;\n\t\tif (fcpreq->fcp_error ||\n\t\t    fcpreq->transferred_length != fcpreq->transfer_length) {\n\t\t\tspin_lock(&fod->flock);\n\t\t\tfod->abort = true;\n\t\t\tspin_unlock(&fod->flock);\n\n\t\t\tnvmet_req_complete(&fod->req, NVME_SC_INTERNAL);\n\t\t\treturn;\n\t\t}\n\n\t\tfod->offset += fcpreq->transferred_length;\n\t\tif (fod->offset != fod->total_length) {\n\t\t\tspin_lock_irqsave(&fod->flock, flags);\n\t\t\tfod->writedataactive = true;\n\t\t\tspin_unlock_irqrestore(&fod->flock, flags);\n\n\t\t\t/* transfer the next chunk */\n\t\t\tnvmet_fc_transfer_fcp_data(tgtport, fod,\n\t\t\t\t\t\tNVMET_FCOP_WRITEDATA);\n\t\t\treturn;\n\t\t}\n\n\t\t/* data transfer complete, resume with nvmet layer */\n\n\t\tfod->req.execute(&fod->req);\n\n\t\tbreak;\n\n\tcase NVMET_FCOP_READDATA:\n\tcase NVMET_FCOP_READDATA_RSP:\n\t\tif (__nvmet_fc_fod_op_abort(fod, abort))\n\t\t\treturn;\n\t\tif (fcpreq->fcp_error ||\n\t\t    fcpreq->transferred_length != fcpreq->transfer_length) {\n\t\t\tnvmet_fc_abort_op(tgtport, fod);\n\t\t\treturn;\n\t\t}\n\n\t\t/* success */\n\n\t\tif (fcpreq->op == NVMET_FCOP_READDATA_RSP) {\n\t\t\t/* data no longer needed */\n\t\t\tnvmet_fc_free_tgt_pgs(fod);\n\t\t\tnvmet_fc_free_fcp_iod(fod->queue, fod);\n\t\t\treturn;\n\t\t}\n\n\t\tfod->offset += fcpreq->transferred_length;\n\t\tif (fod->offset != fod->total_length) {\n\t\t\t/* transfer the next chunk */\n\t\t\tnvmet_fc_transfer_fcp_data(tgtport, fod,\n\t\t\t\t\t\tNVMET_FCOP_READDATA);\n\t\t\treturn;\n\t\t}\n\n\t\t/* data transfer complete, send response */\n\n\t\t/* data no longer needed */\n\t\tnvmet_fc_free_tgt_pgs(fod);\n\n\t\tnvmet_fc_xmt_fcp_rsp(tgtport, fod);\n\n\t\tbreak;\n\n\tcase NVMET_FCOP_RSP:\n\t\tif (__nvmet_fc_fod_op_abort(fod, abort))\n\t\t\treturn;\n\t\tnvmet_fc_free_fcp_iod(fod->queue, fod);\n\t\tbreak;\n\n\tdefault:\n\t\tbreak;\n\t}\n}\n\nstatic void\nnvmet_fc_fcp_rqst_op_done_work(struct work_struct *work)\n{\n\tstruct nvmet_fc_fcp_iod *fod =\n\t\tcontainer_of(work, struct nvmet_fc_fcp_iod, done_work);\n\n\tnvmet_fc_fod_op_done(fod);\n}\n\nstatic void\nnvmet_fc_xmt_fcp_op_done(struct nvmefc_tgt_fcp_req *fcpreq)\n{\n\tstruct nvmet_fc_fcp_iod *fod = fcpreq->nvmet_fc_private;\n\tstruct nvmet_fc_tgt_queue *queue = fod->queue;\n\n\tif (fod->tgtport->ops->target_features & NVMET_FCTGTFEAT_OPDONE_IN_ISR)\n\t\t/* context switch so completion is not in ISR context */\n\t\tqueue_work_on(queue->cpu, queue->work_q, &fod->done_work);\n\telse\n\t\tnvmet_fc_fod_op_done(fod);\n}\n\n/*\n * actual completion handler after execution by the nvmet layer\n */\nstatic void\n__nvmet_fc_fcp_nvme_cmd_done(struct nvmet_fc_tgtport *tgtport,\n\t\t\tstruct nvmet_fc_fcp_iod *fod, int status)\n{\n\tstruct nvme_common_command *sqe = &fod->cmdiubuf.sqe.common;\n\tstruct nvme_completion *cqe = &fod->rspiubuf.cqe;\n\tunsigned long flags;\n\tbool abort;\n\n\tspin_lock_irqsave(&fod->flock, flags);\n\tabort = fod->abort;\n\tspin_unlock_irqrestore(&fod->flock, flags);\n\n\t/* if we have a CQE, snoop the last sq_head value */\n\tif (!status)\n\t\tfod->queue->sqhd = cqe->sq_head;\n\n\tif (abort) {\n\t\tnvmet_fc_abort_op(tgtport, fod);\n\t\treturn;\n\t}\n\n\t/* if an error handling the cmd post initial parsing */\n\tif (status) {\n\t\t/* fudge up a failed CQE status for our transport error */\n\t\tmemset(cqe, 0, sizeof(*cqe));\n\t\tcqe->sq_head = fod->queue->sqhd;\t/* echo last cqe sqhd */\n\t\tcqe->sq_id = cpu_to_le16(fod->queue->qid);\n\t\tcqe->command_id = sqe->command_id;\n\t\tcqe->status = cpu_to_le16(status);\n\t} else {\n\n\t\t/*\n\t\t * try to push the data even if the SQE status is non-zero.\n\t\t * There may be a status where data still was intended to\n\t\t * be moved\n\t\t */\n\t\tif ((fod->io_dir == NVMET_FCP_READ) && (fod->data_sg_cnt)) {\n\t\t\t/* push the data over before sending rsp */\n\t\t\tnvmet_fc_transfer_fcp_data(tgtport, fod,\n\t\t\t\t\t\tNVMET_FCOP_READDATA);\n\t\t\treturn;\n\t\t}\n\n\t\t/* writes & no data - fall thru */\n\t}\n\n\t/* data no longer needed */\n\tnvmet_fc_free_tgt_pgs(fod);\n\n\tnvmet_fc_xmt_fcp_rsp(tgtport, fod);\n}\n\n\nstatic void\nnvmet_fc_fcp_nvme_cmd_done(struct nvmet_req *nvme_req)\n{\n\tstruct nvmet_fc_fcp_iod *fod = nvmet_req_to_fod(nvme_req);\n\tstruct nvmet_fc_tgtport *tgtport = fod->tgtport;\n\n\t__nvmet_fc_fcp_nvme_cmd_done(tgtport, fod, 0);\n}\n\n\n/*\n * Actual processing routine for received FC-NVME LS Requests from the LLD\n */\nstatic void\nnvmet_fc_handle_fcp_rqst(struct nvmet_fc_tgtport *tgtport,\n\t\t\tstruct nvmet_fc_fcp_iod *fod)\n{\n\tstruct nvme_fc_cmd_iu *cmdiu = &fod->cmdiubuf;\n\tint ret;\n\n\t/*\n\t * Fused commands are currently not supported in the linux\n\t * implementation.\n\t *\n\t * As such, the implementation of the FC transport does not\n\t * look at the fused commands and order delivery to the upper\n\t * layer until we have both based on csn.\n\t */\n\n\tfod->fcpreq->done = nvmet_fc_xmt_fcp_op_done;\n\n\tfod->total_length = be32_to_cpu(cmdiu->data_len);\n\tif (cmdiu->flags & FCNVME_CMD_FLAGS_WRITE) {\n\t\tfod->io_dir = NVMET_FCP_WRITE;\n\t\tif (!nvme_is_write(&cmdiu->sqe))\n\t\t\tgoto transport_error;\n\t} else if (cmdiu->flags & FCNVME_CMD_FLAGS_READ) {\n\t\tfod->io_dir = NVMET_FCP_READ;\n\t\tif (nvme_is_write(&cmdiu->sqe))\n\t\t\tgoto transport_error;\n\t} else {\n\t\tfod->io_dir = NVMET_FCP_NODATA;\n\t\tif (fod->total_length)\n\t\t\tgoto transport_error;\n\t}\n\n\tfod->req.cmd = &fod->cmdiubuf.sqe;\n\tfod->req.rsp = &fod->rspiubuf.cqe;\n\tfod->req.port = fod->queue->port;\n\n\t/* ensure nvmet handlers will set cmd handler callback */\n\tfod->req.execute = NULL;\n\n\t/* clear any response payload */\n\tmemset(&fod->rspiubuf, 0, sizeof(fod->rspiubuf));\n\n\tfod->data_sg = NULL;\n\tfod->data_sg_cnt = 0;\n\n\tret = nvmet_req_init(&fod->req,\n\t\t\t\t&fod->queue->nvme_cq,\n\t\t\t\t&fod->queue->nvme_sq,\n\t\t\t\t&nvmet_fc_tgt_fcp_ops);\n\tif (!ret) {\n\t\t/* bad SQE content or invalid ctrl state */\n\t\t/* nvmet layer has already called op done to send rsp. */\n\t\treturn;\n\t}\n\n\t/* keep a running counter of tail position */\n\tatomic_inc(&fod->queue->sqtail);\n\n\tif (fod->total_length) {\n\t\tret = nvmet_fc_alloc_tgt_pgs(fod);\n\t\tif (ret) {\n\t\t\tnvmet_req_complete(&fod->req, ret);\n\t\t\treturn;\n\t\t}\n\t}\n\tfod->req.sg = fod->data_sg;\n\tfod->req.sg_cnt = fod->data_sg_cnt;\n\tfod->offset = 0;\n\n\tif (fod->io_dir == NVMET_FCP_WRITE) {\n\t\t/* pull the data over before invoking nvmet layer */\n\t\tnvmet_fc_transfer_fcp_data(tgtport, fod, NVMET_FCOP_WRITEDATA);\n\t\treturn;\n\t}\n\n\t/*\n\t * Reads or no data:\n\t *\n\t * can invoke the nvmet_layer now. If read data, cmd completion will\n\t * push the data\n\t */\n\n\tfod->req.execute(&fod->req);\n\n\treturn;\n\ntransport_error:\n\tnvmet_fc_abort_op(tgtport, fod);\n}\n\n/*\n * Actual processing routine for received FC-NVME LS Requests from the LLD\n */\nstatic void\nnvmet_fc_handle_fcp_rqst_work(struct work_struct *work)\n{\n\tstruct nvmet_fc_fcp_iod *fod =\n\t\tcontainer_of(work, struct nvmet_fc_fcp_iod, work);\n\tstruct nvmet_fc_tgtport *tgtport = fod->tgtport;\n\n\tnvmet_fc_handle_fcp_rqst(tgtport, fod);\n}\n\n/**\n * nvmet_fc_rcv_fcp_req - transport entry point called by an LLDD\n *                       upon the reception of a NVME FCP CMD IU.\n *\n * Pass a FC-NVME FCP CMD IU received from the FC link to the nvmet-fc\n * layer for processing.\n *\n * The nvmet_fc layer allocates a local job structure (struct\n * nvmet_fc_fcp_iod) from the queue for the io and copies the\n * CMD IU buffer to the job structure. As such, on a successful\n * completion (returns 0), the LLDD may immediately free/reuse\n * the CMD IU buffer passed in the call.\n *\n * However, in some circumstances, due to the packetized nature of FC\n * and the api of the FC LLDD which may issue a hw command to send the\n * response, but the LLDD may not get the hw completion for that command\n * and upcall the nvmet_fc layer before a new command may be\n * asynchronously received - its possible for a command to be received\n * before the LLDD and nvmet_fc have recycled the job structure. It gives\n * the appearance of more commands received than fits in the sq.\n * To alleviate this scenario, a temporary queue is maintained in the\n * transport for pending LLDD requests waiting for a queue job structure.\n * In these \"overrun\" cases, a temporary queue element is allocated\n * the LLDD request and CMD iu buffer information remembered, and the\n * routine returns a -EOVERFLOW status. Subsequently, when a queue job\n * structure is freed, it is immediately reallocated for anything on the\n * pending request list. The LLDDs defer_rcv() callback is called,\n * informing the LLDD that it may reuse the CMD IU buffer, and the io\n * is then started normally with the transport.\n *\n * The LLDD, when receiving an -EOVERFLOW completion status, is to treat\n * the completion as successful but must not reuse the CMD IU buffer\n * until the LLDD's defer_rcv() callback has been called for the\n * corresponding struct nvmefc_tgt_fcp_req pointer.\n *\n * If there is any other condition in which an error occurs, the\n * transport will return a non-zero status indicating the error.\n * In all cases other than -EOVERFLOW, the transport has not accepted the\n * request and the LLDD should abort the exchange.\n *\n * @target_port: pointer to the (registered) target port the FCP CMD IU\n *              was received on.\n * @fcpreq:     pointer to a fcpreq request structure to be used to reference\n *              the exchange corresponding to the FCP Exchange.\n * @cmdiubuf:   pointer to the buffer containing the FCP CMD IU\n * @cmdiubuf_len: length, in bytes, of the received FCP CMD IU\n */\nint\nnvmet_fc_rcv_fcp_req(struct nvmet_fc_target_port *target_port,\n\t\t\tstruct nvmefc_tgt_fcp_req *fcpreq,\n\t\t\tvoid *cmdiubuf, u32 cmdiubuf_len)\n{\n\tstruct nvmet_fc_tgtport *tgtport = targetport_to_tgtport(target_port);\n\tstruct nvme_fc_cmd_iu *cmdiu = cmdiubuf;\n\tstruct nvmet_fc_tgt_queue *queue;\n\tstruct nvmet_fc_fcp_iod *fod;\n\tstruct nvmet_fc_defer_fcp_req *deferfcp;\n\tunsigned long flags;\n\n\t/* validate iu, so the connection id can be used to find the queue */\n\tif ((cmdiubuf_len != sizeof(*cmdiu)) ||\n\t\t\t(cmdiu->scsi_id != NVME_CMD_SCSI_ID) ||\n\t\t\t(cmdiu->fc_id != NVME_CMD_FC_ID) ||\n\t\t\t(be16_to_cpu(cmdiu->iu_len) != (sizeof(*cmdiu)/4)))\n\t\treturn -EIO;\n\n\tqueue = nvmet_fc_find_target_queue(tgtport,\n\t\t\t\tbe64_to_cpu(cmdiu->connection_id));\n\tif (!queue)\n\t\treturn -ENOTCONN;\n\n\t/*\n\t * note: reference taken by find_target_queue\n\t * After successful fod allocation, the fod will inherit the\n\t * ownership of that reference and will remove the reference\n\t * when the fod is freed.\n\t */\n\n\tspin_lock_irqsave(&queue->qlock, flags);\n\n\tfod = nvmet_fc_alloc_fcp_iod(queue);\n\tif (fod) {\n\t\tspin_unlock_irqrestore(&queue->qlock, flags);\n\n\t\tfcpreq->nvmet_fc_private = fod;\n\t\tfod->fcpreq = fcpreq;\n\n\t\tmemcpy(&fod->cmdiubuf, cmdiubuf, cmdiubuf_len);\n\n\t\tnvmet_fc_queue_fcp_req(tgtport, queue, fcpreq);\n\n\t\treturn 0;\n\t}\n\n\tif (!tgtport->ops->defer_rcv) {\n\t\tspin_unlock_irqrestore(&queue->qlock, flags);\n\t\t/* release the queue lookup reference */\n\t\tnvmet_fc_tgt_q_put(queue);\n\t\treturn -ENOENT;\n\t}\n\n\tdeferfcp = list_first_entry_or_null(&queue->avail_defer_list,\n\t\t\tstruct nvmet_fc_defer_fcp_req, req_list);\n\tif (deferfcp) {\n\t\t/* Just re-use one that was previously allocated */\n\t\tlist_del(&deferfcp->req_list);\n\t} else {\n\t\tspin_unlock_irqrestore(&queue->qlock, flags);\n\n\t\t/* Now we need to dynamically allocate one */\n\t\tdeferfcp = kmalloc(sizeof(*deferfcp), GFP_KERNEL);\n\t\tif (!deferfcp) {\n\t\t\t/* release the queue lookup reference */\n\t\t\tnvmet_fc_tgt_q_put(queue);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tspin_lock_irqsave(&queue->qlock, flags);\n\t}\n\n\t/* For now, use rspaddr / rsplen to save payload information */\n\tfcpreq->rspaddr = cmdiubuf;\n\tfcpreq->rsplen  = cmdiubuf_len;\n\tdeferfcp->fcp_req = fcpreq;\n\n\t/* defer processing till a fod becomes available */\n\tlist_add_tail(&deferfcp->req_list, &queue->pending_cmd_list);\n\n\t/* NOTE: the queue lookup reference is still valid */\n\n\tspin_unlock_irqrestore(&queue->qlock, flags);\n\n\treturn -EOVERFLOW;\n}\nEXPORT_SYMBOL_GPL(nvmet_fc_rcv_fcp_req);\n\n/**\n * nvmet_fc_rcv_fcp_abort - transport entry point called by an LLDD\n *                       upon the reception of an ABTS for a FCP command\n *\n * Notify the transport that an ABTS has been received for a FCP command\n * that had been given to the transport via nvmet_fc_rcv_fcp_req(). The\n * LLDD believes the command is still being worked on\n * (template_ops->fcp_req_release() has not been called).\n *\n * The transport will wait for any outstanding work (an op to the LLDD,\n * which the lldd should complete with error due to the ABTS; or the\n * completion from the nvmet layer of the nvme command), then will\n * stop processing and call the nvmet_fc_rcv_fcp_req() callback to\n * return the i/o context to the LLDD.  The LLDD may send the BA_ACC\n * to the ABTS either after return from this function (assuming any\n * outstanding op work has been terminated) or upon the callback being\n * called.\n *\n * @target_port: pointer to the (registered) target port the FCP CMD IU\n *              was received on.\n * @fcpreq:     pointer to the fcpreq request structure that corresponds\n *              to the exchange that received the ABTS.\n */\nvoid\nnvmet_fc_rcv_fcp_abort(struct nvmet_fc_target_port *target_port,\n\t\t\tstruct nvmefc_tgt_fcp_req *fcpreq)\n{\n\tstruct nvmet_fc_fcp_iod *fod = fcpreq->nvmet_fc_private;\n\tstruct nvmet_fc_tgt_queue *queue;\n\tunsigned long flags;\n\n\tif (!fod || fod->fcpreq != fcpreq)\n\t\t/* job appears to have already completed, ignore abort */\n\t\treturn;\n\n\tqueue = fod->queue;\n\n\tspin_lock_irqsave(&queue->qlock, flags);\n\tif (fod->active) {\n\t\t/*\n\t\t * mark as abort. The abort handler, invoked upon completion\n\t\t * of any work, will detect the aborted status and do the\n\t\t * callback.\n\t\t */\n\t\tspin_lock(&fod->flock);\n\t\tfod->abort = true;\n\t\tfod->aborted = true;\n\t\tspin_unlock(&fod->flock);\n\t}\n\tspin_unlock_irqrestore(&queue->qlock, flags);\n}\nEXPORT_SYMBOL_GPL(nvmet_fc_rcv_fcp_abort);\n\n\nstruct nvmet_fc_traddr {\n\tu64\tnn;\n\tu64\tpn;\n};\n\nstatic int\n__nvme_fc_parse_u64(substring_t *sstr, u64 *val)\n{\n\tu64 token64;\n\n\tif (match_u64(sstr, &token64))\n\t\treturn -EINVAL;\n\t*val = token64;\n\n\treturn 0;\n}\n\n/*\n * This routine validates and extracts the WWN's from the TRADDR string.\n * As kernel parsers need the 0x to determine number base, universally\n * build string to parse with 0x prefix before parsing name strings.\n */\nstatic int\nnvme_fc_parse_traddr(struct nvmet_fc_traddr *traddr, char *buf, size_t blen)\n{\n\tchar name[2 + NVME_FC_TRADDR_HEXNAMELEN + 1];\n\tsubstring_t wwn = { name, &name[sizeof(name)-1] };\n\tint nnoffset, pnoffset;\n\n\t/* validate it string one of the 2 allowed formats */\n\tif (strnlen(buf, blen) == NVME_FC_TRADDR_MAXLENGTH &&\n\t\t\t!strncmp(buf, \"nn-0x\", NVME_FC_TRADDR_OXNNLEN) &&\n\t\t\t!strncmp(&buf[NVME_FC_TRADDR_MAX_PN_OFFSET],\n\t\t\t\t\"pn-0x\", NVME_FC_TRADDR_OXNNLEN)) {\n\t\tnnoffset = NVME_FC_TRADDR_OXNNLEN;\n\t\tpnoffset = NVME_FC_TRADDR_MAX_PN_OFFSET +\n\t\t\t\t\t\tNVME_FC_TRADDR_OXNNLEN;\n\t} else if ((strnlen(buf, blen) == NVME_FC_TRADDR_MINLENGTH &&\n\t\t\t!strncmp(buf, \"nn-\", NVME_FC_TRADDR_NNLEN) &&\n\t\t\t!strncmp(&buf[NVME_FC_TRADDR_MIN_PN_OFFSET],\n\t\t\t\t\"pn-\", NVME_FC_TRADDR_NNLEN))) {\n\t\tnnoffset = NVME_FC_TRADDR_NNLEN;\n\t\tpnoffset = NVME_FC_TRADDR_MIN_PN_OFFSET + NVME_FC_TRADDR_NNLEN;\n\t} else\n\t\tgoto out_einval;\n\n\tname[0] = '0';\n\tname[1] = 'x';\n\tname[2 + NVME_FC_TRADDR_HEXNAMELEN] = 0;\n\n\tmemcpy(&name[2], &buf[nnoffset], NVME_FC_TRADDR_HEXNAMELEN);\n\tif (__nvme_fc_parse_u64(&wwn, &traddr->nn))\n\t\tgoto out_einval;\n\n\tmemcpy(&name[2], &buf[pnoffset], NVME_FC_TRADDR_HEXNAMELEN);\n\tif (__nvme_fc_parse_u64(&wwn, &traddr->pn))\n\t\tgoto out_einval;\n\n\treturn 0;\n\nout_einval:\n\tpr_warn(\"%s: bad traddr string\\n\", __func__);\n\treturn -EINVAL;\n}\n\nstatic int\nnvmet_fc_add_port(struct nvmet_port *port)\n{\n\tstruct nvmet_fc_tgtport *tgtport;\n\tstruct nvmet_fc_traddr traddr = { 0L, 0L };\n\tunsigned long flags;\n\tint ret;\n\n\t/* validate the address info */\n\tif ((port->disc_addr.trtype != NVMF_TRTYPE_FC) ||\n\t    (port->disc_addr.adrfam != NVMF_ADDR_FAMILY_FC))\n\t\treturn -EINVAL;\n\n\t/* map the traddr address info to a target port */\n\n\tret = nvme_fc_parse_traddr(&traddr, port->disc_addr.traddr,\n\t\t\tsizeof(port->disc_addr.traddr));\n\tif (ret)\n\t\treturn ret;\n\n\tret = -ENXIO;\n\tspin_lock_irqsave(&nvmet_fc_tgtlock, flags);\n\tlist_for_each_entry(tgtport, &nvmet_fc_target_list, tgt_list) {\n\t\tif ((tgtport->fc_target_port.node_name == traddr.nn) &&\n\t\t    (tgtport->fc_target_port.port_name == traddr.pn)) {\n\t\t\t/* a FC port can only be 1 nvmet port id */\n\t\t\tif (!tgtport->port) {\n\t\t\t\ttgtport->port = port;\n\t\t\t\tport->priv = tgtport;\n\t\t\t\tnvmet_fc_tgtport_get(tgtport);\n\t\t\t\tret = 0;\n\t\t\t} else\n\t\t\t\tret = -EALREADY;\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock_irqrestore(&nvmet_fc_tgtlock, flags);\n\treturn ret;\n}\n\nstatic void\nnvmet_fc_remove_port(struct nvmet_port *port)\n{\n\tstruct nvmet_fc_tgtport *tgtport = port->priv;\n\tunsigned long flags;\n\tbool matched = false;\n\n\tspin_lock_irqsave(&nvmet_fc_tgtlock, flags);\n\tif (tgtport->port == port) {\n\t\tmatched = true;\n\t\ttgtport->port = NULL;\n\t}\n\tspin_unlock_irqrestore(&nvmet_fc_tgtlock, flags);\n\n\tif (matched)\n\t\tnvmet_fc_tgtport_put(tgtport);\n}\n\nstatic struct nvmet_fabrics_ops nvmet_fc_tgt_fcp_ops = {\n\t.owner\t\t\t= THIS_MODULE,\n\t.type\t\t\t= NVMF_TRTYPE_FC,\n\t.msdbd\t\t\t= 1,\n\t.add_port\t\t= nvmet_fc_add_port,\n\t.remove_port\t\t= nvmet_fc_remove_port,\n\t.queue_response\t\t= nvmet_fc_fcp_nvme_cmd_done,\n\t.delete_ctrl\t\t= nvmet_fc_delete_ctrl,\n};\n\nstatic int __init nvmet_fc_init_module(void)\n{\n\treturn nvmet_register_transport(&nvmet_fc_tgt_fcp_ops);\n}\n\nstatic void __exit nvmet_fc_exit_module(void)\n{\n\t/* sanity check - all lports should be removed */\n\tif (!list_empty(&nvmet_fc_target_list))\n\t\tpr_warn(\"%s: targetport list not empty\\n\", __func__);\n\n\tnvmet_unregister_transport(&nvmet_fc_tgt_fcp_ops);\n\n\tida_destroy(&nvmet_fc_tgtport_cnt);\n}\n\nmodule_init(nvmet_fc_init_module);\nmodule_exit(nvmet_fc_exit_module);\n\nMODULE_LICENSE(\"GPL v2\");\n"], "buggy_code_start_loc": [784], "buggy_code_end_loc": [784], "fixing_code_start_loc": [785], "fixing_code_end_loc": [788], "type": "CWE-119", "message": "In the Linux kernel before 4.14, an out of boundary access happened in drivers/nvme/target/fc.c.", "other": {"cve": {"id": "CVE-2017-18379", "sourceIdentifier": "cve@mitre.org", "published": "2019-07-27T22:15:11.573", "lastModified": "2023-01-17T21:21:21.957", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "In the Linux kernel before 4.14, an out of boundary access happened in drivers/nvme/target/fc.c."}, {"lang": "es", "value": "En el kernel de Linux anterior a versi\u00f3n 4.14, se present\u00f3 un acceso fuera del l\u00edmite en el archivo drivers/nvme/target/fc.c."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 9.8, "baseSeverity": "CRITICAL"}, "exploitabilityScore": 3.9, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:L/Au:N/C:P/I:P/A:P", "accessVector": "NETWORK", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "PARTIAL", "availabilityImpact": "PARTIAL", "baseScore": 7.5}, "baseSeverity": "HIGH", "exploitabilityScore": 10.0, "impactScore": 6.4, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-119"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.10", "versionEndExcluding": "4.14", "matchCriteriaId": "AF85AA17-87E7-4A75-BBC4-F88A1E4CEBDE"}]}]}], "references": [{"url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=0c319d3a144d4b8f1ea2047fd614d2149b68f889", "source": "cve@mitre.org", "tags": ["Patch", "Vendor Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/0c319d3a144d4b8f1ea2047fd614d2149b68f889", "source": "cve@mitre.org", "tags": ["Patch", "Vendor Advisory"]}, {"url": "https://support.f5.com/csp/article/K74012105", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://support.f5.com/csp/article/K74012105?utm_source=f5support&amp;utm_medium=RSS", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/0c319d3a144d4b8f1ea2047fd614d2149b68f889"}}