{"buggy_code": ["// SPDX-License-Identifier: GPL-2.0\n/*\n * Copyright (C) 2019 MediaTek Inc.\n * Authors:\n *\tStanley Chu <stanley.chu@mediatek.com>\n *\tPeter Wang <peter.wang@mediatek.com>\n */\n\n#include <linux/arm-smccc.h>\n#include <linux/bitfield.h>\n#include <linux/of.h>\n#include <linux/of_address.h>\n#include <linux/of_device.h>\n#include <linux/phy/phy.h>\n#include <linux/platform_device.h>\n#include <linux/regulator/consumer.h>\n#include <linux/reset.h>\n#include <linux/sched/clock.h>\n#include <linux/soc/mediatek/mtk_sip_svc.h>\n\n#include \"ufshcd.h\"\n#include \"ufshcd-crypto.h\"\n#include \"ufshcd-pltfrm.h\"\n#include \"ufs_quirks.h\"\n#include \"unipro.h\"\n#include \"ufs-mediatek.h\"\n\n#define CREATE_TRACE_POINTS\n#include \"ufs-mediatek-trace.h\"\n\n#define ufs_mtk_smc(cmd, val, res) \\\n\tarm_smccc_smc(MTK_SIP_UFS_CONTROL, \\\n\t\t      cmd, val, 0, 0, 0, 0, 0, &(res))\n\n#define ufs_mtk_va09_pwr_ctrl(res, on) \\\n\tufs_mtk_smc(UFS_MTK_SIP_VA09_PWR_CTRL, on, res)\n\n#define ufs_mtk_crypto_ctrl(res, enable) \\\n\tufs_mtk_smc(UFS_MTK_SIP_CRYPTO_CTRL, enable, res)\n\n#define ufs_mtk_ref_clk_notify(on, res) \\\n\tufs_mtk_smc(UFS_MTK_SIP_REF_CLK_NOTIFICATION, on, res)\n\n#define ufs_mtk_device_reset_ctrl(high, res) \\\n\tufs_mtk_smc(UFS_MTK_SIP_DEVICE_RESET, high, res)\n\nstatic struct ufs_dev_fix ufs_mtk_dev_fixups[] = {\n\tUFS_FIX(UFS_VENDOR_MICRON, UFS_ANY_MODEL,\n\t\tUFS_DEVICE_QUIRK_DELAY_AFTER_LPM),\n\tUFS_FIX(UFS_VENDOR_SKHYNIX, \"H9HQ21AFAMZDAR\",\n\t\tUFS_DEVICE_QUIRK_SUPPORT_EXTENDED_FEATURES),\n\tEND_FIX\n};\n\nstatic const struct of_device_id ufs_mtk_of_match[] = {\n\t{ .compatible = \"mediatek,mt8183-ufshci\" },\n\t{},\n};\n\nstatic bool ufs_mtk_is_boost_crypt_enabled(struct ufs_hba *hba)\n{\n\tstruct ufs_mtk_host *host = ufshcd_get_variant(hba);\n\n\treturn !!(host->caps & UFS_MTK_CAP_BOOST_CRYPT_ENGINE);\n}\n\nstatic bool ufs_mtk_is_va09_supported(struct ufs_hba *hba)\n{\n\tstruct ufs_mtk_host *host = ufshcd_get_variant(hba);\n\n\treturn !!(host->caps & UFS_MTK_CAP_VA09_PWR_CTRL);\n}\n\nstatic bool ufs_mtk_is_broken_vcc(struct ufs_hba *hba)\n{\n\tstruct ufs_mtk_host *host = ufshcd_get_variant(hba);\n\n\treturn !!(host->caps & UFS_MTK_CAP_BROKEN_VCC);\n}\n\nstatic void ufs_mtk_cfg_unipro_cg(struct ufs_hba *hba, bool enable)\n{\n\tu32 tmp;\n\n\tif (enable) {\n\t\tufshcd_dme_get(hba,\n\t\t\t       UIC_ARG_MIB(VS_SAVEPOWERCONTROL), &tmp);\n\t\ttmp = tmp |\n\t\t      (1 << RX_SYMBOL_CLK_GATE_EN) |\n\t\t      (1 << SYS_CLK_GATE_EN) |\n\t\t      (1 << TX_CLK_GATE_EN);\n\t\tufshcd_dme_set(hba,\n\t\t\t       UIC_ARG_MIB(VS_SAVEPOWERCONTROL), tmp);\n\n\t\tufshcd_dme_get(hba,\n\t\t\t       UIC_ARG_MIB(VS_DEBUGCLOCKENABLE), &tmp);\n\t\ttmp = tmp & ~(1 << TX_SYMBOL_CLK_REQ_FORCE);\n\t\tufshcd_dme_set(hba,\n\t\t\t       UIC_ARG_MIB(VS_DEBUGCLOCKENABLE), tmp);\n\t} else {\n\t\tufshcd_dme_get(hba,\n\t\t\t       UIC_ARG_MIB(VS_SAVEPOWERCONTROL), &tmp);\n\t\ttmp = tmp & ~((1 << RX_SYMBOL_CLK_GATE_EN) |\n\t\t\t      (1 << SYS_CLK_GATE_EN) |\n\t\t\t      (1 << TX_CLK_GATE_EN));\n\t\tufshcd_dme_set(hba,\n\t\t\t       UIC_ARG_MIB(VS_SAVEPOWERCONTROL), tmp);\n\n\t\tufshcd_dme_get(hba,\n\t\t\t       UIC_ARG_MIB(VS_DEBUGCLOCKENABLE), &tmp);\n\t\ttmp = tmp | (1 << TX_SYMBOL_CLK_REQ_FORCE);\n\t\tufshcd_dme_set(hba,\n\t\t\t       UIC_ARG_MIB(VS_DEBUGCLOCKENABLE), tmp);\n\t}\n}\n\nstatic void ufs_mtk_crypto_enable(struct ufs_hba *hba)\n{\n\tstruct arm_smccc_res res;\n\n\tufs_mtk_crypto_ctrl(res, 1);\n\tif (res.a0) {\n\t\tdev_info(hba->dev, \"%s: crypto enable failed, err: %lu\\n\",\n\t\t\t __func__, res.a0);\n\t\thba->caps &= ~UFSHCD_CAP_CRYPTO;\n\t}\n}\n\nstatic void ufs_mtk_host_reset(struct ufs_hba *hba)\n{\n\tstruct ufs_mtk_host *host = ufshcd_get_variant(hba);\n\n\treset_control_assert(host->hci_reset);\n\treset_control_assert(host->crypto_reset);\n\treset_control_assert(host->unipro_reset);\n\n\tusleep_range(100, 110);\n\n\treset_control_deassert(host->unipro_reset);\n\treset_control_deassert(host->crypto_reset);\n\treset_control_deassert(host->hci_reset);\n}\n\nstatic void ufs_mtk_init_reset_control(struct ufs_hba *hba,\n\t\t\t\t       struct reset_control **rc,\n\t\t\t\t       char *str)\n{\n\t*rc = devm_reset_control_get(hba->dev, str);\n\tif (IS_ERR(*rc)) {\n\t\tdev_info(hba->dev, \"Failed to get reset control %s: %ld\\n\",\n\t\t\t str, PTR_ERR(*rc));\n\t\t*rc = NULL;\n\t}\n}\n\nstatic void ufs_mtk_init_reset(struct ufs_hba *hba)\n{\n\tstruct ufs_mtk_host *host = ufshcd_get_variant(hba);\n\n\tufs_mtk_init_reset_control(hba, &host->hci_reset,\n\t\t\t\t   \"hci_rst\");\n\tufs_mtk_init_reset_control(hba, &host->unipro_reset,\n\t\t\t\t   \"unipro_rst\");\n\tufs_mtk_init_reset_control(hba, &host->crypto_reset,\n\t\t\t\t   \"crypto_rst\");\n}\n\nstatic int ufs_mtk_hce_enable_notify(struct ufs_hba *hba,\n\t\t\t\t     enum ufs_notify_change_status status)\n{\n\tstruct ufs_mtk_host *host = ufshcd_get_variant(hba);\n\tunsigned long flags;\n\n\tif (status == PRE_CHANGE) {\n\t\tif (host->unipro_lpm) {\n\t\t\thba->vps->hba_enable_delay_us = 0;\n\t\t} else {\n\t\t\thba->vps->hba_enable_delay_us = 600;\n\t\t\tufs_mtk_host_reset(hba);\n\t\t}\n\n\t\tif (hba->caps & UFSHCD_CAP_CRYPTO)\n\t\t\tufs_mtk_crypto_enable(hba);\n\n\t\tif (host->caps & UFS_MTK_CAP_DISABLE_AH8) {\n\t\t\tspin_lock_irqsave(hba->host->host_lock, flags);\n\t\t\tufshcd_writel(hba, 0,\n\t\t\t\t      REG_AUTO_HIBERNATE_IDLE_TIMER);\n\t\t\tspin_unlock_irqrestore(hba->host->host_lock,\n\t\t\t\t\t       flags);\n\n\t\t\thba->capabilities &= ~MASK_AUTO_HIBERN8_SUPPORT;\n\t\t\thba->ahit = 0;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int ufs_mtk_bind_mphy(struct ufs_hba *hba)\n{\n\tstruct ufs_mtk_host *host = ufshcd_get_variant(hba);\n\tstruct device *dev = hba->dev;\n\tstruct device_node *np = dev->of_node;\n\tint err = 0;\n\n\thost->mphy = devm_of_phy_get_by_index(dev, np, 0);\n\n\tif (host->mphy == ERR_PTR(-EPROBE_DEFER)) {\n\t\t/*\n\t\t * UFS driver might be probed before the phy driver does.\n\t\t * In that case we would like to return EPROBE_DEFER code.\n\t\t */\n\t\terr = -EPROBE_DEFER;\n\t\tdev_info(dev,\n\t\t\t \"%s: required phy hasn't probed yet. err = %d\\n\",\n\t\t\t__func__, err);\n\t} else if (IS_ERR(host->mphy)) {\n\t\terr = PTR_ERR(host->mphy);\n\t\tif (err != -ENODEV) {\n\t\t\tdev_info(dev, \"%s: PHY get failed %d\\n\", __func__,\n\t\t\t\t err);\n\t\t}\n\t}\n\n\tif (err)\n\t\thost->mphy = NULL;\n\t/*\n\t * Allow unbound mphy because not every platform needs specific\n\t * mphy control.\n\t */\n\tif (err == -ENODEV)\n\t\terr = 0;\n\n\treturn err;\n}\n\nstatic int ufs_mtk_setup_ref_clk(struct ufs_hba *hba, bool on)\n{\n\tstruct ufs_mtk_host *host = ufshcd_get_variant(hba);\n\tstruct arm_smccc_res res;\n\tktime_t timeout, time_checked;\n\tu32 value;\n\n\tif (host->ref_clk_enabled == on)\n\t\treturn 0;\n\n\tif (on) {\n\t\tufs_mtk_ref_clk_notify(on, res);\n\t\tufshcd_writel(hba, REFCLK_REQUEST, REG_UFS_REFCLK_CTRL);\n\t} else {\n\t\tufshcd_delay_us(host->ref_clk_gating_wait_us, 10);\n\t\tufshcd_writel(hba, REFCLK_RELEASE, REG_UFS_REFCLK_CTRL);\n\t}\n\n\t/* Wait for ack */\n\ttimeout = ktime_add_us(ktime_get(), REFCLK_REQ_TIMEOUT_US);\n\tdo {\n\t\ttime_checked = ktime_get();\n\t\tvalue = ufshcd_readl(hba, REG_UFS_REFCLK_CTRL);\n\n\t\t/* Wait until ack bit equals to req bit */\n\t\tif (((value & REFCLK_ACK) >> 1) == (value & REFCLK_REQUEST))\n\t\t\tgoto out;\n\n\t\tusleep_range(100, 200);\n\t} while (ktime_before(time_checked, timeout));\n\n\tdev_err(hba->dev, \"missing ack of refclk req, reg: 0x%x\\n\", value);\n\n\tufs_mtk_ref_clk_notify(host->ref_clk_enabled, res);\n\n\treturn -ETIMEDOUT;\n\nout:\n\thost->ref_clk_enabled = on;\n\tif (on)\n\t\tufshcd_delay_us(host->ref_clk_ungating_wait_us, 10);\n\telse\n\t\tufs_mtk_ref_clk_notify(on, res);\n\n\treturn 0;\n}\n\nstatic void ufs_mtk_setup_ref_clk_wait_us(struct ufs_hba *hba,\n\t\t\t\t\t  u16 gating_us)\n{\n\tstruct ufs_mtk_host *host = ufshcd_get_variant(hba);\n\n\tif (hba->dev_info.clk_gating_wait_us) {\n\t\thost->ref_clk_gating_wait_us =\n\t\t\thba->dev_info.clk_gating_wait_us;\n\t} else {\n\t\thost->ref_clk_gating_wait_us = gating_us;\n\t}\n\n\thost->ref_clk_ungating_wait_us = REFCLK_DEFAULT_WAIT_US;\n}\n\nstatic void ufs_mtk_dbg_sel(struct ufs_hba *hba)\n{\n\tstruct ufs_mtk_host *host = ufshcd_get_variant(hba);\n\n\tif (((host->ip_ver >> 16) & 0xFF) >= 0x36) {\n\t\tufshcd_writel(hba, 0x820820, REG_UFS_DEBUG_SEL);\n\t\tufshcd_writel(hba, 0x0, REG_UFS_DEBUG_SEL_B0);\n\t\tufshcd_writel(hba, 0x55555555, REG_UFS_DEBUG_SEL_B1);\n\t\tufshcd_writel(hba, 0xaaaaaaaa, REG_UFS_DEBUG_SEL_B2);\n\t\tufshcd_writel(hba, 0xffffffff, REG_UFS_DEBUG_SEL_B3);\n\t} else {\n\t\tufshcd_writel(hba, 0x20, REG_UFS_DEBUG_SEL);\n\t}\n}\n\nstatic void ufs_mtk_wait_idle_state(struct ufs_hba *hba,\n\t\t\t    unsigned long retry_ms)\n{\n\tu64 timeout, time_checked;\n\tu32 val, sm;\n\tbool wait_idle;\n\n\t/* cannot use plain ktime_get() in suspend */\n\ttimeout = ktime_get_mono_fast_ns() + retry_ms * 1000000UL;\n\n\t/* wait a specific time after check base */\n\tudelay(10);\n\twait_idle = false;\n\n\tdo {\n\t\ttime_checked = ktime_get_mono_fast_ns();\n\t\tufs_mtk_dbg_sel(hba);\n\t\tval = ufshcd_readl(hba, REG_UFS_PROBE);\n\n\t\tsm = val & 0x1f;\n\n\t\t/*\n\t\t * if state is in H8 enter and H8 enter confirm\n\t\t * wait until return to idle state.\n\t\t */\n\t\tif ((sm >= VS_HIB_ENTER) && (sm <= VS_HIB_EXIT)) {\n\t\t\twait_idle = true;\n\t\t\tudelay(50);\n\t\t\tcontinue;\n\t\t} else if (!wait_idle)\n\t\t\tbreak;\n\n\t\tif (wait_idle && (sm == VS_HCE_BASE))\n\t\t\tbreak;\n\t} while (time_checked < timeout);\n\n\tif (wait_idle && sm != VS_HCE_BASE)\n\t\tdev_info(hba->dev, \"wait idle tmo: 0x%x\\n\", val);\n}\n\nstatic int ufs_mtk_wait_link_state(struct ufs_hba *hba, u32 state,\n\t\t\t\t   unsigned long max_wait_ms)\n{\n\tktime_t timeout, time_checked;\n\tu32 val;\n\n\ttimeout = ktime_add_ms(ktime_get(), max_wait_ms);\n\tdo {\n\t\ttime_checked = ktime_get();\n\t\tufs_mtk_dbg_sel(hba);\n\t\tval = ufshcd_readl(hba, REG_UFS_PROBE);\n\t\tval = val >> 28;\n\n\t\tif (val == state)\n\t\t\treturn 0;\n\n\t\t/* Sleep for max. 200us */\n\t\tusleep_range(100, 200);\n\t} while (ktime_before(time_checked, timeout));\n\n\tif (val == state)\n\t\treturn 0;\n\n\treturn -ETIMEDOUT;\n}\n\nstatic int ufs_mtk_mphy_power_on(struct ufs_hba *hba, bool on)\n{\n\tstruct ufs_mtk_host *host = ufshcd_get_variant(hba);\n\tstruct phy *mphy = host->mphy;\n\tstruct arm_smccc_res res;\n\tint ret = 0;\n\n\tif (!mphy || !(on ^ host->mphy_powered_on))\n\t\treturn 0;\n\n\tif (on) {\n\t\tif (ufs_mtk_is_va09_supported(hba)) {\n\t\t\tret = regulator_enable(host->reg_va09);\n\t\t\tif (ret < 0)\n\t\t\t\tgoto out;\n\t\t\t/* wait 200 us to stablize VA09 */\n\t\t\tusleep_range(200, 210);\n\t\t\tufs_mtk_va09_pwr_ctrl(res, 1);\n\t\t}\n\t\tphy_power_on(mphy);\n\t} else {\n\t\tphy_power_off(mphy);\n\t\tif (ufs_mtk_is_va09_supported(hba)) {\n\t\t\tufs_mtk_va09_pwr_ctrl(res, 0);\n\t\t\tret = regulator_disable(host->reg_va09);\n\t\t\tif (ret < 0)\n\t\t\t\tgoto out;\n\t\t}\n\t}\nout:\n\tif (ret) {\n\t\tdev_info(hba->dev,\n\t\t\t \"failed to %s va09: %d\\n\",\n\t\t\t on ? \"enable\" : \"disable\",\n\t\t\t ret);\n\t} else {\n\t\thost->mphy_powered_on = on;\n\t}\n\n\treturn ret;\n}\n\nstatic int ufs_mtk_get_host_clk(struct device *dev, const char *name,\n\t\t\t\tstruct clk **clk_out)\n{\n\tstruct clk *clk;\n\tint err = 0;\n\n\tclk = devm_clk_get(dev, name);\n\tif (IS_ERR(clk))\n\t\terr = PTR_ERR(clk);\n\telse\n\t\t*clk_out = clk;\n\n\treturn err;\n}\n\nstatic void ufs_mtk_boost_crypt(struct ufs_hba *hba, bool boost)\n{\n\tstruct ufs_mtk_host *host = ufshcd_get_variant(hba);\n\tstruct ufs_mtk_crypt_cfg *cfg;\n\tstruct regulator *reg;\n\tint volt, ret;\n\n\tif (!ufs_mtk_is_boost_crypt_enabled(hba))\n\t\treturn;\n\n\tcfg = host->crypt;\n\tvolt = cfg->vcore_volt;\n\treg = cfg->reg_vcore;\n\n\tret = clk_prepare_enable(cfg->clk_crypt_mux);\n\tif (ret) {\n\t\tdev_info(hba->dev, \"clk_prepare_enable(): %d\\n\",\n\t\t\t ret);\n\t\treturn;\n\t}\n\n\tif (boost) {\n\t\tret = regulator_set_voltage(reg, volt, INT_MAX);\n\t\tif (ret) {\n\t\t\tdev_info(hba->dev,\n\t\t\t\t \"failed to set vcore to %d\\n\", volt);\n\t\t\tgoto out;\n\t\t}\n\n\t\tret = clk_set_parent(cfg->clk_crypt_mux,\n\t\t\t\t     cfg->clk_crypt_perf);\n\t\tif (ret) {\n\t\t\tdev_info(hba->dev,\n\t\t\t\t \"failed to set clk_crypt_perf\\n\");\n\t\t\tregulator_set_voltage(reg, 0, INT_MAX);\n\t\t\tgoto out;\n\t\t}\n\t} else {\n\t\tret = clk_set_parent(cfg->clk_crypt_mux,\n\t\t\t\t     cfg->clk_crypt_lp);\n\t\tif (ret) {\n\t\t\tdev_info(hba->dev,\n\t\t\t\t \"failed to set clk_crypt_lp\\n\");\n\t\t\tgoto out;\n\t\t}\n\n\t\tret = regulator_set_voltage(reg, 0, INT_MAX);\n\t\tif (ret) {\n\t\t\tdev_info(hba->dev,\n\t\t\t\t \"failed to set vcore to MIN\\n\");\n\t\t}\n\t}\nout:\n\tclk_disable_unprepare(cfg->clk_crypt_mux);\n}\n\nstatic int ufs_mtk_init_host_clk(struct ufs_hba *hba, const char *name,\n\t\t\t\t struct clk **clk)\n{\n\tint ret;\n\n\tret = ufs_mtk_get_host_clk(hba->dev, name, clk);\n\tif (ret) {\n\t\tdev_info(hba->dev, \"%s: failed to get %s: %d\", __func__,\n\t\t\t name, ret);\n\t}\n\n\treturn ret;\n}\n\nstatic void ufs_mtk_init_boost_crypt(struct ufs_hba *hba)\n{\n\tstruct ufs_mtk_host *host = ufshcd_get_variant(hba);\n\tstruct ufs_mtk_crypt_cfg *cfg;\n\tstruct device *dev = hba->dev;\n\tstruct regulator *reg;\n\tu32 volt;\n\n\thost->crypt = devm_kzalloc(dev, sizeof(*(host->crypt)),\n\t\t\t\t   GFP_KERNEL);\n\tif (!host->crypt)\n\t\tgoto disable_caps;\n\n\treg = devm_regulator_get_optional(dev, \"dvfsrc-vcore\");\n\tif (IS_ERR(reg)) {\n\t\tdev_info(dev, \"failed to get dvfsrc-vcore: %ld\",\n\t\t\t PTR_ERR(reg));\n\t\tgoto disable_caps;\n\t}\n\n\tif (of_property_read_u32(dev->of_node, \"boost-crypt-vcore-min\",\n\t\t\t\t &volt)) {\n\t\tdev_info(dev, \"failed to get boost-crypt-vcore-min\");\n\t\tgoto disable_caps;\n\t}\n\n\tcfg = host->crypt;\n\tif (ufs_mtk_init_host_clk(hba, \"crypt_mux\",\n\t\t\t\t  &cfg->clk_crypt_mux))\n\t\tgoto disable_caps;\n\n\tif (ufs_mtk_init_host_clk(hba, \"crypt_lp\",\n\t\t\t\t  &cfg->clk_crypt_lp))\n\t\tgoto disable_caps;\n\n\tif (ufs_mtk_init_host_clk(hba, \"crypt_perf\",\n\t\t\t\t  &cfg->clk_crypt_perf))\n\t\tgoto disable_caps;\n\n\tcfg->reg_vcore = reg;\n\tcfg->vcore_volt = volt;\n\thost->caps |= UFS_MTK_CAP_BOOST_CRYPT_ENGINE;\n\ndisable_caps:\n\treturn;\n}\n\nstatic void ufs_mtk_init_va09_pwr_ctrl(struct ufs_hba *hba)\n{\n\tstruct ufs_mtk_host *host = ufshcd_get_variant(hba);\n\n\thost->reg_va09 = regulator_get(hba->dev, \"va09\");\n\tif (!host->reg_va09)\n\t\tdev_info(hba->dev, \"failed to get va09\");\n\telse\n\t\thost->caps |= UFS_MTK_CAP_VA09_PWR_CTRL;\n}\n\nstatic void ufs_mtk_init_host_caps(struct ufs_hba *hba)\n{\n\tstruct ufs_mtk_host *host = ufshcd_get_variant(hba);\n\tstruct device_node *np = hba->dev->of_node;\n\n\tif (of_property_read_bool(np, \"mediatek,ufs-boost-crypt\"))\n\t\tufs_mtk_init_boost_crypt(hba);\n\n\tif (of_property_read_bool(np, \"mediatek,ufs-support-va09\"))\n\t\tufs_mtk_init_va09_pwr_ctrl(hba);\n\n\tif (of_property_read_bool(np, \"mediatek,ufs-disable-ah8\"))\n\t\thost->caps |= UFS_MTK_CAP_DISABLE_AH8;\n\n\tif (of_property_read_bool(np, \"mediatek,ufs-broken-vcc\"))\n\t\thost->caps |= UFS_MTK_CAP_BROKEN_VCC;\n\n\tdev_info(hba->dev, \"caps: 0x%x\", host->caps);\n}\n\nstatic void ufs_mtk_scale_perf(struct ufs_hba *hba, bool up)\n{\n\tstruct ufs_mtk_host *host = ufshcd_get_variant(hba);\n\n\tufs_mtk_boost_crypt(hba, up);\n\tufs_mtk_setup_ref_clk(hba, up);\n\n\tif (up)\n\t\tphy_power_on(host->mphy);\n\telse\n\t\tphy_power_off(host->mphy);\n}\n\n/**\n * ufs_mtk_setup_clocks - enables/disable clocks\n * @hba: host controller instance\n * @on: If true, enable clocks else disable them.\n * @status: PRE_CHANGE or POST_CHANGE notify\n *\n * Returns 0 on success, non-zero on failure.\n */\nstatic int ufs_mtk_setup_clocks(struct ufs_hba *hba, bool on,\n\t\t\t\tenum ufs_notify_change_status status)\n{\n\tstruct ufs_mtk_host *host = ufshcd_get_variant(hba);\n\tbool clk_pwr_off = false;\n\tint ret = 0;\n\n\t/*\n\t * In case ufs_mtk_init() is not yet done, simply ignore.\n\t * This ufs_mtk_setup_clocks() shall be called from\n\t * ufs_mtk_init() after init is done.\n\t */\n\tif (!host)\n\t\treturn 0;\n\n\tif (!on && status == PRE_CHANGE) {\n\t\tif (ufshcd_is_link_off(hba)) {\n\t\t\tclk_pwr_off = true;\n\t\t} else if (ufshcd_is_link_hibern8(hba) ||\n\t\t\t (!ufshcd_can_hibern8_during_gating(hba) &&\n\t\t\t ufshcd_is_auto_hibern8_enabled(hba))) {\n\t\t\t/*\n\t\t\t * Gate ref-clk and poweroff mphy if link state is in\n\t\t\t * OFF or Hibern8 by either Auto-Hibern8 or\n\t\t\t * ufshcd_link_state_transition().\n\t\t\t */\n\t\t\tret = ufs_mtk_wait_link_state(hba,\n\t\t\t\t\t\t      VS_LINK_HIBERN8,\n\t\t\t\t\t\t      15);\n\t\t\tif (!ret)\n\t\t\t\tclk_pwr_off = true;\n\t\t}\n\n\t\tif (clk_pwr_off)\n\t\t\tufs_mtk_scale_perf(hba, false);\n\t} else if (on && status == POST_CHANGE) {\n\t\tufs_mtk_scale_perf(hba, true);\n\t}\n\n\treturn ret;\n}\n\nstatic void ufs_mtk_get_controller_version(struct ufs_hba *hba)\n{\n\tstruct ufs_mtk_host *host = ufshcd_get_variant(hba);\n\tint ret, ver = 0;\n\n\tif (host->hw_ver.major)\n\t\treturn;\n\n\t/* Set default (minimum) version anyway */\n\thost->hw_ver.major = 2;\n\n\tret = ufshcd_dme_get(hba, UIC_ARG_MIB(PA_LOCALVERINFO), &ver);\n\tif (!ret) {\n\t\tif (ver >= UFS_UNIPRO_VER_1_8) {\n\t\t\thost->hw_ver.major = 3;\n\t\t\t/*\n\t\t\t * Fix HCI version for some platforms with\n\t\t\t * incorrect version\n\t\t\t */\n\t\t\tif (hba->ufs_version < ufshci_version(3, 0))\n\t\t\t\thba->ufs_version = ufshci_version(3, 0);\n\t\t}\n\t}\n}\n\nstatic u32 ufs_mtk_get_ufs_hci_version(struct ufs_hba *hba)\n{\n\treturn hba->ufs_version;\n}\n\n/**\n * ufs_mtk_init - find other essential mmio bases\n * @hba: host controller instance\n *\n * Binds PHY with controller and powers up PHY enabling clocks\n * and regulators.\n *\n * Returns -EPROBE_DEFER if binding fails, returns negative error\n * on phy power up failure and returns zero on success.\n */\nstatic int ufs_mtk_init(struct ufs_hba *hba)\n{\n\tconst struct of_device_id *id;\n\tstruct device *dev = hba->dev;\n\tstruct ufs_mtk_host *host;\n\tint err = 0;\n\n\thost = devm_kzalloc(dev, sizeof(*host), GFP_KERNEL);\n\tif (!host) {\n\t\terr = -ENOMEM;\n\t\tdev_info(dev, \"%s: no memory for mtk ufs host\\n\", __func__);\n\t\tgoto out;\n\t}\n\n\thost->hba = hba;\n\tufshcd_set_variant(hba, host);\n\n\tid = of_match_device(ufs_mtk_of_match, dev);\n\tif (!id) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t/* Initialize host capability */\n\tufs_mtk_init_host_caps(hba);\n\n\terr = ufs_mtk_bind_mphy(hba);\n\tif (err)\n\t\tgoto out_variant_clear;\n\n\tufs_mtk_init_reset(hba);\n\n\t/* Enable runtime autosuspend */\n\thba->caps |= UFSHCD_CAP_RPM_AUTOSUSPEND;\n\n\t/* Enable clock-gating */\n\thba->caps |= UFSHCD_CAP_CLK_GATING;\n\n\t/* Enable inline encryption */\n\thba->caps |= UFSHCD_CAP_CRYPTO;\n\n\t/* Enable WriteBooster */\n\thba->caps |= UFSHCD_CAP_WB_EN;\n\thba->quirks |= UFSHCI_QUIRK_SKIP_MANUAL_WB_FLUSH_CTRL;\n\thba->vps->wb_flush_threshold = UFS_WB_BUF_REMAIN_PERCENT(80);\n\n\tif (host->caps & UFS_MTK_CAP_DISABLE_AH8)\n\t\thba->caps |= UFSHCD_CAP_HIBERN8_WITH_CLK_GATING;\n\n\t/*\n\t * ufshcd_vops_init() is invoked after\n\t * ufshcd_setup_clock(true) in ufshcd_hba_init() thus\n\t * phy clock setup is skipped.\n\t *\n\t * Enable phy clocks specifically here.\n\t */\n\tufs_mtk_mphy_power_on(hba, true);\n\tufs_mtk_setup_clocks(hba, true, POST_CHANGE);\n\n\thost->ip_ver = ufshcd_readl(hba, REG_UFS_MTK_IP_VER);\n\n\tgoto out;\n\nout_variant_clear:\n\tufshcd_set_variant(hba, NULL);\nout:\n\treturn err;\n}\n\nstatic int ufs_mtk_pre_pwr_change(struct ufs_hba *hba,\n\t\t\t\t  struct ufs_pa_layer_attr *dev_max_params,\n\t\t\t\t  struct ufs_pa_layer_attr *dev_req_params)\n{\n\tstruct ufs_mtk_host *host = ufshcd_get_variant(hba);\n\tstruct ufs_dev_params host_cap;\n\tint ret;\n\n\tufshcd_init_pwr_dev_param(&host_cap);\n\thost_cap.hs_rx_gear = UFS_HS_G4;\n\thost_cap.hs_tx_gear = UFS_HS_G4;\n\n\tret = ufshcd_get_pwr_dev_param(&host_cap,\n\t\t\t\t       dev_max_params,\n\t\t\t\t       dev_req_params);\n\tif (ret) {\n\t\tpr_info(\"%s: failed to determine capabilities\\n\",\n\t\t\t__func__);\n\t}\n\n\tif (host->hw_ver.major >= 3) {\n\t\tret = ufshcd_dme_configure_adapt(hba,\n\t\t\t\t\t   dev_req_params->gear_tx,\n\t\t\t\t\t   PA_INITIAL_ADAPT);\n\t}\n\n\treturn ret;\n}\n\nstatic int ufs_mtk_pwr_change_notify(struct ufs_hba *hba,\n\t\t\t\t     enum ufs_notify_change_status stage,\n\t\t\t\t     struct ufs_pa_layer_attr *dev_max_params,\n\t\t\t\t     struct ufs_pa_layer_attr *dev_req_params)\n{\n\tint ret = 0;\n\n\tswitch (stage) {\n\tcase PRE_CHANGE:\n\t\tret = ufs_mtk_pre_pwr_change(hba, dev_max_params,\n\t\t\t\t\t     dev_req_params);\n\t\tbreak;\n\tcase POST_CHANGE:\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nstatic int ufs_mtk_unipro_set_lpm(struct ufs_hba *hba, bool lpm)\n{\n\tint ret;\n\tstruct ufs_mtk_host *host = ufshcd_get_variant(hba);\n\n\tret = ufshcd_dme_set(hba,\n\t\t\t     UIC_ARG_MIB_SEL(VS_UNIPROPOWERDOWNCONTROL, 0),\n\t\t\t     lpm ? 1 : 0);\n\tif (!ret || !lpm) {\n\t\t/*\n\t\t * Forcibly set as non-LPM mode if UIC commands is failed\n\t\t * to use default hba_enable_delay_us value for re-enabling\n\t\t * the host.\n\t\t */\n\t\thost->unipro_lpm = lpm;\n\t}\n\n\treturn ret;\n}\n\nstatic int ufs_mtk_pre_link(struct ufs_hba *hba)\n{\n\tint ret;\n\tu32 tmp;\n\n\tufs_mtk_get_controller_version(hba);\n\n\tret = ufs_mtk_unipro_set_lpm(hba, false);\n\tif (ret)\n\t\treturn ret;\n\n\t/*\n\t * Setting PA_Local_TX_LCC_Enable to 0 before link startup\n\t * to make sure that both host and device TX LCC are disabled\n\t * once link startup is completed.\n\t */\n\tret = ufshcd_disable_host_tx_lcc(hba);\n\tif (ret)\n\t\treturn ret;\n\n\t/* disable deep stall */\n\tret = ufshcd_dme_get(hba, UIC_ARG_MIB(VS_SAVEPOWERCONTROL), &tmp);\n\tif (ret)\n\t\treturn ret;\n\n\ttmp &= ~(1 << 6);\n\n\tret = ufshcd_dme_set(hba, UIC_ARG_MIB(VS_SAVEPOWERCONTROL), tmp);\n\n\treturn ret;\n}\n\nstatic void ufs_mtk_setup_clk_gating(struct ufs_hba *hba)\n{\n\tunsigned long flags;\n\tu32 ah_ms;\n\n\tif (ufshcd_is_clkgating_allowed(hba)) {\n\t\tif (ufshcd_is_auto_hibern8_supported(hba) && hba->ahit)\n\t\t\tah_ms = FIELD_GET(UFSHCI_AHIBERN8_TIMER_MASK,\n\t\t\t\t\t  hba->ahit);\n\t\telse\n\t\t\tah_ms = 10;\n\t\tspin_lock_irqsave(hba->host->host_lock, flags);\n\t\thba->clk_gating.delay_ms = ah_ms + 5;\n\t\tspin_unlock_irqrestore(hba->host->host_lock, flags);\n\t}\n}\n\nstatic int ufs_mtk_post_link(struct ufs_hba *hba)\n{\n\t/* enable unipro clock gating feature */\n\tufs_mtk_cfg_unipro_cg(hba, true);\n\n\t/* will be configured during probe hba */\n\tif (ufshcd_is_auto_hibern8_supported(hba))\n\t\thba->ahit = FIELD_PREP(UFSHCI_AHIBERN8_TIMER_MASK, 10) |\n\t\t\tFIELD_PREP(UFSHCI_AHIBERN8_SCALE_MASK, 3);\n\n\tufs_mtk_setup_clk_gating(hba);\n\n\treturn 0;\n}\n\nstatic int ufs_mtk_link_startup_notify(struct ufs_hba *hba,\n\t\t\t\t       enum ufs_notify_change_status stage)\n{\n\tint ret = 0;\n\n\tswitch (stage) {\n\tcase PRE_CHANGE:\n\t\tret = ufs_mtk_pre_link(hba);\n\t\tbreak;\n\tcase POST_CHANGE:\n\t\tret = ufs_mtk_post_link(hba);\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nstatic int ufs_mtk_device_reset(struct ufs_hba *hba)\n{\n\tstruct arm_smccc_res res;\n\n\t/* disable hba before device reset */\n\tufshcd_hba_stop(hba);\n\n\tufs_mtk_device_reset_ctrl(0, res);\n\n\t/*\n\t * The reset signal is active low. UFS devices shall detect\n\t * more than or equal to 1us of positive or negative RST_n\n\t * pulse width.\n\t *\n\t * To be on safe side, keep the reset low for at least 10us.\n\t */\n\tusleep_range(10, 15);\n\n\tufs_mtk_device_reset_ctrl(1, res);\n\n\t/* Some devices may need time to respond to rst_n */\n\tusleep_range(10000, 15000);\n\n\tdev_info(hba->dev, \"device reset done\\n\");\n\n\treturn 0;\n}\n\nstatic int ufs_mtk_link_set_hpm(struct ufs_hba *hba)\n{\n\tint err;\n\n\terr = ufshcd_hba_enable(hba);\n\tif (err)\n\t\treturn err;\n\n\terr = ufs_mtk_unipro_set_lpm(hba, false);\n\tif (err)\n\t\treturn err;\n\n\terr = ufshcd_uic_hibern8_exit(hba);\n\tif (!err)\n\t\tufshcd_set_link_active(hba);\n\telse\n\t\treturn err;\n\n\terr = ufshcd_make_hba_operational(hba);\n\tif (err)\n\t\treturn err;\n\n\treturn 0;\n}\n\nstatic int ufs_mtk_link_set_lpm(struct ufs_hba *hba)\n{\n\tint err;\n\n\terr = ufs_mtk_unipro_set_lpm(hba, true);\n\tif (err) {\n\t\t/* Resume UniPro state for following error recovery */\n\t\tufs_mtk_unipro_set_lpm(hba, false);\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic void ufs_mtk_vreg_set_lpm(struct ufs_hba *hba, bool lpm)\n{\n\tif (!hba->vreg_info.vccq2 || !hba->vreg_info.vcc)\n\t\treturn;\n\n\tif (lpm && !hba->vreg_info.vcc->enabled)\n\t\tregulator_set_mode(hba->vreg_info.vccq2->reg,\n\t\t\t\t   REGULATOR_MODE_IDLE);\n\telse if (!lpm)\n\t\tregulator_set_mode(hba->vreg_info.vccq2->reg,\n\t\t\t\t   REGULATOR_MODE_NORMAL);\n}\n\nstatic void ufs_mtk_auto_hibern8_disable(struct ufs_hba *hba)\n{\n\tunsigned long flags;\n\tint ret;\n\n\t/* disable auto-hibern8 */\n\tspin_lock_irqsave(hba->host->host_lock, flags);\n\tufshcd_writel(hba, 0, REG_AUTO_HIBERNATE_IDLE_TIMER);\n\tspin_unlock_irqrestore(hba->host->host_lock, flags);\n\n\t/* wait host return to idle state when auto-hibern8 off */\n\tufs_mtk_wait_idle_state(hba, 5);\n\n\tret = ufs_mtk_wait_link_state(hba, VS_LINK_UP, 100);\n\tif (ret)\n\t\tdev_warn(hba->dev, \"exit h8 state fail, ret=%d\\n\", ret);\n}\n\nstatic int ufs_mtk_suspend(struct ufs_hba *hba, enum ufs_pm_op pm_op,\n\tenum ufs_notify_change_status status)\n{\n\tint err;\n\tstruct arm_smccc_res res;\n\n\tif (status == PRE_CHANGE) {\n\t\tif (!ufshcd_is_auto_hibern8_supported(hba))\n\t\t\treturn 0;\n\t\tufs_mtk_auto_hibern8_disable(hba);\n\t\treturn 0;\n\t}\n\n\tif (ufshcd_is_link_hibern8(hba)) {\n\t\terr = ufs_mtk_link_set_lpm(hba);\n\t\tif (err)\n\t\t\tgoto fail;\n\t}\n\n\tif (!ufshcd_is_link_active(hba)) {\n\t\t/*\n\t\t * Make sure no error will be returned to prevent\n\t\t * ufshcd_suspend() re-enabling regulators while vreg is still\n\t\t * in low-power mode.\n\t\t */\n\t\tufs_mtk_vreg_set_lpm(hba, true);\n\t\terr = ufs_mtk_mphy_power_on(hba, false);\n\t\tif (err)\n\t\t\tgoto fail;\n\t}\n\n\tif (ufshcd_is_link_off(hba))\n\t\tufs_mtk_device_reset_ctrl(0, res);\n\n\treturn 0;\nfail:\n\t/*\n\t * Set link as off state enforcedly to trigger\n\t * ufshcd_host_reset_and_restore() in ufshcd_suspend()\n\t * for completed host reset.\n\t */\n\tufshcd_set_link_off(hba);\n\treturn -EAGAIN;\n}\n\nstatic int ufs_mtk_resume(struct ufs_hba *hba, enum ufs_pm_op pm_op)\n{\n\tint err;\n\n\terr = ufs_mtk_mphy_power_on(hba, true);\n\tif (err)\n\t\tgoto fail;\n\n\tufs_mtk_vreg_set_lpm(hba, false);\n\n\tif (ufshcd_is_link_hibern8(hba)) {\n\t\terr = ufs_mtk_link_set_hpm(hba);\n\t\tif (err)\n\t\t\tgoto fail;\n\t}\n\n\treturn 0;\nfail:\n\treturn ufshcd_link_recovery(hba);\n}\n\nstatic void ufs_mtk_dbg_register_dump(struct ufs_hba *hba)\n{\n\tufshcd_dump_regs(hba, REG_UFS_REFCLK_CTRL, 0x4, \"Ref-Clk Ctrl \");\n\n\tufshcd_dump_regs(hba, REG_UFS_EXTREG, 0x4, \"Ext Reg \");\n\n\tufshcd_dump_regs(hba, REG_UFS_MPHYCTRL,\n\t\t\t REG_UFS_REJECT_MON - REG_UFS_MPHYCTRL + 4,\n\t\t\t \"MPHY Ctrl \");\n\n\t/* Direct debugging information to REG_MTK_PROBE */\n\tufs_mtk_dbg_sel(hba);\n\tufshcd_dump_regs(hba, REG_UFS_PROBE, 0x4, \"Debug Probe \");\n}\n\nstatic int ufs_mtk_apply_dev_quirks(struct ufs_hba *hba)\n{\n\tstruct ufs_dev_info *dev_info = &hba->dev_info;\n\tu16 mid = dev_info->wmanufacturerid;\n\n\tif (mid == UFS_VENDOR_SAMSUNG)\n\t\tufshcd_dme_set(hba, UIC_ARG_MIB(PA_TACTIVATE), 6);\n\n\t/*\n\t * Decide waiting time before gating reference clock and\n\t * after ungating reference clock according to vendors'\n\t * requirements.\n\t */\n\tif (mid == UFS_VENDOR_SAMSUNG)\n\t\tufs_mtk_setup_ref_clk_wait_us(hba, 1);\n\telse if (mid == UFS_VENDOR_SKHYNIX)\n\t\tufs_mtk_setup_ref_clk_wait_us(hba, 30);\n\telse if (mid == UFS_VENDOR_TOSHIBA)\n\t\tufs_mtk_setup_ref_clk_wait_us(hba, 100);\n\telse\n\t\tufs_mtk_setup_ref_clk_wait_us(hba,\n\t\t\t\t\t      REFCLK_DEFAULT_WAIT_US);\n\n\treturn 0;\n}\n\nstatic void ufs_mtk_fixup_dev_quirks(struct ufs_hba *hba)\n{\n\tufshcd_fixup_dev_quirks(hba, ufs_mtk_dev_fixups);\n\n\tif (ufs_mtk_is_broken_vcc(hba) && hba->vreg_info.vcc &&\n\t    (hba->dev_quirks & UFS_DEVICE_QUIRK_DELAY_AFTER_LPM)) {\n\t\thba->vreg_info.vcc->always_on = true;\n\t\t/*\n\t\t * VCC will be kept always-on thus we don't\n\t\t * need any delay during regulator operations\n\t\t */\n\t\thba->dev_quirks &= ~(UFS_DEVICE_QUIRK_DELAY_BEFORE_LPM |\n\t\t\tUFS_DEVICE_QUIRK_DELAY_AFTER_LPM);\n\t}\n}\n\nstatic void ufs_mtk_event_notify(struct ufs_hba *hba,\n\t\t\t\t enum ufs_event_type evt, void *data)\n{\n\tunsigned int val = *(u32 *)data;\n\n\ttrace_ufs_mtk_event(evt, val);\n}\n\n/*\n * struct ufs_hba_mtk_vops - UFS MTK specific variant operations\n *\n * The variant operations configure the necessary controller and PHY\n * handshake during initialization.\n */\nstatic const struct ufs_hba_variant_ops ufs_hba_mtk_vops = {\n\t.name                = \"mediatek.ufshci\",\n\t.init                = ufs_mtk_init,\n\t.get_ufs_hci_version = ufs_mtk_get_ufs_hci_version,\n\t.setup_clocks        = ufs_mtk_setup_clocks,\n\t.hce_enable_notify   = ufs_mtk_hce_enable_notify,\n\t.link_startup_notify = ufs_mtk_link_startup_notify,\n\t.pwr_change_notify   = ufs_mtk_pwr_change_notify,\n\t.apply_dev_quirks    = ufs_mtk_apply_dev_quirks,\n\t.fixup_dev_quirks    = ufs_mtk_fixup_dev_quirks,\n\t.suspend             = ufs_mtk_suspend,\n\t.resume              = ufs_mtk_resume,\n\t.dbg_register_dump   = ufs_mtk_dbg_register_dump,\n\t.device_reset        = ufs_mtk_device_reset,\n\t.event_notify        = ufs_mtk_event_notify,\n};\n\n/**\n * ufs_mtk_probe - probe routine of the driver\n * @pdev: pointer to Platform device handle\n *\n * Return zero for success and non-zero for failure\n */\nstatic int ufs_mtk_probe(struct platform_device *pdev)\n{\n\tint err;\n\tstruct device *dev = &pdev->dev;\n\tstruct device_node *reset_node;\n\tstruct platform_device *reset_pdev;\n\tstruct device_link *link;\n\n\treset_node = of_find_compatible_node(NULL, NULL,\n\t\t\t\t\t     \"ti,syscon-reset\");\n\tif (!reset_node) {\n\t\tdev_notice(dev, \"find ti,syscon-reset fail\\n\");\n\t\tgoto skip_reset;\n\t}\n\treset_pdev = of_find_device_by_node(reset_node);\n\tif (!reset_pdev) {\n\t\tdev_notice(dev, \"find reset_pdev fail\\n\");\n\t\tgoto skip_reset;\n\t}\n\tlink = device_link_add(dev, &reset_pdev->dev,\n\t\tDL_FLAG_AUTOPROBE_CONSUMER);\n\tput_device(&reset_pdev->dev);\n\tif (!link) {\n\t\tdev_notice(dev, \"add reset device_link fail\\n\");\n\t\tgoto skip_reset;\n\t}\n\t/* supplier is not probed */\n\tif (link->status == DL_STATE_DORMANT) {\n\t\terr = -EPROBE_DEFER;\n\t\tgoto out;\n\t}\n\nskip_reset:\n\t/* perform generic probe */\n\terr = ufshcd_pltfrm_init(pdev, &ufs_hba_mtk_vops);\n\nout:\n\tif (err)\n\t\tdev_info(dev, \"probe failed %d\\n\", err);\n\n\tof_node_put(reset_node);\n\treturn err;\n}\n\n/**\n * ufs_mtk_remove - set driver_data of the device to NULL\n * @pdev: pointer to platform device handle\n *\n * Always return 0\n */\nstatic int ufs_mtk_remove(struct platform_device *pdev)\n{\n\tstruct ufs_hba *hba =  platform_get_drvdata(pdev);\n\n\tpm_runtime_get_sync(&(pdev)->dev);\n\tufshcd_remove(hba);\n\treturn 0;\n}\n\nstatic const struct dev_pm_ops ufs_mtk_pm_ops = {\n\tSET_SYSTEM_SLEEP_PM_OPS(ufshcd_system_suspend, ufshcd_system_resume)\n\tSET_RUNTIME_PM_OPS(ufshcd_runtime_suspend, ufshcd_runtime_resume, NULL)\n\t.prepare\t = ufshcd_suspend_prepare,\n\t.complete\t = ufshcd_resume_complete,\n};\n\nstatic struct platform_driver ufs_mtk_pltform = {\n\t.probe      = ufs_mtk_probe,\n\t.remove     = ufs_mtk_remove,\n\t.shutdown   = ufshcd_pltfrm_shutdown,\n\t.driver = {\n\t\t.name   = \"ufshcd-mtk\",\n\t\t.pm     = &ufs_mtk_pm_ops,\n\t\t.of_match_table = ufs_mtk_of_match,\n\t},\n};\n\nMODULE_AUTHOR(\"Stanley Chu <stanley.chu@mediatek.com>\");\nMODULE_AUTHOR(\"Peter Wang <peter.wang@mediatek.com>\");\nMODULE_DESCRIPTION(\"MediaTek UFS Host Driver\");\nMODULE_LICENSE(\"GPL v2\");\n\nmodule_platform_driver(ufs_mtk_pltform);\n"], "fixing_code": ["// SPDX-License-Identifier: GPL-2.0\n/*\n * Copyright (C) 2019 MediaTek Inc.\n * Authors:\n *\tStanley Chu <stanley.chu@mediatek.com>\n *\tPeter Wang <peter.wang@mediatek.com>\n */\n\n#include <linux/arm-smccc.h>\n#include <linux/bitfield.h>\n#include <linux/of.h>\n#include <linux/of_address.h>\n#include <linux/of_device.h>\n#include <linux/phy/phy.h>\n#include <linux/platform_device.h>\n#include <linux/regulator/consumer.h>\n#include <linux/reset.h>\n#include <linux/sched/clock.h>\n#include <linux/soc/mediatek/mtk_sip_svc.h>\n\n#include \"ufshcd.h\"\n#include \"ufshcd-crypto.h\"\n#include \"ufshcd-pltfrm.h\"\n#include \"ufs_quirks.h\"\n#include \"unipro.h\"\n#include \"ufs-mediatek.h\"\n\n#define CREATE_TRACE_POINTS\n#include \"ufs-mediatek-trace.h\"\n\n#define ufs_mtk_smc(cmd, val, res) \\\n\tarm_smccc_smc(MTK_SIP_UFS_CONTROL, \\\n\t\t      cmd, val, 0, 0, 0, 0, 0, &(res))\n\n#define ufs_mtk_va09_pwr_ctrl(res, on) \\\n\tufs_mtk_smc(UFS_MTK_SIP_VA09_PWR_CTRL, on, res)\n\n#define ufs_mtk_crypto_ctrl(res, enable) \\\n\tufs_mtk_smc(UFS_MTK_SIP_CRYPTO_CTRL, enable, res)\n\n#define ufs_mtk_ref_clk_notify(on, res) \\\n\tufs_mtk_smc(UFS_MTK_SIP_REF_CLK_NOTIFICATION, on, res)\n\n#define ufs_mtk_device_reset_ctrl(high, res) \\\n\tufs_mtk_smc(UFS_MTK_SIP_DEVICE_RESET, high, res)\n\nstatic struct ufs_dev_fix ufs_mtk_dev_fixups[] = {\n\tUFS_FIX(UFS_VENDOR_MICRON, UFS_ANY_MODEL,\n\t\tUFS_DEVICE_QUIRK_DELAY_AFTER_LPM),\n\tUFS_FIX(UFS_VENDOR_SKHYNIX, \"H9HQ21AFAMZDAR\",\n\t\tUFS_DEVICE_QUIRK_SUPPORT_EXTENDED_FEATURES),\n\tEND_FIX\n};\n\nstatic const struct of_device_id ufs_mtk_of_match[] = {\n\t{ .compatible = \"mediatek,mt8183-ufshci\" },\n\t{},\n};\n\nstatic bool ufs_mtk_is_boost_crypt_enabled(struct ufs_hba *hba)\n{\n\tstruct ufs_mtk_host *host = ufshcd_get_variant(hba);\n\n\treturn !!(host->caps & UFS_MTK_CAP_BOOST_CRYPT_ENGINE);\n}\n\nstatic bool ufs_mtk_is_va09_supported(struct ufs_hba *hba)\n{\n\tstruct ufs_mtk_host *host = ufshcd_get_variant(hba);\n\n\treturn !!(host->caps & UFS_MTK_CAP_VA09_PWR_CTRL);\n}\n\nstatic bool ufs_mtk_is_broken_vcc(struct ufs_hba *hba)\n{\n\tstruct ufs_mtk_host *host = ufshcd_get_variant(hba);\n\n\treturn !!(host->caps & UFS_MTK_CAP_BROKEN_VCC);\n}\n\nstatic void ufs_mtk_cfg_unipro_cg(struct ufs_hba *hba, bool enable)\n{\n\tu32 tmp;\n\n\tif (enable) {\n\t\tufshcd_dme_get(hba,\n\t\t\t       UIC_ARG_MIB(VS_SAVEPOWERCONTROL), &tmp);\n\t\ttmp = tmp |\n\t\t      (1 << RX_SYMBOL_CLK_GATE_EN) |\n\t\t      (1 << SYS_CLK_GATE_EN) |\n\t\t      (1 << TX_CLK_GATE_EN);\n\t\tufshcd_dme_set(hba,\n\t\t\t       UIC_ARG_MIB(VS_SAVEPOWERCONTROL), tmp);\n\n\t\tufshcd_dme_get(hba,\n\t\t\t       UIC_ARG_MIB(VS_DEBUGCLOCKENABLE), &tmp);\n\t\ttmp = tmp & ~(1 << TX_SYMBOL_CLK_REQ_FORCE);\n\t\tufshcd_dme_set(hba,\n\t\t\t       UIC_ARG_MIB(VS_DEBUGCLOCKENABLE), tmp);\n\t} else {\n\t\tufshcd_dme_get(hba,\n\t\t\t       UIC_ARG_MIB(VS_SAVEPOWERCONTROL), &tmp);\n\t\ttmp = tmp & ~((1 << RX_SYMBOL_CLK_GATE_EN) |\n\t\t\t      (1 << SYS_CLK_GATE_EN) |\n\t\t\t      (1 << TX_CLK_GATE_EN));\n\t\tufshcd_dme_set(hba,\n\t\t\t       UIC_ARG_MIB(VS_SAVEPOWERCONTROL), tmp);\n\n\t\tufshcd_dme_get(hba,\n\t\t\t       UIC_ARG_MIB(VS_DEBUGCLOCKENABLE), &tmp);\n\t\ttmp = tmp | (1 << TX_SYMBOL_CLK_REQ_FORCE);\n\t\tufshcd_dme_set(hba,\n\t\t\t       UIC_ARG_MIB(VS_DEBUGCLOCKENABLE), tmp);\n\t}\n}\n\nstatic void ufs_mtk_crypto_enable(struct ufs_hba *hba)\n{\n\tstruct arm_smccc_res res;\n\n\tufs_mtk_crypto_ctrl(res, 1);\n\tif (res.a0) {\n\t\tdev_info(hba->dev, \"%s: crypto enable failed, err: %lu\\n\",\n\t\t\t __func__, res.a0);\n\t\thba->caps &= ~UFSHCD_CAP_CRYPTO;\n\t}\n}\n\nstatic void ufs_mtk_host_reset(struct ufs_hba *hba)\n{\n\tstruct ufs_mtk_host *host = ufshcd_get_variant(hba);\n\n\treset_control_assert(host->hci_reset);\n\treset_control_assert(host->crypto_reset);\n\treset_control_assert(host->unipro_reset);\n\n\tusleep_range(100, 110);\n\n\treset_control_deassert(host->unipro_reset);\n\treset_control_deassert(host->crypto_reset);\n\treset_control_deassert(host->hci_reset);\n}\n\nstatic void ufs_mtk_init_reset_control(struct ufs_hba *hba,\n\t\t\t\t       struct reset_control **rc,\n\t\t\t\t       char *str)\n{\n\t*rc = devm_reset_control_get(hba->dev, str);\n\tif (IS_ERR(*rc)) {\n\t\tdev_info(hba->dev, \"Failed to get reset control %s: %ld\\n\",\n\t\t\t str, PTR_ERR(*rc));\n\t\t*rc = NULL;\n\t}\n}\n\nstatic void ufs_mtk_init_reset(struct ufs_hba *hba)\n{\n\tstruct ufs_mtk_host *host = ufshcd_get_variant(hba);\n\n\tufs_mtk_init_reset_control(hba, &host->hci_reset,\n\t\t\t\t   \"hci_rst\");\n\tufs_mtk_init_reset_control(hba, &host->unipro_reset,\n\t\t\t\t   \"unipro_rst\");\n\tufs_mtk_init_reset_control(hba, &host->crypto_reset,\n\t\t\t\t   \"crypto_rst\");\n}\n\nstatic int ufs_mtk_hce_enable_notify(struct ufs_hba *hba,\n\t\t\t\t     enum ufs_notify_change_status status)\n{\n\tstruct ufs_mtk_host *host = ufshcd_get_variant(hba);\n\tunsigned long flags;\n\n\tif (status == PRE_CHANGE) {\n\t\tif (host->unipro_lpm) {\n\t\t\thba->vps->hba_enable_delay_us = 0;\n\t\t} else {\n\t\t\thba->vps->hba_enable_delay_us = 600;\n\t\t\tufs_mtk_host_reset(hba);\n\t\t}\n\n\t\tif (hba->caps & UFSHCD_CAP_CRYPTO)\n\t\t\tufs_mtk_crypto_enable(hba);\n\n\t\tif (host->caps & UFS_MTK_CAP_DISABLE_AH8) {\n\t\t\tspin_lock_irqsave(hba->host->host_lock, flags);\n\t\t\tufshcd_writel(hba, 0,\n\t\t\t\t      REG_AUTO_HIBERNATE_IDLE_TIMER);\n\t\t\tspin_unlock_irqrestore(hba->host->host_lock,\n\t\t\t\t\t       flags);\n\n\t\t\thba->capabilities &= ~MASK_AUTO_HIBERN8_SUPPORT;\n\t\t\thba->ahit = 0;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int ufs_mtk_bind_mphy(struct ufs_hba *hba)\n{\n\tstruct ufs_mtk_host *host = ufshcd_get_variant(hba);\n\tstruct device *dev = hba->dev;\n\tstruct device_node *np = dev->of_node;\n\tint err = 0;\n\n\thost->mphy = devm_of_phy_get_by_index(dev, np, 0);\n\n\tif (host->mphy == ERR_PTR(-EPROBE_DEFER)) {\n\t\t/*\n\t\t * UFS driver might be probed before the phy driver does.\n\t\t * In that case we would like to return EPROBE_DEFER code.\n\t\t */\n\t\terr = -EPROBE_DEFER;\n\t\tdev_info(dev,\n\t\t\t \"%s: required phy hasn't probed yet. err = %d\\n\",\n\t\t\t__func__, err);\n\t} else if (IS_ERR(host->mphy)) {\n\t\terr = PTR_ERR(host->mphy);\n\t\tif (err != -ENODEV) {\n\t\t\tdev_info(dev, \"%s: PHY get failed %d\\n\", __func__,\n\t\t\t\t err);\n\t\t}\n\t}\n\n\tif (err)\n\t\thost->mphy = NULL;\n\t/*\n\t * Allow unbound mphy because not every platform needs specific\n\t * mphy control.\n\t */\n\tif (err == -ENODEV)\n\t\terr = 0;\n\n\treturn err;\n}\n\nstatic int ufs_mtk_setup_ref_clk(struct ufs_hba *hba, bool on)\n{\n\tstruct ufs_mtk_host *host = ufshcd_get_variant(hba);\n\tstruct arm_smccc_res res;\n\tktime_t timeout, time_checked;\n\tu32 value;\n\n\tif (host->ref_clk_enabled == on)\n\t\treturn 0;\n\n\tif (on) {\n\t\tufs_mtk_ref_clk_notify(on, res);\n\t\tufshcd_writel(hba, REFCLK_REQUEST, REG_UFS_REFCLK_CTRL);\n\t} else {\n\t\tufshcd_delay_us(host->ref_clk_gating_wait_us, 10);\n\t\tufshcd_writel(hba, REFCLK_RELEASE, REG_UFS_REFCLK_CTRL);\n\t}\n\n\t/* Wait for ack */\n\ttimeout = ktime_add_us(ktime_get(), REFCLK_REQ_TIMEOUT_US);\n\tdo {\n\t\ttime_checked = ktime_get();\n\t\tvalue = ufshcd_readl(hba, REG_UFS_REFCLK_CTRL);\n\n\t\t/* Wait until ack bit equals to req bit */\n\t\tif (((value & REFCLK_ACK) >> 1) == (value & REFCLK_REQUEST))\n\t\t\tgoto out;\n\n\t\tusleep_range(100, 200);\n\t} while (ktime_before(time_checked, timeout));\n\n\tdev_err(hba->dev, \"missing ack of refclk req, reg: 0x%x\\n\", value);\n\n\tufs_mtk_ref_clk_notify(host->ref_clk_enabled, res);\n\n\treturn -ETIMEDOUT;\n\nout:\n\thost->ref_clk_enabled = on;\n\tif (on)\n\t\tufshcd_delay_us(host->ref_clk_ungating_wait_us, 10);\n\telse\n\t\tufs_mtk_ref_clk_notify(on, res);\n\n\treturn 0;\n}\n\nstatic void ufs_mtk_setup_ref_clk_wait_us(struct ufs_hba *hba,\n\t\t\t\t\t  u16 gating_us)\n{\n\tstruct ufs_mtk_host *host = ufshcd_get_variant(hba);\n\n\tif (hba->dev_info.clk_gating_wait_us) {\n\t\thost->ref_clk_gating_wait_us =\n\t\t\thba->dev_info.clk_gating_wait_us;\n\t} else {\n\t\thost->ref_clk_gating_wait_us = gating_us;\n\t}\n\n\thost->ref_clk_ungating_wait_us = REFCLK_DEFAULT_WAIT_US;\n}\n\nstatic void ufs_mtk_dbg_sel(struct ufs_hba *hba)\n{\n\tstruct ufs_mtk_host *host = ufshcd_get_variant(hba);\n\n\tif (((host->ip_ver >> 16) & 0xFF) >= 0x36) {\n\t\tufshcd_writel(hba, 0x820820, REG_UFS_DEBUG_SEL);\n\t\tufshcd_writel(hba, 0x0, REG_UFS_DEBUG_SEL_B0);\n\t\tufshcd_writel(hba, 0x55555555, REG_UFS_DEBUG_SEL_B1);\n\t\tufshcd_writel(hba, 0xaaaaaaaa, REG_UFS_DEBUG_SEL_B2);\n\t\tufshcd_writel(hba, 0xffffffff, REG_UFS_DEBUG_SEL_B3);\n\t} else {\n\t\tufshcd_writel(hba, 0x20, REG_UFS_DEBUG_SEL);\n\t}\n}\n\nstatic void ufs_mtk_wait_idle_state(struct ufs_hba *hba,\n\t\t\t    unsigned long retry_ms)\n{\n\tu64 timeout, time_checked;\n\tu32 val, sm;\n\tbool wait_idle;\n\n\t/* cannot use plain ktime_get() in suspend */\n\ttimeout = ktime_get_mono_fast_ns() + retry_ms * 1000000UL;\n\n\t/* wait a specific time after check base */\n\tudelay(10);\n\twait_idle = false;\n\n\tdo {\n\t\ttime_checked = ktime_get_mono_fast_ns();\n\t\tufs_mtk_dbg_sel(hba);\n\t\tval = ufshcd_readl(hba, REG_UFS_PROBE);\n\n\t\tsm = val & 0x1f;\n\n\t\t/*\n\t\t * if state is in H8 enter and H8 enter confirm\n\t\t * wait until return to idle state.\n\t\t */\n\t\tif ((sm >= VS_HIB_ENTER) && (sm <= VS_HIB_EXIT)) {\n\t\t\twait_idle = true;\n\t\t\tudelay(50);\n\t\t\tcontinue;\n\t\t} else if (!wait_idle)\n\t\t\tbreak;\n\n\t\tif (wait_idle && (sm == VS_HCE_BASE))\n\t\t\tbreak;\n\t} while (time_checked < timeout);\n\n\tif (wait_idle && sm != VS_HCE_BASE)\n\t\tdev_info(hba->dev, \"wait idle tmo: 0x%x\\n\", val);\n}\n\nstatic int ufs_mtk_wait_link_state(struct ufs_hba *hba, u32 state,\n\t\t\t\t   unsigned long max_wait_ms)\n{\n\tktime_t timeout, time_checked;\n\tu32 val;\n\n\ttimeout = ktime_add_ms(ktime_get(), max_wait_ms);\n\tdo {\n\t\ttime_checked = ktime_get();\n\t\tufs_mtk_dbg_sel(hba);\n\t\tval = ufshcd_readl(hba, REG_UFS_PROBE);\n\t\tval = val >> 28;\n\n\t\tif (val == state)\n\t\t\treturn 0;\n\n\t\t/* Sleep for max. 200us */\n\t\tusleep_range(100, 200);\n\t} while (ktime_before(time_checked, timeout));\n\n\tif (val == state)\n\t\treturn 0;\n\n\treturn -ETIMEDOUT;\n}\n\nstatic int ufs_mtk_mphy_power_on(struct ufs_hba *hba, bool on)\n{\n\tstruct ufs_mtk_host *host = ufshcd_get_variant(hba);\n\tstruct phy *mphy = host->mphy;\n\tstruct arm_smccc_res res;\n\tint ret = 0;\n\n\tif (!mphy || !(on ^ host->mphy_powered_on))\n\t\treturn 0;\n\n\tif (on) {\n\t\tif (ufs_mtk_is_va09_supported(hba)) {\n\t\t\tret = regulator_enable(host->reg_va09);\n\t\t\tif (ret < 0)\n\t\t\t\tgoto out;\n\t\t\t/* wait 200 us to stablize VA09 */\n\t\t\tusleep_range(200, 210);\n\t\t\tufs_mtk_va09_pwr_ctrl(res, 1);\n\t\t}\n\t\tphy_power_on(mphy);\n\t} else {\n\t\tphy_power_off(mphy);\n\t\tif (ufs_mtk_is_va09_supported(hba)) {\n\t\t\tufs_mtk_va09_pwr_ctrl(res, 0);\n\t\t\tret = regulator_disable(host->reg_va09);\n\t\t\tif (ret < 0)\n\t\t\t\tgoto out;\n\t\t}\n\t}\nout:\n\tif (ret) {\n\t\tdev_info(hba->dev,\n\t\t\t \"failed to %s va09: %d\\n\",\n\t\t\t on ? \"enable\" : \"disable\",\n\t\t\t ret);\n\t} else {\n\t\thost->mphy_powered_on = on;\n\t}\n\n\treturn ret;\n}\n\nstatic int ufs_mtk_get_host_clk(struct device *dev, const char *name,\n\t\t\t\tstruct clk **clk_out)\n{\n\tstruct clk *clk;\n\tint err = 0;\n\n\tclk = devm_clk_get(dev, name);\n\tif (IS_ERR(clk))\n\t\terr = PTR_ERR(clk);\n\telse\n\t\t*clk_out = clk;\n\n\treturn err;\n}\n\nstatic void ufs_mtk_boost_crypt(struct ufs_hba *hba, bool boost)\n{\n\tstruct ufs_mtk_host *host = ufshcd_get_variant(hba);\n\tstruct ufs_mtk_crypt_cfg *cfg;\n\tstruct regulator *reg;\n\tint volt, ret;\n\n\tif (!ufs_mtk_is_boost_crypt_enabled(hba))\n\t\treturn;\n\n\tcfg = host->crypt;\n\tvolt = cfg->vcore_volt;\n\treg = cfg->reg_vcore;\n\n\tret = clk_prepare_enable(cfg->clk_crypt_mux);\n\tif (ret) {\n\t\tdev_info(hba->dev, \"clk_prepare_enable(): %d\\n\",\n\t\t\t ret);\n\t\treturn;\n\t}\n\n\tif (boost) {\n\t\tret = regulator_set_voltage(reg, volt, INT_MAX);\n\t\tif (ret) {\n\t\t\tdev_info(hba->dev,\n\t\t\t\t \"failed to set vcore to %d\\n\", volt);\n\t\t\tgoto out;\n\t\t}\n\n\t\tret = clk_set_parent(cfg->clk_crypt_mux,\n\t\t\t\t     cfg->clk_crypt_perf);\n\t\tif (ret) {\n\t\t\tdev_info(hba->dev,\n\t\t\t\t \"failed to set clk_crypt_perf\\n\");\n\t\t\tregulator_set_voltage(reg, 0, INT_MAX);\n\t\t\tgoto out;\n\t\t}\n\t} else {\n\t\tret = clk_set_parent(cfg->clk_crypt_mux,\n\t\t\t\t     cfg->clk_crypt_lp);\n\t\tif (ret) {\n\t\t\tdev_info(hba->dev,\n\t\t\t\t \"failed to set clk_crypt_lp\\n\");\n\t\t\tgoto out;\n\t\t}\n\n\t\tret = regulator_set_voltage(reg, 0, INT_MAX);\n\t\tif (ret) {\n\t\t\tdev_info(hba->dev,\n\t\t\t\t \"failed to set vcore to MIN\\n\");\n\t\t}\n\t}\nout:\n\tclk_disable_unprepare(cfg->clk_crypt_mux);\n}\n\nstatic int ufs_mtk_init_host_clk(struct ufs_hba *hba, const char *name,\n\t\t\t\t struct clk **clk)\n{\n\tint ret;\n\n\tret = ufs_mtk_get_host_clk(hba->dev, name, clk);\n\tif (ret) {\n\t\tdev_info(hba->dev, \"%s: failed to get %s: %d\", __func__,\n\t\t\t name, ret);\n\t}\n\n\treturn ret;\n}\n\nstatic void ufs_mtk_init_boost_crypt(struct ufs_hba *hba)\n{\n\tstruct ufs_mtk_host *host = ufshcd_get_variant(hba);\n\tstruct ufs_mtk_crypt_cfg *cfg;\n\tstruct device *dev = hba->dev;\n\tstruct regulator *reg;\n\tu32 volt;\n\n\thost->crypt = devm_kzalloc(dev, sizeof(*(host->crypt)),\n\t\t\t\t   GFP_KERNEL);\n\tif (!host->crypt)\n\t\tgoto disable_caps;\n\n\treg = devm_regulator_get_optional(dev, \"dvfsrc-vcore\");\n\tif (IS_ERR(reg)) {\n\t\tdev_info(dev, \"failed to get dvfsrc-vcore: %ld\",\n\t\t\t PTR_ERR(reg));\n\t\tgoto disable_caps;\n\t}\n\n\tif (of_property_read_u32(dev->of_node, \"boost-crypt-vcore-min\",\n\t\t\t\t &volt)) {\n\t\tdev_info(dev, \"failed to get boost-crypt-vcore-min\");\n\t\tgoto disable_caps;\n\t}\n\n\tcfg = host->crypt;\n\tif (ufs_mtk_init_host_clk(hba, \"crypt_mux\",\n\t\t\t\t  &cfg->clk_crypt_mux))\n\t\tgoto disable_caps;\n\n\tif (ufs_mtk_init_host_clk(hba, \"crypt_lp\",\n\t\t\t\t  &cfg->clk_crypt_lp))\n\t\tgoto disable_caps;\n\n\tif (ufs_mtk_init_host_clk(hba, \"crypt_perf\",\n\t\t\t\t  &cfg->clk_crypt_perf))\n\t\tgoto disable_caps;\n\n\tcfg->reg_vcore = reg;\n\tcfg->vcore_volt = volt;\n\thost->caps |= UFS_MTK_CAP_BOOST_CRYPT_ENGINE;\n\ndisable_caps:\n\treturn;\n}\n\nstatic void ufs_mtk_init_va09_pwr_ctrl(struct ufs_hba *hba)\n{\n\tstruct ufs_mtk_host *host = ufshcd_get_variant(hba);\n\n\thost->reg_va09 = regulator_get(hba->dev, \"va09\");\n\tif (IS_ERR(host->reg_va09))\n\t\tdev_info(hba->dev, \"failed to get va09\");\n\telse\n\t\thost->caps |= UFS_MTK_CAP_VA09_PWR_CTRL;\n}\n\nstatic void ufs_mtk_init_host_caps(struct ufs_hba *hba)\n{\n\tstruct ufs_mtk_host *host = ufshcd_get_variant(hba);\n\tstruct device_node *np = hba->dev->of_node;\n\n\tif (of_property_read_bool(np, \"mediatek,ufs-boost-crypt\"))\n\t\tufs_mtk_init_boost_crypt(hba);\n\n\tif (of_property_read_bool(np, \"mediatek,ufs-support-va09\"))\n\t\tufs_mtk_init_va09_pwr_ctrl(hba);\n\n\tif (of_property_read_bool(np, \"mediatek,ufs-disable-ah8\"))\n\t\thost->caps |= UFS_MTK_CAP_DISABLE_AH8;\n\n\tif (of_property_read_bool(np, \"mediatek,ufs-broken-vcc\"))\n\t\thost->caps |= UFS_MTK_CAP_BROKEN_VCC;\n\n\tdev_info(hba->dev, \"caps: 0x%x\", host->caps);\n}\n\nstatic void ufs_mtk_scale_perf(struct ufs_hba *hba, bool up)\n{\n\tstruct ufs_mtk_host *host = ufshcd_get_variant(hba);\n\n\tufs_mtk_boost_crypt(hba, up);\n\tufs_mtk_setup_ref_clk(hba, up);\n\n\tif (up)\n\t\tphy_power_on(host->mphy);\n\telse\n\t\tphy_power_off(host->mphy);\n}\n\n/**\n * ufs_mtk_setup_clocks - enables/disable clocks\n * @hba: host controller instance\n * @on: If true, enable clocks else disable them.\n * @status: PRE_CHANGE or POST_CHANGE notify\n *\n * Returns 0 on success, non-zero on failure.\n */\nstatic int ufs_mtk_setup_clocks(struct ufs_hba *hba, bool on,\n\t\t\t\tenum ufs_notify_change_status status)\n{\n\tstruct ufs_mtk_host *host = ufshcd_get_variant(hba);\n\tbool clk_pwr_off = false;\n\tint ret = 0;\n\n\t/*\n\t * In case ufs_mtk_init() is not yet done, simply ignore.\n\t * This ufs_mtk_setup_clocks() shall be called from\n\t * ufs_mtk_init() after init is done.\n\t */\n\tif (!host)\n\t\treturn 0;\n\n\tif (!on && status == PRE_CHANGE) {\n\t\tif (ufshcd_is_link_off(hba)) {\n\t\t\tclk_pwr_off = true;\n\t\t} else if (ufshcd_is_link_hibern8(hba) ||\n\t\t\t (!ufshcd_can_hibern8_during_gating(hba) &&\n\t\t\t ufshcd_is_auto_hibern8_enabled(hba))) {\n\t\t\t/*\n\t\t\t * Gate ref-clk and poweroff mphy if link state is in\n\t\t\t * OFF or Hibern8 by either Auto-Hibern8 or\n\t\t\t * ufshcd_link_state_transition().\n\t\t\t */\n\t\t\tret = ufs_mtk_wait_link_state(hba,\n\t\t\t\t\t\t      VS_LINK_HIBERN8,\n\t\t\t\t\t\t      15);\n\t\t\tif (!ret)\n\t\t\t\tclk_pwr_off = true;\n\t\t}\n\n\t\tif (clk_pwr_off)\n\t\t\tufs_mtk_scale_perf(hba, false);\n\t} else if (on && status == POST_CHANGE) {\n\t\tufs_mtk_scale_perf(hba, true);\n\t}\n\n\treturn ret;\n}\n\nstatic void ufs_mtk_get_controller_version(struct ufs_hba *hba)\n{\n\tstruct ufs_mtk_host *host = ufshcd_get_variant(hba);\n\tint ret, ver = 0;\n\n\tif (host->hw_ver.major)\n\t\treturn;\n\n\t/* Set default (minimum) version anyway */\n\thost->hw_ver.major = 2;\n\n\tret = ufshcd_dme_get(hba, UIC_ARG_MIB(PA_LOCALVERINFO), &ver);\n\tif (!ret) {\n\t\tif (ver >= UFS_UNIPRO_VER_1_8) {\n\t\t\thost->hw_ver.major = 3;\n\t\t\t/*\n\t\t\t * Fix HCI version for some platforms with\n\t\t\t * incorrect version\n\t\t\t */\n\t\t\tif (hba->ufs_version < ufshci_version(3, 0))\n\t\t\t\thba->ufs_version = ufshci_version(3, 0);\n\t\t}\n\t}\n}\n\nstatic u32 ufs_mtk_get_ufs_hci_version(struct ufs_hba *hba)\n{\n\treturn hba->ufs_version;\n}\n\n/**\n * ufs_mtk_init - find other essential mmio bases\n * @hba: host controller instance\n *\n * Binds PHY with controller and powers up PHY enabling clocks\n * and regulators.\n *\n * Returns -EPROBE_DEFER if binding fails, returns negative error\n * on phy power up failure and returns zero on success.\n */\nstatic int ufs_mtk_init(struct ufs_hba *hba)\n{\n\tconst struct of_device_id *id;\n\tstruct device *dev = hba->dev;\n\tstruct ufs_mtk_host *host;\n\tint err = 0;\n\n\thost = devm_kzalloc(dev, sizeof(*host), GFP_KERNEL);\n\tif (!host) {\n\t\terr = -ENOMEM;\n\t\tdev_info(dev, \"%s: no memory for mtk ufs host\\n\", __func__);\n\t\tgoto out;\n\t}\n\n\thost->hba = hba;\n\tufshcd_set_variant(hba, host);\n\n\tid = of_match_device(ufs_mtk_of_match, dev);\n\tif (!id) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t/* Initialize host capability */\n\tufs_mtk_init_host_caps(hba);\n\n\terr = ufs_mtk_bind_mphy(hba);\n\tif (err)\n\t\tgoto out_variant_clear;\n\n\tufs_mtk_init_reset(hba);\n\n\t/* Enable runtime autosuspend */\n\thba->caps |= UFSHCD_CAP_RPM_AUTOSUSPEND;\n\n\t/* Enable clock-gating */\n\thba->caps |= UFSHCD_CAP_CLK_GATING;\n\n\t/* Enable inline encryption */\n\thba->caps |= UFSHCD_CAP_CRYPTO;\n\n\t/* Enable WriteBooster */\n\thba->caps |= UFSHCD_CAP_WB_EN;\n\thba->quirks |= UFSHCI_QUIRK_SKIP_MANUAL_WB_FLUSH_CTRL;\n\thba->vps->wb_flush_threshold = UFS_WB_BUF_REMAIN_PERCENT(80);\n\n\tif (host->caps & UFS_MTK_CAP_DISABLE_AH8)\n\t\thba->caps |= UFSHCD_CAP_HIBERN8_WITH_CLK_GATING;\n\n\t/*\n\t * ufshcd_vops_init() is invoked after\n\t * ufshcd_setup_clock(true) in ufshcd_hba_init() thus\n\t * phy clock setup is skipped.\n\t *\n\t * Enable phy clocks specifically here.\n\t */\n\tufs_mtk_mphy_power_on(hba, true);\n\tufs_mtk_setup_clocks(hba, true, POST_CHANGE);\n\n\thost->ip_ver = ufshcd_readl(hba, REG_UFS_MTK_IP_VER);\n\n\tgoto out;\n\nout_variant_clear:\n\tufshcd_set_variant(hba, NULL);\nout:\n\treturn err;\n}\n\nstatic int ufs_mtk_pre_pwr_change(struct ufs_hba *hba,\n\t\t\t\t  struct ufs_pa_layer_attr *dev_max_params,\n\t\t\t\t  struct ufs_pa_layer_attr *dev_req_params)\n{\n\tstruct ufs_mtk_host *host = ufshcd_get_variant(hba);\n\tstruct ufs_dev_params host_cap;\n\tint ret;\n\n\tufshcd_init_pwr_dev_param(&host_cap);\n\thost_cap.hs_rx_gear = UFS_HS_G4;\n\thost_cap.hs_tx_gear = UFS_HS_G4;\n\n\tret = ufshcd_get_pwr_dev_param(&host_cap,\n\t\t\t\t       dev_max_params,\n\t\t\t\t       dev_req_params);\n\tif (ret) {\n\t\tpr_info(\"%s: failed to determine capabilities\\n\",\n\t\t\t__func__);\n\t}\n\n\tif (host->hw_ver.major >= 3) {\n\t\tret = ufshcd_dme_configure_adapt(hba,\n\t\t\t\t\t   dev_req_params->gear_tx,\n\t\t\t\t\t   PA_INITIAL_ADAPT);\n\t}\n\n\treturn ret;\n}\n\nstatic int ufs_mtk_pwr_change_notify(struct ufs_hba *hba,\n\t\t\t\t     enum ufs_notify_change_status stage,\n\t\t\t\t     struct ufs_pa_layer_attr *dev_max_params,\n\t\t\t\t     struct ufs_pa_layer_attr *dev_req_params)\n{\n\tint ret = 0;\n\n\tswitch (stage) {\n\tcase PRE_CHANGE:\n\t\tret = ufs_mtk_pre_pwr_change(hba, dev_max_params,\n\t\t\t\t\t     dev_req_params);\n\t\tbreak;\n\tcase POST_CHANGE:\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nstatic int ufs_mtk_unipro_set_lpm(struct ufs_hba *hba, bool lpm)\n{\n\tint ret;\n\tstruct ufs_mtk_host *host = ufshcd_get_variant(hba);\n\n\tret = ufshcd_dme_set(hba,\n\t\t\t     UIC_ARG_MIB_SEL(VS_UNIPROPOWERDOWNCONTROL, 0),\n\t\t\t     lpm ? 1 : 0);\n\tif (!ret || !lpm) {\n\t\t/*\n\t\t * Forcibly set as non-LPM mode if UIC commands is failed\n\t\t * to use default hba_enable_delay_us value for re-enabling\n\t\t * the host.\n\t\t */\n\t\thost->unipro_lpm = lpm;\n\t}\n\n\treturn ret;\n}\n\nstatic int ufs_mtk_pre_link(struct ufs_hba *hba)\n{\n\tint ret;\n\tu32 tmp;\n\n\tufs_mtk_get_controller_version(hba);\n\n\tret = ufs_mtk_unipro_set_lpm(hba, false);\n\tif (ret)\n\t\treturn ret;\n\n\t/*\n\t * Setting PA_Local_TX_LCC_Enable to 0 before link startup\n\t * to make sure that both host and device TX LCC are disabled\n\t * once link startup is completed.\n\t */\n\tret = ufshcd_disable_host_tx_lcc(hba);\n\tif (ret)\n\t\treturn ret;\n\n\t/* disable deep stall */\n\tret = ufshcd_dme_get(hba, UIC_ARG_MIB(VS_SAVEPOWERCONTROL), &tmp);\n\tif (ret)\n\t\treturn ret;\n\n\ttmp &= ~(1 << 6);\n\n\tret = ufshcd_dme_set(hba, UIC_ARG_MIB(VS_SAVEPOWERCONTROL), tmp);\n\n\treturn ret;\n}\n\nstatic void ufs_mtk_setup_clk_gating(struct ufs_hba *hba)\n{\n\tunsigned long flags;\n\tu32 ah_ms;\n\n\tif (ufshcd_is_clkgating_allowed(hba)) {\n\t\tif (ufshcd_is_auto_hibern8_supported(hba) && hba->ahit)\n\t\t\tah_ms = FIELD_GET(UFSHCI_AHIBERN8_TIMER_MASK,\n\t\t\t\t\t  hba->ahit);\n\t\telse\n\t\t\tah_ms = 10;\n\t\tspin_lock_irqsave(hba->host->host_lock, flags);\n\t\thba->clk_gating.delay_ms = ah_ms + 5;\n\t\tspin_unlock_irqrestore(hba->host->host_lock, flags);\n\t}\n}\n\nstatic int ufs_mtk_post_link(struct ufs_hba *hba)\n{\n\t/* enable unipro clock gating feature */\n\tufs_mtk_cfg_unipro_cg(hba, true);\n\n\t/* will be configured during probe hba */\n\tif (ufshcd_is_auto_hibern8_supported(hba))\n\t\thba->ahit = FIELD_PREP(UFSHCI_AHIBERN8_TIMER_MASK, 10) |\n\t\t\tFIELD_PREP(UFSHCI_AHIBERN8_SCALE_MASK, 3);\n\n\tufs_mtk_setup_clk_gating(hba);\n\n\treturn 0;\n}\n\nstatic int ufs_mtk_link_startup_notify(struct ufs_hba *hba,\n\t\t\t\t       enum ufs_notify_change_status stage)\n{\n\tint ret = 0;\n\n\tswitch (stage) {\n\tcase PRE_CHANGE:\n\t\tret = ufs_mtk_pre_link(hba);\n\t\tbreak;\n\tcase POST_CHANGE:\n\t\tret = ufs_mtk_post_link(hba);\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nstatic int ufs_mtk_device_reset(struct ufs_hba *hba)\n{\n\tstruct arm_smccc_res res;\n\n\t/* disable hba before device reset */\n\tufshcd_hba_stop(hba);\n\n\tufs_mtk_device_reset_ctrl(0, res);\n\n\t/*\n\t * The reset signal is active low. UFS devices shall detect\n\t * more than or equal to 1us of positive or negative RST_n\n\t * pulse width.\n\t *\n\t * To be on safe side, keep the reset low for at least 10us.\n\t */\n\tusleep_range(10, 15);\n\n\tufs_mtk_device_reset_ctrl(1, res);\n\n\t/* Some devices may need time to respond to rst_n */\n\tusleep_range(10000, 15000);\n\n\tdev_info(hba->dev, \"device reset done\\n\");\n\n\treturn 0;\n}\n\nstatic int ufs_mtk_link_set_hpm(struct ufs_hba *hba)\n{\n\tint err;\n\n\terr = ufshcd_hba_enable(hba);\n\tif (err)\n\t\treturn err;\n\n\terr = ufs_mtk_unipro_set_lpm(hba, false);\n\tif (err)\n\t\treturn err;\n\n\terr = ufshcd_uic_hibern8_exit(hba);\n\tif (!err)\n\t\tufshcd_set_link_active(hba);\n\telse\n\t\treturn err;\n\n\terr = ufshcd_make_hba_operational(hba);\n\tif (err)\n\t\treturn err;\n\n\treturn 0;\n}\n\nstatic int ufs_mtk_link_set_lpm(struct ufs_hba *hba)\n{\n\tint err;\n\n\terr = ufs_mtk_unipro_set_lpm(hba, true);\n\tif (err) {\n\t\t/* Resume UniPro state for following error recovery */\n\t\tufs_mtk_unipro_set_lpm(hba, false);\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic void ufs_mtk_vreg_set_lpm(struct ufs_hba *hba, bool lpm)\n{\n\tif (!hba->vreg_info.vccq2 || !hba->vreg_info.vcc)\n\t\treturn;\n\n\tif (lpm && !hba->vreg_info.vcc->enabled)\n\t\tregulator_set_mode(hba->vreg_info.vccq2->reg,\n\t\t\t\t   REGULATOR_MODE_IDLE);\n\telse if (!lpm)\n\t\tregulator_set_mode(hba->vreg_info.vccq2->reg,\n\t\t\t\t   REGULATOR_MODE_NORMAL);\n}\n\nstatic void ufs_mtk_auto_hibern8_disable(struct ufs_hba *hba)\n{\n\tunsigned long flags;\n\tint ret;\n\n\t/* disable auto-hibern8 */\n\tspin_lock_irqsave(hba->host->host_lock, flags);\n\tufshcd_writel(hba, 0, REG_AUTO_HIBERNATE_IDLE_TIMER);\n\tspin_unlock_irqrestore(hba->host->host_lock, flags);\n\n\t/* wait host return to idle state when auto-hibern8 off */\n\tufs_mtk_wait_idle_state(hba, 5);\n\n\tret = ufs_mtk_wait_link_state(hba, VS_LINK_UP, 100);\n\tif (ret)\n\t\tdev_warn(hba->dev, \"exit h8 state fail, ret=%d\\n\", ret);\n}\n\nstatic int ufs_mtk_suspend(struct ufs_hba *hba, enum ufs_pm_op pm_op,\n\tenum ufs_notify_change_status status)\n{\n\tint err;\n\tstruct arm_smccc_res res;\n\n\tif (status == PRE_CHANGE) {\n\t\tif (!ufshcd_is_auto_hibern8_supported(hba))\n\t\t\treturn 0;\n\t\tufs_mtk_auto_hibern8_disable(hba);\n\t\treturn 0;\n\t}\n\n\tif (ufshcd_is_link_hibern8(hba)) {\n\t\terr = ufs_mtk_link_set_lpm(hba);\n\t\tif (err)\n\t\t\tgoto fail;\n\t}\n\n\tif (!ufshcd_is_link_active(hba)) {\n\t\t/*\n\t\t * Make sure no error will be returned to prevent\n\t\t * ufshcd_suspend() re-enabling regulators while vreg is still\n\t\t * in low-power mode.\n\t\t */\n\t\tufs_mtk_vreg_set_lpm(hba, true);\n\t\terr = ufs_mtk_mphy_power_on(hba, false);\n\t\tif (err)\n\t\t\tgoto fail;\n\t}\n\n\tif (ufshcd_is_link_off(hba))\n\t\tufs_mtk_device_reset_ctrl(0, res);\n\n\treturn 0;\nfail:\n\t/*\n\t * Set link as off state enforcedly to trigger\n\t * ufshcd_host_reset_and_restore() in ufshcd_suspend()\n\t * for completed host reset.\n\t */\n\tufshcd_set_link_off(hba);\n\treturn -EAGAIN;\n}\n\nstatic int ufs_mtk_resume(struct ufs_hba *hba, enum ufs_pm_op pm_op)\n{\n\tint err;\n\n\terr = ufs_mtk_mphy_power_on(hba, true);\n\tif (err)\n\t\tgoto fail;\n\n\tufs_mtk_vreg_set_lpm(hba, false);\n\n\tif (ufshcd_is_link_hibern8(hba)) {\n\t\terr = ufs_mtk_link_set_hpm(hba);\n\t\tif (err)\n\t\t\tgoto fail;\n\t}\n\n\treturn 0;\nfail:\n\treturn ufshcd_link_recovery(hba);\n}\n\nstatic void ufs_mtk_dbg_register_dump(struct ufs_hba *hba)\n{\n\tufshcd_dump_regs(hba, REG_UFS_REFCLK_CTRL, 0x4, \"Ref-Clk Ctrl \");\n\n\tufshcd_dump_regs(hba, REG_UFS_EXTREG, 0x4, \"Ext Reg \");\n\n\tufshcd_dump_regs(hba, REG_UFS_MPHYCTRL,\n\t\t\t REG_UFS_REJECT_MON - REG_UFS_MPHYCTRL + 4,\n\t\t\t \"MPHY Ctrl \");\n\n\t/* Direct debugging information to REG_MTK_PROBE */\n\tufs_mtk_dbg_sel(hba);\n\tufshcd_dump_regs(hba, REG_UFS_PROBE, 0x4, \"Debug Probe \");\n}\n\nstatic int ufs_mtk_apply_dev_quirks(struct ufs_hba *hba)\n{\n\tstruct ufs_dev_info *dev_info = &hba->dev_info;\n\tu16 mid = dev_info->wmanufacturerid;\n\n\tif (mid == UFS_VENDOR_SAMSUNG)\n\t\tufshcd_dme_set(hba, UIC_ARG_MIB(PA_TACTIVATE), 6);\n\n\t/*\n\t * Decide waiting time before gating reference clock and\n\t * after ungating reference clock according to vendors'\n\t * requirements.\n\t */\n\tif (mid == UFS_VENDOR_SAMSUNG)\n\t\tufs_mtk_setup_ref_clk_wait_us(hba, 1);\n\telse if (mid == UFS_VENDOR_SKHYNIX)\n\t\tufs_mtk_setup_ref_clk_wait_us(hba, 30);\n\telse if (mid == UFS_VENDOR_TOSHIBA)\n\t\tufs_mtk_setup_ref_clk_wait_us(hba, 100);\n\telse\n\t\tufs_mtk_setup_ref_clk_wait_us(hba,\n\t\t\t\t\t      REFCLK_DEFAULT_WAIT_US);\n\n\treturn 0;\n}\n\nstatic void ufs_mtk_fixup_dev_quirks(struct ufs_hba *hba)\n{\n\tufshcd_fixup_dev_quirks(hba, ufs_mtk_dev_fixups);\n\n\tif (ufs_mtk_is_broken_vcc(hba) && hba->vreg_info.vcc &&\n\t    (hba->dev_quirks & UFS_DEVICE_QUIRK_DELAY_AFTER_LPM)) {\n\t\thba->vreg_info.vcc->always_on = true;\n\t\t/*\n\t\t * VCC will be kept always-on thus we don't\n\t\t * need any delay during regulator operations\n\t\t */\n\t\thba->dev_quirks &= ~(UFS_DEVICE_QUIRK_DELAY_BEFORE_LPM |\n\t\t\tUFS_DEVICE_QUIRK_DELAY_AFTER_LPM);\n\t}\n}\n\nstatic void ufs_mtk_event_notify(struct ufs_hba *hba,\n\t\t\t\t enum ufs_event_type evt, void *data)\n{\n\tunsigned int val = *(u32 *)data;\n\n\ttrace_ufs_mtk_event(evt, val);\n}\n\n/*\n * struct ufs_hba_mtk_vops - UFS MTK specific variant operations\n *\n * The variant operations configure the necessary controller and PHY\n * handshake during initialization.\n */\nstatic const struct ufs_hba_variant_ops ufs_hba_mtk_vops = {\n\t.name                = \"mediatek.ufshci\",\n\t.init                = ufs_mtk_init,\n\t.get_ufs_hci_version = ufs_mtk_get_ufs_hci_version,\n\t.setup_clocks        = ufs_mtk_setup_clocks,\n\t.hce_enable_notify   = ufs_mtk_hce_enable_notify,\n\t.link_startup_notify = ufs_mtk_link_startup_notify,\n\t.pwr_change_notify   = ufs_mtk_pwr_change_notify,\n\t.apply_dev_quirks    = ufs_mtk_apply_dev_quirks,\n\t.fixup_dev_quirks    = ufs_mtk_fixup_dev_quirks,\n\t.suspend             = ufs_mtk_suspend,\n\t.resume              = ufs_mtk_resume,\n\t.dbg_register_dump   = ufs_mtk_dbg_register_dump,\n\t.device_reset        = ufs_mtk_device_reset,\n\t.event_notify        = ufs_mtk_event_notify,\n};\n\n/**\n * ufs_mtk_probe - probe routine of the driver\n * @pdev: pointer to Platform device handle\n *\n * Return zero for success and non-zero for failure\n */\nstatic int ufs_mtk_probe(struct platform_device *pdev)\n{\n\tint err;\n\tstruct device *dev = &pdev->dev;\n\tstruct device_node *reset_node;\n\tstruct platform_device *reset_pdev;\n\tstruct device_link *link;\n\n\treset_node = of_find_compatible_node(NULL, NULL,\n\t\t\t\t\t     \"ti,syscon-reset\");\n\tif (!reset_node) {\n\t\tdev_notice(dev, \"find ti,syscon-reset fail\\n\");\n\t\tgoto skip_reset;\n\t}\n\treset_pdev = of_find_device_by_node(reset_node);\n\tif (!reset_pdev) {\n\t\tdev_notice(dev, \"find reset_pdev fail\\n\");\n\t\tgoto skip_reset;\n\t}\n\tlink = device_link_add(dev, &reset_pdev->dev,\n\t\tDL_FLAG_AUTOPROBE_CONSUMER);\n\tput_device(&reset_pdev->dev);\n\tif (!link) {\n\t\tdev_notice(dev, \"add reset device_link fail\\n\");\n\t\tgoto skip_reset;\n\t}\n\t/* supplier is not probed */\n\tif (link->status == DL_STATE_DORMANT) {\n\t\terr = -EPROBE_DEFER;\n\t\tgoto out;\n\t}\n\nskip_reset:\n\t/* perform generic probe */\n\terr = ufshcd_pltfrm_init(pdev, &ufs_hba_mtk_vops);\n\nout:\n\tif (err)\n\t\tdev_info(dev, \"probe failed %d\\n\", err);\n\n\tof_node_put(reset_node);\n\treturn err;\n}\n\n/**\n * ufs_mtk_remove - set driver_data of the device to NULL\n * @pdev: pointer to platform device handle\n *\n * Always return 0\n */\nstatic int ufs_mtk_remove(struct platform_device *pdev)\n{\n\tstruct ufs_hba *hba =  platform_get_drvdata(pdev);\n\n\tpm_runtime_get_sync(&(pdev)->dev);\n\tufshcd_remove(hba);\n\treturn 0;\n}\n\nstatic const struct dev_pm_ops ufs_mtk_pm_ops = {\n\tSET_SYSTEM_SLEEP_PM_OPS(ufshcd_system_suspend, ufshcd_system_resume)\n\tSET_RUNTIME_PM_OPS(ufshcd_runtime_suspend, ufshcd_runtime_resume, NULL)\n\t.prepare\t = ufshcd_suspend_prepare,\n\t.complete\t = ufshcd_resume_complete,\n};\n\nstatic struct platform_driver ufs_mtk_pltform = {\n\t.probe      = ufs_mtk_probe,\n\t.remove     = ufs_mtk_remove,\n\t.shutdown   = ufshcd_pltfrm_shutdown,\n\t.driver = {\n\t\t.name   = \"ufshcd-mtk\",\n\t\t.pm     = &ufs_mtk_pm_ops,\n\t\t.of_match_table = ufs_mtk_of_match,\n\t},\n};\n\nMODULE_AUTHOR(\"Stanley Chu <stanley.chu@mediatek.com>\");\nMODULE_AUTHOR(\"Peter Wang <peter.wang@mediatek.com>\");\nMODULE_DESCRIPTION(\"MediaTek UFS Host Driver\");\nMODULE_LICENSE(\"GPL v2\");\n\nmodule_platform_driver(ufs_mtk_pltform);\n"], "buggy_code_start_loc": [560], "buggy_code_end_loc": [561], "fixing_code_start_loc": [560], "fixing_code_end_loc": [561], "type": "CWE-476", "message": "In the Linux kernel before 5.16.3, drivers/scsi/ufs/ufs-mediatek.c misinterprets the regulator_get return value (expects it to be NULL in the error case, whereas it is actually an error pointer).", "other": {"cve": {"id": "CVE-2023-23001", "sourceIdentifier": "cve@mitre.org", "published": "2023-03-01T20:15:13.270", "lastModified": "2023-03-13T14:28:03.540", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "In the Linux kernel before 5.16.3, drivers/scsi/ufs/ufs-mediatek.c misinterprets the regulator_get return value (expects it to be NULL in the error case, whereas it is actually an error pointer)."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-476"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "5.16.3", "matchCriteriaId": "6FBCF385-C05A-44CC-AF30-EDA38785D4BD"}]}]}], "references": [{"url": "https://cdn.kernel.org/pub/linux/kernel/v5.x/ChangeLog-5.16.3", "source": "cve@mitre.org", "tags": ["Mailing List", "Patch", "Release Notes"]}, {"url": "https://github.com/torvalds/linux/commit/3ba880a12df5aa4488c18281701b5b1bc3d4531a", "source": "cve@mitre.org", "tags": ["Patch"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/3ba880a12df5aa4488c18281701b5b1bc3d4531a"}}