{"buggy_code": ["/*\n  FUSE: Filesystem in Userspace\n  Copyright (C) 2001-2008  Miklos Szeredi <miklos@szeredi.hu>\n\n  This program can be distributed under the terms of the GNU GPL.\n  See the file COPYING.\n*/\n\n#include \"fuse_i.h\"\n\n#include <linux/init.h>\n#include <linux/module.h>\n#include <linux/poll.h>\n#include <linux/uio.h>\n#include <linux/miscdevice.h>\n#include <linux/pagemap.h>\n#include <linux/file.h>\n#include <linux/slab.h>\n#include <linux/pipe_fs_i.h>\n#include <linux/swap.h>\n#include <linux/splice.h>\n\nMODULE_ALIAS_MISCDEV(FUSE_MINOR);\nMODULE_ALIAS(\"devname:fuse\");\n\nstatic struct kmem_cache *fuse_req_cachep;\n\nstatic struct fuse_conn *fuse_get_conn(struct file *file)\n{\n\t/*\n\t * Lockless access is OK, because file->private data is set\n\t * once during mount and is valid until the file is released.\n\t */\n\treturn file->private_data;\n}\n\nstatic void fuse_request_init(struct fuse_req *req)\n{\n\tmemset(req, 0, sizeof(*req));\n\tINIT_LIST_HEAD(&req->list);\n\tINIT_LIST_HEAD(&req->intr_entry);\n\tinit_waitqueue_head(&req->waitq);\n\tatomic_set(&req->count, 1);\n}\n\nstruct fuse_req *fuse_request_alloc(void)\n{\n\tstruct fuse_req *req = kmem_cache_alloc(fuse_req_cachep, GFP_KERNEL);\n\tif (req)\n\t\tfuse_request_init(req);\n\treturn req;\n}\nEXPORT_SYMBOL_GPL(fuse_request_alloc);\n\nstruct fuse_req *fuse_request_alloc_nofs(void)\n{\n\tstruct fuse_req *req = kmem_cache_alloc(fuse_req_cachep, GFP_NOFS);\n\tif (req)\n\t\tfuse_request_init(req);\n\treturn req;\n}\n\nvoid fuse_request_free(struct fuse_req *req)\n{\n\tkmem_cache_free(fuse_req_cachep, req);\n}\n\nstatic void block_sigs(sigset_t *oldset)\n{\n\tsigset_t mask;\n\n\tsiginitsetinv(&mask, sigmask(SIGKILL));\n\tsigprocmask(SIG_BLOCK, &mask, oldset);\n}\n\nstatic void restore_sigs(sigset_t *oldset)\n{\n\tsigprocmask(SIG_SETMASK, oldset, NULL);\n}\n\nstatic void __fuse_get_request(struct fuse_req *req)\n{\n\tatomic_inc(&req->count);\n}\n\n/* Must be called with > 1 refcount */\nstatic void __fuse_put_request(struct fuse_req *req)\n{\n\tBUG_ON(atomic_read(&req->count) < 2);\n\tatomic_dec(&req->count);\n}\n\nstatic void fuse_req_init_context(struct fuse_req *req)\n{\n\treq->in.h.uid = current_fsuid();\n\treq->in.h.gid = current_fsgid();\n\treq->in.h.pid = current->pid;\n}\n\nstruct fuse_req *fuse_get_req(struct fuse_conn *fc)\n{\n\tstruct fuse_req *req;\n\tsigset_t oldset;\n\tint intr;\n\tint err;\n\n\tatomic_inc(&fc->num_waiting);\n\tblock_sigs(&oldset);\n\tintr = wait_event_interruptible(fc->blocked_waitq, !fc->blocked);\n\trestore_sigs(&oldset);\n\terr = -EINTR;\n\tif (intr)\n\t\tgoto out;\n\n\terr = -ENOTCONN;\n\tif (!fc->connected)\n\t\tgoto out;\n\n\treq = fuse_request_alloc();\n\terr = -ENOMEM;\n\tif (!req)\n\t\tgoto out;\n\n\tfuse_req_init_context(req);\n\treq->waiting = 1;\n\treturn req;\n\n out:\n\tatomic_dec(&fc->num_waiting);\n\treturn ERR_PTR(err);\n}\nEXPORT_SYMBOL_GPL(fuse_get_req);\n\n/*\n * Return request in fuse_file->reserved_req.  However that may\n * currently be in use.  If that is the case, wait for it to become\n * available.\n */\nstatic struct fuse_req *get_reserved_req(struct fuse_conn *fc,\n\t\t\t\t\t struct file *file)\n{\n\tstruct fuse_req *req = NULL;\n\tstruct fuse_file *ff = file->private_data;\n\n\tdo {\n\t\twait_event(fc->reserved_req_waitq, ff->reserved_req);\n\t\tspin_lock(&fc->lock);\n\t\tif (ff->reserved_req) {\n\t\t\treq = ff->reserved_req;\n\t\t\tff->reserved_req = NULL;\n\t\t\tget_file(file);\n\t\t\treq->stolen_file = file;\n\t\t}\n\t\tspin_unlock(&fc->lock);\n\t} while (!req);\n\n\treturn req;\n}\n\n/*\n * Put stolen request back into fuse_file->reserved_req\n */\nstatic void put_reserved_req(struct fuse_conn *fc, struct fuse_req *req)\n{\n\tstruct file *file = req->stolen_file;\n\tstruct fuse_file *ff = file->private_data;\n\n\tspin_lock(&fc->lock);\n\tfuse_request_init(req);\n\tBUG_ON(ff->reserved_req);\n\tff->reserved_req = req;\n\twake_up_all(&fc->reserved_req_waitq);\n\tspin_unlock(&fc->lock);\n\tfput(file);\n}\n\n/*\n * Gets a requests for a file operation, always succeeds\n *\n * This is used for sending the FLUSH request, which must get to\n * userspace, due to POSIX locks which may need to be unlocked.\n *\n * If allocation fails due to OOM, use the reserved request in\n * fuse_file.\n *\n * This is very unlikely to deadlock accidentally, since the\n * filesystem should not have it's own file open.  If deadlock is\n * intentional, it can still be broken by \"aborting\" the filesystem.\n */\nstruct fuse_req *fuse_get_req_nofail(struct fuse_conn *fc, struct file *file)\n{\n\tstruct fuse_req *req;\n\n\tatomic_inc(&fc->num_waiting);\n\twait_event(fc->blocked_waitq, !fc->blocked);\n\treq = fuse_request_alloc();\n\tif (!req)\n\t\treq = get_reserved_req(fc, file);\n\n\tfuse_req_init_context(req);\n\treq->waiting = 1;\n\treturn req;\n}\n\nvoid fuse_put_request(struct fuse_conn *fc, struct fuse_req *req)\n{\n\tif (atomic_dec_and_test(&req->count)) {\n\t\tif (req->waiting)\n\t\t\tatomic_dec(&fc->num_waiting);\n\n\t\tif (req->stolen_file)\n\t\t\tput_reserved_req(fc, req);\n\t\telse\n\t\t\tfuse_request_free(req);\n\t}\n}\nEXPORT_SYMBOL_GPL(fuse_put_request);\n\nstatic unsigned len_args(unsigned numargs, struct fuse_arg *args)\n{\n\tunsigned nbytes = 0;\n\tunsigned i;\n\n\tfor (i = 0; i < numargs; i++)\n\t\tnbytes += args[i].size;\n\n\treturn nbytes;\n}\n\nstatic u64 fuse_get_unique(struct fuse_conn *fc)\n{\n\tfc->reqctr++;\n\t/* zero is special */\n\tif (fc->reqctr == 0)\n\t\tfc->reqctr = 1;\n\n\treturn fc->reqctr;\n}\n\nstatic void queue_request(struct fuse_conn *fc, struct fuse_req *req)\n{\n\treq->in.h.len = sizeof(struct fuse_in_header) +\n\t\tlen_args(req->in.numargs, (struct fuse_arg *) req->in.args);\n\tlist_add_tail(&req->list, &fc->pending);\n\treq->state = FUSE_REQ_PENDING;\n\tif (!req->waiting) {\n\t\treq->waiting = 1;\n\t\tatomic_inc(&fc->num_waiting);\n\t}\n\twake_up(&fc->waitq);\n\tkill_fasync(&fc->fasync, SIGIO, POLL_IN);\n}\n\nvoid fuse_queue_forget(struct fuse_conn *fc, struct fuse_forget_link *forget,\n\t\t       u64 nodeid, u64 nlookup)\n{\n\tforget->forget_one.nodeid = nodeid;\n\tforget->forget_one.nlookup = nlookup;\n\n\tspin_lock(&fc->lock);\n\tfc->forget_list_tail->next = forget;\n\tfc->forget_list_tail = forget;\n\twake_up(&fc->waitq);\n\tkill_fasync(&fc->fasync, SIGIO, POLL_IN);\n\tspin_unlock(&fc->lock);\n}\n\nstatic void flush_bg_queue(struct fuse_conn *fc)\n{\n\twhile (fc->active_background < fc->max_background &&\n\t       !list_empty(&fc->bg_queue)) {\n\t\tstruct fuse_req *req;\n\n\t\treq = list_entry(fc->bg_queue.next, struct fuse_req, list);\n\t\tlist_del(&req->list);\n\t\tfc->active_background++;\n\t\treq->in.h.unique = fuse_get_unique(fc);\n\t\tqueue_request(fc, req);\n\t}\n}\n\n/*\n * This function is called when a request is finished.  Either a reply\n * has arrived or it was aborted (and not yet sent) or some error\n * occurred during communication with userspace, or the device file\n * was closed.  The requester thread is woken up (if still waiting),\n * the 'end' callback is called if given, else the reference to the\n * request is released\n *\n * Called with fc->lock, unlocks it\n */\nstatic void request_end(struct fuse_conn *fc, struct fuse_req *req)\n__releases(fc->lock)\n{\n\tvoid (*end) (struct fuse_conn *, struct fuse_req *) = req->end;\n\treq->end = NULL;\n\tlist_del(&req->list);\n\tlist_del(&req->intr_entry);\n\treq->state = FUSE_REQ_FINISHED;\n\tif (req->background) {\n\t\tif (fc->num_background == fc->max_background) {\n\t\t\tfc->blocked = 0;\n\t\t\twake_up_all(&fc->blocked_waitq);\n\t\t}\n\t\tif (fc->num_background == fc->congestion_threshold &&\n\t\t    fc->connected && fc->bdi_initialized) {\n\t\t\tclear_bdi_congested(&fc->bdi, BLK_RW_SYNC);\n\t\t\tclear_bdi_congested(&fc->bdi, BLK_RW_ASYNC);\n\t\t}\n\t\tfc->num_background--;\n\t\tfc->active_background--;\n\t\tflush_bg_queue(fc);\n\t}\n\tspin_unlock(&fc->lock);\n\twake_up(&req->waitq);\n\tif (end)\n\t\tend(fc, req);\n\tfuse_put_request(fc, req);\n}\n\nstatic void wait_answer_interruptible(struct fuse_conn *fc,\n\t\t\t\t      struct fuse_req *req)\n__releases(fc->lock)\n__acquires(fc->lock)\n{\n\tif (signal_pending(current))\n\t\treturn;\n\n\tspin_unlock(&fc->lock);\n\twait_event_interruptible(req->waitq, req->state == FUSE_REQ_FINISHED);\n\tspin_lock(&fc->lock);\n}\n\nstatic void queue_interrupt(struct fuse_conn *fc, struct fuse_req *req)\n{\n\tlist_add_tail(&req->intr_entry, &fc->interrupts);\n\twake_up(&fc->waitq);\n\tkill_fasync(&fc->fasync, SIGIO, POLL_IN);\n}\n\nstatic void request_wait_answer(struct fuse_conn *fc, struct fuse_req *req)\n__releases(fc->lock)\n__acquires(fc->lock)\n{\n\tif (!fc->no_interrupt) {\n\t\t/* Any signal may interrupt this */\n\t\twait_answer_interruptible(fc, req);\n\n\t\tif (req->aborted)\n\t\t\tgoto aborted;\n\t\tif (req->state == FUSE_REQ_FINISHED)\n\t\t\treturn;\n\n\t\treq->interrupted = 1;\n\t\tif (req->state == FUSE_REQ_SENT)\n\t\t\tqueue_interrupt(fc, req);\n\t}\n\n\tif (!req->force) {\n\t\tsigset_t oldset;\n\n\t\t/* Only fatal signals may interrupt this */\n\t\tblock_sigs(&oldset);\n\t\twait_answer_interruptible(fc, req);\n\t\trestore_sigs(&oldset);\n\n\t\tif (req->aborted)\n\t\t\tgoto aborted;\n\t\tif (req->state == FUSE_REQ_FINISHED)\n\t\t\treturn;\n\n\t\t/* Request is not yet in userspace, bail out */\n\t\tif (req->state == FUSE_REQ_PENDING) {\n\t\t\tlist_del(&req->list);\n\t\t\t__fuse_put_request(req);\n\t\t\treq->out.h.error = -EINTR;\n\t\t\treturn;\n\t\t}\n\t}\n\n\t/*\n\t * Either request is already in userspace, or it was forced.\n\t * Wait it out.\n\t */\n\tspin_unlock(&fc->lock);\n\twait_event(req->waitq, req->state == FUSE_REQ_FINISHED);\n\tspin_lock(&fc->lock);\n\n\tif (!req->aborted)\n\t\treturn;\n\n aborted:\n\tBUG_ON(req->state != FUSE_REQ_FINISHED);\n\tif (req->locked) {\n\t\t/* This is uninterruptible sleep, because data is\n\t\t   being copied to/from the buffers of req.  During\n\t\t   locked state, there mustn't be any filesystem\n\t\t   operation (e.g. page fault), since that could lead\n\t\t   to deadlock */\n\t\tspin_unlock(&fc->lock);\n\t\twait_event(req->waitq, !req->locked);\n\t\tspin_lock(&fc->lock);\n\t}\n}\n\nvoid fuse_request_send(struct fuse_conn *fc, struct fuse_req *req)\n{\n\treq->isreply = 1;\n\tspin_lock(&fc->lock);\n\tif (!fc->connected)\n\t\treq->out.h.error = -ENOTCONN;\n\telse if (fc->conn_error)\n\t\treq->out.h.error = -ECONNREFUSED;\n\telse {\n\t\treq->in.h.unique = fuse_get_unique(fc);\n\t\tqueue_request(fc, req);\n\t\t/* acquire extra reference, since request is still needed\n\t\t   after request_end() */\n\t\t__fuse_get_request(req);\n\n\t\trequest_wait_answer(fc, req);\n\t}\n\tspin_unlock(&fc->lock);\n}\nEXPORT_SYMBOL_GPL(fuse_request_send);\n\nstatic void fuse_request_send_nowait_locked(struct fuse_conn *fc,\n\t\t\t\t\t    struct fuse_req *req)\n{\n\treq->background = 1;\n\tfc->num_background++;\n\tif (fc->num_background == fc->max_background)\n\t\tfc->blocked = 1;\n\tif (fc->num_background == fc->congestion_threshold &&\n\t    fc->bdi_initialized) {\n\t\tset_bdi_congested(&fc->bdi, BLK_RW_SYNC);\n\t\tset_bdi_congested(&fc->bdi, BLK_RW_ASYNC);\n\t}\n\tlist_add_tail(&req->list, &fc->bg_queue);\n\tflush_bg_queue(fc);\n}\n\nstatic void fuse_request_send_nowait(struct fuse_conn *fc, struct fuse_req *req)\n{\n\tspin_lock(&fc->lock);\n\tif (fc->connected) {\n\t\tfuse_request_send_nowait_locked(fc, req);\n\t\tspin_unlock(&fc->lock);\n\t} else {\n\t\treq->out.h.error = -ENOTCONN;\n\t\trequest_end(fc, req);\n\t}\n}\n\nvoid fuse_request_send_background(struct fuse_conn *fc, struct fuse_req *req)\n{\n\treq->isreply = 1;\n\tfuse_request_send_nowait(fc, req);\n}\nEXPORT_SYMBOL_GPL(fuse_request_send_background);\n\nstatic int fuse_request_send_notify_reply(struct fuse_conn *fc,\n\t\t\t\t\t  struct fuse_req *req, u64 unique)\n{\n\tint err = -ENODEV;\n\n\treq->isreply = 0;\n\treq->in.h.unique = unique;\n\tspin_lock(&fc->lock);\n\tif (fc->connected) {\n\t\tqueue_request(fc, req);\n\t\terr = 0;\n\t}\n\tspin_unlock(&fc->lock);\n\n\treturn err;\n}\n\n/*\n * Called under fc->lock\n *\n * fc->connected must have been checked previously\n */\nvoid fuse_request_send_background_locked(struct fuse_conn *fc,\n\t\t\t\t\t struct fuse_req *req)\n{\n\treq->isreply = 1;\n\tfuse_request_send_nowait_locked(fc, req);\n}\n\n/*\n * Lock the request.  Up to the next unlock_request() there mustn't be\n * anything that could cause a page-fault.  If the request was already\n * aborted bail out.\n */\nstatic int lock_request(struct fuse_conn *fc, struct fuse_req *req)\n{\n\tint err = 0;\n\tif (req) {\n\t\tspin_lock(&fc->lock);\n\t\tif (req->aborted)\n\t\t\terr = -ENOENT;\n\t\telse\n\t\t\treq->locked = 1;\n\t\tspin_unlock(&fc->lock);\n\t}\n\treturn err;\n}\n\n/*\n * Unlock request.  If it was aborted during being locked, the\n * requester thread is currently waiting for it to be unlocked, so\n * wake it up.\n */\nstatic void unlock_request(struct fuse_conn *fc, struct fuse_req *req)\n{\n\tif (req) {\n\t\tspin_lock(&fc->lock);\n\t\treq->locked = 0;\n\t\tif (req->aborted)\n\t\t\twake_up(&req->waitq);\n\t\tspin_unlock(&fc->lock);\n\t}\n}\n\nstruct fuse_copy_state {\n\tstruct fuse_conn *fc;\n\tint write;\n\tstruct fuse_req *req;\n\tconst struct iovec *iov;\n\tstruct pipe_buffer *pipebufs;\n\tstruct pipe_buffer *currbuf;\n\tstruct pipe_inode_info *pipe;\n\tunsigned long nr_segs;\n\tunsigned long seglen;\n\tunsigned long addr;\n\tstruct page *pg;\n\tvoid *mapaddr;\n\tvoid *buf;\n\tunsigned len;\n\tunsigned move_pages:1;\n};\n\nstatic void fuse_copy_init(struct fuse_copy_state *cs, struct fuse_conn *fc,\n\t\t\t   int write,\n\t\t\t   const struct iovec *iov, unsigned long nr_segs)\n{\n\tmemset(cs, 0, sizeof(*cs));\n\tcs->fc = fc;\n\tcs->write = write;\n\tcs->iov = iov;\n\tcs->nr_segs = nr_segs;\n}\n\n/* Unmap and put previous page of userspace buffer */\nstatic void fuse_copy_finish(struct fuse_copy_state *cs)\n{\n\tif (cs->currbuf) {\n\t\tstruct pipe_buffer *buf = cs->currbuf;\n\n\t\tif (!cs->write) {\n\t\t\tbuf->ops->unmap(cs->pipe, buf, cs->mapaddr);\n\t\t} else {\n\t\t\tkunmap(buf->page);\n\t\t\tbuf->len = PAGE_SIZE - cs->len;\n\t\t}\n\t\tcs->currbuf = NULL;\n\t\tcs->mapaddr = NULL;\n\t} else if (cs->mapaddr) {\n\t\tkunmap(cs->pg);\n\t\tif (cs->write) {\n\t\t\tflush_dcache_page(cs->pg);\n\t\t\tset_page_dirty_lock(cs->pg);\n\t\t}\n\t\tput_page(cs->pg);\n\t\tcs->mapaddr = NULL;\n\t}\n}\n\n/*\n * Get another pagefull of userspace buffer, and map it to kernel\n * address space, and lock request\n */\nstatic int fuse_copy_fill(struct fuse_copy_state *cs)\n{\n\tunsigned long offset;\n\tint err;\n\n\tunlock_request(cs->fc, cs->req);\n\tfuse_copy_finish(cs);\n\tif (cs->pipebufs) {\n\t\tstruct pipe_buffer *buf = cs->pipebufs;\n\n\t\tif (!cs->write) {\n\t\t\terr = buf->ops->confirm(cs->pipe, buf);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tBUG_ON(!cs->nr_segs);\n\t\t\tcs->currbuf = buf;\n\t\t\tcs->mapaddr = buf->ops->map(cs->pipe, buf, 0);\n\t\t\tcs->len = buf->len;\n\t\t\tcs->buf = cs->mapaddr + buf->offset;\n\t\t\tcs->pipebufs++;\n\t\t\tcs->nr_segs--;\n\t\t} else {\n\t\t\tstruct page *page;\n\n\t\t\tif (cs->nr_segs == cs->pipe->buffers)\n\t\t\t\treturn -EIO;\n\n\t\t\tpage = alloc_page(GFP_HIGHUSER);\n\t\t\tif (!page)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tbuf->page = page;\n\t\t\tbuf->offset = 0;\n\t\t\tbuf->len = 0;\n\n\t\t\tcs->currbuf = buf;\n\t\t\tcs->mapaddr = kmap(page);\n\t\t\tcs->buf = cs->mapaddr;\n\t\t\tcs->len = PAGE_SIZE;\n\t\t\tcs->pipebufs++;\n\t\t\tcs->nr_segs++;\n\t\t}\n\t} else {\n\t\tif (!cs->seglen) {\n\t\t\tBUG_ON(!cs->nr_segs);\n\t\t\tcs->seglen = cs->iov[0].iov_len;\n\t\t\tcs->addr = (unsigned long) cs->iov[0].iov_base;\n\t\t\tcs->iov++;\n\t\t\tcs->nr_segs--;\n\t\t}\n\t\terr = get_user_pages_fast(cs->addr, 1, cs->write, &cs->pg);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t\tBUG_ON(err != 1);\n\t\toffset = cs->addr % PAGE_SIZE;\n\t\tcs->mapaddr = kmap(cs->pg);\n\t\tcs->buf = cs->mapaddr + offset;\n\t\tcs->len = min(PAGE_SIZE - offset, cs->seglen);\n\t\tcs->seglen -= cs->len;\n\t\tcs->addr += cs->len;\n\t}\n\n\treturn lock_request(cs->fc, cs->req);\n}\n\n/* Do as much copy to/from userspace buffer as we can */\nstatic int fuse_copy_do(struct fuse_copy_state *cs, void **val, unsigned *size)\n{\n\tunsigned ncpy = min(*size, cs->len);\n\tif (val) {\n\t\tif (cs->write)\n\t\t\tmemcpy(cs->buf, *val, ncpy);\n\t\telse\n\t\t\tmemcpy(*val, cs->buf, ncpy);\n\t\t*val += ncpy;\n\t}\n\t*size -= ncpy;\n\tcs->len -= ncpy;\n\tcs->buf += ncpy;\n\treturn ncpy;\n}\n\nstatic int fuse_check_page(struct page *page)\n{\n\tif (page_mapcount(page) ||\n\t    page->mapping != NULL ||\n\t    page_count(page) != 1 ||\n\t    (page->flags & PAGE_FLAGS_CHECK_AT_PREP &\n\t     ~(1 << PG_locked |\n\t       1 << PG_referenced |\n\t       1 << PG_uptodate |\n\t       1 << PG_lru |\n\t       1 << PG_active |\n\t       1 << PG_reclaim))) {\n\t\tprintk(KERN_WARNING \"fuse: trying to steal weird page\\n\");\n\t\tprintk(KERN_WARNING \"  page=%p index=%li flags=%08lx, count=%i, mapcount=%i, mapping=%p\\n\", page, page->index, page->flags, page_count(page), page_mapcount(page), page->mapping);\n\t\treturn 1;\n\t}\n\treturn 0;\n}\n\nstatic int fuse_try_move_page(struct fuse_copy_state *cs, struct page **pagep)\n{\n\tint err;\n\tstruct page *oldpage = *pagep;\n\tstruct page *newpage;\n\tstruct pipe_buffer *buf = cs->pipebufs;\n\tstruct address_space *mapping;\n\tpgoff_t index;\n\n\tunlock_request(cs->fc, cs->req);\n\tfuse_copy_finish(cs);\n\n\terr = buf->ops->confirm(cs->pipe, buf);\n\tif (err)\n\t\treturn err;\n\n\tBUG_ON(!cs->nr_segs);\n\tcs->currbuf = buf;\n\tcs->len = buf->len;\n\tcs->pipebufs++;\n\tcs->nr_segs--;\n\n\tif (cs->len != PAGE_SIZE)\n\t\tgoto out_fallback;\n\n\tif (buf->ops->steal(cs->pipe, buf) != 0)\n\t\tgoto out_fallback;\n\n\tnewpage = buf->page;\n\n\tif (WARN_ON(!PageUptodate(newpage)))\n\t\treturn -EIO;\n\n\tClearPageMappedToDisk(newpage);\n\n\tif (fuse_check_page(newpage) != 0)\n\t\tgoto out_fallback_unlock;\n\n\tmapping = oldpage->mapping;\n\tindex = oldpage->index;\n\n\t/*\n\t * This is a new and locked page, it shouldn't be mapped or\n\t * have any special flags on it\n\t */\n\tif (WARN_ON(page_mapped(oldpage)))\n\t\tgoto out_fallback_unlock;\n\tif (WARN_ON(page_has_private(oldpage)))\n\t\tgoto out_fallback_unlock;\n\tif (WARN_ON(PageDirty(oldpage) || PageWriteback(oldpage)))\n\t\tgoto out_fallback_unlock;\n\tif (WARN_ON(PageMlocked(oldpage)))\n\t\tgoto out_fallback_unlock;\n\n\terr = replace_page_cache_page(oldpage, newpage, GFP_KERNEL);\n\tif (err) {\n\t\tunlock_page(newpage);\n\t\treturn err;\n\t}\n\n\tpage_cache_get(newpage);\n\n\tif (!(buf->flags & PIPE_BUF_FLAG_LRU))\n\t\tlru_cache_add_file(newpage);\n\n\terr = 0;\n\tspin_lock(&cs->fc->lock);\n\tif (cs->req->aborted)\n\t\terr = -ENOENT;\n\telse\n\t\t*pagep = newpage;\n\tspin_unlock(&cs->fc->lock);\n\n\tif (err) {\n\t\tunlock_page(newpage);\n\t\tpage_cache_release(newpage);\n\t\treturn err;\n\t}\n\n\tunlock_page(oldpage);\n\tpage_cache_release(oldpage);\n\tcs->len = 0;\n\n\treturn 0;\n\nout_fallback_unlock:\n\tunlock_page(newpage);\nout_fallback:\n\tcs->mapaddr = buf->ops->map(cs->pipe, buf, 1);\n\tcs->buf = cs->mapaddr + buf->offset;\n\n\terr = lock_request(cs->fc, cs->req);\n\tif (err)\n\t\treturn err;\n\n\treturn 1;\n}\n\nstatic int fuse_ref_page(struct fuse_copy_state *cs, struct page *page,\n\t\t\t unsigned offset, unsigned count)\n{\n\tstruct pipe_buffer *buf;\n\n\tif (cs->nr_segs == cs->pipe->buffers)\n\t\treturn -EIO;\n\n\tunlock_request(cs->fc, cs->req);\n\tfuse_copy_finish(cs);\n\n\tbuf = cs->pipebufs;\n\tpage_cache_get(page);\n\tbuf->page = page;\n\tbuf->offset = offset;\n\tbuf->len = count;\n\n\tcs->pipebufs++;\n\tcs->nr_segs++;\n\tcs->len = 0;\n\n\treturn 0;\n}\n\n/*\n * Copy a page in the request to/from the userspace buffer.  Must be\n * done atomically\n */\nstatic int fuse_copy_page(struct fuse_copy_state *cs, struct page **pagep,\n\t\t\t  unsigned offset, unsigned count, int zeroing)\n{\n\tint err;\n\tstruct page *page = *pagep;\n\n\tif (page && zeroing && count < PAGE_SIZE)\n\t\tclear_highpage(page);\n\n\twhile (count) {\n\t\tif (cs->write && cs->pipebufs && page) {\n\t\t\treturn fuse_ref_page(cs, page, offset, count);\n\t\t} else if (!cs->len) {\n\t\t\tif (cs->move_pages && page &&\n\t\t\t    offset == 0 && count == PAGE_SIZE) {\n\t\t\t\terr = fuse_try_move_page(cs, pagep);\n\t\t\t\tif (err <= 0)\n\t\t\t\t\treturn err;\n\t\t\t} else {\n\t\t\t\terr = fuse_copy_fill(cs);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\t\tif (page) {\n\t\t\tvoid *mapaddr = kmap_atomic(page, KM_USER0);\n\t\t\tvoid *buf = mapaddr + offset;\n\t\t\toffset += fuse_copy_do(cs, &buf, &count);\n\t\t\tkunmap_atomic(mapaddr, KM_USER0);\n\t\t} else\n\t\t\toffset += fuse_copy_do(cs, NULL, &count);\n\t}\n\tif (page && !cs->write)\n\t\tflush_dcache_page(page);\n\treturn 0;\n}\n\n/* Copy pages in the request to/from userspace buffer */\nstatic int fuse_copy_pages(struct fuse_copy_state *cs, unsigned nbytes,\n\t\t\t   int zeroing)\n{\n\tunsigned i;\n\tstruct fuse_req *req = cs->req;\n\tunsigned offset = req->page_offset;\n\tunsigned count = min(nbytes, (unsigned) PAGE_SIZE - offset);\n\n\tfor (i = 0; i < req->num_pages && (nbytes || zeroing); i++) {\n\t\tint err;\n\n\t\terr = fuse_copy_page(cs, &req->pages[i], offset, count,\n\t\t\t\t     zeroing);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tnbytes -= count;\n\t\tcount = min(nbytes, (unsigned) PAGE_SIZE);\n\t\toffset = 0;\n\t}\n\treturn 0;\n}\n\n/* Copy a single argument in the request to/from userspace buffer */\nstatic int fuse_copy_one(struct fuse_copy_state *cs, void *val, unsigned size)\n{\n\twhile (size) {\n\t\tif (!cs->len) {\n\t\t\tint err = fuse_copy_fill(cs);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\t\tfuse_copy_do(cs, &val, &size);\n\t}\n\treturn 0;\n}\n\n/* Copy request arguments to/from userspace buffer */\nstatic int fuse_copy_args(struct fuse_copy_state *cs, unsigned numargs,\n\t\t\t  unsigned argpages, struct fuse_arg *args,\n\t\t\t  int zeroing)\n{\n\tint err = 0;\n\tunsigned i;\n\n\tfor (i = 0; !err && i < numargs; i++)  {\n\t\tstruct fuse_arg *arg = &args[i];\n\t\tif (i == numargs - 1 && argpages)\n\t\t\terr = fuse_copy_pages(cs, arg->size, zeroing);\n\t\telse\n\t\t\terr = fuse_copy_one(cs, arg->value, arg->size);\n\t}\n\treturn err;\n}\n\nstatic int forget_pending(struct fuse_conn *fc)\n{\n\treturn fc->forget_list_head.next != NULL;\n}\n\nstatic int request_pending(struct fuse_conn *fc)\n{\n\treturn !list_empty(&fc->pending) || !list_empty(&fc->interrupts) ||\n\t\tforget_pending(fc);\n}\n\n/* Wait until a request is available on the pending list */\nstatic void request_wait(struct fuse_conn *fc)\n__releases(fc->lock)\n__acquires(fc->lock)\n{\n\tDECLARE_WAITQUEUE(wait, current);\n\n\tadd_wait_queue_exclusive(&fc->waitq, &wait);\n\twhile (fc->connected && !request_pending(fc)) {\n\t\tset_current_state(TASK_INTERRUPTIBLE);\n\t\tif (signal_pending(current))\n\t\t\tbreak;\n\n\t\tspin_unlock(&fc->lock);\n\t\tschedule();\n\t\tspin_lock(&fc->lock);\n\t}\n\tset_current_state(TASK_RUNNING);\n\tremove_wait_queue(&fc->waitq, &wait);\n}\n\n/*\n * Transfer an interrupt request to userspace\n *\n * Unlike other requests this is assembled on demand, without a need\n * to allocate a separate fuse_req structure.\n *\n * Called with fc->lock held, releases it\n */\nstatic int fuse_read_interrupt(struct fuse_conn *fc, struct fuse_copy_state *cs,\n\t\t\t       size_t nbytes, struct fuse_req *req)\n__releases(fc->lock)\n{\n\tstruct fuse_in_header ih;\n\tstruct fuse_interrupt_in arg;\n\tunsigned reqsize = sizeof(ih) + sizeof(arg);\n\tint err;\n\n\tlist_del_init(&req->intr_entry);\n\treq->intr_unique = fuse_get_unique(fc);\n\tmemset(&ih, 0, sizeof(ih));\n\tmemset(&arg, 0, sizeof(arg));\n\tih.len = reqsize;\n\tih.opcode = FUSE_INTERRUPT;\n\tih.unique = req->intr_unique;\n\targ.unique = req->in.h.unique;\n\n\tspin_unlock(&fc->lock);\n\tif (nbytes < reqsize)\n\t\treturn -EINVAL;\n\n\terr = fuse_copy_one(cs, &ih, sizeof(ih));\n\tif (!err)\n\t\terr = fuse_copy_one(cs, &arg, sizeof(arg));\n\tfuse_copy_finish(cs);\n\n\treturn err ? err : reqsize;\n}\n\nstatic struct fuse_forget_link *dequeue_forget(struct fuse_conn *fc,\n\t\t\t\t\t       unsigned max,\n\t\t\t\t\t       unsigned *countp)\n{\n\tstruct fuse_forget_link *head = fc->forget_list_head.next;\n\tstruct fuse_forget_link **newhead = &head;\n\tunsigned count;\n\n\tfor (count = 0; *newhead != NULL && count < max; count++)\n\t\tnewhead = &(*newhead)->next;\n\n\tfc->forget_list_head.next = *newhead;\n\t*newhead = NULL;\n\tif (fc->forget_list_head.next == NULL)\n\t\tfc->forget_list_tail = &fc->forget_list_head;\n\n\tif (countp != NULL)\n\t\t*countp = count;\n\n\treturn head;\n}\n\nstatic int fuse_read_single_forget(struct fuse_conn *fc,\n\t\t\t\t   struct fuse_copy_state *cs,\n\t\t\t\t   size_t nbytes)\n__releases(fc->lock)\n{\n\tint err;\n\tstruct fuse_forget_link *forget = dequeue_forget(fc, 1, NULL);\n\tstruct fuse_forget_in arg = {\n\t\t.nlookup = forget->forget_one.nlookup,\n\t};\n\tstruct fuse_in_header ih = {\n\t\t.opcode = FUSE_FORGET,\n\t\t.nodeid = forget->forget_one.nodeid,\n\t\t.unique = fuse_get_unique(fc),\n\t\t.len = sizeof(ih) + sizeof(arg),\n\t};\n\n\tspin_unlock(&fc->lock);\n\tkfree(forget);\n\tif (nbytes < ih.len)\n\t\treturn -EINVAL;\n\n\terr = fuse_copy_one(cs, &ih, sizeof(ih));\n\tif (!err)\n\t\terr = fuse_copy_one(cs, &arg, sizeof(arg));\n\tfuse_copy_finish(cs);\n\n\tif (err)\n\t\treturn err;\n\n\treturn ih.len;\n}\n\nstatic int fuse_read_batch_forget(struct fuse_conn *fc,\n\t\t\t\t   struct fuse_copy_state *cs, size_t nbytes)\n__releases(fc->lock)\n{\n\tint err;\n\tunsigned max_forgets;\n\tunsigned count;\n\tstruct fuse_forget_link *head;\n\tstruct fuse_batch_forget_in arg = { .count = 0 };\n\tstruct fuse_in_header ih = {\n\t\t.opcode = FUSE_BATCH_FORGET,\n\t\t.unique = fuse_get_unique(fc),\n\t\t.len = sizeof(ih) + sizeof(arg),\n\t};\n\n\tif (nbytes < ih.len) {\n\t\tspin_unlock(&fc->lock);\n\t\treturn -EINVAL;\n\t}\n\n\tmax_forgets = (nbytes - ih.len) / sizeof(struct fuse_forget_one);\n\thead = dequeue_forget(fc, max_forgets, &count);\n\tspin_unlock(&fc->lock);\n\n\targ.count = count;\n\tih.len += count * sizeof(struct fuse_forget_one);\n\terr = fuse_copy_one(cs, &ih, sizeof(ih));\n\tif (!err)\n\t\terr = fuse_copy_one(cs, &arg, sizeof(arg));\n\n\twhile (head) {\n\t\tstruct fuse_forget_link *forget = head;\n\n\t\tif (!err) {\n\t\t\terr = fuse_copy_one(cs, &forget->forget_one,\n\t\t\t\t\t    sizeof(forget->forget_one));\n\t\t}\n\t\thead = forget->next;\n\t\tkfree(forget);\n\t}\n\n\tfuse_copy_finish(cs);\n\n\tif (err)\n\t\treturn err;\n\n\treturn ih.len;\n}\n\nstatic int fuse_read_forget(struct fuse_conn *fc, struct fuse_copy_state *cs,\n\t\t\t    size_t nbytes)\n__releases(fc->lock)\n{\n\tif (fc->minor < 16 || fc->forget_list_head.next->next == NULL)\n\t\treturn fuse_read_single_forget(fc, cs, nbytes);\n\telse\n\t\treturn fuse_read_batch_forget(fc, cs, nbytes);\n}\n\n/*\n * Read a single request into the userspace filesystem's buffer.  This\n * function waits until a request is available, then removes it from\n * the pending list and copies request data to userspace buffer.  If\n * no reply is needed (FORGET) or request has been aborted or there\n * was an error during the copying then it's finished by calling\n * request_end().  Otherwise add it to the processing list, and set\n * the 'sent' flag.\n */\nstatic ssize_t fuse_dev_do_read(struct fuse_conn *fc, struct file *file,\n\t\t\t\tstruct fuse_copy_state *cs, size_t nbytes)\n{\n\tint err;\n\tstruct fuse_req *req;\n\tstruct fuse_in *in;\n\tunsigned reqsize;\n\n restart:\n\tspin_lock(&fc->lock);\n\terr = -EAGAIN;\n\tif ((file->f_flags & O_NONBLOCK) && fc->connected &&\n\t    !request_pending(fc))\n\t\tgoto err_unlock;\n\n\trequest_wait(fc);\n\terr = -ENODEV;\n\tif (!fc->connected)\n\t\tgoto err_unlock;\n\terr = -ERESTARTSYS;\n\tif (!request_pending(fc))\n\t\tgoto err_unlock;\n\n\tif (!list_empty(&fc->interrupts)) {\n\t\treq = list_entry(fc->interrupts.next, struct fuse_req,\n\t\t\t\t intr_entry);\n\t\treturn fuse_read_interrupt(fc, cs, nbytes, req);\n\t}\n\n\tif (forget_pending(fc)) {\n\t\tif (list_empty(&fc->pending) || fc->forget_batch-- > 0)\n\t\t\treturn fuse_read_forget(fc, cs, nbytes);\n\n\t\tif (fc->forget_batch <= -8)\n\t\t\tfc->forget_batch = 16;\n\t}\n\n\treq = list_entry(fc->pending.next, struct fuse_req, list);\n\treq->state = FUSE_REQ_READING;\n\tlist_move(&req->list, &fc->io);\n\n\tin = &req->in;\n\treqsize = in->h.len;\n\t/* If request is too large, reply with an error and restart the read */\n\tif (nbytes < reqsize) {\n\t\treq->out.h.error = -EIO;\n\t\t/* SETXATTR is special, since it may contain too large data */\n\t\tif (in->h.opcode == FUSE_SETXATTR)\n\t\t\treq->out.h.error = -E2BIG;\n\t\trequest_end(fc, req);\n\t\tgoto restart;\n\t}\n\tspin_unlock(&fc->lock);\n\tcs->req = req;\n\terr = fuse_copy_one(cs, &in->h, sizeof(in->h));\n\tif (!err)\n\t\terr = fuse_copy_args(cs, in->numargs, in->argpages,\n\t\t\t\t     (struct fuse_arg *) in->args, 0);\n\tfuse_copy_finish(cs);\n\tspin_lock(&fc->lock);\n\treq->locked = 0;\n\tif (req->aborted) {\n\t\trequest_end(fc, req);\n\t\treturn -ENODEV;\n\t}\n\tif (err) {\n\t\treq->out.h.error = -EIO;\n\t\trequest_end(fc, req);\n\t\treturn err;\n\t}\n\tif (!req->isreply)\n\t\trequest_end(fc, req);\n\telse {\n\t\treq->state = FUSE_REQ_SENT;\n\t\tlist_move_tail(&req->list, &fc->processing);\n\t\tif (req->interrupted)\n\t\t\tqueue_interrupt(fc, req);\n\t\tspin_unlock(&fc->lock);\n\t}\n\treturn reqsize;\n\n err_unlock:\n\tspin_unlock(&fc->lock);\n\treturn err;\n}\n\nstatic ssize_t fuse_dev_read(struct kiocb *iocb, const struct iovec *iov,\n\t\t\t      unsigned long nr_segs, loff_t pos)\n{\n\tstruct fuse_copy_state cs;\n\tstruct file *file = iocb->ki_filp;\n\tstruct fuse_conn *fc = fuse_get_conn(file);\n\tif (!fc)\n\t\treturn -EPERM;\n\n\tfuse_copy_init(&cs, fc, 1, iov, nr_segs);\n\n\treturn fuse_dev_do_read(fc, file, &cs, iov_length(iov, nr_segs));\n}\n\nstatic int fuse_dev_pipe_buf_steal(struct pipe_inode_info *pipe,\n\t\t\t\t   struct pipe_buffer *buf)\n{\n\treturn 1;\n}\n\nstatic const struct pipe_buf_operations fuse_dev_pipe_buf_ops = {\n\t.can_merge = 0,\n\t.map = generic_pipe_buf_map,\n\t.unmap = generic_pipe_buf_unmap,\n\t.confirm = generic_pipe_buf_confirm,\n\t.release = generic_pipe_buf_release,\n\t.steal = fuse_dev_pipe_buf_steal,\n\t.get = generic_pipe_buf_get,\n};\n\nstatic ssize_t fuse_dev_splice_read(struct file *in, loff_t *ppos,\n\t\t\t\t    struct pipe_inode_info *pipe,\n\t\t\t\t    size_t len, unsigned int flags)\n{\n\tint ret;\n\tint page_nr = 0;\n\tint do_wakeup = 0;\n\tstruct pipe_buffer *bufs;\n\tstruct fuse_copy_state cs;\n\tstruct fuse_conn *fc = fuse_get_conn(in);\n\tif (!fc)\n\t\treturn -EPERM;\n\n\tbufs = kmalloc(pipe->buffers * sizeof(struct pipe_buffer), GFP_KERNEL);\n\tif (!bufs)\n\t\treturn -ENOMEM;\n\n\tfuse_copy_init(&cs, fc, 1, NULL, 0);\n\tcs.pipebufs = bufs;\n\tcs.pipe = pipe;\n\tret = fuse_dev_do_read(fc, in, &cs, len);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tret = 0;\n\tpipe_lock(pipe);\n\n\tif (!pipe->readers) {\n\t\tsend_sig(SIGPIPE, current, 0);\n\t\tif (!ret)\n\t\t\tret = -EPIPE;\n\t\tgoto out_unlock;\n\t}\n\n\tif (pipe->nrbufs + cs.nr_segs > pipe->buffers) {\n\t\tret = -EIO;\n\t\tgoto out_unlock;\n\t}\n\n\twhile (page_nr < cs.nr_segs) {\n\t\tint newbuf = (pipe->curbuf + pipe->nrbufs) & (pipe->buffers - 1);\n\t\tstruct pipe_buffer *buf = pipe->bufs + newbuf;\n\n\t\tbuf->page = bufs[page_nr].page;\n\t\tbuf->offset = bufs[page_nr].offset;\n\t\tbuf->len = bufs[page_nr].len;\n\t\tbuf->ops = &fuse_dev_pipe_buf_ops;\n\n\t\tpipe->nrbufs++;\n\t\tpage_nr++;\n\t\tret += buf->len;\n\n\t\tif (pipe->inode)\n\t\t\tdo_wakeup = 1;\n\t}\n\nout_unlock:\n\tpipe_unlock(pipe);\n\n\tif (do_wakeup) {\n\t\tsmp_mb();\n\t\tif (waitqueue_active(&pipe->wait))\n\t\t\twake_up_interruptible(&pipe->wait);\n\t\tkill_fasync(&pipe->fasync_readers, SIGIO, POLL_IN);\n\t}\n\nout:\n\tfor (; page_nr < cs.nr_segs; page_nr++)\n\t\tpage_cache_release(bufs[page_nr].page);\n\n\tkfree(bufs);\n\treturn ret;\n}\n\nstatic int fuse_notify_poll(struct fuse_conn *fc, unsigned int size,\n\t\t\t    struct fuse_copy_state *cs)\n{\n\tstruct fuse_notify_poll_wakeup_out outarg;\n\tint err = -EINVAL;\n\n\tif (size != sizeof(outarg))\n\t\tgoto err;\n\n\terr = fuse_copy_one(cs, &outarg, sizeof(outarg));\n\tif (err)\n\t\tgoto err;\n\n\tfuse_copy_finish(cs);\n\treturn fuse_notify_poll_wakeup(fc, &outarg);\n\nerr:\n\tfuse_copy_finish(cs);\n\treturn err;\n}\n\nstatic int fuse_notify_inval_inode(struct fuse_conn *fc, unsigned int size,\n\t\t\t\t   struct fuse_copy_state *cs)\n{\n\tstruct fuse_notify_inval_inode_out outarg;\n\tint err = -EINVAL;\n\n\tif (size != sizeof(outarg))\n\t\tgoto err;\n\n\terr = fuse_copy_one(cs, &outarg, sizeof(outarg));\n\tif (err)\n\t\tgoto err;\n\tfuse_copy_finish(cs);\n\n\tdown_read(&fc->killsb);\n\terr = -ENOENT;\n\tif (fc->sb) {\n\t\terr = fuse_reverse_inval_inode(fc->sb, outarg.ino,\n\t\t\t\t\t       outarg.off, outarg.len);\n\t}\n\tup_read(&fc->killsb);\n\treturn err;\n\nerr:\n\tfuse_copy_finish(cs);\n\treturn err;\n}\n\nstatic int fuse_notify_inval_entry(struct fuse_conn *fc, unsigned int size,\n\t\t\t\t   struct fuse_copy_state *cs)\n{\n\tstruct fuse_notify_inval_entry_out outarg;\n\tint err = -ENOMEM;\n\tchar *buf;\n\tstruct qstr name;\n\n\tbuf = kzalloc(FUSE_NAME_MAX + 1, GFP_KERNEL);\n\tif (!buf)\n\t\tgoto err;\n\n\terr = -EINVAL;\n\tif (size < sizeof(outarg))\n\t\tgoto err;\n\n\terr = fuse_copy_one(cs, &outarg, sizeof(outarg));\n\tif (err)\n\t\tgoto err;\n\n\terr = -ENAMETOOLONG;\n\tif (outarg.namelen > FUSE_NAME_MAX)\n\t\tgoto err;\n\n\tname.name = buf;\n\tname.len = outarg.namelen;\n\terr = fuse_copy_one(cs, buf, outarg.namelen + 1);\n\tif (err)\n\t\tgoto err;\n\tfuse_copy_finish(cs);\n\tbuf[outarg.namelen] = 0;\n\tname.hash = full_name_hash(name.name, name.len);\n\n\tdown_read(&fc->killsb);\n\terr = -ENOENT;\n\tif (fc->sb)\n\t\terr = fuse_reverse_inval_entry(fc->sb, outarg.parent, &name);\n\tup_read(&fc->killsb);\n\tkfree(buf);\n\treturn err;\n\nerr:\n\tkfree(buf);\n\tfuse_copy_finish(cs);\n\treturn err;\n}\n\nstatic int fuse_notify_store(struct fuse_conn *fc, unsigned int size,\n\t\t\t     struct fuse_copy_state *cs)\n{\n\tstruct fuse_notify_store_out outarg;\n\tstruct inode *inode;\n\tstruct address_space *mapping;\n\tu64 nodeid;\n\tint err;\n\tpgoff_t index;\n\tunsigned int offset;\n\tunsigned int num;\n\tloff_t file_size;\n\tloff_t end;\n\n\terr = -EINVAL;\n\tif (size < sizeof(outarg))\n\t\tgoto out_finish;\n\n\terr = fuse_copy_one(cs, &outarg, sizeof(outarg));\n\tif (err)\n\t\tgoto out_finish;\n\n\terr = -EINVAL;\n\tif (size - sizeof(outarg) != outarg.size)\n\t\tgoto out_finish;\n\n\tnodeid = outarg.nodeid;\n\n\tdown_read(&fc->killsb);\n\n\terr = -ENOENT;\n\tif (!fc->sb)\n\t\tgoto out_up_killsb;\n\n\tinode = ilookup5(fc->sb, nodeid, fuse_inode_eq, &nodeid);\n\tif (!inode)\n\t\tgoto out_up_killsb;\n\n\tmapping = inode->i_mapping;\n\tindex = outarg.offset >> PAGE_CACHE_SHIFT;\n\toffset = outarg.offset & ~PAGE_CACHE_MASK;\n\tfile_size = i_size_read(inode);\n\tend = outarg.offset + outarg.size;\n\tif (end > file_size) {\n\t\tfile_size = end;\n\t\tfuse_write_update_size(inode, file_size);\n\t}\n\n\tnum = outarg.size;\n\twhile (num) {\n\t\tstruct page *page;\n\t\tunsigned int this_num;\n\n\t\terr = -ENOMEM;\n\t\tpage = find_or_create_page(mapping, index,\n\t\t\t\t\t   mapping_gfp_mask(mapping));\n\t\tif (!page)\n\t\t\tgoto out_iput;\n\n\t\tthis_num = min_t(unsigned, num, PAGE_CACHE_SIZE - offset);\n\t\terr = fuse_copy_page(cs, &page, offset, this_num, 0);\n\t\tif (!err && offset == 0 && (num != 0 || file_size == end))\n\t\t\tSetPageUptodate(page);\n\t\tunlock_page(page);\n\t\tpage_cache_release(page);\n\n\t\tif (err)\n\t\t\tgoto out_iput;\n\n\t\tnum -= this_num;\n\t\toffset = 0;\n\t\tindex++;\n\t}\n\n\terr = 0;\n\nout_iput:\n\tiput(inode);\nout_up_killsb:\n\tup_read(&fc->killsb);\nout_finish:\n\tfuse_copy_finish(cs);\n\treturn err;\n}\n\nstatic void fuse_retrieve_end(struct fuse_conn *fc, struct fuse_req *req)\n{\n\trelease_pages(req->pages, req->num_pages, 0);\n}\n\nstatic int fuse_retrieve(struct fuse_conn *fc, struct inode *inode,\n\t\t\t struct fuse_notify_retrieve_out *outarg)\n{\n\tint err;\n\tstruct address_space *mapping = inode->i_mapping;\n\tstruct fuse_req *req;\n\tpgoff_t index;\n\tloff_t file_size;\n\tunsigned int num;\n\tunsigned int offset;\n\tsize_t total_len = 0;\n\n\treq = fuse_get_req(fc);\n\tif (IS_ERR(req))\n\t\treturn PTR_ERR(req);\n\n\toffset = outarg->offset & ~PAGE_CACHE_MASK;\n\n\treq->in.h.opcode = FUSE_NOTIFY_REPLY;\n\treq->in.h.nodeid = outarg->nodeid;\n\treq->in.numargs = 2;\n\treq->in.argpages = 1;\n\treq->page_offset = offset;\n\treq->end = fuse_retrieve_end;\n\n\tindex = outarg->offset >> PAGE_CACHE_SHIFT;\n\tfile_size = i_size_read(inode);\n\tnum = outarg->size;\n\tif (outarg->offset > file_size)\n\t\tnum = 0;\n\telse if (outarg->offset + num > file_size)\n\t\tnum = file_size - outarg->offset;\n\n\twhile (num) {\n\t\tstruct page *page;\n\t\tunsigned int this_num;\n\n\t\tpage = find_get_page(mapping, index);\n\t\tif (!page)\n\t\t\tbreak;\n\n\t\tthis_num = min_t(unsigned, num, PAGE_CACHE_SIZE - offset);\n\t\treq->pages[req->num_pages] = page;\n\t\treq->num_pages++;\n\n\t\tnum -= this_num;\n\t\ttotal_len += this_num;\n\t}\n\treq->misc.retrieve_in.offset = outarg->offset;\n\treq->misc.retrieve_in.size = total_len;\n\treq->in.args[0].size = sizeof(req->misc.retrieve_in);\n\treq->in.args[0].value = &req->misc.retrieve_in;\n\treq->in.args[1].size = total_len;\n\n\terr = fuse_request_send_notify_reply(fc, req, outarg->notify_unique);\n\tif (err)\n\t\tfuse_retrieve_end(fc, req);\n\n\treturn err;\n}\n\nstatic int fuse_notify_retrieve(struct fuse_conn *fc, unsigned int size,\n\t\t\t\tstruct fuse_copy_state *cs)\n{\n\tstruct fuse_notify_retrieve_out outarg;\n\tstruct inode *inode;\n\tint err;\n\n\terr = -EINVAL;\n\tif (size != sizeof(outarg))\n\t\tgoto copy_finish;\n\n\terr = fuse_copy_one(cs, &outarg, sizeof(outarg));\n\tif (err)\n\t\tgoto copy_finish;\n\n\tfuse_copy_finish(cs);\n\n\tdown_read(&fc->killsb);\n\terr = -ENOENT;\n\tif (fc->sb) {\n\t\tu64 nodeid = outarg.nodeid;\n\n\t\tinode = ilookup5(fc->sb, nodeid, fuse_inode_eq, &nodeid);\n\t\tif (inode) {\n\t\t\terr = fuse_retrieve(fc, inode, &outarg);\n\t\t\tiput(inode);\n\t\t}\n\t}\n\tup_read(&fc->killsb);\n\n\treturn err;\n\ncopy_finish:\n\tfuse_copy_finish(cs);\n\treturn err;\n}\n\nstatic int fuse_notify(struct fuse_conn *fc, enum fuse_notify_code code,\n\t\t       unsigned int size, struct fuse_copy_state *cs)\n{\n\tswitch (code) {\n\tcase FUSE_NOTIFY_POLL:\n\t\treturn fuse_notify_poll(fc, size, cs);\n\n\tcase FUSE_NOTIFY_INVAL_INODE:\n\t\treturn fuse_notify_inval_inode(fc, size, cs);\n\n\tcase FUSE_NOTIFY_INVAL_ENTRY:\n\t\treturn fuse_notify_inval_entry(fc, size, cs);\n\n\tcase FUSE_NOTIFY_STORE:\n\t\treturn fuse_notify_store(fc, size, cs);\n\n\tcase FUSE_NOTIFY_RETRIEVE:\n\t\treturn fuse_notify_retrieve(fc, size, cs);\n\n\tdefault:\n\t\tfuse_copy_finish(cs);\n\t\treturn -EINVAL;\n\t}\n}\n\n/* Look up request on processing list by unique ID */\nstatic struct fuse_req *request_find(struct fuse_conn *fc, u64 unique)\n{\n\tstruct list_head *entry;\n\n\tlist_for_each(entry, &fc->processing) {\n\t\tstruct fuse_req *req;\n\t\treq = list_entry(entry, struct fuse_req, list);\n\t\tif (req->in.h.unique == unique || req->intr_unique == unique)\n\t\t\treturn req;\n\t}\n\treturn NULL;\n}\n\nstatic int copy_out_args(struct fuse_copy_state *cs, struct fuse_out *out,\n\t\t\t unsigned nbytes)\n{\n\tunsigned reqsize = sizeof(struct fuse_out_header);\n\n\tif (out->h.error)\n\t\treturn nbytes != reqsize ? -EINVAL : 0;\n\n\treqsize += len_args(out->numargs, out->args);\n\n\tif (reqsize < nbytes || (reqsize > nbytes && !out->argvar))\n\t\treturn -EINVAL;\n\telse if (reqsize > nbytes) {\n\t\tstruct fuse_arg *lastarg = &out->args[out->numargs-1];\n\t\tunsigned diffsize = reqsize - nbytes;\n\t\tif (diffsize > lastarg->size)\n\t\t\treturn -EINVAL;\n\t\tlastarg->size -= diffsize;\n\t}\n\treturn fuse_copy_args(cs, out->numargs, out->argpages, out->args,\n\t\t\t      out->page_zeroing);\n}\n\n/*\n * Write a single reply to a request.  First the header is copied from\n * the write buffer.  The request is then searched on the processing\n * list by the unique ID found in the header.  If found, then remove\n * it from the list and copy the rest of the buffer to the request.\n * The request is finished by calling request_end()\n */\nstatic ssize_t fuse_dev_do_write(struct fuse_conn *fc,\n\t\t\t\t struct fuse_copy_state *cs, size_t nbytes)\n{\n\tint err;\n\tstruct fuse_req *req;\n\tstruct fuse_out_header oh;\n\n\tif (nbytes < sizeof(struct fuse_out_header))\n\t\treturn -EINVAL;\n\n\terr = fuse_copy_one(cs, &oh, sizeof(oh));\n\tif (err)\n\t\tgoto err_finish;\n\n\terr = -EINVAL;\n\tif (oh.len != nbytes)\n\t\tgoto err_finish;\n\n\t/*\n\t * Zero oh.unique indicates unsolicited notification message\n\t * and error contains notification code.\n\t */\n\tif (!oh.unique) {\n\t\terr = fuse_notify(fc, oh.error, nbytes - sizeof(oh), cs);\n\t\treturn err ? err : nbytes;\n\t}\n\n\terr = -EINVAL;\n\tif (oh.error <= -1000 || oh.error > 0)\n\t\tgoto err_finish;\n\n\tspin_lock(&fc->lock);\n\terr = -ENOENT;\n\tif (!fc->connected)\n\t\tgoto err_unlock;\n\n\treq = request_find(fc, oh.unique);\n\tif (!req)\n\t\tgoto err_unlock;\n\n\tif (req->aborted) {\n\t\tspin_unlock(&fc->lock);\n\t\tfuse_copy_finish(cs);\n\t\tspin_lock(&fc->lock);\n\t\trequest_end(fc, req);\n\t\treturn -ENOENT;\n\t}\n\t/* Is it an interrupt reply? */\n\tif (req->intr_unique == oh.unique) {\n\t\terr = -EINVAL;\n\t\tif (nbytes != sizeof(struct fuse_out_header))\n\t\t\tgoto err_unlock;\n\n\t\tif (oh.error == -ENOSYS)\n\t\t\tfc->no_interrupt = 1;\n\t\telse if (oh.error == -EAGAIN)\n\t\t\tqueue_interrupt(fc, req);\n\n\t\tspin_unlock(&fc->lock);\n\t\tfuse_copy_finish(cs);\n\t\treturn nbytes;\n\t}\n\n\treq->state = FUSE_REQ_WRITING;\n\tlist_move(&req->list, &fc->io);\n\treq->out.h = oh;\n\treq->locked = 1;\n\tcs->req = req;\n\tif (!req->out.page_replace)\n\t\tcs->move_pages = 0;\n\tspin_unlock(&fc->lock);\n\n\terr = copy_out_args(cs, &req->out, nbytes);\n\tfuse_copy_finish(cs);\n\n\tspin_lock(&fc->lock);\n\treq->locked = 0;\n\tif (!err) {\n\t\tif (req->aborted)\n\t\t\terr = -ENOENT;\n\t} else if (!req->aborted)\n\t\treq->out.h.error = -EIO;\n\trequest_end(fc, req);\n\n\treturn err ? err : nbytes;\n\n err_unlock:\n\tspin_unlock(&fc->lock);\n err_finish:\n\tfuse_copy_finish(cs);\n\treturn err;\n}\n\nstatic ssize_t fuse_dev_write(struct kiocb *iocb, const struct iovec *iov,\n\t\t\t      unsigned long nr_segs, loff_t pos)\n{\n\tstruct fuse_copy_state cs;\n\tstruct fuse_conn *fc = fuse_get_conn(iocb->ki_filp);\n\tif (!fc)\n\t\treturn -EPERM;\n\n\tfuse_copy_init(&cs, fc, 0, iov, nr_segs);\n\n\treturn fuse_dev_do_write(fc, &cs, iov_length(iov, nr_segs));\n}\n\nstatic ssize_t fuse_dev_splice_write(struct pipe_inode_info *pipe,\n\t\t\t\t     struct file *out, loff_t *ppos,\n\t\t\t\t     size_t len, unsigned int flags)\n{\n\tunsigned nbuf;\n\tunsigned idx;\n\tstruct pipe_buffer *bufs;\n\tstruct fuse_copy_state cs;\n\tstruct fuse_conn *fc;\n\tsize_t rem;\n\tssize_t ret;\n\n\tfc = fuse_get_conn(out);\n\tif (!fc)\n\t\treturn -EPERM;\n\n\tbufs = kmalloc(pipe->buffers * sizeof(struct pipe_buffer), GFP_KERNEL);\n\tif (!bufs)\n\t\treturn -ENOMEM;\n\n\tpipe_lock(pipe);\n\tnbuf = 0;\n\trem = 0;\n\tfor (idx = 0; idx < pipe->nrbufs && rem < len; idx++)\n\t\trem += pipe->bufs[(pipe->curbuf + idx) & (pipe->buffers - 1)].len;\n\n\tret = -EINVAL;\n\tif (rem < len) {\n\t\tpipe_unlock(pipe);\n\t\tgoto out;\n\t}\n\n\trem = len;\n\twhile (rem) {\n\t\tstruct pipe_buffer *ibuf;\n\t\tstruct pipe_buffer *obuf;\n\n\t\tBUG_ON(nbuf >= pipe->buffers);\n\t\tBUG_ON(!pipe->nrbufs);\n\t\tibuf = &pipe->bufs[pipe->curbuf];\n\t\tobuf = &bufs[nbuf];\n\n\t\tif (rem >= ibuf->len) {\n\t\t\t*obuf = *ibuf;\n\t\t\tibuf->ops = NULL;\n\t\t\tpipe->curbuf = (pipe->curbuf + 1) & (pipe->buffers - 1);\n\t\t\tpipe->nrbufs--;\n\t\t} else {\n\t\t\tibuf->ops->get(pipe, ibuf);\n\t\t\t*obuf = *ibuf;\n\t\t\tobuf->flags &= ~PIPE_BUF_FLAG_GIFT;\n\t\t\tobuf->len = rem;\n\t\t\tibuf->offset += obuf->len;\n\t\t\tibuf->len -= obuf->len;\n\t\t}\n\t\tnbuf++;\n\t\trem -= obuf->len;\n\t}\n\tpipe_unlock(pipe);\n\n\tfuse_copy_init(&cs, fc, 0, NULL, nbuf);\n\tcs.pipebufs = bufs;\n\tcs.pipe = pipe;\n\n\tif (flags & SPLICE_F_MOVE)\n\t\tcs.move_pages = 1;\n\n\tret = fuse_dev_do_write(fc, &cs, len);\n\n\tfor (idx = 0; idx < nbuf; idx++) {\n\t\tstruct pipe_buffer *buf = &bufs[idx];\n\t\tbuf->ops->release(pipe, buf);\n\t}\nout:\n\tkfree(bufs);\n\treturn ret;\n}\n\nstatic unsigned fuse_dev_poll(struct file *file, poll_table *wait)\n{\n\tunsigned mask = POLLOUT | POLLWRNORM;\n\tstruct fuse_conn *fc = fuse_get_conn(file);\n\tif (!fc)\n\t\treturn POLLERR;\n\n\tpoll_wait(file, &fc->waitq, wait);\n\n\tspin_lock(&fc->lock);\n\tif (!fc->connected)\n\t\tmask = POLLERR;\n\telse if (request_pending(fc))\n\t\tmask |= POLLIN | POLLRDNORM;\n\tspin_unlock(&fc->lock);\n\n\treturn mask;\n}\n\n/*\n * Abort all requests on the given list (pending or processing)\n *\n * This function releases and reacquires fc->lock\n */\nstatic void end_requests(struct fuse_conn *fc, struct list_head *head)\n__releases(fc->lock)\n__acquires(fc->lock)\n{\n\twhile (!list_empty(head)) {\n\t\tstruct fuse_req *req;\n\t\treq = list_entry(head->next, struct fuse_req, list);\n\t\treq->out.h.error = -ECONNABORTED;\n\t\trequest_end(fc, req);\n\t\tspin_lock(&fc->lock);\n\t}\n}\n\n/*\n * Abort requests under I/O\n *\n * The requests are set to aborted and finished, and the request\n * waiter is woken up.  This will make request_wait_answer() wait\n * until the request is unlocked and then return.\n *\n * If the request is asynchronous, then the end function needs to be\n * called after waiting for the request to be unlocked (if it was\n * locked).\n */\nstatic void end_io_requests(struct fuse_conn *fc)\n__releases(fc->lock)\n__acquires(fc->lock)\n{\n\twhile (!list_empty(&fc->io)) {\n\t\tstruct fuse_req *req =\n\t\t\tlist_entry(fc->io.next, struct fuse_req, list);\n\t\tvoid (*end) (struct fuse_conn *, struct fuse_req *) = req->end;\n\n\t\treq->aborted = 1;\n\t\treq->out.h.error = -ECONNABORTED;\n\t\treq->state = FUSE_REQ_FINISHED;\n\t\tlist_del_init(&req->list);\n\t\twake_up(&req->waitq);\n\t\tif (end) {\n\t\t\treq->end = NULL;\n\t\t\t__fuse_get_request(req);\n\t\t\tspin_unlock(&fc->lock);\n\t\t\twait_event(req->waitq, !req->locked);\n\t\t\tend(fc, req);\n\t\t\tfuse_put_request(fc, req);\n\t\t\tspin_lock(&fc->lock);\n\t\t}\n\t}\n}\n\nstatic void end_queued_requests(struct fuse_conn *fc)\n__releases(fc->lock)\n__acquires(fc->lock)\n{\n\tfc->max_background = UINT_MAX;\n\tflush_bg_queue(fc);\n\tend_requests(fc, &fc->pending);\n\tend_requests(fc, &fc->processing);\n\twhile (forget_pending(fc))\n\t\tkfree(dequeue_forget(fc, 1, NULL));\n}\n\nstatic void end_polls(struct fuse_conn *fc)\n{\n\tstruct rb_node *p;\n\n\tp = rb_first(&fc->polled_files);\n\n\twhile (p) {\n\t\tstruct fuse_file *ff;\n\t\tff = rb_entry(p, struct fuse_file, polled_node);\n\t\twake_up_interruptible_all(&ff->poll_wait);\n\n\t\tp = rb_next(p);\n\t}\n}\n\n/*\n * Abort all requests.\n *\n * Emergency exit in case of a malicious or accidental deadlock, or\n * just a hung filesystem.\n *\n * The same effect is usually achievable through killing the\n * filesystem daemon and all users of the filesystem.  The exception\n * is the combination of an asynchronous request and the tricky\n * deadlock (see Documentation/filesystems/fuse.txt).\n *\n * During the aborting, progression of requests from the pending and\n * processing lists onto the io list, and progression of new requests\n * onto the pending list is prevented by req->connected being false.\n *\n * Progression of requests under I/O to the processing list is\n * prevented by the req->aborted flag being true for these requests.\n * For this reason requests on the io list must be aborted first.\n */\nvoid fuse_abort_conn(struct fuse_conn *fc)\n{\n\tspin_lock(&fc->lock);\n\tif (fc->connected) {\n\t\tfc->connected = 0;\n\t\tfc->blocked = 0;\n\t\tend_io_requests(fc);\n\t\tend_queued_requests(fc);\n\t\tend_polls(fc);\n\t\twake_up_all(&fc->waitq);\n\t\twake_up_all(&fc->blocked_waitq);\n\t\tkill_fasync(&fc->fasync, SIGIO, POLL_IN);\n\t}\n\tspin_unlock(&fc->lock);\n}\nEXPORT_SYMBOL_GPL(fuse_abort_conn);\n\nint fuse_dev_release(struct inode *inode, struct file *file)\n{\n\tstruct fuse_conn *fc = fuse_get_conn(file);\n\tif (fc) {\n\t\tspin_lock(&fc->lock);\n\t\tfc->connected = 0;\n\t\tfc->blocked = 0;\n\t\tend_queued_requests(fc);\n\t\tend_polls(fc);\n\t\twake_up_all(&fc->blocked_waitq);\n\t\tspin_unlock(&fc->lock);\n\t\tfuse_conn_put(fc);\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(fuse_dev_release);\n\nstatic int fuse_dev_fasync(int fd, struct file *file, int on)\n{\n\tstruct fuse_conn *fc = fuse_get_conn(file);\n\tif (!fc)\n\t\treturn -EPERM;\n\n\t/* No locking - fasync_helper does its own locking */\n\treturn fasync_helper(fd, file, on, &fc->fasync);\n}\n\nconst struct file_operations fuse_dev_operations = {\n\t.owner\t\t= THIS_MODULE,\n\t.llseek\t\t= no_llseek,\n\t.read\t\t= do_sync_read,\n\t.aio_read\t= fuse_dev_read,\n\t.splice_read\t= fuse_dev_splice_read,\n\t.write\t\t= do_sync_write,\n\t.aio_write\t= fuse_dev_write,\n\t.splice_write\t= fuse_dev_splice_write,\n\t.poll\t\t= fuse_dev_poll,\n\t.release\t= fuse_dev_release,\n\t.fasync\t\t= fuse_dev_fasync,\n};\nEXPORT_SYMBOL_GPL(fuse_dev_operations);\n\nstatic struct miscdevice fuse_miscdevice = {\n\t.minor = FUSE_MINOR,\n\t.name  = \"fuse\",\n\t.fops = &fuse_dev_operations,\n};\n\nint __init fuse_dev_init(void)\n{\n\tint err = -ENOMEM;\n\tfuse_req_cachep = kmem_cache_create(\"fuse_request\",\n\t\t\t\t\t    sizeof(struct fuse_req),\n\t\t\t\t\t    0, 0, NULL);\n\tif (!fuse_req_cachep)\n\t\tgoto out;\n\n\terr = misc_register(&fuse_miscdevice);\n\tif (err)\n\t\tgoto out_cache_clean;\n\n\treturn 0;\n\n out_cache_clean:\n\tkmem_cache_destroy(fuse_req_cachep);\n out:\n\treturn err;\n}\n\nvoid fuse_dev_cleanup(void)\n{\n\tmisc_deregister(&fuse_miscdevice);\n\tkmem_cache_destroy(fuse_req_cachep);\n}\n"], "fixing_code": ["/*\n  FUSE: Filesystem in Userspace\n  Copyright (C) 2001-2008  Miklos Szeredi <miklos@szeredi.hu>\n\n  This program can be distributed under the terms of the GNU GPL.\n  See the file COPYING.\n*/\n\n#include \"fuse_i.h\"\n\n#include <linux/init.h>\n#include <linux/module.h>\n#include <linux/poll.h>\n#include <linux/uio.h>\n#include <linux/miscdevice.h>\n#include <linux/pagemap.h>\n#include <linux/file.h>\n#include <linux/slab.h>\n#include <linux/pipe_fs_i.h>\n#include <linux/swap.h>\n#include <linux/splice.h>\n\nMODULE_ALIAS_MISCDEV(FUSE_MINOR);\nMODULE_ALIAS(\"devname:fuse\");\n\nstatic struct kmem_cache *fuse_req_cachep;\n\nstatic struct fuse_conn *fuse_get_conn(struct file *file)\n{\n\t/*\n\t * Lockless access is OK, because file->private data is set\n\t * once during mount and is valid until the file is released.\n\t */\n\treturn file->private_data;\n}\n\nstatic void fuse_request_init(struct fuse_req *req)\n{\n\tmemset(req, 0, sizeof(*req));\n\tINIT_LIST_HEAD(&req->list);\n\tINIT_LIST_HEAD(&req->intr_entry);\n\tinit_waitqueue_head(&req->waitq);\n\tatomic_set(&req->count, 1);\n}\n\nstruct fuse_req *fuse_request_alloc(void)\n{\n\tstruct fuse_req *req = kmem_cache_alloc(fuse_req_cachep, GFP_KERNEL);\n\tif (req)\n\t\tfuse_request_init(req);\n\treturn req;\n}\nEXPORT_SYMBOL_GPL(fuse_request_alloc);\n\nstruct fuse_req *fuse_request_alloc_nofs(void)\n{\n\tstruct fuse_req *req = kmem_cache_alloc(fuse_req_cachep, GFP_NOFS);\n\tif (req)\n\t\tfuse_request_init(req);\n\treturn req;\n}\n\nvoid fuse_request_free(struct fuse_req *req)\n{\n\tkmem_cache_free(fuse_req_cachep, req);\n}\n\nstatic void block_sigs(sigset_t *oldset)\n{\n\tsigset_t mask;\n\n\tsiginitsetinv(&mask, sigmask(SIGKILL));\n\tsigprocmask(SIG_BLOCK, &mask, oldset);\n}\n\nstatic void restore_sigs(sigset_t *oldset)\n{\n\tsigprocmask(SIG_SETMASK, oldset, NULL);\n}\n\nstatic void __fuse_get_request(struct fuse_req *req)\n{\n\tatomic_inc(&req->count);\n}\n\n/* Must be called with > 1 refcount */\nstatic void __fuse_put_request(struct fuse_req *req)\n{\n\tBUG_ON(atomic_read(&req->count) < 2);\n\tatomic_dec(&req->count);\n}\n\nstatic void fuse_req_init_context(struct fuse_req *req)\n{\n\treq->in.h.uid = current_fsuid();\n\treq->in.h.gid = current_fsgid();\n\treq->in.h.pid = current->pid;\n}\n\nstruct fuse_req *fuse_get_req(struct fuse_conn *fc)\n{\n\tstruct fuse_req *req;\n\tsigset_t oldset;\n\tint intr;\n\tint err;\n\n\tatomic_inc(&fc->num_waiting);\n\tblock_sigs(&oldset);\n\tintr = wait_event_interruptible(fc->blocked_waitq, !fc->blocked);\n\trestore_sigs(&oldset);\n\terr = -EINTR;\n\tif (intr)\n\t\tgoto out;\n\n\terr = -ENOTCONN;\n\tif (!fc->connected)\n\t\tgoto out;\n\n\treq = fuse_request_alloc();\n\terr = -ENOMEM;\n\tif (!req)\n\t\tgoto out;\n\n\tfuse_req_init_context(req);\n\treq->waiting = 1;\n\treturn req;\n\n out:\n\tatomic_dec(&fc->num_waiting);\n\treturn ERR_PTR(err);\n}\nEXPORT_SYMBOL_GPL(fuse_get_req);\n\n/*\n * Return request in fuse_file->reserved_req.  However that may\n * currently be in use.  If that is the case, wait for it to become\n * available.\n */\nstatic struct fuse_req *get_reserved_req(struct fuse_conn *fc,\n\t\t\t\t\t struct file *file)\n{\n\tstruct fuse_req *req = NULL;\n\tstruct fuse_file *ff = file->private_data;\n\n\tdo {\n\t\twait_event(fc->reserved_req_waitq, ff->reserved_req);\n\t\tspin_lock(&fc->lock);\n\t\tif (ff->reserved_req) {\n\t\t\treq = ff->reserved_req;\n\t\t\tff->reserved_req = NULL;\n\t\t\tget_file(file);\n\t\t\treq->stolen_file = file;\n\t\t}\n\t\tspin_unlock(&fc->lock);\n\t} while (!req);\n\n\treturn req;\n}\n\n/*\n * Put stolen request back into fuse_file->reserved_req\n */\nstatic void put_reserved_req(struct fuse_conn *fc, struct fuse_req *req)\n{\n\tstruct file *file = req->stolen_file;\n\tstruct fuse_file *ff = file->private_data;\n\n\tspin_lock(&fc->lock);\n\tfuse_request_init(req);\n\tBUG_ON(ff->reserved_req);\n\tff->reserved_req = req;\n\twake_up_all(&fc->reserved_req_waitq);\n\tspin_unlock(&fc->lock);\n\tfput(file);\n}\n\n/*\n * Gets a requests for a file operation, always succeeds\n *\n * This is used for sending the FLUSH request, which must get to\n * userspace, due to POSIX locks which may need to be unlocked.\n *\n * If allocation fails due to OOM, use the reserved request in\n * fuse_file.\n *\n * This is very unlikely to deadlock accidentally, since the\n * filesystem should not have it's own file open.  If deadlock is\n * intentional, it can still be broken by \"aborting\" the filesystem.\n */\nstruct fuse_req *fuse_get_req_nofail(struct fuse_conn *fc, struct file *file)\n{\n\tstruct fuse_req *req;\n\n\tatomic_inc(&fc->num_waiting);\n\twait_event(fc->blocked_waitq, !fc->blocked);\n\treq = fuse_request_alloc();\n\tif (!req)\n\t\treq = get_reserved_req(fc, file);\n\n\tfuse_req_init_context(req);\n\treq->waiting = 1;\n\treturn req;\n}\n\nvoid fuse_put_request(struct fuse_conn *fc, struct fuse_req *req)\n{\n\tif (atomic_dec_and_test(&req->count)) {\n\t\tif (req->waiting)\n\t\t\tatomic_dec(&fc->num_waiting);\n\n\t\tif (req->stolen_file)\n\t\t\tput_reserved_req(fc, req);\n\t\telse\n\t\t\tfuse_request_free(req);\n\t}\n}\nEXPORT_SYMBOL_GPL(fuse_put_request);\n\nstatic unsigned len_args(unsigned numargs, struct fuse_arg *args)\n{\n\tunsigned nbytes = 0;\n\tunsigned i;\n\n\tfor (i = 0; i < numargs; i++)\n\t\tnbytes += args[i].size;\n\n\treturn nbytes;\n}\n\nstatic u64 fuse_get_unique(struct fuse_conn *fc)\n{\n\tfc->reqctr++;\n\t/* zero is special */\n\tif (fc->reqctr == 0)\n\t\tfc->reqctr = 1;\n\n\treturn fc->reqctr;\n}\n\nstatic void queue_request(struct fuse_conn *fc, struct fuse_req *req)\n{\n\treq->in.h.len = sizeof(struct fuse_in_header) +\n\t\tlen_args(req->in.numargs, (struct fuse_arg *) req->in.args);\n\tlist_add_tail(&req->list, &fc->pending);\n\treq->state = FUSE_REQ_PENDING;\n\tif (!req->waiting) {\n\t\treq->waiting = 1;\n\t\tatomic_inc(&fc->num_waiting);\n\t}\n\twake_up(&fc->waitq);\n\tkill_fasync(&fc->fasync, SIGIO, POLL_IN);\n}\n\nvoid fuse_queue_forget(struct fuse_conn *fc, struct fuse_forget_link *forget,\n\t\t       u64 nodeid, u64 nlookup)\n{\n\tforget->forget_one.nodeid = nodeid;\n\tforget->forget_one.nlookup = nlookup;\n\n\tspin_lock(&fc->lock);\n\tfc->forget_list_tail->next = forget;\n\tfc->forget_list_tail = forget;\n\twake_up(&fc->waitq);\n\tkill_fasync(&fc->fasync, SIGIO, POLL_IN);\n\tspin_unlock(&fc->lock);\n}\n\nstatic void flush_bg_queue(struct fuse_conn *fc)\n{\n\twhile (fc->active_background < fc->max_background &&\n\t       !list_empty(&fc->bg_queue)) {\n\t\tstruct fuse_req *req;\n\n\t\treq = list_entry(fc->bg_queue.next, struct fuse_req, list);\n\t\tlist_del(&req->list);\n\t\tfc->active_background++;\n\t\treq->in.h.unique = fuse_get_unique(fc);\n\t\tqueue_request(fc, req);\n\t}\n}\n\n/*\n * This function is called when a request is finished.  Either a reply\n * has arrived or it was aborted (and not yet sent) or some error\n * occurred during communication with userspace, or the device file\n * was closed.  The requester thread is woken up (if still waiting),\n * the 'end' callback is called if given, else the reference to the\n * request is released\n *\n * Called with fc->lock, unlocks it\n */\nstatic void request_end(struct fuse_conn *fc, struct fuse_req *req)\n__releases(fc->lock)\n{\n\tvoid (*end) (struct fuse_conn *, struct fuse_req *) = req->end;\n\treq->end = NULL;\n\tlist_del(&req->list);\n\tlist_del(&req->intr_entry);\n\treq->state = FUSE_REQ_FINISHED;\n\tif (req->background) {\n\t\tif (fc->num_background == fc->max_background) {\n\t\t\tfc->blocked = 0;\n\t\t\twake_up_all(&fc->blocked_waitq);\n\t\t}\n\t\tif (fc->num_background == fc->congestion_threshold &&\n\t\t    fc->connected && fc->bdi_initialized) {\n\t\t\tclear_bdi_congested(&fc->bdi, BLK_RW_SYNC);\n\t\t\tclear_bdi_congested(&fc->bdi, BLK_RW_ASYNC);\n\t\t}\n\t\tfc->num_background--;\n\t\tfc->active_background--;\n\t\tflush_bg_queue(fc);\n\t}\n\tspin_unlock(&fc->lock);\n\twake_up(&req->waitq);\n\tif (end)\n\t\tend(fc, req);\n\tfuse_put_request(fc, req);\n}\n\nstatic void wait_answer_interruptible(struct fuse_conn *fc,\n\t\t\t\t      struct fuse_req *req)\n__releases(fc->lock)\n__acquires(fc->lock)\n{\n\tif (signal_pending(current))\n\t\treturn;\n\n\tspin_unlock(&fc->lock);\n\twait_event_interruptible(req->waitq, req->state == FUSE_REQ_FINISHED);\n\tspin_lock(&fc->lock);\n}\n\nstatic void queue_interrupt(struct fuse_conn *fc, struct fuse_req *req)\n{\n\tlist_add_tail(&req->intr_entry, &fc->interrupts);\n\twake_up(&fc->waitq);\n\tkill_fasync(&fc->fasync, SIGIO, POLL_IN);\n}\n\nstatic void request_wait_answer(struct fuse_conn *fc, struct fuse_req *req)\n__releases(fc->lock)\n__acquires(fc->lock)\n{\n\tif (!fc->no_interrupt) {\n\t\t/* Any signal may interrupt this */\n\t\twait_answer_interruptible(fc, req);\n\n\t\tif (req->aborted)\n\t\t\tgoto aborted;\n\t\tif (req->state == FUSE_REQ_FINISHED)\n\t\t\treturn;\n\n\t\treq->interrupted = 1;\n\t\tif (req->state == FUSE_REQ_SENT)\n\t\t\tqueue_interrupt(fc, req);\n\t}\n\n\tif (!req->force) {\n\t\tsigset_t oldset;\n\n\t\t/* Only fatal signals may interrupt this */\n\t\tblock_sigs(&oldset);\n\t\twait_answer_interruptible(fc, req);\n\t\trestore_sigs(&oldset);\n\n\t\tif (req->aborted)\n\t\t\tgoto aborted;\n\t\tif (req->state == FUSE_REQ_FINISHED)\n\t\t\treturn;\n\n\t\t/* Request is not yet in userspace, bail out */\n\t\tif (req->state == FUSE_REQ_PENDING) {\n\t\t\tlist_del(&req->list);\n\t\t\t__fuse_put_request(req);\n\t\t\treq->out.h.error = -EINTR;\n\t\t\treturn;\n\t\t}\n\t}\n\n\t/*\n\t * Either request is already in userspace, or it was forced.\n\t * Wait it out.\n\t */\n\tspin_unlock(&fc->lock);\n\twait_event(req->waitq, req->state == FUSE_REQ_FINISHED);\n\tspin_lock(&fc->lock);\n\n\tif (!req->aborted)\n\t\treturn;\n\n aborted:\n\tBUG_ON(req->state != FUSE_REQ_FINISHED);\n\tif (req->locked) {\n\t\t/* This is uninterruptible sleep, because data is\n\t\t   being copied to/from the buffers of req.  During\n\t\t   locked state, there mustn't be any filesystem\n\t\t   operation (e.g. page fault), since that could lead\n\t\t   to deadlock */\n\t\tspin_unlock(&fc->lock);\n\t\twait_event(req->waitq, !req->locked);\n\t\tspin_lock(&fc->lock);\n\t}\n}\n\nvoid fuse_request_send(struct fuse_conn *fc, struct fuse_req *req)\n{\n\treq->isreply = 1;\n\tspin_lock(&fc->lock);\n\tif (!fc->connected)\n\t\treq->out.h.error = -ENOTCONN;\n\telse if (fc->conn_error)\n\t\treq->out.h.error = -ECONNREFUSED;\n\telse {\n\t\treq->in.h.unique = fuse_get_unique(fc);\n\t\tqueue_request(fc, req);\n\t\t/* acquire extra reference, since request is still needed\n\t\t   after request_end() */\n\t\t__fuse_get_request(req);\n\n\t\trequest_wait_answer(fc, req);\n\t}\n\tspin_unlock(&fc->lock);\n}\nEXPORT_SYMBOL_GPL(fuse_request_send);\n\nstatic void fuse_request_send_nowait_locked(struct fuse_conn *fc,\n\t\t\t\t\t    struct fuse_req *req)\n{\n\treq->background = 1;\n\tfc->num_background++;\n\tif (fc->num_background == fc->max_background)\n\t\tfc->blocked = 1;\n\tif (fc->num_background == fc->congestion_threshold &&\n\t    fc->bdi_initialized) {\n\t\tset_bdi_congested(&fc->bdi, BLK_RW_SYNC);\n\t\tset_bdi_congested(&fc->bdi, BLK_RW_ASYNC);\n\t}\n\tlist_add_tail(&req->list, &fc->bg_queue);\n\tflush_bg_queue(fc);\n}\n\nstatic void fuse_request_send_nowait(struct fuse_conn *fc, struct fuse_req *req)\n{\n\tspin_lock(&fc->lock);\n\tif (fc->connected) {\n\t\tfuse_request_send_nowait_locked(fc, req);\n\t\tspin_unlock(&fc->lock);\n\t} else {\n\t\treq->out.h.error = -ENOTCONN;\n\t\trequest_end(fc, req);\n\t}\n}\n\nvoid fuse_request_send_background(struct fuse_conn *fc, struct fuse_req *req)\n{\n\treq->isreply = 1;\n\tfuse_request_send_nowait(fc, req);\n}\nEXPORT_SYMBOL_GPL(fuse_request_send_background);\n\nstatic int fuse_request_send_notify_reply(struct fuse_conn *fc,\n\t\t\t\t\t  struct fuse_req *req, u64 unique)\n{\n\tint err = -ENODEV;\n\n\treq->isreply = 0;\n\treq->in.h.unique = unique;\n\tspin_lock(&fc->lock);\n\tif (fc->connected) {\n\t\tqueue_request(fc, req);\n\t\terr = 0;\n\t}\n\tspin_unlock(&fc->lock);\n\n\treturn err;\n}\n\n/*\n * Called under fc->lock\n *\n * fc->connected must have been checked previously\n */\nvoid fuse_request_send_background_locked(struct fuse_conn *fc,\n\t\t\t\t\t struct fuse_req *req)\n{\n\treq->isreply = 1;\n\tfuse_request_send_nowait_locked(fc, req);\n}\n\n/*\n * Lock the request.  Up to the next unlock_request() there mustn't be\n * anything that could cause a page-fault.  If the request was already\n * aborted bail out.\n */\nstatic int lock_request(struct fuse_conn *fc, struct fuse_req *req)\n{\n\tint err = 0;\n\tif (req) {\n\t\tspin_lock(&fc->lock);\n\t\tif (req->aborted)\n\t\t\terr = -ENOENT;\n\t\telse\n\t\t\treq->locked = 1;\n\t\tspin_unlock(&fc->lock);\n\t}\n\treturn err;\n}\n\n/*\n * Unlock request.  If it was aborted during being locked, the\n * requester thread is currently waiting for it to be unlocked, so\n * wake it up.\n */\nstatic void unlock_request(struct fuse_conn *fc, struct fuse_req *req)\n{\n\tif (req) {\n\t\tspin_lock(&fc->lock);\n\t\treq->locked = 0;\n\t\tif (req->aborted)\n\t\t\twake_up(&req->waitq);\n\t\tspin_unlock(&fc->lock);\n\t}\n}\n\nstruct fuse_copy_state {\n\tstruct fuse_conn *fc;\n\tint write;\n\tstruct fuse_req *req;\n\tconst struct iovec *iov;\n\tstruct pipe_buffer *pipebufs;\n\tstruct pipe_buffer *currbuf;\n\tstruct pipe_inode_info *pipe;\n\tunsigned long nr_segs;\n\tunsigned long seglen;\n\tunsigned long addr;\n\tstruct page *pg;\n\tvoid *mapaddr;\n\tvoid *buf;\n\tunsigned len;\n\tunsigned move_pages:1;\n};\n\nstatic void fuse_copy_init(struct fuse_copy_state *cs, struct fuse_conn *fc,\n\t\t\t   int write,\n\t\t\t   const struct iovec *iov, unsigned long nr_segs)\n{\n\tmemset(cs, 0, sizeof(*cs));\n\tcs->fc = fc;\n\tcs->write = write;\n\tcs->iov = iov;\n\tcs->nr_segs = nr_segs;\n}\n\n/* Unmap and put previous page of userspace buffer */\nstatic void fuse_copy_finish(struct fuse_copy_state *cs)\n{\n\tif (cs->currbuf) {\n\t\tstruct pipe_buffer *buf = cs->currbuf;\n\n\t\tif (!cs->write) {\n\t\t\tbuf->ops->unmap(cs->pipe, buf, cs->mapaddr);\n\t\t} else {\n\t\t\tkunmap(buf->page);\n\t\t\tbuf->len = PAGE_SIZE - cs->len;\n\t\t}\n\t\tcs->currbuf = NULL;\n\t\tcs->mapaddr = NULL;\n\t} else if (cs->mapaddr) {\n\t\tkunmap(cs->pg);\n\t\tif (cs->write) {\n\t\t\tflush_dcache_page(cs->pg);\n\t\t\tset_page_dirty_lock(cs->pg);\n\t\t}\n\t\tput_page(cs->pg);\n\t\tcs->mapaddr = NULL;\n\t}\n}\n\n/*\n * Get another pagefull of userspace buffer, and map it to kernel\n * address space, and lock request\n */\nstatic int fuse_copy_fill(struct fuse_copy_state *cs)\n{\n\tunsigned long offset;\n\tint err;\n\n\tunlock_request(cs->fc, cs->req);\n\tfuse_copy_finish(cs);\n\tif (cs->pipebufs) {\n\t\tstruct pipe_buffer *buf = cs->pipebufs;\n\n\t\tif (!cs->write) {\n\t\t\terr = buf->ops->confirm(cs->pipe, buf);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tBUG_ON(!cs->nr_segs);\n\t\t\tcs->currbuf = buf;\n\t\t\tcs->mapaddr = buf->ops->map(cs->pipe, buf, 0);\n\t\t\tcs->len = buf->len;\n\t\t\tcs->buf = cs->mapaddr + buf->offset;\n\t\t\tcs->pipebufs++;\n\t\t\tcs->nr_segs--;\n\t\t} else {\n\t\t\tstruct page *page;\n\n\t\t\tif (cs->nr_segs == cs->pipe->buffers)\n\t\t\t\treturn -EIO;\n\n\t\t\tpage = alloc_page(GFP_HIGHUSER);\n\t\t\tif (!page)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tbuf->page = page;\n\t\t\tbuf->offset = 0;\n\t\t\tbuf->len = 0;\n\n\t\t\tcs->currbuf = buf;\n\t\t\tcs->mapaddr = kmap(page);\n\t\t\tcs->buf = cs->mapaddr;\n\t\t\tcs->len = PAGE_SIZE;\n\t\t\tcs->pipebufs++;\n\t\t\tcs->nr_segs++;\n\t\t}\n\t} else {\n\t\tif (!cs->seglen) {\n\t\t\tBUG_ON(!cs->nr_segs);\n\t\t\tcs->seglen = cs->iov[0].iov_len;\n\t\t\tcs->addr = (unsigned long) cs->iov[0].iov_base;\n\t\t\tcs->iov++;\n\t\t\tcs->nr_segs--;\n\t\t}\n\t\terr = get_user_pages_fast(cs->addr, 1, cs->write, &cs->pg);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t\tBUG_ON(err != 1);\n\t\toffset = cs->addr % PAGE_SIZE;\n\t\tcs->mapaddr = kmap(cs->pg);\n\t\tcs->buf = cs->mapaddr + offset;\n\t\tcs->len = min(PAGE_SIZE - offset, cs->seglen);\n\t\tcs->seglen -= cs->len;\n\t\tcs->addr += cs->len;\n\t}\n\n\treturn lock_request(cs->fc, cs->req);\n}\n\n/* Do as much copy to/from userspace buffer as we can */\nstatic int fuse_copy_do(struct fuse_copy_state *cs, void **val, unsigned *size)\n{\n\tunsigned ncpy = min(*size, cs->len);\n\tif (val) {\n\t\tif (cs->write)\n\t\t\tmemcpy(cs->buf, *val, ncpy);\n\t\telse\n\t\t\tmemcpy(*val, cs->buf, ncpy);\n\t\t*val += ncpy;\n\t}\n\t*size -= ncpy;\n\tcs->len -= ncpy;\n\tcs->buf += ncpy;\n\treturn ncpy;\n}\n\nstatic int fuse_check_page(struct page *page)\n{\n\tif (page_mapcount(page) ||\n\t    page->mapping != NULL ||\n\t    page_count(page) != 1 ||\n\t    (page->flags & PAGE_FLAGS_CHECK_AT_PREP &\n\t     ~(1 << PG_locked |\n\t       1 << PG_referenced |\n\t       1 << PG_uptodate |\n\t       1 << PG_lru |\n\t       1 << PG_active |\n\t       1 << PG_reclaim))) {\n\t\tprintk(KERN_WARNING \"fuse: trying to steal weird page\\n\");\n\t\tprintk(KERN_WARNING \"  page=%p index=%li flags=%08lx, count=%i, mapcount=%i, mapping=%p\\n\", page, page->index, page->flags, page_count(page), page_mapcount(page), page->mapping);\n\t\treturn 1;\n\t}\n\treturn 0;\n}\n\nstatic int fuse_try_move_page(struct fuse_copy_state *cs, struct page **pagep)\n{\n\tint err;\n\tstruct page *oldpage = *pagep;\n\tstruct page *newpage;\n\tstruct pipe_buffer *buf = cs->pipebufs;\n\tstruct address_space *mapping;\n\tpgoff_t index;\n\n\tunlock_request(cs->fc, cs->req);\n\tfuse_copy_finish(cs);\n\n\terr = buf->ops->confirm(cs->pipe, buf);\n\tif (err)\n\t\treturn err;\n\n\tBUG_ON(!cs->nr_segs);\n\tcs->currbuf = buf;\n\tcs->len = buf->len;\n\tcs->pipebufs++;\n\tcs->nr_segs--;\n\n\tif (cs->len != PAGE_SIZE)\n\t\tgoto out_fallback;\n\n\tif (buf->ops->steal(cs->pipe, buf) != 0)\n\t\tgoto out_fallback;\n\n\tnewpage = buf->page;\n\n\tif (WARN_ON(!PageUptodate(newpage)))\n\t\treturn -EIO;\n\n\tClearPageMappedToDisk(newpage);\n\n\tif (fuse_check_page(newpage) != 0)\n\t\tgoto out_fallback_unlock;\n\n\tmapping = oldpage->mapping;\n\tindex = oldpage->index;\n\n\t/*\n\t * This is a new and locked page, it shouldn't be mapped or\n\t * have any special flags on it\n\t */\n\tif (WARN_ON(page_mapped(oldpage)))\n\t\tgoto out_fallback_unlock;\n\tif (WARN_ON(page_has_private(oldpage)))\n\t\tgoto out_fallback_unlock;\n\tif (WARN_ON(PageDirty(oldpage) || PageWriteback(oldpage)))\n\t\tgoto out_fallback_unlock;\n\tif (WARN_ON(PageMlocked(oldpage)))\n\t\tgoto out_fallback_unlock;\n\n\terr = replace_page_cache_page(oldpage, newpage, GFP_KERNEL);\n\tif (err) {\n\t\tunlock_page(newpage);\n\t\treturn err;\n\t}\n\n\tpage_cache_get(newpage);\n\n\tif (!(buf->flags & PIPE_BUF_FLAG_LRU))\n\t\tlru_cache_add_file(newpage);\n\n\terr = 0;\n\tspin_lock(&cs->fc->lock);\n\tif (cs->req->aborted)\n\t\terr = -ENOENT;\n\telse\n\t\t*pagep = newpage;\n\tspin_unlock(&cs->fc->lock);\n\n\tif (err) {\n\t\tunlock_page(newpage);\n\t\tpage_cache_release(newpage);\n\t\treturn err;\n\t}\n\n\tunlock_page(oldpage);\n\tpage_cache_release(oldpage);\n\tcs->len = 0;\n\n\treturn 0;\n\nout_fallback_unlock:\n\tunlock_page(newpage);\nout_fallback:\n\tcs->mapaddr = buf->ops->map(cs->pipe, buf, 1);\n\tcs->buf = cs->mapaddr + buf->offset;\n\n\terr = lock_request(cs->fc, cs->req);\n\tif (err)\n\t\treturn err;\n\n\treturn 1;\n}\n\nstatic int fuse_ref_page(struct fuse_copy_state *cs, struct page *page,\n\t\t\t unsigned offset, unsigned count)\n{\n\tstruct pipe_buffer *buf;\n\n\tif (cs->nr_segs == cs->pipe->buffers)\n\t\treturn -EIO;\n\n\tunlock_request(cs->fc, cs->req);\n\tfuse_copy_finish(cs);\n\n\tbuf = cs->pipebufs;\n\tpage_cache_get(page);\n\tbuf->page = page;\n\tbuf->offset = offset;\n\tbuf->len = count;\n\n\tcs->pipebufs++;\n\tcs->nr_segs++;\n\tcs->len = 0;\n\n\treturn 0;\n}\n\n/*\n * Copy a page in the request to/from the userspace buffer.  Must be\n * done atomically\n */\nstatic int fuse_copy_page(struct fuse_copy_state *cs, struct page **pagep,\n\t\t\t  unsigned offset, unsigned count, int zeroing)\n{\n\tint err;\n\tstruct page *page = *pagep;\n\n\tif (page && zeroing && count < PAGE_SIZE)\n\t\tclear_highpage(page);\n\n\twhile (count) {\n\t\tif (cs->write && cs->pipebufs && page) {\n\t\t\treturn fuse_ref_page(cs, page, offset, count);\n\t\t} else if (!cs->len) {\n\t\t\tif (cs->move_pages && page &&\n\t\t\t    offset == 0 && count == PAGE_SIZE) {\n\t\t\t\terr = fuse_try_move_page(cs, pagep);\n\t\t\t\tif (err <= 0)\n\t\t\t\t\treturn err;\n\t\t\t} else {\n\t\t\t\terr = fuse_copy_fill(cs);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\t\tif (page) {\n\t\t\tvoid *mapaddr = kmap_atomic(page, KM_USER0);\n\t\t\tvoid *buf = mapaddr + offset;\n\t\t\toffset += fuse_copy_do(cs, &buf, &count);\n\t\t\tkunmap_atomic(mapaddr, KM_USER0);\n\t\t} else\n\t\t\toffset += fuse_copy_do(cs, NULL, &count);\n\t}\n\tif (page && !cs->write)\n\t\tflush_dcache_page(page);\n\treturn 0;\n}\n\n/* Copy pages in the request to/from userspace buffer */\nstatic int fuse_copy_pages(struct fuse_copy_state *cs, unsigned nbytes,\n\t\t\t   int zeroing)\n{\n\tunsigned i;\n\tstruct fuse_req *req = cs->req;\n\tunsigned offset = req->page_offset;\n\tunsigned count = min(nbytes, (unsigned) PAGE_SIZE - offset);\n\n\tfor (i = 0; i < req->num_pages && (nbytes || zeroing); i++) {\n\t\tint err;\n\n\t\terr = fuse_copy_page(cs, &req->pages[i], offset, count,\n\t\t\t\t     zeroing);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tnbytes -= count;\n\t\tcount = min(nbytes, (unsigned) PAGE_SIZE);\n\t\toffset = 0;\n\t}\n\treturn 0;\n}\n\n/* Copy a single argument in the request to/from userspace buffer */\nstatic int fuse_copy_one(struct fuse_copy_state *cs, void *val, unsigned size)\n{\n\twhile (size) {\n\t\tif (!cs->len) {\n\t\t\tint err = fuse_copy_fill(cs);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\t\tfuse_copy_do(cs, &val, &size);\n\t}\n\treturn 0;\n}\n\n/* Copy request arguments to/from userspace buffer */\nstatic int fuse_copy_args(struct fuse_copy_state *cs, unsigned numargs,\n\t\t\t  unsigned argpages, struct fuse_arg *args,\n\t\t\t  int zeroing)\n{\n\tint err = 0;\n\tunsigned i;\n\n\tfor (i = 0; !err && i < numargs; i++)  {\n\t\tstruct fuse_arg *arg = &args[i];\n\t\tif (i == numargs - 1 && argpages)\n\t\t\terr = fuse_copy_pages(cs, arg->size, zeroing);\n\t\telse\n\t\t\terr = fuse_copy_one(cs, arg->value, arg->size);\n\t}\n\treturn err;\n}\n\nstatic int forget_pending(struct fuse_conn *fc)\n{\n\treturn fc->forget_list_head.next != NULL;\n}\n\nstatic int request_pending(struct fuse_conn *fc)\n{\n\treturn !list_empty(&fc->pending) || !list_empty(&fc->interrupts) ||\n\t\tforget_pending(fc);\n}\n\n/* Wait until a request is available on the pending list */\nstatic void request_wait(struct fuse_conn *fc)\n__releases(fc->lock)\n__acquires(fc->lock)\n{\n\tDECLARE_WAITQUEUE(wait, current);\n\n\tadd_wait_queue_exclusive(&fc->waitq, &wait);\n\twhile (fc->connected && !request_pending(fc)) {\n\t\tset_current_state(TASK_INTERRUPTIBLE);\n\t\tif (signal_pending(current))\n\t\t\tbreak;\n\n\t\tspin_unlock(&fc->lock);\n\t\tschedule();\n\t\tspin_lock(&fc->lock);\n\t}\n\tset_current_state(TASK_RUNNING);\n\tremove_wait_queue(&fc->waitq, &wait);\n}\n\n/*\n * Transfer an interrupt request to userspace\n *\n * Unlike other requests this is assembled on demand, without a need\n * to allocate a separate fuse_req structure.\n *\n * Called with fc->lock held, releases it\n */\nstatic int fuse_read_interrupt(struct fuse_conn *fc, struct fuse_copy_state *cs,\n\t\t\t       size_t nbytes, struct fuse_req *req)\n__releases(fc->lock)\n{\n\tstruct fuse_in_header ih;\n\tstruct fuse_interrupt_in arg;\n\tunsigned reqsize = sizeof(ih) + sizeof(arg);\n\tint err;\n\n\tlist_del_init(&req->intr_entry);\n\treq->intr_unique = fuse_get_unique(fc);\n\tmemset(&ih, 0, sizeof(ih));\n\tmemset(&arg, 0, sizeof(arg));\n\tih.len = reqsize;\n\tih.opcode = FUSE_INTERRUPT;\n\tih.unique = req->intr_unique;\n\targ.unique = req->in.h.unique;\n\n\tspin_unlock(&fc->lock);\n\tif (nbytes < reqsize)\n\t\treturn -EINVAL;\n\n\terr = fuse_copy_one(cs, &ih, sizeof(ih));\n\tif (!err)\n\t\terr = fuse_copy_one(cs, &arg, sizeof(arg));\n\tfuse_copy_finish(cs);\n\n\treturn err ? err : reqsize;\n}\n\nstatic struct fuse_forget_link *dequeue_forget(struct fuse_conn *fc,\n\t\t\t\t\t       unsigned max,\n\t\t\t\t\t       unsigned *countp)\n{\n\tstruct fuse_forget_link *head = fc->forget_list_head.next;\n\tstruct fuse_forget_link **newhead = &head;\n\tunsigned count;\n\n\tfor (count = 0; *newhead != NULL && count < max; count++)\n\t\tnewhead = &(*newhead)->next;\n\n\tfc->forget_list_head.next = *newhead;\n\t*newhead = NULL;\n\tif (fc->forget_list_head.next == NULL)\n\t\tfc->forget_list_tail = &fc->forget_list_head;\n\n\tif (countp != NULL)\n\t\t*countp = count;\n\n\treturn head;\n}\n\nstatic int fuse_read_single_forget(struct fuse_conn *fc,\n\t\t\t\t   struct fuse_copy_state *cs,\n\t\t\t\t   size_t nbytes)\n__releases(fc->lock)\n{\n\tint err;\n\tstruct fuse_forget_link *forget = dequeue_forget(fc, 1, NULL);\n\tstruct fuse_forget_in arg = {\n\t\t.nlookup = forget->forget_one.nlookup,\n\t};\n\tstruct fuse_in_header ih = {\n\t\t.opcode = FUSE_FORGET,\n\t\t.nodeid = forget->forget_one.nodeid,\n\t\t.unique = fuse_get_unique(fc),\n\t\t.len = sizeof(ih) + sizeof(arg),\n\t};\n\n\tspin_unlock(&fc->lock);\n\tkfree(forget);\n\tif (nbytes < ih.len)\n\t\treturn -EINVAL;\n\n\terr = fuse_copy_one(cs, &ih, sizeof(ih));\n\tif (!err)\n\t\terr = fuse_copy_one(cs, &arg, sizeof(arg));\n\tfuse_copy_finish(cs);\n\n\tif (err)\n\t\treturn err;\n\n\treturn ih.len;\n}\n\nstatic int fuse_read_batch_forget(struct fuse_conn *fc,\n\t\t\t\t   struct fuse_copy_state *cs, size_t nbytes)\n__releases(fc->lock)\n{\n\tint err;\n\tunsigned max_forgets;\n\tunsigned count;\n\tstruct fuse_forget_link *head;\n\tstruct fuse_batch_forget_in arg = { .count = 0 };\n\tstruct fuse_in_header ih = {\n\t\t.opcode = FUSE_BATCH_FORGET,\n\t\t.unique = fuse_get_unique(fc),\n\t\t.len = sizeof(ih) + sizeof(arg),\n\t};\n\n\tif (nbytes < ih.len) {\n\t\tspin_unlock(&fc->lock);\n\t\treturn -EINVAL;\n\t}\n\n\tmax_forgets = (nbytes - ih.len) / sizeof(struct fuse_forget_one);\n\thead = dequeue_forget(fc, max_forgets, &count);\n\tspin_unlock(&fc->lock);\n\n\targ.count = count;\n\tih.len += count * sizeof(struct fuse_forget_one);\n\terr = fuse_copy_one(cs, &ih, sizeof(ih));\n\tif (!err)\n\t\terr = fuse_copy_one(cs, &arg, sizeof(arg));\n\n\twhile (head) {\n\t\tstruct fuse_forget_link *forget = head;\n\n\t\tif (!err) {\n\t\t\terr = fuse_copy_one(cs, &forget->forget_one,\n\t\t\t\t\t    sizeof(forget->forget_one));\n\t\t}\n\t\thead = forget->next;\n\t\tkfree(forget);\n\t}\n\n\tfuse_copy_finish(cs);\n\n\tif (err)\n\t\treturn err;\n\n\treturn ih.len;\n}\n\nstatic int fuse_read_forget(struct fuse_conn *fc, struct fuse_copy_state *cs,\n\t\t\t    size_t nbytes)\n__releases(fc->lock)\n{\n\tif (fc->minor < 16 || fc->forget_list_head.next->next == NULL)\n\t\treturn fuse_read_single_forget(fc, cs, nbytes);\n\telse\n\t\treturn fuse_read_batch_forget(fc, cs, nbytes);\n}\n\n/*\n * Read a single request into the userspace filesystem's buffer.  This\n * function waits until a request is available, then removes it from\n * the pending list and copies request data to userspace buffer.  If\n * no reply is needed (FORGET) or request has been aborted or there\n * was an error during the copying then it's finished by calling\n * request_end().  Otherwise add it to the processing list, and set\n * the 'sent' flag.\n */\nstatic ssize_t fuse_dev_do_read(struct fuse_conn *fc, struct file *file,\n\t\t\t\tstruct fuse_copy_state *cs, size_t nbytes)\n{\n\tint err;\n\tstruct fuse_req *req;\n\tstruct fuse_in *in;\n\tunsigned reqsize;\n\n restart:\n\tspin_lock(&fc->lock);\n\terr = -EAGAIN;\n\tif ((file->f_flags & O_NONBLOCK) && fc->connected &&\n\t    !request_pending(fc))\n\t\tgoto err_unlock;\n\n\trequest_wait(fc);\n\terr = -ENODEV;\n\tif (!fc->connected)\n\t\tgoto err_unlock;\n\terr = -ERESTARTSYS;\n\tif (!request_pending(fc))\n\t\tgoto err_unlock;\n\n\tif (!list_empty(&fc->interrupts)) {\n\t\treq = list_entry(fc->interrupts.next, struct fuse_req,\n\t\t\t\t intr_entry);\n\t\treturn fuse_read_interrupt(fc, cs, nbytes, req);\n\t}\n\n\tif (forget_pending(fc)) {\n\t\tif (list_empty(&fc->pending) || fc->forget_batch-- > 0)\n\t\t\treturn fuse_read_forget(fc, cs, nbytes);\n\n\t\tif (fc->forget_batch <= -8)\n\t\t\tfc->forget_batch = 16;\n\t}\n\n\treq = list_entry(fc->pending.next, struct fuse_req, list);\n\treq->state = FUSE_REQ_READING;\n\tlist_move(&req->list, &fc->io);\n\n\tin = &req->in;\n\treqsize = in->h.len;\n\t/* If request is too large, reply with an error and restart the read */\n\tif (nbytes < reqsize) {\n\t\treq->out.h.error = -EIO;\n\t\t/* SETXATTR is special, since it may contain too large data */\n\t\tif (in->h.opcode == FUSE_SETXATTR)\n\t\t\treq->out.h.error = -E2BIG;\n\t\trequest_end(fc, req);\n\t\tgoto restart;\n\t}\n\tspin_unlock(&fc->lock);\n\tcs->req = req;\n\terr = fuse_copy_one(cs, &in->h, sizeof(in->h));\n\tif (!err)\n\t\terr = fuse_copy_args(cs, in->numargs, in->argpages,\n\t\t\t\t     (struct fuse_arg *) in->args, 0);\n\tfuse_copy_finish(cs);\n\tspin_lock(&fc->lock);\n\treq->locked = 0;\n\tif (req->aborted) {\n\t\trequest_end(fc, req);\n\t\treturn -ENODEV;\n\t}\n\tif (err) {\n\t\treq->out.h.error = -EIO;\n\t\trequest_end(fc, req);\n\t\treturn err;\n\t}\n\tif (!req->isreply)\n\t\trequest_end(fc, req);\n\telse {\n\t\treq->state = FUSE_REQ_SENT;\n\t\tlist_move_tail(&req->list, &fc->processing);\n\t\tif (req->interrupted)\n\t\t\tqueue_interrupt(fc, req);\n\t\tspin_unlock(&fc->lock);\n\t}\n\treturn reqsize;\n\n err_unlock:\n\tspin_unlock(&fc->lock);\n\treturn err;\n}\n\nstatic ssize_t fuse_dev_read(struct kiocb *iocb, const struct iovec *iov,\n\t\t\t      unsigned long nr_segs, loff_t pos)\n{\n\tstruct fuse_copy_state cs;\n\tstruct file *file = iocb->ki_filp;\n\tstruct fuse_conn *fc = fuse_get_conn(file);\n\tif (!fc)\n\t\treturn -EPERM;\n\n\tfuse_copy_init(&cs, fc, 1, iov, nr_segs);\n\n\treturn fuse_dev_do_read(fc, file, &cs, iov_length(iov, nr_segs));\n}\n\nstatic int fuse_dev_pipe_buf_steal(struct pipe_inode_info *pipe,\n\t\t\t\t   struct pipe_buffer *buf)\n{\n\treturn 1;\n}\n\nstatic const struct pipe_buf_operations fuse_dev_pipe_buf_ops = {\n\t.can_merge = 0,\n\t.map = generic_pipe_buf_map,\n\t.unmap = generic_pipe_buf_unmap,\n\t.confirm = generic_pipe_buf_confirm,\n\t.release = generic_pipe_buf_release,\n\t.steal = fuse_dev_pipe_buf_steal,\n\t.get = generic_pipe_buf_get,\n};\n\nstatic ssize_t fuse_dev_splice_read(struct file *in, loff_t *ppos,\n\t\t\t\t    struct pipe_inode_info *pipe,\n\t\t\t\t    size_t len, unsigned int flags)\n{\n\tint ret;\n\tint page_nr = 0;\n\tint do_wakeup = 0;\n\tstruct pipe_buffer *bufs;\n\tstruct fuse_copy_state cs;\n\tstruct fuse_conn *fc = fuse_get_conn(in);\n\tif (!fc)\n\t\treturn -EPERM;\n\n\tbufs = kmalloc(pipe->buffers * sizeof(struct pipe_buffer), GFP_KERNEL);\n\tif (!bufs)\n\t\treturn -ENOMEM;\n\n\tfuse_copy_init(&cs, fc, 1, NULL, 0);\n\tcs.pipebufs = bufs;\n\tcs.pipe = pipe;\n\tret = fuse_dev_do_read(fc, in, &cs, len);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tret = 0;\n\tpipe_lock(pipe);\n\n\tif (!pipe->readers) {\n\t\tsend_sig(SIGPIPE, current, 0);\n\t\tif (!ret)\n\t\t\tret = -EPIPE;\n\t\tgoto out_unlock;\n\t}\n\n\tif (pipe->nrbufs + cs.nr_segs > pipe->buffers) {\n\t\tret = -EIO;\n\t\tgoto out_unlock;\n\t}\n\n\twhile (page_nr < cs.nr_segs) {\n\t\tint newbuf = (pipe->curbuf + pipe->nrbufs) & (pipe->buffers - 1);\n\t\tstruct pipe_buffer *buf = pipe->bufs + newbuf;\n\n\t\tbuf->page = bufs[page_nr].page;\n\t\tbuf->offset = bufs[page_nr].offset;\n\t\tbuf->len = bufs[page_nr].len;\n\t\tbuf->ops = &fuse_dev_pipe_buf_ops;\n\n\t\tpipe->nrbufs++;\n\t\tpage_nr++;\n\t\tret += buf->len;\n\n\t\tif (pipe->inode)\n\t\t\tdo_wakeup = 1;\n\t}\n\nout_unlock:\n\tpipe_unlock(pipe);\n\n\tif (do_wakeup) {\n\t\tsmp_mb();\n\t\tif (waitqueue_active(&pipe->wait))\n\t\t\twake_up_interruptible(&pipe->wait);\n\t\tkill_fasync(&pipe->fasync_readers, SIGIO, POLL_IN);\n\t}\n\nout:\n\tfor (; page_nr < cs.nr_segs; page_nr++)\n\t\tpage_cache_release(bufs[page_nr].page);\n\n\tkfree(bufs);\n\treturn ret;\n}\n\nstatic int fuse_notify_poll(struct fuse_conn *fc, unsigned int size,\n\t\t\t    struct fuse_copy_state *cs)\n{\n\tstruct fuse_notify_poll_wakeup_out outarg;\n\tint err = -EINVAL;\n\n\tif (size != sizeof(outarg))\n\t\tgoto err;\n\n\terr = fuse_copy_one(cs, &outarg, sizeof(outarg));\n\tif (err)\n\t\tgoto err;\n\n\tfuse_copy_finish(cs);\n\treturn fuse_notify_poll_wakeup(fc, &outarg);\n\nerr:\n\tfuse_copy_finish(cs);\n\treturn err;\n}\n\nstatic int fuse_notify_inval_inode(struct fuse_conn *fc, unsigned int size,\n\t\t\t\t   struct fuse_copy_state *cs)\n{\n\tstruct fuse_notify_inval_inode_out outarg;\n\tint err = -EINVAL;\n\n\tif (size != sizeof(outarg))\n\t\tgoto err;\n\n\terr = fuse_copy_one(cs, &outarg, sizeof(outarg));\n\tif (err)\n\t\tgoto err;\n\tfuse_copy_finish(cs);\n\n\tdown_read(&fc->killsb);\n\terr = -ENOENT;\n\tif (fc->sb) {\n\t\terr = fuse_reverse_inval_inode(fc->sb, outarg.ino,\n\t\t\t\t\t       outarg.off, outarg.len);\n\t}\n\tup_read(&fc->killsb);\n\treturn err;\n\nerr:\n\tfuse_copy_finish(cs);\n\treturn err;\n}\n\nstatic int fuse_notify_inval_entry(struct fuse_conn *fc, unsigned int size,\n\t\t\t\t   struct fuse_copy_state *cs)\n{\n\tstruct fuse_notify_inval_entry_out outarg;\n\tint err = -ENOMEM;\n\tchar *buf;\n\tstruct qstr name;\n\n\tbuf = kzalloc(FUSE_NAME_MAX + 1, GFP_KERNEL);\n\tif (!buf)\n\t\tgoto err;\n\n\terr = -EINVAL;\n\tif (size < sizeof(outarg))\n\t\tgoto err;\n\n\terr = fuse_copy_one(cs, &outarg, sizeof(outarg));\n\tif (err)\n\t\tgoto err;\n\n\terr = -ENAMETOOLONG;\n\tif (outarg.namelen > FUSE_NAME_MAX)\n\t\tgoto err;\n\n\terr = -EINVAL;\n\tif (size != sizeof(outarg) + outarg.namelen + 1)\n\t\tgoto err;\n\n\tname.name = buf;\n\tname.len = outarg.namelen;\n\terr = fuse_copy_one(cs, buf, outarg.namelen + 1);\n\tif (err)\n\t\tgoto err;\n\tfuse_copy_finish(cs);\n\tbuf[outarg.namelen] = 0;\n\tname.hash = full_name_hash(name.name, name.len);\n\n\tdown_read(&fc->killsb);\n\terr = -ENOENT;\n\tif (fc->sb)\n\t\terr = fuse_reverse_inval_entry(fc->sb, outarg.parent, &name);\n\tup_read(&fc->killsb);\n\tkfree(buf);\n\treturn err;\n\nerr:\n\tkfree(buf);\n\tfuse_copy_finish(cs);\n\treturn err;\n}\n\nstatic int fuse_notify_store(struct fuse_conn *fc, unsigned int size,\n\t\t\t     struct fuse_copy_state *cs)\n{\n\tstruct fuse_notify_store_out outarg;\n\tstruct inode *inode;\n\tstruct address_space *mapping;\n\tu64 nodeid;\n\tint err;\n\tpgoff_t index;\n\tunsigned int offset;\n\tunsigned int num;\n\tloff_t file_size;\n\tloff_t end;\n\n\terr = -EINVAL;\n\tif (size < sizeof(outarg))\n\t\tgoto out_finish;\n\n\terr = fuse_copy_one(cs, &outarg, sizeof(outarg));\n\tif (err)\n\t\tgoto out_finish;\n\n\terr = -EINVAL;\n\tif (size - sizeof(outarg) != outarg.size)\n\t\tgoto out_finish;\n\n\tnodeid = outarg.nodeid;\n\n\tdown_read(&fc->killsb);\n\n\terr = -ENOENT;\n\tif (!fc->sb)\n\t\tgoto out_up_killsb;\n\n\tinode = ilookup5(fc->sb, nodeid, fuse_inode_eq, &nodeid);\n\tif (!inode)\n\t\tgoto out_up_killsb;\n\n\tmapping = inode->i_mapping;\n\tindex = outarg.offset >> PAGE_CACHE_SHIFT;\n\toffset = outarg.offset & ~PAGE_CACHE_MASK;\n\tfile_size = i_size_read(inode);\n\tend = outarg.offset + outarg.size;\n\tif (end > file_size) {\n\t\tfile_size = end;\n\t\tfuse_write_update_size(inode, file_size);\n\t}\n\n\tnum = outarg.size;\n\twhile (num) {\n\t\tstruct page *page;\n\t\tunsigned int this_num;\n\n\t\terr = -ENOMEM;\n\t\tpage = find_or_create_page(mapping, index,\n\t\t\t\t\t   mapping_gfp_mask(mapping));\n\t\tif (!page)\n\t\t\tgoto out_iput;\n\n\t\tthis_num = min_t(unsigned, num, PAGE_CACHE_SIZE - offset);\n\t\terr = fuse_copy_page(cs, &page, offset, this_num, 0);\n\t\tif (!err && offset == 0 && (num != 0 || file_size == end))\n\t\t\tSetPageUptodate(page);\n\t\tunlock_page(page);\n\t\tpage_cache_release(page);\n\n\t\tif (err)\n\t\t\tgoto out_iput;\n\n\t\tnum -= this_num;\n\t\toffset = 0;\n\t\tindex++;\n\t}\n\n\terr = 0;\n\nout_iput:\n\tiput(inode);\nout_up_killsb:\n\tup_read(&fc->killsb);\nout_finish:\n\tfuse_copy_finish(cs);\n\treturn err;\n}\n\nstatic void fuse_retrieve_end(struct fuse_conn *fc, struct fuse_req *req)\n{\n\trelease_pages(req->pages, req->num_pages, 0);\n}\n\nstatic int fuse_retrieve(struct fuse_conn *fc, struct inode *inode,\n\t\t\t struct fuse_notify_retrieve_out *outarg)\n{\n\tint err;\n\tstruct address_space *mapping = inode->i_mapping;\n\tstruct fuse_req *req;\n\tpgoff_t index;\n\tloff_t file_size;\n\tunsigned int num;\n\tunsigned int offset;\n\tsize_t total_len = 0;\n\n\treq = fuse_get_req(fc);\n\tif (IS_ERR(req))\n\t\treturn PTR_ERR(req);\n\n\toffset = outarg->offset & ~PAGE_CACHE_MASK;\n\n\treq->in.h.opcode = FUSE_NOTIFY_REPLY;\n\treq->in.h.nodeid = outarg->nodeid;\n\treq->in.numargs = 2;\n\treq->in.argpages = 1;\n\treq->page_offset = offset;\n\treq->end = fuse_retrieve_end;\n\n\tindex = outarg->offset >> PAGE_CACHE_SHIFT;\n\tfile_size = i_size_read(inode);\n\tnum = outarg->size;\n\tif (outarg->offset > file_size)\n\t\tnum = 0;\n\telse if (outarg->offset + num > file_size)\n\t\tnum = file_size - outarg->offset;\n\n\twhile (num) {\n\t\tstruct page *page;\n\t\tunsigned int this_num;\n\n\t\tpage = find_get_page(mapping, index);\n\t\tif (!page)\n\t\t\tbreak;\n\n\t\tthis_num = min_t(unsigned, num, PAGE_CACHE_SIZE - offset);\n\t\treq->pages[req->num_pages] = page;\n\t\treq->num_pages++;\n\n\t\tnum -= this_num;\n\t\ttotal_len += this_num;\n\t}\n\treq->misc.retrieve_in.offset = outarg->offset;\n\treq->misc.retrieve_in.size = total_len;\n\treq->in.args[0].size = sizeof(req->misc.retrieve_in);\n\treq->in.args[0].value = &req->misc.retrieve_in;\n\treq->in.args[1].size = total_len;\n\n\terr = fuse_request_send_notify_reply(fc, req, outarg->notify_unique);\n\tif (err)\n\t\tfuse_retrieve_end(fc, req);\n\n\treturn err;\n}\n\nstatic int fuse_notify_retrieve(struct fuse_conn *fc, unsigned int size,\n\t\t\t\tstruct fuse_copy_state *cs)\n{\n\tstruct fuse_notify_retrieve_out outarg;\n\tstruct inode *inode;\n\tint err;\n\n\terr = -EINVAL;\n\tif (size != sizeof(outarg))\n\t\tgoto copy_finish;\n\n\terr = fuse_copy_one(cs, &outarg, sizeof(outarg));\n\tif (err)\n\t\tgoto copy_finish;\n\n\tfuse_copy_finish(cs);\n\n\tdown_read(&fc->killsb);\n\terr = -ENOENT;\n\tif (fc->sb) {\n\t\tu64 nodeid = outarg.nodeid;\n\n\t\tinode = ilookup5(fc->sb, nodeid, fuse_inode_eq, &nodeid);\n\t\tif (inode) {\n\t\t\terr = fuse_retrieve(fc, inode, &outarg);\n\t\t\tiput(inode);\n\t\t}\n\t}\n\tup_read(&fc->killsb);\n\n\treturn err;\n\ncopy_finish:\n\tfuse_copy_finish(cs);\n\treturn err;\n}\n\nstatic int fuse_notify(struct fuse_conn *fc, enum fuse_notify_code code,\n\t\t       unsigned int size, struct fuse_copy_state *cs)\n{\n\tswitch (code) {\n\tcase FUSE_NOTIFY_POLL:\n\t\treturn fuse_notify_poll(fc, size, cs);\n\n\tcase FUSE_NOTIFY_INVAL_INODE:\n\t\treturn fuse_notify_inval_inode(fc, size, cs);\n\n\tcase FUSE_NOTIFY_INVAL_ENTRY:\n\t\treturn fuse_notify_inval_entry(fc, size, cs);\n\n\tcase FUSE_NOTIFY_STORE:\n\t\treturn fuse_notify_store(fc, size, cs);\n\n\tcase FUSE_NOTIFY_RETRIEVE:\n\t\treturn fuse_notify_retrieve(fc, size, cs);\n\n\tdefault:\n\t\tfuse_copy_finish(cs);\n\t\treturn -EINVAL;\n\t}\n}\n\n/* Look up request on processing list by unique ID */\nstatic struct fuse_req *request_find(struct fuse_conn *fc, u64 unique)\n{\n\tstruct list_head *entry;\n\n\tlist_for_each(entry, &fc->processing) {\n\t\tstruct fuse_req *req;\n\t\treq = list_entry(entry, struct fuse_req, list);\n\t\tif (req->in.h.unique == unique || req->intr_unique == unique)\n\t\t\treturn req;\n\t}\n\treturn NULL;\n}\n\nstatic int copy_out_args(struct fuse_copy_state *cs, struct fuse_out *out,\n\t\t\t unsigned nbytes)\n{\n\tunsigned reqsize = sizeof(struct fuse_out_header);\n\n\tif (out->h.error)\n\t\treturn nbytes != reqsize ? -EINVAL : 0;\n\n\treqsize += len_args(out->numargs, out->args);\n\n\tif (reqsize < nbytes || (reqsize > nbytes && !out->argvar))\n\t\treturn -EINVAL;\n\telse if (reqsize > nbytes) {\n\t\tstruct fuse_arg *lastarg = &out->args[out->numargs-1];\n\t\tunsigned diffsize = reqsize - nbytes;\n\t\tif (diffsize > lastarg->size)\n\t\t\treturn -EINVAL;\n\t\tlastarg->size -= diffsize;\n\t}\n\treturn fuse_copy_args(cs, out->numargs, out->argpages, out->args,\n\t\t\t      out->page_zeroing);\n}\n\n/*\n * Write a single reply to a request.  First the header is copied from\n * the write buffer.  The request is then searched on the processing\n * list by the unique ID found in the header.  If found, then remove\n * it from the list and copy the rest of the buffer to the request.\n * The request is finished by calling request_end()\n */\nstatic ssize_t fuse_dev_do_write(struct fuse_conn *fc,\n\t\t\t\t struct fuse_copy_state *cs, size_t nbytes)\n{\n\tint err;\n\tstruct fuse_req *req;\n\tstruct fuse_out_header oh;\n\n\tif (nbytes < sizeof(struct fuse_out_header))\n\t\treturn -EINVAL;\n\n\terr = fuse_copy_one(cs, &oh, sizeof(oh));\n\tif (err)\n\t\tgoto err_finish;\n\n\terr = -EINVAL;\n\tif (oh.len != nbytes)\n\t\tgoto err_finish;\n\n\t/*\n\t * Zero oh.unique indicates unsolicited notification message\n\t * and error contains notification code.\n\t */\n\tif (!oh.unique) {\n\t\terr = fuse_notify(fc, oh.error, nbytes - sizeof(oh), cs);\n\t\treturn err ? err : nbytes;\n\t}\n\n\terr = -EINVAL;\n\tif (oh.error <= -1000 || oh.error > 0)\n\t\tgoto err_finish;\n\n\tspin_lock(&fc->lock);\n\terr = -ENOENT;\n\tif (!fc->connected)\n\t\tgoto err_unlock;\n\n\treq = request_find(fc, oh.unique);\n\tif (!req)\n\t\tgoto err_unlock;\n\n\tif (req->aborted) {\n\t\tspin_unlock(&fc->lock);\n\t\tfuse_copy_finish(cs);\n\t\tspin_lock(&fc->lock);\n\t\trequest_end(fc, req);\n\t\treturn -ENOENT;\n\t}\n\t/* Is it an interrupt reply? */\n\tif (req->intr_unique == oh.unique) {\n\t\terr = -EINVAL;\n\t\tif (nbytes != sizeof(struct fuse_out_header))\n\t\t\tgoto err_unlock;\n\n\t\tif (oh.error == -ENOSYS)\n\t\t\tfc->no_interrupt = 1;\n\t\telse if (oh.error == -EAGAIN)\n\t\t\tqueue_interrupt(fc, req);\n\n\t\tspin_unlock(&fc->lock);\n\t\tfuse_copy_finish(cs);\n\t\treturn nbytes;\n\t}\n\n\treq->state = FUSE_REQ_WRITING;\n\tlist_move(&req->list, &fc->io);\n\treq->out.h = oh;\n\treq->locked = 1;\n\tcs->req = req;\n\tif (!req->out.page_replace)\n\t\tcs->move_pages = 0;\n\tspin_unlock(&fc->lock);\n\n\terr = copy_out_args(cs, &req->out, nbytes);\n\tfuse_copy_finish(cs);\n\n\tspin_lock(&fc->lock);\n\treq->locked = 0;\n\tif (!err) {\n\t\tif (req->aborted)\n\t\t\terr = -ENOENT;\n\t} else if (!req->aborted)\n\t\treq->out.h.error = -EIO;\n\trequest_end(fc, req);\n\n\treturn err ? err : nbytes;\n\n err_unlock:\n\tspin_unlock(&fc->lock);\n err_finish:\n\tfuse_copy_finish(cs);\n\treturn err;\n}\n\nstatic ssize_t fuse_dev_write(struct kiocb *iocb, const struct iovec *iov,\n\t\t\t      unsigned long nr_segs, loff_t pos)\n{\n\tstruct fuse_copy_state cs;\n\tstruct fuse_conn *fc = fuse_get_conn(iocb->ki_filp);\n\tif (!fc)\n\t\treturn -EPERM;\n\n\tfuse_copy_init(&cs, fc, 0, iov, nr_segs);\n\n\treturn fuse_dev_do_write(fc, &cs, iov_length(iov, nr_segs));\n}\n\nstatic ssize_t fuse_dev_splice_write(struct pipe_inode_info *pipe,\n\t\t\t\t     struct file *out, loff_t *ppos,\n\t\t\t\t     size_t len, unsigned int flags)\n{\n\tunsigned nbuf;\n\tunsigned idx;\n\tstruct pipe_buffer *bufs;\n\tstruct fuse_copy_state cs;\n\tstruct fuse_conn *fc;\n\tsize_t rem;\n\tssize_t ret;\n\n\tfc = fuse_get_conn(out);\n\tif (!fc)\n\t\treturn -EPERM;\n\n\tbufs = kmalloc(pipe->buffers * sizeof(struct pipe_buffer), GFP_KERNEL);\n\tif (!bufs)\n\t\treturn -ENOMEM;\n\n\tpipe_lock(pipe);\n\tnbuf = 0;\n\trem = 0;\n\tfor (idx = 0; idx < pipe->nrbufs && rem < len; idx++)\n\t\trem += pipe->bufs[(pipe->curbuf + idx) & (pipe->buffers - 1)].len;\n\n\tret = -EINVAL;\n\tif (rem < len) {\n\t\tpipe_unlock(pipe);\n\t\tgoto out;\n\t}\n\n\trem = len;\n\twhile (rem) {\n\t\tstruct pipe_buffer *ibuf;\n\t\tstruct pipe_buffer *obuf;\n\n\t\tBUG_ON(nbuf >= pipe->buffers);\n\t\tBUG_ON(!pipe->nrbufs);\n\t\tibuf = &pipe->bufs[pipe->curbuf];\n\t\tobuf = &bufs[nbuf];\n\n\t\tif (rem >= ibuf->len) {\n\t\t\t*obuf = *ibuf;\n\t\t\tibuf->ops = NULL;\n\t\t\tpipe->curbuf = (pipe->curbuf + 1) & (pipe->buffers - 1);\n\t\t\tpipe->nrbufs--;\n\t\t} else {\n\t\t\tibuf->ops->get(pipe, ibuf);\n\t\t\t*obuf = *ibuf;\n\t\t\tobuf->flags &= ~PIPE_BUF_FLAG_GIFT;\n\t\t\tobuf->len = rem;\n\t\t\tibuf->offset += obuf->len;\n\t\t\tibuf->len -= obuf->len;\n\t\t}\n\t\tnbuf++;\n\t\trem -= obuf->len;\n\t}\n\tpipe_unlock(pipe);\n\n\tfuse_copy_init(&cs, fc, 0, NULL, nbuf);\n\tcs.pipebufs = bufs;\n\tcs.pipe = pipe;\n\n\tif (flags & SPLICE_F_MOVE)\n\t\tcs.move_pages = 1;\n\n\tret = fuse_dev_do_write(fc, &cs, len);\n\n\tfor (idx = 0; idx < nbuf; idx++) {\n\t\tstruct pipe_buffer *buf = &bufs[idx];\n\t\tbuf->ops->release(pipe, buf);\n\t}\nout:\n\tkfree(bufs);\n\treturn ret;\n}\n\nstatic unsigned fuse_dev_poll(struct file *file, poll_table *wait)\n{\n\tunsigned mask = POLLOUT | POLLWRNORM;\n\tstruct fuse_conn *fc = fuse_get_conn(file);\n\tif (!fc)\n\t\treturn POLLERR;\n\n\tpoll_wait(file, &fc->waitq, wait);\n\n\tspin_lock(&fc->lock);\n\tif (!fc->connected)\n\t\tmask = POLLERR;\n\telse if (request_pending(fc))\n\t\tmask |= POLLIN | POLLRDNORM;\n\tspin_unlock(&fc->lock);\n\n\treturn mask;\n}\n\n/*\n * Abort all requests on the given list (pending or processing)\n *\n * This function releases and reacquires fc->lock\n */\nstatic void end_requests(struct fuse_conn *fc, struct list_head *head)\n__releases(fc->lock)\n__acquires(fc->lock)\n{\n\twhile (!list_empty(head)) {\n\t\tstruct fuse_req *req;\n\t\treq = list_entry(head->next, struct fuse_req, list);\n\t\treq->out.h.error = -ECONNABORTED;\n\t\trequest_end(fc, req);\n\t\tspin_lock(&fc->lock);\n\t}\n}\n\n/*\n * Abort requests under I/O\n *\n * The requests are set to aborted and finished, and the request\n * waiter is woken up.  This will make request_wait_answer() wait\n * until the request is unlocked and then return.\n *\n * If the request is asynchronous, then the end function needs to be\n * called after waiting for the request to be unlocked (if it was\n * locked).\n */\nstatic void end_io_requests(struct fuse_conn *fc)\n__releases(fc->lock)\n__acquires(fc->lock)\n{\n\twhile (!list_empty(&fc->io)) {\n\t\tstruct fuse_req *req =\n\t\t\tlist_entry(fc->io.next, struct fuse_req, list);\n\t\tvoid (*end) (struct fuse_conn *, struct fuse_req *) = req->end;\n\n\t\treq->aborted = 1;\n\t\treq->out.h.error = -ECONNABORTED;\n\t\treq->state = FUSE_REQ_FINISHED;\n\t\tlist_del_init(&req->list);\n\t\twake_up(&req->waitq);\n\t\tif (end) {\n\t\t\treq->end = NULL;\n\t\t\t__fuse_get_request(req);\n\t\t\tspin_unlock(&fc->lock);\n\t\t\twait_event(req->waitq, !req->locked);\n\t\t\tend(fc, req);\n\t\t\tfuse_put_request(fc, req);\n\t\t\tspin_lock(&fc->lock);\n\t\t}\n\t}\n}\n\nstatic void end_queued_requests(struct fuse_conn *fc)\n__releases(fc->lock)\n__acquires(fc->lock)\n{\n\tfc->max_background = UINT_MAX;\n\tflush_bg_queue(fc);\n\tend_requests(fc, &fc->pending);\n\tend_requests(fc, &fc->processing);\n\twhile (forget_pending(fc))\n\t\tkfree(dequeue_forget(fc, 1, NULL));\n}\n\nstatic void end_polls(struct fuse_conn *fc)\n{\n\tstruct rb_node *p;\n\n\tp = rb_first(&fc->polled_files);\n\n\twhile (p) {\n\t\tstruct fuse_file *ff;\n\t\tff = rb_entry(p, struct fuse_file, polled_node);\n\t\twake_up_interruptible_all(&ff->poll_wait);\n\n\t\tp = rb_next(p);\n\t}\n}\n\n/*\n * Abort all requests.\n *\n * Emergency exit in case of a malicious or accidental deadlock, or\n * just a hung filesystem.\n *\n * The same effect is usually achievable through killing the\n * filesystem daemon and all users of the filesystem.  The exception\n * is the combination of an asynchronous request and the tricky\n * deadlock (see Documentation/filesystems/fuse.txt).\n *\n * During the aborting, progression of requests from the pending and\n * processing lists onto the io list, and progression of new requests\n * onto the pending list is prevented by req->connected being false.\n *\n * Progression of requests under I/O to the processing list is\n * prevented by the req->aborted flag being true for these requests.\n * For this reason requests on the io list must be aborted first.\n */\nvoid fuse_abort_conn(struct fuse_conn *fc)\n{\n\tspin_lock(&fc->lock);\n\tif (fc->connected) {\n\t\tfc->connected = 0;\n\t\tfc->blocked = 0;\n\t\tend_io_requests(fc);\n\t\tend_queued_requests(fc);\n\t\tend_polls(fc);\n\t\twake_up_all(&fc->waitq);\n\t\twake_up_all(&fc->blocked_waitq);\n\t\tkill_fasync(&fc->fasync, SIGIO, POLL_IN);\n\t}\n\tspin_unlock(&fc->lock);\n}\nEXPORT_SYMBOL_GPL(fuse_abort_conn);\n\nint fuse_dev_release(struct inode *inode, struct file *file)\n{\n\tstruct fuse_conn *fc = fuse_get_conn(file);\n\tif (fc) {\n\t\tspin_lock(&fc->lock);\n\t\tfc->connected = 0;\n\t\tfc->blocked = 0;\n\t\tend_queued_requests(fc);\n\t\tend_polls(fc);\n\t\twake_up_all(&fc->blocked_waitq);\n\t\tspin_unlock(&fc->lock);\n\t\tfuse_conn_put(fc);\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(fuse_dev_release);\n\nstatic int fuse_dev_fasync(int fd, struct file *file, int on)\n{\n\tstruct fuse_conn *fc = fuse_get_conn(file);\n\tif (!fc)\n\t\treturn -EPERM;\n\n\t/* No locking - fasync_helper does its own locking */\n\treturn fasync_helper(fd, file, on, &fc->fasync);\n}\n\nconst struct file_operations fuse_dev_operations = {\n\t.owner\t\t= THIS_MODULE,\n\t.llseek\t\t= no_llseek,\n\t.read\t\t= do_sync_read,\n\t.aio_read\t= fuse_dev_read,\n\t.splice_read\t= fuse_dev_splice_read,\n\t.write\t\t= do_sync_write,\n\t.aio_write\t= fuse_dev_write,\n\t.splice_write\t= fuse_dev_splice_write,\n\t.poll\t\t= fuse_dev_poll,\n\t.release\t= fuse_dev_release,\n\t.fasync\t\t= fuse_dev_fasync,\n};\nEXPORT_SYMBOL_GPL(fuse_dev_operations);\n\nstatic struct miscdevice fuse_miscdevice = {\n\t.minor = FUSE_MINOR,\n\t.name  = \"fuse\",\n\t.fops = &fuse_dev_operations,\n};\n\nint __init fuse_dev_init(void)\n{\n\tint err = -ENOMEM;\n\tfuse_req_cachep = kmem_cache_create(\"fuse_request\",\n\t\t\t\t\t    sizeof(struct fuse_req),\n\t\t\t\t\t    0, 0, NULL);\n\tif (!fuse_req_cachep)\n\t\tgoto out;\n\n\terr = misc_register(&fuse_miscdevice);\n\tif (err)\n\t\tgoto out_cache_clean;\n\n\treturn 0;\n\n out_cache_clean:\n\tkmem_cache_destroy(fuse_req_cachep);\n out:\n\treturn err;\n}\n\nvoid fuse_dev_cleanup(void)\n{\n\tmisc_deregister(&fuse_miscdevice);\n\tkmem_cache_destroy(fuse_req_cachep);\n}\n"], "buggy_code_start_loc": [1360], "buggy_code_end_loc": [1360], "fixing_code_start_loc": [1361], "fixing_code_end_loc": [1365], "type": "CWE-120", "message": "Buffer overflow in the fuse_notify_inval_entry function in fs/fuse/dev.c in the Linux kernel before 3.1 allows local users to cause a denial of service (BUG_ON and system crash) by leveraging the ability to mount a FUSE filesystem.", "other": {"cve": {"id": "CVE-2011-3353", "sourceIdentifier": "secalert@redhat.com", "published": "2012-05-24T23:55:02.307", "lastModified": "2023-02-13T01:20:46.850", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "Buffer overflow in the fuse_notify_inval_entry function in fs/fuse/dev.c in the Linux kernel before 3.1 allows local users to cause a denial of service (BUG_ON and system crash) by leveraging the ability to mount a FUSE filesystem."}, {"lang": "es", "value": "Desbordamiento de b\u00fafer en la funci\u00f3n fuse_notify_inval_entry function in fs/fuse/dev.c en el kernel de Linux antes de v3.1 permite a usuarios locales causar una denegaci\u00f3n de servicio (BUG_ON y ca\u00edda del sistema) mediante el aprovechamiento de la capacidad de montar un sistema de archivos FUSE."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:N/I:N/A:C", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "COMPLETE", "baseScore": 4.9}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.9, "impactScore": 6.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-120"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "3.1", "matchCriteriaId": "156989A4-23D9-434A-B512-9C0F3583D13D"}]}]}], "references": [{"url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux-2.6.git%3Ba=commit%3Bh=c2183d1e9b3f313dd8ba2b1b0197c8d9fb86a7ae", "source": "secalert@redhat.com"}, {"url": "http://www.kernel.org/pub/linux/kernel/v3.x/ChangeLog-3.1", "source": "secalert@redhat.com", "tags": ["Mailing List", "Patch", "Vendor Advisory"]}, {"url": "http://www.openwall.com/lists/oss-security/2011/09/09/6", "source": "secalert@redhat.com", "tags": ["Mailing List", "Patch", "Third Party Advisory"]}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=736761", "source": "secalert@redhat.com", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/c2183d1e9b3f313dd8ba2b1b0197c8d9fb86a7ae", "source": "secalert@redhat.com", "tags": ["Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/c2183d1e9b3f313dd8ba2b1b0197c8d9fb86a7ae"}}