{"buggy_code": ["#ifndef _ASM_X86_PTRACE_H\n#define _ASM_X86_PTRACE_H\n\n#include <asm/segment.h>\n#include <asm/page_types.h>\n#include <uapi/asm/ptrace.h>\n\n#ifndef __ASSEMBLY__\n#ifdef __i386__\n\nstruct pt_regs {\n\tunsigned long bx;\n\tunsigned long cx;\n\tunsigned long dx;\n\tunsigned long si;\n\tunsigned long di;\n\tunsigned long bp;\n\tunsigned long ax;\n\tunsigned long ds;\n\tunsigned long es;\n\tunsigned long fs;\n\tunsigned long gs;\n\tunsigned long orig_ax;\n\tunsigned long ip;\n\tunsigned long cs;\n\tunsigned long flags;\n\tunsigned long sp;\n\tunsigned long ss;\n};\n\n#else /* __i386__ */\n\nstruct pt_regs {\n\tunsigned long r15;\n\tunsigned long r14;\n\tunsigned long r13;\n\tunsigned long r12;\n\tunsigned long bp;\n\tunsigned long bx;\n/* arguments: non interrupts/non tracing syscalls only save up to here*/\n\tunsigned long r11;\n\tunsigned long r10;\n\tunsigned long r9;\n\tunsigned long r8;\n\tunsigned long ax;\n\tunsigned long cx;\n\tunsigned long dx;\n\tunsigned long si;\n\tunsigned long di;\n\tunsigned long orig_ax;\n/* end of arguments */\n/* cpu exception frame or undefined */\n\tunsigned long ip;\n\tunsigned long cs;\n\tunsigned long flags;\n\tunsigned long sp;\n\tunsigned long ss;\n/* top of stack page */\n};\n\n#endif /* !__i386__ */\n\n#ifdef CONFIG_PARAVIRT\n#include <asm/paravirt_types.h>\n#endif\n\nstruct cpuinfo_x86;\nstruct task_struct;\n\nextern unsigned long profile_pc(struct pt_regs *regs);\n#define profile_pc profile_pc\n\nextern unsigned long\nconvert_ip_to_linear(struct task_struct *child, struct pt_regs *regs);\nextern void send_sigtrap(struct task_struct *tsk, struct pt_regs *regs,\n\t\t\t int error_code, int si_code);\n\nextern long syscall_trace_enter(struct pt_regs *);\nextern void syscall_trace_leave(struct pt_regs *);\n\nstatic inline unsigned long regs_return_value(struct pt_regs *regs)\n{\n\treturn regs->ax;\n}\n\n/*\n * user_mode_vm(regs) determines whether a register set came from user mode.\n * This is true if V8086 mode was enabled OR if the register set was from\n * protected mode with RPL-3 CS value.  This tricky test checks that with\n * one comparison.  Many places in the kernel can bypass this full check\n * if they have already ruled out V8086 mode, so user_mode(regs) can be used.\n */\nstatic inline int user_mode(struct pt_regs *regs)\n{\n#ifdef CONFIG_X86_32\n\treturn (regs->cs & SEGMENT_RPL_MASK) == USER_RPL;\n#else\n\treturn !!(regs->cs & 3);\n#endif\n}\n\nstatic inline int user_mode_vm(struct pt_regs *regs)\n{\n#ifdef CONFIG_X86_32\n\treturn ((regs->cs & SEGMENT_RPL_MASK) | (regs->flags & X86_VM_MASK)) >=\n\t\tUSER_RPL;\n#else\n\treturn user_mode(regs);\n#endif\n}\n\nstatic inline int v8086_mode(struct pt_regs *regs)\n{\n#ifdef CONFIG_X86_32\n\treturn (regs->flags & X86_VM_MASK);\n#else\n\treturn 0;\t/* No V86 mode support in long mode */\n#endif\n}\n\n#ifdef CONFIG_X86_64\nstatic inline bool user_64bit_mode(struct pt_regs *regs)\n{\n#ifndef CONFIG_PARAVIRT\n\t/*\n\t * On non-paravirt systems, this is the only long mode CPL 3\n\t * selector.  We do not allow long mode selectors in the LDT.\n\t */\n\treturn regs->cs == __USER_CS;\n#else\n\t/* Headers are too twisted for this to go in paravirt.h. */\n\treturn regs->cs == __USER_CS || regs->cs == pv_info.extra_user_64bit_cs;\n#endif\n}\n\n#define current_user_stack_pointer()\tthis_cpu_read(old_rsp)\n/* ia32 vs. x32 difference */\n#define compat_user_stack_pointer()\t\\\n\t(test_thread_flag(TIF_IA32) \t\\\n\t ? current_pt_regs()->sp \t\\\n\t : this_cpu_read(old_rsp))\n#endif\n\n#ifdef CONFIG_X86_32\nextern unsigned long kernel_stack_pointer(struct pt_regs *regs);\n#else\nstatic inline unsigned long kernel_stack_pointer(struct pt_regs *regs)\n{\n\treturn regs->sp;\n}\n#endif\n\n#define GET_IP(regs) ((regs)->ip)\n#define GET_FP(regs) ((regs)->bp)\n#define GET_USP(regs) ((regs)->sp)\n\n#include <asm-generic/ptrace.h>\n\n/* Query offset/name of register from its name/offset */\nextern int regs_query_register_offset(const char *name);\nextern const char *regs_query_register_name(unsigned int offset);\n#define MAX_REG_OFFSET (offsetof(struct pt_regs, ss))\n\n/**\n * regs_get_register() - get register value from its offset\n * @regs:\tpt_regs from which register value is gotten.\n * @offset:\toffset number of the register.\n *\n * regs_get_register returns the value of a register. The @offset is the\n * offset of the register in struct pt_regs address which specified by @regs.\n * If @offset is bigger than MAX_REG_OFFSET, this returns 0.\n */\nstatic inline unsigned long regs_get_register(struct pt_regs *regs,\n\t\t\t\t\t      unsigned int offset)\n{\n\tif (unlikely(offset > MAX_REG_OFFSET))\n\t\treturn 0;\n#ifdef CONFIG_X86_32\n\t/*\n\t * Traps from the kernel do not save sp and ss.\n\t * Use the helper function to retrieve sp.\n\t */\n\tif (offset == offsetof(struct pt_regs, sp) &&\n\t    regs->cs == __KERNEL_CS)\n\t\treturn kernel_stack_pointer(regs);\n#endif\n\treturn *(unsigned long *)((unsigned long)regs + offset);\n}\n\n/**\n * regs_within_kernel_stack() - check the address in the stack\n * @regs:\tpt_regs which contains kernel stack pointer.\n * @addr:\taddress which is checked.\n *\n * regs_within_kernel_stack() checks @addr is within the kernel stack page(s).\n * If @addr is within the kernel stack, it returns true. If not, returns false.\n */\nstatic inline int regs_within_kernel_stack(struct pt_regs *regs,\n\t\t\t\t\t   unsigned long addr)\n{\n\treturn ((addr & ~(THREAD_SIZE - 1))  ==\n\t\t(kernel_stack_pointer(regs) & ~(THREAD_SIZE - 1)));\n}\n\n/**\n * regs_get_kernel_stack_nth() - get Nth entry of the stack\n * @regs:\tpt_regs which contains kernel stack pointer.\n * @n:\t\tstack entry number.\n *\n * regs_get_kernel_stack_nth() returns @n th entry of the kernel stack which\n * is specified by @regs. If the @n th entry is NOT in the kernel stack,\n * this returns 0.\n */\nstatic inline unsigned long regs_get_kernel_stack_nth(struct pt_regs *regs,\n\t\t\t\t\t\t      unsigned int n)\n{\n\tunsigned long *addr = (unsigned long *)kernel_stack_pointer(regs);\n\taddr += n;\n\tif (regs_within_kernel_stack(regs, (unsigned long)addr))\n\t\treturn *addr;\n\telse\n\t\treturn 0;\n}\n\n#define arch_has_single_step()\t(1)\n#ifdef CONFIG_X86_DEBUGCTLMSR\n#define arch_has_block_step()\t(1)\n#else\n#define arch_has_block_step()\t(boot_cpu_data.x86 >= 6)\n#endif\n\n#define ARCH_HAS_USER_SINGLE_STEP_INFO\n\nstruct user_desc;\nextern int do_get_thread_area(struct task_struct *p, int idx,\n\t\t\t      struct user_desc __user *info);\nextern int do_set_thread_area(struct task_struct *p, int idx,\n\t\t\t      struct user_desc __user *info, int can_allocate);\n\n#endif /* !__ASSEMBLY__ */\n#endif /* _ASM_X86_PTRACE_H */\n", "#ifndef _LINUX_PTRACE_H\n#define _LINUX_PTRACE_H\n\n#include <linux/compiler.h>\t\t/* For unlikely.  */\n#include <linux/sched.h>\t\t/* For struct task_struct.  */\n#include <linux/err.h>\t\t\t/* for IS_ERR_VALUE */\n#include <linux/bug.h>\t\t\t/* For BUG_ON.  */\n#include <linux/pid_namespace.h>\t/* For task_active_pid_ns.  */\n#include <uapi/linux/ptrace.h>\n\n/*\n * Ptrace flags\n *\n * The owner ship rules for task->ptrace which holds the ptrace\n * flags is simple.  When a task is running it owns it's task->ptrace\n * flags.  When the a task is stopped the ptracer owns task->ptrace.\n */\n\n#define PT_SEIZED\t0x00010000\t/* SEIZE used, enable new behavior */\n#define PT_PTRACED\t0x00000001\n#define PT_DTRACE\t0x00000002\t/* delayed trace (used on m68k, i386) */\n#define PT_PTRACE_CAP\t0x00000004\t/* ptracer can follow suid-exec */\n\n#define PT_OPT_FLAG_SHIFT\t3\n/* PT_TRACE_* event enable flags */\n#define PT_EVENT_FLAG(event)\t(1 << (PT_OPT_FLAG_SHIFT + (event)))\n#define PT_TRACESYSGOOD\t\tPT_EVENT_FLAG(0)\n#define PT_TRACE_FORK\t\tPT_EVENT_FLAG(PTRACE_EVENT_FORK)\n#define PT_TRACE_VFORK\t\tPT_EVENT_FLAG(PTRACE_EVENT_VFORK)\n#define PT_TRACE_CLONE\t\tPT_EVENT_FLAG(PTRACE_EVENT_CLONE)\n#define PT_TRACE_EXEC\t\tPT_EVENT_FLAG(PTRACE_EVENT_EXEC)\n#define PT_TRACE_VFORK_DONE\tPT_EVENT_FLAG(PTRACE_EVENT_VFORK_DONE)\n#define PT_TRACE_EXIT\t\tPT_EVENT_FLAG(PTRACE_EVENT_EXIT)\n#define PT_TRACE_SECCOMP\tPT_EVENT_FLAG(PTRACE_EVENT_SECCOMP)\n\n#define PT_EXITKILL\t\t(PTRACE_O_EXITKILL << PT_OPT_FLAG_SHIFT)\n\n/* single stepping state bits (used on ARM and PA-RISC) */\n#define PT_SINGLESTEP_BIT\t31\n#define PT_SINGLESTEP\t\t(1<<PT_SINGLESTEP_BIT)\n#define PT_BLOCKSTEP_BIT\t30\n#define PT_BLOCKSTEP\t\t(1<<PT_BLOCKSTEP_BIT)\n\nextern long arch_ptrace(struct task_struct *child, long request,\n\t\t\tunsigned long addr, unsigned long data);\nextern int ptrace_readdata(struct task_struct *tsk, unsigned long src, char __user *dst, int len);\nextern int ptrace_writedata(struct task_struct *tsk, char __user *src, unsigned long dst, int len);\nextern void ptrace_disable(struct task_struct *);\nextern int ptrace_request(struct task_struct *child, long request,\n\t\t\t  unsigned long addr, unsigned long data);\nextern void ptrace_notify(int exit_code);\nextern void __ptrace_link(struct task_struct *child,\n\t\t\t  struct task_struct *new_parent);\nextern void __ptrace_unlink(struct task_struct *child);\nextern void exit_ptrace(struct task_struct *tracer);\n#define PTRACE_MODE_READ\t0x01\n#define PTRACE_MODE_ATTACH\t0x02\n#define PTRACE_MODE_NOAUDIT\t0x04\n/* Returns true on success, false on denial. */\nextern bool ptrace_may_access(struct task_struct *task, unsigned int mode);\n\nstatic inline int ptrace_reparented(struct task_struct *child)\n{\n\treturn !same_thread_group(child->real_parent, child->parent);\n}\n\nstatic inline void ptrace_unlink(struct task_struct *child)\n{\n\tif (unlikely(child->ptrace))\n\t\t__ptrace_unlink(child);\n}\n\nint generic_ptrace_peekdata(struct task_struct *tsk, unsigned long addr,\n\t\t\t    unsigned long data);\nint generic_ptrace_pokedata(struct task_struct *tsk, unsigned long addr,\n\t\t\t    unsigned long data);\n\n/**\n * ptrace_parent - return the task that is tracing the given task\n * @task: task to consider\n *\n * Returns %NULL if no one is tracing @task, or the &struct task_struct\n * pointer to its tracer.\n *\n * Must called under rcu_read_lock().  The pointer returned might be kept\n * live only by RCU.  During exec, this may be called with task_lock() held\n * on @task, still held from when check_unsafe_exec() was called.\n */\nstatic inline struct task_struct *ptrace_parent(struct task_struct *task)\n{\n\tif (unlikely(task->ptrace))\n\t\treturn rcu_dereference(task->parent);\n\treturn NULL;\n}\n\n/**\n * ptrace_event_enabled - test whether a ptrace event is enabled\n * @task: ptracee of interest\n * @event: %PTRACE_EVENT_* to test\n *\n * Test whether @event is enabled for ptracee @task.\n *\n * Returns %true if @event is enabled, %false otherwise.\n */\nstatic inline bool ptrace_event_enabled(struct task_struct *task, int event)\n{\n\treturn task->ptrace & PT_EVENT_FLAG(event);\n}\n\n/**\n * ptrace_event - possibly stop for a ptrace event notification\n * @event:\t%PTRACE_EVENT_* value to report\n * @message:\tvalue for %PTRACE_GETEVENTMSG to return\n *\n * Check whether @event is enabled and, if so, report @event and @message\n * to the ptrace parent.\n *\n * Called without locks.\n */\nstatic inline void ptrace_event(int event, unsigned long message)\n{\n\tif (unlikely(ptrace_event_enabled(current, event))) {\n\t\tcurrent->ptrace_message = message;\n\t\tptrace_notify((event << 8) | SIGTRAP);\n\t} else if (event == PTRACE_EVENT_EXEC) {\n\t\t/* legacy EXEC report via SIGTRAP */\n\t\tif ((current->ptrace & (PT_PTRACED|PT_SEIZED)) == PT_PTRACED)\n\t\t\tsend_sig(SIGTRAP, current, 0);\n\t}\n}\n\n/**\n * ptrace_event_pid - possibly stop for a ptrace event notification\n * @event:\t%PTRACE_EVENT_* value to report\n * @pid:\tprocess identifier for %PTRACE_GETEVENTMSG to return\n *\n * Check whether @event is enabled and, if so, report @event and @pid\n * to the ptrace parent.  @pid is reported as the pid_t seen from the\n * the ptrace parent's pid namespace.\n *\n * Called without locks.\n */\nstatic inline void ptrace_event_pid(int event, struct pid *pid)\n{\n\t/*\n\t * FIXME: There's a potential race if a ptracer in a different pid\n\t * namespace than parent attaches between computing message below and\n\t * when we acquire tasklist_lock in ptrace_stop().  If this happens,\n\t * the ptracer will get a bogus pid from PTRACE_GETEVENTMSG.\n\t */\n\tunsigned long message = 0;\n\tstruct pid_namespace *ns;\n\n\trcu_read_lock();\n\tns = task_active_pid_ns(rcu_dereference(current->parent));\n\tif (ns)\n\t\tmessage = pid_nr_ns(pid, ns);\n\trcu_read_unlock();\n\n\tptrace_event(event, message);\n}\n\n/**\n * ptrace_init_task - initialize ptrace state for a new child\n * @child:\t\tnew child task\n * @ptrace:\t\ttrue if child should be ptrace'd by parent's tracer\n *\n * This is called immediately after adding @child to its parent's children\n * list.  @ptrace is false in the normal case, and true to ptrace @child.\n *\n * Called with current's siglock and write_lock_irq(&tasklist_lock) held.\n */\nstatic inline void ptrace_init_task(struct task_struct *child, bool ptrace)\n{\n\tINIT_LIST_HEAD(&child->ptrace_entry);\n\tINIT_LIST_HEAD(&child->ptraced);\n\tchild->jobctl = 0;\n\tchild->ptrace = 0;\n\tchild->parent = child->real_parent;\n\n\tif (unlikely(ptrace) && current->ptrace) {\n\t\tchild->ptrace = current->ptrace;\n\t\t__ptrace_link(child, current->parent);\n\n\t\tif (child->ptrace & PT_SEIZED)\n\t\t\ttask_set_jobctl_pending(child, JOBCTL_TRAP_STOP);\n\t\telse\n\t\t\tsigaddset(&child->pending.signal, SIGSTOP);\n\n\t\tset_tsk_thread_flag(child, TIF_SIGPENDING);\n\t}\n}\n\n/**\n * ptrace_release_task - final ptrace-related cleanup of a zombie being reaped\n * @task:\ttask in %EXIT_DEAD state\n *\n * Called with write_lock(&tasklist_lock) held.\n */\nstatic inline void ptrace_release_task(struct task_struct *task)\n{\n\tBUG_ON(!list_empty(&task->ptraced));\n\tptrace_unlink(task);\n\tBUG_ON(!list_empty(&task->ptrace_entry));\n}\n\n#ifndef force_successful_syscall_return\n/*\n * System call handlers that, upon successful completion, need to return a\n * negative value should call force_successful_syscall_return() right before\n * returning.  On architectures where the syscall convention provides for a\n * separate error flag (e.g., alpha, ia64, ppc{,64}, sparc{,64}, possibly\n * others), this macro can be used to ensure that the error flag will not get\n * set.  On architectures which do not support a separate error flag, the macro\n * is a no-op and the spurious error condition needs to be filtered out by some\n * other means (e.g., in user-level, by passing an extra argument to the\n * syscall handler, or something along those lines).\n */\n#define force_successful_syscall_return() do { } while (0)\n#endif\n\n#ifndef is_syscall_success\n/*\n * On most systems we can tell if a syscall is a success based on if the retval\n * is an error value.  On some systems like ia64 and powerpc they have different\n * indicators of success/failure and must define their own.\n */\n#define is_syscall_success(regs) (!IS_ERR_VALUE((unsigned long)(regs_return_value(regs))))\n#endif\n\n/*\n * <asm/ptrace.h> should define the following things inside #ifdef __KERNEL__.\n *\n * These do-nothing inlines are used when the arch does not\n * implement single-step.  The kerneldoc comments are here\n * to document the interface for all arch definitions.\n */\n\n#ifndef arch_has_single_step\n/**\n * arch_has_single_step - does this CPU support user-mode single-step?\n *\n * If this is defined, then there must be function declarations or\n * inlines for user_enable_single_step() and user_disable_single_step().\n * arch_has_single_step() should evaluate to nonzero iff the machine\n * supports instruction single-step for user mode.\n * It can be a constant or it can test a CPU feature bit.\n */\n#define arch_has_single_step()\t\t(0)\n\n/**\n * user_enable_single_step - single-step in user-mode task\n * @task: either current or a task stopped in %TASK_TRACED\n *\n * This can only be called when arch_has_single_step() has returned nonzero.\n * Set @task so that when it returns to user mode, it will trap after the\n * next single instruction executes.  If arch_has_block_step() is defined,\n * this must clear the effects of user_enable_block_step() too.\n */\nstatic inline void user_enable_single_step(struct task_struct *task)\n{\n\tBUG();\t\t\t/* This can never be called.  */\n}\n\n/**\n * user_disable_single_step - cancel user-mode single-step\n * @task: either current or a task stopped in %TASK_TRACED\n *\n * Clear @task of the effects of user_enable_single_step() and\n * user_enable_block_step().  This can be called whether or not either\n * of those was ever called on @task, and even if arch_has_single_step()\n * returned zero.\n */\nstatic inline void user_disable_single_step(struct task_struct *task)\n{\n}\n#else\nextern void user_enable_single_step(struct task_struct *);\nextern void user_disable_single_step(struct task_struct *);\n#endif\t/* arch_has_single_step */\n\n#ifndef arch_has_block_step\n/**\n * arch_has_block_step - does this CPU support user-mode block-step?\n *\n * If this is defined, then there must be a function declaration or inline\n * for user_enable_block_step(), and arch_has_single_step() must be defined\n * too.  arch_has_block_step() should evaluate to nonzero iff the machine\n * supports step-until-branch for user mode.  It can be a constant or it\n * can test a CPU feature bit.\n */\n#define arch_has_block_step()\t\t(0)\n\n/**\n * user_enable_block_step - step until branch in user-mode task\n * @task: either current or a task stopped in %TASK_TRACED\n *\n * This can only be called when arch_has_block_step() has returned nonzero,\n * and will never be called when single-instruction stepping is being used.\n * Set @task so that when it returns to user mode, it will trap after the\n * next branch or trap taken.\n */\nstatic inline void user_enable_block_step(struct task_struct *task)\n{\n\tBUG();\t\t\t/* This can never be called.  */\n}\n#else\nextern void user_enable_block_step(struct task_struct *);\n#endif\t/* arch_has_block_step */\n\n#ifdef ARCH_HAS_USER_SINGLE_STEP_INFO\nextern void user_single_step_siginfo(struct task_struct *tsk,\n\t\t\t\tstruct pt_regs *regs, siginfo_t *info);\n#else\nstatic inline void user_single_step_siginfo(struct task_struct *tsk,\n\t\t\t\tstruct pt_regs *regs, siginfo_t *info)\n{\n\tmemset(info, 0, sizeof(*info));\n\tinfo->si_signo = SIGTRAP;\n}\n#endif\n\n#ifndef arch_ptrace_stop_needed\n/**\n * arch_ptrace_stop_needed - Decide whether arch_ptrace_stop() should be called\n * @code:\tcurrent->exit_code value ptrace will stop with\n * @info:\tsiginfo_t pointer (or %NULL) for signal ptrace will stop with\n *\n * This is called with the siglock held, to decide whether or not it's\n * necessary to release the siglock and call arch_ptrace_stop() with the\n * same @code and @info arguments.  It can be defined to a constant if\n * arch_ptrace_stop() is never required, or always is.  On machines where\n * this makes sense, it should be defined to a quick test to optimize out\n * calling arch_ptrace_stop() when it would be superfluous.  For example,\n * if the thread has not been back to user mode since the last stop, the\n * thread state might indicate that nothing needs to be done.\n */\n#define arch_ptrace_stop_needed(code, info)\t(0)\n#endif\n\n#ifndef arch_ptrace_stop\n/**\n * arch_ptrace_stop - Do machine-specific work before stopping for ptrace\n * @code:\tcurrent->exit_code value ptrace will stop with\n * @info:\tsiginfo_t pointer (or %NULL) for signal ptrace will stop with\n *\n * This is called with no locks held when arch_ptrace_stop_needed() has\n * just returned nonzero.  It is allowed to block, e.g. for user memory\n * access.  The arch can have machine-specific work to be done before\n * ptrace stops.  On ia64, register backing store gets written back to user\n * memory here.  Since this can be costly (requires dropping the siglock),\n * we only do it when the arch requires it for this particular stop, as\n * indicated by arch_ptrace_stop_needed().\n */\n#define arch_ptrace_stop(code, info)\t\tdo { } while (0)\n#endif\n\n#ifndef current_pt_regs\n#define current_pt_regs() task_pt_regs(current)\n#endif\n\n#ifndef ptrace_signal_deliver\n#define ptrace_signal_deliver() ((void)0)\n#endif\n\n/*\n * unlike current_pt_regs(), this one is equal to task_pt_regs(current)\n * on *all* architectures; the only reason to have a per-arch definition\n * is optimisation.\n */\n#ifndef signal_pt_regs\n#define signal_pt_regs() task_pt_regs(current)\n#endif\n\n#ifndef current_user_stack_pointer\n#define current_user_stack_pointer() user_stack_pointer(current_pt_regs())\n#endif\n\nextern int task_current_syscall(struct task_struct *target, long *callno,\n\t\t\t\tunsigned long args[6], unsigned int maxargs,\n\t\t\t\tunsigned long *sp, unsigned long *pc);\n\n#endif\n"], "fixing_code": ["#ifndef _ASM_X86_PTRACE_H\n#define _ASM_X86_PTRACE_H\n\n#include <asm/segment.h>\n#include <asm/page_types.h>\n#include <uapi/asm/ptrace.h>\n\n#ifndef __ASSEMBLY__\n#ifdef __i386__\n\nstruct pt_regs {\n\tunsigned long bx;\n\tunsigned long cx;\n\tunsigned long dx;\n\tunsigned long si;\n\tunsigned long di;\n\tunsigned long bp;\n\tunsigned long ax;\n\tunsigned long ds;\n\tunsigned long es;\n\tunsigned long fs;\n\tunsigned long gs;\n\tunsigned long orig_ax;\n\tunsigned long ip;\n\tunsigned long cs;\n\tunsigned long flags;\n\tunsigned long sp;\n\tunsigned long ss;\n};\n\n#else /* __i386__ */\n\nstruct pt_regs {\n\tunsigned long r15;\n\tunsigned long r14;\n\tunsigned long r13;\n\tunsigned long r12;\n\tunsigned long bp;\n\tunsigned long bx;\n/* arguments: non interrupts/non tracing syscalls only save up to here*/\n\tunsigned long r11;\n\tunsigned long r10;\n\tunsigned long r9;\n\tunsigned long r8;\n\tunsigned long ax;\n\tunsigned long cx;\n\tunsigned long dx;\n\tunsigned long si;\n\tunsigned long di;\n\tunsigned long orig_ax;\n/* end of arguments */\n/* cpu exception frame or undefined */\n\tunsigned long ip;\n\tunsigned long cs;\n\tunsigned long flags;\n\tunsigned long sp;\n\tunsigned long ss;\n/* top of stack page */\n};\n\n#endif /* !__i386__ */\n\n#ifdef CONFIG_PARAVIRT\n#include <asm/paravirt_types.h>\n#endif\n\nstruct cpuinfo_x86;\nstruct task_struct;\n\nextern unsigned long profile_pc(struct pt_regs *regs);\n#define profile_pc profile_pc\n\nextern unsigned long\nconvert_ip_to_linear(struct task_struct *child, struct pt_regs *regs);\nextern void send_sigtrap(struct task_struct *tsk, struct pt_regs *regs,\n\t\t\t int error_code, int si_code);\n\nextern long syscall_trace_enter(struct pt_regs *);\nextern void syscall_trace_leave(struct pt_regs *);\n\nstatic inline unsigned long regs_return_value(struct pt_regs *regs)\n{\n\treturn regs->ax;\n}\n\n/*\n * user_mode_vm(regs) determines whether a register set came from user mode.\n * This is true if V8086 mode was enabled OR if the register set was from\n * protected mode with RPL-3 CS value.  This tricky test checks that with\n * one comparison.  Many places in the kernel can bypass this full check\n * if they have already ruled out V8086 mode, so user_mode(regs) can be used.\n */\nstatic inline int user_mode(struct pt_regs *regs)\n{\n#ifdef CONFIG_X86_32\n\treturn (regs->cs & SEGMENT_RPL_MASK) == USER_RPL;\n#else\n\treturn !!(regs->cs & 3);\n#endif\n}\n\nstatic inline int user_mode_vm(struct pt_regs *regs)\n{\n#ifdef CONFIG_X86_32\n\treturn ((regs->cs & SEGMENT_RPL_MASK) | (regs->flags & X86_VM_MASK)) >=\n\t\tUSER_RPL;\n#else\n\treturn user_mode(regs);\n#endif\n}\n\nstatic inline int v8086_mode(struct pt_regs *regs)\n{\n#ifdef CONFIG_X86_32\n\treturn (regs->flags & X86_VM_MASK);\n#else\n\treturn 0;\t/* No V86 mode support in long mode */\n#endif\n}\n\n#ifdef CONFIG_X86_64\nstatic inline bool user_64bit_mode(struct pt_regs *regs)\n{\n#ifndef CONFIG_PARAVIRT\n\t/*\n\t * On non-paravirt systems, this is the only long mode CPL 3\n\t * selector.  We do not allow long mode selectors in the LDT.\n\t */\n\treturn regs->cs == __USER_CS;\n#else\n\t/* Headers are too twisted for this to go in paravirt.h. */\n\treturn regs->cs == __USER_CS || regs->cs == pv_info.extra_user_64bit_cs;\n#endif\n}\n\n#define current_user_stack_pointer()\tthis_cpu_read(old_rsp)\n/* ia32 vs. x32 difference */\n#define compat_user_stack_pointer()\t\\\n\t(test_thread_flag(TIF_IA32) \t\\\n\t ? current_pt_regs()->sp \t\\\n\t : this_cpu_read(old_rsp))\n#endif\n\n#ifdef CONFIG_X86_32\nextern unsigned long kernel_stack_pointer(struct pt_regs *regs);\n#else\nstatic inline unsigned long kernel_stack_pointer(struct pt_regs *regs)\n{\n\treturn regs->sp;\n}\n#endif\n\n#define GET_IP(regs) ((regs)->ip)\n#define GET_FP(regs) ((regs)->bp)\n#define GET_USP(regs) ((regs)->sp)\n\n#include <asm-generic/ptrace.h>\n\n/* Query offset/name of register from its name/offset */\nextern int regs_query_register_offset(const char *name);\nextern const char *regs_query_register_name(unsigned int offset);\n#define MAX_REG_OFFSET (offsetof(struct pt_regs, ss))\n\n/**\n * regs_get_register() - get register value from its offset\n * @regs:\tpt_regs from which register value is gotten.\n * @offset:\toffset number of the register.\n *\n * regs_get_register returns the value of a register. The @offset is the\n * offset of the register in struct pt_regs address which specified by @regs.\n * If @offset is bigger than MAX_REG_OFFSET, this returns 0.\n */\nstatic inline unsigned long regs_get_register(struct pt_regs *regs,\n\t\t\t\t\t      unsigned int offset)\n{\n\tif (unlikely(offset > MAX_REG_OFFSET))\n\t\treturn 0;\n#ifdef CONFIG_X86_32\n\t/*\n\t * Traps from the kernel do not save sp and ss.\n\t * Use the helper function to retrieve sp.\n\t */\n\tif (offset == offsetof(struct pt_regs, sp) &&\n\t    regs->cs == __KERNEL_CS)\n\t\treturn kernel_stack_pointer(regs);\n#endif\n\treturn *(unsigned long *)((unsigned long)regs + offset);\n}\n\n/**\n * regs_within_kernel_stack() - check the address in the stack\n * @regs:\tpt_regs which contains kernel stack pointer.\n * @addr:\taddress which is checked.\n *\n * regs_within_kernel_stack() checks @addr is within the kernel stack page(s).\n * If @addr is within the kernel stack, it returns true. If not, returns false.\n */\nstatic inline int regs_within_kernel_stack(struct pt_regs *regs,\n\t\t\t\t\t   unsigned long addr)\n{\n\treturn ((addr & ~(THREAD_SIZE - 1))  ==\n\t\t(kernel_stack_pointer(regs) & ~(THREAD_SIZE - 1)));\n}\n\n/**\n * regs_get_kernel_stack_nth() - get Nth entry of the stack\n * @regs:\tpt_regs which contains kernel stack pointer.\n * @n:\t\tstack entry number.\n *\n * regs_get_kernel_stack_nth() returns @n th entry of the kernel stack which\n * is specified by @regs. If the @n th entry is NOT in the kernel stack,\n * this returns 0.\n */\nstatic inline unsigned long regs_get_kernel_stack_nth(struct pt_regs *regs,\n\t\t\t\t\t\t      unsigned int n)\n{\n\tunsigned long *addr = (unsigned long *)kernel_stack_pointer(regs);\n\taddr += n;\n\tif (regs_within_kernel_stack(regs, (unsigned long)addr))\n\t\treturn *addr;\n\telse\n\t\treturn 0;\n}\n\n#define arch_has_single_step()\t(1)\n#ifdef CONFIG_X86_DEBUGCTLMSR\n#define arch_has_block_step()\t(1)\n#else\n#define arch_has_block_step()\t(boot_cpu_data.x86 >= 6)\n#endif\n\n#define ARCH_HAS_USER_SINGLE_STEP_INFO\n\n/*\n * When hitting ptrace_stop(), we cannot return using SYSRET because\n * that does not restore the full CPU state, only a minimal set.  The\n * ptracer can change arbitrary register values, which is usually okay\n * because the usual ptrace stops run off the signal delivery path which\n * forces IRET; however, ptrace_event() stops happen in arbitrary places\n * in the kernel and don't force IRET path.\n *\n * So force IRET path after a ptrace stop.\n */\n#define arch_ptrace_stop_needed(code, info)\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tset_thread_flag(TIF_NOTIFY_RESUME);\t\t\t\t\\\n\tfalse;\t\t\t\t\t\t\t\t\\\n})\n\nstruct user_desc;\nextern int do_get_thread_area(struct task_struct *p, int idx,\n\t\t\t      struct user_desc __user *info);\nextern int do_set_thread_area(struct task_struct *p, int idx,\n\t\t\t      struct user_desc __user *info, int can_allocate);\n\n#endif /* !__ASSEMBLY__ */\n#endif /* _ASM_X86_PTRACE_H */\n", "#ifndef _LINUX_PTRACE_H\n#define _LINUX_PTRACE_H\n\n#include <linux/compiler.h>\t\t/* For unlikely.  */\n#include <linux/sched.h>\t\t/* For struct task_struct.  */\n#include <linux/err.h>\t\t\t/* for IS_ERR_VALUE */\n#include <linux/bug.h>\t\t\t/* For BUG_ON.  */\n#include <linux/pid_namespace.h>\t/* For task_active_pid_ns.  */\n#include <uapi/linux/ptrace.h>\n\n/*\n * Ptrace flags\n *\n * The owner ship rules for task->ptrace which holds the ptrace\n * flags is simple.  When a task is running it owns it's task->ptrace\n * flags.  When the a task is stopped the ptracer owns task->ptrace.\n */\n\n#define PT_SEIZED\t0x00010000\t/* SEIZE used, enable new behavior */\n#define PT_PTRACED\t0x00000001\n#define PT_DTRACE\t0x00000002\t/* delayed trace (used on m68k, i386) */\n#define PT_PTRACE_CAP\t0x00000004\t/* ptracer can follow suid-exec */\n\n#define PT_OPT_FLAG_SHIFT\t3\n/* PT_TRACE_* event enable flags */\n#define PT_EVENT_FLAG(event)\t(1 << (PT_OPT_FLAG_SHIFT + (event)))\n#define PT_TRACESYSGOOD\t\tPT_EVENT_FLAG(0)\n#define PT_TRACE_FORK\t\tPT_EVENT_FLAG(PTRACE_EVENT_FORK)\n#define PT_TRACE_VFORK\t\tPT_EVENT_FLAG(PTRACE_EVENT_VFORK)\n#define PT_TRACE_CLONE\t\tPT_EVENT_FLAG(PTRACE_EVENT_CLONE)\n#define PT_TRACE_EXEC\t\tPT_EVENT_FLAG(PTRACE_EVENT_EXEC)\n#define PT_TRACE_VFORK_DONE\tPT_EVENT_FLAG(PTRACE_EVENT_VFORK_DONE)\n#define PT_TRACE_EXIT\t\tPT_EVENT_FLAG(PTRACE_EVENT_EXIT)\n#define PT_TRACE_SECCOMP\tPT_EVENT_FLAG(PTRACE_EVENT_SECCOMP)\n\n#define PT_EXITKILL\t\t(PTRACE_O_EXITKILL << PT_OPT_FLAG_SHIFT)\n\n/* single stepping state bits (used on ARM and PA-RISC) */\n#define PT_SINGLESTEP_BIT\t31\n#define PT_SINGLESTEP\t\t(1<<PT_SINGLESTEP_BIT)\n#define PT_BLOCKSTEP_BIT\t30\n#define PT_BLOCKSTEP\t\t(1<<PT_BLOCKSTEP_BIT)\n\nextern long arch_ptrace(struct task_struct *child, long request,\n\t\t\tunsigned long addr, unsigned long data);\nextern int ptrace_readdata(struct task_struct *tsk, unsigned long src, char __user *dst, int len);\nextern int ptrace_writedata(struct task_struct *tsk, char __user *src, unsigned long dst, int len);\nextern void ptrace_disable(struct task_struct *);\nextern int ptrace_request(struct task_struct *child, long request,\n\t\t\t  unsigned long addr, unsigned long data);\nextern void ptrace_notify(int exit_code);\nextern void __ptrace_link(struct task_struct *child,\n\t\t\t  struct task_struct *new_parent);\nextern void __ptrace_unlink(struct task_struct *child);\nextern void exit_ptrace(struct task_struct *tracer);\n#define PTRACE_MODE_READ\t0x01\n#define PTRACE_MODE_ATTACH\t0x02\n#define PTRACE_MODE_NOAUDIT\t0x04\n/* Returns true on success, false on denial. */\nextern bool ptrace_may_access(struct task_struct *task, unsigned int mode);\n\nstatic inline int ptrace_reparented(struct task_struct *child)\n{\n\treturn !same_thread_group(child->real_parent, child->parent);\n}\n\nstatic inline void ptrace_unlink(struct task_struct *child)\n{\n\tif (unlikely(child->ptrace))\n\t\t__ptrace_unlink(child);\n}\n\nint generic_ptrace_peekdata(struct task_struct *tsk, unsigned long addr,\n\t\t\t    unsigned long data);\nint generic_ptrace_pokedata(struct task_struct *tsk, unsigned long addr,\n\t\t\t    unsigned long data);\n\n/**\n * ptrace_parent - return the task that is tracing the given task\n * @task: task to consider\n *\n * Returns %NULL if no one is tracing @task, or the &struct task_struct\n * pointer to its tracer.\n *\n * Must called under rcu_read_lock().  The pointer returned might be kept\n * live only by RCU.  During exec, this may be called with task_lock() held\n * on @task, still held from when check_unsafe_exec() was called.\n */\nstatic inline struct task_struct *ptrace_parent(struct task_struct *task)\n{\n\tif (unlikely(task->ptrace))\n\t\treturn rcu_dereference(task->parent);\n\treturn NULL;\n}\n\n/**\n * ptrace_event_enabled - test whether a ptrace event is enabled\n * @task: ptracee of interest\n * @event: %PTRACE_EVENT_* to test\n *\n * Test whether @event is enabled for ptracee @task.\n *\n * Returns %true if @event is enabled, %false otherwise.\n */\nstatic inline bool ptrace_event_enabled(struct task_struct *task, int event)\n{\n\treturn task->ptrace & PT_EVENT_FLAG(event);\n}\n\n/**\n * ptrace_event - possibly stop for a ptrace event notification\n * @event:\t%PTRACE_EVENT_* value to report\n * @message:\tvalue for %PTRACE_GETEVENTMSG to return\n *\n * Check whether @event is enabled and, if so, report @event and @message\n * to the ptrace parent.\n *\n * Called without locks.\n */\nstatic inline void ptrace_event(int event, unsigned long message)\n{\n\tif (unlikely(ptrace_event_enabled(current, event))) {\n\t\tcurrent->ptrace_message = message;\n\t\tptrace_notify((event << 8) | SIGTRAP);\n\t} else if (event == PTRACE_EVENT_EXEC) {\n\t\t/* legacy EXEC report via SIGTRAP */\n\t\tif ((current->ptrace & (PT_PTRACED|PT_SEIZED)) == PT_PTRACED)\n\t\t\tsend_sig(SIGTRAP, current, 0);\n\t}\n}\n\n/**\n * ptrace_event_pid - possibly stop for a ptrace event notification\n * @event:\t%PTRACE_EVENT_* value to report\n * @pid:\tprocess identifier for %PTRACE_GETEVENTMSG to return\n *\n * Check whether @event is enabled and, if so, report @event and @pid\n * to the ptrace parent.  @pid is reported as the pid_t seen from the\n * the ptrace parent's pid namespace.\n *\n * Called without locks.\n */\nstatic inline void ptrace_event_pid(int event, struct pid *pid)\n{\n\t/*\n\t * FIXME: There's a potential race if a ptracer in a different pid\n\t * namespace than parent attaches between computing message below and\n\t * when we acquire tasklist_lock in ptrace_stop().  If this happens,\n\t * the ptracer will get a bogus pid from PTRACE_GETEVENTMSG.\n\t */\n\tunsigned long message = 0;\n\tstruct pid_namespace *ns;\n\n\trcu_read_lock();\n\tns = task_active_pid_ns(rcu_dereference(current->parent));\n\tif (ns)\n\t\tmessage = pid_nr_ns(pid, ns);\n\trcu_read_unlock();\n\n\tptrace_event(event, message);\n}\n\n/**\n * ptrace_init_task - initialize ptrace state for a new child\n * @child:\t\tnew child task\n * @ptrace:\t\ttrue if child should be ptrace'd by parent's tracer\n *\n * This is called immediately after adding @child to its parent's children\n * list.  @ptrace is false in the normal case, and true to ptrace @child.\n *\n * Called with current's siglock and write_lock_irq(&tasklist_lock) held.\n */\nstatic inline void ptrace_init_task(struct task_struct *child, bool ptrace)\n{\n\tINIT_LIST_HEAD(&child->ptrace_entry);\n\tINIT_LIST_HEAD(&child->ptraced);\n\tchild->jobctl = 0;\n\tchild->ptrace = 0;\n\tchild->parent = child->real_parent;\n\n\tif (unlikely(ptrace) && current->ptrace) {\n\t\tchild->ptrace = current->ptrace;\n\t\t__ptrace_link(child, current->parent);\n\n\t\tif (child->ptrace & PT_SEIZED)\n\t\t\ttask_set_jobctl_pending(child, JOBCTL_TRAP_STOP);\n\t\telse\n\t\t\tsigaddset(&child->pending.signal, SIGSTOP);\n\n\t\tset_tsk_thread_flag(child, TIF_SIGPENDING);\n\t}\n}\n\n/**\n * ptrace_release_task - final ptrace-related cleanup of a zombie being reaped\n * @task:\ttask in %EXIT_DEAD state\n *\n * Called with write_lock(&tasklist_lock) held.\n */\nstatic inline void ptrace_release_task(struct task_struct *task)\n{\n\tBUG_ON(!list_empty(&task->ptraced));\n\tptrace_unlink(task);\n\tBUG_ON(!list_empty(&task->ptrace_entry));\n}\n\n#ifndef force_successful_syscall_return\n/*\n * System call handlers that, upon successful completion, need to return a\n * negative value should call force_successful_syscall_return() right before\n * returning.  On architectures where the syscall convention provides for a\n * separate error flag (e.g., alpha, ia64, ppc{,64}, sparc{,64}, possibly\n * others), this macro can be used to ensure that the error flag will not get\n * set.  On architectures which do not support a separate error flag, the macro\n * is a no-op and the spurious error condition needs to be filtered out by some\n * other means (e.g., in user-level, by passing an extra argument to the\n * syscall handler, or something along those lines).\n */\n#define force_successful_syscall_return() do { } while (0)\n#endif\n\n#ifndef is_syscall_success\n/*\n * On most systems we can tell if a syscall is a success based on if the retval\n * is an error value.  On some systems like ia64 and powerpc they have different\n * indicators of success/failure and must define their own.\n */\n#define is_syscall_success(regs) (!IS_ERR_VALUE((unsigned long)(regs_return_value(regs))))\n#endif\n\n/*\n * <asm/ptrace.h> should define the following things inside #ifdef __KERNEL__.\n *\n * These do-nothing inlines are used when the arch does not\n * implement single-step.  The kerneldoc comments are here\n * to document the interface for all arch definitions.\n */\n\n#ifndef arch_has_single_step\n/**\n * arch_has_single_step - does this CPU support user-mode single-step?\n *\n * If this is defined, then there must be function declarations or\n * inlines for user_enable_single_step() and user_disable_single_step().\n * arch_has_single_step() should evaluate to nonzero iff the machine\n * supports instruction single-step for user mode.\n * It can be a constant or it can test a CPU feature bit.\n */\n#define arch_has_single_step()\t\t(0)\n\n/**\n * user_enable_single_step - single-step in user-mode task\n * @task: either current or a task stopped in %TASK_TRACED\n *\n * This can only be called when arch_has_single_step() has returned nonzero.\n * Set @task so that when it returns to user mode, it will trap after the\n * next single instruction executes.  If arch_has_block_step() is defined,\n * this must clear the effects of user_enable_block_step() too.\n */\nstatic inline void user_enable_single_step(struct task_struct *task)\n{\n\tBUG();\t\t\t/* This can never be called.  */\n}\n\n/**\n * user_disable_single_step - cancel user-mode single-step\n * @task: either current or a task stopped in %TASK_TRACED\n *\n * Clear @task of the effects of user_enable_single_step() and\n * user_enable_block_step().  This can be called whether or not either\n * of those was ever called on @task, and even if arch_has_single_step()\n * returned zero.\n */\nstatic inline void user_disable_single_step(struct task_struct *task)\n{\n}\n#else\nextern void user_enable_single_step(struct task_struct *);\nextern void user_disable_single_step(struct task_struct *);\n#endif\t/* arch_has_single_step */\n\n#ifndef arch_has_block_step\n/**\n * arch_has_block_step - does this CPU support user-mode block-step?\n *\n * If this is defined, then there must be a function declaration or inline\n * for user_enable_block_step(), and arch_has_single_step() must be defined\n * too.  arch_has_block_step() should evaluate to nonzero iff the machine\n * supports step-until-branch for user mode.  It can be a constant or it\n * can test a CPU feature bit.\n */\n#define arch_has_block_step()\t\t(0)\n\n/**\n * user_enable_block_step - step until branch in user-mode task\n * @task: either current or a task stopped in %TASK_TRACED\n *\n * This can only be called when arch_has_block_step() has returned nonzero,\n * and will never be called when single-instruction stepping is being used.\n * Set @task so that when it returns to user mode, it will trap after the\n * next branch or trap taken.\n */\nstatic inline void user_enable_block_step(struct task_struct *task)\n{\n\tBUG();\t\t\t/* This can never be called.  */\n}\n#else\nextern void user_enable_block_step(struct task_struct *);\n#endif\t/* arch_has_block_step */\n\n#ifdef ARCH_HAS_USER_SINGLE_STEP_INFO\nextern void user_single_step_siginfo(struct task_struct *tsk,\n\t\t\t\tstruct pt_regs *regs, siginfo_t *info);\n#else\nstatic inline void user_single_step_siginfo(struct task_struct *tsk,\n\t\t\t\tstruct pt_regs *regs, siginfo_t *info)\n{\n\tmemset(info, 0, sizeof(*info));\n\tinfo->si_signo = SIGTRAP;\n}\n#endif\n\n#ifndef arch_ptrace_stop_needed\n/**\n * arch_ptrace_stop_needed - Decide whether arch_ptrace_stop() should be called\n * @code:\tcurrent->exit_code value ptrace will stop with\n * @info:\tsiginfo_t pointer (or %NULL) for signal ptrace will stop with\n *\n * This is called with the siglock held, to decide whether or not it's\n * necessary to release the siglock and call arch_ptrace_stop() with the\n * same @code and @info arguments.  It can be defined to a constant if\n * arch_ptrace_stop() is never required, or always is.  On machines where\n * this makes sense, it should be defined to a quick test to optimize out\n * calling arch_ptrace_stop() when it would be superfluous.  For example,\n * if the thread has not been back to user mode since the last stop, the\n * thread state might indicate that nothing needs to be done.\n *\n * This is guaranteed to be invoked once before a task stops for ptrace and\n * may include arch-specific operations necessary prior to a ptrace stop.\n */\n#define arch_ptrace_stop_needed(code, info)\t(0)\n#endif\n\n#ifndef arch_ptrace_stop\n/**\n * arch_ptrace_stop - Do machine-specific work before stopping for ptrace\n * @code:\tcurrent->exit_code value ptrace will stop with\n * @info:\tsiginfo_t pointer (or %NULL) for signal ptrace will stop with\n *\n * This is called with no locks held when arch_ptrace_stop_needed() has\n * just returned nonzero.  It is allowed to block, e.g. for user memory\n * access.  The arch can have machine-specific work to be done before\n * ptrace stops.  On ia64, register backing store gets written back to user\n * memory here.  Since this can be costly (requires dropping the siglock),\n * we only do it when the arch requires it for this particular stop, as\n * indicated by arch_ptrace_stop_needed().\n */\n#define arch_ptrace_stop(code, info)\t\tdo { } while (0)\n#endif\n\n#ifndef current_pt_regs\n#define current_pt_regs() task_pt_regs(current)\n#endif\n\n#ifndef ptrace_signal_deliver\n#define ptrace_signal_deliver() ((void)0)\n#endif\n\n/*\n * unlike current_pt_regs(), this one is equal to task_pt_regs(current)\n * on *all* architectures; the only reason to have a per-arch definition\n * is optimisation.\n */\n#ifndef signal_pt_regs\n#define signal_pt_regs() task_pt_regs(current)\n#endif\n\n#ifndef current_user_stack_pointer\n#define current_user_stack_pointer() user_stack_pointer(current_pt_regs())\n#endif\n\nextern int task_current_syscall(struct task_struct *target, long *callno,\n\t\t\t\tunsigned long args[6], unsigned int maxargs,\n\t\t\t\tunsigned long *sp, unsigned long *pc);\n\n#endif\n"], "buggy_code_start_loc": [233, 336], "buggy_code_end_loc": [233, 336], "fixing_code_start_loc": [234, 337], "fixing_code_end_loc": [250, 340], "type": "CWE-362", "message": "The Linux kernel before 3.15.4 on Intel processors does not properly restrict use of a non-canonical value for the saved RIP address in the case of a system call that does not use IRET, which allows local users to leverage a race condition and gain privileges, or cause a denial of service (double fault), via a crafted application that makes ptrace and fork system calls.", "other": {"cve": {"id": "CVE-2014-4699", "sourceIdentifier": "cve@mitre.org", "published": "2014-07-09T11:07:03.477", "lastModified": "2020-08-14T18:03:35.487", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "The Linux kernel before 3.15.4 on Intel processors does not properly restrict use of a non-canonical value for the saved RIP address in the case of a system call that does not use IRET, which allows local users to leverage a race condition and gain privileges, or cause a denial of service (double fault), via a crafted application that makes ptrace and fork system calls."}, {"lang": "es", "value": "El kernel de Linux anterior a 3.15.4 en los procesadores Intel no restringe debidamente el uso de un valor no can\u00f3nico para la direcci\u00f3n RIP guardada en el caso de una llamada del sistema que no utilice IRET, lo que permite a usuarios locales aprovechar una condici\u00f3n de carrera y ganar privilegios, o causar una denegaci\u00f3n de servicio (fallo doble), a trav\u00e9s de una aplicaci\u00f3n manipulada que realice llamadas de sistemas ptrace y fork."}], "metrics": {"cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:M/Au:N/C:C/I:C/A:C", "accessVector": "LOCAL", "accessComplexity": "MEDIUM", "authentication": "NONE", "confidentialityImpact": "COMPLETE", "integrityImpact": "COMPLETE", "availabilityImpact": "COMPLETE", "baseScore": 6.9}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.4, "impactScore": 10.0, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-362"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "3.15.4", "matchCriteriaId": "0D5E9AD0-230F-420A-80A8-B9AEB7CA4987"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:7.0:*:*:*:*:*:*:*", "matchCriteriaId": "16F59A04-14CF-49E2-9973-645477EA09DA"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:10.04:*:*:*:-:*:*:*", "matchCriteriaId": "01EDA41C-6B2E-49AF-B503-EB3882265C11"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:12.04:*:*:*:esm:*:*:*", "matchCriteriaId": "8D305F7A-D159-4716-AB26-5E38BB5CD991"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:13.10:*:*:*:*:*:*:*", "matchCriteriaId": "7F61F047-129C-41A6-8A27-FFCBB8563E91"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:14.04:*:*:*:esm:*:*:*", "matchCriteriaId": "815D70A8-47D3-459C-A32C-9FEACA0659D1"}]}]}], "references": [{"url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux-2.6.git;a=commit;h=b9cd18de4db3c9ffa7e17b0dc0ca99ed5aa4d43a", "source": "cve@mitre.org", "tags": ["Patch", "Vendor Advisory"]}, {"url": "http://linux.oracle.com/errata/ELSA-2014-0924.html", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "http://linux.oracle.com/errata/ELSA-2014-3047.html", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "http://linux.oracle.com/errata/ELSA-2014-3048.html", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "http://openwall.com/lists/oss-security/2014/07/05/4", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://openwall.com/lists/oss-security/2014/07/08/16", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://openwall.com/lists/oss-security/2014/07/08/5", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://packetstormsecurity.com/files/127573/Linux-Kernel-ptrace-sysret-Local-Privilege-Escalation.html", "source": "cve@mitre.org", "tags": ["Exploit", "Third Party Advisory", "VDB Entry"]}, {"url": "http://www.debian.org/security/2014/dsa-2972", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "http://www.exploit-db.com/exploits/34134", "source": "cve@mitre.org", "tags": ["Exploit", "Third Party Advisory", "VDB Entry"]}, {"url": "http://www.kernel.org/pub/linux/kernel/v3.x/ChangeLog-3.15.4", "source": "cve@mitre.org", "tags": ["Release Notes", "Vendor Advisory"]}, {"url": "http://www.openwall.com/lists/oss-security/2014/07/04/4", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://www.ubuntu.com/usn/USN-2266-1", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "http://www.ubuntu.com/usn/USN-2267-1", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "http://www.ubuntu.com/usn/USN-2268-1", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "http://www.ubuntu.com/usn/USN-2269-1", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "http://www.ubuntu.com/usn/USN-2270-1", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "http://www.ubuntu.com/usn/USN-2271-1", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "http://www.ubuntu.com/usn/USN-2272-1", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "http://www.ubuntu.com/usn/USN-2273-1", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "http://www.ubuntu.com/usn/USN-2274-1", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=1115927", "source": "cve@mitre.org", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/b9cd18de4db3c9ffa7e17b0dc0ca99ed5aa4d43a", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://www.kernel.org/pub/linux/kernel/v3.x/ChangeLog-3.10.47", "source": "cve@mitre.org", "tags": ["Release Notes", "Vendor Advisory"]}, {"url": "https://www.kernel.org/pub/linux/kernel/v3.x/ChangeLog-3.14.11", "source": "cve@mitre.org", "tags": ["Release Notes", "Vendor Advisory"]}, {"url": "https://www.kernel.org/pub/linux/kernel/v3.x/ChangeLog-3.4.97", "source": "cve@mitre.org", "tags": ["Release Notes", "Vendor Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/b9cd18de4db3c9ffa7e17b0dc0ca99ed5aa4d43a"}}