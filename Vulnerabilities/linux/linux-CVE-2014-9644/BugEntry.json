{"buggy_code": ["/*\n * FPU: Wrapper for blkcipher touching fpu\n *\n * Copyright (c) Intel Corp.\n *   Author: Huang Ying <ying.huang@intel.com>\n *\n * This program is free software; you can redistribute it and/or modify it\n * under the terms of the GNU General Public License as published by the Free\n * Software Foundation; either version 2 of the License, or (at your option)\n * any later version.\n *\n */\n\n#include <crypto/algapi.h>\n#include <linux/err.h>\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/slab.h>\n#include <asm/i387.h>\n\nstruct crypto_fpu_ctx {\n\tstruct crypto_blkcipher *child;\n};\n\nstatic int crypto_fpu_setkey(struct crypto_tfm *parent, const u8 *key,\n\t\t\t     unsigned int keylen)\n{\n\tstruct crypto_fpu_ctx *ctx = crypto_tfm_ctx(parent);\n\tstruct crypto_blkcipher *child = ctx->child;\n\tint err;\n\n\tcrypto_blkcipher_clear_flags(child, CRYPTO_TFM_REQ_MASK);\n\tcrypto_blkcipher_set_flags(child, crypto_tfm_get_flags(parent) &\n\t\t\t\t   CRYPTO_TFM_REQ_MASK);\n\terr = crypto_blkcipher_setkey(child, key, keylen);\n\tcrypto_tfm_set_flags(parent, crypto_blkcipher_get_flags(child) &\n\t\t\t\t     CRYPTO_TFM_RES_MASK);\n\treturn err;\n}\n\nstatic int crypto_fpu_encrypt(struct blkcipher_desc *desc_in,\n\t\t\t      struct scatterlist *dst, struct scatterlist *src,\n\t\t\t      unsigned int nbytes)\n{\n\tint err;\n\tstruct crypto_fpu_ctx *ctx = crypto_blkcipher_ctx(desc_in->tfm);\n\tstruct crypto_blkcipher *child = ctx->child;\n\tstruct blkcipher_desc desc = {\n\t\t.tfm = child,\n\t\t.info = desc_in->info,\n\t\t.flags = desc_in->flags & ~CRYPTO_TFM_REQ_MAY_SLEEP,\n\t};\n\n\tkernel_fpu_begin();\n\terr = crypto_blkcipher_crt(desc.tfm)->encrypt(&desc, dst, src, nbytes);\n\tkernel_fpu_end();\n\treturn err;\n}\n\nstatic int crypto_fpu_decrypt(struct blkcipher_desc *desc_in,\n\t\t\t      struct scatterlist *dst, struct scatterlist *src,\n\t\t\t      unsigned int nbytes)\n{\n\tint err;\n\tstruct crypto_fpu_ctx *ctx = crypto_blkcipher_ctx(desc_in->tfm);\n\tstruct crypto_blkcipher *child = ctx->child;\n\tstruct blkcipher_desc desc = {\n\t\t.tfm = child,\n\t\t.info = desc_in->info,\n\t\t.flags = desc_in->flags & ~CRYPTO_TFM_REQ_MAY_SLEEP,\n\t};\n\n\tkernel_fpu_begin();\n\terr = crypto_blkcipher_crt(desc.tfm)->decrypt(&desc, dst, src, nbytes);\n\tkernel_fpu_end();\n\treturn err;\n}\n\nstatic int crypto_fpu_init_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_instance *inst = crypto_tfm_alg_instance(tfm);\n\tstruct crypto_spawn *spawn = crypto_instance_ctx(inst);\n\tstruct crypto_fpu_ctx *ctx = crypto_tfm_ctx(tfm);\n\tstruct crypto_blkcipher *cipher;\n\n\tcipher = crypto_spawn_blkcipher(spawn);\n\tif (IS_ERR(cipher))\n\t\treturn PTR_ERR(cipher);\n\n\tctx->child = cipher;\n\treturn 0;\n}\n\nstatic void crypto_fpu_exit_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_fpu_ctx *ctx = crypto_tfm_ctx(tfm);\n\tcrypto_free_blkcipher(ctx->child);\n}\n\nstatic struct crypto_instance *crypto_fpu_alloc(struct rtattr **tb)\n{\n\tstruct crypto_instance *inst;\n\tstruct crypto_alg *alg;\n\tint err;\n\n\terr = crypto_check_attr_type(tb, CRYPTO_ALG_TYPE_BLKCIPHER);\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\talg = crypto_get_attr_alg(tb, CRYPTO_ALG_TYPE_BLKCIPHER,\n\t\t\t\t  CRYPTO_ALG_TYPE_MASK);\n\tif (IS_ERR(alg))\n\t\treturn ERR_CAST(alg);\n\n\tinst = crypto_alloc_instance(\"fpu\", alg);\n\tif (IS_ERR(inst))\n\t\tgoto out_put_alg;\n\n\tinst->alg.cra_flags = alg->cra_flags;\n\tinst->alg.cra_priority = alg->cra_priority;\n\tinst->alg.cra_blocksize = alg->cra_blocksize;\n\tinst->alg.cra_alignmask = alg->cra_alignmask;\n\tinst->alg.cra_type = alg->cra_type;\n\tinst->alg.cra_blkcipher.ivsize = alg->cra_blkcipher.ivsize;\n\tinst->alg.cra_blkcipher.min_keysize = alg->cra_blkcipher.min_keysize;\n\tinst->alg.cra_blkcipher.max_keysize = alg->cra_blkcipher.max_keysize;\n\tinst->alg.cra_ctxsize = sizeof(struct crypto_fpu_ctx);\n\tinst->alg.cra_init = crypto_fpu_init_tfm;\n\tinst->alg.cra_exit = crypto_fpu_exit_tfm;\n\tinst->alg.cra_blkcipher.setkey = crypto_fpu_setkey;\n\tinst->alg.cra_blkcipher.encrypt = crypto_fpu_encrypt;\n\tinst->alg.cra_blkcipher.decrypt = crypto_fpu_decrypt;\n\nout_put_alg:\n\tcrypto_mod_put(alg);\n\treturn inst;\n}\n\nstatic void crypto_fpu_free(struct crypto_instance *inst)\n{\n\tcrypto_drop_spawn(crypto_instance_ctx(inst));\n\tkfree(inst);\n}\n\nstatic struct crypto_template crypto_fpu_tmpl = {\n\t.name = \"fpu\",\n\t.alloc = crypto_fpu_alloc,\n\t.free = crypto_fpu_free,\n\t.module = THIS_MODULE,\n};\n\nint __init crypto_fpu_init(void)\n{\n\treturn crypto_register_template(&crypto_fpu_tmpl);\n}\n\nvoid __exit crypto_fpu_exit(void)\n{\n\tcrypto_unregister_template(&crypto_fpu_tmpl);\n}\n", "/*\n * Cryptographic API for algorithms (i.e., low-level API).\n *\n * Copyright (c) 2006 Herbert Xu <herbert@gondor.apana.org.au>\n *\n * This program is free software; you can redistribute it and/or modify it\n * under the terms of the GNU General Public License as published by the Free\n * Software Foundation; either version 2 of the License, or (at your option)\n * any later version.\n *\n */\n\n#include <linux/err.h>\n#include <linux/errno.h>\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/list.h>\n#include <linux/module.h>\n#include <linux/rtnetlink.h>\n#include <linux/slab.h>\n#include <linux/string.h>\n\n#include \"internal.h\"\n\nstatic LIST_HEAD(crypto_template_list);\n\nstatic inline int crypto_set_driver_name(struct crypto_alg *alg)\n{\n\tstatic const char suffix[] = \"-generic\";\n\tchar *driver_name = alg->cra_driver_name;\n\tint len;\n\n\tif (*driver_name)\n\t\treturn 0;\n\n\tlen = strlcpy(driver_name, alg->cra_name, CRYPTO_MAX_ALG_NAME);\n\tif (len + sizeof(suffix) > CRYPTO_MAX_ALG_NAME)\n\t\treturn -ENAMETOOLONG;\n\n\tmemcpy(driver_name + len, suffix, sizeof(suffix));\n\treturn 0;\n}\n\nstatic inline void crypto_check_module_sig(struct module *mod)\n{\n#ifdef CONFIG_CRYPTO_FIPS\n\tif (fips_enabled && mod && !mod->sig_ok)\n\t\tpanic(\"Module %s signature verification failed in FIPS mode\\n\",\n\t\t      mod->name);\n#endif\n\treturn;\n}\n\nstatic int crypto_check_alg(struct crypto_alg *alg)\n{\n\tcrypto_check_module_sig(alg->cra_module);\n\n\tif (alg->cra_alignmask & (alg->cra_alignmask + 1))\n\t\treturn -EINVAL;\n\n\tif (alg->cra_blocksize > PAGE_SIZE / 8)\n\t\treturn -EINVAL;\n\n\tif (alg->cra_priority < 0)\n\t\treturn -EINVAL;\n\n\treturn crypto_set_driver_name(alg);\n}\n\nstatic void crypto_destroy_instance(struct crypto_alg *alg)\n{\n\tstruct crypto_instance *inst = (void *)alg;\n\tstruct crypto_template *tmpl = inst->tmpl;\n\n\ttmpl->free(inst);\n\tcrypto_tmpl_put(tmpl);\n}\n\nstatic struct list_head *crypto_more_spawns(struct crypto_alg *alg,\n\t\t\t\t\t    struct list_head *stack,\n\t\t\t\t\t    struct list_head *top,\n\t\t\t\t\t    struct list_head *secondary_spawns)\n{\n\tstruct crypto_spawn *spawn, *n;\n\n\tif (list_empty(stack))\n\t\treturn NULL;\n\n\tspawn = list_first_entry(stack, struct crypto_spawn, list);\n\tn = list_entry(spawn->list.next, struct crypto_spawn, list);\n\n\tif (spawn->alg && &n->list != stack && !n->alg)\n\t\tn->alg = (n->list.next == stack) ? alg :\n\t\t\t &list_entry(n->list.next, struct crypto_spawn,\n\t\t\t\t     list)->inst->alg;\n\n\tlist_move(&spawn->list, secondary_spawns);\n\n\treturn &n->list == stack ? top : &n->inst->alg.cra_users;\n}\n\nstatic void crypto_remove_spawn(struct crypto_spawn *spawn,\n\t\t\t\tstruct list_head *list)\n{\n\tstruct crypto_instance *inst = spawn->inst;\n\tstruct crypto_template *tmpl = inst->tmpl;\n\n\tif (crypto_is_dead(&inst->alg))\n\t\treturn;\n\n\tinst->alg.cra_flags |= CRYPTO_ALG_DEAD;\n\tif (hlist_unhashed(&inst->list))\n\t\treturn;\n\n\tif (!tmpl || !crypto_tmpl_get(tmpl))\n\t\treturn;\n\n\tcrypto_notify(CRYPTO_MSG_ALG_UNREGISTER, &inst->alg);\n\tlist_move(&inst->alg.cra_list, list);\n\thlist_del(&inst->list);\n\tinst->alg.cra_destroy = crypto_destroy_instance;\n\n\tBUG_ON(!list_empty(&inst->alg.cra_users));\n}\n\nvoid crypto_remove_spawns(struct crypto_alg *alg, struct list_head *list,\n\t\t\t  struct crypto_alg *nalg)\n{\n\tu32 new_type = (nalg ?: alg)->cra_flags;\n\tstruct crypto_spawn *spawn, *n;\n\tLIST_HEAD(secondary_spawns);\n\tstruct list_head *spawns;\n\tLIST_HEAD(stack);\n\tLIST_HEAD(top);\n\n\tspawns = &alg->cra_users;\n\tlist_for_each_entry_safe(spawn, n, spawns, list) {\n\t\tif ((spawn->alg->cra_flags ^ new_type) & spawn->mask)\n\t\t\tcontinue;\n\n\t\tlist_move(&spawn->list, &top);\n\t}\n\n\tspawns = &top;\n\tdo {\n\t\twhile (!list_empty(spawns)) {\n\t\t\tstruct crypto_instance *inst;\n\n\t\t\tspawn = list_first_entry(spawns, struct crypto_spawn,\n\t\t\t\t\t\t list);\n\t\t\tinst = spawn->inst;\n\n\t\t\tBUG_ON(&inst->alg == alg);\n\n\t\t\tlist_move(&spawn->list, &stack);\n\n\t\t\tif (&inst->alg == nalg)\n\t\t\t\tbreak;\n\n\t\t\tspawn->alg = NULL;\n\t\t\tspawns = &inst->alg.cra_users;\n\t\t}\n\t} while ((spawns = crypto_more_spawns(alg, &stack, &top,\n\t\t\t\t\t      &secondary_spawns)));\n\n\tlist_for_each_entry_safe(spawn, n, &secondary_spawns, list) {\n\t\tif (spawn->alg)\n\t\t\tlist_move(&spawn->list, &spawn->alg->cra_users);\n\t\telse\n\t\t\tcrypto_remove_spawn(spawn, list);\n\t}\n}\nEXPORT_SYMBOL_GPL(crypto_remove_spawns);\n\nstatic struct crypto_larval *__crypto_register_alg(struct crypto_alg *alg)\n{\n\tstruct crypto_alg *q;\n\tstruct crypto_larval *larval;\n\tint ret = -EAGAIN;\n\n\tif (crypto_is_dead(alg))\n\t\tgoto err;\n\n\tINIT_LIST_HEAD(&alg->cra_users);\n\n\t/* No cheating! */\n\talg->cra_flags &= ~CRYPTO_ALG_TESTED;\n\n\tret = -EEXIST;\n\n\tatomic_set(&alg->cra_refcnt, 1);\n\tlist_for_each_entry(q, &crypto_alg_list, cra_list) {\n\t\tif (q == alg)\n\t\t\tgoto err;\n\n\t\tif (crypto_is_moribund(q))\n\t\t\tcontinue;\n\n\t\tif (crypto_is_larval(q)) {\n\t\t\tif (!strcmp(alg->cra_driver_name, q->cra_driver_name))\n\t\t\t\tgoto err;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!strcmp(q->cra_driver_name, alg->cra_name) ||\n\t\t    !strcmp(q->cra_name, alg->cra_driver_name))\n\t\t\tgoto err;\n\t}\n\n\tlarval = crypto_larval_alloc(alg->cra_name,\n\t\t\t\t     alg->cra_flags | CRYPTO_ALG_TESTED, 0);\n\tif (IS_ERR(larval))\n\t\tgoto out;\n\n\tret = -ENOENT;\n\tlarval->adult = crypto_mod_get(alg);\n\tif (!larval->adult)\n\t\tgoto free_larval;\n\n\tatomic_set(&larval->alg.cra_refcnt, 1);\n\tmemcpy(larval->alg.cra_driver_name, alg->cra_driver_name,\n\t       CRYPTO_MAX_ALG_NAME);\n\tlarval->alg.cra_priority = alg->cra_priority;\n\n\tlist_add(&alg->cra_list, &crypto_alg_list);\n\tlist_add(&larval->alg.cra_list, &crypto_alg_list);\n\nout:\n\treturn larval;\n\nfree_larval:\n\tkfree(larval);\nerr:\n\tlarval = ERR_PTR(ret);\n\tgoto out;\n}\n\nvoid crypto_alg_tested(const char *name, int err)\n{\n\tstruct crypto_larval *test;\n\tstruct crypto_alg *alg;\n\tstruct crypto_alg *q;\n\tLIST_HEAD(list);\n\n\tdown_write(&crypto_alg_sem);\n\tlist_for_each_entry(q, &crypto_alg_list, cra_list) {\n\t\tif (crypto_is_moribund(q) || !crypto_is_larval(q))\n\t\t\tcontinue;\n\n\t\ttest = (struct crypto_larval *)q;\n\n\t\tif (!strcmp(q->cra_driver_name, name))\n\t\t\tgoto found;\n\t}\n\n\tprintk(KERN_ERR \"alg: Unexpected test result for %s: %d\\n\", name, err);\n\tgoto unlock;\n\nfound:\n\tq->cra_flags |= CRYPTO_ALG_DEAD;\n\talg = test->adult;\n\tif (err || list_empty(&alg->cra_list))\n\t\tgoto complete;\n\n\talg->cra_flags |= CRYPTO_ALG_TESTED;\n\n\tlist_for_each_entry(q, &crypto_alg_list, cra_list) {\n\t\tif (q == alg)\n\t\t\tcontinue;\n\n\t\tif (crypto_is_moribund(q))\n\t\t\tcontinue;\n\n\t\tif (crypto_is_larval(q)) {\n\t\t\tstruct crypto_larval *larval = (void *)q;\n\n\t\t\t/*\n\t\t\t * Check to see if either our generic name or\n\t\t\t * specific name can satisfy the name requested\n\t\t\t * by the larval entry q.\n\t\t\t */\n\t\t\tif (strcmp(alg->cra_name, q->cra_name) &&\n\t\t\t    strcmp(alg->cra_driver_name, q->cra_name))\n\t\t\t\tcontinue;\n\n\t\t\tif (larval->adult)\n\t\t\t\tcontinue;\n\t\t\tif ((q->cra_flags ^ alg->cra_flags) & larval->mask)\n\t\t\t\tcontinue;\n\t\t\tif (!crypto_mod_get(alg))\n\t\t\t\tcontinue;\n\n\t\t\tlarval->adult = alg;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (strcmp(alg->cra_name, q->cra_name))\n\t\t\tcontinue;\n\n\t\tif (strcmp(alg->cra_driver_name, q->cra_driver_name) &&\n\t\t    q->cra_priority > alg->cra_priority)\n\t\t\tcontinue;\n\n\t\tcrypto_remove_spawns(q, &list, alg);\n\t}\n\ncomplete:\n\tcomplete_all(&test->completion);\n\nunlock:\n\tup_write(&crypto_alg_sem);\n\n\tcrypto_remove_final(&list);\n}\nEXPORT_SYMBOL_GPL(crypto_alg_tested);\n\nvoid crypto_remove_final(struct list_head *list)\n{\n\tstruct crypto_alg *alg;\n\tstruct crypto_alg *n;\n\n\tlist_for_each_entry_safe(alg, n, list, cra_list) {\n\t\tlist_del_init(&alg->cra_list);\n\t\tcrypto_alg_put(alg);\n\t}\n}\nEXPORT_SYMBOL_GPL(crypto_remove_final);\n\nstatic void crypto_wait_for_test(struct crypto_larval *larval)\n{\n\tint err;\n\n\terr = crypto_probing_notify(CRYPTO_MSG_ALG_REGISTER, larval->adult);\n\tif (err != NOTIFY_STOP) {\n\t\tif (WARN_ON(err != NOTIFY_DONE))\n\t\t\tgoto out;\n\t\tcrypto_alg_tested(larval->alg.cra_driver_name, 0);\n\t}\n\n\terr = wait_for_completion_interruptible(&larval->completion);\n\tWARN_ON(err);\n\nout:\n\tcrypto_larval_kill(&larval->alg);\n}\n\nint crypto_register_alg(struct crypto_alg *alg)\n{\n\tstruct crypto_larval *larval;\n\tint err;\n\n\terr = crypto_check_alg(alg);\n\tif (err)\n\t\treturn err;\n\n\tdown_write(&crypto_alg_sem);\n\tlarval = __crypto_register_alg(alg);\n\tup_write(&crypto_alg_sem);\n\n\tif (IS_ERR(larval))\n\t\treturn PTR_ERR(larval);\n\n\tcrypto_wait_for_test(larval);\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(crypto_register_alg);\n\nstatic int crypto_remove_alg(struct crypto_alg *alg, struct list_head *list)\n{\n\tif (unlikely(list_empty(&alg->cra_list)))\n\t\treturn -ENOENT;\n\n\talg->cra_flags |= CRYPTO_ALG_DEAD;\n\n\tcrypto_notify(CRYPTO_MSG_ALG_UNREGISTER, alg);\n\tlist_del_init(&alg->cra_list);\n\tcrypto_remove_spawns(alg, list, NULL);\n\n\treturn 0;\n}\n\nint crypto_unregister_alg(struct crypto_alg *alg)\n{\n\tint ret;\n\tLIST_HEAD(list);\n\n\tdown_write(&crypto_alg_sem);\n\tret = crypto_remove_alg(alg, &list);\n\tup_write(&crypto_alg_sem);\n\n\tif (ret)\n\t\treturn ret;\n\n\tBUG_ON(atomic_read(&alg->cra_refcnt) != 1);\n\tif (alg->cra_destroy)\n\t\talg->cra_destroy(alg);\n\n\tcrypto_remove_final(&list);\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(crypto_unregister_alg);\n\nint crypto_register_algs(struct crypto_alg *algs, int count)\n{\n\tint i, ret;\n\n\tfor (i = 0; i < count; i++) {\n\t\tret = crypto_register_alg(&algs[i]);\n\t\tif (ret)\n\t\t\tgoto err;\n\t}\n\n\treturn 0;\n\nerr:\n\tfor (--i; i >= 0; --i)\n\t\tcrypto_unregister_alg(&algs[i]);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(crypto_register_algs);\n\nint crypto_unregister_algs(struct crypto_alg *algs, int count)\n{\n\tint i, ret;\n\n\tfor (i = 0; i < count; i++) {\n\t\tret = crypto_unregister_alg(&algs[i]);\n\t\tif (ret)\n\t\t\tpr_err(\"Failed to unregister %s %s: %d\\n\",\n\t\t\t       algs[i].cra_driver_name, algs[i].cra_name, ret);\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(crypto_unregister_algs);\n\nint crypto_register_template(struct crypto_template *tmpl)\n{\n\tstruct crypto_template *q;\n\tint err = -EEXIST;\n\n\tdown_write(&crypto_alg_sem);\n\n\tcrypto_check_module_sig(tmpl->module);\n\n\tlist_for_each_entry(q, &crypto_template_list, list) {\n\t\tif (q == tmpl)\n\t\t\tgoto out;\n\t}\n\n\tlist_add(&tmpl->list, &crypto_template_list);\n\tcrypto_notify(CRYPTO_MSG_TMPL_REGISTER, tmpl);\n\terr = 0;\nout:\n\tup_write(&crypto_alg_sem);\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(crypto_register_template);\n\nvoid crypto_unregister_template(struct crypto_template *tmpl)\n{\n\tstruct crypto_instance *inst;\n\tstruct hlist_node *n;\n\tstruct hlist_head *list;\n\tLIST_HEAD(users);\n\n\tdown_write(&crypto_alg_sem);\n\n\tBUG_ON(list_empty(&tmpl->list));\n\tlist_del_init(&tmpl->list);\n\n\tlist = &tmpl->instances;\n\thlist_for_each_entry(inst, list, list) {\n\t\tint err = crypto_remove_alg(&inst->alg, &users);\n\t\tBUG_ON(err);\n\t}\n\n\tcrypto_notify(CRYPTO_MSG_TMPL_UNREGISTER, tmpl);\n\n\tup_write(&crypto_alg_sem);\n\n\thlist_for_each_entry_safe(inst, n, list, list) {\n\t\tBUG_ON(atomic_read(&inst->alg.cra_refcnt) != 1);\n\t\ttmpl->free(inst);\n\t}\n\tcrypto_remove_final(&users);\n}\nEXPORT_SYMBOL_GPL(crypto_unregister_template);\n\nstatic struct crypto_template *__crypto_lookup_template(const char *name)\n{\n\tstruct crypto_template *q, *tmpl = NULL;\n\n\tdown_read(&crypto_alg_sem);\n\tlist_for_each_entry(q, &crypto_template_list, list) {\n\t\tif (strcmp(q->name, name))\n\t\t\tcontinue;\n\t\tif (unlikely(!crypto_tmpl_get(q)))\n\t\t\tcontinue;\n\n\t\ttmpl = q;\n\t\tbreak;\n\t}\n\tup_read(&crypto_alg_sem);\n\n\treturn tmpl;\n}\n\nstruct crypto_template *crypto_lookup_template(const char *name)\n{\n\treturn try_then_request_module(__crypto_lookup_template(name), \"%s\",\n\t\t\t\t       name);\n}\nEXPORT_SYMBOL_GPL(crypto_lookup_template);\n\nint crypto_register_instance(struct crypto_template *tmpl,\n\t\t\t     struct crypto_instance *inst)\n{\n\tstruct crypto_larval *larval;\n\tint err;\n\n\terr = crypto_check_alg(&inst->alg);\n\tif (err)\n\t\tgoto err;\n\n\tinst->alg.cra_module = tmpl->module;\n\tinst->alg.cra_flags |= CRYPTO_ALG_INSTANCE;\n\n\tdown_write(&crypto_alg_sem);\n\n\tlarval = __crypto_register_alg(&inst->alg);\n\tif (IS_ERR(larval))\n\t\tgoto unlock;\n\n\thlist_add_head(&inst->list, &tmpl->instances);\n\tinst->tmpl = tmpl;\n\nunlock:\n\tup_write(&crypto_alg_sem);\n\n\terr = PTR_ERR(larval);\n\tif (IS_ERR(larval))\n\t\tgoto err;\n\n\tcrypto_wait_for_test(larval);\n\terr = 0;\n\nerr:\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(crypto_register_instance);\n\nint crypto_unregister_instance(struct crypto_alg *alg)\n{\n\tint err;\n\tstruct crypto_instance *inst = (void *)alg;\n\tstruct crypto_template *tmpl = inst->tmpl;\n\tLIST_HEAD(users);\n\n\tif (!(alg->cra_flags & CRYPTO_ALG_INSTANCE))\n\t\treturn -EINVAL;\n\n\tBUG_ON(atomic_read(&alg->cra_refcnt) != 1);\n\n\tdown_write(&crypto_alg_sem);\n\n\thlist_del_init(&inst->list);\n\terr = crypto_remove_alg(alg, &users);\n\n\tup_write(&crypto_alg_sem);\n\n\tif (err)\n\t\treturn err;\n\n\ttmpl->free(inst);\n\tcrypto_remove_final(&users);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(crypto_unregister_instance);\n\nint crypto_init_spawn(struct crypto_spawn *spawn, struct crypto_alg *alg,\n\t\t      struct crypto_instance *inst, u32 mask)\n{\n\tint err = -EAGAIN;\n\n\tspawn->inst = inst;\n\tspawn->mask = mask;\n\n\tdown_write(&crypto_alg_sem);\n\tif (!crypto_is_moribund(alg)) {\n\t\tlist_add(&spawn->list, &alg->cra_users);\n\t\tspawn->alg = alg;\n\t\terr = 0;\n\t}\n\tup_write(&crypto_alg_sem);\n\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(crypto_init_spawn);\n\nint crypto_init_spawn2(struct crypto_spawn *spawn, struct crypto_alg *alg,\n\t\t       struct crypto_instance *inst,\n\t\t       const struct crypto_type *frontend)\n{\n\tint err = -EINVAL;\n\n\tif ((alg->cra_flags ^ frontend->type) & frontend->maskset)\n\t\tgoto out;\n\n\tspawn->frontend = frontend;\n\terr = crypto_init_spawn(spawn, alg, inst, frontend->maskset);\n\nout:\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(crypto_init_spawn2);\n\nvoid crypto_drop_spawn(struct crypto_spawn *spawn)\n{\n\tif (!spawn->alg)\n\t\treturn;\n\n\tdown_write(&crypto_alg_sem);\n\tlist_del(&spawn->list);\n\tup_write(&crypto_alg_sem);\n}\nEXPORT_SYMBOL_GPL(crypto_drop_spawn);\n\nstatic struct crypto_alg *crypto_spawn_alg(struct crypto_spawn *spawn)\n{\n\tstruct crypto_alg *alg;\n\tstruct crypto_alg *alg2;\n\n\tdown_read(&crypto_alg_sem);\n\talg = spawn->alg;\n\talg2 = alg;\n\tif (alg2)\n\t\talg2 = crypto_mod_get(alg2);\n\tup_read(&crypto_alg_sem);\n\n\tif (!alg2) {\n\t\tif (alg)\n\t\t\tcrypto_shoot_alg(alg);\n\t\treturn ERR_PTR(-EAGAIN);\n\t}\n\n\treturn alg;\n}\n\nstruct crypto_tfm *crypto_spawn_tfm(struct crypto_spawn *spawn, u32 type,\n\t\t\t\t    u32 mask)\n{\n\tstruct crypto_alg *alg;\n\tstruct crypto_tfm *tfm;\n\n\talg = crypto_spawn_alg(spawn);\n\tif (IS_ERR(alg))\n\t\treturn ERR_CAST(alg);\n\n\ttfm = ERR_PTR(-EINVAL);\n\tif (unlikely((alg->cra_flags ^ type) & mask))\n\t\tgoto out_put_alg;\n\n\ttfm = __crypto_alloc_tfm(alg, type, mask);\n\tif (IS_ERR(tfm))\n\t\tgoto out_put_alg;\n\n\treturn tfm;\n\nout_put_alg:\n\tcrypto_mod_put(alg);\n\treturn tfm;\n}\nEXPORT_SYMBOL_GPL(crypto_spawn_tfm);\n\nvoid *crypto_spawn_tfm2(struct crypto_spawn *spawn)\n{\n\tstruct crypto_alg *alg;\n\tstruct crypto_tfm *tfm;\n\n\talg = crypto_spawn_alg(spawn);\n\tif (IS_ERR(alg))\n\t\treturn ERR_CAST(alg);\n\n\ttfm = crypto_create_tfm(alg, spawn->frontend);\n\tif (IS_ERR(tfm))\n\t\tgoto out_put_alg;\n\n\treturn tfm;\n\nout_put_alg:\n\tcrypto_mod_put(alg);\n\treturn tfm;\n}\nEXPORT_SYMBOL_GPL(crypto_spawn_tfm2);\n\nint crypto_register_notifier(struct notifier_block *nb)\n{\n\treturn blocking_notifier_chain_register(&crypto_chain, nb);\n}\nEXPORT_SYMBOL_GPL(crypto_register_notifier);\n\nint crypto_unregister_notifier(struct notifier_block *nb)\n{\n\treturn blocking_notifier_chain_unregister(&crypto_chain, nb);\n}\nEXPORT_SYMBOL_GPL(crypto_unregister_notifier);\n\nstruct crypto_attr_type *crypto_get_attr_type(struct rtattr **tb)\n{\n\tstruct rtattr *rta = tb[0];\n\tstruct crypto_attr_type *algt;\n\n\tif (!rta)\n\t\treturn ERR_PTR(-ENOENT);\n\tif (RTA_PAYLOAD(rta) < sizeof(*algt))\n\t\treturn ERR_PTR(-EINVAL);\n\tif (rta->rta_type != CRYPTOA_TYPE)\n\t\treturn ERR_PTR(-EINVAL);\n\n\talgt = RTA_DATA(rta);\n\n\treturn algt;\n}\nEXPORT_SYMBOL_GPL(crypto_get_attr_type);\n\nint crypto_check_attr_type(struct rtattr **tb, u32 type)\n{\n\tstruct crypto_attr_type *algt;\n\n\talgt = crypto_get_attr_type(tb);\n\tif (IS_ERR(algt))\n\t\treturn PTR_ERR(algt);\n\n\tif ((algt->type ^ type) & algt->mask)\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(crypto_check_attr_type);\n\nconst char *crypto_attr_alg_name(struct rtattr *rta)\n{\n\tstruct crypto_attr_alg *alga;\n\n\tif (!rta)\n\t\treturn ERR_PTR(-ENOENT);\n\tif (RTA_PAYLOAD(rta) < sizeof(*alga))\n\t\treturn ERR_PTR(-EINVAL);\n\tif (rta->rta_type != CRYPTOA_ALG)\n\t\treturn ERR_PTR(-EINVAL);\n\n\talga = RTA_DATA(rta);\n\talga->name[CRYPTO_MAX_ALG_NAME - 1] = 0;\n\n\treturn alga->name;\n}\nEXPORT_SYMBOL_GPL(crypto_attr_alg_name);\n\nstruct crypto_alg *crypto_attr_alg2(struct rtattr *rta,\n\t\t\t\t    const struct crypto_type *frontend,\n\t\t\t\t    u32 type, u32 mask)\n{\n\tconst char *name;\n\n\tname = crypto_attr_alg_name(rta);\n\tif (IS_ERR(name))\n\t\treturn ERR_CAST(name);\n\n\treturn crypto_find_alg(name, frontend, type, mask);\n}\nEXPORT_SYMBOL_GPL(crypto_attr_alg2);\n\nint crypto_attr_u32(struct rtattr *rta, u32 *num)\n{\n\tstruct crypto_attr_u32 *nu32;\n\n\tif (!rta)\n\t\treturn -ENOENT;\n\tif (RTA_PAYLOAD(rta) < sizeof(*nu32))\n\t\treturn -EINVAL;\n\tif (rta->rta_type != CRYPTOA_U32)\n\t\treturn -EINVAL;\n\n\tnu32 = RTA_DATA(rta);\n\t*num = nu32->num;\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(crypto_attr_u32);\n\nvoid *crypto_alloc_instance2(const char *name, struct crypto_alg *alg,\n\t\t\t     unsigned int head)\n{\n\tstruct crypto_instance *inst;\n\tchar *p;\n\tint err;\n\n\tp = kzalloc(head + sizeof(*inst) + sizeof(struct crypto_spawn),\n\t\t    GFP_KERNEL);\n\tif (!p)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tinst = (void *)(p + head);\n\n\terr = -ENAMETOOLONG;\n\tif (snprintf(inst->alg.cra_name, CRYPTO_MAX_ALG_NAME, \"%s(%s)\", name,\n\t\t     alg->cra_name) >= CRYPTO_MAX_ALG_NAME)\n\t\tgoto err_free_inst;\n\n\tif (snprintf(inst->alg.cra_driver_name, CRYPTO_MAX_ALG_NAME, \"%s(%s)\",\n\t\t     name, alg->cra_driver_name) >= CRYPTO_MAX_ALG_NAME)\n\t\tgoto err_free_inst;\n\n\treturn p;\n\nerr_free_inst:\n\tkfree(p);\n\treturn ERR_PTR(err);\n}\nEXPORT_SYMBOL_GPL(crypto_alloc_instance2);\n\nstruct crypto_instance *crypto_alloc_instance(const char *name,\n\t\t\t\t\t      struct crypto_alg *alg)\n{\n\tstruct crypto_instance *inst;\n\tstruct crypto_spawn *spawn;\n\tint err;\n\n\tinst = crypto_alloc_instance2(name, alg, 0);\n\tif (IS_ERR(inst))\n\t\tgoto out;\n\n\tspawn = crypto_instance_ctx(inst);\n\terr = crypto_init_spawn(spawn, alg, inst,\n\t\t\t\tCRYPTO_ALG_TYPE_MASK | CRYPTO_ALG_ASYNC);\n\n\tif (err)\n\t\tgoto err_free_inst;\n\n\treturn inst;\n\nerr_free_inst:\n\tkfree(inst);\n\tinst = ERR_PTR(err);\n\nout:\n\treturn inst;\n}\nEXPORT_SYMBOL_GPL(crypto_alloc_instance);\n\nvoid crypto_init_queue(struct crypto_queue *queue, unsigned int max_qlen)\n{\n\tINIT_LIST_HEAD(&queue->list);\n\tqueue->backlog = &queue->list;\n\tqueue->qlen = 0;\n\tqueue->max_qlen = max_qlen;\n}\nEXPORT_SYMBOL_GPL(crypto_init_queue);\n\nint crypto_enqueue_request(struct crypto_queue *queue,\n\t\t\t   struct crypto_async_request *request)\n{\n\tint err = -EINPROGRESS;\n\n\tif (unlikely(queue->qlen >= queue->max_qlen)) {\n\t\terr = -EBUSY;\n\t\tif (!(request->flags & CRYPTO_TFM_REQ_MAY_BACKLOG))\n\t\t\tgoto out;\n\t\tif (queue->backlog == &queue->list)\n\t\t\tqueue->backlog = &request->list;\n\t}\n\n\tqueue->qlen++;\n\tlist_add_tail(&request->list, &queue->list);\n\nout:\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(crypto_enqueue_request);\n\nvoid *__crypto_dequeue_request(struct crypto_queue *queue, unsigned int offset)\n{\n\tstruct list_head *request;\n\n\tif (unlikely(!queue->qlen))\n\t\treturn NULL;\n\n\tqueue->qlen--;\n\n\tif (queue->backlog != &queue->list)\n\t\tqueue->backlog = queue->backlog->next;\n\n\trequest = queue->list.next;\n\tlist_del(request);\n\n\treturn (char *)list_entry(request, struct crypto_async_request, list) -\n\t       offset;\n}\nEXPORT_SYMBOL_GPL(__crypto_dequeue_request);\n\nstruct crypto_async_request *crypto_dequeue_request(struct crypto_queue *queue)\n{\n\treturn __crypto_dequeue_request(queue, 0);\n}\nEXPORT_SYMBOL_GPL(crypto_dequeue_request);\n\nint crypto_tfm_in_queue(struct crypto_queue *queue, struct crypto_tfm *tfm)\n{\n\tstruct crypto_async_request *req;\n\n\tlist_for_each_entry(req, &queue->list, list) {\n\t\tif (req->tfm == tfm)\n\t\t\treturn 1;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(crypto_tfm_in_queue);\n\nstatic inline void crypto_inc_byte(u8 *a, unsigned int size)\n{\n\tu8 *b = (a + size);\n\tu8 c;\n\n\tfor (; size; size--) {\n\t\tc = *--b + 1;\n\t\t*b = c;\n\t\tif (c)\n\t\t\tbreak;\n\t}\n}\n\nvoid crypto_inc(u8 *a, unsigned int size)\n{\n\t__be32 *b = (__be32 *)(a + size);\n\tu32 c;\n\n\tfor (; size >= 4; size -= 4) {\n\t\tc = be32_to_cpu(*--b) + 1;\n\t\t*b = cpu_to_be32(c);\n\t\tif (c)\n\t\t\treturn;\n\t}\n\n\tcrypto_inc_byte(a, size);\n}\nEXPORT_SYMBOL_GPL(crypto_inc);\n\nstatic inline void crypto_xor_byte(u8 *a, const u8 *b, unsigned int size)\n{\n\tfor (; size; size--)\n\t\t*a++ ^= *b++;\n}\n\nvoid crypto_xor(u8 *dst, const u8 *src, unsigned int size)\n{\n\tu32 *a = (u32 *)dst;\n\tu32 *b = (u32 *)src;\n\n\tfor (; size >= 4; size -= 4)\n\t\t*a++ ^= *b++;\n\n\tcrypto_xor_byte((u8 *)a, (u8 *)b, size);\n}\nEXPORT_SYMBOL_GPL(crypto_xor);\n\nstatic int __init crypto_algapi_init(void)\n{\n\tcrypto_init_proc();\n\treturn 0;\n}\n\nstatic void __exit crypto_algapi_exit(void)\n{\n\tcrypto_exit_proc();\n}\n\nmodule_init(crypto_algapi_init);\nmodule_exit(crypto_algapi_exit);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"Cryptographic algorithms API\");\n", "/*\n * Authenc: Simple AEAD wrapper for IPsec\n *\n * Copyright (c) 2007 Herbert Xu <herbert@gondor.apana.org.au>\n *\n * This program is free software; you can redistribute it and/or modify it\n * under the terms of the GNU General Public License as published by the Free\n * Software Foundation; either version 2 of the License, or (at your option)\n * any later version.\n *\n */\n\n#include <crypto/aead.h>\n#include <crypto/internal/hash.h>\n#include <crypto/internal/skcipher.h>\n#include <crypto/authenc.h>\n#include <crypto/scatterwalk.h>\n#include <linux/err.h>\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/rtnetlink.h>\n#include <linux/slab.h>\n#include <linux/spinlock.h>\n\ntypedef u8 *(*authenc_ahash_t)(struct aead_request *req, unsigned int flags);\n\nstruct authenc_instance_ctx {\n\tstruct crypto_ahash_spawn auth;\n\tstruct crypto_skcipher_spawn enc;\n};\n\nstruct crypto_authenc_ctx {\n\tunsigned int reqoff;\n\tstruct crypto_ahash *auth;\n\tstruct crypto_ablkcipher *enc;\n};\n\nstruct authenc_request_ctx {\n\tunsigned int cryptlen;\n\tstruct scatterlist *sg;\n\tstruct scatterlist asg[2];\n\tstruct scatterlist cipher[2];\n\tcrypto_completion_t complete;\n\tcrypto_completion_t update_complete;\n\tchar tail[];\n};\n\nstatic void authenc_request_complete(struct aead_request *req, int err)\n{\n\tif (err != -EINPROGRESS)\n\t\taead_request_complete(req, err);\n}\n\nint crypto_authenc_extractkeys(struct crypto_authenc_keys *keys, const u8 *key,\n\t\t\t       unsigned int keylen)\n{\n\tstruct rtattr *rta = (struct rtattr *)key;\n\tstruct crypto_authenc_key_param *param;\n\n\tif (!RTA_OK(rta, keylen))\n\t\treturn -EINVAL;\n\tif (rta->rta_type != CRYPTO_AUTHENC_KEYA_PARAM)\n\t\treturn -EINVAL;\n\tif (RTA_PAYLOAD(rta) < sizeof(*param))\n\t\treturn -EINVAL;\n\n\tparam = RTA_DATA(rta);\n\tkeys->enckeylen = be32_to_cpu(param->enckeylen);\n\n\tkey += RTA_ALIGN(rta->rta_len);\n\tkeylen -= RTA_ALIGN(rta->rta_len);\n\n\tif (keylen < keys->enckeylen)\n\t\treturn -EINVAL;\n\n\tkeys->authkeylen = keylen - keys->enckeylen;\n\tkeys->authkey = key;\n\tkeys->enckey = key + keys->authkeylen;\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(crypto_authenc_extractkeys);\n\nstatic int crypto_authenc_setkey(struct crypto_aead *authenc, const u8 *key,\n\t\t\t\t unsigned int keylen)\n{\n\tstruct crypto_authenc_ctx *ctx = crypto_aead_ctx(authenc);\n\tstruct crypto_ahash *auth = ctx->auth;\n\tstruct crypto_ablkcipher *enc = ctx->enc;\n\tstruct crypto_authenc_keys keys;\n\tint err = -EINVAL;\n\n\tif (crypto_authenc_extractkeys(&keys, key, keylen) != 0)\n\t\tgoto badkey;\n\n\tcrypto_ahash_clear_flags(auth, CRYPTO_TFM_REQ_MASK);\n\tcrypto_ahash_set_flags(auth, crypto_aead_get_flags(authenc) &\n\t\t\t\t    CRYPTO_TFM_REQ_MASK);\n\terr = crypto_ahash_setkey(auth, keys.authkey, keys.authkeylen);\n\tcrypto_aead_set_flags(authenc, crypto_ahash_get_flags(auth) &\n\t\t\t\t       CRYPTO_TFM_RES_MASK);\n\n\tif (err)\n\t\tgoto out;\n\n\tcrypto_ablkcipher_clear_flags(enc, CRYPTO_TFM_REQ_MASK);\n\tcrypto_ablkcipher_set_flags(enc, crypto_aead_get_flags(authenc) &\n\t\t\t\t\t CRYPTO_TFM_REQ_MASK);\n\terr = crypto_ablkcipher_setkey(enc, keys.enckey, keys.enckeylen);\n\tcrypto_aead_set_flags(authenc, crypto_ablkcipher_get_flags(enc) &\n\t\t\t\t       CRYPTO_TFM_RES_MASK);\n\nout:\n\treturn err;\n\nbadkey:\n\tcrypto_aead_set_flags(authenc, CRYPTO_TFM_RES_BAD_KEY_LEN);\n\tgoto out;\n}\n\nstatic void authenc_geniv_ahash_update_done(struct crypto_async_request *areq,\n\t\t\t\t\t    int err)\n{\n\tstruct aead_request *req = areq->data;\n\tstruct crypto_aead *authenc = crypto_aead_reqtfm(req);\n\tstruct crypto_authenc_ctx *ctx = crypto_aead_ctx(authenc);\n\tstruct authenc_request_ctx *areq_ctx = aead_request_ctx(req);\n\tstruct ahash_request *ahreq = (void *)(areq_ctx->tail + ctx->reqoff);\n\n\tif (err)\n\t\tgoto out;\n\n\tahash_request_set_crypt(ahreq, areq_ctx->sg, ahreq->result,\n\t\t\t\tareq_ctx->cryptlen);\n\tahash_request_set_callback(ahreq, aead_request_flags(req) &\n\t\t\t\t\t  CRYPTO_TFM_REQ_MAY_SLEEP,\n\t\t\t\t   areq_ctx->complete, req);\n\n\terr = crypto_ahash_finup(ahreq);\n\tif (err)\n\t\tgoto out;\n\n\tscatterwalk_map_and_copy(ahreq->result, areq_ctx->sg,\n\t\t\t\t areq_ctx->cryptlen,\n\t\t\t\t crypto_aead_authsize(authenc), 1);\n\nout:\n\tauthenc_request_complete(req, err);\n}\n\nstatic void authenc_geniv_ahash_done(struct crypto_async_request *areq, int err)\n{\n\tstruct aead_request *req = areq->data;\n\tstruct crypto_aead *authenc = crypto_aead_reqtfm(req);\n\tstruct crypto_authenc_ctx *ctx = crypto_aead_ctx(authenc);\n\tstruct authenc_request_ctx *areq_ctx = aead_request_ctx(req);\n\tstruct ahash_request *ahreq = (void *)(areq_ctx->tail + ctx->reqoff);\n\n\tif (err)\n\t\tgoto out;\n\n\tscatterwalk_map_and_copy(ahreq->result, areq_ctx->sg,\n\t\t\t\t areq_ctx->cryptlen,\n\t\t\t\t crypto_aead_authsize(authenc), 1);\n\nout:\n\taead_request_complete(req, err);\n}\n\nstatic void authenc_verify_ahash_update_done(struct crypto_async_request *areq,\n\t\t\t\t\t     int err)\n{\n\tu8 *ihash;\n\tunsigned int authsize;\n\tstruct ablkcipher_request *abreq;\n\tstruct aead_request *req = areq->data;\n\tstruct crypto_aead *authenc = crypto_aead_reqtfm(req);\n\tstruct crypto_authenc_ctx *ctx = crypto_aead_ctx(authenc);\n\tstruct authenc_request_ctx *areq_ctx = aead_request_ctx(req);\n\tstruct ahash_request *ahreq = (void *)(areq_ctx->tail + ctx->reqoff);\n\tunsigned int cryptlen = req->cryptlen;\n\n\tif (err)\n\t\tgoto out;\n\n\tahash_request_set_crypt(ahreq, areq_ctx->sg, ahreq->result,\n\t\t\t\tareq_ctx->cryptlen);\n\tahash_request_set_callback(ahreq, aead_request_flags(req) &\n\t\t\t\t\t  CRYPTO_TFM_REQ_MAY_SLEEP,\n\t\t\t\t   areq_ctx->complete, req);\n\n\terr = crypto_ahash_finup(ahreq);\n\tif (err)\n\t\tgoto out;\n\n\tauthsize = crypto_aead_authsize(authenc);\n\tcryptlen -= authsize;\n\tihash = ahreq->result + authsize;\n\tscatterwalk_map_and_copy(ihash, areq_ctx->sg, areq_ctx->cryptlen,\n\t\t\t\t authsize, 0);\n\n\terr = crypto_memneq(ihash, ahreq->result, authsize) ? -EBADMSG : 0;\n\tif (err)\n\t\tgoto out;\n\n\tabreq = aead_request_ctx(req);\n\tablkcipher_request_set_tfm(abreq, ctx->enc);\n\tablkcipher_request_set_callback(abreq, aead_request_flags(req),\n\t\t\t\t\treq->base.complete, req->base.data);\n\tablkcipher_request_set_crypt(abreq, req->src, req->dst,\n\t\t\t\t     cryptlen, req->iv);\n\n\terr = crypto_ablkcipher_decrypt(abreq);\n\nout:\n\tauthenc_request_complete(req, err);\n}\n\nstatic void authenc_verify_ahash_done(struct crypto_async_request *areq,\n\t\t\t\t      int err)\n{\n\tu8 *ihash;\n\tunsigned int authsize;\n\tstruct ablkcipher_request *abreq;\n\tstruct aead_request *req = areq->data;\n\tstruct crypto_aead *authenc = crypto_aead_reqtfm(req);\n\tstruct crypto_authenc_ctx *ctx = crypto_aead_ctx(authenc);\n\tstruct authenc_request_ctx *areq_ctx = aead_request_ctx(req);\n\tstruct ahash_request *ahreq = (void *)(areq_ctx->tail + ctx->reqoff);\n\tunsigned int cryptlen = req->cryptlen;\n\n\tif (err)\n\t\tgoto out;\n\n\tauthsize = crypto_aead_authsize(authenc);\n\tcryptlen -= authsize;\n\tihash = ahreq->result + authsize;\n\tscatterwalk_map_and_copy(ihash, areq_ctx->sg, areq_ctx->cryptlen,\n\t\t\t\t authsize, 0);\n\n\terr = crypto_memneq(ihash, ahreq->result, authsize) ? -EBADMSG : 0;\n\tif (err)\n\t\tgoto out;\n\n\tabreq = aead_request_ctx(req);\n\tablkcipher_request_set_tfm(abreq, ctx->enc);\n\tablkcipher_request_set_callback(abreq, aead_request_flags(req),\n\t\t\t\t\treq->base.complete, req->base.data);\n\tablkcipher_request_set_crypt(abreq, req->src, req->dst,\n\t\t\t\t     cryptlen, req->iv);\n\n\terr = crypto_ablkcipher_decrypt(abreq);\n\nout:\n\tauthenc_request_complete(req, err);\n}\n\nstatic u8 *crypto_authenc_ahash_fb(struct aead_request *req, unsigned int flags)\n{\n\tstruct crypto_aead *authenc = crypto_aead_reqtfm(req);\n\tstruct crypto_authenc_ctx *ctx = crypto_aead_ctx(authenc);\n\tstruct crypto_ahash *auth = ctx->auth;\n\tstruct authenc_request_ctx *areq_ctx = aead_request_ctx(req);\n\tstruct ahash_request *ahreq = (void *)(areq_ctx->tail + ctx->reqoff);\n\tu8 *hash = areq_ctx->tail;\n\tint err;\n\n\thash = (u8 *)ALIGN((unsigned long)hash + crypto_ahash_alignmask(auth),\n\t\t\t    crypto_ahash_alignmask(auth) + 1);\n\n\tahash_request_set_tfm(ahreq, auth);\n\n\terr = crypto_ahash_init(ahreq);\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\tahash_request_set_crypt(ahreq, req->assoc, hash, req->assoclen);\n\tahash_request_set_callback(ahreq, aead_request_flags(req) & flags,\n\t\t\t\t   areq_ctx->update_complete, req);\n\n\terr = crypto_ahash_update(ahreq);\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\tahash_request_set_crypt(ahreq, areq_ctx->sg, hash,\n\t\t\t\tareq_ctx->cryptlen);\n\tahash_request_set_callback(ahreq, aead_request_flags(req) & flags,\n\t\t\t\t   areq_ctx->complete, req);\n\n\terr = crypto_ahash_finup(ahreq);\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\treturn hash;\n}\n\nstatic u8 *crypto_authenc_ahash(struct aead_request *req, unsigned int flags)\n{\n\tstruct crypto_aead *authenc = crypto_aead_reqtfm(req);\n\tstruct crypto_authenc_ctx *ctx = crypto_aead_ctx(authenc);\n\tstruct crypto_ahash *auth = ctx->auth;\n\tstruct authenc_request_ctx *areq_ctx = aead_request_ctx(req);\n\tstruct ahash_request *ahreq = (void *)(areq_ctx->tail + ctx->reqoff);\n\tu8 *hash = areq_ctx->tail;\n\tint err;\n\n\thash = (u8 *)ALIGN((unsigned long)hash + crypto_ahash_alignmask(auth),\n\t\t\t   crypto_ahash_alignmask(auth) + 1);\n\n\tahash_request_set_tfm(ahreq, auth);\n\tahash_request_set_crypt(ahreq, areq_ctx->sg, hash,\n\t\t\t\tareq_ctx->cryptlen);\n\tahash_request_set_callback(ahreq, aead_request_flags(req) & flags,\n\t\t\t\t   areq_ctx->complete, req);\n\n\terr = crypto_ahash_digest(ahreq);\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\treturn hash;\n}\n\nstatic int crypto_authenc_genicv(struct aead_request *req, u8 *iv,\n\t\t\t\t unsigned int flags)\n{\n\tstruct crypto_aead *authenc = crypto_aead_reqtfm(req);\n\tstruct authenc_request_ctx *areq_ctx = aead_request_ctx(req);\n\tstruct scatterlist *dst = req->dst;\n\tstruct scatterlist *assoc = req->assoc;\n\tstruct scatterlist *cipher = areq_ctx->cipher;\n\tstruct scatterlist *asg = areq_ctx->asg;\n\tunsigned int ivsize = crypto_aead_ivsize(authenc);\n\tunsigned int cryptlen = req->cryptlen;\n\tauthenc_ahash_t authenc_ahash_fn = crypto_authenc_ahash_fb;\n\tstruct page *dstp;\n\tu8 *vdst;\n\tu8 *hash;\n\n\tdstp = sg_page(dst);\n\tvdst = PageHighMem(dstp) ? NULL : page_address(dstp) + dst->offset;\n\n\tif (ivsize) {\n\t\tsg_init_table(cipher, 2);\n\t\tsg_set_buf(cipher, iv, ivsize);\n\t\tscatterwalk_crypto_chain(cipher, dst, vdst == iv + ivsize, 2);\n\t\tdst = cipher;\n\t\tcryptlen += ivsize;\n\t}\n\n\tif (req->assoclen && sg_is_last(assoc)) {\n\t\tauthenc_ahash_fn = crypto_authenc_ahash;\n\t\tsg_init_table(asg, 2);\n\t\tsg_set_page(asg, sg_page(assoc), assoc->length, assoc->offset);\n\t\tscatterwalk_crypto_chain(asg, dst, 0, 2);\n\t\tdst = asg;\n\t\tcryptlen += req->assoclen;\n\t}\n\n\tareq_ctx->cryptlen = cryptlen;\n\tareq_ctx->sg = dst;\n\n\tareq_ctx->complete = authenc_geniv_ahash_done;\n\tareq_ctx->update_complete = authenc_geniv_ahash_update_done;\n\n\thash = authenc_ahash_fn(req, flags);\n\tif (IS_ERR(hash))\n\t\treturn PTR_ERR(hash);\n\n\tscatterwalk_map_and_copy(hash, dst, cryptlen,\n\t\t\t\t crypto_aead_authsize(authenc), 1);\n\treturn 0;\n}\n\nstatic void crypto_authenc_encrypt_done(struct crypto_async_request *req,\n\t\t\t\t\tint err)\n{\n\tstruct aead_request *areq = req->data;\n\n\tif (!err) {\n\t\tstruct crypto_aead *authenc = crypto_aead_reqtfm(areq);\n\t\tstruct crypto_authenc_ctx *ctx = crypto_aead_ctx(authenc);\n\t\tstruct authenc_request_ctx *areq_ctx = aead_request_ctx(areq);\n\t\tstruct ablkcipher_request *abreq = (void *)(areq_ctx->tail\n\t\t\t\t\t\t\t    + ctx->reqoff);\n\t\tu8 *iv = (u8 *)abreq - crypto_ablkcipher_ivsize(ctx->enc);\n\n\t\terr = crypto_authenc_genicv(areq, iv, 0);\n\t}\n\n\tauthenc_request_complete(areq, err);\n}\n\nstatic int crypto_authenc_encrypt(struct aead_request *req)\n{\n\tstruct crypto_aead *authenc = crypto_aead_reqtfm(req);\n\tstruct crypto_authenc_ctx *ctx = crypto_aead_ctx(authenc);\n\tstruct authenc_request_ctx *areq_ctx = aead_request_ctx(req);\n\tstruct crypto_ablkcipher *enc = ctx->enc;\n\tstruct scatterlist *dst = req->dst;\n\tunsigned int cryptlen = req->cryptlen;\n\tstruct ablkcipher_request *abreq = (void *)(areq_ctx->tail\n\t\t\t\t\t\t    + ctx->reqoff);\n\tu8 *iv = (u8 *)abreq - crypto_ablkcipher_ivsize(enc);\n\tint err;\n\n\tablkcipher_request_set_tfm(abreq, enc);\n\tablkcipher_request_set_callback(abreq, aead_request_flags(req),\n\t\t\t\t\tcrypto_authenc_encrypt_done, req);\n\tablkcipher_request_set_crypt(abreq, req->src, dst, cryptlen, req->iv);\n\n\tmemcpy(iv, req->iv, crypto_aead_ivsize(authenc));\n\n\terr = crypto_ablkcipher_encrypt(abreq);\n\tif (err)\n\t\treturn err;\n\n\treturn crypto_authenc_genicv(req, iv, CRYPTO_TFM_REQ_MAY_SLEEP);\n}\n\nstatic void crypto_authenc_givencrypt_done(struct crypto_async_request *req,\n\t\t\t\t\t   int err)\n{\n\tstruct aead_request *areq = req->data;\n\n\tif (!err) {\n\t\tstruct skcipher_givcrypt_request *greq = aead_request_ctx(areq);\n\n\t\terr = crypto_authenc_genicv(areq, greq->giv, 0);\n\t}\n\n\tauthenc_request_complete(areq, err);\n}\n\nstatic int crypto_authenc_givencrypt(struct aead_givcrypt_request *req)\n{\n\tstruct crypto_aead *authenc = aead_givcrypt_reqtfm(req);\n\tstruct crypto_authenc_ctx *ctx = crypto_aead_ctx(authenc);\n\tstruct aead_request *areq = &req->areq;\n\tstruct skcipher_givcrypt_request *greq = aead_request_ctx(areq);\n\tu8 *iv = req->giv;\n\tint err;\n\n\tskcipher_givcrypt_set_tfm(greq, ctx->enc);\n\tskcipher_givcrypt_set_callback(greq, aead_request_flags(areq),\n\t\t\t\t       crypto_authenc_givencrypt_done, areq);\n\tskcipher_givcrypt_set_crypt(greq, areq->src, areq->dst, areq->cryptlen,\n\t\t\t\t    areq->iv);\n\tskcipher_givcrypt_set_giv(greq, iv, req->seq);\n\n\terr = crypto_skcipher_givencrypt(greq);\n\tif (err)\n\t\treturn err;\n\n\treturn crypto_authenc_genicv(areq, iv, CRYPTO_TFM_REQ_MAY_SLEEP);\n}\n\nstatic int crypto_authenc_verify(struct aead_request *req,\n\t\t\t\t authenc_ahash_t authenc_ahash_fn)\n{\n\tstruct crypto_aead *authenc = crypto_aead_reqtfm(req);\n\tstruct authenc_request_ctx *areq_ctx = aead_request_ctx(req);\n\tu8 *ohash;\n\tu8 *ihash;\n\tunsigned int authsize;\n\n\tareq_ctx->complete = authenc_verify_ahash_done;\n\tareq_ctx->update_complete = authenc_verify_ahash_update_done;\n\n\tohash = authenc_ahash_fn(req, CRYPTO_TFM_REQ_MAY_SLEEP);\n\tif (IS_ERR(ohash))\n\t\treturn PTR_ERR(ohash);\n\n\tauthsize = crypto_aead_authsize(authenc);\n\tihash = ohash + authsize;\n\tscatterwalk_map_and_copy(ihash, areq_ctx->sg, areq_ctx->cryptlen,\n\t\t\t\t authsize, 0);\n\treturn crypto_memneq(ihash, ohash, authsize) ? -EBADMSG : 0;\n}\n\nstatic int crypto_authenc_iverify(struct aead_request *req, u8 *iv,\n\t\t\t\t  unsigned int cryptlen)\n{\n\tstruct crypto_aead *authenc = crypto_aead_reqtfm(req);\n\tstruct authenc_request_ctx *areq_ctx = aead_request_ctx(req);\n\tstruct scatterlist *src = req->src;\n\tstruct scatterlist *assoc = req->assoc;\n\tstruct scatterlist *cipher = areq_ctx->cipher;\n\tstruct scatterlist *asg = areq_ctx->asg;\n\tunsigned int ivsize = crypto_aead_ivsize(authenc);\n\tauthenc_ahash_t authenc_ahash_fn = crypto_authenc_ahash_fb;\n\tstruct page *srcp;\n\tu8 *vsrc;\n\n\tsrcp = sg_page(src);\n\tvsrc = PageHighMem(srcp) ? NULL : page_address(srcp) + src->offset;\n\n\tif (ivsize) {\n\t\tsg_init_table(cipher, 2);\n\t\tsg_set_buf(cipher, iv, ivsize);\n\t\tscatterwalk_crypto_chain(cipher, src, vsrc == iv + ivsize, 2);\n\t\tsrc = cipher;\n\t\tcryptlen += ivsize;\n\t}\n\n\tif (req->assoclen && sg_is_last(assoc)) {\n\t\tauthenc_ahash_fn = crypto_authenc_ahash;\n\t\tsg_init_table(asg, 2);\n\t\tsg_set_page(asg, sg_page(assoc), assoc->length, assoc->offset);\n\t\tscatterwalk_crypto_chain(asg, src, 0, 2);\n\t\tsrc = asg;\n\t\tcryptlen += req->assoclen;\n\t}\n\n\tareq_ctx->cryptlen = cryptlen;\n\tareq_ctx->sg = src;\n\n\treturn crypto_authenc_verify(req, authenc_ahash_fn);\n}\n\nstatic int crypto_authenc_decrypt(struct aead_request *req)\n{\n\tstruct crypto_aead *authenc = crypto_aead_reqtfm(req);\n\tstruct crypto_authenc_ctx *ctx = crypto_aead_ctx(authenc);\n\tstruct ablkcipher_request *abreq = aead_request_ctx(req);\n\tunsigned int cryptlen = req->cryptlen;\n\tunsigned int authsize = crypto_aead_authsize(authenc);\n\tu8 *iv = req->iv;\n\tint err;\n\n\tif (cryptlen < authsize)\n\t\treturn -EINVAL;\n\tcryptlen -= authsize;\n\n\terr = crypto_authenc_iverify(req, iv, cryptlen);\n\tif (err)\n\t\treturn err;\n\n\tablkcipher_request_set_tfm(abreq, ctx->enc);\n\tablkcipher_request_set_callback(abreq, aead_request_flags(req),\n\t\t\t\t\treq->base.complete, req->base.data);\n\tablkcipher_request_set_crypt(abreq, req->src, req->dst, cryptlen, iv);\n\n\treturn crypto_ablkcipher_decrypt(abreq);\n}\n\nstatic int crypto_authenc_init_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_instance *inst = crypto_tfm_alg_instance(tfm);\n\tstruct authenc_instance_ctx *ictx = crypto_instance_ctx(inst);\n\tstruct crypto_authenc_ctx *ctx = crypto_tfm_ctx(tfm);\n\tstruct crypto_ahash *auth;\n\tstruct crypto_ablkcipher *enc;\n\tint err;\n\n\tauth = crypto_spawn_ahash(&ictx->auth);\n\tif (IS_ERR(auth))\n\t\treturn PTR_ERR(auth);\n\n\tenc = crypto_spawn_skcipher(&ictx->enc);\n\terr = PTR_ERR(enc);\n\tif (IS_ERR(enc))\n\t\tgoto err_free_ahash;\n\n\tctx->auth = auth;\n\tctx->enc = enc;\n\n\tctx->reqoff = ALIGN(2 * crypto_ahash_digestsize(auth) +\n\t\t\t    crypto_ahash_alignmask(auth),\n\t\t\t    crypto_ahash_alignmask(auth) + 1) +\n\t\t      crypto_ablkcipher_ivsize(enc);\n\n\ttfm->crt_aead.reqsize = sizeof(struct authenc_request_ctx) +\n\t\t\t\tctx->reqoff +\n\t\t\t\tmax_t(unsigned int,\n\t\t\t\tcrypto_ahash_reqsize(auth) +\n\t\t\t\tsizeof(struct ahash_request),\n\t\t\t\tsizeof(struct skcipher_givcrypt_request) +\n\t\t\t\tcrypto_ablkcipher_reqsize(enc));\n\n\treturn 0;\n\nerr_free_ahash:\n\tcrypto_free_ahash(auth);\n\treturn err;\n}\n\nstatic void crypto_authenc_exit_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_authenc_ctx *ctx = crypto_tfm_ctx(tfm);\n\n\tcrypto_free_ahash(ctx->auth);\n\tcrypto_free_ablkcipher(ctx->enc);\n}\n\nstatic struct crypto_instance *crypto_authenc_alloc(struct rtattr **tb)\n{\n\tstruct crypto_attr_type *algt;\n\tstruct crypto_instance *inst;\n\tstruct hash_alg_common *auth;\n\tstruct crypto_alg *auth_base;\n\tstruct crypto_alg *enc;\n\tstruct authenc_instance_ctx *ctx;\n\tconst char *enc_name;\n\tint err;\n\n\talgt = crypto_get_attr_type(tb);\n\tif (IS_ERR(algt))\n\t\treturn ERR_CAST(algt);\n\n\tif ((algt->type ^ CRYPTO_ALG_TYPE_AEAD) & algt->mask)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tauth = ahash_attr_alg(tb[1], CRYPTO_ALG_TYPE_HASH,\n\t\t\t       CRYPTO_ALG_TYPE_AHASH_MASK);\n\tif (IS_ERR(auth))\n\t\treturn ERR_CAST(auth);\n\n\tauth_base = &auth->base;\n\n\tenc_name = crypto_attr_alg_name(tb[2]);\n\terr = PTR_ERR(enc_name);\n\tif (IS_ERR(enc_name))\n\t\tgoto out_put_auth;\n\n\tinst = kzalloc(sizeof(*inst) + sizeof(*ctx), GFP_KERNEL);\n\terr = -ENOMEM;\n\tif (!inst)\n\t\tgoto out_put_auth;\n\n\tctx = crypto_instance_ctx(inst);\n\n\terr = crypto_init_ahash_spawn(&ctx->auth, auth, inst);\n\tif (err)\n\t\tgoto err_free_inst;\n\n\tcrypto_set_skcipher_spawn(&ctx->enc, inst);\n\terr = crypto_grab_skcipher(&ctx->enc, enc_name, 0,\n\t\t\t\t   crypto_requires_sync(algt->type,\n\t\t\t\t\t\t\talgt->mask));\n\tif (err)\n\t\tgoto err_drop_auth;\n\n\tenc = crypto_skcipher_spawn_alg(&ctx->enc);\n\n\terr = -ENAMETOOLONG;\n\tif (snprintf(inst->alg.cra_name, CRYPTO_MAX_ALG_NAME,\n\t\t     \"authenc(%s,%s)\", auth_base->cra_name, enc->cra_name) >=\n\t    CRYPTO_MAX_ALG_NAME)\n\t\tgoto err_drop_enc;\n\n\tif (snprintf(inst->alg.cra_driver_name, CRYPTO_MAX_ALG_NAME,\n\t\t     \"authenc(%s,%s)\", auth_base->cra_driver_name,\n\t\t     enc->cra_driver_name) >= CRYPTO_MAX_ALG_NAME)\n\t\tgoto err_drop_enc;\n\n\tinst->alg.cra_flags = CRYPTO_ALG_TYPE_AEAD;\n\tinst->alg.cra_flags |= enc->cra_flags & CRYPTO_ALG_ASYNC;\n\tinst->alg.cra_priority = enc->cra_priority *\n\t\t\t\t 10 + auth_base->cra_priority;\n\tinst->alg.cra_blocksize = enc->cra_blocksize;\n\tinst->alg.cra_alignmask = auth_base->cra_alignmask | enc->cra_alignmask;\n\tinst->alg.cra_type = &crypto_aead_type;\n\n\tinst->alg.cra_aead.ivsize = enc->cra_ablkcipher.ivsize;\n\tinst->alg.cra_aead.maxauthsize = auth->digestsize;\n\n\tinst->alg.cra_ctxsize = sizeof(struct crypto_authenc_ctx);\n\n\tinst->alg.cra_init = crypto_authenc_init_tfm;\n\tinst->alg.cra_exit = crypto_authenc_exit_tfm;\n\n\tinst->alg.cra_aead.setkey = crypto_authenc_setkey;\n\tinst->alg.cra_aead.encrypt = crypto_authenc_encrypt;\n\tinst->alg.cra_aead.decrypt = crypto_authenc_decrypt;\n\tinst->alg.cra_aead.givencrypt = crypto_authenc_givencrypt;\n\nout:\n\tcrypto_mod_put(auth_base);\n\treturn inst;\n\nerr_drop_enc:\n\tcrypto_drop_skcipher(&ctx->enc);\nerr_drop_auth:\n\tcrypto_drop_ahash(&ctx->auth);\nerr_free_inst:\n\tkfree(inst);\nout_put_auth:\n\tinst = ERR_PTR(err);\n\tgoto out;\n}\n\nstatic void crypto_authenc_free(struct crypto_instance *inst)\n{\n\tstruct authenc_instance_ctx *ctx = crypto_instance_ctx(inst);\n\n\tcrypto_drop_skcipher(&ctx->enc);\n\tcrypto_drop_ahash(&ctx->auth);\n\tkfree(inst);\n}\n\nstatic struct crypto_template crypto_authenc_tmpl = {\n\t.name = \"authenc\",\n\t.alloc = crypto_authenc_alloc,\n\t.free = crypto_authenc_free,\n\t.module = THIS_MODULE,\n};\n\nstatic int __init crypto_authenc_module_init(void)\n{\n\treturn crypto_register_template(&crypto_authenc_tmpl);\n}\n\nstatic void __exit crypto_authenc_module_exit(void)\n{\n\tcrypto_unregister_template(&crypto_authenc_tmpl);\n}\n\nmodule_init(crypto_authenc_module_init);\nmodule_exit(crypto_authenc_module_exit);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"Simple AEAD wrapper for IPsec\");\n", "/*\n * authencesn.c - AEAD wrapper for IPsec with extended sequence numbers,\n *                 derived from authenc.c\n *\n * Copyright (C) 2010 secunet Security Networks AG\n * Copyright (C) 2010 Steffen Klassert <steffen.klassert@secunet.com>\n *\n * This program is free software; you can redistribute it and/or modify it\n * under the terms of the GNU General Public License as published by the Free\n * Software Foundation; either version 2 of the License, or (at your option)\n * any later version.\n *\n */\n\n#include <crypto/aead.h>\n#include <crypto/internal/hash.h>\n#include <crypto/internal/skcipher.h>\n#include <crypto/authenc.h>\n#include <crypto/scatterwalk.h>\n#include <linux/err.h>\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/rtnetlink.h>\n#include <linux/slab.h>\n#include <linux/spinlock.h>\n\nstruct authenc_esn_instance_ctx {\n\tstruct crypto_ahash_spawn auth;\n\tstruct crypto_skcipher_spawn enc;\n};\n\nstruct crypto_authenc_esn_ctx {\n\tunsigned int reqoff;\n\tstruct crypto_ahash *auth;\n\tstruct crypto_ablkcipher *enc;\n};\n\nstruct authenc_esn_request_ctx {\n\tunsigned int cryptlen;\n\tunsigned int headlen;\n\tunsigned int trailen;\n\tstruct scatterlist *sg;\n\tstruct scatterlist hsg[2];\n\tstruct scatterlist tsg[1];\n\tstruct scatterlist cipher[2];\n\tcrypto_completion_t complete;\n\tcrypto_completion_t update_complete;\n\tcrypto_completion_t update_complete2;\n\tchar tail[];\n};\n\nstatic void authenc_esn_request_complete(struct aead_request *req, int err)\n{\n\tif (err != -EINPROGRESS)\n\t\taead_request_complete(req, err);\n}\n\nstatic int crypto_authenc_esn_setkey(struct crypto_aead *authenc_esn, const u8 *key,\n\t\t\t\t     unsigned int keylen)\n{\n\tstruct crypto_authenc_esn_ctx *ctx = crypto_aead_ctx(authenc_esn);\n\tstruct crypto_ahash *auth = ctx->auth;\n\tstruct crypto_ablkcipher *enc = ctx->enc;\n\tstruct crypto_authenc_keys keys;\n\tint err = -EINVAL;\n\n\tif (crypto_authenc_extractkeys(&keys, key, keylen) != 0)\n\t\tgoto badkey;\n\n\tcrypto_ahash_clear_flags(auth, CRYPTO_TFM_REQ_MASK);\n\tcrypto_ahash_set_flags(auth, crypto_aead_get_flags(authenc_esn) &\n\t\t\t\t     CRYPTO_TFM_REQ_MASK);\n\terr = crypto_ahash_setkey(auth, keys.authkey, keys.authkeylen);\n\tcrypto_aead_set_flags(authenc_esn, crypto_ahash_get_flags(auth) &\n\t\t\t\t\t   CRYPTO_TFM_RES_MASK);\n\n\tif (err)\n\t\tgoto out;\n\n\tcrypto_ablkcipher_clear_flags(enc, CRYPTO_TFM_REQ_MASK);\n\tcrypto_ablkcipher_set_flags(enc, crypto_aead_get_flags(authenc_esn) &\n\t\t\t\t\t CRYPTO_TFM_REQ_MASK);\n\terr = crypto_ablkcipher_setkey(enc, keys.enckey, keys.enckeylen);\n\tcrypto_aead_set_flags(authenc_esn, crypto_ablkcipher_get_flags(enc) &\n\t\t\t\t\t   CRYPTO_TFM_RES_MASK);\n\nout:\n\treturn err;\n\nbadkey:\n\tcrypto_aead_set_flags(authenc_esn, CRYPTO_TFM_RES_BAD_KEY_LEN);\n\tgoto out;\n}\n\nstatic void authenc_esn_geniv_ahash_update_done(struct crypto_async_request *areq,\n\t\t\t\t\t\tint err)\n{\n\tstruct aead_request *req = areq->data;\n\tstruct crypto_aead *authenc_esn = crypto_aead_reqtfm(req);\n\tstruct crypto_authenc_esn_ctx *ctx = crypto_aead_ctx(authenc_esn);\n\tstruct authenc_esn_request_ctx *areq_ctx = aead_request_ctx(req);\n\tstruct ahash_request *ahreq = (void *)(areq_ctx->tail + ctx->reqoff);\n\n\tif (err)\n\t\tgoto out;\n\n\tahash_request_set_crypt(ahreq, areq_ctx->sg, ahreq->result,\n\t\t\t\tareq_ctx->cryptlen);\n\tahash_request_set_callback(ahreq, aead_request_flags(req) &\n\t\t\t\t\t  CRYPTO_TFM_REQ_MAY_SLEEP,\n\t\t\t\t   areq_ctx->update_complete2, req);\n\n\terr = crypto_ahash_update(ahreq);\n\tif (err)\n\t\tgoto out;\n\n\tahash_request_set_crypt(ahreq, areq_ctx->tsg, ahreq->result,\n\t\t\t\tareq_ctx->trailen);\n\tahash_request_set_callback(ahreq, aead_request_flags(req) &\n\t\t\t\t\t  CRYPTO_TFM_REQ_MAY_SLEEP,\n\t\t\t\t   areq_ctx->complete, req);\n\n\terr = crypto_ahash_finup(ahreq);\n\tif (err)\n\t\tgoto out;\n\n\tscatterwalk_map_and_copy(ahreq->result, areq_ctx->sg,\n\t\t\t\t areq_ctx->cryptlen,\n\t\t\t\t crypto_aead_authsize(authenc_esn), 1);\n\nout:\n\tauthenc_esn_request_complete(req, err);\n}\n\nstatic void authenc_esn_geniv_ahash_update_done2(struct crypto_async_request *areq,\n\t\t\t\t\t\t int err)\n{\n\tstruct aead_request *req = areq->data;\n\tstruct crypto_aead *authenc_esn = crypto_aead_reqtfm(req);\n\tstruct crypto_authenc_esn_ctx *ctx = crypto_aead_ctx(authenc_esn);\n\tstruct authenc_esn_request_ctx *areq_ctx = aead_request_ctx(req);\n\tstruct ahash_request *ahreq = (void *)(areq_ctx->tail + ctx->reqoff);\n\n\tif (err)\n\t\tgoto out;\n\n\tahash_request_set_crypt(ahreq, areq_ctx->tsg, ahreq->result,\n\t\t\t\tareq_ctx->trailen);\n\tahash_request_set_callback(ahreq, aead_request_flags(req) &\n\t\t\t\t\t  CRYPTO_TFM_REQ_MAY_SLEEP,\n\t\t\t\t   areq_ctx->complete, req);\n\n\terr = crypto_ahash_finup(ahreq);\n\tif (err)\n\t\tgoto out;\n\n\tscatterwalk_map_and_copy(ahreq->result, areq_ctx->sg,\n\t\t\t\t areq_ctx->cryptlen,\n\t\t\t\t crypto_aead_authsize(authenc_esn), 1);\n\nout:\n\tauthenc_esn_request_complete(req, err);\n}\n\n\nstatic void authenc_esn_geniv_ahash_done(struct crypto_async_request *areq,\n\t\t\t\t\t int err)\n{\n\tstruct aead_request *req = areq->data;\n\tstruct crypto_aead *authenc_esn = crypto_aead_reqtfm(req);\n\tstruct crypto_authenc_esn_ctx *ctx = crypto_aead_ctx(authenc_esn);\n\tstruct authenc_esn_request_ctx *areq_ctx = aead_request_ctx(req);\n\tstruct ahash_request *ahreq = (void *)(areq_ctx->tail + ctx->reqoff);\n\n\tif (err)\n\t\tgoto out;\n\n\tscatterwalk_map_and_copy(ahreq->result, areq_ctx->sg,\n\t\t\t\t areq_ctx->cryptlen,\n\t\t\t\t crypto_aead_authsize(authenc_esn), 1);\n\nout:\n\taead_request_complete(req, err);\n}\n\n\nstatic void authenc_esn_verify_ahash_update_done(struct crypto_async_request *areq,\n\t\t\t\t\t\t int err)\n{\n\tu8 *ihash;\n\tunsigned int authsize;\n\tstruct ablkcipher_request *abreq;\n\tstruct aead_request *req = areq->data;\n\tstruct crypto_aead *authenc_esn = crypto_aead_reqtfm(req);\n\tstruct crypto_authenc_esn_ctx *ctx = crypto_aead_ctx(authenc_esn);\n\tstruct authenc_esn_request_ctx *areq_ctx = aead_request_ctx(req);\n\tstruct ahash_request *ahreq = (void *)(areq_ctx->tail + ctx->reqoff);\n\tunsigned int cryptlen = req->cryptlen;\n\n\tif (err)\n\t\tgoto out;\n\n\tahash_request_set_crypt(ahreq, areq_ctx->sg, ahreq->result,\n\t\t\t\tareq_ctx->cryptlen);\n\n\tahash_request_set_callback(ahreq,\n\t\t\t\t   aead_request_flags(req) &\n\t\t\t\t   CRYPTO_TFM_REQ_MAY_SLEEP,\n\t\t\t\t   areq_ctx->update_complete2, req);\n\n\terr = crypto_ahash_update(ahreq);\n\tif (err)\n\t\tgoto out;\n\n\tahash_request_set_crypt(ahreq, areq_ctx->tsg, ahreq->result,\n\t\t\t\tareq_ctx->trailen);\n\tahash_request_set_callback(ahreq, aead_request_flags(req) &\n\t\t\t\t\t  CRYPTO_TFM_REQ_MAY_SLEEP,\n\t\t\t\t   areq_ctx->complete, req);\n\n\terr = crypto_ahash_finup(ahreq);\n\tif (err)\n\t\tgoto out;\n\n\tauthsize = crypto_aead_authsize(authenc_esn);\n\tcryptlen -= authsize;\n\tihash = ahreq->result + authsize;\n\tscatterwalk_map_and_copy(ihash, areq_ctx->sg, areq_ctx->cryptlen,\n\t\t\t\t authsize, 0);\n\n\terr = crypto_memneq(ihash, ahreq->result, authsize) ? -EBADMSG : 0;\n\tif (err)\n\t\tgoto out;\n\n\tabreq = aead_request_ctx(req);\n\tablkcipher_request_set_tfm(abreq, ctx->enc);\n\tablkcipher_request_set_callback(abreq, aead_request_flags(req),\n\t\t\t\t\treq->base.complete, req->base.data);\n\tablkcipher_request_set_crypt(abreq, req->src, req->dst,\n\t\t\t\t     cryptlen, req->iv);\n\n\terr = crypto_ablkcipher_decrypt(abreq);\n\nout:\n\tauthenc_esn_request_complete(req, err);\n}\n\nstatic void authenc_esn_verify_ahash_update_done2(struct crypto_async_request *areq,\n\t\t\t\t\t\t  int err)\n{\n\tu8 *ihash;\n\tunsigned int authsize;\n\tstruct ablkcipher_request *abreq;\n\tstruct aead_request *req = areq->data;\n\tstruct crypto_aead *authenc_esn = crypto_aead_reqtfm(req);\n\tstruct crypto_authenc_esn_ctx *ctx = crypto_aead_ctx(authenc_esn);\n\tstruct authenc_esn_request_ctx *areq_ctx = aead_request_ctx(req);\n\tstruct ahash_request *ahreq = (void *)(areq_ctx->tail + ctx->reqoff);\n\tunsigned int cryptlen = req->cryptlen;\n\n\tif (err)\n\t\tgoto out;\n\n\tahash_request_set_crypt(ahreq, areq_ctx->tsg, ahreq->result,\n\t\t\t\tareq_ctx->trailen);\n\tahash_request_set_callback(ahreq, aead_request_flags(req) &\n\t\t\t\t\t  CRYPTO_TFM_REQ_MAY_SLEEP,\n\t\t\t\t   areq_ctx->complete, req);\n\n\terr = crypto_ahash_finup(ahreq);\n\tif (err)\n\t\tgoto out;\n\n\tauthsize = crypto_aead_authsize(authenc_esn);\n\tcryptlen -= authsize;\n\tihash = ahreq->result + authsize;\n\tscatterwalk_map_and_copy(ihash, areq_ctx->sg, areq_ctx->cryptlen,\n\t\t\t\t authsize, 0);\n\n\terr = crypto_memneq(ihash, ahreq->result, authsize) ? -EBADMSG : 0;\n\tif (err)\n\t\tgoto out;\n\n\tabreq = aead_request_ctx(req);\n\tablkcipher_request_set_tfm(abreq, ctx->enc);\n\tablkcipher_request_set_callback(abreq, aead_request_flags(req),\n\t\t\t\t\treq->base.complete, req->base.data);\n\tablkcipher_request_set_crypt(abreq, req->src, req->dst,\n\t\t\t\t     cryptlen, req->iv);\n\n\terr = crypto_ablkcipher_decrypt(abreq);\n\nout:\n\tauthenc_esn_request_complete(req, err);\n}\n\n\nstatic void authenc_esn_verify_ahash_done(struct crypto_async_request *areq,\n\t\t\t\t\t  int err)\n{\n\tu8 *ihash;\n\tunsigned int authsize;\n\tstruct ablkcipher_request *abreq;\n\tstruct aead_request *req = areq->data;\n\tstruct crypto_aead *authenc_esn = crypto_aead_reqtfm(req);\n\tstruct crypto_authenc_esn_ctx *ctx = crypto_aead_ctx(authenc_esn);\n\tstruct authenc_esn_request_ctx *areq_ctx = aead_request_ctx(req);\n\tstruct ahash_request *ahreq = (void *)(areq_ctx->tail + ctx->reqoff);\n\tunsigned int cryptlen = req->cryptlen;\n\n\tif (err)\n\t\tgoto out;\n\n\tauthsize = crypto_aead_authsize(authenc_esn);\n\tcryptlen -= authsize;\n\tihash = ahreq->result + authsize;\n\tscatterwalk_map_and_copy(ihash, areq_ctx->sg, areq_ctx->cryptlen,\n\t\t\t\t authsize, 0);\n\n\terr = crypto_memneq(ihash, ahreq->result, authsize) ? -EBADMSG : 0;\n\tif (err)\n\t\tgoto out;\n\n\tabreq = aead_request_ctx(req);\n\tablkcipher_request_set_tfm(abreq, ctx->enc);\n\tablkcipher_request_set_callback(abreq, aead_request_flags(req),\n\t\t\t\t\treq->base.complete, req->base.data);\n\tablkcipher_request_set_crypt(abreq, req->src, req->dst,\n\t\t\t\t     cryptlen, req->iv);\n\n\terr = crypto_ablkcipher_decrypt(abreq);\n\nout:\n\tauthenc_esn_request_complete(req, err);\n}\n\nstatic u8 *crypto_authenc_esn_ahash(struct aead_request *req,\n\t\t\t\t    unsigned int flags)\n{\n\tstruct crypto_aead *authenc_esn = crypto_aead_reqtfm(req);\n\tstruct crypto_authenc_esn_ctx *ctx = crypto_aead_ctx(authenc_esn);\n\tstruct crypto_ahash *auth = ctx->auth;\n\tstruct authenc_esn_request_ctx *areq_ctx = aead_request_ctx(req);\n\tstruct ahash_request *ahreq = (void *)(areq_ctx->tail + ctx->reqoff);\n\tu8 *hash = areq_ctx->tail;\n\tint err;\n\n\thash = (u8 *)ALIGN((unsigned long)hash + crypto_ahash_alignmask(auth),\n\t\t\t    crypto_ahash_alignmask(auth) + 1);\n\n\tahash_request_set_tfm(ahreq, auth);\n\n\terr = crypto_ahash_init(ahreq);\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\tahash_request_set_crypt(ahreq, areq_ctx->hsg, hash, areq_ctx->headlen);\n\tahash_request_set_callback(ahreq, aead_request_flags(req) & flags,\n\t\t\t\t   areq_ctx->update_complete, req);\n\n\terr = crypto_ahash_update(ahreq);\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\tahash_request_set_crypt(ahreq, areq_ctx->sg, hash, areq_ctx->cryptlen);\n\tahash_request_set_callback(ahreq, aead_request_flags(req) & flags,\n\t\t\t\t   areq_ctx->update_complete2, req);\n\n\terr = crypto_ahash_update(ahreq);\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\tahash_request_set_crypt(ahreq, areq_ctx->tsg, hash,\n\t\t\t\tareq_ctx->trailen);\n\tahash_request_set_callback(ahreq, aead_request_flags(req) & flags,\n\t\t\t\t   areq_ctx->complete, req);\n\n\terr = crypto_ahash_finup(ahreq);\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\treturn hash;\n}\n\nstatic int crypto_authenc_esn_genicv(struct aead_request *req, u8 *iv,\n\t\t\t\t     unsigned int flags)\n{\n\tstruct crypto_aead *authenc_esn = crypto_aead_reqtfm(req);\n\tstruct authenc_esn_request_ctx *areq_ctx = aead_request_ctx(req);\n\tstruct scatterlist *dst = req->dst;\n\tstruct scatterlist *assoc = req->assoc;\n\tstruct scatterlist *cipher = areq_ctx->cipher;\n\tstruct scatterlist *hsg = areq_ctx->hsg;\n\tstruct scatterlist *tsg = areq_ctx->tsg;\n\tstruct scatterlist *assoc1;\n\tstruct scatterlist *assoc2;\n\tunsigned int ivsize = crypto_aead_ivsize(authenc_esn);\n\tunsigned int cryptlen = req->cryptlen;\n\tstruct page *dstp;\n\tu8 *vdst;\n\tu8 *hash;\n\n\tdstp = sg_page(dst);\n\tvdst = PageHighMem(dstp) ? NULL : page_address(dstp) + dst->offset;\n\n\tif (ivsize) {\n\t\tsg_init_table(cipher, 2);\n\t\tsg_set_buf(cipher, iv, ivsize);\n\t\tscatterwalk_crypto_chain(cipher, dst, vdst == iv + ivsize, 2);\n\t\tdst = cipher;\n\t\tcryptlen += ivsize;\n\t}\n\n\tif (sg_is_last(assoc))\n\t\treturn -EINVAL;\n\n\tassoc1 = assoc + 1;\n\tif (sg_is_last(assoc1))\n\t\treturn -EINVAL;\n\n\tassoc2 = assoc + 2;\n\tif (!sg_is_last(assoc2))\n\t\treturn -EINVAL;\n\n\tsg_init_table(hsg, 2);\n\tsg_set_page(hsg, sg_page(assoc), assoc->length, assoc->offset);\n\tsg_set_page(hsg + 1, sg_page(assoc2), assoc2->length, assoc2->offset);\n\n\tsg_init_table(tsg, 1);\n\tsg_set_page(tsg, sg_page(assoc1), assoc1->length, assoc1->offset);\n\n\tareq_ctx->cryptlen = cryptlen;\n\tareq_ctx->headlen = assoc->length + assoc2->length;\n\tareq_ctx->trailen = assoc1->length;\n\tareq_ctx->sg = dst;\n\n\tareq_ctx->complete = authenc_esn_geniv_ahash_done;\n\tareq_ctx->update_complete = authenc_esn_geniv_ahash_update_done;\n\tareq_ctx->update_complete2 = authenc_esn_geniv_ahash_update_done2;\n\n\thash = crypto_authenc_esn_ahash(req, flags);\n\tif (IS_ERR(hash))\n\t\treturn PTR_ERR(hash);\n\n\tscatterwalk_map_and_copy(hash, dst, cryptlen,\n\t\t\t\t crypto_aead_authsize(authenc_esn), 1);\n\treturn 0;\n}\n\n\nstatic void crypto_authenc_esn_encrypt_done(struct crypto_async_request *req,\n\t\t\t\t\t    int err)\n{\n\tstruct aead_request *areq = req->data;\n\n\tif (!err) {\n\t\tstruct crypto_aead *authenc_esn = crypto_aead_reqtfm(areq);\n\t\tstruct crypto_authenc_esn_ctx *ctx = crypto_aead_ctx(authenc_esn);\n\t\tstruct ablkcipher_request *abreq = aead_request_ctx(areq);\n\t\tu8 *iv = (u8 *)(abreq + 1) +\n\t\t\t crypto_ablkcipher_reqsize(ctx->enc);\n\n\t\terr = crypto_authenc_esn_genicv(areq, iv, 0);\n\t}\n\n\tauthenc_esn_request_complete(areq, err);\n}\n\nstatic int crypto_authenc_esn_encrypt(struct aead_request *req)\n{\n\tstruct crypto_aead *authenc_esn = crypto_aead_reqtfm(req);\n\tstruct crypto_authenc_esn_ctx *ctx = crypto_aead_ctx(authenc_esn);\n\tstruct authenc_esn_request_ctx *areq_ctx = aead_request_ctx(req);\n\tstruct crypto_ablkcipher *enc = ctx->enc;\n\tstruct scatterlist *dst = req->dst;\n\tunsigned int cryptlen = req->cryptlen;\n\tstruct ablkcipher_request *abreq = (void *)(areq_ctx->tail\n\t\t\t\t\t\t    + ctx->reqoff);\n\tu8 *iv = (u8 *)abreq - crypto_ablkcipher_ivsize(enc);\n\tint err;\n\n\tablkcipher_request_set_tfm(abreq, enc);\n\tablkcipher_request_set_callback(abreq, aead_request_flags(req),\n\t\t\t\t\tcrypto_authenc_esn_encrypt_done, req);\n\tablkcipher_request_set_crypt(abreq, req->src, dst, cryptlen, req->iv);\n\n\tmemcpy(iv, req->iv, crypto_aead_ivsize(authenc_esn));\n\n\terr = crypto_ablkcipher_encrypt(abreq);\n\tif (err)\n\t\treturn err;\n\n\treturn crypto_authenc_esn_genicv(req, iv, CRYPTO_TFM_REQ_MAY_SLEEP);\n}\n\nstatic void crypto_authenc_esn_givencrypt_done(struct crypto_async_request *req,\n\t\t\t\t\t       int err)\n{\n\tstruct aead_request *areq = req->data;\n\n\tif (!err) {\n\t\tstruct skcipher_givcrypt_request *greq = aead_request_ctx(areq);\n\n\t\terr = crypto_authenc_esn_genicv(areq, greq->giv, 0);\n\t}\n\n\tauthenc_esn_request_complete(areq, err);\n}\n\nstatic int crypto_authenc_esn_givencrypt(struct aead_givcrypt_request *req)\n{\n\tstruct crypto_aead *authenc_esn = aead_givcrypt_reqtfm(req);\n\tstruct crypto_authenc_esn_ctx *ctx = crypto_aead_ctx(authenc_esn);\n\tstruct aead_request *areq = &req->areq;\n\tstruct skcipher_givcrypt_request *greq = aead_request_ctx(areq);\n\tu8 *iv = req->giv;\n\tint err;\n\n\tskcipher_givcrypt_set_tfm(greq, ctx->enc);\n\tskcipher_givcrypt_set_callback(greq, aead_request_flags(areq),\n\t\t\t\t       crypto_authenc_esn_givencrypt_done, areq);\n\tskcipher_givcrypt_set_crypt(greq, areq->src, areq->dst, areq->cryptlen,\n\t\t\t\t    areq->iv);\n\tskcipher_givcrypt_set_giv(greq, iv, req->seq);\n\n\terr = crypto_skcipher_givencrypt(greq);\n\tif (err)\n\t\treturn err;\n\n\treturn crypto_authenc_esn_genicv(areq, iv, CRYPTO_TFM_REQ_MAY_SLEEP);\n}\n\nstatic int crypto_authenc_esn_verify(struct aead_request *req)\n{\n\tstruct crypto_aead *authenc_esn = crypto_aead_reqtfm(req);\n\tstruct authenc_esn_request_ctx *areq_ctx = aead_request_ctx(req);\n\tu8 *ohash;\n\tu8 *ihash;\n\tunsigned int authsize;\n\n\tareq_ctx->complete = authenc_esn_verify_ahash_done;\n\tareq_ctx->update_complete = authenc_esn_verify_ahash_update_done;\n\n\tohash = crypto_authenc_esn_ahash(req, CRYPTO_TFM_REQ_MAY_SLEEP);\n\tif (IS_ERR(ohash))\n\t\treturn PTR_ERR(ohash);\n\n\tauthsize = crypto_aead_authsize(authenc_esn);\n\tihash = ohash + authsize;\n\tscatterwalk_map_and_copy(ihash, areq_ctx->sg, areq_ctx->cryptlen,\n\t\t\t\t authsize, 0);\n\treturn crypto_memneq(ihash, ohash, authsize) ? -EBADMSG : 0;\n}\n\nstatic int crypto_authenc_esn_iverify(struct aead_request *req, u8 *iv,\n\t\t\t\t      unsigned int cryptlen)\n{\n\tstruct crypto_aead *authenc_esn = crypto_aead_reqtfm(req);\n\tstruct authenc_esn_request_ctx *areq_ctx = aead_request_ctx(req);\n\tstruct scatterlist *src = req->src;\n\tstruct scatterlist *assoc = req->assoc;\n\tstruct scatterlist *cipher = areq_ctx->cipher;\n\tstruct scatterlist *hsg = areq_ctx->hsg;\n\tstruct scatterlist *tsg = areq_ctx->tsg;\n\tstruct scatterlist *assoc1;\n\tstruct scatterlist *assoc2;\n\tunsigned int ivsize = crypto_aead_ivsize(authenc_esn);\n\tstruct page *srcp;\n\tu8 *vsrc;\n\n\tsrcp = sg_page(src);\n\tvsrc = PageHighMem(srcp) ? NULL : page_address(srcp) + src->offset;\n\n\tif (ivsize) {\n\t\tsg_init_table(cipher, 2);\n\t\tsg_set_buf(cipher, iv, ivsize);\n\t\tscatterwalk_crypto_chain(cipher, src, vsrc == iv + ivsize, 2);\n\t\tsrc = cipher;\n\t\tcryptlen += ivsize;\n\t}\n\n\tif (sg_is_last(assoc))\n\t\treturn -EINVAL;\n\n\tassoc1 = assoc + 1;\n\tif (sg_is_last(assoc1))\n\t\treturn -EINVAL;\n\n\tassoc2 = assoc + 2;\n\tif (!sg_is_last(assoc2))\n\t\treturn -EINVAL;\n\n\tsg_init_table(hsg, 2);\n\tsg_set_page(hsg, sg_page(assoc), assoc->length, assoc->offset);\n\tsg_set_page(hsg + 1, sg_page(assoc2), assoc2->length, assoc2->offset);\n\n\tsg_init_table(tsg, 1);\n\tsg_set_page(tsg, sg_page(assoc1), assoc1->length, assoc1->offset);\n\n\tareq_ctx->cryptlen = cryptlen;\n\tareq_ctx->headlen = assoc->length + assoc2->length;\n\tareq_ctx->trailen = assoc1->length;\n\tareq_ctx->sg = src;\n\n\tareq_ctx->complete = authenc_esn_verify_ahash_done;\n\tareq_ctx->update_complete = authenc_esn_verify_ahash_update_done;\n\tareq_ctx->update_complete2 = authenc_esn_verify_ahash_update_done2;\n\n\treturn crypto_authenc_esn_verify(req);\n}\n\nstatic int crypto_authenc_esn_decrypt(struct aead_request *req)\n{\n\tstruct crypto_aead *authenc_esn = crypto_aead_reqtfm(req);\n\tstruct crypto_authenc_esn_ctx *ctx = crypto_aead_ctx(authenc_esn);\n\tstruct ablkcipher_request *abreq = aead_request_ctx(req);\n\tunsigned int cryptlen = req->cryptlen;\n\tunsigned int authsize = crypto_aead_authsize(authenc_esn);\n\tu8 *iv = req->iv;\n\tint err;\n\n\tif (cryptlen < authsize)\n\t\treturn -EINVAL;\n\tcryptlen -= authsize;\n\n\terr = crypto_authenc_esn_iverify(req, iv, cryptlen);\n\tif (err)\n\t\treturn err;\n\n\tablkcipher_request_set_tfm(abreq, ctx->enc);\n\tablkcipher_request_set_callback(abreq, aead_request_flags(req),\n\t\t\t\t\treq->base.complete, req->base.data);\n\tablkcipher_request_set_crypt(abreq, req->src, req->dst, cryptlen, iv);\n\n\treturn crypto_ablkcipher_decrypt(abreq);\n}\n\nstatic int crypto_authenc_esn_init_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_instance *inst = crypto_tfm_alg_instance(tfm);\n\tstruct authenc_esn_instance_ctx *ictx = crypto_instance_ctx(inst);\n\tstruct crypto_authenc_esn_ctx *ctx = crypto_tfm_ctx(tfm);\n\tstruct crypto_ahash *auth;\n\tstruct crypto_ablkcipher *enc;\n\tint err;\n\n\tauth = crypto_spawn_ahash(&ictx->auth);\n\tif (IS_ERR(auth))\n\t\treturn PTR_ERR(auth);\n\n\tenc = crypto_spawn_skcipher(&ictx->enc);\n\terr = PTR_ERR(enc);\n\tif (IS_ERR(enc))\n\t\tgoto err_free_ahash;\n\n\tctx->auth = auth;\n\tctx->enc = enc;\n\n\tctx->reqoff = ALIGN(2 * crypto_ahash_digestsize(auth) +\n\t\t\t    crypto_ahash_alignmask(auth),\n\t\t\t    crypto_ahash_alignmask(auth) + 1) +\n\t\t      crypto_ablkcipher_ivsize(enc);\n\n\ttfm->crt_aead.reqsize = sizeof(struct authenc_esn_request_ctx) +\n\t\t\t\tctx->reqoff +\n\t\t\t\tmax_t(unsigned int,\n\t\t\t\tcrypto_ahash_reqsize(auth) +\n\t\t\t\tsizeof(struct ahash_request),\n\t\t\t\tsizeof(struct skcipher_givcrypt_request) +\n\t\t\t\tcrypto_ablkcipher_reqsize(enc));\n\n\treturn 0;\n\nerr_free_ahash:\n\tcrypto_free_ahash(auth);\n\treturn err;\n}\n\nstatic void crypto_authenc_esn_exit_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_authenc_esn_ctx *ctx = crypto_tfm_ctx(tfm);\n\n\tcrypto_free_ahash(ctx->auth);\n\tcrypto_free_ablkcipher(ctx->enc);\n}\n\nstatic struct crypto_instance *crypto_authenc_esn_alloc(struct rtattr **tb)\n{\n\tstruct crypto_attr_type *algt;\n\tstruct crypto_instance *inst;\n\tstruct hash_alg_common *auth;\n\tstruct crypto_alg *auth_base;\n\tstruct crypto_alg *enc;\n\tstruct authenc_esn_instance_ctx *ctx;\n\tconst char *enc_name;\n\tint err;\n\n\talgt = crypto_get_attr_type(tb);\n\tif (IS_ERR(algt))\n\t\treturn ERR_CAST(algt);\n\n\tif ((algt->type ^ CRYPTO_ALG_TYPE_AEAD) & algt->mask)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tauth = ahash_attr_alg(tb[1], CRYPTO_ALG_TYPE_HASH,\n\t\t\t       CRYPTO_ALG_TYPE_AHASH_MASK);\n\tif (IS_ERR(auth))\n\t\treturn ERR_CAST(auth);\n\n\tauth_base = &auth->base;\n\n\tenc_name = crypto_attr_alg_name(tb[2]);\n\terr = PTR_ERR(enc_name);\n\tif (IS_ERR(enc_name))\n\t\tgoto out_put_auth;\n\n\tinst = kzalloc(sizeof(*inst) + sizeof(*ctx), GFP_KERNEL);\n\terr = -ENOMEM;\n\tif (!inst)\n\t\tgoto out_put_auth;\n\n\tctx = crypto_instance_ctx(inst);\n\n\terr = crypto_init_ahash_spawn(&ctx->auth, auth, inst);\n\tif (err)\n\t\tgoto err_free_inst;\n\n\tcrypto_set_skcipher_spawn(&ctx->enc, inst);\n\terr = crypto_grab_skcipher(&ctx->enc, enc_name, 0,\n\t\t\t\t   crypto_requires_sync(algt->type,\n\t\t\t\t\t\t\talgt->mask));\n\tif (err)\n\t\tgoto err_drop_auth;\n\n\tenc = crypto_skcipher_spawn_alg(&ctx->enc);\n\n\terr = -ENAMETOOLONG;\n\tif (snprintf(inst->alg.cra_name, CRYPTO_MAX_ALG_NAME,\n\t\t     \"authencesn(%s,%s)\", auth_base->cra_name, enc->cra_name) >=\n\t    CRYPTO_MAX_ALG_NAME)\n\t\tgoto err_drop_enc;\n\n\tif (snprintf(inst->alg.cra_driver_name, CRYPTO_MAX_ALG_NAME,\n\t\t     \"authencesn(%s,%s)\", auth_base->cra_driver_name,\n\t\t     enc->cra_driver_name) >= CRYPTO_MAX_ALG_NAME)\n\t\tgoto err_drop_enc;\n\n\tinst->alg.cra_flags = CRYPTO_ALG_TYPE_AEAD;\n\tinst->alg.cra_flags |= enc->cra_flags & CRYPTO_ALG_ASYNC;\n\tinst->alg.cra_priority = enc->cra_priority *\n\t\t\t\t 10 + auth_base->cra_priority;\n\tinst->alg.cra_blocksize = enc->cra_blocksize;\n\tinst->alg.cra_alignmask = auth_base->cra_alignmask | enc->cra_alignmask;\n\tinst->alg.cra_type = &crypto_aead_type;\n\n\tinst->alg.cra_aead.ivsize = enc->cra_ablkcipher.ivsize;\n\tinst->alg.cra_aead.maxauthsize = auth->digestsize;\n\n\tinst->alg.cra_ctxsize = sizeof(struct crypto_authenc_esn_ctx);\n\n\tinst->alg.cra_init = crypto_authenc_esn_init_tfm;\n\tinst->alg.cra_exit = crypto_authenc_esn_exit_tfm;\n\n\tinst->alg.cra_aead.setkey = crypto_authenc_esn_setkey;\n\tinst->alg.cra_aead.encrypt = crypto_authenc_esn_encrypt;\n\tinst->alg.cra_aead.decrypt = crypto_authenc_esn_decrypt;\n\tinst->alg.cra_aead.givencrypt = crypto_authenc_esn_givencrypt;\n\nout:\n\tcrypto_mod_put(auth_base);\n\treturn inst;\n\nerr_drop_enc:\n\tcrypto_drop_skcipher(&ctx->enc);\nerr_drop_auth:\n\tcrypto_drop_ahash(&ctx->auth);\nerr_free_inst:\n\tkfree(inst);\nout_put_auth:\n\tinst = ERR_PTR(err);\n\tgoto out;\n}\n\nstatic void crypto_authenc_esn_free(struct crypto_instance *inst)\n{\n\tstruct authenc_esn_instance_ctx *ctx = crypto_instance_ctx(inst);\n\n\tcrypto_drop_skcipher(&ctx->enc);\n\tcrypto_drop_ahash(&ctx->auth);\n\tkfree(inst);\n}\n\nstatic struct crypto_template crypto_authenc_esn_tmpl = {\n\t.name = \"authencesn\",\n\t.alloc = crypto_authenc_esn_alloc,\n\t.free = crypto_authenc_esn_free,\n\t.module = THIS_MODULE,\n};\n\nstatic int __init crypto_authenc_esn_module_init(void)\n{\n\treturn crypto_register_template(&crypto_authenc_esn_tmpl);\n}\n\nstatic void __exit crypto_authenc_esn_module_exit(void)\n{\n\tcrypto_unregister_template(&crypto_authenc_esn_tmpl);\n}\n\nmodule_init(crypto_authenc_esn_module_init);\nmodule_exit(crypto_authenc_esn_module_exit);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_AUTHOR(\"Steffen Klassert <steffen.klassert@secunet.com>\");\nMODULE_DESCRIPTION(\"AEAD wrapper for IPsec with extended sequence numbers\");\n", "/*\n * CBC: Cipher Block Chaining mode\n *\n * Copyright (c) 2006 Herbert Xu <herbert@gondor.apana.org.au>\n *\n * This program is free software; you can redistribute it and/or modify it\n * under the terms of the GNU General Public License as published by the Free\n * Software Foundation; either version 2 of the License, or (at your option)\n * any later version.\n *\n */\n\n#include <crypto/algapi.h>\n#include <linux/err.h>\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/log2.h>\n#include <linux/module.h>\n#include <linux/scatterlist.h>\n#include <linux/slab.h>\n\nstruct crypto_cbc_ctx {\n\tstruct crypto_cipher *child;\n};\n\nstatic int crypto_cbc_setkey(struct crypto_tfm *parent, const u8 *key,\n\t\t\t     unsigned int keylen)\n{\n\tstruct crypto_cbc_ctx *ctx = crypto_tfm_ctx(parent);\n\tstruct crypto_cipher *child = ctx->child;\n\tint err;\n\n\tcrypto_cipher_clear_flags(child, CRYPTO_TFM_REQ_MASK);\n\tcrypto_cipher_set_flags(child, crypto_tfm_get_flags(parent) &\n\t\t\t\t       CRYPTO_TFM_REQ_MASK);\n\terr = crypto_cipher_setkey(child, key, keylen);\n\tcrypto_tfm_set_flags(parent, crypto_cipher_get_flags(child) &\n\t\t\t\t     CRYPTO_TFM_RES_MASK);\n\treturn err;\n}\n\nstatic int crypto_cbc_encrypt_segment(struct blkcipher_desc *desc,\n\t\t\t\t      struct blkcipher_walk *walk,\n\t\t\t\t      struct crypto_cipher *tfm)\n{\n\tvoid (*fn)(struct crypto_tfm *, u8 *, const u8 *) =\n\t\tcrypto_cipher_alg(tfm)->cia_encrypt;\n\tint bsize = crypto_cipher_blocksize(tfm);\n\tunsigned int nbytes = walk->nbytes;\n\tu8 *src = walk->src.virt.addr;\n\tu8 *dst = walk->dst.virt.addr;\n\tu8 *iv = walk->iv;\n\n\tdo {\n\t\tcrypto_xor(iv, src, bsize);\n\t\tfn(crypto_cipher_tfm(tfm), dst, iv);\n\t\tmemcpy(iv, dst, bsize);\n\n\t\tsrc += bsize;\n\t\tdst += bsize;\n\t} while ((nbytes -= bsize) >= bsize);\n\n\treturn nbytes;\n}\n\nstatic int crypto_cbc_encrypt_inplace(struct blkcipher_desc *desc,\n\t\t\t\t      struct blkcipher_walk *walk,\n\t\t\t\t      struct crypto_cipher *tfm)\n{\n\tvoid (*fn)(struct crypto_tfm *, u8 *, const u8 *) =\n\t\tcrypto_cipher_alg(tfm)->cia_encrypt;\n\tint bsize = crypto_cipher_blocksize(tfm);\n\tunsigned int nbytes = walk->nbytes;\n\tu8 *src = walk->src.virt.addr;\n\tu8 *iv = walk->iv;\n\n\tdo {\n\t\tcrypto_xor(src, iv, bsize);\n\t\tfn(crypto_cipher_tfm(tfm), src, src);\n\t\tiv = src;\n\n\t\tsrc += bsize;\n\t} while ((nbytes -= bsize) >= bsize);\n\n\tmemcpy(walk->iv, iv, bsize);\n\n\treturn nbytes;\n}\n\nstatic int crypto_cbc_encrypt(struct blkcipher_desc *desc,\n\t\t\t      struct scatterlist *dst, struct scatterlist *src,\n\t\t\t      unsigned int nbytes)\n{\n\tstruct blkcipher_walk walk;\n\tstruct crypto_blkcipher *tfm = desc->tfm;\n\tstruct crypto_cbc_ctx *ctx = crypto_blkcipher_ctx(tfm);\n\tstruct crypto_cipher *child = ctx->child;\n\tint err;\n\n\tblkcipher_walk_init(&walk, dst, src, nbytes);\n\terr = blkcipher_walk_virt(desc, &walk);\n\n\twhile ((nbytes = walk.nbytes)) {\n\t\tif (walk.src.virt.addr == walk.dst.virt.addr)\n\t\t\tnbytes = crypto_cbc_encrypt_inplace(desc, &walk, child);\n\t\telse\n\t\t\tnbytes = crypto_cbc_encrypt_segment(desc, &walk, child);\n\t\terr = blkcipher_walk_done(desc, &walk, nbytes);\n\t}\n\n\treturn err;\n}\n\nstatic int crypto_cbc_decrypt_segment(struct blkcipher_desc *desc,\n\t\t\t\t      struct blkcipher_walk *walk,\n\t\t\t\t      struct crypto_cipher *tfm)\n{\n\tvoid (*fn)(struct crypto_tfm *, u8 *, const u8 *) =\n\t\tcrypto_cipher_alg(tfm)->cia_decrypt;\n\tint bsize = crypto_cipher_blocksize(tfm);\n\tunsigned int nbytes = walk->nbytes;\n\tu8 *src = walk->src.virt.addr;\n\tu8 *dst = walk->dst.virt.addr;\n\tu8 *iv = walk->iv;\n\n\tdo {\n\t\tfn(crypto_cipher_tfm(tfm), dst, src);\n\t\tcrypto_xor(dst, iv, bsize);\n\t\tiv = src;\n\n\t\tsrc += bsize;\n\t\tdst += bsize;\n\t} while ((nbytes -= bsize) >= bsize);\n\n\tmemcpy(walk->iv, iv, bsize);\n\n\treturn nbytes;\n}\n\nstatic int crypto_cbc_decrypt_inplace(struct blkcipher_desc *desc,\n\t\t\t\t      struct blkcipher_walk *walk,\n\t\t\t\t      struct crypto_cipher *tfm)\n{\n\tvoid (*fn)(struct crypto_tfm *, u8 *, const u8 *) =\n\t\tcrypto_cipher_alg(tfm)->cia_decrypt;\n\tint bsize = crypto_cipher_blocksize(tfm);\n\tunsigned int nbytes = walk->nbytes;\n\tu8 *src = walk->src.virt.addr;\n\tu8 last_iv[bsize];\n\n\t/* Start of the last block. */\n\tsrc += nbytes - (nbytes & (bsize - 1)) - bsize;\n\tmemcpy(last_iv, src, bsize);\n\n\tfor (;;) {\n\t\tfn(crypto_cipher_tfm(tfm), src, src);\n\t\tif ((nbytes -= bsize) < bsize)\n\t\t\tbreak;\n\t\tcrypto_xor(src, src - bsize, bsize);\n\t\tsrc -= bsize;\n\t}\n\n\tcrypto_xor(src, walk->iv, bsize);\n\tmemcpy(walk->iv, last_iv, bsize);\n\n\treturn nbytes;\n}\n\nstatic int crypto_cbc_decrypt(struct blkcipher_desc *desc,\n\t\t\t      struct scatterlist *dst, struct scatterlist *src,\n\t\t\t      unsigned int nbytes)\n{\n\tstruct blkcipher_walk walk;\n\tstruct crypto_blkcipher *tfm = desc->tfm;\n\tstruct crypto_cbc_ctx *ctx = crypto_blkcipher_ctx(tfm);\n\tstruct crypto_cipher *child = ctx->child;\n\tint err;\n\n\tblkcipher_walk_init(&walk, dst, src, nbytes);\n\terr = blkcipher_walk_virt(desc, &walk);\n\n\twhile ((nbytes = walk.nbytes)) {\n\t\tif (walk.src.virt.addr == walk.dst.virt.addr)\n\t\t\tnbytes = crypto_cbc_decrypt_inplace(desc, &walk, child);\n\t\telse\n\t\t\tnbytes = crypto_cbc_decrypt_segment(desc, &walk, child);\n\t\terr = blkcipher_walk_done(desc, &walk, nbytes);\n\t}\n\n\treturn err;\n}\n\nstatic int crypto_cbc_init_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_instance *inst = (void *)tfm->__crt_alg;\n\tstruct crypto_spawn *spawn = crypto_instance_ctx(inst);\n\tstruct crypto_cbc_ctx *ctx = crypto_tfm_ctx(tfm);\n\tstruct crypto_cipher *cipher;\n\n\tcipher = crypto_spawn_cipher(spawn);\n\tif (IS_ERR(cipher))\n\t\treturn PTR_ERR(cipher);\n\n\tctx->child = cipher;\n\treturn 0;\n}\n\nstatic void crypto_cbc_exit_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_cbc_ctx *ctx = crypto_tfm_ctx(tfm);\n\tcrypto_free_cipher(ctx->child);\n}\n\nstatic struct crypto_instance *crypto_cbc_alloc(struct rtattr **tb)\n{\n\tstruct crypto_instance *inst;\n\tstruct crypto_alg *alg;\n\tint err;\n\n\terr = crypto_check_attr_type(tb, CRYPTO_ALG_TYPE_BLKCIPHER);\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\talg = crypto_get_attr_alg(tb, CRYPTO_ALG_TYPE_CIPHER,\n\t\t\t\t  CRYPTO_ALG_TYPE_MASK);\n\tif (IS_ERR(alg))\n\t\treturn ERR_CAST(alg);\n\n\tinst = ERR_PTR(-EINVAL);\n\tif (!is_power_of_2(alg->cra_blocksize))\n\t\tgoto out_put_alg;\n\n\tinst = crypto_alloc_instance(\"cbc\", alg);\n\tif (IS_ERR(inst))\n\t\tgoto out_put_alg;\n\n\tinst->alg.cra_flags = CRYPTO_ALG_TYPE_BLKCIPHER;\n\tinst->alg.cra_priority = alg->cra_priority;\n\tinst->alg.cra_blocksize = alg->cra_blocksize;\n\tinst->alg.cra_alignmask = alg->cra_alignmask;\n\tinst->alg.cra_type = &crypto_blkcipher_type;\n\n\t/* We access the data as u32s when xoring. */\n\tinst->alg.cra_alignmask |= __alignof__(u32) - 1;\n\n\tinst->alg.cra_blkcipher.ivsize = alg->cra_blocksize;\n\tinst->alg.cra_blkcipher.min_keysize = alg->cra_cipher.cia_min_keysize;\n\tinst->alg.cra_blkcipher.max_keysize = alg->cra_cipher.cia_max_keysize;\n\n\tinst->alg.cra_ctxsize = sizeof(struct crypto_cbc_ctx);\n\n\tinst->alg.cra_init = crypto_cbc_init_tfm;\n\tinst->alg.cra_exit = crypto_cbc_exit_tfm;\n\n\tinst->alg.cra_blkcipher.setkey = crypto_cbc_setkey;\n\tinst->alg.cra_blkcipher.encrypt = crypto_cbc_encrypt;\n\tinst->alg.cra_blkcipher.decrypt = crypto_cbc_decrypt;\n\nout_put_alg:\n\tcrypto_mod_put(alg);\n\treturn inst;\n}\n\nstatic void crypto_cbc_free(struct crypto_instance *inst)\n{\n\tcrypto_drop_spawn(crypto_instance_ctx(inst));\n\tkfree(inst);\n}\n\nstatic struct crypto_template crypto_cbc_tmpl = {\n\t.name = \"cbc\",\n\t.alloc = crypto_cbc_alloc,\n\t.free = crypto_cbc_free,\n\t.module = THIS_MODULE,\n};\n\nstatic int __init crypto_cbc_module_init(void)\n{\n\treturn crypto_register_template(&crypto_cbc_tmpl);\n}\n\nstatic void __exit crypto_cbc_module_exit(void)\n{\n\tcrypto_unregister_template(&crypto_cbc_tmpl);\n}\n\nmodule_init(crypto_cbc_module_init);\nmodule_exit(crypto_cbc_module_exit);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"CBC block cipher algorithm\");\n", "/*\n * CCM: Counter with CBC-MAC\n *\n * (C) Copyright IBM Corp. 2007 - Joy Latten <latten@us.ibm.com>\n *\n * This program is free software; you can redistribute it and/or modify it\n * under the terms of the GNU General Public License as published by the Free\n * Software Foundation; either version 2 of the License, or (at your option)\n * any later version.\n *\n */\n\n#include <crypto/internal/aead.h>\n#include <crypto/internal/skcipher.h>\n#include <crypto/scatterwalk.h>\n#include <linux/err.h>\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/slab.h>\n\n#include \"internal.h\"\n\nstruct ccm_instance_ctx {\n\tstruct crypto_skcipher_spawn ctr;\n\tstruct crypto_spawn cipher;\n};\n\nstruct crypto_ccm_ctx {\n\tstruct crypto_cipher *cipher;\n\tstruct crypto_ablkcipher *ctr;\n};\n\nstruct crypto_rfc4309_ctx {\n\tstruct crypto_aead *child;\n\tu8 nonce[3];\n};\n\nstruct crypto_ccm_req_priv_ctx {\n\tu8 odata[16];\n\tu8 idata[16];\n\tu8 auth_tag[16];\n\tu32 ilen;\n\tu32 flags;\n\tstruct scatterlist src[2];\n\tstruct scatterlist dst[2];\n\tstruct ablkcipher_request abreq;\n};\n\nstatic inline struct crypto_ccm_req_priv_ctx *crypto_ccm_reqctx(\n\tstruct aead_request *req)\n{\n\tunsigned long align = crypto_aead_alignmask(crypto_aead_reqtfm(req));\n\n\treturn (void *)PTR_ALIGN((u8 *)aead_request_ctx(req), align + 1);\n}\n\nstatic int set_msg_len(u8 *block, unsigned int msglen, int csize)\n{\n\t__be32 data;\n\n\tmemset(block, 0, csize);\n\tblock += csize;\n\n\tif (csize >= 4)\n\t\tcsize = 4;\n\telse if (msglen > (1 << (8 * csize)))\n\t\treturn -EOVERFLOW;\n\n\tdata = cpu_to_be32(msglen);\n\tmemcpy(block - csize, (u8 *)&data + 4 - csize, csize);\n\n\treturn 0;\n}\n\nstatic int crypto_ccm_setkey(struct crypto_aead *aead, const u8 *key,\n\t\t\t     unsigned int keylen)\n{\n\tstruct crypto_ccm_ctx *ctx = crypto_aead_ctx(aead);\n\tstruct crypto_ablkcipher *ctr = ctx->ctr;\n\tstruct crypto_cipher *tfm = ctx->cipher;\n\tint err = 0;\n\n\tcrypto_ablkcipher_clear_flags(ctr, CRYPTO_TFM_REQ_MASK);\n\tcrypto_ablkcipher_set_flags(ctr, crypto_aead_get_flags(aead) &\n\t\t\t\t    CRYPTO_TFM_REQ_MASK);\n\terr = crypto_ablkcipher_setkey(ctr, key, keylen);\n\tcrypto_aead_set_flags(aead, crypto_ablkcipher_get_flags(ctr) &\n\t\t\t      CRYPTO_TFM_RES_MASK);\n\tif (err)\n\t\tgoto out;\n\n\tcrypto_cipher_clear_flags(tfm, CRYPTO_TFM_REQ_MASK);\n\tcrypto_cipher_set_flags(tfm, crypto_aead_get_flags(aead) &\n\t\t\t\t    CRYPTO_TFM_REQ_MASK);\n\terr = crypto_cipher_setkey(tfm, key, keylen);\n\tcrypto_aead_set_flags(aead, crypto_cipher_get_flags(tfm) &\n\t\t\t      CRYPTO_TFM_RES_MASK);\n\nout:\n\treturn err;\n}\n\nstatic int crypto_ccm_setauthsize(struct crypto_aead *tfm,\n\t\t\t\t  unsigned int authsize)\n{\n\tswitch (authsize) {\n\tcase 4:\n\tcase 6:\n\tcase 8:\n\tcase 10:\n\tcase 12:\n\tcase 14:\n\tcase 16:\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int format_input(u8 *info, struct aead_request *req,\n\t\t\tunsigned int cryptlen)\n{\n\tstruct crypto_aead *aead = crypto_aead_reqtfm(req);\n\tunsigned int lp = req->iv[0];\n\tunsigned int l = lp + 1;\n\tunsigned int m;\n\n\tm = crypto_aead_authsize(aead);\n\n\tmemcpy(info, req->iv, 16);\n\n\t/* format control info per RFC 3610 and\n\t * NIST Special Publication 800-38C\n\t */\n\t*info |= (8 * ((m - 2) / 2));\n\tif (req->assoclen)\n\t\t*info |= 64;\n\n\treturn set_msg_len(info + 16 - l, cryptlen, l);\n}\n\nstatic int format_adata(u8 *adata, unsigned int a)\n{\n\tint len = 0;\n\n\t/* add control info for associated data\n\t * RFC 3610 and NIST Special Publication 800-38C\n\t */\n\tif (a < 65280) {\n\t\t*(__be16 *)adata = cpu_to_be16(a);\n\t\tlen = 2;\n\t} else  {\n\t\t*(__be16 *)adata = cpu_to_be16(0xfffe);\n\t\t*(__be32 *)&adata[2] = cpu_to_be32(a);\n\t\tlen = 6;\n\t}\n\n\treturn len;\n}\n\nstatic void compute_mac(struct crypto_cipher *tfm, u8 *data, int n,\n\t\t       struct crypto_ccm_req_priv_ctx *pctx)\n{\n\tunsigned int bs = 16;\n\tu8 *odata = pctx->odata;\n\tu8 *idata = pctx->idata;\n\tint datalen, getlen;\n\n\tdatalen = n;\n\n\t/* first time in here, block may be partially filled. */\n\tgetlen = bs - pctx->ilen;\n\tif (datalen >= getlen) {\n\t\tmemcpy(idata + pctx->ilen, data, getlen);\n\t\tcrypto_xor(odata, idata, bs);\n\t\tcrypto_cipher_encrypt_one(tfm, odata, odata);\n\t\tdatalen -= getlen;\n\t\tdata += getlen;\n\t\tpctx->ilen = 0;\n\t}\n\n\t/* now encrypt rest of data */\n\twhile (datalen >= bs) {\n\t\tcrypto_xor(odata, data, bs);\n\t\tcrypto_cipher_encrypt_one(tfm, odata, odata);\n\n\t\tdatalen -= bs;\n\t\tdata += bs;\n\t}\n\n\t/* check and see if there's leftover data that wasn't\n\t * enough to fill a block.\n\t */\n\tif (datalen) {\n\t\tmemcpy(idata + pctx->ilen, data, datalen);\n\t\tpctx->ilen += datalen;\n\t}\n}\n\nstatic void get_data_to_compute(struct crypto_cipher *tfm,\n\t\t\t       struct crypto_ccm_req_priv_ctx *pctx,\n\t\t\t       struct scatterlist *sg, unsigned int len)\n{\n\tstruct scatter_walk walk;\n\tu8 *data_src;\n\tint n;\n\n\tscatterwalk_start(&walk, sg);\n\n\twhile (len) {\n\t\tn = scatterwalk_clamp(&walk, len);\n\t\tif (!n) {\n\t\t\tscatterwalk_start(&walk, sg_next(walk.sg));\n\t\t\tn = scatterwalk_clamp(&walk, len);\n\t\t}\n\t\tdata_src = scatterwalk_map(&walk);\n\n\t\tcompute_mac(tfm, data_src, n, pctx);\n\t\tlen -= n;\n\n\t\tscatterwalk_unmap(data_src);\n\t\tscatterwalk_advance(&walk, n);\n\t\tscatterwalk_done(&walk, 0, len);\n\t\tif (len)\n\t\t\tcrypto_yield(pctx->flags);\n\t}\n\n\t/* any leftover needs padding and then encrypted */\n\tif (pctx->ilen) {\n\t\tint padlen;\n\t\tu8 *odata = pctx->odata;\n\t\tu8 *idata = pctx->idata;\n\n\t\tpadlen = 16 - pctx->ilen;\n\t\tmemset(idata + pctx->ilen, 0, padlen);\n\t\tcrypto_xor(odata, idata, 16);\n\t\tcrypto_cipher_encrypt_one(tfm, odata, odata);\n\t\tpctx->ilen = 0;\n\t}\n}\n\nstatic int crypto_ccm_auth(struct aead_request *req, struct scatterlist *plain,\n\t\t\t   unsigned int cryptlen)\n{\n\tstruct crypto_aead *aead = crypto_aead_reqtfm(req);\n\tstruct crypto_ccm_ctx *ctx = crypto_aead_ctx(aead);\n\tstruct crypto_ccm_req_priv_ctx *pctx = crypto_ccm_reqctx(req);\n\tstruct crypto_cipher *cipher = ctx->cipher;\n\tunsigned int assoclen = req->assoclen;\n\tu8 *odata = pctx->odata;\n\tu8 *idata = pctx->idata;\n\tint err;\n\n\t/* format control data for input */\n\terr = format_input(odata, req, cryptlen);\n\tif (err)\n\t\tgoto out;\n\n\t/* encrypt first block to use as start in computing mac  */\n\tcrypto_cipher_encrypt_one(cipher, odata, odata);\n\n\t/* format associated data and compute into mac */\n\tif (assoclen) {\n\t\tpctx->ilen = format_adata(idata, assoclen);\n\t\tget_data_to_compute(cipher, pctx, req->assoc, req->assoclen);\n\t} else {\n\t\tpctx->ilen = 0;\n\t}\n\n\t/* compute plaintext into mac */\n\tif (cryptlen)\n\t\tget_data_to_compute(cipher, pctx, plain, cryptlen);\n\nout:\n\treturn err;\n}\n\nstatic void crypto_ccm_encrypt_done(struct crypto_async_request *areq, int err)\n{\n\tstruct aead_request *req = areq->data;\n\tstruct crypto_aead *aead = crypto_aead_reqtfm(req);\n\tstruct crypto_ccm_req_priv_ctx *pctx = crypto_ccm_reqctx(req);\n\tu8 *odata = pctx->odata;\n\n\tif (!err)\n\t\tscatterwalk_map_and_copy(odata, req->dst, req->cryptlen,\n\t\t\t\t\t crypto_aead_authsize(aead), 1);\n\taead_request_complete(req, err);\n}\n\nstatic inline int crypto_ccm_check_iv(const u8 *iv)\n{\n\t/* 2 <= L <= 8, so 1 <= L' <= 7. */\n\tif (1 > iv[0] || iv[0] > 7)\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic int crypto_ccm_encrypt(struct aead_request *req)\n{\n\tstruct crypto_aead *aead = crypto_aead_reqtfm(req);\n\tstruct crypto_ccm_ctx *ctx = crypto_aead_ctx(aead);\n\tstruct crypto_ccm_req_priv_ctx *pctx = crypto_ccm_reqctx(req);\n\tstruct ablkcipher_request *abreq = &pctx->abreq;\n\tstruct scatterlist *dst;\n\tunsigned int cryptlen = req->cryptlen;\n\tu8 *odata = pctx->odata;\n\tu8 *iv = req->iv;\n\tint err;\n\n\terr = crypto_ccm_check_iv(iv);\n\tif (err)\n\t\treturn err;\n\n\tpctx->flags = aead_request_flags(req);\n\n\terr = crypto_ccm_auth(req, req->src, cryptlen);\n\tif (err)\n\t\treturn err;\n\n\t /* Note: rfc 3610 and NIST 800-38C require counter of\n\t * zero to encrypt auth tag.\n\t */\n\tmemset(iv + 15 - iv[0], 0, iv[0] + 1);\n\n\tsg_init_table(pctx->src, 2);\n\tsg_set_buf(pctx->src, odata, 16);\n\tscatterwalk_sg_chain(pctx->src, 2, req->src);\n\n\tdst = pctx->src;\n\tif (req->src != req->dst) {\n\t\tsg_init_table(pctx->dst, 2);\n\t\tsg_set_buf(pctx->dst, odata, 16);\n\t\tscatterwalk_sg_chain(pctx->dst, 2, req->dst);\n\t\tdst = pctx->dst;\n\t}\n\n\tablkcipher_request_set_tfm(abreq, ctx->ctr);\n\tablkcipher_request_set_callback(abreq, pctx->flags,\n\t\t\t\t\tcrypto_ccm_encrypt_done, req);\n\tablkcipher_request_set_crypt(abreq, pctx->src, dst, cryptlen + 16, iv);\n\terr = crypto_ablkcipher_encrypt(abreq);\n\tif (err)\n\t\treturn err;\n\n\t/* copy authtag to end of dst */\n\tscatterwalk_map_and_copy(odata, req->dst, cryptlen,\n\t\t\t\t crypto_aead_authsize(aead), 1);\n\treturn err;\n}\n\nstatic void crypto_ccm_decrypt_done(struct crypto_async_request *areq,\n\t\t\t\t   int err)\n{\n\tstruct aead_request *req = areq->data;\n\tstruct crypto_ccm_req_priv_ctx *pctx = crypto_ccm_reqctx(req);\n\tstruct crypto_aead *aead = crypto_aead_reqtfm(req);\n\tunsigned int authsize = crypto_aead_authsize(aead);\n\tunsigned int cryptlen = req->cryptlen - authsize;\n\n\tif (!err) {\n\t\terr = crypto_ccm_auth(req, req->dst, cryptlen);\n\t\tif (!err && crypto_memneq(pctx->auth_tag, pctx->odata, authsize))\n\t\t\terr = -EBADMSG;\n\t}\n\taead_request_complete(req, err);\n}\n\nstatic int crypto_ccm_decrypt(struct aead_request *req)\n{\n\tstruct crypto_aead *aead = crypto_aead_reqtfm(req);\n\tstruct crypto_ccm_ctx *ctx = crypto_aead_ctx(aead);\n\tstruct crypto_ccm_req_priv_ctx *pctx = crypto_ccm_reqctx(req);\n\tstruct ablkcipher_request *abreq = &pctx->abreq;\n\tstruct scatterlist *dst;\n\tunsigned int authsize = crypto_aead_authsize(aead);\n\tunsigned int cryptlen = req->cryptlen;\n\tu8 *authtag = pctx->auth_tag;\n\tu8 *odata = pctx->odata;\n\tu8 *iv = req->iv;\n\tint err;\n\n\tif (cryptlen < authsize)\n\t\treturn -EINVAL;\n\tcryptlen -= authsize;\n\n\terr = crypto_ccm_check_iv(iv);\n\tif (err)\n\t\treturn err;\n\n\tpctx->flags = aead_request_flags(req);\n\n\tscatterwalk_map_and_copy(authtag, req->src, cryptlen, authsize, 0);\n\n\tmemset(iv + 15 - iv[0], 0, iv[0] + 1);\n\n\tsg_init_table(pctx->src, 2);\n\tsg_set_buf(pctx->src, authtag, 16);\n\tscatterwalk_sg_chain(pctx->src, 2, req->src);\n\n\tdst = pctx->src;\n\tif (req->src != req->dst) {\n\t\tsg_init_table(pctx->dst, 2);\n\t\tsg_set_buf(pctx->dst, authtag, 16);\n\t\tscatterwalk_sg_chain(pctx->dst, 2, req->dst);\n\t\tdst = pctx->dst;\n\t}\n\n\tablkcipher_request_set_tfm(abreq, ctx->ctr);\n\tablkcipher_request_set_callback(abreq, pctx->flags,\n\t\t\t\t\tcrypto_ccm_decrypt_done, req);\n\tablkcipher_request_set_crypt(abreq, pctx->src, dst, cryptlen + 16, iv);\n\terr = crypto_ablkcipher_decrypt(abreq);\n\tif (err)\n\t\treturn err;\n\n\terr = crypto_ccm_auth(req, req->dst, cryptlen);\n\tif (err)\n\t\treturn err;\n\n\t/* verify */\n\tif (crypto_memneq(authtag, odata, authsize))\n\t\treturn -EBADMSG;\n\n\treturn err;\n}\n\nstatic int crypto_ccm_init_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_instance *inst = (void *)tfm->__crt_alg;\n\tstruct ccm_instance_ctx *ictx = crypto_instance_ctx(inst);\n\tstruct crypto_ccm_ctx *ctx = crypto_tfm_ctx(tfm);\n\tstruct crypto_cipher *cipher;\n\tstruct crypto_ablkcipher *ctr;\n\tunsigned long align;\n\tint err;\n\n\tcipher = crypto_spawn_cipher(&ictx->cipher);\n\tif (IS_ERR(cipher))\n\t\treturn PTR_ERR(cipher);\n\n\tctr = crypto_spawn_skcipher(&ictx->ctr);\n\terr = PTR_ERR(ctr);\n\tif (IS_ERR(ctr))\n\t\tgoto err_free_cipher;\n\n\tctx->cipher = cipher;\n\tctx->ctr = ctr;\n\n\talign = crypto_tfm_alg_alignmask(tfm);\n\talign &= ~(crypto_tfm_ctx_alignment() - 1);\n\ttfm->crt_aead.reqsize = align +\n\t\t\t\tsizeof(struct crypto_ccm_req_priv_ctx) +\n\t\t\t\tcrypto_ablkcipher_reqsize(ctr);\n\n\treturn 0;\n\nerr_free_cipher:\n\tcrypto_free_cipher(cipher);\n\treturn err;\n}\n\nstatic void crypto_ccm_exit_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_ccm_ctx *ctx = crypto_tfm_ctx(tfm);\n\n\tcrypto_free_cipher(ctx->cipher);\n\tcrypto_free_ablkcipher(ctx->ctr);\n}\n\nstatic struct crypto_instance *crypto_ccm_alloc_common(struct rtattr **tb,\n\t\t\t\t\t\t       const char *full_name,\n\t\t\t\t\t\t       const char *ctr_name,\n\t\t\t\t\t\t       const char *cipher_name)\n{\n\tstruct crypto_attr_type *algt;\n\tstruct crypto_instance *inst;\n\tstruct crypto_alg *ctr;\n\tstruct crypto_alg *cipher;\n\tstruct ccm_instance_ctx *ictx;\n\tint err;\n\n\talgt = crypto_get_attr_type(tb);\n\tif (IS_ERR(algt))\n\t\treturn ERR_CAST(algt);\n\n\tif ((algt->type ^ CRYPTO_ALG_TYPE_AEAD) & algt->mask)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tcipher = crypto_alg_mod_lookup(cipher_name,  CRYPTO_ALG_TYPE_CIPHER,\n\t\t\t\t       CRYPTO_ALG_TYPE_MASK);\n\tif (IS_ERR(cipher))\n\t\treturn ERR_CAST(cipher);\n\n\terr = -EINVAL;\n\tif (cipher->cra_blocksize != 16)\n\t\tgoto out_put_cipher;\n\n\tinst = kzalloc(sizeof(*inst) + sizeof(*ictx), GFP_KERNEL);\n\terr = -ENOMEM;\n\tif (!inst)\n\t\tgoto out_put_cipher;\n\n\tictx = crypto_instance_ctx(inst);\n\n\terr = crypto_init_spawn(&ictx->cipher, cipher, inst,\n\t\t\t\tCRYPTO_ALG_TYPE_MASK);\n\tif (err)\n\t\tgoto err_free_inst;\n\n\tcrypto_set_skcipher_spawn(&ictx->ctr, inst);\n\terr = crypto_grab_skcipher(&ictx->ctr, ctr_name, 0,\n\t\t\t\t   crypto_requires_sync(algt->type,\n\t\t\t\t\t\t\talgt->mask));\n\tif (err)\n\t\tgoto err_drop_cipher;\n\n\tctr = crypto_skcipher_spawn_alg(&ictx->ctr);\n\n\t/* Not a stream cipher? */\n\terr = -EINVAL;\n\tif (ctr->cra_blocksize != 1)\n\t\tgoto err_drop_ctr;\n\n\t/* We want the real thing! */\n\tif (ctr->cra_ablkcipher.ivsize != 16)\n\t\tgoto err_drop_ctr;\n\n\terr = -ENAMETOOLONG;\n\tif (snprintf(inst->alg.cra_driver_name, CRYPTO_MAX_ALG_NAME,\n\t\t     \"ccm_base(%s,%s)\", ctr->cra_driver_name,\n\t\t     cipher->cra_driver_name) >= CRYPTO_MAX_ALG_NAME)\n\t\tgoto err_drop_ctr;\n\n\tmemcpy(inst->alg.cra_name, full_name, CRYPTO_MAX_ALG_NAME);\n\n\tinst->alg.cra_flags = CRYPTO_ALG_TYPE_AEAD;\n\tinst->alg.cra_flags |= ctr->cra_flags & CRYPTO_ALG_ASYNC;\n\tinst->alg.cra_priority = cipher->cra_priority + ctr->cra_priority;\n\tinst->alg.cra_blocksize = 1;\n\tinst->alg.cra_alignmask = cipher->cra_alignmask | ctr->cra_alignmask |\n\t\t\t\t  (__alignof__(u32) - 1);\n\tinst->alg.cra_type = &crypto_aead_type;\n\tinst->alg.cra_aead.ivsize = 16;\n\tinst->alg.cra_aead.maxauthsize = 16;\n\tinst->alg.cra_ctxsize = sizeof(struct crypto_ccm_ctx);\n\tinst->alg.cra_init = crypto_ccm_init_tfm;\n\tinst->alg.cra_exit = crypto_ccm_exit_tfm;\n\tinst->alg.cra_aead.setkey = crypto_ccm_setkey;\n\tinst->alg.cra_aead.setauthsize = crypto_ccm_setauthsize;\n\tinst->alg.cra_aead.encrypt = crypto_ccm_encrypt;\n\tinst->alg.cra_aead.decrypt = crypto_ccm_decrypt;\n\nout:\n\tcrypto_mod_put(cipher);\n\treturn inst;\n\nerr_drop_ctr:\n\tcrypto_drop_skcipher(&ictx->ctr);\nerr_drop_cipher:\n\tcrypto_drop_spawn(&ictx->cipher);\nerr_free_inst:\n\tkfree(inst);\nout_put_cipher:\n\tinst = ERR_PTR(err);\n\tgoto out;\n}\n\nstatic struct crypto_instance *crypto_ccm_alloc(struct rtattr **tb)\n{\n\tconst char *cipher_name;\n\tchar ctr_name[CRYPTO_MAX_ALG_NAME];\n\tchar full_name[CRYPTO_MAX_ALG_NAME];\n\n\tcipher_name = crypto_attr_alg_name(tb[1]);\n\tif (IS_ERR(cipher_name))\n\t\treturn ERR_CAST(cipher_name);\n\n\tif (snprintf(ctr_name, CRYPTO_MAX_ALG_NAME, \"ctr(%s)\",\n\t\t     cipher_name) >= CRYPTO_MAX_ALG_NAME)\n\t\treturn ERR_PTR(-ENAMETOOLONG);\n\n\tif (snprintf(full_name, CRYPTO_MAX_ALG_NAME, \"ccm(%s)\", cipher_name) >=\n\t    CRYPTO_MAX_ALG_NAME)\n\t\treturn ERR_PTR(-ENAMETOOLONG);\n\n\treturn crypto_ccm_alloc_common(tb, full_name, ctr_name, cipher_name);\n}\n\nstatic void crypto_ccm_free(struct crypto_instance *inst)\n{\n\tstruct ccm_instance_ctx *ctx = crypto_instance_ctx(inst);\n\n\tcrypto_drop_spawn(&ctx->cipher);\n\tcrypto_drop_skcipher(&ctx->ctr);\n\tkfree(inst);\n}\n\nstatic struct crypto_template crypto_ccm_tmpl = {\n\t.name = \"ccm\",\n\t.alloc = crypto_ccm_alloc,\n\t.free = crypto_ccm_free,\n\t.module = THIS_MODULE,\n};\n\nstatic struct crypto_instance *crypto_ccm_base_alloc(struct rtattr **tb)\n{\n\tconst char *ctr_name;\n\tconst char *cipher_name;\n\tchar full_name[CRYPTO_MAX_ALG_NAME];\n\n\tctr_name = crypto_attr_alg_name(tb[1]);\n\tif (IS_ERR(ctr_name))\n\t\treturn ERR_CAST(ctr_name);\n\n\tcipher_name = crypto_attr_alg_name(tb[2]);\n\tif (IS_ERR(cipher_name))\n\t\treturn ERR_CAST(cipher_name);\n\n\tif (snprintf(full_name, CRYPTO_MAX_ALG_NAME, \"ccm_base(%s,%s)\",\n\t\t     ctr_name, cipher_name) >= CRYPTO_MAX_ALG_NAME)\n\t\treturn ERR_PTR(-ENAMETOOLONG);\n\n\treturn crypto_ccm_alloc_common(tb, full_name, ctr_name, cipher_name);\n}\n\nstatic struct crypto_template crypto_ccm_base_tmpl = {\n\t.name = \"ccm_base\",\n\t.alloc = crypto_ccm_base_alloc,\n\t.free = crypto_ccm_free,\n\t.module = THIS_MODULE,\n};\n\nstatic int crypto_rfc4309_setkey(struct crypto_aead *parent, const u8 *key,\n\t\t\t\t unsigned int keylen)\n{\n\tstruct crypto_rfc4309_ctx *ctx = crypto_aead_ctx(parent);\n\tstruct crypto_aead *child = ctx->child;\n\tint err;\n\n\tif (keylen < 3)\n\t\treturn -EINVAL;\n\n\tkeylen -= 3;\n\tmemcpy(ctx->nonce, key + keylen, 3);\n\n\tcrypto_aead_clear_flags(child, CRYPTO_TFM_REQ_MASK);\n\tcrypto_aead_set_flags(child, crypto_aead_get_flags(parent) &\n\t\t\t\t     CRYPTO_TFM_REQ_MASK);\n\terr = crypto_aead_setkey(child, key, keylen);\n\tcrypto_aead_set_flags(parent, crypto_aead_get_flags(child) &\n\t\t\t\t      CRYPTO_TFM_RES_MASK);\n\n\treturn err;\n}\n\nstatic int crypto_rfc4309_setauthsize(struct crypto_aead *parent,\n\t\t\t\t      unsigned int authsize)\n{\n\tstruct crypto_rfc4309_ctx *ctx = crypto_aead_ctx(parent);\n\n\tswitch (authsize) {\n\tcase 8:\n\tcase 12:\n\tcase 16:\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn crypto_aead_setauthsize(ctx->child, authsize);\n}\n\nstatic struct aead_request *crypto_rfc4309_crypt(struct aead_request *req)\n{\n\tstruct aead_request *subreq = aead_request_ctx(req);\n\tstruct crypto_aead *aead = crypto_aead_reqtfm(req);\n\tstruct crypto_rfc4309_ctx *ctx = crypto_aead_ctx(aead);\n\tstruct crypto_aead *child = ctx->child;\n\tu8 *iv = PTR_ALIGN((u8 *)(subreq + 1) + crypto_aead_reqsize(child),\n\t\t\t   crypto_aead_alignmask(child) + 1);\n\n\t/* L' */\n\tiv[0] = 3;\n\n\tmemcpy(iv + 1, ctx->nonce, 3);\n\tmemcpy(iv + 4, req->iv, 8);\n\n\taead_request_set_tfm(subreq, child);\n\taead_request_set_callback(subreq, req->base.flags, req->base.complete,\n\t\t\t\t  req->base.data);\n\taead_request_set_crypt(subreq, req->src, req->dst, req->cryptlen, iv);\n\taead_request_set_assoc(subreq, req->assoc, req->assoclen);\n\n\treturn subreq;\n}\n\nstatic int crypto_rfc4309_encrypt(struct aead_request *req)\n{\n\treq = crypto_rfc4309_crypt(req);\n\n\treturn crypto_aead_encrypt(req);\n}\n\nstatic int crypto_rfc4309_decrypt(struct aead_request *req)\n{\n\treq = crypto_rfc4309_crypt(req);\n\n\treturn crypto_aead_decrypt(req);\n}\n\nstatic int crypto_rfc4309_init_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_instance *inst = (void *)tfm->__crt_alg;\n\tstruct crypto_aead_spawn *spawn = crypto_instance_ctx(inst);\n\tstruct crypto_rfc4309_ctx *ctx = crypto_tfm_ctx(tfm);\n\tstruct crypto_aead *aead;\n\tunsigned long align;\n\n\taead = crypto_spawn_aead(spawn);\n\tif (IS_ERR(aead))\n\t\treturn PTR_ERR(aead);\n\n\tctx->child = aead;\n\n\talign = crypto_aead_alignmask(aead);\n\talign &= ~(crypto_tfm_ctx_alignment() - 1);\n\ttfm->crt_aead.reqsize = sizeof(struct aead_request) +\n\t\t\t\tALIGN(crypto_aead_reqsize(aead),\n\t\t\t\t      crypto_tfm_ctx_alignment()) +\n\t\t\t\talign + 16;\n\n\treturn 0;\n}\n\nstatic void crypto_rfc4309_exit_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_rfc4309_ctx *ctx = crypto_tfm_ctx(tfm);\n\n\tcrypto_free_aead(ctx->child);\n}\n\nstatic struct crypto_instance *crypto_rfc4309_alloc(struct rtattr **tb)\n{\n\tstruct crypto_attr_type *algt;\n\tstruct crypto_instance *inst;\n\tstruct crypto_aead_spawn *spawn;\n\tstruct crypto_alg *alg;\n\tconst char *ccm_name;\n\tint err;\n\n\talgt = crypto_get_attr_type(tb);\n\tif (IS_ERR(algt))\n\t\treturn ERR_CAST(algt);\n\n\tif ((algt->type ^ CRYPTO_ALG_TYPE_AEAD) & algt->mask)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tccm_name = crypto_attr_alg_name(tb[1]);\n\tif (IS_ERR(ccm_name))\n\t\treturn ERR_CAST(ccm_name);\n\n\tinst = kzalloc(sizeof(*inst) + sizeof(*spawn), GFP_KERNEL);\n\tif (!inst)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tspawn = crypto_instance_ctx(inst);\n\tcrypto_set_aead_spawn(spawn, inst);\n\terr = crypto_grab_aead(spawn, ccm_name, 0,\n\t\t\t       crypto_requires_sync(algt->type, algt->mask));\n\tif (err)\n\t\tgoto out_free_inst;\n\n\talg = crypto_aead_spawn_alg(spawn);\n\n\terr = -EINVAL;\n\n\t/* We only support 16-byte blocks. */\n\tif (alg->cra_aead.ivsize != 16)\n\t\tgoto out_drop_alg;\n\n\t/* Not a stream cipher? */\n\tif (alg->cra_blocksize != 1)\n\t\tgoto out_drop_alg;\n\n\terr = -ENAMETOOLONG;\n\tif (snprintf(inst->alg.cra_name, CRYPTO_MAX_ALG_NAME,\n\t\t     \"rfc4309(%s)\", alg->cra_name) >= CRYPTO_MAX_ALG_NAME ||\n\t    snprintf(inst->alg.cra_driver_name, CRYPTO_MAX_ALG_NAME,\n\t\t     \"rfc4309(%s)\", alg->cra_driver_name) >=\n\t    CRYPTO_MAX_ALG_NAME)\n\t\tgoto out_drop_alg;\n\n\tinst->alg.cra_flags = CRYPTO_ALG_TYPE_AEAD;\n\tinst->alg.cra_flags |= alg->cra_flags & CRYPTO_ALG_ASYNC;\n\tinst->alg.cra_priority = alg->cra_priority;\n\tinst->alg.cra_blocksize = 1;\n\tinst->alg.cra_alignmask = alg->cra_alignmask;\n\tinst->alg.cra_type = &crypto_nivaead_type;\n\n\tinst->alg.cra_aead.ivsize = 8;\n\tinst->alg.cra_aead.maxauthsize = 16;\n\n\tinst->alg.cra_ctxsize = sizeof(struct crypto_rfc4309_ctx);\n\n\tinst->alg.cra_init = crypto_rfc4309_init_tfm;\n\tinst->alg.cra_exit = crypto_rfc4309_exit_tfm;\n\n\tinst->alg.cra_aead.setkey = crypto_rfc4309_setkey;\n\tinst->alg.cra_aead.setauthsize = crypto_rfc4309_setauthsize;\n\tinst->alg.cra_aead.encrypt = crypto_rfc4309_encrypt;\n\tinst->alg.cra_aead.decrypt = crypto_rfc4309_decrypt;\n\n\tinst->alg.cra_aead.geniv = \"seqiv\";\n\nout:\n\treturn inst;\n\nout_drop_alg:\n\tcrypto_drop_aead(spawn);\nout_free_inst:\n\tkfree(inst);\n\tinst = ERR_PTR(err);\n\tgoto out;\n}\n\nstatic void crypto_rfc4309_free(struct crypto_instance *inst)\n{\n\tcrypto_drop_spawn(crypto_instance_ctx(inst));\n\tkfree(inst);\n}\n\nstatic struct crypto_template crypto_rfc4309_tmpl = {\n\t.name = \"rfc4309\",\n\t.alloc = crypto_rfc4309_alloc,\n\t.free = crypto_rfc4309_free,\n\t.module = THIS_MODULE,\n};\n\nstatic int __init crypto_ccm_module_init(void)\n{\n\tint err;\n\n\terr = crypto_register_template(&crypto_ccm_base_tmpl);\n\tif (err)\n\t\tgoto out;\n\n\terr = crypto_register_template(&crypto_ccm_tmpl);\n\tif (err)\n\t\tgoto out_undo_base;\n\n\terr = crypto_register_template(&crypto_rfc4309_tmpl);\n\tif (err)\n\t\tgoto out_undo_ccm;\n\nout:\n\treturn err;\n\nout_undo_ccm:\n\tcrypto_unregister_template(&crypto_ccm_tmpl);\nout_undo_base:\n\tcrypto_unregister_template(&crypto_ccm_base_tmpl);\n\tgoto out;\n}\n\nstatic void __exit crypto_ccm_module_exit(void)\n{\n\tcrypto_unregister_template(&crypto_rfc4309_tmpl);\n\tcrypto_unregister_template(&crypto_ccm_tmpl);\n\tcrypto_unregister_template(&crypto_ccm_base_tmpl);\n}\n\nmodule_init(crypto_ccm_module_init);\nmodule_exit(crypto_ccm_module_exit);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"Counter with CBC MAC\");\nMODULE_ALIAS_CRYPTO(\"ccm_base\");\nMODULE_ALIAS_CRYPTO(\"rfc4309\");\n", "/*\n * chainiv: Chain IV Generator\n *\n * Generate IVs simply be using the last block of the previous encryption.\n * This is mainly useful for CBC with a synchronous algorithm.\n *\n * Copyright (c) 2007 Herbert Xu <herbert@gondor.apana.org.au>\n *\n * This program is free software; you can redistribute it and/or modify it\n * under the terms of the GNU General Public License as published by the Free\n * Software Foundation; either version 2 of the License, or (at your option)\n * any later version.\n *\n */\n\n#include <crypto/internal/skcipher.h>\n#include <crypto/rng.h>\n#include <crypto/crypto_wq.h>\n#include <linux/err.h>\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/spinlock.h>\n#include <linux/string.h>\n#include <linux/workqueue.h>\n\nenum {\n\tCHAINIV_STATE_INUSE = 0,\n};\n\nstruct chainiv_ctx {\n\tspinlock_t lock;\n\tchar iv[];\n};\n\nstruct async_chainiv_ctx {\n\tunsigned long state;\n\n\tspinlock_t lock;\n\tint err;\n\n\tstruct crypto_queue queue;\n\tstruct work_struct postponed;\n\n\tchar iv[];\n};\n\nstatic int chainiv_givencrypt(struct skcipher_givcrypt_request *req)\n{\n\tstruct crypto_ablkcipher *geniv = skcipher_givcrypt_reqtfm(req);\n\tstruct chainiv_ctx *ctx = crypto_ablkcipher_ctx(geniv);\n\tstruct ablkcipher_request *subreq = skcipher_givcrypt_reqctx(req);\n\tunsigned int ivsize;\n\tint err;\n\n\tablkcipher_request_set_tfm(subreq, skcipher_geniv_cipher(geniv));\n\tablkcipher_request_set_callback(subreq, req->creq.base.flags &\n\t\t\t\t\t\t~CRYPTO_TFM_REQ_MAY_SLEEP,\n\t\t\t\t\treq->creq.base.complete,\n\t\t\t\t\treq->creq.base.data);\n\tablkcipher_request_set_crypt(subreq, req->creq.src, req->creq.dst,\n\t\t\t\t     req->creq.nbytes, req->creq.info);\n\n\tspin_lock_bh(&ctx->lock);\n\n\tivsize = crypto_ablkcipher_ivsize(geniv);\n\n\tmemcpy(req->giv, ctx->iv, ivsize);\n\tmemcpy(subreq->info, ctx->iv, ivsize);\n\n\terr = crypto_ablkcipher_encrypt(subreq);\n\tif (err)\n\t\tgoto unlock;\n\n\tmemcpy(ctx->iv, subreq->info, ivsize);\n\nunlock:\n\tspin_unlock_bh(&ctx->lock);\n\n\treturn err;\n}\n\nstatic int chainiv_givencrypt_first(struct skcipher_givcrypt_request *req)\n{\n\tstruct crypto_ablkcipher *geniv = skcipher_givcrypt_reqtfm(req);\n\tstruct chainiv_ctx *ctx = crypto_ablkcipher_ctx(geniv);\n\tint err = 0;\n\n\tspin_lock_bh(&ctx->lock);\n\tif (crypto_ablkcipher_crt(geniv)->givencrypt !=\n\t    chainiv_givencrypt_first)\n\t\tgoto unlock;\n\n\tcrypto_ablkcipher_crt(geniv)->givencrypt = chainiv_givencrypt;\n\terr = crypto_rng_get_bytes(crypto_default_rng, ctx->iv,\n\t\t\t\t   crypto_ablkcipher_ivsize(geniv));\n\nunlock:\n\tspin_unlock_bh(&ctx->lock);\n\n\tif (err)\n\t\treturn err;\n\n\treturn chainiv_givencrypt(req);\n}\n\nstatic int chainiv_init_common(struct crypto_tfm *tfm)\n{\n\ttfm->crt_ablkcipher.reqsize = sizeof(struct ablkcipher_request);\n\n\treturn skcipher_geniv_init(tfm);\n}\n\nstatic int chainiv_init(struct crypto_tfm *tfm)\n{\n\tstruct chainiv_ctx *ctx = crypto_tfm_ctx(tfm);\n\n\tspin_lock_init(&ctx->lock);\n\n\treturn chainiv_init_common(tfm);\n}\n\nstatic int async_chainiv_schedule_work(struct async_chainiv_ctx *ctx)\n{\n\tint queued;\n\tint err = ctx->err;\n\n\tif (!ctx->queue.qlen) {\n\t\tsmp_mb__before_atomic();\n\t\tclear_bit(CHAINIV_STATE_INUSE, &ctx->state);\n\n\t\tif (!ctx->queue.qlen ||\n\t\t    test_and_set_bit(CHAINIV_STATE_INUSE, &ctx->state))\n\t\t\tgoto out;\n\t}\n\n\tqueued = queue_work(kcrypto_wq, &ctx->postponed);\n\tBUG_ON(!queued);\n\nout:\n\treturn err;\n}\n\nstatic int async_chainiv_postpone_request(struct skcipher_givcrypt_request *req)\n{\n\tstruct crypto_ablkcipher *geniv = skcipher_givcrypt_reqtfm(req);\n\tstruct async_chainiv_ctx *ctx = crypto_ablkcipher_ctx(geniv);\n\tint err;\n\n\tspin_lock_bh(&ctx->lock);\n\terr = skcipher_enqueue_givcrypt(&ctx->queue, req);\n\tspin_unlock_bh(&ctx->lock);\n\n\tif (test_and_set_bit(CHAINIV_STATE_INUSE, &ctx->state))\n\t\treturn err;\n\n\tctx->err = err;\n\treturn async_chainiv_schedule_work(ctx);\n}\n\nstatic int async_chainiv_givencrypt_tail(struct skcipher_givcrypt_request *req)\n{\n\tstruct crypto_ablkcipher *geniv = skcipher_givcrypt_reqtfm(req);\n\tstruct async_chainiv_ctx *ctx = crypto_ablkcipher_ctx(geniv);\n\tstruct ablkcipher_request *subreq = skcipher_givcrypt_reqctx(req);\n\tunsigned int ivsize = crypto_ablkcipher_ivsize(geniv);\n\n\tmemcpy(req->giv, ctx->iv, ivsize);\n\tmemcpy(subreq->info, ctx->iv, ivsize);\n\n\tctx->err = crypto_ablkcipher_encrypt(subreq);\n\tif (ctx->err)\n\t\tgoto out;\n\n\tmemcpy(ctx->iv, subreq->info, ivsize);\n\nout:\n\treturn async_chainiv_schedule_work(ctx);\n}\n\nstatic int async_chainiv_givencrypt(struct skcipher_givcrypt_request *req)\n{\n\tstruct crypto_ablkcipher *geniv = skcipher_givcrypt_reqtfm(req);\n\tstruct async_chainiv_ctx *ctx = crypto_ablkcipher_ctx(geniv);\n\tstruct ablkcipher_request *subreq = skcipher_givcrypt_reqctx(req);\n\n\tablkcipher_request_set_tfm(subreq, skcipher_geniv_cipher(geniv));\n\tablkcipher_request_set_callback(subreq, req->creq.base.flags,\n\t\t\t\t\treq->creq.base.complete,\n\t\t\t\t\treq->creq.base.data);\n\tablkcipher_request_set_crypt(subreq, req->creq.src, req->creq.dst,\n\t\t\t\t     req->creq.nbytes, req->creq.info);\n\n\tif (test_and_set_bit(CHAINIV_STATE_INUSE, &ctx->state))\n\t\tgoto postpone;\n\n\tif (ctx->queue.qlen) {\n\t\tclear_bit(CHAINIV_STATE_INUSE, &ctx->state);\n\t\tgoto postpone;\n\t}\n\n\treturn async_chainiv_givencrypt_tail(req);\n\npostpone:\n\treturn async_chainiv_postpone_request(req);\n}\n\nstatic int async_chainiv_givencrypt_first(struct skcipher_givcrypt_request *req)\n{\n\tstruct crypto_ablkcipher *geniv = skcipher_givcrypt_reqtfm(req);\n\tstruct async_chainiv_ctx *ctx = crypto_ablkcipher_ctx(geniv);\n\tint err = 0;\n\n\tif (test_and_set_bit(CHAINIV_STATE_INUSE, &ctx->state))\n\t\tgoto out;\n\n\tif (crypto_ablkcipher_crt(geniv)->givencrypt !=\n\t    async_chainiv_givencrypt_first)\n\t\tgoto unlock;\n\n\tcrypto_ablkcipher_crt(geniv)->givencrypt = async_chainiv_givencrypt;\n\terr = crypto_rng_get_bytes(crypto_default_rng, ctx->iv,\n\t\t\t\t   crypto_ablkcipher_ivsize(geniv));\n\nunlock:\n\tclear_bit(CHAINIV_STATE_INUSE, &ctx->state);\n\n\tif (err)\n\t\treturn err;\n\nout:\n\treturn async_chainiv_givencrypt(req);\n}\n\nstatic void async_chainiv_do_postponed(struct work_struct *work)\n{\n\tstruct async_chainiv_ctx *ctx = container_of(work,\n\t\t\t\t\t\t     struct async_chainiv_ctx,\n\t\t\t\t\t\t     postponed);\n\tstruct skcipher_givcrypt_request *req;\n\tstruct ablkcipher_request *subreq;\n\tint err;\n\n\t/* Only handle one request at a time to avoid hogging keventd. */\n\tspin_lock_bh(&ctx->lock);\n\treq = skcipher_dequeue_givcrypt(&ctx->queue);\n\tspin_unlock_bh(&ctx->lock);\n\n\tif (!req) {\n\t\tasync_chainiv_schedule_work(ctx);\n\t\treturn;\n\t}\n\n\tsubreq = skcipher_givcrypt_reqctx(req);\n\tsubreq->base.flags |= CRYPTO_TFM_REQ_MAY_SLEEP;\n\n\terr = async_chainiv_givencrypt_tail(req);\n\n\tlocal_bh_disable();\n\tskcipher_givcrypt_complete(req, err);\n\tlocal_bh_enable();\n}\n\nstatic int async_chainiv_init(struct crypto_tfm *tfm)\n{\n\tstruct async_chainiv_ctx *ctx = crypto_tfm_ctx(tfm);\n\n\tspin_lock_init(&ctx->lock);\n\n\tcrypto_init_queue(&ctx->queue, 100);\n\tINIT_WORK(&ctx->postponed, async_chainiv_do_postponed);\n\n\treturn chainiv_init_common(tfm);\n}\n\nstatic void async_chainiv_exit(struct crypto_tfm *tfm)\n{\n\tstruct async_chainiv_ctx *ctx = crypto_tfm_ctx(tfm);\n\n\tBUG_ON(test_bit(CHAINIV_STATE_INUSE, &ctx->state) || ctx->queue.qlen);\n\n\tskcipher_geniv_exit(tfm);\n}\n\nstatic struct crypto_template chainiv_tmpl;\n\nstatic struct crypto_instance *chainiv_alloc(struct rtattr **tb)\n{\n\tstruct crypto_attr_type *algt;\n\tstruct crypto_instance *inst;\n\tint err;\n\n\talgt = crypto_get_attr_type(tb);\n\tif (IS_ERR(algt))\n\t\treturn ERR_CAST(algt);\n\n\terr = crypto_get_default_rng();\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\tinst = skcipher_geniv_alloc(&chainiv_tmpl, tb, 0, 0);\n\tif (IS_ERR(inst))\n\t\tgoto put_rng;\n\n\tinst->alg.cra_ablkcipher.givencrypt = chainiv_givencrypt_first;\n\n\tinst->alg.cra_init = chainiv_init;\n\tinst->alg.cra_exit = skcipher_geniv_exit;\n\n\tinst->alg.cra_ctxsize = sizeof(struct chainiv_ctx);\n\n\tif (!crypto_requires_sync(algt->type, algt->mask)) {\n\t\tinst->alg.cra_flags |= CRYPTO_ALG_ASYNC;\n\n\t\tinst->alg.cra_ablkcipher.givencrypt =\n\t\t\tasync_chainiv_givencrypt_first;\n\n\t\tinst->alg.cra_init = async_chainiv_init;\n\t\tinst->alg.cra_exit = async_chainiv_exit;\n\n\t\tinst->alg.cra_ctxsize = sizeof(struct async_chainiv_ctx);\n\t}\n\n\tinst->alg.cra_ctxsize += inst->alg.cra_ablkcipher.ivsize;\n\nout:\n\treturn inst;\n\nput_rng:\n\tcrypto_put_default_rng();\n\tgoto out;\n}\n\nstatic void chainiv_free(struct crypto_instance *inst)\n{\n\tskcipher_geniv_free(inst);\n\tcrypto_put_default_rng();\n}\n\nstatic struct crypto_template chainiv_tmpl = {\n\t.name = \"chainiv\",\n\t.alloc = chainiv_alloc,\n\t.free = chainiv_free,\n\t.module = THIS_MODULE,\n};\n\nstatic int __init chainiv_module_init(void)\n{\n\treturn crypto_register_template(&chainiv_tmpl);\n}\n\nstatic void chainiv_module_exit(void)\n{\n\tcrypto_unregister_template(&chainiv_tmpl);\n}\n\nmodule_init(chainiv_module_init);\nmodule_exit(chainiv_module_exit);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"Chain IV Generator\");\n", "/*\n * CMAC: Cipher Block Mode for Authentication\n *\n * Copyright \u00a9 2013 Jussi Kivilinna <jussi.kivilinna@iki.fi>\n *\n * Based on work by:\n *  Copyright \u00a9 2013 Tom St Denis <tstdenis@elliptictech.com>\n * Based on crypto/xcbc.c:\n *  Copyright \u00a9 2006 USAGI/WIDE Project,\n *   Author: Kazunori Miyazawa <miyazawa@linux-ipv6.org>\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License as published by\n * the Free Software Foundation; either version 2 of the License, or\n * (at your option) any later version.\n *\n */\n\n#include <crypto/internal/hash.h>\n#include <linux/err.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n\n/*\n * +------------------------\n * | <parent tfm>\n * +------------------------\n * | cmac_tfm_ctx\n * +------------------------\n * | consts (block size * 2)\n * +------------------------\n */\nstruct cmac_tfm_ctx {\n\tstruct crypto_cipher *child;\n\tu8 ctx[];\n};\n\n/*\n * +------------------------\n * | <shash desc>\n * +------------------------\n * | cmac_desc_ctx\n * +------------------------\n * | odds (block size)\n * +------------------------\n * | prev (block size)\n * +------------------------\n */\nstruct cmac_desc_ctx {\n\tunsigned int len;\n\tu8 ctx[];\n};\n\nstatic int crypto_cmac_digest_setkey(struct crypto_shash *parent,\n\t\t\t\t     const u8 *inkey, unsigned int keylen)\n{\n\tunsigned long alignmask = crypto_shash_alignmask(parent);\n\tstruct cmac_tfm_ctx *ctx = crypto_shash_ctx(parent);\n\tunsigned int bs = crypto_shash_blocksize(parent);\n\t__be64 *consts = PTR_ALIGN((void *)ctx->ctx, alignmask + 1);\n\tu64 _const[2];\n\tint i, err = 0;\n\tu8 msb_mask, gfmask;\n\n\terr = crypto_cipher_setkey(ctx->child, inkey, keylen);\n\tif (err)\n\t\treturn err;\n\n\t/* encrypt the zero block */\n\tmemset(consts, 0, bs);\n\tcrypto_cipher_encrypt_one(ctx->child, (u8 *)consts, (u8 *)consts);\n\n\tswitch (bs) {\n\tcase 16:\n\t\tgfmask = 0x87;\n\t\t_const[0] = be64_to_cpu(consts[1]);\n\t\t_const[1] = be64_to_cpu(consts[0]);\n\n\t\t/* gf(2^128) multiply zero-ciphertext with u and u^2 */\n\t\tfor (i = 0; i < 4; i += 2) {\n\t\t\tmsb_mask = ((s64)_const[1] >> 63) & gfmask;\n\t\t\t_const[1] = (_const[1] << 1) | (_const[0] >> 63);\n\t\t\t_const[0] = (_const[0] << 1) ^ msb_mask;\n\n\t\t\tconsts[i + 0] = cpu_to_be64(_const[1]);\n\t\t\tconsts[i + 1] = cpu_to_be64(_const[0]);\n\t\t}\n\n\t\tbreak;\n\tcase 8:\n\t\tgfmask = 0x1B;\n\t\t_const[0] = be64_to_cpu(consts[0]);\n\n\t\t/* gf(2^64) multiply zero-ciphertext with u and u^2 */\n\t\tfor (i = 0; i < 2; i++) {\n\t\t\tmsb_mask = ((s64)_const[0] >> 63) & gfmask;\n\t\t\t_const[0] = (_const[0] << 1) ^ msb_mask;\n\n\t\t\tconsts[i] = cpu_to_be64(_const[0]);\n\t\t}\n\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\nstatic int crypto_cmac_digest_init(struct shash_desc *pdesc)\n{\n\tunsigned long alignmask = crypto_shash_alignmask(pdesc->tfm);\n\tstruct cmac_desc_ctx *ctx = shash_desc_ctx(pdesc);\n\tint bs = crypto_shash_blocksize(pdesc->tfm);\n\tu8 *prev = PTR_ALIGN((void *)ctx->ctx, alignmask + 1) + bs;\n\n\tctx->len = 0;\n\tmemset(prev, 0, bs);\n\n\treturn 0;\n}\n\nstatic int crypto_cmac_digest_update(struct shash_desc *pdesc, const u8 *p,\n\t\t\t\t     unsigned int len)\n{\n\tstruct crypto_shash *parent = pdesc->tfm;\n\tunsigned long alignmask = crypto_shash_alignmask(parent);\n\tstruct cmac_tfm_ctx *tctx = crypto_shash_ctx(parent);\n\tstruct cmac_desc_ctx *ctx = shash_desc_ctx(pdesc);\n\tstruct crypto_cipher *tfm = tctx->child;\n\tint bs = crypto_shash_blocksize(parent);\n\tu8 *odds = PTR_ALIGN((void *)ctx->ctx, alignmask + 1);\n\tu8 *prev = odds + bs;\n\n\t/* checking the data can fill the block */\n\tif ((ctx->len + len) <= bs) {\n\t\tmemcpy(odds + ctx->len, p, len);\n\t\tctx->len += len;\n\t\treturn 0;\n\t}\n\n\t/* filling odds with new data and encrypting it */\n\tmemcpy(odds + ctx->len, p, bs - ctx->len);\n\tlen -= bs - ctx->len;\n\tp += bs - ctx->len;\n\n\tcrypto_xor(prev, odds, bs);\n\tcrypto_cipher_encrypt_one(tfm, prev, prev);\n\n\t/* clearing the length */\n\tctx->len = 0;\n\n\t/* encrypting the rest of data */\n\twhile (len > bs) {\n\t\tcrypto_xor(prev, p, bs);\n\t\tcrypto_cipher_encrypt_one(tfm, prev, prev);\n\t\tp += bs;\n\t\tlen -= bs;\n\t}\n\n\t/* keeping the surplus of blocksize */\n\tif (len) {\n\t\tmemcpy(odds, p, len);\n\t\tctx->len = len;\n\t}\n\n\treturn 0;\n}\n\nstatic int crypto_cmac_digest_final(struct shash_desc *pdesc, u8 *out)\n{\n\tstruct crypto_shash *parent = pdesc->tfm;\n\tunsigned long alignmask = crypto_shash_alignmask(parent);\n\tstruct cmac_tfm_ctx *tctx = crypto_shash_ctx(parent);\n\tstruct cmac_desc_ctx *ctx = shash_desc_ctx(pdesc);\n\tstruct crypto_cipher *tfm = tctx->child;\n\tint bs = crypto_shash_blocksize(parent);\n\tu8 *consts = PTR_ALIGN((void *)tctx->ctx, alignmask + 1);\n\tu8 *odds = PTR_ALIGN((void *)ctx->ctx, alignmask + 1);\n\tu8 *prev = odds + bs;\n\tunsigned int offset = 0;\n\n\tif (ctx->len != bs) {\n\t\tunsigned int rlen;\n\t\tu8 *p = odds + ctx->len;\n\n\t\t*p = 0x80;\n\t\tp++;\n\n\t\trlen = bs - ctx->len - 1;\n\t\tif (rlen)\n\t\t\tmemset(p, 0, rlen);\n\n\t\toffset += bs;\n\t}\n\n\tcrypto_xor(prev, odds, bs);\n\tcrypto_xor(prev, consts + offset, bs);\n\n\tcrypto_cipher_encrypt_one(tfm, out, prev);\n\n\treturn 0;\n}\n\nstatic int cmac_init_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_cipher *cipher;\n\tstruct crypto_instance *inst = (void *)tfm->__crt_alg;\n\tstruct crypto_spawn *spawn = crypto_instance_ctx(inst);\n\tstruct cmac_tfm_ctx *ctx = crypto_tfm_ctx(tfm);\n\n\tcipher = crypto_spawn_cipher(spawn);\n\tif (IS_ERR(cipher))\n\t\treturn PTR_ERR(cipher);\n\n\tctx->child = cipher;\n\n\treturn 0;\n};\n\nstatic void cmac_exit_tfm(struct crypto_tfm *tfm)\n{\n\tstruct cmac_tfm_ctx *ctx = crypto_tfm_ctx(tfm);\n\tcrypto_free_cipher(ctx->child);\n}\n\nstatic int cmac_create(struct crypto_template *tmpl, struct rtattr **tb)\n{\n\tstruct shash_instance *inst;\n\tstruct crypto_alg *alg;\n\tunsigned long alignmask;\n\tint err;\n\n\terr = crypto_check_attr_type(tb, CRYPTO_ALG_TYPE_SHASH);\n\tif (err)\n\t\treturn err;\n\n\talg = crypto_get_attr_alg(tb, CRYPTO_ALG_TYPE_CIPHER,\n\t\t\t\t  CRYPTO_ALG_TYPE_MASK);\n\tif (IS_ERR(alg))\n\t\treturn PTR_ERR(alg);\n\n\tswitch (alg->cra_blocksize) {\n\tcase 16:\n\tcase 8:\n\t\tbreak;\n\tdefault:\n\t\tgoto out_put_alg;\n\t}\n\n\tinst = shash_alloc_instance(\"cmac\", alg);\n\terr = PTR_ERR(inst);\n\tif (IS_ERR(inst))\n\t\tgoto out_put_alg;\n\n\terr = crypto_init_spawn(shash_instance_ctx(inst), alg,\n\t\t\t\tshash_crypto_instance(inst),\n\t\t\t\tCRYPTO_ALG_TYPE_MASK);\n\tif (err)\n\t\tgoto out_free_inst;\n\n\talignmask = alg->cra_alignmask | (sizeof(long) - 1);\n\tinst->alg.base.cra_alignmask = alignmask;\n\tinst->alg.base.cra_priority = alg->cra_priority;\n\tinst->alg.base.cra_blocksize = alg->cra_blocksize;\n\n\tinst->alg.digestsize = alg->cra_blocksize;\n\tinst->alg.descsize =\n\t\tALIGN(sizeof(struct cmac_desc_ctx), crypto_tfm_ctx_alignment())\n\t\t+ (alignmask & ~(crypto_tfm_ctx_alignment() - 1))\n\t\t+ alg->cra_blocksize * 2;\n\n\tinst->alg.base.cra_ctxsize =\n\t\tALIGN(sizeof(struct cmac_tfm_ctx), alignmask + 1)\n\t\t+ alg->cra_blocksize * 2;\n\n\tinst->alg.base.cra_init = cmac_init_tfm;\n\tinst->alg.base.cra_exit = cmac_exit_tfm;\n\n\tinst->alg.init = crypto_cmac_digest_init;\n\tinst->alg.update = crypto_cmac_digest_update;\n\tinst->alg.final = crypto_cmac_digest_final;\n\tinst->alg.setkey = crypto_cmac_digest_setkey;\n\n\terr = shash_register_instance(tmpl, inst);\n\tif (err) {\nout_free_inst:\n\t\tshash_free_instance(shash_crypto_instance(inst));\n\t}\n\nout_put_alg:\n\tcrypto_mod_put(alg);\n\treturn err;\n}\n\nstatic struct crypto_template crypto_cmac_tmpl = {\n\t.name = \"cmac\",\n\t.create = cmac_create,\n\t.free = shash_free_instance,\n\t.module = THIS_MODULE,\n};\n\nstatic int __init crypto_cmac_module_init(void)\n{\n\treturn crypto_register_template(&crypto_cmac_tmpl);\n}\n\nstatic void __exit crypto_cmac_module_exit(void)\n{\n\tcrypto_unregister_template(&crypto_cmac_tmpl);\n}\n\nmodule_init(crypto_cmac_module_init);\nmodule_exit(crypto_cmac_module_exit);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"CMAC keyed hash algorithm\");\n", "/*\n * Software async crypto daemon.\n *\n * Copyright (c) 2006 Herbert Xu <herbert@gondor.apana.org.au>\n *\n * Added AEAD support to cryptd.\n *    Authors: Tadeusz Struk (tadeusz.struk@intel.com)\n *             Adrian Hoban <adrian.hoban@intel.com>\n *             Gabriele Paoloni <gabriele.paoloni@intel.com>\n *             Aidan O'Mahony (aidan.o.mahony@intel.com)\n *    Copyright (c) 2010, Intel Corporation.\n *\n * This program is free software; you can redistribute it and/or modify it\n * under the terms of the GNU General Public License as published by the Free\n * Software Foundation; either version 2 of the License, or (at your option)\n * any later version.\n *\n */\n\n#include <crypto/algapi.h>\n#include <crypto/internal/hash.h>\n#include <crypto/internal/aead.h>\n#include <crypto/cryptd.h>\n#include <crypto/crypto_wq.h>\n#include <linux/err.h>\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/list.h>\n#include <linux/module.h>\n#include <linux/scatterlist.h>\n#include <linux/sched.h>\n#include <linux/slab.h>\n\n#define CRYPTD_MAX_CPU_QLEN 100\n\nstruct cryptd_cpu_queue {\n\tstruct crypto_queue queue;\n\tstruct work_struct work;\n};\n\nstruct cryptd_queue {\n\tstruct cryptd_cpu_queue __percpu *cpu_queue;\n};\n\nstruct cryptd_instance_ctx {\n\tstruct crypto_spawn spawn;\n\tstruct cryptd_queue *queue;\n};\n\nstruct hashd_instance_ctx {\n\tstruct crypto_shash_spawn spawn;\n\tstruct cryptd_queue *queue;\n};\n\nstruct aead_instance_ctx {\n\tstruct crypto_aead_spawn aead_spawn;\n\tstruct cryptd_queue *queue;\n};\n\nstruct cryptd_blkcipher_ctx {\n\tstruct crypto_blkcipher *child;\n};\n\nstruct cryptd_blkcipher_request_ctx {\n\tcrypto_completion_t complete;\n};\n\nstruct cryptd_hash_ctx {\n\tstruct crypto_shash *child;\n};\n\nstruct cryptd_hash_request_ctx {\n\tcrypto_completion_t complete;\n\tstruct shash_desc desc;\n};\n\nstruct cryptd_aead_ctx {\n\tstruct crypto_aead *child;\n};\n\nstruct cryptd_aead_request_ctx {\n\tcrypto_completion_t complete;\n};\n\nstatic void cryptd_queue_worker(struct work_struct *work);\n\nstatic int cryptd_init_queue(struct cryptd_queue *queue,\n\t\t\t     unsigned int max_cpu_qlen)\n{\n\tint cpu;\n\tstruct cryptd_cpu_queue *cpu_queue;\n\n\tqueue->cpu_queue = alloc_percpu(struct cryptd_cpu_queue);\n\tif (!queue->cpu_queue)\n\t\treturn -ENOMEM;\n\tfor_each_possible_cpu(cpu) {\n\t\tcpu_queue = per_cpu_ptr(queue->cpu_queue, cpu);\n\t\tcrypto_init_queue(&cpu_queue->queue, max_cpu_qlen);\n\t\tINIT_WORK(&cpu_queue->work, cryptd_queue_worker);\n\t}\n\treturn 0;\n}\n\nstatic void cryptd_fini_queue(struct cryptd_queue *queue)\n{\n\tint cpu;\n\tstruct cryptd_cpu_queue *cpu_queue;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tcpu_queue = per_cpu_ptr(queue->cpu_queue, cpu);\n\t\tBUG_ON(cpu_queue->queue.qlen);\n\t}\n\tfree_percpu(queue->cpu_queue);\n}\n\nstatic int cryptd_enqueue_request(struct cryptd_queue *queue,\n\t\t\t\t  struct crypto_async_request *request)\n{\n\tint cpu, err;\n\tstruct cryptd_cpu_queue *cpu_queue;\n\n\tcpu = get_cpu();\n\tcpu_queue = this_cpu_ptr(queue->cpu_queue);\n\terr = crypto_enqueue_request(&cpu_queue->queue, request);\n\tqueue_work_on(cpu, kcrypto_wq, &cpu_queue->work);\n\tput_cpu();\n\n\treturn err;\n}\n\n/* Called in workqueue context, do one real cryption work (via\n * req->complete) and reschedule itself if there are more work to\n * do. */\nstatic void cryptd_queue_worker(struct work_struct *work)\n{\n\tstruct cryptd_cpu_queue *cpu_queue;\n\tstruct crypto_async_request *req, *backlog;\n\n\tcpu_queue = container_of(work, struct cryptd_cpu_queue, work);\n\t/*\n\t * Only handle one request at a time to avoid hogging crypto workqueue.\n\t * preempt_disable/enable is used to prevent being preempted by\n\t * cryptd_enqueue_request(). local_bh_disable/enable is used to prevent\n\t * cryptd_enqueue_request() being accessed from software interrupts.\n\t */\n\tlocal_bh_disable();\n\tpreempt_disable();\n\tbacklog = crypto_get_backlog(&cpu_queue->queue);\n\treq = crypto_dequeue_request(&cpu_queue->queue);\n\tpreempt_enable();\n\tlocal_bh_enable();\n\n\tif (!req)\n\t\treturn;\n\n\tif (backlog)\n\t\tbacklog->complete(backlog, -EINPROGRESS);\n\treq->complete(req, 0);\n\n\tif (cpu_queue->queue.qlen)\n\t\tqueue_work(kcrypto_wq, &cpu_queue->work);\n}\n\nstatic inline struct cryptd_queue *cryptd_get_queue(struct crypto_tfm *tfm)\n{\n\tstruct crypto_instance *inst = crypto_tfm_alg_instance(tfm);\n\tstruct cryptd_instance_ctx *ictx = crypto_instance_ctx(inst);\n\treturn ictx->queue;\n}\n\nstatic int cryptd_blkcipher_setkey(struct crypto_ablkcipher *parent,\n\t\t\t\t   const u8 *key, unsigned int keylen)\n{\n\tstruct cryptd_blkcipher_ctx *ctx = crypto_ablkcipher_ctx(parent);\n\tstruct crypto_blkcipher *child = ctx->child;\n\tint err;\n\n\tcrypto_blkcipher_clear_flags(child, CRYPTO_TFM_REQ_MASK);\n\tcrypto_blkcipher_set_flags(child, crypto_ablkcipher_get_flags(parent) &\n\t\t\t\t\t  CRYPTO_TFM_REQ_MASK);\n\terr = crypto_blkcipher_setkey(child, key, keylen);\n\tcrypto_ablkcipher_set_flags(parent, crypto_blkcipher_get_flags(child) &\n\t\t\t\t\t    CRYPTO_TFM_RES_MASK);\n\treturn err;\n}\n\nstatic void cryptd_blkcipher_crypt(struct ablkcipher_request *req,\n\t\t\t\t   struct crypto_blkcipher *child,\n\t\t\t\t   int err,\n\t\t\t\t   int (*crypt)(struct blkcipher_desc *desc,\n\t\t\t\t\t\tstruct scatterlist *dst,\n\t\t\t\t\t\tstruct scatterlist *src,\n\t\t\t\t\t\tunsigned int len))\n{\n\tstruct cryptd_blkcipher_request_ctx *rctx;\n\tstruct blkcipher_desc desc;\n\n\trctx = ablkcipher_request_ctx(req);\n\n\tif (unlikely(err == -EINPROGRESS))\n\t\tgoto out;\n\n\tdesc.tfm = child;\n\tdesc.info = req->info;\n\tdesc.flags = CRYPTO_TFM_REQ_MAY_SLEEP;\n\n\terr = crypt(&desc, req->dst, req->src, req->nbytes);\n\n\treq->base.complete = rctx->complete;\n\nout:\n\tlocal_bh_disable();\n\trctx->complete(&req->base, err);\n\tlocal_bh_enable();\n}\n\nstatic void cryptd_blkcipher_encrypt(struct crypto_async_request *req, int err)\n{\n\tstruct cryptd_blkcipher_ctx *ctx = crypto_tfm_ctx(req->tfm);\n\tstruct crypto_blkcipher *child = ctx->child;\n\n\tcryptd_blkcipher_crypt(ablkcipher_request_cast(req), child, err,\n\t\t\t       crypto_blkcipher_crt(child)->encrypt);\n}\n\nstatic void cryptd_blkcipher_decrypt(struct crypto_async_request *req, int err)\n{\n\tstruct cryptd_blkcipher_ctx *ctx = crypto_tfm_ctx(req->tfm);\n\tstruct crypto_blkcipher *child = ctx->child;\n\n\tcryptd_blkcipher_crypt(ablkcipher_request_cast(req), child, err,\n\t\t\t       crypto_blkcipher_crt(child)->decrypt);\n}\n\nstatic int cryptd_blkcipher_enqueue(struct ablkcipher_request *req,\n\t\t\t\t    crypto_completion_t compl)\n{\n\tstruct cryptd_blkcipher_request_ctx *rctx = ablkcipher_request_ctx(req);\n\tstruct crypto_ablkcipher *tfm = crypto_ablkcipher_reqtfm(req);\n\tstruct cryptd_queue *queue;\n\n\tqueue = cryptd_get_queue(crypto_ablkcipher_tfm(tfm));\n\trctx->complete = req->base.complete;\n\treq->base.complete = compl;\n\n\treturn cryptd_enqueue_request(queue, &req->base);\n}\n\nstatic int cryptd_blkcipher_encrypt_enqueue(struct ablkcipher_request *req)\n{\n\treturn cryptd_blkcipher_enqueue(req, cryptd_blkcipher_encrypt);\n}\n\nstatic int cryptd_blkcipher_decrypt_enqueue(struct ablkcipher_request *req)\n{\n\treturn cryptd_blkcipher_enqueue(req, cryptd_blkcipher_decrypt);\n}\n\nstatic int cryptd_blkcipher_init_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_instance *inst = crypto_tfm_alg_instance(tfm);\n\tstruct cryptd_instance_ctx *ictx = crypto_instance_ctx(inst);\n\tstruct crypto_spawn *spawn = &ictx->spawn;\n\tstruct cryptd_blkcipher_ctx *ctx = crypto_tfm_ctx(tfm);\n\tstruct crypto_blkcipher *cipher;\n\n\tcipher = crypto_spawn_blkcipher(spawn);\n\tif (IS_ERR(cipher))\n\t\treturn PTR_ERR(cipher);\n\n\tctx->child = cipher;\n\ttfm->crt_ablkcipher.reqsize =\n\t\tsizeof(struct cryptd_blkcipher_request_ctx);\n\treturn 0;\n}\n\nstatic void cryptd_blkcipher_exit_tfm(struct crypto_tfm *tfm)\n{\n\tstruct cryptd_blkcipher_ctx *ctx = crypto_tfm_ctx(tfm);\n\n\tcrypto_free_blkcipher(ctx->child);\n}\n\nstatic void *cryptd_alloc_instance(struct crypto_alg *alg, unsigned int head,\n\t\t\t\t   unsigned int tail)\n{\n\tchar *p;\n\tstruct crypto_instance *inst;\n\tint err;\n\n\tp = kzalloc(head + sizeof(*inst) + tail, GFP_KERNEL);\n\tif (!p)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tinst = (void *)(p + head);\n\n\terr = -ENAMETOOLONG;\n\tif (snprintf(inst->alg.cra_driver_name, CRYPTO_MAX_ALG_NAME,\n\t\t     \"cryptd(%s)\", alg->cra_driver_name) >= CRYPTO_MAX_ALG_NAME)\n\t\tgoto out_free_inst;\n\n\tmemcpy(inst->alg.cra_name, alg->cra_name, CRYPTO_MAX_ALG_NAME);\n\n\tinst->alg.cra_priority = alg->cra_priority + 50;\n\tinst->alg.cra_blocksize = alg->cra_blocksize;\n\tinst->alg.cra_alignmask = alg->cra_alignmask;\n\nout:\n\treturn p;\n\nout_free_inst:\n\tkfree(p);\n\tp = ERR_PTR(err);\n\tgoto out;\n}\n\nstatic int cryptd_create_blkcipher(struct crypto_template *tmpl,\n\t\t\t\t   struct rtattr **tb,\n\t\t\t\t   struct cryptd_queue *queue)\n{\n\tstruct cryptd_instance_ctx *ctx;\n\tstruct crypto_instance *inst;\n\tstruct crypto_alg *alg;\n\tint err;\n\n\talg = crypto_get_attr_alg(tb, CRYPTO_ALG_TYPE_BLKCIPHER,\n\t\t\t\t  CRYPTO_ALG_TYPE_MASK);\n\tif (IS_ERR(alg))\n\t\treturn PTR_ERR(alg);\n\n\tinst = cryptd_alloc_instance(alg, 0, sizeof(*ctx));\n\terr = PTR_ERR(inst);\n\tif (IS_ERR(inst))\n\t\tgoto out_put_alg;\n\n\tctx = crypto_instance_ctx(inst);\n\tctx->queue = queue;\n\n\terr = crypto_init_spawn(&ctx->spawn, alg, inst,\n\t\t\t\tCRYPTO_ALG_TYPE_MASK | CRYPTO_ALG_ASYNC);\n\tif (err)\n\t\tgoto out_free_inst;\n\n\tinst->alg.cra_flags = CRYPTO_ALG_TYPE_ABLKCIPHER | CRYPTO_ALG_ASYNC;\n\tinst->alg.cra_type = &crypto_ablkcipher_type;\n\n\tinst->alg.cra_ablkcipher.ivsize = alg->cra_blkcipher.ivsize;\n\tinst->alg.cra_ablkcipher.min_keysize = alg->cra_blkcipher.min_keysize;\n\tinst->alg.cra_ablkcipher.max_keysize = alg->cra_blkcipher.max_keysize;\n\n\tinst->alg.cra_ablkcipher.geniv = alg->cra_blkcipher.geniv;\n\n\tinst->alg.cra_ctxsize = sizeof(struct cryptd_blkcipher_ctx);\n\n\tinst->alg.cra_init = cryptd_blkcipher_init_tfm;\n\tinst->alg.cra_exit = cryptd_blkcipher_exit_tfm;\n\n\tinst->alg.cra_ablkcipher.setkey = cryptd_blkcipher_setkey;\n\tinst->alg.cra_ablkcipher.encrypt = cryptd_blkcipher_encrypt_enqueue;\n\tinst->alg.cra_ablkcipher.decrypt = cryptd_blkcipher_decrypt_enqueue;\n\n\terr = crypto_register_instance(tmpl, inst);\n\tif (err) {\n\t\tcrypto_drop_spawn(&ctx->spawn);\nout_free_inst:\n\t\tkfree(inst);\n\t}\n\nout_put_alg:\n\tcrypto_mod_put(alg);\n\treturn err;\n}\n\nstatic int cryptd_hash_init_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_instance *inst = crypto_tfm_alg_instance(tfm);\n\tstruct hashd_instance_ctx *ictx = crypto_instance_ctx(inst);\n\tstruct crypto_shash_spawn *spawn = &ictx->spawn;\n\tstruct cryptd_hash_ctx *ctx = crypto_tfm_ctx(tfm);\n\tstruct crypto_shash *hash;\n\n\thash = crypto_spawn_shash(spawn);\n\tif (IS_ERR(hash))\n\t\treturn PTR_ERR(hash);\n\n\tctx->child = hash;\n\tcrypto_ahash_set_reqsize(__crypto_ahash_cast(tfm),\n\t\t\t\t sizeof(struct cryptd_hash_request_ctx) +\n\t\t\t\t crypto_shash_descsize(hash));\n\treturn 0;\n}\n\nstatic void cryptd_hash_exit_tfm(struct crypto_tfm *tfm)\n{\n\tstruct cryptd_hash_ctx *ctx = crypto_tfm_ctx(tfm);\n\n\tcrypto_free_shash(ctx->child);\n}\n\nstatic int cryptd_hash_setkey(struct crypto_ahash *parent,\n\t\t\t\t   const u8 *key, unsigned int keylen)\n{\n\tstruct cryptd_hash_ctx *ctx   = crypto_ahash_ctx(parent);\n\tstruct crypto_shash *child = ctx->child;\n\tint err;\n\n\tcrypto_shash_clear_flags(child, CRYPTO_TFM_REQ_MASK);\n\tcrypto_shash_set_flags(child, crypto_ahash_get_flags(parent) &\n\t\t\t\t      CRYPTO_TFM_REQ_MASK);\n\terr = crypto_shash_setkey(child, key, keylen);\n\tcrypto_ahash_set_flags(parent, crypto_shash_get_flags(child) &\n\t\t\t\t       CRYPTO_TFM_RES_MASK);\n\treturn err;\n}\n\nstatic int cryptd_hash_enqueue(struct ahash_request *req,\n\t\t\t\tcrypto_completion_t compl)\n{\n\tstruct cryptd_hash_request_ctx *rctx = ahash_request_ctx(req);\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct cryptd_queue *queue =\n\t\tcryptd_get_queue(crypto_ahash_tfm(tfm));\n\n\trctx->complete = req->base.complete;\n\treq->base.complete = compl;\n\n\treturn cryptd_enqueue_request(queue, &req->base);\n}\n\nstatic void cryptd_hash_init(struct crypto_async_request *req_async, int err)\n{\n\tstruct cryptd_hash_ctx *ctx = crypto_tfm_ctx(req_async->tfm);\n\tstruct crypto_shash *child = ctx->child;\n\tstruct ahash_request *req = ahash_request_cast(req_async);\n\tstruct cryptd_hash_request_ctx *rctx = ahash_request_ctx(req);\n\tstruct shash_desc *desc = &rctx->desc;\n\n\tif (unlikely(err == -EINPROGRESS))\n\t\tgoto out;\n\n\tdesc->tfm = child;\n\tdesc->flags = CRYPTO_TFM_REQ_MAY_SLEEP;\n\n\terr = crypto_shash_init(desc);\n\n\treq->base.complete = rctx->complete;\n\nout:\n\tlocal_bh_disable();\n\trctx->complete(&req->base, err);\n\tlocal_bh_enable();\n}\n\nstatic int cryptd_hash_init_enqueue(struct ahash_request *req)\n{\n\treturn cryptd_hash_enqueue(req, cryptd_hash_init);\n}\n\nstatic void cryptd_hash_update(struct crypto_async_request *req_async, int err)\n{\n\tstruct ahash_request *req = ahash_request_cast(req_async);\n\tstruct cryptd_hash_request_ctx *rctx;\n\n\trctx = ahash_request_ctx(req);\n\n\tif (unlikely(err == -EINPROGRESS))\n\t\tgoto out;\n\n\terr = shash_ahash_update(req, &rctx->desc);\n\n\treq->base.complete = rctx->complete;\n\nout:\n\tlocal_bh_disable();\n\trctx->complete(&req->base, err);\n\tlocal_bh_enable();\n}\n\nstatic int cryptd_hash_update_enqueue(struct ahash_request *req)\n{\n\treturn cryptd_hash_enqueue(req, cryptd_hash_update);\n}\n\nstatic void cryptd_hash_final(struct crypto_async_request *req_async, int err)\n{\n\tstruct ahash_request *req = ahash_request_cast(req_async);\n\tstruct cryptd_hash_request_ctx *rctx = ahash_request_ctx(req);\n\n\tif (unlikely(err == -EINPROGRESS))\n\t\tgoto out;\n\n\terr = crypto_shash_final(&rctx->desc, req->result);\n\n\treq->base.complete = rctx->complete;\n\nout:\n\tlocal_bh_disable();\n\trctx->complete(&req->base, err);\n\tlocal_bh_enable();\n}\n\nstatic int cryptd_hash_final_enqueue(struct ahash_request *req)\n{\n\treturn cryptd_hash_enqueue(req, cryptd_hash_final);\n}\n\nstatic void cryptd_hash_finup(struct crypto_async_request *req_async, int err)\n{\n\tstruct ahash_request *req = ahash_request_cast(req_async);\n\tstruct cryptd_hash_request_ctx *rctx = ahash_request_ctx(req);\n\n\tif (unlikely(err == -EINPROGRESS))\n\t\tgoto out;\n\n\terr = shash_ahash_finup(req, &rctx->desc);\n\n\treq->base.complete = rctx->complete;\n\nout:\n\tlocal_bh_disable();\n\trctx->complete(&req->base, err);\n\tlocal_bh_enable();\n}\n\nstatic int cryptd_hash_finup_enqueue(struct ahash_request *req)\n{\n\treturn cryptd_hash_enqueue(req, cryptd_hash_finup);\n}\n\nstatic void cryptd_hash_digest(struct crypto_async_request *req_async, int err)\n{\n\tstruct cryptd_hash_ctx *ctx = crypto_tfm_ctx(req_async->tfm);\n\tstruct crypto_shash *child = ctx->child;\n\tstruct ahash_request *req = ahash_request_cast(req_async);\n\tstruct cryptd_hash_request_ctx *rctx = ahash_request_ctx(req);\n\tstruct shash_desc *desc = &rctx->desc;\n\n\tif (unlikely(err == -EINPROGRESS))\n\t\tgoto out;\n\n\tdesc->tfm = child;\n\tdesc->flags = CRYPTO_TFM_REQ_MAY_SLEEP;\n\n\terr = shash_ahash_digest(req, desc);\n\n\treq->base.complete = rctx->complete;\n\nout:\n\tlocal_bh_disable();\n\trctx->complete(&req->base, err);\n\tlocal_bh_enable();\n}\n\nstatic int cryptd_hash_digest_enqueue(struct ahash_request *req)\n{\n\treturn cryptd_hash_enqueue(req, cryptd_hash_digest);\n}\n\nstatic int cryptd_hash_export(struct ahash_request *req, void *out)\n{\n\tstruct cryptd_hash_request_ctx *rctx = ahash_request_ctx(req);\n\n\treturn crypto_shash_export(&rctx->desc, out);\n}\n\nstatic int cryptd_hash_import(struct ahash_request *req, const void *in)\n{\n\tstruct cryptd_hash_request_ctx *rctx = ahash_request_ctx(req);\n\n\treturn crypto_shash_import(&rctx->desc, in);\n}\n\nstatic int cryptd_create_hash(struct crypto_template *tmpl, struct rtattr **tb,\n\t\t\t      struct cryptd_queue *queue)\n{\n\tstruct hashd_instance_ctx *ctx;\n\tstruct ahash_instance *inst;\n\tstruct shash_alg *salg;\n\tstruct crypto_alg *alg;\n\tint err;\n\n\tsalg = shash_attr_alg(tb[1], 0, 0);\n\tif (IS_ERR(salg))\n\t\treturn PTR_ERR(salg);\n\n\talg = &salg->base;\n\tinst = cryptd_alloc_instance(alg, ahash_instance_headroom(),\n\t\t\t\t     sizeof(*ctx));\n\terr = PTR_ERR(inst);\n\tif (IS_ERR(inst))\n\t\tgoto out_put_alg;\n\n\tctx = ahash_instance_ctx(inst);\n\tctx->queue = queue;\n\n\terr = crypto_init_shash_spawn(&ctx->spawn, salg,\n\t\t\t\t      ahash_crypto_instance(inst));\n\tif (err)\n\t\tgoto out_free_inst;\n\n\tinst->alg.halg.base.cra_flags = CRYPTO_ALG_ASYNC;\n\n\tinst->alg.halg.digestsize = salg->digestsize;\n\tinst->alg.halg.base.cra_ctxsize = sizeof(struct cryptd_hash_ctx);\n\n\tinst->alg.halg.base.cra_init = cryptd_hash_init_tfm;\n\tinst->alg.halg.base.cra_exit = cryptd_hash_exit_tfm;\n\n\tinst->alg.init   = cryptd_hash_init_enqueue;\n\tinst->alg.update = cryptd_hash_update_enqueue;\n\tinst->alg.final  = cryptd_hash_final_enqueue;\n\tinst->alg.finup  = cryptd_hash_finup_enqueue;\n\tinst->alg.export = cryptd_hash_export;\n\tinst->alg.import = cryptd_hash_import;\n\tinst->alg.setkey = cryptd_hash_setkey;\n\tinst->alg.digest = cryptd_hash_digest_enqueue;\n\n\terr = ahash_register_instance(tmpl, inst);\n\tif (err) {\n\t\tcrypto_drop_shash(&ctx->spawn);\nout_free_inst:\n\t\tkfree(inst);\n\t}\n\nout_put_alg:\n\tcrypto_mod_put(alg);\n\treturn err;\n}\n\nstatic void cryptd_aead_crypt(struct aead_request *req,\n\t\t\tstruct crypto_aead *child,\n\t\t\tint err,\n\t\t\tint (*crypt)(struct aead_request *req))\n{\n\tstruct cryptd_aead_request_ctx *rctx;\n\trctx = aead_request_ctx(req);\n\n\tif (unlikely(err == -EINPROGRESS))\n\t\tgoto out;\n\taead_request_set_tfm(req, child);\n\terr = crypt( req );\n\treq->base.complete = rctx->complete;\nout:\n\tlocal_bh_disable();\n\trctx->complete(&req->base, err);\n\tlocal_bh_enable();\n}\n\nstatic void cryptd_aead_encrypt(struct crypto_async_request *areq, int err)\n{\n\tstruct cryptd_aead_ctx *ctx = crypto_tfm_ctx(areq->tfm);\n\tstruct crypto_aead *child = ctx->child;\n\tstruct aead_request *req;\n\n\treq = container_of(areq, struct aead_request, base);\n\tcryptd_aead_crypt(req, child, err, crypto_aead_crt(child)->encrypt);\n}\n\nstatic void cryptd_aead_decrypt(struct crypto_async_request *areq, int err)\n{\n\tstruct cryptd_aead_ctx *ctx = crypto_tfm_ctx(areq->tfm);\n\tstruct crypto_aead *child = ctx->child;\n\tstruct aead_request *req;\n\n\treq = container_of(areq, struct aead_request, base);\n\tcryptd_aead_crypt(req, child, err, crypto_aead_crt(child)->decrypt);\n}\n\nstatic int cryptd_aead_enqueue(struct aead_request *req,\n\t\t\t\t    crypto_completion_t compl)\n{\n\tstruct cryptd_aead_request_ctx *rctx = aead_request_ctx(req);\n\tstruct crypto_aead *tfm = crypto_aead_reqtfm(req);\n\tstruct cryptd_queue *queue = cryptd_get_queue(crypto_aead_tfm(tfm));\n\n\trctx->complete = req->base.complete;\n\treq->base.complete = compl;\n\treturn cryptd_enqueue_request(queue, &req->base);\n}\n\nstatic int cryptd_aead_encrypt_enqueue(struct aead_request *req)\n{\n\treturn cryptd_aead_enqueue(req, cryptd_aead_encrypt );\n}\n\nstatic int cryptd_aead_decrypt_enqueue(struct aead_request *req)\n{\n\treturn cryptd_aead_enqueue(req, cryptd_aead_decrypt );\n}\n\nstatic int cryptd_aead_init_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_instance *inst = crypto_tfm_alg_instance(tfm);\n\tstruct aead_instance_ctx *ictx = crypto_instance_ctx(inst);\n\tstruct crypto_aead_spawn *spawn = &ictx->aead_spawn;\n\tstruct cryptd_aead_ctx *ctx = crypto_tfm_ctx(tfm);\n\tstruct crypto_aead *cipher;\n\n\tcipher = crypto_spawn_aead(spawn);\n\tif (IS_ERR(cipher))\n\t\treturn PTR_ERR(cipher);\n\n\tcrypto_aead_set_flags(cipher, CRYPTO_TFM_REQ_MAY_SLEEP);\n\tctx->child = cipher;\n\ttfm->crt_aead.reqsize = sizeof(struct cryptd_aead_request_ctx);\n\treturn 0;\n}\n\nstatic void cryptd_aead_exit_tfm(struct crypto_tfm *tfm)\n{\n\tstruct cryptd_aead_ctx *ctx = crypto_tfm_ctx(tfm);\n\tcrypto_free_aead(ctx->child);\n}\n\nstatic int cryptd_create_aead(struct crypto_template *tmpl,\n\t\t              struct rtattr **tb,\n\t\t\t      struct cryptd_queue *queue)\n{\n\tstruct aead_instance_ctx *ctx;\n\tstruct crypto_instance *inst;\n\tstruct crypto_alg *alg;\n\tint err;\n\n\talg = crypto_get_attr_alg(tb, CRYPTO_ALG_TYPE_AEAD,\n\t\t\t\tCRYPTO_ALG_TYPE_MASK);\n        if (IS_ERR(alg))\n\t\treturn PTR_ERR(alg);\n\n\tinst = cryptd_alloc_instance(alg, 0, sizeof(*ctx));\n\terr = PTR_ERR(inst);\n\tif (IS_ERR(inst))\n\t\tgoto out_put_alg;\n\n\tctx = crypto_instance_ctx(inst);\n\tctx->queue = queue;\n\n\terr = crypto_init_spawn(&ctx->aead_spawn.base, alg, inst,\n\t\t\tCRYPTO_ALG_TYPE_MASK | CRYPTO_ALG_ASYNC);\n\tif (err)\n\t\tgoto out_free_inst;\n\n\tinst->alg.cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC;\n\tinst->alg.cra_type = alg->cra_type;\n\tinst->alg.cra_ctxsize = sizeof(struct cryptd_aead_ctx);\n\tinst->alg.cra_init = cryptd_aead_init_tfm;\n\tinst->alg.cra_exit = cryptd_aead_exit_tfm;\n\tinst->alg.cra_aead.setkey      = alg->cra_aead.setkey;\n\tinst->alg.cra_aead.setauthsize = alg->cra_aead.setauthsize;\n\tinst->alg.cra_aead.geniv       = alg->cra_aead.geniv;\n\tinst->alg.cra_aead.ivsize      = alg->cra_aead.ivsize;\n\tinst->alg.cra_aead.maxauthsize = alg->cra_aead.maxauthsize;\n\tinst->alg.cra_aead.encrypt     = cryptd_aead_encrypt_enqueue;\n\tinst->alg.cra_aead.decrypt     = cryptd_aead_decrypt_enqueue;\n\tinst->alg.cra_aead.givencrypt  = alg->cra_aead.givencrypt;\n\tinst->alg.cra_aead.givdecrypt  = alg->cra_aead.givdecrypt;\n\n\terr = crypto_register_instance(tmpl, inst);\n\tif (err) {\n\t\tcrypto_drop_spawn(&ctx->aead_spawn.base);\nout_free_inst:\n\t\tkfree(inst);\n\t}\nout_put_alg:\n\tcrypto_mod_put(alg);\n\treturn err;\n}\n\nstatic struct cryptd_queue queue;\n\nstatic int cryptd_create(struct crypto_template *tmpl, struct rtattr **tb)\n{\n\tstruct crypto_attr_type *algt;\n\n\talgt = crypto_get_attr_type(tb);\n\tif (IS_ERR(algt))\n\t\treturn PTR_ERR(algt);\n\n\tswitch (algt->type & algt->mask & CRYPTO_ALG_TYPE_MASK) {\n\tcase CRYPTO_ALG_TYPE_BLKCIPHER:\n\t\treturn cryptd_create_blkcipher(tmpl, tb, &queue);\n\tcase CRYPTO_ALG_TYPE_DIGEST:\n\t\treturn cryptd_create_hash(tmpl, tb, &queue);\n\tcase CRYPTO_ALG_TYPE_AEAD:\n\t\treturn cryptd_create_aead(tmpl, tb, &queue);\n\t}\n\n\treturn -EINVAL;\n}\n\nstatic void cryptd_free(struct crypto_instance *inst)\n{\n\tstruct cryptd_instance_ctx *ctx = crypto_instance_ctx(inst);\n\tstruct hashd_instance_ctx *hctx = crypto_instance_ctx(inst);\n\tstruct aead_instance_ctx *aead_ctx = crypto_instance_ctx(inst);\n\n\tswitch (inst->alg.cra_flags & CRYPTO_ALG_TYPE_MASK) {\n\tcase CRYPTO_ALG_TYPE_AHASH:\n\t\tcrypto_drop_shash(&hctx->spawn);\n\t\tkfree(ahash_instance(inst));\n\t\treturn;\n\tcase CRYPTO_ALG_TYPE_AEAD:\n\t\tcrypto_drop_spawn(&aead_ctx->aead_spawn.base);\n\t\tkfree(inst);\n\t\treturn;\n\tdefault:\n\t\tcrypto_drop_spawn(&ctx->spawn);\n\t\tkfree(inst);\n\t}\n}\n\nstatic struct crypto_template cryptd_tmpl = {\n\t.name = \"cryptd\",\n\t.create = cryptd_create,\n\t.free = cryptd_free,\n\t.module = THIS_MODULE,\n};\n\nstruct cryptd_ablkcipher *cryptd_alloc_ablkcipher(const char *alg_name,\n\t\t\t\t\t\t  u32 type, u32 mask)\n{\n\tchar cryptd_alg_name[CRYPTO_MAX_ALG_NAME];\n\tstruct crypto_tfm *tfm;\n\n\tif (snprintf(cryptd_alg_name, CRYPTO_MAX_ALG_NAME,\n\t\t     \"cryptd(%s)\", alg_name) >= CRYPTO_MAX_ALG_NAME)\n\t\treturn ERR_PTR(-EINVAL);\n\ttype &= ~(CRYPTO_ALG_TYPE_MASK | CRYPTO_ALG_GENIV);\n\ttype |= CRYPTO_ALG_TYPE_BLKCIPHER;\n\tmask &= ~CRYPTO_ALG_TYPE_MASK;\n\tmask |= (CRYPTO_ALG_GENIV | CRYPTO_ALG_TYPE_BLKCIPHER_MASK);\n\ttfm = crypto_alloc_base(cryptd_alg_name, type, mask);\n\tif (IS_ERR(tfm))\n\t\treturn ERR_CAST(tfm);\n\tif (tfm->__crt_alg->cra_module != THIS_MODULE) {\n\t\tcrypto_free_tfm(tfm);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\treturn __cryptd_ablkcipher_cast(__crypto_ablkcipher_cast(tfm));\n}\nEXPORT_SYMBOL_GPL(cryptd_alloc_ablkcipher);\n\nstruct crypto_blkcipher *cryptd_ablkcipher_child(struct cryptd_ablkcipher *tfm)\n{\n\tstruct cryptd_blkcipher_ctx *ctx = crypto_ablkcipher_ctx(&tfm->base);\n\treturn ctx->child;\n}\nEXPORT_SYMBOL_GPL(cryptd_ablkcipher_child);\n\nvoid cryptd_free_ablkcipher(struct cryptd_ablkcipher *tfm)\n{\n\tcrypto_free_ablkcipher(&tfm->base);\n}\nEXPORT_SYMBOL_GPL(cryptd_free_ablkcipher);\n\nstruct cryptd_ahash *cryptd_alloc_ahash(const char *alg_name,\n\t\t\t\t\tu32 type, u32 mask)\n{\n\tchar cryptd_alg_name[CRYPTO_MAX_ALG_NAME];\n\tstruct crypto_ahash *tfm;\n\n\tif (snprintf(cryptd_alg_name, CRYPTO_MAX_ALG_NAME,\n\t\t     \"cryptd(%s)\", alg_name) >= CRYPTO_MAX_ALG_NAME)\n\t\treturn ERR_PTR(-EINVAL);\n\ttfm = crypto_alloc_ahash(cryptd_alg_name, type, mask);\n\tif (IS_ERR(tfm))\n\t\treturn ERR_CAST(tfm);\n\tif (tfm->base.__crt_alg->cra_module != THIS_MODULE) {\n\t\tcrypto_free_ahash(tfm);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\treturn __cryptd_ahash_cast(tfm);\n}\nEXPORT_SYMBOL_GPL(cryptd_alloc_ahash);\n\nstruct crypto_shash *cryptd_ahash_child(struct cryptd_ahash *tfm)\n{\n\tstruct cryptd_hash_ctx *ctx = crypto_ahash_ctx(&tfm->base);\n\n\treturn ctx->child;\n}\nEXPORT_SYMBOL_GPL(cryptd_ahash_child);\n\nstruct shash_desc *cryptd_shash_desc(struct ahash_request *req)\n{\n\tstruct cryptd_hash_request_ctx *rctx = ahash_request_ctx(req);\n\treturn &rctx->desc;\n}\nEXPORT_SYMBOL_GPL(cryptd_shash_desc);\n\nvoid cryptd_free_ahash(struct cryptd_ahash *tfm)\n{\n\tcrypto_free_ahash(&tfm->base);\n}\nEXPORT_SYMBOL_GPL(cryptd_free_ahash);\n\nstruct cryptd_aead *cryptd_alloc_aead(const char *alg_name,\n\t\t\t\t\t\t  u32 type, u32 mask)\n{\n\tchar cryptd_alg_name[CRYPTO_MAX_ALG_NAME];\n\tstruct crypto_aead *tfm;\n\n\tif (snprintf(cryptd_alg_name, CRYPTO_MAX_ALG_NAME,\n\t\t     \"cryptd(%s)\", alg_name) >= CRYPTO_MAX_ALG_NAME)\n\t\treturn ERR_PTR(-EINVAL);\n\ttfm = crypto_alloc_aead(cryptd_alg_name, type, mask);\n\tif (IS_ERR(tfm))\n\t\treturn ERR_CAST(tfm);\n\tif (tfm->base.__crt_alg->cra_module != THIS_MODULE) {\n\t\tcrypto_free_aead(tfm);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\treturn __cryptd_aead_cast(tfm);\n}\nEXPORT_SYMBOL_GPL(cryptd_alloc_aead);\n\nstruct crypto_aead *cryptd_aead_child(struct cryptd_aead *tfm)\n{\n\tstruct cryptd_aead_ctx *ctx;\n\tctx = crypto_aead_ctx(&tfm->base);\n\treturn ctx->child;\n}\nEXPORT_SYMBOL_GPL(cryptd_aead_child);\n\nvoid cryptd_free_aead(struct cryptd_aead *tfm)\n{\n\tcrypto_free_aead(&tfm->base);\n}\nEXPORT_SYMBOL_GPL(cryptd_free_aead);\n\nstatic int __init cryptd_init(void)\n{\n\tint err;\n\n\terr = cryptd_init_queue(&queue, CRYPTD_MAX_CPU_QLEN);\n\tif (err)\n\t\treturn err;\n\n\terr = crypto_register_template(&cryptd_tmpl);\n\tif (err)\n\t\tcryptd_fini_queue(&queue);\n\n\treturn err;\n}\n\nstatic void __exit cryptd_exit(void)\n{\n\tcryptd_fini_queue(&queue);\n\tcrypto_unregister_template(&cryptd_tmpl);\n}\n\nsubsys_initcall(cryptd_init);\nmodule_exit(cryptd_exit);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"Software async crypto daemon\");\n", "/*\n * CTR: Counter mode\n *\n * (C) Copyright IBM Corp. 2007 - Joy Latten <latten@us.ibm.com>\n *\n * This program is free software; you can redistribute it and/or modify it\n * under the terms of the GNU General Public License as published by the Free\n * Software Foundation; either version 2 of the License, or (at your option)\n * any later version.\n *\n */\n\n#include <crypto/algapi.h>\n#include <crypto/ctr.h>\n#include <crypto/internal/skcipher.h>\n#include <linux/err.h>\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/random.h>\n#include <linux/scatterlist.h>\n#include <linux/slab.h>\n\nstruct crypto_ctr_ctx {\n\tstruct crypto_cipher *child;\n};\n\nstruct crypto_rfc3686_ctx {\n\tstruct crypto_ablkcipher *child;\n\tu8 nonce[CTR_RFC3686_NONCE_SIZE];\n};\n\nstruct crypto_rfc3686_req_ctx {\n\tu8 iv[CTR_RFC3686_BLOCK_SIZE];\n\tstruct ablkcipher_request subreq CRYPTO_MINALIGN_ATTR;\n};\n\nstatic int crypto_ctr_setkey(struct crypto_tfm *parent, const u8 *key,\n\t\t\t     unsigned int keylen)\n{\n\tstruct crypto_ctr_ctx *ctx = crypto_tfm_ctx(parent);\n\tstruct crypto_cipher *child = ctx->child;\n\tint err;\n\n\tcrypto_cipher_clear_flags(child, CRYPTO_TFM_REQ_MASK);\n\tcrypto_cipher_set_flags(child, crypto_tfm_get_flags(parent) &\n\t\t\t\tCRYPTO_TFM_REQ_MASK);\n\terr = crypto_cipher_setkey(child, key, keylen);\n\tcrypto_tfm_set_flags(parent, crypto_cipher_get_flags(child) &\n\t\t\t     CRYPTO_TFM_RES_MASK);\n\n\treturn err;\n}\n\nstatic void crypto_ctr_crypt_final(struct blkcipher_walk *walk,\n\t\t\t\t   struct crypto_cipher *tfm)\n{\n\tunsigned int bsize = crypto_cipher_blocksize(tfm);\n\tunsigned long alignmask = crypto_cipher_alignmask(tfm);\n\tu8 *ctrblk = walk->iv;\n\tu8 tmp[bsize + alignmask];\n\tu8 *keystream = PTR_ALIGN(tmp + 0, alignmask + 1);\n\tu8 *src = walk->src.virt.addr;\n\tu8 *dst = walk->dst.virt.addr;\n\tunsigned int nbytes = walk->nbytes;\n\n\tcrypto_cipher_encrypt_one(tfm, keystream, ctrblk);\n\tcrypto_xor(keystream, src, nbytes);\n\tmemcpy(dst, keystream, nbytes);\n\n\tcrypto_inc(ctrblk, bsize);\n}\n\nstatic int crypto_ctr_crypt_segment(struct blkcipher_walk *walk,\n\t\t\t\t    struct crypto_cipher *tfm)\n{\n\tvoid (*fn)(struct crypto_tfm *, u8 *, const u8 *) =\n\t\t   crypto_cipher_alg(tfm)->cia_encrypt;\n\tunsigned int bsize = crypto_cipher_blocksize(tfm);\n\tu8 *ctrblk = walk->iv;\n\tu8 *src = walk->src.virt.addr;\n\tu8 *dst = walk->dst.virt.addr;\n\tunsigned int nbytes = walk->nbytes;\n\n\tdo {\n\t\t/* create keystream */\n\t\tfn(crypto_cipher_tfm(tfm), dst, ctrblk);\n\t\tcrypto_xor(dst, src, bsize);\n\n\t\t/* increment counter in counterblock */\n\t\tcrypto_inc(ctrblk, bsize);\n\n\t\tsrc += bsize;\n\t\tdst += bsize;\n\t} while ((nbytes -= bsize) >= bsize);\n\n\treturn nbytes;\n}\n\nstatic int crypto_ctr_crypt_inplace(struct blkcipher_walk *walk,\n\t\t\t\t    struct crypto_cipher *tfm)\n{\n\tvoid (*fn)(struct crypto_tfm *, u8 *, const u8 *) =\n\t\t   crypto_cipher_alg(tfm)->cia_encrypt;\n\tunsigned int bsize = crypto_cipher_blocksize(tfm);\n\tunsigned long alignmask = crypto_cipher_alignmask(tfm);\n\tunsigned int nbytes = walk->nbytes;\n\tu8 *ctrblk = walk->iv;\n\tu8 *src = walk->src.virt.addr;\n\tu8 tmp[bsize + alignmask];\n\tu8 *keystream = PTR_ALIGN(tmp + 0, alignmask + 1);\n\n\tdo {\n\t\t/* create keystream */\n\t\tfn(crypto_cipher_tfm(tfm), keystream, ctrblk);\n\t\tcrypto_xor(src, keystream, bsize);\n\n\t\t/* increment counter in counterblock */\n\t\tcrypto_inc(ctrblk, bsize);\n\n\t\tsrc += bsize;\n\t} while ((nbytes -= bsize) >= bsize);\n\n\treturn nbytes;\n}\n\nstatic int crypto_ctr_crypt(struct blkcipher_desc *desc,\n\t\t\t      struct scatterlist *dst, struct scatterlist *src,\n\t\t\t      unsigned int nbytes)\n{\n\tstruct blkcipher_walk walk;\n\tstruct crypto_blkcipher *tfm = desc->tfm;\n\tstruct crypto_ctr_ctx *ctx = crypto_blkcipher_ctx(tfm);\n\tstruct crypto_cipher *child = ctx->child;\n\tunsigned int bsize = crypto_cipher_blocksize(child);\n\tint err;\n\n\tblkcipher_walk_init(&walk, dst, src, nbytes);\n\terr = blkcipher_walk_virt_block(desc, &walk, bsize);\n\n\twhile (walk.nbytes >= bsize) {\n\t\tif (walk.src.virt.addr == walk.dst.virt.addr)\n\t\t\tnbytes = crypto_ctr_crypt_inplace(&walk, child);\n\t\telse\n\t\t\tnbytes = crypto_ctr_crypt_segment(&walk, child);\n\n\t\terr = blkcipher_walk_done(desc, &walk, nbytes);\n\t}\n\n\tif (walk.nbytes) {\n\t\tcrypto_ctr_crypt_final(&walk, child);\n\t\terr = blkcipher_walk_done(desc, &walk, 0);\n\t}\n\n\treturn err;\n}\n\nstatic int crypto_ctr_init_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_instance *inst = (void *)tfm->__crt_alg;\n\tstruct crypto_spawn *spawn = crypto_instance_ctx(inst);\n\tstruct crypto_ctr_ctx *ctx = crypto_tfm_ctx(tfm);\n\tstruct crypto_cipher *cipher;\n\n\tcipher = crypto_spawn_cipher(spawn);\n\tif (IS_ERR(cipher))\n\t\treturn PTR_ERR(cipher);\n\n\tctx->child = cipher;\n\n\treturn 0;\n}\n\nstatic void crypto_ctr_exit_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_ctr_ctx *ctx = crypto_tfm_ctx(tfm);\n\n\tcrypto_free_cipher(ctx->child);\n}\n\nstatic struct crypto_instance *crypto_ctr_alloc(struct rtattr **tb)\n{\n\tstruct crypto_instance *inst;\n\tstruct crypto_alg *alg;\n\tint err;\n\n\terr = crypto_check_attr_type(tb, CRYPTO_ALG_TYPE_BLKCIPHER);\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\talg = crypto_attr_alg(tb[1], CRYPTO_ALG_TYPE_CIPHER,\n\t\t\t\t  CRYPTO_ALG_TYPE_MASK);\n\tif (IS_ERR(alg))\n\t\treturn ERR_CAST(alg);\n\n\t/* Block size must be >= 4 bytes. */\n\terr = -EINVAL;\n\tif (alg->cra_blocksize < 4)\n\t\tgoto out_put_alg;\n\n\t/* If this is false we'd fail the alignment of crypto_inc. */\n\tif (alg->cra_blocksize % 4)\n\t\tgoto out_put_alg;\n\n\tinst = crypto_alloc_instance(\"ctr\", alg);\n\tif (IS_ERR(inst))\n\t\tgoto out;\n\n\tinst->alg.cra_flags = CRYPTO_ALG_TYPE_BLKCIPHER;\n\tinst->alg.cra_priority = alg->cra_priority;\n\tinst->alg.cra_blocksize = 1;\n\tinst->alg.cra_alignmask = alg->cra_alignmask | (__alignof__(u32) - 1);\n\tinst->alg.cra_type = &crypto_blkcipher_type;\n\n\tinst->alg.cra_blkcipher.ivsize = alg->cra_blocksize;\n\tinst->alg.cra_blkcipher.min_keysize = alg->cra_cipher.cia_min_keysize;\n\tinst->alg.cra_blkcipher.max_keysize = alg->cra_cipher.cia_max_keysize;\n\n\tinst->alg.cra_ctxsize = sizeof(struct crypto_ctr_ctx);\n\n\tinst->alg.cra_init = crypto_ctr_init_tfm;\n\tinst->alg.cra_exit = crypto_ctr_exit_tfm;\n\n\tinst->alg.cra_blkcipher.setkey = crypto_ctr_setkey;\n\tinst->alg.cra_blkcipher.encrypt = crypto_ctr_crypt;\n\tinst->alg.cra_blkcipher.decrypt = crypto_ctr_crypt;\n\n\tinst->alg.cra_blkcipher.geniv = \"chainiv\";\n\nout:\n\tcrypto_mod_put(alg);\n\treturn inst;\n\nout_put_alg:\n\tinst = ERR_PTR(err);\n\tgoto out;\n}\n\nstatic void crypto_ctr_free(struct crypto_instance *inst)\n{\n\tcrypto_drop_spawn(crypto_instance_ctx(inst));\n\tkfree(inst);\n}\n\nstatic struct crypto_template crypto_ctr_tmpl = {\n\t.name = \"ctr\",\n\t.alloc = crypto_ctr_alloc,\n\t.free = crypto_ctr_free,\n\t.module = THIS_MODULE,\n};\n\nstatic int crypto_rfc3686_setkey(struct crypto_ablkcipher *parent,\n\t\t\t\t const u8 *key, unsigned int keylen)\n{\n\tstruct crypto_rfc3686_ctx *ctx = crypto_ablkcipher_ctx(parent);\n\tstruct crypto_ablkcipher *child = ctx->child;\n\tint err;\n\n\t/* the nonce is stored in bytes at end of key */\n\tif (keylen < CTR_RFC3686_NONCE_SIZE)\n\t\treturn -EINVAL;\n\n\tmemcpy(ctx->nonce, key + (keylen - CTR_RFC3686_NONCE_SIZE),\n\t       CTR_RFC3686_NONCE_SIZE);\n\n\tkeylen -= CTR_RFC3686_NONCE_SIZE;\n\n\tcrypto_ablkcipher_clear_flags(child, CRYPTO_TFM_REQ_MASK);\n\tcrypto_ablkcipher_set_flags(child, crypto_ablkcipher_get_flags(parent) &\n\t\t\t\t    CRYPTO_TFM_REQ_MASK);\n\terr = crypto_ablkcipher_setkey(child, key, keylen);\n\tcrypto_ablkcipher_set_flags(parent, crypto_ablkcipher_get_flags(child) &\n\t\t\t\t    CRYPTO_TFM_RES_MASK);\n\n\treturn err;\n}\n\nstatic int crypto_rfc3686_crypt(struct ablkcipher_request *req)\n{\n\tstruct crypto_ablkcipher *tfm = crypto_ablkcipher_reqtfm(req);\n\tstruct crypto_rfc3686_ctx *ctx = crypto_ablkcipher_ctx(tfm);\n\tstruct crypto_ablkcipher *child = ctx->child;\n\tunsigned long align = crypto_ablkcipher_alignmask(tfm);\n\tstruct crypto_rfc3686_req_ctx *rctx =\n\t\t(void *)PTR_ALIGN((u8 *)ablkcipher_request_ctx(req), align + 1);\n\tstruct ablkcipher_request *subreq = &rctx->subreq;\n\tu8 *iv = rctx->iv;\n\n\t/* set up counter block */\n\tmemcpy(iv, ctx->nonce, CTR_RFC3686_NONCE_SIZE);\n\tmemcpy(iv + CTR_RFC3686_NONCE_SIZE, req->info, CTR_RFC3686_IV_SIZE);\n\n\t/* initialize counter portion of counter block */\n\t*(__be32 *)(iv + CTR_RFC3686_NONCE_SIZE + CTR_RFC3686_IV_SIZE) =\n\t\tcpu_to_be32(1);\n\n\tablkcipher_request_set_tfm(subreq, child);\n\tablkcipher_request_set_callback(subreq, req->base.flags,\n\t\t\t\t\treq->base.complete, req->base.data);\n\tablkcipher_request_set_crypt(subreq, req->src, req->dst, req->nbytes,\n\t\t\t\t     iv);\n\n\treturn crypto_ablkcipher_encrypt(subreq);\n}\n\nstatic int crypto_rfc3686_init_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_instance *inst = (void *)tfm->__crt_alg;\n\tstruct crypto_skcipher_spawn *spawn = crypto_instance_ctx(inst);\n\tstruct crypto_rfc3686_ctx *ctx = crypto_tfm_ctx(tfm);\n\tstruct crypto_ablkcipher *cipher;\n\tunsigned long align;\n\n\tcipher = crypto_spawn_skcipher(spawn);\n\tif (IS_ERR(cipher))\n\t\treturn PTR_ERR(cipher);\n\n\tctx->child = cipher;\n\n\talign = crypto_tfm_alg_alignmask(tfm);\n\talign &= ~(crypto_tfm_ctx_alignment() - 1);\n\ttfm->crt_ablkcipher.reqsize = align +\n\t\tsizeof(struct crypto_rfc3686_req_ctx) +\n\t\tcrypto_ablkcipher_reqsize(cipher);\n\n\treturn 0;\n}\n\nstatic void crypto_rfc3686_exit_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_rfc3686_ctx *ctx = crypto_tfm_ctx(tfm);\n\n\tcrypto_free_ablkcipher(ctx->child);\n}\n\nstatic struct crypto_instance *crypto_rfc3686_alloc(struct rtattr **tb)\n{\n\tstruct crypto_attr_type *algt;\n\tstruct crypto_instance *inst;\n\tstruct crypto_alg *alg;\n\tstruct crypto_skcipher_spawn *spawn;\n\tconst char *cipher_name;\n\tint err;\n\n\talgt = crypto_get_attr_type(tb);\n\tif (IS_ERR(algt))\n\t\treturn ERR_CAST(algt);\n\n\tif ((algt->type ^ CRYPTO_ALG_TYPE_BLKCIPHER) & algt->mask)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tcipher_name = crypto_attr_alg_name(tb[1]);\n\tif (IS_ERR(cipher_name))\n\t\treturn ERR_CAST(cipher_name);\n\n\tinst = kzalloc(sizeof(*inst) + sizeof(*spawn), GFP_KERNEL);\n\tif (!inst)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tspawn = crypto_instance_ctx(inst);\n\n\tcrypto_set_skcipher_spawn(spawn, inst);\n\terr = crypto_grab_skcipher(spawn, cipher_name, 0,\n\t\t\t\t   crypto_requires_sync(algt->type,\n\t\t\t\t\t\t\talgt->mask));\n\tif (err)\n\t\tgoto err_free_inst;\n\n\talg = crypto_skcipher_spawn_alg(spawn);\n\n\t/* We only support 16-byte blocks. */\n\terr = -EINVAL;\n\tif (alg->cra_ablkcipher.ivsize != CTR_RFC3686_BLOCK_SIZE)\n\t\tgoto err_drop_spawn;\n\n\t/* Not a stream cipher? */\n\tif (alg->cra_blocksize != 1)\n\t\tgoto err_drop_spawn;\n\n\terr = -ENAMETOOLONG;\n\tif (snprintf(inst->alg.cra_name, CRYPTO_MAX_ALG_NAME, \"rfc3686(%s)\",\n\t\t     alg->cra_name) >= CRYPTO_MAX_ALG_NAME)\n\t\tgoto err_drop_spawn;\n\tif (snprintf(inst->alg.cra_driver_name, CRYPTO_MAX_ALG_NAME,\n\t\t     \"rfc3686(%s)\", alg->cra_driver_name) >=\n\t\t\tCRYPTO_MAX_ALG_NAME)\n\t\tgoto err_drop_spawn;\n\n\tinst->alg.cra_priority = alg->cra_priority;\n\tinst->alg.cra_blocksize = 1;\n\tinst->alg.cra_alignmask = alg->cra_alignmask;\n\n\tinst->alg.cra_flags = CRYPTO_ALG_TYPE_ABLKCIPHER |\n\t\t\t      (alg->cra_flags & CRYPTO_ALG_ASYNC);\n\tinst->alg.cra_type = &crypto_ablkcipher_type;\n\n\tinst->alg.cra_ablkcipher.ivsize = CTR_RFC3686_IV_SIZE;\n\tinst->alg.cra_ablkcipher.min_keysize =\n\t\talg->cra_ablkcipher.min_keysize + CTR_RFC3686_NONCE_SIZE;\n\tinst->alg.cra_ablkcipher.max_keysize =\n\t\talg->cra_ablkcipher.max_keysize + CTR_RFC3686_NONCE_SIZE;\n\n\tinst->alg.cra_ablkcipher.geniv = \"seqiv\";\n\n\tinst->alg.cra_ablkcipher.setkey = crypto_rfc3686_setkey;\n\tinst->alg.cra_ablkcipher.encrypt = crypto_rfc3686_crypt;\n\tinst->alg.cra_ablkcipher.decrypt = crypto_rfc3686_crypt;\n\n\tinst->alg.cra_ctxsize = sizeof(struct crypto_rfc3686_ctx);\n\n\tinst->alg.cra_init = crypto_rfc3686_init_tfm;\n\tinst->alg.cra_exit = crypto_rfc3686_exit_tfm;\n\n\treturn inst;\n\nerr_drop_spawn:\n\tcrypto_drop_skcipher(spawn);\nerr_free_inst:\n\tkfree(inst);\n\treturn ERR_PTR(err);\n}\n\nstatic void crypto_rfc3686_free(struct crypto_instance *inst)\n{\n\tstruct crypto_skcipher_spawn *spawn = crypto_instance_ctx(inst);\n\n\tcrypto_drop_skcipher(spawn);\n\tkfree(inst);\n}\n\nstatic struct crypto_template crypto_rfc3686_tmpl = {\n\t.name = \"rfc3686\",\n\t.alloc = crypto_rfc3686_alloc,\n\t.free = crypto_rfc3686_free,\n\t.module = THIS_MODULE,\n};\n\nstatic int __init crypto_ctr_module_init(void)\n{\n\tint err;\n\n\terr = crypto_register_template(&crypto_ctr_tmpl);\n\tif (err)\n\t\tgoto out;\n\n\terr = crypto_register_template(&crypto_rfc3686_tmpl);\n\tif (err)\n\t\tgoto out_drop_ctr;\n\nout:\n\treturn err;\n\nout_drop_ctr:\n\tcrypto_unregister_template(&crypto_ctr_tmpl);\n\tgoto out;\n}\n\nstatic void __exit crypto_ctr_module_exit(void)\n{\n\tcrypto_unregister_template(&crypto_rfc3686_tmpl);\n\tcrypto_unregister_template(&crypto_ctr_tmpl);\n}\n\nmodule_init(crypto_ctr_module_init);\nmodule_exit(crypto_ctr_module_exit);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"CTR Counter block mode\");\nMODULE_ALIAS_CRYPTO(\"rfc3686\");\n", "/*\n * CTS: Cipher Text Stealing mode\n *\n * COPYRIGHT (c) 2008\n * The Regents of the University of Michigan\n * ALL RIGHTS RESERVED\n *\n * Permission is granted to use, copy, create derivative works\n * and redistribute this software and such derivative works\n * for any purpose, so long as the name of The University of\n * Michigan is not used in any advertising or publicity\n * pertaining to the use of distribution of this software\n * without specific, written prior authorization.  If the\n * above copyright notice or any other identification of the\n * University of Michigan is included in any copy of any\n * portion of this software, then the disclaimer below must\n * also be included.\n *\n * THIS SOFTWARE IS PROVIDED AS IS, WITHOUT REPRESENTATION\n * FROM THE UNIVERSITY OF MICHIGAN AS TO ITS FITNESS FOR ANY\n * PURPOSE, AND WITHOUT WARRANTY BY THE UNIVERSITY OF\n * MICHIGAN OF ANY KIND, EITHER EXPRESS OR IMPLIED, INCLUDING\n * WITHOUT LIMITATION THE IMPLIED WARRANTIES OF\n * MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE\n * REGENTS OF THE UNIVERSITY OF MICHIGAN SHALL NOT BE LIABLE\n * FOR ANY DAMAGES, INCLUDING SPECIAL, INDIRECT, INCIDENTAL, OR\n * CONSEQUENTIAL DAMAGES, WITH RESPECT TO ANY CLAIM ARISING\n * OUT OF OR IN CONNECTION WITH THE USE OF THE SOFTWARE, EVEN\n * IF IT HAS BEEN OR IS HEREAFTER ADVISED OF THE POSSIBILITY OF\n * SUCH DAMAGES.\n */\n\n/* Derived from various:\n *\tCopyright (c) 2006 Herbert Xu <herbert@gondor.apana.org.au>\n */\n\n/*\n * This is the Cipher Text Stealing mode as described by\n * Section 8 of rfc2040 and referenced by rfc3962.\n * rfc3962 includes errata information in its Appendix A.\n */\n\n#include <crypto/algapi.h>\n#include <linux/err.h>\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/log2.h>\n#include <linux/module.h>\n#include <linux/scatterlist.h>\n#include <crypto/scatterwalk.h>\n#include <linux/slab.h>\n\nstruct crypto_cts_ctx {\n\tstruct crypto_blkcipher *child;\n};\n\nstatic int crypto_cts_setkey(struct crypto_tfm *parent, const u8 *key,\n\t\t\t     unsigned int keylen)\n{\n\tstruct crypto_cts_ctx *ctx = crypto_tfm_ctx(parent);\n\tstruct crypto_blkcipher *child = ctx->child;\n\tint err;\n\n\tcrypto_blkcipher_clear_flags(child, CRYPTO_TFM_REQ_MASK);\n\tcrypto_blkcipher_set_flags(child, crypto_tfm_get_flags(parent) &\n\t\t\t\t       CRYPTO_TFM_REQ_MASK);\n\terr = crypto_blkcipher_setkey(child, key, keylen);\n\tcrypto_tfm_set_flags(parent, crypto_blkcipher_get_flags(child) &\n\t\t\t\t     CRYPTO_TFM_RES_MASK);\n\treturn err;\n}\n\nstatic int cts_cbc_encrypt(struct crypto_cts_ctx *ctx,\n\t\t\t   struct blkcipher_desc *desc,\n\t\t\t   struct scatterlist *dst,\n\t\t\t   struct scatterlist *src,\n\t\t\t   unsigned int offset,\n\t\t\t   unsigned int nbytes)\n{\n\tint bsize = crypto_blkcipher_blocksize(desc->tfm);\n\tu8 tmp[bsize], tmp2[bsize];\n\tstruct blkcipher_desc lcldesc;\n\tstruct scatterlist sgsrc[1], sgdst[1];\n\tint lastn = nbytes - bsize;\n\tu8 iv[bsize];\n\tu8 s[bsize * 2], d[bsize * 2];\n\tint err;\n\n\tif (lastn < 0)\n\t\treturn -EINVAL;\n\n\tsg_init_table(sgsrc, 1);\n\tsg_init_table(sgdst, 1);\n\n\tmemset(s, 0, sizeof(s));\n\tscatterwalk_map_and_copy(s, src, offset, nbytes, 0);\n\n\tmemcpy(iv, desc->info, bsize);\n\n\tlcldesc.tfm = ctx->child;\n\tlcldesc.info = iv;\n\tlcldesc.flags = desc->flags;\n\n\tsg_set_buf(&sgsrc[0], s, bsize);\n\tsg_set_buf(&sgdst[0], tmp, bsize);\n\terr = crypto_blkcipher_encrypt_iv(&lcldesc, sgdst, sgsrc, bsize);\n\n\tmemcpy(d + bsize, tmp, lastn);\n\n\tlcldesc.info = tmp;\n\n\tsg_set_buf(&sgsrc[0], s + bsize, bsize);\n\tsg_set_buf(&sgdst[0], tmp2, bsize);\n\terr = crypto_blkcipher_encrypt_iv(&lcldesc, sgdst, sgsrc, bsize);\n\n\tmemcpy(d, tmp2, bsize);\n\n\tscatterwalk_map_and_copy(d, dst, offset, nbytes, 1);\n\n\tmemcpy(desc->info, tmp2, bsize);\n\n\treturn err;\n}\n\nstatic int crypto_cts_encrypt(struct blkcipher_desc *desc,\n\t\t\t      struct scatterlist *dst, struct scatterlist *src,\n\t\t\t      unsigned int nbytes)\n{\n\tstruct crypto_cts_ctx *ctx = crypto_blkcipher_ctx(desc->tfm);\n\tint bsize = crypto_blkcipher_blocksize(desc->tfm);\n\tint tot_blocks = (nbytes + bsize - 1) / bsize;\n\tint cbc_blocks = tot_blocks > 2 ? tot_blocks - 2 : 0;\n\tstruct blkcipher_desc lcldesc;\n\tint err;\n\n\tlcldesc.tfm = ctx->child;\n\tlcldesc.info = desc->info;\n\tlcldesc.flags = desc->flags;\n\n\tif (tot_blocks == 1) {\n\t\terr = crypto_blkcipher_encrypt_iv(&lcldesc, dst, src, bsize);\n\t} else if (nbytes <= bsize * 2) {\n\t\terr = cts_cbc_encrypt(ctx, desc, dst, src, 0, nbytes);\n\t} else {\n\t\t/* do normal function for tot_blocks - 2 */\n\t\terr = crypto_blkcipher_encrypt_iv(&lcldesc, dst, src,\n\t\t\t\t\t\t\tcbc_blocks * bsize);\n\t\tif (err == 0) {\n\t\t\t/* do cts for final two blocks */\n\t\t\terr = cts_cbc_encrypt(ctx, desc, dst, src,\n\t\t\t\t\t\tcbc_blocks * bsize,\n\t\t\t\t\t\tnbytes - (cbc_blocks * bsize));\n\t\t}\n\t}\n\n\treturn err;\n}\n\nstatic int cts_cbc_decrypt(struct crypto_cts_ctx *ctx,\n\t\t\t   struct blkcipher_desc *desc,\n\t\t\t   struct scatterlist *dst,\n\t\t\t   struct scatterlist *src,\n\t\t\t   unsigned int offset,\n\t\t\t   unsigned int nbytes)\n{\n\tint bsize = crypto_blkcipher_blocksize(desc->tfm);\n\tu8 tmp[bsize];\n\tstruct blkcipher_desc lcldesc;\n\tstruct scatterlist sgsrc[1], sgdst[1];\n\tint lastn = nbytes - bsize;\n\tu8 iv[bsize];\n\tu8 s[bsize * 2], d[bsize * 2];\n\tint err;\n\n\tif (lastn < 0)\n\t\treturn -EINVAL;\n\n\tsg_init_table(sgsrc, 1);\n\tsg_init_table(sgdst, 1);\n\n\tscatterwalk_map_and_copy(s, src, offset, nbytes, 0);\n\n\tlcldesc.tfm = ctx->child;\n\tlcldesc.info = iv;\n\tlcldesc.flags = desc->flags;\n\n\t/* 1. Decrypt Cn-1 (s) to create Dn (tmp)*/\n\tmemset(iv, 0, sizeof(iv));\n\tsg_set_buf(&sgsrc[0], s, bsize);\n\tsg_set_buf(&sgdst[0], tmp, bsize);\n\terr = crypto_blkcipher_decrypt_iv(&lcldesc, sgdst, sgsrc, bsize);\n\tif (err)\n\t\treturn err;\n\t/* 2. Pad Cn with zeros at the end to create C of length BB */\n\tmemset(iv, 0, sizeof(iv));\n\tmemcpy(iv, s + bsize, lastn);\n\t/* 3. Exclusive-or Dn (tmp) with C (iv) to create Xn (tmp) */\n\tcrypto_xor(tmp, iv, bsize);\n\t/* 4. Select the first Ln bytes of Xn (tmp) to create Pn */\n\tmemcpy(d + bsize, tmp, lastn);\n\n\t/* 5. Append the tail (BB - Ln) bytes of Xn (tmp) to Cn to create En */\n\tmemcpy(s + bsize + lastn, tmp + lastn, bsize - lastn);\n\t/* 6. Decrypt En to create Pn-1 */\n\tmemzero_explicit(iv, sizeof(iv));\n\n\tsg_set_buf(&sgsrc[0], s + bsize, bsize);\n\tsg_set_buf(&sgdst[0], d, bsize);\n\terr = crypto_blkcipher_decrypt_iv(&lcldesc, sgdst, sgsrc, bsize);\n\n\t/* XOR with previous block */\n\tcrypto_xor(d, desc->info, bsize);\n\n\tscatterwalk_map_and_copy(d, dst, offset, nbytes, 1);\n\n\tmemcpy(desc->info, s, bsize);\n\treturn err;\n}\n\nstatic int crypto_cts_decrypt(struct blkcipher_desc *desc,\n\t\t\t      struct scatterlist *dst, struct scatterlist *src,\n\t\t\t      unsigned int nbytes)\n{\n\tstruct crypto_cts_ctx *ctx = crypto_blkcipher_ctx(desc->tfm);\n\tint bsize = crypto_blkcipher_blocksize(desc->tfm);\n\tint tot_blocks = (nbytes + bsize - 1) / bsize;\n\tint cbc_blocks = tot_blocks > 2 ? tot_blocks - 2 : 0;\n\tstruct blkcipher_desc lcldesc;\n\tint err;\n\n\tlcldesc.tfm = ctx->child;\n\tlcldesc.info = desc->info;\n\tlcldesc.flags = desc->flags;\n\n\tif (tot_blocks == 1) {\n\t\terr = crypto_blkcipher_decrypt_iv(&lcldesc, dst, src, bsize);\n\t} else if (nbytes <= bsize * 2) {\n\t\terr = cts_cbc_decrypt(ctx, desc, dst, src, 0, nbytes);\n\t} else {\n\t\t/* do normal function for tot_blocks - 2 */\n\t\terr = crypto_blkcipher_decrypt_iv(&lcldesc, dst, src,\n\t\t\t\t\t\t\tcbc_blocks * bsize);\n\t\tif (err == 0) {\n\t\t\t/* do cts for final two blocks */\n\t\t\terr = cts_cbc_decrypt(ctx, desc, dst, src,\n\t\t\t\t\t\tcbc_blocks * bsize,\n\t\t\t\t\t\tnbytes - (cbc_blocks * bsize));\n\t\t}\n\t}\n\treturn err;\n}\n\nstatic int crypto_cts_init_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_instance *inst = (void *)tfm->__crt_alg;\n\tstruct crypto_spawn *spawn = crypto_instance_ctx(inst);\n\tstruct crypto_cts_ctx *ctx = crypto_tfm_ctx(tfm);\n\tstruct crypto_blkcipher *cipher;\n\n\tcipher = crypto_spawn_blkcipher(spawn);\n\tif (IS_ERR(cipher))\n\t\treturn PTR_ERR(cipher);\n\n\tctx->child = cipher;\n\treturn 0;\n}\n\nstatic void crypto_cts_exit_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_cts_ctx *ctx = crypto_tfm_ctx(tfm);\n\tcrypto_free_blkcipher(ctx->child);\n}\n\nstatic struct crypto_instance *crypto_cts_alloc(struct rtattr **tb)\n{\n\tstruct crypto_instance *inst;\n\tstruct crypto_alg *alg;\n\tint err;\n\n\terr = crypto_check_attr_type(tb, CRYPTO_ALG_TYPE_BLKCIPHER);\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\talg = crypto_attr_alg(tb[1], CRYPTO_ALG_TYPE_BLKCIPHER,\n\t\t\t\t  CRYPTO_ALG_TYPE_MASK);\n\tif (IS_ERR(alg))\n\t\treturn ERR_CAST(alg);\n\n\tinst = ERR_PTR(-EINVAL);\n\tif (!is_power_of_2(alg->cra_blocksize))\n\t\tgoto out_put_alg;\n\n\tinst = crypto_alloc_instance(\"cts\", alg);\n\tif (IS_ERR(inst))\n\t\tgoto out_put_alg;\n\n\tinst->alg.cra_flags = CRYPTO_ALG_TYPE_BLKCIPHER;\n\tinst->alg.cra_priority = alg->cra_priority;\n\tinst->alg.cra_blocksize = alg->cra_blocksize;\n\tinst->alg.cra_alignmask = alg->cra_alignmask;\n\tinst->alg.cra_type = &crypto_blkcipher_type;\n\n\t/* We access the data as u32s when xoring. */\n\tinst->alg.cra_alignmask |= __alignof__(u32) - 1;\n\n\tinst->alg.cra_blkcipher.ivsize = alg->cra_blocksize;\n\tinst->alg.cra_blkcipher.min_keysize = alg->cra_blkcipher.min_keysize;\n\tinst->alg.cra_blkcipher.max_keysize = alg->cra_blkcipher.max_keysize;\n\n\tinst->alg.cra_blkcipher.geniv = \"seqiv\";\n\n\tinst->alg.cra_ctxsize = sizeof(struct crypto_cts_ctx);\n\n\tinst->alg.cra_init = crypto_cts_init_tfm;\n\tinst->alg.cra_exit = crypto_cts_exit_tfm;\n\n\tinst->alg.cra_blkcipher.setkey = crypto_cts_setkey;\n\tinst->alg.cra_blkcipher.encrypt = crypto_cts_encrypt;\n\tinst->alg.cra_blkcipher.decrypt = crypto_cts_decrypt;\n\nout_put_alg:\n\tcrypto_mod_put(alg);\n\treturn inst;\n}\n\nstatic void crypto_cts_free(struct crypto_instance *inst)\n{\n\tcrypto_drop_spawn(crypto_instance_ctx(inst));\n\tkfree(inst);\n}\n\nstatic struct crypto_template crypto_cts_tmpl = {\n\t.name = \"cts\",\n\t.alloc = crypto_cts_alloc,\n\t.free = crypto_cts_free,\n\t.module = THIS_MODULE,\n};\n\nstatic int __init crypto_cts_module_init(void)\n{\n\treturn crypto_register_template(&crypto_cts_tmpl);\n}\n\nstatic void __exit crypto_cts_module_exit(void)\n{\n\tcrypto_unregister_template(&crypto_cts_tmpl);\n}\n\nmodule_init(crypto_cts_module_init);\nmodule_exit(crypto_cts_module_exit);\n\nMODULE_LICENSE(\"Dual BSD/GPL\");\nMODULE_DESCRIPTION(\"CTS-CBC CipherText Stealing for CBC\");\n", "/*\n * ECB: Electronic CodeBook mode\n *\n * Copyright (c) 2006 Herbert Xu <herbert@gondor.apana.org.au>\n *\n * This program is free software; you can redistribute it and/or modify it\n * under the terms of the GNU General Public License as published by the Free\n * Software Foundation; either version 2 of the License, or (at your option)\n * any later version.\n *\n */\n\n#include <crypto/algapi.h>\n#include <linux/err.h>\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/scatterlist.h>\n#include <linux/slab.h>\n\nstruct crypto_ecb_ctx {\n\tstruct crypto_cipher *child;\n};\n\nstatic int crypto_ecb_setkey(struct crypto_tfm *parent, const u8 *key,\n\t\t\t     unsigned int keylen)\n{\n\tstruct crypto_ecb_ctx *ctx = crypto_tfm_ctx(parent);\n\tstruct crypto_cipher *child = ctx->child;\n\tint err;\n\n\tcrypto_cipher_clear_flags(child, CRYPTO_TFM_REQ_MASK);\n\tcrypto_cipher_set_flags(child, crypto_tfm_get_flags(parent) &\n\t\t\t\t       CRYPTO_TFM_REQ_MASK);\n\terr = crypto_cipher_setkey(child, key, keylen);\n\tcrypto_tfm_set_flags(parent, crypto_cipher_get_flags(child) &\n\t\t\t\t     CRYPTO_TFM_RES_MASK);\n\treturn err;\n}\n\nstatic int crypto_ecb_crypt(struct blkcipher_desc *desc,\n\t\t\t    struct blkcipher_walk *walk,\n\t\t\t    struct crypto_cipher *tfm,\n\t\t\t    void (*fn)(struct crypto_tfm *, u8 *, const u8 *))\n{\n\tint bsize = crypto_cipher_blocksize(tfm);\n\tunsigned int nbytes;\n\tint err;\n\n\terr = blkcipher_walk_virt(desc, walk);\n\n\twhile ((nbytes = walk->nbytes)) {\n\t\tu8 *wsrc = walk->src.virt.addr;\n\t\tu8 *wdst = walk->dst.virt.addr;\n\n\t\tdo {\n\t\t\tfn(crypto_cipher_tfm(tfm), wdst, wsrc);\n\n\t\t\twsrc += bsize;\n\t\t\twdst += bsize;\n\t\t} while ((nbytes -= bsize) >= bsize);\n\n\t\terr = blkcipher_walk_done(desc, walk, nbytes);\n\t}\n\n\treturn err;\n}\n\nstatic int crypto_ecb_encrypt(struct blkcipher_desc *desc,\n\t\t\t      struct scatterlist *dst, struct scatterlist *src,\n\t\t\t      unsigned int nbytes)\n{\n\tstruct blkcipher_walk walk;\n\tstruct crypto_blkcipher *tfm = desc->tfm;\n\tstruct crypto_ecb_ctx *ctx = crypto_blkcipher_ctx(tfm);\n\tstruct crypto_cipher *child = ctx->child;\n\n\tblkcipher_walk_init(&walk, dst, src, nbytes);\n\treturn crypto_ecb_crypt(desc, &walk, child,\n\t\t\t\tcrypto_cipher_alg(child)->cia_encrypt);\n}\n\nstatic int crypto_ecb_decrypt(struct blkcipher_desc *desc,\n\t\t\t      struct scatterlist *dst, struct scatterlist *src,\n\t\t\t      unsigned int nbytes)\n{\n\tstruct blkcipher_walk walk;\n\tstruct crypto_blkcipher *tfm = desc->tfm;\n\tstruct crypto_ecb_ctx *ctx = crypto_blkcipher_ctx(tfm);\n\tstruct crypto_cipher *child = ctx->child;\n\n\tblkcipher_walk_init(&walk, dst, src, nbytes);\n\treturn crypto_ecb_crypt(desc, &walk, child,\n\t\t\t\tcrypto_cipher_alg(child)->cia_decrypt);\n}\n\nstatic int crypto_ecb_init_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_instance *inst = (void *)tfm->__crt_alg;\n\tstruct crypto_spawn *spawn = crypto_instance_ctx(inst);\n\tstruct crypto_ecb_ctx *ctx = crypto_tfm_ctx(tfm);\n\tstruct crypto_cipher *cipher;\n\n\tcipher = crypto_spawn_cipher(spawn);\n\tif (IS_ERR(cipher))\n\t\treturn PTR_ERR(cipher);\n\n\tctx->child = cipher;\n\treturn 0;\n}\n\nstatic void crypto_ecb_exit_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_ecb_ctx *ctx = crypto_tfm_ctx(tfm);\n\tcrypto_free_cipher(ctx->child);\n}\n\nstatic struct crypto_instance *crypto_ecb_alloc(struct rtattr **tb)\n{\n\tstruct crypto_instance *inst;\n\tstruct crypto_alg *alg;\n\tint err;\n\n\terr = crypto_check_attr_type(tb, CRYPTO_ALG_TYPE_BLKCIPHER);\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\talg = crypto_get_attr_alg(tb, CRYPTO_ALG_TYPE_CIPHER,\n\t\t\t\t  CRYPTO_ALG_TYPE_MASK);\n\tif (IS_ERR(alg))\n\t\treturn ERR_CAST(alg);\n\n\tinst = crypto_alloc_instance(\"ecb\", alg);\n\tif (IS_ERR(inst))\n\t\tgoto out_put_alg;\n\n\tinst->alg.cra_flags = CRYPTO_ALG_TYPE_BLKCIPHER;\n\tinst->alg.cra_priority = alg->cra_priority;\n\tinst->alg.cra_blocksize = alg->cra_blocksize;\n\tinst->alg.cra_alignmask = alg->cra_alignmask;\n\tinst->alg.cra_type = &crypto_blkcipher_type;\n\n\tinst->alg.cra_blkcipher.min_keysize = alg->cra_cipher.cia_min_keysize;\n\tinst->alg.cra_blkcipher.max_keysize = alg->cra_cipher.cia_max_keysize;\n\n\tinst->alg.cra_ctxsize = sizeof(struct crypto_ecb_ctx);\n\n\tinst->alg.cra_init = crypto_ecb_init_tfm;\n\tinst->alg.cra_exit = crypto_ecb_exit_tfm;\n\n\tinst->alg.cra_blkcipher.setkey = crypto_ecb_setkey;\n\tinst->alg.cra_blkcipher.encrypt = crypto_ecb_encrypt;\n\tinst->alg.cra_blkcipher.decrypt = crypto_ecb_decrypt;\n\nout_put_alg:\n\tcrypto_mod_put(alg);\n\treturn inst;\n}\n\nstatic void crypto_ecb_free(struct crypto_instance *inst)\n{\n\tcrypto_drop_spawn(crypto_instance_ctx(inst));\n\tkfree(inst);\n}\n\nstatic struct crypto_template crypto_ecb_tmpl = {\n\t.name = \"ecb\",\n\t.alloc = crypto_ecb_alloc,\n\t.free = crypto_ecb_free,\n\t.module = THIS_MODULE,\n};\n\nstatic int __init crypto_ecb_module_init(void)\n{\n\treturn crypto_register_template(&crypto_ecb_tmpl);\n}\n\nstatic void __exit crypto_ecb_module_exit(void)\n{\n\tcrypto_unregister_template(&crypto_ecb_tmpl);\n}\n\nmodule_init(crypto_ecb_module_init);\nmodule_exit(crypto_ecb_module_exit);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"ECB block cipher algorithm\");\n", "/*\n * eseqiv: Encrypted Sequence Number IV Generator\n *\n * This generator generates an IV based on a sequence number by xoring it\n * with a salt and then encrypting it with the same key as used to encrypt\n * the plain text.  This algorithm requires that the block size be equal\n * to the IV size.  It is mainly useful for CBC.\n *\n * Copyright (c) 2007 Herbert Xu <herbert@gondor.apana.org.au>\n *\n * This program is free software; you can redistribute it and/or modify it\n * under the terms of the GNU General Public License as published by the Free\n * Software Foundation; either version 2 of the License, or (at your option)\n * any later version.\n *\n */\n\n#include <crypto/internal/skcipher.h>\n#include <crypto/rng.h>\n#include <crypto/scatterwalk.h>\n#include <linux/err.h>\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/mm.h>\n#include <linux/module.h>\n#include <linux/scatterlist.h>\n#include <linux/spinlock.h>\n#include <linux/string.h>\n\nstruct eseqiv_request_ctx {\n\tstruct scatterlist src[2];\n\tstruct scatterlist dst[2];\n\tchar tail[];\n};\n\nstruct eseqiv_ctx {\n\tspinlock_t lock;\n\tunsigned int reqoff;\n\tchar salt[];\n};\n\nstatic void eseqiv_complete2(struct skcipher_givcrypt_request *req)\n{\n\tstruct crypto_ablkcipher *geniv = skcipher_givcrypt_reqtfm(req);\n\tstruct eseqiv_request_ctx *reqctx = skcipher_givcrypt_reqctx(req);\n\n\tmemcpy(req->giv, PTR_ALIGN((u8 *)reqctx->tail,\n\t\t\t crypto_ablkcipher_alignmask(geniv) + 1),\n\t       crypto_ablkcipher_ivsize(geniv));\n}\n\nstatic void eseqiv_complete(struct crypto_async_request *base, int err)\n{\n\tstruct skcipher_givcrypt_request *req = base->data;\n\n\tif (err)\n\t\tgoto out;\n\n\teseqiv_complete2(req);\n\nout:\n\tskcipher_givcrypt_complete(req, err);\n}\n\nstatic int eseqiv_givencrypt(struct skcipher_givcrypt_request *req)\n{\n\tstruct crypto_ablkcipher *geniv = skcipher_givcrypt_reqtfm(req);\n\tstruct eseqiv_ctx *ctx = crypto_ablkcipher_ctx(geniv);\n\tstruct eseqiv_request_ctx *reqctx = skcipher_givcrypt_reqctx(req);\n\tstruct ablkcipher_request *subreq;\n\tcrypto_completion_t compl;\n\tvoid *data;\n\tstruct scatterlist *osrc, *odst;\n\tstruct scatterlist *dst;\n\tstruct page *srcp;\n\tstruct page *dstp;\n\tu8 *giv;\n\tu8 *vsrc;\n\tu8 *vdst;\n\t__be64 seq;\n\tunsigned int ivsize;\n\tunsigned int len;\n\tint err;\n\n\tsubreq = (void *)(reqctx->tail + ctx->reqoff);\n\tablkcipher_request_set_tfm(subreq, skcipher_geniv_cipher(geniv));\n\n\tgiv = req->giv;\n\tcompl = req->creq.base.complete;\n\tdata = req->creq.base.data;\n\n\tosrc = req->creq.src;\n\todst = req->creq.dst;\n\tsrcp = sg_page(osrc);\n\tdstp = sg_page(odst);\n\tvsrc = PageHighMem(srcp) ? NULL : page_address(srcp) + osrc->offset;\n\tvdst = PageHighMem(dstp) ? NULL : page_address(dstp) + odst->offset;\n\n\tivsize = crypto_ablkcipher_ivsize(geniv);\n\n\tif (vsrc != giv + ivsize && vdst != giv + ivsize) {\n\t\tgiv = PTR_ALIGN((u8 *)reqctx->tail,\n\t\t\t\tcrypto_ablkcipher_alignmask(geniv) + 1);\n\t\tcompl = eseqiv_complete;\n\t\tdata = req;\n\t}\n\n\tablkcipher_request_set_callback(subreq, req->creq.base.flags, compl,\n\t\t\t\t\tdata);\n\n\tsg_init_table(reqctx->src, 2);\n\tsg_set_buf(reqctx->src, giv, ivsize);\n\tscatterwalk_crypto_chain(reqctx->src, osrc, vsrc == giv + ivsize, 2);\n\n\tdst = reqctx->src;\n\tif (osrc != odst) {\n\t\tsg_init_table(reqctx->dst, 2);\n\t\tsg_set_buf(reqctx->dst, giv, ivsize);\n\t\tscatterwalk_crypto_chain(reqctx->dst, odst, vdst == giv + ivsize, 2);\n\n\t\tdst = reqctx->dst;\n\t}\n\n\tablkcipher_request_set_crypt(subreq, reqctx->src, dst,\n\t\t\t\t     req->creq.nbytes + ivsize,\n\t\t\t\t     req->creq.info);\n\n\tmemcpy(req->creq.info, ctx->salt, ivsize);\n\n\tlen = ivsize;\n\tif (ivsize > sizeof(u64)) {\n\t\tmemset(req->giv, 0, ivsize - sizeof(u64));\n\t\tlen = sizeof(u64);\n\t}\n\tseq = cpu_to_be64(req->seq);\n\tmemcpy(req->giv + ivsize - len, &seq, len);\n\n\terr = crypto_ablkcipher_encrypt(subreq);\n\tif (err)\n\t\tgoto out;\n\n\tif (giv != req->giv)\n\t\teseqiv_complete2(req);\n\nout:\n\treturn err;\n}\n\nstatic int eseqiv_givencrypt_first(struct skcipher_givcrypt_request *req)\n{\n\tstruct crypto_ablkcipher *geniv = skcipher_givcrypt_reqtfm(req);\n\tstruct eseqiv_ctx *ctx = crypto_ablkcipher_ctx(geniv);\n\tint err = 0;\n\n\tspin_lock_bh(&ctx->lock);\n\tif (crypto_ablkcipher_crt(geniv)->givencrypt != eseqiv_givencrypt_first)\n\t\tgoto unlock;\n\n\tcrypto_ablkcipher_crt(geniv)->givencrypt = eseqiv_givencrypt;\n\terr = crypto_rng_get_bytes(crypto_default_rng, ctx->salt,\n\t\t\t\t   crypto_ablkcipher_ivsize(geniv));\n\nunlock:\n\tspin_unlock_bh(&ctx->lock);\n\n\tif (err)\n\t\treturn err;\n\n\treturn eseqiv_givencrypt(req);\n}\n\nstatic int eseqiv_init(struct crypto_tfm *tfm)\n{\n\tstruct crypto_ablkcipher *geniv = __crypto_ablkcipher_cast(tfm);\n\tstruct eseqiv_ctx *ctx = crypto_ablkcipher_ctx(geniv);\n\tunsigned long alignmask;\n\tunsigned int reqsize;\n\n\tspin_lock_init(&ctx->lock);\n\n\talignmask = crypto_tfm_ctx_alignment() - 1;\n\treqsize = sizeof(struct eseqiv_request_ctx);\n\n\tif (alignmask & reqsize) {\n\t\talignmask &= reqsize;\n\t\talignmask--;\n\t}\n\n\talignmask = ~alignmask;\n\talignmask &= crypto_ablkcipher_alignmask(geniv);\n\n\treqsize += alignmask;\n\treqsize += crypto_ablkcipher_ivsize(geniv);\n\treqsize = ALIGN(reqsize, crypto_tfm_ctx_alignment());\n\n\tctx->reqoff = reqsize - sizeof(struct eseqiv_request_ctx);\n\n\ttfm->crt_ablkcipher.reqsize = reqsize +\n\t\t\t\t      sizeof(struct ablkcipher_request);\n\n\treturn skcipher_geniv_init(tfm);\n}\n\nstatic struct crypto_template eseqiv_tmpl;\n\nstatic struct crypto_instance *eseqiv_alloc(struct rtattr **tb)\n{\n\tstruct crypto_instance *inst;\n\tint err;\n\n\terr = crypto_get_default_rng();\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\tinst = skcipher_geniv_alloc(&eseqiv_tmpl, tb, 0, 0);\n\tif (IS_ERR(inst))\n\t\tgoto put_rng;\n\n\terr = -EINVAL;\n\tif (inst->alg.cra_ablkcipher.ivsize != inst->alg.cra_blocksize)\n\t\tgoto free_inst;\n\n\tinst->alg.cra_ablkcipher.givencrypt = eseqiv_givencrypt_first;\n\n\tinst->alg.cra_init = eseqiv_init;\n\tinst->alg.cra_exit = skcipher_geniv_exit;\n\n\tinst->alg.cra_ctxsize = sizeof(struct eseqiv_ctx);\n\tinst->alg.cra_ctxsize += inst->alg.cra_ablkcipher.ivsize;\n\nout:\n\treturn inst;\n\nfree_inst:\n\tskcipher_geniv_free(inst);\n\tinst = ERR_PTR(err);\nput_rng:\n\tcrypto_put_default_rng();\n\tgoto out;\n}\n\nstatic void eseqiv_free(struct crypto_instance *inst)\n{\n\tskcipher_geniv_free(inst);\n\tcrypto_put_default_rng();\n}\n\nstatic struct crypto_template eseqiv_tmpl = {\n\t.name = \"eseqiv\",\n\t.alloc = eseqiv_alloc,\n\t.free = eseqiv_free,\n\t.module = THIS_MODULE,\n};\n\nstatic int __init eseqiv_module_init(void)\n{\n\treturn crypto_register_template(&eseqiv_tmpl);\n}\n\nstatic void __exit eseqiv_module_exit(void)\n{\n\tcrypto_unregister_template(&eseqiv_tmpl);\n}\n\nmodule_init(eseqiv_module_init);\nmodule_exit(eseqiv_module_exit);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"Encrypted Sequence Number IV Generator\");\n", "/*\n * GCM: Galois/Counter Mode.\n *\n * Copyright (c) 2007 Nokia Siemens Networks - Mikko Herranen <mh1@iki.fi>\n *\n * This program is free software; you can redistribute it and/or modify it\n * under the terms of the GNU General Public License version 2 as published\n * by the Free Software Foundation.\n */\n\n#include <crypto/gf128mul.h>\n#include <crypto/internal/aead.h>\n#include <crypto/internal/skcipher.h>\n#include <crypto/internal/hash.h>\n#include <crypto/scatterwalk.h>\n#include <crypto/hash.h>\n#include \"internal.h\"\n#include <linux/completion.h>\n#include <linux/err.h>\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/slab.h>\n\nstruct gcm_instance_ctx {\n\tstruct crypto_skcipher_spawn ctr;\n\tstruct crypto_ahash_spawn ghash;\n};\n\nstruct crypto_gcm_ctx {\n\tstruct crypto_ablkcipher *ctr;\n\tstruct crypto_ahash *ghash;\n};\n\nstruct crypto_rfc4106_ctx {\n\tstruct crypto_aead *child;\n\tu8 nonce[4];\n};\n\nstruct crypto_rfc4543_instance_ctx {\n\tstruct crypto_aead_spawn aead;\n\tstruct crypto_skcipher_spawn null;\n};\n\nstruct crypto_rfc4543_ctx {\n\tstruct crypto_aead *child;\n\tstruct crypto_blkcipher *null;\n\tu8 nonce[4];\n};\n\nstruct crypto_rfc4543_req_ctx {\n\tu8 auth_tag[16];\n\tu8 assocbuf[32];\n\tstruct scatterlist cipher[1];\n\tstruct scatterlist payload[2];\n\tstruct scatterlist assoc[2];\n\tstruct aead_request subreq;\n};\n\nstruct crypto_gcm_ghash_ctx {\n\tunsigned int cryptlen;\n\tstruct scatterlist *src;\n\tvoid (*complete)(struct aead_request *req, int err);\n};\n\nstruct crypto_gcm_req_priv_ctx {\n\tu8 auth_tag[16];\n\tu8 iauth_tag[16];\n\tstruct scatterlist src[2];\n\tstruct scatterlist dst[2];\n\tstruct crypto_gcm_ghash_ctx ghash_ctx;\n\tunion {\n\t\tstruct ahash_request ahreq;\n\t\tstruct ablkcipher_request abreq;\n\t} u;\n};\n\nstruct crypto_gcm_setkey_result {\n\tint err;\n\tstruct completion completion;\n};\n\nstatic void *gcm_zeroes;\n\nstatic inline struct crypto_gcm_req_priv_ctx *crypto_gcm_reqctx(\n\tstruct aead_request *req)\n{\n\tunsigned long align = crypto_aead_alignmask(crypto_aead_reqtfm(req));\n\n\treturn (void *)PTR_ALIGN((u8 *)aead_request_ctx(req), align + 1);\n}\n\nstatic void crypto_gcm_setkey_done(struct crypto_async_request *req, int err)\n{\n\tstruct crypto_gcm_setkey_result *result = req->data;\n\n\tif (err == -EINPROGRESS)\n\t\treturn;\n\n\tresult->err = err;\n\tcomplete(&result->completion);\n}\n\nstatic int crypto_gcm_setkey(struct crypto_aead *aead, const u8 *key,\n\t\t\t     unsigned int keylen)\n{\n\tstruct crypto_gcm_ctx *ctx = crypto_aead_ctx(aead);\n\tstruct crypto_ahash *ghash = ctx->ghash;\n\tstruct crypto_ablkcipher *ctr = ctx->ctr;\n\tstruct {\n\t\tbe128 hash;\n\t\tu8 iv[8];\n\n\t\tstruct crypto_gcm_setkey_result result;\n\n\t\tstruct scatterlist sg[1];\n\t\tstruct ablkcipher_request req;\n\t} *data;\n\tint err;\n\n\tcrypto_ablkcipher_clear_flags(ctr, CRYPTO_TFM_REQ_MASK);\n\tcrypto_ablkcipher_set_flags(ctr, crypto_aead_get_flags(aead) &\n\t\t\t\t   CRYPTO_TFM_REQ_MASK);\n\n\terr = crypto_ablkcipher_setkey(ctr, key, keylen);\n\tif (err)\n\t\treturn err;\n\n\tcrypto_aead_set_flags(aead, crypto_ablkcipher_get_flags(ctr) &\n\t\t\t\t       CRYPTO_TFM_RES_MASK);\n\n\tdata = kzalloc(sizeof(*data) + crypto_ablkcipher_reqsize(ctr),\n\t\t       GFP_KERNEL);\n\tif (!data)\n\t\treturn -ENOMEM;\n\n\tinit_completion(&data->result.completion);\n\tsg_init_one(data->sg, &data->hash, sizeof(data->hash));\n\tablkcipher_request_set_tfm(&data->req, ctr);\n\tablkcipher_request_set_callback(&data->req, CRYPTO_TFM_REQ_MAY_SLEEP |\n\t\t\t\t\t\t    CRYPTO_TFM_REQ_MAY_BACKLOG,\n\t\t\t\t\tcrypto_gcm_setkey_done,\n\t\t\t\t\t&data->result);\n\tablkcipher_request_set_crypt(&data->req, data->sg, data->sg,\n\t\t\t\t     sizeof(data->hash), data->iv);\n\n\terr = crypto_ablkcipher_encrypt(&data->req);\n\tif (err == -EINPROGRESS || err == -EBUSY) {\n\t\terr = wait_for_completion_interruptible(\n\t\t\t&data->result.completion);\n\t\tif (!err)\n\t\t\terr = data->result.err;\n\t}\n\n\tif (err)\n\t\tgoto out;\n\n\tcrypto_ahash_clear_flags(ghash, CRYPTO_TFM_REQ_MASK);\n\tcrypto_ahash_set_flags(ghash, crypto_aead_get_flags(aead) &\n\t\t\t       CRYPTO_TFM_REQ_MASK);\n\terr = crypto_ahash_setkey(ghash, (u8 *)&data->hash, sizeof(be128));\n\tcrypto_aead_set_flags(aead, crypto_ahash_get_flags(ghash) &\n\t\t\t      CRYPTO_TFM_RES_MASK);\n\nout:\n\tkfree(data);\n\treturn err;\n}\n\nstatic int crypto_gcm_setauthsize(struct crypto_aead *tfm,\n\t\t\t\t  unsigned int authsize)\n{\n\tswitch (authsize) {\n\tcase 4:\n\tcase 8:\n\tcase 12:\n\tcase 13:\n\tcase 14:\n\tcase 15:\n\tcase 16:\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic void crypto_gcm_init_crypt(struct ablkcipher_request *ablk_req,\n\t\t\t\t  struct aead_request *req,\n\t\t\t\t  unsigned int cryptlen)\n{\n\tstruct crypto_aead *aead = crypto_aead_reqtfm(req);\n\tstruct crypto_gcm_ctx *ctx = crypto_aead_ctx(aead);\n\tstruct crypto_gcm_req_priv_ctx *pctx = crypto_gcm_reqctx(req);\n\tstruct scatterlist *dst;\n\t__be32 counter = cpu_to_be32(1);\n\n\tmemset(pctx->auth_tag, 0, sizeof(pctx->auth_tag));\n\tmemcpy(req->iv + 12, &counter, 4);\n\n\tsg_init_table(pctx->src, 2);\n\tsg_set_buf(pctx->src, pctx->auth_tag, sizeof(pctx->auth_tag));\n\tscatterwalk_sg_chain(pctx->src, 2, req->src);\n\n\tdst = pctx->src;\n\tif (req->src != req->dst) {\n\t\tsg_init_table(pctx->dst, 2);\n\t\tsg_set_buf(pctx->dst, pctx->auth_tag, sizeof(pctx->auth_tag));\n\t\tscatterwalk_sg_chain(pctx->dst, 2, req->dst);\n\t\tdst = pctx->dst;\n\t}\n\n\tablkcipher_request_set_tfm(ablk_req, ctx->ctr);\n\tablkcipher_request_set_crypt(ablk_req, pctx->src, dst,\n\t\t\t\t     cryptlen + sizeof(pctx->auth_tag),\n\t\t\t\t     req->iv);\n}\n\nstatic inline unsigned int gcm_remain(unsigned int len)\n{\n\tlen &= 0xfU;\n\treturn len ? 16 - len : 0;\n}\n\nstatic void gcm_hash_len_done(struct crypto_async_request *areq, int err);\nstatic void gcm_hash_final_done(struct crypto_async_request *areq, int err);\n\nstatic int gcm_hash_update(struct aead_request *req,\n\t\t\t   struct crypto_gcm_req_priv_ctx *pctx,\n\t\t\t   crypto_completion_t compl,\n\t\t\t   struct scatterlist *src,\n\t\t\t   unsigned int len)\n{\n\tstruct ahash_request *ahreq = &pctx->u.ahreq;\n\n\tahash_request_set_callback(ahreq, aead_request_flags(req),\n\t\t\t\t   compl, req);\n\tahash_request_set_crypt(ahreq, src, NULL, len);\n\n\treturn crypto_ahash_update(ahreq);\n}\n\nstatic int gcm_hash_remain(struct aead_request *req,\n\t\t\t   struct crypto_gcm_req_priv_ctx *pctx,\n\t\t\t   unsigned int remain,\n\t\t\t   crypto_completion_t compl)\n{\n\tstruct ahash_request *ahreq = &pctx->u.ahreq;\n\n\tahash_request_set_callback(ahreq, aead_request_flags(req),\n\t\t\t\t   compl, req);\n\tsg_init_one(pctx->src, gcm_zeroes, remain);\n\tahash_request_set_crypt(ahreq, pctx->src, NULL, remain);\n\n\treturn crypto_ahash_update(ahreq);\n}\n\nstatic int gcm_hash_len(struct aead_request *req,\n\t\t\tstruct crypto_gcm_req_priv_ctx *pctx)\n{\n\tstruct ahash_request *ahreq = &pctx->u.ahreq;\n\tstruct crypto_gcm_ghash_ctx *gctx = &pctx->ghash_ctx;\n\tu128 lengths;\n\n\tlengths.a = cpu_to_be64(req->assoclen * 8);\n\tlengths.b = cpu_to_be64(gctx->cryptlen * 8);\n\tmemcpy(pctx->iauth_tag, &lengths, 16);\n\tsg_init_one(pctx->src, pctx->iauth_tag, 16);\n\tahash_request_set_callback(ahreq, aead_request_flags(req),\n\t\t\t\t   gcm_hash_len_done, req);\n\tahash_request_set_crypt(ahreq, pctx->src,\n\t\t\t\tNULL, sizeof(lengths));\n\n\treturn crypto_ahash_update(ahreq);\n}\n\nstatic int gcm_hash_final(struct aead_request *req,\n\t\t\t  struct crypto_gcm_req_priv_ctx *pctx)\n{\n\tstruct ahash_request *ahreq = &pctx->u.ahreq;\n\n\tahash_request_set_callback(ahreq, aead_request_flags(req),\n\t\t\t\t   gcm_hash_final_done, req);\n\tahash_request_set_crypt(ahreq, NULL, pctx->iauth_tag, 0);\n\n\treturn crypto_ahash_final(ahreq);\n}\n\nstatic void __gcm_hash_final_done(struct aead_request *req, int err)\n{\n\tstruct crypto_gcm_req_priv_ctx *pctx = crypto_gcm_reqctx(req);\n\tstruct crypto_gcm_ghash_ctx *gctx = &pctx->ghash_ctx;\n\n\tif (!err)\n\t\tcrypto_xor(pctx->auth_tag, pctx->iauth_tag, 16);\n\n\tgctx->complete(req, err);\n}\n\nstatic void gcm_hash_final_done(struct crypto_async_request *areq, int err)\n{\n\tstruct aead_request *req = areq->data;\n\n\t__gcm_hash_final_done(req, err);\n}\n\nstatic void __gcm_hash_len_done(struct aead_request *req, int err)\n{\n\tstruct crypto_gcm_req_priv_ctx *pctx = crypto_gcm_reqctx(req);\n\n\tif (!err) {\n\t\terr = gcm_hash_final(req, pctx);\n\t\tif (err == -EINPROGRESS || err == -EBUSY)\n\t\t\treturn;\n\t}\n\n\t__gcm_hash_final_done(req, err);\n}\n\nstatic void gcm_hash_len_done(struct crypto_async_request *areq, int err)\n{\n\tstruct aead_request *req = areq->data;\n\n\t__gcm_hash_len_done(req, err);\n}\n\nstatic void __gcm_hash_crypt_remain_done(struct aead_request *req, int err)\n{\n\tstruct crypto_gcm_req_priv_ctx *pctx = crypto_gcm_reqctx(req);\n\n\tif (!err) {\n\t\terr = gcm_hash_len(req, pctx);\n\t\tif (err == -EINPROGRESS || err == -EBUSY)\n\t\t\treturn;\n\t}\n\n\t__gcm_hash_len_done(req, err);\n}\n\nstatic void gcm_hash_crypt_remain_done(struct crypto_async_request *areq,\n\t\t\t\t       int err)\n{\n\tstruct aead_request *req = areq->data;\n\n\t__gcm_hash_crypt_remain_done(req, err);\n}\n\nstatic void __gcm_hash_crypt_done(struct aead_request *req, int err)\n{\n\tstruct crypto_gcm_req_priv_ctx *pctx = crypto_gcm_reqctx(req);\n\tstruct crypto_gcm_ghash_ctx *gctx = &pctx->ghash_ctx;\n\tunsigned int remain;\n\n\tif (!err) {\n\t\tremain = gcm_remain(gctx->cryptlen);\n\t\tBUG_ON(!remain);\n\t\terr = gcm_hash_remain(req, pctx, remain,\n\t\t\t\t      gcm_hash_crypt_remain_done);\n\t\tif (err == -EINPROGRESS || err == -EBUSY)\n\t\t\treturn;\n\t}\n\n\t__gcm_hash_crypt_remain_done(req, err);\n}\n\nstatic void gcm_hash_crypt_done(struct crypto_async_request *areq, int err)\n{\n\tstruct aead_request *req = areq->data;\n\n\t__gcm_hash_crypt_done(req, err);\n}\n\nstatic void __gcm_hash_assoc_remain_done(struct aead_request *req, int err)\n{\n\tstruct crypto_gcm_req_priv_ctx *pctx = crypto_gcm_reqctx(req);\n\tstruct crypto_gcm_ghash_ctx *gctx = &pctx->ghash_ctx;\n\tcrypto_completion_t compl;\n\tunsigned int remain = 0;\n\n\tif (!err && gctx->cryptlen) {\n\t\tremain = gcm_remain(gctx->cryptlen);\n\t\tcompl = remain ? gcm_hash_crypt_done :\n\t\t\tgcm_hash_crypt_remain_done;\n\t\terr = gcm_hash_update(req, pctx, compl,\n\t\t\t\t      gctx->src, gctx->cryptlen);\n\t\tif (err == -EINPROGRESS || err == -EBUSY)\n\t\t\treturn;\n\t}\n\n\tif (remain)\n\t\t__gcm_hash_crypt_done(req, err);\n\telse\n\t\t__gcm_hash_crypt_remain_done(req, err);\n}\n\nstatic void gcm_hash_assoc_remain_done(struct crypto_async_request *areq,\n\t\t\t\t       int err)\n{\n\tstruct aead_request *req = areq->data;\n\n\t__gcm_hash_assoc_remain_done(req, err);\n}\n\nstatic void __gcm_hash_assoc_done(struct aead_request *req, int err)\n{\n\tstruct crypto_gcm_req_priv_ctx *pctx = crypto_gcm_reqctx(req);\n\tunsigned int remain;\n\n\tif (!err) {\n\t\tremain = gcm_remain(req->assoclen);\n\t\tBUG_ON(!remain);\n\t\terr = gcm_hash_remain(req, pctx, remain,\n\t\t\t\t      gcm_hash_assoc_remain_done);\n\t\tif (err == -EINPROGRESS || err == -EBUSY)\n\t\t\treturn;\n\t}\n\n\t__gcm_hash_assoc_remain_done(req, err);\n}\n\nstatic void gcm_hash_assoc_done(struct crypto_async_request *areq, int err)\n{\n\tstruct aead_request *req = areq->data;\n\n\t__gcm_hash_assoc_done(req, err);\n}\n\nstatic void __gcm_hash_init_done(struct aead_request *req, int err)\n{\n\tstruct crypto_gcm_req_priv_ctx *pctx = crypto_gcm_reqctx(req);\n\tcrypto_completion_t compl;\n\tunsigned int remain = 0;\n\n\tif (!err && req->assoclen) {\n\t\tremain = gcm_remain(req->assoclen);\n\t\tcompl = remain ? gcm_hash_assoc_done :\n\t\t\tgcm_hash_assoc_remain_done;\n\t\terr = gcm_hash_update(req, pctx, compl,\n\t\t\t\t      req->assoc, req->assoclen);\n\t\tif (err == -EINPROGRESS || err == -EBUSY)\n\t\t\treturn;\n\t}\n\n\tif (remain)\n\t\t__gcm_hash_assoc_done(req, err);\n\telse\n\t\t__gcm_hash_assoc_remain_done(req, err);\n}\n\nstatic void gcm_hash_init_done(struct crypto_async_request *areq, int err)\n{\n\tstruct aead_request *req = areq->data;\n\n\t__gcm_hash_init_done(req, err);\n}\n\nstatic int gcm_hash(struct aead_request *req,\n\t\t    struct crypto_gcm_req_priv_ctx *pctx)\n{\n\tstruct ahash_request *ahreq = &pctx->u.ahreq;\n\tstruct crypto_gcm_ghash_ctx *gctx = &pctx->ghash_ctx;\n\tstruct crypto_gcm_ctx *ctx = crypto_tfm_ctx(req->base.tfm);\n\tunsigned int remain;\n\tcrypto_completion_t compl;\n\tint err;\n\n\tahash_request_set_tfm(ahreq, ctx->ghash);\n\n\tahash_request_set_callback(ahreq, aead_request_flags(req),\n\t\t\t\t   gcm_hash_init_done, req);\n\terr = crypto_ahash_init(ahreq);\n\tif (err)\n\t\treturn err;\n\tremain = gcm_remain(req->assoclen);\n\tcompl = remain ? gcm_hash_assoc_done : gcm_hash_assoc_remain_done;\n\terr = gcm_hash_update(req, pctx, compl, req->assoc, req->assoclen);\n\tif (err)\n\t\treturn err;\n\tif (remain) {\n\t\terr = gcm_hash_remain(req, pctx, remain,\n\t\t\t\t      gcm_hash_assoc_remain_done);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\tremain = gcm_remain(gctx->cryptlen);\n\tcompl = remain ? gcm_hash_crypt_done : gcm_hash_crypt_remain_done;\n\terr = gcm_hash_update(req, pctx, compl, gctx->src, gctx->cryptlen);\n\tif (err)\n\t\treturn err;\n\tif (remain) {\n\t\terr = gcm_hash_remain(req, pctx, remain,\n\t\t\t\t      gcm_hash_crypt_remain_done);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\terr = gcm_hash_len(req, pctx);\n\tif (err)\n\t\treturn err;\n\terr = gcm_hash_final(req, pctx);\n\tif (err)\n\t\treturn err;\n\n\treturn 0;\n}\n\nstatic void gcm_enc_copy_hash(struct aead_request *req,\n\t\t\t      struct crypto_gcm_req_priv_ctx *pctx)\n{\n\tstruct crypto_aead *aead = crypto_aead_reqtfm(req);\n\tu8 *auth_tag = pctx->auth_tag;\n\n\tscatterwalk_map_and_copy(auth_tag, req->dst, req->cryptlen,\n\t\t\t\t crypto_aead_authsize(aead), 1);\n}\n\nstatic void gcm_enc_hash_done(struct aead_request *req, int err)\n{\n\tstruct crypto_gcm_req_priv_ctx *pctx = crypto_gcm_reqctx(req);\n\n\tif (!err)\n\t\tgcm_enc_copy_hash(req, pctx);\n\n\taead_request_complete(req, err);\n}\n\nstatic void gcm_encrypt_done(struct crypto_async_request *areq, int err)\n{\n\tstruct aead_request *req = areq->data;\n\tstruct crypto_gcm_req_priv_ctx *pctx = crypto_gcm_reqctx(req);\n\n\tif (!err) {\n\t\terr = gcm_hash(req, pctx);\n\t\tif (err == -EINPROGRESS || err == -EBUSY)\n\t\t\treturn;\n\t\telse if (!err) {\n\t\t\tcrypto_xor(pctx->auth_tag, pctx->iauth_tag, 16);\n\t\t\tgcm_enc_copy_hash(req, pctx);\n\t\t}\n\t}\n\n\taead_request_complete(req, err);\n}\n\nstatic int crypto_gcm_encrypt(struct aead_request *req)\n{\n\tstruct crypto_gcm_req_priv_ctx *pctx = crypto_gcm_reqctx(req);\n\tstruct ablkcipher_request *abreq = &pctx->u.abreq;\n\tstruct crypto_gcm_ghash_ctx *gctx = &pctx->ghash_ctx;\n\tint err;\n\n\tcrypto_gcm_init_crypt(abreq, req, req->cryptlen);\n\tablkcipher_request_set_callback(abreq, aead_request_flags(req),\n\t\t\t\t\tgcm_encrypt_done, req);\n\n\tgctx->src = req->dst;\n\tgctx->cryptlen = req->cryptlen;\n\tgctx->complete = gcm_enc_hash_done;\n\n\terr = crypto_ablkcipher_encrypt(abreq);\n\tif (err)\n\t\treturn err;\n\n\terr = gcm_hash(req, pctx);\n\tif (err)\n\t\treturn err;\n\n\tcrypto_xor(pctx->auth_tag, pctx->iauth_tag, 16);\n\tgcm_enc_copy_hash(req, pctx);\n\n\treturn 0;\n}\n\nstatic int crypto_gcm_verify(struct aead_request *req,\n\t\t\t     struct crypto_gcm_req_priv_ctx *pctx)\n{\n\tstruct crypto_aead *aead = crypto_aead_reqtfm(req);\n\tu8 *auth_tag = pctx->auth_tag;\n\tu8 *iauth_tag = pctx->iauth_tag;\n\tunsigned int authsize = crypto_aead_authsize(aead);\n\tunsigned int cryptlen = req->cryptlen - authsize;\n\n\tcrypto_xor(auth_tag, iauth_tag, 16);\n\tscatterwalk_map_and_copy(iauth_tag, req->src, cryptlen, authsize, 0);\n\treturn crypto_memneq(iauth_tag, auth_tag, authsize) ? -EBADMSG : 0;\n}\n\nstatic void gcm_decrypt_done(struct crypto_async_request *areq, int err)\n{\n\tstruct aead_request *req = areq->data;\n\tstruct crypto_gcm_req_priv_ctx *pctx = crypto_gcm_reqctx(req);\n\n\tif (!err)\n\t\terr = crypto_gcm_verify(req, pctx);\n\n\taead_request_complete(req, err);\n}\n\nstatic void gcm_dec_hash_done(struct aead_request *req, int err)\n{\n\tstruct crypto_gcm_req_priv_ctx *pctx = crypto_gcm_reqctx(req);\n\tstruct ablkcipher_request *abreq = &pctx->u.abreq;\n\tstruct crypto_gcm_ghash_ctx *gctx = &pctx->ghash_ctx;\n\n\tif (!err) {\n\t\tablkcipher_request_set_callback(abreq, aead_request_flags(req),\n\t\t\t\t\t\tgcm_decrypt_done, req);\n\t\tcrypto_gcm_init_crypt(abreq, req, gctx->cryptlen);\n\t\terr = crypto_ablkcipher_decrypt(abreq);\n\t\tif (err == -EINPROGRESS || err == -EBUSY)\n\t\t\treturn;\n\t\telse if (!err)\n\t\t\terr = crypto_gcm_verify(req, pctx);\n\t}\n\n\taead_request_complete(req, err);\n}\n\nstatic int crypto_gcm_decrypt(struct aead_request *req)\n{\n\tstruct crypto_aead *aead = crypto_aead_reqtfm(req);\n\tstruct crypto_gcm_req_priv_ctx *pctx = crypto_gcm_reqctx(req);\n\tstruct ablkcipher_request *abreq = &pctx->u.abreq;\n\tstruct crypto_gcm_ghash_ctx *gctx = &pctx->ghash_ctx;\n\tunsigned int authsize = crypto_aead_authsize(aead);\n\tunsigned int cryptlen = req->cryptlen;\n\tint err;\n\n\tif (cryptlen < authsize)\n\t\treturn -EINVAL;\n\tcryptlen -= authsize;\n\n\tgctx->src = req->src;\n\tgctx->cryptlen = cryptlen;\n\tgctx->complete = gcm_dec_hash_done;\n\n\terr = gcm_hash(req, pctx);\n\tif (err)\n\t\treturn err;\n\n\tablkcipher_request_set_callback(abreq, aead_request_flags(req),\n\t\t\t\t\tgcm_decrypt_done, req);\n\tcrypto_gcm_init_crypt(abreq, req, cryptlen);\n\terr = crypto_ablkcipher_decrypt(abreq);\n\tif (err)\n\t\treturn err;\n\n\treturn crypto_gcm_verify(req, pctx);\n}\n\nstatic int crypto_gcm_init_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_instance *inst = (void *)tfm->__crt_alg;\n\tstruct gcm_instance_ctx *ictx = crypto_instance_ctx(inst);\n\tstruct crypto_gcm_ctx *ctx = crypto_tfm_ctx(tfm);\n\tstruct crypto_ablkcipher *ctr;\n\tstruct crypto_ahash *ghash;\n\tunsigned long align;\n\tint err;\n\n\tghash = crypto_spawn_ahash(&ictx->ghash);\n\tif (IS_ERR(ghash))\n\t\treturn PTR_ERR(ghash);\n\n\tctr = crypto_spawn_skcipher(&ictx->ctr);\n\terr = PTR_ERR(ctr);\n\tif (IS_ERR(ctr))\n\t\tgoto err_free_hash;\n\n\tctx->ctr = ctr;\n\tctx->ghash = ghash;\n\n\talign = crypto_tfm_alg_alignmask(tfm);\n\talign &= ~(crypto_tfm_ctx_alignment() - 1);\n\ttfm->crt_aead.reqsize = align +\n\t\toffsetof(struct crypto_gcm_req_priv_ctx, u) +\n\t\tmax(sizeof(struct ablkcipher_request) +\n\t\t    crypto_ablkcipher_reqsize(ctr),\n\t\t    sizeof(struct ahash_request) +\n\t\t    crypto_ahash_reqsize(ghash));\n\n\treturn 0;\n\nerr_free_hash:\n\tcrypto_free_ahash(ghash);\n\treturn err;\n}\n\nstatic void crypto_gcm_exit_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_gcm_ctx *ctx = crypto_tfm_ctx(tfm);\n\n\tcrypto_free_ahash(ctx->ghash);\n\tcrypto_free_ablkcipher(ctx->ctr);\n}\n\nstatic struct crypto_instance *crypto_gcm_alloc_common(struct rtattr **tb,\n\t\t\t\t\t\t       const char *full_name,\n\t\t\t\t\t\t       const char *ctr_name,\n\t\t\t\t\t\t       const char *ghash_name)\n{\n\tstruct crypto_attr_type *algt;\n\tstruct crypto_instance *inst;\n\tstruct crypto_alg *ctr;\n\tstruct crypto_alg *ghash_alg;\n\tstruct ahash_alg *ghash_ahash_alg;\n\tstruct gcm_instance_ctx *ctx;\n\tint err;\n\n\talgt = crypto_get_attr_type(tb);\n\tif (IS_ERR(algt))\n\t\treturn ERR_CAST(algt);\n\n\tif ((algt->type ^ CRYPTO_ALG_TYPE_AEAD) & algt->mask)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tghash_alg = crypto_find_alg(ghash_name, &crypto_ahash_type,\n\t\t\t\t    CRYPTO_ALG_TYPE_HASH,\n\t\t\t\t    CRYPTO_ALG_TYPE_AHASH_MASK);\n\tif (IS_ERR(ghash_alg))\n\t\treturn ERR_CAST(ghash_alg);\n\n\terr = -ENOMEM;\n\tinst = kzalloc(sizeof(*inst) + sizeof(*ctx), GFP_KERNEL);\n\tif (!inst)\n\t\tgoto out_put_ghash;\n\n\tctx = crypto_instance_ctx(inst);\n\tghash_ahash_alg = container_of(ghash_alg, struct ahash_alg, halg.base);\n\terr = crypto_init_ahash_spawn(&ctx->ghash, &ghash_ahash_alg->halg,\n\t\t\t\t      inst);\n\tif (err)\n\t\tgoto err_free_inst;\n\n\tcrypto_set_skcipher_spawn(&ctx->ctr, inst);\n\terr = crypto_grab_skcipher(&ctx->ctr, ctr_name, 0,\n\t\t\t\t   crypto_requires_sync(algt->type,\n\t\t\t\t\t\t\talgt->mask));\n\tif (err)\n\t\tgoto err_drop_ghash;\n\n\tctr = crypto_skcipher_spawn_alg(&ctx->ctr);\n\n\t/* We only support 16-byte blocks. */\n\tif (ctr->cra_ablkcipher.ivsize != 16)\n\t\tgoto out_put_ctr;\n\n\t/* Not a stream cipher? */\n\terr = -EINVAL;\n\tif (ctr->cra_blocksize != 1)\n\t\tgoto out_put_ctr;\n\n\terr = -ENAMETOOLONG;\n\tif (snprintf(inst->alg.cra_driver_name, CRYPTO_MAX_ALG_NAME,\n\t\t     \"gcm_base(%s,%s)\", ctr->cra_driver_name,\n\t\t     ghash_alg->cra_driver_name) >=\n\t    CRYPTO_MAX_ALG_NAME)\n\t\tgoto out_put_ctr;\n\n\tmemcpy(inst->alg.cra_name, full_name, CRYPTO_MAX_ALG_NAME);\n\n\tinst->alg.cra_flags = CRYPTO_ALG_TYPE_AEAD;\n\tinst->alg.cra_flags |= ctr->cra_flags & CRYPTO_ALG_ASYNC;\n\tinst->alg.cra_priority = ctr->cra_priority;\n\tinst->alg.cra_blocksize = 1;\n\tinst->alg.cra_alignmask = ctr->cra_alignmask | (__alignof__(u64) - 1);\n\tinst->alg.cra_type = &crypto_aead_type;\n\tinst->alg.cra_aead.ivsize = 16;\n\tinst->alg.cra_aead.maxauthsize = 16;\n\tinst->alg.cra_ctxsize = sizeof(struct crypto_gcm_ctx);\n\tinst->alg.cra_init = crypto_gcm_init_tfm;\n\tinst->alg.cra_exit = crypto_gcm_exit_tfm;\n\tinst->alg.cra_aead.setkey = crypto_gcm_setkey;\n\tinst->alg.cra_aead.setauthsize = crypto_gcm_setauthsize;\n\tinst->alg.cra_aead.encrypt = crypto_gcm_encrypt;\n\tinst->alg.cra_aead.decrypt = crypto_gcm_decrypt;\n\nout:\n\tcrypto_mod_put(ghash_alg);\n\treturn inst;\n\nout_put_ctr:\n\tcrypto_drop_skcipher(&ctx->ctr);\nerr_drop_ghash:\n\tcrypto_drop_ahash(&ctx->ghash);\nerr_free_inst:\n\tkfree(inst);\nout_put_ghash:\n\tinst = ERR_PTR(err);\n\tgoto out;\n}\n\nstatic struct crypto_instance *crypto_gcm_alloc(struct rtattr **tb)\n{\n\tconst char *cipher_name;\n\tchar ctr_name[CRYPTO_MAX_ALG_NAME];\n\tchar full_name[CRYPTO_MAX_ALG_NAME];\n\n\tcipher_name = crypto_attr_alg_name(tb[1]);\n\tif (IS_ERR(cipher_name))\n\t\treturn ERR_CAST(cipher_name);\n\n\tif (snprintf(ctr_name, CRYPTO_MAX_ALG_NAME, \"ctr(%s)\", cipher_name) >=\n\t    CRYPTO_MAX_ALG_NAME)\n\t\treturn ERR_PTR(-ENAMETOOLONG);\n\n\tif (snprintf(full_name, CRYPTO_MAX_ALG_NAME, \"gcm(%s)\", cipher_name) >=\n\t    CRYPTO_MAX_ALG_NAME)\n\t\treturn ERR_PTR(-ENAMETOOLONG);\n\n\treturn crypto_gcm_alloc_common(tb, full_name, ctr_name, \"ghash\");\n}\n\nstatic void crypto_gcm_free(struct crypto_instance *inst)\n{\n\tstruct gcm_instance_ctx *ctx = crypto_instance_ctx(inst);\n\n\tcrypto_drop_skcipher(&ctx->ctr);\n\tcrypto_drop_ahash(&ctx->ghash);\n\tkfree(inst);\n}\n\nstatic struct crypto_template crypto_gcm_tmpl = {\n\t.name = \"gcm\",\n\t.alloc = crypto_gcm_alloc,\n\t.free = crypto_gcm_free,\n\t.module = THIS_MODULE,\n};\n\nstatic struct crypto_instance *crypto_gcm_base_alloc(struct rtattr **tb)\n{\n\tconst char *ctr_name;\n\tconst char *ghash_name;\n\tchar full_name[CRYPTO_MAX_ALG_NAME];\n\n\tctr_name = crypto_attr_alg_name(tb[1]);\n\tif (IS_ERR(ctr_name))\n\t\treturn ERR_CAST(ctr_name);\n\n\tghash_name = crypto_attr_alg_name(tb[2]);\n\tif (IS_ERR(ghash_name))\n\t\treturn ERR_CAST(ghash_name);\n\n\tif (snprintf(full_name, CRYPTO_MAX_ALG_NAME, \"gcm_base(%s,%s)\",\n\t\t     ctr_name, ghash_name) >= CRYPTO_MAX_ALG_NAME)\n\t\treturn ERR_PTR(-ENAMETOOLONG);\n\n\treturn crypto_gcm_alloc_common(tb, full_name, ctr_name, ghash_name);\n}\n\nstatic struct crypto_template crypto_gcm_base_tmpl = {\n\t.name = \"gcm_base\",\n\t.alloc = crypto_gcm_base_alloc,\n\t.free = crypto_gcm_free,\n\t.module = THIS_MODULE,\n};\n\nstatic int crypto_rfc4106_setkey(struct crypto_aead *parent, const u8 *key,\n\t\t\t\t unsigned int keylen)\n{\n\tstruct crypto_rfc4106_ctx *ctx = crypto_aead_ctx(parent);\n\tstruct crypto_aead *child = ctx->child;\n\tint err;\n\n\tif (keylen < 4)\n\t\treturn -EINVAL;\n\n\tkeylen -= 4;\n\tmemcpy(ctx->nonce, key + keylen, 4);\n\n\tcrypto_aead_clear_flags(child, CRYPTO_TFM_REQ_MASK);\n\tcrypto_aead_set_flags(child, crypto_aead_get_flags(parent) &\n\t\t\t\t     CRYPTO_TFM_REQ_MASK);\n\terr = crypto_aead_setkey(child, key, keylen);\n\tcrypto_aead_set_flags(parent, crypto_aead_get_flags(child) &\n\t\t\t\t      CRYPTO_TFM_RES_MASK);\n\n\treturn err;\n}\n\nstatic int crypto_rfc4106_setauthsize(struct crypto_aead *parent,\n\t\t\t\t      unsigned int authsize)\n{\n\tstruct crypto_rfc4106_ctx *ctx = crypto_aead_ctx(parent);\n\n\tswitch (authsize) {\n\tcase 8:\n\tcase 12:\n\tcase 16:\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn crypto_aead_setauthsize(ctx->child, authsize);\n}\n\nstatic struct aead_request *crypto_rfc4106_crypt(struct aead_request *req)\n{\n\tstruct aead_request *subreq = aead_request_ctx(req);\n\tstruct crypto_aead *aead = crypto_aead_reqtfm(req);\n\tstruct crypto_rfc4106_ctx *ctx = crypto_aead_ctx(aead);\n\tstruct crypto_aead *child = ctx->child;\n\tu8 *iv = PTR_ALIGN((u8 *)(subreq + 1) + crypto_aead_reqsize(child),\n\t\t\t   crypto_aead_alignmask(child) + 1);\n\n\tmemcpy(iv, ctx->nonce, 4);\n\tmemcpy(iv + 4, req->iv, 8);\n\n\taead_request_set_tfm(subreq, child);\n\taead_request_set_callback(subreq, req->base.flags, req->base.complete,\n\t\t\t\t  req->base.data);\n\taead_request_set_crypt(subreq, req->src, req->dst, req->cryptlen, iv);\n\taead_request_set_assoc(subreq, req->assoc, req->assoclen);\n\n\treturn subreq;\n}\n\nstatic int crypto_rfc4106_encrypt(struct aead_request *req)\n{\n\treq = crypto_rfc4106_crypt(req);\n\n\treturn crypto_aead_encrypt(req);\n}\n\nstatic int crypto_rfc4106_decrypt(struct aead_request *req)\n{\n\treq = crypto_rfc4106_crypt(req);\n\n\treturn crypto_aead_decrypt(req);\n}\n\nstatic int crypto_rfc4106_init_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_instance *inst = (void *)tfm->__crt_alg;\n\tstruct crypto_aead_spawn *spawn = crypto_instance_ctx(inst);\n\tstruct crypto_rfc4106_ctx *ctx = crypto_tfm_ctx(tfm);\n\tstruct crypto_aead *aead;\n\tunsigned long align;\n\n\taead = crypto_spawn_aead(spawn);\n\tif (IS_ERR(aead))\n\t\treturn PTR_ERR(aead);\n\n\tctx->child = aead;\n\n\talign = crypto_aead_alignmask(aead);\n\talign &= ~(crypto_tfm_ctx_alignment() - 1);\n\ttfm->crt_aead.reqsize = sizeof(struct aead_request) +\n\t\t\t\tALIGN(crypto_aead_reqsize(aead),\n\t\t\t\t      crypto_tfm_ctx_alignment()) +\n\t\t\t\talign + 16;\n\n\treturn 0;\n}\n\nstatic void crypto_rfc4106_exit_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_rfc4106_ctx *ctx = crypto_tfm_ctx(tfm);\n\n\tcrypto_free_aead(ctx->child);\n}\n\nstatic struct crypto_instance *crypto_rfc4106_alloc(struct rtattr **tb)\n{\n\tstruct crypto_attr_type *algt;\n\tstruct crypto_instance *inst;\n\tstruct crypto_aead_spawn *spawn;\n\tstruct crypto_alg *alg;\n\tconst char *ccm_name;\n\tint err;\n\n\talgt = crypto_get_attr_type(tb);\n\tif (IS_ERR(algt))\n\t\treturn ERR_CAST(algt);\n\n\tif ((algt->type ^ CRYPTO_ALG_TYPE_AEAD) & algt->mask)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tccm_name = crypto_attr_alg_name(tb[1]);\n\tif (IS_ERR(ccm_name))\n\t\treturn ERR_CAST(ccm_name);\n\n\tinst = kzalloc(sizeof(*inst) + sizeof(*spawn), GFP_KERNEL);\n\tif (!inst)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tspawn = crypto_instance_ctx(inst);\n\tcrypto_set_aead_spawn(spawn, inst);\n\terr = crypto_grab_aead(spawn, ccm_name, 0,\n\t\t\t       crypto_requires_sync(algt->type, algt->mask));\n\tif (err)\n\t\tgoto out_free_inst;\n\n\talg = crypto_aead_spawn_alg(spawn);\n\n\terr = -EINVAL;\n\n\t/* We only support 16-byte blocks. */\n\tif (alg->cra_aead.ivsize != 16)\n\t\tgoto out_drop_alg;\n\n\t/* Not a stream cipher? */\n\tif (alg->cra_blocksize != 1)\n\t\tgoto out_drop_alg;\n\n\terr = -ENAMETOOLONG;\n\tif (snprintf(inst->alg.cra_name, CRYPTO_MAX_ALG_NAME,\n\t\t     \"rfc4106(%s)\", alg->cra_name) >= CRYPTO_MAX_ALG_NAME ||\n\t    snprintf(inst->alg.cra_driver_name, CRYPTO_MAX_ALG_NAME,\n\t\t     \"rfc4106(%s)\", alg->cra_driver_name) >=\n\t    CRYPTO_MAX_ALG_NAME)\n\t\tgoto out_drop_alg;\n\n\tinst->alg.cra_flags = CRYPTO_ALG_TYPE_AEAD;\n\tinst->alg.cra_flags |= alg->cra_flags & CRYPTO_ALG_ASYNC;\n\tinst->alg.cra_priority = alg->cra_priority;\n\tinst->alg.cra_blocksize = 1;\n\tinst->alg.cra_alignmask = alg->cra_alignmask;\n\tinst->alg.cra_type = &crypto_nivaead_type;\n\n\tinst->alg.cra_aead.ivsize = 8;\n\tinst->alg.cra_aead.maxauthsize = 16;\n\n\tinst->alg.cra_ctxsize = sizeof(struct crypto_rfc4106_ctx);\n\n\tinst->alg.cra_init = crypto_rfc4106_init_tfm;\n\tinst->alg.cra_exit = crypto_rfc4106_exit_tfm;\n\n\tinst->alg.cra_aead.setkey = crypto_rfc4106_setkey;\n\tinst->alg.cra_aead.setauthsize = crypto_rfc4106_setauthsize;\n\tinst->alg.cra_aead.encrypt = crypto_rfc4106_encrypt;\n\tinst->alg.cra_aead.decrypt = crypto_rfc4106_decrypt;\n\n\tinst->alg.cra_aead.geniv = \"seqiv\";\n\nout:\n\treturn inst;\n\nout_drop_alg:\n\tcrypto_drop_aead(spawn);\nout_free_inst:\n\tkfree(inst);\n\tinst = ERR_PTR(err);\n\tgoto out;\n}\n\nstatic void crypto_rfc4106_free(struct crypto_instance *inst)\n{\n\tcrypto_drop_spawn(crypto_instance_ctx(inst));\n\tkfree(inst);\n}\n\nstatic struct crypto_template crypto_rfc4106_tmpl = {\n\t.name = \"rfc4106\",\n\t.alloc = crypto_rfc4106_alloc,\n\t.free = crypto_rfc4106_free,\n\t.module = THIS_MODULE,\n};\n\nstatic inline struct crypto_rfc4543_req_ctx *crypto_rfc4543_reqctx(\n\tstruct aead_request *req)\n{\n\tunsigned long align = crypto_aead_alignmask(crypto_aead_reqtfm(req));\n\n\treturn (void *)PTR_ALIGN((u8 *)aead_request_ctx(req), align + 1);\n}\n\nstatic int crypto_rfc4543_setkey(struct crypto_aead *parent, const u8 *key,\n\t\t\t\t unsigned int keylen)\n{\n\tstruct crypto_rfc4543_ctx *ctx = crypto_aead_ctx(parent);\n\tstruct crypto_aead *child = ctx->child;\n\tint err;\n\n\tif (keylen < 4)\n\t\treturn -EINVAL;\n\n\tkeylen -= 4;\n\tmemcpy(ctx->nonce, key + keylen, 4);\n\n\tcrypto_aead_clear_flags(child, CRYPTO_TFM_REQ_MASK);\n\tcrypto_aead_set_flags(child, crypto_aead_get_flags(parent) &\n\t\t\t\t     CRYPTO_TFM_REQ_MASK);\n\terr = crypto_aead_setkey(child, key, keylen);\n\tcrypto_aead_set_flags(parent, crypto_aead_get_flags(child) &\n\t\t\t\t      CRYPTO_TFM_RES_MASK);\n\n\treturn err;\n}\n\nstatic int crypto_rfc4543_setauthsize(struct crypto_aead *parent,\n\t\t\t\t      unsigned int authsize)\n{\n\tstruct crypto_rfc4543_ctx *ctx = crypto_aead_ctx(parent);\n\n\tif (authsize != 16)\n\t\treturn -EINVAL;\n\n\treturn crypto_aead_setauthsize(ctx->child, authsize);\n}\n\nstatic void crypto_rfc4543_done(struct crypto_async_request *areq, int err)\n{\n\tstruct aead_request *req = areq->data;\n\tstruct crypto_aead *aead = crypto_aead_reqtfm(req);\n\tstruct crypto_rfc4543_req_ctx *rctx = crypto_rfc4543_reqctx(req);\n\n\tif (!err) {\n\t\tscatterwalk_map_and_copy(rctx->auth_tag, req->dst,\n\t\t\t\t\t req->cryptlen,\n\t\t\t\t\t crypto_aead_authsize(aead), 1);\n\t}\n\n\taead_request_complete(req, err);\n}\n\nstatic struct aead_request *crypto_rfc4543_crypt(struct aead_request *req,\n\t\t\t\t\t\t bool enc)\n{\n\tstruct crypto_aead *aead = crypto_aead_reqtfm(req);\n\tstruct crypto_rfc4543_ctx *ctx = crypto_aead_ctx(aead);\n\tstruct crypto_rfc4543_req_ctx *rctx = crypto_rfc4543_reqctx(req);\n\tstruct aead_request *subreq = &rctx->subreq;\n\tstruct scatterlist *src = req->src;\n\tstruct scatterlist *cipher = rctx->cipher;\n\tstruct scatterlist *payload = rctx->payload;\n\tstruct scatterlist *assoc = rctx->assoc;\n\tunsigned int authsize = crypto_aead_authsize(aead);\n\tunsigned int assoclen = req->assoclen;\n\tstruct page *srcp;\n\tu8 *vsrc;\n\tu8 *iv = PTR_ALIGN((u8 *)(rctx + 1) + crypto_aead_reqsize(ctx->child),\n\t\t\t   crypto_aead_alignmask(ctx->child) + 1);\n\n\tmemcpy(iv, ctx->nonce, 4);\n\tmemcpy(iv + 4, req->iv, 8);\n\n\t/* construct cipher/plaintext */\n\tif (enc)\n\t\tmemset(rctx->auth_tag, 0, authsize);\n\telse\n\t\tscatterwalk_map_and_copy(rctx->auth_tag, src,\n\t\t\t\t\t req->cryptlen - authsize,\n\t\t\t\t\t authsize, 0);\n\n\tsg_init_one(cipher, rctx->auth_tag, authsize);\n\n\t/* construct the aad */\n\tsrcp = sg_page(src);\n\tvsrc = PageHighMem(srcp) ? NULL : page_address(srcp) + src->offset;\n\n\tsg_init_table(payload, 2);\n\tsg_set_buf(payload, req->iv, 8);\n\tscatterwalk_crypto_chain(payload, src, vsrc == req->iv + 8, 2);\n\tassoclen += 8 + req->cryptlen - (enc ? 0 : authsize);\n\n\tif (req->assoc->length == req->assoclen) {\n\t\tsg_init_table(assoc, 2);\n\t\tsg_set_page(assoc, sg_page(req->assoc), req->assoc->length,\n\t\t\t    req->assoc->offset);\n\t} else {\n\t\tBUG_ON(req->assoclen > sizeof(rctx->assocbuf));\n\n\t\tscatterwalk_map_and_copy(rctx->assocbuf, req->assoc, 0,\n\t\t\t\t\t req->assoclen, 0);\n\n\t\tsg_init_table(assoc, 2);\n\t\tsg_set_buf(assoc, rctx->assocbuf, req->assoclen);\n\t}\n\tscatterwalk_crypto_chain(assoc, payload, 0, 2);\n\n\taead_request_set_tfm(subreq, ctx->child);\n\taead_request_set_callback(subreq, req->base.flags, crypto_rfc4543_done,\n\t\t\t\t  req);\n\taead_request_set_crypt(subreq, cipher, cipher, enc ? 0 : authsize, iv);\n\taead_request_set_assoc(subreq, assoc, assoclen);\n\n\treturn subreq;\n}\n\nstatic int crypto_rfc4543_copy_src_to_dst(struct aead_request *req, bool enc)\n{\n\tstruct crypto_aead *aead = crypto_aead_reqtfm(req);\n\tstruct crypto_rfc4543_ctx *ctx = crypto_aead_ctx(aead);\n\tunsigned int authsize = crypto_aead_authsize(aead);\n\tunsigned int nbytes = req->cryptlen - (enc ? 0 : authsize);\n\tstruct blkcipher_desc desc = {\n\t\t.tfm = ctx->null,\n\t};\n\n\treturn crypto_blkcipher_encrypt(&desc, req->dst, req->src, nbytes);\n}\n\nstatic int crypto_rfc4543_encrypt(struct aead_request *req)\n{\n\tstruct crypto_aead *aead = crypto_aead_reqtfm(req);\n\tstruct crypto_rfc4543_req_ctx *rctx = crypto_rfc4543_reqctx(req);\n\tstruct aead_request *subreq;\n\tint err;\n\n\tif (req->src != req->dst) {\n\t\terr = crypto_rfc4543_copy_src_to_dst(req, true);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tsubreq = crypto_rfc4543_crypt(req, true);\n\terr = crypto_aead_encrypt(subreq);\n\tif (err)\n\t\treturn err;\n\n\tscatterwalk_map_and_copy(rctx->auth_tag, req->dst, req->cryptlen,\n\t\t\t\t crypto_aead_authsize(aead), 1);\n\n\treturn 0;\n}\n\nstatic int crypto_rfc4543_decrypt(struct aead_request *req)\n{\n\tint err;\n\n\tif (req->src != req->dst) {\n\t\terr = crypto_rfc4543_copy_src_to_dst(req, false);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\treq = crypto_rfc4543_crypt(req, false);\n\n\treturn crypto_aead_decrypt(req);\n}\n\nstatic int crypto_rfc4543_init_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_instance *inst = (void *)tfm->__crt_alg;\n\tstruct crypto_rfc4543_instance_ctx *ictx = crypto_instance_ctx(inst);\n\tstruct crypto_aead_spawn *spawn = &ictx->aead;\n\tstruct crypto_rfc4543_ctx *ctx = crypto_tfm_ctx(tfm);\n\tstruct crypto_aead *aead;\n\tstruct crypto_blkcipher *null;\n\tunsigned long align;\n\tint err = 0;\n\n\taead = crypto_spawn_aead(spawn);\n\tif (IS_ERR(aead))\n\t\treturn PTR_ERR(aead);\n\n\tnull = crypto_spawn_blkcipher(&ictx->null.base);\n\terr = PTR_ERR(null);\n\tif (IS_ERR(null))\n\t\tgoto err_free_aead;\n\n\tctx->child = aead;\n\tctx->null = null;\n\n\talign = crypto_aead_alignmask(aead);\n\talign &= ~(crypto_tfm_ctx_alignment() - 1);\n\ttfm->crt_aead.reqsize = sizeof(struct crypto_rfc4543_req_ctx) +\n\t\t\t\tALIGN(crypto_aead_reqsize(aead),\n\t\t\t\t      crypto_tfm_ctx_alignment()) +\n\t\t\t\talign + 16;\n\n\treturn 0;\n\nerr_free_aead:\n\tcrypto_free_aead(aead);\n\treturn err;\n}\n\nstatic void crypto_rfc4543_exit_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_rfc4543_ctx *ctx = crypto_tfm_ctx(tfm);\n\n\tcrypto_free_aead(ctx->child);\n\tcrypto_free_blkcipher(ctx->null);\n}\n\nstatic struct crypto_instance *crypto_rfc4543_alloc(struct rtattr **tb)\n{\n\tstruct crypto_attr_type *algt;\n\tstruct crypto_instance *inst;\n\tstruct crypto_aead_spawn *spawn;\n\tstruct crypto_alg *alg;\n\tstruct crypto_rfc4543_instance_ctx *ctx;\n\tconst char *ccm_name;\n\tint err;\n\n\talgt = crypto_get_attr_type(tb);\n\tif (IS_ERR(algt))\n\t\treturn ERR_CAST(algt);\n\n\tif ((algt->type ^ CRYPTO_ALG_TYPE_AEAD) & algt->mask)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tccm_name = crypto_attr_alg_name(tb[1]);\n\tif (IS_ERR(ccm_name))\n\t\treturn ERR_CAST(ccm_name);\n\n\tinst = kzalloc(sizeof(*inst) + sizeof(*ctx), GFP_KERNEL);\n\tif (!inst)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tctx = crypto_instance_ctx(inst);\n\tspawn = &ctx->aead;\n\tcrypto_set_aead_spawn(spawn, inst);\n\terr = crypto_grab_aead(spawn, ccm_name, 0,\n\t\t\t       crypto_requires_sync(algt->type, algt->mask));\n\tif (err)\n\t\tgoto out_free_inst;\n\n\talg = crypto_aead_spawn_alg(spawn);\n\n\tcrypto_set_skcipher_spawn(&ctx->null, inst);\n\terr = crypto_grab_skcipher(&ctx->null, \"ecb(cipher_null)\", 0,\n\t\t\t\t   CRYPTO_ALG_ASYNC);\n\tif (err)\n\t\tgoto out_drop_alg;\n\n\tcrypto_skcipher_spawn_alg(&ctx->null);\n\n\terr = -EINVAL;\n\n\t/* We only support 16-byte blocks. */\n\tif (alg->cra_aead.ivsize != 16)\n\t\tgoto out_drop_ecbnull;\n\n\t/* Not a stream cipher? */\n\tif (alg->cra_blocksize != 1)\n\t\tgoto out_drop_ecbnull;\n\n\terr = -ENAMETOOLONG;\n\tif (snprintf(inst->alg.cra_name, CRYPTO_MAX_ALG_NAME,\n\t\t     \"rfc4543(%s)\", alg->cra_name) >= CRYPTO_MAX_ALG_NAME ||\n\t    snprintf(inst->alg.cra_driver_name, CRYPTO_MAX_ALG_NAME,\n\t\t     \"rfc4543(%s)\", alg->cra_driver_name) >=\n\t    CRYPTO_MAX_ALG_NAME)\n\t\tgoto out_drop_ecbnull;\n\n\tinst->alg.cra_flags = CRYPTO_ALG_TYPE_AEAD;\n\tinst->alg.cra_flags |= alg->cra_flags & CRYPTO_ALG_ASYNC;\n\tinst->alg.cra_priority = alg->cra_priority;\n\tinst->alg.cra_blocksize = 1;\n\tinst->alg.cra_alignmask = alg->cra_alignmask;\n\tinst->alg.cra_type = &crypto_nivaead_type;\n\n\tinst->alg.cra_aead.ivsize = 8;\n\tinst->alg.cra_aead.maxauthsize = 16;\n\n\tinst->alg.cra_ctxsize = sizeof(struct crypto_rfc4543_ctx);\n\n\tinst->alg.cra_init = crypto_rfc4543_init_tfm;\n\tinst->alg.cra_exit = crypto_rfc4543_exit_tfm;\n\n\tinst->alg.cra_aead.setkey = crypto_rfc4543_setkey;\n\tinst->alg.cra_aead.setauthsize = crypto_rfc4543_setauthsize;\n\tinst->alg.cra_aead.encrypt = crypto_rfc4543_encrypt;\n\tinst->alg.cra_aead.decrypt = crypto_rfc4543_decrypt;\n\n\tinst->alg.cra_aead.geniv = \"seqiv\";\n\nout:\n\treturn inst;\n\nout_drop_ecbnull:\n\tcrypto_drop_skcipher(&ctx->null);\nout_drop_alg:\n\tcrypto_drop_aead(spawn);\nout_free_inst:\n\tkfree(inst);\n\tinst = ERR_PTR(err);\n\tgoto out;\n}\n\nstatic void crypto_rfc4543_free(struct crypto_instance *inst)\n{\n\tstruct crypto_rfc4543_instance_ctx *ctx = crypto_instance_ctx(inst);\n\n\tcrypto_drop_aead(&ctx->aead);\n\tcrypto_drop_skcipher(&ctx->null);\n\n\tkfree(inst);\n}\n\nstatic struct crypto_template crypto_rfc4543_tmpl = {\n\t.name = \"rfc4543\",\n\t.alloc = crypto_rfc4543_alloc,\n\t.free = crypto_rfc4543_free,\n\t.module = THIS_MODULE,\n};\n\nstatic int __init crypto_gcm_module_init(void)\n{\n\tint err;\n\n\tgcm_zeroes = kzalloc(16, GFP_KERNEL);\n\tif (!gcm_zeroes)\n\t\treturn -ENOMEM;\n\n\terr = crypto_register_template(&crypto_gcm_base_tmpl);\n\tif (err)\n\t\tgoto out;\n\n\terr = crypto_register_template(&crypto_gcm_tmpl);\n\tif (err)\n\t\tgoto out_undo_base;\n\n\terr = crypto_register_template(&crypto_rfc4106_tmpl);\n\tif (err)\n\t\tgoto out_undo_gcm;\n\n\terr = crypto_register_template(&crypto_rfc4543_tmpl);\n\tif (err)\n\t\tgoto out_undo_rfc4106;\n\n\treturn 0;\n\nout_undo_rfc4106:\n\tcrypto_unregister_template(&crypto_rfc4106_tmpl);\nout_undo_gcm:\n\tcrypto_unregister_template(&crypto_gcm_tmpl);\nout_undo_base:\n\tcrypto_unregister_template(&crypto_gcm_base_tmpl);\nout:\n\tkfree(gcm_zeroes);\n\treturn err;\n}\n\nstatic void __exit crypto_gcm_module_exit(void)\n{\n\tkfree(gcm_zeroes);\n\tcrypto_unregister_template(&crypto_rfc4543_tmpl);\n\tcrypto_unregister_template(&crypto_rfc4106_tmpl);\n\tcrypto_unregister_template(&crypto_gcm_tmpl);\n\tcrypto_unregister_template(&crypto_gcm_base_tmpl);\n}\n\nmodule_init(crypto_gcm_module_init);\nmodule_exit(crypto_gcm_module_exit);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"Galois/Counter Mode\");\nMODULE_AUTHOR(\"Mikko Herranen <mh1@iki.fi>\");\nMODULE_ALIAS_CRYPTO(\"gcm_base\");\nMODULE_ALIAS_CRYPTO(\"rfc4106\");\nMODULE_ALIAS_CRYPTO(\"rfc4543\");\n", "/*\n * Cryptographic API.\n *\n * HMAC: Keyed-Hashing for Message Authentication (RFC2104).\n *\n * Copyright (c) 2002 James Morris <jmorris@intercode.com.au>\n * Copyright (c) 2006 Herbert Xu <herbert@gondor.apana.org.au>\n *\n * The HMAC implementation is derived from USAGI.\n * Copyright (c) 2002 Kazunori Miyazawa <miyazawa@linux-ipv6.org> / USAGI\n *\n * This program is free software; you can redistribute it and/or modify it\n * under the terms of the GNU General Public License as published by the Free\n * Software Foundation; either version 2 of the License, or (at your option)\n * any later version.\n *\n */\n\n#include <crypto/internal/hash.h>\n#include <crypto/scatterwalk.h>\n#include <linux/err.h>\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/scatterlist.h>\n#include <linux/string.h>\n\nstruct hmac_ctx {\n\tstruct crypto_shash *hash;\n};\n\nstatic inline void *align_ptr(void *p, unsigned int align)\n{\n\treturn (void *)ALIGN((unsigned long)p, align);\n}\n\nstatic inline struct hmac_ctx *hmac_ctx(struct crypto_shash *tfm)\n{\n\treturn align_ptr(crypto_shash_ctx_aligned(tfm) +\n\t\t\t crypto_shash_statesize(tfm) * 2,\n\t\t\t crypto_tfm_ctx_alignment());\n}\n\nstatic int hmac_setkey(struct crypto_shash *parent,\n\t\t       const u8 *inkey, unsigned int keylen)\n{\n\tint bs = crypto_shash_blocksize(parent);\n\tint ds = crypto_shash_digestsize(parent);\n\tint ss = crypto_shash_statesize(parent);\n\tchar *ipad = crypto_shash_ctx_aligned(parent);\n\tchar *opad = ipad + ss;\n\tstruct hmac_ctx *ctx = align_ptr(opad + ss,\n\t\t\t\t\t crypto_tfm_ctx_alignment());\n\tstruct crypto_shash *hash = ctx->hash;\n\tSHASH_DESC_ON_STACK(shash, hash);\n\tunsigned int i;\n\n\tshash->tfm = hash;\n\tshash->flags = crypto_shash_get_flags(parent)\n\t\t& CRYPTO_TFM_REQ_MAY_SLEEP;\n\n\tif (keylen > bs) {\n\t\tint err;\n\n\t\terr = crypto_shash_digest(shash, inkey, keylen, ipad);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tkeylen = ds;\n\t} else\n\t\tmemcpy(ipad, inkey, keylen);\n\n\tmemset(ipad + keylen, 0, bs - keylen);\n\tmemcpy(opad, ipad, bs);\n\n\tfor (i = 0; i < bs; i++) {\n\t\tipad[i] ^= 0x36;\n\t\topad[i] ^= 0x5c;\n\t}\n\n\treturn crypto_shash_init(shash) ?:\n\t       crypto_shash_update(shash, ipad, bs) ?:\n\t       crypto_shash_export(shash, ipad) ?:\n\t       crypto_shash_init(shash) ?:\n\t       crypto_shash_update(shash, opad, bs) ?:\n\t       crypto_shash_export(shash, opad);\n}\n\nstatic int hmac_export(struct shash_desc *pdesc, void *out)\n{\n\tstruct shash_desc *desc = shash_desc_ctx(pdesc);\n\n\tdesc->flags = pdesc->flags & CRYPTO_TFM_REQ_MAY_SLEEP;\n\n\treturn crypto_shash_export(desc, out);\n}\n\nstatic int hmac_import(struct shash_desc *pdesc, const void *in)\n{\n\tstruct shash_desc *desc = shash_desc_ctx(pdesc);\n\tstruct hmac_ctx *ctx = hmac_ctx(pdesc->tfm);\n\n\tdesc->tfm = ctx->hash;\n\tdesc->flags = pdesc->flags & CRYPTO_TFM_REQ_MAY_SLEEP;\n\n\treturn crypto_shash_import(desc, in);\n}\n\nstatic int hmac_init(struct shash_desc *pdesc)\n{\n\treturn hmac_import(pdesc, crypto_shash_ctx_aligned(pdesc->tfm));\n}\n\nstatic int hmac_update(struct shash_desc *pdesc,\n\t\t       const u8 *data, unsigned int nbytes)\n{\n\tstruct shash_desc *desc = shash_desc_ctx(pdesc);\n\n\tdesc->flags = pdesc->flags & CRYPTO_TFM_REQ_MAY_SLEEP;\n\n\treturn crypto_shash_update(desc, data, nbytes);\n}\n\nstatic int hmac_final(struct shash_desc *pdesc, u8 *out)\n{\n\tstruct crypto_shash *parent = pdesc->tfm;\n\tint ds = crypto_shash_digestsize(parent);\n\tint ss = crypto_shash_statesize(parent);\n\tchar *opad = crypto_shash_ctx_aligned(parent) + ss;\n\tstruct shash_desc *desc = shash_desc_ctx(pdesc);\n\n\tdesc->flags = pdesc->flags & CRYPTO_TFM_REQ_MAY_SLEEP;\n\n\treturn crypto_shash_final(desc, out) ?:\n\t       crypto_shash_import(desc, opad) ?:\n\t       crypto_shash_finup(desc, out, ds, out);\n}\n\nstatic int hmac_finup(struct shash_desc *pdesc, const u8 *data,\n\t\t      unsigned int nbytes, u8 *out)\n{\n\n\tstruct crypto_shash *parent = pdesc->tfm;\n\tint ds = crypto_shash_digestsize(parent);\n\tint ss = crypto_shash_statesize(parent);\n\tchar *opad = crypto_shash_ctx_aligned(parent) + ss;\n\tstruct shash_desc *desc = shash_desc_ctx(pdesc);\n\n\tdesc->flags = pdesc->flags & CRYPTO_TFM_REQ_MAY_SLEEP;\n\n\treturn crypto_shash_finup(desc, data, nbytes, out) ?:\n\t       crypto_shash_import(desc, opad) ?:\n\t       crypto_shash_finup(desc, out, ds, out);\n}\n\nstatic int hmac_init_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_shash *parent = __crypto_shash_cast(tfm);\n\tstruct crypto_shash *hash;\n\tstruct crypto_instance *inst = (void *)tfm->__crt_alg;\n\tstruct crypto_shash_spawn *spawn = crypto_instance_ctx(inst);\n\tstruct hmac_ctx *ctx = hmac_ctx(parent);\n\n\thash = crypto_spawn_shash(spawn);\n\tif (IS_ERR(hash))\n\t\treturn PTR_ERR(hash);\n\n\tparent->descsize = sizeof(struct shash_desc) +\n\t\t\t   crypto_shash_descsize(hash);\n\n\tctx->hash = hash;\n\treturn 0;\n}\n\nstatic void hmac_exit_tfm(struct crypto_tfm *tfm)\n{\n\tstruct hmac_ctx *ctx = hmac_ctx(__crypto_shash_cast(tfm));\n\tcrypto_free_shash(ctx->hash);\n}\n\nstatic int hmac_create(struct crypto_template *tmpl, struct rtattr **tb)\n{\n\tstruct shash_instance *inst;\n\tstruct crypto_alg *alg;\n\tstruct shash_alg *salg;\n\tint err;\n\tint ds;\n\tint ss;\n\n\terr = crypto_check_attr_type(tb, CRYPTO_ALG_TYPE_SHASH);\n\tif (err)\n\t\treturn err;\n\n\tsalg = shash_attr_alg(tb[1], 0, 0);\n\tif (IS_ERR(salg))\n\t\treturn PTR_ERR(salg);\n\n\terr = -EINVAL;\n\tds = salg->digestsize;\n\tss = salg->statesize;\n\talg = &salg->base;\n\tif (ds > alg->cra_blocksize ||\n\t    ss < alg->cra_blocksize)\n\t\tgoto out_put_alg;\n\n\tinst = shash_alloc_instance(\"hmac\", alg);\n\terr = PTR_ERR(inst);\n\tif (IS_ERR(inst))\n\t\tgoto out_put_alg;\n\n\terr = crypto_init_shash_spawn(shash_instance_ctx(inst), salg,\n\t\t\t\t      shash_crypto_instance(inst));\n\tif (err)\n\t\tgoto out_free_inst;\n\n\tinst->alg.base.cra_priority = alg->cra_priority;\n\tinst->alg.base.cra_blocksize = alg->cra_blocksize;\n\tinst->alg.base.cra_alignmask = alg->cra_alignmask;\n\n\tss = ALIGN(ss, alg->cra_alignmask + 1);\n\tinst->alg.digestsize = ds;\n\tinst->alg.statesize = ss;\n\n\tinst->alg.base.cra_ctxsize = sizeof(struct hmac_ctx) +\n\t\t\t\t     ALIGN(ss * 2, crypto_tfm_ctx_alignment());\n\n\tinst->alg.base.cra_init = hmac_init_tfm;\n\tinst->alg.base.cra_exit = hmac_exit_tfm;\n\n\tinst->alg.init = hmac_init;\n\tinst->alg.update = hmac_update;\n\tinst->alg.final = hmac_final;\n\tinst->alg.finup = hmac_finup;\n\tinst->alg.export = hmac_export;\n\tinst->alg.import = hmac_import;\n\tinst->alg.setkey = hmac_setkey;\n\n\terr = shash_register_instance(tmpl, inst);\n\tif (err) {\nout_free_inst:\n\t\tshash_free_instance(shash_crypto_instance(inst));\n\t}\n\nout_put_alg:\n\tcrypto_mod_put(alg);\n\treturn err;\n}\n\nstatic struct crypto_template hmac_tmpl = {\n\t.name = \"hmac\",\n\t.create = hmac_create,\n\t.free = shash_free_instance,\n\t.module = THIS_MODULE,\n};\n\nstatic int __init hmac_module_init(void)\n{\n\treturn crypto_register_template(&hmac_tmpl);\n}\n\nstatic void __exit hmac_module_exit(void)\n{\n\tcrypto_unregister_template(&hmac_tmpl);\n}\n\nmodule_init(hmac_module_init);\nmodule_exit(hmac_module_exit);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"HMAC hash algorithm\");\n", "/* LRW: as defined by Cyril Guyot in\n *\thttp://grouper.ieee.org/groups/1619/email/pdf00017.pdf\n *\n * Copyright (c) 2006 Rik Snel <rsnel@cube.dyndns.org>\n *\n * Based on ecb.c\n * Copyright (c) 2006 Herbert Xu <herbert@gondor.apana.org.au>\n *\n * This program is free software; you can redistribute it and/or modify it\n * under the terms of the GNU General Public License as published by the Free\n * Software Foundation; either version 2 of the License, or (at your option)\n * any later version.\n */\n/* This implementation is checked against the test vectors in the above\n * document and by a test vector provided by Ken Buchanan at\n * http://www.mail-archive.com/stds-p1619@listserv.ieee.org/msg00173.html\n *\n * The test vectors are included in the testing module tcrypt.[ch] */\n\n#include <crypto/algapi.h>\n#include <linux/err.h>\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/scatterlist.h>\n#include <linux/slab.h>\n\n#include <crypto/b128ops.h>\n#include <crypto/gf128mul.h>\n#include <crypto/lrw.h>\n\nstruct priv {\n\tstruct crypto_cipher *child;\n\tstruct lrw_table_ctx table;\n};\n\nstatic inline void setbit128_bbe(void *b, int bit)\n{\n\t__set_bit(bit ^ (0x80 -\n#ifdef __BIG_ENDIAN\n\t\t\t BITS_PER_LONG\n#else\n\t\t\t BITS_PER_BYTE\n#endif\n\t\t\t), b);\n}\n\nint lrw_init_table(struct lrw_table_ctx *ctx, const u8 *tweak)\n{\n\tbe128 tmp = { 0 };\n\tint i;\n\n\tif (ctx->table)\n\t\tgf128mul_free_64k(ctx->table);\n\n\t/* initialize multiplication table for Key2 */\n\tctx->table = gf128mul_init_64k_bbe((be128 *)tweak);\n\tif (!ctx->table)\n\t\treturn -ENOMEM;\n\n\t/* initialize optimization table */\n\tfor (i = 0; i < 128; i++) {\n\t\tsetbit128_bbe(&tmp, i);\n\t\tctx->mulinc[i] = tmp;\n\t\tgf128mul_64k_bbe(&ctx->mulinc[i], ctx->table);\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(lrw_init_table);\n\nvoid lrw_free_table(struct lrw_table_ctx *ctx)\n{\n\tif (ctx->table)\n\t\tgf128mul_free_64k(ctx->table);\n}\nEXPORT_SYMBOL_GPL(lrw_free_table);\n\nstatic int setkey(struct crypto_tfm *parent, const u8 *key,\n\t\t  unsigned int keylen)\n{\n\tstruct priv *ctx = crypto_tfm_ctx(parent);\n\tstruct crypto_cipher *child = ctx->child;\n\tint err, bsize = LRW_BLOCK_SIZE;\n\tconst u8 *tweak = key + keylen - bsize;\n\n\tcrypto_cipher_clear_flags(child, CRYPTO_TFM_REQ_MASK);\n\tcrypto_cipher_set_flags(child, crypto_tfm_get_flags(parent) &\n\t\t\t\t       CRYPTO_TFM_REQ_MASK);\n\terr = crypto_cipher_setkey(child, key, keylen - bsize);\n\tif (err)\n\t\treturn err;\n\tcrypto_tfm_set_flags(parent, crypto_cipher_get_flags(child) &\n\t\t\t\t     CRYPTO_TFM_RES_MASK);\n\n\treturn lrw_init_table(&ctx->table, tweak);\n}\n\nstruct sinfo {\n\tbe128 t;\n\tstruct crypto_tfm *tfm;\n\tvoid (*fn)(struct crypto_tfm *, u8 *, const u8 *);\n};\n\nstatic inline void inc(be128 *iv)\n{\n\tbe64_add_cpu(&iv->b, 1);\n\tif (!iv->b)\n\t\tbe64_add_cpu(&iv->a, 1);\n}\n\nstatic inline void lrw_round(struct sinfo *s, void *dst, const void *src)\n{\n\tbe128_xor(dst, &s->t, src);\t\t/* PP <- T xor P */\n\ts->fn(s->tfm, dst, dst);\t\t/* CC <- E(Key2,PP) */\n\tbe128_xor(dst, dst, &s->t);\t\t/* C <- T xor CC */\n}\n\n/* this returns the number of consequative 1 bits starting\n * from the right, get_index128(00 00 00 00 00 00 ... 00 00 10 FB) = 2 */\nstatic inline int get_index128(be128 *block)\n{\n\tint x;\n\t__be32 *p = (__be32 *) block;\n\n\tfor (p += 3, x = 0; x < 128; p--, x += 32) {\n\t\tu32 val = be32_to_cpup(p);\n\n\t\tif (!~val)\n\t\t\tcontinue;\n\n\t\treturn x + ffz(val);\n\t}\n\n\treturn x;\n}\n\nstatic int crypt(struct blkcipher_desc *d,\n\t\t struct blkcipher_walk *w, struct priv *ctx,\n\t\t void (*fn)(struct crypto_tfm *, u8 *, const u8 *))\n{\n\tint err;\n\tunsigned int avail;\n\tconst int bs = LRW_BLOCK_SIZE;\n\tstruct sinfo s = {\n\t\t.tfm = crypto_cipher_tfm(ctx->child),\n\t\t.fn = fn\n\t};\n\tbe128 *iv;\n\tu8 *wsrc;\n\tu8 *wdst;\n\n\terr = blkcipher_walk_virt(d, w);\n\tif (!(avail = w->nbytes))\n\t\treturn err;\n\n\twsrc = w->src.virt.addr;\n\twdst = w->dst.virt.addr;\n\n\t/* calculate first value of T */\n\tiv = (be128 *)w->iv;\n\ts.t = *iv;\n\n\t/* T <- I*Key2 */\n\tgf128mul_64k_bbe(&s.t, ctx->table.table);\n\n\tgoto first;\n\n\tfor (;;) {\n\t\tdo {\n\t\t\t/* T <- I*Key2, using the optimization\n\t\t\t * discussed in the specification */\n\t\t\tbe128_xor(&s.t, &s.t,\n\t\t\t\t  &ctx->table.mulinc[get_index128(iv)]);\n\t\t\tinc(iv);\n\nfirst:\n\t\t\tlrw_round(&s, wdst, wsrc);\n\n\t\t\twsrc += bs;\n\t\t\twdst += bs;\n\t\t} while ((avail -= bs) >= bs);\n\n\t\terr = blkcipher_walk_done(d, w, avail);\n\t\tif (!(avail = w->nbytes))\n\t\t\tbreak;\n\n\t\twsrc = w->src.virt.addr;\n\t\twdst = w->dst.virt.addr;\n\t}\n\n\treturn err;\n}\n\nstatic int encrypt(struct blkcipher_desc *desc, struct scatterlist *dst,\n\t\t   struct scatterlist *src, unsigned int nbytes)\n{\n\tstruct priv *ctx = crypto_blkcipher_ctx(desc->tfm);\n\tstruct blkcipher_walk w;\n\n\tblkcipher_walk_init(&w, dst, src, nbytes);\n\treturn crypt(desc, &w, ctx,\n\t\t     crypto_cipher_alg(ctx->child)->cia_encrypt);\n}\n\nstatic int decrypt(struct blkcipher_desc *desc, struct scatterlist *dst,\n\t\t   struct scatterlist *src, unsigned int nbytes)\n{\n\tstruct priv *ctx = crypto_blkcipher_ctx(desc->tfm);\n\tstruct blkcipher_walk w;\n\n\tblkcipher_walk_init(&w, dst, src, nbytes);\n\treturn crypt(desc, &w, ctx,\n\t\t     crypto_cipher_alg(ctx->child)->cia_decrypt);\n}\n\nint lrw_crypt(struct blkcipher_desc *desc, struct scatterlist *sdst,\n\t      struct scatterlist *ssrc, unsigned int nbytes,\n\t      struct lrw_crypt_req *req)\n{\n\tconst unsigned int bsize = LRW_BLOCK_SIZE;\n\tconst unsigned int max_blks = req->tbuflen / bsize;\n\tstruct lrw_table_ctx *ctx = req->table_ctx;\n\tstruct blkcipher_walk walk;\n\tunsigned int nblocks;\n\tbe128 *iv, *src, *dst, *t;\n\tbe128 *t_buf = req->tbuf;\n\tint err, i;\n\n\tBUG_ON(max_blks < 1);\n\n\tblkcipher_walk_init(&walk, sdst, ssrc, nbytes);\n\n\terr = blkcipher_walk_virt(desc, &walk);\n\tnbytes = walk.nbytes;\n\tif (!nbytes)\n\t\treturn err;\n\n\tnblocks = min(walk.nbytes / bsize, max_blks);\n\tsrc = (be128 *)walk.src.virt.addr;\n\tdst = (be128 *)walk.dst.virt.addr;\n\n\t/* calculate first value of T */\n\tiv = (be128 *)walk.iv;\n\tt_buf[0] = *iv;\n\n\t/* T <- I*Key2 */\n\tgf128mul_64k_bbe(&t_buf[0], ctx->table);\n\n\ti = 0;\n\tgoto first;\n\n\tfor (;;) {\n\t\tdo {\n\t\t\tfor (i = 0; i < nblocks; i++) {\n\t\t\t\t/* T <- I*Key2, using the optimization\n\t\t\t\t * discussed in the specification */\n\t\t\t\tbe128_xor(&t_buf[i], t,\n\t\t\t\t\t\t&ctx->mulinc[get_index128(iv)]);\n\t\t\t\tinc(iv);\nfirst:\n\t\t\t\tt = &t_buf[i];\n\n\t\t\t\t/* PP <- T xor P */\n\t\t\t\tbe128_xor(dst + i, t, src + i);\n\t\t\t}\n\n\t\t\t/* CC <- E(Key2,PP) */\n\t\t\treq->crypt_fn(req->crypt_ctx, (u8 *)dst,\n\t\t\t\t      nblocks * bsize);\n\n\t\t\t/* C <- T xor CC */\n\t\t\tfor (i = 0; i < nblocks; i++)\n\t\t\t\tbe128_xor(dst + i, dst + i, &t_buf[i]);\n\n\t\t\tsrc += nblocks;\n\t\t\tdst += nblocks;\n\t\t\tnbytes -= nblocks * bsize;\n\t\t\tnblocks = min(nbytes / bsize, max_blks);\n\t\t} while (nblocks > 0);\n\n\t\terr = blkcipher_walk_done(desc, &walk, nbytes);\n\t\tnbytes = walk.nbytes;\n\t\tif (!nbytes)\n\t\t\tbreak;\n\n\t\tnblocks = min(nbytes / bsize, max_blks);\n\t\tsrc = (be128 *)walk.src.virt.addr;\n\t\tdst = (be128 *)walk.dst.virt.addr;\n\t}\n\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(lrw_crypt);\n\nstatic int init_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_cipher *cipher;\n\tstruct crypto_instance *inst = (void *)tfm->__crt_alg;\n\tstruct crypto_spawn *spawn = crypto_instance_ctx(inst);\n\tstruct priv *ctx = crypto_tfm_ctx(tfm);\n\tu32 *flags = &tfm->crt_flags;\n\n\tcipher = crypto_spawn_cipher(spawn);\n\tif (IS_ERR(cipher))\n\t\treturn PTR_ERR(cipher);\n\n\tif (crypto_cipher_blocksize(cipher) != LRW_BLOCK_SIZE) {\n\t\t*flags |= CRYPTO_TFM_RES_BAD_BLOCK_LEN;\n\t\tcrypto_free_cipher(cipher);\n\t\treturn -EINVAL;\n\t}\n\n\tctx->child = cipher;\n\treturn 0;\n}\n\nstatic void exit_tfm(struct crypto_tfm *tfm)\n{\n\tstruct priv *ctx = crypto_tfm_ctx(tfm);\n\n\tlrw_free_table(&ctx->table);\n\tcrypto_free_cipher(ctx->child);\n}\n\nstatic struct crypto_instance *alloc(struct rtattr **tb)\n{\n\tstruct crypto_instance *inst;\n\tstruct crypto_alg *alg;\n\tint err;\n\n\terr = crypto_check_attr_type(tb, CRYPTO_ALG_TYPE_BLKCIPHER);\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\talg = crypto_get_attr_alg(tb, CRYPTO_ALG_TYPE_CIPHER,\n\t\t\t\t  CRYPTO_ALG_TYPE_MASK);\n\tif (IS_ERR(alg))\n\t\treturn ERR_CAST(alg);\n\n\tinst = crypto_alloc_instance(\"lrw\", alg);\n\tif (IS_ERR(inst))\n\t\tgoto out_put_alg;\n\n\tinst->alg.cra_flags = CRYPTO_ALG_TYPE_BLKCIPHER;\n\tinst->alg.cra_priority = alg->cra_priority;\n\tinst->alg.cra_blocksize = alg->cra_blocksize;\n\n\tif (alg->cra_alignmask < 7) inst->alg.cra_alignmask = 7;\n\telse inst->alg.cra_alignmask = alg->cra_alignmask;\n\tinst->alg.cra_type = &crypto_blkcipher_type;\n\n\tif (!(alg->cra_blocksize % 4))\n\t\tinst->alg.cra_alignmask |= 3;\n\tinst->alg.cra_blkcipher.ivsize = alg->cra_blocksize;\n\tinst->alg.cra_blkcipher.min_keysize =\n\t\talg->cra_cipher.cia_min_keysize + alg->cra_blocksize;\n\tinst->alg.cra_blkcipher.max_keysize =\n\t\talg->cra_cipher.cia_max_keysize + alg->cra_blocksize;\n\n\tinst->alg.cra_ctxsize = sizeof(struct priv);\n\n\tinst->alg.cra_init = init_tfm;\n\tinst->alg.cra_exit = exit_tfm;\n\n\tinst->alg.cra_blkcipher.setkey = setkey;\n\tinst->alg.cra_blkcipher.encrypt = encrypt;\n\tinst->alg.cra_blkcipher.decrypt = decrypt;\n\nout_put_alg:\n\tcrypto_mod_put(alg);\n\treturn inst;\n}\n\nstatic void free(struct crypto_instance *inst)\n{\n\tcrypto_drop_spawn(crypto_instance_ctx(inst));\n\tkfree(inst);\n}\n\nstatic struct crypto_template crypto_tmpl = {\n\t.name = \"lrw\",\n\t.alloc = alloc,\n\t.free = free,\n\t.module = THIS_MODULE,\n};\n\nstatic int __init crypto_module_init(void)\n{\n\treturn crypto_register_template(&crypto_tmpl);\n}\n\nstatic void __exit crypto_module_exit(void)\n{\n\tcrypto_unregister_template(&crypto_tmpl);\n}\n\nmodule_init(crypto_module_init);\nmodule_exit(crypto_module_exit);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"LRW block cipher mode\");\n", "/*\n * Software multibuffer async crypto daemon.\n *\n * Copyright (c) 2014 Tim Chen <tim.c.chen@linux.intel.com>\n *\n * Adapted from crypto daemon.\n *\n * This program is free software; you can redistribute it and/or modify it\n * under the terms of the GNU General Public License as published by the Free\n * Software Foundation; either version 2 of the License, or (at your option)\n * any later version.\n *\n */\n\n#include <crypto/algapi.h>\n#include <crypto/internal/hash.h>\n#include <crypto/internal/aead.h>\n#include <crypto/mcryptd.h>\n#include <crypto/crypto_wq.h>\n#include <linux/err.h>\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/list.h>\n#include <linux/module.h>\n#include <linux/scatterlist.h>\n#include <linux/sched.h>\n#include <linux/slab.h>\n#include <linux/hardirq.h>\n\n#define MCRYPTD_MAX_CPU_QLEN 100\n#define MCRYPTD_BATCH 9\n\nstatic void *mcryptd_alloc_instance(struct crypto_alg *alg, unsigned int head,\n\t\t\t\t   unsigned int tail);\n\nstruct mcryptd_flush_list {\n\tstruct list_head list;\n\tstruct mutex lock;\n};\n\nstatic struct mcryptd_flush_list __percpu *mcryptd_flist;\n\nstruct hashd_instance_ctx {\n\tstruct crypto_shash_spawn spawn;\n\tstruct mcryptd_queue *queue;\n};\n\nstatic void mcryptd_queue_worker(struct work_struct *work);\n\nvoid mcryptd_arm_flusher(struct mcryptd_alg_cstate *cstate, unsigned long delay)\n{\n\tstruct mcryptd_flush_list *flist;\n\n\tif (!cstate->flusher_engaged) {\n\t\t/* put the flusher on the flush list */\n\t\tflist = per_cpu_ptr(mcryptd_flist, smp_processor_id());\n\t\tmutex_lock(&flist->lock);\n\t\tlist_add_tail(&cstate->flush_list, &flist->list);\n\t\tcstate->flusher_engaged = true;\n\t\tcstate->next_flush = jiffies + delay;\n\t\tqueue_delayed_work_on(smp_processor_id(), kcrypto_wq,\n\t\t\t&cstate->flush, delay);\n\t\tmutex_unlock(&flist->lock);\n\t}\n}\nEXPORT_SYMBOL(mcryptd_arm_flusher);\n\nstatic int mcryptd_init_queue(struct mcryptd_queue *queue,\n\t\t\t     unsigned int max_cpu_qlen)\n{\n\tint cpu;\n\tstruct mcryptd_cpu_queue *cpu_queue;\n\n\tqueue->cpu_queue = alloc_percpu(struct mcryptd_cpu_queue);\n\tpr_debug(\"mqueue:%p mcryptd_cpu_queue %p\\n\", queue, queue->cpu_queue);\n\tif (!queue->cpu_queue)\n\t\treturn -ENOMEM;\n\tfor_each_possible_cpu(cpu) {\n\t\tcpu_queue = per_cpu_ptr(queue->cpu_queue, cpu);\n\t\tpr_debug(\"cpu_queue #%d %p\\n\", cpu, queue->cpu_queue);\n\t\tcrypto_init_queue(&cpu_queue->queue, max_cpu_qlen);\n\t\tINIT_WORK(&cpu_queue->work, mcryptd_queue_worker);\n\t}\n\treturn 0;\n}\n\nstatic void mcryptd_fini_queue(struct mcryptd_queue *queue)\n{\n\tint cpu;\n\tstruct mcryptd_cpu_queue *cpu_queue;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tcpu_queue = per_cpu_ptr(queue->cpu_queue, cpu);\n\t\tBUG_ON(cpu_queue->queue.qlen);\n\t}\n\tfree_percpu(queue->cpu_queue);\n}\n\nstatic int mcryptd_enqueue_request(struct mcryptd_queue *queue,\n\t\t\t\t  struct crypto_async_request *request,\n\t\t\t\t  struct mcryptd_hash_request_ctx *rctx)\n{\n\tint cpu, err;\n\tstruct mcryptd_cpu_queue *cpu_queue;\n\n\tcpu = get_cpu();\n\tcpu_queue = this_cpu_ptr(queue->cpu_queue);\n\trctx->tag.cpu = cpu;\n\n\terr = crypto_enqueue_request(&cpu_queue->queue, request);\n\tpr_debug(\"enqueue request: cpu %d cpu_queue %p request %p\\n\",\n\t\t cpu, cpu_queue, request);\n\tqueue_work_on(cpu, kcrypto_wq, &cpu_queue->work);\n\tput_cpu();\n\n\treturn err;\n}\n\n/*\n * Try to opportunisticlly flush the partially completed jobs if\n * crypto daemon is the only task running.\n */\nstatic void mcryptd_opportunistic_flush(void)\n{\n\tstruct mcryptd_flush_list *flist;\n\tstruct mcryptd_alg_cstate *cstate;\n\n\tflist = per_cpu_ptr(mcryptd_flist, smp_processor_id());\n\twhile (single_task_running()) {\n\t\tmutex_lock(&flist->lock);\n\t\tif (list_empty(&flist->list)) {\n\t\t\tmutex_unlock(&flist->lock);\n\t\t\treturn;\n\t\t}\n\t\tcstate = list_entry(flist->list.next,\n\t\t\t\tstruct mcryptd_alg_cstate, flush_list);\n\t\tif (!cstate->flusher_engaged) {\n\t\t\tmutex_unlock(&flist->lock);\n\t\t\treturn;\n\t\t}\n\t\tlist_del(&cstate->flush_list);\n\t\tcstate->flusher_engaged = false;\n\t\tmutex_unlock(&flist->lock);\n\t\tcstate->alg_state->flusher(cstate);\n\t}\n}\n\n/*\n * Called in workqueue context, do one real cryption work (via\n * req->complete) and reschedule itself if there are more work to\n * do.\n */\nstatic void mcryptd_queue_worker(struct work_struct *work)\n{\n\tstruct mcryptd_cpu_queue *cpu_queue;\n\tstruct crypto_async_request *req, *backlog;\n\tint i;\n\n\t/*\n\t * Need to loop through more than once for multi-buffer to\n\t * be effective.\n\t */\n\n\tcpu_queue = container_of(work, struct mcryptd_cpu_queue, work);\n\ti = 0;\n\twhile (i < MCRYPTD_BATCH || single_task_running()) {\n\t\t/*\n\t\t * preempt_disable/enable is used to prevent\n\t\t * being preempted by mcryptd_enqueue_request()\n\t\t */\n\t\tlocal_bh_disable();\n\t\tpreempt_disable();\n\t\tbacklog = crypto_get_backlog(&cpu_queue->queue);\n\t\treq = crypto_dequeue_request(&cpu_queue->queue);\n\t\tpreempt_enable();\n\t\tlocal_bh_enable();\n\n\t\tif (!req) {\n\t\t\tmcryptd_opportunistic_flush();\n\t\t\treturn;\n\t\t}\n\n\t\tif (backlog)\n\t\t\tbacklog->complete(backlog, -EINPROGRESS);\n\t\treq->complete(req, 0);\n\t\tif (!cpu_queue->queue.qlen)\n\t\t\treturn;\n\t\t++i;\n\t}\n\tif (cpu_queue->queue.qlen)\n\t\tqueue_work(kcrypto_wq, &cpu_queue->work);\n}\n\nvoid mcryptd_flusher(struct work_struct *__work)\n{\n\tstruct\tmcryptd_alg_cstate\t*alg_cpu_state;\n\tstruct\tmcryptd_alg_state\t*alg_state;\n\tstruct\tmcryptd_flush_list\t*flist;\n\tint\tcpu;\n\n\tcpu = smp_processor_id();\n\talg_cpu_state = container_of(to_delayed_work(__work),\n\t\t\t\t     struct mcryptd_alg_cstate, flush);\n\talg_state = alg_cpu_state->alg_state;\n\tif (alg_cpu_state->cpu != cpu)\n\t\tpr_debug(\"mcryptd error: work on cpu %d, should be cpu %d\\n\",\n\t\t\t\tcpu, alg_cpu_state->cpu);\n\n\tif (alg_cpu_state->flusher_engaged) {\n\t\tflist = per_cpu_ptr(mcryptd_flist, cpu);\n\t\tmutex_lock(&flist->lock);\n\t\tlist_del(&alg_cpu_state->flush_list);\n\t\talg_cpu_state->flusher_engaged = false;\n\t\tmutex_unlock(&flist->lock);\n\t\talg_state->flusher(alg_cpu_state);\n\t}\n}\nEXPORT_SYMBOL_GPL(mcryptd_flusher);\n\nstatic inline struct mcryptd_queue *mcryptd_get_queue(struct crypto_tfm *tfm)\n{\n\tstruct crypto_instance *inst = crypto_tfm_alg_instance(tfm);\n\tstruct mcryptd_instance_ctx *ictx = crypto_instance_ctx(inst);\n\n\treturn ictx->queue;\n}\n\nstatic void *mcryptd_alloc_instance(struct crypto_alg *alg, unsigned int head,\n\t\t\t\t   unsigned int tail)\n{\n\tchar *p;\n\tstruct crypto_instance *inst;\n\tint err;\n\n\tp = kzalloc(head + sizeof(*inst) + tail, GFP_KERNEL);\n\tif (!p)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tinst = (void *)(p + head);\n\n\terr = -ENAMETOOLONG;\n\tif (snprintf(inst->alg.cra_driver_name, CRYPTO_MAX_ALG_NAME,\n\t\t    \"mcryptd(%s)\", alg->cra_driver_name) >= CRYPTO_MAX_ALG_NAME)\n\t\tgoto out_free_inst;\n\n\tmemcpy(inst->alg.cra_name, alg->cra_name, CRYPTO_MAX_ALG_NAME);\n\n\tinst->alg.cra_priority = alg->cra_priority + 50;\n\tinst->alg.cra_blocksize = alg->cra_blocksize;\n\tinst->alg.cra_alignmask = alg->cra_alignmask;\n\nout:\n\treturn p;\n\nout_free_inst:\n\tkfree(p);\n\tp = ERR_PTR(err);\n\tgoto out;\n}\n\nstatic int mcryptd_hash_init_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_instance *inst = crypto_tfm_alg_instance(tfm);\n\tstruct hashd_instance_ctx *ictx = crypto_instance_ctx(inst);\n\tstruct crypto_shash_spawn *spawn = &ictx->spawn;\n\tstruct mcryptd_hash_ctx *ctx = crypto_tfm_ctx(tfm);\n\tstruct crypto_shash *hash;\n\n\thash = crypto_spawn_shash(spawn);\n\tif (IS_ERR(hash))\n\t\treturn PTR_ERR(hash);\n\n\tctx->child = hash;\n\tcrypto_ahash_set_reqsize(__crypto_ahash_cast(tfm),\n\t\t\t\t sizeof(struct mcryptd_hash_request_ctx) +\n\t\t\t\t crypto_shash_descsize(hash));\n\treturn 0;\n}\n\nstatic void mcryptd_hash_exit_tfm(struct crypto_tfm *tfm)\n{\n\tstruct mcryptd_hash_ctx *ctx = crypto_tfm_ctx(tfm);\n\n\tcrypto_free_shash(ctx->child);\n}\n\nstatic int mcryptd_hash_setkey(struct crypto_ahash *parent,\n\t\t\t\t   const u8 *key, unsigned int keylen)\n{\n\tstruct mcryptd_hash_ctx *ctx   = crypto_ahash_ctx(parent);\n\tstruct crypto_shash *child = ctx->child;\n\tint err;\n\n\tcrypto_shash_clear_flags(child, CRYPTO_TFM_REQ_MASK);\n\tcrypto_shash_set_flags(child, crypto_ahash_get_flags(parent) &\n\t\t\t\t      CRYPTO_TFM_REQ_MASK);\n\terr = crypto_shash_setkey(child, key, keylen);\n\tcrypto_ahash_set_flags(parent, crypto_shash_get_flags(child) &\n\t\t\t\t       CRYPTO_TFM_RES_MASK);\n\treturn err;\n}\n\nstatic int mcryptd_hash_enqueue(struct ahash_request *req,\n\t\t\t\tcrypto_completion_t complete)\n{\n\tint ret;\n\n\tstruct mcryptd_hash_request_ctx *rctx = ahash_request_ctx(req);\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct mcryptd_queue *queue =\n\t\tmcryptd_get_queue(crypto_ahash_tfm(tfm));\n\n\trctx->complete = req->base.complete;\n\treq->base.complete = complete;\n\n\tret = mcryptd_enqueue_request(queue, &req->base, rctx);\n\n\treturn ret;\n}\n\nstatic void mcryptd_hash_init(struct crypto_async_request *req_async, int err)\n{\n\tstruct mcryptd_hash_ctx *ctx = crypto_tfm_ctx(req_async->tfm);\n\tstruct crypto_shash *child = ctx->child;\n\tstruct ahash_request *req = ahash_request_cast(req_async);\n\tstruct mcryptd_hash_request_ctx *rctx = ahash_request_ctx(req);\n\tstruct shash_desc *desc = &rctx->desc;\n\n\tif (unlikely(err == -EINPROGRESS))\n\t\tgoto out;\n\n\tdesc->tfm = child;\n\tdesc->flags = CRYPTO_TFM_REQ_MAY_SLEEP;\n\n\terr = crypto_shash_init(desc);\n\n\treq->base.complete = rctx->complete;\n\nout:\n\tlocal_bh_disable();\n\trctx->complete(&req->base, err);\n\tlocal_bh_enable();\n}\n\nstatic int mcryptd_hash_init_enqueue(struct ahash_request *req)\n{\n\treturn mcryptd_hash_enqueue(req, mcryptd_hash_init);\n}\n\nstatic void mcryptd_hash_update(struct crypto_async_request *req_async, int err)\n{\n\tstruct ahash_request *req = ahash_request_cast(req_async);\n\tstruct mcryptd_hash_request_ctx *rctx = ahash_request_ctx(req);\n\n\tif (unlikely(err == -EINPROGRESS))\n\t\tgoto out;\n\n\terr = shash_ahash_mcryptd_update(req, &rctx->desc);\n\tif (err) {\n\t\treq->base.complete = rctx->complete;\n\t\tgoto out;\n\t}\n\n\treturn;\nout:\n\tlocal_bh_disable();\n\trctx->complete(&req->base, err);\n\tlocal_bh_enable();\n}\n\nstatic int mcryptd_hash_update_enqueue(struct ahash_request *req)\n{\n\treturn mcryptd_hash_enqueue(req, mcryptd_hash_update);\n}\n\nstatic void mcryptd_hash_final(struct crypto_async_request *req_async, int err)\n{\n\tstruct ahash_request *req = ahash_request_cast(req_async);\n\tstruct mcryptd_hash_request_ctx *rctx = ahash_request_ctx(req);\n\n\tif (unlikely(err == -EINPROGRESS))\n\t\tgoto out;\n\n\terr = shash_ahash_mcryptd_final(req, &rctx->desc);\n\tif (err) {\n\t\treq->base.complete = rctx->complete;\n\t\tgoto out;\n\t}\n\n\treturn;\nout:\n\tlocal_bh_disable();\n\trctx->complete(&req->base, err);\n\tlocal_bh_enable();\n}\n\nstatic int mcryptd_hash_final_enqueue(struct ahash_request *req)\n{\n\treturn mcryptd_hash_enqueue(req, mcryptd_hash_final);\n}\n\nstatic void mcryptd_hash_finup(struct crypto_async_request *req_async, int err)\n{\n\tstruct ahash_request *req = ahash_request_cast(req_async);\n\tstruct mcryptd_hash_request_ctx *rctx = ahash_request_ctx(req);\n\n\tif (unlikely(err == -EINPROGRESS))\n\t\tgoto out;\n\n\terr = shash_ahash_mcryptd_finup(req, &rctx->desc);\n\n\tif (err) {\n\t\treq->base.complete = rctx->complete;\n\t\tgoto out;\n\t}\n\n\treturn;\nout:\n\tlocal_bh_disable();\n\trctx->complete(&req->base, err);\n\tlocal_bh_enable();\n}\n\nstatic int mcryptd_hash_finup_enqueue(struct ahash_request *req)\n{\n\treturn mcryptd_hash_enqueue(req, mcryptd_hash_finup);\n}\n\nstatic void mcryptd_hash_digest(struct crypto_async_request *req_async, int err)\n{\n\tstruct mcryptd_hash_ctx *ctx = crypto_tfm_ctx(req_async->tfm);\n\tstruct crypto_shash *child = ctx->child;\n\tstruct ahash_request *req = ahash_request_cast(req_async);\n\tstruct mcryptd_hash_request_ctx *rctx = ahash_request_ctx(req);\n\tstruct shash_desc *desc = &rctx->desc;\n\n\tif (unlikely(err == -EINPROGRESS))\n\t\tgoto out;\n\n\tdesc->tfm = child;\n\tdesc->flags = CRYPTO_TFM_REQ_MAY_SLEEP;  /* check this again */\n\n\terr = shash_ahash_mcryptd_digest(req, desc);\n\n\tif (err) {\n\t\treq->base.complete = rctx->complete;\n\t\tgoto out;\n\t}\n\n\treturn;\nout:\n\tlocal_bh_disable();\n\trctx->complete(&req->base, err);\n\tlocal_bh_enable();\n}\n\nstatic int mcryptd_hash_digest_enqueue(struct ahash_request *req)\n{\n\treturn mcryptd_hash_enqueue(req, mcryptd_hash_digest);\n}\n\nstatic int mcryptd_hash_export(struct ahash_request *req, void *out)\n{\n\tstruct mcryptd_hash_request_ctx *rctx = ahash_request_ctx(req);\n\n\treturn crypto_shash_export(&rctx->desc, out);\n}\n\nstatic int mcryptd_hash_import(struct ahash_request *req, const void *in)\n{\n\tstruct mcryptd_hash_request_ctx *rctx = ahash_request_ctx(req);\n\n\treturn crypto_shash_import(&rctx->desc, in);\n}\n\nstatic int mcryptd_create_hash(struct crypto_template *tmpl, struct rtattr **tb,\n\t\t\t      struct mcryptd_queue *queue)\n{\n\tstruct hashd_instance_ctx *ctx;\n\tstruct ahash_instance *inst;\n\tstruct shash_alg *salg;\n\tstruct crypto_alg *alg;\n\tint err;\n\n\tsalg = shash_attr_alg(tb[1], 0, 0);\n\tif (IS_ERR(salg))\n\t\treturn PTR_ERR(salg);\n\n\talg = &salg->base;\n\tpr_debug(\"crypto: mcryptd hash alg: %s\\n\", alg->cra_name);\n\tinst = mcryptd_alloc_instance(alg, ahash_instance_headroom(),\n\t\t\t\t\tsizeof(*ctx));\n\terr = PTR_ERR(inst);\n\tif (IS_ERR(inst))\n\t\tgoto out_put_alg;\n\n\tctx = ahash_instance_ctx(inst);\n\tctx->queue = queue;\n\n\terr = crypto_init_shash_spawn(&ctx->spawn, salg,\n\t\t\t\t      ahash_crypto_instance(inst));\n\tif (err)\n\t\tgoto out_free_inst;\n\n\tinst->alg.halg.base.cra_flags = CRYPTO_ALG_ASYNC;\n\n\tinst->alg.halg.digestsize = salg->digestsize;\n\tinst->alg.halg.base.cra_ctxsize = sizeof(struct mcryptd_hash_ctx);\n\n\tinst->alg.halg.base.cra_init = mcryptd_hash_init_tfm;\n\tinst->alg.halg.base.cra_exit = mcryptd_hash_exit_tfm;\n\n\tinst->alg.init   = mcryptd_hash_init_enqueue;\n\tinst->alg.update = mcryptd_hash_update_enqueue;\n\tinst->alg.final  = mcryptd_hash_final_enqueue;\n\tinst->alg.finup  = mcryptd_hash_finup_enqueue;\n\tinst->alg.export = mcryptd_hash_export;\n\tinst->alg.import = mcryptd_hash_import;\n\tinst->alg.setkey = mcryptd_hash_setkey;\n\tinst->alg.digest = mcryptd_hash_digest_enqueue;\n\n\terr = ahash_register_instance(tmpl, inst);\n\tif (err) {\n\t\tcrypto_drop_shash(&ctx->spawn);\nout_free_inst:\n\t\tkfree(inst);\n\t}\n\nout_put_alg:\n\tcrypto_mod_put(alg);\n\treturn err;\n}\n\nstatic struct mcryptd_queue mqueue;\n\nstatic int mcryptd_create(struct crypto_template *tmpl, struct rtattr **tb)\n{\n\tstruct crypto_attr_type *algt;\n\n\talgt = crypto_get_attr_type(tb);\n\tif (IS_ERR(algt))\n\t\treturn PTR_ERR(algt);\n\n\tswitch (algt->type & algt->mask & CRYPTO_ALG_TYPE_MASK) {\n\tcase CRYPTO_ALG_TYPE_DIGEST:\n\t\treturn mcryptd_create_hash(tmpl, tb, &mqueue);\n\tbreak;\n\t}\n\n\treturn -EINVAL;\n}\n\nstatic void mcryptd_free(struct crypto_instance *inst)\n{\n\tstruct mcryptd_instance_ctx *ctx = crypto_instance_ctx(inst);\n\tstruct hashd_instance_ctx *hctx = crypto_instance_ctx(inst);\n\n\tswitch (inst->alg.cra_flags & CRYPTO_ALG_TYPE_MASK) {\n\tcase CRYPTO_ALG_TYPE_AHASH:\n\t\tcrypto_drop_shash(&hctx->spawn);\n\t\tkfree(ahash_instance(inst));\n\t\treturn;\n\tdefault:\n\t\tcrypto_drop_spawn(&ctx->spawn);\n\t\tkfree(inst);\n\t}\n}\n\nstatic struct crypto_template mcryptd_tmpl = {\n\t.name = \"mcryptd\",\n\t.create = mcryptd_create,\n\t.free = mcryptd_free,\n\t.module = THIS_MODULE,\n};\n\nstruct mcryptd_ahash *mcryptd_alloc_ahash(const char *alg_name,\n\t\t\t\t\tu32 type, u32 mask)\n{\n\tchar mcryptd_alg_name[CRYPTO_MAX_ALG_NAME];\n\tstruct crypto_ahash *tfm;\n\n\tif (snprintf(mcryptd_alg_name, CRYPTO_MAX_ALG_NAME,\n\t\t     \"mcryptd(%s)\", alg_name) >= CRYPTO_MAX_ALG_NAME)\n\t\treturn ERR_PTR(-EINVAL);\n\ttfm = crypto_alloc_ahash(mcryptd_alg_name, type, mask);\n\tif (IS_ERR(tfm))\n\t\treturn ERR_CAST(tfm);\n\tif (tfm->base.__crt_alg->cra_module != THIS_MODULE) {\n\t\tcrypto_free_ahash(tfm);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\treturn __mcryptd_ahash_cast(tfm);\n}\nEXPORT_SYMBOL_GPL(mcryptd_alloc_ahash);\n\nint shash_ahash_mcryptd_digest(struct ahash_request *req,\n\t\t\t       struct shash_desc *desc)\n{\n\tint err;\n\n\terr = crypto_shash_init(desc) ?:\n\t      shash_ahash_mcryptd_finup(req, desc);\n\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(shash_ahash_mcryptd_digest);\n\nint shash_ahash_mcryptd_update(struct ahash_request *req,\n\t\t\t       struct shash_desc *desc)\n{\n\tstruct crypto_shash *tfm = desc->tfm;\n\tstruct shash_alg *shash = crypto_shash_alg(tfm);\n\n\t/* alignment is to be done by multi-buffer crypto algorithm if needed */\n\n\treturn shash->update(desc, NULL, 0);\n}\nEXPORT_SYMBOL_GPL(shash_ahash_mcryptd_update);\n\nint shash_ahash_mcryptd_finup(struct ahash_request *req,\n\t\t\t      struct shash_desc *desc)\n{\n\tstruct crypto_shash *tfm = desc->tfm;\n\tstruct shash_alg *shash = crypto_shash_alg(tfm);\n\n\t/* alignment is to be done by multi-buffer crypto algorithm if needed */\n\n\treturn shash->finup(desc, NULL, 0, req->result);\n}\nEXPORT_SYMBOL_GPL(shash_ahash_mcryptd_finup);\n\nint shash_ahash_mcryptd_final(struct ahash_request *req,\n\t\t\t      struct shash_desc *desc)\n{\n\tstruct crypto_shash *tfm = desc->tfm;\n\tstruct shash_alg *shash = crypto_shash_alg(tfm);\n\n\t/* alignment is to be done by multi-buffer crypto algorithm if needed */\n\n\treturn shash->final(desc, req->result);\n}\nEXPORT_SYMBOL_GPL(shash_ahash_mcryptd_final);\n\nstruct crypto_shash *mcryptd_ahash_child(struct mcryptd_ahash *tfm)\n{\n\tstruct mcryptd_hash_ctx *ctx = crypto_ahash_ctx(&tfm->base);\n\n\treturn ctx->child;\n}\nEXPORT_SYMBOL_GPL(mcryptd_ahash_child);\n\nstruct shash_desc *mcryptd_shash_desc(struct ahash_request *req)\n{\n\tstruct mcryptd_hash_request_ctx *rctx = ahash_request_ctx(req);\n\treturn &rctx->desc;\n}\nEXPORT_SYMBOL_GPL(mcryptd_shash_desc);\n\nvoid mcryptd_free_ahash(struct mcryptd_ahash *tfm)\n{\n\tcrypto_free_ahash(&tfm->base);\n}\nEXPORT_SYMBOL_GPL(mcryptd_free_ahash);\n\n\nstatic int __init mcryptd_init(void)\n{\n\tint err, cpu;\n\tstruct mcryptd_flush_list *flist;\n\n\tmcryptd_flist = alloc_percpu(struct mcryptd_flush_list);\n\tfor_each_possible_cpu(cpu) {\n\t\tflist = per_cpu_ptr(mcryptd_flist, cpu);\n\t\tINIT_LIST_HEAD(&flist->list);\n\t\tmutex_init(&flist->lock);\n\t}\n\n\terr = mcryptd_init_queue(&mqueue, MCRYPTD_MAX_CPU_QLEN);\n\tif (err) {\n\t\tfree_percpu(mcryptd_flist);\n\t\treturn err;\n\t}\n\n\terr = crypto_register_template(&mcryptd_tmpl);\n\tif (err) {\n\t\tmcryptd_fini_queue(&mqueue);\n\t\tfree_percpu(mcryptd_flist);\n\t}\n\n\treturn err;\n}\n\nstatic void __exit mcryptd_exit(void)\n{\n\tmcryptd_fini_queue(&mqueue);\n\tcrypto_unregister_template(&mcryptd_tmpl);\n\tfree_percpu(mcryptd_flist);\n}\n\nsubsys_initcall(mcryptd_init);\nmodule_exit(mcryptd_exit);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"Software async multibuffer crypto daemon\");\n", "/*\n * PCBC: Propagating Cipher Block Chaining mode\n *\n * Copyright (C) 2006 Red Hat, Inc. All Rights Reserved.\n * Written by David Howells (dhowells@redhat.com)\n *\n * Derived from cbc.c\n * - Copyright (c) 2006 Herbert Xu <herbert@gondor.apana.org.au>\n *\n * This program is free software; you can redistribute it and/or modify it\n * under the terms of the GNU General Public License as published by the Free\n * Software Foundation; either version 2 of the License, or (at your option)\n * any later version.\n *\n */\n\n#include <crypto/algapi.h>\n#include <linux/err.h>\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/scatterlist.h>\n#include <linux/slab.h>\n\nstruct crypto_pcbc_ctx {\n\tstruct crypto_cipher *child;\n};\n\nstatic int crypto_pcbc_setkey(struct crypto_tfm *parent, const u8 *key,\n\t\t\t      unsigned int keylen)\n{\n\tstruct crypto_pcbc_ctx *ctx = crypto_tfm_ctx(parent);\n\tstruct crypto_cipher *child = ctx->child;\n\tint err;\n\n\tcrypto_cipher_clear_flags(child, CRYPTO_TFM_REQ_MASK);\n\tcrypto_cipher_set_flags(child, crypto_tfm_get_flags(parent) &\n\t\t\t\tCRYPTO_TFM_REQ_MASK);\n\terr = crypto_cipher_setkey(child, key, keylen);\n\tcrypto_tfm_set_flags(parent, crypto_cipher_get_flags(child) &\n\t\t\t     CRYPTO_TFM_RES_MASK);\n\treturn err;\n}\n\nstatic int crypto_pcbc_encrypt_segment(struct blkcipher_desc *desc,\n\t\t\t\t       struct blkcipher_walk *walk,\n\t\t\t\t       struct crypto_cipher *tfm)\n{\n\tvoid (*fn)(struct crypto_tfm *, u8 *, const u8 *) =\n\t\tcrypto_cipher_alg(tfm)->cia_encrypt;\n\tint bsize = crypto_cipher_blocksize(tfm);\n\tunsigned int nbytes = walk->nbytes;\n\tu8 *src = walk->src.virt.addr;\n\tu8 *dst = walk->dst.virt.addr;\n\tu8 *iv = walk->iv;\n\n\tdo {\n\t\tcrypto_xor(iv, src, bsize);\n\t\tfn(crypto_cipher_tfm(tfm), dst, iv);\n\t\tmemcpy(iv, dst, bsize);\n\t\tcrypto_xor(iv, src, bsize);\n\n\t\tsrc += bsize;\n\t\tdst += bsize;\n\t} while ((nbytes -= bsize) >= bsize);\n\n\treturn nbytes;\n}\n\nstatic int crypto_pcbc_encrypt_inplace(struct blkcipher_desc *desc,\n\t\t\t\t       struct blkcipher_walk *walk,\n\t\t\t\t       struct crypto_cipher *tfm)\n{\n\tvoid (*fn)(struct crypto_tfm *, u8 *, const u8 *) =\n\t\tcrypto_cipher_alg(tfm)->cia_encrypt;\n\tint bsize = crypto_cipher_blocksize(tfm);\n\tunsigned int nbytes = walk->nbytes;\n\tu8 *src = walk->src.virt.addr;\n\tu8 *iv = walk->iv;\n\tu8 tmpbuf[bsize];\n\n\tdo {\n\t\tmemcpy(tmpbuf, src, bsize);\n\t\tcrypto_xor(iv, src, bsize);\n\t\tfn(crypto_cipher_tfm(tfm), src, iv);\n\t\tmemcpy(iv, tmpbuf, bsize);\n\t\tcrypto_xor(iv, src, bsize);\n\n\t\tsrc += bsize;\n\t} while ((nbytes -= bsize) >= bsize);\n\n\tmemcpy(walk->iv, iv, bsize);\n\n\treturn nbytes;\n}\n\nstatic int crypto_pcbc_encrypt(struct blkcipher_desc *desc,\n\t\t\t       struct scatterlist *dst, struct scatterlist *src,\n\t\t\t       unsigned int nbytes)\n{\n\tstruct blkcipher_walk walk;\n\tstruct crypto_blkcipher *tfm = desc->tfm;\n\tstruct crypto_pcbc_ctx *ctx = crypto_blkcipher_ctx(tfm);\n\tstruct crypto_cipher *child = ctx->child;\n\tint err;\n\n\tblkcipher_walk_init(&walk, dst, src, nbytes);\n\terr = blkcipher_walk_virt(desc, &walk);\n\n\twhile ((nbytes = walk.nbytes)) {\n\t\tif (walk.src.virt.addr == walk.dst.virt.addr)\n\t\t\tnbytes = crypto_pcbc_encrypt_inplace(desc, &walk,\n\t\t\t\t\t\t\t     child);\n\t\telse\n\t\t\tnbytes = crypto_pcbc_encrypt_segment(desc, &walk,\n\t\t\t\t\t\t\t     child);\n\t\terr = blkcipher_walk_done(desc, &walk, nbytes);\n\t}\n\n\treturn err;\n}\n\nstatic int crypto_pcbc_decrypt_segment(struct blkcipher_desc *desc,\n\t\t\t\t       struct blkcipher_walk *walk,\n\t\t\t\t       struct crypto_cipher *tfm)\n{\n\tvoid (*fn)(struct crypto_tfm *, u8 *, const u8 *) =\n\t\tcrypto_cipher_alg(tfm)->cia_decrypt;\n\tint bsize = crypto_cipher_blocksize(tfm);\n\tunsigned int nbytes = walk->nbytes;\n\tu8 *src = walk->src.virt.addr;\n\tu8 *dst = walk->dst.virt.addr;\n\tu8 *iv = walk->iv;\n\n\tdo {\n\t\tfn(crypto_cipher_tfm(tfm), dst, src);\n\t\tcrypto_xor(dst, iv, bsize);\n\t\tmemcpy(iv, src, bsize);\n\t\tcrypto_xor(iv, dst, bsize);\n\n\t\tsrc += bsize;\n\t\tdst += bsize;\n\t} while ((nbytes -= bsize) >= bsize);\n\n\tmemcpy(walk->iv, iv, bsize);\n\n\treturn nbytes;\n}\n\nstatic int crypto_pcbc_decrypt_inplace(struct blkcipher_desc *desc,\n\t\t\t\t       struct blkcipher_walk *walk,\n\t\t\t\t       struct crypto_cipher *tfm)\n{\n\tvoid (*fn)(struct crypto_tfm *, u8 *, const u8 *) =\n\t\tcrypto_cipher_alg(tfm)->cia_decrypt;\n\tint bsize = crypto_cipher_blocksize(tfm);\n\tunsigned int nbytes = walk->nbytes;\n\tu8 *src = walk->src.virt.addr;\n\tu8 *iv = walk->iv;\n\tu8 tmpbuf[bsize];\n\n\tdo {\n\t\tmemcpy(tmpbuf, src, bsize);\n\t\tfn(crypto_cipher_tfm(tfm), src, src);\n\t\tcrypto_xor(src, iv, bsize);\n\t\tmemcpy(iv, tmpbuf, bsize);\n\t\tcrypto_xor(iv, src, bsize);\n\n\t\tsrc += bsize;\n\t} while ((nbytes -= bsize) >= bsize);\n\n\tmemcpy(walk->iv, iv, bsize);\n\n\treturn nbytes;\n}\n\nstatic int crypto_pcbc_decrypt(struct blkcipher_desc *desc,\n\t\t\t       struct scatterlist *dst, struct scatterlist *src,\n\t\t\t       unsigned int nbytes)\n{\n\tstruct blkcipher_walk walk;\n\tstruct crypto_blkcipher *tfm = desc->tfm;\n\tstruct crypto_pcbc_ctx *ctx = crypto_blkcipher_ctx(tfm);\n\tstruct crypto_cipher *child = ctx->child;\n\tint err;\n\n\tblkcipher_walk_init(&walk, dst, src, nbytes);\n\terr = blkcipher_walk_virt(desc, &walk);\n\n\twhile ((nbytes = walk.nbytes)) {\n\t\tif (walk.src.virt.addr == walk.dst.virt.addr)\n\t\t\tnbytes = crypto_pcbc_decrypt_inplace(desc, &walk,\n\t\t\t\t\t\t\t     child);\n\t\telse\n\t\t\tnbytes = crypto_pcbc_decrypt_segment(desc, &walk,\n\t\t\t\t\t\t\t     child);\n\t\terr = blkcipher_walk_done(desc, &walk, nbytes);\n\t}\n\n\treturn err;\n}\n\nstatic int crypto_pcbc_init_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_instance *inst = (void *)tfm->__crt_alg;\n\tstruct crypto_spawn *spawn = crypto_instance_ctx(inst);\n\tstruct crypto_pcbc_ctx *ctx = crypto_tfm_ctx(tfm);\n\tstruct crypto_cipher *cipher;\n\n\tcipher = crypto_spawn_cipher(spawn);\n\tif (IS_ERR(cipher))\n\t\treturn PTR_ERR(cipher);\n\n\tctx->child = cipher;\n\treturn 0;\n}\n\nstatic void crypto_pcbc_exit_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_pcbc_ctx *ctx = crypto_tfm_ctx(tfm);\n\tcrypto_free_cipher(ctx->child);\n}\n\nstatic struct crypto_instance *crypto_pcbc_alloc(struct rtattr **tb)\n{\n\tstruct crypto_instance *inst;\n\tstruct crypto_alg *alg;\n\tint err;\n\n\terr = crypto_check_attr_type(tb, CRYPTO_ALG_TYPE_BLKCIPHER);\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\talg = crypto_get_attr_alg(tb, CRYPTO_ALG_TYPE_CIPHER,\n\t\t\t\t  CRYPTO_ALG_TYPE_MASK);\n\tif (IS_ERR(alg))\n\t\treturn ERR_CAST(alg);\n\n\tinst = crypto_alloc_instance(\"pcbc\", alg);\n\tif (IS_ERR(inst))\n\t\tgoto out_put_alg;\n\n\tinst->alg.cra_flags = CRYPTO_ALG_TYPE_BLKCIPHER;\n\tinst->alg.cra_priority = alg->cra_priority;\n\tinst->alg.cra_blocksize = alg->cra_blocksize;\n\tinst->alg.cra_alignmask = alg->cra_alignmask;\n\tinst->alg.cra_type = &crypto_blkcipher_type;\n\n\t/* We access the data as u32s when xoring. */\n\tinst->alg.cra_alignmask |= __alignof__(u32) - 1;\n\n\tinst->alg.cra_blkcipher.ivsize = alg->cra_blocksize;\n\tinst->alg.cra_blkcipher.min_keysize = alg->cra_cipher.cia_min_keysize;\n\tinst->alg.cra_blkcipher.max_keysize = alg->cra_cipher.cia_max_keysize;\n\n\tinst->alg.cra_ctxsize = sizeof(struct crypto_pcbc_ctx);\n\n\tinst->alg.cra_init = crypto_pcbc_init_tfm;\n\tinst->alg.cra_exit = crypto_pcbc_exit_tfm;\n\n\tinst->alg.cra_blkcipher.setkey = crypto_pcbc_setkey;\n\tinst->alg.cra_blkcipher.encrypt = crypto_pcbc_encrypt;\n\tinst->alg.cra_blkcipher.decrypt = crypto_pcbc_decrypt;\n\nout_put_alg:\n\tcrypto_mod_put(alg);\n\treturn inst;\n}\n\nstatic void crypto_pcbc_free(struct crypto_instance *inst)\n{\n\tcrypto_drop_spawn(crypto_instance_ctx(inst));\n\tkfree(inst);\n}\n\nstatic struct crypto_template crypto_pcbc_tmpl = {\n\t.name = \"pcbc\",\n\t.alloc = crypto_pcbc_alloc,\n\t.free = crypto_pcbc_free,\n\t.module = THIS_MODULE,\n};\n\nstatic int __init crypto_pcbc_module_init(void)\n{\n\treturn crypto_register_template(&crypto_pcbc_tmpl);\n}\n\nstatic void __exit crypto_pcbc_module_exit(void)\n{\n\tcrypto_unregister_template(&crypto_pcbc_tmpl);\n}\n\nmodule_init(crypto_pcbc_module_init);\nmodule_exit(crypto_pcbc_module_exit);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"PCBC block cipher algorithm\");\n", "/*\n * pcrypt - Parallel crypto wrapper.\n *\n * Copyright (C) 2009 secunet Security Networks AG\n * Copyright (C) 2009 Steffen Klassert <steffen.klassert@secunet.com>\n *\n * This program is free software; you can redistribute it and/or modify it\n * under the terms and conditions of the GNU General Public License,\n * version 2, as published by the Free Software Foundation.\n *\n * This program is distributed in the hope it will be useful, but WITHOUT\n * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for\n * more details.\n *\n * You should have received a copy of the GNU General Public License along with\n * this program; if not, write to the Free Software Foundation, Inc.,\n * 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.\n */\n\n#include <crypto/algapi.h>\n#include <crypto/internal/aead.h>\n#include <linux/err.h>\n#include <linux/init.h>\n#include <linux/module.h>\n#include <linux/slab.h>\n#include <linux/notifier.h>\n#include <linux/kobject.h>\n#include <linux/cpu.h>\n#include <crypto/pcrypt.h>\n\nstruct padata_pcrypt {\n\tstruct padata_instance *pinst;\n\tstruct workqueue_struct *wq;\n\n\t/*\n\t * Cpumask for callback CPUs. It should be\n\t * equal to serial cpumask of corresponding padata instance,\n\t * so it is updated when padata notifies us about serial\n\t * cpumask change.\n\t *\n\t * cb_cpumask is protected by RCU. This fact prevents us from\n\t * using cpumask_var_t directly because the actual type of\n\t * cpumsak_var_t depends on kernel configuration(particularly on\n\t * CONFIG_CPUMASK_OFFSTACK macro). Depending on the configuration\n\t * cpumask_var_t may be either a pointer to the struct cpumask\n\t * or a variable allocated on the stack. Thus we can not safely use\n\t * cpumask_var_t with RCU operations such as rcu_assign_pointer or\n\t * rcu_dereference. So cpumask_var_t is wrapped with struct\n\t * pcrypt_cpumask which makes possible to use it with RCU.\n\t */\n\tstruct pcrypt_cpumask {\n\t\tcpumask_var_t mask;\n\t} *cb_cpumask;\n\tstruct notifier_block nblock;\n};\n\nstatic struct padata_pcrypt pencrypt;\nstatic struct padata_pcrypt pdecrypt;\nstatic struct kset           *pcrypt_kset;\n\nstruct pcrypt_instance_ctx {\n\tstruct crypto_spawn spawn;\n\tunsigned int tfm_count;\n};\n\nstruct pcrypt_aead_ctx {\n\tstruct crypto_aead *child;\n\tunsigned int cb_cpu;\n};\n\nstatic int pcrypt_do_parallel(struct padata_priv *padata, unsigned int *cb_cpu,\n\t\t\t      struct padata_pcrypt *pcrypt)\n{\n\tunsigned int cpu_index, cpu, i;\n\tstruct pcrypt_cpumask *cpumask;\n\n\tcpu = *cb_cpu;\n\n\trcu_read_lock_bh();\n\tcpumask = rcu_dereference_bh(pcrypt->cb_cpumask);\n\tif (cpumask_test_cpu(cpu, cpumask->mask))\n\t\t\tgoto out;\n\n\tif (!cpumask_weight(cpumask->mask))\n\t\t\tgoto out;\n\n\tcpu_index = cpu % cpumask_weight(cpumask->mask);\n\n\tcpu = cpumask_first(cpumask->mask);\n\tfor (i = 0; i < cpu_index; i++)\n\t\tcpu = cpumask_next(cpu, cpumask->mask);\n\n\t*cb_cpu = cpu;\n\nout:\n\trcu_read_unlock_bh();\n\treturn padata_do_parallel(pcrypt->pinst, padata, cpu);\n}\n\nstatic int pcrypt_aead_setkey(struct crypto_aead *parent,\n\t\t\t      const u8 *key, unsigned int keylen)\n{\n\tstruct pcrypt_aead_ctx *ctx = crypto_aead_ctx(parent);\n\n\treturn crypto_aead_setkey(ctx->child, key, keylen);\n}\n\nstatic int pcrypt_aead_setauthsize(struct crypto_aead *parent,\n\t\t\t\t   unsigned int authsize)\n{\n\tstruct pcrypt_aead_ctx *ctx = crypto_aead_ctx(parent);\n\n\treturn crypto_aead_setauthsize(ctx->child, authsize);\n}\n\nstatic void pcrypt_aead_serial(struct padata_priv *padata)\n{\n\tstruct pcrypt_request *preq = pcrypt_padata_request(padata);\n\tstruct aead_request *req = pcrypt_request_ctx(preq);\n\n\taead_request_complete(req->base.data, padata->info);\n}\n\nstatic void pcrypt_aead_giv_serial(struct padata_priv *padata)\n{\n\tstruct pcrypt_request *preq = pcrypt_padata_request(padata);\n\tstruct aead_givcrypt_request *req = pcrypt_request_ctx(preq);\n\n\taead_request_complete(req->areq.base.data, padata->info);\n}\n\nstatic void pcrypt_aead_done(struct crypto_async_request *areq, int err)\n{\n\tstruct aead_request *req = areq->data;\n\tstruct pcrypt_request *preq = aead_request_ctx(req);\n\tstruct padata_priv *padata = pcrypt_request_padata(preq);\n\n\tpadata->info = err;\n\treq->base.flags &= ~CRYPTO_TFM_REQ_MAY_SLEEP;\n\n\tpadata_do_serial(padata);\n}\n\nstatic void pcrypt_aead_enc(struct padata_priv *padata)\n{\n\tstruct pcrypt_request *preq = pcrypt_padata_request(padata);\n\tstruct aead_request *req = pcrypt_request_ctx(preq);\n\n\tpadata->info = crypto_aead_encrypt(req);\n\n\tif (padata->info == -EINPROGRESS)\n\t\treturn;\n\n\tpadata_do_serial(padata);\n}\n\nstatic int pcrypt_aead_encrypt(struct aead_request *req)\n{\n\tint err;\n\tstruct pcrypt_request *preq = aead_request_ctx(req);\n\tstruct aead_request *creq = pcrypt_request_ctx(preq);\n\tstruct padata_priv *padata = pcrypt_request_padata(preq);\n\tstruct crypto_aead *aead = crypto_aead_reqtfm(req);\n\tstruct pcrypt_aead_ctx *ctx = crypto_aead_ctx(aead);\n\tu32 flags = aead_request_flags(req);\n\n\tmemset(padata, 0, sizeof(struct padata_priv));\n\n\tpadata->parallel = pcrypt_aead_enc;\n\tpadata->serial = pcrypt_aead_serial;\n\n\taead_request_set_tfm(creq, ctx->child);\n\taead_request_set_callback(creq, flags & ~CRYPTO_TFM_REQ_MAY_SLEEP,\n\t\t\t\t  pcrypt_aead_done, req);\n\taead_request_set_crypt(creq, req->src, req->dst,\n\t\t\t       req->cryptlen, req->iv);\n\taead_request_set_assoc(creq, req->assoc, req->assoclen);\n\n\terr = pcrypt_do_parallel(padata, &ctx->cb_cpu, &pencrypt);\n\tif (!err)\n\t\treturn -EINPROGRESS;\n\n\treturn err;\n}\n\nstatic void pcrypt_aead_dec(struct padata_priv *padata)\n{\n\tstruct pcrypt_request *preq = pcrypt_padata_request(padata);\n\tstruct aead_request *req = pcrypt_request_ctx(preq);\n\n\tpadata->info = crypto_aead_decrypt(req);\n\n\tif (padata->info == -EINPROGRESS)\n\t\treturn;\n\n\tpadata_do_serial(padata);\n}\n\nstatic int pcrypt_aead_decrypt(struct aead_request *req)\n{\n\tint err;\n\tstruct pcrypt_request *preq = aead_request_ctx(req);\n\tstruct aead_request *creq = pcrypt_request_ctx(preq);\n\tstruct padata_priv *padata = pcrypt_request_padata(preq);\n\tstruct crypto_aead *aead = crypto_aead_reqtfm(req);\n\tstruct pcrypt_aead_ctx *ctx = crypto_aead_ctx(aead);\n\tu32 flags = aead_request_flags(req);\n\n\tmemset(padata, 0, sizeof(struct padata_priv));\n\n\tpadata->parallel = pcrypt_aead_dec;\n\tpadata->serial = pcrypt_aead_serial;\n\n\taead_request_set_tfm(creq, ctx->child);\n\taead_request_set_callback(creq, flags & ~CRYPTO_TFM_REQ_MAY_SLEEP,\n\t\t\t\t  pcrypt_aead_done, req);\n\taead_request_set_crypt(creq, req->src, req->dst,\n\t\t\t       req->cryptlen, req->iv);\n\taead_request_set_assoc(creq, req->assoc, req->assoclen);\n\n\terr = pcrypt_do_parallel(padata, &ctx->cb_cpu, &pdecrypt);\n\tif (!err)\n\t\treturn -EINPROGRESS;\n\n\treturn err;\n}\n\nstatic void pcrypt_aead_givenc(struct padata_priv *padata)\n{\n\tstruct pcrypt_request *preq = pcrypt_padata_request(padata);\n\tstruct aead_givcrypt_request *req = pcrypt_request_ctx(preq);\n\n\tpadata->info = crypto_aead_givencrypt(req);\n\n\tif (padata->info == -EINPROGRESS)\n\t\treturn;\n\n\tpadata_do_serial(padata);\n}\n\nstatic int pcrypt_aead_givencrypt(struct aead_givcrypt_request *req)\n{\n\tint err;\n\tstruct aead_request *areq = &req->areq;\n\tstruct pcrypt_request *preq = aead_request_ctx(areq);\n\tstruct aead_givcrypt_request *creq = pcrypt_request_ctx(preq);\n\tstruct padata_priv *padata = pcrypt_request_padata(preq);\n\tstruct crypto_aead *aead = aead_givcrypt_reqtfm(req);\n\tstruct pcrypt_aead_ctx *ctx = crypto_aead_ctx(aead);\n\tu32 flags = aead_request_flags(areq);\n\n\tmemset(padata, 0, sizeof(struct padata_priv));\n\n\tpadata->parallel = pcrypt_aead_givenc;\n\tpadata->serial = pcrypt_aead_giv_serial;\n\n\taead_givcrypt_set_tfm(creq, ctx->child);\n\taead_givcrypt_set_callback(creq, flags & ~CRYPTO_TFM_REQ_MAY_SLEEP,\n\t\t\t\t   pcrypt_aead_done, areq);\n\taead_givcrypt_set_crypt(creq, areq->src, areq->dst,\n\t\t\t\tareq->cryptlen, areq->iv);\n\taead_givcrypt_set_assoc(creq, areq->assoc, areq->assoclen);\n\taead_givcrypt_set_giv(creq, req->giv, req->seq);\n\n\terr = pcrypt_do_parallel(padata, &ctx->cb_cpu, &pencrypt);\n\tif (!err)\n\t\treturn -EINPROGRESS;\n\n\treturn err;\n}\n\nstatic int pcrypt_aead_init_tfm(struct crypto_tfm *tfm)\n{\n\tint cpu, cpu_index;\n\tstruct crypto_instance *inst = crypto_tfm_alg_instance(tfm);\n\tstruct pcrypt_instance_ctx *ictx = crypto_instance_ctx(inst);\n\tstruct pcrypt_aead_ctx *ctx = crypto_tfm_ctx(tfm);\n\tstruct crypto_aead *cipher;\n\n\tictx->tfm_count++;\n\n\tcpu_index = ictx->tfm_count % cpumask_weight(cpu_online_mask);\n\n\tctx->cb_cpu = cpumask_first(cpu_online_mask);\n\tfor (cpu = 0; cpu < cpu_index; cpu++)\n\t\tctx->cb_cpu = cpumask_next(ctx->cb_cpu, cpu_online_mask);\n\n\tcipher = crypto_spawn_aead(crypto_instance_ctx(inst));\n\n\tif (IS_ERR(cipher))\n\t\treturn PTR_ERR(cipher);\n\n\tctx->child = cipher;\n\ttfm->crt_aead.reqsize = sizeof(struct pcrypt_request)\n\t\t+ sizeof(struct aead_givcrypt_request)\n\t\t+ crypto_aead_reqsize(cipher);\n\n\treturn 0;\n}\n\nstatic void pcrypt_aead_exit_tfm(struct crypto_tfm *tfm)\n{\n\tstruct pcrypt_aead_ctx *ctx = crypto_tfm_ctx(tfm);\n\n\tcrypto_free_aead(ctx->child);\n}\n\nstatic struct crypto_instance *pcrypt_alloc_instance(struct crypto_alg *alg)\n{\n\tstruct crypto_instance *inst;\n\tstruct pcrypt_instance_ctx *ctx;\n\tint err;\n\n\tinst = kzalloc(sizeof(*inst) + sizeof(*ctx), GFP_KERNEL);\n\tif (!inst) {\n\t\tinst = ERR_PTR(-ENOMEM);\n\t\tgoto out;\n\t}\n\n\terr = -ENAMETOOLONG;\n\tif (snprintf(inst->alg.cra_driver_name, CRYPTO_MAX_ALG_NAME,\n\t\t     \"pcrypt(%s)\", alg->cra_driver_name) >= CRYPTO_MAX_ALG_NAME)\n\t\tgoto out_free_inst;\n\n\tmemcpy(inst->alg.cra_name, alg->cra_name, CRYPTO_MAX_ALG_NAME);\n\n\tctx = crypto_instance_ctx(inst);\n\terr = crypto_init_spawn(&ctx->spawn, alg, inst,\n\t\t\t\tCRYPTO_ALG_TYPE_MASK);\n\tif (err)\n\t\tgoto out_free_inst;\n\n\tinst->alg.cra_priority = alg->cra_priority + 100;\n\tinst->alg.cra_blocksize = alg->cra_blocksize;\n\tinst->alg.cra_alignmask = alg->cra_alignmask;\n\nout:\n\treturn inst;\n\nout_free_inst:\n\tkfree(inst);\n\tinst = ERR_PTR(err);\n\tgoto out;\n}\n\nstatic struct crypto_instance *pcrypt_alloc_aead(struct rtattr **tb,\n\t\t\t\t\t\t u32 type, u32 mask)\n{\n\tstruct crypto_instance *inst;\n\tstruct crypto_alg *alg;\n\n\talg = crypto_get_attr_alg(tb, type, (mask & CRYPTO_ALG_TYPE_MASK));\n\tif (IS_ERR(alg))\n\t\treturn ERR_CAST(alg);\n\n\tinst = pcrypt_alloc_instance(alg);\n\tif (IS_ERR(inst))\n\t\tgoto out_put_alg;\n\n\tinst->alg.cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC;\n\tinst->alg.cra_type = &crypto_aead_type;\n\n\tinst->alg.cra_aead.ivsize = alg->cra_aead.ivsize;\n\tinst->alg.cra_aead.geniv = alg->cra_aead.geniv;\n\tinst->alg.cra_aead.maxauthsize = alg->cra_aead.maxauthsize;\n\n\tinst->alg.cra_ctxsize = sizeof(struct pcrypt_aead_ctx);\n\n\tinst->alg.cra_init = pcrypt_aead_init_tfm;\n\tinst->alg.cra_exit = pcrypt_aead_exit_tfm;\n\n\tinst->alg.cra_aead.setkey = pcrypt_aead_setkey;\n\tinst->alg.cra_aead.setauthsize = pcrypt_aead_setauthsize;\n\tinst->alg.cra_aead.encrypt = pcrypt_aead_encrypt;\n\tinst->alg.cra_aead.decrypt = pcrypt_aead_decrypt;\n\tinst->alg.cra_aead.givencrypt = pcrypt_aead_givencrypt;\n\nout_put_alg:\n\tcrypto_mod_put(alg);\n\treturn inst;\n}\n\nstatic struct crypto_instance *pcrypt_alloc(struct rtattr **tb)\n{\n\tstruct crypto_attr_type *algt;\n\n\talgt = crypto_get_attr_type(tb);\n\tif (IS_ERR(algt))\n\t\treturn ERR_CAST(algt);\n\n\tswitch (algt->type & algt->mask & CRYPTO_ALG_TYPE_MASK) {\n\tcase CRYPTO_ALG_TYPE_AEAD:\n\t\treturn pcrypt_alloc_aead(tb, algt->type, algt->mask);\n\t}\n\n\treturn ERR_PTR(-EINVAL);\n}\n\nstatic void pcrypt_free(struct crypto_instance *inst)\n{\n\tstruct pcrypt_instance_ctx *ctx = crypto_instance_ctx(inst);\n\n\tcrypto_drop_spawn(&ctx->spawn);\n\tkfree(inst);\n}\n\nstatic int pcrypt_cpumask_change_notify(struct notifier_block *self,\n\t\t\t\t\tunsigned long val, void *data)\n{\n\tstruct padata_pcrypt *pcrypt;\n\tstruct pcrypt_cpumask *new_mask, *old_mask;\n\tstruct padata_cpumask *cpumask = (struct padata_cpumask *)data;\n\n\tif (!(val & PADATA_CPU_SERIAL))\n\t\treturn 0;\n\n\tpcrypt = container_of(self, struct padata_pcrypt, nblock);\n\tnew_mask = kmalloc(sizeof(*new_mask), GFP_KERNEL);\n\tif (!new_mask)\n\t\treturn -ENOMEM;\n\tif (!alloc_cpumask_var(&new_mask->mask, GFP_KERNEL)) {\n\t\tkfree(new_mask);\n\t\treturn -ENOMEM;\n\t}\n\n\told_mask = pcrypt->cb_cpumask;\n\n\tcpumask_copy(new_mask->mask, cpumask->cbcpu);\n\trcu_assign_pointer(pcrypt->cb_cpumask, new_mask);\n\tsynchronize_rcu_bh();\n\n\tfree_cpumask_var(old_mask->mask);\n\tkfree(old_mask);\n\treturn 0;\n}\n\nstatic int pcrypt_sysfs_add(struct padata_instance *pinst, const char *name)\n{\n\tint ret;\n\n\tpinst->kobj.kset = pcrypt_kset;\n\tret = kobject_add(&pinst->kobj, NULL, name);\n\tif (!ret)\n\t\tkobject_uevent(&pinst->kobj, KOBJ_ADD);\n\n\treturn ret;\n}\n\nstatic int pcrypt_init_padata(struct padata_pcrypt *pcrypt,\n\t\t\t      const char *name)\n{\n\tint ret = -ENOMEM;\n\tstruct pcrypt_cpumask *mask;\n\n\tget_online_cpus();\n\n\tpcrypt->wq = alloc_workqueue(\"%s\", WQ_MEM_RECLAIM | WQ_CPU_INTENSIVE,\n\t\t\t\t     1, name);\n\tif (!pcrypt->wq)\n\t\tgoto err;\n\n\tpcrypt->pinst = padata_alloc_possible(pcrypt->wq);\n\tif (!pcrypt->pinst)\n\t\tgoto err_destroy_workqueue;\n\n\tmask = kmalloc(sizeof(*mask), GFP_KERNEL);\n\tif (!mask)\n\t\tgoto err_free_padata;\n\tif (!alloc_cpumask_var(&mask->mask, GFP_KERNEL)) {\n\t\tkfree(mask);\n\t\tgoto err_free_padata;\n\t}\n\n\tcpumask_and(mask->mask, cpu_possible_mask, cpu_online_mask);\n\trcu_assign_pointer(pcrypt->cb_cpumask, mask);\n\n\tpcrypt->nblock.notifier_call = pcrypt_cpumask_change_notify;\n\tret = padata_register_cpumask_notifier(pcrypt->pinst, &pcrypt->nblock);\n\tif (ret)\n\t\tgoto err_free_cpumask;\n\n\tret = pcrypt_sysfs_add(pcrypt->pinst, name);\n\tif (ret)\n\t\tgoto err_unregister_notifier;\n\n\tput_online_cpus();\n\n\treturn ret;\n\nerr_unregister_notifier:\n\tpadata_unregister_cpumask_notifier(pcrypt->pinst, &pcrypt->nblock);\nerr_free_cpumask:\n\tfree_cpumask_var(mask->mask);\n\tkfree(mask);\nerr_free_padata:\n\tpadata_free(pcrypt->pinst);\nerr_destroy_workqueue:\n\tdestroy_workqueue(pcrypt->wq);\nerr:\n\tput_online_cpus();\n\n\treturn ret;\n}\n\nstatic void pcrypt_fini_padata(struct padata_pcrypt *pcrypt)\n{\n\tfree_cpumask_var(pcrypt->cb_cpumask->mask);\n\tkfree(pcrypt->cb_cpumask);\n\n\tpadata_stop(pcrypt->pinst);\n\tpadata_unregister_cpumask_notifier(pcrypt->pinst, &pcrypt->nblock);\n\tdestroy_workqueue(pcrypt->wq);\n\tpadata_free(pcrypt->pinst);\n}\n\nstatic struct crypto_template pcrypt_tmpl = {\n\t.name = \"pcrypt\",\n\t.alloc = pcrypt_alloc,\n\t.free = pcrypt_free,\n\t.module = THIS_MODULE,\n};\n\nstatic int __init pcrypt_init(void)\n{\n\tint err = -ENOMEM;\n\n\tpcrypt_kset = kset_create_and_add(\"pcrypt\", NULL, kernel_kobj);\n\tif (!pcrypt_kset)\n\t\tgoto err;\n\n\terr = pcrypt_init_padata(&pencrypt, \"pencrypt\");\n\tif (err)\n\t\tgoto err_unreg_kset;\n\n\terr = pcrypt_init_padata(&pdecrypt, \"pdecrypt\");\n\tif (err)\n\t\tgoto err_deinit_pencrypt;\n\n\tpadata_start(pencrypt.pinst);\n\tpadata_start(pdecrypt.pinst);\n\n\treturn crypto_register_template(&pcrypt_tmpl);\n\nerr_deinit_pencrypt:\n\tpcrypt_fini_padata(&pencrypt);\nerr_unreg_kset:\n\tkset_unregister(pcrypt_kset);\nerr:\n\treturn err;\n}\n\nstatic void __exit pcrypt_exit(void)\n{\n\tpcrypt_fini_padata(&pencrypt);\n\tpcrypt_fini_padata(&pdecrypt);\n\n\tkset_unregister(pcrypt_kset);\n\tcrypto_unregister_template(&pcrypt_tmpl);\n}\n\nmodule_init(pcrypt_init);\nmodule_exit(pcrypt_exit);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_AUTHOR(\"Steffen Klassert <steffen.klassert@secunet.com>\");\nMODULE_DESCRIPTION(\"Parallel crypto wrapper\");\n", "/*\n * seqiv: Sequence Number IV Generator\n *\n * This generator generates an IV based on a sequence number by xoring it\n * with a salt.  This algorithm is mainly useful for CTR and similar modes.\n *\n * Copyright (c) 2007 Herbert Xu <herbert@gondor.apana.org.au>\n *\n * This program is free software; you can redistribute it and/or modify it\n * under the terms of the GNU General Public License as published by the Free\n * Software Foundation; either version 2 of the License, or (at your option)\n * any later version.\n *\n */\n\n#include <crypto/internal/aead.h>\n#include <crypto/internal/skcipher.h>\n#include <crypto/rng.h>\n#include <linux/err.h>\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/slab.h>\n#include <linux/spinlock.h>\n#include <linux/string.h>\n\nstruct seqiv_ctx {\n\tspinlock_t lock;\n\tu8 salt[] __attribute__ ((aligned(__alignof__(u32))));\n};\n\nstatic void seqiv_complete2(struct skcipher_givcrypt_request *req, int err)\n{\n\tstruct ablkcipher_request *subreq = skcipher_givcrypt_reqctx(req);\n\tstruct crypto_ablkcipher *geniv;\n\n\tif (err == -EINPROGRESS)\n\t\treturn;\n\n\tif (err)\n\t\tgoto out;\n\n\tgeniv = skcipher_givcrypt_reqtfm(req);\n\tmemcpy(req->creq.info, subreq->info, crypto_ablkcipher_ivsize(geniv));\n\nout:\n\tkfree(subreq->info);\n}\n\nstatic void seqiv_complete(struct crypto_async_request *base, int err)\n{\n\tstruct skcipher_givcrypt_request *req = base->data;\n\n\tseqiv_complete2(req, err);\n\tskcipher_givcrypt_complete(req, err);\n}\n\nstatic void seqiv_aead_complete2(struct aead_givcrypt_request *req, int err)\n{\n\tstruct aead_request *subreq = aead_givcrypt_reqctx(req);\n\tstruct crypto_aead *geniv;\n\n\tif (err == -EINPROGRESS)\n\t\treturn;\n\n\tif (err)\n\t\tgoto out;\n\n\tgeniv = aead_givcrypt_reqtfm(req);\n\tmemcpy(req->areq.iv, subreq->iv, crypto_aead_ivsize(geniv));\n\nout:\n\tkfree(subreq->iv);\n}\n\nstatic void seqiv_aead_complete(struct crypto_async_request *base, int err)\n{\n\tstruct aead_givcrypt_request *req = base->data;\n\n\tseqiv_aead_complete2(req, err);\n\taead_givcrypt_complete(req, err);\n}\n\nstatic void seqiv_geniv(struct seqiv_ctx *ctx, u8 *info, u64 seq,\n\t\t\tunsigned int ivsize)\n{\n\tunsigned int len = ivsize;\n\n\tif (ivsize > sizeof(u64)) {\n\t\tmemset(info, 0, ivsize - sizeof(u64));\n\t\tlen = sizeof(u64);\n\t}\n\tseq = cpu_to_be64(seq);\n\tmemcpy(info + ivsize - len, &seq, len);\n\tcrypto_xor(info, ctx->salt, ivsize);\n}\n\nstatic int seqiv_givencrypt(struct skcipher_givcrypt_request *req)\n{\n\tstruct crypto_ablkcipher *geniv = skcipher_givcrypt_reqtfm(req);\n\tstruct seqiv_ctx *ctx = crypto_ablkcipher_ctx(geniv);\n\tstruct ablkcipher_request *subreq = skcipher_givcrypt_reqctx(req);\n\tcrypto_completion_t compl;\n\tvoid *data;\n\tu8 *info;\n\tunsigned int ivsize;\n\tint err;\n\n\tablkcipher_request_set_tfm(subreq, skcipher_geniv_cipher(geniv));\n\n\tcompl = req->creq.base.complete;\n\tdata = req->creq.base.data;\n\tinfo = req->creq.info;\n\n\tivsize = crypto_ablkcipher_ivsize(geniv);\n\n\tif (unlikely(!IS_ALIGNED((unsigned long)info,\n\t\t\t\t crypto_ablkcipher_alignmask(geniv) + 1))) {\n\t\tinfo = kmalloc(ivsize, req->creq.base.flags &\n\t\t\t\t       CRYPTO_TFM_REQ_MAY_SLEEP ? GFP_KERNEL:\n\t\t\t\t\t\t\t\t  GFP_ATOMIC);\n\t\tif (!info)\n\t\t\treturn -ENOMEM;\n\n\t\tcompl = seqiv_complete;\n\t\tdata = req;\n\t}\n\n\tablkcipher_request_set_callback(subreq, req->creq.base.flags, compl,\n\t\t\t\t\tdata);\n\tablkcipher_request_set_crypt(subreq, req->creq.src, req->creq.dst,\n\t\t\t\t     req->creq.nbytes, info);\n\n\tseqiv_geniv(ctx, info, req->seq, ivsize);\n\tmemcpy(req->giv, info, ivsize);\n\n\terr = crypto_ablkcipher_encrypt(subreq);\n\tif (unlikely(info != req->creq.info))\n\t\tseqiv_complete2(req, err);\n\treturn err;\n}\n\nstatic int seqiv_aead_givencrypt(struct aead_givcrypt_request *req)\n{\n\tstruct crypto_aead *geniv = aead_givcrypt_reqtfm(req);\n\tstruct seqiv_ctx *ctx = crypto_aead_ctx(geniv);\n\tstruct aead_request *areq = &req->areq;\n\tstruct aead_request *subreq = aead_givcrypt_reqctx(req);\n\tcrypto_completion_t compl;\n\tvoid *data;\n\tu8 *info;\n\tunsigned int ivsize;\n\tint err;\n\n\taead_request_set_tfm(subreq, aead_geniv_base(geniv));\n\n\tcompl = areq->base.complete;\n\tdata = areq->base.data;\n\tinfo = areq->iv;\n\n\tivsize = crypto_aead_ivsize(geniv);\n\n\tif (unlikely(!IS_ALIGNED((unsigned long)info,\n\t\t\t\t crypto_aead_alignmask(geniv) + 1))) {\n\t\tinfo = kmalloc(ivsize, areq->base.flags &\n\t\t\t\t       CRYPTO_TFM_REQ_MAY_SLEEP ? GFP_KERNEL:\n\t\t\t\t\t\t\t\t  GFP_ATOMIC);\n\t\tif (!info)\n\t\t\treturn -ENOMEM;\n\n\t\tcompl = seqiv_aead_complete;\n\t\tdata = req;\n\t}\n\n\taead_request_set_callback(subreq, areq->base.flags, compl, data);\n\taead_request_set_crypt(subreq, areq->src, areq->dst, areq->cryptlen,\n\t\t\t       info);\n\taead_request_set_assoc(subreq, areq->assoc, areq->assoclen);\n\n\tseqiv_geniv(ctx, info, req->seq, ivsize);\n\tmemcpy(req->giv, info, ivsize);\n\n\terr = crypto_aead_encrypt(subreq);\n\tif (unlikely(info != areq->iv))\n\t\tseqiv_aead_complete2(req, err);\n\treturn err;\n}\n\nstatic int seqiv_givencrypt_first(struct skcipher_givcrypt_request *req)\n{\n\tstruct crypto_ablkcipher *geniv = skcipher_givcrypt_reqtfm(req);\n\tstruct seqiv_ctx *ctx = crypto_ablkcipher_ctx(geniv);\n\tint err = 0;\n\n\tspin_lock_bh(&ctx->lock);\n\tif (crypto_ablkcipher_crt(geniv)->givencrypt != seqiv_givencrypt_first)\n\t\tgoto unlock;\n\n\tcrypto_ablkcipher_crt(geniv)->givencrypt = seqiv_givencrypt;\n\terr = crypto_rng_get_bytes(crypto_default_rng, ctx->salt,\n\t\t\t\t   crypto_ablkcipher_ivsize(geniv));\n\nunlock:\n\tspin_unlock_bh(&ctx->lock);\n\n\tif (err)\n\t\treturn err;\n\n\treturn seqiv_givencrypt(req);\n}\n\nstatic int seqiv_aead_givencrypt_first(struct aead_givcrypt_request *req)\n{\n\tstruct crypto_aead *geniv = aead_givcrypt_reqtfm(req);\n\tstruct seqiv_ctx *ctx = crypto_aead_ctx(geniv);\n\tint err = 0;\n\n\tspin_lock_bh(&ctx->lock);\n\tif (crypto_aead_crt(geniv)->givencrypt != seqiv_aead_givencrypt_first)\n\t\tgoto unlock;\n\n\tcrypto_aead_crt(geniv)->givencrypt = seqiv_aead_givencrypt;\n\terr = crypto_rng_get_bytes(crypto_default_rng, ctx->salt,\n\t\t\t\t   crypto_aead_ivsize(geniv));\n\nunlock:\n\tspin_unlock_bh(&ctx->lock);\n\n\tif (err)\n\t\treturn err;\n\n\treturn seqiv_aead_givencrypt(req);\n}\n\nstatic int seqiv_init(struct crypto_tfm *tfm)\n{\n\tstruct crypto_ablkcipher *geniv = __crypto_ablkcipher_cast(tfm);\n\tstruct seqiv_ctx *ctx = crypto_ablkcipher_ctx(geniv);\n\n\tspin_lock_init(&ctx->lock);\n\n\ttfm->crt_ablkcipher.reqsize = sizeof(struct ablkcipher_request);\n\n\treturn skcipher_geniv_init(tfm);\n}\n\nstatic int seqiv_aead_init(struct crypto_tfm *tfm)\n{\n\tstruct crypto_aead *geniv = __crypto_aead_cast(tfm);\n\tstruct seqiv_ctx *ctx = crypto_aead_ctx(geniv);\n\n\tspin_lock_init(&ctx->lock);\n\n\ttfm->crt_aead.reqsize = sizeof(struct aead_request);\n\n\treturn aead_geniv_init(tfm);\n}\n\nstatic struct crypto_template seqiv_tmpl;\n\nstatic struct crypto_instance *seqiv_ablkcipher_alloc(struct rtattr **tb)\n{\n\tstruct crypto_instance *inst;\n\n\tinst = skcipher_geniv_alloc(&seqiv_tmpl, tb, 0, 0);\n\n\tif (IS_ERR(inst))\n\t\tgoto out;\n\n\tinst->alg.cra_ablkcipher.givencrypt = seqiv_givencrypt_first;\n\n\tinst->alg.cra_init = seqiv_init;\n\tinst->alg.cra_exit = skcipher_geniv_exit;\n\n\tinst->alg.cra_ctxsize += inst->alg.cra_ablkcipher.ivsize;\n\nout:\n\treturn inst;\n}\n\nstatic struct crypto_instance *seqiv_aead_alloc(struct rtattr **tb)\n{\n\tstruct crypto_instance *inst;\n\n\tinst = aead_geniv_alloc(&seqiv_tmpl, tb, 0, 0);\n\n\tif (IS_ERR(inst))\n\t\tgoto out;\n\n\tinst->alg.cra_aead.givencrypt = seqiv_aead_givencrypt_first;\n\n\tinst->alg.cra_init = seqiv_aead_init;\n\tinst->alg.cra_exit = aead_geniv_exit;\n\n\tinst->alg.cra_ctxsize = inst->alg.cra_aead.ivsize;\n\nout:\n\treturn inst;\n}\n\nstatic struct crypto_instance *seqiv_alloc(struct rtattr **tb)\n{\n\tstruct crypto_attr_type *algt;\n\tstruct crypto_instance *inst;\n\tint err;\n\n\talgt = crypto_get_attr_type(tb);\n\tif (IS_ERR(algt))\n\t\treturn ERR_CAST(algt);\n\n\terr = crypto_get_default_rng();\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\tif ((algt->type ^ CRYPTO_ALG_TYPE_AEAD) & CRYPTO_ALG_TYPE_MASK)\n\t\tinst = seqiv_ablkcipher_alloc(tb);\n\telse\n\t\tinst = seqiv_aead_alloc(tb);\n\n\tif (IS_ERR(inst))\n\t\tgoto put_rng;\n\n\tinst->alg.cra_alignmask |= __alignof__(u32) - 1;\n\tinst->alg.cra_ctxsize += sizeof(struct seqiv_ctx);\n\nout:\n\treturn inst;\n\nput_rng:\n\tcrypto_put_default_rng();\n\tgoto out;\n}\n\nstatic void seqiv_free(struct crypto_instance *inst)\n{\n\tif ((inst->alg.cra_flags ^ CRYPTO_ALG_TYPE_AEAD) & CRYPTO_ALG_TYPE_MASK)\n\t\tskcipher_geniv_free(inst);\n\telse\n\t\taead_geniv_free(inst);\n\tcrypto_put_default_rng();\n}\n\nstatic struct crypto_template seqiv_tmpl = {\n\t.name = \"seqiv\",\n\t.alloc = seqiv_alloc,\n\t.free = seqiv_free,\n\t.module = THIS_MODULE,\n};\n\nstatic int __init seqiv_module_init(void)\n{\n\treturn crypto_register_template(&seqiv_tmpl);\n}\n\nstatic void __exit seqiv_module_exit(void)\n{\n\tcrypto_unregister_template(&seqiv_tmpl);\n}\n\nmodule_init(seqiv_module_init);\nmodule_exit(seqiv_module_exit);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"Sequence Number IV Generator\");\n", "/*\n * Modified to interface to the Linux kernel\n * Copyright (c) 2009, Intel Corporation.\n *\n * This program is free software; you can redistribute it and/or modify it\n * under the terms and conditions of the GNU General Public License,\n * version 2, as published by the Free Software Foundation.\n *\n * This program is distributed in the hope it will be useful, but WITHOUT\n * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for\n * more details.\n *\n * You should have received a copy of the GNU General Public License along with\n * this program; if not, write to the Free Software Foundation, Inc., 59 Temple\n * Place - Suite 330, Boston, MA 02111-1307 USA.\n */\n\n/* --------------------------------------------------------------------------\n * VMAC and VHASH Implementation by Ted Krovetz (tdk@acm.org) and Wei Dai.\n * This implementation is herby placed in the public domain.\n * The authors offers no warranty. Use at your own risk.\n * Please send bug reports to the authors.\n * Last modified: 17 APR 08, 1700 PDT\n * ----------------------------------------------------------------------- */\n\n#include <linux/init.h>\n#include <linux/types.h>\n#include <linux/crypto.h>\n#include <linux/module.h>\n#include <linux/scatterlist.h>\n#include <asm/byteorder.h>\n#include <crypto/scatterwalk.h>\n#include <crypto/vmac.h>\n#include <crypto/internal/hash.h>\n\n/*\n * Constants and masks\n */\n#define UINT64_C(x) x##ULL\nstatic const u64 p64   = UINT64_C(0xfffffffffffffeff);\t/* 2^64 - 257 prime  */\nstatic const u64 m62   = UINT64_C(0x3fffffffffffffff);\t/* 62-bit mask       */\nstatic const u64 m63   = UINT64_C(0x7fffffffffffffff);\t/* 63-bit mask       */\nstatic const u64 m64   = UINT64_C(0xffffffffffffffff);\t/* 64-bit mask       */\nstatic const u64 mpoly = UINT64_C(0x1fffffff1fffffff);\t/* Poly key mask     */\n\n#define pe64_to_cpup le64_to_cpup\t\t/* Prefer little endian */\n\n#ifdef __LITTLE_ENDIAN\n#define INDEX_HIGH 1\n#define INDEX_LOW 0\n#else\n#define INDEX_HIGH 0\n#define INDEX_LOW 1\n#endif\n\n/*\n * The following routines are used in this implementation. They are\n * written via macros to simulate zero-overhead call-by-reference.\n *\n * MUL64: 64x64->128-bit multiplication\n * PMUL64: assumes top bits cleared on inputs\n * ADD128: 128x128->128-bit addition\n */\n\n#define ADD128(rh, rl, ih, il)\t\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tu64 _il = (il);\t\t\t\t\t\t\\\n\t\t(rl) += (_il);\t\t\t\t\t\t\\\n\t\tif ((rl) < (_il))\t\t\t\t\t\\\n\t\t\t(rh)++;\t\t\t\t\t\t\\\n\t\t(rh) += (ih);\t\t\t\t\t\t\\\n\t} while (0)\n\n#define MUL32(i1, i2)\t((u64)(u32)(i1)*(u32)(i2))\n\n#define PMUL64(rh, rl, i1, i2)\t/* Assumes m doesn't overflow */\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tu64 _i1 = (i1), _i2 = (i2);\t\t\t\t\\\n\t\tu64 m = MUL32(_i1, _i2>>32) + MUL32(_i1>>32, _i2);\t\\\n\t\trh = MUL32(_i1>>32, _i2>>32);\t\t\t\t\\\n\t\trl = MUL32(_i1, _i2);\t\t\t\t\t\\\n\t\tADD128(rh, rl, (m >> 32), (m << 32));\t\t\t\\\n\t} while (0)\n\n#define MUL64(rh, rl, i1, i2)\t\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tu64 _i1 = (i1), _i2 = (i2);\t\t\t\t\\\n\t\tu64 m1 = MUL32(_i1, _i2>>32);\t\t\t\t\\\n\t\tu64 m2 = MUL32(_i1>>32, _i2);\t\t\t\t\\\n\t\trh = MUL32(_i1>>32, _i2>>32);\t\t\t\t\\\n\t\trl = MUL32(_i1, _i2);\t\t\t\t\t\\\n\t\tADD128(rh, rl, (m1 >> 32), (m1 << 32));\t\t\t\\\n\t\tADD128(rh, rl, (m2 >> 32), (m2 << 32));\t\t\t\\\n\t} while (0)\n\n/*\n * For highest performance the L1 NH and L2 polynomial hashes should be\n * carefully implemented to take advantage of one's target architecture.\n * Here these two hash functions are defined multiple time; once for\n * 64-bit architectures, once for 32-bit SSE2 architectures, and once\n * for the rest (32-bit) architectures.\n * For each, nh_16 *must* be defined (works on multiples of 16 bytes).\n * Optionally, nh_vmac_nhbytes can be defined (for multiples of\n * VMAC_NHBYTES), and nh_16_2 and nh_vmac_nhbytes_2 (versions that do two\n * NH computations at once).\n */\n\n#ifdef CONFIG_64BIT\n\n#define nh_16(mp, kp, nw, rh, rl)\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tint i; u64 th, tl;\t\t\t\t\t\\\n\t\trh = rl = 0;\t\t\t\t\t\t\\\n\t\tfor (i = 0; i < nw; i += 2) {\t\t\t\t\\\n\t\t\tMUL64(th, tl, pe64_to_cpup((mp)+i)+(kp)[i],\t\\\n\t\t\t\tpe64_to_cpup((mp)+i+1)+(kp)[i+1]);\t\\\n\t\t\tADD128(rh, rl, th, tl);\t\t\t\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t} while (0)\n\n#define nh_16_2(mp, kp, nw, rh, rl, rh1, rl1)\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tint i; u64 th, tl;\t\t\t\t\t\\\n\t\trh1 = rl1 = rh = rl = 0;\t\t\t\t\\\n\t\tfor (i = 0; i < nw; i += 2) {\t\t\t\t\\\n\t\t\tMUL64(th, tl, pe64_to_cpup((mp)+i)+(kp)[i],\t\\\n\t\t\t\tpe64_to_cpup((mp)+i+1)+(kp)[i+1]);\t\\\n\t\t\tADD128(rh, rl, th, tl);\t\t\t\t\\\n\t\t\tMUL64(th, tl, pe64_to_cpup((mp)+i)+(kp)[i+2],\t\\\n\t\t\t\tpe64_to_cpup((mp)+i+1)+(kp)[i+3]);\t\\\n\t\t\tADD128(rh1, rl1, th, tl);\t\t\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t} while (0)\n\n#if (VMAC_NHBYTES >= 64) /* These versions do 64-bytes of message at a time */\n#define nh_vmac_nhbytes(mp, kp, nw, rh, rl)\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tint i; u64 th, tl;\t\t\t\t\t\\\n\t\trh = rl = 0;\t\t\t\t\t\t\\\n\t\tfor (i = 0; i < nw; i += 8) {\t\t\t\t\\\n\t\t\tMUL64(th, tl, pe64_to_cpup((mp)+i)+(kp)[i],\t\\\n\t\t\t\tpe64_to_cpup((mp)+i+1)+(kp)[i+1]);\t\\\n\t\t\tADD128(rh, rl, th, tl);\t\t\t\t\\\n\t\t\tMUL64(th, tl, pe64_to_cpup((mp)+i+2)+(kp)[i+2],\t\\\n\t\t\t\tpe64_to_cpup((mp)+i+3)+(kp)[i+3]);\t\\\n\t\t\tADD128(rh, rl, th, tl);\t\t\t\t\\\n\t\t\tMUL64(th, tl, pe64_to_cpup((mp)+i+4)+(kp)[i+4],\t\\\n\t\t\t\tpe64_to_cpup((mp)+i+5)+(kp)[i+5]);\t\\\n\t\t\tADD128(rh, rl, th, tl);\t\t\t\t\\\n\t\t\tMUL64(th, tl, pe64_to_cpup((mp)+i+6)+(kp)[i+6],\t\\\n\t\t\t\tpe64_to_cpup((mp)+i+7)+(kp)[i+7]);\t\\\n\t\t\tADD128(rh, rl, th, tl);\t\t\t\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t} while (0)\n\n#define nh_vmac_nhbytes_2(mp, kp, nw, rh, rl, rh1, rl1)\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tint i; u64 th, tl;\t\t\t\t\t\\\n\t\trh1 = rl1 = rh = rl = 0;\t\t\t\t\\\n\t\tfor (i = 0; i < nw; i += 8) {\t\t\t\t\\\n\t\t\tMUL64(th, tl, pe64_to_cpup((mp)+i)+(kp)[i],\t\\\n\t\t\t\tpe64_to_cpup((mp)+i+1)+(kp)[i+1]);\t\\\n\t\t\tADD128(rh, rl, th, tl);\t\t\t\t\\\n\t\t\tMUL64(th, tl, pe64_to_cpup((mp)+i)+(kp)[i+2],\t\\\n\t\t\t\tpe64_to_cpup((mp)+i+1)+(kp)[i+3]);\t\\\n\t\t\tADD128(rh1, rl1, th, tl);\t\t\t\\\n\t\t\tMUL64(th, tl, pe64_to_cpup((mp)+i+2)+(kp)[i+2],\t\\\n\t\t\t\tpe64_to_cpup((mp)+i+3)+(kp)[i+3]);\t\\\n\t\t\tADD128(rh, rl, th, tl);\t\t\t\t\\\n\t\t\tMUL64(th, tl, pe64_to_cpup((mp)+i+2)+(kp)[i+4],\t\\\n\t\t\t\tpe64_to_cpup((mp)+i+3)+(kp)[i+5]);\t\\\n\t\t\tADD128(rh1, rl1, th, tl);\t\t\t\\\n\t\t\tMUL64(th, tl, pe64_to_cpup((mp)+i+4)+(kp)[i+4],\t\\\n\t\t\t\tpe64_to_cpup((mp)+i+5)+(kp)[i+5]);\t\\\n\t\t\tADD128(rh, rl, th, tl);\t\t\t\t\\\n\t\t\tMUL64(th, tl, pe64_to_cpup((mp)+i+4)+(kp)[i+6],\t\\\n\t\t\t\tpe64_to_cpup((mp)+i+5)+(kp)[i+7]);\t\\\n\t\t\tADD128(rh1, rl1, th, tl);\t\t\t\\\n\t\t\tMUL64(th, tl, pe64_to_cpup((mp)+i+6)+(kp)[i+6],\t\\\n\t\t\t\tpe64_to_cpup((mp)+i+7)+(kp)[i+7]);\t\\\n\t\t\tADD128(rh, rl, th, tl);\t\t\t\t\\\n\t\t\tMUL64(th, tl, pe64_to_cpup((mp)+i+6)+(kp)[i+8],\t\\\n\t\t\t\tpe64_to_cpup((mp)+i+7)+(kp)[i+9]);\t\\\n\t\t\tADD128(rh1, rl1, th, tl);\t\t\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t} while (0)\n#endif\n\n#define poly_step(ah, al, kh, kl, mh, ml)\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tu64 t1h, t1l, t2h, t2l, t3h, t3l, z = 0;\t\t\\\n\t\t/* compute ab*cd, put bd into result registers */\t\\\n\t\tPMUL64(t3h, t3l, al, kh);\t\t\t\t\\\n\t\tPMUL64(t2h, t2l, ah, kl);\t\t\t\t\\\n\t\tPMUL64(t1h, t1l, ah, 2*kh);\t\t\t\t\\\n\t\tPMUL64(ah, al, al, kl);\t\t\t\t\t\\\n\t\t/* add 2 * ac to result */\t\t\t\t\\\n\t\tADD128(ah, al, t1h, t1l);\t\t\t\t\\\n\t\t/* add together ad + bc */\t\t\t\t\\\n\t\tADD128(t2h, t2l, t3h, t3l);\t\t\t\t\\\n\t\t/* now (ah,al), (t2l,2*t2h) need summing */\t\t\\\n\t\t/* first add the high registers, carrying into t2h */\t\\\n\t\tADD128(t2h, ah, z, t2l);\t\t\t\t\\\n\t\t/* double t2h and add top bit of ah */\t\t\t\\\n\t\tt2h = 2 * t2h + (ah >> 63);\t\t\t\t\\\n\t\tah &= m63;\t\t\t\t\t\t\\\n\t\t/* now add the low registers */\t\t\t\t\\\n\t\tADD128(ah, al, mh, ml);\t\t\t\t\t\\\n\t\tADD128(ah, al, z, t2h);\t\t\t\t\t\\\n\t} while (0)\n\n#else /* ! CONFIG_64BIT */\n\n#ifndef nh_16\n#define nh_16(mp, kp, nw, rh, rl)\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tu64 t1, t2, m1, m2, t;\t\t\t\t\t\\\n\t\tint i;\t\t\t\t\t\t\t\\\n\t\trh = rl = t = 0;\t\t\t\t\t\\\n\t\tfor (i = 0; i < nw; i += 2)  {\t\t\t\t\\\n\t\t\tt1 = pe64_to_cpup(mp+i) + kp[i];\t\t\\\n\t\t\tt2 = pe64_to_cpup(mp+i+1) + kp[i+1];\t\t\\\n\t\t\tm2 = MUL32(t1 >> 32, t2);\t\t\t\\\n\t\t\tm1 = MUL32(t1, t2 >> 32);\t\t\t\\\n\t\t\tADD128(rh, rl, MUL32(t1 >> 32, t2 >> 32),\t\\\n\t\t\t\tMUL32(t1, t2));\t\t\t\t\\\n\t\t\trh += (u64)(u32)(m1 >> 32)\t\t\t\\\n\t\t\t\t+ (u32)(m2 >> 32);\t\t\t\\\n\t\t\tt += (u64)(u32)m1 + (u32)m2;\t\t\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t\tADD128(rh, rl, (t >> 32), (t << 32));\t\t\t\\\n\t} while (0)\n#endif\n\nstatic void poly_step_func(u64 *ahi, u64 *alo,\n\t\t\tconst u64 *kh, const u64 *kl,\n\t\t\tconst u64 *mh, const u64 *ml)\n{\n#define a0 (*(((u32 *)alo)+INDEX_LOW))\n#define a1 (*(((u32 *)alo)+INDEX_HIGH))\n#define a2 (*(((u32 *)ahi)+INDEX_LOW))\n#define a3 (*(((u32 *)ahi)+INDEX_HIGH))\n#define k0 (*(((u32 *)kl)+INDEX_LOW))\n#define k1 (*(((u32 *)kl)+INDEX_HIGH))\n#define k2 (*(((u32 *)kh)+INDEX_LOW))\n#define k3 (*(((u32 *)kh)+INDEX_HIGH))\n\n\tu64 p, q, t;\n\tu32 t2;\n\n\tp = MUL32(a3, k3);\n\tp += p;\n\tp += *(u64 *)mh;\n\tp += MUL32(a0, k2);\n\tp += MUL32(a1, k1);\n\tp += MUL32(a2, k0);\n\tt = (u32)(p);\n\tp >>= 32;\n\tp += MUL32(a0, k3);\n\tp += MUL32(a1, k2);\n\tp += MUL32(a2, k1);\n\tp += MUL32(a3, k0);\n\tt |= ((u64)((u32)p & 0x7fffffff)) << 32;\n\tp >>= 31;\n\tp += (u64)(((u32 *)ml)[INDEX_LOW]);\n\tp += MUL32(a0, k0);\n\tq =  MUL32(a1, k3);\n\tq += MUL32(a2, k2);\n\tq += MUL32(a3, k1);\n\tq += q;\n\tp += q;\n\tt2 = (u32)(p);\n\tp >>= 32;\n\tp += (u64)(((u32 *)ml)[INDEX_HIGH]);\n\tp += MUL32(a0, k1);\n\tp += MUL32(a1, k0);\n\tq =  MUL32(a2, k3);\n\tq += MUL32(a3, k2);\n\tq += q;\n\tp += q;\n\t*(u64 *)(alo) = (p << 32) | t2;\n\tp >>= 32;\n\t*(u64 *)(ahi) = p + t;\n\n#undef a0\n#undef a1\n#undef a2\n#undef a3\n#undef k0\n#undef k1\n#undef k2\n#undef k3\n}\n\n#define poly_step(ah, al, kh, kl, mh, ml)\t\t\t\t\\\n\tpoly_step_func(&(ah), &(al), &(kh), &(kl), &(mh), &(ml))\n\n#endif  /* end of specialized NH and poly definitions */\n\n/* At least nh_16 is defined. Defined others as needed here */\n#ifndef nh_16_2\n#define nh_16_2(mp, kp, nw, rh, rl, rh2, rl2)\t\t\t\t\\\n\tdo { \t\t\t\t\t\t\t\t\\\n\t\tnh_16(mp, kp, nw, rh, rl);\t\t\t\t\\\n\t\tnh_16(mp, ((kp)+2), nw, rh2, rl2);\t\t\t\\\n\t} while (0)\n#endif\n#ifndef nh_vmac_nhbytes\n#define nh_vmac_nhbytes(mp, kp, nw, rh, rl)\t\t\t\t\\\n\tnh_16(mp, kp, nw, rh, rl)\n#endif\n#ifndef nh_vmac_nhbytes_2\n#define nh_vmac_nhbytes_2(mp, kp, nw, rh, rl, rh2, rl2)\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tnh_vmac_nhbytes(mp, kp, nw, rh, rl);\t\t\t\\\n\t\tnh_vmac_nhbytes(mp, ((kp)+2), nw, rh2, rl2);\t\t\\\n\t} while (0)\n#endif\n\nstatic void vhash_abort(struct vmac_ctx *ctx)\n{\n\tctx->polytmp[0] = ctx->polykey[0] ;\n\tctx->polytmp[1] = ctx->polykey[1] ;\n\tctx->first_block_processed = 0;\n}\n\nstatic u64 l3hash(u64 p1, u64 p2, u64 k1, u64 k2, u64 len)\n{\n\tu64 rh, rl, t, z = 0;\n\n\t/* fully reduce (p1,p2)+(len,0) mod p127 */\n\tt = p1 >> 63;\n\tp1 &= m63;\n\tADD128(p1, p2, len, t);\n\t/* At this point, (p1,p2) is at most 2^127+(len<<64) */\n\tt = (p1 > m63) + ((p1 == m63) && (p2 == m64));\n\tADD128(p1, p2, z, t);\n\tp1 &= m63;\n\n\t/* compute (p1,p2)/(2^64-2^32) and (p1,p2)%(2^64-2^32) */\n\tt = p1 + (p2 >> 32);\n\tt += (t >> 32);\n\tt += (u32)t > 0xfffffffeu;\n\tp1 += (t >> 32);\n\tp2 += (p1 << 32);\n\n\t/* compute (p1+k1)%p64 and (p2+k2)%p64 */\n\tp1 += k1;\n\tp1 += (0 - (p1 < k1)) & 257;\n\tp2 += k2;\n\tp2 += (0 - (p2 < k2)) & 257;\n\n\t/* compute (p1+k1)*(p2+k2)%p64 */\n\tMUL64(rh, rl, p1, p2);\n\tt = rh >> 56;\n\tADD128(t, rl, z, rh);\n\trh <<= 8;\n\tADD128(t, rl, z, rh);\n\tt += t << 8;\n\trl += t;\n\trl += (0 - (rl < t)) & 257;\n\trl += (0 - (rl > p64-1)) & 257;\n\treturn rl;\n}\n\nstatic void vhash_update(const unsigned char *m,\n\t\t\tunsigned int mbytes, /* Pos multiple of VMAC_NHBYTES */\n\t\t\tstruct vmac_ctx *ctx)\n{\n\tu64 rh, rl, *mptr;\n\tconst u64 *kptr = (u64 *)ctx->nhkey;\n\tint i;\n\tu64 ch, cl;\n\tu64 pkh = ctx->polykey[0];\n\tu64 pkl = ctx->polykey[1];\n\n\tif (!mbytes)\n\t\treturn;\n\n\tBUG_ON(mbytes % VMAC_NHBYTES);\n\n\tmptr = (u64 *)m;\n\ti = mbytes / VMAC_NHBYTES;  /* Must be non-zero */\n\n\tch = ctx->polytmp[0];\n\tcl = ctx->polytmp[1];\n\n\tif (!ctx->first_block_processed) {\n\t\tctx->first_block_processed = 1;\n\t\tnh_vmac_nhbytes(mptr, kptr, VMAC_NHBYTES/8, rh, rl);\n\t\trh &= m62;\n\t\tADD128(ch, cl, rh, rl);\n\t\tmptr += (VMAC_NHBYTES/sizeof(u64));\n\t\ti--;\n\t}\n\n\twhile (i--) {\n\t\tnh_vmac_nhbytes(mptr, kptr, VMAC_NHBYTES/8, rh, rl);\n\t\trh &= m62;\n\t\tpoly_step(ch, cl, pkh, pkl, rh, rl);\n\t\tmptr += (VMAC_NHBYTES/sizeof(u64));\n\t}\n\n\tctx->polytmp[0] = ch;\n\tctx->polytmp[1] = cl;\n}\n\nstatic u64 vhash(unsigned char m[], unsigned int mbytes,\n\t\t\tu64 *tagl, struct vmac_ctx *ctx)\n{\n\tu64 rh, rl, *mptr;\n\tconst u64 *kptr = (u64 *)ctx->nhkey;\n\tint i, remaining;\n\tu64 ch, cl;\n\tu64 pkh = ctx->polykey[0];\n\tu64 pkl = ctx->polykey[1];\n\n\tmptr = (u64 *)m;\n\ti = mbytes / VMAC_NHBYTES;\n\tremaining = mbytes % VMAC_NHBYTES;\n\n\tif (ctx->first_block_processed) {\n\t\tch = ctx->polytmp[0];\n\t\tcl = ctx->polytmp[1];\n\t} else if (i) {\n\t\tnh_vmac_nhbytes(mptr, kptr, VMAC_NHBYTES/8, ch, cl);\n\t\tch &= m62;\n\t\tADD128(ch, cl, pkh, pkl);\n\t\tmptr += (VMAC_NHBYTES/sizeof(u64));\n\t\ti--;\n\t} else if (remaining) {\n\t\tnh_16(mptr, kptr, 2*((remaining+15)/16), ch, cl);\n\t\tch &= m62;\n\t\tADD128(ch, cl, pkh, pkl);\n\t\tmptr += (VMAC_NHBYTES/sizeof(u64));\n\t\tgoto do_l3;\n\t} else {/* Empty String */\n\t\tch = pkh; cl = pkl;\n\t\tgoto do_l3;\n\t}\n\n\twhile (i--) {\n\t\tnh_vmac_nhbytes(mptr, kptr, VMAC_NHBYTES/8, rh, rl);\n\t\trh &= m62;\n\t\tpoly_step(ch, cl, pkh, pkl, rh, rl);\n\t\tmptr += (VMAC_NHBYTES/sizeof(u64));\n\t}\n\tif (remaining) {\n\t\tnh_16(mptr, kptr, 2*((remaining+15)/16), rh, rl);\n\t\trh &= m62;\n\t\tpoly_step(ch, cl, pkh, pkl, rh, rl);\n\t}\n\ndo_l3:\n\tvhash_abort(ctx);\n\tremaining *= 8;\n\treturn l3hash(ch, cl, ctx->l3key[0], ctx->l3key[1], remaining);\n}\n\nstatic u64 vmac(unsigned char m[], unsigned int mbytes,\n\t\t\tconst unsigned char n[16], u64 *tagl,\n\t\t\tstruct vmac_ctx_t *ctx)\n{\n\tu64 *in_n, *out_p;\n\tu64 p, h;\n\tint i;\n\n\tin_n = ctx->__vmac_ctx.cached_nonce;\n\tout_p = ctx->__vmac_ctx.cached_aes;\n\n\ti = n[15] & 1;\n\tif ((*(u64 *)(n+8) != in_n[1]) || (*(u64 *)(n) != in_n[0])) {\n\t\tin_n[0] = *(u64 *)(n);\n\t\tin_n[1] = *(u64 *)(n+8);\n\t\t((unsigned char *)in_n)[15] &= 0xFE;\n\t\tcrypto_cipher_encrypt_one(ctx->child,\n\t\t\t(unsigned char *)out_p, (unsigned char *)in_n);\n\n\t\t((unsigned char *)in_n)[15] |= (unsigned char)(1-i);\n\t}\n\tp = be64_to_cpup(out_p + i);\n\th = vhash(m, mbytes, (u64 *)0, &ctx->__vmac_ctx);\n\treturn le64_to_cpu(p + h);\n}\n\nstatic int vmac_set_key(unsigned char user_key[], struct vmac_ctx_t *ctx)\n{\n\tu64 in[2] = {0}, out[2];\n\tunsigned i;\n\tint err = 0;\n\n\terr = crypto_cipher_setkey(ctx->child, user_key, VMAC_KEY_LEN);\n\tif (err)\n\t\treturn err;\n\n\t/* Fill nh key */\n\t((unsigned char *)in)[0] = 0x80;\n\tfor (i = 0; i < sizeof(ctx->__vmac_ctx.nhkey)/8; i += 2) {\n\t\tcrypto_cipher_encrypt_one(ctx->child,\n\t\t\t(unsigned char *)out, (unsigned char *)in);\n\t\tctx->__vmac_ctx.nhkey[i] = be64_to_cpup(out);\n\t\tctx->__vmac_ctx.nhkey[i+1] = be64_to_cpup(out+1);\n\t\t((unsigned char *)in)[15] += 1;\n\t}\n\n\t/* Fill poly key */\n\t((unsigned char *)in)[0] = 0xC0;\n\tin[1] = 0;\n\tfor (i = 0; i < sizeof(ctx->__vmac_ctx.polykey)/8; i += 2) {\n\t\tcrypto_cipher_encrypt_one(ctx->child,\n\t\t\t(unsigned char *)out, (unsigned char *)in);\n\t\tctx->__vmac_ctx.polytmp[i] =\n\t\t\tctx->__vmac_ctx.polykey[i] =\n\t\t\t\tbe64_to_cpup(out) & mpoly;\n\t\tctx->__vmac_ctx.polytmp[i+1] =\n\t\t\tctx->__vmac_ctx.polykey[i+1] =\n\t\t\t\tbe64_to_cpup(out+1) & mpoly;\n\t\t((unsigned char *)in)[15] += 1;\n\t}\n\n\t/* Fill ip key */\n\t((unsigned char *)in)[0] = 0xE0;\n\tin[1] = 0;\n\tfor (i = 0; i < sizeof(ctx->__vmac_ctx.l3key)/8; i += 2) {\n\t\tdo {\n\t\t\tcrypto_cipher_encrypt_one(ctx->child,\n\t\t\t\t(unsigned char *)out, (unsigned char *)in);\n\t\t\tctx->__vmac_ctx.l3key[i] = be64_to_cpup(out);\n\t\t\tctx->__vmac_ctx.l3key[i+1] = be64_to_cpup(out+1);\n\t\t\t((unsigned char *)in)[15] += 1;\n\t\t} while (ctx->__vmac_ctx.l3key[i] >= p64\n\t\t\t|| ctx->__vmac_ctx.l3key[i+1] >= p64);\n\t}\n\n\t/* Invalidate nonce/aes cache and reset other elements */\n\tctx->__vmac_ctx.cached_nonce[0] = (u64)-1; /* Ensure illegal nonce */\n\tctx->__vmac_ctx.cached_nonce[1] = (u64)0;  /* Ensure illegal nonce */\n\tctx->__vmac_ctx.first_block_processed = 0;\n\n\treturn err;\n}\n\nstatic int vmac_setkey(struct crypto_shash *parent,\n\t\tconst u8 *key, unsigned int keylen)\n{\n\tstruct vmac_ctx_t *ctx = crypto_shash_ctx(parent);\n\n\tif (keylen != VMAC_KEY_LEN) {\n\t\tcrypto_shash_set_flags(parent, CRYPTO_TFM_RES_BAD_KEY_LEN);\n\t\treturn -EINVAL;\n\t}\n\n\treturn vmac_set_key((u8 *)key, ctx);\n}\n\nstatic int vmac_init(struct shash_desc *pdesc)\n{\n\treturn 0;\n}\n\nstatic int vmac_update(struct shash_desc *pdesc, const u8 *p,\n\t\tunsigned int len)\n{\n\tstruct crypto_shash *parent = pdesc->tfm;\n\tstruct vmac_ctx_t *ctx = crypto_shash_ctx(parent);\n\tint expand;\n\tint min;\n\n\texpand = VMAC_NHBYTES - ctx->partial_size > 0 ?\n\t\t\tVMAC_NHBYTES - ctx->partial_size : 0;\n\n\tmin = len < expand ? len : expand;\n\n\tmemcpy(ctx->partial + ctx->partial_size, p, min);\n\tctx->partial_size += min;\n\n\tif (len < expand)\n\t\treturn 0;\n\n\tvhash_update(ctx->partial, VMAC_NHBYTES, &ctx->__vmac_ctx);\n\tctx->partial_size = 0;\n\n\tlen -= expand;\n\tp += expand;\n\n\tif (len % VMAC_NHBYTES) {\n\t\tmemcpy(ctx->partial, p + len - (len % VMAC_NHBYTES),\n\t\t\tlen % VMAC_NHBYTES);\n\t\tctx->partial_size = len % VMAC_NHBYTES;\n\t}\n\n\tvhash_update(p, len - len % VMAC_NHBYTES, &ctx->__vmac_ctx);\n\n\treturn 0;\n}\n\nstatic int vmac_final(struct shash_desc *pdesc, u8 *out)\n{\n\tstruct crypto_shash *parent = pdesc->tfm;\n\tstruct vmac_ctx_t *ctx = crypto_shash_ctx(parent);\n\tvmac_t mac;\n\tu8 nonce[16] = {};\n\n\t/* vmac() ends up accessing outside the array bounds that\n\t * we specify.  In appears to access up to the next 2-word\n\t * boundary.  We'll just be uber cautious and zero the\n\t * unwritten bytes in the buffer.\n\t */\n\tif (ctx->partial_size) {\n\t\tmemset(ctx->partial + ctx->partial_size, 0,\n\t\t\tVMAC_NHBYTES - ctx->partial_size);\n\t}\n\tmac = vmac(ctx->partial, ctx->partial_size, nonce, NULL, ctx);\n\tmemcpy(out, &mac, sizeof(vmac_t));\n\tmemzero_explicit(&mac, sizeof(vmac_t));\n\tmemset(&ctx->__vmac_ctx, 0, sizeof(struct vmac_ctx));\n\tctx->partial_size = 0;\n\treturn 0;\n}\n\nstatic int vmac_init_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_cipher *cipher;\n\tstruct crypto_instance *inst = (void *)tfm->__crt_alg;\n\tstruct crypto_spawn *spawn = crypto_instance_ctx(inst);\n\tstruct vmac_ctx_t *ctx = crypto_tfm_ctx(tfm);\n\n\tcipher = crypto_spawn_cipher(spawn);\n\tif (IS_ERR(cipher))\n\t\treturn PTR_ERR(cipher);\n\n\tctx->child = cipher;\n\treturn 0;\n}\n\nstatic void vmac_exit_tfm(struct crypto_tfm *tfm)\n{\n\tstruct vmac_ctx_t *ctx = crypto_tfm_ctx(tfm);\n\tcrypto_free_cipher(ctx->child);\n}\n\nstatic int vmac_create(struct crypto_template *tmpl, struct rtattr **tb)\n{\n\tstruct shash_instance *inst;\n\tstruct crypto_alg *alg;\n\tint err;\n\n\terr = crypto_check_attr_type(tb, CRYPTO_ALG_TYPE_SHASH);\n\tif (err)\n\t\treturn err;\n\n\talg = crypto_get_attr_alg(tb, CRYPTO_ALG_TYPE_CIPHER,\n\t\t\tCRYPTO_ALG_TYPE_MASK);\n\tif (IS_ERR(alg))\n\t\treturn PTR_ERR(alg);\n\n\tinst = shash_alloc_instance(\"vmac\", alg);\n\terr = PTR_ERR(inst);\n\tif (IS_ERR(inst))\n\t\tgoto out_put_alg;\n\n\terr = crypto_init_spawn(shash_instance_ctx(inst), alg,\n\t\t\tshash_crypto_instance(inst),\n\t\t\tCRYPTO_ALG_TYPE_MASK);\n\tif (err)\n\t\tgoto out_free_inst;\n\n\tinst->alg.base.cra_priority = alg->cra_priority;\n\tinst->alg.base.cra_blocksize = alg->cra_blocksize;\n\tinst->alg.base.cra_alignmask = alg->cra_alignmask;\n\n\tinst->alg.digestsize = sizeof(vmac_t);\n\tinst->alg.base.cra_ctxsize = sizeof(struct vmac_ctx_t);\n\tinst->alg.base.cra_init = vmac_init_tfm;\n\tinst->alg.base.cra_exit = vmac_exit_tfm;\n\n\tinst->alg.init = vmac_init;\n\tinst->alg.update = vmac_update;\n\tinst->alg.final = vmac_final;\n\tinst->alg.setkey = vmac_setkey;\n\n\terr = shash_register_instance(tmpl, inst);\n\tif (err) {\nout_free_inst:\n\t\tshash_free_instance(shash_crypto_instance(inst));\n\t}\n\nout_put_alg:\n\tcrypto_mod_put(alg);\n\treturn err;\n}\n\nstatic struct crypto_template vmac_tmpl = {\n\t.name = \"vmac\",\n\t.create = vmac_create,\n\t.free = shash_free_instance,\n\t.module = THIS_MODULE,\n};\n\nstatic int __init vmac_module_init(void)\n{\n\treturn crypto_register_template(&vmac_tmpl);\n}\n\nstatic void __exit vmac_module_exit(void)\n{\n\tcrypto_unregister_template(&vmac_tmpl);\n}\n\nmodule_init(vmac_module_init);\nmodule_exit(vmac_module_exit);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"VMAC hash algorithm\");\n", "/*\n * Copyright (C)2006 USAGI/WIDE Project\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License as published by\n * the Free Software Foundation; either version 2 of the License, or\n * (at your option) any later version.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n * GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public License\n * along with this program; if not, write to the Free Software\n * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA\n *\n * Author:\n * \tKazunori Miyazawa <miyazawa@linux-ipv6.org>\n */\n\n#include <crypto/internal/hash.h>\n#include <linux/err.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n\nstatic u_int32_t ks[12] = {0x01010101, 0x01010101, 0x01010101, 0x01010101,\n\t\t\t   0x02020202, 0x02020202, 0x02020202, 0x02020202,\n\t\t\t   0x03030303, 0x03030303, 0x03030303, 0x03030303};\n\n/*\n * +------------------------\n * | <parent tfm>\n * +------------------------\n * | xcbc_tfm_ctx\n * +------------------------\n * | consts (block size * 2)\n * +------------------------\n */\nstruct xcbc_tfm_ctx {\n\tstruct crypto_cipher *child;\n\tu8 ctx[];\n};\n\n/*\n * +------------------------\n * | <shash desc>\n * +------------------------\n * | xcbc_desc_ctx\n * +------------------------\n * | odds (block size)\n * +------------------------\n * | prev (block size)\n * +------------------------\n */\nstruct xcbc_desc_ctx {\n\tunsigned int len;\n\tu8 ctx[];\n};\n\nstatic int crypto_xcbc_digest_setkey(struct crypto_shash *parent,\n\t\t\t\t     const u8 *inkey, unsigned int keylen)\n{\n\tunsigned long alignmask = crypto_shash_alignmask(parent);\n\tstruct xcbc_tfm_ctx *ctx = crypto_shash_ctx(parent);\n\tint bs = crypto_shash_blocksize(parent);\n\tu8 *consts = PTR_ALIGN(&ctx->ctx[0], alignmask + 1);\n\tint err = 0;\n\tu8 key1[bs];\n\n\tif ((err = crypto_cipher_setkey(ctx->child, inkey, keylen)))\n\t\treturn err;\n\n\tcrypto_cipher_encrypt_one(ctx->child, consts, (u8 *)ks + bs);\n\tcrypto_cipher_encrypt_one(ctx->child, consts + bs, (u8 *)ks + bs * 2);\n\tcrypto_cipher_encrypt_one(ctx->child, key1, (u8 *)ks);\n\n\treturn crypto_cipher_setkey(ctx->child, key1, bs);\n\n}\n\nstatic int crypto_xcbc_digest_init(struct shash_desc *pdesc)\n{\n\tunsigned long alignmask = crypto_shash_alignmask(pdesc->tfm);\n\tstruct xcbc_desc_ctx *ctx = shash_desc_ctx(pdesc);\n\tint bs = crypto_shash_blocksize(pdesc->tfm);\n\tu8 *prev = PTR_ALIGN(&ctx->ctx[0], alignmask + 1) + bs;\n\n\tctx->len = 0;\n\tmemset(prev, 0, bs);\n\n\treturn 0;\n}\n\nstatic int crypto_xcbc_digest_update(struct shash_desc *pdesc, const u8 *p,\n\t\t\t\t     unsigned int len)\n{\n\tstruct crypto_shash *parent = pdesc->tfm;\n\tunsigned long alignmask = crypto_shash_alignmask(parent);\n\tstruct xcbc_tfm_ctx *tctx = crypto_shash_ctx(parent);\n\tstruct xcbc_desc_ctx *ctx = shash_desc_ctx(pdesc);\n\tstruct crypto_cipher *tfm = tctx->child;\n\tint bs = crypto_shash_blocksize(parent);\n\tu8 *odds = PTR_ALIGN(&ctx->ctx[0], alignmask + 1);\n\tu8 *prev = odds + bs;\n\n\t/* checking the data can fill the block */\n\tif ((ctx->len + len) <= bs) {\n\t\tmemcpy(odds + ctx->len, p, len);\n\t\tctx->len += len;\n\t\treturn 0;\n\t}\n\n\t/* filling odds with new data and encrypting it */\n\tmemcpy(odds + ctx->len, p, bs - ctx->len);\n\tlen -= bs - ctx->len;\n\tp += bs - ctx->len;\n\n\tcrypto_xor(prev, odds, bs);\n\tcrypto_cipher_encrypt_one(tfm, prev, prev);\n\n\t/* clearing the length */\n\tctx->len = 0;\n\n\t/* encrypting the rest of data */\n\twhile (len > bs) {\n\t\tcrypto_xor(prev, p, bs);\n\t\tcrypto_cipher_encrypt_one(tfm, prev, prev);\n\t\tp += bs;\n\t\tlen -= bs;\n\t}\n\n\t/* keeping the surplus of blocksize */\n\tif (len) {\n\t\tmemcpy(odds, p, len);\n\t\tctx->len = len;\n\t}\n\n\treturn 0;\n}\n\nstatic int crypto_xcbc_digest_final(struct shash_desc *pdesc, u8 *out)\n{\n\tstruct crypto_shash *parent = pdesc->tfm;\n\tunsigned long alignmask = crypto_shash_alignmask(parent);\n\tstruct xcbc_tfm_ctx *tctx = crypto_shash_ctx(parent);\n\tstruct xcbc_desc_ctx *ctx = shash_desc_ctx(pdesc);\n\tstruct crypto_cipher *tfm = tctx->child;\n\tint bs = crypto_shash_blocksize(parent);\n\tu8 *consts = PTR_ALIGN(&tctx->ctx[0], alignmask + 1);\n\tu8 *odds = PTR_ALIGN(&ctx->ctx[0], alignmask + 1);\n\tu8 *prev = odds + bs;\n\tunsigned int offset = 0;\n\n\tif (ctx->len != bs) {\n\t\tunsigned int rlen;\n\t\tu8 *p = odds + ctx->len;\n\n\t\t*p = 0x80;\n\t\tp++;\n\n\t\trlen = bs - ctx->len -1;\n\t\tif (rlen)\n\t\t\tmemset(p, 0, rlen);\n\n\t\toffset += bs;\n\t}\n\n\tcrypto_xor(prev, odds, bs);\n\tcrypto_xor(prev, consts + offset, bs);\n\n\tcrypto_cipher_encrypt_one(tfm, out, prev);\n\n\treturn 0;\n}\n\nstatic int xcbc_init_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_cipher *cipher;\n\tstruct crypto_instance *inst = (void *)tfm->__crt_alg;\n\tstruct crypto_spawn *spawn = crypto_instance_ctx(inst);\n\tstruct xcbc_tfm_ctx *ctx = crypto_tfm_ctx(tfm);\n\n\tcipher = crypto_spawn_cipher(spawn);\n\tif (IS_ERR(cipher))\n\t\treturn PTR_ERR(cipher);\n\n\tctx->child = cipher;\n\n\treturn 0;\n};\n\nstatic void xcbc_exit_tfm(struct crypto_tfm *tfm)\n{\n\tstruct xcbc_tfm_ctx *ctx = crypto_tfm_ctx(tfm);\n\tcrypto_free_cipher(ctx->child);\n}\n\nstatic int xcbc_create(struct crypto_template *tmpl, struct rtattr **tb)\n{\n\tstruct shash_instance *inst;\n\tstruct crypto_alg *alg;\n\tunsigned long alignmask;\n\tint err;\n\n\terr = crypto_check_attr_type(tb, CRYPTO_ALG_TYPE_SHASH);\n\tif (err)\n\t\treturn err;\n\n\talg = crypto_get_attr_alg(tb, CRYPTO_ALG_TYPE_CIPHER,\n\t\t\t\t  CRYPTO_ALG_TYPE_MASK);\n\tif (IS_ERR(alg))\n\t\treturn PTR_ERR(alg);\n\n\tswitch(alg->cra_blocksize) {\n\tcase 16:\n\t\tbreak;\n\tdefault:\n\t\tgoto out_put_alg;\n\t}\n\n\tinst = shash_alloc_instance(\"xcbc\", alg);\n\terr = PTR_ERR(inst);\n\tif (IS_ERR(inst))\n\t\tgoto out_put_alg;\n\n\terr = crypto_init_spawn(shash_instance_ctx(inst), alg,\n\t\t\t\tshash_crypto_instance(inst),\n\t\t\t\tCRYPTO_ALG_TYPE_MASK);\n\tif (err)\n\t\tgoto out_free_inst;\n\n\talignmask = alg->cra_alignmask | 3;\n\tinst->alg.base.cra_alignmask = alignmask;\n\tinst->alg.base.cra_priority = alg->cra_priority;\n\tinst->alg.base.cra_blocksize = alg->cra_blocksize;\n\n\tinst->alg.digestsize = alg->cra_blocksize;\n\tinst->alg.descsize = ALIGN(sizeof(struct xcbc_desc_ctx),\n\t\t\t\t   crypto_tfm_ctx_alignment()) +\n\t\t\t     (alignmask &\n\t\t\t      ~(crypto_tfm_ctx_alignment() - 1)) +\n\t\t\t     alg->cra_blocksize * 2;\n\n\tinst->alg.base.cra_ctxsize = ALIGN(sizeof(struct xcbc_tfm_ctx),\n\t\t\t\t\t   alignmask + 1) +\n\t\t\t\t     alg->cra_blocksize * 2;\n\tinst->alg.base.cra_init = xcbc_init_tfm;\n\tinst->alg.base.cra_exit = xcbc_exit_tfm;\n\n\tinst->alg.init = crypto_xcbc_digest_init;\n\tinst->alg.update = crypto_xcbc_digest_update;\n\tinst->alg.final = crypto_xcbc_digest_final;\n\tinst->alg.setkey = crypto_xcbc_digest_setkey;\n\n\terr = shash_register_instance(tmpl, inst);\n\tif (err) {\nout_free_inst:\n\t\tshash_free_instance(shash_crypto_instance(inst));\n\t}\n\nout_put_alg:\n\tcrypto_mod_put(alg);\n\treturn err;\n}\n\nstatic struct crypto_template crypto_xcbc_tmpl = {\n\t.name = \"xcbc\",\n\t.create = xcbc_create,\n\t.free = shash_free_instance,\n\t.module = THIS_MODULE,\n};\n\nstatic int __init crypto_xcbc_module_init(void)\n{\n\treturn crypto_register_template(&crypto_xcbc_tmpl);\n}\n\nstatic void __exit crypto_xcbc_module_exit(void)\n{\n\tcrypto_unregister_template(&crypto_xcbc_tmpl);\n}\n\nmodule_init(crypto_xcbc_module_init);\nmodule_exit(crypto_xcbc_module_exit);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"XCBC keyed hash algorithm\");\n", "/* XTS: as defined in IEEE1619/D16\n *\thttp://grouper.ieee.org/groups/1619/email/pdf00086.pdf\n *\t(sector sizes which are not a multiple of 16 bytes are,\n *\thowever currently unsupported)\n *\n * Copyright (c) 2007 Rik Snel <rsnel@cube.dyndns.org>\n *\n * Based om ecb.c\n * Copyright (c) 2006 Herbert Xu <herbert@gondor.apana.org.au>\n *\n * This program is free software; you can redistribute it and/or modify it\n * under the terms of the GNU General Public License as published by the Free\n * Software Foundation; either version 2 of the License, or (at your option)\n * any later version.\n */\n#include <crypto/algapi.h>\n#include <linux/err.h>\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/scatterlist.h>\n#include <linux/slab.h>\n\n#include <crypto/xts.h>\n#include <crypto/b128ops.h>\n#include <crypto/gf128mul.h>\n\nstruct priv {\n\tstruct crypto_cipher *child;\n\tstruct crypto_cipher *tweak;\n};\n\nstatic int setkey(struct crypto_tfm *parent, const u8 *key,\n\t\t  unsigned int keylen)\n{\n\tstruct priv *ctx = crypto_tfm_ctx(parent);\n\tstruct crypto_cipher *child = ctx->tweak;\n\tu32 *flags = &parent->crt_flags;\n\tint err;\n\n\t/* key consists of keys of equal size concatenated, therefore\n\t * the length must be even */\n\tif (keylen % 2) {\n\t\t/* tell the user why there was an error */\n\t\t*flags |= CRYPTO_TFM_RES_BAD_KEY_LEN;\n\t\treturn -EINVAL;\n\t}\n\n\t/* we need two cipher instances: one to compute the initial 'tweak'\n\t * by encrypting the IV (usually the 'plain' iv) and the other\n\t * one to encrypt and decrypt the data */\n\n\t/* tweak cipher, uses Key2 i.e. the second half of *key */\n\tcrypto_cipher_clear_flags(child, CRYPTO_TFM_REQ_MASK);\n\tcrypto_cipher_set_flags(child, crypto_tfm_get_flags(parent) &\n\t\t\t\t       CRYPTO_TFM_REQ_MASK);\n\terr = crypto_cipher_setkey(child, key + keylen/2, keylen/2);\n\tif (err)\n\t\treturn err;\n\n\tcrypto_tfm_set_flags(parent, crypto_cipher_get_flags(child) &\n\t\t\t\t     CRYPTO_TFM_RES_MASK);\n\n\tchild = ctx->child;\n\n\t/* data cipher, uses Key1 i.e. the first half of *key */\n\tcrypto_cipher_clear_flags(child, CRYPTO_TFM_REQ_MASK);\n\tcrypto_cipher_set_flags(child, crypto_tfm_get_flags(parent) &\n\t\t\t\t       CRYPTO_TFM_REQ_MASK);\n\terr = crypto_cipher_setkey(child, key, keylen/2);\n\tif (err)\n\t\treturn err;\n\n\tcrypto_tfm_set_flags(parent, crypto_cipher_get_flags(child) &\n\t\t\t\t     CRYPTO_TFM_RES_MASK);\n\n\treturn 0;\n}\n\nstruct sinfo {\n\tbe128 *t;\n\tstruct crypto_tfm *tfm;\n\tvoid (*fn)(struct crypto_tfm *, u8 *, const u8 *);\n};\n\nstatic inline void xts_round(struct sinfo *s, void *dst, const void *src)\n{\n\tbe128_xor(dst, s->t, src);\t\t/* PP <- T xor P */\n\ts->fn(s->tfm, dst, dst);\t\t/* CC <- E(Key1,PP) */\n\tbe128_xor(dst, dst, s->t);\t\t/* C <- T xor CC */\n}\n\nstatic int crypt(struct blkcipher_desc *d,\n\t\t struct blkcipher_walk *w, struct priv *ctx,\n\t\t void (*tw)(struct crypto_tfm *, u8 *, const u8 *),\n\t\t void (*fn)(struct crypto_tfm *, u8 *, const u8 *))\n{\n\tint err;\n\tunsigned int avail;\n\tconst int bs = XTS_BLOCK_SIZE;\n\tstruct sinfo s = {\n\t\t.tfm = crypto_cipher_tfm(ctx->child),\n\t\t.fn = fn\n\t};\n\tu8 *wsrc;\n\tu8 *wdst;\n\n\terr = blkcipher_walk_virt(d, w);\n\tif (!w->nbytes)\n\t\treturn err;\n\n\ts.t = (be128 *)w->iv;\n\tavail = w->nbytes;\n\n\twsrc = w->src.virt.addr;\n\twdst = w->dst.virt.addr;\n\n\t/* calculate first value of T */\n\ttw(crypto_cipher_tfm(ctx->tweak), w->iv, w->iv);\n\n\tgoto first;\n\n\tfor (;;) {\n\t\tdo {\n\t\t\tgf128mul_x_ble(s.t, s.t);\n\nfirst:\n\t\t\txts_round(&s, wdst, wsrc);\n\n\t\t\twsrc += bs;\n\t\t\twdst += bs;\n\t\t} while ((avail -= bs) >= bs);\n\n\t\terr = blkcipher_walk_done(d, w, avail);\n\t\tif (!w->nbytes)\n\t\t\tbreak;\n\n\t\tavail = w->nbytes;\n\n\t\twsrc = w->src.virt.addr;\n\t\twdst = w->dst.virt.addr;\n\t}\n\n\treturn err;\n}\n\nstatic int encrypt(struct blkcipher_desc *desc, struct scatterlist *dst,\n\t\t   struct scatterlist *src, unsigned int nbytes)\n{\n\tstruct priv *ctx = crypto_blkcipher_ctx(desc->tfm);\n\tstruct blkcipher_walk w;\n\n\tblkcipher_walk_init(&w, dst, src, nbytes);\n\treturn crypt(desc, &w, ctx, crypto_cipher_alg(ctx->tweak)->cia_encrypt,\n\t\t     crypto_cipher_alg(ctx->child)->cia_encrypt);\n}\n\nstatic int decrypt(struct blkcipher_desc *desc, struct scatterlist *dst,\n\t\t   struct scatterlist *src, unsigned int nbytes)\n{\n\tstruct priv *ctx = crypto_blkcipher_ctx(desc->tfm);\n\tstruct blkcipher_walk w;\n\n\tblkcipher_walk_init(&w, dst, src, nbytes);\n\treturn crypt(desc, &w, ctx, crypto_cipher_alg(ctx->tweak)->cia_encrypt,\n\t\t     crypto_cipher_alg(ctx->child)->cia_decrypt);\n}\n\nint xts_crypt(struct blkcipher_desc *desc, struct scatterlist *sdst,\n\t      struct scatterlist *ssrc, unsigned int nbytes,\n\t      struct xts_crypt_req *req)\n{\n\tconst unsigned int bsize = XTS_BLOCK_SIZE;\n\tconst unsigned int max_blks = req->tbuflen / bsize;\n\tstruct blkcipher_walk walk;\n\tunsigned int nblocks;\n\tbe128 *src, *dst, *t;\n\tbe128 *t_buf = req->tbuf;\n\tint err, i;\n\n\tBUG_ON(max_blks < 1);\n\n\tblkcipher_walk_init(&walk, sdst, ssrc, nbytes);\n\n\terr = blkcipher_walk_virt(desc, &walk);\n\tnbytes = walk.nbytes;\n\tif (!nbytes)\n\t\treturn err;\n\n\tnblocks = min(nbytes / bsize, max_blks);\n\tsrc = (be128 *)walk.src.virt.addr;\n\tdst = (be128 *)walk.dst.virt.addr;\n\n\t/* calculate first value of T */\n\treq->tweak_fn(req->tweak_ctx, (u8 *)&t_buf[0], walk.iv);\n\n\ti = 0;\n\tgoto first;\n\n\tfor (;;) {\n\t\tdo {\n\t\t\tfor (i = 0; i < nblocks; i++) {\n\t\t\t\tgf128mul_x_ble(&t_buf[i], t);\nfirst:\n\t\t\t\tt = &t_buf[i];\n\n\t\t\t\t/* PP <- T xor P */\n\t\t\t\tbe128_xor(dst + i, t, src + i);\n\t\t\t}\n\n\t\t\t/* CC <- E(Key2,PP) */\n\t\t\treq->crypt_fn(req->crypt_ctx, (u8 *)dst,\n\t\t\t\t      nblocks * bsize);\n\n\t\t\t/* C <- T xor CC */\n\t\t\tfor (i = 0; i < nblocks; i++)\n\t\t\t\tbe128_xor(dst + i, dst + i, &t_buf[i]);\n\n\t\t\tsrc += nblocks;\n\t\t\tdst += nblocks;\n\t\t\tnbytes -= nblocks * bsize;\n\t\t\tnblocks = min(nbytes / bsize, max_blks);\n\t\t} while (nblocks > 0);\n\n\t\t*(be128 *)walk.iv = *t;\n\n\t\terr = blkcipher_walk_done(desc, &walk, nbytes);\n\t\tnbytes = walk.nbytes;\n\t\tif (!nbytes)\n\t\t\tbreak;\n\n\t\tnblocks = min(nbytes / bsize, max_blks);\n\t\tsrc = (be128 *)walk.src.virt.addr;\n\t\tdst = (be128 *)walk.dst.virt.addr;\n\t}\n\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(xts_crypt);\n\nstatic int init_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_cipher *cipher;\n\tstruct crypto_instance *inst = (void *)tfm->__crt_alg;\n\tstruct crypto_spawn *spawn = crypto_instance_ctx(inst);\n\tstruct priv *ctx = crypto_tfm_ctx(tfm);\n\tu32 *flags = &tfm->crt_flags;\n\n\tcipher = crypto_spawn_cipher(spawn);\n\tif (IS_ERR(cipher))\n\t\treturn PTR_ERR(cipher);\n\n\tif (crypto_cipher_blocksize(cipher) != XTS_BLOCK_SIZE) {\n\t\t*flags |= CRYPTO_TFM_RES_BAD_BLOCK_LEN;\n\t\tcrypto_free_cipher(cipher);\n\t\treturn -EINVAL;\n\t}\n\n\tctx->child = cipher;\n\n\tcipher = crypto_spawn_cipher(spawn);\n\tif (IS_ERR(cipher)) {\n\t\tcrypto_free_cipher(ctx->child);\n\t\treturn PTR_ERR(cipher);\n\t}\n\n\t/* this check isn't really needed, leave it here just in case */\n\tif (crypto_cipher_blocksize(cipher) != XTS_BLOCK_SIZE) {\n\t\tcrypto_free_cipher(cipher);\n\t\tcrypto_free_cipher(ctx->child);\n\t\t*flags |= CRYPTO_TFM_RES_BAD_BLOCK_LEN;\n\t\treturn -EINVAL;\n\t}\n\n\tctx->tweak = cipher;\n\n\treturn 0;\n}\n\nstatic void exit_tfm(struct crypto_tfm *tfm)\n{\n\tstruct priv *ctx = crypto_tfm_ctx(tfm);\n\tcrypto_free_cipher(ctx->child);\n\tcrypto_free_cipher(ctx->tweak);\n}\n\nstatic struct crypto_instance *alloc(struct rtattr **tb)\n{\n\tstruct crypto_instance *inst;\n\tstruct crypto_alg *alg;\n\tint err;\n\n\terr = crypto_check_attr_type(tb, CRYPTO_ALG_TYPE_BLKCIPHER);\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\talg = crypto_get_attr_alg(tb, CRYPTO_ALG_TYPE_CIPHER,\n\t\t\t\t  CRYPTO_ALG_TYPE_MASK);\n\tif (IS_ERR(alg))\n\t\treturn ERR_CAST(alg);\n\n\tinst = crypto_alloc_instance(\"xts\", alg);\n\tif (IS_ERR(inst))\n\t\tgoto out_put_alg;\n\n\tinst->alg.cra_flags = CRYPTO_ALG_TYPE_BLKCIPHER;\n\tinst->alg.cra_priority = alg->cra_priority;\n\tinst->alg.cra_blocksize = alg->cra_blocksize;\n\n\tif (alg->cra_alignmask < 7)\n\t\tinst->alg.cra_alignmask = 7;\n\telse\n\t\tinst->alg.cra_alignmask = alg->cra_alignmask;\n\n\tinst->alg.cra_type = &crypto_blkcipher_type;\n\n\tinst->alg.cra_blkcipher.ivsize = alg->cra_blocksize;\n\tinst->alg.cra_blkcipher.min_keysize =\n\t\t2 * alg->cra_cipher.cia_min_keysize;\n\tinst->alg.cra_blkcipher.max_keysize =\n\t\t2 * alg->cra_cipher.cia_max_keysize;\n\n\tinst->alg.cra_ctxsize = sizeof(struct priv);\n\n\tinst->alg.cra_init = init_tfm;\n\tinst->alg.cra_exit = exit_tfm;\n\n\tinst->alg.cra_blkcipher.setkey = setkey;\n\tinst->alg.cra_blkcipher.encrypt = encrypt;\n\tinst->alg.cra_blkcipher.decrypt = decrypt;\n\nout_put_alg:\n\tcrypto_mod_put(alg);\n\treturn inst;\n}\n\nstatic void free(struct crypto_instance *inst)\n{\n\tcrypto_drop_spawn(crypto_instance_ctx(inst));\n\tkfree(inst);\n}\n\nstatic struct crypto_template crypto_tmpl = {\n\t.name = \"xts\",\n\t.alloc = alloc,\n\t.free = free,\n\t.module = THIS_MODULE,\n};\n\nstatic int __init crypto_module_init(void)\n{\n\treturn crypto_register_template(&crypto_tmpl);\n}\n\nstatic void __exit crypto_module_exit(void)\n{\n\tcrypto_unregister_template(&crypto_tmpl);\n}\n\nmodule_init(crypto_module_init);\nmodule_exit(crypto_module_exit);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"XTS block cipher mode\");\n"], "fixing_code": ["/*\n * FPU: Wrapper for blkcipher touching fpu\n *\n * Copyright (c) Intel Corp.\n *   Author: Huang Ying <ying.huang@intel.com>\n *\n * This program is free software; you can redistribute it and/or modify it\n * under the terms of the GNU General Public License as published by the Free\n * Software Foundation; either version 2 of the License, or (at your option)\n * any later version.\n *\n */\n\n#include <crypto/algapi.h>\n#include <linux/err.h>\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/slab.h>\n#include <linux/crypto.h>\n#include <asm/i387.h>\n\nstruct crypto_fpu_ctx {\n\tstruct crypto_blkcipher *child;\n};\n\nstatic int crypto_fpu_setkey(struct crypto_tfm *parent, const u8 *key,\n\t\t\t     unsigned int keylen)\n{\n\tstruct crypto_fpu_ctx *ctx = crypto_tfm_ctx(parent);\n\tstruct crypto_blkcipher *child = ctx->child;\n\tint err;\n\n\tcrypto_blkcipher_clear_flags(child, CRYPTO_TFM_REQ_MASK);\n\tcrypto_blkcipher_set_flags(child, crypto_tfm_get_flags(parent) &\n\t\t\t\t   CRYPTO_TFM_REQ_MASK);\n\terr = crypto_blkcipher_setkey(child, key, keylen);\n\tcrypto_tfm_set_flags(parent, crypto_blkcipher_get_flags(child) &\n\t\t\t\t     CRYPTO_TFM_RES_MASK);\n\treturn err;\n}\n\nstatic int crypto_fpu_encrypt(struct blkcipher_desc *desc_in,\n\t\t\t      struct scatterlist *dst, struct scatterlist *src,\n\t\t\t      unsigned int nbytes)\n{\n\tint err;\n\tstruct crypto_fpu_ctx *ctx = crypto_blkcipher_ctx(desc_in->tfm);\n\tstruct crypto_blkcipher *child = ctx->child;\n\tstruct blkcipher_desc desc = {\n\t\t.tfm = child,\n\t\t.info = desc_in->info,\n\t\t.flags = desc_in->flags & ~CRYPTO_TFM_REQ_MAY_SLEEP,\n\t};\n\n\tkernel_fpu_begin();\n\terr = crypto_blkcipher_crt(desc.tfm)->encrypt(&desc, dst, src, nbytes);\n\tkernel_fpu_end();\n\treturn err;\n}\n\nstatic int crypto_fpu_decrypt(struct blkcipher_desc *desc_in,\n\t\t\t      struct scatterlist *dst, struct scatterlist *src,\n\t\t\t      unsigned int nbytes)\n{\n\tint err;\n\tstruct crypto_fpu_ctx *ctx = crypto_blkcipher_ctx(desc_in->tfm);\n\tstruct crypto_blkcipher *child = ctx->child;\n\tstruct blkcipher_desc desc = {\n\t\t.tfm = child,\n\t\t.info = desc_in->info,\n\t\t.flags = desc_in->flags & ~CRYPTO_TFM_REQ_MAY_SLEEP,\n\t};\n\n\tkernel_fpu_begin();\n\terr = crypto_blkcipher_crt(desc.tfm)->decrypt(&desc, dst, src, nbytes);\n\tkernel_fpu_end();\n\treturn err;\n}\n\nstatic int crypto_fpu_init_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_instance *inst = crypto_tfm_alg_instance(tfm);\n\tstruct crypto_spawn *spawn = crypto_instance_ctx(inst);\n\tstruct crypto_fpu_ctx *ctx = crypto_tfm_ctx(tfm);\n\tstruct crypto_blkcipher *cipher;\n\n\tcipher = crypto_spawn_blkcipher(spawn);\n\tif (IS_ERR(cipher))\n\t\treturn PTR_ERR(cipher);\n\n\tctx->child = cipher;\n\treturn 0;\n}\n\nstatic void crypto_fpu_exit_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_fpu_ctx *ctx = crypto_tfm_ctx(tfm);\n\tcrypto_free_blkcipher(ctx->child);\n}\n\nstatic struct crypto_instance *crypto_fpu_alloc(struct rtattr **tb)\n{\n\tstruct crypto_instance *inst;\n\tstruct crypto_alg *alg;\n\tint err;\n\n\terr = crypto_check_attr_type(tb, CRYPTO_ALG_TYPE_BLKCIPHER);\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\talg = crypto_get_attr_alg(tb, CRYPTO_ALG_TYPE_BLKCIPHER,\n\t\t\t\t  CRYPTO_ALG_TYPE_MASK);\n\tif (IS_ERR(alg))\n\t\treturn ERR_CAST(alg);\n\n\tinst = crypto_alloc_instance(\"fpu\", alg);\n\tif (IS_ERR(inst))\n\t\tgoto out_put_alg;\n\n\tinst->alg.cra_flags = alg->cra_flags;\n\tinst->alg.cra_priority = alg->cra_priority;\n\tinst->alg.cra_blocksize = alg->cra_blocksize;\n\tinst->alg.cra_alignmask = alg->cra_alignmask;\n\tinst->alg.cra_type = alg->cra_type;\n\tinst->alg.cra_blkcipher.ivsize = alg->cra_blkcipher.ivsize;\n\tinst->alg.cra_blkcipher.min_keysize = alg->cra_blkcipher.min_keysize;\n\tinst->alg.cra_blkcipher.max_keysize = alg->cra_blkcipher.max_keysize;\n\tinst->alg.cra_ctxsize = sizeof(struct crypto_fpu_ctx);\n\tinst->alg.cra_init = crypto_fpu_init_tfm;\n\tinst->alg.cra_exit = crypto_fpu_exit_tfm;\n\tinst->alg.cra_blkcipher.setkey = crypto_fpu_setkey;\n\tinst->alg.cra_blkcipher.encrypt = crypto_fpu_encrypt;\n\tinst->alg.cra_blkcipher.decrypt = crypto_fpu_decrypt;\n\nout_put_alg:\n\tcrypto_mod_put(alg);\n\treturn inst;\n}\n\nstatic void crypto_fpu_free(struct crypto_instance *inst)\n{\n\tcrypto_drop_spawn(crypto_instance_ctx(inst));\n\tkfree(inst);\n}\n\nstatic struct crypto_template crypto_fpu_tmpl = {\n\t.name = \"fpu\",\n\t.alloc = crypto_fpu_alloc,\n\t.free = crypto_fpu_free,\n\t.module = THIS_MODULE,\n};\n\nint __init crypto_fpu_init(void)\n{\n\treturn crypto_register_template(&crypto_fpu_tmpl);\n}\n\nvoid __exit crypto_fpu_exit(void)\n{\n\tcrypto_unregister_template(&crypto_fpu_tmpl);\n}\n\nMODULE_ALIAS_CRYPTO(\"fpu\");\n", "/*\n * Cryptographic API for algorithms (i.e., low-level API).\n *\n * Copyright (c) 2006 Herbert Xu <herbert@gondor.apana.org.au>\n *\n * This program is free software; you can redistribute it and/or modify it\n * under the terms of the GNU General Public License as published by the Free\n * Software Foundation; either version 2 of the License, or (at your option)\n * any later version.\n *\n */\n\n#include <linux/err.h>\n#include <linux/errno.h>\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/list.h>\n#include <linux/module.h>\n#include <linux/rtnetlink.h>\n#include <linux/slab.h>\n#include <linux/string.h>\n\n#include \"internal.h\"\n\nstatic LIST_HEAD(crypto_template_list);\n\nstatic inline int crypto_set_driver_name(struct crypto_alg *alg)\n{\n\tstatic const char suffix[] = \"-generic\";\n\tchar *driver_name = alg->cra_driver_name;\n\tint len;\n\n\tif (*driver_name)\n\t\treturn 0;\n\n\tlen = strlcpy(driver_name, alg->cra_name, CRYPTO_MAX_ALG_NAME);\n\tif (len + sizeof(suffix) > CRYPTO_MAX_ALG_NAME)\n\t\treturn -ENAMETOOLONG;\n\n\tmemcpy(driver_name + len, suffix, sizeof(suffix));\n\treturn 0;\n}\n\nstatic inline void crypto_check_module_sig(struct module *mod)\n{\n#ifdef CONFIG_CRYPTO_FIPS\n\tif (fips_enabled && mod && !mod->sig_ok)\n\t\tpanic(\"Module %s signature verification failed in FIPS mode\\n\",\n\t\t      mod->name);\n#endif\n\treturn;\n}\n\nstatic int crypto_check_alg(struct crypto_alg *alg)\n{\n\tcrypto_check_module_sig(alg->cra_module);\n\n\tif (alg->cra_alignmask & (alg->cra_alignmask + 1))\n\t\treturn -EINVAL;\n\n\tif (alg->cra_blocksize > PAGE_SIZE / 8)\n\t\treturn -EINVAL;\n\n\tif (alg->cra_priority < 0)\n\t\treturn -EINVAL;\n\n\treturn crypto_set_driver_name(alg);\n}\n\nstatic void crypto_destroy_instance(struct crypto_alg *alg)\n{\n\tstruct crypto_instance *inst = (void *)alg;\n\tstruct crypto_template *tmpl = inst->tmpl;\n\n\ttmpl->free(inst);\n\tcrypto_tmpl_put(tmpl);\n}\n\nstatic struct list_head *crypto_more_spawns(struct crypto_alg *alg,\n\t\t\t\t\t    struct list_head *stack,\n\t\t\t\t\t    struct list_head *top,\n\t\t\t\t\t    struct list_head *secondary_spawns)\n{\n\tstruct crypto_spawn *spawn, *n;\n\n\tif (list_empty(stack))\n\t\treturn NULL;\n\n\tspawn = list_first_entry(stack, struct crypto_spawn, list);\n\tn = list_entry(spawn->list.next, struct crypto_spawn, list);\n\n\tif (spawn->alg && &n->list != stack && !n->alg)\n\t\tn->alg = (n->list.next == stack) ? alg :\n\t\t\t &list_entry(n->list.next, struct crypto_spawn,\n\t\t\t\t     list)->inst->alg;\n\n\tlist_move(&spawn->list, secondary_spawns);\n\n\treturn &n->list == stack ? top : &n->inst->alg.cra_users;\n}\n\nstatic void crypto_remove_spawn(struct crypto_spawn *spawn,\n\t\t\t\tstruct list_head *list)\n{\n\tstruct crypto_instance *inst = spawn->inst;\n\tstruct crypto_template *tmpl = inst->tmpl;\n\n\tif (crypto_is_dead(&inst->alg))\n\t\treturn;\n\n\tinst->alg.cra_flags |= CRYPTO_ALG_DEAD;\n\tif (hlist_unhashed(&inst->list))\n\t\treturn;\n\n\tif (!tmpl || !crypto_tmpl_get(tmpl))\n\t\treturn;\n\n\tcrypto_notify(CRYPTO_MSG_ALG_UNREGISTER, &inst->alg);\n\tlist_move(&inst->alg.cra_list, list);\n\thlist_del(&inst->list);\n\tinst->alg.cra_destroy = crypto_destroy_instance;\n\n\tBUG_ON(!list_empty(&inst->alg.cra_users));\n}\n\nvoid crypto_remove_spawns(struct crypto_alg *alg, struct list_head *list,\n\t\t\t  struct crypto_alg *nalg)\n{\n\tu32 new_type = (nalg ?: alg)->cra_flags;\n\tstruct crypto_spawn *spawn, *n;\n\tLIST_HEAD(secondary_spawns);\n\tstruct list_head *spawns;\n\tLIST_HEAD(stack);\n\tLIST_HEAD(top);\n\n\tspawns = &alg->cra_users;\n\tlist_for_each_entry_safe(spawn, n, spawns, list) {\n\t\tif ((spawn->alg->cra_flags ^ new_type) & spawn->mask)\n\t\t\tcontinue;\n\n\t\tlist_move(&spawn->list, &top);\n\t}\n\n\tspawns = &top;\n\tdo {\n\t\twhile (!list_empty(spawns)) {\n\t\t\tstruct crypto_instance *inst;\n\n\t\t\tspawn = list_first_entry(spawns, struct crypto_spawn,\n\t\t\t\t\t\t list);\n\t\t\tinst = spawn->inst;\n\n\t\t\tBUG_ON(&inst->alg == alg);\n\n\t\t\tlist_move(&spawn->list, &stack);\n\n\t\t\tif (&inst->alg == nalg)\n\t\t\t\tbreak;\n\n\t\t\tspawn->alg = NULL;\n\t\t\tspawns = &inst->alg.cra_users;\n\t\t}\n\t} while ((spawns = crypto_more_spawns(alg, &stack, &top,\n\t\t\t\t\t      &secondary_spawns)));\n\n\tlist_for_each_entry_safe(spawn, n, &secondary_spawns, list) {\n\t\tif (spawn->alg)\n\t\t\tlist_move(&spawn->list, &spawn->alg->cra_users);\n\t\telse\n\t\t\tcrypto_remove_spawn(spawn, list);\n\t}\n}\nEXPORT_SYMBOL_GPL(crypto_remove_spawns);\n\nstatic struct crypto_larval *__crypto_register_alg(struct crypto_alg *alg)\n{\n\tstruct crypto_alg *q;\n\tstruct crypto_larval *larval;\n\tint ret = -EAGAIN;\n\n\tif (crypto_is_dead(alg))\n\t\tgoto err;\n\n\tINIT_LIST_HEAD(&alg->cra_users);\n\n\t/* No cheating! */\n\talg->cra_flags &= ~CRYPTO_ALG_TESTED;\n\n\tret = -EEXIST;\n\n\tatomic_set(&alg->cra_refcnt, 1);\n\tlist_for_each_entry(q, &crypto_alg_list, cra_list) {\n\t\tif (q == alg)\n\t\t\tgoto err;\n\n\t\tif (crypto_is_moribund(q))\n\t\t\tcontinue;\n\n\t\tif (crypto_is_larval(q)) {\n\t\t\tif (!strcmp(alg->cra_driver_name, q->cra_driver_name))\n\t\t\t\tgoto err;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!strcmp(q->cra_driver_name, alg->cra_name) ||\n\t\t    !strcmp(q->cra_name, alg->cra_driver_name))\n\t\t\tgoto err;\n\t}\n\n\tlarval = crypto_larval_alloc(alg->cra_name,\n\t\t\t\t     alg->cra_flags | CRYPTO_ALG_TESTED, 0);\n\tif (IS_ERR(larval))\n\t\tgoto out;\n\n\tret = -ENOENT;\n\tlarval->adult = crypto_mod_get(alg);\n\tif (!larval->adult)\n\t\tgoto free_larval;\n\n\tatomic_set(&larval->alg.cra_refcnt, 1);\n\tmemcpy(larval->alg.cra_driver_name, alg->cra_driver_name,\n\t       CRYPTO_MAX_ALG_NAME);\n\tlarval->alg.cra_priority = alg->cra_priority;\n\n\tlist_add(&alg->cra_list, &crypto_alg_list);\n\tlist_add(&larval->alg.cra_list, &crypto_alg_list);\n\nout:\n\treturn larval;\n\nfree_larval:\n\tkfree(larval);\nerr:\n\tlarval = ERR_PTR(ret);\n\tgoto out;\n}\n\nvoid crypto_alg_tested(const char *name, int err)\n{\n\tstruct crypto_larval *test;\n\tstruct crypto_alg *alg;\n\tstruct crypto_alg *q;\n\tLIST_HEAD(list);\n\n\tdown_write(&crypto_alg_sem);\n\tlist_for_each_entry(q, &crypto_alg_list, cra_list) {\n\t\tif (crypto_is_moribund(q) || !crypto_is_larval(q))\n\t\t\tcontinue;\n\n\t\ttest = (struct crypto_larval *)q;\n\n\t\tif (!strcmp(q->cra_driver_name, name))\n\t\t\tgoto found;\n\t}\n\n\tprintk(KERN_ERR \"alg: Unexpected test result for %s: %d\\n\", name, err);\n\tgoto unlock;\n\nfound:\n\tq->cra_flags |= CRYPTO_ALG_DEAD;\n\talg = test->adult;\n\tif (err || list_empty(&alg->cra_list))\n\t\tgoto complete;\n\n\talg->cra_flags |= CRYPTO_ALG_TESTED;\n\n\tlist_for_each_entry(q, &crypto_alg_list, cra_list) {\n\t\tif (q == alg)\n\t\t\tcontinue;\n\n\t\tif (crypto_is_moribund(q))\n\t\t\tcontinue;\n\n\t\tif (crypto_is_larval(q)) {\n\t\t\tstruct crypto_larval *larval = (void *)q;\n\n\t\t\t/*\n\t\t\t * Check to see if either our generic name or\n\t\t\t * specific name can satisfy the name requested\n\t\t\t * by the larval entry q.\n\t\t\t */\n\t\t\tif (strcmp(alg->cra_name, q->cra_name) &&\n\t\t\t    strcmp(alg->cra_driver_name, q->cra_name))\n\t\t\t\tcontinue;\n\n\t\t\tif (larval->adult)\n\t\t\t\tcontinue;\n\t\t\tif ((q->cra_flags ^ alg->cra_flags) & larval->mask)\n\t\t\t\tcontinue;\n\t\t\tif (!crypto_mod_get(alg))\n\t\t\t\tcontinue;\n\n\t\t\tlarval->adult = alg;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (strcmp(alg->cra_name, q->cra_name))\n\t\t\tcontinue;\n\n\t\tif (strcmp(alg->cra_driver_name, q->cra_driver_name) &&\n\t\t    q->cra_priority > alg->cra_priority)\n\t\t\tcontinue;\n\n\t\tcrypto_remove_spawns(q, &list, alg);\n\t}\n\ncomplete:\n\tcomplete_all(&test->completion);\n\nunlock:\n\tup_write(&crypto_alg_sem);\n\n\tcrypto_remove_final(&list);\n}\nEXPORT_SYMBOL_GPL(crypto_alg_tested);\n\nvoid crypto_remove_final(struct list_head *list)\n{\n\tstruct crypto_alg *alg;\n\tstruct crypto_alg *n;\n\n\tlist_for_each_entry_safe(alg, n, list, cra_list) {\n\t\tlist_del_init(&alg->cra_list);\n\t\tcrypto_alg_put(alg);\n\t}\n}\nEXPORT_SYMBOL_GPL(crypto_remove_final);\n\nstatic void crypto_wait_for_test(struct crypto_larval *larval)\n{\n\tint err;\n\n\terr = crypto_probing_notify(CRYPTO_MSG_ALG_REGISTER, larval->adult);\n\tif (err != NOTIFY_STOP) {\n\t\tif (WARN_ON(err != NOTIFY_DONE))\n\t\t\tgoto out;\n\t\tcrypto_alg_tested(larval->alg.cra_driver_name, 0);\n\t}\n\n\terr = wait_for_completion_interruptible(&larval->completion);\n\tWARN_ON(err);\n\nout:\n\tcrypto_larval_kill(&larval->alg);\n}\n\nint crypto_register_alg(struct crypto_alg *alg)\n{\n\tstruct crypto_larval *larval;\n\tint err;\n\n\terr = crypto_check_alg(alg);\n\tif (err)\n\t\treturn err;\n\n\tdown_write(&crypto_alg_sem);\n\tlarval = __crypto_register_alg(alg);\n\tup_write(&crypto_alg_sem);\n\n\tif (IS_ERR(larval))\n\t\treturn PTR_ERR(larval);\n\n\tcrypto_wait_for_test(larval);\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(crypto_register_alg);\n\nstatic int crypto_remove_alg(struct crypto_alg *alg, struct list_head *list)\n{\n\tif (unlikely(list_empty(&alg->cra_list)))\n\t\treturn -ENOENT;\n\n\talg->cra_flags |= CRYPTO_ALG_DEAD;\n\n\tcrypto_notify(CRYPTO_MSG_ALG_UNREGISTER, alg);\n\tlist_del_init(&alg->cra_list);\n\tcrypto_remove_spawns(alg, list, NULL);\n\n\treturn 0;\n}\n\nint crypto_unregister_alg(struct crypto_alg *alg)\n{\n\tint ret;\n\tLIST_HEAD(list);\n\n\tdown_write(&crypto_alg_sem);\n\tret = crypto_remove_alg(alg, &list);\n\tup_write(&crypto_alg_sem);\n\n\tif (ret)\n\t\treturn ret;\n\n\tBUG_ON(atomic_read(&alg->cra_refcnt) != 1);\n\tif (alg->cra_destroy)\n\t\talg->cra_destroy(alg);\n\n\tcrypto_remove_final(&list);\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(crypto_unregister_alg);\n\nint crypto_register_algs(struct crypto_alg *algs, int count)\n{\n\tint i, ret;\n\n\tfor (i = 0; i < count; i++) {\n\t\tret = crypto_register_alg(&algs[i]);\n\t\tif (ret)\n\t\t\tgoto err;\n\t}\n\n\treturn 0;\n\nerr:\n\tfor (--i; i >= 0; --i)\n\t\tcrypto_unregister_alg(&algs[i]);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(crypto_register_algs);\n\nint crypto_unregister_algs(struct crypto_alg *algs, int count)\n{\n\tint i, ret;\n\n\tfor (i = 0; i < count; i++) {\n\t\tret = crypto_unregister_alg(&algs[i]);\n\t\tif (ret)\n\t\t\tpr_err(\"Failed to unregister %s %s: %d\\n\",\n\t\t\t       algs[i].cra_driver_name, algs[i].cra_name, ret);\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(crypto_unregister_algs);\n\nint crypto_register_template(struct crypto_template *tmpl)\n{\n\tstruct crypto_template *q;\n\tint err = -EEXIST;\n\n\tdown_write(&crypto_alg_sem);\n\n\tcrypto_check_module_sig(tmpl->module);\n\n\tlist_for_each_entry(q, &crypto_template_list, list) {\n\t\tif (q == tmpl)\n\t\t\tgoto out;\n\t}\n\n\tlist_add(&tmpl->list, &crypto_template_list);\n\tcrypto_notify(CRYPTO_MSG_TMPL_REGISTER, tmpl);\n\terr = 0;\nout:\n\tup_write(&crypto_alg_sem);\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(crypto_register_template);\n\nvoid crypto_unregister_template(struct crypto_template *tmpl)\n{\n\tstruct crypto_instance *inst;\n\tstruct hlist_node *n;\n\tstruct hlist_head *list;\n\tLIST_HEAD(users);\n\n\tdown_write(&crypto_alg_sem);\n\n\tBUG_ON(list_empty(&tmpl->list));\n\tlist_del_init(&tmpl->list);\n\n\tlist = &tmpl->instances;\n\thlist_for_each_entry(inst, list, list) {\n\t\tint err = crypto_remove_alg(&inst->alg, &users);\n\t\tBUG_ON(err);\n\t}\n\n\tcrypto_notify(CRYPTO_MSG_TMPL_UNREGISTER, tmpl);\n\n\tup_write(&crypto_alg_sem);\n\n\thlist_for_each_entry_safe(inst, n, list, list) {\n\t\tBUG_ON(atomic_read(&inst->alg.cra_refcnt) != 1);\n\t\ttmpl->free(inst);\n\t}\n\tcrypto_remove_final(&users);\n}\nEXPORT_SYMBOL_GPL(crypto_unregister_template);\n\nstatic struct crypto_template *__crypto_lookup_template(const char *name)\n{\n\tstruct crypto_template *q, *tmpl = NULL;\n\n\tdown_read(&crypto_alg_sem);\n\tlist_for_each_entry(q, &crypto_template_list, list) {\n\t\tif (strcmp(q->name, name))\n\t\t\tcontinue;\n\t\tif (unlikely(!crypto_tmpl_get(q)))\n\t\t\tcontinue;\n\n\t\ttmpl = q;\n\t\tbreak;\n\t}\n\tup_read(&crypto_alg_sem);\n\n\treturn tmpl;\n}\n\nstruct crypto_template *crypto_lookup_template(const char *name)\n{\n\treturn try_then_request_module(__crypto_lookup_template(name),\n\t\t\t\t       \"crypto-%s\", name);\n}\nEXPORT_SYMBOL_GPL(crypto_lookup_template);\n\nint crypto_register_instance(struct crypto_template *tmpl,\n\t\t\t     struct crypto_instance *inst)\n{\n\tstruct crypto_larval *larval;\n\tint err;\n\n\terr = crypto_check_alg(&inst->alg);\n\tif (err)\n\t\tgoto err;\n\n\tinst->alg.cra_module = tmpl->module;\n\tinst->alg.cra_flags |= CRYPTO_ALG_INSTANCE;\n\n\tdown_write(&crypto_alg_sem);\n\n\tlarval = __crypto_register_alg(&inst->alg);\n\tif (IS_ERR(larval))\n\t\tgoto unlock;\n\n\thlist_add_head(&inst->list, &tmpl->instances);\n\tinst->tmpl = tmpl;\n\nunlock:\n\tup_write(&crypto_alg_sem);\n\n\terr = PTR_ERR(larval);\n\tif (IS_ERR(larval))\n\t\tgoto err;\n\n\tcrypto_wait_for_test(larval);\n\terr = 0;\n\nerr:\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(crypto_register_instance);\n\nint crypto_unregister_instance(struct crypto_alg *alg)\n{\n\tint err;\n\tstruct crypto_instance *inst = (void *)alg;\n\tstruct crypto_template *tmpl = inst->tmpl;\n\tLIST_HEAD(users);\n\n\tif (!(alg->cra_flags & CRYPTO_ALG_INSTANCE))\n\t\treturn -EINVAL;\n\n\tBUG_ON(atomic_read(&alg->cra_refcnt) != 1);\n\n\tdown_write(&crypto_alg_sem);\n\n\thlist_del_init(&inst->list);\n\terr = crypto_remove_alg(alg, &users);\n\n\tup_write(&crypto_alg_sem);\n\n\tif (err)\n\t\treturn err;\n\n\ttmpl->free(inst);\n\tcrypto_remove_final(&users);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(crypto_unregister_instance);\n\nint crypto_init_spawn(struct crypto_spawn *spawn, struct crypto_alg *alg,\n\t\t      struct crypto_instance *inst, u32 mask)\n{\n\tint err = -EAGAIN;\n\n\tspawn->inst = inst;\n\tspawn->mask = mask;\n\n\tdown_write(&crypto_alg_sem);\n\tif (!crypto_is_moribund(alg)) {\n\t\tlist_add(&spawn->list, &alg->cra_users);\n\t\tspawn->alg = alg;\n\t\terr = 0;\n\t}\n\tup_write(&crypto_alg_sem);\n\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(crypto_init_spawn);\n\nint crypto_init_spawn2(struct crypto_spawn *spawn, struct crypto_alg *alg,\n\t\t       struct crypto_instance *inst,\n\t\t       const struct crypto_type *frontend)\n{\n\tint err = -EINVAL;\n\n\tif ((alg->cra_flags ^ frontend->type) & frontend->maskset)\n\t\tgoto out;\n\n\tspawn->frontend = frontend;\n\terr = crypto_init_spawn(spawn, alg, inst, frontend->maskset);\n\nout:\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(crypto_init_spawn2);\n\nvoid crypto_drop_spawn(struct crypto_spawn *spawn)\n{\n\tif (!spawn->alg)\n\t\treturn;\n\n\tdown_write(&crypto_alg_sem);\n\tlist_del(&spawn->list);\n\tup_write(&crypto_alg_sem);\n}\nEXPORT_SYMBOL_GPL(crypto_drop_spawn);\n\nstatic struct crypto_alg *crypto_spawn_alg(struct crypto_spawn *spawn)\n{\n\tstruct crypto_alg *alg;\n\tstruct crypto_alg *alg2;\n\n\tdown_read(&crypto_alg_sem);\n\talg = spawn->alg;\n\talg2 = alg;\n\tif (alg2)\n\t\talg2 = crypto_mod_get(alg2);\n\tup_read(&crypto_alg_sem);\n\n\tif (!alg2) {\n\t\tif (alg)\n\t\t\tcrypto_shoot_alg(alg);\n\t\treturn ERR_PTR(-EAGAIN);\n\t}\n\n\treturn alg;\n}\n\nstruct crypto_tfm *crypto_spawn_tfm(struct crypto_spawn *spawn, u32 type,\n\t\t\t\t    u32 mask)\n{\n\tstruct crypto_alg *alg;\n\tstruct crypto_tfm *tfm;\n\n\talg = crypto_spawn_alg(spawn);\n\tif (IS_ERR(alg))\n\t\treturn ERR_CAST(alg);\n\n\ttfm = ERR_PTR(-EINVAL);\n\tif (unlikely((alg->cra_flags ^ type) & mask))\n\t\tgoto out_put_alg;\n\n\ttfm = __crypto_alloc_tfm(alg, type, mask);\n\tif (IS_ERR(tfm))\n\t\tgoto out_put_alg;\n\n\treturn tfm;\n\nout_put_alg:\n\tcrypto_mod_put(alg);\n\treturn tfm;\n}\nEXPORT_SYMBOL_GPL(crypto_spawn_tfm);\n\nvoid *crypto_spawn_tfm2(struct crypto_spawn *spawn)\n{\n\tstruct crypto_alg *alg;\n\tstruct crypto_tfm *tfm;\n\n\talg = crypto_spawn_alg(spawn);\n\tif (IS_ERR(alg))\n\t\treturn ERR_CAST(alg);\n\n\ttfm = crypto_create_tfm(alg, spawn->frontend);\n\tif (IS_ERR(tfm))\n\t\tgoto out_put_alg;\n\n\treturn tfm;\n\nout_put_alg:\n\tcrypto_mod_put(alg);\n\treturn tfm;\n}\nEXPORT_SYMBOL_GPL(crypto_spawn_tfm2);\n\nint crypto_register_notifier(struct notifier_block *nb)\n{\n\treturn blocking_notifier_chain_register(&crypto_chain, nb);\n}\nEXPORT_SYMBOL_GPL(crypto_register_notifier);\n\nint crypto_unregister_notifier(struct notifier_block *nb)\n{\n\treturn blocking_notifier_chain_unregister(&crypto_chain, nb);\n}\nEXPORT_SYMBOL_GPL(crypto_unregister_notifier);\n\nstruct crypto_attr_type *crypto_get_attr_type(struct rtattr **tb)\n{\n\tstruct rtattr *rta = tb[0];\n\tstruct crypto_attr_type *algt;\n\n\tif (!rta)\n\t\treturn ERR_PTR(-ENOENT);\n\tif (RTA_PAYLOAD(rta) < sizeof(*algt))\n\t\treturn ERR_PTR(-EINVAL);\n\tif (rta->rta_type != CRYPTOA_TYPE)\n\t\treturn ERR_PTR(-EINVAL);\n\n\talgt = RTA_DATA(rta);\n\n\treturn algt;\n}\nEXPORT_SYMBOL_GPL(crypto_get_attr_type);\n\nint crypto_check_attr_type(struct rtattr **tb, u32 type)\n{\n\tstruct crypto_attr_type *algt;\n\n\talgt = crypto_get_attr_type(tb);\n\tif (IS_ERR(algt))\n\t\treturn PTR_ERR(algt);\n\n\tif ((algt->type ^ type) & algt->mask)\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(crypto_check_attr_type);\n\nconst char *crypto_attr_alg_name(struct rtattr *rta)\n{\n\tstruct crypto_attr_alg *alga;\n\n\tif (!rta)\n\t\treturn ERR_PTR(-ENOENT);\n\tif (RTA_PAYLOAD(rta) < sizeof(*alga))\n\t\treturn ERR_PTR(-EINVAL);\n\tif (rta->rta_type != CRYPTOA_ALG)\n\t\treturn ERR_PTR(-EINVAL);\n\n\talga = RTA_DATA(rta);\n\talga->name[CRYPTO_MAX_ALG_NAME - 1] = 0;\n\n\treturn alga->name;\n}\nEXPORT_SYMBOL_GPL(crypto_attr_alg_name);\n\nstruct crypto_alg *crypto_attr_alg2(struct rtattr *rta,\n\t\t\t\t    const struct crypto_type *frontend,\n\t\t\t\t    u32 type, u32 mask)\n{\n\tconst char *name;\n\n\tname = crypto_attr_alg_name(rta);\n\tif (IS_ERR(name))\n\t\treturn ERR_CAST(name);\n\n\treturn crypto_find_alg(name, frontend, type, mask);\n}\nEXPORT_SYMBOL_GPL(crypto_attr_alg2);\n\nint crypto_attr_u32(struct rtattr *rta, u32 *num)\n{\n\tstruct crypto_attr_u32 *nu32;\n\n\tif (!rta)\n\t\treturn -ENOENT;\n\tif (RTA_PAYLOAD(rta) < sizeof(*nu32))\n\t\treturn -EINVAL;\n\tif (rta->rta_type != CRYPTOA_U32)\n\t\treturn -EINVAL;\n\n\tnu32 = RTA_DATA(rta);\n\t*num = nu32->num;\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(crypto_attr_u32);\n\nvoid *crypto_alloc_instance2(const char *name, struct crypto_alg *alg,\n\t\t\t     unsigned int head)\n{\n\tstruct crypto_instance *inst;\n\tchar *p;\n\tint err;\n\n\tp = kzalloc(head + sizeof(*inst) + sizeof(struct crypto_spawn),\n\t\t    GFP_KERNEL);\n\tif (!p)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tinst = (void *)(p + head);\n\n\terr = -ENAMETOOLONG;\n\tif (snprintf(inst->alg.cra_name, CRYPTO_MAX_ALG_NAME, \"%s(%s)\", name,\n\t\t     alg->cra_name) >= CRYPTO_MAX_ALG_NAME)\n\t\tgoto err_free_inst;\n\n\tif (snprintf(inst->alg.cra_driver_name, CRYPTO_MAX_ALG_NAME, \"%s(%s)\",\n\t\t     name, alg->cra_driver_name) >= CRYPTO_MAX_ALG_NAME)\n\t\tgoto err_free_inst;\n\n\treturn p;\n\nerr_free_inst:\n\tkfree(p);\n\treturn ERR_PTR(err);\n}\nEXPORT_SYMBOL_GPL(crypto_alloc_instance2);\n\nstruct crypto_instance *crypto_alloc_instance(const char *name,\n\t\t\t\t\t      struct crypto_alg *alg)\n{\n\tstruct crypto_instance *inst;\n\tstruct crypto_spawn *spawn;\n\tint err;\n\n\tinst = crypto_alloc_instance2(name, alg, 0);\n\tif (IS_ERR(inst))\n\t\tgoto out;\n\n\tspawn = crypto_instance_ctx(inst);\n\terr = crypto_init_spawn(spawn, alg, inst,\n\t\t\t\tCRYPTO_ALG_TYPE_MASK | CRYPTO_ALG_ASYNC);\n\n\tif (err)\n\t\tgoto err_free_inst;\n\n\treturn inst;\n\nerr_free_inst:\n\tkfree(inst);\n\tinst = ERR_PTR(err);\n\nout:\n\treturn inst;\n}\nEXPORT_SYMBOL_GPL(crypto_alloc_instance);\n\nvoid crypto_init_queue(struct crypto_queue *queue, unsigned int max_qlen)\n{\n\tINIT_LIST_HEAD(&queue->list);\n\tqueue->backlog = &queue->list;\n\tqueue->qlen = 0;\n\tqueue->max_qlen = max_qlen;\n}\nEXPORT_SYMBOL_GPL(crypto_init_queue);\n\nint crypto_enqueue_request(struct crypto_queue *queue,\n\t\t\t   struct crypto_async_request *request)\n{\n\tint err = -EINPROGRESS;\n\n\tif (unlikely(queue->qlen >= queue->max_qlen)) {\n\t\terr = -EBUSY;\n\t\tif (!(request->flags & CRYPTO_TFM_REQ_MAY_BACKLOG))\n\t\t\tgoto out;\n\t\tif (queue->backlog == &queue->list)\n\t\t\tqueue->backlog = &request->list;\n\t}\n\n\tqueue->qlen++;\n\tlist_add_tail(&request->list, &queue->list);\n\nout:\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(crypto_enqueue_request);\n\nvoid *__crypto_dequeue_request(struct crypto_queue *queue, unsigned int offset)\n{\n\tstruct list_head *request;\n\n\tif (unlikely(!queue->qlen))\n\t\treturn NULL;\n\n\tqueue->qlen--;\n\n\tif (queue->backlog != &queue->list)\n\t\tqueue->backlog = queue->backlog->next;\n\n\trequest = queue->list.next;\n\tlist_del(request);\n\n\treturn (char *)list_entry(request, struct crypto_async_request, list) -\n\t       offset;\n}\nEXPORT_SYMBOL_GPL(__crypto_dequeue_request);\n\nstruct crypto_async_request *crypto_dequeue_request(struct crypto_queue *queue)\n{\n\treturn __crypto_dequeue_request(queue, 0);\n}\nEXPORT_SYMBOL_GPL(crypto_dequeue_request);\n\nint crypto_tfm_in_queue(struct crypto_queue *queue, struct crypto_tfm *tfm)\n{\n\tstruct crypto_async_request *req;\n\n\tlist_for_each_entry(req, &queue->list, list) {\n\t\tif (req->tfm == tfm)\n\t\t\treturn 1;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(crypto_tfm_in_queue);\n\nstatic inline void crypto_inc_byte(u8 *a, unsigned int size)\n{\n\tu8 *b = (a + size);\n\tu8 c;\n\n\tfor (; size; size--) {\n\t\tc = *--b + 1;\n\t\t*b = c;\n\t\tif (c)\n\t\t\tbreak;\n\t}\n}\n\nvoid crypto_inc(u8 *a, unsigned int size)\n{\n\t__be32 *b = (__be32 *)(a + size);\n\tu32 c;\n\n\tfor (; size >= 4; size -= 4) {\n\t\tc = be32_to_cpu(*--b) + 1;\n\t\t*b = cpu_to_be32(c);\n\t\tif (c)\n\t\t\treturn;\n\t}\n\n\tcrypto_inc_byte(a, size);\n}\nEXPORT_SYMBOL_GPL(crypto_inc);\n\nstatic inline void crypto_xor_byte(u8 *a, const u8 *b, unsigned int size)\n{\n\tfor (; size; size--)\n\t\t*a++ ^= *b++;\n}\n\nvoid crypto_xor(u8 *dst, const u8 *src, unsigned int size)\n{\n\tu32 *a = (u32 *)dst;\n\tu32 *b = (u32 *)src;\n\n\tfor (; size >= 4; size -= 4)\n\t\t*a++ ^= *b++;\n\n\tcrypto_xor_byte((u8 *)a, (u8 *)b, size);\n}\nEXPORT_SYMBOL_GPL(crypto_xor);\n\nstatic int __init crypto_algapi_init(void)\n{\n\tcrypto_init_proc();\n\treturn 0;\n}\n\nstatic void __exit crypto_algapi_exit(void)\n{\n\tcrypto_exit_proc();\n}\n\nmodule_init(crypto_algapi_init);\nmodule_exit(crypto_algapi_exit);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"Cryptographic algorithms API\");\n", "/*\n * Authenc: Simple AEAD wrapper for IPsec\n *\n * Copyright (c) 2007 Herbert Xu <herbert@gondor.apana.org.au>\n *\n * This program is free software; you can redistribute it and/or modify it\n * under the terms of the GNU General Public License as published by the Free\n * Software Foundation; either version 2 of the License, or (at your option)\n * any later version.\n *\n */\n\n#include <crypto/aead.h>\n#include <crypto/internal/hash.h>\n#include <crypto/internal/skcipher.h>\n#include <crypto/authenc.h>\n#include <crypto/scatterwalk.h>\n#include <linux/err.h>\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/rtnetlink.h>\n#include <linux/slab.h>\n#include <linux/spinlock.h>\n\ntypedef u8 *(*authenc_ahash_t)(struct aead_request *req, unsigned int flags);\n\nstruct authenc_instance_ctx {\n\tstruct crypto_ahash_spawn auth;\n\tstruct crypto_skcipher_spawn enc;\n};\n\nstruct crypto_authenc_ctx {\n\tunsigned int reqoff;\n\tstruct crypto_ahash *auth;\n\tstruct crypto_ablkcipher *enc;\n};\n\nstruct authenc_request_ctx {\n\tunsigned int cryptlen;\n\tstruct scatterlist *sg;\n\tstruct scatterlist asg[2];\n\tstruct scatterlist cipher[2];\n\tcrypto_completion_t complete;\n\tcrypto_completion_t update_complete;\n\tchar tail[];\n};\n\nstatic void authenc_request_complete(struct aead_request *req, int err)\n{\n\tif (err != -EINPROGRESS)\n\t\taead_request_complete(req, err);\n}\n\nint crypto_authenc_extractkeys(struct crypto_authenc_keys *keys, const u8 *key,\n\t\t\t       unsigned int keylen)\n{\n\tstruct rtattr *rta = (struct rtattr *)key;\n\tstruct crypto_authenc_key_param *param;\n\n\tif (!RTA_OK(rta, keylen))\n\t\treturn -EINVAL;\n\tif (rta->rta_type != CRYPTO_AUTHENC_KEYA_PARAM)\n\t\treturn -EINVAL;\n\tif (RTA_PAYLOAD(rta) < sizeof(*param))\n\t\treturn -EINVAL;\n\n\tparam = RTA_DATA(rta);\n\tkeys->enckeylen = be32_to_cpu(param->enckeylen);\n\n\tkey += RTA_ALIGN(rta->rta_len);\n\tkeylen -= RTA_ALIGN(rta->rta_len);\n\n\tif (keylen < keys->enckeylen)\n\t\treturn -EINVAL;\n\n\tkeys->authkeylen = keylen - keys->enckeylen;\n\tkeys->authkey = key;\n\tkeys->enckey = key + keys->authkeylen;\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(crypto_authenc_extractkeys);\n\nstatic int crypto_authenc_setkey(struct crypto_aead *authenc, const u8 *key,\n\t\t\t\t unsigned int keylen)\n{\n\tstruct crypto_authenc_ctx *ctx = crypto_aead_ctx(authenc);\n\tstruct crypto_ahash *auth = ctx->auth;\n\tstruct crypto_ablkcipher *enc = ctx->enc;\n\tstruct crypto_authenc_keys keys;\n\tint err = -EINVAL;\n\n\tif (crypto_authenc_extractkeys(&keys, key, keylen) != 0)\n\t\tgoto badkey;\n\n\tcrypto_ahash_clear_flags(auth, CRYPTO_TFM_REQ_MASK);\n\tcrypto_ahash_set_flags(auth, crypto_aead_get_flags(authenc) &\n\t\t\t\t    CRYPTO_TFM_REQ_MASK);\n\terr = crypto_ahash_setkey(auth, keys.authkey, keys.authkeylen);\n\tcrypto_aead_set_flags(authenc, crypto_ahash_get_flags(auth) &\n\t\t\t\t       CRYPTO_TFM_RES_MASK);\n\n\tif (err)\n\t\tgoto out;\n\n\tcrypto_ablkcipher_clear_flags(enc, CRYPTO_TFM_REQ_MASK);\n\tcrypto_ablkcipher_set_flags(enc, crypto_aead_get_flags(authenc) &\n\t\t\t\t\t CRYPTO_TFM_REQ_MASK);\n\terr = crypto_ablkcipher_setkey(enc, keys.enckey, keys.enckeylen);\n\tcrypto_aead_set_flags(authenc, crypto_ablkcipher_get_flags(enc) &\n\t\t\t\t       CRYPTO_TFM_RES_MASK);\n\nout:\n\treturn err;\n\nbadkey:\n\tcrypto_aead_set_flags(authenc, CRYPTO_TFM_RES_BAD_KEY_LEN);\n\tgoto out;\n}\n\nstatic void authenc_geniv_ahash_update_done(struct crypto_async_request *areq,\n\t\t\t\t\t    int err)\n{\n\tstruct aead_request *req = areq->data;\n\tstruct crypto_aead *authenc = crypto_aead_reqtfm(req);\n\tstruct crypto_authenc_ctx *ctx = crypto_aead_ctx(authenc);\n\tstruct authenc_request_ctx *areq_ctx = aead_request_ctx(req);\n\tstruct ahash_request *ahreq = (void *)(areq_ctx->tail + ctx->reqoff);\n\n\tif (err)\n\t\tgoto out;\n\n\tahash_request_set_crypt(ahreq, areq_ctx->sg, ahreq->result,\n\t\t\t\tareq_ctx->cryptlen);\n\tahash_request_set_callback(ahreq, aead_request_flags(req) &\n\t\t\t\t\t  CRYPTO_TFM_REQ_MAY_SLEEP,\n\t\t\t\t   areq_ctx->complete, req);\n\n\terr = crypto_ahash_finup(ahreq);\n\tif (err)\n\t\tgoto out;\n\n\tscatterwalk_map_and_copy(ahreq->result, areq_ctx->sg,\n\t\t\t\t areq_ctx->cryptlen,\n\t\t\t\t crypto_aead_authsize(authenc), 1);\n\nout:\n\tauthenc_request_complete(req, err);\n}\n\nstatic void authenc_geniv_ahash_done(struct crypto_async_request *areq, int err)\n{\n\tstruct aead_request *req = areq->data;\n\tstruct crypto_aead *authenc = crypto_aead_reqtfm(req);\n\tstruct crypto_authenc_ctx *ctx = crypto_aead_ctx(authenc);\n\tstruct authenc_request_ctx *areq_ctx = aead_request_ctx(req);\n\tstruct ahash_request *ahreq = (void *)(areq_ctx->tail + ctx->reqoff);\n\n\tif (err)\n\t\tgoto out;\n\n\tscatterwalk_map_and_copy(ahreq->result, areq_ctx->sg,\n\t\t\t\t areq_ctx->cryptlen,\n\t\t\t\t crypto_aead_authsize(authenc), 1);\n\nout:\n\taead_request_complete(req, err);\n}\n\nstatic void authenc_verify_ahash_update_done(struct crypto_async_request *areq,\n\t\t\t\t\t     int err)\n{\n\tu8 *ihash;\n\tunsigned int authsize;\n\tstruct ablkcipher_request *abreq;\n\tstruct aead_request *req = areq->data;\n\tstruct crypto_aead *authenc = crypto_aead_reqtfm(req);\n\tstruct crypto_authenc_ctx *ctx = crypto_aead_ctx(authenc);\n\tstruct authenc_request_ctx *areq_ctx = aead_request_ctx(req);\n\tstruct ahash_request *ahreq = (void *)(areq_ctx->tail + ctx->reqoff);\n\tunsigned int cryptlen = req->cryptlen;\n\n\tif (err)\n\t\tgoto out;\n\n\tahash_request_set_crypt(ahreq, areq_ctx->sg, ahreq->result,\n\t\t\t\tareq_ctx->cryptlen);\n\tahash_request_set_callback(ahreq, aead_request_flags(req) &\n\t\t\t\t\t  CRYPTO_TFM_REQ_MAY_SLEEP,\n\t\t\t\t   areq_ctx->complete, req);\n\n\terr = crypto_ahash_finup(ahreq);\n\tif (err)\n\t\tgoto out;\n\n\tauthsize = crypto_aead_authsize(authenc);\n\tcryptlen -= authsize;\n\tihash = ahreq->result + authsize;\n\tscatterwalk_map_and_copy(ihash, areq_ctx->sg, areq_ctx->cryptlen,\n\t\t\t\t authsize, 0);\n\n\terr = crypto_memneq(ihash, ahreq->result, authsize) ? -EBADMSG : 0;\n\tif (err)\n\t\tgoto out;\n\n\tabreq = aead_request_ctx(req);\n\tablkcipher_request_set_tfm(abreq, ctx->enc);\n\tablkcipher_request_set_callback(abreq, aead_request_flags(req),\n\t\t\t\t\treq->base.complete, req->base.data);\n\tablkcipher_request_set_crypt(abreq, req->src, req->dst,\n\t\t\t\t     cryptlen, req->iv);\n\n\terr = crypto_ablkcipher_decrypt(abreq);\n\nout:\n\tauthenc_request_complete(req, err);\n}\n\nstatic void authenc_verify_ahash_done(struct crypto_async_request *areq,\n\t\t\t\t      int err)\n{\n\tu8 *ihash;\n\tunsigned int authsize;\n\tstruct ablkcipher_request *abreq;\n\tstruct aead_request *req = areq->data;\n\tstruct crypto_aead *authenc = crypto_aead_reqtfm(req);\n\tstruct crypto_authenc_ctx *ctx = crypto_aead_ctx(authenc);\n\tstruct authenc_request_ctx *areq_ctx = aead_request_ctx(req);\n\tstruct ahash_request *ahreq = (void *)(areq_ctx->tail + ctx->reqoff);\n\tunsigned int cryptlen = req->cryptlen;\n\n\tif (err)\n\t\tgoto out;\n\n\tauthsize = crypto_aead_authsize(authenc);\n\tcryptlen -= authsize;\n\tihash = ahreq->result + authsize;\n\tscatterwalk_map_and_copy(ihash, areq_ctx->sg, areq_ctx->cryptlen,\n\t\t\t\t authsize, 0);\n\n\terr = crypto_memneq(ihash, ahreq->result, authsize) ? -EBADMSG : 0;\n\tif (err)\n\t\tgoto out;\n\n\tabreq = aead_request_ctx(req);\n\tablkcipher_request_set_tfm(abreq, ctx->enc);\n\tablkcipher_request_set_callback(abreq, aead_request_flags(req),\n\t\t\t\t\treq->base.complete, req->base.data);\n\tablkcipher_request_set_crypt(abreq, req->src, req->dst,\n\t\t\t\t     cryptlen, req->iv);\n\n\terr = crypto_ablkcipher_decrypt(abreq);\n\nout:\n\tauthenc_request_complete(req, err);\n}\n\nstatic u8 *crypto_authenc_ahash_fb(struct aead_request *req, unsigned int flags)\n{\n\tstruct crypto_aead *authenc = crypto_aead_reqtfm(req);\n\tstruct crypto_authenc_ctx *ctx = crypto_aead_ctx(authenc);\n\tstruct crypto_ahash *auth = ctx->auth;\n\tstruct authenc_request_ctx *areq_ctx = aead_request_ctx(req);\n\tstruct ahash_request *ahreq = (void *)(areq_ctx->tail + ctx->reqoff);\n\tu8 *hash = areq_ctx->tail;\n\tint err;\n\n\thash = (u8 *)ALIGN((unsigned long)hash + crypto_ahash_alignmask(auth),\n\t\t\t    crypto_ahash_alignmask(auth) + 1);\n\n\tahash_request_set_tfm(ahreq, auth);\n\n\terr = crypto_ahash_init(ahreq);\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\tahash_request_set_crypt(ahreq, req->assoc, hash, req->assoclen);\n\tahash_request_set_callback(ahreq, aead_request_flags(req) & flags,\n\t\t\t\t   areq_ctx->update_complete, req);\n\n\terr = crypto_ahash_update(ahreq);\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\tahash_request_set_crypt(ahreq, areq_ctx->sg, hash,\n\t\t\t\tareq_ctx->cryptlen);\n\tahash_request_set_callback(ahreq, aead_request_flags(req) & flags,\n\t\t\t\t   areq_ctx->complete, req);\n\n\terr = crypto_ahash_finup(ahreq);\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\treturn hash;\n}\n\nstatic u8 *crypto_authenc_ahash(struct aead_request *req, unsigned int flags)\n{\n\tstruct crypto_aead *authenc = crypto_aead_reqtfm(req);\n\tstruct crypto_authenc_ctx *ctx = crypto_aead_ctx(authenc);\n\tstruct crypto_ahash *auth = ctx->auth;\n\tstruct authenc_request_ctx *areq_ctx = aead_request_ctx(req);\n\tstruct ahash_request *ahreq = (void *)(areq_ctx->tail + ctx->reqoff);\n\tu8 *hash = areq_ctx->tail;\n\tint err;\n\n\thash = (u8 *)ALIGN((unsigned long)hash + crypto_ahash_alignmask(auth),\n\t\t\t   crypto_ahash_alignmask(auth) + 1);\n\n\tahash_request_set_tfm(ahreq, auth);\n\tahash_request_set_crypt(ahreq, areq_ctx->sg, hash,\n\t\t\t\tareq_ctx->cryptlen);\n\tahash_request_set_callback(ahreq, aead_request_flags(req) & flags,\n\t\t\t\t   areq_ctx->complete, req);\n\n\terr = crypto_ahash_digest(ahreq);\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\treturn hash;\n}\n\nstatic int crypto_authenc_genicv(struct aead_request *req, u8 *iv,\n\t\t\t\t unsigned int flags)\n{\n\tstruct crypto_aead *authenc = crypto_aead_reqtfm(req);\n\tstruct authenc_request_ctx *areq_ctx = aead_request_ctx(req);\n\tstruct scatterlist *dst = req->dst;\n\tstruct scatterlist *assoc = req->assoc;\n\tstruct scatterlist *cipher = areq_ctx->cipher;\n\tstruct scatterlist *asg = areq_ctx->asg;\n\tunsigned int ivsize = crypto_aead_ivsize(authenc);\n\tunsigned int cryptlen = req->cryptlen;\n\tauthenc_ahash_t authenc_ahash_fn = crypto_authenc_ahash_fb;\n\tstruct page *dstp;\n\tu8 *vdst;\n\tu8 *hash;\n\n\tdstp = sg_page(dst);\n\tvdst = PageHighMem(dstp) ? NULL : page_address(dstp) + dst->offset;\n\n\tif (ivsize) {\n\t\tsg_init_table(cipher, 2);\n\t\tsg_set_buf(cipher, iv, ivsize);\n\t\tscatterwalk_crypto_chain(cipher, dst, vdst == iv + ivsize, 2);\n\t\tdst = cipher;\n\t\tcryptlen += ivsize;\n\t}\n\n\tif (req->assoclen && sg_is_last(assoc)) {\n\t\tauthenc_ahash_fn = crypto_authenc_ahash;\n\t\tsg_init_table(asg, 2);\n\t\tsg_set_page(asg, sg_page(assoc), assoc->length, assoc->offset);\n\t\tscatterwalk_crypto_chain(asg, dst, 0, 2);\n\t\tdst = asg;\n\t\tcryptlen += req->assoclen;\n\t}\n\n\tareq_ctx->cryptlen = cryptlen;\n\tareq_ctx->sg = dst;\n\n\tareq_ctx->complete = authenc_geniv_ahash_done;\n\tareq_ctx->update_complete = authenc_geniv_ahash_update_done;\n\n\thash = authenc_ahash_fn(req, flags);\n\tif (IS_ERR(hash))\n\t\treturn PTR_ERR(hash);\n\n\tscatterwalk_map_and_copy(hash, dst, cryptlen,\n\t\t\t\t crypto_aead_authsize(authenc), 1);\n\treturn 0;\n}\n\nstatic void crypto_authenc_encrypt_done(struct crypto_async_request *req,\n\t\t\t\t\tint err)\n{\n\tstruct aead_request *areq = req->data;\n\n\tif (!err) {\n\t\tstruct crypto_aead *authenc = crypto_aead_reqtfm(areq);\n\t\tstruct crypto_authenc_ctx *ctx = crypto_aead_ctx(authenc);\n\t\tstruct authenc_request_ctx *areq_ctx = aead_request_ctx(areq);\n\t\tstruct ablkcipher_request *abreq = (void *)(areq_ctx->tail\n\t\t\t\t\t\t\t    + ctx->reqoff);\n\t\tu8 *iv = (u8 *)abreq - crypto_ablkcipher_ivsize(ctx->enc);\n\n\t\terr = crypto_authenc_genicv(areq, iv, 0);\n\t}\n\n\tauthenc_request_complete(areq, err);\n}\n\nstatic int crypto_authenc_encrypt(struct aead_request *req)\n{\n\tstruct crypto_aead *authenc = crypto_aead_reqtfm(req);\n\tstruct crypto_authenc_ctx *ctx = crypto_aead_ctx(authenc);\n\tstruct authenc_request_ctx *areq_ctx = aead_request_ctx(req);\n\tstruct crypto_ablkcipher *enc = ctx->enc;\n\tstruct scatterlist *dst = req->dst;\n\tunsigned int cryptlen = req->cryptlen;\n\tstruct ablkcipher_request *abreq = (void *)(areq_ctx->tail\n\t\t\t\t\t\t    + ctx->reqoff);\n\tu8 *iv = (u8 *)abreq - crypto_ablkcipher_ivsize(enc);\n\tint err;\n\n\tablkcipher_request_set_tfm(abreq, enc);\n\tablkcipher_request_set_callback(abreq, aead_request_flags(req),\n\t\t\t\t\tcrypto_authenc_encrypt_done, req);\n\tablkcipher_request_set_crypt(abreq, req->src, dst, cryptlen, req->iv);\n\n\tmemcpy(iv, req->iv, crypto_aead_ivsize(authenc));\n\n\terr = crypto_ablkcipher_encrypt(abreq);\n\tif (err)\n\t\treturn err;\n\n\treturn crypto_authenc_genicv(req, iv, CRYPTO_TFM_REQ_MAY_SLEEP);\n}\n\nstatic void crypto_authenc_givencrypt_done(struct crypto_async_request *req,\n\t\t\t\t\t   int err)\n{\n\tstruct aead_request *areq = req->data;\n\n\tif (!err) {\n\t\tstruct skcipher_givcrypt_request *greq = aead_request_ctx(areq);\n\n\t\terr = crypto_authenc_genicv(areq, greq->giv, 0);\n\t}\n\n\tauthenc_request_complete(areq, err);\n}\n\nstatic int crypto_authenc_givencrypt(struct aead_givcrypt_request *req)\n{\n\tstruct crypto_aead *authenc = aead_givcrypt_reqtfm(req);\n\tstruct crypto_authenc_ctx *ctx = crypto_aead_ctx(authenc);\n\tstruct aead_request *areq = &req->areq;\n\tstruct skcipher_givcrypt_request *greq = aead_request_ctx(areq);\n\tu8 *iv = req->giv;\n\tint err;\n\n\tskcipher_givcrypt_set_tfm(greq, ctx->enc);\n\tskcipher_givcrypt_set_callback(greq, aead_request_flags(areq),\n\t\t\t\t       crypto_authenc_givencrypt_done, areq);\n\tskcipher_givcrypt_set_crypt(greq, areq->src, areq->dst, areq->cryptlen,\n\t\t\t\t    areq->iv);\n\tskcipher_givcrypt_set_giv(greq, iv, req->seq);\n\n\terr = crypto_skcipher_givencrypt(greq);\n\tif (err)\n\t\treturn err;\n\n\treturn crypto_authenc_genicv(areq, iv, CRYPTO_TFM_REQ_MAY_SLEEP);\n}\n\nstatic int crypto_authenc_verify(struct aead_request *req,\n\t\t\t\t authenc_ahash_t authenc_ahash_fn)\n{\n\tstruct crypto_aead *authenc = crypto_aead_reqtfm(req);\n\tstruct authenc_request_ctx *areq_ctx = aead_request_ctx(req);\n\tu8 *ohash;\n\tu8 *ihash;\n\tunsigned int authsize;\n\n\tareq_ctx->complete = authenc_verify_ahash_done;\n\tareq_ctx->update_complete = authenc_verify_ahash_update_done;\n\n\tohash = authenc_ahash_fn(req, CRYPTO_TFM_REQ_MAY_SLEEP);\n\tif (IS_ERR(ohash))\n\t\treturn PTR_ERR(ohash);\n\n\tauthsize = crypto_aead_authsize(authenc);\n\tihash = ohash + authsize;\n\tscatterwalk_map_and_copy(ihash, areq_ctx->sg, areq_ctx->cryptlen,\n\t\t\t\t authsize, 0);\n\treturn crypto_memneq(ihash, ohash, authsize) ? -EBADMSG : 0;\n}\n\nstatic int crypto_authenc_iverify(struct aead_request *req, u8 *iv,\n\t\t\t\t  unsigned int cryptlen)\n{\n\tstruct crypto_aead *authenc = crypto_aead_reqtfm(req);\n\tstruct authenc_request_ctx *areq_ctx = aead_request_ctx(req);\n\tstruct scatterlist *src = req->src;\n\tstruct scatterlist *assoc = req->assoc;\n\tstruct scatterlist *cipher = areq_ctx->cipher;\n\tstruct scatterlist *asg = areq_ctx->asg;\n\tunsigned int ivsize = crypto_aead_ivsize(authenc);\n\tauthenc_ahash_t authenc_ahash_fn = crypto_authenc_ahash_fb;\n\tstruct page *srcp;\n\tu8 *vsrc;\n\n\tsrcp = sg_page(src);\n\tvsrc = PageHighMem(srcp) ? NULL : page_address(srcp) + src->offset;\n\n\tif (ivsize) {\n\t\tsg_init_table(cipher, 2);\n\t\tsg_set_buf(cipher, iv, ivsize);\n\t\tscatterwalk_crypto_chain(cipher, src, vsrc == iv + ivsize, 2);\n\t\tsrc = cipher;\n\t\tcryptlen += ivsize;\n\t}\n\n\tif (req->assoclen && sg_is_last(assoc)) {\n\t\tauthenc_ahash_fn = crypto_authenc_ahash;\n\t\tsg_init_table(asg, 2);\n\t\tsg_set_page(asg, sg_page(assoc), assoc->length, assoc->offset);\n\t\tscatterwalk_crypto_chain(asg, src, 0, 2);\n\t\tsrc = asg;\n\t\tcryptlen += req->assoclen;\n\t}\n\n\tareq_ctx->cryptlen = cryptlen;\n\tareq_ctx->sg = src;\n\n\treturn crypto_authenc_verify(req, authenc_ahash_fn);\n}\n\nstatic int crypto_authenc_decrypt(struct aead_request *req)\n{\n\tstruct crypto_aead *authenc = crypto_aead_reqtfm(req);\n\tstruct crypto_authenc_ctx *ctx = crypto_aead_ctx(authenc);\n\tstruct ablkcipher_request *abreq = aead_request_ctx(req);\n\tunsigned int cryptlen = req->cryptlen;\n\tunsigned int authsize = crypto_aead_authsize(authenc);\n\tu8 *iv = req->iv;\n\tint err;\n\n\tif (cryptlen < authsize)\n\t\treturn -EINVAL;\n\tcryptlen -= authsize;\n\n\terr = crypto_authenc_iverify(req, iv, cryptlen);\n\tif (err)\n\t\treturn err;\n\n\tablkcipher_request_set_tfm(abreq, ctx->enc);\n\tablkcipher_request_set_callback(abreq, aead_request_flags(req),\n\t\t\t\t\treq->base.complete, req->base.data);\n\tablkcipher_request_set_crypt(abreq, req->src, req->dst, cryptlen, iv);\n\n\treturn crypto_ablkcipher_decrypt(abreq);\n}\n\nstatic int crypto_authenc_init_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_instance *inst = crypto_tfm_alg_instance(tfm);\n\tstruct authenc_instance_ctx *ictx = crypto_instance_ctx(inst);\n\tstruct crypto_authenc_ctx *ctx = crypto_tfm_ctx(tfm);\n\tstruct crypto_ahash *auth;\n\tstruct crypto_ablkcipher *enc;\n\tint err;\n\n\tauth = crypto_spawn_ahash(&ictx->auth);\n\tif (IS_ERR(auth))\n\t\treturn PTR_ERR(auth);\n\n\tenc = crypto_spawn_skcipher(&ictx->enc);\n\terr = PTR_ERR(enc);\n\tif (IS_ERR(enc))\n\t\tgoto err_free_ahash;\n\n\tctx->auth = auth;\n\tctx->enc = enc;\n\n\tctx->reqoff = ALIGN(2 * crypto_ahash_digestsize(auth) +\n\t\t\t    crypto_ahash_alignmask(auth),\n\t\t\t    crypto_ahash_alignmask(auth) + 1) +\n\t\t      crypto_ablkcipher_ivsize(enc);\n\n\ttfm->crt_aead.reqsize = sizeof(struct authenc_request_ctx) +\n\t\t\t\tctx->reqoff +\n\t\t\t\tmax_t(unsigned int,\n\t\t\t\tcrypto_ahash_reqsize(auth) +\n\t\t\t\tsizeof(struct ahash_request),\n\t\t\t\tsizeof(struct skcipher_givcrypt_request) +\n\t\t\t\tcrypto_ablkcipher_reqsize(enc));\n\n\treturn 0;\n\nerr_free_ahash:\n\tcrypto_free_ahash(auth);\n\treturn err;\n}\n\nstatic void crypto_authenc_exit_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_authenc_ctx *ctx = crypto_tfm_ctx(tfm);\n\n\tcrypto_free_ahash(ctx->auth);\n\tcrypto_free_ablkcipher(ctx->enc);\n}\n\nstatic struct crypto_instance *crypto_authenc_alloc(struct rtattr **tb)\n{\n\tstruct crypto_attr_type *algt;\n\tstruct crypto_instance *inst;\n\tstruct hash_alg_common *auth;\n\tstruct crypto_alg *auth_base;\n\tstruct crypto_alg *enc;\n\tstruct authenc_instance_ctx *ctx;\n\tconst char *enc_name;\n\tint err;\n\n\talgt = crypto_get_attr_type(tb);\n\tif (IS_ERR(algt))\n\t\treturn ERR_CAST(algt);\n\n\tif ((algt->type ^ CRYPTO_ALG_TYPE_AEAD) & algt->mask)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tauth = ahash_attr_alg(tb[1], CRYPTO_ALG_TYPE_HASH,\n\t\t\t       CRYPTO_ALG_TYPE_AHASH_MASK);\n\tif (IS_ERR(auth))\n\t\treturn ERR_CAST(auth);\n\n\tauth_base = &auth->base;\n\n\tenc_name = crypto_attr_alg_name(tb[2]);\n\terr = PTR_ERR(enc_name);\n\tif (IS_ERR(enc_name))\n\t\tgoto out_put_auth;\n\n\tinst = kzalloc(sizeof(*inst) + sizeof(*ctx), GFP_KERNEL);\n\terr = -ENOMEM;\n\tif (!inst)\n\t\tgoto out_put_auth;\n\n\tctx = crypto_instance_ctx(inst);\n\n\terr = crypto_init_ahash_spawn(&ctx->auth, auth, inst);\n\tif (err)\n\t\tgoto err_free_inst;\n\n\tcrypto_set_skcipher_spawn(&ctx->enc, inst);\n\terr = crypto_grab_skcipher(&ctx->enc, enc_name, 0,\n\t\t\t\t   crypto_requires_sync(algt->type,\n\t\t\t\t\t\t\talgt->mask));\n\tif (err)\n\t\tgoto err_drop_auth;\n\n\tenc = crypto_skcipher_spawn_alg(&ctx->enc);\n\n\terr = -ENAMETOOLONG;\n\tif (snprintf(inst->alg.cra_name, CRYPTO_MAX_ALG_NAME,\n\t\t     \"authenc(%s,%s)\", auth_base->cra_name, enc->cra_name) >=\n\t    CRYPTO_MAX_ALG_NAME)\n\t\tgoto err_drop_enc;\n\n\tif (snprintf(inst->alg.cra_driver_name, CRYPTO_MAX_ALG_NAME,\n\t\t     \"authenc(%s,%s)\", auth_base->cra_driver_name,\n\t\t     enc->cra_driver_name) >= CRYPTO_MAX_ALG_NAME)\n\t\tgoto err_drop_enc;\n\n\tinst->alg.cra_flags = CRYPTO_ALG_TYPE_AEAD;\n\tinst->alg.cra_flags |= enc->cra_flags & CRYPTO_ALG_ASYNC;\n\tinst->alg.cra_priority = enc->cra_priority *\n\t\t\t\t 10 + auth_base->cra_priority;\n\tinst->alg.cra_blocksize = enc->cra_blocksize;\n\tinst->alg.cra_alignmask = auth_base->cra_alignmask | enc->cra_alignmask;\n\tinst->alg.cra_type = &crypto_aead_type;\n\n\tinst->alg.cra_aead.ivsize = enc->cra_ablkcipher.ivsize;\n\tinst->alg.cra_aead.maxauthsize = auth->digestsize;\n\n\tinst->alg.cra_ctxsize = sizeof(struct crypto_authenc_ctx);\n\n\tinst->alg.cra_init = crypto_authenc_init_tfm;\n\tinst->alg.cra_exit = crypto_authenc_exit_tfm;\n\n\tinst->alg.cra_aead.setkey = crypto_authenc_setkey;\n\tinst->alg.cra_aead.encrypt = crypto_authenc_encrypt;\n\tinst->alg.cra_aead.decrypt = crypto_authenc_decrypt;\n\tinst->alg.cra_aead.givencrypt = crypto_authenc_givencrypt;\n\nout:\n\tcrypto_mod_put(auth_base);\n\treturn inst;\n\nerr_drop_enc:\n\tcrypto_drop_skcipher(&ctx->enc);\nerr_drop_auth:\n\tcrypto_drop_ahash(&ctx->auth);\nerr_free_inst:\n\tkfree(inst);\nout_put_auth:\n\tinst = ERR_PTR(err);\n\tgoto out;\n}\n\nstatic void crypto_authenc_free(struct crypto_instance *inst)\n{\n\tstruct authenc_instance_ctx *ctx = crypto_instance_ctx(inst);\n\n\tcrypto_drop_skcipher(&ctx->enc);\n\tcrypto_drop_ahash(&ctx->auth);\n\tkfree(inst);\n}\n\nstatic struct crypto_template crypto_authenc_tmpl = {\n\t.name = \"authenc\",\n\t.alloc = crypto_authenc_alloc,\n\t.free = crypto_authenc_free,\n\t.module = THIS_MODULE,\n};\n\nstatic int __init crypto_authenc_module_init(void)\n{\n\treturn crypto_register_template(&crypto_authenc_tmpl);\n}\n\nstatic void __exit crypto_authenc_module_exit(void)\n{\n\tcrypto_unregister_template(&crypto_authenc_tmpl);\n}\n\nmodule_init(crypto_authenc_module_init);\nmodule_exit(crypto_authenc_module_exit);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"Simple AEAD wrapper for IPsec\");\nMODULE_ALIAS_CRYPTO(\"authenc\");\n", "/*\n * authencesn.c - AEAD wrapper for IPsec with extended sequence numbers,\n *                 derived from authenc.c\n *\n * Copyright (C) 2010 secunet Security Networks AG\n * Copyright (C) 2010 Steffen Klassert <steffen.klassert@secunet.com>\n *\n * This program is free software; you can redistribute it and/or modify it\n * under the terms of the GNU General Public License as published by the Free\n * Software Foundation; either version 2 of the License, or (at your option)\n * any later version.\n *\n */\n\n#include <crypto/aead.h>\n#include <crypto/internal/hash.h>\n#include <crypto/internal/skcipher.h>\n#include <crypto/authenc.h>\n#include <crypto/scatterwalk.h>\n#include <linux/err.h>\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/rtnetlink.h>\n#include <linux/slab.h>\n#include <linux/spinlock.h>\n\nstruct authenc_esn_instance_ctx {\n\tstruct crypto_ahash_spawn auth;\n\tstruct crypto_skcipher_spawn enc;\n};\n\nstruct crypto_authenc_esn_ctx {\n\tunsigned int reqoff;\n\tstruct crypto_ahash *auth;\n\tstruct crypto_ablkcipher *enc;\n};\n\nstruct authenc_esn_request_ctx {\n\tunsigned int cryptlen;\n\tunsigned int headlen;\n\tunsigned int trailen;\n\tstruct scatterlist *sg;\n\tstruct scatterlist hsg[2];\n\tstruct scatterlist tsg[1];\n\tstruct scatterlist cipher[2];\n\tcrypto_completion_t complete;\n\tcrypto_completion_t update_complete;\n\tcrypto_completion_t update_complete2;\n\tchar tail[];\n};\n\nstatic void authenc_esn_request_complete(struct aead_request *req, int err)\n{\n\tif (err != -EINPROGRESS)\n\t\taead_request_complete(req, err);\n}\n\nstatic int crypto_authenc_esn_setkey(struct crypto_aead *authenc_esn, const u8 *key,\n\t\t\t\t     unsigned int keylen)\n{\n\tstruct crypto_authenc_esn_ctx *ctx = crypto_aead_ctx(authenc_esn);\n\tstruct crypto_ahash *auth = ctx->auth;\n\tstruct crypto_ablkcipher *enc = ctx->enc;\n\tstruct crypto_authenc_keys keys;\n\tint err = -EINVAL;\n\n\tif (crypto_authenc_extractkeys(&keys, key, keylen) != 0)\n\t\tgoto badkey;\n\n\tcrypto_ahash_clear_flags(auth, CRYPTO_TFM_REQ_MASK);\n\tcrypto_ahash_set_flags(auth, crypto_aead_get_flags(authenc_esn) &\n\t\t\t\t     CRYPTO_TFM_REQ_MASK);\n\terr = crypto_ahash_setkey(auth, keys.authkey, keys.authkeylen);\n\tcrypto_aead_set_flags(authenc_esn, crypto_ahash_get_flags(auth) &\n\t\t\t\t\t   CRYPTO_TFM_RES_MASK);\n\n\tif (err)\n\t\tgoto out;\n\n\tcrypto_ablkcipher_clear_flags(enc, CRYPTO_TFM_REQ_MASK);\n\tcrypto_ablkcipher_set_flags(enc, crypto_aead_get_flags(authenc_esn) &\n\t\t\t\t\t CRYPTO_TFM_REQ_MASK);\n\terr = crypto_ablkcipher_setkey(enc, keys.enckey, keys.enckeylen);\n\tcrypto_aead_set_flags(authenc_esn, crypto_ablkcipher_get_flags(enc) &\n\t\t\t\t\t   CRYPTO_TFM_RES_MASK);\n\nout:\n\treturn err;\n\nbadkey:\n\tcrypto_aead_set_flags(authenc_esn, CRYPTO_TFM_RES_BAD_KEY_LEN);\n\tgoto out;\n}\n\nstatic void authenc_esn_geniv_ahash_update_done(struct crypto_async_request *areq,\n\t\t\t\t\t\tint err)\n{\n\tstruct aead_request *req = areq->data;\n\tstruct crypto_aead *authenc_esn = crypto_aead_reqtfm(req);\n\tstruct crypto_authenc_esn_ctx *ctx = crypto_aead_ctx(authenc_esn);\n\tstruct authenc_esn_request_ctx *areq_ctx = aead_request_ctx(req);\n\tstruct ahash_request *ahreq = (void *)(areq_ctx->tail + ctx->reqoff);\n\n\tif (err)\n\t\tgoto out;\n\n\tahash_request_set_crypt(ahreq, areq_ctx->sg, ahreq->result,\n\t\t\t\tareq_ctx->cryptlen);\n\tahash_request_set_callback(ahreq, aead_request_flags(req) &\n\t\t\t\t\t  CRYPTO_TFM_REQ_MAY_SLEEP,\n\t\t\t\t   areq_ctx->update_complete2, req);\n\n\terr = crypto_ahash_update(ahreq);\n\tif (err)\n\t\tgoto out;\n\n\tahash_request_set_crypt(ahreq, areq_ctx->tsg, ahreq->result,\n\t\t\t\tareq_ctx->trailen);\n\tahash_request_set_callback(ahreq, aead_request_flags(req) &\n\t\t\t\t\t  CRYPTO_TFM_REQ_MAY_SLEEP,\n\t\t\t\t   areq_ctx->complete, req);\n\n\terr = crypto_ahash_finup(ahreq);\n\tif (err)\n\t\tgoto out;\n\n\tscatterwalk_map_and_copy(ahreq->result, areq_ctx->sg,\n\t\t\t\t areq_ctx->cryptlen,\n\t\t\t\t crypto_aead_authsize(authenc_esn), 1);\n\nout:\n\tauthenc_esn_request_complete(req, err);\n}\n\nstatic void authenc_esn_geniv_ahash_update_done2(struct crypto_async_request *areq,\n\t\t\t\t\t\t int err)\n{\n\tstruct aead_request *req = areq->data;\n\tstruct crypto_aead *authenc_esn = crypto_aead_reqtfm(req);\n\tstruct crypto_authenc_esn_ctx *ctx = crypto_aead_ctx(authenc_esn);\n\tstruct authenc_esn_request_ctx *areq_ctx = aead_request_ctx(req);\n\tstruct ahash_request *ahreq = (void *)(areq_ctx->tail + ctx->reqoff);\n\n\tif (err)\n\t\tgoto out;\n\n\tahash_request_set_crypt(ahreq, areq_ctx->tsg, ahreq->result,\n\t\t\t\tareq_ctx->trailen);\n\tahash_request_set_callback(ahreq, aead_request_flags(req) &\n\t\t\t\t\t  CRYPTO_TFM_REQ_MAY_SLEEP,\n\t\t\t\t   areq_ctx->complete, req);\n\n\terr = crypto_ahash_finup(ahreq);\n\tif (err)\n\t\tgoto out;\n\n\tscatterwalk_map_and_copy(ahreq->result, areq_ctx->sg,\n\t\t\t\t areq_ctx->cryptlen,\n\t\t\t\t crypto_aead_authsize(authenc_esn), 1);\n\nout:\n\tauthenc_esn_request_complete(req, err);\n}\n\n\nstatic void authenc_esn_geniv_ahash_done(struct crypto_async_request *areq,\n\t\t\t\t\t int err)\n{\n\tstruct aead_request *req = areq->data;\n\tstruct crypto_aead *authenc_esn = crypto_aead_reqtfm(req);\n\tstruct crypto_authenc_esn_ctx *ctx = crypto_aead_ctx(authenc_esn);\n\tstruct authenc_esn_request_ctx *areq_ctx = aead_request_ctx(req);\n\tstruct ahash_request *ahreq = (void *)(areq_ctx->tail + ctx->reqoff);\n\n\tif (err)\n\t\tgoto out;\n\n\tscatterwalk_map_and_copy(ahreq->result, areq_ctx->sg,\n\t\t\t\t areq_ctx->cryptlen,\n\t\t\t\t crypto_aead_authsize(authenc_esn), 1);\n\nout:\n\taead_request_complete(req, err);\n}\n\n\nstatic void authenc_esn_verify_ahash_update_done(struct crypto_async_request *areq,\n\t\t\t\t\t\t int err)\n{\n\tu8 *ihash;\n\tunsigned int authsize;\n\tstruct ablkcipher_request *abreq;\n\tstruct aead_request *req = areq->data;\n\tstruct crypto_aead *authenc_esn = crypto_aead_reqtfm(req);\n\tstruct crypto_authenc_esn_ctx *ctx = crypto_aead_ctx(authenc_esn);\n\tstruct authenc_esn_request_ctx *areq_ctx = aead_request_ctx(req);\n\tstruct ahash_request *ahreq = (void *)(areq_ctx->tail + ctx->reqoff);\n\tunsigned int cryptlen = req->cryptlen;\n\n\tif (err)\n\t\tgoto out;\n\n\tahash_request_set_crypt(ahreq, areq_ctx->sg, ahreq->result,\n\t\t\t\tareq_ctx->cryptlen);\n\n\tahash_request_set_callback(ahreq,\n\t\t\t\t   aead_request_flags(req) &\n\t\t\t\t   CRYPTO_TFM_REQ_MAY_SLEEP,\n\t\t\t\t   areq_ctx->update_complete2, req);\n\n\terr = crypto_ahash_update(ahreq);\n\tif (err)\n\t\tgoto out;\n\n\tahash_request_set_crypt(ahreq, areq_ctx->tsg, ahreq->result,\n\t\t\t\tareq_ctx->trailen);\n\tahash_request_set_callback(ahreq, aead_request_flags(req) &\n\t\t\t\t\t  CRYPTO_TFM_REQ_MAY_SLEEP,\n\t\t\t\t   areq_ctx->complete, req);\n\n\terr = crypto_ahash_finup(ahreq);\n\tif (err)\n\t\tgoto out;\n\n\tauthsize = crypto_aead_authsize(authenc_esn);\n\tcryptlen -= authsize;\n\tihash = ahreq->result + authsize;\n\tscatterwalk_map_and_copy(ihash, areq_ctx->sg, areq_ctx->cryptlen,\n\t\t\t\t authsize, 0);\n\n\terr = crypto_memneq(ihash, ahreq->result, authsize) ? -EBADMSG : 0;\n\tif (err)\n\t\tgoto out;\n\n\tabreq = aead_request_ctx(req);\n\tablkcipher_request_set_tfm(abreq, ctx->enc);\n\tablkcipher_request_set_callback(abreq, aead_request_flags(req),\n\t\t\t\t\treq->base.complete, req->base.data);\n\tablkcipher_request_set_crypt(abreq, req->src, req->dst,\n\t\t\t\t     cryptlen, req->iv);\n\n\terr = crypto_ablkcipher_decrypt(abreq);\n\nout:\n\tauthenc_esn_request_complete(req, err);\n}\n\nstatic void authenc_esn_verify_ahash_update_done2(struct crypto_async_request *areq,\n\t\t\t\t\t\t  int err)\n{\n\tu8 *ihash;\n\tunsigned int authsize;\n\tstruct ablkcipher_request *abreq;\n\tstruct aead_request *req = areq->data;\n\tstruct crypto_aead *authenc_esn = crypto_aead_reqtfm(req);\n\tstruct crypto_authenc_esn_ctx *ctx = crypto_aead_ctx(authenc_esn);\n\tstruct authenc_esn_request_ctx *areq_ctx = aead_request_ctx(req);\n\tstruct ahash_request *ahreq = (void *)(areq_ctx->tail + ctx->reqoff);\n\tunsigned int cryptlen = req->cryptlen;\n\n\tif (err)\n\t\tgoto out;\n\n\tahash_request_set_crypt(ahreq, areq_ctx->tsg, ahreq->result,\n\t\t\t\tareq_ctx->trailen);\n\tahash_request_set_callback(ahreq, aead_request_flags(req) &\n\t\t\t\t\t  CRYPTO_TFM_REQ_MAY_SLEEP,\n\t\t\t\t   areq_ctx->complete, req);\n\n\terr = crypto_ahash_finup(ahreq);\n\tif (err)\n\t\tgoto out;\n\n\tauthsize = crypto_aead_authsize(authenc_esn);\n\tcryptlen -= authsize;\n\tihash = ahreq->result + authsize;\n\tscatterwalk_map_and_copy(ihash, areq_ctx->sg, areq_ctx->cryptlen,\n\t\t\t\t authsize, 0);\n\n\terr = crypto_memneq(ihash, ahreq->result, authsize) ? -EBADMSG : 0;\n\tif (err)\n\t\tgoto out;\n\n\tabreq = aead_request_ctx(req);\n\tablkcipher_request_set_tfm(abreq, ctx->enc);\n\tablkcipher_request_set_callback(abreq, aead_request_flags(req),\n\t\t\t\t\treq->base.complete, req->base.data);\n\tablkcipher_request_set_crypt(abreq, req->src, req->dst,\n\t\t\t\t     cryptlen, req->iv);\n\n\terr = crypto_ablkcipher_decrypt(abreq);\n\nout:\n\tauthenc_esn_request_complete(req, err);\n}\n\n\nstatic void authenc_esn_verify_ahash_done(struct crypto_async_request *areq,\n\t\t\t\t\t  int err)\n{\n\tu8 *ihash;\n\tunsigned int authsize;\n\tstruct ablkcipher_request *abreq;\n\tstruct aead_request *req = areq->data;\n\tstruct crypto_aead *authenc_esn = crypto_aead_reqtfm(req);\n\tstruct crypto_authenc_esn_ctx *ctx = crypto_aead_ctx(authenc_esn);\n\tstruct authenc_esn_request_ctx *areq_ctx = aead_request_ctx(req);\n\tstruct ahash_request *ahreq = (void *)(areq_ctx->tail + ctx->reqoff);\n\tunsigned int cryptlen = req->cryptlen;\n\n\tif (err)\n\t\tgoto out;\n\n\tauthsize = crypto_aead_authsize(authenc_esn);\n\tcryptlen -= authsize;\n\tihash = ahreq->result + authsize;\n\tscatterwalk_map_and_copy(ihash, areq_ctx->sg, areq_ctx->cryptlen,\n\t\t\t\t authsize, 0);\n\n\terr = crypto_memneq(ihash, ahreq->result, authsize) ? -EBADMSG : 0;\n\tif (err)\n\t\tgoto out;\n\n\tabreq = aead_request_ctx(req);\n\tablkcipher_request_set_tfm(abreq, ctx->enc);\n\tablkcipher_request_set_callback(abreq, aead_request_flags(req),\n\t\t\t\t\treq->base.complete, req->base.data);\n\tablkcipher_request_set_crypt(abreq, req->src, req->dst,\n\t\t\t\t     cryptlen, req->iv);\n\n\terr = crypto_ablkcipher_decrypt(abreq);\n\nout:\n\tauthenc_esn_request_complete(req, err);\n}\n\nstatic u8 *crypto_authenc_esn_ahash(struct aead_request *req,\n\t\t\t\t    unsigned int flags)\n{\n\tstruct crypto_aead *authenc_esn = crypto_aead_reqtfm(req);\n\tstruct crypto_authenc_esn_ctx *ctx = crypto_aead_ctx(authenc_esn);\n\tstruct crypto_ahash *auth = ctx->auth;\n\tstruct authenc_esn_request_ctx *areq_ctx = aead_request_ctx(req);\n\tstruct ahash_request *ahreq = (void *)(areq_ctx->tail + ctx->reqoff);\n\tu8 *hash = areq_ctx->tail;\n\tint err;\n\n\thash = (u8 *)ALIGN((unsigned long)hash + crypto_ahash_alignmask(auth),\n\t\t\t    crypto_ahash_alignmask(auth) + 1);\n\n\tahash_request_set_tfm(ahreq, auth);\n\n\terr = crypto_ahash_init(ahreq);\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\tahash_request_set_crypt(ahreq, areq_ctx->hsg, hash, areq_ctx->headlen);\n\tahash_request_set_callback(ahreq, aead_request_flags(req) & flags,\n\t\t\t\t   areq_ctx->update_complete, req);\n\n\terr = crypto_ahash_update(ahreq);\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\tahash_request_set_crypt(ahreq, areq_ctx->sg, hash, areq_ctx->cryptlen);\n\tahash_request_set_callback(ahreq, aead_request_flags(req) & flags,\n\t\t\t\t   areq_ctx->update_complete2, req);\n\n\terr = crypto_ahash_update(ahreq);\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\tahash_request_set_crypt(ahreq, areq_ctx->tsg, hash,\n\t\t\t\tareq_ctx->trailen);\n\tahash_request_set_callback(ahreq, aead_request_flags(req) & flags,\n\t\t\t\t   areq_ctx->complete, req);\n\n\terr = crypto_ahash_finup(ahreq);\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\treturn hash;\n}\n\nstatic int crypto_authenc_esn_genicv(struct aead_request *req, u8 *iv,\n\t\t\t\t     unsigned int flags)\n{\n\tstruct crypto_aead *authenc_esn = crypto_aead_reqtfm(req);\n\tstruct authenc_esn_request_ctx *areq_ctx = aead_request_ctx(req);\n\tstruct scatterlist *dst = req->dst;\n\tstruct scatterlist *assoc = req->assoc;\n\tstruct scatterlist *cipher = areq_ctx->cipher;\n\tstruct scatterlist *hsg = areq_ctx->hsg;\n\tstruct scatterlist *tsg = areq_ctx->tsg;\n\tstruct scatterlist *assoc1;\n\tstruct scatterlist *assoc2;\n\tunsigned int ivsize = crypto_aead_ivsize(authenc_esn);\n\tunsigned int cryptlen = req->cryptlen;\n\tstruct page *dstp;\n\tu8 *vdst;\n\tu8 *hash;\n\n\tdstp = sg_page(dst);\n\tvdst = PageHighMem(dstp) ? NULL : page_address(dstp) + dst->offset;\n\n\tif (ivsize) {\n\t\tsg_init_table(cipher, 2);\n\t\tsg_set_buf(cipher, iv, ivsize);\n\t\tscatterwalk_crypto_chain(cipher, dst, vdst == iv + ivsize, 2);\n\t\tdst = cipher;\n\t\tcryptlen += ivsize;\n\t}\n\n\tif (sg_is_last(assoc))\n\t\treturn -EINVAL;\n\n\tassoc1 = assoc + 1;\n\tif (sg_is_last(assoc1))\n\t\treturn -EINVAL;\n\n\tassoc2 = assoc + 2;\n\tif (!sg_is_last(assoc2))\n\t\treturn -EINVAL;\n\n\tsg_init_table(hsg, 2);\n\tsg_set_page(hsg, sg_page(assoc), assoc->length, assoc->offset);\n\tsg_set_page(hsg + 1, sg_page(assoc2), assoc2->length, assoc2->offset);\n\n\tsg_init_table(tsg, 1);\n\tsg_set_page(tsg, sg_page(assoc1), assoc1->length, assoc1->offset);\n\n\tareq_ctx->cryptlen = cryptlen;\n\tareq_ctx->headlen = assoc->length + assoc2->length;\n\tareq_ctx->trailen = assoc1->length;\n\tareq_ctx->sg = dst;\n\n\tareq_ctx->complete = authenc_esn_geniv_ahash_done;\n\tareq_ctx->update_complete = authenc_esn_geniv_ahash_update_done;\n\tareq_ctx->update_complete2 = authenc_esn_geniv_ahash_update_done2;\n\n\thash = crypto_authenc_esn_ahash(req, flags);\n\tif (IS_ERR(hash))\n\t\treturn PTR_ERR(hash);\n\n\tscatterwalk_map_and_copy(hash, dst, cryptlen,\n\t\t\t\t crypto_aead_authsize(authenc_esn), 1);\n\treturn 0;\n}\n\n\nstatic void crypto_authenc_esn_encrypt_done(struct crypto_async_request *req,\n\t\t\t\t\t    int err)\n{\n\tstruct aead_request *areq = req->data;\n\n\tif (!err) {\n\t\tstruct crypto_aead *authenc_esn = crypto_aead_reqtfm(areq);\n\t\tstruct crypto_authenc_esn_ctx *ctx = crypto_aead_ctx(authenc_esn);\n\t\tstruct ablkcipher_request *abreq = aead_request_ctx(areq);\n\t\tu8 *iv = (u8 *)(abreq + 1) +\n\t\t\t crypto_ablkcipher_reqsize(ctx->enc);\n\n\t\terr = crypto_authenc_esn_genicv(areq, iv, 0);\n\t}\n\n\tauthenc_esn_request_complete(areq, err);\n}\n\nstatic int crypto_authenc_esn_encrypt(struct aead_request *req)\n{\n\tstruct crypto_aead *authenc_esn = crypto_aead_reqtfm(req);\n\tstruct crypto_authenc_esn_ctx *ctx = crypto_aead_ctx(authenc_esn);\n\tstruct authenc_esn_request_ctx *areq_ctx = aead_request_ctx(req);\n\tstruct crypto_ablkcipher *enc = ctx->enc;\n\tstruct scatterlist *dst = req->dst;\n\tunsigned int cryptlen = req->cryptlen;\n\tstruct ablkcipher_request *abreq = (void *)(areq_ctx->tail\n\t\t\t\t\t\t    + ctx->reqoff);\n\tu8 *iv = (u8 *)abreq - crypto_ablkcipher_ivsize(enc);\n\tint err;\n\n\tablkcipher_request_set_tfm(abreq, enc);\n\tablkcipher_request_set_callback(abreq, aead_request_flags(req),\n\t\t\t\t\tcrypto_authenc_esn_encrypt_done, req);\n\tablkcipher_request_set_crypt(abreq, req->src, dst, cryptlen, req->iv);\n\n\tmemcpy(iv, req->iv, crypto_aead_ivsize(authenc_esn));\n\n\terr = crypto_ablkcipher_encrypt(abreq);\n\tif (err)\n\t\treturn err;\n\n\treturn crypto_authenc_esn_genicv(req, iv, CRYPTO_TFM_REQ_MAY_SLEEP);\n}\n\nstatic void crypto_authenc_esn_givencrypt_done(struct crypto_async_request *req,\n\t\t\t\t\t       int err)\n{\n\tstruct aead_request *areq = req->data;\n\n\tif (!err) {\n\t\tstruct skcipher_givcrypt_request *greq = aead_request_ctx(areq);\n\n\t\terr = crypto_authenc_esn_genicv(areq, greq->giv, 0);\n\t}\n\n\tauthenc_esn_request_complete(areq, err);\n}\n\nstatic int crypto_authenc_esn_givencrypt(struct aead_givcrypt_request *req)\n{\n\tstruct crypto_aead *authenc_esn = aead_givcrypt_reqtfm(req);\n\tstruct crypto_authenc_esn_ctx *ctx = crypto_aead_ctx(authenc_esn);\n\tstruct aead_request *areq = &req->areq;\n\tstruct skcipher_givcrypt_request *greq = aead_request_ctx(areq);\n\tu8 *iv = req->giv;\n\tint err;\n\n\tskcipher_givcrypt_set_tfm(greq, ctx->enc);\n\tskcipher_givcrypt_set_callback(greq, aead_request_flags(areq),\n\t\t\t\t       crypto_authenc_esn_givencrypt_done, areq);\n\tskcipher_givcrypt_set_crypt(greq, areq->src, areq->dst, areq->cryptlen,\n\t\t\t\t    areq->iv);\n\tskcipher_givcrypt_set_giv(greq, iv, req->seq);\n\n\terr = crypto_skcipher_givencrypt(greq);\n\tif (err)\n\t\treturn err;\n\n\treturn crypto_authenc_esn_genicv(areq, iv, CRYPTO_TFM_REQ_MAY_SLEEP);\n}\n\nstatic int crypto_authenc_esn_verify(struct aead_request *req)\n{\n\tstruct crypto_aead *authenc_esn = crypto_aead_reqtfm(req);\n\tstruct authenc_esn_request_ctx *areq_ctx = aead_request_ctx(req);\n\tu8 *ohash;\n\tu8 *ihash;\n\tunsigned int authsize;\n\n\tareq_ctx->complete = authenc_esn_verify_ahash_done;\n\tareq_ctx->update_complete = authenc_esn_verify_ahash_update_done;\n\n\tohash = crypto_authenc_esn_ahash(req, CRYPTO_TFM_REQ_MAY_SLEEP);\n\tif (IS_ERR(ohash))\n\t\treturn PTR_ERR(ohash);\n\n\tauthsize = crypto_aead_authsize(authenc_esn);\n\tihash = ohash + authsize;\n\tscatterwalk_map_and_copy(ihash, areq_ctx->sg, areq_ctx->cryptlen,\n\t\t\t\t authsize, 0);\n\treturn crypto_memneq(ihash, ohash, authsize) ? -EBADMSG : 0;\n}\n\nstatic int crypto_authenc_esn_iverify(struct aead_request *req, u8 *iv,\n\t\t\t\t      unsigned int cryptlen)\n{\n\tstruct crypto_aead *authenc_esn = crypto_aead_reqtfm(req);\n\tstruct authenc_esn_request_ctx *areq_ctx = aead_request_ctx(req);\n\tstruct scatterlist *src = req->src;\n\tstruct scatterlist *assoc = req->assoc;\n\tstruct scatterlist *cipher = areq_ctx->cipher;\n\tstruct scatterlist *hsg = areq_ctx->hsg;\n\tstruct scatterlist *tsg = areq_ctx->tsg;\n\tstruct scatterlist *assoc1;\n\tstruct scatterlist *assoc2;\n\tunsigned int ivsize = crypto_aead_ivsize(authenc_esn);\n\tstruct page *srcp;\n\tu8 *vsrc;\n\n\tsrcp = sg_page(src);\n\tvsrc = PageHighMem(srcp) ? NULL : page_address(srcp) + src->offset;\n\n\tif (ivsize) {\n\t\tsg_init_table(cipher, 2);\n\t\tsg_set_buf(cipher, iv, ivsize);\n\t\tscatterwalk_crypto_chain(cipher, src, vsrc == iv + ivsize, 2);\n\t\tsrc = cipher;\n\t\tcryptlen += ivsize;\n\t}\n\n\tif (sg_is_last(assoc))\n\t\treturn -EINVAL;\n\n\tassoc1 = assoc + 1;\n\tif (sg_is_last(assoc1))\n\t\treturn -EINVAL;\n\n\tassoc2 = assoc + 2;\n\tif (!sg_is_last(assoc2))\n\t\treturn -EINVAL;\n\n\tsg_init_table(hsg, 2);\n\tsg_set_page(hsg, sg_page(assoc), assoc->length, assoc->offset);\n\tsg_set_page(hsg + 1, sg_page(assoc2), assoc2->length, assoc2->offset);\n\n\tsg_init_table(tsg, 1);\n\tsg_set_page(tsg, sg_page(assoc1), assoc1->length, assoc1->offset);\n\n\tareq_ctx->cryptlen = cryptlen;\n\tareq_ctx->headlen = assoc->length + assoc2->length;\n\tareq_ctx->trailen = assoc1->length;\n\tareq_ctx->sg = src;\n\n\tareq_ctx->complete = authenc_esn_verify_ahash_done;\n\tareq_ctx->update_complete = authenc_esn_verify_ahash_update_done;\n\tareq_ctx->update_complete2 = authenc_esn_verify_ahash_update_done2;\n\n\treturn crypto_authenc_esn_verify(req);\n}\n\nstatic int crypto_authenc_esn_decrypt(struct aead_request *req)\n{\n\tstruct crypto_aead *authenc_esn = crypto_aead_reqtfm(req);\n\tstruct crypto_authenc_esn_ctx *ctx = crypto_aead_ctx(authenc_esn);\n\tstruct ablkcipher_request *abreq = aead_request_ctx(req);\n\tunsigned int cryptlen = req->cryptlen;\n\tunsigned int authsize = crypto_aead_authsize(authenc_esn);\n\tu8 *iv = req->iv;\n\tint err;\n\n\tif (cryptlen < authsize)\n\t\treturn -EINVAL;\n\tcryptlen -= authsize;\n\n\terr = crypto_authenc_esn_iverify(req, iv, cryptlen);\n\tif (err)\n\t\treturn err;\n\n\tablkcipher_request_set_tfm(abreq, ctx->enc);\n\tablkcipher_request_set_callback(abreq, aead_request_flags(req),\n\t\t\t\t\treq->base.complete, req->base.data);\n\tablkcipher_request_set_crypt(abreq, req->src, req->dst, cryptlen, iv);\n\n\treturn crypto_ablkcipher_decrypt(abreq);\n}\n\nstatic int crypto_authenc_esn_init_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_instance *inst = crypto_tfm_alg_instance(tfm);\n\tstruct authenc_esn_instance_ctx *ictx = crypto_instance_ctx(inst);\n\tstruct crypto_authenc_esn_ctx *ctx = crypto_tfm_ctx(tfm);\n\tstruct crypto_ahash *auth;\n\tstruct crypto_ablkcipher *enc;\n\tint err;\n\n\tauth = crypto_spawn_ahash(&ictx->auth);\n\tif (IS_ERR(auth))\n\t\treturn PTR_ERR(auth);\n\n\tenc = crypto_spawn_skcipher(&ictx->enc);\n\terr = PTR_ERR(enc);\n\tif (IS_ERR(enc))\n\t\tgoto err_free_ahash;\n\n\tctx->auth = auth;\n\tctx->enc = enc;\n\n\tctx->reqoff = ALIGN(2 * crypto_ahash_digestsize(auth) +\n\t\t\t    crypto_ahash_alignmask(auth),\n\t\t\t    crypto_ahash_alignmask(auth) + 1) +\n\t\t      crypto_ablkcipher_ivsize(enc);\n\n\ttfm->crt_aead.reqsize = sizeof(struct authenc_esn_request_ctx) +\n\t\t\t\tctx->reqoff +\n\t\t\t\tmax_t(unsigned int,\n\t\t\t\tcrypto_ahash_reqsize(auth) +\n\t\t\t\tsizeof(struct ahash_request),\n\t\t\t\tsizeof(struct skcipher_givcrypt_request) +\n\t\t\t\tcrypto_ablkcipher_reqsize(enc));\n\n\treturn 0;\n\nerr_free_ahash:\n\tcrypto_free_ahash(auth);\n\treturn err;\n}\n\nstatic void crypto_authenc_esn_exit_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_authenc_esn_ctx *ctx = crypto_tfm_ctx(tfm);\n\n\tcrypto_free_ahash(ctx->auth);\n\tcrypto_free_ablkcipher(ctx->enc);\n}\n\nstatic struct crypto_instance *crypto_authenc_esn_alloc(struct rtattr **tb)\n{\n\tstruct crypto_attr_type *algt;\n\tstruct crypto_instance *inst;\n\tstruct hash_alg_common *auth;\n\tstruct crypto_alg *auth_base;\n\tstruct crypto_alg *enc;\n\tstruct authenc_esn_instance_ctx *ctx;\n\tconst char *enc_name;\n\tint err;\n\n\talgt = crypto_get_attr_type(tb);\n\tif (IS_ERR(algt))\n\t\treturn ERR_CAST(algt);\n\n\tif ((algt->type ^ CRYPTO_ALG_TYPE_AEAD) & algt->mask)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tauth = ahash_attr_alg(tb[1], CRYPTO_ALG_TYPE_HASH,\n\t\t\t       CRYPTO_ALG_TYPE_AHASH_MASK);\n\tif (IS_ERR(auth))\n\t\treturn ERR_CAST(auth);\n\n\tauth_base = &auth->base;\n\n\tenc_name = crypto_attr_alg_name(tb[2]);\n\terr = PTR_ERR(enc_name);\n\tif (IS_ERR(enc_name))\n\t\tgoto out_put_auth;\n\n\tinst = kzalloc(sizeof(*inst) + sizeof(*ctx), GFP_KERNEL);\n\terr = -ENOMEM;\n\tif (!inst)\n\t\tgoto out_put_auth;\n\n\tctx = crypto_instance_ctx(inst);\n\n\terr = crypto_init_ahash_spawn(&ctx->auth, auth, inst);\n\tif (err)\n\t\tgoto err_free_inst;\n\n\tcrypto_set_skcipher_spawn(&ctx->enc, inst);\n\terr = crypto_grab_skcipher(&ctx->enc, enc_name, 0,\n\t\t\t\t   crypto_requires_sync(algt->type,\n\t\t\t\t\t\t\talgt->mask));\n\tif (err)\n\t\tgoto err_drop_auth;\n\n\tenc = crypto_skcipher_spawn_alg(&ctx->enc);\n\n\terr = -ENAMETOOLONG;\n\tif (snprintf(inst->alg.cra_name, CRYPTO_MAX_ALG_NAME,\n\t\t     \"authencesn(%s,%s)\", auth_base->cra_name, enc->cra_name) >=\n\t    CRYPTO_MAX_ALG_NAME)\n\t\tgoto err_drop_enc;\n\n\tif (snprintf(inst->alg.cra_driver_name, CRYPTO_MAX_ALG_NAME,\n\t\t     \"authencesn(%s,%s)\", auth_base->cra_driver_name,\n\t\t     enc->cra_driver_name) >= CRYPTO_MAX_ALG_NAME)\n\t\tgoto err_drop_enc;\n\n\tinst->alg.cra_flags = CRYPTO_ALG_TYPE_AEAD;\n\tinst->alg.cra_flags |= enc->cra_flags & CRYPTO_ALG_ASYNC;\n\tinst->alg.cra_priority = enc->cra_priority *\n\t\t\t\t 10 + auth_base->cra_priority;\n\tinst->alg.cra_blocksize = enc->cra_blocksize;\n\tinst->alg.cra_alignmask = auth_base->cra_alignmask | enc->cra_alignmask;\n\tinst->alg.cra_type = &crypto_aead_type;\n\n\tinst->alg.cra_aead.ivsize = enc->cra_ablkcipher.ivsize;\n\tinst->alg.cra_aead.maxauthsize = auth->digestsize;\n\n\tinst->alg.cra_ctxsize = sizeof(struct crypto_authenc_esn_ctx);\n\n\tinst->alg.cra_init = crypto_authenc_esn_init_tfm;\n\tinst->alg.cra_exit = crypto_authenc_esn_exit_tfm;\n\n\tinst->alg.cra_aead.setkey = crypto_authenc_esn_setkey;\n\tinst->alg.cra_aead.encrypt = crypto_authenc_esn_encrypt;\n\tinst->alg.cra_aead.decrypt = crypto_authenc_esn_decrypt;\n\tinst->alg.cra_aead.givencrypt = crypto_authenc_esn_givencrypt;\n\nout:\n\tcrypto_mod_put(auth_base);\n\treturn inst;\n\nerr_drop_enc:\n\tcrypto_drop_skcipher(&ctx->enc);\nerr_drop_auth:\n\tcrypto_drop_ahash(&ctx->auth);\nerr_free_inst:\n\tkfree(inst);\nout_put_auth:\n\tinst = ERR_PTR(err);\n\tgoto out;\n}\n\nstatic void crypto_authenc_esn_free(struct crypto_instance *inst)\n{\n\tstruct authenc_esn_instance_ctx *ctx = crypto_instance_ctx(inst);\n\n\tcrypto_drop_skcipher(&ctx->enc);\n\tcrypto_drop_ahash(&ctx->auth);\n\tkfree(inst);\n}\n\nstatic struct crypto_template crypto_authenc_esn_tmpl = {\n\t.name = \"authencesn\",\n\t.alloc = crypto_authenc_esn_alloc,\n\t.free = crypto_authenc_esn_free,\n\t.module = THIS_MODULE,\n};\n\nstatic int __init crypto_authenc_esn_module_init(void)\n{\n\treturn crypto_register_template(&crypto_authenc_esn_tmpl);\n}\n\nstatic void __exit crypto_authenc_esn_module_exit(void)\n{\n\tcrypto_unregister_template(&crypto_authenc_esn_tmpl);\n}\n\nmodule_init(crypto_authenc_esn_module_init);\nmodule_exit(crypto_authenc_esn_module_exit);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_AUTHOR(\"Steffen Klassert <steffen.klassert@secunet.com>\");\nMODULE_DESCRIPTION(\"AEAD wrapper for IPsec with extended sequence numbers\");\nMODULE_ALIAS_CRYPTO(\"authencesn\");\n", "/*\n * CBC: Cipher Block Chaining mode\n *\n * Copyright (c) 2006 Herbert Xu <herbert@gondor.apana.org.au>\n *\n * This program is free software; you can redistribute it and/or modify it\n * under the terms of the GNU General Public License as published by the Free\n * Software Foundation; either version 2 of the License, or (at your option)\n * any later version.\n *\n */\n\n#include <crypto/algapi.h>\n#include <linux/err.h>\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/log2.h>\n#include <linux/module.h>\n#include <linux/scatterlist.h>\n#include <linux/slab.h>\n\nstruct crypto_cbc_ctx {\n\tstruct crypto_cipher *child;\n};\n\nstatic int crypto_cbc_setkey(struct crypto_tfm *parent, const u8 *key,\n\t\t\t     unsigned int keylen)\n{\n\tstruct crypto_cbc_ctx *ctx = crypto_tfm_ctx(parent);\n\tstruct crypto_cipher *child = ctx->child;\n\tint err;\n\n\tcrypto_cipher_clear_flags(child, CRYPTO_TFM_REQ_MASK);\n\tcrypto_cipher_set_flags(child, crypto_tfm_get_flags(parent) &\n\t\t\t\t       CRYPTO_TFM_REQ_MASK);\n\terr = crypto_cipher_setkey(child, key, keylen);\n\tcrypto_tfm_set_flags(parent, crypto_cipher_get_flags(child) &\n\t\t\t\t     CRYPTO_TFM_RES_MASK);\n\treturn err;\n}\n\nstatic int crypto_cbc_encrypt_segment(struct blkcipher_desc *desc,\n\t\t\t\t      struct blkcipher_walk *walk,\n\t\t\t\t      struct crypto_cipher *tfm)\n{\n\tvoid (*fn)(struct crypto_tfm *, u8 *, const u8 *) =\n\t\tcrypto_cipher_alg(tfm)->cia_encrypt;\n\tint bsize = crypto_cipher_blocksize(tfm);\n\tunsigned int nbytes = walk->nbytes;\n\tu8 *src = walk->src.virt.addr;\n\tu8 *dst = walk->dst.virt.addr;\n\tu8 *iv = walk->iv;\n\n\tdo {\n\t\tcrypto_xor(iv, src, bsize);\n\t\tfn(crypto_cipher_tfm(tfm), dst, iv);\n\t\tmemcpy(iv, dst, bsize);\n\n\t\tsrc += bsize;\n\t\tdst += bsize;\n\t} while ((nbytes -= bsize) >= bsize);\n\n\treturn nbytes;\n}\n\nstatic int crypto_cbc_encrypt_inplace(struct blkcipher_desc *desc,\n\t\t\t\t      struct blkcipher_walk *walk,\n\t\t\t\t      struct crypto_cipher *tfm)\n{\n\tvoid (*fn)(struct crypto_tfm *, u8 *, const u8 *) =\n\t\tcrypto_cipher_alg(tfm)->cia_encrypt;\n\tint bsize = crypto_cipher_blocksize(tfm);\n\tunsigned int nbytes = walk->nbytes;\n\tu8 *src = walk->src.virt.addr;\n\tu8 *iv = walk->iv;\n\n\tdo {\n\t\tcrypto_xor(src, iv, bsize);\n\t\tfn(crypto_cipher_tfm(tfm), src, src);\n\t\tiv = src;\n\n\t\tsrc += bsize;\n\t} while ((nbytes -= bsize) >= bsize);\n\n\tmemcpy(walk->iv, iv, bsize);\n\n\treturn nbytes;\n}\n\nstatic int crypto_cbc_encrypt(struct blkcipher_desc *desc,\n\t\t\t      struct scatterlist *dst, struct scatterlist *src,\n\t\t\t      unsigned int nbytes)\n{\n\tstruct blkcipher_walk walk;\n\tstruct crypto_blkcipher *tfm = desc->tfm;\n\tstruct crypto_cbc_ctx *ctx = crypto_blkcipher_ctx(tfm);\n\tstruct crypto_cipher *child = ctx->child;\n\tint err;\n\n\tblkcipher_walk_init(&walk, dst, src, nbytes);\n\terr = blkcipher_walk_virt(desc, &walk);\n\n\twhile ((nbytes = walk.nbytes)) {\n\t\tif (walk.src.virt.addr == walk.dst.virt.addr)\n\t\t\tnbytes = crypto_cbc_encrypt_inplace(desc, &walk, child);\n\t\telse\n\t\t\tnbytes = crypto_cbc_encrypt_segment(desc, &walk, child);\n\t\terr = blkcipher_walk_done(desc, &walk, nbytes);\n\t}\n\n\treturn err;\n}\n\nstatic int crypto_cbc_decrypt_segment(struct blkcipher_desc *desc,\n\t\t\t\t      struct blkcipher_walk *walk,\n\t\t\t\t      struct crypto_cipher *tfm)\n{\n\tvoid (*fn)(struct crypto_tfm *, u8 *, const u8 *) =\n\t\tcrypto_cipher_alg(tfm)->cia_decrypt;\n\tint bsize = crypto_cipher_blocksize(tfm);\n\tunsigned int nbytes = walk->nbytes;\n\tu8 *src = walk->src.virt.addr;\n\tu8 *dst = walk->dst.virt.addr;\n\tu8 *iv = walk->iv;\n\n\tdo {\n\t\tfn(crypto_cipher_tfm(tfm), dst, src);\n\t\tcrypto_xor(dst, iv, bsize);\n\t\tiv = src;\n\n\t\tsrc += bsize;\n\t\tdst += bsize;\n\t} while ((nbytes -= bsize) >= bsize);\n\n\tmemcpy(walk->iv, iv, bsize);\n\n\treturn nbytes;\n}\n\nstatic int crypto_cbc_decrypt_inplace(struct blkcipher_desc *desc,\n\t\t\t\t      struct blkcipher_walk *walk,\n\t\t\t\t      struct crypto_cipher *tfm)\n{\n\tvoid (*fn)(struct crypto_tfm *, u8 *, const u8 *) =\n\t\tcrypto_cipher_alg(tfm)->cia_decrypt;\n\tint bsize = crypto_cipher_blocksize(tfm);\n\tunsigned int nbytes = walk->nbytes;\n\tu8 *src = walk->src.virt.addr;\n\tu8 last_iv[bsize];\n\n\t/* Start of the last block. */\n\tsrc += nbytes - (nbytes & (bsize - 1)) - bsize;\n\tmemcpy(last_iv, src, bsize);\n\n\tfor (;;) {\n\t\tfn(crypto_cipher_tfm(tfm), src, src);\n\t\tif ((nbytes -= bsize) < bsize)\n\t\t\tbreak;\n\t\tcrypto_xor(src, src - bsize, bsize);\n\t\tsrc -= bsize;\n\t}\n\n\tcrypto_xor(src, walk->iv, bsize);\n\tmemcpy(walk->iv, last_iv, bsize);\n\n\treturn nbytes;\n}\n\nstatic int crypto_cbc_decrypt(struct blkcipher_desc *desc,\n\t\t\t      struct scatterlist *dst, struct scatterlist *src,\n\t\t\t      unsigned int nbytes)\n{\n\tstruct blkcipher_walk walk;\n\tstruct crypto_blkcipher *tfm = desc->tfm;\n\tstruct crypto_cbc_ctx *ctx = crypto_blkcipher_ctx(tfm);\n\tstruct crypto_cipher *child = ctx->child;\n\tint err;\n\n\tblkcipher_walk_init(&walk, dst, src, nbytes);\n\terr = blkcipher_walk_virt(desc, &walk);\n\n\twhile ((nbytes = walk.nbytes)) {\n\t\tif (walk.src.virt.addr == walk.dst.virt.addr)\n\t\t\tnbytes = crypto_cbc_decrypt_inplace(desc, &walk, child);\n\t\telse\n\t\t\tnbytes = crypto_cbc_decrypt_segment(desc, &walk, child);\n\t\terr = blkcipher_walk_done(desc, &walk, nbytes);\n\t}\n\n\treturn err;\n}\n\nstatic int crypto_cbc_init_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_instance *inst = (void *)tfm->__crt_alg;\n\tstruct crypto_spawn *spawn = crypto_instance_ctx(inst);\n\tstruct crypto_cbc_ctx *ctx = crypto_tfm_ctx(tfm);\n\tstruct crypto_cipher *cipher;\n\n\tcipher = crypto_spawn_cipher(spawn);\n\tif (IS_ERR(cipher))\n\t\treturn PTR_ERR(cipher);\n\n\tctx->child = cipher;\n\treturn 0;\n}\n\nstatic void crypto_cbc_exit_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_cbc_ctx *ctx = crypto_tfm_ctx(tfm);\n\tcrypto_free_cipher(ctx->child);\n}\n\nstatic struct crypto_instance *crypto_cbc_alloc(struct rtattr **tb)\n{\n\tstruct crypto_instance *inst;\n\tstruct crypto_alg *alg;\n\tint err;\n\n\terr = crypto_check_attr_type(tb, CRYPTO_ALG_TYPE_BLKCIPHER);\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\talg = crypto_get_attr_alg(tb, CRYPTO_ALG_TYPE_CIPHER,\n\t\t\t\t  CRYPTO_ALG_TYPE_MASK);\n\tif (IS_ERR(alg))\n\t\treturn ERR_CAST(alg);\n\n\tinst = ERR_PTR(-EINVAL);\n\tif (!is_power_of_2(alg->cra_blocksize))\n\t\tgoto out_put_alg;\n\n\tinst = crypto_alloc_instance(\"cbc\", alg);\n\tif (IS_ERR(inst))\n\t\tgoto out_put_alg;\n\n\tinst->alg.cra_flags = CRYPTO_ALG_TYPE_BLKCIPHER;\n\tinst->alg.cra_priority = alg->cra_priority;\n\tinst->alg.cra_blocksize = alg->cra_blocksize;\n\tinst->alg.cra_alignmask = alg->cra_alignmask;\n\tinst->alg.cra_type = &crypto_blkcipher_type;\n\n\t/* We access the data as u32s when xoring. */\n\tinst->alg.cra_alignmask |= __alignof__(u32) - 1;\n\n\tinst->alg.cra_blkcipher.ivsize = alg->cra_blocksize;\n\tinst->alg.cra_blkcipher.min_keysize = alg->cra_cipher.cia_min_keysize;\n\tinst->alg.cra_blkcipher.max_keysize = alg->cra_cipher.cia_max_keysize;\n\n\tinst->alg.cra_ctxsize = sizeof(struct crypto_cbc_ctx);\n\n\tinst->alg.cra_init = crypto_cbc_init_tfm;\n\tinst->alg.cra_exit = crypto_cbc_exit_tfm;\n\n\tinst->alg.cra_blkcipher.setkey = crypto_cbc_setkey;\n\tinst->alg.cra_blkcipher.encrypt = crypto_cbc_encrypt;\n\tinst->alg.cra_blkcipher.decrypt = crypto_cbc_decrypt;\n\nout_put_alg:\n\tcrypto_mod_put(alg);\n\treturn inst;\n}\n\nstatic void crypto_cbc_free(struct crypto_instance *inst)\n{\n\tcrypto_drop_spawn(crypto_instance_ctx(inst));\n\tkfree(inst);\n}\n\nstatic struct crypto_template crypto_cbc_tmpl = {\n\t.name = \"cbc\",\n\t.alloc = crypto_cbc_alloc,\n\t.free = crypto_cbc_free,\n\t.module = THIS_MODULE,\n};\n\nstatic int __init crypto_cbc_module_init(void)\n{\n\treturn crypto_register_template(&crypto_cbc_tmpl);\n}\n\nstatic void __exit crypto_cbc_module_exit(void)\n{\n\tcrypto_unregister_template(&crypto_cbc_tmpl);\n}\n\nmodule_init(crypto_cbc_module_init);\nmodule_exit(crypto_cbc_module_exit);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"CBC block cipher algorithm\");\nMODULE_ALIAS_CRYPTO(\"cbc\");\n", "/*\n * CCM: Counter with CBC-MAC\n *\n * (C) Copyright IBM Corp. 2007 - Joy Latten <latten@us.ibm.com>\n *\n * This program is free software; you can redistribute it and/or modify it\n * under the terms of the GNU General Public License as published by the Free\n * Software Foundation; either version 2 of the License, or (at your option)\n * any later version.\n *\n */\n\n#include <crypto/internal/aead.h>\n#include <crypto/internal/skcipher.h>\n#include <crypto/scatterwalk.h>\n#include <linux/err.h>\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/slab.h>\n\n#include \"internal.h\"\n\nstruct ccm_instance_ctx {\n\tstruct crypto_skcipher_spawn ctr;\n\tstruct crypto_spawn cipher;\n};\n\nstruct crypto_ccm_ctx {\n\tstruct crypto_cipher *cipher;\n\tstruct crypto_ablkcipher *ctr;\n};\n\nstruct crypto_rfc4309_ctx {\n\tstruct crypto_aead *child;\n\tu8 nonce[3];\n};\n\nstruct crypto_ccm_req_priv_ctx {\n\tu8 odata[16];\n\tu8 idata[16];\n\tu8 auth_tag[16];\n\tu32 ilen;\n\tu32 flags;\n\tstruct scatterlist src[2];\n\tstruct scatterlist dst[2];\n\tstruct ablkcipher_request abreq;\n};\n\nstatic inline struct crypto_ccm_req_priv_ctx *crypto_ccm_reqctx(\n\tstruct aead_request *req)\n{\n\tunsigned long align = crypto_aead_alignmask(crypto_aead_reqtfm(req));\n\n\treturn (void *)PTR_ALIGN((u8 *)aead_request_ctx(req), align + 1);\n}\n\nstatic int set_msg_len(u8 *block, unsigned int msglen, int csize)\n{\n\t__be32 data;\n\n\tmemset(block, 0, csize);\n\tblock += csize;\n\n\tif (csize >= 4)\n\t\tcsize = 4;\n\telse if (msglen > (1 << (8 * csize)))\n\t\treturn -EOVERFLOW;\n\n\tdata = cpu_to_be32(msglen);\n\tmemcpy(block - csize, (u8 *)&data + 4 - csize, csize);\n\n\treturn 0;\n}\n\nstatic int crypto_ccm_setkey(struct crypto_aead *aead, const u8 *key,\n\t\t\t     unsigned int keylen)\n{\n\tstruct crypto_ccm_ctx *ctx = crypto_aead_ctx(aead);\n\tstruct crypto_ablkcipher *ctr = ctx->ctr;\n\tstruct crypto_cipher *tfm = ctx->cipher;\n\tint err = 0;\n\n\tcrypto_ablkcipher_clear_flags(ctr, CRYPTO_TFM_REQ_MASK);\n\tcrypto_ablkcipher_set_flags(ctr, crypto_aead_get_flags(aead) &\n\t\t\t\t    CRYPTO_TFM_REQ_MASK);\n\terr = crypto_ablkcipher_setkey(ctr, key, keylen);\n\tcrypto_aead_set_flags(aead, crypto_ablkcipher_get_flags(ctr) &\n\t\t\t      CRYPTO_TFM_RES_MASK);\n\tif (err)\n\t\tgoto out;\n\n\tcrypto_cipher_clear_flags(tfm, CRYPTO_TFM_REQ_MASK);\n\tcrypto_cipher_set_flags(tfm, crypto_aead_get_flags(aead) &\n\t\t\t\t    CRYPTO_TFM_REQ_MASK);\n\terr = crypto_cipher_setkey(tfm, key, keylen);\n\tcrypto_aead_set_flags(aead, crypto_cipher_get_flags(tfm) &\n\t\t\t      CRYPTO_TFM_RES_MASK);\n\nout:\n\treturn err;\n}\n\nstatic int crypto_ccm_setauthsize(struct crypto_aead *tfm,\n\t\t\t\t  unsigned int authsize)\n{\n\tswitch (authsize) {\n\tcase 4:\n\tcase 6:\n\tcase 8:\n\tcase 10:\n\tcase 12:\n\tcase 14:\n\tcase 16:\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int format_input(u8 *info, struct aead_request *req,\n\t\t\tunsigned int cryptlen)\n{\n\tstruct crypto_aead *aead = crypto_aead_reqtfm(req);\n\tunsigned int lp = req->iv[0];\n\tunsigned int l = lp + 1;\n\tunsigned int m;\n\n\tm = crypto_aead_authsize(aead);\n\n\tmemcpy(info, req->iv, 16);\n\n\t/* format control info per RFC 3610 and\n\t * NIST Special Publication 800-38C\n\t */\n\t*info |= (8 * ((m - 2) / 2));\n\tif (req->assoclen)\n\t\t*info |= 64;\n\n\treturn set_msg_len(info + 16 - l, cryptlen, l);\n}\n\nstatic int format_adata(u8 *adata, unsigned int a)\n{\n\tint len = 0;\n\n\t/* add control info for associated data\n\t * RFC 3610 and NIST Special Publication 800-38C\n\t */\n\tif (a < 65280) {\n\t\t*(__be16 *)adata = cpu_to_be16(a);\n\t\tlen = 2;\n\t} else  {\n\t\t*(__be16 *)adata = cpu_to_be16(0xfffe);\n\t\t*(__be32 *)&adata[2] = cpu_to_be32(a);\n\t\tlen = 6;\n\t}\n\n\treturn len;\n}\n\nstatic void compute_mac(struct crypto_cipher *tfm, u8 *data, int n,\n\t\t       struct crypto_ccm_req_priv_ctx *pctx)\n{\n\tunsigned int bs = 16;\n\tu8 *odata = pctx->odata;\n\tu8 *idata = pctx->idata;\n\tint datalen, getlen;\n\n\tdatalen = n;\n\n\t/* first time in here, block may be partially filled. */\n\tgetlen = bs - pctx->ilen;\n\tif (datalen >= getlen) {\n\t\tmemcpy(idata + pctx->ilen, data, getlen);\n\t\tcrypto_xor(odata, idata, bs);\n\t\tcrypto_cipher_encrypt_one(tfm, odata, odata);\n\t\tdatalen -= getlen;\n\t\tdata += getlen;\n\t\tpctx->ilen = 0;\n\t}\n\n\t/* now encrypt rest of data */\n\twhile (datalen >= bs) {\n\t\tcrypto_xor(odata, data, bs);\n\t\tcrypto_cipher_encrypt_one(tfm, odata, odata);\n\n\t\tdatalen -= bs;\n\t\tdata += bs;\n\t}\n\n\t/* check and see if there's leftover data that wasn't\n\t * enough to fill a block.\n\t */\n\tif (datalen) {\n\t\tmemcpy(idata + pctx->ilen, data, datalen);\n\t\tpctx->ilen += datalen;\n\t}\n}\n\nstatic void get_data_to_compute(struct crypto_cipher *tfm,\n\t\t\t       struct crypto_ccm_req_priv_ctx *pctx,\n\t\t\t       struct scatterlist *sg, unsigned int len)\n{\n\tstruct scatter_walk walk;\n\tu8 *data_src;\n\tint n;\n\n\tscatterwalk_start(&walk, sg);\n\n\twhile (len) {\n\t\tn = scatterwalk_clamp(&walk, len);\n\t\tif (!n) {\n\t\t\tscatterwalk_start(&walk, sg_next(walk.sg));\n\t\t\tn = scatterwalk_clamp(&walk, len);\n\t\t}\n\t\tdata_src = scatterwalk_map(&walk);\n\n\t\tcompute_mac(tfm, data_src, n, pctx);\n\t\tlen -= n;\n\n\t\tscatterwalk_unmap(data_src);\n\t\tscatterwalk_advance(&walk, n);\n\t\tscatterwalk_done(&walk, 0, len);\n\t\tif (len)\n\t\t\tcrypto_yield(pctx->flags);\n\t}\n\n\t/* any leftover needs padding and then encrypted */\n\tif (pctx->ilen) {\n\t\tint padlen;\n\t\tu8 *odata = pctx->odata;\n\t\tu8 *idata = pctx->idata;\n\n\t\tpadlen = 16 - pctx->ilen;\n\t\tmemset(idata + pctx->ilen, 0, padlen);\n\t\tcrypto_xor(odata, idata, 16);\n\t\tcrypto_cipher_encrypt_one(tfm, odata, odata);\n\t\tpctx->ilen = 0;\n\t}\n}\n\nstatic int crypto_ccm_auth(struct aead_request *req, struct scatterlist *plain,\n\t\t\t   unsigned int cryptlen)\n{\n\tstruct crypto_aead *aead = crypto_aead_reqtfm(req);\n\tstruct crypto_ccm_ctx *ctx = crypto_aead_ctx(aead);\n\tstruct crypto_ccm_req_priv_ctx *pctx = crypto_ccm_reqctx(req);\n\tstruct crypto_cipher *cipher = ctx->cipher;\n\tunsigned int assoclen = req->assoclen;\n\tu8 *odata = pctx->odata;\n\tu8 *idata = pctx->idata;\n\tint err;\n\n\t/* format control data for input */\n\terr = format_input(odata, req, cryptlen);\n\tif (err)\n\t\tgoto out;\n\n\t/* encrypt first block to use as start in computing mac  */\n\tcrypto_cipher_encrypt_one(cipher, odata, odata);\n\n\t/* format associated data and compute into mac */\n\tif (assoclen) {\n\t\tpctx->ilen = format_adata(idata, assoclen);\n\t\tget_data_to_compute(cipher, pctx, req->assoc, req->assoclen);\n\t} else {\n\t\tpctx->ilen = 0;\n\t}\n\n\t/* compute plaintext into mac */\n\tif (cryptlen)\n\t\tget_data_to_compute(cipher, pctx, plain, cryptlen);\n\nout:\n\treturn err;\n}\n\nstatic void crypto_ccm_encrypt_done(struct crypto_async_request *areq, int err)\n{\n\tstruct aead_request *req = areq->data;\n\tstruct crypto_aead *aead = crypto_aead_reqtfm(req);\n\tstruct crypto_ccm_req_priv_ctx *pctx = crypto_ccm_reqctx(req);\n\tu8 *odata = pctx->odata;\n\n\tif (!err)\n\t\tscatterwalk_map_and_copy(odata, req->dst, req->cryptlen,\n\t\t\t\t\t crypto_aead_authsize(aead), 1);\n\taead_request_complete(req, err);\n}\n\nstatic inline int crypto_ccm_check_iv(const u8 *iv)\n{\n\t/* 2 <= L <= 8, so 1 <= L' <= 7. */\n\tif (1 > iv[0] || iv[0] > 7)\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic int crypto_ccm_encrypt(struct aead_request *req)\n{\n\tstruct crypto_aead *aead = crypto_aead_reqtfm(req);\n\tstruct crypto_ccm_ctx *ctx = crypto_aead_ctx(aead);\n\tstruct crypto_ccm_req_priv_ctx *pctx = crypto_ccm_reqctx(req);\n\tstruct ablkcipher_request *abreq = &pctx->abreq;\n\tstruct scatterlist *dst;\n\tunsigned int cryptlen = req->cryptlen;\n\tu8 *odata = pctx->odata;\n\tu8 *iv = req->iv;\n\tint err;\n\n\terr = crypto_ccm_check_iv(iv);\n\tif (err)\n\t\treturn err;\n\n\tpctx->flags = aead_request_flags(req);\n\n\terr = crypto_ccm_auth(req, req->src, cryptlen);\n\tif (err)\n\t\treturn err;\n\n\t /* Note: rfc 3610 and NIST 800-38C require counter of\n\t * zero to encrypt auth tag.\n\t */\n\tmemset(iv + 15 - iv[0], 0, iv[0] + 1);\n\n\tsg_init_table(pctx->src, 2);\n\tsg_set_buf(pctx->src, odata, 16);\n\tscatterwalk_sg_chain(pctx->src, 2, req->src);\n\n\tdst = pctx->src;\n\tif (req->src != req->dst) {\n\t\tsg_init_table(pctx->dst, 2);\n\t\tsg_set_buf(pctx->dst, odata, 16);\n\t\tscatterwalk_sg_chain(pctx->dst, 2, req->dst);\n\t\tdst = pctx->dst;\n\t}\n\n\tablkcipher_request_set_tfm(abreq, ctx->ctr);\n\tablkcipher_request_set_callback(abreq, pctx->flags,\n\t\t\t\t\tcrypto_ccm_encrypt_done, req);\n\tablkcipher_request_set_crypt(abreq, pctx->src, dst, cryptlen + 16, iv);\n\terr = crypto_ablkcipher_encrypt(abreq);\n\tif (err)\n\t\treturn err;\n\n\t/* copy authtag to end of dst */\n\tscatterwalk_map_and_copy(odata, req->dst, cryptlen,\n\t\t\t\t crypto_aead_authsize(aead), 1);\n\treturn err;\n}\n\nstatic void crypto_ccm_decrypt_done(struct crypto_async_request *areq,\n\t\t\t\t   int err)\n{\n\tstruct aead_request *req = areq->data;\n\tstruct crypto_ccm_req_priv_ctx *pctx = crypto_ccm_reqctx(req);\n\tstruct crypto_aead *aead = crypto_aead_reqtfm(req);\n\tunsigned int authsize = crypto_aead_authsize(aead);\n\tunsigned int cryptlen = req->cryptlen - authsize;\n\n\tif (!err) {\n\t\terr = crypto_ccm_auth(req, req->dst, cryptlen);\n\t\tif (!err && crypto_memneq(pctx->auth_tag, pctx->odata, authsize))\n\t\t\terr = -EBADMSG;\n\t}\n\taead_request_complete(req, err);\n}\n\nstatic int crypto_ccm_decrypt(struct aead_request *req)\n{\n\tstruct crypto_aead *aead = crypto_aead_reqtfm(req);\n\tstruct crypto_ccm_ctx *ctx = crypto_aead_ctx(aead);\n\tstruct crypto_ccm_req_priv_ctx *pctx = crypto_ccm_reqctx(req);\n\tstruct ablkcipher_request *abreq = &pctx->abreq;\n\tstruct scatterlist *dst;\n\tunsigned int authsize = crypto_aead_authsize(aead);\n\tunsigned int cryptlen = req->cryptlen;\n\tu8 *authtag = pctx->auth_tag;\n\tu8 *odata = pctx->odata;\n\tu8 *iv = req->iv;\n\tint err;\n\n\tif (cryptlen < authsize)\n\t\treturn -EINVAL;\n\tcryptlen -= authsize;\n\n\terr = crypto_ccm_check_iv(iv);\n\tif (err)\n\t\treturn err;\n\n\tpctx->flags = aead_request_flags(req);\n\n\tscatterwalk_map_and_copy(authtag, req->src, cryptlen, authsize, 0);\n\n\tmemset(iv + 15 - iv[0], 0, iv[0] + 1);\n\n\tsg_init_table(pctx->src, 2);\n\tsg_set_buf(pctx->src, authtag, 16);\n\tscatterwalk_sg_chain(pctx->src, 2, req->src);\n\n\tdst = pctx->src;\n\tif (req->src != req->dst) {\n\t\tsg_init_table(pctx->dst, 2);\n\t\tsg_set_buf(pctx->dst, authtag, 16);\n\t\tscatterwalk_sg_chain(pctx->dst, 2, req->dst);\n\t\tdst = pctx->dst;\n\t}\n\n\tablkcipher_request_set_tfm(abreq, ctx->ctr);\n\tablkcipher_request_set_callback(abreq, pctx->flags,\n\t\t\t\t\tcrypto_ccm_decrypt_done, req);\n\tablkcipher_request_set_crypt(abreq, pctx->src, dst, cryptlen + 16, iv);\n\terr = crypto_ablkcipher_decrypt(abreq);\n\tif (err)\n\t\treturn err;\n\n\terr = crypto_ccm_auth(req, req->dst, cryptlen);\n\tif (err)\n\t\treturn err;\n\n\t/* verify */\n\tif (crypto_memneq(authtag, odata, authsize))\n\t\treturn -EBADMSG;\n\n\treturn err;\n}\n\nstatic int crypto_ccm_init_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_instance *inst = (void *)tfm->__crt_alg;\n\tstruct ccm_instance_ctx *ictx = crypto_instance_ctx(inst);\n\tstruct crypto_ccm_ctx *ctx = crypto_tfm_ctx(tfm);\n\tstruct crypto_cipher *cipher;\n\tstruct crypto_ablkcipher *ctr;\n\tunsigned long align;\n\tint err;\n\n\tcipher = crypto_spawn_cipher(&ictx->cipher);\n\tif (IS_ERR(cipher))\n\t\treturn PTR_ERR(cipher);\n\n\tctr = crypto_spawn_skcipher(&ictx->ctr);\n\terr = PTR_ERR(ctr);\n\tif (IS_ERR(ctr))\n\t\tgoto err_free_cipher;\n\n\tctx->cipher = cipher;\n\tctx->ctr = ctr;\n\n\talign = crypto_tfm_alg_alignmask(tfm);\n\talign &= ~(crypto_tfm_ctx_alignment() - 1);\n\ttfm->crt_aead.reqsize = align +\n\t\t\t\tsizeof(struct crypto_ccm_req_priv_ctx) +\n\t\t\t\tcrypto_ablkcipher_reqsize(ctr);\n\n\treturn 0;\n\nerr_free_cipher:\n\tcrypto_free_cipher(cipher);\n\treturn err;\n}\n\nstatic void crypto_ccm_exit_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_ccm_ctx *ctx = crypto_tfm_ctx(tfm);\n\n\tcrypto_free_cipher(ctx->cipher);\n\tcrypto_free_ablkcipher(ctx->ctr);\n}\n\nstatic struct crypto_instance *crypto_ccm_alloc_common(struct rtattr **tb,\n\t\t\t\t\t\t       const char *full_name,\n\t\t\t\t\t\t       const char *ctr_name,\n\t\t\t\t\t\t       const char *cipher_name)\n{\n\tstruct crypto_attr_type *algt;\n\tstruct crypto_instance *inst;\n\tstruct crypto_alg *ctr;\n\tstruct crypto_alg *cipher;\n\tstruct ccm_instance_ctx *ictx;\n\tint err;\n\n\talgt = crypto_get_attr_type(tb);\n\tif (IS_ERR(algt))\n\t\treturn ERR_CAST(algt);\n\n\tif ((algt->type ^ CRYPTO_ALG_TYPE_AEAD) & algt->mask)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tcipher = crypto_alg_mod_lookup(cipher_name,  CRYPTO_ALG_TYPE_CIPHER,\n\t\t\t\t       CRYPTO_ALG_TYPE_MASK);\n\tif (IS_ERR(cipher))\n\t\treturn ERR_CAST(cipher);\n\n\terr = -EINVAL;\n\tif (cipher->cra_blocksize != 16)\n\t\tgoto out_put_cipher;\n\n\tinst = kzalloc(sizeof(*inst) + sizeof(*ictx), GFP_KERNEL);\n\terr = -ENOMEM;\n\tif (!inst)\n\t\tgoto out_put_cipher;\n\n\tictx = crypto_instance_ctx(inst);\n\n\terr = crypto_init_spawn(&ictx->cipher, cipher, inst,\n\t\t\t\tCRYPTO_ALG_TYPE_MASK);\n\tif (err)\n\t\tgoto err_free_inst;\n\n\tcrypto_set_skcipher_spawn(&ictx->ctr, inst);\n\terr = crypto_grab_skcipher(&ictx->ctr, ctr_name, 0,\n\t\t\t\t   crypto_requires_sync(algt->type,\n\t\t\t\t\t\t\talgt->mask));\n\tif (err)\n\t\tgoto err_drop_cipher;\n\n\tctr = crypto_skcipher_spawn_alg(&ictx->ctr);\n\n\t/* Not a stream cipher? */\n\terr = -EINVAL;\n\tif (ctr->cra_blocksize != 1)\n\t\tgoto err_drop_ctr;\n\n\t/* We want the real thing! */\n\tif (ctr->cra_ablkcipher.ivsize != 16)\n\t\tgoto err_drop_ctr;\n\n\terr = -ENAMETOOLONG;\n\tif (snprintf(inst->alg.cra_driver_name, CRYPTO_MAX_ALG_NAME,\n\t\t     \"ccm_base(%s,%s)\", ctr->cra_driver_name,\n\t\t     cipher->cra_driver_name) >= CRYPTO_MAX_ALG_NAME)\n\t\tgoto err_drop_ctr;\n\n\tmemcpy(inst->alg.cra_name, full_name, CRYPTO_MAX_ALG_NAME);\n\n\tinst->alg.cra_flags = CRYPTO_ALG_TYPE_AEAD;\n\tinst->alg.cra_flags |= ctr->cra_flags & CRYPTO_ALG_ASYNC;\n\tinst->alg.cra_priority = cipher->cra_priority + ctr->cra_priority;\n\tinst->alg.cra_blocksize = 1;\n\tinst->alg.cra_alignmask = cipher->cra_alignmask | ctr->cra_alignmask |\n\t\t\t\t  (__alignof__(u32) - 1);\n\tinst->alg.cra_type = &crypto_aead_type;\n\tinst->alg.cra_aead.ivsize = 16;\n\tinst->alg.cra_aead.maxauthsize = 16;\n\tinst->alg.cra_ctxsize = sizeof(struct crypto_ccm_ctx);\n\tinst->alg.cra_init = crypto_ccm_init_tfm;\n\tinst->alg.cra_exit = crypto_ccm_exit_tfm;\n\tinst->alg.cra_aead.setkey = crypto_ccm_setkey;\n\tinst->alg.cra_aead.setauthsize = crypto_ccm_setauthsize;\n\tinst->alg.cra_aead.encrypt = crypto_ccm_encrypt;\n\tinst->alg.cra_aead.decrypt = crypto_ccm_decrypt;\n\nout:\n\tcrypto_mod_put(cipher);\n\treturn inst;\n\nerr_drop_ctr:\n\tcrypto_drop_skcipher(&ictx->ctr);\nerr_drop_cipher:\n\tcrypto_drop_spawn(&ictx->cipher);\nerr_free_inst:\n\tkfree(inst);\nout_put_cipher:\n\tinst = ERR_PTR(err);\n\tgoto out;\n}\n\nstatic struct crypto_instance *crypto_ccm_alloc(struct rtattr **tb)\n{\n\tconst char *cipher_name;\n\tchar ctr_name[CRYPTO_MAX_ALG_NAME];\n\tchar full_name[CRYPTO_MAX_ALG_NAME];\n\n\tcipher_name = crypto_attr_alg_name(tb[1]);\n\tif (IS_ERR(cipher_name))\n\t\treturn ERR_CAST(cipher_name);\n\n\tif (snprintf(ctr_name, CRYPTO_MAX_ALG_NAME, \"ctr(%s)\",\n\t\t     cipher_name) >= CRYPTO_MAX_ALG_NAME)\n\t\treturn ERR_PTR(-ENAMETOOLONG);\n\n\tif (snprintf(full_name, CRYPTO_MAX_ALG_NAME, \"ccm(%s)\", cipher_name) >=\n\t    CRYPTO_MAX_ALG_NAME)\n\t\treturn ERR_PTR(-ENAMETOOLONG);\n\n\treturn crypto_ccm_alloc_common(tb, full_name, ctr_name, cipher_name);\n}\n\nstatic void crypto_ccm_free(struct crypto_instance *inst)\n{\n\tstruct ccm_instance_ctx *ctx = crypto_instance_ctx(inst);\n\n\tcrypto_drop_spawn(&ctx->cipher);\n\tcrypto_drop_skcipher(&ctx->ctr);\n\tkfree(inst);\n}\n\nstatic struct crypto_template crypto_ccm_tmpl = {\n\t.name = \"ccm\",\n\t.alloc = crypto_ccm_alloc,\n\t.free = crypto_ccm_free,\n\t.module = THIS_MODULE,\n};\n\nstatic struct crypto_instance *crypto_ccm_base_alloc(struct rtattr **tb)\n{\n\tconst char *ctr_name;\n\tconst char *cipher_name;\n\tchar full_name[CRYPTO_MAX_ALG_NAME];\n\n\tctr_name = crypto_attr_alg_name(tb[1]);\n\tif (IS_ERR(ctr_name))\n\t\treturn ERR_CAST(ctr_name);\n\n\tcipher_name = crypto_attr_alg_name(tb[2]);\n\tif (IS_ERR(cipher_name))\n\t\treturn ERR_CAST(cipher_name);\n\n\tif (snprintf(full_name, CRYPTO_MAX_ALG_NAME, \"ccm_base(%s,%s)\",\n\t\t     ctr_name, cipher_name) >= CRYPTO_MAX_ALG_NAME)\n\t\treturn ERR_PTR(-ENAMETOOLONG);\n\n\treturn crypto_ccm_alloc_common(tb, full_name, ctr_name, cipher_name);\n}\n\nstatic struct crypto_template crypto_ccm_base_tmpl = {\n\t.name = \"ccm_base\",\n\t.alloc = crypto_ccm_base_alloc,\n\t.free = crypto_ccm_free,\n\t.module = THIS_MODULE,\n};\n\nstatic int crypto_rfc4309_setkey(struct crypto_aead *parent, const u8 *key,\n\t\t\t\t unsigned int keylen)\n{\n\tstruct crypto_rfc4309_ctx *ctx = crypto_aead_ctx(parent);\n\tstruct crypto_aead *child = ctx->child;\n\tint err;\n\n\tif (keylen < 3)\n\t\treturn -EINVAL;\n\n\tkeylen -= 3;\n\tmemcpy(ctx->nonce, key + keylen, 3);\n\n\tcrypto_aead_clear_flags(child, CRYPTO_TFM_REQ_MASK);\n\tcrypto_aead_set_flags(child, crypto_aead_get_flags(parent) &\n\t\t\t\t     CRYPTO_TFM_REQ_MASK);\n\terr = crypto_aead_setkey(child, key, keylen);\n\tcrypto_aead_set_flags(parent, crypto_aead_get_flags(child) &\n\t\t\t\t      CRYPTO_TFM_RES_MASK);\n\n\treturn err;\n}\n\nstatic int crypto_rfc4309_setauthsize(struct crypto_aead *parent,\n\t\t\t\t      unsigned int authsize)\n{\n\tstruct crypto_rfc4309_ctx *ctx = crypto_aead_ctx(parent);\n\n\tswitch (authsize) {\n\tcase 8:\n\tcase 12:\n\tcase 16:\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn crypto_aead_setauthsize(ctx->child, authsize);\n}\n\nstatic struct aead_request *crypto_rfc4309_crypt(struct aead_request *req)\n{\n\tstruct aead_request *subreq = aead_request_ctx(req);\n\tstruct crypto_aead *aead = crypto_aead_reqtfm(req);\n\tstruct crypto_rfc4309_ctx *ctx = crypto_aead_ctx(aead);\n\tstruct crypto_aead *child = ctx->child;\n\tu8 *iv = PTR_ALIGN((u8 *)(subreq + 1) + crypto_aead_reqsize(child),\n\t\t\t   crypto_aead_alignmask(child) + 1);\n\n\t/* L' */\n\tiv[0] = 3;\n\n\tmemcpy(iv + 1, ctx->nonce, 3);\n\tmemcpy(iv + 4, req->iv, 8);\n\n\taead_request_set_tfm(subreq, child);\n\taead_request_set_callback(subreq, req->base.flags, req->base.complete,\n\t\t\t\t  req->base.data);\n\taead_request_set_crypt(subreq, req->src, req->dst, req->cryptlen, iv);\n\taead_request_set_assoc(subreq, req->assoc, req->assoclen);\n\n\treturn subreq;\n}\n\nstatic int crypto_rfc4309_encrypt(struct aead_request *req)\n{\n\treq = crypto_rfc4309_crypt(req);\n\n\treturn crypto_aead_encrypt(req);\n}\n\nstatic int crypto_rfc4309_decrypt(struct aead_request *req)\n{\n\treq = crypto_rfc4309_crypt(req);\n\n\treturn crypto_aead_decrypt(req);\n}\n\nstatic int crypto_rfc4309_init_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_instance *inst = (void *)tfm->__crt_alg;\n\tstruct crypto_aead_spawn *spawn = crypto_instance_ctx(inst);\n\tstruct crypto_rfc4309_ctx *ctx = crypto_tfm_ctx(tfm);\n\tstruct crypto_aead *aead;\n\tunsigned long align;\n\n\taead = crypto_spawn_aead(spawn);\n\tif (IS_ERR(aead))\n\t\treturn PTR_ERR(aead);\n\n\tctx->child = aead;\n\n\talign = crypto_aead_alignmask(aead);\n\talign &= ~(crypto_tfm_ctx_alignment() - 1);\n\ttfm->crt_aead.reqsize = sizeof(struct aead_request) +\n\t\t\t\tALIGN(crypto_aead_reqsize(aead),\n\t\t\t\t      crypto_tfm_ctx_alignment()) +\n\t\t\t\talign + 16;\n\n\treturn 0;\n}\n\nstatic void crypto_rfc4309_exit_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_rfc4309_ctx *ctx = crypto_tfm_ctx(tfm);\n\n\tcrypto_free_aead(ctx->child);\n}\n\nstatic struct crypto_instance *crypto_rfc4309_alloc(struct rtattr **tb)\n{\n\tstruct crypto_attr_type *algt;\n\tstruct crypto_instance *inst;\n\tstruct crypto_aead_spawn *spawn;\n\tstruct crypto_alg *alg;\n\tconst char *ccm_name;\n\tint err;\n\n\talgt = crypto_get_attr_type(tb);\n\tif (IS_ERR(algt))\n\t\treturn ERR_CAST(algt);\n\n\tif ((algt->type ^ CRYPTO_ALG_TYPE_AEAD) & algt->mask)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tccm_name = crypto_attr_alg_name(tb[1]);\n\tif (IS_ERR(ccm_name))\n\t\treturn ERR_CAST(ccm_name);\n\n\tinst = kzalloc(sizeof(*inst) + sizeof(*spawn), GFP_KERNEL);\n\tif (!inst)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tspawn = crypto_instance_ctx(inst);\n\tcrypto_set_aead_spawn(spawn, inst);\n\terr = crypto_grab_aead(spawn, ccm_name, 0,\n\t\t\t       crypto_requires_sync(algt->type, algt->mask));\n\tif (err)\n\t\tgoto out_free_inst;\n\n\talg = crypto_aead_spawn_alg(spawn);\n\n\terr = -EINVAL;\n\n\t/* We only support 16-byte blocks. */\n\tif (alg->cra_aead.ivsize != 16)\n\t\tgoto out_drop_alg;\n\n\t/* Not a stream cipher? */\n\tif (alg->cra_blocksize != 1)\n\t\tgoto out_drop_alg;\n\n\terr = -ENAMETOOLONG;\n\tif (snprintf(inst->alg.cra_name, CRYPTO_MAX_ALG_NAME,\n\t\t     \"rfc4309(%s)\", alg->cra_name) >= CRYPTO_MAX_ALG_NAME ||\n\t    snprintf(inst->alg.cra_driver_name, CRYPTO_MAX_ALG_NAME,\n\t\t     \"rfc4309(%s)\", alg->cra_driver_name) >=\n\t    CRYPTO_MAX_ALG_NAME)\n\t\tgoto out_drop_alg;\n\n\tinst->alg.cra_flags = CRYPTO_ALG_TYPE_AEAD;\n\tinst->alg.cra_flags |= alg->cra_flags & CRYPTO_ALG_ASYNC;\n\tinst->alg.cra_priority = alg->cra_priority;\n\tinst->alg.cra_blocksize = 1;\n\tinst->alg.cra_alignmask = alg->cra_alignmask;\n\tinst->alg.cra_type = &crypto_nivaead_type;\n\n\tinst->alg.cra_aead.ivsize = 8;\n\tinst->alg.cra_aead.maxauthsize = 16;\n\n\tinst->alg.cra_ctxsize = sizeof(struct crypto_rfc4309_ctx);\n\n\tinst->alg.cra_init = crypto_rfc4309_init_tfm;\n\tinst->alg.cra_exit = crypto_rfc4309_exit_tfm;\n\n\tinst->alg.cra_aead.setkey = crypto_rfc4309_setkey;\n\tinst->alg.cra_aead.setauthsize = crypto_rfc4309_setauthsize;\n\tinst->alg.cra_aead.encrypt = crypto_rfc4309_encrypt;\n\tinst->alg.cra_aead.decrypt = crypto_rfc4309_decrypt;\n\n\tinst->alg.cra_aead.geniv = \"seqiv\";\n\nout:\n\treturn inst;\n\nout_drop_alg:\n\tcrypto_drop_aead(spawn);\nout_free_inst:\n\tkfree(inst);\n\tinst = ERR_PTR(err);\n\tgoto out;\n}\n\nstatic void crypto_rfc4309_free(struct crypto_instance *inst)\n{\n\tcrypto_drop_spawn(crypto_instance_ctx(inst));\n\tkfree(inst);\n}\n\nstatic struct crypto_template crypto_rfc4309_tmpl = {\n\t.name = \"rfc4309\",\n\t.alloc = crypto_rfc4309_alloc,\n\t.free = crypto_rfc4309_free,\n\t.module = THIS_MODULE,\n};\n\nstatic int __init crypto_ccm_module_init(void)\n{\n\tint err;\n\n\terr = crypto_register_template(&crypto_ccm_base_tmpl);\n\tif (err)\n\t\tgoto out;\n\n\terr = crypto_register_template(&crypto_ccm_tmpl);\n\tif (err)\n\t\tgoto out_undo_base;\n\n\terr = crypto_register_template(&crypto_rfc4309_tmpl);\n\tif (err)\n\t\tgoto out_undo_ccm;\n\nout:\n\treturn err;\n\nout_undo_ccm:\n\tcrypto_unregister_template(&crypto_ccm_tmpl);\nout_undo_base:\n\tcrypto_unregister_template(&crypto_ccm_base_tmpl);\n\tgoto out;\n}\n\nstatic void __exit crypto_ccm_module_exit(void)\n{\n\tcrypto_unregister_template(&crypto_rfc4309_tmpl);\n\tcrypto_unregister_template(&crypto_ccm_tmpl);\n\tcrypto_unregister_template(&crypto_ccm_base_tmpl);\n}\n\nmodule_init(crypto_ccm_module_init);\nmodule_exit(crypto_ccm_module_exit);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"Counter with CBC MAC\");\nMODULE_ALIAS_CRYPTO(\"ccm_base\");\nMODULE_ALIAS_CRYPTO(\"rfc4309\");\nMODULE_ALIAS_CRYPTO(\"ccm\");\n", "/*\n * chainiv: Chain IV Generator\n *\n * Generate IVs simply be using the last block of the previous encryption.\n * This is mainly useful for CBC with a synchronous algorithm.\n *\n * Copyright (c) 2007 Herbert Xu <herbert@gondor.apana.org.au>\n *\n * This program is free software; you can redistribute it and/or modify it\n * under the terms of the GNU General Public License as published by the Free\n * Software Foundation; either version 2 of the License, or (at your option)\n * any later version.\n *\n */\n\n#include <crypto/internal/skcipher.h>\n#include <crypto/rng.h>\n#include <crypto/crypto_wq.h>\n#include <linux/err.h>\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/spinlock.h>\n#include <linux/string.h>\n#include <linux/workqueue.h>\n\nenum {\n\tCHAINIV_STATE_INUSE = 0,\n};\n\nstruct chainiv_ctx {\n\tspinlock_t lock;\n\tchar iv[];\n};\n\nstruct async_chainiv_ctx {\n\tunsigned long state;\n\n\tspinlock_t lock;\n\tint err;\n\n\tstruct crypto_queue queue;\n\tstruct work_struct postponed;\n\n\tchar iv[];\n};\n\nstatic int chainiv_givencrypt(struct skcipher_givcrypt_request *req)\n{\n\tstruct crypto_ablkcipher *geniv = skcipher_givcrypt_reqtfm(req);\n\tstruct chainiv_ctx *ctx = crypto_ablkcipher_ctx(geniv);\n\tstruct ablkcipher_request *subreq = skcipher_givcrypt_reqctx(req);\n\tunsigned int ivsize;\n\tint err;\n\n\tablkcipher_request_set_tfm(subreq, skcipher_geniv_cipher(geniv));\n\tablkcipher_request_set_callback(subreq, req->creq.base.flags &\n\t\t\t\t\t\t~CRYPTO_TFM_REQ_MAY_SLEEP,\n\t\t\t\t\treq->creq.base.complete,\n\t\t\t\t\treq->creq.base.data);\n\tablkcipher_request_set_crypt(subreq, req->creq.src, req->creq.dst,\n\t\t\t\t     req->creq.nbytes, req->creq.info);\n\n\tspin_lock_bh(&ctx->lock);\n\n\tivsize = crypto_ablkcipher_ivsize(geniv);\n\n\tmemcpy(req->giv, ctx->iv, ivsize);\n\tmemcpy(subreq->info, ctx->iv, ivsize);\n\n\terr = crypto_ablkcipher_encrypt(subreq);\n\tif (err)\n\t\tgoto unlock;\n\n\tmemcpy(ctx->iv, subreq->info, ivsize);\n\nunlock:\n\tspin_unlock_bh(&ctx->lock);\n\n\treturn err;\n}\n\nstatic int chainiv_givencrypt_first(struct skcipher_givcrypt_request *req)\n{\n\tstruct crypto_ablkcipher *geniv = skcipher_givcrypt_reqtfm(req);\n\tstruct chainiv_ctx *ctx = crypto_ablkcipher_ctx(geniv);\n\tint err = 0;\n\n\tspin_lock_bh(&ctx->lock);\n\tif (crypto_ablkcipher_crt(geniv)->givencrypt !=\n\t    chainiv_givencrypt_first)\n\t\tgoto unlock;\n\n\tcrypto_ablkcipher_crt(geniv)->givencrypt = chainiv_givencrypt;\n\terr = crypto_rng_get_bytes(crypto_default_rng, ctx->iv,\n\t\t\t\t   crypto_ablkcipher_ivsize(geniv));\n\nunlock:\n\tspin_unlock_bh(&ctx->lock);\n\n\tif (err)\n\t\treturn err;\n\n\treturn chainiv_givencrypt(req);\n}\n\nstatic int chainiv_init_common(struct crypto_tfm *tfm)\n{\n\ttfm->crt_ablkcipher.reqsize = sizeof(struct ablkcipher_request);\n\n\treturn skcipher_geniv_init(tfm);\n}\n\nstatic int chainiv_init(struct crypto_tfm *tfm)\n{\n\tstruct chainiv_ctx *ctx = crypto_tfm_ctx(tfm);\n\n\tspin_lock_init(&ctx->lock);\n\n\treturn chainiv_init_common(tfm);\n}\n\nstatic int async_chainiv_schedule_work(struct async_chainiv_ctx *ctx)\n{\n\tint queued;\n\tint err = ctx->err;\n\n\tif (!ctx->queue.qlen) {\n\t\tsmp_mb__before_atomic();\n\t\tclear_bit(CHAINIV_STATE_INUSE, &ctx->state);\n\n\t\tif (!ctx->queue.qlen ||\n\t\t    test_and_set_bit(CHAINIV_STATE_INUSE, &ctx->state))\n\t\t\tgoto out;\n\t}\n\n\tqueued = queue_work(kcrypto_wq, &ctx->postponed);\n\tBUG_ON(!queued);\n\nout:\n\treturn err;\n}\n\nstatic int async_chainiv_postpone_request(struct skcipher_givcrypt_request *req)\n{\n\tstruct crypto_ablkcipher *geniv = skcipher_givcrypt_reqtfm(req);\n\tstruct async_chainiv_ctx *ctx = crypto_ablkcipher_ctx(geniv);\n\tint err;\n\n\tspin_lock_bh(&ctx->lock);\n\terr = skcipher_enqueue_givcrypt(&ctx->queue, req);\n\tspin_unlock_bh(&ctx->lock);\n\n\tif (test_and_set_bit(CHAINIV_STATE_INUSE, &ctx->state))\n\t\treturn err;\n\n\tctx->err = err;\n\treturn async_chainiv_schedule_work(ctx);\n}\n\nstatic int async_chainiv_givencrypt_tail(struct skcipher_givcrypt_request *req)\n{\n\tstruct crypto_ablkcipher *geniv = skcipher_givcrypt_reqtfm(req);\n\tstruct async_chainiv_ctx *ctx = crypto_ablkcipher_ctx(geniv);\n\tstruct ablkcipher_request *subreq = skcipher_givcrypt_reqctx(req);\n\tunsigned int ivsize = crypto_ablkcipher_ivsize(geniv);\n\n\tmemcpy(req->giv, ctx->iv, ivsize);\n\tmemcpy(subreq->info, ctx->iv, ivsize);\n\n\tctx->err = crypto_ablkcipher_encrypt(subreq);\n\tif (ctx->err)\n\t\tgoto out;\n\n\tmemcpy(ctx->iv, subreq->info, ivsize);\n\nout:\n\treturn async_chainiv_schedule_work(ctx);\n}\n\nstatic int async_chainiv_givencrypt(struct skcipher_givcrypt_request *req)\n{\n\tstruct crypto_ablkcipher *geniv = skcipher_givcrypt_reqtfm(req);\n\tstruct async_chainiv_ctx *ctx = crypto_ablkcipher_ctx(geniv);\n\tstruct ablkcipher_request *subreq = skcipher_givcrypt_reqctx(req);\n\n\tablkcipher_request_set_tfm(subreq, skcipher_geniv_cipher(geniv));\n\tablkcipher_request_set_callback(subreq, req->creq.base.flags,\n\t\t\t\t\treq->creq.base.complete,\n\t\t\t\t\treq->creq.base.data);\n\tablkcipher_request_set_crypt(subreq, req->creq.src, req->creq.dst,\n\t\t\t\t     req->creq.nbytes, req->creq.info);\n\n\tif (test_and_set_bit(CHAINIV_STATE_INUSE, &ctx->state))\n\t\tgoto postpone;\n\n\tif (ctx->queue.qlen) {\n\t\tclear_bit(CHAINIV_STATE_INUSE, &ctx->state);\n\t\tgoto postpone;\n\t}\n\n\treturn async_chainiv_givencrypt_tail(req);\n\npostpone:\n\treturn async_chainiv_postpone_request(req);\n}\n\nstatic int async_chainiv_givencrypt_first(struct skcipher_givcrypt_request *req)\n{\n\tstruct crypto_ablkcipher *geniv = skcipher_givcrypt_reqtfm(req);\n\tstruct async_chainiv_ctx *ctx = crypto_ablkcipher_ctx(geniv);\n\tint err = 0;\n\n\tif (test_and_set_bit(CHAINIV_STATE_INUSE, &ctx->state))\n\t\tgoto out;\n\n\tif (crypto_ablkcipher_crt(geniv)->givencrypt !=\n\t    async_chainiv_givencrypt_first)\n\t\tgoto unlock;\n\n\tcrypto_ablkcipher_crt(geniv)->givencrypt = async_chainiv_givencrypt;\n\terr = crypto_rng_get_bytes(crypto_default_rng, ctx->iv,\n\t\t\t\t   crypto_ablkcipher_ivsize(geniv));\n\nunlock:\n\tclear_bit(CHAINIV_STATE_INUSE, &ctx->state);\n\n\tif (err)\n\t\treturn err;\n\nout:\n\treturn async_chainiv_givencrypt(req);\n}\n\nstatic void async_chainiv_do_postponed(struct work_struct *work)\n{\n\tstruct async_chainiv_ctx *ctx = container_of(work,\n\t\t\t\t\t\t     struct async_chainiv_ctx,\n\t\t\t\t\t\t     postponed);\n\tstruct skcipher_givcrypt_request *req;\n\tstruct ablkcipher_request *subreq;\n\tint err;\n\n\t/* Only handle one request at a time to avoid hogging keventd. */\n\tspin_lock_bh(&ctx->lock);\n\treq = skcipher_dequeue_givcrypt(&ctx->queue);\n\tspin_unlock_bh(&ctx->lock);\n\n\tif (!req) {\n\t\tasync_chainiv_schedule_work(ctx);\n\t\treturn;\n\t}\n\n\tsubreq = skcipher_givcrypt_reqctx(req);\n\tsubreq->base.flags |= CRYPTO_TFM_REQ_MAY_SLEEP;\n\n\terr = async_chainiv_givencrypt_tail(req);\n\n\tlocal_bh_disable();\n\tskcipher_givcrypt_complete(req, err);\n\tlocal_bh_enable();\n}\n\nstatic int async_chainiv_init(struct crypto_tfm *tfm)\n{\n\tstruct async_chainiv_ctx *ctx = crypto_tfm_ctx(tfm);\n\n\tspin_lock_init(&ctx->lock);\n\n\tcrypto_init_queue(&ctx->queue, 100);\n\tINIT_WORK(&ctx->postponed, async_chainiv_do_postponed);\n\n\treturn chainiv_init_common(tfm);\n}\n\nstatic void async_chainiv_exit(struct crypto_tfm *tfm)\n{\n\tstruct async_chainiv_ctx *ctx = crypto_tfm_ctx(tfm);\n\n\tBUG_ON(test_bit(CHAINIV_STATE_INUSE, &ctx->state) || ctx->queue.qlen);\n\n\tskcipher_geniv_exit(tfm);\n}\n\nstatic struct crypto_template chainiv_tmpl;\n\nstatic struct crypto_instance *chainiv_alloc(struct rtattr **tb)\n{\n\tstruct crypto_attr_type *algt;\n\tstruct crypto_instance *inst;\n\tint err;\n\n\talgt = crypto_get_attr_type(tb);\n\tif (IS_ERR(algt))\n\t\treturn ERR_CAST(algt);\n\n\terr = crypto_get_default_rng();\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\tinst = skcipher_geniv_alloc(&chainiv_tmpl, tb, 0, 0);\n\tif (IS_ERR(inst))\n\t\tgoto put_rng;\n\n\tinst->alg.cra_ablkcipher.givencrypt = chainiv_givencrypt_first;\n\n\tinst->alg.cra_init = chainiv_init;\n\tinst->alg.cra_exit = skcipher_geniv_exit;\n\n\tinst->alg.cra_ctxsize = sizeof(struct chainiv_ctx);\n\n\tif (!crypto_requires_sync(algt->type, algt->mask)) {\n\t\tinst->alg.cra_flags |= CRYPTO_ALG_ASYNC;\n\n\t\tinst->alg.cra_ablkcipher.givencrypt =\n\t\t\tasync_chainiv_givencrypt_first;\n\n\t\tinst->alg.cra_init = async_chainiv_init;\n\t\tinst->alg.cra_exit = async_chainiv_exit;\n\n\t\tinst->alg.cra_ctxsize = sizeof(struct async_chainiv_ctx);\n\t}\n\n\tinst->alg.cra_ctxsize += inst->alg.cra_ablkcipher.ivsize;\n\nout:\n\treturn inst;\n\nput_rng:\n\tcrypto_put_default_rng();\n\tgoto out;\n}\n\nstatic void chainiv_free(struct crypto_instance *inst)\n{\n\tskcipher_geniv_free(inst);\n\tcrypto_put_default_rng();\n}\n\nstatic struct crypto_template chainiv_tmpl = {\n\t.name = \"chainiv\",\n\t.alloc = chainiv_alloc,\n\t.free = chainiv_free,\n\t.module = THIS_MODULE,\n};\n\nstatic int __init chainiv_module_init(void)\n{\n\treturn crypto_register_template(&chainiv_tmpl);\n}\n\nstatic void chainiv_module_exit(void)\n{\n\tcrypto_unregister_template(&chainiv_tmpl);\n}\n\nmodule_init(chainiv_module_init);\nmodule_exit(chainiv_module_exit);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"Chain IV Generator\");\nMODULE_ALIAS_CRYPTO(\"chainiv\");\n", "/*\n * CMAC: Cipher Block Mode for Authentication\n *\n * Copyright \u00a9 2013 Jussi Kivilinna <jussi.kivilinna@iki.fi>\n *\n * Based on work by:\n *  Copyright \u00a9 2013 Tom St Denis <tstdenis@elliptictech.com>\n * Based on crypto/xcbc.c:\n *  Copyright \u00a9 2006 USAGI/WIDE Project,\n *   Author: Kazunori Miyazawa <miyazawa@linux-ipv6.org>\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License as published by\n * the Free Software Foundation; either version 2 of the License, or\n * (at your option) any later version.\n *\n */\n\n#include <crypto/internal/hash.h>\n#include <linux/err.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n\n/*\n * +------------------------\n * | <parent tfm>\n * +------------------------\n * | cmac_tfm_ctx\n * +------------------------\n * | consts (block size * 2)\n * +------------------------\n */\nstruct cmac_tfm_ctx {\n\tstruct crypto_cipher *child;\n\tu8 ctx[];\n};\n\n/*\n * +------------------------\n * | <shash desc>\n * +------------------------\n * | cmac_desc_ctx\n * +------------------------\n * | odds (block size)\n * +------------------------\n * | prev (block size)\n * +------------------------\n */\nstruct cmac_desc_ctx {\n\tunsigned int len;\n\tu8 ctx[];\n};\n\nstatic int crypto_cmac_digest_setkey(struct crypto_shash *parent,\n\t\t\t\t     const u8 *inkey, unsigned int keylen)\n{\n\tunsigned long alignmask = crypto_shash_alignmask(parent);\n\tstruct cmac_tfm_ctx *ctx = crypto_shash_ctx(parent);\n\tunsigned int bs = crypto_shash_blocksize(parent);\n\t__be64 *consts = PTR_ALIGN((void *)ctx->ctx, alignmask + 1);\n\tu64 _const[2];\n\tint i, err = 0;\n\tu8 msb_mask, gfmask;\n\n\terr = crypto_cipher_setkey(ctx->child, inkey, keylen);\n\tif (err)\n\t\treturn err;\n\n\t/* encrypt the zero block */\n\tmemset(consts, 0, bs);\n\tcrypto_cipher_encrypt_one(ctx->child, (u8 *)consts, (u8 *)consts);\n\n\tswitch (bs) {\n\tcase 16:\n\t\tgfmask = 0x87;\n\t\t_const[0] = be64_to_cpu(consts[1]);\n\t\t_const[1] = be64_to_cpu(consts[0]);\n\n\t\t/* gf(2^128) multiply zero-ciphertext with u and u^2 */\n\t\tfor (i = 0; i < 4; i += 2) {\n\t\t\tmsb_mask = ((s64)_const[1] >> 63) & gfmask;\n\t\t\t_const[1] = (_const[1] << 1) | (_const[0] >> 63);\n\t\t\t_const[0] = (_const[0] << 1) ^ msb_mask;\n\n\t\t\tconsts[i + 0] = cpu_to_be64(_const[1]);\n\t\t\tconsts[i + 1] = cpu_to_be64(_const[0]);\n\t\t}\n\n\t\tbreak;\n\tcase 8:\n\t\tgfmask = 0x1B;\n\t\t_const[0] = be64_to_cpu(consts[0]);\n\n\t\t/* gf(2^64) multiply zero-ciphertext with u and u^2 */\n\t\tfor (i = 0; i < 2; i++) {\n\t\t\tmsb_mask = ((s64)_const[0] >> 63) & gfmask;\n\t\t\t_const[0] = (_const[0] << 1) ^ msb_mask;\n\n\t\t\tconsts[i] = cpu_to_be64(_const[0]);\n\t\t}\n\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\nstatic int crypto_cmac_digest_init(struct shash_desc *pdesc)\n{\n\tunsigned long alignmask = crypto_shash_alignmask(pdesc->tfm);\n\tstruct cmac_desc_ctx *ctx = shash_desc_ctx(pdesc);\n\tint bs = crypto_shash_blocksize(pdesc->tfm);\n\tu8 *prev = PTR_ALIGN((void *)ctx->ctx, alignmask + 1) + bs;\n\n\tctx->len = 0;\n\tmemset(prev, 0, bs);\n\n\treturn 0;\n}\n\nstatic int crypto_cmac_digest_update(struct shash_desc *pdesc, const u8 *p,\n\t\t\t\t     unsigned int len)\n{\n\tstruct crypto_shash *parent = pdesc->tfm;\n\tunsigned long alignmask = crypto_shash_alignmask(parent);\n\tstruct cmac_tfm_ctx *tctx = crypto_shash_ctx(parent);\n\tstruct cmac_desc_ctx *ctx = shash_desc_ctx(pdesc);\n\tstruct crypto_cipher *tfm = tctx->child;\n\tint bs = crypto_shash_blocksize(parent);\n\tu8 *odds = PTR_ALIGN((void *)ctx->ctx, alignmask + 1);\n\tu8 *prev = odds + bs;\n\n\t/* checking the data can fill the block */\n\tif ((ctx->len + len) <= bs) {\n\t\tmemcpy(odds + ctx->len, p, len);\n\t\tctx->len += len;\n\t\treturn 0;\n\t}\n\n\t/* filling odds with new data and encrypting it */\n\tmemcpy(odds + ctx->len, p, bs - ctx->len);\n\tlen -= bs - ctx->len;\n\tp += bs - ctx->len;\n\n\tcrypto_xor(prev, odds, bs);\n\tcrypto_cipher_encrypt_one(tfm, prev, prev);\n\n\t/* clearing the length */\n\tctx->len = 0;\n\n\t/* encrypting the rest of data */\n\twhile (len > bs) {\n\t\tcrypto_xor(prev, p, bs);\n\t\tcrypto_cipher_encrypt_one(tfm, prev, prev);\n\t\tp += bs;\n\t\tlen -= bs;\n\t}\n\n\t/* keeping the surplus of blocksize */\n\tif (len) {\n\t\tmemcpy(odds, p, len);\n\t\tctx->len = len;\n\t}\n\n\treturn 0;\n}\n\nstatic int crypto_cmac_digest_final(struct shash_desc *pdesc, u8 *out)\n{\n\tstruct crypto_shash *parent = pdesc->tfm;\n\tunsigned long alignmask = crypto_shash_alignmask(parent);\n\tstruct cmac_tfm_ctx *tctx = crypto_shash_ctx(parent);\n\tstruct cmac_desc_ctx *ctx = shash_desc_ctx(pdesc);\n\tstruct crypto_cipher *tfm = tctx->child;\n\tint bs = crypto_shash_blocksize(parent);\n\tu8 *consts = PTR_ALIGN((void *)tctx->ctx, alignmask + 1);\n\tu8 *odds = PTR_ALIGN((void *)ctx->ctx, alignmask + 1);\n\tu8 *prev = odds + bs;\n\tunsigned int offset = 0;\n\n\tif (ctx->len != bs) {\n\t\tunsigned int rlen;\n\t\tu8 *p = odds + ctx->len;\n\n\t\t*p = 0x80;\n\t\tp++;\n\n\t\trlen = bs - ctx->len - 1;\n\t\tif (rlen)\n\t\t\tmemset(p, 0, rlen);\n\n\t\toffset += bs;\n\t}\n\n\tcrypto_xor(prev, odds, bs);\n\tcrypto_xor(prev, consts + offset, bs);\n\n\tcrypto_cipher_encrypt_one(tfm, out, prev);\n\n\treturn 0;\n}\n\nstatic int cmac_init_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_cipher *cipher;\n\tstruct crypto_instance *inst = (void *)tfm->__crt_alg;\n\tstruct crypto_spawn *spawn = crypto_instance_ctx(inst);\n\tstruct cmac_tfm_ctx *ctx = crypto_tfm_ctx(tfm);\n\n\tcipher = crypto_spawn_cipher(spawn);\n\tif (IS_ERR(cipher))\n\t\treturn PTR_ERR(cipher);\n\n\tctx->child = cipher;\n\n\treturn 0;\n};\n\nstatic void cmac_exit_tfm(struct crypto_tfm *tfm)\n{\n\tstruct cmac_tfm_ctx *ctx = crypto_tfm_ctx(tfm);\n\tcrypto_free_cipher(ctx->child);\n}\n\nstatic int cmac_create(struct crypto_template *tmpl, struct rtattr **tb)\n{\n\tstruct shash_instance *inst;\n\tstruct crypto_alg *alg;\n\tunsigned long alignmask;\n\tint err;\n\n\terr = crypto_check_attr_type(tb, CRYPTO_ALG_TYPE_SHASH);\n\tif (err)\n\t\treturn err;\n\n\talg = crypto_get_attr_alg(tb, CRYPTO_ALG_TYPE_CIPHER,\n\t\t\t\t  CRYPTO_ALG_TYPE_MASK);\n\tif (IS_ERR(alg))\n\t\treturn PTR_ERR(alg);\n\n\tswitch (alg->cra_blocksize) {\n\tcase 16:\n\tcase 8:\n\t\tbreak;\n\tdefault:\n\t\tgoto out_put_alg;\n\t}\n\n\tinst = shash_alloc_instance(\"cmac\", alg);\n\terr = PTR_ERR(inst);\n\tif (IS_ERR(inst))\n\t\tgoto out_put_alg;\n\n\terr = crypto_init_spawn(shash_instance_ctx(inst), alg,\n\t\t\t\tshash_crypto_instance(inst),\n\t\t\t\tCRYPTO_ALG_TYPE_MASK);\n\tif (err)\n\t\tgoto out_free_inst;\n\n\talignmask = alg->cra_alignmask | (sizeof(long) - 1);\n\tinst->alg.base.cra_alignmask = alignmask;\n\tinst->alg.base.cra_priority = alg->cra_priority;\n\tinst->alg.base.cra_blocksize = alg->cra_blocksize;\n\n\tinst->alg.digestsize = alg->cra_blocksize;\n\tinst->alg.descsize =\n\t\tALIGN(sizeof(struct cmac_desc_ctx), crypto_tfm_ctx_alignment())\n\t\t+ (alignmask & ~(crypto_tfm_ctx_alignment() - 1))\n\t\t+ alg->cra_blocksize * 2;\n\n\tinst->alg.base.cra_ctxsize =\n\t\tALIGN(sizeof(struct cmac_tfm_ctx), alignmask + 1)\n\t\t+ alg->cra_blocksize * 2;\n\n\tinst->alg.base.cra_init = cmac_init_tfm;\n\tinst->alg.base.cra_exit = cmac_exit_tfm;\n\n\tinst->alg.init = crypto_cmac_digest_init;\n\tinst->alg.update = crypto_cmac_digest_update;\n\tinst->alg.final = crypto_cmac_digest_final;\n\tinst->alg.setkey = crypto_cmac_digest_setkey;\n\n\terr = shash_register_instance(tmpl, inst);\n\tif (err) {\nout_free_inst:\n\t\tshash_free_instance(shash_crypto_instance(inst));\n\t}\n\nout_put_alg:\n\tcrypto_mod_put(alg);\n\treturn err;\n}\n\nstatic struct crypto_template crypto_cmac_tmpl = {\n\t.name = \"cmac\",\n\t.create = cmac_create,\n\t.free = shash_free_instance,\n\t.module = THIS_MODULE,\n};\n\nstatic int __init crypto_cmac_module_init(void)\n{\n\treturn crypto_register_template(&crypto_cmac_tmpl);\n}\n\nstatic void __exit crypto_cmac_module_exit(void)\n{\n\tcrypto_unregister_template(&crypto_cmac_tmpl);\n}\n\nmodule_init(crypto_cmac_module_init);\nmodule_exit(crypto_cmac_module_exit);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"CMAC keyed hash algorithm\");\nMODULE_ALIAS_CRYPTO(\"cmac\");\n", "/*\n * Software async crypto daemon.\n *\n * Copyright (c) 2006 Herbert Xu <herbert@gondor.apana.org.au>\n *\n * Added AEAD support to cryptd.\n *    Authors: Tadeusz Struk (tadeusz.struk@intel.com)\n *             Adrian Hoban <adrian.hoban@intel.com>\n *             Gabriele Paoloni <gabriele.paoloni@intel.com>\n *             Aidan O'Mahony (aidan.o.mahony@intel.com)\n *    Copyright (c) 2010, Intel Corporation.\n *\n * This program is free software; you can redistribute it and/or modify it\n * under the terms of the GNU General Public License as published by the Free\n * Software Foundation; either version 2 of the License, or (at your option)\n * any later version.\n *\n */\n\n#include <crypto/algapi.h>\n#include <crypto/internal/hash.h>\n#include <crypto/internal/aead.h>\n#include <crypto/cryptd.h>\n#include <crypto/crypto_wq.h>\n#include <linux/err.h>\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/list.h>\n#include <linux/module.h>\n#include <linux/scatterlist.h>\n#include <linux/sched.h>\n#include <linux/slab.h>\n\n#define CRYPTD_MAX_CPU_QLEN 100\n\nstruct cryptd_cpu_queue {\n\tstruct crypto_queue queue;\n\tstruct work_struct work;\n};\n\nstruct cryptd_queue {\n\tstruct cryptd_cpu_queue __percpu *cpu_queue;\n};\n\nstruct cryptd_instance_ctx {\n\tstruct crypto_spawn spawn;\n\tstruct cryptd_queue *queue;\n};\n\nstruct hashd_instance_ctx {\n\tstruct crypto_shash_spawn spawn;\n\tstruct cryptd_queue *queue;\n};\n\nstruct aead_instance_ctx {\n\tstruct crypto_aead_spawn aead_spawn;\n\tstruct cryptd_queue *queue;\n};\n\nstruct cryptd_blkcipher_ctx {\n\tstruct crypto_blkcipher *child;\n};\n\nstruct cryptd_blkcipher_request_ctx {\n\tcrypto_completion_t complete;\n};\n\nstruct cryptd_hash_ctx {\n\tstruct crypto_shash *child;\n};\n\nstruct cryptd_hash_request_ctx {\n\tcrypto_completion_t complete;\n\tstruct shash_desc desc;\n};\n\nstruct cryptd_aead_ctx {\n\tstruct crypto_aead *child;\n};\n\nstruct cryptd_aead_request_ctx {\n\tcrypto_completion_t complete;\n};\n\nstatic void cryptd_queue_worker(struct work_struct *work);\n\nstatic int cryptd_init_queue(struct cryptd_queue *queue,\n\t\t\t     unsigned int max_cpu_qlen)\n{\n\tint cpu;\n\tstruct cryptd_cpu_queue *cpu_queue;\n\n\tqueue->cpu_queue = alloc_percpu(struct cryptd_cpu_queue);\n\tif (!queue->cpu_queue)\n\t\treturn -ENOMEM;\n\tfor_each_possible_cpu(cpu) {\n\t\tcpu_queue = per_cpu_ptr(queue->cpu_queue, cpu);\n\t\tcrypto_init_queue(&cpu_queue->queue, max_cpu_qlen);\n\t\tINIT_WORK(&cpu_queue->work, cryptd_queue_worker);\n\t}\n\treturn 0;\n}\n\nstatic void cryptd_fini_queue(struct cryptd_queue *queue)\n{\n\tint cpu;\n\tstruct cryptd_cpu_queue *cpu_queue;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tcpu_queue = per_cpu_ptr(queue->cpu_queue, cpu);\n\t\tBUG_ON(cpu_queue->queue.qlen);\n\t}\n\tfree_percpu(queue->cpu_queue);\n}\n\nstatic int cryptd_enqueue_request(struct cryptd_queue *queue,\n\t\t\t\t  struct crypto_async_request *request)\n{\n\tint cpu, err;\n\tstruct cryptd_cpu_queue *cpu_queue;\n\n\tcpu = get_cpu();\n\tcpu_queue = this_cpu_ptr(queue->cpu_queue);\n\terr = crypto_enqueue_request(&cpu_queue->queue, request);\n\tqueue_work_on(cpu, kcrypto_wq, &cpu_queue->work);\n\tput_cpu();\n\n\treturn err;\n}\n\n/* Called in workqueue context, do one real cryption work (via\n * req->complete) and reschedule itself if there are more work to\n * do. */\nstatic void cryptd_queue_worker(struct work_struct *work)\n{\n\tstruct cryptd_cpu_queue *cpu_queue;\n\tstruct crypto_async_request *req, *backlog;\n\n\tcpu_queue = container_of(work, struct cryptd_cpu_queue, work);\n\t/*\n\t * Only handle one request at a time to avoid hogging crypto workqueue.\n\t * preempt_disable/enable is used to prevent being preempted by\n\t * cryptd_enqueue_request(). local_bh_disable/enable is used to prevent\n\t * cryptd_enqueue_request() being accessed from software interrupts.\n\t */\n\tlocal_bh_disable();\n\tpreempt_disable();\n\tbacklog = crypto_get_backlog(&cpu_queue->queue);\n\treq = crypto_dequeue_request(&cpu_queue->queue);\n\tpreempt_enable();\n\tlocal_bh_enable();\n\n\tif (!req)\n\t\treturn;\n\n\tif (backlog)\n\t\tbacklog->complete(backlog, -EINPROGRESS);\n\treq->complete(req, 0);\n\n\tif (cpu_queue->queue.qlen)\n\t\tqueue_work(kcrypto_wq, &cpu_queue->work);\n}\n\nstatic inline struct cryptd_queue *cryptd_get_queue(struct crypto_tfm *tfm)\n{\n\tstruct crypto_instance *inst = crypto_tfm_alg_instance(tfm);\n\tstruct cryptd_instance_ctx *ictx = crypto_instance_ctx(inst);\n\treturn ictx->queue;\n}\n\nstatic int cryptd_blkcipher_setkey(struct crypto_ablkcipher *parent,\n\t\t\t\t   const u8 *key, unsigned int keylen)\n{\n\tstruct cryptd_blkcipher_ctx *ctx = crypto_ablkcipher_ctx(parent);\n\tstruct crypto_blkcipher *child = ctx->child;\n\tint err;\n\n\tcrypto_blkcipher_clear_flags(child, CRYPTO_TFM_REQ_MASK);\n\tcrypto_blkcipher_set_flags(child, crypto_ablkcipher_get_flags(parent) &\n\t\t\t\t\t  CRYPTO_TFM_REQ_MASK);\n\terr = crypto_blkcipher_setkey(child, key, keylen);\n\tcrypto_ablkcipher_set_flags(parent, crypto_blkcipher_get_flags(child) &\n\t\t\t\t\t    CRYPTO_TFM_RES_MASK);\n\treturn err;\n}\n\nstatic void cryptd_blkcipher_crypt(struct ablkcipher_request *req,\n\t\t\t\t   struct crypto_blkcipher *child,\n\t\t\t\t   int err,\n\t\t\t\t   int (*crypt)(struct blkcipher_desc *desc,\n\t\t\t\t\t\tstruct scatterlist *dst,\n\t\t\t\t\t\tstruct scatterlist *src,\n\t\t\t\t\t\tunsigned int len))\n{\n\tstruct cryptd_blkcipher_request_ctx *rctx;\n\tstruct blkcipher_desc desc;\n\n\trctx = ablkcipher_request_ctx(req);\n\n\tif (unlikely(err == -EINPROGRESS))\n\t\tgoto out;\n\n\tdesc.tfm = child;\n\tdesc.info = req->info;\n\tdesc.flags = CRYPTO_TFM_REQ_MAY_SLEEP;\n\n\terr = crypt(&desc, req->dst, req->src, req->nbytes);\n\n\treq->base.complete = rctx->complete;\n\nout:\n\tlocal_bh_disable();\n\trctx->complete(&req->base, err);\n\tlocal_bh_enable();\n}\n\nstatic void cryptd_blkcipher_encrypt(struct crypto_async_request *req, int err)\n{\n\tstruct cryptd_blkcipher_ctx *ctx = crypto_tfm_ctx(req->tfm);\n\tstruct crypto_blkcipher *child = ctx->child;\n\n\tcryptd_blkcipher_crypt(ablkcipher_request_cast(req), child, err,\n\t\t\t       crypto_blkcipher_crt(child)->encrypt);\n}\n\nstatic void cryptd_blkcipher_decrypt(struct crypto_async_request *req, int err)\n{\n\tstruct cryptd_blkcipher_ctx *ctx = crypto_tfm_ctx(req->tfm);\n\tstruct crypto_blkcipher *child = ctx->child;\n\n\tcryptd_blkcipher_crypt(ablkcipher_request_cast(req), child, err,\n\t\t\t       crypto_blkcipher_crt(child)->decrypt);\n}\n\nstatic int cryptd_blkcipher_enqueue(struct ablkcipher_request *req,\n\t\t\t\t    crypto_completion_t compl)\n{\n\tstruct cryptd_blkcipher_request_ctx *rctx = ablkcipher_request_ctx(req);\n\tstruct crypto_ablkcipher *tfm = crypto_ablkcipher_reqtfm(req);\n\tstruct cryptd_queue *queue;\n\n\tqueue = cryptd_get_queue(crypto_ablkcipher_tfm(tfm));\n\trctx->complete = req->base.complete;\n\treq->base.complete = compl;\n\n\treturn cryptd_enqueue_request(queue, &req->base);\n}\n\nstatic int cryptd_blkcipher_encrypt_enqueue(struct ablkcipher_request *req)\n{\n\treturn cryptd_blkcipher_enqueue(req, cryptd_blkcipher_encrypt);\n}\n\nstatic int cryptd_blkcipher_decrypt_enqueue(struct ablkcipher_request *req)\n{\n\treturn cryptd_blkcipher_enqueue(req, cryptd_blkcipher_decrypt);\n}\n\nstatic int cryptd_blkcipher_init_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_instance *inst = crypto_tfm_alg_instance(tfm);\n\tstruct cryptd_instance_ctx *ictx = crypto_instance_ctx(inst);\n\tstruct crypto_spawn *spawn = &ictx->spawn;\n\tstruct cryptd_blkcipher_ctx *ctx = crypto_tfm_ctx(tfm);\n\tstruct crypto_blkcipher *cipher;\n\n\tcipher = crypto_spawn_blkcipher(spawn);\n\tif (IS_ERR(cipher))\n\t\treturn PTR_ERR(cipher);\n\n\tctx->child = cipher;\n\ttfm->crt_ablkcipher.reqsize =\n\t\tsizeof(struct cryptd_blkcipher_request_ctx);\n\treturn 0;\n}\n\nstatic void cryptd_blkcipher_exit_tfm(struct crypto_tfm *tfm)\n{\n\tstruct cryptd_blkcipher_ctx *ctx = crypto_tfm_ctx(tfm);\n\n\tcrypto_free_blkcipher(ctx->child);\n}\n\nstatic void *cryptd_alloc_instance(struct crypto_alg *alg, unsigned int head,\n\t\t\t\t   unsigned int tail)\n{\n\tchar *p;\n\tstruct crypto_instance *inst;\n\tint err;\n\n\tp = kzalloc(head + sizeof(*inst) + tail, GFP_KERNEL);\n\tif (!p)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tinst = (void *)(p + head);\n\n\terr = -ENAMETOOLONG;\n\tif (snprintf(inst->alg.cra_driver_name, CRYPTO_MAX_ALG_NAME,\n\t\t     \"cryptd(%s)\", alg->cra_driver_name) >= CRYPTO_MAX_ALG_NAME)\n\t\tgoto out_free_inst;\n\n\tmemcpy(inst->alg.cra_name, alg->cra_name, CRYPTO_MAX_ALG_NAME);\n\n\tinst->alg.cra_priority = alg->cra_priority + 50;\n\tinst->alg.cra_blocksize = alg->cra_blocksize;\n\tinst->alg.cra_alignmask = alg->cra_alignmask;\n\nout:\n\treturn p;\n\nout_free_inst:\n\tkfree(p);\n\tp = ERR_PTR(err);\n\tgoto out;\n}\n\nstatic int cryptd_create_blkcipher(struct crypto_template *tmpl,\n\t\t\t\t   struct rtattr **tb,\n\t\t\t\t   struct cryptd_queue *queue)\n{\n\tstruct cryptd_instance_ctx *ctx;\n\tstruct crypto_instance *inst;\n\tstruct crypto_alg *alg;\n\tint err;\n\n\talg = crypto_get_attr_alg(tb, CRYPTO_ALG_TYPE_BLKCIPHER,\n\t\t\t\t  CRYPTO_ALG_TYPE_MASK);\n\tif (IS_ERR(alg))\n\t\treturn PTR_ERR(alg);\n\n\tinst = cryptd_alloc_instance(alg, 0, sizeof(*ctx));\n\terr = PTR_ERR(inst);\n\tif (IS_ERR(inst))\n\t\tgoto out_put_alg;\n\n\tctx = crypto_instance_ctx(inst);\n\tctx->queue = queue;\n\n\terr = crypto_init_spawn(&ctx->spawn, alg, inst,\n\t\t\t\tCRYPTO_ALG_TYPE_MASK | CRYPTO_ALG_ASYNC);\n\tif (err)\n\t\tgoto out_free_inst;\n\n\tinst->alg.cra_flags = CRYPTO_ALG_TYPE_ABLKCIPHER | CRYPTO_ALG_ASYNC;\n\tinst->alg.cra_type = &crypto_ablkcipher_type;\n\n\tinst->alg.cra_ablkcipher.ivsize = alg->cra_blkcipher.ivsize;\n\tinst->alg.cra_ablkcipher.min_keysize = alg->cra_blkcipher.min_keysize;\n\tinst->alg.cra_ablkcipher.max_keysize = alg->cra_blkcipher.max_keysize;\n\n\tinst->alg.cra_ablkcipher.geniv = alg->cra_blkcipher.geniv;\n\n\tinst->alg.cra_ctxsize = sizeof(struct cryptd_blkcipher_ctx);\n\n\tinst->alg.cra_init = cryptd_blkcipher_init_tfm;\n\tinst->alg.cra_exit = cryptd_blkcipher_exit_tfm;\n\n\tinst->alg.cra_ablkcipher.setkey = cryptd_blkcipher_setkey;\n\tinst->alg.cra_ablkcipher.encrypt = cryptd_blkcipher_encrypt_enqueue;\n\tinst->alg.cra_ablkcipher.decrypt = cryptd_blkcipher_decrypt_enqueue;\n\n\terr = crypto_register_instance(tmpl, inst);\n\tif (err) {\n\t\tcrypto_drop_spawn(&ctx->spawn);\nout_free_inst:\n\t\tkfree(inst);\n\t}\n\nout_put_alg:\n\tcrypto_mod_put(alg);\n\treturn err;\n}\n\nstatic int cryptd_hash_init_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_instance *inst = crypto_tfm_alg_instance(tfm);\n\tstruct hashd_instance_ctx *ictx = crypto_instance_ctx(inst);\n\tstruct crypto_shash_spawn *spawn = &ictx->spawn;\n\tstruct cryptd_hash_ctx *ctx = crypto_tfm_ctx(tfm);\n\tstruct crypto_shash *hash;\n\n\thash = crypto_spawn_shash(spawn);\n\tif (IS_ERR(hash))\n\t\treturn PTR_ERR(hash);\n\n\tctx->child = hash;\n\tcrypto_ahash_set_reqsize(__crypto_ahash_cast(tfm),\n\t\t\t\t sizeof(struct cryptd_hash_request_ctx) +\n\t\t\t\t crypto_shash_descsize(hash));\n\treturn 0;\n}\n\nstatic void cryptd_hash_exit_tfm(struct crypto_tfm *tfm)\n{\n\tstruct cryptd_hash_ctx *ctx = crypto_tfm_ctx(tfm);\n\n\tcrypto_free_shash(ctx->child);\n}\n\nstatic int cryptd_hash_setkey(struct crypto_ahash *parent,\n\t\t\t\t   const u8 *key, unsigned int keylen)\n{\n\tstruct cryptd_hash_ctx *ctx   = crypto_ahash_ctx(parent);\n\tstruct crypto_shash *child = ctx->child;\n\tint err;\n\n\tcrypto_shash_clear_flags(child, CRYPTO_TFM_REQ_MASK);\n\tcrypto_shash_set_flags(child, crypto_ahash_get_flags(parent) &\n\t\t\t\t      CRYPTO_TFM_REQ_MASK);\n\terr = crypto_shash_setkey(child, key, keylen);\n\tcrypto_ahash_set_flags(parent, crypto_shash_get_flags(child) &\n\t\t\t\t       CRYPTO_TFM_RES_MASK);\n\treturn err;\n}\n\nstatic int cryptd_hash_enqueue(struct ahash_request *req,\n\t\t\t\tcrypto_completion_t compl)\n{\n\tstruct cryptd_hash_request_ctx *rctx = ahash_request_ctx(req);\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct cryptd_queue *queue =\n\t\tcryptd_get_queue(crypto_ahash_tfm(tfm));\n\n\trctx->complete = req->base.complete;\n\treq->base.complete = compl;\n\n\treturn cryptd_enqueue_request(queue, &req->base);\n}\n\nstatic void cryptd_hash_init(struct crypto_async_request *req_async, int err)\n{\n\tstruct cryptd_hash_ctx *ctx = crypto_tfm_ctx(req_async->tfm);\n\tstruct crypto_shash *child = ctx->child;\n\tstruct ahash_request *req = ahash_request_cast(req_async);\n\tstruct cryptd_hash_request_ctx *rctx = ahash_request_ctx(req);\n\tstruct shash_desc *desc = &rctx->desc;\n\n\tif (unlikely(err == -EINPROGRESS))\n\t\tgoto out;\n\n\tdesc->tfm = child;\n\tdesc->flags = CRYPTO_TFM_REQ_MAY_SLEEP;\n\n\terr = crypto_shash_init(desc);\n\n\treq->base.complete = rctx->complete;\n\nout:\n\tlocal_bh_disable();\n\trctx->complete(&req->base, err);\n\tlocal_bh_enable();\n}\n\nstatic int cryptd_hash_init_enqueue(struct ahash_request *req)\n{\n\treturn cryptd_hash_enqueue(req, cryptd_hash_init);\n}\n\nstatic void cryptd_hash_update(struct crypto_async_request *req_async, int err)\n{\n\tstruct ahash_request *req = ahash_request_cast(req_async);\n\tstruct cryptd_hash_request_ctx *rctx;\n\n\trctx = ahash_request_ctx(req);\n\n\tif (unlikely(err == -EINPROGRESS))\n\t\tgoto out;\n\n\terr = shash_ahash_update(req, &rctx->desc);\n\n\treq->base.complete = rctx->complete;\n\nout:\n\tlocal_bh_disable();\n\trctx->complete(&req->base, err);\n\tlocal_bh_enable();\n}\n\nstatic int cryptd_hash_update_enqueue(struct ahash_request *req)\n{\n\treturn cryptd_hash_enqueue(req, cryptd_hash_update);\n}\n\nstatic void cryptd_hash_final(struct crypto_async_request *req_async, int err)\n{\n\tstruct ahash_request *req = ahash_request_cast(req_async);\n\tstruct cryptd_hash_request_ctx *rctx = ahash_request_ctx(req);\n\n\tif (unlikely(err == -EINPROGRESS))\n\t\tgoto out;\n\n\terr = crypto_shash_final(&rctx->desc, req->result);\n\n\treq->base.complete = rctx->complete;\n\nout:\n\tlocal_bh_disable();\n\trctx->complete(&req->base, err);\n\tlocal_bh_enable();\n}\n\nstatic int cryptd_hash_final_enqueue(struct ahash_request *req)\n{\n\treturn cryptd_hash_enqueue(req, cryptd_hash_final);\n}\n\nstatic void cryptd_hash_finup(struct crypto_async_request *req_async, int err)\n{\n\tstruct ahash_request *req = ahash_request_cast(req_async);\n\tstruct cryptd_hash_request_ctx *rctx = ahash_request_ctx(req);\n\n\tif (unlikely(err == -EINPROGRESS))\n\t\tgoto out;\n\n\terr = shash_ahash_finup(req, &rctx->desc);\n\n\treq->base.complete = rctx->complete;\n\nout:\n\tlocal_bh_disable();\n\trctx->complete(&req->base, err);\n\tlocal_bh_enable();\n}\n\nstatic int cryptd_hash_finup_enqueue(struct ahash_request *req)\n{\n\treturn cryptd_hash_enqueue(req, cryptd_hash_finup);\n}\n\nstatic void cryptd_hash_digest(struct crypto_async_request *req_async, int err)\n{\n\tstruct cryptd_hash_ctx *ctx = crypto_tfm_ctx(req_async->tfm);\n\tstruct crypto_shash *child = ctx->child;\n\tstruct ahash_request *req = ahash_request_cast(req_async);\n\tstruct cryptd_hash_request_ctx *rctx = ahash_request_ctx(req);\n\tstruct shash_desc *desc = &rctx->desc;\n\n\tif (unlikely(err == -EINPROGRESS))\n\t\tgoto out;\n\n\tdesc->tfm = child;\n\tdesc->flags = CRYPTO_TFM_REQ_MAY_SLEEP;\n\n\terr = shash_ahash_digest(req, desc);\n\n\treq->base.complete = rctx->complete;\n\nout:\n\tlocal_bh_disable();\n\trctx->complete(&req->base, err);\n\tlocal_bh_enable();\n}\n\nstatic int cryptd_hash_digest_enqueue(struct ahash_request *req)\n{\n\treturn cryptd_hash_enqueue(req, cryptd_hash_digest);\n}\n\nstatic int cryptd_hash_export(struct ahash_request *req, void *out)\n{\n\tstruct cryptd_hash_request_ctx *rctx = ahash_request_ctx(req);\n\n\treturn crypto_shash_export(&rctx->desc, out);\n}\n\nstatic int cryptd_hash_import(struct ahash_request *req, const void *in)\n{\n\tstruct cryptd_hash_request_ctx *rctx = ahash_request_ctx(req);\n\n\treturn crypto_shash_import(&rctx->desc, in);\n}\n\nstatic int cryptd_create_hash(struct crypto_template *tmpl, struct rtattr **tb,\n\t\t\t      struct cryptd_queue *queue)\n{\n\tstruct hashd_instance_ctx *ctx;\n\tstruct ahash_instance *inst;\n\tstruct shash_alg *salg;\n\tstruct crypto_alg *alg;\n\tint err;\n\n\tsalg = shash_attr_alg(tb[1], 0, 0);\n\tif (IS_ERR(salg))\n\t\treturn PTR_ERR(salg);\n\n\talg = &salg->base;\n\tinst = cryptd_alloc_instance(alg, ahash_instance_headroom(),\n\t\t\t\t     sizeof(*ctx));\n\terr = PTR_ERR(inst);\n\tif (IS_ERR(inst))\n\t\tgoto out_put_alg;\n\n\tctx = ahash_instance_ctx(inst);\n\tctx->queue = queue;\n\n\terr = crypto_init_shash_spawn(&ctx->spawn, salg,\n\t\t\t\t      ahash_crypto_instance(inst));\n\tif (err)\n\t\tgoto out_free_inst;\n\n\tinst->alg.halg.base.cra_flags = CRYPTO_ALG_ASYNC;\n\n\tinst->alg.halg.digestsize = salg->digestsize;\n\tinst->alg.halg.base.cra_ctxsize = sizeof(struct cryptd_hash_ctx);\n\n\tinst->alg.halg.base.cra_init = cryptd_hash_init_tfm;\n\tinst->alg.halg.base.cra_exit = cryptd_hash_exit_tfm;\n\n\tinst->alg.init   = cryptd_hash_init_enqueue;\n\tinst->alg.update = cryptd_hash_update_enqueue;\n\tinst->alg.final  = cryptd_hash_final_enqueue;\n\tinst->alg.finup  = cryptd_hash_finup_enqueue;\n\tinst->alg.export = cryptd_hash_export;\n\tinst->alg.import = cryptd_hash_import;\n\tinst->alg.setkey = cryptd_hash_setkey;\n\tinst->alg.digest = cryptd_hash_digest_enqueue;\n\n\terr = ahash_register_instance(tmpl, inst);\n\tif (err) {\n\t\tcrypto_drop_shash(&ctx->spawn);\nout_free_inst:\n\t\tkfree(inst);\n\t}\n\nout_put_alg:\n\tcrypto_mod_put(alg);\n\treturn err;\n}\n\nstatic void cryptd_aead_crypt(struct aead_request *req,\n\t\t\tstruct crypto_aead *child,\n\t\t\tint err,\n\t\t\tint (*crypt)(struct aead_request *req))\n{\n\tstruct cryptd_aead_request_ctx *rctx;\n\trctx = aead_request_ctx(req);\n\n\tif (unlikely(err == -EINPROGRESS))\n\t\tgoto out;\n\taead_request_set_tfm(req, child);\n\terr = crypt( req );\n\treq->base.complete = rctx->complete;\nout:\n\tlocal_bh_disable();\n\trctx->complete(&req->base, err);\n\tlocal_bh_enable();\n}\n\nstatic void cryptd_aead_encrypt(struct crypto_async_request *areq, int err)\n{\n\tstruct cryptd_aead_ctx *ctx = crypto_tfm_ctx(areq->tfm);\n\tstruct crypto_aead *child = ctx->child;\n\tstruct aead_request *req;\n\n\treq = container_of(areq, struct aead_request, base);\n\tcryptd_aead_crypt(req, child, err, crypto_aead_crt(child)->encrypt);\n}\n\nstatic void cryptd_aead_decrypt(struct crypto_async_request *areq, int err)\n{\n\tstruct cryptd_aead_ctx *ctx = crypto_tfm_ctx(areq->tfm);\n\tstruct crypto_aead *child = ctx->child;\n\tstruct aead_request *req;\n\n\treq = container_of(areq, struct aead_request, base);\n\tcryptd_aead_crypt(req, child, err, crypto_aead_crt(child)->decrypt);\n}\n\nstatic int cryptd_aead_enqueue(struct aead_request *req,\n\t\t\t\t    crypto_completion_t compl)\n{\n\tstruct cryptd_aead_request_ctx *rctx = aead_request_ctx(req);\n\tstruct crypto_aead *tfm = crypto_aead_reqtfm(req);\n\tstruct cryptd_queue *queue = cryptd_get_queue(crypto_aead_tfm(tfm));\n\n\trctx->complete = req->base.complete;\n\treq->base.complete = compl;\n\treturn cryptd_enqueue_request(queue, &req->base);\n}\n\nstatic int cryptd_aead_encrypt_enqueue(struct aead_request *req)\n{\n\treturn cryptd_aead_enqueue(req, cryptd_aead_encrypt );\n}\n\nstatic int cryptd_aead_decrypt_enqueue(struct aead_request *req)\n{\n\treturn cryptd_aead_enqueue(req, cryptd_aead_decrypt );\n}\n\nstatic int cryptd_aead_init_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_instance *inst = crypto_tfm_alg_instance(tfm);\n\tstruct aead_instance_ctx *ictx = crypto_instance_ctx(inst);\n\tstruct crypto_aead_spawn *spawn = &ictx->aead_spawn;\n\tstruct cryptd_aead_ctx *ctx = crypto_tfm_ctx(tfm);\n\tstruct crypto_aead *cipher;\n\n\tcipher = crypto_spawn_aead(spawn);\n\tif (IS_ERR(cipher))\n\t\treturn PTR_ERR(cipher);\n\n\tcrypto_aead_set_flags(cipher, CRYPTO_TFM_REQ_MAY_SLEEP);\n\tctx->child = cipher;\n\ttfm->crt_aead.reqsize = sizeof(struct cryptd_aead_request_ctx);\n\treturn 0;\n}\n\nstatic void cryptd_aead_exit_tfm(struct crypto_tfm *tfm)\n{\n\tstruct cryptd_aead_ctx *ctx = crypto_tfm_ctx(tfm);\n\tcrypto_free_aead(ctx->child);\n}\n\nstatic int cryptd_create_aead(struct crypto_template *tmpl,\n\t\t              struct rtattr **tb,\n\t\t\t      struct cryptd_queue *queue)\n{\n\tstruct aead_instance_ctx *ctx;\n\tstruct crypto_instance *inst;\n\tstruct crypto_alg *alg;\n\tint err;\n\n\talg = crypto_get_attr_alg(tb, CRYPTO_ALG_TYPE_AEAD,\n\t\t\t\tCRYPTO_ALG_TYPE_MASK);\n        if (IS_ERR(alg))\n\t\treturn PTR_ERR(alg);\n\n\tinst = cryptd_alloc_instance(alg, 0, sizeof(*ctx));\n\terr = PTR_ERR(inst);\n\tif (IS_ERR(inst))\n\t\tgoto out_put_alg;\n\n\tctx = crypto_instance_ctx(inst);\n\tctx->queue = queue;\n\n\terr = crypto_init_spawn(&ctx->aead_spawn.base, alg, inst,\n\t\t\tCRYPTO_ALG_TYPE_MASK | CRYPTO_ALG_ASYNC);\n\tif (err)\n\t\tgoto out_free_inst;\n\n\tinst->alg.cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC;\n\tinst->alg.cra_type = alg->cra_type;\n\tinst->alg.cra_ctxsize = sizeof(struct cryptd_aead_ctx);\n\tinst->alg.cra_init = cryptd_aead_init_tfm;\n\tinst->alg.cra_exit = cryptd_aead_exit_tfm;\n\tinst->alg.cra_aead.setkey      = alg->cra_aead.setkey;\n\tinst->alg.cra_aead.setauthsize = alg->cra_aead.setauthsize;\n\tinst->alg.cra_aead.geniv       = alg->cra_aead.geniv;\n\tinst->alg.cra_aead.ivsize      = alg->cra_aead.ivsize;\n\tinst->alg.cra_aead.maxauthsize = alg->cra_aead.maxauthsize;\n\tinst->alg.cra_aead.encrypt     = cryptd_aead_encrypt_enqueue;\n\tinst->alg.cra_aead.decrypt     = cryptd_aead_decrypt_enqueue;\n\tinst->alg.cra_aead.givencrypt  = alg->cra_aead.givencrypt;\n\tinst->alg.cra_aead.givdecrypt  = alg->cra_aead.givdecrypt;\n\n\terr = crypto_register_instance(tmpl, inst);\n\tif (err) {\n\t\tcrypto_drop_spawn(&ctx->aead_spawn.base);\nout_free_inst:\n\t\tkfree(inst);\n\t}\nout_put_alg:\n\tcrypto_mod_put(alg);\n\treturn err;\n}\n\nstatic struct cryptd_queue queue;\n\nstatic int cryptd_create(struct crypto_template *tmpl, struct rtattr **tb)\n{\n\tstruct crypto_attr_type *algt;\n\n\talgt = crypto_get_attr_type(tb);\n\tif (IS_ERR(algt))\n\t\treturn PTR_ERR(algt);\n\n\tswitch (algt->type & algt->mask & CRYPTO_ALG_TYPE_MASK) {\n\tcase CRYPTO_ALG_TYPE_BLKCIPHER:\n\t\treturn cryptd_create_blkcipher(tmpl, tb, &queue);\n\tcase CRYPTO_ALG_TYPE_DIGEST:\n\t\treturn cryptd_create_hash(tmpl, tb, &queue);\n\tcase CRYPTO_ALG_TYPE_AEAD:\n\t\treturn cryptd_create_aead(tmpl, tb, &queue);\n\t}\n\n\treturn -EINVAL;\n}\n\nstatic void cryptd_free(struct crypto_instance *inst)\n{\n\tstruct cryptd_instance_ctx *ctx = crypto_instance_ctx(inst);\n\tstruct hashd_instance_ctx *hctx = crypto_instance_ctx(inst);\n\tstruct aead_instance_ctx *aead_ctx = crypto_instance_ctx(inst);\n\n\tswitch (inst->alg.cra_flags & CRYPTO_ALG_TYPE_MASK) {\n\tcase CRYPTO_ALG_TYPE_AHASH:\n\t\tcrypto_drop_shash(&hctx->spawn);\n\t\tkfree(ahash_instance(inst));\n\t\treturn;\n\tcase CRYPTO_ALG_TYPE_AEAD:\n\t\tcrypto_drop_spawn(&aead_ctx->aead_spawn.base);\n\t\tkfree(inst);\n\t\treturn;\n\tdefault:\n\t\tcrypto_drop_spawn(&ctx->spawn);\n\t\tkfree(inst);\n\t}\n}\n\nstatic struct crypto_template cryptd_tmpl = {\n\t.name = \"cryptd\",\n\t.create = cryptd_create,\n\t.free = cryptd_free,\n\t.module = THIS_MODULE,\n};\n\nstruct cryptd_ablkcipher *cryptd_alloc_ablkcipher(const char *alg_name,\n\t\t\t\t\t\t  u32 type, u32 mask)\n{\n\tchar cryptd_alg_name[CRYPTO_MAX_ALG_NAME];\n\tstruct crypto_tfm *tfm;\n\n\tif (snprintf(cryptd_alg_name, CRYPTO_MAX_ALG_NAME,\n\t\t     \"cryptd(%s)\", alg_name) >= CRYPTO_MAX_ALG_NAME)\n\t\treturn ERR_PTR(-EINVAL);\n\ttype &= ~(CRYPTO_ALG_TYPE_MASK | CRYPTO_ALG_GENIV);\n\ttype |= CRYPTO_ALG_TYPE_BLKCIPHER;\n\tmask &= ~CRYPTO_ALG_TYPE_MASK;\n\tmask |= (CRYPTO_ALG_GENIV | CRYPTO_ALG_TYPE_BLKCIPHER_MASK);\n\ttfm = crypto_alloc_base(cryptd_alg_name, type, mask);\n\tif (IS_ERR(tfm))\n\t\treturn ERR_CAST(tfm);\n\tif (tfm->__crt_alg->cra_module != THIS_MODULE) {\n\t\tcrypto_free_tfm(tfm);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\treturn __cryptd_ablkcipher_cast(__crypto_ablkcipher_cast(tfm));\n}\nEXPORT_SYMBOL_GPL(cryptd_alloc_ablkcipher);\n\nstruct crypto_blkcipher *cryptd_ablkcipher_child(struct cryptd_ablkcipher *tfm)\n{\n\tstruct cryptd_blkcipher_ctx *ctx = crypto_ablkcipher_ctx(&tfm->base);\n\treturn ctx->child;\n}\nEXPORT_SYMBOL_GPL(cryptd_ablkcipher_child);\n\nvoid cryptd_free_ablkcipher(struct cryptd_ablkcipher *tfm)\n{\n\tcrypto_free_ablkcipher(&tfm->base);\n}\nEXPORT_SYMBOL_GPL(cryptd_free_ablkcipher);\n\nstruct cryptd_ahash *cryptd_alloc_ahash(const char *alg_name,\n\t\t\t\t\tu32 type, u32 mask)\n{\n\tchar cryptd_alg_name[CRYPTO_MAX_ALG_NAME];\n\tstruct crypto_ahash *tfm;\n\n\tif (snprintf(cryptd_alg_name, CRYPTO_MAX_ALG_NAME,\n\t\t     \"cryptd(%s)\", alg_name) >= CRYPTO_MAX_ALG_NAME)\n\t\treturn ERR_PTR(-EINVAL);\n\ttfm = crypto_alloc_ahash(cryptd_alg_name, type, mask);\n\tif (IS_ERR(tfm))\n\t\treturn ERR_CAST(tfm);\n\tif (tfm->base.__crt_alg->cra_module != THIS_MODULE) {\n\t\tcrypto_free_ahash(tfm);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\treturn __cryptd_ahash_cast(tfm);\n}\nEXPORT_SYMBOL_GPL(cryptd_alloc_ahash);\n\nstruct crypto_shash *cryptd_ahash_child(struct cryptd_ahash *tfm)\n{\n\tstruct cryptd_hash_ctx *ctx = crypto_ahash_ctx(&tfm->base);\n\n\treturn ctx->child;\n}\nEXPORT_SYMBOL_GPL(cryptd_ahash_child);\n\nstruct shash_desc *cryptd_shash_desc(struct ahash_request *req)\n{\n\tstruct cryptd_hash_request_ctx *rctx = ahash_request_ctx(req);\n\treturn &rctx->desc;\n}\nEXPORT_SYMBOL_GPL(cryptd_shash_desc);\n\nvoid cryptd_free_ahash(struct cryptd_ahash *tfm)\n{\n\tcrypto_free_ahash(&tfm->base);\n}\nEXPORT_SYMBOL_GPL(cryptd_free_ahash);\n\nstruct cryptd_aead *cryptd_alloc_aead(const char *alg_name,\n\t\t\t\t\t\t  u32 type, u32 mask)\n{\n\tchar cryptd_alg_name[CRYPTO_MAX_ALG_NAME];\n\tstruct crypto_aead *tfm;\n\n\tif (snprintf(cryptd_alg_name, CRYPTO_MAX_ALG_NAME,\n\t\t     \"cryptd(%s)\", alg_name) >= CRYPTO_MAX_ALG_NAME)\n\t\treturn ERR_PTR(-EINVAL);\n\ttfm = crypto_alloc_aead(cryptd_alg_name, type, mask);\n\tif (IS_ERR(tfm))\n\t\treturn ERR_CAST(tfm);\n\tif (tfm->base.__crt_alg->cra_module != THIS_MODULE) {\n\t\tcrypto_free_aead(tfm);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\treturn __cryptd_aead_cast(tfm);\n}\nEXPORT_SYMBOL_GPL(cryptd_alloc_aead);\n\nstruct crypto_aead *cryptd_aead_child(struct cryptd_aead *tfm)\n{\n\tstruct cryptd_aead_ctx *ctx;\n\tctx = crypto_aead_ctx(&tfm->base);\n\treturn ctx->child;\n}\nEXPORT_SYMBOL_GPL(cryptd_aead_child);\n\nvoid cryptd_free_aead(struct cryptd_aead *tfm)\n{\n\tcrypto_free_aead(&tfm->base);\n}\nEXPORT_SYMBOL_GPL(cryptd_free_aead);\n\nstatic int __init cryptd_init(void)\n{\n\tint err;\n\n\terr = cryptd_init_queue(&queue, CRYPTD_MAX_CPU_QLEN);\n\tif (err)\n\t\treturn err;\n\n\terr = crypto_register_template(&cryptd_tmpl);\n\tif (err)\n\t\tcryptd_fini_queue(&queue);\n\n\treturn err;\n}\n\nstatic void __exit cryptd_exit(void)\n{\n\tcryptd_fini_queue(&queue);\n\tcrypto_unregister_template(&cryptd_tmpl);\n}\n\nsubsys_initcall(cryptd_init);\nmodule_exit(cryptd_exit);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"Software async crypto daemon\");\nMODULE_ALIAS_CRYPTO(\"cryptd\");\n", "/*\n * CTR: Counter mode\n *\n * (C) Copyright IBM Corp. 2007 - Joy Latten <latten@us.ibm.com>\n *\n * This program is free software; you can redistribute it and/or modify it\n * under the terms of the GNU General Public License as published by the Free\n * Software Foundation; either version 2 of the License, or (at your option)\n * any later version.\n *\n */\n\n#include <crypto/algapi.h>\n#include <crypto/ctr.h>\n#include <crypto/internal/skcipher.h>\n#include <linux/err.h>\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/random.h>\n#include <linux/scatterlist.h>\n#include <linux/slab.h>\n\nstruct crypto_ctr_ctx {\n\tstruct crypto_cipher *child;\n};\n\nstruct crypto_rfc3686_ctx {\n\tstruct crypto_ablkcipher *child;\n\tu8 nonce[CTR_RFC3686_NONCE_SIZE];\n};\n\nstruct crypto_rfc3686_req_ctx {\n\tu8 iv[CTR_RFC3686_BLOCK_SIZE];\n\tstruct ablkcipher_request subreq CRYPTO_MINALIGN_ATTR;\n};\n\nstatic int crypto_ctr_setkey(struct crypto_tfm *parent, const u8 *key,\n\t\t\t     unsigned int keylen)\n{\n\tstruct crypto_ctr_ctx *ctx = crypto_tfm_ctx(parent);\n\tstruct crypto_cipher *child = ctx->child;\n\tint err;\n\n\tcrypto_cipher_clear_flags(child, CRYPTO_TFM_REQ_MASK);\n\tcrypto_cipher_set_flags(child, crypto_tfm_get_flags(parent) &\n\t\t\t\tCRYPTO_TFM_REQ_MASK);\n\terr = crypto_cipher_setkey(child, key, keylen);\n\tcrypto_tfm_set_flags(parent, crypto_cipher_get_flags(child) &\n\t\t\t     CRYPTO_TFM_RES_MASK);\n\n\treturn err;\n}\n\nstatic void crypto_ctr_crypt_final(struct blkcipher_walk *walk,\n\t\t\t\t   struct crypto_cipher *tfm)\n{\n\tunsigned int bsize = crypto_cipher_blocksize(tfm);\n\tunsigned long alignmask = crypto_cipher_alignmask(tfm);\n\tu8 *ctrblk = walk->iv;\n\tu8 tmp[bsize + alignmask];\n\tu8 *keystream = PTR_ALIGN(tmp + 0, alignmask + 1);\n\tu8 *src = walk->src.virt.addr;\n\tu8 *dst = walk->dst.virt.addr;\n\tunsigned int nbytes = walk->nbytes;\n\n\tcrypto_cipher_encrypt_one(tfm, keystream, ctrblk);\n\tcrypto_xor(keystream, src, nbytes);\n\tmemcpy(dst, keystream, nbytes);\n\n\tcrypto_inc(ctrblk, bsize);\n}\n\nstatic int crypto_ctr_crypt_segment(struct blkcipher_walk *walk,\n\t\t\t\t    struct crypto_cipher *tfm)\n{\n\tvoid (*fn)(struct crypto_tfm *, u8 *, const u8 *) =\n\t\t   crypto_cipher_alg(tfm)->cia_encrypt;\n\tunsigned int bsize = crypto_cipher_blocksize(tfm);\n\tu8 *ctrblk = walk->iv;\n\tu8 *src = walk->src.virt.addr;\n\tu8 *dst = walk->dst.virt.addr;\n\tunsigned int nbytes = walk->nbytes;\n\n\tdo {\n\t\t/* create keystream */\n\t\tfn(crypto_cipher_tfm(tfm), dst, ctrblk);\n\t\tcrypto_xor(dst, src, bsize);\n\n\t\t/* increment counter in counterblock */\n\t\tcrypto_inc(ctrblk, bsize);\n\n\t\tsrc += bsize;\n\t\tdst += bsize;\n\t} while ((nbytes -= bsize) >= bsize);\n\n\treturn nbytes;\n}\n\nstatic int crypto_ctr_crypt_inplace(struct blkcipher_walk *walk,\n\t\t\t\t    struct crypto_cipher *tfm)\n{\n\tvoid (*fn)(struct crypto_tfm *, u8 *, const u8 *) =\n\t\t   crypto_cipher_alg(tfm)->cia_encrypt;\n\tunsigned int bsize = crypto_cipher_blocksize(tfm);\n\tunsigned long alignmask = crypto_cipher_alignmask(tfm);\n\tunsigned int nbytes = walk->nbytes;\n\tu8 *ctrblk = walk->iv;\n\tu8 *src = walk->src.virt.addr;\n\tu8 tmp[bsize + alignmask];\n\tu8 *keystream = PTR_ALIGN(tmp + 0, alignmask + 1);\n\n\tdo {\n\t\t/* create keystream */\n\t\tfn(crypto_cipher_tfm(tfm), keystream, ctrblk);\n\t\tcrypto_xor(src, keystream, bsize);\n\n\t\t/* increment counter in counterblock */\n\t\tcrypto_inc(ctrblk, bsize);\n\n\t\tsrc += bsize;\n\t} while ((nbytes -= bsize) >= bsize);\n\n\treturn nbytes;\n}\n\nstatic int crypto_ctr_crypt(struct blkcipher_desc *desc,\n\t\t\t      struct scatterlist *dst, struct scatterlist *src,\n\t\t\t      unsigned int nbytes)\n{\n\tstruct blkcipher_walk walk;\n\tstruct crypto_blkcipher *tfm = desc->tfm;\n\tstruct crypto_ctr_ctx *ctx = crypto_blkcipher_ctx(tfm);\n\tstruct crypto_cipher *child = ctx->child;\n\tunsigned int bsize = crypto_cipher_blocksize(child);\n\tint err;\n\n\tblkcipher_walk_init(&walk, dst, src, nbytes);\n\terr = blkcipher_walk_virt_block(desc, &walk, bsize);\n\n\twhile (walk.nbytes >= bsize) {\n\t\tif (walk.src.virt.addr == walk.dst.virt.addr)\n\t\t\tnbytes = crypto_ctr_crypt_inplace(&walk, child);\n\t\telse\n\t\t\tnbytes = crypto_ctr_crypt_segment(&walk, child);\n\n\t\terr = blkcipher_walk_done(desc, &walk, nbytes);\n\t}\n\n\tif (walk.nbytes) {\n\t\tcrypto_ctr_crypt_final(&walk, child);\n\t\terr = blkcipher_walk_done(desc, &walk, 0);\n\t}\n\n\treturn err;\n}\n\nstatic int crypto_ctr_init_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_instance *inst = (void *)tfm->__crt_alg;\n\tstruct crypto_spawn *spawn = crypto_instance_ctx(inst);\n\tstruct crypto_ctr_ctx *ctx = crypto_tfm_ctx(tfm);\n\tstruct crypto_cipher *cipher;\n\n\tcipher = crypto_spawn_cipher(spawn);\n\tif (IS_ERR(cipher))\n\t\treturn PTR_ERR(cipher);\n\n\tctx->child = cipher;\n\n\treturn 0;\n}\n\nstatic void crypto_ctr_exit_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_ctr_ctx *ctx = crypto_tfm_ctx(tfm);\n\n\tcrypto_free_cipher(ctx->child);\n}\n\nstatic struct crypto_instance *crypto_ctr_alloc(struct rtattr **tb)\n{\n\tstruct crypto_instance *inst;\n\tstruct crypto_alg *alg;\n\tint err;\n\n\terr = crypto_check_attr_type(tb, CRYPTO_ALG_TYPE_BLKCIPHER);\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\talg = crypto_attr_alg(tb[1], CRYPTO_ALG_TYPE_CIPHER,\n\t\t\t\t  CRYPTO_ALG_TYPE_MASK);\n\tif (IS_ERR(alg))\n\t\treturn ERR_CAST(alg);\n\n\t/* Block size must be >= 4 bytes. */\n\terr = -EINVAL;\n\tif (alg->cra_blocksize < 4)\n\t\tgoto out_put_alg;\n\n\t/* If this is false we'd fail the alignment of crypto_inc. */\n\tif (alg->cra_blocksize % 4)\n\t\tgoto out_put_alg;\n\n\tinst = crypto_alloc_instance(\"ctr\", alg);\n\tif (IS_ERR(inst))\n\t\tgoto out;\n\n\tinst->alg.cra_flags = CRYPTO_ALG_TYPE_BLKCIPHER;\n\tinst->alg.cra_priority = alg->cra_priority;\n\tinst->alg.cra_blocksize = 1;\n\tinst->alg.cra_alignmask = alg->cra_alignmask | (__alignof__(u32) - 1);\n\tinst->alg.cra_type = &crypto_blkcipher_type;\n\n\tinst->alg.cra_blkcipher.ivsize = alg->cra_blocksize;\n\tinst->alg.cra_blkcipher.min_keysize = alg->cra_cipher.cia_min_keysize;\n\tinst->alg.cra_blkcipher.max_keysize = alg->cra_cipher.cia_max_keysize;\n\n\tinst->alg.cra_ctxsize = sizeof(struct crypto_ctr_ctx);\n\n\tinst->alg.cra_init = crypto_ctr_init_tfm;\n\tinst->alg.cra_exit = crypto_ctr_exit_tfm;\n\n\tinst->alg.cra_blkcipher.setkey = crypto_ctr_setkey;\n\tinst->alg.cra_blkcipher.encrypt = crypto_ctr_crypt;\n\tinst->alg.cra_blkcipher.decrypt = crypto_ctr_crypt;\n\n\tinst->alg.cra_blkcipher.geniv = \"chainiv\";\n\nout:\n\tcrypto_mod_put(alg);\n\treturn inst;\n\nout_put_alg:\n\tinst = ERR_PTR(err);\n\tgoto out;\n}\n\nstatic void crypto_ctr_free(struct crypto_instance *inst)\n{\n\tcrypto_drop_spawn(crypto_instance_ctx(inst));\n\tkfree(inst);\n}\n\nstatic struct crypto_template crypto_ctr_tmpl = {\n\t.name = \"ctr\",\n\t.alloc = crypto_ctr_alloc,\n\t.free = crypto_ctr_free,\n\t.module = THIS_MODULE,\n};\n\nstatic int crypto_rfc3686_setkey(struct crypto_ablkcipher *parent,\n\t\t\t\t const u8 *key, unsigned int keylen)\n{\n\tstruct crypto_rfc3686_ctx *ctx = crypto_ablkcipher_ctx(parent);\n\tstruct crypto_ablkcipher *child = ctx->child;\n\tint err;\n\n\t/* the nonce is stored in bytes at end of key */\n\tif (keylen < CTR_RFC3686_NONCE_SIZE)\n\t\treturn -EINVAL;\n\n\tmemcpy(ctx->nonce, key + (keylen - CTR_RFC3686_NONCE_SIZE),\n\t       CTR_RFC3686_NONCE_SIZE);\n\n\tkeylen -= CTR_RFC3686_NONCE_SIZE;\n\n\tcrypto_ablkcipher_clear_flags(child, CRYPTO_TFM_REQ_MASK);\n\tcrypto_ablkcipher_set_flags(child, crypto_ablkcipher_get_flags(parent) &\n\t\t\t\t    CRYPTO_TFM_REQ_MASK);\n\terr = crypto_ablkcipher_setkey(child, key, keylen);\n\tcrypto_ablkcipher_set_flags(parent, crypto_ablkcipher_get_flags(child) &\n\t\t\t\t    CRYPTO_TFM_RES_MASK);\n\n\treturn err;\n}\n\nstatic int crypto_rfc3686_crypt(struct ablkcipher_request *req)\n{\n\tstruct crypto_ablkcipher *tfm = crypto_ablkcipher_reqtfm(req);\n\tstruct crypto_rfc3686_ctx *ctx = crypto_ablkcipher_ctx(tfm);\n\tstruct crypto_ablkcipher *child = ctx->child;\n\tunsigned long align = crypto_ablkcipher_alignmask(tfm);\n\tstruct crypto_rfc3686_req_ctx *rctx =\n\t\t(void *)PTR_ALIGN((u8 *)ablkcipher_request_ctx(req), align + 1);\n\tstruct ablkcipher_request *subreq = &rctx->subreq;\n\tu8 *iv = rctx->iv;\n\n\t/* set up counter block */\n\tmemcpy(iv, ctx->nonce, CTR_RFC3686_NONCE_SIZE);\n\tmemcpy(iv + CTR_RFC3686_NONCE_SIZE, req->info, CTR_RFC3686_IV_SIZE);\n\n\t/* initialize counter portion of counter block */\n\t*(__be32 *)(iv + CTR_RFC3686_NONCE_SIZE + CTR_RFC3686_IV_SIZE) =\n\t\tcpu_to_be32(1);\n\n\tablkcipher_request_set_tfm(subreq, child);\n\tablkcipher_request_set_callback(subreq, req->base.flags,\n\t\t\t\t\treq->base.complete, req->base.data);\n\tablkcipher_request_set_crypt(subreq, req->src, req->dst, req->nbytes,\n\t\t\t\t     iv);\n\n\treturn crypto_ablkcipher_encrypt(subreq);\n}\n\nstatic int crypto_rfc3686_init_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_instance *inst = (void *)tfm->__crt_alg;\n\tstruct crypto_skcipher_spawn *spawn = crypto_instance_ctx(inst);\n\tstruct crypto_rfc3686_ctx *ctx = crypto_tfm_ctx(tfm);\n\tstruct crypto_ablkcipher *cipher;\n\tunsigned long align;\n\n\tcipher = crypto_spawn_skcipher(spawn);\n\tif (IS_ERR(cipher))\n\t\treturn PTR_ERR(cipher);\n\n\tctx->child = cipher;\n\n\talign = crypto_tfm_alg_alignmask(tfm);\n\talign &= ~(crypto_tfm_ctx_alignment() - 1);\n\ttfm->crt_ablkcipher.reqsize = align +\n\t\tsizeof(struct crypto_rfc3686_req_ctx) +\n\t\tcrypto_ablkcipher_reqsize(cipher);\n\n\treturn 0;\n}\n\nstatic void crypto_rfc3686_exit_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_rfc3686_ctx *ctx = crypto_tfm_ctx(tfm);\n\n\tcrypto_free_ablkcipher(ctx->child);\n}\n\nstatic struct crypto_instance *crypto_rfc3686_alloc(struct rtattr **tb)\n{\n\tstruct crypto_attr_type *algt;\n\tstruct crypto_instance *inst;\n\tstruct crypto_alg *alg;\n\tstruct crypto_skcipher_spawn *spawn;\n\tconst char *cipher_name;\n\tint err;\n\n\talgt = crypto_get_attr_type(tb);\n\tif (IS_ERR(algt))\n\t\treturn ERR_CAST(algt);\n\n\tif ((algt->type ^ CRYPTO_ALG_TYPE_BLKCIPHER) & algt->mask)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tcipher_name = crypto_attr_alg_name(tb[1]);\n\tif (IS_ERR(cipher_name))\n\t\treturn ERR_CAST(cipher_name);\n\n\tinst = kzalloc(sizeof(*inst) + sizeof(*spawn), GFP_KERNEL);\n\tif (!inst)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tspawn = crypto_instance_ctx(inst);\n\n\tcrypto_set_skcipher_spawn(spawn, inst);\n\terr = crypto_grab_skcipher(spawn, cipher_name, 0,\n\t\t\t\t   crypto_requires_sync(algt->type,\n\t\t\t\t\t\t\talgt->mask));\n\tif (err)\n\t\tgoto err_free_inst;\n\n\talg = crypto_skcipher_spawn_alg(spawn);\n\n\t/* We only support 16-byte blocks. */\n\terr = -EINVAL;\n\tif (alg->cra_ablkcipher.ivsize != CTR_RFC3686_BLOCK_SIZE)\n\t\tgoto err_drop_spawn;\n\n\t/* Not a stream cipher? */\n\tif (alg->cra_blocksize != 1)\n\t\tgoto err_drop_spawn;\n\n\terr = -ENAMETOOLONG;\n\tif (snprintf(inst->alg.cra_name, CRYPTO_MAX_ALG_NAME, \"rfc3686(%s)\",\n\t\t     alg->cra_name) >= CRYPTO_MAX_ALG_NAME)\n\t\tgoto err_drop_spawn;\n\tif (snprintf(inst->alg.cra_driver_name, CRYPTO_MAX_ALG_NAME,\n\t\t     \"rfc3686(%s)\", alg->cra_driver_name) >=\n\t\t\tCRYPTO_MAX_ALG_NAME)\n\t\tgoto err_drop_spawn;\n\n\tinst->alg.cra_priority = alg->cra_priority;\n\tinst->alg.cra_blocksize = 1;\n\tinst->alg.cra_alignmask = alg->cra_alignmask;\n\n\tinst->alg.cra_flags = CRYPTO_ALG_TYPE_ABLKCIPHER |\n\t\t\t      (alg->cra_flags & CRYPTO_ALG_ASYNC);\n\tinst->alg.cra_type = &crypto_ablkcipher_type;\n\n\tinst->alg.cra_ablkcipher.ivsize = CTR_RFC3686_IV_SIZE;\n\tinst->alg.cra_ablkcipher.min_keysize =\n\t\talg->cra_ablkcipher.min_keysize + CTR_RFC3686_NONCE_SIZE;\n\tinst->alg.cra_ablkcipher.max_keysize =\n\t\talg->cra_ablkcipher.max_keysize + CTR_RFC3686_NONCE_SIZE;\n\n\tinst->alg.cra_ablkcipher.geniv = \"seqiv\";\n\n\tinst->alg.cra_ablkcipher.setkey = crypto_rfc3686_setkey;\n\tinst->alg.cra_ablkcipher.encrypt = crypto_rfc3686_crypt;\n\tinst->alg.cra_ablkcipher.decrypt = crypto_rfc3686_crypt;\n\n\tinst->alg.cra_ctxsize = sizeof(struct crypto_rfc3686_ctx);\n\n\tinst->alg.cra_init = crypto_rfc3686_init_tfm;\n\tinst->alg.cra_exit = crypto_rfc3686_exit_tfm;\n\n\treturn inst;\n\nerr_drop_spawn:\n\tcrypto_drop_skcipher(spawn);\nerr_free_inst:\n\tkfree(inst);\n\treturn ERR_PTR(err);\n}\n\nstatic void crypto_rfc3686_free(struct crypto_instance *inst)\n{\n\tstruct crypto_skcipher_spawn *spawn = crypto_instance_ctx(inst);\n\n\tcrypto_drop_skcipher(spawn);\n\tkfree(inst);\n}\n\nstatic struct crypto_template crypto_rfc3686_tmpl = {\n\t.name = \"rfc3686\",\n\t.alloc = crypto_rfc3686_alloc,\n\t.free = crypto_rfc3686_free,\n\t.module = THIS_MODULE,\n};\n\nstatic int __init crypto_ctr_module_init(void)\n{\n\tint err;\n\n\terr = crypto_register_template(&crypto_ctr_tmpl);\n\tif (err)\n\t\tgoto out;\n\n\terr = crypto_register_template(&crypto_rfc3686_tmpl);\n\tif (err)\n\t\tgoto out_drop_ctr;\n\nout:\n\treturn err;\n\nout_drop_ctr:\n\tcrypto_unregister_template(&crypto_ctr_tmpl);\n\tgoto out;\n}\n\nstatic void __exit crypto_ctr_module_exit(void)\n{\n\tcrypto_unregister_template(&crypto_rfc3686_tmpl);\n\tcrypto_unregister_template(&crypto_ctr_tmpl);\n}\n\nmodule_init(crypto_ctr_module_init);\nmodule_exit(crypto_ctr_module_exit);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"CTR Counter block mode\");\nMODULE_ALIAS_CRYPTO(\"rfc3686\");\nMODULE_ALIAS_CRYPTO(\"ctr\");\n", "/*\n * CTS: Cipher Text Stealing mode\n *\n * COPYRIGHT (c) 2008\n * The Regents of the University of Michigan\n * ALL RIGHTS RESERVED\n *\n * Permission is granted to use, copy, create derivative works\n * and redistribute this software and such derivative works\n * for any purpose, so long as the name of The University of\n * Michigan is not used in any advertising or publicity\n * pertaining to the use of distribution of this software\n * without specific, written prior authorization.  If the\n * above copyright notice or any other identification of the\n * University of Michigan is included in any copy of any\n * portion of this software, then the disclaimer below must\n * also be included.\n *\n * THIS SOFTWARE IS PROVIDED AS IS, WITHOUT REPRESENTATION\n * FROM THE UNIVERSITY OF MICHIGAN AS TO ITS FITNESS FOR ANY\n * PURPOSE, AND WITHOUT WARRANTY BY THE UNIVERSITY OF\n * MICHIGAN OF ANY KIND, EITHER EXPRESS OR IMPLIED, INCLUDING\n * WITHOUT LIMITATION THE IMPLIED WARRANTIES OF\n * MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE\n * REGENTS OF THE UNIVERSITY OF MICHIGAN SHALL NOT BE LIABLE\n * FOR ANY DAMAGES, INCLUDING SPECIAL, INDIRECT, INCIDENTAL, OR\n * CONSEQUENTIAL DAMAGES, WITH RESPECT TO ANY CLAIM ARISING\n * OUT OF OR IN CONNECTION WITH THE USE OF THE SOFTWARE, EVEN\n * IF IT HAS BEEN OR IS HEREAFTER ADVISED OF THE POSSIBILITY OF\n * SUCH DAMAGES.\n */\n\n/* Derived from various:\n *\tCopyright (c) 2006 Herbert Xu <herbert@gondor.apana.org.au>\n */\n\n/*\n * This is the Cipher Text Stealing mode as described by\n * Section 8 of rfc2040 and referenced by rfc3962.\n * rfc3962 includes errata information in its Appendix A.\n */\n\n#include <crypto/algapi.h>\n#include <linux/err.h>\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/log2.h>\n#include <linux/module.h>\n#include <linux/scatterlist.h>\n#include <crypto/scatterwalk.h>\n#include <linux/slab.h>\n\nstruct crypto_cts_ctx {\n\tstruct crypto_blkcipher *child;\n};\n\nstatic int crypto_cts_setkey(struct crypto_tfm *parent, const u8 *key,\n\t\t\t     unsigned int keylen)\n{\n\tstruct crypto_cts_ctx *ctx = crypto_tfm_ctx(parent);\n\tstruct crypto_blkcipher *child = ctx->child;\n\tint err;\n\n\tcrypto_blkcipher_clear_flags(child, CRYPTO_TFM_REQ_MASK);\n\tcrypto_blkcipher_set_flags(child, crypto_tfm_get_flags(parent) &\n\t\t\t\t       CRYPTO_TFM_REQ_MASK);\n\terr = crypto_blkcipher_setkey(child, key, keylen);\n\tcrypto_tfm_set_flags(parent, crypto_blkcipher_get_flags(child) &\n\t\t\t\t     CRYPTO_TFM_RES_MASK);\n\treturn err;\n}\n\nstatic int cts_cbc_encrypt(struct crypto_cts_ctx *ctx,\n\t\t\t   struct blkcipher_desc *desc,\n\t\t\t   struct scatterlist *dst,\n\t\t\t   struct scatterlist *src,\n\t\t\t   unsigned int offset,\n\t\t\t   unsigned int nbytes)\n{\n\tint bsize = crypto_blkcipher_blocksize(desc->tfm);\n\tu8 tmp[bsize], tmp2[bsize];\n\tstruct blkcipher_desc lcldesc;\n\tstruct scatterlist sgsrc[1], sgdst[1];\n\tint lastn = nbytes - bsize;\n\tu8 iv[bsize];\n\tu8 s[bsize * 2], d[bsize * 2];\n\tint err;\n\n\tif (lastn < 0)\n\t\treturn -EINVAL;\n\n\tsg_init_table(sgsrc, 1);\n\tsg_init_table(sgdst, 1);\n\n\tmemset(s, 0, sizeof(s));\n\tscatterwalk_map_and_copy(s, src, offset, nbytes, 0);\n\n\tmemcpy(iv, desc->info, bsize);\n\n\tlcldesc.tfm = ctx->child;\n\tlcldesc.info = iv;\n\tlcldesc.flags = desc->flags;\n\n\tsg_set_buf(&sgsrc[0], s, bsize);\n\tsg_set_buf(&sgdst[0], tmp, bsize);\n\terr = crypto_blkcipher_encrypt_iv(&lcldesc, sgdst, sgsrc, bsize);\n\n\tmemcpy(d + bsize, tmp, lastn);\n\n\tlcldesc.info = tmp;\n\n\tsg_set_buf(&sgsrc[0], s + bsize, bsize);\n\tsg_set_buf(&sgdst[0], tmp2, bsize);\n\terr = crypto_blkcipher_encrypt_iv(&lcldesc, sgdst, sgsrc, bsize);\n\n\tmemcpy(d, tmp2, bsize);\n\n\tscatterwalk_map_and_copy(d, dst, offset, nbytes, 1);\n\n\tmemcpy(desc->info, tmp2, bsize);\n\n\treturn err;\n}\n\nstatic int crypto_cts_encrypt(struct blkcipher_desc *desc,\n\t\t\t      struct scatterlist *dst, struct scatterlist *src,\n\t\t\t      unsigned int nbytes)\n{\n\tstruct crypto_cts_ctx *ctx = crypto_blkcipher_ctx(desc->tfm);\n\tint bsize = crypto_blkcipher_blocksize(desc->tfm);\n\tint tot_blocks = (nbytes + bsize - 1) / bsize;\n\tint cbc_blocks = tot_blocks > 2 ? tot_blocks - 2 : 0;\n\tstruct blkcipher_desc lcldesc;\n\tint err;\n\n\tlcldesc.tfm = ctx->child;\n\tlcldesc.info = desc->info;\n\tlcldesc.flags = desc->flags;\n\n\tif (tot_blocks == 1) {\n\t\terr = crypto_blkcipher_encrypt_iv(&lcldesc, dst, src, bsize);\n\t} else if (nbytes <= bsize * 2) {\n\t\terr = cts_cbc_encrypt(ctx, desc, dst, src, 0, nbytes);\n\t} else {\n\t\t/* do normal function for tot_blocks - 2 */\n\t\terr = crypto_blkcipher_encrypt_iv(&lcldesc, dst, src,\n\t\t\t\t\t\t\tcbc_blocks * bsize);\n\t\tif (err == 0) {\n\t\t\t/* do cts for final two blocks */\n\t\t\terr = cts_cbc_encrypt(ctx, desc, dst, src,\n\t\t\t\t\t\tcbc_blocks * bsize,\n\t\t\t\t\t\tnbytes - (cbc_blocks * bsize));\n\t\t}\n\t}\n\n\treturn err;\n}\n\nstatic int cts_cbc_decrypt(struct crypto_cts_ctx *ctx,\n\t\t\t   struct blkcipher_desc *desc,\n\t\t\t   struct scatterlist *dst,\n\t\t\t   struct scatterlist *src,\n\t\t\t   unsigned int offset,\n\t\t\t   unsigned int nbytes)\n{\n\tint bsize = crypto_blkcipher_blocksize(desc->tfm);\n\tu8 tmp[bsize];\n\tstruct blkcipher_desc lcldesc;\n\tstruct scatterlist sgsrc[1], sgdst[1];\n\tint lastn = nbytes - bsize;\n\tu8 iv[bsize];\n\tu8 s[bsize * 2], d[bsize * 2];\n\tint err;\n\n\tif (lastn < 0)\n\t\treturn -EINVAL;\n\n\tsg_init_table(sgsrc, 1);\n\tsg_init_table(sgdst, 1);\n\n\tscatterwalk_map_and_copy(s, src, offset, nbytes, 0);\n\n\tlcldesc.tfm = ctx->child;\n\tlcldesc.info = iv;\n\tlcldesc.flags = desc->flags;\n\n\t/* 1. Decrypt Cn-1 (s) to create Dn (tmp)*/\n\tmemset(iv, 0, sizeof(iv));\n\tsg_set_buf(&sgsrc[0], s, bsize);\n\tsg_set_buf(&sgdst[0], tmp, bsize);\n\terr = crypto_blkcipher_decrypt_iv(&lcldesc, sgdst, sgsrc, bsize);\n\tif (err)\n\t\treturn err;\n\t/* 2. Pad Cn with zeros at the end to create C of length BB */\n\tmemset(iv, 0, sizeof(iv));\n\tmemcpy(iv, s + bsize, lastn);\n\t/* 3. Exclusive-or Dn (tmp) with C (iv) to create Xn (tmp) */\n\tcrypto_xor(tmp, iv, bsize);\n\t/* 4. Select the first Ln bytes of Xn (tmp) to create Pn */\n\tmemcpy(d + bsize, tmp, lastn);\n\n\t/* 5. Append the tail (BB - Ln) bytes of Xn (tmp) to Cn to create En */\n\tmemcpy(s + bsize + lastn, tmp + lastn, bsize - lastn);\n\t/* 6. Decrypt En to create Pn-1 */\n\tmemzero_explicit(iv, sizeof(iv));\n\n\tsg_set_buf(&sgsrc[0], s + bsize, bsize);\n\tsg_set_buf(&sgdst[0], d, bsize);\n\terr = crypto_blkcipher_decrypt_iv(&lcldesc, sgdst, sgsrc, bsize);\n\n\t/* XOR with previous block */\n\tcrypto_xor(d, desc->info, bsize);\n\n\tscatterwalk_map_and_copy(d, dst, offset, nbytes, 1);\n\n\tmemcpy(desc->info, s, bsize);\n\treturn err;\n}\n\nstatic int crypto_cts_decrypt(struct blkcipher_desc *desc,\n\t\t\t      struct scatterlist *dst, struct scatterlist *src,\n\t\t\t      unsigned int nbytes)\n{\n\tstruct crypto_cts_ctx *ctx = crypto_blkcipher_ctx(desc->tfm);\n\tint bsize = crypto_blkcipher_blocksize(desc->tfm);\n\tint tot_blocks = (nbytes + bsize - 1) / bsize;\n\tint cbc_blocks = tot_blocks > 2 ? tot_blocks - 2 : 0;\n\tstruct blkcipher_desc lcldesc;\n\tint err;\n\n\tlcldesc.tfm = ctx->child;\n\tlcldesc.info = desc->info;\n\tlcldesc.flags = desc->flags;\n\n\tif (tot_blocks == 1) {\n\t\terr = crypto_blkcipher_decrypt_iv(&lcldesc, dst, src, bsize);\n\t} else if (nbytes <= bsize * 2) {\n\t\terr = cts_cbc_decrypt(ctx, desc, dst, src, 0, nbytes);\n\t} else {\n\t\t/* do normal function for tot_blocks - 2 */\n\t\terr = crypto_blkcipher_decrypt_iv(&lcldesc, dst, src,\n\t\t\t\t\t\t\tcbc_blocks * bsize);\n\t\tif (err == 0) {\n\t\t\t/* do cts for final two blocks */\n\t\t\terr = cts_cbc_decrypt(ctx, desc, dst, src,\n\t\t\t\t\t\tcbc_blocks * bsize,\n\t\t\t\t\t\tnbytes - (cbc_blocks * bsize));\n\t\t}\n\t}\n\treturn err;\n}\n\nstatic int crypto_cts_init_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_instance *inst = (void *)tfm->__crt_alg;\n\tstruct crypto_spawn *spawn = crypto_instance_ctx(inst);\n\tstruct crypto_cts_ctx *ctx = crypto_tfm_ctx(tfm);\n\tstruct crypto_blkcipher *cipher;\n\n\tcipher = crypto_spawn_blkcipher(spawn);\n\tif (IS_ERR(cipher))\n\t\treturn PTR_ERR(cipher);\n\n\tctx->child = cipher;\n\treturn 0;\n}\n\nstatic void crypto_cts_exit_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_cts_ctx *ctx = crypto_tfm_ctx(tfm);\n\tcrypto_free_blkcipher(ctx->child);\n}\n\nstatic struct crypto_instance *crypto_cts_alloc(struct rtattr **tb)\n{\n\tstruct crypto_instance *inst;\n\tstruct crypto_alg *alg;\n\tint err;\n\n\terr = crypto_check_attr_type(tb, CRYPTO_ALG_TYPE_BLKCIPHER);\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\talg = crypto_attr_alg(tb[1], CRYPTO_ALG_TYPE_BLKCIPHER,\n\t\t\t\t  CRYPTO_ALG_TYPE_MASK);\n\tif (IS_ERR(alg))\n\t\treturn ERR_CAST(alg);\n\n\tinst = ERR_PTR(-EINVAL);\n\tif (!is_power_of_2(alg->cra_blocksize))\n\t\tgoto out_put_alg;\n\n\tinst = crypto_alloc_instance(\"cts\", alg);\n\tif (IS_ERR(inst))\n\t\tgoto out_put_alg;\n\n\tinst->alg.cra_flags = CRYPTO_ALG_TYPE_BLKCIPHER;\n\tinst->alg.cra_priority = alg->cra_priority;\n\tinst->alg.cra_blocksize = alg->cra_blocksize;\n\tinst->alg.cra_alignmask = alg->cra_alignmask;\n\tinst->alg.cra_type = &crypto_blkcipher_type;\n\n\t/* We access the data as u32s when xoring. */\n\tinst->alg.cra_alignmask |= __alignof__(u32) - 1;\n\n\tinst->alg.cra_blkcipher.ivsize = alg->cra_blocksize;\n\tinst->alg.cra_blkcipher.min_keysize = alg->cra_blkcipher.min_keysize;\n\tinst->alg.cra_blkcipher.max_keysize = alg->cra_blkcipher.max_keysize;\n\n\tinst->alg.cra_blkcipher.geniv = \"seqiv\";\n\n\tinst->alg.cra_ctxsize = sizeof(struct crypto_cts_ctx);\n\n\tinst->alg.cra_init = crypto_cts_init_tfm;\n\tinst->alg.cra_exit = crypto_cts_exit_tfm;\n\n\tinst->alg.cra_blkcipher.setkey = crypto_cts_setkey;\n\tinst->alg.cra_blkcipher.encrypt = crypto_cts_encrypt;\n\tinst->alg.cra_blkcipher.decrypt = crypto_cts_decrypt;\n\nout_put_alg:\n\tcrypto_mod_put(alg);\n\treturn inst;\n}\n\nstatic void crypto_cts_free(struct crypto_instance *inst)\n{\n\tcrypto_drop_spawn(crypto_instance_ctx(inst));\n\tkfree(inst);\n}\n\nstatic struct crypto_template crypto_cts_tmpl = {\n\t.name = \"cts\",\n\t.alloc = crypto_cts_alloc,\n\t.free = crypto_cts_free,\n\t.module = THIS_MODULE,\n};\n\nstatic int __init crypto_cts_module_init(void)\n{\n\treturn crypto_register_template(&crypto_cts_tmpl);\n}\n\nstatic void __exit crypto_cts_module_exit(void)\n{\n\tcrypto_unregister_template(&crypto_cts_tmpl);\n}\n\nmodule_init(crypto_cts_module_init);\nmodule_exit(crypto_cts_module_exit);\n\nMODULE_LICENSE(\"Dual BSD/GPL\");\nMODULE_DESCRIPTION(\"CTS-CBC CipherText Stealing for CBC\");\nMODULE_ALIAS_CRYPTO(\"cts\");\n", "/*\n * ECB: Electronic CodeBook mode\n *\n * Copyright (c) 2006 Herbert Xu <herbert@gondor.apana.org.au>\n *\n * This program is free software; you can redistribute it and/or modify it\n * under the terms of the GNU General Public License as published by the Free\n * Software Foundation; either version 2 of the License, or (at your option)\n * any later version.\n *\n */\n\n#include <crypto/algapi.h>\n#include <linux/err.h>\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/scatterlist.h>\n#include <linux/slab.h>\n\nstruct crypto_ecb_ctx {\n\tstruct crypto_cipher *child;\n};\n\nstatic int crypto_ecb_setkey(struct crypto_tfm *parent, const u8 *key,\n\t\t\t     unsigned int keylen)\n{\n\tstruct crypto_ecb_ctx *ctx = crypto_tfm_ctx(parent);\n\tstruct crypto_cipher *child = ctx->child;\n\tint err;\n\n\tcrypto_cipher_clear_flags(child, CRYPTO_TFM_REQ_MASK);\n\tcrypto_cipher_set_flags(child, crypto_tfm_get_flags(parent) &\n\t\t\t\t       CRYPTO_TFM_REQ_MASK);\n\terr = crypto_cipher_setkey(child, key, keylen);\n\tcrypto_tfm_set_flags(parent, crypto_cipher_get_flags(child) &\n\t\t\t\t     CRYPTO_TFM_RES_MASK);\n\treturn err;\n}\n\nstatic int crypto_ecb_crypt(struct blkcipher_desc *desc,\n\t\t\t    struct blkcipher_walk *walk,\n\t\t\t    struct crypto_cipher *tfm,\n\t\t\t    void (*fn)(struct crypto_tfm *, u8 *, const u8 *))\n{\n\tint bsize = crypto_cipher_blocksize(tfm);\n\tunsigned int nbytes;\n\tint err;\n\n\terr = blkcipher_walk_virt(desc, walk);\n\n\twhile ((nbytes = walk->nbytes)) {\n\t\tu8 *wsrc = walk->src.virt.addr;\n\t\tu8 *wdst = walk->dst.virt.addr;\n\n\t\tdo {\n\t\t\tfn(crypto_cipher_tfm(tfm), wdst, wsrc);\n\n\t\t\twsrc += bsize;\n\t\t\twdst += bsize;\n\t\t} while ((nbytes -= bsize) >= bsize);\n\n\t\terr = blkcipher_walk_done(desc, walk, nbytes);\n\t}\n\n\treturn err;\n}\n\nstatic int crypto_ecb_encrypt(struct blkcipher_desc *desc,\n\t\t\t      struct scatterlist *dst, struct scatterlist *src,\n\t\t\t      unsigned int nbytes)\n{\n\tstruct blkcipher_walk walk;\n\tstruct crypto_blkcipher *tfm = desc->tfm;\n\tstruct crypto_ecb_ctx *ctx = crypto_blkcipher_ctx(tfm);\n\tstruct crypto_cipher *child = ctx->child;\n\n\tblkcipher_walk_init(&walk, dst, src, nbytes);\n\treturn crypto_ecb_crypt(desc, &walk, child,\n\t\t\t\tcrypto_cipher_alg(child)->cia_encrypt);\n}\n\nstatic int crypto_ecb_decrypt(struct blkcipher_desc *desc,\n\t\t\t      struct scatterlist *dst, struct scatterlist *src,\n\t\t\t      unsigned int nbytes)\n{\n\tstruct blkcipher_walk walk;\n\tstruct crypto_blkcipher *tfm = desc->tfm;\n\tstruct crypto_ecb_ctx *ctx = crypto_blkcipher_ctx(tfm);\n\tstruct crypto_cipher *child = ctx->child;\n\n\tblkcipher_walk_init(&walk, dst, src, nbytes);\n\treturn crypto_ecb_crypt(desc, &walk, child,\n\t\t\t\tcrypto_cipher_alg(child)->cia_decrypt);\n}\n\nstatic int crypto_ecb_init_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_instance *inst = (void *)tfm->__crt_alg;\n\tstruct crypto_spawn *spawn = crypto_instance_ctx(inst);\n\tstruct crypto_ecb_ctx *ctx = crypto_tfm_ctx(tfm);\n\tstruct crypto_cipher *cipher;\n\n\tcipher = crypto_spawn_cipher(spawn);\n\tif (IS_ERR(cipher))\n\t\treturn PTR_ERR(cipher);\n\n\tctx->child = cipher;\n\treturn 0;\n}\n\nstatic void crypto_ecb_exit_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_ecb_ctx *ctx = crypto_tfm_ctx(tfm);\n\tcrypto_free_cipher(ctx->child);\n}\n\nstatic struct crypto_instance *crypto_ecb_alloc(struct rtattr **tb)\n{\n\tstruct crypto_instance *inst;\n\tstruct crypto_alg *alg;\n\tint err;\n\n\terr = crypto_check_attr_type(tb, CRYPTO_ALG_TYPE_BLKCIPHER);\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\talg = crypto_get_attr_alg(tb, CRYPTO_ALG_TYPE_CIPHER,\n\t\t\t\t  CRYPTO_ALG_TYPE_MASK);\n\tif (IS_ERR(alg))\n\t\treturn ERR_CAST(alg);\n\n\tinst = crypto_alloc_instance(\"ecb\", alg);\n\tif (IS_ERR(inst))\n\t\tgoto out_put_alg;\n\n\tinst->alg.cra_flags = CRYPTO_ALG_TYPE_BLKCIPHER;\n\tinst->alg.cra_priority = alg->cra_priority;\n\tinst->alg.cra_blocksize = alg->cra_blocksize;\n\tinst->alg.cra_alignmask = alg->cra_alignmask;\n\tinst->alg.cra_type = &crypto_blkcipher_type;\n\n\tinst->alg.cra_blkcipher.min_keysize = alg->cra_cipher.cia_min_keysize;\n\tinst->alg.cra_blkcipher.max_keysize = alg->cra_cipher.cia_max_keysize;\n\n\tinst->alg.cra_ctxsize = sizeof(struct crypto_ecb_ctx);\n\n\tinst->alg.cra_init = crypto_ecb_init_tfm;\n\tinst->alg.cra_exit = crypto_ecb_exit_tfm;\n\n\tinst->alg.cra_blkcipher.setkey = crypto_ecb_setkey;\n\tinst->alg.cra_blkcipher.encrypt = crypto_ecb_encrypt;\n\tinst->alg.cra_blkcipher.decrypt = crypto_ecb_decrypt;\n\nout_put_alg:\n\tcrypto_mod_put(alg);\n\treturn inst;\n}\n\nstatic void crypto_ecb_free(struct crypto_instance *inst)\n{\n\tcrypto_drop_spawn(crypto_instance_ctx(inst));\n\tkfree(inst);\n}\n\nstatic struct crypto_template crypto_ecb_tmpl = {\n\t.name = \"ecb\",\n\t.alloc = crypto_ecb_alloc,\n\t.free = crypto_ecb_free,\n\t.module = THIS_MODULE,\n};\n\nstatic int __init crypto_ecb_module_init(void)\n{\n\treturn crypto_register_template(&crypto_ecb_tmpl);\n}\n\nstatic void __exit crypto_ecb_module_exit(void)\n{\n\tcrypto_unregister_template(&crypto_ecb_tmpl);\n}\n\nmodule_init(crypto_ecb_module_init);\nmodule_exit(crypto_ecb_module_exit);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"ECB block cipher algorithm\");\nMODULE_ALIAS_CRYPTO(\"ecb\");\n", "/*\n * eseqiv: Encrypted Sequence Number IV Generator\n *\n * This generator generates an IV based on a sequence number by xoring it\n * with a salt and then encrypting it with the same key as used to encrypt\n * the plain text.  This algorithm requires that the block size be equal\n * to the IV size.  It is mainly useful for CBC.\n *\n * Copyright (c) 2007 Herbert Xu <herbert@gondor.apana.org.au>\n *\n * This program is free software; you can redistribute it and/or modify it\n * under the terms of the GNU General Public License as published by the Free\n * Software Foundation; either version 2 of the License, or (at your option)\n * any later version.\n *\n */\n\n#include <crypto/internal/skcipher.h>\n#include <crypto/rng.h>\n#include <crypto/scatterwalk.h>\n#include <linux/err.h>\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/mm.h>\n#include <linux/module.h>\n#include <linux/scatterlist.h>\n#include <linux/spinlock.h>\n#include <linux/string.h>\n\nstruct eseqiv_request_ctx {\n\tstruct scatterlist src[2];\n\tstruct scatterlist dst[2];\n\tchar tail[];\n};\n\nstruct eseqiv_ctx {\n\tspinlock_t lock;\n\tunsigned int reqoff;\n\tchar salt[];\n};\n\nstatic void eseqiv_complete2(struct skcipher_givcrypt_request *req)\n{\n\tstruct crypto_ablkcipher *geniv = skcipher_givcrypt_reqtfm(req);\n\tstruct eseqiv_request_ctx *reqctx = skcipher_givcrypt_reqctx(req);\n\n\tmemcpy(req->giv, PTR_ALIGN((u8 *)reqctx->tail,\n\t\t\t crypto_ablkcipher_alignmask(geniv) + 1),\n\t       crypto_ablkcipher_ivsize(geniv));\n}\n\nstatic void eseqiv_complete(struct crypto_async_request *base, int err)\n{\n\tstruct skcipher_givcrypt_request *req = base->data;\n\n\tif (err)\n\t\tgoto out;\n\n\teseqiv_complete2(req);\n\nout:\n\tskcipher_givcrypt_complete(req, err);\n}\n\nstatic int eseqiv_givencrypt(struct skcipher_givcrypt_request *req)\n{\n\tstruct crypto_ablkcipher *geniv = skcipher_givcrypt_reqtfm(req);\n\tstruct eseqiv_ctx *ctx = crypto_ablkcipher_ctx(geniv);\n\tstruct eseqiv_request_ctx *reqctx = skcipher_givcrypt_reqctx(req);\n\tstruct ablkcipher_request *subreq;\n\tcrypto_completion_t compl;\n\tvoid *data;\n\tstruct scatterlist *osrc, *odst;\n\tstruct scatterlist *dst;\n\tstruct page *srcp;\n\tstruct page *dstp;\n\tu8 *giv;\n\tu8 *vsrc;\n\tu8 *vdst;\n\t__be64 seq;\n\tunsigned int ivsize;\n\tunsigned int len;\n\tint err;\n\n\tsubreq = (void *)(reqctx->tail + ctx->reqoff);\n\tablkcipher_request_set_tfm(subreq, skcipher_geniv_cipher(geniv));\n\n\tgiv = req->giv;\n\tcompl = req->creq.base.complete;\n\tdata = req->creq.base.data;\n\n\tosrc = req->creq.src;\n\todst = req->creq.dst;\n\tsrcp = sg_page(osrc);\n\tdstp = sg_page(odst);\n\tvsrc = PageHighMem(srcp) ? NULL : page_address(srcp) + osrc->offset;\n\tvdst = PageHighMem(dstp) ? NULL : page_address(dstp) + odst->offset;\n\n\tivsize = crypto_ablkcipher_ivsize(geniv);\n\n\tif (vsrc != giv + ivsize && vdst != giv + ivsize) {\n\t\tgiv = PTR_ALIGN((u8 *)reqctx->tail,\n\t\t\t\tcrypto_ablkcipher_alignmask(geniv) + 1);\n\t\tcompl = eseqiv_complete;\n\t\tdata = req;\n\t}\n\n\tablkcipher_request_set_callback(subreq, req->creq.base.flags, compl,\n\t\t\t\t\tdata);\n\n\tsg_init_table(reqctx->src, 2);\n\tsg_set_buf(reqctx->src, giv, ivsize);\n\tscatterwalk_crypto_chain(reqctx->src, osrc, vsrc == giv + ivsize, 2);\n\n\tdst = reqctx->src;\n\tif (osrc != odst) {\n\t\tsg_init_table(reqctx->dst, 2);\n\t\tsg_set_buf(reqctx->dst, giv, ivsize);\n\t\tscatterwalk_crypto_chain(reqctx->dst, odst, vdst == giv + ivsize, 2);\n\n\t\tdst = reqctx->dst;\n\t}\n\n\tablkcipher_request_set_crypt(subreq, reqctx->src, dst,\n\t\t\t\t     req->creq.nbytes + ivsize,\n\t\t\t\t     req->creq.info);\n\n\tmemcpy(req->creq.info, ctx->salt, ivsize);\n\n\tlen = ivsize;\n\tif (ivsize > sizeof(u64)) {\n\t\tmemset(req->giv, 0, ivsize - sizeof(u64));\n\t\tlen = sizeof(u64);\n\t}\n\tseq = cpu_to_be64(req->seq);\n\tmemcpy(req->giv + ivsize - len, &seq, len);\n\n\terr = crypto_ablkcipher_encrypt(subreq);\n\tif (err)\n\t\tgoto out;\n\n\tif (giv != req->giv)\n\t\teseqiv_complete2(req);\n\nout:\n\treturn err;\n}\n\nstatic int eseqiv_givencrypt_first(struct skcipher_givcrypt_request *req)\n{\n\tstruct crypto_ablkcipher *geniv = skcipher_givcrypt_reqtfm(req);\n\tstruct eseqiv_ctx *ctx = crypto_ablkcipher_ctx(geniv);\n\tint err = 0;\n\n\tspin_lock_bh(&ctx->lock);\n\tif (crypto_ablkcipher_crt(geniv)->givencrypt != eseqiv_givencrypt_first)\n\t\tgoto unlock;\n\n\tcrypto_ablkcipher_crt(geniv)->givencrypt = eseqiv_givencrypt;\n\terr = crypto_rng_get_bytes(crypto_default_rng, ctx->salt,\n\t\t\t\t   crypto_ablkcipher_ivsize(geniv));\n\nunlock:\n\tspin_unlock_bh(&ctx->lock);\n\n\tif (err)\n\t\treturn err;\n\n\treturn eseqiv_givencrypt(req);\n}\n\nstatic int eseqiv_init(struct crypto_tfm *tfm)\n{\n\tstruct crypto_ablkcipher *geniv = __crypto_ablkcipher_cast(tfm);\n\tstruct eseqiv_ctx *ctx = crypto_ablkcipher_ctx(geniv);\n\tunsigned long alignmask;\n\tunsigned int reqsize;\n\n\tspin_lock_init(&ctx->lock);\n\n\talignmask = crypto_tfm_ctx_alignment() - 1;\n\treqsize = sizeof(struct eseqiv_request_ctx);\n\n\tif (alignmask & reqsize) {\n\t\talignmask &= reqsize;\n\t\talignmask--;\n\t}\n\n\talignmask = ~alignmask;\n\talignmask &= crypto_ablkcipher_alignmask(geniv);\n\n\treqsize += alignmask;\n\treqsize += crypto_ablkcipher_ivsize(geniv);\n\treqsize = ALIGN(reqsize, crypto_tfm_ctx_alignment());\n\n\tctx->reqoff = reqsize - sizeof(struct eseqiv_request_ctx);\n\n\ttfm->crt_ablkcipher.reqsize = reqsize +\n\t\t\t\t      sizeof(struct ablkcipher_request);\n\n\treturn skcipher_geniv_init(tfm);\n}\n\nstatic struct crypto_template eseqiv_tmpl;\n\nstatic struct crypto_instance *eseqiv_alloc(struct rtattr **tb)\n{\n\tstruct crypto_instance *inst;\n\tint err;\n\n\terr = crypto_get_default_rng();\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\tinst = skcipher_geniv_alloc(&eseqiv_tmpl, tb, 0, 0);\n\tif (IS_ERR(inst))\n\t\tgoto put_rng;\n\n\terr = -EINVAL;\n\tif (inst->alg.cra_ablkcipher.ivsize != inst->alg.cra_blocksize)\n\t\tgoto free_inst;\n\n\tinst->alg.cra_ablkcipher.givencrypt = eseqiv_givencrypt_first;\n\n\tinst->alg.cra_init = eseqiv_init;\n\tinst->alg.cra_exit = skcipher_geniv_exit;\n\n\tinst->alg.cra_ctxsize = sizeof(struct eseqiv_ctx);\n\tinst->alg.cra_ctxsize += inst->alg.cra_ablkcipher.ivsize;\n\nout:\n\treturn inst;\n\nfree_inst:\n\tskcipher_geniv_free(inst);\n\tinst = ERR_PTR(err);\nput_rng:\n\tcrypto_put_default_rng();\n\tgoto out;\n}\n\nstatic void eseqiv_free(struct crypto_instance *inst)\n{\n\tskcipher_geniv_free(inst);\n\tcrypto_put_default_rng();\n}\n\nstatic struct crypto_template eseqiv_tmpl = {\n\t.name = \"eseqiv\",\n\t.alloc = eseqiv_alloc,\n\t.free = eseqiv_free,\n\t.module = THIS_MODULE,\n};\n\nstatic int __init eseqiv_module_init(void)\n{\n\treturn crypto_register_template(&eseqiv_tmpl);\n}\n\nstatic void __exit eseqiv_module_exit(void)\n{\n\tcrypto_unregister_template(&eseqiv_tmpl);\n}\n\nmodule_init(eseqiv_module_init);\nmodule_exit(eseqiv_module_exit);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"Encrypted Sequence Number IV Generator\");\nMODULE_ALIAS_CRYPTO(\"eseqiv\");\n", "/*\n * GCM: Galois/Counter Mode.\n *\n * Copyright (c) 2007 Nokia Siemens Networks - Mikko Herranen <mh1@iki.fi>\n *\n * This program is free software; you can redistribute it and/or modify it\n * under the terms of the GNU General Public License version 2 as published\n * by the Free Software Foundation.\n */\n\n#include <crypto/gf128mul.h>\n#include <crypto/internal/aead.h>\n#include <crypto/internal/skcipher.h>\n#include <crypto/internal/hash.h>\n#include <crypto/scatterwalk.h>\n#include <crypto/hash.h>\n#include \"internal.h\"\n#include <linux/completion.h>\n#include <linux/err.h>\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/slab.h>\n\nstruct gcm_instance_ctx {\n\tstruct crypto_skcipher_spawn ctr;\n\tstruct crypto_ahash_spawn ghash;\n};\n\nstruct crypto_gcm_ctx {\n\tstruct crypto_ablkcipher *ctr;\n\tstruct crypto_ahash *ghash;\n};\n\nstruct crypto_rfc4106_ctx {\n\tstruct crypto_aead *child;\n\tu8 nonce[4];\n};\n\nstruct crypto_rfc4543_instance_ctx {\n\tstruct crypto_aead_spawn aead;\n\tstruct crypto_skcipher_spawn null;\n};\n\nstruct crypto_rfc4543_ctx {\n\tstruct crypto_aead *child;\n\tstruct crypto_blkcipher *null;\n\tu8 nonce[4];\n};\n\nstruct crypto_rfc4543_req_ctx {\n\tu8 auth_tag[16];\n\tu8 assocbuf[32];\n\tstruct scatterlist cipher[1];\n\tstruct scatterlist payload[2];\n\tstruct scatterlist assoc[2];\n\tstruct aead_request subreq;\n};\n\nstruct crypto_gcm_ghash_ctx {\n\tunsigned int cryptlen;\n\tstruct scatterlist *src;\n\tvoid (*complete)(struct aead_request *req, int err);\n};\n\nstruct crypto_gcm_req_priv_ctx {\n\tu8 auth_tag[16];\n\tu8 iauth_tag[16];\n\tstruct scatterlist src[2];\n\tstruct scatterlist dst[2];\n\tstruct crypto_gcm_ghash_ctx ghash_ctx;\n\tunion {\n\t\tstruct ahash_request ahreq;\n\t\tstruct ablkcipher_request abreq;\n\t} u;\n};\n\nstruct crypto_gcm_setkey_result {\n\tint err;\n\tstruct completion completion;\n};\n\nstatic void *gcm_zeroes;\n\nstatic inline struct crypto_gcm_req_priv_ctx *crypto_gcm_reqctx(\n\tstruct aead_request *req)\n{\n\tunsigned long align = crypto_aead_alignmask(crypto_aead_reqtfm(req));\n\n\treturn (void *)PTR_ALIGN((u8 *)aead_request_ctx(req), align + 1);\n}\n\nstatic void crypto_gcm_setkey_done(struct crypto_async_request *req, int err)\n{\n\tstruct crypto_gcm_setkey_result *result = req->data;\n\n\tif (err == -EINPROGRESS)\n\t\treturn;\n\n\tresult->err = err;\n\tcomplete(&result->completion);\n}\n\nstatic int crypto_gcm_setkey(struct crypto_aead *aead, const u8 *key,\n\t\t\t     unsigned int keylen)\n{\n\tstruct crypto_gcm_ctx *ctx = crypto_aead_ctx(aead);\n\tstruct crypto_ahash *ghash = ctx->ghash;\n\tstruct crypto_ablkcipher *ctr = ctx->ctr;\n\tstruct {\n\t\tbe128 hash;\n\t\tu8 iv[8];\n\n\t\tstruct crypto_gcm_setkey_result result;\n\n\t\tstruct scatterlist sg[1];\n\t\tstruct ablkcipher_request req;\n\t} *data;\n\tint err;\n\n\tcrypto_ablkcipher_clear_flags(ctr, CRYPTO_TFM_REQ_MASK);\n\tcrypto_ablkcipher_set_flags(ctr, crypto_aead_get_flags(aead) &\n\t\t\t\t   CRYPTO_TFM_REQ_MASK);\n\n\terr = crypto_ablkcipher_setkey(ctr, key, keylen);\n\tif (err)\n\t\treturn err;\n\n\tcrypto_aead_set_flags(aead, crypto_ablkcipher_get_flags(ctr) &\n\t\t\t\t       CRYPTO_TFM_RES_MASK);\n\n\tdata = kzalloc(sizeof(*data) + crypto_ablkcipher_reqsize(ctr),\n\t\t       GFP_KERNEL);\n\tif (!data)\n\t\treturn -ENOMEM;\n\n\tinit_completion(&data->result.completion);\n\tsg_init_one(data->sg, &data->hash, sizeof(data->hash));\n\tablkcipher_request_set_tfm(&data->req, ctr);\n\tablkcipher_request_set_callback(&data->req, CRYPTO_TFM_REQ_MAY_SLEEP |\n\t\t\t\t\t\t    CRYPTO_TFM_REQ_MAY_BACKLOG,\n\t\t\t\t\tcrypto_gcm_setkey_done,\n\t\t\t\t\t&data->result);\n\tablkcipher_request_set_crypt(&data->req, data->sg, data->sg,\n\t\t\t\t     sizeof(data->hash), data->iv);\n\n\terr = crypto_ablkcipher_encrypt(&data->req);\n\tif (err == -EINPROGRESS || err == -EBUSY) {\n\t\terr = wait_for_completion_interruptible(\n\t\t\t&data->result.completion);\n\t\tif (!err)\n\t\t\terr = data->result.err;\n\t}\n\n\tif (err)\n\t\tgoto out;\n\n\tcrypto_ahash_clear_flags(ghash, CRYPTO_TFM_REQ_MASK);\n\tcrypto_ahash_set_flags(ghash, crypto_aead_get_flags(aead) &\n\t\t\t       CRYPTO_TFM_REQ_MASK);\n\terr = crypto_ahash_setkey(ghash, (u8 *)&data->hash, sizeof(be128));\n\tcrypto_aead_set_flags(aead, crypto_ahash_get_flags(ghash) &\n\t\t\t      CRYPTO_TFM_RES_MASK);\n\nout:\n\tkfree(data);\n\treturn err;\n}\n\nstatic int crypto_gcm_setauthsize(struct crypto_aead *tfm,\n\t\t\t\t  unsigned int authsize)\n{\n\tswitch (authsize) {\n\tcase 4:\n\tcase 8:\n\tcase 12:\n\tcase 13:\n\tcase 14:\n\tcase 15:\n\tcase 16:\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic void crypto_gcm_init_crypt(struct ablkcipher_request *ablk_req,\n\t\t\t\t  struct aead_request *req,\n\t\t\t\t  unsigned int cryptlen)\n{\n\tstruct crypto_aead *aead = crypto_aead_reqtfm(req);\n\tstruct crypto_gcm_ctx *ctx = crypto_aead_ctx(aead);\n\tstruct crypto_gcm_req_priv_ctx *pctx = crypto_gcm_reqctx(req);\n\tstruct scatterlist *dst;\n\t__be32 counter = cpu_to_be32(1);\n\n\tmemset(pctx->auth_tag, 0, sizeof(pctx->auth_tag));\n\tmemcpy(req->iv + 12, &counter, 4);\n\n\tsg_init_table(pctx->src, 2);\n\tsg_set_buf(pctx->src, pctx->auth_tag, sizeof(pctx->auth_tag));\n\tscatterwalk_sg_chain(pctx->src, 2, req->src);\n\n\tdst = pctx->src;\n\tif (req->src != req->dst) {\n\t\tsg_init_table(pctx->dst, 2);\n\t\tsg_set_buf(pctx->dst, pctx->auth_tag, sizeof(pctx->auth_tag));\n\t\tscatterwalk_sg_chain(pctx->dst, 2, req->dst);\n\t\tdst = pctx->dst;\n\t}\n\n\tablkcipher_request_set_tfm(ablk_req, ctx->ctr);\n\tablkcipher_request_set_crypt(ablk_req, pctx->src, dst,\n\t\t\t\t     cryptlen + sizeof(pctx->auth_tag),\n\t\t\t\t     req->iv);\n}\n\nstatic inline unsigned int gcm_remain(unsigned int len)\n{\n\tlen &= 0xfU;\n\treturn len ? 16 - len : 0;\n}\n\nstatic void gcm_hash_len_done(struct crypto_async_request *areq, int err);\nstatic void gcm_hash_final_done(struct crypto_async_request *areq, int err);\n\nstatic int gcm_hash_update(struct aead_request *req,\n\t\t\t   struct crypto_gcm_req_priv_ctx *pctx,\n\t\t\t   crypto_completion_t compl,\n\t\t\t   struct scatterlist *src,\n\t\t\t   unsigned int len)\n{\n\tstruct ahash_request *ahreq = &pctx->u.ahreq;\n\n\tahash_request_set_callback(ahreq, aead_request_flags(req),\n\t\t\t\t   compl, req);\n\tahash_request_set_crypt(ahreq, src, NULL, len);\n\n\treturn crypto_ahash_update(ahreq);\n}\n\nstatic int gcm_hash_remain(struct aead_request *req,\n\t\t\t   struct crypto_gcm_req_priv_ctx *pctx,\n\t\t\t   unsigned int remain,\n\t\t\t   crypto_completion_t compl)\n{\n\tstruct ahash_request *ahreq = &pctx->u.ahreq;\n\n\tahash_request_set_callback(ahreq, aead_request_flags(req),\n\t\t\t\t   compl, req);\n\tsg_init_one(pctx->src, gcm_zeroes, remain);\n\tahash_request_set_crypt(ahreq, pctx->src, NULL, remain);\n\n\treturn crypto_ahash_update(ahreq);\n}\n\nstatic int gcm_hash_len(struct aead_request *req,\n\t\t\tstruct crypto_gcm_req_priv_ctx *pctx)\n{\n\tstruct ahash_request *ahreq = &pctx->u.ahreq;\n\tstruct crypto_gcm_ghash_ctx *gctx = &pctx->ghash_ctx;\n\tu128 lengths;\n\n\tlengths.a = cpu_to_be64(req->assoclen * 8);\n\tlengths.b = cpu_to_be64(gctx->cryptlen * 8);\n\tmemcpy(pctx->iauth_tag, &lengths, 16);\n\tsg_init_one(pctx->src, pctx->iauth_tag, 16);\n\tahash_request_set_callback(ahreq, aead_request_flags(req),\n\t\t\t\t   gcm_hash_len_done, req);\n\tahash_request_set_crypt(ahreq, pctx->src,\n\t\t\t\tNULL, sizeof(lengths));\n\n\treturn crypto_ahash_update(ahreq);\n}\n\nstatic int gcm_hash_final(struct aead_request *req,\n\t\t\t  struct crypto_gcm_req_priv_ctx *pctx)\n{\n\tstruct ahash_request *ahreq = &pctx->u.ahreq;\n\n\tahash_request_set_callback(ahreq, aead_request_flags(req),\n\t\t\t\t   gcm_hash_final_done, req);\n\tahash_request_set_crypt(ahreq, NULL, pctx->iauth_tag, 0);\n\n\treturn crypto_ahash_final(ahreq);\n}\n\nstatic void __gcm_hash_final_done(struct aead_request *req, int err)\n{\n\tstruct crypto_gcm_req_priv_ctx *pctx = crypto_gcm_reqctx(req);\n\tstruct crypto_gcm_ghash_ctx *gctx = &pctx->ghash_ctx;\n\n\tif (!err)\n\t\tcrypto_xor(pctx->auth_tag, pctx->iauth_tag, 16);\n\n\tgctx->complete(req, err);\n}\n\nstatic void gcm_hash_final_done(struct crypto_async_request *areq, int err)\n{\n\tstruct aead_request *req = areq->data;\n\n\t__gcm_hash_final_done(req, err);\n}\n\nstatic void __gcm_hash_len_done(struct aead_request *req, int err)\n{\n\tstruct crypto_gcm_req_priv_ctx *pctx = crypto_gcm_reqctx(req);\n\n\tif (!err) {\n\t\terr = gcm_hash_final(req, pctx);\n\t\tif (err == -EINPROGRESS || err == -EBUSY)\n\t\t\treturn;\n\t}\n\n\t__gcm_hash_final_done(req, err);\n}\n\nstatic void gcm_hash_len_done(struct crypto_async_request *areq, int err)\n{\n\tstruct aead_request *req = areq->data;\n\n\t__gcm_hash_len_done(req, err);\n}\n\nstatic void __gcm_hash_crypt_remain_done(struct aead_request *req, int err)\n{\n\tstruct crypto_gcm_req_priv_ctx *pctx = crypto_gcm_reqctx(req);\n\n\tif (!err) {\n\t\terr = gcm_hash_len(req, pctx);\n\t\tif (err == -EINPROGRESS || err == -EBUSY)\n\t\t\treturn;\n\t}\n\n\t__gcm_hash_len_done(req, err);\n}\n\nstatic void gcm_hash_crypt_remain_done(struct crypto_async_request *areq,\n\t\t\t\t       int err)\n{\n\tstruct aead_request *req = areq->data;\n\n\t__gcm_hash_crypt_remain_done(req, err);\n}\n\nstatic void __gcm_hash_crypt_done(struct aead_request *req, int err)\n{\n\tstruct crypto_gcm_req_priv_ctx *pctx = crypto_gcm_reqctx(req);\n\tstruct crypto_gcm_ghash_ctx *gctx = &pctx->ghash_ctx;\n\tunsigned int remain;\n\n\tif (!err) {\n\t\tremain = gcm_remain(gctx->cryptlen);\n\t\tBUG_ON(!remain);\n\t\terr = gcm_hash_remain(req, pctx, remain,\n\t\t\t\t      gcm_hash_crypt_remain_done);\n\t\tif (err == -EINPROGRESS || err == -EBUSY)\n\t\t\treturn;\n\t}\n\n\t__gcm_hash_crypt_remain_done(req, err);\n}\n\nstatic void gcm_hash_crypt_done(struct crypto_async_request *areq, int err)\n{\n\tstruct aead_request *req = areq->data;\n\n\t__gcm_hash_crypt_done(req, err);\n}\n\nstatic void __gcm_hash_assoc_remain_done(struct aead_request *req, int err)\n{\n\tstruct crypto_gcm_req_priv_ctx *pctx = crypto_gcm_reqctx(req);\n\tstruct crypto_gcm_ghash_ctx *gctx = &pctx->ghash_ctx;\n\tcrypto_completion_t compl;\n\tunsigned int remain = 0;\n\n\tif (!err && gctx->cryptlen) {\n\t\tremain = gcm_remain(gctx->cryptlen);\n\t\tcompl = remain ? gcm_hash_crypt_done :\n\t\t\tgcm_hash_crypt_remain_done;\n\t\terr = gcm_hash_update(req, pctx, compl,\n\t\t\t\t      gctx->src, gctx->cryptlen);\n\t\tif (err == -EINPROGRESS || err == -EBUSY)\n\t\t\treturn;\n\t}\n\n\tif (remain)\n\t\t__gcm_hash_crypt_done(req, err);\n\telse\n\t\t__gcm_hash_crypt_remain_done(req, err);\n}\n\nstatic void gcm_hash_assoc_remain_done(struct crypto_async_request *areq,\n\t\t\t\t       int err)\n{\n\tstruct aead_request *req = areq->data;\n\n\t__gcm_hash_assoc_remain_done(req, err);\n}\n\nstatic void __gcm_hash_assoc_done(struct aead_request *req, int err)\n{\n\tstruct crypto_gcm_req_priv_ctx *pctx = crypto_gcm_reqctx(req);\n\tunsigned int remain;\n\n\tif (!err) {\n\t\tremain = gcm_remain(req->assoclen);\n\t\tBUG_ON(!remain);\n\t\terr = gcm_hash_remain(req, pctx, remain,\n\t\t\t\t      gcm_hash_assoc_remain_done);\n\t\tif (err == -EINPROGRESS || err == -EBUSY)\n\t\t\treturn;\n\t}\n\n\t__gcm_hash_assoc_remain_done(req, err);\n}\n\nstatic void gcm_hash_assoc_done(struct crypto_async_request *areq, int err)\n{\n\tstruct aead_request *req = areq->data;\n\n\t__gcm_hash_assoc_done(req, err);\n}\n\nstatic void __gcm_hash_init_done(struct aead_request *req, int err)\n{\n\tstruct crypto_gcm_req_priv_ctx *pctx = crypto_gcm_reqctx(req);\n\tcrypto_completion_t compl;\n\tunsigned int remain = 0;\n\n\tif (!err && req->assoclen) {\n\t\tremain = gcm_remain(req->assoclen);\n\t\tcompl = remain ? gcm_hash_assoc_done :\n\t\t\tgcm_hash_assoc_remain_done;\n\t\terr = gcm_hash_update(req, pctx, compl,\n\t\t\t\t      req->assoc, req->assoclen);\n\t\tif (err == -EINPROGRESS || err == -EBUSY)\n\t\t\treturn;\n\t}\n\n\tif (remain)\n\t\t__gcm_hash_assoc_done(req, err);\n\telse\n\t\t__gcm_hash_assoc_remain_done(req, err);\n}\n\nstatic void gcm_hash_init_done(struct crypto_async_request *areq, int err)\n{\n\tstruct aead_request *req = areq->data;\n\n\t__gcm_hash_init_done(req, err);\n}\n\nstatic int gcm_hash(struct aead_request *req,\n\t\t    struct crypto_gcm_req_priv_ctx *pctx)\n{\n\tstruct ahash_request *ahreq = &pctx->u.ahreq;\n\tstruct crypto_gcm_ghash_ctx *gctx = &pctx->ghash_ctx;\n\tstruct crypto_gcm_ctx *ctx = crypto_tfm_ctx(req->base.tfm);\n\tunsigned int remain;\n\tcrypto_completion_t compl;\n\tint err;\n\n\tahash_request_set_tfm(ahreq, ctx->ghash);\n\n\tahash_request_set_callback(ahreq, aead_request_flags(req),\n\t\t\t\t   gcm_hash_init_done, req);\n\terr = crypto_ahash_init(ahreq);\n\tif (err)\n\t\treturn err;\n\tremain = gcm_remain(req->assoclen);\n\tcompl = remain ? gcm_hash_assoc_done : gcm_hash_assoc_remain_done;\n\terr = gcm_hash_update(req, pctx, compl, req->assoc, req->assoclen);\n\tif (err)\n\t\treturn err;\n\tif (remain) {\n\t\terr = gcm_hash_remain(req, pctx, remain,\n\t\t\t\t      gcm_hash_assoc_remain_done);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\tremain = gcm_remain(gctx->cryptlen);\n\tcompl = remain ? gcm_hash_crypt_done : gcm_hash_crypt_remain_done;\n\terr = gcm_hash_update(req, pctx, compl, gctx->src, gctx->cryptlen);\n\tif (err)\n\t\treturn err;\n\tif (remain) {\n\t\terr = gcm_hash_remain(req, pctx, remain,\n\t\t\t\t      gcm_hash_crypt_remain_done);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\terr = gcm_hash_len(req, pctx);\n\tif (err)\n\t\treturn err;\n\terr = gcm_hash_final(req, pctx);\n\tif (err)\n\t\treturn err;\n\n\treturn 0;\n}\n\nstatic void gcm_enc_copy_hash(struct aead_request *req,\n\t\t\t      struct crypto_gcm_req_priv_ctx *pctx)\n{\n\tstruct crypto_aead *aead = crypto_aead_reqtfm(req);\n\tu8 *auth_tag = pctx->auth_tag;\n\n\tscatterwalk_map_and_copy(auth_tag, req->dst, req->cryptlen,\n\t\t\t\t crypto_aead_authsize(aead), 1);\n}\n\nstatic void gcm_enc_hash_done(struct aead_request *req, int err)\n{\n\tstruct crypto_gcm_req_priv_ctx *pctx = crypto_gcm_reqctx(req);\n\n\tif (!err)\n\t\tgcm_enc_copy_hash(req, pctx);\n\n\taead_request_complete(req, err);\n}\n\nstatic void gcm_encrypt_done(struct crypto_async_request *areq, int err)\n{\n\tstruct aead_request *req = areq->data;\n\tstruct crypto_gcm_req_priv_ctx *pctx = crypto_gcm_reqctx(req);\n\n\tif (!err) {\n\t\terr = gcm_hash(req, pctx);\n\t\tif (err == -EINPROGRESS || err == -EBUSY)\n\t\t\treturn;\n\t\telse if (!err) {\n\t\t\tcrypto_xor(pctx->auth_tag, pctx->iauth_tag, 16);\n\t\t\tgcm_enc_copy_hash(req, pctx);\n\t\t}\n\t}\n\n\taead_request_complete(req, err);\n}\n\nstatic int crypto_gcm_encrypt(struct aead_request *req)\n{\n\tstruct crypto_gcm_req_priv_ctx *pctx = crypto_gcm_reqctx(req);\n\tstruct ablkcipher_request *abreq = &pctx->u.abreq;\n\tstruct crypto_gcm_ghash_ctx *gctx = &pctx->ghash_ctx;\n\tint err;\n\n\tcrypto_gcm_init_crypt(abreq, req, req->cryptlen);\n\tablkcipher_request_set_callback(abreq, aead_request_flags(req),\n\t\t\t\t\tgcm_encrypt_done, req);\n\n\tgctx->src = req->dst;\n\tgctx->cryptlen = req->cryptlen;\n\tgctx->complete = gcm_enc_hash_done;\n\n\terr = crypto_ablkcipher_encrypt(abreq);\n\tif (err)\n\t\treturn err;\n\n\terr = gcm_hash(req, pctx);\n\tif (err)\n\t\treturn err;\n\n\tcrypto_xor(pctx->auth_tag, pctx->iauth_tag, 16);\n\tgcm_enc_copy_hash(req, pctx);\n\n\treturn 0;\n}\n\nstatic int crypto_gcm_verify(struct aead_request *req,\n\t\t\t     struct crypto_gcm_req_priv_ctx *pctx)\n{\n\tstruct crypto_aead *aead = crypto_aead_reqtfm(req);\n\tu8 *auth_tag = pctx->auth_tag;\n\tu8 *iauth_tag = pctx->iauth_tag;\n\tunsigned int authsize = crypto_aead_authsize(aead);\n\tunsigned int cryptlen = req->cryptlen - authsize;\n\n\tcrypto_xor(auth_tag, iauth_tag, 16);\n\tscatterwalk_map_and_copy(iauth_tag, req->src, cryptlen, authsize, 0);\n\treturn crypto_memneq(iauth_tag, auth_tag, authsize) ? -EBADMSG : 0;\n}\n\nstatic void gcm_decrypt_done(struct crypto_async_request *areq, int err)\n{\n\tstruct aead_request *req = areq->data;\n\tstruct crypto_gcm_req_priv_ctx *pctx = crypto_gcm_reqctx(req);\n\n\tif (!err)\n\t\terr = crypto_gcm_verify(req, pctx);\n\n\taead_request_complete(req, err);\n}\n\nstatic void gcm_dec_hash_done(struct aead_request *req, int err)\n{\n\tstruct crypto_gcm_req_priv_ctx *pctx = crypto_gcm_reqctx(req);\n\tstruct ablkcipher_request *abreq = &pctx->u.abreq;\n\tstruct crypto_gcm_ghash_ctx *gctx = &pctx->ghash_ctx;\n\n\tif (!err) {\n\t\tablkcipher_request_set_callback(abreq, aead_request_flags(req),\n\t\t\t\t\t\tgcm_decrypt_done, req);\n\t\tcrypto_gcm_init_crypt(abreq, req, gctx->cryptlen);\n\t\terr = crypto_ablkcipher_decrypt(abreq);\n\t\tif (err == -EINPROGRESS || err == -EBUSY)\n\t\t\treturn;\n\t\telse if (!err)\n\t\t\terr = crypto_gcm_verify(req, pctx);\n\t}\n\n\taead_request_complete(req, err);\n}\n\nstatic int crypto_gcm_decrypt(struct aead_request *req)\n{\n\tstruct crypto_aead *aead = crypto_aead_reqtfm(req);\n\tstruct crypto_gcm_req_priv_ctx *pctx = crypto_gcm_reqctx(req);\n\tstruct ablkcipher_request *abreq = &pctx->u.abreq;\n\tstruct crypto_gcm_ghash_ctx *gctx = &pctx->ghash_ctx;\n\tunsigned int authsize = crypto_aead_authsize(aead);\n\tunsigned int cryptlen = req->cryptlen;\n\tint err;\n\n\tif (cryptlen < authsize)\n\t\treturn -EINVAL;\n\tcryptlen -= authsize;\n\n\tgctx->src = req->src;\n\tgctx->cryptlen = cryptlen;\n\tgctx->complete = gcm_dec_hash_done;\n\n\terr = gcm_hash(req, pctx);\n\tif (err)\n\t\treturn err;\n\n\tablkcipher_request_set_callback(abreq, aead_request_flags(req),\n\t\t\t\t\tgcm_decrypt_done, req);\n\tcrypto_gcm_init_crypt(abreq, req, cryptlen);\n\terr = crypto_ablkcipher_decrypt(abreq);\n\tif (err)\n\t\treturn err;\n\n\treturn crypto_gcm_verify(req, pctx);\n}\n\nstatic int crypto_gcm_init_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_instance *inst = (void *)tfm->__crt_alg;\n\tstruct gcm_instance_ctx *ictx = crypto_instance_ctx(inst);\n\tstruct crypto_gcm_ctx *ctx = crypto_tfm_ctx(tfm);\n\tstruct crypto_ablkcipher *ctr;\n\tstruct crypto_ahash *ghash;\n\tunsigned long align;\n\tint err;\n\n\tghash = crypto_spawn_ahash(&ictx->ghash);\n\tif (IS_ERR(ghash))\n\t\treturn PTR_ERR(ghash);\n\n\tctr = crypto_spawn_skcipher(&ictx->ctr);\n\terr = PTR_ERR(ctr);\n\tif (IS_ERR(ctr))\n\t\tgoto err_free_hash;\n\n\tctx->ctr = ctr;\n\tctx->ghash = ghash;\n\n\talign = crypto_tfm_alg_alignmask(tfm);\n\talign &= ~(crypto_tfm_ctx_alignment() - 1);\n\ttfm->crt_aead.reqsize = align +\n\t\toffsetof(struct crypto_gcm_req_priv_ctx, u) +\n\t\tmax(sizeof(struct ablkcipher_request) +\n\t\t    crypto_ablkcipher_reqsize(ctr),\n\t\t    sizeof(struct ahash_request) +\n\t\t    crypto_ahash_reqsize(ghash));\n\n\treturn 0;\n\nerr_free_hash:\n\tcrypto_free_ahash(ghash);\n\treturn err;\n}\n\nstatic void crypto_gcm_exit_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_gcm_ctx *ctx = crypto_tfm_ctx(tfm);\n\n\tcrypto_free_ahash(ctx->ghash);\n\tcrypto_free_ablkcipher(ctx->ctr);\n}\n\nstatic struct crypto_instance *crypto_gcm_alloc_common(struct rtattr **tb,\n\t\t\t\t\t\t       const char *full_name,\n\t\t\t\t\t\t       const char *ctr_name,\n\t\t\t\t\t\t       const char *ghash_name)\n{\n\tstruct crypto_attr_type *algt;\n\tstruct crypto_instance *inst;\n\tstruct crypto_alg *ctr;\n\tstruct crypto_alg *ghash_alg;\n\tstruct ahash_alg *ghash_ahash_alg;\n\tstruct gcm_instance_ctx *ctx;\n\tint err;\n\n\talgt = crypto_get_attr_type(tb);\n\tif (IS_ERR(algt))\n\t\treturn ERR_CAST(algt);\n\n\tif ((algt->type ^ CRYPTO_ALG_TYPE_AEAD) & algt->mask)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tghash_alg = crypto_find_alg(ghash_name, &crypto_ahash_type,\n\t\t\t\t    CRYPTO_ALG_TYPE_HASH,\n\t\t\t\t    CRYPTO_ALG_TYPE_AHASH_MASK);\n\tif (IS_ERR(ghash_alg))\n\t\treturn ERR_CAST(ghash_alg);\n\n\terr = -ENOMEM;\n\tinst = kzalloc(sizeof(*inst) + sizeof(*ctx), GFP_KERNEL);\n\tif (!inst)\n\t\tgoto out_put_ghash;\n\n\tctx = crypto_instance_ctx(inst);\n\tghash_ahash_alg = container_of(ghash_alg, struct ahash_alg, halg.base);\n\terr = crypto_init_ahash_spawn(&ctx->ghash, &ghash_ahash_alg->halg,\n\t\t\t\t      inst);\n\tif (err)\n\t\tgoto err_free_inst;\n\n\tcrypto_set_skcipher_spawn(&ctx->ctr, inst);\n\terr = crypto_grab_skcipher(&ctx->ctr, ctr_name, 0,\n\t\t\t\t   crypto_requires_sync(algt->type,\n\t\t\t\t\t\t\talgt->mask));\n\tif (err)\n\t\tgoto err_drop_ghash;\n\n\tctr = crypto_skcipher_spawn_alg(&ctx->ctr);\n\n\t/* We only support 16-byte blocks. */\n\tif (ctr->cra_ablkcipher.ivsize != 16)\n\t\tgoto out_put_ctr;\n\n\t/* Not a stream cipher? */\n\terr = -EINVAL;\n\tif (ctr->cra_blocksize != 1)\n\t\tgoto out_put_ctr;\n\n\terr = -ENAMETOOLONG;\n\tif (snprintf(inst->alg.cra_driver_name, CRYPTO_MAX_ALG_NAME,\n\t\t     \"gcm_base(%s,%s)\", ctr->cra_driver_name,\n\t\t     ghash_alg->cra_driver_name) >=\n\t    CRYPTO_MAX_ALG_NAME)\n\t\tgoto out_put_ctr;\n\n\tmemcpy(inst->alg.cra_name, full_name, CRYPTO_MAX_ALG_NAME);\n\n\tinst->alg.cra_flags = CRYPTO_ALG_TYPE_AEAD;\n\tinst->alg.cra_flags |= ctr->cra_flags & CRYPTO_ALG_ASYNC;\n\tinst->alg.cra_priority = ctr->cra_priority;\n\tinst->alg.cra_blocksize = 1;\n\tinst->alg.cra_alignmask = ctr->cra_alignmask | (__alignof__(u64) - 1);\n\tinst->alg.cra_type = &crypto_aead_type;\n\tinst->alg.cra_aead.ivsize = 16;\n\tinst->alg.cra_aead.maxauthsize = 16;\n\tinst->alg.cra_ctxsize = sizeof(struct crypto_gcm_ctx);\n\tinst->alg.cra_init = crypto_gcm_init_tfm;\n\tinst->alg.cra_exit = crypto_gcm_exit_tfm;\n\tinst->alg.cra_aead.setkey = crypto_gcm_setkey;\n\tinst->alg.cra_aead.setauthsize = crypto_gcm_setauthsize;\n\tinst->alg.cra_aead.encrypt = crypto_gcm_encrypt;\n\tinst->alg.cra_aead.decrypt = crypto_gcm_decrypt;\n\nout:\n\tcrypto_mod_put(ghash_alg);\n\treturn inst;\n\nout_put_ctr:\n\tcrypto_drop_skcipher(&ctx->ctr);\nerr_drop_ghash:\n\tcrypto_drop_ahash(&ctx->ghash);\nerr_free_inst:\n\tkfree(inst);\nout_put_ghash:\n\tinst = ERR_PTR(err);\n\tgoto out;\n}\n\nstatic struct crypto_instance *crypto_gcm_alloc(struct rtattr **tb)\n{\n\tconst char *cipher_name;\n\tchar ctr_name[CRYPTO_MAX_ALG_NAME];\n\tchar full_name[CRYPTO_MAX_ALG_NAME];\n\n\tcipher_name = crypto_attr_alg_name(tb[1]);\n\tif (IS_ERR(cipher_name))\n\t\treturn ERR_CAST(cipher_name);\n\n\tif (snprintf(ctr_name, CRYPTO_MAX_ALG_NAME, \"ctr(%s)\", cipher_name) >=\n\t    CRYPTO_MAX_ALG_NAME)\n\t\treturn ERR_PTR(-ENAMETOOLONG);\n\n\tif (snprintf(full_name, CRYPTO_MAX_ALG_NAME, \"gcm(%s)\", cipher_name) >=\n\t    CRYPTO_MAX_ALG_NAME)\n\t\treturn ERR_PTR(-ENAMETOOLONG);\n\n\treturn crypto_gcm_alloc_common(tb, full_name, ctr_name, \"ghash\");\n}\n\nstatic void crypto_gcm_free(struct crypto_instance *inst)\n{\n\tstruct gcm_instance_ctx *ctx = crypto_instance_ctx(inst);\n\n\tcrypto_drop_skcipher(&ctx->ctr);\n\tcrypto_drop_ahash(&ctx->ghash);\n\tkfree(inst);\n}\n\nstatic struct crypto_template crypto_gcm_tmpl = {\n\t.name = \"gcm\",\n\t.alloc = crypto_gcm_alloc,\n\t.free = crypto_gcm_free,\n\t.module = THIS_MODULE,\n};\n\nstatic struct crypto_instance *crypto_gcm_base_alloc(struct rtattr **tb)\n{\n\tconst char *ctr_name;\n\tconst char *ghash_name;\n\tchar full_name[CRYPTO_MAX_ALG_NAME];\n\n\tctr_name = crypto_attr_alg_name(tb[1]);\n\tif (IS_ERR(ctr_name))\n\t\treturn ERR_CAST(ctr_name);\n\n\tghash_name = crypto_attr_alg_name(tb[2]);\n\tif (IS_ERR(ghash_name))\n\t\treturn ERR_CAST(ghash_name);\n\n\tif (snprintf(full_name, CRYPTO_MAX_ALG_NAME, \"gcm_base(%s,%s)\",\n\t\t     ctr_name, ghash_name) >= CRYPTO_MAX_ALG_NAME)\n\t\treturn ERR_PTR(-ENAMETOOLONG);\n\n\treturn crypto_gcm_alloc_common(tb, full_name, ctr_name, ghash_name);\n}\n\nstatic struct crypto_template crypto_gcm_base_tmpl = {\n\t.name = \"gcm_base\",\n\t.alloc = crypto_gcm_base_alloc,\n\t.free = crypto_gcm_free,\n\t.module = THIS_MODULE,\n};\n\nstatic int crypto_rfc4106_setkey(struct crypto_aead *parent, const u8 *key,\n\t\t\t\t unsigned int keylen)\n{\n\tstruct crypto_rfc4106_ctx *ctx = crypto_aead_ctx(parent);\n\tstruct crypto_aead *child = ctx->child;\n\tint err;\n\n\tif (keylen < 4)\n\t\treturn -EINVAL;\n\n\tkeylen -= 4;\n\tmemcpy(ctx->nonce, key + keylen, 4);\n\n\tcrypto_aead_clear_flags(child, CRYPTO_TFM_REQ_MASK);\n\tcrypto_aead_set_flags(child, crypto_aead_get_flags(parent) &\n\t\t\t\t     CRYPTO_TFM_REQ_MASK);\n\terr = crypto_aead_setkey(child, key, keylen);\n\tcrypto_aead_set_flags(parent, crypto_aead_get_flags(child) &\n\t\t\t\t      CRYPTO_TFM_RES_MASK);\n\n\treturn err;\n}\n\nstatic int crypto_rfc4106_setauthsize(struct crypto_aead *parent,\n\t\t\t\t      unsigned int authsize)\n{\n\tstruct crypto_rfc4106_ctx *ctx = crypto_aead_ctx(parent);\n\n\tswitch (authsize) {\n\tcase 8:\n\tcase 12:\n\tcase 16:\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn crypto_aead_setauthsize(ctx->child, authsize);\n}\n\nstatic struct aead_request *crypto_rfc4106_crypt(struct aead_request *req)\n{\n\tstruct aead_request *subreq = aead_request_ctx(req);\n\tstruct crypto_aead *aead = crypto_aead_reqtfm(req);\n\tstruct crypto_rfc4106_ctx *ctx = crypto_aead_ctx(aead);\n\tstruct crypto_aead *child = ctx->child;\n\tu8 *iv = PTR_ALIGN((u8 *)(subreq + 1) + crypto_aead_reqsize(child),\n\t\t\t   crypto_aead_alignmask(child) + 1);\n\n\tmemcpy(iv, ctx->nonce, 4);\n\tmemcpy(iv + 4, req->iv, 8);\n\n\taead_request_set_tfm(subreq, child);\n\taead_request_set_callback(subreq, req->base.flags, req->base.complete,\n\t\t\t\t  req->base.data);\n\taead_request_set_crypt(subreq, req->src, req->dst, req->cryptlen, iv);\n\taead_request_set_assoc(subreq, req->assoc, req->assoclen);\n\n\treturn subreq;\n}\n\nstatic int crypto_rfc4106_encrypt(struct aead_request *req)\n{\n\treq = crypto_rfc4106_crypt(req);\n\n\treturn crypto_aead_encrypt(req);\n}\n\nstatic int crypto_rfc4106_decrypt(struct aead_request *req)\n{\n\treq = crypto_rfc4106_crypt(req);\n\n\treturn crypto_aead_decrypt(req);\n}\n\nstatic int crypto_rfc4106_init_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_instance *inst = (void *)tfm->__crt_alg;\n\tstruct crypto_aead_spawn *spawn = crypto_instance_ctx(inst);\n\tstruct crypto_rfc4106_ctx *ctx = crypto_tfm_ctx(tfm);\n\tstruct crypto_aead *aead;\n\tunsigned long align;\n\n\taead = crypto_spawn_aead(spawn);\n\tif (IS_ERR(aead))\n\t\treturn PTR_ERR(aead);\n\n\tctx->child = aead;\n\n\talign = crypto_aead_alignmask(aead);\n\talign &= ~(crypto_tfm_ctx_alignment() - 1);\n\ttfm->crt_aead.reqsize = sizeof(struct aead_request) +\n\t\t\t\tALIGN(crypto_aead_reqsize(aead),\n\t\t\t\t      crypto_tfm_ctx_alignment()) +\n\t\t\t\talign + 16;\n\n\treturn 0;\n}\n\nstatic void crypto_rfc4106_exit_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_rfc4106_ctx *ctx = crypto_tfm_ctx(tfm);\n\n\tcrypto_free_aead(ctx->child);\n}\n\nstatic struct crypto_instance *crypto_rfc4106_alloc(struct rtattr **tb)\n{\n\tstruct crypto_attr_type *algt;\n\tstruct crypto_instance *inst;\n\tstruct crypto_aead_spawn *spawn;\n\tstruct crypto_alg *alg;\n\tconst char *ccm_name;\n\tint err;\n\n\talgt = crypto_get_attr_type(tb);\n\tif (IS_ERR(algt))\n\t\treturn ERR_CAST(algt);\n\n\tif ((algt->type ^ CRYPTO_ALG_TYPE_AEAD) & algt->mask)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tccm_name = crypto_attr_alg_name(tb[1]);\n\tif (IS_ERR(ccm_name))\n\t\treturn ERR_CAST(ccm_name);\n\n\tinst = kzalloc(sizeof(*inst) + sizeof(*spawn), GFP_KERNEL);\n\tif (!inst)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tspawn = crypto_instance_ctx(inst);\n\tcrypto_set_aead_spawn(spawn, inst);\n\terr = crypto_grab_aead(spawn, ccm_name, 0,\n\t\t\t       crypto_requires_sync(algt->type, algt->mask));\n\tif (err)\n\t\tgoto out_free_inst;\n\n\talg = crypto_aead_spawn_alg(spawn);\n\n\terr = -EINVAL;\n\n\t/* We only support 16-byte blocks. */\n\tif (alg->cra_aead.ivsize != 16)\n\t\tgoto out_drop_alg;\n\n\t/* Not a stream cipher? */\n\tif (alg->cra_blocksize != 1)\n\t\tgoto out_drop_alg;\n\n\terr = -ENAMETOOLONG;\n\tif (snprintf(inst->alg.cra_name, CRYPTO_MAX_ALG_NAME,\n\t\t     \"rfc4106(%s)\", alg->cra_name) >= CRYPTO_MAX_ALG_NAME ||\n\t    snprintf(inst->alg.cra_driver_name, CRYPTO_MAX_ALG_NAME,\n\t\t     \"rfc4106(%s)\", alg->cra_driver_name) >=\n\t    CRYPTO_MAX_ALG_NAME)\n\t\tgoto out_drop_alg;\n\n\tinst->alg.cra_flags = CRYPTO_ALG_TYPE_AEAD;\n\tinst->alg.cra_flags |= alg->cra_flags & CRYPTO_ALG_ASYNC;\n\tinst->alg.cra_priority = alg->cra_priority;\n\tinst->alg.cra_blocksize = 1;\n\tinst->alg.cra_alignmask = alg->cra_alignmask;\n\tinst->alg.cra_type = &crypto_nivaead_type;\n\n\tinst->alg.cra_aead.ivsize = 8;\n\tinst->alg.cra_aead.maxauthsize = 16;\n\n\tinst->alg.cra_ctxsize = sizeof(struct crypto_rfc4106_ctx);\n\n\tinst->alg.cra_init = crypto_rfc4106_init_tfm;\n\tinst->alg.cra_exit = crypto_rfc4106_exit_tfm;\n\n\tinst->alg.cra_aead.setkey = crypto_rfc4106_setkey;\n\tinst->alg.cra_aead.setauthsize = crypto_rfc4106_setauthsize;\n\tinst->alg.cra_aead.encrypt = crypto_rfc4106_encrypt;\n\tinst->alg.cra_aead.decrypt = crypto_rfc4106_decrypt;\n\n\tinst->alg.cra_aead.geniv = \"seqiv\";\n\nout:\n\treturn inst;\n\nout_drop_alg:\n\tcrypto_drop_aead(spawn);\nout_free_inst:\n\tkfree(inst);\n\tinst = ERR_PTR(err);\n\tgoto out;\n}\n\nstatic void crypto_rfc4106_free(struct crypto_instance *inst)\n{\n\tcrypto_drop_spawn(crypto_instance_ctx(inst));\n\tkfree(inst);\n}\n\nstatic struct crypto_template crypto_rfc4106_tmpl = {\n\t.name = \"rfc4106\",\n\t.alloc = crypto_rfc4106_alloc,\n\t.free = crypto_rfc4106_free,\n\t.module = THIS_MODULE,\n};\n\nstatic inline struct crypto_rfc4543_req_ctx *crypto_rfc4543_reqctx(\n\tstruct aead_request *req)\n{\n\tunsigned long align = crypto_aead_alignmask(crypto_aead_reqtfm(req));\n\n\treturn (void *)PTR_ALIGN((u8 *)aead_request_ctx(req), align + 1);\n}\n\nstatic int crypto_rfc4543_setkey(struct crypto_aead *parent, const u8 *key,\n\t\t\t\t unsigned int keylen)\n{\n\tstruct crypto_rfc4543_ctx *ctx = crypto_aead_ctx(parent);\n\tstruct crypto_aead *child = ctx->child;\n\tint err;\n\n\tif (keylen < 4)\n\t\treturn -EINVAL;\n\n\tkeylen -= 4;\n\tmemcpy(ctx->nonce, key + keylen, 4);\n\n\tcrypto_aead_clear_flags(child, CRYPTO_TFM_REQ_MASK);\n\tcrypto_aead_set_flags(child, crypto_aead_get_flags(parent) &\n\t\t\t\t     CRYPTO_TFM_REQ_MASK);\n\terr = crypto_aead_setkey(child, key, keylen);\n\tcrypto_aead_set_flags(parent, crypto_aead_get_flags(child) &\n\t\t\t\t      CRYPTO_TFM_RES_MASK);\n\n\treturn err;\n}\n\nstatic int crypto_rfc4543_setauthsize(struct crypto_aead *parent,\n\t\t\t\t      unsigned int authsize)\n{\n\tstruct crypto_rfc4543_ctx *ctx = crypto_aead_ctx(parent);\n\n\tif (authsize != 16)\n\t\treturn -EINVAL;\n\n\treturn crypto_aead_setauthsize(ctx->child, authsize);\n}\n\nstatic void crypto_rfc4543_done(struct crypto_async_request *areq, int err)\n{\n\tstruct aead_request *req = areq->data;\n\tstruct crypto_aead *aead = crypto_aead_reqtfm(req);\n\tstruct crypto_rfc4543_req_ctx *rctx = crypto_rfc4543_reqctx(req);\n\n\tif (!err) {\n\t\tscatterwalk_map_and_copy(rctx->auth_tag, req->dst,\n\t\t\t\t\t req->cryptlen,\n\t\t\t\t\t crypto_aead_authsize(aead), 1);\n\t}\n\n\taead_request_complete(req, err);\n}\n\nstatic struct aead_request *crypto_rfc4543_crypt(struct aead_request *req,\n\t\t\t\t\t\t bool enc)\n{\n\tstruct crypto_aead *aead = crypto_aead_reqtfm(req);\n\tstruct crypto_rfc4543_ctx *ctx = crypto_aead_ctx(aead);\n\tstruct crypto_rfc4543_req_ctx *rctx = crypto_rfc4543_reqctx(req);\n\tstruct aead_request *subreq = &rctx->subreq;\n\tstruct scatterlist *src = req->src;\n\tstruct scatterlist *cipher = rctx->cipher;\n\tstruct scatterlist *payload = rctx->payload;\n\tstruct scatterlist *assoc = rctx->assoc;\n\tunsigned int authsize = crypto_aead_authsize(aead);\n\tunsigned int assoclen = req->assoclen;\n\tstruct page *srcp;\n\tu8 *vsrc;\n\tu8 *iv = PTR_ALIGN((u8 *)(rctx + 1) + crypto_aead_reqsize(ctx->child),\n\t\t\t   crypto_aead_alignmask(ctx->child) + 1);\n\n\tmemcpy(iv, ctx->nonce, 4);\n\tmemcpy(iv + 4, req->iv, 8);\n\n\t/* construct cipher/plaintext */\n\tif (enc)\n\t\tmemset(rctx->auth_tag, 0, authsize);\n\telse\n\t\tscatterwalk_map_and_copy(rctx->auth_tag, src,\n\t\t\t\t\t req->cryptlen - authsize,\n\t\t\t\t\t authsize, 0);\n\n\tsg_init_one(cipher, rctx->auth_tag, authsize);\n\n\t/* construct the aad */\n\tsrcp = sg_page(src);\n\tvsrc = PageHighMem(srcp) ? NULL : page_address(srcp) + src->offset;\n\n\tsg_init_table(payload, 2);\n\tsg_set_buf(payload, req->iv, 8);\n\tscatterwalk_crypto_chain(payload, src, vsrc == req->iv + 8, 2);\n\tassoclen += 8 + req->cryptlen - (enc ? 0 : authsize);\n\n\tif (req->assoc->length == req->assoclen) {\n\t\tsg_init_table(assoc, 2);\n\t\tsg_set_page(assoc, sg_page(req->assoc), req->assoc->length,\n\t\t\t    req->assoc->offset);\n\t} else {\n\t\tBUG_ON(req->assoclen > sizeof(rctx->assocbuf));\n\n\t\tscatterwalk_map_and_copy(rctx->assocbuf, req->assoc, 0,\n\t\t\t\t\t req->assoclen, 0);\n\n\t\tsg_init_table(assoc, 2);\n\t\tsg_set_buf(assoc, rctx->assocbuf, req->assoclen);\n\t}\n\tscatterwalk_crypto_chain(assoc, payload, 0, 2);\n\n\taead_request_set_tfm(subreq, ctx->child);\n\taead_request_set_callback(subreq, req->base.flags, crypto_rfc4543_done,\n\t\t\t\t  req);\n\taead_request_set_crypt(subreq, cipher, cipher, enc ? 0 : authsize, iv);\n\taead_request_set_assoc(subreq, assoc, assoclen);\n\n\treturn subreq;\n}\n\nstatic int crypto_rfc4543_copy_src_to_dst(struct aead_request *req, bool enc)\n{\n\tstruct crypto_aead *aead = crypto_aead_reqtfm(req);\n\tstruct crypto_rfc4543_ctx *ctx = crypto_aead_ctx(aead);\n\tunsigned int authsize = crypto_aead_authsize(aead);\n\tunsigned int nbytes = req->cryptlen - (enc ? 0 : authsize);\n\tstruct blkcipher_desc desc = {\n\t\t.tfm = ctx->null,\n\t};\n\n\treturn crypto_blkcipher_encrypt(&desc, req->dst, req->src, nbytes);\n}\n\nstatic int crypto_rfc4543_encrypt(struct aead_request *req)\n{\n\tstruct crypto_aead *aead = crypto_aead_reqtfm(req);\n\tstruct crypto_rfc4543_req_ctx *rctx = crypto_rfc4543_reqctx(req);\n\tstruct aead_request *subreq;\n\tint err;\n\n\tif (req->src != req->dst) {\n\t\terr = crypto_rfc4543_copy_src_to_dst(req, true);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tsubreq = crypto_rfc4543_crypt(req, true);\n\terr = crypto_aead_encrypt(subreq);\n\tif (err)\n\t\treturn err;\n\n\tscatterwalk_map_and_copy(rctx->auth_tag, req->dst, req->cryptlen,\n\t\t\t\t crypto_aead_authsize(aead), 1);\n\n\treturn 0;\n}\n\nstatic int crypto_rfc4543_decrypt(struct aead_request *req)\n{\n\tint err;\n\n\tif (req->src != req->dst) {\n\t\terr = crypto_rfc4543_copy_src_to_dst(req, false);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\treq = crypto_rfc4543_crypt(req, false);\n\n\treturn crypto_aead_decrypt(req);\n}\n\nstatic int crypto_rfc4543_init_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_instance *inst = (void *)tfm->__crt_alg;\n\tstruct crypto_rfc4543_instance_ctx *ictx = crypto_instance_ctx(inst);\n\tstruct crypto_aead_spawn *spawn = &ictx->aead;\n\tstruct crypto_rfc4543_ctx *ctx = crypto_tfm_ctx(tfm);\n\tstruct crypto_aead *aead;\n\tstruct crypto_blkcipher *null;\n\tunsigned long align;\n\tint err = 0;\n\n\taead = crypto_spawn_aead(spawn);\n\tif (IS_ERR(aead))\n\t\treturn PTR_ERR(aead);\n\n\tnull = crypto_spawn_blkcipher(&ictx->null.base);\n\terr = PTR_ERR(null);\n\tif (IS_ERR(null))\n\t\tgoto err_free_aead;\n\n\tctx->child = aead;\n\tctx->null = null;\n\n\talign = crypto_aead_alignmask(aead);\n\talign &= ~(crypto_tfm_ctx_alignment() - 1);\n\ttfm->crt_aead.reqsize = sizeof(struct crypto_rfc4543_req_ctx) +\n\t\t\t\tALIGN(crypto_aead_reqsize(aead),\n\t\t\t\t      crypto_tfm_ctx_alignment()) +\n\t\t\t\talign + 16;\n\n\treturn 0;\n\nerr_free_aead:\n\tcrypto_free_aead(aead);\n\treturn err;\n}\n\nstatic void crypto_rfc4543_exit_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_rfc4543_ctx *ctx = crypto_tfm_ctx(tfm);\n\n\tcrypto_free_aead(ctx->child);\n\tcrypto_free_blkcipher(ctx->null);\n}\n\nstatic struct crypto_instance *crypto_rfc4543_alloc(struct rtattr **tb)\n{\n\tstruct crypto_attr_type *algt;\n\tstruct crypto_instance *inst;\n\tstruct crypto_aead_spawn *spawn;\n\tstruct crypto_alg *alg;\n\tstruct crypto_rfc4543_instance_ctx *ctx;\n\tconst char *ccm_name;\n\tint err;\n\n\talgt = crypto_get_attr_type(tb);\n\tif (IS_ERR(algt))\n\t\treturn ERR_CAST(algt);\n\n\tif ((algt->type ^ CRYPTO_ALG_TYPE_AEAD) & algt->mask)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tccm_name = crypto_attr_alg_name(tb[1]);\n\tif (IS_ERR(ccm_name))\n\t\treturn ERR_CAST(ccm_name);\n\n\tinst = kzalloc(sizeof(*inst) + sizeof(*ctx), GFP_KERNEL);\n\tif (!inst)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tctx = crypto_instance_ctx(inst);\n\tspawn = &ctx->aead;\n\tcrypto_set_aead_spawn(spawn, inst);\n\terr = crypto_grab_aead(spawn, ccm_name, 0,\n\t\t\t       crypto_requires_sync(algt->type, algt->mask));\n\tif (err)\n\t\tgoto out_free_inst;\n\n\talg = crypto_aead_spawn_alg(spawn);\n\n\tcrypto_set_skcipher_spawn(&ctx->null, inst);\n\terr = crypto_grab_skcipher(&ctx->null, \"ecb(cipher_null)\", 0,\n\t\t\t\t   CRYPTO_ALG_ASYNC);\n\tif (err)\n\t\tgoto out_drop_alg;\n\n\tcrypto_skcipher_spawn_alg(&ctx->null);\n\n\terr = -EINVAL;\n\n\t/* We only support 16-byte blocks. */\n\tif (alg->cra_aead.ivsize != 16)\n\t\tgoto out_drop_ecbnull;\n\n\t/* Not a stream cipher? */\n\tif (alg->cra_blocksize != 1)\n\t\tgoto out_drop_ecbnull;\n\n\terr = -ENAMETOOLONG;\n\tif (snprintf(inst->alg.cra_name, CRYPTO_MAX_ALG_NAME,\n\t\t     \"rfc4543(%s)\", alg->cra_name) >= CRYPTO_MAX_ALG_NAME ||\n\t    snprintf(inst->alg.cra_driver_name, CRYPTO_MAX_ALG_NAME,\n\t\t     \"rfc4543(%s)\", alg->cra_driver_name) >=\n\t    CRYPTO_MAX_ALG_NAME)\n\t\tgoto out_drop_ecbnull;\n\n\tinst->alg.cra_flags = CRYPTO_ALG_TYPE_AEAD;\n\tinst->alg.cra_flags |= alg->cra_flags & CRYPTO_ALG_ASYNC;\n\tinst->alg.cra_priority = alg->cra_priority;\n\tinst->alg.cra_blocksize = 1;\n\tinst->alg.cra_alignmask = alg->cra_alignmask;\n\tinst->alg.cra_type = &crypto_nivaead_type;\n\n\tinst->alg.cra_aead.ivsize = 8;\n\tinst->alg.cra_aead.maxauthsize = 16;\n\n\tinst->alg.cra_ctxsize = sizeof(struct crypto_rfc4543_ctx);\n\n\tinst->alg.cra_init = crypto_rfc4543_init_tfm;\n\tinst->alg.cra_exit = crypto_rfc4543_exit_tfm;\n\n\tinst->alg.cra_aead.setkey = crypto_rfc4543_setkey;\n\tinst->alg.cra_aead.setauthsize = crypto_rfc4543_setauthsize;\n\tinst->alg.cra_aead.encrypt = crypto_rfc4543_encrypt;\n\tinst->alg.cra_aead.decrypt = crypto_rfc4543_decrypt;\n\n\tinst->alg.cra_aead.geniv = \"seqiv\";\n\nout:\n\treturn inst;\n\nout_drop_ecbnull:\n\tcrypto_drop_skcipher(&ctx->null);\nout_drop_alg:\n\tcrypto_drop_aead(spawn);\nout_free_inst:\n\tkfree(inst);\n\tinst = ERR_PTR(err);\n\tgoto out;\n}\n\nstatic void crypto_rfc4543_free(struct crypto_instance *inst)\n{\n\tstruct crypto_rfc4543_instance_ctx *ctx = crypto_instance_ctx(inst);\n\n\tcrypto_drop_aead(&ctx->aead);\n\tcrypto_drop_skcipher(&ctx->null);\n\n\tkfree(inst);\n}\n\nstatic struct crypto_template crypto_rfc4543_tmpl = {\n\t.name = \"rfc4543\",\n\t.alloc = crypto_rfc4543_alloc,\n\t.free = crypto_rfc4543_free,\n\t.module = THIS_MODULE,\n};\n\nstatic int __init crypto_gcm_module_init(void)\n{\n\tint err;\n\n\tgcm_zeroes = kzalloc(16, GFP_KERNEL);\n\tif (!gcm_zeroes)\n\t\treturn -ENOMEM;\n\n\terr = crypto_register_template(&crypto_gcm_base_tmpl);\n\tif (err)\n\t\tgoto out;\n\n\terr = crypto_register_template(&crypto_gcm_tmpl);\n\tif (err)\n\t\tgoto out_undo_base;\n\n\terr = crypto_register_template(&crypto_rfc4106_tmpl);\n\tif (err)\n\t\tgoto out_undo_gcm;\n\n\terr = crypto_register_template(&crypto_rfc4543_tmpl);\n\tif (err)\n\t\tgoto out_undo_rfc4106;\n\n\treturn 0;\n\nout_undo_rfc4106:\n\tcrypto_unregister_template(&crypto_rfc4106_tmpl);\nout_undo_gcm:\n\tcrypto_unregister_template(&crypto_gcm_tmpl);\nout_undo_base:\n\tcrypto_unregister_template(&crypto_gcm_base_tmpl);\nout:\n\tkfree(gcm_zeroes);\n\treturn err;\n}\n\nstatic void __exit crypto_gcm_module_exit(void)\n{\n\tkfree(gcm_zeroes);\n\tcrypto_unregister_template(&crypto_rfc4543_tmpl);\n\tcrypto_unregister_template(&crypto_rfc4106_tmpl);\n\tcrypto_unregister_template(&crypto_gcm_tmpl);\n\tcrypto_unregister_template(&crypto_gcm_base_tmpl);\n}\n\nmodule_init(crypto_gcm_module_init);\nmodule_exit(crypto_gcm_module_exit);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"Galois/Counter Mode\");\nMODULE_AUTHOR(\"Mikko Herranen <mh1@iki.fi>\");\nMODULE_ALIAS_CRYPTO(\"gcm_base\");\nMODULE_ALIAS_CRYPTO(\"rfc4106\");\nMODULE_ALIAS_CRYPTO(\"rfc4543\");\nMODULE_ALIAS_CRYPTO(\"gcm\");\n", "/*\n * Cryptographic API.\n *\n * HMAC: Keyed-Hashing for Message Authentication (RFC2104).\n *\n * Copyright (c) 2002 James Morris <jmorris@intercode.com.au>\n * Copyright (c) 2006 Herbert Xu <herbert@gondor.apana.org.au>\n *\n * The HMAC implementation is derived from USAGI.\n * Copyright (c) 2002 Kazunori Miyazawa <miyazawa@linux-ipv6.org> / USAGI\n *\n * This program is free software; you can redistribute it and/or modify it\n * under the terms of the GNU General Public License as published by the Free\n * Software Foundation; either version 2 of the License, or (at your option)\n * any later version.\n *\n */\n\n#include <crypto/internal/hash.h>\n#include <crypto/scatterwalk.h>\n#include <linux/err.h>\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/scatterlist.h>\n#include <linux/string.h>\n\nstruct hmac_ctx {\n\tstruct crypto_shash *hash;\n};\n\nstatic inline void *align_ptr(void *p, unsigned int align)\n{\n\treturn (void *)ALIGN((unsigned long)p, align);\n}\n\nstatic inline struct hmac_ctx *hmac_ctx(struct crypto_shash *tfm)\n{\n\treturn align_ptr(crypto_shash_ctx_aligned(tfm) +\n\t\t\t crypto_shash_statesize(tfm) * 2,\n\t\t\t crypto_tfm_ctx_alignment());\n}\n\nstatic int hmac_setkey(struct crypto_shash *parent,\n\t\t       const u8 *inkey, unsigned int keylen)\n{\n\tint bs = crypto_shash_blocksize(parent);\n\tint ds = crypto_shash_digestsize(parent);\n\tint ss = crypto_shash_statesize(parent);\n\tchar *ipad = crypto_shash_ctx_aligned(parent);\n\tchar *opad = ipad + ss;\n\tstruct hmac_ctx *ctx = align_ptr(opad + ss,\n\t\t\t\t\t crypto_tfm_ctx_alignment());\n\tstruct crypto_shash *hash = ctx->hash;\n\tSHASH_DESC_ON_STACK(shash, hash);\n\tunsigned int i;\n\n\tshash->tfm = hash;\n\tshash->flags = crypto_shash_get_flags(parent)\n\t\t& CRYPTO_TFM_REQ_MAY_SLEEP;\n\n\tif (keylen > bs) {\n\t\tint err;\n\n\t\terr = crypto_shash_digest(shash, inkey, keylen, ipad);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tkeylen = ds;\n\t} else\n\t\tmemcpy(ipad, inkey, keylen);\n\n\tmemset(ipad + keylen, 0, bs - keylen);\n\tmemcpy(opad, ipad, bs);\n\n\tfor (i = 0; i < bs; i++) {\n\t\tipad[i] ^= 0x36;\n\t\topad[i] ^= 0x5c;\n\t}\n\n\treturn crypto_shash_init(shash) ?:\n\t       crypto_shash_update(shash, ipad, bs) ?:\n\t       crypto_shash_export(shash, ipad) ?:\n\t       crypto_shash_init(shash) ?:\n\t       crypto_shash_update(shash, opad, bs) ?:\n\t       crypto_shash_export(shash, opad);\n}\n\nstatic int hmac_export(struct shash_desc *pdesc, void *out)\n{\n\tstruct shash_desc *desc = shash_desc_ctx(pdesc);\n\n\tdesc->flags = pdesc->flags & CRYPTO_TFM_REQ_MAY_SLEEP;\n\n\treturn crypto_shash_export(desc, out);\n}\n\nstatic int hmac_import(struct shash_desc *pdesc, const void *in)\n{\n\tstruct shash_desc *desc = shash_desc_ctx(pdesc);\n\tstruct hmac_ctx *ctx = hmac_ctx(pdesc->tfm);\n\n\tdesc->tfm = ctx->hash;\n\tdesc->flags = pdesc->flags & CRYPTO_TFM_REQ_MAY_SLEEP;\n\n\treturn crypto_shash_import(desc, in);\n}\n\nstatic int hmac_init(struct shash_desc *pdesc)\n{\n\treturn hmac_import(pdesc, crypto_shash_ctx_aligned(pdesc->tfm));\n}\n\nstatic int hmac_update(struct shash_desc *pdesc,\n\t\t       const u8 *data, unsigned int nbytes)\n{\n\tstruct shash_desc *desc = shash_desc_ctx(pdesc);\n\n\tdesc->flags = pdesc->flags & CRYPTO_TFM_REQ_MAY_SLEEP;\n\n\treturn crypto_shash_update(desc, data, nbytes);\n}\n\nstatic int hmac_final(struct shash_desc *pdesc, u8 *out)\n{\n\tstruct crypto_shash *parent = pdesc->tfm;\n\tint ds = crypto_shash_digestsize(parent);\n\tint ss = crypto_shash_statesize(parent);\n\tchar *opad = crypto_shash_ctx_aligned(parent) + ss;\n\tstruct shash_desc *desc = shash_desc_ctx(pdesc);\n\n\tdesc->flags = pdesc->flags & CRYPTO_TFM_REQ_MAY_SLEEP;\n\n\treturn crypto_shash_final(desc, out) ?:\n\t       crypto_shash_import(desc, opad) ?:\n\t       crypto_shash_finup(desc, out, ds, out);\n}\n\nstatic int hmac_finup(struct shash_desc *pdesc, const u8 *data,\n\t\t      unsigned int nbytes, u8 *out)\n{\n\n\tstruct crypto_shash *parent = pdesc->tfm;\n\tint ds = crypto_shash_digestsize(parent);\n\tint ss = crypto_shash_statesize(parent);\n\tchar *opad = crypto_shash_ctx_aligned(parent) + ss;\n\tstruct shash_desc *desc = shash_desc_ctx(pdesc);\n\n\tdesc->flags = pdesc->flags & CRYPTO_TFM_REQ_MAY_SLEEP;\n\n\treturn crypto_shash_finup(desc, data, nbytes, out) ?:\n\t       crypto_shash_import(desc, opad) ?:\n\t       crypto_shash_finup(desc, out, ds, out);\n}\n\nstatic int hmac_init_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_shash *parent = __crypto_shash_cast(tfm);\n\tstruct crypto_shash *hash;\n\tstruct crypto_instance *inst = (void *)tfm->__crt_alg;\n\tstruct crypto_shash_spawn *spawn = crypto_instance_ctx(inst);\n\tstruct hmac_ctx *ctx = hmac_ctx(parent);\n\n\thash = crypto_spawn_shash(spawn);\n\tif (IS_ERR(hash))\n\t\treturn PTR_ERR(hash);\n\n\tparent->descsize = sizeof(struct shash_desc) +\n\t\t\t   crypto_shash_descsize(hash);\n\n\tctx->hash = hash;\n\treturn 0;\n}\n\nstatic void hmac_exit_tfm(struct crypto_tfm *tfm)\n{\n\tstruct hmac_ctx *ctx = hmac_ctx(__crypto_shash_cast(tfm));\n\tcrypto_free_shash(ctx->hash);\n}\n\nstatic int hmac_create(struct crypto_template *tmpl, struct rtattr **tb)\n{\n\tstruct shash_instance *inst;\n\tstruct crypto_alg *alg;\n\tstruct shash_alg *salg;\n\tint err;\n\tint ds;\n\tint ss;\n\n\terr = crypto_check_attr_type(tb, CRYPTO_ALG_TYPE_SHASH);\n\tif (err)\n\t\treturn err;\n\n\tsalg = shash_attr_alg(tb[1], 0, 0);\n\tif (IS_ERR(salg))\n\t\treturn PTR_ERR(salg);\n\n\terr = -EINVAL;\n\tds = salg->digestsize;\n\tss = salg->statesize;\n\talg = &salg->base;\n\tif (ds > alg->cra_blocksize ||\n\t    ss < alg->cra_blocksize)\n\t\tgoto out_put_alg;\n\n\tinst = shash_alloc_instance(\"hmac\", alg);\n\terr = PTR_ERR(inst);\n\tif (IS_ERR(inst))\n\t\tgoto out_put_alg;\n\n\terr = crypto_init_shash_spawn(shash_instance_ctx(inst), salg,\n\t\t\t\t      shash_crypto_instance(inst));\n\tif (err)\n\t\tgoto out_free_inst;\n\n\tinst->alg.base.cra_priority = alg->cra_priority;\n\tinst->alg.base.cra_blocksize = alg->cra_blocksize;\n\tinst->alg.base.cra_alignmask = alg->cra_alignmask;\n\n\tss = ALIGN(ss, alg->cra_alignmask + 1);\n\tinst->alg.digestsize = ds;\n\tinst->alg.statesize = ss;\n\n\tinst->alg.base.cra_ctxsize = sizeof(struct hmac_ctx) +\n\t\t\t\t     ALIGN(ss * 2, crypto_tfm_ctx_alignment());\n\n\tinst->alg.base.cra_init = hmac_init_tfm;\n\tinst->alg.base.cra_exit = hmac_exit_tfm;\n\n\tinst->alg.init = hmac_init;\n\tinst->alg.update = hmac_update;\n\tinst->alg.final = hmac_final;\n\tinst->alg.finup = hmac_finup;\n\tinst->alg.export = hmac_export;\n\tinst->alg.import = hmac_import;\n\tinst->alg.setkey = hmac_setkey;\n\n\terr = shash_register_instance(tmpl, inst);\n\tif (err) {\nout_free_inst:\n\t\tshash_free_instance(shash_crypto_instance(inst));\n\t}\n\nout_put_alg:\n\tcrypto_mod_put(alg);\n\treturn err;\n}\n\nstatic struct crypto_template hmac_tmpl = {\n\t.name = \"hmac\",\n\t.create = hmac_create,\n\t.free = shash_free_instance,\n\t.module = THIS_MODULE,\n};\n\nstatic int __init hmac_module_init(void)\n{\n\treturn crypto_register_template(&hmac_tmpl);\n}\n\nstatic void __exit hmac_module_exit(void)\n{\n\tcrypto_unregister_template(&hmac_tmpl);\n}\n\nmodule_init(hmac_module_init);\nmodule_exit(hmac_module_exit);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"HMAC hash algorithm\");\nMODULE_ALIAS_CRYPTO(\"hmac\");\n", "/* LRW: as defined by Cyril Guyot in\n *\thttp://grouper.ieee.org/groups/1619/email/pdf00017.pdf\n *\n * Copyright (c) 2006 Rik Snel <rsnel@cube.dyndns.org>\n *\n * Based on ecb.c\n * Copyright (c) 2006 Herbert Xu <herbert@gondor.apana.org.au>\n *\n * This program is free software; you can redistribute it and/or modify it\n * under the terms of the GNU General Public License as published by the Free\n * Software Foundation; either version 2 of the License, or (at your option)\n * any later version.\n */\n/* This implementation is checked against the test vectors in the above\n * document and by a test vector provided by Ken Buchanan at\n * http://www.mail-archive.com/stds-p1619@listserv.ieee.org/msg00173.html\n *\n * The test vectors are included in the testing module tcrypt.[ch] */\n\n#include <crypto/algapi.h>\n#include <linux/err.h>\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/scatterlist.h>\n#include <linux/slab.h>\n\n#include <crypto/b128ops.h>\n#include <crypto/gf128mul.h>\n#include <crypto/lrw.h>\n\nstruct priv {\n\tstruct crypto_cipher *child;\n\tstruct lrw_table_ctx table;\n};\n\nstatic inline void setbit128_bbe(void *b, int bit)\n{\n\t__set_bit(bit ^ (0x80 -\n#ifdef __BIG_ENDIAN\n\t\t\t BITS_PER_LONG\n#else\n\t\t\t BITS_PER_BYTE\n#endif\n\t\t\t), b);\n}\n\nint lrw_init_table(struct lrw_table_ctx *ctx, const u8 *tweak)\n{\n\tbe128 tmp = { 0 };\n\tint i;\n\n\tif (ctx->table)\n\t\tgf128mul_free_64k(ctx->table);\n\n\t/* initialize multiplication table for Key2 */\n\tctx->table = gf128mul_init_64k_bbe((be128 *)tweak);\n\tif (!ctx->table)\n\t\treturn -ENOMEM;\n\n\t/* initialize optimization table */\n\tfor (i = 0; i < 128; i++) {\n\t\tsetbit128_bbe(&tmp, i);\n\t\tctx->mulinc[i] = tmp;\n\t\tgf128mul_64k_bbe(&ctx->mulinc[i], ctx->table);\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(lrw_init_table);\n\nvoid lrw_free_table(struct lrw_table_ctx *ctx)\n{\n\tif (ctx->table)\n\t\tgf128mul_free_64k(ctx->table);\n}\nEXPORT_SYMBOL_GPL(lrw_free_table);\n\nstatic int setkey(struct crypto_tfm *parent, const u8 *key,\n\t\t  unsigned int keylen)\n{\n\tstruct priv *ctx = crypto_tfm_ctx(parent);\n\tstruct crypto_cipher *child = ctx->child;\n\tint err, bsize = LRW_BLOCK_SIZE;\n\tconst u8 *tweak = key + keylen - bsize;\n\n\tcrypto_cipher_clear_flags(child, CRYPTO_TFM_REQ_MASK);\n\tcrypto_cipher_set_flags(child, crypto_tfm_get_flags(parent) &\n\t\t\t\t       CRYPTO_TFM_REQ_MASK);\n\terr = crypto_cipher_setkey(child, key, keylen - bsize);\n\tif (err)\n\t\treturn err;\n\tcrypto_tfm_set_flags(parent, crypto_cipher_get_flags(child) &\n\t\t\t\t     CRYPTO_TFM_RES_MASK);\n\n\treturn lrw_init_table(&ctx->table, tweak);\n}\n\nstruct sinfo {\n\tbe128 t;\n\tstruct crypto_tfm *tfm;\n\tvoid (*fn)(struct crypto_tfm *, u8 *, const u8 *);\n};\n\nstatic inline void inc(be128 *iv)\n{\n\tbe64_add_cpu(&iv->b, 1);\n\tif (!iv->b)\n\t\tbe64_add_cpu(&iv->a, 1);\n}\n\nstatic inline void lrw_round(struct sinfo *s, void *dst, const void *src)\n{\n\tbe128_xor(dst, &s->t, src);\t\t/* PP <- T xor P */\n\ts->fn(s->tfm, dst, dst);\t\t/* CC <- E(Key2,PP) */\n\tbe128_xor(dst, dst, &s->t);\t\t/* C <- T xor CC */\n}\n\n/* this returns the number of consequative 1 bits starting\n * from the right, get_index128(00 00 00 00 00 00 ... 00 00 10 FB) = 2 */\nstatic inline int get_index128(be128 *block)\n{\n\tint x;\n\t__be32 *p = (__be32 *) block;\n\n\tfor (p += 3, x = 0; x < 128; p--, x += 32) {\n\t\tu32 val = be32_to_cpup(p);\n\n\t\tif (!~val)\n\t\t\tcontinue;\n\n\t\treturn x + ffz(val);\n\t}\n\n\treturn x;\n}\n\nstatic int crypt(struct blkcipher_desc *d,\n\t\t struct blkcipher_walk *w, struct priv *ctx,\n\t\t void (*fn)(struct crypto_tfm *, u8 *, const u8 *))\n{\n\tint err;\n\tunsigned int avail;\n\tconst int bs = LRW_BLOCK_SIZE;\n\tstruct sinfo s = {\n\t\t.tfm = crypto_cipher_tfm(ctx->child),\n\t\t.fn = fn\n\t};\n\tbe128 *iv;\n\tu8 *wsrc;\n\tu8 *wdst;\n\n\terr = blkcipher_walk_virt(d, w);\n\tif (!(avail = w->nbytes))\n\t\treturn err;\n\n\twsrc = w->src.virt.addr;\n\twdst = w->dst.virt.addr;\n\n\t/* calculate first value of T */\n\tiv = (be128 *)w->iv;\n\ts.t = *iv;\n\n\t/* T <- I*Key2 */\n\tgf128mul_64k_bbe(&s.t, ctx->table.table);\n\n\tgoto first;\n\n\tfor (;;) {\n\t\tdo {\n\t\t\t/* T <- I*Key2, using the optimization\n\t\t\t * discussed in the specification */\n\t\t\tbe128_xor(&s.t, &s.t,\n\t\t\t\t  &ctx->table.mulinc[get_index128(iv)]);\n\t\t\tinc(iv);\n\nfirst:\n\t\t\tlrw_round(&s, wdst, wsrc);\n\n\t\t\twsrc += bs;\n\t\t\twdst += bs;\n\t\t} while ((avail -= bs) >= bs);\n\n\t\terr = blkcipher_walk_done(d, w, avail);\n\t\tif (!(avail = w->nbytes))\n\t\t\tbreak;\n\n\t\twsrc = w->src.virt.addr;\n\t\twdst = w->dst.virt.addr;\n\t}\n\n\treturn err;\n}\n\nstatic int encrypt(struct blkcipher_desc *desc, struct scatterlist *dst,\n\t\t   struct scatterlist *src, unsigned int nbytes)\n{\n\tstruct priv *ctx = crypto_blkcipher_ctx(desc->tfm);\n\tstruct blkcipher_walk w;\n\n\tblkcipher_walk_init(&w, dst, src, nbytes);\n\treturn crypt(desc, &w, ctx,\n\t\t     crypto_cipher_alg(ctx->child)->cia_encrypt);\n}\n\nstatic int decrypt(struct blkcipher_desc *desc, struct scatterlist *dst,\n\t\t   struct scatterlist *src, unsigned int nbytes)\n{\n\tstruct priv *ctx = crypto_blkcipher_ctx(desc->tfm);\n\tstruct blkcipher_walk w;\n\n\tblkcipher_walk_init(&w, dst, src, nbytes);\n\treturn crypt(desc, &w, ctx,\n\t\t     crypto_cipher_alg(ctx->child)->cia_decrypt);\n}\n\nint lrw_crypt(struct blkcipher_desc *desc, struct scatterlist *sdst,\n\t      struct scatterlist *ssrc, unsigned int nbytes,\n\t      struct lrw_crypt_req *req)\n{\n\tconst unsigned int bsize = LRW_BLOCK_SIZE;\n\tconst unsigned int max_blks = req->tbuflen / bsize;\n\tstruct lrw_table_ctx *ctx = req->table_ctx;\n\tstruct blkcipher_walk walk;\n\tunsigned int nblocks;\n\tbe128 *iv, *src, *dst, *t;\n\tbe128 *t_buf = req->tbuf;\n\tint err, i;\n\n\tBUG_ON(max_blks < 1);\n\n\tblkcipher_walk_init(&walk, sdst, ssrc, nbytes);\n\n\terr = blkcipher_walk_virt(desc, &walk);\n\tnbytes = walk.nbytes;\n\tif (!nbytes)\n\t\treturn err;\n\n\tnblocks = min(walk.nbytes / bsize, max_blks);\n\tsrc = (be128 *)walk.src.virt.addr;\n\tdst = (be128 *)walk.dst.virt.addr;\n\n\t/* calculate first value of T */\n\tiv = (be128 *)walk.iv;\n\tt_buf[0] = *iv;\n\n\t/* T <- I*Key2 */\n\tgf128mul_64k_bbe(&t_buf[0], ctx->table);\n\n\ti = 0;\n\tgoto first;\n\n\tfor (;;) {\n\t\tdo {\n\t\t\tfor (i = 0; i < nblocks; i++) {\n\t\t\t\t/* T <- I*Key2, using the optimization\n\t\t\t\t * discussed in the specification */\n\t\t\t\tbe128_xor(&t_buf[i], t,\n\t\t\t\t\t\t&ctx->mulinc[get_index128(iv)]);\n\t\t\t\tinc(iv);\nfirst:\n\t\t\t\tt = &t_buf[i];\n\n\t\t\t\t/* PP <- T xor P */\n\t\t\t\tbe128_xor(dst + i, t, src + i);\n\t\t\t}\n\n\t\t\t/* CC <- E(Key2,PP) */\n\t\t\treq->crypt_fn(req->crypt_ctx, (u8 *)dst,\n\t\t\t\t      nblocks * bsize);\n\n\t\t\t/* C <- T xor CC */\n\t\t\tfor (i = 0; i < nblocks; i++)\n\t\t\t\tbe128_xor(dst + i, dst + i, &t_buf[i]);\n\n\t\t\tsrc += nblocks;\n\t\t\tdst += nblocks;\n\t\t\tnbytes -= nblocks * bsize;\n\t\t\tnblocks = min(nbytes / bsize, max_blks);\n\t\t} while (nblocks > 0);\n\n\t\terr = blkcipher_walk_done(desc, &walk, nbytes);\n\t\tnbytes = walk.nbytes;\n\t\tif (!nbytes)\n\t\t\tbreak;\n\n\t\tnblocks = min(nbytes / bsize, max_blks);\n\t\tsrc = (be128 *)walk.src.virt.addr;\n\t\tdst = (be128 *)walk.dst.virt.addr;\n\t}\n\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(lrw_crypt);\n\nstatic int init_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_cipher *cipher;\n\tstruct crypto_instance *inst = (void *)tfm->__crt_alg;\n\tstruct crypto_spawn *spawn = crypto_instance_ctx(inst);\n\tstruct priv *ctx = crypto_tfm_ctx(tfm);\n\tu32 *flags = &tfm->crt_flags;\n\n\tcipher = crypto_spawn_cipher(spawn);\n\tif (IS_ERR(cipher))\n\t\treturn PTR_ERR(cipher);\n\n\tif (crypto_cipher_blocksize(cipher) != LRW_BLOCK_SIZE) {\n\t\t*flags |= CRYPTO_TFM_RES_BAD_BLOCK_LEN;\n\t\tcrypto_free_cipher(cipher);\n\t\treturn -EINVAL;\n\t}\n\n\tctx->child = cipher;\n\treturn 0;\n}\n\nstatic void exit_tfm(struct crypto_tfm *tfm)\n{\n\tstruct priv *ctx = crypto_tfm_ctx(tfm);\n\n\tlrw_free_table(&ctx->table);\n\tcrypto_free_cipher(ctx->child);\n}\n\nstatic struct crypto_instance *alloc(struct rtattr **tb)\n{\n\tstruct crypto_instance *inst;\n\tstruct crypto_alg *alg;\n\tint err;\n\n\terr = crypto_check_attr_type(tb, CRYPTO_ALG_TYPE_BLKCIPHER);\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\talg = crypto_get_attr_alg(tb, CRYPTO_ALG_TYPE_CIPHER,\n\t\t\t\t  CRYPTO_ALG_TYPE_MASK);\n\tif (IS_ERR(alg))\n\t\treturn ERR_CAST(alg);\n\n\tinst = crypto_alloc_instance(\"lrw\", alg);\n\tif (IS_ERR(inst))\n\t\tgoto out_put_alg;\n\n\tinst->alg.cra_flags = CRYPTO_ALG_TYPE_BLKCIPHER;\n\tinst->alg.cra_priority = alg->cra_priority;\n\tinst->alg.cra_blocksize = alg->cra_blocksize;\n\n\tif (alg->cra_alignmask < 7) inst->alg.cra_alignmask = 7;\n\telse inst->alg.cra_alignmask = alg->cra_alignmask;\n\tinst->alg.cra_type = &crypto_blkcipher_type;\n\n\tif (!(alg->cra_blocksize % 4))\n\t\tinst->alg.cra_alignmask |= 3;\n\tinst->alg.cra_blkcipher.ivsize = alg->cra_blocksize;\n\tinst->alg.cra_blkcipher.min_keysize =\n\t\talg->cra_cipher.cia_min_keysize + alg->cra_blocksize;\n\tinst->alg.cra_blkcipher.max_keysize =\n\t\talg->cra_cipher.cia_max_keysize + alg->cra_blocksize;\n\n\tinst->alg.cra_ctxsize = sizeof(struct priv);\n\n\tinst->alg.cra_init = init_tfm;\n\tinst->alg.cra_exit = exit_tfm;\n\n\tinst->alg.cra_blkcipher.setkey = setkey;\n\tinst->alg.cra_blkcipher.encrypt = encrypt;\n\tinst->alg.cra_blkcipher.decrypt = decrypt;\n\nout_put_alg:\n\tcrypto_mod_put(alg);\n\treturn inst;\n}\n\nstatic void free(struct crypto_instance *inst)\n{\n\tcrypto_drop_spawn(crypto_instance_ctx(inst));\n\tkfree(inst);\n}\n\nstatic struct crypto_template crypto_tmpl = {\n\t.name = \"lrw\",\n\t.alloc = alloc,\n\t.free = free,\n\t.module = THIS_MODULE,\n};\n\nstatic int __init crypto_module_init(void)\n{\n\treturn crypto_register_template(&crypto_tmpl);\n}\n\nstatic void __exit crypto_module_exit(void)\n{\n\tcrypto_unregister_template(&crypto_tmpl);\n}\n\nmodule_init(crypto_module_init);\nmodule_exit(crypto_module_exit);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"LRW block cipher mode\");\nMODULE_ALIAS_CRYPTO(\"lrw\");\n", "/*\n * Software multibuffer async crypto daemon.\n *\n * Copyright (c) 2014 Tim Chen <tim.c.chen@linux.intel.com>\n *\n * Adapted from crypto daemon.\n *\n * This program is free software; you can redistribute it and/or modify it\n * under the terms of the GNU General Public License as published by the Free\n * Software Foundation; either version 2 of the License, or (at your option)\n * any later version.\n *\n */\n\n#include <crypto/algapi.h>\n#include <crypto/internal/hash.h>\n#include <crypto/internal/aead.h>\n#include <crypto/mcryptd.h>\n#include <crypto/crypto_wq.h>\n#include <linux/err.h>\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/list.h>\n#include <linux/module.h>\n#include <linux/scatterlist.h>\n#include <linux/sched.h>\n#include <linux/slab.h>\n#include <linux/hardirq.h>\n\n#define MCRYPTD_MAX_CPU_QLEN 100\n#define MCRYPTD_BATCH 9\n\nstatic void *mcryptd_alloc_instance(struct crypto_alg *alg, unsigned int head,\n\t\t\t\t   unsigned int tail);\n\nstruct mcryptd_flush_list {\n\tstruct list_head list;\n\tstruct mutex lock;\n};\n\nstatic struct mcryptd_flush_list __percpu *mcryptd_flist;\n\nstruct hashd_instance_ctx {\n\tstruct crypto_shash_spawn spawn;\n\tstruct mcryptd_queue *queue;\n};\n\nstatic void mcryptd_queue_worker(struct work_struct *work);\n\nvoid mcryptd_arm_flusher(struct mcryptd_alg_cstate *cstate, unsigned long delay)\n{\n\tstruct mcryptd_flush_list *flist;\n\n\tif (!cstate->flusher_engaged) {\n\t\t/* put the flusher on the flush list */\n\t\tflist = per_cpu_ptr(mcryptd_flist, smp_processor_id());\n\t\tmutex_lock(&flist->lock);\n\t\tlist_add_tail(&cstate->flush_list, &flist->list);\n\t\tcstate->flusher_engaged = true;\n\t\tcstate->next_flush = jiffies + delay;\n\t\tqueue_delayed_work_on(smp_processor_id(), kcrypto_wq,\n\t\t\t&cstate->flush, delay);\n\t\tmutex_unlock(&flist->lock);\n\t}\n}\nEXPORT_SYMBOL(mcryptd_arm_flusher);\n\nstatic int mcryptd_init_queue(struct mcryptd_queue *queue,\n\t\t\t     unsigned int max_cpu_qlen)\n{\n\tint cpu;\n\tstruct mcryptd_cpu_queue *cpu_queue;\n\n\tqueue->cpu_queue = alloc_percpu(struct mcryptd_cpu_queue);\n\tpr_debug(\"mqueue:%p mcryptd_cpu_queue %p\\n\", queue, queue->cpu_queue);\n\tif (!queue->cpu_queue)\n\t\treturn -ENOMEM;\n\tfor_each_possible_cpu(cpu) {\n\t\tcpu_queue = per_cpu_ptr(queue->cpu_queue, cpu);\n\t\tpr_debug(\"cpu_queue #%d %p\\n\", cpu, queue->cpu_queue);\n\t\tcrypto_init_queue(&cpu_queue->queue, max_cpu_qlen);\n\t\tINIT_WORK(&cpu_queue->work, mcryptd_queue_worker);\n\t}\n\treturn 0;\n}\n\nstatic void mcryptd_fini_queue(struct mcryptd_queue *queue)\n{\n\tint cpu;\n\tstruct mcryptd_cpu_queue *cpu_queue;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tcpu_queue = per_cpu_ptr(queue->cpu_queue, cpu);\n\t\tBUG_ON(cpu_queue->queue.qlen);\n\t}\n\tfree_percpu(queue->cpu_queue);\n}\n\nstatic int mcryptd_enqueue_request(struct mcryptd_queue *queue,\n\t\t\t\t  struct crypto_async_request *request,\n\t\t\t\t  struct mcryptd_hash_request_ctx *rctx)\n{\n\tint cpu, err;\n\tstruct mcryptd_cpu_queue *cpu_queue;\n\n\tcpu = get_cpu();\n\tcpu_queue = this_cpu_ptr(queue->cpu_queue);\n\trctx->tag.cpu = cpu;\n\n\terr = crypto_enqueue_request(&cpu_queue->queue, request);\n\tpr_debug(\"enqueue request: cpu %d cpu_queue %p request %p\\n\",\n\t\t cpu, cpu_queue, request);\n\tqueue_work_on(cpu, kcrypto_wq, &cpu_queue->work);\n\tput_cpu();\n\n\treturn err;\n}\n\n/*\n * Try to opportunisticlly flush the partially completed jobs if\n * crypto daemon is the only task running.\n */\nstatic void mcryptd_opportunistic_flush(void)\n{\n\tstruct mcryptd_flush_list *flist;\n\tstruct mcryptd_alg_cstate *cstate;\n\n\tflist = per_cpu_ptr(mcryptd_flist, smp_processor_id());\n\twhile (single_task_running()) {\n\t\tmutex_lock(&flist->lock);\n\t\tif (list_empty(&flist->list)) {\n\t\t\tmutex_unlock(&flist->lock);\n\t\t\treturn;\n\t\t}\n\t\tcstate = list_entry(flist->list.next,\n\t\t\t\tstruct mcryptd_alg_cstate, flush_list);\n\t\tif (!cstate->flusher_engaged) {\n\t\t\tmutex_unlock(&flist->lock);\n\t\t\treturn;\n\t\t}\n\t\tlist_del(&cstate->flush_list);\n\t\tcstate->flusher_engaged = false;\n\t\tmutex_unlock(&flist->lock);\n\t\tcstate->alg_state->flusher(cstate);\n\t}\n}\n\n/*\n * Called in workqueue context, do one real cryption work (via\n * req->complete) and reschedule itself if there are more work to\n * do.\n */\nstatic void mcryptd_queue_worker(struct work_struct *work)\n{\n\tstruct mcryptd_cpu_queue *cpu_queue;\n\tstruct crypto_async_request *req, *backlog;\n\tint i;\n\n\t/*\n\t * Need to loop through more than once for multi-buffer to\n\t * be effective.\n\t */\n\n\tcpu_queue = container_of(work, struct mcryptd_cpu_queue, work);\n\ti = 0;\n\twhile (i < MCRYPTD_BATCH || single_task_running()) {\n\t\t/*\n\t\t * preempt_disable/enable is used to prevent\n\t\t * being preempted by mcryptd_enqueue_request()\n\t\t */\n\t\tlocal_bh_disable();\n\t\tpreempt_disable();\n\t\tbacklog = crypto_get_backlog(&cpu_queue->queue);\n\t\treq = crypto_dequeue_request(&cpu_queue->queue);\n\t\tpreempt_enable();\n\t\tlocal_bh_enable();\n\n\t\tif (!req) {\n\t\t\tmcryptd_opportunistic_flush();\n\t\t\treturn;\n\t\t}\n\n\t\tif (backlog)\n\t\t\tbacklog->complete(backlog, -EINPROGRESS);\n\t\treq->complete(req, 0);\n\t\tif (!cpu_queue->queue.qlen)\n\t\t\treturn;\n\t\t++i;\n\t}\n\tif (cpu_queue->queue.qlen)\n\t\tqueue_work(kcrypto_wq, &cpu_queue->work);\n}\n\nvoid mcryptd_flusher(struct work_struct *__work)\n{\n\tstruct\tmcryptd_alg_cstate\t*alg_cpu_state;\n\tstruct\tmcryptd_alg_state\t*alg_state;\n\tstruct\tmcryptd_flush_list\t*flist;\n\tint\tcpu;\n\n\tcpu = smp_processor_id();\n\talg_cpu_state = container_of(to_delayed_work(__work),\n\t\t\t\t     struct mcryptd_alg_cstate, flush);\n\talg_state = alg_cpu_state->alg_state;\n\tif (alg_cpu_state->cpu != cpu)\n\t\tpr_debug(\"mcryptd error: work on cpu %d, should be cpu %d\\n\",\n\t\t\t\tcpu, alg_cpu_state->cpu);\n\n\tif (alg_cpu_state->flusher_engaged) {\n\t\tflist = per_cpu_ptr(mcryptd_flist, cpu);\n\t\tmutex_lock(&flist->lock);\n\t\tlist_del(&alg_cpu_state->flush_list);\n\t\talg_cpu_state->flusher_engaged = false;\n\t\tmutex_unlock(&flist->lock);\n\t\talg_state->flusher(alg_cpu_state);\n\t}\n}\nEXPORT_SYMBOL_GPL(mcryptd_flusher);\n\nstatic inline struct mcryptd_queue *mcryptd_get_queue(struct crypto_tfm *tfm)\n{\n\tstruct crypto_instance *inst = crypto_tfm_alg_instance(tfm);\n\tstruct mcryptd_instance_ctx *ictx = crypto_instance_ctx(inst);\n\n\treturn ictx->queue;\n}\n\nstatic void *mcryptd_alloc_instance(struct crypto_alg *alg, unsigned int head,\n\t\t\t\t   unsigned int tail)\n{\n\tchar *p;\n\tstruct crypto_instance *inst;\n\tint err;\n\n\tp = kzalloc(head + sizeof(*inst) + tail, GFP_KERNEL);\n\tif (!p)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tinst = (void *)(p + head);\n\n\terr = -ENAMETOOLONG;\n\tif (snprintf(inst->alg.cra_driver_name, CRYPTO_MAX_ALG_NAME,\n\t\t    \"mcryptd(%s)\", alg->cra_driver_name) >= CRYPTO_MAX_ALG_NAME)\n\t\tgoto out_free_inst;\n\n\tmemcpy(inst->alg.cra_name, alg->cra_name, CRYPTO_MAX_ALG_NAME);\n\n\tinst->alg.cra_priority = alg->cra_priority + 50;\n\tinst->alg.cra_blocksize = alg->cra_blocksize;\n\tinst->alg.cra_alignmask = alg->cra_alignmask;\n\nout:\n\treturn p;\n\nout_free_inst:\n\tkfree(p);\n\tp = ERR_PTR(err);\n\tgoto out;\n}\n\nstatic int mcryptd_hash_init_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_instance *inst = crypto_tfm_alg_instance(tfm);\n\tstruct hashd_instance_ctx *ictx = crypto_instance_ctx(inst);\n\tstruct crypto_shash_spawn *spawn = &ictx->spawn;\n\tstruct mcryptd_hash_ctx *ctx = crypto_tfm_ctx(tfm);\n\tstruct crypto_shash *hash;\n\n\thash = crypto_spawn_shash(spawn);\n\tif (IS_ERR(hash))\n\t\treturn PTR_ERR(hash);\n\n\tctx->child = hash;\n\tcrypto_ahash_set_reqsize(__crypto_ahash_cast(tfm),\n\t\t\t\t sizeof(struct mcryptd_hash_request_ctx) +\n\t\t\t\t crypto_shash_descsize(hash));\n\treturn 0;\n}\n\nstatic void mcryptd_hash_exit_tfm(struct crypto_tfm *tfm)\n{\n\tstruct mcryptd_hash_ctx *ctx = crypto_tfm_ctx(tfm);\n\n\tcrypto_free_shash(ctx->child);\n}\n\nstatic int mcryptd_hash_setkey(struct crypto_ahash *parent,\n\t\t\t\t   const u8 *key, unsigned int keylen)\n{\n\tstruct mcryptd_hash_ctx *ctx   = crypto_ahash_ctx(parent);\n\tstruct crypto_shash *child = ctx->child;\n\tint err;\n\n\tcrypto_shash_clear_flags(child, CRYPTO_TFM_REQ_MASK);\n\tcrypto_shash_set_flags(child, crypto_ahash_get_flags(parent) &\n\t\t\t\t      CRYPTO_TFM_REQ_MASK);\n\terr = crypto_shash_setkey(child, key, keylen);\n\tcrypto_ahash_set_flags(parent, crypto_shash_get_flags(child) &\n\t\t\t\t       CRYPTO_TFM_RES_MASK);\n\treturn err;\n}\n\nstatic int mcryptd_hash_enqueue(struct ahash_request *req,\n\t\t\t\tcrypto_completion_t complete)\n{\n\tint ret;\n\n\tstruct mcryptd_hash_request_ctx *rctx = ahash_request_ctx(req);\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct mcryptd_queue *queue =\n\t\tmcryptd_get_queue(crypto_ahash_tfm(tfm));\n\n\trctx->complete = req->base.complete;\n\treq->base.complete = complete;\n\n\tret = mcryptd_enqueue_request(queue, &req->base, rctx);\n\n\treturn ret;\n}\n\nstatic void mcryptd_hash_init(struct crypto_async_request *req_async, int err)\n{\n\tstruct mcryptd_hash_ctx *ctx = crypto_tfm_ctx(req_async->tfm);\n\tstruct crypto_shash *child = ctx->child;\n\tstruct ahash_request *req = ahash_request_cast(req_async);\n\tstruct mcryptd_hash_request_ctx *rctx = ahash_request_ctx(req);\n\tstruct shash_desc *desc = &rctx->desc;\n\n\tif (unlikely(err == -EINPROGRESS))\n\t\tgoto out;\n\n\tdesc->tfm = child;\n\tdesc->flags = CRYPTO_TFM_REQ_MAY_SLEEP;\n\n\terr = crypto_shash_init(desc);\n\n\treq->base.complete = rctx->complete;\n\nout:\n\tlocal_bh_disable();\n\trctx->complete(&req->base, err);\n\tlocal_bh_enable();\n}\n\nstatic int mcryptd_hash_init_enqueue(struct ahash_request *req)\n{\n\treturn mcryptd_hash_enqueue(req, mcryptd_hash_init);\n}\n\nstatic void mcryptd_hash_update(struct crypto_async_request *req_async, int err)\n{\n\tstruct ahash_request *req = ahash_request_cast(req_async);\n\tstruct mcryptd_hash_request_ctx *rctx = ahash_request_ctx(req);\n\n\tif (unlikely(err == -EINPROGRESS))\n\t\tgoto out;\n\n\terr = shash_ahash_mcryptd_update(req, &rctx->desc);\n\tif (err) {\n\t\treq->base.complete = rctx->complete;\n\t\tgoto out;\n\t}\n\n\treturn;\nout:\n\tlocal_bh_disable();\n\trctx->complete(&req->base, err);\n\tlocal_bh_enable();\n}\n\nstatic int mcryptd_hash_update_enqueue(struct ahash_request *req)\n{\n\treturn mcryptd_hash_enqueue(req, mcryptd_hash_update);\n}\n\nstatic void mcryptd_hash_final(struct crypto_async_request *req_async, int err)\n{\n\tstruct ahash_request *req = ahash_request_cast(req_async);\n\tstruct mcryptd_hash_request_ctx *rctx = ahash_request_ctx(req);\n\n\tif (unlikely(err == -EINPROGRESS))\n\t\tgoto out;\n\n\terr = shash_ahash_mcryptd_final(req, &rctx->desc);\n\tif (err) {\n\t\treq->base.complete = rctx->complete;\n\t\tgoto out;\n\t}\n\n\treturn;\nout:\n\tlocal_bh_disable();\n\trctx->complete(&req->base, err);\n\tlocal_bh_enable();\n}\n\nstatic int mcryptd_hash_final_enqueue(struct ahash_request *req)\n{\n\treturn mcryptd_hash_enqueue(req, mcryptd_hash_final);\n}\n\nstatic void mcryptd_hash_finup(struct crypto_async_request *req_async, int err)\n{\n\tstruct ahash_request *req = ahash_request_cast(req_async);\n\tstruct mcryptd_hash_request_ctx *rctx = ahash_request_ctx(req);\n\n\tif (unlikely(err == -EINPROGRESS))\n\t\tgoto out;\n\n\terr = shash_ahash_mcryptd_finup(req, &rctx->desc);\n\n\tif (err) {\n\t\treq->base.complete = rctx->complete;\n\t\tgoto out;\n\t}\n\n\treturn;\nout:\n\tlocal_bh_disable();\n\trctx->complete(&req->base, err);\n\tlocal_bh_enable();\n}\n\nstatic int mcryptd_hash_finup_enqueue(struct ahash_request *req)\n{\n\treturn mcryptd_hash_enqueue(req, mcryptd_hash_finup);\n}\n\nstatic void mcryptd_hash_digest(struct crypto_async_request *req_async, int err)\n{\n\tstruct mcryptd_hash_ctx *ctx = crypto_tfm_ctx(req_async->tfm);\n\tstruct crypto_shash *child = ctx->child;\n\tstruct ahash_request *req = ahash_request_cast(req_async);\n\tstruct mcryptd_hash_request_ctx *rctx = ahash_request_ctx(req);\n\tstruct shash_desc *desc = &rctx->desc;\n\n\tif (unlikely(err == -EINPROGRESS))\n\t\tgoto out;\n\n\tdesc->tfm = child;\n\tdesc->flags = CRYPTO_TFM_REQ_MAY_SLEEP;  /* check this again */\n\n\terr = shash_ahash_mcryptd_digest(req, desc);\n\n\tif (err) {\n\t\treq->base.complete = rctx->complete;\n\t\tgoto out;\n\t}\n\n\treturn;\nout:\n\tlocal_bh_disable();\n\trctx->complete(&req->base, err);\n\tlocal_bh_enable();\n}\n\nstatic int mcryptd_hash_digest_enqueue(struct ahash_request *req)\n{\n\treturn mcryptd_hash_enqueue(req, mcryptd_hash_digest);\n}\n\nstatic int mcryptd_hash_export(struct ahash_request *req, void *out)\n{\n\tstruct mcryptd_hash_request_ctx *rctx = ahash_request_ctx(req);\n\n\treturn crypto_shash_export(&rctx->desc, out);\n}\n\nstatic int mcryptd_hash_import(struct ahash_request *req, const void *in)\n{\n\tstruct mcryptd_hash_request_ctx *rctx = ahash_request_ctx(req);\n\n\treturn crypto_shash_import(&rctx->desc, in);\n}\n\nstatic int mcryptd_create_hash(struct crypto_template *tmpl, struct rtattr **tb,\n\t\t\t      struct mcryptd_queue *queue)\n{\n\tstruct hashd_instance_ctx *ctx;\n\tstruct ahash_instance *inst;\n\tstruct shash_alg *salg;\n\tstruct crypto_alg *alg;\n\tint err;\n\n\tsalg = shash_attr_alg(tb[1], 0, 0);\n\tif (IS_ERR(salg))\n\t\treturn PTR_ERR(salg);\n\n\talg = &salg->base;\n\tpr_debug(\"crypto: mcryptd hash alg: %s\\n\", alg->cra_name);\n\tinst = mcryptd_alloc_instance(alg, ahash_instance_headroom(),\n\t\t\t\t\tsizeof(*ctx));\n\terr = PTR_ERR(inst);\n\tif (IS_ERR(inst))\n\t\tgoto out_put_alg;\n\n\tctx = ahash_instance_ctx(inst);\n\tctx->queue = queue;\n\n\terr = crypto_init_shash_spawn(&ctx->spawn, salg,\n\t\t\t\t      ahash_crypto_instance(inst));\n\tif (err)\n\t\tgoto out_free_inst;\n\n\tinst->alg.halg.base.cra_flags = CRYPTO_ALG_ASYNC;\n\n\tinst->alg.halg.digestsize = salg->digestsize;\n\tinst->alg.halg.base.cra_ctxsize = sizeof(struct mcryptd_hash_ctx);\n\n\tinst->alg.halg.base.cra_init = mcryptd_hash_init_tfm;\n\tinst->alg.halg.base.cra_exit = mcryptd_hash_exit_tfm;\n\n\tinst->alg.init   = mcryptd_hash_init_enqueue;\n\tinst->alg.update = mcryptd_hash_update_enqueue;\n\tinst->alg.final  = mcryptd_hash_final_enqueue;\n\tinst->alg.finup  = mcryptd_hash_finup_enqueue;\n\tinst->alg.export = mcryptd_hash_export;\n\tinst->alg.import = mcryptd_hash_import;\n\tinst->alg.setkey = mcryptd_hash_setkey;\n\tinst->alg.digest = mcryptd_hash_digest_enqueue;\n\n\terr = ahash_register_instance(tmpl, inst);\n\tif (err) {\n\t\tcrypto_drop_shash(&ctx->spawn);\nout_free_inst:\n\t\tkfree(inst);\n\t}\n\nout_put_alg:\n\tcrypto_mod_put(alg);\n\treturn err;\n}\n\nstatic struct mcryptd_queue mqueue;\n\nstatic int mcryptd_create(struct crypto_template *tmpl, struct rtattr **tb)\n{\n\tstruct crypto_attr_type *algt;\n\n\talgt = crypto_get_attr_type(tb);\n\tif (IS_ERR(algt))\n\t\treturn PTR_ERR(algt);\n\n\tswitch (algt->type & algt->mask & CRYPTO_ALG_TYPE_MASK) {\n\tcase CRYPTO_ALG_TYPE_DIGEST:\n\t\treturn mcryptd_create_hash(tmpl, tb, &mqueue);\n\tbreak;\n\t}\n\n\treturn -EINVAL;\n}\n\nstatic void mcryptd_free(struct crypto_instance *inst)\n{\n\tstruct mcryptd_instance_ctx *ctx = crypto_instance_ctx(inst);\n\tstruct hashd_instance_ctx *hctx = crypto_instance_ctx(inst);\n\n\tswitch (inst->alg.cra_flags & CRYPTO_ALG_TYPE_MASK) {\n\tcase CRYPTO_ALG_TYPE_AHASH:\n\t\tcrypto_drop_shash(&hctx->spawn);\n\t\tkfree(ahash_instance(inst));\n\t\treturn;\n\tdefault:\n\t\tcrypto_drop_spawn(&ctx->spawn);\n\t\tkfree(inst);\n\t}\n}\n\nstatic struct crypto_template mcryptd_tmpl = {\n\t.name = \"mcryptd\",\n\t.create = mcryptd_create,\n\t.free = mcryptd_free,\n\t.module = THIS_MODULE,\n};\n\nstruct mcryptd_ahash *mcryptd_alloc_ahash(const char *alg_name,\n\t\t\t\t\tu32 type, u32 mask)\n{\n\tchar mcryptd_alg_name[CRYPTO_MAX_ALG_NAME];\n\tstruct crypto_ahash *tfm;\n\n\tif (snprintf(mcryptd_alg_name, CRYPTO_MAX_ALG_NAME,\n\t\t     \"mcryptd(%s)\", alg_name) >= CRYPTO_MAX_ALG_NAME)\n\t\treturn ERR_PTR(-EINVAL);\n\ttfm = crypto_alloc_ahash(mcryptd_alg_name, type, mask);\n\tif (IS_ERR(tfm))\n\t\treturn ERR_CAST(tfm);\n\tif (tfm->base.__crt_alg->cra_module != THIS_MODULE) {\n\t\tcrypto_free_ahash(tfm);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\treturn __mcryptd_ahash_cast(tfm);\n}\nEXPORT_SYMBOL_GPL(mcryptd_alloc_ahash);\n\nint shash_ahash_mcryptd_digest(struct ahash_request *req,\n\t\t\t       struct shash_desc *desc)\n{\n\tint err;\n\n\terr = crypto_shash_init(desc) ?:\n\t      shash_ahash_mcryptd_finup(req, desc);\n\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(shash_ahash_mcryptd_digest);\n\nint shash_ahash_mcryptd_update(struct ahash_request *req,\n\t\t\t       struct shash_desc *desc)\n{\n\tstruct crypto_shash *tfm = desc->tfm;\n\tstruct shash_alg *shash = crypto_shash_alg(tfm);\n\n\t/* alignment is to be done by multi-buffer crypto algorithm if needed */\n\n\treturn shash->update(desc, NULL, 0);\n}\nEXPORT_SYMBOL_GPL(shash_ahash_mcryptd_update);\n\nint shash_ahash_mcryptd_finup(struct ahash_request *req,\n\t\t\t      struct shash_desc *desc)\n{\n\tstruct crypto_shash *tfm = desc->tfm;\n\tstruct shash_alg *shash = crypto_shash_alg(tfm);\n\n\t/* alignment is to be done by multi-buffer crypto algorithm if needed */\n\n\treturn shash->finup(desc, NULL, 0, req->result);\n}\nEXPORT_SYMBOL_GPL(shash_ahash_mcryptd_finup);\n\nint shash_ahash_mcryptd_final(struct ahash_request *req,\n\t\t\t      struct shash_desc *desc)\n{\n\tstruct crypto_shash *tfm = desc->tfm;\n\tstruct shash_alg *shash = crypto_shash_alg(tfm);\n\n\t/* alignment is to be done by multi-buffer crypto algorithm if needed */\n\n\treturn shash->final(desc, req->result);\n}\nEXPORT_SYMBOL_GPL(shash_ahash_mcryptd_final);\n\nstruct crypto_shash *mcryptd_ahash_child(struct mcryptd_ahash *tfm)\n{\n\tstruct mcryptd_hash_ctx *ctx = crypto_ahash_ctx(&tfm->base);\n\n\treturn ctx->child;\n}\nEXPORT_SYMBOL_GPL(mcryptd_ahash_child);\n\nstruct shash_desc *mcryptd_shash_desc(struct ahash_request *req)\n{\n\tstruct mcryptd_hash_request_ctx *rctx = ahash_request_ctx(req);\n\treturn &rctx->desc;\n}\nEXPORT_SYMBOL_GPL(mcryptd_shash_desc);\n\nvoid mcryptd_free_ahash(struct mcryptd_ahash *tfm)\n{\n\tcrypto_free_ahash(&tfm->base);\n}\nEXPORT_SYMBOL_GPL(mcryptd_free_ahash);\n\n\nstatic int __init mcryptd_init(void)\n{\n\tint err, cpu;\n\tstruct mcryptd_flush_list *flist;\n\n\tmcryptd_flist = alloc_percpu(struct mcryptd_flush_list);\n\tfor_each_possible_cpu(cpu) {\n\t\tflist = per_cpu_ptr(mcryptd_flist, cpu);\n\t\tINIT_LIST_HEAD(&flist->list);\n\t\tmutex_init(&flist->lock);\n\t}\n\n\terr = mcryptd_init_queue(&mqueue, MCRYPTD_MAX_CPU_QLEN);\n\tif (err) {\n\t\tfree_percpu(mcryptd_flist);\n\t\treturn err;\n\t}\n\n\terr = crypto_register_template(&mcryptd_tmpl);\n\tif (err) {\n\t\tmcryptd_fini_queue(&mqueue);\n\t\tfree_percpu(mcryptd_flist);\n\t}\n\n\treturn err;\n}\n\nstatic void __exit mcryptd_exit(void)\n{\n\tmcryptd_fini_queue(&mqueue);\n\tcrypto_unregister_template(&mcryptd_tmpl);\n\tfree_percpu(mcryptd_flist);\n}\n\nsubsys_initcall(mcryptd_init);\nmodule_exit(mcryptd_exit);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"Software async multibuffer crypto daemon\");\nMODULE_ALIAS_CRYPTO(\"mcryptd\");\n", "/*\n * PCBC: Propagating Cipher Block Chaining mode\n *\n * Copyright (C) 2006 Red Hat, Inc. All Rights Reserved.\n * Written by David Howells (dhowells@redhat.com)\n *\n * Derived from cbc.c\n * - Copyright (c) 2006 Herbert Xu <herbert@gondor.apana.org.au>\n *\n * This program is free software; you can redistribute it and/or modify it\n * under the terms of the GNU General Public License as published by the Free\n * Software Foundation; either version 2 of the License, or (at your option)\n * any later version.\n *\n */\n\n#include <crypto/algapi.h>\n#include <linux/err.h>\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/scatterlist.h>\n#include <linux/slab.h>\n\nstruct crypto_pcbc_ctx {\n\tstruct crypto_cipher *child;\n};\n\nstatic int crypto_pcbc_setkey(struct crypto_tfm *parent, const u8 *key,\n\t\t\t      unsigned int keylen)\n{\n\tstruct crypto_pcbc_ctx *ctx = crypto_tfm_ctx(parent);\n\tstruct crypto_cipher *child = ctx->child;\n\tint err;\n\n\tcrypto_cipher_clear_flags(child, CRYPTO_TFM_REQ_MASK);\n\tcrypto_cipher_set_flags(child, crypto_tfm_get_flags(parent) &\n\t\t\t\tCRYPTO_TFM_REQ_MASK);\n\terr = crypto_cipher_setkey(child, key, keylen);\n\tcrypto_tfm_set_flags(parent, crypto_cipher_get_flags(child) &\n\t\t\t     CRYPTO_TFM_RES_MASK);\n\treturn err;\n}\n\nstatic int crypto_pcbc_encrypt_segment(struct blkcipher_desc *desc,\n\t\t\t\t       struct blkcipher_walk *walk,\n\t\t\t\t       struct crypto_cipher *tfm)\n{\n\tvoid (*fn)(struct crypto_tfm *, u8 *, const u8 *) =\n\t\tcrypto_cipher_alg(tfm)->cia_encrypt;\n\tint bsize = crypto_cipher_blocksize(tfm);\n\tunsigned int nbytes = walk->nbytes;\n\tu8 *src = walk->src.virt.addr;\n\tu8 *dst = walk->dst.virt.addr;\n\tu8 *iv = walk->iv;\n\n\tdo {\n\t\tcrypto_xor(iv, src, bsize);\n\t\tfn(crypto_cipher_tfm(tfm), dst, iv);\n\t\tmemcpy(iv, dst, bsize);\n\t\tcrypto_xor(iv, src, bsize);\n\n\t\tsrc += bsize;\n\t\tdst += bsize;\n\t} while ((nbytes -= bsize) >= bsize);\n\n\treturn nbytes;\n}\n\nstatic int crypto_pcbc_encrypt_inplace(struct blkcipher_desc *desc,\n\t\t\t\t       struct blkcipher_walk *walk,\n\t\t\t\t       struct crypto_cipher *tfm)\n{\n\tvoid (*fn)(struct crypto_tfm *, u8 *, const u8 *) =\n\t\tcrypto_cipher_alg(tfm)->cia_encrypt;\n\tint bsize = crypto_cipher_blocksize(tfm);\n\tunsigned int nbytes = walk->nbytes;\n\tu8 *src = walk->src.virt.addr;\n\tu8 *iv = walk->iv;\n\tu8 tmpbuf[bsize];\n\n\tdo {\n\t\tmemcpy(tmpbuf, src, bsize);\n\t\tcrypto_xor(iv, src, bsize);\n\t\tfn(crypto_cipher_tfm(tfm), src, iv);\n\t\tmemcpy(iv, tmpbuf, bsize);\n\t\tcrypto_xor(iv, src, bsize);\n\n\t\tsrc += bsize;\n\t} while ((nbytes -= bsize) >= bsize);\n\n\tmemcpy(walk->iv, iv, bsize);\n\n\treturn nbytes;\n}\n\nstatic int crypto_pcbc_encrypt(struct blkcipher_desc *desc,\n\t\t\t       struct scatterlist *dst, struct scatterlist *src,\n\t\t\t       unsigned int nbytes)\n{\n\tstruct blkcipher_walk walk;\n\tstruct crypto_blkcipher *tfm = desc->tfm;\n\tstruct crypto_pcbc_ctx *ctx = crypto_blkcipher_ctx(tfm);\n\tstruct crypto_cipher *child = ctx->child;\n\tint err;\n\n\tblkcipher_walk_init(&walk, dst, src, nbytes);\n\terr = blkcipher_walk_virt(desc, &walk);\n\n\twhile ((nbytes = walk.nbytes)) {\n\t\tif (walk.src.virt.addr == walk.dst.virt.addr)\n\t\t\tnbytes = crypto_pcbc_encrypt_inplace(desc, &walk,\n\t\t\t\t\t\t\t     child);\n\t\telse\n\t\t\tnbytes = crypto_pcbc_encrypt_segment(desc, &walk,\n\t\t\t\t\t\t\t     child);\n\t\terr = blkcipher_walk_done(desc, &walk, nbytes);\n\t}\n\n\treturn err;\n}\n\nstatic int crypto_pcbc_decrypt_segment(struct blkcipher_desc *desc,\n\t\t\t\t       struct blkcipher_walk *walk,\n\t\t\t\t       struct crypto_cipher *tfm)\n{\n\tvoid (*fn)(struct crypto_tfm *, u8 *, const u8 *) =\n\t\tcrypto_cipher_alg(tfm)->cia_decrypt;\n\tint bsize = crypto_cipher_blocksize(tfm);\n\tunsigned int nbytes = walk->nbytes;\n\tu8 *src = walk->src.virt.addr;\n\tu8 *dst = walk->dst.virt.addr;\n\tu8 *iv = walk->iv;\n\n\tdo {\n\t\tfn(crypto_cipher_tfm(tfm), dst, src);\n\t\tcrypto_xor(dst, iv, bsize);\n\t\tmemcpy(iv, src, bsize);\n\t\tcrypto_xor(iv, dst, bsize);\n\n\t\tsrc += bsize;\n\t\tdst += bsize;\n\t} while ((nbytes -= bsize) >= bsize);\n\n\tmemcpy(walk->iv, iv, bsize);\n\n\treturn nbytes;\n}\n\nstatic int crypto_pcbc_decrypt_inplace(struct blkcipher_desc *desc,\n\t\t\t\t       struct blkcipher_walk *walk,\n\t\t\t\t       struct crypto_cipher *tfm)\n{\n\tvoid (*fn)(struct crypto_tfm *, u8 *, const u8 *) =\n\t\tcrypto_cipher_alg(tfm)->cia_decrypt;\n\tint bsize = crypto_cipher_blocksize(tfm);\n\tunsigned int nbytes = walk->nbytes;\n\tu8 *src = walk->src.virt.addr;\n\tu8 *iv = walk->iv;\n\tu8 tmpbuf[bsize];\n\n\tdo {\n\t\tmemcpy(tmpbuf, src, bsize);\n\t\tfn(crypto_cipher_tfm(tfm), src, src);\n\t\tcrypto_xor(src, iv, bsize);\n\t\tmemcpy(iv, tmpbuf, bsize);\n\t\tcrypto_xor(iv, src, bsize);\n\n\t\tsrc += bsize;\n\t} while ((nbytes -= bsize) >= bsize);\n\n\tmemcpy(walk->iv, iv, bsize);\n\n\treturn nbytes;\n}\n\nstatic int crypto_pcbc_decrypt(struct blkcipher_desc *desc,\n\t\t\t       struct scatterlist *dst, struct scatterlist *src,\n\t\t\t       unsigned int nbytes)\n{\n\tstruct blkcipher_walk walk;\n\tstruct crypto_blkcipher *tfm = desc->tfm;\n\tstruct crypto_pcbc_ctx *ctx = crypto_blkcipher_ctx(tfm);\n\tstruct crypto_cipher *child = ctx->child;\n\tint err;\n\n\tblkcipher_walk_init(&walk, dst, src, nbytes);\n\terr = blkcipher_walk_virt(desc, &walk);\n\n\twhile ((nbytes = walk.nbytes)) {\n\t\tif (walk.src.virt.addr == walk.dst.virt.addr)\n\t\t\tnbytes = crypto_pcbc_decrypt_inplace(desc, &walk,\n\t\t\t\t\t\t\t     child);\n\t\telse\n\t\t\tnbytes = crypto_pcbc_decrypt_segment(desc, &walk,\n\t\t\t\t\t\t\t     child);\n\t\terr = blkcipher_walk_done(desc, &walk, nbytes);\n\t}\n\n\treturn err;\n}\n\nstatic int crypto_pcbc_init_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_instance *inst = (void *)tfm->__crt_alg;\n\tstruct crypto_spawn *spawn = crypto_instance_ctx(inst);\n\tstruct crypto_pcbc_ctx *ctx = crypto_tfm_ctx(tfm);\n\tstruct crypto_cipher *cipher;\n\n\tcipher = crypto_spawn_cipher(spawn);\n\tif (IS_ERR(cipher))\n\t\treturn PTR_ERR(cipher);\n\n\tctx->child = cipher;\n\treturn 0;\n}\n\nstatic void crypto_pcbc_exit_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_pcbc_ctx *ctx = crypto_tfm_ctx(tfm);\n\tcrypto_free_cipher(ctx->child);\n}\n\nstatic struct crypto_instance *crypto_pcbc_alloc(struct rtattr **tb)\n{\n\tstruct crypto_instance *inst;\n\tstruct crypto_alg *alg;\n\tint err;\n\n\terr = crypto_check_attr_type(tb, CRYPTO_ALG_TYPE_BLKCIPHER);\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\talg = crypto_get_attr_alg(tb, CRYPTO_ALG_TYPE_CIPHER,\n\t\t\t\t  CRYPTO_ALG_TYPE_MASK);\n\tif (IS_ERR(alg))\n\t\treturn ERR_CAST(alg);\n\n\tinst = crypto_alloc_instance(\"pcbc\", alg);\n\tif (IS_ERR(inst))\n\t\tgoto out_put_alg;\n\n\tinst->alg.cra_flags = CRYPTO_ALG_TYPE_BLKCIPHER;\n\tinst->alg.cra_priority = alg->cra_priority;\n\tinst->alg.cra_blocksize = alg->cra_blocksize;\n\tinst->alg.cra_alignmask = alg->cra_alignmask;\n\tinst->alg.cra_type = &crypto_blkcipher_type;\n\n\t/* We access the data as u32s when xoring. */\n\tinst->alg.cra_alignmask |= __alignof__(u32) - 1;\n\n\tinst->alg.cra_blkcipher.ivsize = alg->cra_blocksize;\n\tinst->alg.cra_blkcipher.min_keysize = alg->cra_cipher.cia_min_keysize;\n\tinst->alg.cra_blkcipher.max_keysize = alg->cra_cipher.cia_max_keysize;\n\n\tinst->alg.cra_ctxsize = sizeof(struct crypto_pcbc_ctx);\n\n\tinst->alg.cra_init = crypto_pcbc_init_tfm;\n\tinst->alg.cra_exit = crypto_pcbc_exit_tfm;\n\n\tinst->alg.cra_blkcipher.setkey = crypto_pcbc_setkey;\n\tinst->alg.cra_blkcipher.encrypt = crypto_pcbc_encrypt;\n\tinst->alg.cra_blkcipher.decrypt = crypto_pcbc_decrypt;\n\nout_put_alg:\n\tcrypto_mod_put(alg);\n\treturn inst;\n}\n\nstatic void crypto_pcbc_free(struct crypto_instance *inst)\n{\n\tcrypto_drop_spawn(crypto_instance_ctx(inst));\n\tkfree(inst);\n}\n\nstatic struct crypto_template crypto_pcbc_tmpl = {\n\t.name = \"pcbc\",\n\t.alloc = crypto_pcbc_alloc,\n\t.free = crypto_pcbc_free,\n\t.module = THIS_MODULE,\n};\n\nstatic int __init crypto_pcbc_module_init(void)\n{\n\treturn crypto_register_template(&crypto_pcbc_tmpl);\n}\n\nstatic void __exit crypto_pcbc_module_exit(void)\n{\n\tcrypto_unregister_template(&crypto_pcbc_tmpl);\n}\n\nmodule_init(crypto_pcbc_module_init);\nmodule_exit(crypto_pcbc_module_exit);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"PCBC block cipher algorithm\");\nMODULE_ALIAS_CRYPTO(\"pcbc\");\n", "/*\n * pcrypt - Parallel crypto wrapper.\n *\n * Copyright (C) 2009 secunet Security Networks AG\n * Copyright (C) 2009 Steffen Klassert <steffen.klassert@secunet.com>\n *\n * This program is free software; you can redistribute it and/or modify it\n * under the terms and conditions of the GNU General Public License,\n * version 2, as published by the Free Software Foundation.\n *\n * This program is distributed in the hope it will be useful, but WITHOUT\n * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for\n * more details.\n *\n * You should have received a copy of the GNU General Public License along with\n * this program; if not, write to the Free Software Foundation, Inc.,\n * 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.\n */\n\n#include <crypto/algapi.h>\n#include <crypto/internal/aead.h>\n#include <linux/err.h>\n#include <linux/init.h>\n#include <linux/module.h>\n#include <linux/slab.h>\n#include <linux/notifier.h>\n#include <linux/kobject.h>\n#include <linux/cpu.h>\n#include <crypto/pcrypt.h>\n\nstruct padata_pcrypt {\n\tstruct padata_instance *pinst;\n\tstruct workqueue_struct *wq;\n\n\t/*\n\t * Cpumask for callback CPUs. It should be\n\t * equal to serial cpumask of corresponding padata instance,\n\t * so it is updated when padata notifies us about serial\n\t * cpumask change.\n\t *\n\t * cb_cpumask is protected by RCU. This fact prevents us from\n\t * using cpumask_var_t directly because the actual type of\n\t * cpumsak_var_t depends on kernel configuration(particularly on\n\t * CONFIG_CPUMASK_OFFSTACK macro). Depending on the configuration\n\t * cpumask_var_t may be either a pointer to the struct cpumask\n\t * or a variable allocated on the stack. Thus we can not safely use\n\t * cpumask_var_t with RCU operations such as rcu_assign_pointer or\n\t * rcu_dereference. So cpumask_var_t is wrapped with struct\n\t * pcrypt_cpumask which makes possible to use it with RCU.\n\t */\n\tstruct pcrypt_cpumask {\n\t\tcpumask_var_t mask;\n\t} *cb_cpumask;\n\tstruct notifier_block nblock;\n};\n\nstatic struct padata_pcrypt pencrypt;\nstatic struct padata_pcrypt pdecrypt;\nstatic struct kset           *pcrypt_kset;\n\nstruct pcrypt_instance_ctx {\n\tstruct crypto_spawn spawn;\n\tunsigned int tfm_count;\n};\n\nstruct pcrypt_aead_ctx {\n\tstruct crypto_aead *child;\n\tunsigned int cb_cpu;\n};\n\nstatic int pcrypt_do_parallel(struct padata_priv *padata, unsigned int *cb_cpu,\n\t\t\t      struct padata_pcrypt *pcrypt)\n{\n\tunsigned int cpu_index, cpu, i;\n\tstruct pcrypt_cpumask *cpumask;\n\n\tcpu = *cb_cpu;\n\n\trcu_read_lock_bh();\n\tcpumask = rcu_dereference_bh(pcrypt->cb_cpumask);\n\tif (cpumask_test_cpu(cpu, cpumask->mask))\n\t\t\tgoto out;\n\n\tif (!cpumask_weight(cpumask->mask))\n\t\t\tgoto out;\n\n\tcpu_index = cpu % cpumask_weight(cpumask->mask);\n\n\tcpu = cpumask_first(cpumask->mask);\n\tfor (i = 0; i < cpu_index; i++)\n\t\tcpu = cpumask_next(cpu, cpumask->mask);\n\n\t*cb_cpu = cpu;\n\nout:\n\trcu_read_unlock_bh();\n\treturn padata_do_parallel(pcrypt->pinst, padata, cpu);\n}\n\nstatic int pcrypt_aead_setkey(struct crypto_aead *parent,\n\t\t\t      const u8 *key, unsigned int keylen)\n{\n\tstruct pcrypt_aead_ctx *ctx = crypto_aead_ctx(parent);\n\n\treturn crypto_aead_setkey(ctx->child, key, keylen);\n}\n\nstatic int pcrypt_aead_setauthsize(struct crypto_aead *parent,\n\t\t\t\t   unsigned int authsize)\n{\n\tstruct pcrypt_aead_ctx *ctx = crypto_aead_ctx(parent);\n\n\treturn crypto_aead_setauthsize(ctx->child, authsize);\n}\n\nstatic void pcrypt_aead_serial(struct padata_priv *padata)\n{\n\tstruct pcrypt_request *preq = pcrypt_padata_request(padata);\n\tstruct aead_request *req = pcrypt_request_ctx(preq);\n\n\taead_request_complete(req->base.data, padata->info);\n}\n\nstatic void pcrypt_aead_giv_serial(struct padata_priv *padata)\n{\n\tstruct pcrypt_request *preq = pcrypt_padata_request(padata);\n\tstruct aead_givcrypt_request *req = pcrypt_request_ctx(preq);\n\n\taead_request_complete(req->areq.base.data, padata->info);\n}\n\nstatic void pcrypt_aead_done(struct crypto_async_request *areq, int err)\n{\n\tstruct aead_request *req = areq->data;\n\tstruct pcrypt_request *preq = aead_request_ctx(req);\n\tstruct padata_priv *padata = pcrypt_request_padata(preq);\n\n\tpadata->info = err;\n\treq->base.flags &= ~CRYPTO_TFM_REQ_MAY_SLEEP;\n\n\tpadata_do_serial(padata);\n}\n\nstatic void pcrypt_aead_enc(struct padata_priv *padata)\n{\n\tstruct pcrypt_request *preq = pcrypt_padata_request(padata);\n\tstruct aead_request *req = pcrypt_request_ctx(preq);\n\n\tpadata->info = crypto_aead_encrypt(req);\n\n\tif (padata->info == -EINPROGRESS)\n\t\treturn;\n\n\tpadata_do_serial(padata);\n}\n\nstatic int pcrypt_aead_encrypt(struct aead_request *req)\n{\n\tint err;\n\tstruct pcrypt_request *preq = aead_request_ctx(req);\n\tstruct aead_request *creq = pcrypt_request_ctx(preq);\n\tstruct padata_priv *padata = pcrypt_request_padata(preq);\n\tstruct crypto_aead *aead = crypto_aead_reqtfm(req);\n\tstruct pcrypt_aead_ctx *ctx = crypto_aead_ctx(aead);\n\tu32 flags = aead_request_flags(req);\n\n\tmemset(padata, 0, sizeof(struct padata_priv));\n\n\tpadata->parallel = pcrypt_aead_enc;\n\tpadata->serial = pcrypt_aead_serial;\n\n\taead_request_set_tfm(creq, ctx->child);\n\taead_request_set_callback(creq, flags & ~CRYPTO_TFM_REQ_MAY_SLEEP,\n\t\t\t\t  pcrypt_aead_done, req);\n\taead_request_set_crypt(creq, req->src, req->dst,\n\t\t\t       req->cryptlen, req->iv);\n\taead_request_set_assoc(creq, req->assoc, req->assoclen);\n\n\terr = pcrypt_do_parallel(padata, &ctx->cb_cpu, &pencrypt);\n\tif (!err)\n\t\treturn -EINPROGRESS;\n\n\treturn err;\n}\n\nstatic void pcrypt_aead_dec(struct padata_priv *padata)\n{\n\tstruct pcrypt_request *preq = pcrypt_padata_request(padata);\n\tstruct aead_request *req = pcrypt_request_ctx(preq);\n\n\tpadata->info = crypto_aead_decrypt(req);\n\n\tif (padata->info == -EINPROGRESS)\n\t\treturn;\n\n\tpadata_do_serial(padata);\n}\n\nstatic int pcrypt_aead_decrypt(struct aead_request *req)\n{\n\tint err;\n\tstruct pcrypt_request *preq = aead_request_ctx(req);\n\tstruct aead_request *creq = pcrypt_request_ctx(preq);\n\tstruct padata_priv *padata = pcrypt_request_padata(preq);\n\tstruct crypto_aead *aead = crypto_aead_reqtfm(req);\n\tstruct pcrypt_aead_ctx *ctx = crypto_aead_ctx(aead);\n\tu32 flags = aead_request_flags(req);\n\n\tmemset(padata, 0, sizeof(struct padata_priv));\n\n\tpadata->parallel = pcrypt_aead_dec;\n\tpadata->serial = pcrypt_aead_serial;\n\n\taead_request_set_tfm(creq, ctx->child);\n\taead_request_set_callback(creq, flags & ~CRYPTO_TFM_REQ_MAY_SLEEP,\n\t\t\t\t  pcrypt_aead_done, req);\n\taead_request_set_crypt(creq, req->src, req->dst,\n\t\t\t       req->cryptlen, req->iv);\n\taead_request_set_assoc(creq, req->assoc, req->assoclen);\n\n\terr = pcrypt_do_parallel(padata, &ctx->cb_cpu, &pdecrypt);\n\tif (!err)\n\t\treturn -EINPROGRESS;\n\n\treturn err;\n}\n\nstatic void pcrypt_aead_givenc(struct padata_priv *padata)\n{\n\tstruct pcrypt_request *preq = pcrypt_padata_request(padata);\n\tstruct aead_givcrypt_request *req = pcrypt_request_ctx(preq);\n\n\tpadata->info = crypto_aead_givencrypt(req);\n\n\tif (padata->info == -EINPROGRESS)\n\t\treturn;\n\n\tpadata_do_serial(padata);\n}\n\nstatic int pcrypt_aead_givencrypt(struct aead_givcrypt_request *req)\n{\n\tint err;\n\tstruct aead_request *areq = &req->areq;\n\tstruct pcrypt_request *preq = aead_request_ctx(areq);\n\tstruct aead_givcrypt_request *creq = pcrypt_request_ctx(preq);\n\tstruct padata_priv *padata = pcrypt_request_padata(preq);\n\tstruct crypto_aead *aead = aead_givcrypt_reqtfm(req);\n\tstruct pcrypt_aead_ctx *ctx = crypto_aead_ctx(aead);\n\tu32 flags = aead_request_flags(areq);\n\n\tmemset(padata, 0, sizeof(struct padata_priv));\n\n\tpadata->parallel = pcrypt_aead_givenc;\n\tpadata->serial = pcrypt_aead_giv_serial;\n\n\taead_givcrypt_set_tfm(creq, ctx->child);\n\taead_givcrypt_set_callback(creq, flags & ~CRYPTO_TFM_REQ_MAY_SLEEP,\n\t\t\t\t   pcrypt_aead_done, areq);\n\taead_givcrypt_set_crypt(creq, areq->src, areq->dst,\n\t\t\t\tareq->cryptlen, areq->iv);\n\taead_givcrypt_set_assoc(creq, areq->assoc, areq->assoclen);\n\taead_givcrypt_set_giv(creq, req->giv, req->seq);\n\n\terr = pcrypt_do_parallel(padata, &ctx->cb_cpu, &pencrypt);\n\tif (!err)\n\t\treturn -EINPROGRESS;\n\n\treturn err;\n}\n\nstatic int pcrypt_aead_init_tfm(struct crypto_tfm *tfm)\n{\n\tint cpu, cpu_index;\n\tstruct crypto_instance *inst = crypto_tfm_alg_instance(tfm);\n\tstruct pcrypt_instance_ctx *ictx = crypto_instance_ctx(inst);\n\tstruct pcrypt_aead_ctx *ctx = crypto_tfm_ctx(tfm);\n\tstruct crypto_aead *cipher;\n\n\tictx->tfm_count++;\n\n\tcpu_index = ictx->tfm_count % cpumask_weight(cpu_online_mask);\n\n\tctx->cb_cpu = cpumask_first(cpu_online_mask);\n\tfor (cpu = 0; cpu < cpu_index; cpu++)\n\t\tctx->cb_cpu = cpumask_next(ctx->cb_cpu, cpu_online_mask);\n\n\tcipher = crypto_spawn_aead(crypto_instance_ctx(inst));\n\n\tif (IS_ERR(cipher))\n\t\treturn PTR_ERR(cipher);\n\n\tctx->child = cipher;\n\ttfm->crt_aead.reqsize = sizeof(struct pcrypt_request)\n\t\t+ sizeof(struct aead_givcrypt_request)\n\t\t+ crypto_aead_reqsize(cipher);\n\n\treturn 0;\n}\n\nstatic void pcrypt_aead_exit_tfm(struct crypto_tfm *tfm)\n{\n\tstruct pcrypt_aead_ctx *ctx = crypto_tfm_ctx(tfm);\n\n\tcrypto_free_aead(ctx->child);\n}\n\nstatic struct crypto_instance *pcrypt_alloc_instance(struct crypto_alg *alg)\n{\n\tstruct crypto_instance *inst;\n\tstruct pcrypt_instance_ctx *ctx;\n\tint err;\n\n\tinst = kzalloc(sizeof(*inst) + sizeof(*ctx), GFP_KERNEL);\n\tif (!inst) {\n\t\tinst = ERR_PTR(-ENOMEM);\n\t\tgoto out;\n\t}\n\n\terr = -ENAMETOOLONG;\n\tif (snprintf(inst->alg.cra_driver_name, CRYPTO_MAX_ALG_NAME,\n\t\t     \"pcrypt(%s)\", alg->cra_driver_name) >= CRYPTO_MAX_ALG_NAME)\n\t\tgoto out_free_inst;\n\n\tmemcpy(inst->alg.cra_name, alg->cra_name, CRYPTO_MAX_ALG_NAME);\n\n\tctx = crypto_instance_ctx(inst);\n\terr = crypto_init_spawn(&ctx->spawn, alg, inst,\n\t\t\t\tCRYPTO_ALG_TYPE_MASK);\n\tif (err)\n\t\tgoto out_free_inst;\n\n\tinst->alg.cra_priority = alg->cra_priority + 100;\n\tinst->alg.cra_blocksize = alg->cra_blocksize;\n\tinst->alg.cra_alignmask = alg->cra_alignmask;\n\nout:\n\treturn inst;\n\nout_free_inst:\n\tkfree(inst);\n\tinst = ERR_PTR(err);\n\tgoto out;\n}\n\nstatic struct crypto_instance *pcrypt_alloc_aead(struct rtattr **tb,\n\t\t\t\t\t\t u32 type, u32 mask)\n{\n\tstruct crypto_instance *inst;\n\tstruct crypto_alg *alg;\n\n\talg = crypto_get_attr_alg(tb, type, (mask & CRYPTO_ALG_TYPE_MASK));\n\tif (IS_ERR(alg))\n\t\treturn ERR_CAST(alg);\n\n\tinst = pcrypt_alloc_instance(alg);\n\tif (IS_ERR(inst))\n\t\tgoto out_put_alg;\n\n\tinst->alg.cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC;\n\tinst->alg.cra_type = &crypto_aead_type;\n\n\tinst->alg.cra_aead.ivsize = alg->cra_aead.ivsize;\n\tinst->alg.cra_aead.geniv = alg->cra_aead.geniv;\n\tinst->alg.cra_aead.maxauthsize = alg->cra_aead.maxauthsize;\n\n\tinst->alg.cra_ctxsize = sizeof(struct pcrypt_aead_ctx);\n\n\tinst->alg.cra_init = pcrypt_aead_init_tfm;\n\tinst->alg.cra_exit = pcrypt_aead_exit_tfm;\n\n\tinst->alg.cra_aead.setkey = pcrypt_aead_setkey;\n\tinst->alg.cra_aead.setauthsize = pcrypt_aead_setauthsize;\n\tinst->alg.cra_aead.encrypt = pcrypt_aead_encrypt;\n\tinst->alg.cra_aead.decrypt = pcrypt_aead_decrypt;\n\tinst->alg.cra_aead.givencrypt = pcrypt_aead_givencrypt;\n\nout_put_alg:\n\tcrypto_mod_put(alg);\n\treturn inst;\n}\n\nstatic struct crypto_instance *pcrypt_alloc(struct rtattr **tb)\n{\n\tstruct crypto_attr_type *algt;\n\n\talgt = crypto_get_attr_type(tb);\n\tif (IS_ERR(algt))\n\t\treturn ERR_CAST(algt);\n\n\tswitch (algt->type & algt->mask & CRYPTO_ALG_TYPE_MASK) {\n\tcase CRYPTO_ALG_TYPE_AEAD:\n\t\treturn pcrypt_alloc_aead(tb, algt->type, algt->mask);\n\t}\n\n\treturn ERR_PTR(-EINVAL);\n}\n\nstatic void pcrypt_free(struct crypto_instance *inst)\n{\n\tstruct pcrypt_instance_ctx *ctx = crypto_instance_ctx(inst);\n\n\tcrypto_drop_spawn(&ctx->spawn);\n\tkfree(inst);\n}\n\nstatic int pcrypt_cpumask_change_notify(struct notifier_block *self,\n\t\t\t\t\tunsigned long val, void *data)\n{\n\tstruct padata_pcrypt *pcrypt;\n\tstruct pcrypt_cpumask *new_mask, *old_mask;\n\tstruct padata_cpumask *cpumask = (struct padata_cpumask *)data;\n\n\tif (!(val & PADATA_CPU_SERIAL))\n\t\treturn 0;\n\n\tpcrypt = container_of(self, struct padata_pcrypt, nblock);\n\tnew_mask = kmalloc(sizeof(*new_mask), GFP_KERNEL);\n\tif (!new_mask)\n\t\treturn -ENOMEM;\n\tif (!alloc_cpumask_var(&new_mask->mask, GFP_KERNEL)) {\n\t\tkfree(new_mask);\n\t\treturn -ENOMEM;\n\t}\n\n\told_mask = pcrypt->cb_cpumask;\n\n\tcpumask_copy(new_mask->mask, cpumask->cbcpu);\n\trcu_assign_pointer(pcrypt->cb_cpumask, new_mask);\n\tsynchronize_rcu_bh();\n\n\tfree_cpumask_var(old_mask->mask);\n\tkfree(old_mask);\n\treturn 0;\n}\n\nstatic int pcrypt_sysfs_add(struct padata_instance *pinst, const char *name)\n{\n\tint ret;\n\n\tpinst->kobj.kset = pcrypt_kset;\n\tret = kobject_add(&pinst->kobj, NULL, name);\n\tif (!ret)\n\t\tkobject_uevent(&pinst->kobj, KOBJ_ADD);\n\n\treturn ret;\n}\n\nstatic int pcrypt_init_padata(struct padata_pcrypt *pcrypt,\n\t\t\t      const char *name)\n{\n\tint ret = -ENOMEM;\n\tstruct pcrypt_cpumask *mask;\n\n\tget_online_cpus();\n\n\tpcrypt->wq = alloc_workqueue(\"%s\", WQ_MEM_RECLAIM | WQ_CPU_INTENSIVE,\n\t\t\t\t     1, name);\n\tif (!pcrypt->wq)\n\t\tgoto err;\n\n\tpcrypt->pinst = padata_alloc_possible(pcrypt->wq);\n\tif (!pcrypt->pinst)\n\t\tgoto err_destroy_workqueue;\n\n\tmask = kmalloc(sizeof(*mask), GFP_KERNEL);\n\tif (!mask)\n\t\tgoto err_free_padata;\n\tif (!alloc_cpumask_var(&mask->mask, GFP_KERNEL)) {\n\t\tkfree(mask);\n\t\tgoto err_free_padata;\n\t}\n\n\tcpumask_and(mask->mask, cpu_possible_mask, cpu_online_mask);\n\trcu_assign_pointer(pcrypt->cb_cpumask, mask);\n\n\tpcrypt->nblock.notifier_call = pcrypt_cpumask_change_notify;\n\tret = padata_register_cpumask_notifier(pcrypt->pinst, &pcrypt->nblock);\n\tif (ret)\n\t\tgoto err_free_cpumask;\n\n\tret = pcrypt_sysfs_add(pcrypt->pinst, name);\n\tif (ret)\n\t\tgoto err_unregister_notifier;\n\n\tput_online_cpus();\n\n\treturn ret;\n\nerr_unregister_notifier:\n\tpadata_unregister_cpumask_notifier(pcrypt->pinst, &pcrypt->nblock);\nerr_free_cpumask:\n\tfree_cpumask_var(mask->mask);\n\tkfree(mask);\nerr_free_padata:\n\tpadata_free(pcrypt->pinst);\nerr_destroy_workqueue:\n\tdestroy_workqueue(pcrypt->wq);\nerr:\n\tput_online_cpus();\n\n\treturn ret;\n}\n\nstatic void pcrypt_fini_padata(struct padata_pcrypt *pcrypt)\n{\n\tfree_cpumask_var(pcrypt->cb_cpumask->mask);\n\tkfree(pcrypt->cb_cpumask);\n\n\tpadata_stop(pcrypt->pinst);\n\tpadata_unregister_cpumask_notifier(pcrypt->pinst, &pcrypt->nblock);\n\tdestroy_workqueue(pcrypt->wq);\n\tpadata_free(pcrypt->pinst);\n}\n\nstatic struct crypto_template pcrypt_tmpl = {\n\t.name = \"pcrypt\",\n\t.alloc = pcrypt_alloc,\n\t.free = pcrypt_free,\n\t.module = THIS_MODULE,\n};\n\nstatic int __init pcrypt_init(void)\n{\n\tint err = -ENOMEM;\n\n\tpcrypt_kset = kset_create_and_add(\"pcrypt\", NULL, kernel_kobj);\n\tif (!pcrypt_kset)\n\t\tgoto err;\n\n\terr = pcrypt_init_padata(&pencrypt, \"pencrypt\");\n\tif (err)\n\t\tgoto err_unreg_kset;\n\n\terr = pcrypt_init_padata(&pdecrypt, \"pdecrypt\");\n\tif (err)\n\t\tgoto err_deinit_pencrypt;\n\n\tpadata_start(pencrypt.pinst);\n\tpadata_start(pdecrypt.pinst);\n\n\treturn crypto_register_template(&pcrypt_tmpl);\n\nerr_deinit_pencrypt:\n\tpcrypt_fini_padata(&pencrypt);\nerr_unreg_kset:\n\tkset_unregister(pcrypt_kset);\nerr:\n\treturn err;\n}\n\nstatic void __exit pcrypt_exit(void)\n{\n\tpcrypt_fini_padata(&pencrypt);\n\tpcrypt_fini_padata(&pdecrypt);\n\n\tkset_unregister(pcrypt_kset);\n\tcrypto_unregister_template(&pcrypt_tmpl);\n}\n\nmodule_init(pcrypt_init);\nmodule_exit(pcrypt_exit);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_AUTHOR(\"Steffen Klassert <steffen.klassert@secunet.com>\");\nMODULE_DESCRIPTION(\"Parallel crypto wrapper\");\nMODULE_ALIAS_CRYPTO(\"pcrypt\");\n", "/*\n * seqiv: Sequence Number IV Generator\n *\n * This generator generates an IV based on a sequence number by xoring it\n * with a salt.  This algorithm is mainly useful for CTR and similar modes.\n *\n * Copyright (c) 2007 Herbert Xu <herbert@gondor.apana.org.au>\n *\n * This program is free software; you can redistribute it and/or modify it\n * under the terms of the GNU General Public License as published by the Free\n * Software Foundation; either version 2 of the License, or (at your option)\n * any later version.\n *\n */\n\n#include <crypto/internal/aead.h>\n#include <crypto/internal/skcipher.h>\n#include <crypto/rng.h>\n#include <linux/err.h>\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/slab.h>\n#include <linux/spinlock.h>\n#include <linux/string.h>\n\nstruct seqiv_ctx {\n\tspinlock_t lock;\n\tu8 salt[] __attribute__ ((aligned(__alignof__(u32))));\n};\n\nstatic void seqiv_complete2(struct skcipher_givcrypt_request *req, int err)\n{\n\tstruct ablkcipher_request *subreq = skcipher_givcrypt_reqctx(req);\n\tstruct crypto_ablkcipher *geniv;\n\n\tif (err == -EINPROGRESS)\n\t\treturn;\n\n\tif (err)\n\t\tgoto out;\n\n\tgeniv = skcipher_givcrypt_reqtfm(req);\n\tmemcpy(req->creq.info, subreq->info, crypto_ablkcipher_ivsize(geniv));\n\nout:\n\tkfree(subreq->info);\n}\n\nstatic void seqiv_complete(struct crypto_async_request *base, int err)\n{\n\tstruct skcipher_givcrypt_request *req = base->data;\n\n\tseqiv_complete2(req, err);\n\tskcipher_givcrypt_complete(req, err);\n}\n\nstatic void seqiv_aead_complete2(struct aead_givcrypt_request *req, int err)\n{\n\tstruct aead_request *subreq = aead_givcrypt_reqctx(req);\n\tstruct crypto_aead *geniv;\n\n\tif (err == -EINPROGRESS)\n\t\treturn;\n\n\tif (err)\n\t\tgoto out;\n\n\tgeniv = aead_givcrypt_reqtfm(req);\n\tmemcpy(req->areq.iv, subreq->iv, crypto_aead_ivsize(geniv));\n\nout:\n\tkfree(subreq->iv);\n}\n\nstatic void seqiv_aead_complete(struct crypto_async_request *base, int err)\n{\n\tstruct aead_givcrypt_request *req = base->data;\n\n\tseqiv_aead_complete2(req, err);\n\taead_givcrypt_complete(req, err);\n}\n\nstatic void seqiv_geniv(struct seqiv_ctx *ctx, u8 *info, u64 seq,\n\t\t\tunsigned int ivsize)\n{\n\tunsigned int len = ivsize;\n\n\tif (ivsize > sizeof(u64)) {\n\t\tmemset(info, 0, ivsize - sizeof(u64));\n\t\tlen = sizeof(u64);\n\t}\n\tseq = cpu_to_be64(seq);\n\tmemcpy(info + ivsize - len, &seq, len);\n\tcrypto_xor(info, ctx->salt, ivsize);\n}\n\nstatic int seqiv_givencrypt(struct skcipher_givcrypt_request *req)\n{\n\tstruct crypto_ablkcipher *geniv = skcipher_givcrypt_reqtfm(req);\n\tstruct seqiv_ctx *ctx = crypto_ablkcipher_ctx(geniv);\n\tstruct ablkcipher_request *subreq = skcipher_givcrypt_reqctx(req);\n\tcrypto_completion_t compl;\n\tvoid *data;\n\tu8 *info;\n\tunsigned int ivsize;\n\tint err;\n\n\tablkcipher_request_set_tfm(subreq, skcipher_geniv_cipher(geniv));\n\n\tcompl = req->creq.base.complete;\n\tdata = req->creq.base.data;\n\tinfo = req->creq.info;\n\n\tivsize = crypto_ablkcipher_ivsize(geniv);\n\n\tif (unlikely(!IS_ALIGNED((unsigned long)info,\n\t\t\t\t crypto_ablkcipher_alignmask(geniv) + 1))) {\n\t\tinfo = kmalloc(ivsize, req->creq.base.flags &\n\t\t\t\t       CRYPTO_TFM_REQ_MAY_SLEEP ? GFP_KERNEL:\n\t\t\t\t\t\t\t\t  GFP_ATOMIC);\n\t\tif (!info)\n\t\t\treturn -ENOMEM;\n\n\t\tcompl = seqiv_complete;\n\t\tdata = req;\n\t}\n\n\tablkcipher_request_set_callback(subreq, req->creq.base.flags, compl,\n\t\t\t\t\tdata);\n\tablkcipher_request_set_crypt(subreq, req->creq.src, req->creq.dst,\n\t\t\t\t     req->creq.nbytes, info);\n\n\tseqiv_geniv(ctx, info, req->seq, ivsize);\n\tmemcpy(req->giv, info, ivsize);\n\n\terr = crypto_ablkcipher_encrypt(subreq);\n\tif (unlikely(info != req->creq.info))\n\t\tseqiv_complete2(req, err);\n\treturn err;\n}\n\nstatic int seqiv_aead_givencrypt(struct aead_givcrypt_request *req)\n{\n\tstruct crypto_aead *geniv = aead_givcrypt_reqtfm(req);\n\tstruct seqiv_ctx *ctx = crypto_aead_ctx(geniv);\n\tstruct aead_request *areq = &req->areq;\n\tstruct aead_request *subreq = aead_givcrypt_reqctx(req);\n\tcrypto_completion_t compl;\n\tvoid *data;\n\tu8 *info;\n\tunsigned int ivsize;\n\tint err;\n\n\taead_request_set_tfm(subreq, aead_geniv_base(geniv));\n\n\tcompl = areq->base.complete;\n\tdata = areq->base.data;\n\tinfo = areq->iv;\n\n\tivsize = crypto_aead_ivsize(geniv);\n\n\tif (unlikely(!IS_ALIGNED((unsigned long)info,\n\t\t\t\t crypto_aead_alignmask(geniv) + 1))) {\n\t\tinfo = kmalloc(ivsize, areq->base.flags &\n\t\t\t\t       CRYPTO_TFM_REQ_MAY_SLEEP ? GFP_KERNEL:\n\t\t\t\t\t\t\t\t  GFP_ATOMIC);\n\t\tif (!info)\n\t\t\treturn -ENOMEM;\n\n\t\tcompl = seqiv_aead_complete;\n\t\tdata = req;\n\t}\n\n\taead_request_set_callback(subreq, areq->base.flags, compl, data);\n\taead_request_set_crypt(subreq, areq->src, areq->dst, areq->cryptlen,\n\t\t\t       info);\n\taead_request_set_assoc(subreq, areq->assoc, areq->assoclen);\n\n\tseqiv_geniv(ctx, info, req->seq, ivsize);\n\tmemcpy(req->giv, info, ivsize);\n\n\terr = crypto_aead_encrypt(subreq);\n\tif (unlikely(info != areq->iv))\n\t\tseqiv_aead_complete2(req, err);\n\treturn err;\n}\n\nstatic int seqiv_givencrypt_first(struct skcipher_givcrypt_request *req)\n{\n\tstruct crypto_ablkcipher *geniv = skcipher_givcrypt_reqtfm(req);\n\tstruct seqiv_ctx *ctx = crypto_ablkcipher_ctx(geniv);\n\tint err = 0;\n\n\tspin_lock_bh(&ctx->lock);\n\tif (crypto_ablkcipher_crt(geniv)->givencrypt != seqiv_givencrypt_first)\n\t\tgoto unlock;\n\n\tcrypto_ablkcipher_crt(geniv)->givencrypt = seqiv_givencrypt;\n\terr = crypto_rng_get_bytes(crypto_default_rng, ctx->salt,\n\t\t\t\t   crypto_ablkcipher_ivsize(geniv));\n\nunlock:\n\tspin_unlock_bh(&ctx->lock);\n\n\tif (err)\n\t\treturn err;\n\n\treturn seqiv_givencrypt(req);\n}\n\nstatic int seqiv_aead_givencrypt_first(struct aead_givcrypt_request *req)\n{\n\tstruct crypto_aead *geniv = aead_givcrypt_reqtfm(req);\n\tstruct seqiv_ctx *ctx = crypto_aead_ctx(geniv);\n\tint err = 0;\n\n\tspin_lock_bh(&ctx->lock);\n\tif (crypto_aead_crt(geniv)->givencrypt != seqiv_aead_givencrypt_first)\n\t\tgoto unlock;\n\n\tcrypto_aead_crt(geniv)->givencrypt = seqiv_aead_givencrypt;\n\terr = crypto_rng_get_bytes(crypto_default_rng, ctx->salt,\n\t\t\t\t   crypto_aead_ivsize(geniv));\n\nunlock:\n\tspin_unlock_bh(&ctx->lock);\n\n\tif (err)\n\t\treturn err;\n\n\treturn seqiv_aead_givencrypt(req);\n}\n\nstatic int seqiv_init(struct crypto_tfm *tfm)\n{\n\tstruct crypto_ablkcipher *geniv = __crypto_ablkcipher_cast(tfm);\n\tstruct seqiv_ctx *ctx = crypto_ablkcipher_ctx(geniv);\n\n\tspin_lock_init(&ctx->lock);\n\n\ttfm->crt_ablkcipher.reqsize = sizeof(struct ablkcipher_request);\n\n\treturn skcipher_geniv_init(tfm);\n}\n\nstatic int seqiv_aead_init(struct crypto_tfm *tfm)\n{\n\tstruct crypto_aead *geniv = __crypto_aead_cast(tfm);\n\tstruct seqiv_ctx *ctx = crypto_aead_ctx(geniv);\n\n\tspin_lock_init(&ctx->lock);\n\n\ttfm->crt_aead.reqsize = sizeof(struct aead_request);\n\n\treturn aead_geniv_init(tfm);\n}\n\nstatic struct crypto_template seqiv_tmpl;\n\nstatic struct crypto_instance *seqiv_ablkcipher_alloc(struct rtattr **tb)\n{\n\tstruct crypto_instance *inst;\n\n\tinst = skcipher_geniv_alloc(&seqiv_tmpl, tb, 0, 0);\n\n\tif (IS_ERR(inst))\n\t\tgoto out;\n\n\tinst->alg.cra_ablkcipher.givencrypt = seqiv_givencrypt_first;\n\n\tinst->alg.cra_init = seqiv_init;\n\tinst->alg.cra_exit = skcipher_geniv_exit;\n\n\tinst->alg.cra_ctxsize += inst->alg.cra_ablkcipher.ivsize;\n\nout:\n\treturn inst;\n}\n\nstatic struct crypto_instance *seqiv_aead_alloc(struct rtattr **tb)\n{\n\tstruct crypto_instance *inst;\n\n\tinst = aead_geniv_alloc(&seqiv_tmpl, tb, 0, 0);\n\n\tif (IS_ERR(inst))\n\t\tgoto out;\n\n\tinst->alg.cra_aead.givencrypt = seqiv_aead_givencrypt_first;\n\n\tinst->alg.cra_init = seqiv_aead_init;\n\tinst->alg.cra_exit = aead_geniv_exit;\n\n\tinst->alg.cra_ctxsize = inst->alg.cra_aead.ivsize;\n\nout:\n\treturn inst;\n}\n\nstatic struct crypto_instance *seqiv_alloc(struct rtattr **tb)\n{\n\tstruct crypto_attr_type *algt;\n\tstruct crypto_instance *inst;\n\tint err;\n\n\talgt = crypto_get_attr_type(tb);\n\tif (IS_ERR(algt))\n\t\treturn ERR_CAST(algt);\n\n\terr = crypto_get_default_rng();\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\tif ((algt->type ^ CRYPTO_ALG_TYPE_AEAD) & CRYPTO_ALG_TYPE_MASK)\n\t\tinst = seqiv_ablkcipher_alloc(tb);\n\telse\n\t\tinst = seqiv_aead_alloc(tb);\n\n\tif (IS_ERR(inst))\n\t\tgoto put_rng;\n\n\tinst->alg.cra_alignmask |= __alignof__(u32) - 1;\n\tinst->alg.cra_ctxsize += sizeof(struct seqiv_ctx);\n\nout:\n\treturn inst;\n\nput_rng:\n\tcrypto_put_default_rng();\n\tgoto out;\n}\n\nstatic void seqiv_free(struct crypto_instance *inst)\n{\n\tif ((inst->alg.cra_flags ^ CRYPTO_ALG_TYPE_AEAD) & CRYPTO_ALG_TYPE_MASK)\n\t\tskcipher_geniv_free(inst);\n\telse\n\t\taead_geniv_free(inst);\n\tcrypto_put_default_rng();\n}\n\nstatic struct crypto_template seqiv_tmpl = {\n\t.name = \"seqiv\",\n\t.alloc = seqiv_alloc,\n\t.free = seqiv_free,\n\t.module = THIS_MODULE,\n};\n\nstatic int __init seqiv_module_init(void)\n{\n\treturn crypto_register_template(&seqiv_tmpl);\n}\n\nstatic void __exit seqiv_module_exit(void)\n{\n\tcrypto_unregister_template(&seqiv_tmpl);\n}\n\nmodule_init(seqiv_module_init);\nmodule_exit(seqiv_module_exit);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"Sequence Number IV Generator\");\nMODULE_ALIAS_CRYPTO(\"seqiv\");\n", "/*\n * Modified to interface to the Linux kernel\n * Copyright (c) 2009, Intel Corporation.\n *\n * This program is free software; you can redistribute it and/or modify it\n * under the terms and conditions of the GNU General Public License,\n * version 2, as published by the Free Software Foundation.\n *\n * This program is distributed in the hope it will be useful, but WITHOUT\n * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for\n * more details.\n *\n * You should have received a copy of the GNU General Public License along with\n * this program; if not, write to the Free Software Foundation, Inc., 59 Temple\n * Place - Suite 330, Boston, MA 02111-1307 USA.\n */\n\n/* --------------------------------------------------------------------------\n * VMAC and VHASH Implementation by Ted Krovetz (tdk@acm.org) and Wei Dai.\n * This implementation is herby placed in the public domain.\n * The authors offers no warranty. Use at your own risk.\n * Please send bug reports to the authors.\n * Last modified: 17 APR 08, 1700 PDT\n * ----------------------------------------------------------------------- */\n\n#include <linux/init.h>\n#include <linux/types.h>\n#include <linux/crypto.h>\n#include <linux/module.h>\n#include <linux/scatterlist.h>\n#include <asm/byteorder.h>\n#include <crypto/scatterwalk.h>\n#include <crypto/vmac.h>\n#include <crypto/internal/hash.h>\n\n/*\n * Constants and masks\n */\n#define UINT64_C(x) x##ULL\nstatic const u64 p64   = UINT64_C(0xfffffffffffffeff);\t/* 2^64 - 257 prime  */\nstatic const u64 m62   = UINT64_C(0x3fffffffffffffff);\t/* 62-bit mask       */\nstatic const u64 m63   = UINT64_C(0x7fffffffffffffff);\t/* 63-bit mask       */\nstatic const u64 m64   = UINT64_C(0xffffffffffffffff);\t/* 64-bit mask       */\nstatic const u64 mpoly = UINT64_C(0x1fffffff1fffffff);\t/* Poly key mask     */\n\n#define pe64_to_cpup le64_to_cpup\t\t/* Prefer little endian */\n\n#ifdef __LITTLE_ENDIAN\n#define INDEX_HIGH 1\n#define INDEX_LOW 0\n#else\n#define INDEX_HIGH 0\n#define INDEX_LOW 1\n#endif\n\n/*\n * The following routines are used in this implementation. They are\n * written via macros to simulate zero-overhead call-by-reference.\n *\n * MUL64: 64x64->128-bit multiplication\n * PMUL64: assumes top bits cleared on inputs\n * ADD128: 128x128->128-bit addition\n */\n\n#define ADD128(rh, rl, ih, il)\t\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tu64 _il = (il);\t\t\t\t\t\t\\\n\t\t(rl) += (_il);\t\t\t\t\t\t\\\n\t\tif ((rl) < (_il))\t\t\t\t\t\\\n\t\t\t(rh)++;\t\t\t\t\t\t\\\n\t\t(rh) += (ih);\t\t\t\t\t\t\\\n\t} while (0)\n\n#define MUL32(i1, i2)\t((u64)(u32)(i1)*(u32)(i2))\n\n#define PMUL64(rh, rl, i1, i2)\t/* Assumes m doesn't overflow */\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tu64 _i1 = (i1), _i2 = (i2);\t\t\t\t\\\n\t\tu64 m = MUL32(_i1, _i2>>32) + MUL32(_i1>>32, _i2);\t\\\n\t\trh = MUL32(_i1>>32, _i2>>32);\t\t\t\t\\\n\t\trl = MUL32(_i1, _i2);\t\t\t\t\t\\\n\t\tADD128(rh, rl, (m >> 32), (m << 32));\t\t\t\\\n\t} while (0)\n\n#define MUL64(rh, rl, i1, i2)\t\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tu64 _i1 = (i1), _i2 = (i2);\t\t\t\t\\\n\t\tu64 m1 = MUL32(_i1, _i2>>32);\t\t\t\t\\\n\t\tu64 m2 = MUL32(_i1>>32, _i2);\t\t\t\t\\\n\t\trh = MUL32(_i1>>32, _i2>>32);\t\t\t\t\\\n\t\trl = MUL32(_i1, _i2);\t\t\t\t\t\\\n\t\tADD128(rh, rl, (m1 >> 32), (m1 << 32));\t\t\t\\\n\t\tADD128(rh, rl, (m2 >> 32), (m2 << 32));\t\t\t\\\n\t} while (0)\n\n/*\n * For highest performance the L1 NH and L2 polynomial hashes should be\n * carefully implemented to take advantage of one's target architecture.\n * Here these two hash functions are defined multiple time; once for\n * 64-bit architectures, once for 32-bit SSE2 architectures, and once\n * for the rest (32-bit) architectures.\n * For each, nh_16 *must* be defined (works on multiples of 16 bytes).\n * Optionally, nh_vmac_nhbytes can be defined (for multiples of\n * VMAC_NHBYTES), and nh_16_2 and nh_vmac_nhbytes_2 (versions that do two\n * NH computations at once).\n */\n\n#ifdef CONFIG_64BIT\n\n#define nh_16(mp, kp, nw, rh, rl)\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tint i; u64 th, tl;\t\t\t\t\t\\\n\t\trh = rl = 0;\t\t\t\t\t\t\\\n\t\tfor (i = 0; i < nw; i += 2) {\t\t\t\t\\\n\t\t\tMUL64(th, tl, pe64_to_cpup((mp)+i)+(kp)[i],\t\\\n\t\t\t\tpe64_to_cpup((mp)+i+1)+(kp)[i+1]);\t\\\n\t\t\tADD128(rh, rl, th, tl);\t\t\t\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t} while (0)\n\n#define nh_16_2(mp, kp, nw, rh, rl, rh1, rl1)\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tint i; u64 th, tl;\t\t\t\t\t\\\n\t\trh1 = rl1 = rh = rl = 0;\t\t\t\t\\\n\t\tfor (i = 0; i < nw; i += 2) {\t\t\t\t\\\n\t\t\tMUL64(th, tl, pe64_to_cpup((mp)+i)+(kp)[i],\t\\\n\t\t\t\tpe64_to_cpup((mp)+i+1)+(kp)[i+1]);\t\\\n\t\t\tADD128(rh, rl, th, tl);\t\t\t\t\\\n\t\t\tMUL64(th, tl, pe64_to_cpup((mp)+i)+(kp)[i+2],\t\\\n\t\t\t\tpe64_to_cpup((mp)+i+1)+(kp)[i+3]);\t\\\n\t\t\tADD128(rh1, rl1, th, tl);\t\t\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t} while (0)\n\n#if (VMAC_NHBYTES >= 64) /* These versions do 64-bytes of message at a time */\n#define nh_vmac_nhbytes(mp, kp, nw, rh, rl)\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tint i; u64 th, tl;\t\t\t\t\t\\\n\t\trh = rl = 0;\t\t\t\t\t\t\\\n\t\tfor (i = 0; i < nw; i += 8) {\t\t\t\t\\\n\t\t\tMUL64(th, tl, pe64_to_cpup((mp)+i)+(kp)[i],\t\\\n\t\t\t\tpe64_to_cpup((mp)+i+1)+(kp)[i+1]);\t\\\n\t\t\tADD128(rh, rl, th, tl);\t\t\t\t\\\n\t\t\tMUL64(th, tl, pe64_to_cpup((mp)+i+2)+(kp)[i+2],\t\\\n\t\t\t\tpe64_to_cpup((mp)+i+3)+(kp)[i+3]);\t\\\n\t\t\tADD128(rh, rl, th, tl);\t\t\t\t\\\n\t\t\tMUL64(th, tl, pe64_to_cpup((mp)+i+4)+(kp)[i+4],\t\\\n\t\t\t\tpe64_to_cpup((mp)+i+5)+(kp)[i+5]);\t\\\n\t\t\tADD128(rh, rl, th, tl);\t\t\t\t\\\n\t\t\tMUL64(th, tl, pe64_to_cpup((mp)+i+6)+(kp)[i+6],\t\\\n\t\t\t\tpe64_to_cpup((mp)+i+7)+(kp)[i+7]);\t\\\n\t\t\tADD128(rh, rl, th, tl);\t\t\t\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t} while (0)\n\n#define nh_vmac_nhbytes_2(mp, kp, nw, rh, rl, rh1, rl1)\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tint i; u64 th, tl;\t\t\t\t\t\\\n\t\trh1 = rl1 = rh = rl = 0;\t\t\t\t\\\n\t\tfor (i = 0; i < nw; i += 8) {\t\t\t\t\\\n\t\t\tMUL64(th, tl, pe64_to_cpup((mp)+i)+(kp)[i],\t\\\n\t\t\t\tpe64_to_cpup((mp)+i+1)+(kp)[i+1]);\t\\\n\t\t\tADD128(rh, rl, th, tl);\t\t\t\t\\\n\t\t\tMUL64(th, tl, pe64_to_cpup((mp)+i)+(kp)[i+2],\t\\\n\t\t\t\tpe64_to_cpup((mp)+i+1)+(kp)[i+3]);\t\\\n\t\t\tADD128(rh1, rl1, th, tl);\t\t\t\\\n\t\t\tMUL64(th, tl, pe64_to_cpup((mp)+i+2)+(kp)[i+2],\t\\\n\t\t\t\tpe64_to_cpup((mp)+i+3)+(kp)[i+3]);\t\\\n\t\t\tADD128(rh, rl, th, tl);\t\t\t\t\\\n\t\t\tMUL64(th, tl, pe64_to_cpup((mp)+i+2)+(kp)[i+4],\t\\\n\t\t\t\tpe64_to_cpup((mp)+i+3)+(kp)[i+5]);\t\\\n\t\t\tADD128(rh1, rl1, th, tl);\t\t\t\\\n\t\t\tMUL64(th, tl, pe64_to_cpup((mp)+i+4)+(kp)[i+4],\t\\\n\t\t\t\tpe64_to_cpup((mp)+i+5)+(kp)[i+5]);\t\\\n\t\t\tADD128(rh, rl, th, tl);\t\t\t\t\\\n\t\t\tMUL64(th, tl, pe64_to_cpup((mp)+i+4)+(kp)[i+6],\t\\\n\t\t\t\tpe64_to_cpup((mp)+i+5)+(kp)[i+7]);\t\\\n\t\t\tADD128(rh1, rl1, th, tl);\t\t\t\\\n\t\t\tMUL64(th, tl, pe64_to_cpup((mp)+i+6)+(kp)[i+6],\t\\\n\t\t\t\tpe64_to_cpup((mp)+i+7)+(kp)[i+7]);\t\\\n\t\t\tADD128(rh, rl, th, tl);\t\t\t\t\\\n\t\t\tMUL64(th, tl, pe64_to_cpup((mp)+i+6)+(kp)[i+8],\t\\\n\t\t\t\tpe64_to_cpup((mp)+i+7)+(kp)[i+9]);\t\\\n\t\t\tADD128(rh1, rl1, th, tl);\t\t\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t} while (0)\n#endif\n\n#define poly_step(ah, al, kh, kl, mh, ml)\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tu64 t1h, t1l, t2h, t2l, t3h, t3l, z = 0;\t\t\\\n\t\t/* compute ab*cd, put bd into result registers */\t\\\n\t\tPMUL64(t3h, t3l, al, kh);\t\t\t\t\\\n\t\tPMUL64(t2h, t2l, ah, kl);\t\t\t\t\\\n\t\tPMUL64(t1h, t1l, ah, 2*kh);\t\t\t\t\\\n\t\tPMUL64(ah, al, al, kl);\t\t\t\t\t\\\n\t\t/* add 2 * ac to result */\t\t\t\t\\\n\t\tADD128(ah, al, t1h, t1l);\t\t\t\t\\\n\t\t/* add together ad + bc */\t\t\t\t\\\n\t\tADD128(t2h, t2l, t3h, t3l);\t\t\t\t\\\n\t\t/* now (ah,al), (t2l,2*t2h) need summing */\t\t\\\n\t\t/* first add the high registers, carrying into t2h */\t\\\n\t\tADD128(t2h, ah, z, t2l);\t\t\t\t\\\n\t\t/* double t2h and add top bit of ah */\t\t\t\\\n\t\tt2h = 2 * t2h + (ah >> 63);\t\t\t\t\\\n\t\tah &= m63;\t\t\t\t\t\t\\\n\t\t/* now add the low registers */\t\t\t\t\\\n\t\tADD128(ah, al, mh, ml);\t\t\t\t\t\\\n\t\tADD128(ah, al, z, t2h);\t\t\t\t\t\\\n\t} while (0)\n\n#else /* ! CONFIG_64BIT */\n\n#ifndef nh_16\n#define nh_16(mp, kp, nw, rh, rl)\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tu64 t1, t2, m1, m2, t;\t\t\t\t\t\\\n\t\tint i;\t\t\t\t\t\t\t\\\n\t\trh = rl = t = 0;\t\t\t\t\t\\\n\t\tfor (i = 0; i < nw; i += 2)  {\t\t\t\t\\\n\t\t\tt1 = pe64_to_cpup(mp+i) + kp[i];\t\t\\\n\t\t\tt2 = pe64_to_cpup(mp+i+1) + kp[i+1];\t\t\\\n\t\t\tm2 = MUL32(t1 >> 32, t2);\t\t\t\\\n\t\t\tm1 = MUL32(t1, t2 >> 32);\t\t\t\\\n\t\t\tADD128(rh, rl, MUL32(t1 >> 32, t2 >> 32),\t\\\n\t\t\t\tMUL32(t1, t2));\t\t\t\t\\\n\t\t\trh += (u64)(u32)(m1 >> 32)\t\t\t\\\n\t\t\t\t+ (u32)(m2 >> 32);\t\t\t\\\n\t\t\tt += (u64)(u32)m1 + (u32)m2;\t\t\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t\tADD128(rh, rl, (t >> 32), (t << 32));\t\t\t\\\n\t} while (0)\n#endif\n\nstatic void poly_step_func(u64 *ahi, u64 *alo,\n\t\t\tconst u64 *kh, const u64 *kl,\n\t\t\tconst u64 *mh, const u64 *ml)\n{\n#define a0 (*(((u32 *)alo)+INDEX_LOW))\n#define a1 (*(((u32 *)alo)+INDEX_HIGH))\n#define a2 (*(((u32 *)ahi)+INDEX_LOW))\n#define a3 (*(((u32 *)ahi)+INDEX_HIGH))\n#define k0 (*(((u32 *)kl)+INDEX_LOW))\n#define k1 (*(((u32 *)kl)+INDEX_HIGH))\n#define k2 (*(((u32 *)kh)+INDEX_LOW))\n#define k3 (*(((u32 *)kh)+INDEX_HIGH))\n\n\tu64 p, q, t;\n\tu32 t2;\n\n\tp = MUL32(a3, k3);\n\tp += p;\n\tp += *(u64 *)mh;\n\tp += MUL32(a0, k2);\n\tp += MUL32(a1, k1);\n\tp += MUL32(a2, k0);\n\tt = (u32)(p);\n\tp >>= 32;\n\tp += MUL32(a0, k3);\n\tp += MUL32(a1, k2);\n\tp += MUL32(a2, k1);\n\tp += MUL32(a3, k0);\n\tt |= ((u64)((u32)p & 0x7fffffff)) << 32;\n\tp >>= 31;\n\tp += (u64)(((u32 *)ml)[INDEX_LOW]);\n\tp += MUL32(a0, k0);\n\tq =  MUL32(a1, k3);\n\tq += MUL32(a2, k2);\n\tq += MUL32(a3, k1);\n\tq += q;\n\tp += q;\n\tt2 = (u32)(p);\n\tp >>= 32;\n\tp += (u64)(((u32 *)ml)[INDEX_HIGH]);\n\tp += MUL32(a0, k1);\n\tp += MUL32(a1, k0);\n\tq =  MUL32(a2, k3);\n\tq += MUL32(a3, k2);\n\tq += q;\n\tp += q;\n\t*(u64 *)(alo) = (p << 32) | t2;\n\tp >>= 32;\n\t*(u64 *)(ahi) = p + t;\n\n#undef a0\n#undef a1\n#undef a2\n#undef a3\n#undef k0\n#undef k1\n#undef k2\n#undef k3\n}\n\n#define poly_step(ah, al, kh, kl, mh, ml)\t\t\t\t\\\n\tpoly_step_func(&(ah), &(al), &(kh), &(kl), &(mh), &(ml))\n\n#endif  /* end of specialized NH and poly definitions */\n\n/* At least nh_16 is defined. Defined others as needed here */\n#ifndef nh_16_2\n#define nh_16_2(mp, kp, nw, rh, rl, rh2, rl2)\t\t\t\t\\\n\tdo { \t\t\t\t\t\t\t\t\\\n\t\tnh_16(mp, kp, nw, rh, rl);\t\t\t\t\\\n\t\tnh_16(mp, ((kp)+2), nw, rh2, rl2);\t\t\t\\\n\t} while (0)\n#endif\n#ifndef nh_vmac_nhbytes\n#define nh_vmac_nhbytes(mp, kp, nw, rh, rl)\t\t\t\t\\\n\tnh_16(mp, kp, nw, rh, rl)\n#endif\n#ifndef nh_vmac_nhbytes_2\n#define nh_vmac_nhbytes_2(mp, kp, nw, rh, rl, rh2, rl2)\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tnh_vmac_nhbytes(mp, kp, nw, rh, rl);\t\t\t\\\n\t\tnh_vmac_nhbytes(mp, ((kp)+2), nw, rh2, rl2);\t\t\\\n\t} while (0)\n#endif\n\nstatic void vhash_abort(struct vmac_ctx *ctx)\n{\n\tctx->polytmp[0] = ctx->polykey[0] ;\n\tctx->polytmp[1] = ctx->polykey[1] ;\n\tctx->first_block_processed = 0;\n}\n\nstatic u64 l3hash(u64 p1, u64 p2, u64 k1, u64 k2, u64 len)\n{\n\tu64 rh, rl, t, z = 0;\n\n\t/* fully reduce (p1,p2)+(len,0) mod p127 */\n\tt = p1 >> 63;\n\tp1 &= m63;\n\tADD128(p1, p2, len, t);\n\t/* At this point, (p1,p2) is at most 2^127+(len<<64) */\n\tt = (p1 > m63) + ((p1 == m63) && (p2 == m64));\n\tADD128(p1, p2, z, t);\n\tp1 &= m63;\n\n\t/* compute (p1,p2)/(2^64-2^32) and (p1,p2)%(2^64-2^32) */\n\tt = p1 + (p2 >> 32);\n\tt += (t >> 32);\n\tt += (u32)t > 0xfffffffeu;\n\tp1 += (t >> 32);\n\tp2 += (p1 << 32);\n\n\t/* compute (p1+k1)%p64 and (p2+k2)%p64 */\n\tp1 += k1;\n\tp1 += (0 - (p1 < k1)) & 257;\n\tp2 += k2;\n\tp2 += (0 - (p2 < k2)) & 257;\n\n\t/* compute (p1+k1)*(p2+k2)%p64 */\n\tMUL64(rh, rl, p1, p2);\n\tt = rh >> 56;\n\tADD128(t, rl, z, rh);\n\trh <<= 8;\n\tADD128(t, rl, z, rh);\n\tt += t << 8;\n\trl += t;\n\trl += (0 - (rl < t)) & 257;\n\trl += (0 - (rl > p64-1)) & 257;\n\treturn rl;\n}\n\nstatic void vhash_update(const unsigned char *m,\n\t\t\tunsigned int mbytes, /* Pos multiple of VMAC_NHBYTES */\n\t\t\tstruct vmac_ctx *ctx)\n{\n\tu64 rh, rl, *mptr;\n\tconst u64 *kptr = (u64 *)ctx->nhkey;\n\tint i;\n\tu64 ch, cl;\n\tu64 pkh = ctx->polykey[0];\n\tu64 pkl = ctx->polykey[1];\n\n\tif (!mbytes)\n\t\treturn;\n\n\tBUG_ON(mbytes % VMAC_NHBYTES);\n\n\tmptr = (u64 *)m;\n\ti = mbytes / VMAC_NHBYTES;  /* Must be non-zero */\n\n\tch = ctx->polytmp[0];\n\tcl = ctx->polytmp[1];\n\n\tif (!ctx->first_block_processed) {\n\t\tctx->first_block_processed = 1;\n\t\tnh_vmac_nhbytes(mptr, kptr, VMAC_NHBYTES/8, rh, rl);\n\t\trh &= m62;\n\t\tADD128(ch, cl, rh, rl);\n\t\tmptr += (VMAC_NHBYTES/sizeof(u64));\n\t\ti--;\n\t}\n\n\twhile (i--) {\n\t\tnh_vmac_nhbytes(mptr, kptr, VMAC_NHBYTES/8, rh, rl);\n\t\trh &= m62;\n\t\tpoly_step(ch, cl, pkh, pkl, rh, rl);\n\t\tmptr += (VMAC_NHBYTES/sizeof(u64));\n\t}\n\n\tctx->polytmp[0] = ch;\n\tctx->polytmp[1] = cl;\n}\n\nstatic u64 vhash(unsigned char m[], unsigned int mbytes,\n\t\t\tu64 *tagl, struct vmac_ctx *ctx)\n{\n\tu64 rh, rl, *mptr;\n\tconst u64 *kptr = (u64 *)ctx->nhkey;\n\tint i, remaining;\n\tu64 ch, cl;\n\tu64 pkh = ctx->polykey[0];\n\tu64 pkl = ctx->polykey[1];\n\n\tmptr = (u64 *)m;\n\ti = mbytes / VMAC_NHBYTES;\n\tremaining = mbytes % VMAC_NHBYTES;\n\n\tif (ctx->first_block_processed) {\n\t\tch = ctx->polytmp[0];\n\t\tcl = ctx->polytmp[1];\n\t} else if (i) {\n\t\tnh_vmac_nhbytes(mptr, kptr, VMAC_NHBYTES/8, ch, cl);\n\t\tch &= m62;\n\t\tADD128(ch, cl, pkh, pkl);\n\t\tmptr += (VMAC_NHBYTES/sizeof(u64));\n\t\ti--;\n\t} else if (remaining) {\n\t\tnh_16(mptr, kptr, 2*((remaining+15)/16), ch, cl);\n\t\tch &= m62;\n\t\tADD128(ch, cl, pkh, pkl);\n\t\tmptr += (VMAC_NHBYTES/sizeof(u64));\n\t\tgoto do_l3;\n\t} else {/* Empty String */\n\t\tch = pkh; cl = pkl;\n\t\tgoto do_l3;\n\t}\n\n\twhile (i--) {\n\t\tnh_vmac_nhbytes(mptr, kptr, VMAC_NHBYTES/8, rh, rl);\n\t\trh &= m62;\n\t\tpoly_step(ch, cl, pkh, pkl, rh, rl);\n\t\tmptr += (VMAC_NHBYTES/sizeof(u64));\n\t}\n\tif (remaining) {\n\t\tnh_16(mptr, kptr, 2*((remaining+15)/16), rh, rl);\n\t\trh &= m62;\n\t\tpoly_step(ch, cl, pkh, pkl, rh, rl);\n\t}\n\ndo_l3:\n\tvhash_abort(ctx);\n\tremaining *= 8;\n\treturn l3hash(ch, cl, ctx->l3key[0], ctx->l3key[1], remaining);\n}\n\nstatic u64 vmac(unsigned char m[], unsigned int mbytes,\n\t\t\tconst unsigned char n[16], u64 *tagl,\n\t\t\tstruct vmac_ctx_t *ctx)\n{\n\tu64 *in_n, *out_p;\n\tu64 p, h;\n\tint i;\n\n\tin_n = ctx->__vmac_ctx.cached_nonce;\n\tout_p = ctx->__vmac_ctx.cached_aes;\n\n\ti = n[15] & 1;\n\tif ((*(u64 *)(n+8) != in_n[1]) || (*(u64 *)(n) != in_n[0])) {\n\t\tin_n[0] = *(u64 *)(n);\n\t\tin_n[1] = *(u64 *)(n+8);\n\t\t((unsigned char *)in_n)[15] &= 0xFE;\n\t\tcrypto_cipher_encrypt_one(ctx->child,\n\t\t\t(unsigned char *)out_p, (unsigned char *)in_n);\n\n\t\t((unsigned char *)in_n)[15] |= (unsigned char)(1-i);\n\t}\n\tp = be64_to_cpup(out_p + i);\n\th = vhash(m, mbytes, (u64 *)0, &ctx->__vmac_ctx);\n\treturn le64_to_cpu(p + h);\n}\n\nstatic int vmac_set_key(unsigned char user_key[], struct vmac_ctx_t *ctx)\n{\n\tu64 in[2] = {0}, out[2];\n\tunsigned i;\n\tint err = 0;\n\n\terr = crypto_cipher_setkey(ctx->child, user_key, VMAC_KEY_LEN);\n\tif (err)\n\t\treturn err;\n\n\t/* Fill nh key */\n\t((unsigned char *)in)[0] = 0x80;\n\tfor (i = 0; i < sizeof(ctx->__vmac_ctx.nhkey)/8; i += 2) {\n\t\tcrypto_cipher_encrypt_one(ctx->child,\n\t\t\t(unsigned char *)out, (unsigned char *)in);\n\t\tctx->__vmac_ctx.nhkey[i] = be64_to_cpup(out);\n\t\tctx->__vmac_ctx.nhkey[i+1] = be64_to_cpup(out+1);\n\t\t((unsigned char *)in)[15] += 1;\n\t}\n\n\t/* Fill poly key */\n\t((unsigned char *)in)[0] = 0xC0;\n\tin[1] = 0;\n\tfor (i = 0; i < sizeof(ctx->__vmac_ctx.polykey)/8; i += 2) {\n\t\tcrypto_cipher_encrypt_one(ctx->child,\n\t\t\t(unsigned char *)out, (unsigned char *)in);\n\t\tctx->__vmac_ctx.polytmp[i] =\n\t\t\tctx->__vmac_ctx.polykey[i] =\n\t\t\t\tbe64_to_cpup(out) & mpoly;\n\t\tctx->__vmac_ctx.polytmp[i+1] =\n\t\t\tctx->__vmac_ctx.polykey[i+1] =\n\t\t\t\tbe64_to_cpup(out+1) & mpoly;\n\t\t((unsigned char *)in)[15] += 1;\n\t}\n\n\t/* Fill ip key */\n\t((unsigned char *)in)[0] = 0xE0;\n\tin[1] = 0;\n\tfor (i = 0; i < sizeof(ctx->__vmac_ctx.l3key)/8; i += 2) {\n\t\tdo {\n\t\t\tcrypto_cipher_encrypt_one(ctx->child,\n\t\t\t\t(unsigned char *)out, (unsigned char *)in);\n\t\t\tctx->__vmac_ctx.l3key[i] = be64_to_cpup(out);\n\t\t\tctx->__vmac_ctx.l3key[i+1] = be64_to_cpup(out+1);\n\t\t\t((unsigned char *)in)[15] += 1;\n\t\t} while (ctx->__vmac_ctx.l3key[i] >= p64\n\t\t\t|| ctx->__vmac_ctx.l3key[i+1] >= p64);\n\t}\n\n\t/* Invalidate nonce/aes cache and reset other elements */\n\tctx->__vmac_ctx.cached_nonce[0] = (u64)-1; /* Ensure illegal nonce */\n\tctx->__vmac_ctx.cached_nonce[1] = (u64)0;  /* Ensure illegal nonce */\n\tctx->__vmac_ctx.first_block_processed = 0;\n\n\treturn err;\n}\n\nstatic int vmac_setkey(struct crypto_shash *parent,\n\t\tconst u8 *key, unsigned int keylen)\n{\n\tstruct vmac_ctx_t *ctx = crypto_shash_ctx(parent);\n\n\tif (keylen != VMAC_KEY_LEN) {\n\t\tcrypto_shash_set_flags(parent, CRYPTO_TFM_RES_BAD_KEY_LEN);\n\t\treturn -EINVAL;\n\t}\n\n\treturn vmac_set_key((u8 *)key, ctx);\n}\n\nstatic int vmac_init(struct shash_desc *pdesc)\n{\n\treturn 0;\n}\n\nstatic int vmac_update(struct shash_desc *pdesc, const u8 *p,\n\t\tunsigned int len)\n{\n\tstruct crypto_shash *parent = pdesc->tfm;\n\tstruct vmac_ctx_t *ctx = crypto_shash_ctx(parent);\n\tint expand;\n\tint min;\n\n\texpand = VMAC_NHBYTES - ctx->partial_size > 0 ?\n\t\t\tVMAC_NHBYTES - ctx->partial_size : 0;\n\n\tmin = len < expand ? len : expand;\n\n\tmemcpy(ctx->partial + ctx->partial_size, p, min);\n\tctx->partial_size += min;\n\n\tif (len < expand)\n\t\treturn 0;\n\n\tvhash_update(ctx->partial, VMAC_NHBYTES, &ctx->__vmac_ctx);\n\tctx->partial_size = 0;\n\n\tlen -= expand;\n\tp += expand;\n\n\tif (len % VMAC_NHBYTES) {\n\t\tmemcpy(ctx->partial, p + len - (len % VMAC_NHBYTES),\n\t\t\tlen % VMAC_NHBYTES);\n\t\tctx->partial_size = len % VMAC_NHBYTES;\n\t}\n\n\tvhash_update(p, len - len % VMAC_NHBYTES, &ctx->__vmac_ctx);\n\n\treturn 0;\n}\n\nstatic int vmac_final(struct shash_desc *pdesc, u8 *out)\n{\n\tstruct crypto_shash *parent = pdesc->tfm;\n\tstruct vmac_ctx_t *ctx = crypto_shash_ctx(parent);\n\tvmac_t mac;\n\tu8 nonce[16] = {};\n\n\t/* vmac() ends up accessing outside the array bounds that\n\t * we specify.  In appears to access up to the next 2-word\n\t * boundary.  We'll just be uber cautious and zero the\n\t * unwritten bytes in the buffer.\n\t */\n\tif (ctx->partial_size) {\n\t\tmemset(ctx->partial + ctx->partial_size, 0,\n\t\t\tVMAC_NHBYTES - ctx->partial_size);\n\t}\n\tmac = vmac(ctx->partial, ctx->partial_size, nonce, NULL, ctx);\n\tmemcpy(out, &mac, sizeof(vmac_t));\n\tmemzero_explicit(&mac, sizeof(vmac_t));\n\tmemset(&ctx->__vmac_ctx, 0, sizeof(struct vmac_ctx));\n\tctx->partial_size = 0;\n\treturn 0;\n}\n\nstatic int vmac_init_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_cipher *cipher;\n\tstruct crypto_instance *inst = (void *)tfm->__crt_alg;\n\tstruct crypto_spawn *spawn = crypto_instance_ctx(inst);\n\tstruct vmac_ctx_t *ctx = crypto_tfm_ctx(tfm);\n\n\tcipher = crypto_spawn_cipher(spawn);\n\tif (IS_ERR(cipher))\n\t\treturn PTR_ERR(cipher);\n\n\tctx->child = cipher;\n\treturn 0;\n}\n\nstatic void vmac_exit_tfm(struct crypto_tfm *tfm)\n{\n\tstruct vmac_ctx_t *ctx = crypto_tfm_ctx(tfm);\n\tcrypto_free_cipher(ctx->child);\n}\n\nstatic int vmac_create(struct crypto_template *tmpl, struct rtattr **tb)\n{\n\tstruct shash_instance *inst;\n\tstruct crypto_alg *alg;\n\tint err;\n\n\terr = crypto_check_attr_type(tb, CRYPTO_ALG_TYPE_SHASH);\n\tif (err)\n\t\treturn err;\n\n\talg = crypto_get_attr_alg(tb, CRYPTO_ALG_TYPE_CIPHER,\n\t\t\tCRYPTO_ALG_TYPE_MASK);\n\tif (IS_ERR(alg))\n\t\treturn PTR_ERR(alg);\n\n\tinst = shash_alloc_instance(\"vmac\", alg);\n\terr = PTR_ERR(inst);\n\tif (IS_ERR(inst))\n\t\tgoto out_put_alg;\n\n\terr = crypto_init_spawn(shash_instance_ctx(inst), alg,\n\t\t\tshash_crypto_instance(inst),\n\t\t\tCRYPTO_ALG_TYPE_MASK);\n\tif (err)\n\t\tgoto out_free_inst;\n\n\tinst->alg.base.cra_priority = alg->cra_priority;\n\tinst->alg.base.cra_blocksize = alg->cra_blocksize;\n\tinst->alg.base.cra_alignmask = alg->cra_alignmask;\n\n\tinst->alg.digestsize = sizeof(vmac_t);\n\tinst->alg.base.cra_ctxsize = sizeof(struct vmac_ctx_t);\n\tinst->alg.base.cra_init = vmac_init_tfm;\n\tinst->alg.base.cra_exit = vmac_exit_tfm;\n\n\tinst->alg.init = vmac_init;\n\tinst->alg.update = vmac_update;\n\tinst->alg.final = vmac_final;\n\tinst->alg.setkey = vmac_setkey;\n\n\terr = shash_register_instance(tmpl, inst);\n\tif (err) {\nout_free_inst:\n\t\tshash_free_instance(shash_crypto_instance(inst));\n\t}\n\nout_put_alg:\n\tcrypto_mod_put(alg);\n\treturn err;\n}\n\nstatic struct crypto_template vmac_tmpl = {\n\t.name = \"vmac\",\n\t.create = vmac_create,\n\t.free = shash_free_instance,\n\t.module = THIS_MODULE,\n};\n\nstatic int __init vmac_module_init(void)\n{\n\treturn crypto_register_template(&vmac_tmpl);\n}\n\nstatic void __exit vmac_module_exit(void)\n{\n\tcrypto_unregister_template(&vmac_tmpl);\n}\n\nmodule_init(vmac_module_init);\nmodule_exit(vmac_module_exit);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"VMAC hash algorithm\");\nMODULE_ALIAS_CRYPTO(\"vmac\");\n", "/*\n * Copyright (C)2006 USAGI/WIDE Project\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License as published by\n * the Free Software Foundation; either version 2 of the License, or\n * (at your option) any later version.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n * GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public License\n * along with this program; if not, write to the Free Software\n * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA\n *\n * Author:\n * \tKazunori Miyazawa <miyazawa@linux-ipv6.org>\n */\n\n#include <crypto/internal/hash.h>\n#include <linux/err.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n\nstatic u_int32_t ks[12] = {0x01010101, 0x01010101, 0x01010101, 0x01010101,\n\t\t\t   0x02020202, 0x02020202, 0x02020202, 0x02020202,\n\t\t\t   0x03030303, 0x03030303, 0x03030303, 0x03030303};\n\n/*\n * +------------------------\n * | <parent tfm>\n * +------------------------\n * | xcbc_tfm_ctx\n * +------------------------\n * | consts (block size * 2)\n * +------------------------\n */\nstruct xcbc_tfm_ctx {\n\tstruct crypto_cipher *child;\n\tu8 ctx[];\n};\n\n/*\n * +------------------------\n * | <shash desc>\n * +------------------------\n * | xcbc_desc_ctx\n * +------------------------\n * | odds (block size)\n * +------------------------\n * | prev (block size)\n * +------------------------\n */\nstruct xcbc_desc_ctx {\n\tunsigned int len;\n\tu8 ctx[];\n};\n\nstatic int crypto_xcbc_digest_setkey(struct crypto_shash *parent,\n\t\t\t\t     const u8 *inkey, unsigned int keylen)\n{\n\tunsigned long alignmask = crypto_shash_alignmask(parent);\n\tstruct xcbc_tfm_ctx *ctx = crypto_shash_ctx(parent);\n\tint bs = crypto_shash_blocksize(parent);\n\tu8 *consts = PTR_ALIGN(&ctx->ctx[0], alignmask + 1);\n\tint err = 0;\n\tu8 key1[bs];\n\n\tif ((err = crypto_cipher_setkey(ctx->child, inkey, keylen)))\n\t\treturn err;\n\n\tcrypto_cipher_encrypt_one(ctx->child, consts, (u8 *)ks + bs);\n\tcrypto_cipher_encrypt_one(ctx->child, consts + bs, (u8 *)ks + bs * 2);\n\tcrypto_cipher_encrypt_one(ctx->child, key1, (u8 *)ks);\n\n\treturn crypto_cipher_setkey(ctx->child, key1, bs);\n\n}\n\nstatic int crypto_xcbc_digest_init(struct shash_desc *pdesc)\n{\n\tunsigned long alignmask = crypto_shash_alignmask(pdesc->tfm);\n\tstruct xcbc_desc_ctx *ctx = shash_desc_ctx(pdesc);\n\tint bs = crypto_shash_blocksize(pdesc->tfm);\n\tu8 *prev = PTR_ALIGN(&ctx->ctx[0], alignmask + 1) + bs;\n\n\tctx->len = 0;\n\tmemset(prev, 0, bs);\n\n\treturn 0;\n}\n\nstatic int crypto_xcbc_digest_update(struct shash_desc *pdesc, const u8 *p,\n\t\t\t\t     unsigned int len)\n{\n\tstruct crypto_shash *parent = pdesc->tfm;\n\tunsigned long alignmask = crypto_shash_alignmask(parent);\n\tstruct xcbc_tfm_ctx *tctx = crypto_shash_ctx(parent);\n\tstruct xcbc_desc_ctx *ctx = shash_desc_ctx(pdesc);\n\tstruct crypto_cipher *tfm = tctx->child;\n\tint bs = crypto_shash_blocksize(parent);\n\tu8 *odds = PTR_ALIGN(&ctx->ctx[0], alignmask + 1);\n\tu8 *prev = odds + bs;\n\n\t/* checking the data can fill the block */\n\tif ((ctx->len + len) <= bs) {\n\t\tmemcpy(odds + ctx->len, p, len);\n\t\tctx->len += len;\n\t\treturn 0;\n\t}\n\n\t/* filling odds with new data and encrypting it */\n\tmemcpy(odds + ctx->len, p, bs - ctx->len);\n\tlen -= bs - ctx->len;\n\tp += bs - ctx->len;\n\n\tcrypto_xor(prev, odds, bs);\n\tcrypto_cipher_encrypt_one(tfm, prev, prev);\n\n\t/* clearing the length */\n\tctx->len = 0;\n\n\t/* encrypting the rest of data */\n\twhile (len > bs) {\n\t\tcrypto_xor(prev, p, bs);\n\t\tcrypto_cipher_encrypt_one(tfm, prev, prev);\n\t\tp += bs;\n\t\tlen -= bs;\n\t}\n\n\t/* keeping the surplus of blocksize */\n\tif (len) {\n\t\tmemcpy(odds, p, len);\n\t\tctx->len = len;\n\t}\n\n\treturn 0;\n}\n\nstatic int crypto_xcbc_digest_final(struct shash_desc *pdesc, u8 *out)\n{\n\tstruct crypto_shash *parent = pdesc->tfm;\n\tunsigned long alignmask = crypto_shash_alignmask(parent);\n\tstruct xcbc_tfm_ctx *tctx = crypto_shash_ctx(parent);\n\tstruct xcbc_desc_ctx *ctx = shash_desc_ctx(pdesc);\n\tstruct crypto_cipher *tfm = tctx->child;\n\tint bs = crypto_shash_blocksize(parent);\n\tu8 *consts = PTR_ALIGN(&tctx->ctx[0], alignmask + 1);\n\tu8 *odds = PTR_ALIGN(&ctx->ctx[0], alignmask + 1);\n\tu8 *prev = odds + bs;\n\tunsigned int offset = 0;\n\n\tif (ctx->len != bs) {\n\t\tunsigned int rlen;\n\t\tu8 *p = odds + ctx->len;\n\n\t\t*p = 0x80;\n\t\tp++;\n\n\t\trlen = bs - ctx->len -1;\n\t\tif (rlen)\n\t\t\tmemset(p, 0, rlen);\n\n\t\toffset += bs;\n\t}\n\n\tcrypto_xor(prev, odds, bs);\n\tcrypto_xor(prev, consts + offset, bs);\n\n\tcrypto_cipher_encrypt_one(tfm, out, prev);\n\n\treturn 0;\n}\n\nstatic int xcbc_init_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_cipher *cipher;\n\tstruct crypto_instance *inst = (void *)tfm->__crt_alg;\n\tstruct crypto_spawn *spawn = crypto_instance_ctx(inst);\n\tstruct xcbc_tfm_ctx *ctx = crypto_tfm_ctx(tfm);\n\n\tcipher = crypto_spawn_cipher(spawn);\n\tif (IS_ERR(cipher))\n\t\treturn PTR_ERR(cipher);\n\n\tctx->child = cipher;\n\n\treturn 0;\n};\n\nstatic void xcbc_exit_tfm(struct crypto_tfm *tfm)\n{\n\tstruct xcbc_tfm_ctx *ctx = crypto_tfm_ctx(tfm);\n\tcrypto_free_cipher(ctx->child);\n}\n\nstatic int xcbc_create(struct crypto_template *tmpl, struct rtattr **tb)\n{\n\tstruct shash_instance *inst;\n\tstruct crypto_alg *alg;\n\tunsigned long alignmask;\n\tint err;\n\n\terr = crypto_check_attr_type(tb, CRYPTO_ALG_TYPE_SHASH);\n\tif (err)\n\t\treturn err;\n\n\talg = crypto_get_attr_alg(tb, CRYPTO_ALG_TYPE_CIPHER,\n\t\t\t\t  CRYPTO_ALG_TYPE_MASK);\n\tif (IS_ERR(alg))\n\t\treturn PTR_ERR(alg);\n\n\tswitch(alg->cra_blocksize) {\n\tcase 16:\n\t\tbreak;\n\tdefault:\n\t\tgoto out_put_alg;\n\t}\n\n\tinst = shash_alloc_instance(\"xcbc\", alg);\n\terr = PTR_ERR(inst);\n\tif (IS_ERR(inst))\n\t\tgoto out_put_alg;\n\n\terr = crypto_init_spawn(shash_instance_ctx(inst), alg,\n\t\t\t\tshash_crypto_instance(inst),\n\t\t\t\tCRYPTO_ALG_TYPE_MASK);\n\tif (err)\n\t\tgoto out_free_inst;\n\n\talignmask = alg->cra_alignmask | 3;\n\tinst->alg.base.cra_alignmask = alignmask;\n\tinst->alg.base.cra_priority = alg->cra_priority;\n\tinst->alg.base.cra_blocksize = alg->cra_blocksize;\n\n\tinst->alg.digestsize = alg->cra_blocksize;\n\tinst->alg.descsize = ALIGN(sizeof(struct xcbc_desc_ctx),\n\t\t\t\t   crypto_tfm_ctx_alignment()) +\n\t\t\t     (alignmask &\n\t\t\t      ~(crypto_tfm_ctx_alignment() - 1)) +\n\t\t\t     alg->cra_blocksize * 2;\n\n\tinst->alg.base.cra_ctxsize = ALIGN(sizeof(struct xcbc_tfm_ctx),\n\t\t\t\t\t   alignmask + 1) +\n\t\t\t\t     alg->cra_blocksize * 2;\n\tinst->alg.base.cra_init = xcbc_init_tfm;\n\tinst->alg.base.cra_exit = xcbc_exit_tfm;\n\n\tinst->alg.init = crypto_xcbc_digest_init;\n\tinst->alg.update = crypto_xcbc_digest_update;\n\tinst->alg.final = crypto_xcbc_digest_final;\n\tinst->alg.setkey = crypto_xcbc_digest_setkey;\n\n\terr = shash_register_instance(tmpl, inst);\n\tif (err) {\nout_free_inst:\n\t\tshash_free_instance(shash_crypto_instance(inst));\n\t}\n\nout_put_alg:\n\tcrypto_mod_put(alg);\n\treturn err;\n}\n\nstatic struct crypto_template crypto_xcbc_tmpl = {\n\t.name = \"xcbc\",\n\t.create = xcbc_create,\n\t.free = shash_free_instance,\n\t.module = THIS_MODULE,\n};\n\nstatic int __init crypto_xcbc_module_init(void)\n{\n\treturn crypto_register_template(&crypto_xcbc_tmpl);\n}\n\nstatic void __exit crypto_xcbc_module_exit(void)\n{\n\tcrypto_unregister_template(&crypto_xcbc_tmpl);\n}\n\nmodule_init(crypto_xcbc_module_init);\nmodule_exit(crypto_xcbc_module_exit);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"XCBC keyed hash algorithm\");\nMODULE_ALIAS_CRYPTO(\"xcbc\");\n", "/* XTS: as defined in IEEE1619/D16\n *\thttp://grouper.ieee.org/groups/1619/email/pdf00086.pdf\n *\t(sector sizes which are not a multiple of 16 bytes are,\n *\thowever currently unsupported)\n *\n * Copyright (c) 2007 Rik Snel <rsnel@cube.dyndns.org>\n *\n * Based om ecb.c\n * Copyright (c) 2006 Herbert Xu <herbert@gondor.apana.org.au>\n *\n * This program is free software; you can redistribute it and/or modify it\n * under the terms of the GNU General Public License as published by the Free\n * Software Foundation; either version 2 of the License, or (at your option)\n * any later version.\n */\n#include <crypto/algapi.h>\n#include <linux/err.h>\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/scatterlist.h>\n#include <linux/slab.h>\n\n#include <crypto/xts.h>\n#include <crypto/b128ops.h>\n#include <crypto/gf128mul.h>\n\nstruct priv {\n\tstruct crypto_cipher *child;\n\tstruct crypto_cipher *tweak;\n};\n\nstatic int setkey(struct crypto_tfm *parent, const u8 *key,\n\t\t  unsigned int keylen)\n{\n\tstruct priv *ctx = crypto_tfm_ctx(parent);\n\tstruct crypto_cipher *child = ctx->tweak;\n\tu32 *flags = &parent->crt_flags;\n\tint err;\n\n\t/* key consists of keys of equal size concatenated, therefore\n\t * the length must be even */\n\tif (keylen % 2) {\n\t\t/* tell the user why there was an error */\n\t\t*flags |= CRYPTO_TFM_RES_BAD_KEY_LEN;\n\t\treturn -EINVAL;\n\t}\n\n\t/* we need two cipher instances: one to compute the initial 'tweak'\n\t * by encrypting the IV (usually the 'plain' iv) and the other\n\t * one to encrypt and decrypt the data */\n\n\t/* tweak cipher, uses Key2 i.e. the second half of *key */\n\tcrypto_cipher_clear_flags(child, CRYPTO_TFM_REQ_MASK);\n\tcrypto_cipher_set_flags(child, crypto_tfm_get_flags(parent) &\n\t\t\t\t       CRYPTO_TFM_REQ_MASK);\n\terr = crypto_cipher_setkey(child, key + keylen/2, keylen/2);\n\tif (err)\n\t\treturn err;\n\n\tcrypto_tfm_set_flags(parent, crypto_cipher_get_flags(child) &\n\t\t\t\t     CRYPTO_TFM_RES_MASK);\n\n\tchild = ctx->child;\n\n\t/* data cipher, uses Key1 i.e. the first half of *key */\n\tcrypto_cipher_clear_flags(child, CRYPTO_TFM_REQ_MASK);\n\tcrypto_cipher_set_flags(child, crypto_tfm_get_flags(parent) &\n\t\t\t\t       CRYPTO_TFM_REQ_MASK);\n\terr = crypto_cipher_setkey(child, key, keylen/2);\n\tif (err)\n\t\treturn err;\n\n\tcrypto_tfm_set_flags(parent, crypto_cipher_get_flags(child) &\n\t\t\t\t     CRYPTO_TFM_RES_MASK);\n\n\treturn 0;\n}\n\nstruct sinfo {\n\tbe128 *t;\n\tstruct crypto_tfm *tfm;\n\tvoid (*fn)(struct crypto_tfm *, u8 *, const u8 *);\n};\n\nstatic inline void xts_round(struct sinfo *s, void *dst, const void *src)\n{\n\tbe128_xor(dst, s->t, src);\t\t/* PP <- T xor P */\n\ts->fn(s->tfm, dst, dst);\t\t/* CC <- E(Key1,PP) */\n\tbe128_xor(dst, dst, s->t);\t\t/* C <- T xor CC */\n}\n\nstatic int crypt(struct blkcipher_desc *d,\n\t\t struct blkcipher_walk *w, struct priv *ctx,\n\t\t void (*tw)(struct crypto_tfm *, u8 *, const u8 *),\n\t\t void (*fn)(struct crypto_tfm *, u8 *, const u8 *))\n{\n\tint err;\n\tunsigned int avail;\n\tconst int bs = XTS_BLOCK_SIZE;\n\tstruct sinfo s = {\n\t\t.tfm = crypto_cipher_tfm(ctx->child),\n\t\t.fn = fn\n\t};\n\tu8 *wsrc;\n\tu8 *wdst;\n\n\terr = blkcipher_walk_virt(d, w);\n\tif (!w->nbytes)\n\t\treturn err;\n\n\ts.t = (be128 *)w->iv;\n\tavail = w->nbytes;\n\n\twsrc = w->src.virt.addr;\n\twdst = w->dst.virt.addr;\n\n\t/* calculate first value of T */\n\ttw(crypto_cipher_tfm(ctx->tweak), w->iv, w->iv);\n\n\tgoto first;\n\n\tfor (;;) {\n\t\tdo {\n\t\t\tgf128mul_x_ble(s.t, s.t);\n\nfirst:\n\t\t\txts_round(&s, wdst, wsrc);\n\n\t\t\twsrc += bs;\n\t\t\twdst += bs;\n\t\t} while ((avail -= bs) >= bs);\n\n\t\terr = blkcipher_walk_done(d, w, avail);\n\t\tif (!w->nbytes)\n\t\t\tbreak;\n\n\t\tavail = w->nbytes;\n\n\t\twsrc = w->src.virt.addr;\n\t\twdst = w->dst.virt.addr;\n\t}\n\n\treturn err;\n}\n\nstatic int encrypt(struct blkcipher_desc *desc, struct scatterlist *dst,\n\t\t   struct scatterlist *src, unsigned int nbytes)\n{\n\tstruct priv *ctx = crypto_blkcipher_ctx(desc->tfm);\n\tstruct blkcipher_walk w;\n\n\tblkcipher_walk_init(&w, dst, src, nbytes);\n\treturn crypt(desc, &w, ctx, crypto_cipher_alg(ctx->tweak)->cia_encrypt,\n\t\t     crypto_cipher_alg(ctx->child)->cia_encrypt);\n}\n\nstatic int decrypt(struct blkcipher_desc *desc, struct scatterlist *dst,\n\t\t   struct scatterlist *src, unsigned int nbytes)\n{\n\tstruct priv *ctx = crypto_blkcipher_ctx(desc->tfm);\n\tstruct blkcipher_walk w;\n\n\tblkcipher_walk_init(&w, dst, src, nbytes);\n\treturn crypt(desc, &w, ctx, crypto_cipher_alg(ctx->tweak)->cia_encrypt,\n\t\t     crypto_cipher_alg(ctx->child)->cia_decrypt);\n}\n\nint xts_crypt(struct blkcipher_desc *desc, struct scatterlist *sdst,\n\t      struct scatterlist *ssrc, unsigned int nbytes,\n\t      struct xts_crypt_req *req)\n{\n\tconst unsigned int bsize = XTS_BLOCK_SIZE;\n\tconst unsigned int max_blks = req->tbuflen / bsize;\n\tstruct blkcipher_walk walk;\n\tunsigned int nblocks;\n\tbe128 *src, *dst, *t;\n\tbe128 *t_buf = req->tbuf;\n\tint err, i;\n\n\tBUG_ON(max_blks < 1);\n\n\tblkcipher_walk_init(&walk, sdst, ssrc, nbytes);\n\n\terr = blkcipher_walk_virt(desc, &walk);\n\tnbytes = walk.nbytes;\n\tif (!nbytes)\n\t\treturn err;\n\n\tnblocks = min(nbytes / bsize, max_blks);\n\tsrc = (be128 *)walk.src.virt.addr;\n\tdst = (be128 *)walk.dst.virt.addr;\n\n\t/* calculate first value of T */\n\treq->tweak_fn(req->tweak_ctx, (u8 *)&t_buf[0], walk.iv);\n\n\ti = 0;\n\tgoto first;\n\n\tfor (;;) {\n\t\tdo {\n\t\t\tfor (i = 0; i < nblocks; i++) {\n\t\t\t\tgf128mul_x_ble(&t_buf[i], t);\nfirst:\n\t\t\t\tt = &t_buf[i];\n\n\t\t\t\t/* PP <- T xor P */\n\t\t\t\tbe128_xor(dst + i, t, src + i);\n\t\t\t}\n\n\t\t\t/* CC <- E(Key2,PP) */\n\t\t\treq->crypt_fn(req->crypt_ctx, (u8 *)dst,\n\t\t\t\t      nblocks * bsize);\n\n\t\t\t/* C <- T xor CC */\n\t\t\tfor (i = 0; i < nblocks; i++)\n\t\t\t\tbe128_xor(dst + i, dst + i, &t_buf[i]);\n\n\t\t\tsrc += nblocks;\n\t\t\tdst += nblocks;\n\t\t\tnbytes -= nblocks * bsize;\n\t\t\tnblocks = min(nbytes / bsize, max_blks);\n\t\t} while (nblocks > 0);\n\n\t\t*(be128 *)walk.iv = *t;\n\n\t\terr = blkcipher_walk_done(desc, &walk, nbytes);\n\t\tnbytes = walk.nbytes;\n\t\tif (!nbytes)\n\t\t\tbreak;\n\n\t\tnblocks = min(nbytes / bsize, max_blks);\n\t\tsrc = (be128 *)walk.src.virt.addr;\n\t\tdst = (be128 *)walk.dst.virt.addr;\n\t}\n\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(xts_crypt);\n\nstatic int init_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_cipher *cipher;\n\tstruct crypto_instance *inst = (void *)tfm->__crt_alg;\n\tstruct crypto_spawn *spawn = crypto_instance_ctx(inst);\n\tstruct priv *ctx = crypto_tfm_ctx(tfm);\n\tu32 *flags = &tfm->crt_flags;\n\n\tcipher = crypto_spawn_cipher(spawn);\n\tif (IS_ERR(cipher))\n\t\treturn PTR_ERR(cipher);\n\n\tif (crypto_cipher_blocksize(cipher) != XTS_BLOCK_SIZE) {\n\t\t*flags |= CRYPTO_TFM_RES_BAD_BLOCK_LEN;\n\t\tcrypto_free_cipher(cipher);\n\t\treturn -EINVAL;\n\t}\n\n\tctx->child = cipher;\n\n\tcipher = crypto_spawn_cipher(spawn);\n\tif (IS_ERR(cipher)) {\n\t\tcrypto_free_cipher(ctx->child);\n\t\treturn PTR_ERR(cipher);\n\t}\n\n\t/* this check isn't really needed, leave it here just in case */\n\tif (crypto_cipher_blocksize(cipher) != XTS_BLOCK_SIZE) {\n\t\tcrypto_free_cipher(cipher);\n\t\tcrypto_free_cipher(ctx->child);\n\t\t*flags |= CRYPTO_TFM_RES_BAD_BLOCK_LEN;\n\t\treturn -EINVAL;\n\t}\n\n\tctx->tweak = cipher;\n\n\treturn 0;\n}\n\nstatic void exit_tfm(struct crypto_tfm *tfm)\n{\n\tstruct priv *ctx = crypto_tfm_ctx(tfm);\n\tcrypto_free_cipher(ctx->child);\n\tcrypto_free_cipher(ctx->tweak);\n}\n\nstatic struct crypto_instance *alloc(struct rtattr **tb)\n{\n\tstruct crypto_instance *inst;\n\tstruct crypto_alg *alg;\n\tint err;\n\n\terr = crypto_check_attr_type(tb, CRYPTO_ALG_TYPE_BLKCIPHER);\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\talg = crypto_get_attr_alg(tb, CRYPTO_ALG_TYPE_CIPHER,\n\t\t\t\t  CRYPTO_ALG_TYPE_MASK);\n\tif (IS_ERR(alg))\n\t\treturn ERR_CAST(alg);\n\n\tinst = crypto_alloc_instance(\"xts\", alg);\n\tif (IS_ERR(inst))\n\t\tgoto out_put_alg;\n\n\tinst->alg.cra_flags = CRYPTO_ALG_TYPE_BLKCIPHER;\n\tinst->alg.cra_priority = alg->cra_priority;\n\tinst->alg.cra_blocksize = alg->cra_blocksize;\n\n\tif (alg->cra_alignmask < 7)\n\t\tinst->alg.cra_alignmask = 7;\n\telse\n\t\tinst->alg.cra_alignmask = alg->cra_alignmask;\n\n\tinst->alg.cra_type = &crypto_blkcipher_type;\n\n\tinst->alg.cra_blkcipher.ivsize = alg->cra_blocksize;\n\tinst->alg.cra_blkcipher.min_keysize =\n\t\t2 * alg->cra_cipher.cia_min_keysize;\n\tinst->alg.cra_blkcipher.max_keysize =\n\t\t2 * alg->cra_cipher.cia_max_keysize;\n\n\tinst->alg.cra_ctxsize = sizeof(struct priv);\n\n\tinst->alg.cra_init = init_tfm;\n\tinst->alg.cra_exit = exit_tfm;\n\n\tinst->alg.cra_blkcipher.setkey = setkey;\n\tinst->alg.cra_blkcipher.encrypt = encrypt;\n\tinst->alg.cra_blkcipher.decrypt = decrypt;\n\nout_put_alg:\n\tcrypto_mod_put(alg);\n\treturn inst;\n}\n\nstatic void free(struct crypto_instance *inst)\n{\n\tcrypto_drop_spawn(crypto_instance_ctx(inst));\n\tkfree(inst);\n}\n\nstatic struct crypto_template crypto_tmpl = {\n\t.name = \"xts\",\n\t.alloc = alloc,\n\t.free = free,\n\t.module = THIS_MODULE,\n};\n\nstatic int __init crypto_module_init(void)\n{\n\treturn crypto_register_template(&crypto_tmpl);\n}\n\nstatic void __exit crypto_module_exit(void)\n{\n\tcrypto_unregister_template(&crypto_tmpl);\n}\n\nmodule_init(crypto_module_init);\nmodule_exit(crypto_module_exit);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"XTS block cipher mode\");\nMODULE_ALIAS_CRYPTO(\"xts\");\n"], "buggy_code_start_loc": [19, 512, 723, 816, 291, 883, 361, 315, 957, 469, 353, 187, 269, 1446, 270, 402, 705, 297, 567, 364, 715, 288, 364], "buggy_code_end_loc": [161, 514, 723, 816, 291, 883, 361, 315, 957, 469, 353, 187, 269, 1446, 270, 402, 705, 297, 567, 364, 715, 288, 364], "fixing_code_start_loc": [20, 512, 724, 817, 292, 884, 362, 316, 958, 470, 354, 188, 270, 1447, 271, 403, 706, 298, 568, 365, 716, 289, 365], "fixing_code_end_loc": [165, 514, 725, 818, 293, 885, 363, 317, 959, 471, 355, 189, 271, 1448, 272, 404, 707, 299, 569, 366, 717, 290, 366], "type": "CWE-269", "message": "The Crypto API in the Linux kernel before 3.18.5 allows local users to load arbitrary kernel modules via a bind system call for an AF_ALG socket with a parenthesized module template expression in the salg_name field, as demonstrated by the vfat(aes) expression, a different vulnerability than CVE-2013-7421.", "other": {"cve": {"id": "CVE-2014-9644", "sourceIdentifier": "cve@mitre.org", "published": "2015-03-02T11:59:03.660", "lastModified": "2020-05-21T13:11:24.680", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "The Crypto API in the Linux kernel before 3.18.5 allows local users to load arbitrary kernel modules via a bind system call for an AF_ALG socket with a parenthesized module template expression in the salg_name field, as demonstrated by the vfat(aes) expression, a different vulnerability than CVE-2013-7421."}, {"lang": "es", "value": "La API Crypto en el kernel de Linux anterior a 3.18.5 permite a usuarios locales cargar m\u00f3dulos del kernel arbitrarios a trav\u00e9s de una llamada al sistema de enlaces para un socket AF_ALG con una expresi\u00f3n de plantilla de m\u00f3dulos entre par\u00e9ntesis en el campo salg_name, tal y como fue demostrado por la expresi\u00f3n vfat(aes), una vulnerabilidad diferente a CVE-2013-7421."}], "metrics": {"cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:N/I:P/A:N", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "PARTIAL", "availabilityImpact": "NONE", "baseScore": 2.1}, "baseSeverity": "LOW", "exploitabilityScore": 3.9, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-269"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "3.18.5", "matchCriteriaId": "C8C3FC9D-A833-4B7B-865B-54510F1C7EBF"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:7.0:*:*:*:*:*:*:*", "matchCriteriaId": "16F59A04-14CF-49E2-9973-645477EA09DA"}, {"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:8.0:*:*:*:*:*:*:*", "matchCriteriaId": "C11E6FB0-C8C0-4527-9AA0-CB9B316F8F43"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:12.04:*:*:*:lts:*:*:*", "matchCriteriaId": "B6B7CAD7-9D4E-4FDB-88E3-1E583210A01F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:14.04:*:*:*:lts:*:*:*", "matchCriteriaId": "B5A6F2F3-4894-4392-8296-3B8DD2679084"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:14.10:*:*:*:*:*:*:*", "matchCriteriaId": "49A63F39-30BE-443F-AF10-6245587D3359"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:oracle:linux:5:-:*:*:*:*:*:*", "matchCriteriaId": "62A2AC02-A933-4E51-810E-5D040B476B7B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:oracle:linux:6:-:*:*:*:*:*:*", "matchCriteriaId": "D7B037A8-72A6-4DFF-94B2-D688A5F6F876"}, {"vulnerable": true, "criteria": "cpe:2.3:o:oracle:linux:7:-:*:*:*:*:*:*", "matchCriteriaId": "44B8FEDF-6CB0-46E9-9AD7-4445B001C158"}]}]}], "references": [{"url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux-2.6.git;a=commit;h=4943ba16bbc2db05115707b3ff7b4874e9e3c560", "source": "cve@mitre.org", "tags": ["Patch", "Vendor Advisory"]}, {"url": "http://rhn.redhat.com/errata/RHSA-2016-0068.html", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "http://www.debian.org/security/2015/dsa-3170", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "http://www.kernel.org/pub/linux/kernel/v3.x/ChangeLog-3.18.5", "source": "cve@mitre.org", "tags": ["Release Notes", "Vendor Advisory"]}, {"url": "http://www.mandriva.com/security/advisories?name=MDVSA-2015:057", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "http://www.mandriva.com/security/advisories?name=MDVSA-2015:058", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "http://www.openwall.com/lists/oss-security/2015/01/24/4", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://www.oracle.com/technetwork/topics/security/linuxbulletinjan2016-2867209.html", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "http://www.oracle.com/technetwork/topics/security/linuxbulletinoct2015-2719645.html", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "http://www.securityfocus.com/bid/72320", "source": "cve@mitre.org", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "http://www.ubuntu.com/usn/USN-2513-1", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "http://www.ubuntu.com/usn/USN-2514-1", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "http://www.ubuntu.com/usn/USN-2543-1", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "http://www.ubuntu.com/usn/USN-2544-1", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "http://www.ubuntu.com/usn/USN-2545-1", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "http://www.ubuntu.com/usn/USN-2546-1", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=1190546", "source": "cve@mitre.org", "tags": ["Issue Tracking", "Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/4943ba16bbc2db05115707b3ff7b4874e9e3c560", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://plus.google.com/+MathiasKrause/posts/PqFCo4bfrWu", "source": "cve@mitre.org", "tags": ["Permissions Required"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/4943ba16bbc2db05115707b3ff7b4874e9e3c560"}}