{"buggy_code": ["/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include \"tensorflow/core/framework/common_shape_fns.h\"\n#include \"tensorflow/core/framework/dataset_stateful_op_allowlist.h\"\n#include \"tensorflow/core/framework/op.h\"\n#include \"tensorflow/core/framework/op_def_builder.h\"\n#include \"tensorflow/core/framework/shape_inference.h\"\n\nnamespace tensorflow {\n\nusing shape_inference::DimensionHandle;\nusing shape_inference::InferenceContext;\nusing shape_inference::ShapeAndType;\nusing shape_inference::ShapeHandle;\n\n// --------------------------------------------------------------------------\n\nnamespace {\nStatus TwoElementVectorInputsAndScalarOutputs(InferenceContext* c) {\n  ShapeHandle handle;\n  DimensionHandle unused_handle;\n  for (int i = 0; i < c->num_inputs(); ++i) {\n    TF_RETURN_IF_ERROR(c->WithRank(c->input(i), 1, &handle));\n    TF_RETURN_IF_ERROR(c->WithValue(c->Dim(handle, 0), 2, &unused_handle));\n  }\n  for (int i = 0; i < c->num_outputs(); ++i) {\n    c->set_output(i, c->Scalar());\n  }\n  return OkStatus();\n}\n\nStatus ScalarAndTwoElementVectorInputsAndScalarOutputs(InferenceContext* c) {\n  ShapeHandle handle;\n  DimensionHandle unused_handle;\n  TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 0, &handle));\n  for (int i = 1; i < c->num_inputs(); ++i) {\n    TF_RETURN_IF_ERROR(c->WithRank(c->input(i), 1, &handle));\n    TF_RETURN_IF_ERROR(c->WithValue(c->Dim(handle, 0), 2, &unused_handle));\n  }\n  for (int i = 0; i < c->num_outputs(); ++i) {\n    c->set_output(i, c->Scalar());\n  }\n  return OkStatus();\n}\n\nStatus TwoElementOutput(InferenceContext* c) {\n  c->set_output(0, c->Vector(2));\n  return OkStatus();\n}\n\nStatus ScalarOutput(InferenceContext* c) {\n  c->set_output(0, c->Scalar());\n  return OkStatus();\n}\n}  // namespace\n\nREGISTER_OP(\"LookupTableFind\")\n    .Input(\"table_handle: Ref(string)\")\n    .Input(\"keys: Tin\")\n    .Input(\"default_value: Tout\")\n    .Output(\"values: Tout\")\n    .Attr(\"Tin: type\")\n    .Attr(\"Tout: type\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle handle;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 1, &handle));\n      DimensionHandle unused_dim;\n      TF_RETURN_IF_ERROR(c->WithValue(c->Dim(handle, 0), 2, &unused_dim));\n\n      // Default value must be scalar or vector.\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(2), 1, &unused));\n      c->set_output(0, c->UnknownShape());\n      return OkStatus();\n    });\n\nStatus ValidateTableType(InferenceContext* c,\n                         const ShapeAndType& key_shape_and_type,\n                         const string& key_dtype_attr,\n                         const ShapeAndType& value_shape_and_type,\n                         const string& value_dtype_attr) {\n  DataType key_dtype;\n  TF_RETURN_IF_ERROR(c->GetAttr(key_dtype_attr, &key_dtype));\n  if (key_shape_and_type.dtype != key_dtype) {\n    return errors::InvalidArgument(\n        \"Trying to read value with wrong dtype. \"\n        \"Expected \",\n        DataTypeString(key_shape_and_type.dtype), \" got \",\n        DataTypeString(key_dtype));\n  }\n  DataType value_dtype;\n  TF_RETURN_IF_ERROR(c->GetAttr(value_dtype_attr, &value_dtype));\n  if (value_shape_and_type.dtype != value_dtype) {\n    return errors::InvalidArgument(\n        \"Trying to read value with wrong dtype. \"\n        \"Expected \",\n        DataTypeString(value_shape_and_type.dtype), \" got \",\n        DataTypeString(value_dtype));\n  }\n  return OkStatus();\n}\n\nStatus ValidateTableResourceHandle(InferenceContext* c, ShapeHandle keys,\n                                   const string& key_dtype_attr,\n                                   const string& value_dtype_attr,\n                                   ShapeAndType* output_shape_and_type) {\n  auto* handle_data = c->input_handle_shapes_and_types(0);\n  if (handle_data == nullptr || handle_data->size() != 2) {\n    output_shape_and_type->shape = c->UnknownShape();\n    output_shape_and_type->dtype = DT_INVALID;\n  } else {\n    const ShapeAndType& key_shape_and_type = (*handle_data)[0];\n    const ShapeAndType& value_shape_and_type = (*handle_data)[1];\n    TF_RETURN_IF_ERROR(ValidateTableType(c, key_shape_and_type, key_dtype_attr,\n                                         value_shape_and_type,\n                                         value_dtype_attr));\n    output_shape_and_type->dtype = value_shape_and_type.dtype;\n    if (c->RankKnown(key_shape_and_type.shape) && c->RankKnown(keys)) {\n      int keys_rank = c->Rank(keys);\n      int key_suffix_rank = c->Rank(key_shape_and_type.shape);\n      if (keys_rank < key_suffix_rank) {\n        return errors::InvalidArgument(\n            \"Expected keys to have suffix \",\n            c->DebugString(key_shape_and_type.shape),\n            \" but saw shape: \", c->DebugString(keys));\n      }\n      for (int d = 0; d < key_suffix_rank; d++) {\n        // Ensure the suffix of keys match what's in the Table.\n        DimensionHandle dim = c->Dim(key_shape_and_type.shape, d);\n        TF_RETURN_IF_ERROR(\n            c->ReplaceDim(keys, keys_rank - key_suffix_rank + d, dim, &keys));\n      }\n      std::vector<DimensionHandle> keys_prefix_vec;\n      keys_prefix_vec.reserve(keys_rank - key_suffix_rank);\n      for (int d = 0; d < keys_rank - key_suffix_rank; ++d) {\n        keys_prefix_vec.push_back(c->Dim(keys, d));\n      }\n      ShapeHandle keys_prefix = c->MakeShape(keys_prefix_vec);\n      TF_RETURN_IF_ERROR(c->Concatenate(keys_prefix, value_shape_and_type.shape,\n                                        &output_shape_and_type->shape));\n    } else {\n      output_shape_and_type->shape = c->UnknownShape();\n    }\n  }\n  return OkStatus();\n}\n\nREGISTER_OP(\"LookupTableFindV2\")\n    .Input(\"table_handle: resource\")\n    .Input(\"keys: Tin\")\n    .Input(\"default_value: Tout\")\n    .Output(\"values: Tout\")\n    .Attr(\"Tin: type\")\n    .Attr(\"Tout: type\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle handle;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 0, &handle));\n\n      ShapeAndType value_shape_and_type;\n      TF_RETURN_IF_ERROR(ValidateTableResourceHandle(\n          c,\n          /*keys=*/c->input(1),\n          /*key_dtype_attr=*/\"Tin\",\n          /*value_dtype_attr=*/\"Tout\", &value_shape_and_type));\n      c->set_output(0, value_shape_and_type.shape);\n\n      return OkStatus();\n    });\nALLOW_STATEFUL_OP_FOR_DATASET_FUNCTIONS(\"LookupTableFindV2\");\n// TODO(b/72710477): Update this.\n\nREGISTER_OP(\"LookupTableInsert\")\n    .Input(\"table_handle: Ref(string)\")\n    .Input(\"keys: Tin\")\n    .Input(\"values: Tout\")\n    .Attr(\"Tin: type\")\n    .Attr(\"Tout: type\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle handle;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 1, &handle));\n      DimensionHandle unused_dim;\n      TF_RETURN_IF_ERROR(c->WithValue(c->Dim(handle, 0), 2, &unused_dim));\n\n      // TODO(ebrevdo): Validate keys and values shape.\n      return OkStatus();\n    });\n\nREGISTER_OP(\"LookupTableInsertV2\")\n    .Input(\"table_handle: resource\")\n    .Input(\"keys: Tin\")\n    .Input(\"values: Tout\")\n    .Attr(\"Tin: type\")\n    .Attr(\"Tout: type\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle handle;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 0, &handle));\n\n      // TODO: Validate keys and values shape.\n      return OkStatus();\n    });\n\nREGISTER_OP(\"LookupTableRemoveV2\")\n    .Input(\"table_handle: resource\")\n    .Input(\"keys: Tin\")\n    .Attr(\"Tin: type\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle handle;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 0, &handle));\n      TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(1), 1, &handle));\n\n      // TODO(turboale): Validate keys shape.\n      return OkStatus();\n    });\n\nREGISTER_OP(\"LookupTableSize\")\n    .Input(\"table_handle: Ref(string)\")\n    .Output(\"size: int64\")\n    .SetShapeFn(TwoElementVectorInputsAndScalarOutputs);\nALLOW_STATEFUL_OP_FOR_DATASET_FUNCTIONS(\"LookupTableSize\");\n\nREGISTER_OP(\"LookupTableSizeV2\")\n    .Input(\"table_handle: resource\")\n    .Output(\"size: int64\")\n    .SetShapeFn(ScalarAndTwoElementVectorInputsAndScalarOutputs);\nALLOW_STATEFUL_OP_FOR_DATASET_FUNCTIONS(\"LookupTableSizeV2\");\n\nREGISTER_OP(\"LookupTableExport\")\n    .Input(\"table_handle: Ref(string)\")\n    .Output(\"keys: Tkeys\")\n    .Output(\"values: Tvalues\")\n    .Attr(\"Tkeys: type\")\n    .Attr(\"Tvalues: type\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle handle;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 1, &handle));\n      DimensionHandle unused_dim;\n      TF_RETURN_IF_ERROR(c->WithValue(c->Dim(handle, 0), 2, &unused_dim));\n\n      ShapeHandle values = c->UnknownShape();\n      TF_RETURN_IF_ERROR(c->WithRankAtLeast(values, 1, &values));\n      ShapeHandle keys = c->Vector(c->Dim(values, 0));\n      c->set_output(0, keys);\n      c->set_output(1, values);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"LookupTableExportV2\")\n    .Input(\"table_handle: resource\")\n    .Output(\"keys: Tkeys\")\n    .Output(\"values: Tvalues\")\n    .Attr(\"Tkeys: type\")\n    .Attr(\"Tvalues: type\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle handle;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 0, &handle));\n      auto* handle_data = c->input_handle_shapes_and_types(0);\n      if (handle_data != nullptr && handle_data->size() == 2) {\n        const ShapeAndType& key_shape_and_type = (*handle_data)[0];\n        const ShapeAndType& value_shape_and_type = (*handle_data)[1];\n        TF_RETURN_IF_ERROR(ValidateTableType(c, key_shape_and_type,\n                                             /*key_dtype_attr*/ \"Tkeys\",\n                                             value_shape_and_type,\n                                             /*value_dtype_attr*/ \"Tvalues\"));\n      }\n      // Different lookup tables have different output shapes.\n      c->set_output(0, c->UnknownShape());\n      c->set_output(1, c->UnknownShape());\n      return OkStatus();\n    });\n\nREGISTER_OP(\"LookupTableImport\")\n    .Input(\"table_handle: Ref(string)\")\n    .Input(\"keys: Tin\")\n    .Input(\"values: Tout\")\n    .Attr(\"Tin: type\")\n    .Attr(\"Tout: type\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle handle;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 1, &handle));\n      DimensionHandle unused_dim;\n      TF_RETURN_IF_ERROR(c->WithValue(c->Dim(handle, 0), 2, &unused_dim));\n\n      // TODO(ebrevdo): Validate keys and values shape.\n      return OkStatus();\n    });\n\nREGISTER_OP(\"LookupTableImportV2\")\n    .Input(\"table_handle: resource\")\n    .Input(\"keys: Tin\")\n    .Input(\"values: Tout\")\n    .Attr(\"Tin: type\")\n    .Attr(\"Tout: type\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle handle;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 0, &handle));\n\n      ShapeHandle keys;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &keys));\n      DimensionHandle unused;\n      TF_RETURN_IF_ERROR(\n          c->Merge(c->Dim(keys, 0), c->Dim(c->input(2), 0), &unused));\n      return OkStatus();\n    });\n\nStatus MutableHashTableShape(InferenceContext* c, const ShapeHandle& key,\n                             const ShapeHandle& value) {\n  c->set_output(0, c->Scalar());\n\n  ShapeHandle key_s;\n  TF_RETURN_IF_ERROR(c->WithRankAtMost(key, 1, &key_s));\n\n  DataType key_t;\n  TF_RETURN_IF_ERROR(c->GetAttr(\"key_dtype\", &key_t));\n\n  DataType value_t;\n  TF_RETURN_IF_ERROR(c->GetAttr(\"value_dtype\", &value_t));\n\n  // ShapeAndType vector for {key, value}.\n  c->set_output_handle_shapes_and_types(\n      0, std::vector<ShapeAndType>{{key_s, key_t}, {value, value_t}});\n\n  return OkStatus();\n}\n\nStatus MutableHashTableShapeFn(InferenceContext* c) {\n  return MutableHashTableShape(c, /*key=*/c->Scalar(),\n                               /*value=*/c->Scalar());\n}\n\nStatus MutableHashTableOfTensorsShapeFn(InferenceContext* c) {\n  PartialTensorShape value_p;\n  TF_RETURN_IF_ERROR(c->GetAttr(\"value_shape\", &value_p));\n  ShapeHandle value_s;\n  TF_RETURN_IF_ERROR(c->MakeShapeFromPartialTensorShape(value_p, &value_s));\n  return MutableHashTableShape(c, /*key=*/c->Scalar(), /*value=*/value_s);\n}\n\nStatus MutableDenseHashTableShapeFn(InferenceContext* c) {\n  PartialTensorShape value_p;\n  TF_RETURN_IF_ERROR(c->GetAttr(\"value_shape\", &value_p));\n  ShapeHandle value_s;\n  TF_RETURN_IF_ERROR(c->MakeShapeFromPartialTensorShape(value_p, &value_s));\n  return MutableHashTableShape(c, /*key=*/c->input(0), /*value=*/value_s);\n}\n\nREGISTER_OP(\"HashTable\")\n    .Output(\"table_handle: Ref(string)\")\n    .Attr(\"container: string = ''\")\n    .Attr(\"shared_name: string = ''\")\n    .Attr(\"use_node_name_sharing: bool = false\")\n    .Attr(\"key_dtype: type\")\n    .Attr(\"value_dtype: type\")\n    .SetIsStateful()\n    .SetShapeFn(TwoElementOutput);\n\nREGISTER_OP(\"HashTableV2\")\n    .Output(\"table_handle: resource\")\n    .Attr(\"container: string = ''\")\n    .Attr(\"shared_name: string = ''\")\n    .Attr(\"use_node_name_sharing: bool = false\")\n    .Attr(\"key_dtype: type\")\n    .Attr(\"value_dtype: type\")\n    .SetIsStateful()\n    .SetShapeFn(ScalarOutput);\n\nREGISTER_OP(\"AnonymousHashTable\")\n    .Output(\"table_handle: resource\")\n    .Attr(\"key_dtype: type\")\n    .Attr(\"value_dtype: type\")\n    .SetIsStateful()\n    .SetShapeFn(ScalarOutput);\n\nREGISTER_OP(\"MutableHashTable\")\n    .Output(\"table_handle: Ref(string)\")\n    .Attr(\"container: string = ''\")\n    .Attr(\"shared_name: string = ''\")\n    .Attr(\"use_node_name_sharing: bool = false\")\n    .Attr(\"key_dtype: type\")\n    .Attr(\"value_dtype: type\")\n    .SetIsStateful()\n    .SetShapeFn(TwoElementOutput);\n\nREGISTER_OP(\"MutableHashTableV2\")\n    .Output(\"table_handle: resource\")\n    .Attr(\"container: string = ''\")\n    .Attr(\"shared_name: string = ''\")\n    .Attr(\"use_node_name_sharing: bool = false\")\n    .Attr(\"key_dtype: type\")\n    .Attr(\"value_dtype: type\")\n    .SetIsStateful()\n    .SetShapeFn(MutableHashTableShapeFn);\n\nREGISTER_OP(\"AnonymousMutableHashTable\")\n    .Output(\"table_handle: resource\")\n    .Attr(\"key_dtype: type\")\n    .Attr(\"value_dtype: type\")\n    .SetIsStateful()\n    .SetShapeFn(MutableHashTableShapeFn);\n\nREGISTER_OP(\"MutableHashTableOfTensors\")\n    .Output(\"table_handle: Ref(string)\")\n    .Attr(\"container: string = ''\")\n    .Attr(\"shared_name: string = ''\")\n    .Attr(\"use_node_name_sharing: bool = false\")\n    .Attr(\"key_dtype: type\")\n    .Attr(\"value_dtype: type\")\n    .Attr(\"value_shape: shape = {}\")\n    .SetIsStateful()\n    .SetShapeFn(TwoElementOutput);\n\nREGISTER_OP(\"MutableHashTableOfTensorsV2\")\n    .Output(\"table_handle: resource\")\n    .Attr(\"container: string = ''\")\n    .Attr(\"shared_name: string = ''\")\n    .Attr(\"use_node_name_sharing: bool = false\")\n    .Attr(\"key_dtype: type\")\n    .Attr(\"value_dtype: type\")\n    .Attr(\"value_shape: shape = {}\")\n    .SetIsStateful()\n    .SetShapeFn(MutableHashTableOfTensorsShapeFn);\n\nREGISTER_OP(\"AnonymousMutableHashTableOfTensors\")\n    .Output(\"table_handle: resource\")\n    .Attr(\"key_dtype: type\")\n    .Attr(\"value_dtype: type\")\n    .Attr(\"value_shape: shape = {}\")\n    .SetIsStateful()\n    .SetShapeFn(MutableHashTableOfTensorsShapeFn);\n\nREGISTER_OP(\"MutableDenseHashTable\")\n    .Input(\"empty_key: key_dtype\")\n    .Output(\"table_handle: Ref(string)\")\n    .Attr(\"container: string = ''\")\n    .Attr(\"shared_name: string = ''\")\n    .Attr(\"use_node_name_sharing: bool = false\")\n    .Attr(\"key_dtype: type\")\n    .Attr(\"value_dtype: type\")\n    .Attr(\"value_shape: shape = {}\")\n    .Attr(\"initial_num_buckets: int = 131072\")  // 2^17\n    .Attr(\"max_load_factor: float = 0.8\")\n    .SetIsStateful()\n    .SetShapeFn(TwoElementOutput);\n\nREGISTER_OP(\"MutableDenseHashTableV2\")\n    .Input(\"empty_key: key_dtype\")\n    .Input(\"deleted_key: key_dtype\")\n    .Output(\"table_handle: resource\")\n    .Attr(\"container: string = ''\")\n    .Attr(\"shared_name: string = ''\")\n    .Attr(\"use_node_name_sharing: bool = false\")\n    .Attr(\"key_dtype: type\")\n    .Attr(\"value_dtype: type\")\n    .Attr(\"value_shape: shape = {}\")\n    .Attr(\"initial_num_buckets: int = 131072\")  // 2^17\n    .Attr(\"max_load_factor: float = 0.8\")\n    .SetIsStateful()\n    .SetShapeFn(MutableDenseHashTableShapeFn);\n\nREGISTER_OP(\"AnonymousMutableDenseHashTable\")\n    .Input(\"empty_key: key_dtype\")\n    .Input(\"deleted_key: key_dtype\")\n    .Output(\"table_handle: resource\")\n    .Attr(\"key_dtype: type\")\n    .Attr(\"value_dtype: type\")\n    .Attr(\"value_shape: shape = {}\")\n    .Attr(\"initial_num_buckets: int = 131072\")  // 2^17\n    .Attr(\"max_load_factor: float = 0.8\")\n    .SetIsStateful()\n    .SetShapeFn(MutableDenseHashTableShapeFn);\n\nREGISTER_OP(\"InitializeTable\")\n    .Input(\"table_handle: Ref(string)\")\n    .Input(\"keys: Tkey\")\n    .Input(\"values: Tval\")\n    .Attr(\"Tkey: type\")\n    .Attr(\"Tval: type\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle handle;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 1, &handle));\n      DimensionHandle unused_dim;\n      TF_RETURN_IF_ERROR(c->WithValue(c->Dim(handle, 0), 2, &unused_dim));\n\n      ShapeHandle keys;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &keys));\n      TF_RETURN_IF_ERROR(c->Merge(keys, c->input(2), &keys));\n      return OkStatus();\n    });\n\nREGISTER_OP(\"InitializeTableV2\")\n    .Input(\"table_handle: resource\")\n    .Input(\"keys: Tkey\")\n    .Input(\"values: Tval\")\n    .Attr(\"Tkey: type\")\n    .Attr(\"Tval: type\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle handle;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 0, &handle));\n\n      ShapeHandle keys;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &keys));\n      TF_RETURN_IF_ERROR(c->Merge(keys, c->input(2), &keys));\n      return OkStatus();\n    });\n\nREGISTER_OP(\"InitializeTableFromTextFile\")\n    .Input(\"table_handle: Ref(string)\")\n    .Input(\"filename: string\")\n    .Attr(\"key_index: int >= -2\")\n    .Attr(\"value_index: int >= -2\")\n    .Attr(\"vocab_size: int >= -1 = -1\")\n    .Attr(\"delimiter: string = '\\t'\")\n    .Attr(\"offset: int = 0\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle handle;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 1, &handle));\n      DimensionHandle unused_dim;\n      TF_RETURN_IF_ERROR(c->WithValue(c->Dim(handle, 0), 2, &unused_dim));\n\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &handle));\n      return OkStatus();\n    });\n\nREGISTER_OP(\"InitializeTableFromTextFileV2\")\n    .Input(\"table_handle: resource\")\n    .Input(\"filename: string\")\n    .Attr(\"key_index: int >= -2\")\n    .Attr(\"value_index: int >= -2\")\n    .Attr(\"vocab_size: int >= -1 = -1\")\n    .Attr(\"delimiter: string = '\\t'\")\n    .Attr(\"offset: int = 0\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle handle;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 0, &handle));\n\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &handle));\n      return OkStatus();\n    });\n\n}  // namespace tensorflow\n", "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for lookup ops.\"\"\"\nimport os\nimport tempfile\nimport unittest\n\nfrom absl.testing import parameterized\nimport numpy as np\n\nfrom tensorflow.python import tf2\nfrom tensorflow.python.checkpoint import checkpoint as trackable\nfrom tensorflow.python.checkpoint import graph_view\nfrom tensorflow.python.checkpoint import util as checkpoint_util\nfrom tensorflow.python.client import session\nfrom tensorflow.python.data.experimental.ops import counter\nfrom tensorflow.python.data.ops import dataset_ops\nfrom tensorflow.python.eager import backprop\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.eager import def_function\nfrom tensorflow.python.eager import wrap_function\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors_impl\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import sparse_tensor\nfrom tensorflow.python.framework import tensor_spec\nfrom tensorflow.python.framework import test_ops\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import lookup_ops\nfrom tensorflow.python.ops import map_fn\nfrom tensorflow.python.ops import variables\nfrom tensorflow.python.ops.ragged import ragged_tensor\nfrom tensorflow.python.platform import test\nfrom tensorflow.python.saved_model import load as saved_model_load\nfrom tensorflow.python.saved_model import save as saved_model_save\nfrom tensorflow.python.trackable import asset\nfrom tensorflow.python.trackable import autotrackable\nfrom tensorflow.python.training import saver\nfrom tensorflow.python.training import server_lib\nfrom tensorflow.python.util import compat\n\n\nclass BaseLookupTableTest(test.TestCase):\n\n  def getHashTable(self):\n    if tf2.enabled():\n      return lookup_ops.StaticHashTable\n    else:\n      return lookup_ops.StaticHashTableV1\n\n  def getVocabularyTable(self):\n    if tf2.enabled():\n      return lookup_ops.StaticVocabularyTable\n    else:\n      return lookup_ops.StaticVocabularyTableV1\n\n  def initialize_table(self, table):\n    if not tf2.enabled():\n      self.evaluate(table.initializer)\n\n\nSKIP_ANONYMOUS_IN_TF1_REASON = (\n    \"In v1 graph mode, each self.evaluate call will execute the handle \"\n    \"creation op (e.g. AnonymousHashTable) which will create a new table \"\n    \"resource unrelated to other self.evaluate calls, so we can't test \"\n    \"anonymous resources with self.evaluate .\"\n)\n\n\n@parameterized.named_parameters(\n    (f\"_{is_anonymous}\", is_anonymous) for is_anonymous in [False, True])\nclass StaticHashTableTest(BaseLookupTableTest, parameterized.TestCase):\n\n  def testStaticHashTable(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    default_val = -1\n    keys = constant_op.constant([\"brain\", \"salad\", \"surgery\"])\n    values = constant_op.constant([0, 1, 2], dtypes.int64)\n    table = self.getHashTable()(\n        lookup_ops.KeyValueTensorInitializer(keys, values),\n        default_val,\n        experimental_is_anonymous=is_anonymous)\n    self.assertEqual(table._is_anonymous, is_anonymous)\n    self.initialize_table(table)\n\n    self.assertAllEqual(3, self.evaluate(table.size()))\n\n    input_string = constant_op.constant([\"brain\", \"salad\", \"tank\"])\n    output = table.lookup(input_string)\n    self.assertAllEqual([3], output.get_shape())\n\n    result = self.evaluate(output)\n    self.assertAllEqual([0, 1, -1], result)\n\n    exported_keys_tensor, exported_values_tensor = table.export()\n\n    self.assertItemsEqual([b\"brain\", b\"salad\", b\"surgery\"],\n                          self.evaluate(exported_keys_tensor))\n    self.assertItemsEqual([0, 1, 2], self.evaluate(exported_values_tensor))\n\n  def testStaticHashTableFindHighRank(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    default_val = -1\n    keys = constant_op.constant([\"brain\", \"salad\", \"surgery\"])\n    values = constant_op.constant([0, 1, 2], dtypes.int64)\n    table = self.getHashTable()(\n        lookup_ops.KeyValueTensorInitializer(keys, values),\n        default_val,\n        experimental_is_anonymous=is_anonymous)\n    self.initialize_table(table)\n\n    self.assertAllEqual(3, self.evaluate(table.size()))\n\n    input_string = constant_op.constant([[\"brain\", \"salad\"],\n                                         [\"tank\", \"tarkus\"]])\n    output = table.lookup(input_string)\n\n    result = self.evaluate(output)\n    self.assertAllEqual([[0, 1], [-1, -1]], result)\n\n  def testStaticHashTableInitWithPythonArrays(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    default_val = -1\n    keys = [\"brain\", \"salad\", \"surgery\"]\n    values = [0, 1, 2]\n    table = self.getHashTable()(\n        lookup_ops.KeyValueTensorInitializer(\n            keys, values, value_dtype=dtypes.int64),\n        default_val,\n        experimental_is_anonymous=is_anonymous)\n    self.initialize_table(table)\n\n    self.assertAllEqual(3, self.evaluate(table.size()))\n\n    input_string = constant_op.constant([\"brain\", \"salad\", \"tank\"])\n    output = table.lookup(input_string)\n\n    result = self.evaluate(output)\n    self.assertAllEqual([0, 1, -1], result)\n\n  def testStaticHashTableInitWithNumPyArrays(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    default_val = -1\n    keys = np.array([\"brain\", \"salad\", \"surgery\"], dtype=np.str_)\n    values = np.array([0, 1, 2], dtype=np.int64)\n    table = self.getHashTable()(\n        lookup_ops.KeyValueTensorInitializer(keys, values),\n        default_val,\n        experimental_is_anonymous=is_anonymous)\n    self.initialize_table(table)\n\n    self.assertAllEqual(3, self.evaluate(table.size()))\n\n    input_string = constant_op.constant([\"brain\", \"salad\", \"tank\"])\n    output = table.lookup(input_string)\n\n    result = self.evaluate(output)\n    self.assertAllEqual([0, 1, -1], result)\n\n  def testMultipleStaticHashTables(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    default_val = -1\n    keys = constant_op.constant([\"brain\", \"salad\", \"surgery\"])\n    values = constant_op.constant([0, 1, 2], dtypes.int64)\n\n    table1 = self.getHashTable()(\n        lookup_ops.KeyValueTensorInitializer(keys, values),\n        default_val,\n        experimental_is_anonymous=is_anonymous)\n    table2 = self.getHashTable()(\n        lookup_ops.KeyValueTensorInitializer(keys, values),\n        default_val,\n        experimental_is_anonymous=is_anonymous)\n    table3 = self.getHashTable()(\n        lookup_ops.KeyValueTensorInitializer(keys, values),\n        default_val,\n        experimental_is_anonymous=is_anonymous)\n\n    self.initialize_table(table1)\n    self.initialize_table(table2)\n    self.initialize_table(table3)\n    self.assertAllEqual(3, self.evaluate(table1.size()))\n    self.assertAllEqual(3, self.evaluate(table2.size()))\n    self.assertAllEqual(3, self.evaluate(table3.size()))\n\n    input_string = constant_op.constant([\"brain\", \"salad\", \"tank\"])\n    output1 = table1.lookup(input_string)\n    output2 = table2.lookup(input_string)\n    output3 = table3.lookup(input_string)\n\n    out1, out2, out3 = self.evaluate([output1, output2, output3])\n    self.assertAllEqual([0, 1, -1], out1)\n    self.assertAllEqual([0, 1, -1], out2)\n    self.assertAllEqual([0, 1, -1], out3)\n\n  def testStaticHashTableWithTensorDefault(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    default_val = constant_op.constant(-1, dtypes.int64)\n    keys = constant_op.constant([\"brain\", \"salad\", \"surgery\"])\n    values = constant_op.constant([0, 1, 2], dtypes.int64)\n    table = self.getHashTable()(\n        lookup_ops.KeyValueTensorInitializer(keys, values),\n        default_val,\n        experimental_is_anonymous=is_anonymous)\n    self.initialize_table(table)\n\n    input_string = constant_op.constant([\"brain\", \"salad\", \"tank\"])\n    output = table.lookup(input_string)\n\n    result = self.evaluate(output)\n    self.assertAllEqual([0, 1, -1], result)\n\n  def testStaticHashTableGetItem(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    default_val = constant_op.constant(-1, dtypes.int64)\n    keys = constant_op.constant([\"brain\", \"salad\", \"surgery\"])\n    values = constant_op.constant([0, 1, 2], dtypes.int64)\n    table = self.getHashTable()(\n        lookup_ops.KeyValueTensorInitializer(keys, values),\n        default_val,\n        experimental_is_anonymous=is_anonymous)\n    self.initialize_table(table)\n\n    input_string = constant_op.constant([\"brain\", \"salad\", \"tank\"])\n    output = table[input_string]\n\n    result = self.evaluate(output)\n    self.assertAllEqual([0, 1, -1], result)\n\n  def testStaticHashTableWithSparseTensorInput(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    default_val = constant_op.constant(-1, dtypes.int64)\n    keys = constant_op.constant([\"brain\", \"salad\", \"surgery\"])\n    values = constant_op.constant([0, 1, 2], dtypes.int64)\n    table = self.getHashTable()(\n        lookup_ops.KeyValueTensorInitializer(keys, values),\n        default_val,\n        experimental_is_anonymous=is_anonymous)\n    self.initialize_table(table)\n\n    sp_indices = [[0, 0], [0, 1], [1, 0]]\n    sp_shape = [2, 2]\n    input_tensor = sparse_tensor.SparseTensor(\n        constant_op.constant(sp_indices, dtypes.int64),\n        constant_op.constant([\"brain\", \"salad\", \"tank\"]),\n        constant_op.constant(sp_shape, dtypes.int64))\n    output = table.lookup(input_tensor)\n\n    out_indices, out_values, out_shape = self.evaluate(output)\n\n    self.assertAllEqual([0, 1, -1], out_values)\n    self.assertAllEqual(sp_indices, out_indices)\n    self.assertAllEqual(sp_shape, out_shape)\n\n  def testStaticHashTableWithRaggedTensorInput(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    default_val = constant_op.constant(-1, dtypes.int64)\n    keys = constant_op.constant([\"brain\", \"salad\", \"surgery\"])\n    values = constant_op.constant([0, 1, 2], dtypes.int64)\n    table = self.getHashTable()(\n        lookup_ops.KeyValueTensorInitializer(keys, values),\n        default_val,\n        experimental_is_anonymous=is_anonymous)\n    self.initialize_table(table)\n\n    row_splits = [0, 2, 3]\n    input_tensor = ragged_tensor.RaggedTensor.from_row_splits(\n        constant_op.constant([\"brain\", \"salad\", \"tank\"]),\n        constant_op.constant(row_splits, dtypes.int64))\n    output = table.lookup(input_tensor)\n\n    out = self.evaluate(output)\n\n    self.assertAllEqual([0, 1, -1], out.values)\n    self.assertAllEqual(row_splits, out.row_splits)\n\n  def testSignatureMismatch(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    default_val = -1\n    keys = constant_op.constant([\"brain\", \"salad\", \"surgery\"])\n    values = constant_op.constant([0, 1, 2], dtypes.int64)\n    table = self.getHashTable()(\n        lookup_ops.KeyValueTensorInitializer(keys, values),\n        default_val,\n        experimental_is_anonymous=is_anonymous)\n    self.initialize_table(table)\n\n    # Ref types do not produce a lookup signature mismatch.\n    input_string_ref = variables.Variable(\"brain\")\n    self.evaluate(input_string_ref.initializer)\n    self.assertEqual(0, self.evaluate(table.lookup(input_string_ref)))\n\n    input_string = constant_op.constant([1, 2, 3], dtypes.int64)\n    with self.assertRaises(TypeError):\n      table.lookup(input_string)\n\n    with self.assertRaises(TypeError):\n      self.getHashTable()(\n          lookup_ops.KeyValueTensorInitializer(keys, values),\n          \"UNK\",\n          experimental_is_anonymous=is_anonymous)\n\n  def testDTypes(self, is_anonymous):\n    default_val = -1\n    with self.assertRaises(TypeError):\n      self.getHashTable()(\n          lookup_ops.KeyValueTensorInitializer([\"a\"], [1], [dtypes.string],\n                                               dtypes.int64),\n          default_val,\n          experimental_is_anonymous=is_anonymous)\n\n  @test_util.run_v1_only(\"(Cached) Sessions not available in TF2.0\")\n  def testNotInitialized(self, is_anonymous):\n    with self.cached_session():\n      default_val = -1\n      table = self.getHashTable()(\n          lookup_ops.KeyValueTensorInitializer([\"a\"], [1],\n                                               value_dtype=dtypes.int64),\n          default_val,\n          experimental_is_anonymous=is_anonymous)\n\n      input_string = constant_op.constant([\"brain\", \"salad\", \"surgery\"])\n      output = table.lookup(input_string)\n\n      with self.assertRaisesOpError(\"Table not initialized\"):\n        self.evaluate(output)\n\n  @test_util.run_v1_only(\"(Cached) Sessions not available in TF2.0\")\n  def testInitializeTwice(self, is_anonymous):\n    with self.cached_session():\n      default_val = -1\n      keys = constant_op.constant([\"brain\", \"salad\", \"surgery\"])\n      values = constant_op.constant([0, 1, 2], dtypes.int64)\n      table = self.getHashTable()(\n          lookup_ops.KeyValueTensorInitializer(keys, values),\n          default_val,\n          experimental_is_anonymous=is_anonymous)\n      self.initialize_table(table)\n      # Make sure that initializing twice doesn't throw any errors.\n      self.initialize_table(table)\n\n  def testInitializationWithInvalidDimensions(self, is_anonymous):\n    default_val = -1\n    keys = constant_op.constant([\"brain\", \"salad\", \"surgery\"])\n    values = constant_op.constant([0, 1, 2, 3, 4], dtypes.int64)\n\n    raised_error = ValueError\n    if context.executing_eagerly():\n      raised_error = errors_impl.InvalidArgumentError\n    with self.assertRaises(raised_error):\n      self.getHashTable()(\n          lookup_ops.KeyValueTensorInitializer(keys, values),\n          default_val,\n          experimental_is_anonymous=is_anonymous)\n\n  @test_util.run_v1_only(\"Sessions not available in TF2.0\")\n  def testMultipleSessions(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    # Start a server\n    server = server_lib.Server({\"local0\": [\"localhost:0\"]},\n                               protocol=\"grpc\",\n                               start=True)\n    # Create two sessions sharing the same state\n    session1 = session.Session(server.target)\n    session2 = session.Session(server.target)\n\n    default_val = -1\n    keys = constant_op.constant([\"brain\", \"salad\", \"surgery\"])\n    values = constant_op.constant([0, 1, 2], dtypes.int64)\n    table = self.getHashTable()(\n        lookup_ops.KeyValueTensorInitializer(keys, values),\n        default_val,\n        name=\"t1\",\n        experimental_is_anonymous=is_anonymous)\n\n    # Init the table in the first session.\n    with session1:\n      self.initialize_table(table)\n      self.assertAllEqual(3, self.evaluate(table.size()))\n\n    # Init the table in the second session and verify that we do not get a\n    # \"Table already initialized\" error.\n    with session2:\n      self.evaluate(table.initializer)\n      self.assertAllEqual(3, self.evaluate(table.size()))\n\n  @test_util.run_v2_only\n  def testImportedHashTable(self, is_anonymous):\n    g = ops.Graph()\n    with g.as_default():\n      t = lookup_ops.StaticHashTable(\n          lookup_ops.KeyValueTensorInitializer([\"a\"], [1]),\n          2)\n      init_op = t._init_op\n      op = t.lookup(ops.convert_to_tensor([\"a\"]))\n      meta_graph = saver.export_meta_graph()\n\n    def f():\n      saver.import_meta_graph(meta_graph)\n      return ops.get_default_graph().get_tensor_by_name(op.name)\n\n    wrapped = wrap_function.wrap_function(f, [])\n    pruned_init_fn = wrapped.prune(\n        (), [wrapped.graph.get_operation_by_name(init_op.name)])\n    self.evaluate(pruned_init_fn())\n    self.assertAllEqual([1], wrapped())\n\n  def testStaticHashTableInt32String(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    default_val = \"n/a\"\n    keys = constant_op.constant([0, 1, 2], dtypes.int32)\n    values = constant_op.constant([\"brain\", \"salad\", \"surgery\"])\n    table = self.getHashTable()(\n        lookup_ops.KeyValueTensorInitializer(keys, values),\n        default_val,\n        experimental_is_anonymous=is_anonymous)\n    self.initialize_table(table)\n\n    input_tensor = constant_op.constant([0, 1, -1])\n    output = table.lookup(input_tensor)\n\n    result = self.evaluate(output)\n    self.assertAllEqual([b\"brain\", b\"salad\", b\"n/a\"], result)\n\n  def testTableUseInFunction(self, is_anonymous):\n    if not context.executing_eagerly():\n      self.skipTest(\"Only Eager mode test.\")\n    keys = constant_op.constant([0, 1, 2], dtypes.int32)\n    values = constant_op.constant([\"brain\", \"salad\", \"surgery\"])\n    table = self.getHashTable()(\n        lookup_ops.KeyValueTensorInitializer(keys, values),\n        \"n/a\",\n        experimental_is_anonymous=is_anonymous)\n\n    @def_function.function\n    def lookup_table_func(k):\n      return table.lookup(k)\n\n    result = lookup_table_func(constant_op.constant([0, 1, -1]))\n    self.assertAllEqual([b\"brain\", b\"salad\", b\"n/a\"], result)\n    result = lookup_table_func(constant_op.constant([2, -1, 1]))\n    self.assertAllEqual([b\"surgery\", b\"n/a\", b\"salad\"], result)\n\n  def testTableCreatedInFunction(self, is_anonymous):\n    if not context.executing_eagerly():\n      self.skipTest(\"Only Eager mode test.\")\n    keys = constant_op.constant([0, 1, 2], dtypes.int32)\n    values = constant_op.constant([\"brain\", \"salad\", \"surgery\"])\n\n    @def_function.function\n    def lookup_table_func(k):\n      table = self.getHashTable()(\n          lookup_ops.KeyValueTensorInitializer(keys, values),\n          \"n/a\",\n          experimental_is_anonymous=is_anonymous)\n      return table.lookup(k)\n\n    result = lookup_table_func(constant_op.constant([0, 1, -1]))\n    self.assertAllEqual([b\"brain\", b\"salad\", b\"n/a\"], result)\n    result = lookup_table_func(constant_op.constant([2, -1, 1]))\n    self.assertAllEqual([b\"surgery\", b\"n/a\", b\"salad\"], result)\n\n  def testTwoTablesInControlFlow(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    keys = constant_op.constant([1, 2, 3], dtypes.int32)\n    values = constant_op.constant([5, 10, 15], dtypes.int32)\n\n    def table_func1(x):\n      table = self.getHashTable()(\n          lookup_ops.KeyValueTensorInitializer(keys, values),\n          -1,\n          experimental_is_anonymous=is_anonymous)\n      return table.lookup(x)\n\n    elems = np.array([2, 4, 1], dtype=np.int32)\n    result1 = map_fn.map_fn(table_func1, elems, dtype=dtypes.int32)\n\n    def table_func2(x):\n      table = self.getHashTable()(\n          lookup_ops.KeyValueTensorInitializer(keys, values),\n          -1,\n          experimental_is_anonymous=is_anonymous)\n      return table.lookup(x)\n\n    elems = np.array([2, 4, 1], dtype=np.int32)\n    result2 = map_fn.map_fn(table_func2, elems, dtype=dtypes.int32)\n\n    self.evaluate(lookup_ops.tables_initializer())\n\n    self.assertAllEqual([10, -1, 5], self.evaluate(result1))\n    self.assertAllEqual([10, -1, 5], self.evaluate(result2))\n\n  @test_util.enable_control_flow_v2\n  def testLookupTableInWhileV2(self, is_anonymous):\n    lookup = self.getHashTable()(\n        lookup_ops.KeyValueTensorInitializer(\n            constant_op.constant([2, 5], dtype=dtypes.int64),\n            constant_op.constant([-10.0, 1], dtype=dtypes.float32)),\n        -1,\n        experimental_is_anonymous=is_anonymous)\n\n    beta = variables.Variable(1.0, trainable=True)\n\n    @def_function.function\n    def get_loss(unused_beta):\n      return map_fn.map_fn(\n          lookup.lookup,\n          constant_op.constant([2, 3], dtype=dtypes.int64),\n          dtype=dtypes.float32)\n\n    with backprop.GradientTape() as tape:\n      loss = get_loss(beta)\n\n    self.assertIsNone(tape.gradient(loss, beta))\n\n  @test_util.enable_control_flow_v2\n  def testLookupTableInCondV2(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    lookup = self.getHashTable()(\n        lookup_ops.KeyValueTensorInitializer(\n            constant_op.constant([2, 5], dtype=dtypes.int64),\n            constant_op.constant([-10.0, 1], dtype=dtypes.float32)),\n        -1,\n        experimental_is_anonymous=is_anonymous)\n\n    beta = variables.Variable(1.0, trainable=True)\n\n    @def_function.function\n    def get_loss(beta):\n\n      def true_fn():\n        return lookup.lookup(constant_op.constant(2, dtype=dtypes.int64))\n\n      def false_fn():\n        return constant_op.constant(0, dtype=dtypes.float32)\n\n      return beta * control_flow_ops.cond(\n          constant_op.constant(True), true_fn=true_fn, false_fn=false_fn)\n\n    with backprop.GradientTape() as tape:\n      loss = get_loss(beta)\n    grad = tape.gradient(loss, beta)\n    self.evaluate(variables.global_variables_initializer())\n    self.evaluate(lookup_ops.tables_initializer())\n    self.assertAllEqual(grad, -10.)\n\n  def testExportShapeInference(self, is_anonymous):\n    table = self.getHashTable()(\n        lookup_ops.KeyValueTensorInitializer(\n            constant_op.constant([2, 5], dtype=dtypes.int64),\n            constant_op.constant([-10.0, 1], dtype=dtypes.float32)),\n        -1,\n        experimental_is_anonymous=is_anonymous)\n    actual_shapes = [t.shape for t in table.export()]\n    inferred_shapes = []\n\n    @def_function.function\n    def f():\n      for t in table.export():\n        inferred_shapes.append(t.shape)\n\n    f()\n    self.assertLen(actual_shapes, 2)\n    self.assertLen(inferred_shapes, 2)\n    self.assertTrue(inferred_shapes[0].is_compatible_with(actual_shapes[0]))\n    self.assertTrue(inferred_shapes[1].is_compatible_with(actual_shapes[1]))\n\n  @test_util.run_v2_only\n  def testSavedModelSaveRestore(self, is_anonymous):\n    save_dir = os.path.join(self.get_temp_dir(), \"save_restore\")\n    save_path = os.path.join(tempfile.mkdtemp(prefix=save_dir), \"hash\")\n\n    root = autotrackable.AutoTrackable()\n\n    default_value = -1\n    keys = constant_op.constant([11, 12, 13], dtypes.int64)\n    values = constant_op.constant([0, 1, 2], dtypes.int64)\n    root.table = self.getHashTable()(\n        lookup_ops.KeyValueTensorInitializer(keys, values),\n        default_value,\n        experimental_is_anonymous=is_anonymous)\n\n    @def_function.function(\n        input_signature=[tensor_spec.TensorSpec((), dtypes.int64)])\n    def lookup(key):\n      return root.table.lookup(key)\n\n    @def_function.function(input_signature=[])\n    def size():\n      return root.table.size()\n\n    @def_function.function(input_signature=[])\n    def is_ref_counting():\n      return test_ops.is_resource_handle_ref_counting(\n          root.table.resource_handle)\n\n    root.lookup = lookup\n    root.size = size\n    root.is_ref_counting = is_ref_counting\n\n    self.assertEqual(root.table.size(), 3)\n    self.assertEqual(root.lookup(12), 1)\n    self.assertEqual(root.lookup(10), -1)\n    self.assertLen(root.table.export()[0], 3)\n    self.assertEqual(root.is_ref_counting(), is_anonymous)\n\n    saved_model_save.save(root, save_path)\n\n    del root\n    loaded = saved_model_load.load(save_path)\n    self.assertEqual(loaded.size(), 3)\n    self.assertEqual(loaded.lookup(12), 1)\n    self.assertEqual(loaded.lookup(10), -1)\n    self.assertEqual(loaded.is_ref_counting(), is_anonymous)\n\n\n@parameterized.named_parameters(\n    (f\"_{is_anonymous}\", is_anonymous) for is_anonymous in [False, True])\nclass KeyValueTensorInitializerTest(BaseLookupTableTest):\n\n  def test_string(self, is_anonymous):\n    init = lookup_ops.KeyValueTensorInitializer(\n        (\"brain\", \"salad\", \"surgery\"), (0, 1, 2), dtypes.string, dtypes.int64)\n    table = self.getHashTable()(\n        init, default_value=-1, experimental_is_anonymous=is_anonymous)\n    self.initialize_table(table)\n\n  def test_multiple_tables(self, is_anonymous):\n    with ops.name_scope(\"table_scope\"):\n      init1 = lookup_ops.KeyValueTensorInitializer(\n          (\"brain\", \"salad\", \"surgery\"), (0, 1, 2), dtypes.string, dtypes.int64)\n      table1 = self.getHashTable()(\n          init1, default_value=-1, experimental_is_anonymous=is_anonymous)\n      if not context.executing_eagerly():\n        self.assertEqual(\"hash_table\", table1.name)\n        self.assertEqual(\"table_scope/hash_table\",\n                         table1.resource_handle.op.name)\n      init2 = lookup_ops.KeyValueTensorInitializer(\n          (\"brain\", \"salad\", \"surgery\"), (0, 1, 2), dtypes.string, dtypes.int64)\n      table2 = self.getHashTable()(\n          init2, default_value=-1, experimental_is_anonymous=is_anonymous)\n      if not context.executing_eagerly():\n        self.assertEqual(\"hash_table_1\", table2.name)\n        self.assertEqual(\"table_scope/hash_table_1\",\n                         table2.resource_handle.op.name)\n\n  def test_int64(self, is_anonymous):\n    init = lookup_ops.KeyValueTensorInitializer((42, 1, -1000), (0, 1, 2),\n                                                dtypes.int64, dtypes.int64)\n    table = self.getHashTable()(\n        init, default_value=-1, experimental_is_anonymous=is_anonymous)\n    self.initialize_table(table)\n\n  def test_int32(self, is_anonymous):\n    init = lookup_ops.KeyValueTensorInitializer((42, 1, -1000), (0, 1, 2),\n                                                dtypes.int32, dtypes.int64)\n    with self.assertRaises(errors_impl.OpError):\n      table = self.getHashTable()(\n          init, default_value=-1, experimental_is_anonymous=is_anonymous)\n      self.initialize_table(table)\n\n\n@parameterized.named_parameters(\n    (f\"_{is_anonymous}\", is_anonymous) for is_anonymous in [False, True])\nclass InitializeTableFromFileOpTest(BaseLookupTableTest):\n\n  def _createVocabFile(self, basename, values=(\"brain\", \"salad\", \"surgery\")):\n    vocabulary_file = os.path.join(self.get_temp_dir(), basename)\n    with open(vocabulary_file, \"w\") as f:\n      f.write(\"\\n\".join(values) + \"\\n\")\n    return vocabulary_file\n\n  def testInitializeStringTable(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    vocabulary_file = self._createVocabFile(\"one_column_1.txt\")\n    default_value = -1\n    init = lookup_ops.TextFileInitializer(\n        vocabulary_file, dtypes.string, lookup_ops.TextFileIndex.WHOLE_LINE,\n        dtypes.int64, lookup_ops.TextFileIndex.LINE_NUMBER)\n    self.assertIn(\"one_column_1.txt_-2_-1\", init._shared_name)\n    table = self.getHashTable()(\n        init, default_value, experimental_is_anonymous=is_anonymous)\n    self.initialize_table(table)\n\n    output = table.lookup(constant_op.constant([\"brain\", \"salad\", \"tank\"]))\n\n    result = self.evaluate(output)\n    self.assertAllEqual([0, 1, -1], result)\n\n  def testInitializeInt64Table(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    vocabulary_file = self._createVocabFile(\n        \"one_column_int64.txt\", values=(\"42\", \"1\", \"-1000\"))\n\n    with self.cached_session():\n      default_value = -1\n      init = lookup_ops.TextFileInitializer(\n          vocabulary_file, dtypes.int64, lookup_ops.TextFileIndex.WHOLE_LINE,\n          dtypes.int64, lookup_ops.TextFileIndex.LINE_NUMBER)\n      self.assertIn(\"one_column_int64.txt_-2_-1\", init._shared_name)\n      table = self.getHashTable()(\n          init, default_value, experimental_is_anonymous=is_anonymous)\n      self.initialize_table(table)\n\n      output = table.lookup(\n          constant_op.constant((42, 1, 11), dtype=dtypes.int64))\n\n      result = self.evaluate(output)\n      self.assertAllEqual([0, 1, -1], result)\n\n  def testInitializeIndexTable(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    vocabulary_file = self._createVocabFile(\"one_column_2.txt\")\n\n    with self.cached_session():\n      default_value = \"UNK\"\n      key_index = lookup_ops.TextFileIndex.LINE_NUMBER\n      value_index = lookup_ops.TextFileIndex.WHOLE_LINE\n      init = lookup_ops.TextFileInitializer(\n          vocabulary_file, dtypes.int64, key_index, dtypes.string, value_index)\n      self.assertIn(\"one_column_2.txt_-1_-2\", init._shared_name)\n      table = self.getHashTable()(\n          init, default_value, experimental_is_anonymous=is_anonymous)\n      self.initialize_table(table)\n\n      input_values = constant_op.constant([0, 1, 2, 3], dtypes.int64)\n      output = table.lookup(input_values)\n\n      result = self.evaluate(output)\n      self.assertAllEqual([b\"brain\", b\"salad\", b\"surgery\", b\"UNK\"], result)\n\n  def testMultiColumn(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    vocabulary_file = os.path.join(self.get_temp_dir(), \"three_columns.txt\")\n    with open(vocabulary_file, \"w\") as f:\n      f.write(\"\\n\".join([\"0\\tbrain\\t1\", \"1\\tsalad\\t5\", \"2\\tsurgery\\t6\"]) + \"\\n\")\n\n    with self.cached_session():\n      default_value = -1\n      key_index = 1\n      value_index = 2\n\n      init = lookup_ops.TextFileInitializer(\n          vocabulary_file, dtypes.string, key_index, dtypes.int64, value_index)\n      self.assertIn(\"three_columns.txt_1_2\", init._shared_name)\n      table = self.getHashTable()(\n          init, default_value, experimental_is_anonymous=is_anonymous)\n      self.initialize_table(table)\n\n      input_string = constant_op.constant([\"brain\", \"salad\", \"surgery\"])\n      output = table.lookup(input_string)\n\n      result = self.evaluate(output)\n      self.assertAllEqual([1, 5, 6], result)\n\n  def testInvalidDataTypeInMultiColumn(self, is_anonymous):\n    vocabulary_file = os.path.join(self.get_temp_dir(), \"three_columns.txt\")\n    with open(vocabulary_file, \"w\") as f:\n      f.write(\"\\n\".join([\"0\\tbrain\\t1\", \"1\\tsalad\\t5\", \"2\\tsurgery\\t6\"]) + \"\\n\")\n\n    with self.cached_session():\n      default_value = -1\n      key_index = 2\n      value_index = 1\n      init = lookup_ops.TextFileInitializer(\n          vocabulary_file, dtypes.string, key_index, dtypes.int64, value_index)\n      self.assertIn(\"three_columns.txt_2_1\", init._shared_name)\n      with self.assertRaisesOpError(\"is not a valid\"):\n        table = self.getHashTable()(\n            init, default_value, experimental_is_anonymous=is_anonymous)\n        self.initialize_table(table)\n\n  def testInvalidDataType(self, is_anonymous):\n    vocabulary_file = self._createVocabFile(\"one_column_3.txt\")\n\n    with self.cached_session():\n      default_value = \"UNK\"\n      key_index = lookup_ops.TextFileIndex.WHOLE_LINE\n      value_index = lookup_ops.TextFileIndex.LINE_NUMBER\n\n      with self.assertRaises(ValueError):\n        init = lookup_ops.TextFileInitializer(vocabulary_file, dtypes.int64,\n                                              key_index, dtypes.string,\n                                              value_index)\n        self.assertIn(\"one_column_3.txt_-2_-1\", init._shared_name)\n        self.getHashTable()(\n            init, default_value, experimental_is_anonymous=is_anonymous)\n\n  def testInvalidIndex(self, is_anonymous):\n    vocabulary_file = self._createVocabFile(\"one_column_4.txt\")\n    with self.cached_session():\n      default_value = -1\n      key_index = 1  # second column of the line\n      value_index = lookup_ops.TextFileIndex.LINE_NUMBER\n      init = lookup_ops.TextFileInitializer(\n          vocabulary_file, dtypes.string, key_index, dtypes.int64, value_index)\n      self.assertIn(\"one_column_4.txt_1_-1\", init._shared_name)\n\n      with self.assertRaisesOpError(\"Invalid number of columns\"):\n        table = self.getHashTable()(\n            init, default_value, experimental_is_anonymous=is_anonymous)\n        self.initialize_table(table)\n\n  def testInitializeSameTableWithMultipleNodes(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    vocabulary_file = self._createVocabFile(\"one_column_5.txt\")\n\n    with self.cached_session():\n      default_value = -1\n      init1 = lookup_ops.TextFileInitializer(\n          vocabulary_file, dtypes.string, lookup_ops.TextFileIndex.WHOLE_LINE,\n          dtypes.int64, lookup_ops.TextFileIndex.LINE_NUMBER)\n      self.assertIn(\"one_column_5.txt_-2_-1\", init1._shared_name)\n      table1 = self.getHashTable()(\n          init1, default_value, experimental_is_anonymous=is_anonymous)\n      init2 = lookup_ops.TextFileInitializer(\n          vocabulary_file, dtypes.string, lookup_ops.TextFileIndex.WHOLE_LINE,\n          dtypes.int64, lookup_ops.TextFileIndex.LINE_NUMBER)\n      self.assertIn(\"one_column_5.txt_-2_-1\", init2._shared_name)\n      table2 = self.getHashTable()(\n          init2, default_value, experimental_is_anonymous=is_anonymous)\n      init3 = lookup_ops.TextFileInitializer(\n          vocabulary_file, dtypes.string, lookup_ops.TextFileIndex.WHOLE_LINE,\n          dtypes.int64, lookup_ops.TextFileIndex.LINE_NUMBER)\n      self.assertIn(\"one_column_5.txt_-2_-1\", init3._shared_name)\n      table3 = self.getHashTable()(\n          init3, default_value, experimental_is_anonymous=is_anonymous)\n\n      self.evaluate(lookup_ops.tables_initializer())\n\n      input_string = constant_op.constant([\"brain\", \"salad\", \"tank\"])\n\n      output1 = table1.lookup(input_string)\n      output2 = table2.lookup(input_string)\n      output3 = table3.lookup(input_string)\n\n      out1, out2, out3 = self.evaluate([output1, output2, output3])\n      self.assertAllEqual([0, 1, -1], out1)\n      self.assertAllEqual([0, 1, -1], out2)\n      self.assertAllEqual([0, 1, -1], out3)\n\n  def testInitializeTableWithNoFilename(self, is_anonymous):\n    with self.cached_session():\n      default_value = -1\n      with self.assertRaises(ValueError):\n        self.getHashTable()(\n            lookup_ops.TextFileInitializer(\n                \"\", dtypes.string, lookup_ops.TextFileIndex.WHOLE_LINE,\n                dtypes.int64, lookup_ops.TextFileIndex.LINE_NUMBER),\n            default_value,\n            experimental_is_anonymous=is_anonymous)\n\n  def testInitializeWithVocabSize(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    with self.cached_session():\n      default_value = -1\n      vocab_size = 3\n      vocabulary_file1 = self._createVocabFile(\"one_column6.txt\")\n      init1 = lookup_ops.TextFileInitializer(\n          vocabulary_file1,\n          dtypes.string,\n          lookup_ops.TextFileIndex.WHOLE_LINE,\n          dtypes.int64,\n          lookup_ops.TextFileIndex.LINE_NUMBER,\n          vocab_size=vocab_size)\n      self.assertIn(\"one_column6.txt_3_-2_-1\", init1._shared_name)\n      table1 = self.getHashTable()(\n          init1, default_value, experimental_is_anonymous=is_anonymous)\n\n      # Initialize from file.\n      self.initialize_table(table1)\n      self.assertEqual(vocab_size, self.evaluate(table1.size()))\n\n      vocabulary_file2 = self._createVocabFile(\"one_column7.txt\")\n      vocab_size = 5\n      init2 = lookup_ops.TextFileInitializer(\n          vocabulary_file2,\n          dtypes.string,\n          lookup_ops.TextFileIndex.WHOLE_LINE,\n          dtypes.int64,\n          lookup_ops.TextFileIndex.LINE_NUMBER,\n          vocab_size=vocab_size)\n      self.assertIn(\"one_column7.txt_5_-2_-1\", init2._shared_name)\n      with self.assertRaisesOpError(\"Invalid vocab_size\"):\n        table2 = self.getHashTable()(\n            init2, default_value, experimental_is_anonymous=is_anonymous)\n        self.initialize_table(table2)\n\n      vocab_size = 1\n      vocabulary_file3 = self._createVocabFile(\"one_column3.txt\")\n      init3 = lookup_ops.TextFileInitializer(\n          vocabulary_file3,\n          dtypes.string,\n          lookup_ops.TextFileIndex.WHOLE_LINE,\n          dtypes.int64,\n          lookup_ops.TextFileIndex.LINE_NUMBER,\n          vocab_size=vocab_size)\n      self.assertIn(\"one_column3.txt_1_-2_-1\", init3._shared_name)\n      table3 = self.getHashTable()(\n          init3, default_value, experimental_is_anonymous=is_anonymous)\n\n      # Smaller vocab size reads only vocab_size records.\n      self.initialize_table(table3)\n      self.assertEqual(vocab_size, self.evaluate(table3.size()))\n\n  @test_util.run_v1_only(\"placeholder usage\")\n  def testFeedVocabularyName(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    vocabulary_file = self._createVocabFile(\"feed_vocabulary.txt\")\n\n    with self.cached_session():\n      default_value = -1\n      init = lookup_ops.TextFileInitializer(\n          \"old_file.txt\", dtypes.string, lookup_ops.TextFileIndex.WHOLE_LINE,\n          dtypes.int64, lookup_ops.TextFileIndex.LINE_NUMBER)\n      self.assertIn(\"old_file.txt_-2_-1\", init._shared_name)\n      table = self.getHashTable()(\n          init, default_value, experimental_is_anonymous=is_anonymous)\n\n      # Initialize with non existing file (old_file.txt) should fail.\n      # TODO(yleon): Update message, which might change per FileSystem.\n      with self.assertRaisesOpError(\"old_file.txt\"):\n        self.evaluate(table.initializer)\n\n      # Initialize the model feeding the vocabulary file.\n      filenames = ops.get_collection(ops.GraphKeys.ASSET_FILEPATHS)\n      table.initializer.run(feed_dict={filenames[0]: vocabulary_file})\n\n      input_string = constant_op.constant([\"brain\", \"salad\", \"tank\"])\n      output = table.lookup(input_string)\n\n      result = self.evaluate(output)\n      self.assertAllEqual([0, 1, -1], result)\n\n  def testInvalidFilenames(self, is_anonymous):\n    vocabulary_file = self._createVocabFile(\"filename_shape.txt\")\n\n    with self.cached_session():\n      default_value = -1\n\n      # Invalid data type\n      other_type = constant_op.constant(1)\n      with self.assertRaises(Exception) as cm:\n        self.getHashTable()(\n            lookup_ops.TextFileInitializer(\n                other_type, dtypes.string, lookup_ops.TextFileIndex.WHOLE_LINE,\n                dtypes.int64, lookup_ops.TextFileIndex.LINE_NUMBER),\n            default_value,\n            experimental_is_anonymous=is_anonymous)\n      self.assertIsInstance(cm.exception, (ValueError, TypeError))\n\n      # Non-scalar filename\n      filenames = constant_op.constant([vocabulary_file, vocabulary_file])\n      if not context.executing_eagerly():\n        with self.assertRaises(Exception) as cm:\n          self.getHashTable()(\n              lookup_ops.TextFileInitializer(\n                  filenames, dtypes.string, lookup_ops.TextFileIndex.WHOLE_LINE,\n                  dtypes.int64, lookup_ops.TextFileIndex.LINE_NUMBER),\n              default_value,\n              experimental_is_anonymous=is_anonymous)\n        self.assertIsInstance(cm.exception, (ValueError, TypeError))\n      else:\n        with self.assertRaises(errors_impl.InvalidArgumentError):\n          self.getHashTable()(\n              lookup_ops.TextFileInitializer(\n                  filenames, dtypes.string, lookup_ops.TextFileIndex.WHOLE_LINE,\n                  dtypes.int64, lookup_ops.TextFileIndex.LINE_NUMBER),\n              default_value,\n              experimental_is_anonymous=is_anonymous)\n\n  def testIdToStringTable(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    vocab_file = self._createVocabFile(\"feat_to_id_1.txt\")\n    with self.cached_session():\n      default_value = \"UNK\"\n      vocab_size = 3\n      init = lookup_ops.TextFileStringTableInitializer(\n          vocab_file, vocab_size=vocab_size)\n      self.assertTrue(\"feat_to_id_1.txt_3_-1_-2\", init._shared_name)\n      table = self.getHashTable()(\n          init, default_value, experimental_is_anonymous=is_anonymous)\n\n      self.initialize_table(table)\n\n      input_values = constant_op.constant([0, 1, 2, 3], dtypes.int64)\n\n      out = table.lookup(input_values)\n      self.assertAllEqual([b\"brain\", b\"salad\", b\"surgery\", b\"UNK\"],\n                          self.evaluate(out))\n      self.assertEqual(vocab_size, self.evaluate(table.size()))\n\n  def testStringToIdTable(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    vocab_file = self._createVocabFile(\"feat_to_id_2.txt\")\n    with self.cached_session():\n      default_value = -1\n      vocab_size = 3\n      init = lookup_ops.TextFileIdTableInitializer(\n          vocab_file, vocab_size=vocab_size)\n      self.assertTrue(\"feat_to_id_2.txt_3_-1_-2\", init._shared_name)\n      table = self.getHashTable()(\n          init, default_value, experimental_is_anonymous=is_anonymous)\n      self.initialize_table(table)\n\n      input_string = constant_op.constant([\"brain\", \"salad\", \"surgery\", \"UNK\"])\n\n      out = table.lookup(input_string)\n      self.assertAllEqual([0, 1, 2, -1], self.evaluate(out))\n      self.assertEqual(vocab_size, self.evaluate(table.size()))\n\n  def testInt64ToIdTable(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    vocab_file = self._createVocabFile(\n        \"feat_to_id_3.txt\", values=(\"42\", \"1\", \"-1000\"))\n    with self.cached_session():\n      default_value = -1\n      vocab_size = 3\n      init = lookup_ops.TextFileIdTableInitializer(\n          vocab_file, vocab_size=vocab_size, key_dtype=dtypes.int64)\n      self.assertTrue(\"feat_to_id_3.txt_3_-1_-2\", init._shared_name)\n      table = self.getHashTable()(\n          init, default_value, experimental_is_anonymous=is_anonymous)\n      self.initialize_table(table)\n\n      out = table.lookup(\n          constant_op.constant((42, 1, -1000, 11), dtype=dtypes.int64))\n      self.assertAllEqual((0, 1, 2, -1), self.evaluate(out))\n      self.assertEqual(vocab_size, self.evaluate(table.size()))\n\n\n@parameterized.named_parameters(\n    (f\"_{is_anonymous}\", is_anonymous) for is_anonymous in [False, True])\nclass StaticVocabularyTableTest(BaseLookupTableTest):\n\n  def _createVocabFile(self, basename, values=(\"brain\", \"salad\", \"surgery\")):\n    vocabulary_file = os.path.join(self.get_temp_dir(), basename)\n    with open(vocabulary_file, \"w\") as f:\n      f.write(\"\\n\".join(values) + \"\\n\")\n    return vocabulary_file\n\n  def testStringStaticVocabularyTable(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    vocab_file = self._createVocabFile(\"feat_to_id_1.txt\")\n    vocab_size = 3\n    oov_buckets = 1\n    table = self.getVocabularyTable()(\n        lookup_ops.TextFileIdTableInitializer(\n            vocab_file, vocab_size=vocab_size),\n        oov_buckets,\n        experimental_is_anonymous=is_anonymous)\n\n    self.initialize_table(table)\n\n    input_string = constant_op.constant([\"brain\", \"salad\", \"surgery\", \"UNK\"])\n\n    out = table.lookup(input_string)\n    self.assertAllEqual([0, 1, 2, 3], self.evaluate(out))\n    self.assertEqual(vocab_size + oov_buckets, self.evaluate(table.size()))\n\n  def testStaticVocabularyTableGetItem(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    vocab_file = self._createVocabFile(\"feat_to_id_1.txt\")\n    vocab_size = 3\n    oov_buckets = 1\n    table = self.getVocabularyTable()(\n        lookup_ops.TextFileIdTableInitializer(\n            vocab_file, vocab_size=vocab_size),\n        oov_buckets,\n        experimental_is_anonymous=is_anonymous)\n\n    self.initialize_table(table)\n\n    input_string = constant_op.constant([\"brain\", \"salad\", \"surgery\", \"UNK\"])\n\n    out = table[input_string]\n    self.assertAllEqual([0, 1, 2, 3], self.evaluate(out))\n    self.assertEqual(vocab_size + oov_buckets, self.evaluate(table.size()))\n\n  def testInt32StaticVocabularyTable(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    vocab_file = self._createVocabFile(\"feat_to_id_2.txt\", (\"42\", \"1\", \"-1000\"))\n    vocab_size = 3\n    oov_buckets = 1\n    table = self.getVocabularyTable()(\n        lookup_ops.TextFileIdTableInitializer(\n            vocab_file, vocab_size=vocab_size, key_dtype=dtypes.int64),\n        oov_buckets,\n        lookup_key_dtype=dtypes.int32,\n        experimental_is_anonymous=is_anonymous)\n\n    self.initialize_table(table)\n\n    values = constant_op.constant((42, 1, -1000, 11), dtype=dtypes.int32)\n\n    out = table.lookup(values)\n    self.assertAllEqual([0, 1, 2, 3], self.evaluate(out))\n    self.assertEqual(vocab_size + oov_buckets, self.evaluate(table.size()))\n\n  def testInt64StaticVocabularyTable(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    vocab_file = self._createVocabFile(\"feat_to_id_3.txt\", (\"42\", \"1\", \"-1000\"))\n    vocab_size = 3\n    oov_buckets = 1\n    table = self.getVocabularyTable()(\n        lookup_ops.TextFileIdTableInitializer(\n            vocab_file, vocab_size=vocab_size, key_dtype=dtypes.int64),\n        oov_buckets,\n        experimental_is_anonymous=is_anonymous)\n\n    self.initialize_table(table)\n\n    values = constant_op.constant((42, 1, -1000, 11), dtype=dtypes.int64)\n\n    out = table.lookup(values)\n    self.assertAllEqual([0, 1, 2, 3], self.evaluate(out))\n    self.assertEqual(vocab_size + oov_buckets, self.evaluate(table.size()))\n\n  def testStringStaticVocabularyTableNoInitializer(self, is_anonymous):\n    oov_buckets = 5\n\n    # Set a table that only uses hash buckets, for each input value returns\n    # an id calculated by fingerprint(\"input\") mod oov_buckets.\n    table = self.getVocabularyTable()(\n        None, oov_buckets, experimental_is_anonymous=is_anonymous)\n    self.initialize_table(table)\n\n    values = constant_op.constant((\"brain\", \"salad\", \"surgery\"))\n\n    out = table.lookup(values)\n    self.assertAllEqual(\n        [\n            3,  # fingerprint(\"brain\") mod 5.\n            1,  # fingerprint(\"salad\") mod 5.\n            4  # fingerprint(\"surgery\") mod 5\n        ],\n        self.evaluate(out))\n    self.assertEqual(oov_buckets, self.evaluate(table.size()))\n\n  def testStaticVocabularyTableWithMultipleInitializers(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    vocab_file = self._createVocabFile(\"feat_to_id_4.txt\")\n    vocab_size = 3\n    oov_buckets = 3\n\n    init = lookup_ops.TextFileIdTableInitializer(\n        vocab_file, vocab_size=vocab_size)\n    table1 = self.getVocabularyTable()(\n        init,\n        oov_buckets,\n        name=\"table1\",\n        experimental_is_anonymous=is_anonymous)\n\n    table2 = self.getVocabularyTable()(\n        init,\n        oov_buckets,\n        name=\"table2\",\n        experimental_is_anonymous=is_anonymous)\n\n    self.evaluate(lookup_ops.tables_initializer())\n\n    input_string = constant_op.constant(\n        [\"fruit\", \"brain\", \"salad\", \"surgery\", \"UNK\"])\n\n    out1 = table1.lookup(input_string)\n    out2 = table2.lookup(input_string)\n\n    out1, out2 = self.evaluate([out1, out2])\n    self.assertAllEqual([5, 0, 1, 2, 5], out1)\n    self.assertAllEqual([5, 0, 1, 2, 5], out2)\n    self.assertEqual(vocab_size + oov_buckets, self.evaluate(table1.size()))\n    self.assertEqual(vocab_size + oov_buckets, self.evaluate(table2.size()))\n\n  def testStaticVocabularyTableInitializationAcrossSessions(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    vocab_file = self._createVocabFile(\"feat_to_id_5.txt\")\n    with self.cached_session():\n      vocab_size = 3\n      oov_buckets = 1\n      table1 = self.getVocabularyTable()(\n          lookup_ops.TextFileIdTableInitializer(\n              vocab_file, vocab_size=vocab_size),\n          oov_buckets,\n          experimental_is_anonymous=is_anonymous)\n\n      self.initialize_table(table1)\n\n      input_string_1 = constant_op.constant(\n          [\"brain\", \"salad\", \"surgery\", \"UNK\"])\n\n      out1 = table1.lookup(input_string_1)\n\n      self.assertAllEqual([0, 1, 2, 3], self.evaluate(out1))\n      self.assertEqual(vocab_size + oov_buckets, self.evaluate(table1.size()))\n\n    with self.cached_session():\n      vocab_size = 3\n      oov_buckets = 1\n\n      # Underlying lookup table already initialized in previous session.\n      # No need to initialize table2\n      table2 = self.getVocabularyTable()(\n          lookup_ops.TextFileIdTableInitializer(\n              vocab_file, vocab_size=vocab_size),\n          oov_buckets,\n          experimental_is_anonymous=is_anonymous)\n\n      input_string_2 = constant_op.constant([\"fruit\", \"salad\", \"UNK\"])\n\n      out2 = table2.lookup(input_string_2)\n\n      self.assertAllEqual([3, 1, 3], self.evaluate(out2))\n      self.assertEqual(vocab_size + oov_buckets, self.evaluate(table2.size()))\n\n  def testStaticVocabularyTableAssetTracking(self, is_anonymous):\n    vocab_file = self._createVocabFile(\"vocab.txt\")\n    vocab_size = 3\n    oov_buckets = 1\n    table = self.getVocabularyTable()(\n        lookup_ops.TextFileIdTableInitializer(\n            vocab_file, vocab_size=vocab_size),\n        oov_buckets,\n        experimental_is_anonymous=is_anonymous)\n    objects = checkpoint_util.list_objects(graph_view.ObjectGraphView(table))\n    assets = list(filter(lambda obj: isinstance(obj, asset.Asset), objects))\n    self.assertLen(assets, 1)\n    self.assertEqual(\n        self.evaluate(assets[0].asset_path), compat.as_bytes(vocab_file))\n\n  def testSparseTensor(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    vocab_file = self._createVocabFile(\"feat_to_id_7.txt\")\n    input_indices = [[0, 0], [0, 1], [2, 0], [2, 2], [3, 0]]\n    input_shape = [4, 4]\n    sp_features = sparse_tensor.SparseTensor(\n        constant_op.constant(input_indices, dtypes.int64),\n        constant_op.constant([\"brain\", \"salad\", \"brain\", \"surgery\", \"tarkus\"],\n                             dtypes.string),\n        constant_op.constant(input_shape, dtypes.int64))\n\n    table = self.getVocabularyTable()(\n        lookup_ops.TextFileIdTableInitializer(vocab_file, vocab_size=3),\n        1,\n        experimental_is_anonymous=is_anonymous)\n    self.initialize_table(table)\n\n    sp_ids = table.lookup(sp_features)\n\n    self.assertAllEqual([5], sp_ids.values._shape_as_list())\n\n    sp_ids_ind, sp_ids_val, sp_ids_shape = self.evaluate(\n        [sp_ids.indices, sp_ids.values, sp_ids.dense_shape])\n\n    self.assertAllEqual(input_indices, sp_ids_ind)\n    self.assertAllEqual([0, 1, 0, 2, 3], sp_ids_val)\n    self.assertAllEqual(input_shape, sp_ids_shape)\n\n  def testRaggedTensor(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    vocab_file = self._createVocabFile(\"feat_to_id_7.txt\")\n    input_row_splits = [0, 2, 4, 5]\n    ragged_features = ragged_tensor.RaggedTensor.from_row_splits(\n        constant_op.constant([\"brain\", \"salad\", \"brain\", \"surgery\", \"tarkus\"],\n                             dtypes.string),\n        constant_op.constant(input_row_splits, dtypes.int64))\n\n    table = self.getVocabularyTable()(\n        lookup_ops.TextFileIdTableInitializer(vocab_file, vocab_size=3),\n        1,\n        experimental_is_anonymous=is_anonymous)\n    self.initialize_table(table)\n\n    ragged_ids = table.lookup(ragged_features)\n\n    self.assertAllEqual([5], ragged_ids.values._shape_as_list())\n\n    ragged_ids_val, ragged_ids_row_splits = self.evaluate(\n        [ragged_ids.values, ragged_ids.row_splits])\n\n    self.assertAllEqual([0, 1, 0, 2, 3], ragged_ids_val)\n    self.assertAllEqual(input_row_splits, ragged_ids_row_splits)\n\n  def testInt32SparseTensor(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    input_indices = [[0, 0], [0, 1], [2, 0], [2, 2], [3, 0]]\n    input_shape = [4, 4]\n    sp_features = sparse_tensor.SparseTensor(\n        constant_op.constant(input_indices, dtypes.int64),\n        constant_op.constant([42, 1, 42, -1000, 11], dtypes.int32),\n        constant_op.constant(input_shape, dtypes.int64))\n\n    table = self.getVocabularyTable()(\n        lookup_ops.KeyValueTensorInitializer((42, 1, -1000), (0, 1, 2),\n                                             dtypes.int64, dtypes.int64),\n        1,\n        lookup_key_dtype=dtypes.int32,\n        experimental_is_anonymous=is_anonymous)\n    self.initialize_table(table)\n\n    sp_ids = table.lookup(sp_features)\n\n    self.assertAllEqual([5], sp_ids.values._shape_as_list())\n\n    sp_ids_ind, sp_ids_val, sp_ids_shape = self.evaluate(\n        [sp_ids.indices, sp_ids.values, sp_ids.dense_shape])\n\n    self.assertAllEqual(input_indices, sp_ids_ind)\n    self.assertAllEqual([0, 1, 0, 2, 3], sp_ids_val)\n    self.assertAllEqual(input_shape, sp_ids_shape)\n\n  def testInt32RaggedTensor(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    input_row_splits = [0, 2, 4, 5]\n    ragged_features = ragged_tensor.RaggedTensor.from_row_splits(\n        constant_op.constant([42, 1, 42, -1000, 11], dtypes.int32),\n        constant_op.constant(input_row_splits, dtypes.int64))\n\n    table = self.getVocabularyTable()(\n        lookup_ops.KeyValueTensorInitializer((42, 1, -1000), (0, 1, 2),\n                                             dtypes.int64, dtypes.int64),\n        1,\n        lookup_key_dtype=dtypes.int32,\n        experimental_is_anonymous=is_anonymous)\n    self.initialize_table(table)\n\n    ragged_ids = table.lookup(ragged_features)\n\n    self.assertAllEqual([5], ragged_ids.values._shape_as_list())\n\n    ragged_ids_val, ragged_ids_row_splits = self.evaluate(\n        [ragged_ids.values, ragged_ids.row_splits])\n\n    self.assertAllEqual([0, 1, 0, 2, 3], ragged_ids_val)\n    self.assertAllEqual(input_row_splits, ragged_ids_row_splits)\n\n  def testInt64SparseTensor(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    input_indices = [[0, 0], [0, 1], [2, 0], [2, 2], [3, 0]]\n    input_shape = [4, 4]\n    sp_features = sparse_tensor.SparseTensor(\n        constant_op.constant(input_indices, dtypes.int64),\n        constant_op.constant([42, 1, 42, -1000, 11], dtypes.int64),\n        constant_op.constant(input_shape, dtypes.int64))\n\n    table = self.getVocabularyTable()(\n        lookup_ops.KeyValueTensorInitializer((42, 1, -1000), (0, 1, 2),\n                                             dtypes.int64, dtypes.int64),\n        1,\n        experimental_is_anonymous=is_anonymous)\n    self.initialize_table(table)\n\n    sp_ids = table.lookup(sp_features)\n\n    self.assertAllEqual([5], sp_ids.values._shape_as_list())\n\n    sp_ids_ind, sp_ids_val, sp_ids_shape = self.evaluate(\n        [sp_ids.indices, sp_ids.values, sp_ids.dense_shape])\n\n    self.assertAllEqual(input_indices, sp_ids_ind)\n    self.assertAllEqual([0, 1, 0, 2, 3], sp_ids_val)\n    self.assertAllEqual(input_shape, sp_ids_shape)\n\n  def testInt64RaggedTensor(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    input_row_splits = [0, 2, 4, 5]\n    ragged_features = ragged_tensor.RaggedTensor.from_row_splits(\n        constant_op.constant([42, 1, 42, -1000, 11], dtypes.int64),\n        constant_op.constant(input_row_splits, dtypes.int64))\n\n    table = self.getVocabularyTable()(\n        lookup_ops.KeyValueTensorInitializer((42, 1, -1000), (0, 1, 2),\n                                             dtypes.int64, dtypes.int64),\n        1,\n        experimental_is_anonymous=is_anonymous)\n    self.initialize_table(table)\n\n    ragged_ids = table.lookup(ragged_features)\n\n    self.assertAllEqual([5], ragged_ids.values._shape_as_list())\n\n    ragged_ids_val, ragged_ids_row_splits = self.evaluate(\n        [ragged_ids.values, ragged_ids.row_splits])\n\n    self.assertAllEqual([0, 1, 0, 2, 3], ragged_ids_val)\n    self.assertAllEqual(input_row_splits, ragged_ids_row_splits)\n\n  def testStaticVocabularyTableNoInnerTable(self, is_anonymous):\n    table = self.getVocabularyTable()(\n        None, num_oov_buckets=1, experimental_is_anonymous=is_anonymous)\n    self.assertIsNone(table.resource_handle)\n\n  @test_util.run_v2_only\n  def testSavedModelSaveRestore(self, is_anonymous):\n    save_dir = os.path.join(self.get_temp_dir(), \"save_restore\")\n    save_path = os.path.join(tempfile.mkdtemp(prefix=save_dir), \"hash\")\n\n    root = autotrackable.AutoTrackable()\n\n    vocab_file = self._createVocabFile(\"feat_to_id_3.txt\", (\"11\", \"12\", \"13\"))\n    vocab_size = 3\n    oov_buckets = 1\n    root.table = self.getVocabularyTable()(\n        lookup_ops.TextFileIdTableInitializer(\n            vocab_file, vocab_size=vocab_size, key_dtype=dtypes.int64),\n        oov_buckets,\n        experimental_is_anonymous=is_anonymous)\n\n    @def_function.function(\n        input_signature=[tensor_spec.TensorSpec((), dtypes.int64)])\n    def lookup(key):\n      return root.table.lookup(key)\n\n    @def_function.function(input_signature=[])\n    def size():\n      return root.table.size()\n\n    @def_function.function(input_signature=[])\n    def is_ref_counting():\n      return test_ops.is_resource_handle_ref_counting(\n          root.table.resource_handle)\n\n    root.lookup = lookup\n    root.size = size\n    root.is_ref_counting = is_ref_counting\n\n    self.assertEqual(root.table.size(), 4)\n    self.assertEqual(root.lookup(12), 1)\n    self.assertEqual(root.lookup(10), 3)\n    self.assertEqual(root.is_ref_counting(), is_anonymous)\n\n    saved_model_save.save(root, save_path)\n\n    del root\n    loaded = saved_model_load.load(save_path)\n    self.assertEqual(loaded.size(), 4)\n    self.assertEqual(loaded.lookup(12), 1)\n    self.assertEqual(loaded.lookup(10), 3)\n    self.assertEqual(loaded.is_ref_counting(), is_anonymous)\n\n\n@parameterized.named_parameters(\n    (f\"_{is_anonymous}\", is_anonymous) for is_anonymous in [False, True])\nclass DenseHashTableOpTest(test.TestCase):\n\n  def testBasic(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    keys = constant_op.constant([11, 12, 13, 14], dtypes.int64)\n    values = constant_op.constant([0, 1, 2, 3], dtypes.int64)\n    table = lookup_ops.DenseHashTable(\n        dtypes.int64,\n        dtypes.int64,\n        default_value=-1,\n        empty_key=0,\n        deleted_key=-1,\n        experimental_is_anonymous=is_anonymous)\n    self.assertAllEqual(0, self.evaluate(table.size()))\n\n    self.evaluate(table.insert(keys, values))\n    self.assertAllEqual(4, self.evaluate(table.size()))\n\n    remove_string = constant_op.constant([12, 15], dtypes.int64)\n    self.evaluate(table.remove(remove_string))\n    self.assertAllEqual(3, self.evaluate(table.size()))\n\n    input_string = constant_op.constant([11, 12, 15], dtypes.int64)\n    output = table.lookup(input_string)\n    self.assertAllEqual([3], output.get_shape())\n\n    result = self.evaluate(output)\n    self.assertAllEqual([0, -1, -1], result)\n\n  def testGetItem(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    keys = constant_op.constant([11, 12, 13, 14], dtypes.int64)\n    values = constant_op.constant([0, 1, 2, 3], dtypes.int64)\n    table = lookup_ops.DenseHashTable(\n        dtypes.int64,\n        dtypes.int64,\n        default_value=-1,\n        empty_key=0,\n        deleted_key=-1,\n        experimental_is_anonymous=is_anonymous)\n\n    self.evaluate(table.insert(keys, values))\n\n    input_string = constant_op.constant([11, 12, 15], dtypes.int64)\n    output = table[input_string]\n    self.assertAllEqual([3], output.get_shape())\n\n    result = self.evaluate(output)\n    self.assertAllEqual([0, 1, -1], result)\n\n  def testBasicBool(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    keys = constant_op.constant([11, 12, 13, 14], dtypes.int64)\n    values = constant_op.constant([True, True, True, True], dtypes.bool)\n    table = lookup_ops.DenseHashTable(\n        dtypes.int64,\n        dtypes.bool,\n        default_value=False,\n        empty_key=0,\n        deleted_key=-1,\n        experimental_is_anonymous=is_anonymous)\n    self.assertAllEqual(0, self.evaluate(table.size()))\n\n    self.evaluate(table.insert(keys, values))\n    self.assertAllEqual(4, self.evaluate(table.size()))\n\n    remove_string = constant_op.constant([11, 15], dtypes.int64)\n    self.evaluate(table.remove(remove_string))\n    self.assertAllEqual(3, self.evaluate(table.size()))\n\n    input_string = constant_op.constant([11, 12, 15], dtypes.int64)\n    output = table.lookup(input_string)\n    self.assertAllEqual([3], output.get_shape())\n\n    result = self.evaluate(output)\n    self.assertAllEqual([False, True, False], result)\n\n  def testSameEmptyAndDeletedKey(self, is_anonymous):\n    with self.assertRaisesRegex(errors_impl.InvalidArgumentError,\n                                \"Empty and deleted keys\"):\n      table = lookup_ops.DenseHashTable(\n          dtypes.int64,\n          dtypes.int64,\n          default_value=-1,\n          empty_key=42,\n          deleted_key=42,\n          experimental_is_anonymous=is_anonymous)\n      self.assertAllEqual(0, self.evaluate(table.size()))\n\n  @test_util.run_v1_only(\"uses placeholders\")\n  def testLookupUnknownShape(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    with self.cached_session():\n      keys = constant_op.constant([11, 12, 13], dtypes.int64)\n      values = constant_op.constant([0, 1, 2], dtypes.int64)\n      table = lookup_ops.DenseHashTable(\n          dtypes.int64,\n          dtypes.int64,\n          default_value=-1,\n          empty_key=0,\n          deleted_key=-1,\n          experimental_is_anonymous=is_anonymous)\n\n      self.evaluate(table.insert(keys, values))\n      self.assertAllEqual(3, self.evaluate(table.size()))\n\n      placeholder_keys = array_ops.placeholder(dtypes.int64)\n      output = table.lookup(placeholder_keys)\n      self.assertAllEqual(None, output.get_shape())\n      result = output.eval({placeholder_keys: [11, 12, 15]})\n      self.assertAllEqual([0, 1, -1], result)\n\n  def testMapStringToFloat(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    keys = constant_op.constant([\"a\", \"b\", \"c\", \"d\"], dtypes.string)\n    values = constant_op.constant([0.0, 1.1, 2.2, 3.3], dtypes.float32)\n    default_value = constant_op.constant(-1.5, dtypes.float32)\n    table = lookup_ops.DenseHashTable(\n        dtypes.string,\n        dtypes.float32,\n        default_value=default_value,\n        empty_key=\"\",\n        deleted_key=\"$\",\n        experimental_is_anonymous=is_anonymous)\n    self.assertAllEqual(0, self.evaluate(table.size()))\n\n    self.evaluate(table.insert(keys, values))\n    self.assertAllEqual(4, self.evaluate(table.size()))\n\n    remove_string = constant_op.constant([\"b\", \"e\"])\n    self.evaluate(table.remove(remove_string))\n    self.assertAllEqual(3, self.evaluate(table.size()))\n\n    input_string = constant_op.constant([\"a\", \"b\", \"d\", \"e\"], dtypes.string)\n    output = table.lookup(input_string)\n    self.assertAllEqual([4], output.get_shape())\n\n    result = self.evaluate(output)\n    self.assertAllClose([0, -1.5, 3.3, -1.5], result)\n\n  def testMapInt64ToFloat(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    for float_dtype in [dtypes.float32, dtypes.float64]:\n      keys = constant_op.constant([11, 12, 13, 14], dtypes.int64)\n      values = constant_op.constant([0.0, 1.1, 2.2, 3.3], float_dtype)\n      default_value = constant_op.constant(-1.5, float_dtype)\n      table = lookup_ops.DenseHashTable(\n          dtypes.int64,\n          float_dtype,\n          default_value=default_value,\n          empty_key=0,\n          deleted_key=-1,\n          experimental_is_anonymous=is_anonymous)\n      self.assertAllEqual(0, self.evaluate(table.size()))\n\n      self.evaluate(table.insert(keys, values))\n      self.assertAllEqual(4, self.evaluate(table.size()))\n\n      remove_string = constant_op.constant([12, 15], dtypes.int64)\n      self.evaluate(table.remove(remove_string))\n      self.assertAllEqual(3, self.evaluate(table.size()))\n\n      input_string = constant_op.constant([11, 12, 14, 15], dtypes.int64)\n      output = table.lookup(input_string)\n      self.assertAllEqual([4], output.get_shape())\n\n      result = self.evaluate(output)\n      self.assertAllClose([0, -1.5, 3.3, -1.5], result)\n\n  def testVectorValues(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    keys = constant_op.constant([11, 12, 13], dtypes.int64)\n    values = constant_op.constant([[0, 1, 2, 3], [3, 4, 5, 6], [6, 7, 8, 9]],\n                                  dtypes.int64)\n    default_value = constant_op.constant([-1, -2, -3, -4], dtypes.int64)\n    table = lookup_ops.DenseHashTable(\n        dtypes.int64,\n        dtypes.int64,\n        default_value=default_value,\n        empty_key=0,\n        deleted_key=-1,\n        initial_num_buckets=4,\n        experimental_is_anonymous=is_anonymous)\n    self.assertAllEqual(0, self.evaluate(table.size()))\n\n    self.evaluate(table.insert(keys, values))\n    self.assertAllEqual(3, self.evaluate(table.size()))\n    self.assertAllEqual(4, len(self.evaluate(table.export()[0])))\n\n    self.evaluate(\n        table.insert(\n            constant_op.constant([14], dtypes.int64),\n            constant_op.constant([[2, 3, 4, 5]], dtypes.int64)))\n    self.assertAllEqual(4, self.evaluate(table.size()))\n    self.assertAllEqual(8, len(self.evaluate(table.export()[0])))\n\n    remove_string = constant_op.constant([12, 16], dtypes.int64)\n    self.evaluate(table.remove(remove_string))\n    self.assertAllEqual(3, self.evaluate(table.size()))\n    self.assertAllEqual(8, len(self.evaluate(table.export()[0])))\n\n    input_string = constant_op.constant([11, 12, 14, 15], dtypes.int64)\n    output = table.lookup(input_string)\n    self.assertAllEqual([4, 4],\n                        output.shape,\n                        msg=\"Saw shape: %s\" % output.shape)\n\n    result = self.evaluate(output)\n    self.assertAllEqual(\n        [[0, 1, 2, 3], [-1, -2, -3, -4], [2, 3, 4, 5], [-1, -2, -3, -4]],\n        result)\n\n  def testVectorKeys(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    keys = constant_op.constant([[0, 1], [1, 2], [1, 3]], dtypes.int64)\n    values = constant_op.constant([10, 11, 12], dtypes.int64)\n    empty_key = constant_op.constant([0, 3], dtypes.int64)\n    deleted_key = constant_op.constant([-1, -1], dtypes.int64)\n    default_value = constant_op.constant(-1, dtypes.int64)\n    table = lookup_ops.DenseHashTable(\n        dtypes.int64,\n        dtypes.int64,\n        default_value=default_value,\n        empty_key=empty_key,\n        deleted_key=deleted_key,\n        initial_num_buckets=8,\n        experimental_is_anonymous=is_anonymous)\n    self.assertAllEqual(0, self.evaluate(table.size()))\n\n    self.evaluate(table.insert(keys, values))\n    self.assertAllEqual(3, self.evaluate(table.size()))\n\n    self.evaluate(\n        table.insert(\n            constant_op.constant([[0, 0]], dtypes.int64),\n            constant_op.constant([13], dtypes.int64)))\n    self.assertAllEqual(4, self.evaluate(table.size()))\n    self.assertAllEqual(8, len(self.evaluate(table.export()[0])))\n\n    remove_string = constant_op.constant([[1, 2], [7, 8]], dtypes.int64)\n    self.evaluate(table.remove(remove_string))\n    self.assertAllEqual(3, self.evaluate(table.size()))\n    self.assertAllEqual(8, len(self.evaluate(table.export()[0])))\n\n    input_string = constant_op.constant([[0, 1], [1, 2], [1, 3], [0, 2]],\n                                        dtypes.int64)\n    output = table.lookup(input_string)\n    self.assertAllEqual([4], output.get_shape())\n\n    result = self.evaluate(output)\n    self.assertAllEqual([10, -1, 12, -1], result)\n\n  def testResize(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    keys = constant_op.constant([11, 12, 13], dtypes.int64)\n    values = constant_op.constant([0, 1, 2], dtypes.int64)\n    table = lookup_ops.DenseHashTable(\n        dtypes.int64,\n        dtypes.int64,\n        default_value=-1,\n        empty_key=0,\n        deleted_key=-1,\n        initial_num_buckets=4,\n        experimental_is_anonymous=is_anonymous)\n    self.assertAllEqual(0, self.evaluate(table.size()))\n\n    self.evaluate(table.insert(keys, values))\n    self.assertAllEqual(3, self.evaluate(table.size()))\n    self.assertAllEqual(4, len(self.evaluate(table.export()[0])))\n\n    keys2 = constant_op.constant([12, 99], dtypes.int64)\n    self.evaluate(table.remove(keys2))\n    self.assertAllEqual(2, self.evaluate(table.size()))\n    self.assertAllEqual(4, len(self.evaluate(table.export()[0])))\n\n    keys3 = constant_op.constant([13, 14, 15, 16, 17], dtypes.int64)\n    values3 = constant_op.constant([3, 4, 5, 6, 7], dtypes.int64)\n\n    self.evaluate(table.insert(keys3, values3))\n    self.assertAllEqual(6, self.evaluate(table.size()))\n    self.assertAllEqual(16, len(self.evaluate(table.export()[0])))\n\n    keys4 = constant_op.constant([10, 11, 12, 13, 14, 15, 16, 17, 18],\n                                 dtypes.int64)\n    output = table.lookup(keys4)\n    self.assertAllEqual([-1, 0, -1, 3, 4, 5, 6, 7, -1], self.evaluate(output))\n\n  def testExport(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    keys = constant_op.constant([11, 12, 13, 14], dtypes.int64)\n    values = constant_op.constant([1, 2, 3, 4], dtypes.int64)\n    table = lookup_ops.DenseHashTable(\n        dtypes.int64,\n        dtypes.int64,\n        default_value=-1,\n        empty_key=100,\n        deleted_key=200,\n        initial_num_buckets=8,\n        experimental_is_anonymous=is_anonymous)\n    self.assertAllEqual(0, self.evaluate(table.size()))\n\n    self.evaluate(table.insert(keys, values))\n    self.assertAllEqual(4, self.evaluate(table.size()))\n\n    keys2 = constant_op.constant([12, 15], dtypes.int64)\n    self.evaluate(table.remove(keys2))\n    self.assertAllEqual(3, self.evaluate(table.size()))\n\n    exported_keys, exported_values = table.export()\n\n    np_keys = self.evaluate(exported_keys)\n    np_values = self.evaluate(exported_values)\n\n    self.assertAllEqual(8, len(np_keys))\n    self.assertAllEqual(8, len(np_values))\n\n    # pair up keys and values, drop extra added dimension\n    pairs = np.dstack((np_keys.flatten(), np_values.flatten()))[0]\n    # sort by key\n    pairs = pairs[pairs[:, 0].argsort()]\n    self.assertAllEqual([[11, 1], [13, 3], [14, 4], [100, 0], [100, 0],\n                         [100, 0], [100, 0], [200, 2]], pairs)\n\n  @test_util.run_v1_only(\"Saver V1 only\")\n  def testSaveRestore(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    save_dir = os.path.join(self.get_temp_dir(), \"save_restore\")\n    save_path = os.path.join(tempfile.mkdtemp(prefix=save_dir), \"hash\")\n\n    with self.session(graph=ops.Graph()) as sess:\n      default_value = -1\n      empty_key = 0\n      deleted_key = -1\n      keys = constant_op.constant([11, 12, 13, 14], dtypes.int64)\n      values = constant_op.constant([0, 1, 2, 3], dtypes.int64)\n      table = lookup_ops.DenseHashTable(\n          dtypes.int64,\n          dtypes.int64,\n          default_value=default_value,\n          empty_key=empty_key,\n          deleted_key=deleted_key,\n          name=\"t1\",\n          checkpoint=True,\n          initial_num_buckets=32,\n          experimental_is_anonymous=is_anonymous)\n\n      save = saver.Saver()\n\n      self.assertAllEqual(0, table.size())\n      table.insert(keys, values).run()\n      self.assertAllEqual(4, table.size())\n      self.assertAllEqual(32, len(table.export()[0].eval()))\n\n      keys2 = constant_op.constant([12, 15], dtypes.int64)\n      table.remove(keys2).run()\n      self.assertAllEqual(3, table.size())\n      self.assertAllEqual(32, len(table.export()[0].eval()))\n\n      val = save.save(sess, save_path)\n      self.assertIsInstance(val, str)\n      self.assertEqual(save_path, val)\n\n    with self.session(graph=ops.Graph()) as sess:\n      table = lookup_ops.DenseHashTable(\n          dtypes.int64,\n          dtypes.int64,\n          default_value=default_value,\n          empty_key=empty_key,\n          deleted_key=deleted_key,\n          name=\"t1\",\n          checkpoint=True,\n          initial_num_buckets=64,\n          experimental_is_anonymous=is_anonymous)\n      table.insert(\n          constant_op.constant([11, 14], dtypes.int64),\n          constant_op.constant([12, 24], dtypes.int64)).run()\n      self.assertAllEqual(2, table.size())\n      self.assertAllEqual(64, len(table.export()[0].eval()))\n\n      save = saver.Saver()\n\n      # Restore the saved values in the parameter nodes.\n      save.restore(sess, save_path)\n\n      self.assertAllEqual(3, table.size())\n      self.assertAllEqual(32, len(table.export()[0].eval()))\n\n      input_string = constant_op.constant([10, 11, 12, 13, 14], dtypes.int64)\n      output = table.lookup(input_string)\n      self.assertAllEqual([-1, 0, -1, 2, 3], output)\n\n  @test_util.run_v1_only(\"Saver V1 only\")\n  def testSaveRestoreOnlyTable(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    save_dir = os.path.join(self.get_temp_dir(), \"save_restore\")\n    save_path = os.path.join(tempfile.mkdtemp(prefix=save_dir), \"hash\")\n\n    with self.session(graph=ops.Graph()) as sess:\n      default_value = -1\n      empty_key = 0\n      deleted_key = -1\n      keys = constant_op.constant([11, 12, 13, 14], dtypes.int64)\n      values = constant_op.constant([0, 1, 2, 3], dtypes.int64)\n      table = lookup_ops.DenseHashTable(\n          dtypes.int64,\n          dtypes.int64,\n          default_value=default_value,\n          empty_key=empty_key,\n          deleted_key=deleted_key,\n          name=\"t1\",\n          checkpoint=True,\n          initial_num_buckets=32,\n          experimental_is_anonymous=is_anonymous)\n\n      save = saver.Saver([table])\n\n      self.assertAllEqual(0, table.size())\n      table.insert(keys, values).run()\n      self.assertAllEqual(4, table.size())\n      self.assertAllEqual(32, len(table.export()[0].eval()))\n\n      keys2 = constant_op.constant([12, 15], dtypes.int64)\n      table.remove(keys2).run()\n      self.assertAllEqual(3, table.size())\n      self.assertAllEqual(32, len(table.export()[0].eval()))\n\n      val = save.save(sess, save_path)\n      self.assertIsInstance(val, str)\n      self.assertEqual(save_path, val)\n\n    with self.session(graph=ops.Graph()) as sess:\n      table = lookup_ops.DenseHashTable(\n          dtypes.int64,\n          dtypes.int64,\n          default_value=default_value,\n          empty_key=empty_key,\n          deleted_key=deleted_key,\n          name=\"t1\",\n          checkpoint=True,\n          initial_num_buckets=64,\n          experimental_is_anonymous=is_anonymous)\n      table.insert(\n          constant_op.constant([11, 14], dtypes.int64),\n          constant_op.constant([12, 24], dtypes.int64)).run()\n      self.assertAllEqual(2, table.size())\n      self.assertAllEqual(64, len(table.export()[0].eval()))\n\n      save = saver.Saver([table])\n\n      # Restore the saved values in the parameter nodes.\n      save.restore(sess, save_path)\n\n      self.assertAllEqual(3, table.size())\n      self.assertAllEqual(32, len(table.export()[0].eval()))\n\n      input_string = constant_op.constant([10, 11, 12, 13, 14], dtypes.int64)\n      output = table.lookup(input_string)\n      self.assertAllEqual([-1, 0, -1, 2, 3], output)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testObjectSaveRestore(self, is_anonymous):\n    if is_anonymous and not context.executing_eagerly():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    save_dir = os.path.join(self.get_temp_dir(), \"save_restore\")\n    save_prefix = os.path.join(tempfile.mkdtemp(prefix=save_dir), \"hash\")\n\n    default_value = -1\n    empty_key = 0\n    deleted_key = -1\n    keys = constant_op.constant([11, 12, 13], dtypes.int64)\n    values = constant_op.constant([0, 1, 2], dtypes.int64)\n    save_table = lookup_ops.DenseHashTable(\n        dtypes.int64,\n        dtypes.int64,\n        default_value=default_value,\n        empty_key=empty_key,\n        deleted_key=deleted_key,\n        name=\"t1\",\n        checkpoint=True,\n        initial_num_buckets=32,\n        experimental_is_anonymous=is_anonymous)\n\n    save_checkpoint = trackable.Checkpoint(table=save_table)\n\n    self.assertAllEqual(0, self.evaluate(save_table.size()))\n    self.evaluate(save_table.insert(keys, values))\n    self.assertAllEqual(3, self.evaluate(save_table.size()))\n    self.assertAllEqual(32, len(self.evaluate(save_table.export()[0])))\n\n    save_path = save_checkpoint.save(save_prefix)\n    del save_table, save_checkpoint\n\n    load_table = lookup_ops.DenseHashTable(\n        dtypes.int64,\n        dtypes.int64,\n        default_value=default_value,\n        empty_key=empty_key,\n        deleted_key=deleted_key,\n        name=\"t1\",\n        checkpoint=True,\n        initial_num_buckets=64,\n        experimental_is_anonymous=is_anonymous)\n    self.evaluate(\n        load_table.insert(\n            constant_op.constant([11, 14], dtypes.int64),\n            constant_op.constant([12, 24], dtypes.int64)))\n    self.assertAllEqual(2, self.evaluate(load_table.size()))\n    self.assertAllEqual(64, len(self.evaluate(load_table.export()[0])))\n\n    restore_checkpoint = trackable.Checkpoint(table=load_table)\n\n    # Restore the saved values in the parameter nodes.\n    restore_checkpoint.restore(save_path).run_restore_ops()\n\n    self.assertAllEqual(3, self.evaluate(load_table.size()))\n    self.assertAllEqual(32, len(self.evaluate(load_table.export()[0])))\n\n    input_string = constant_op.constant([10, 11, 12, 13, 14], dtypes.int64)\n    output = load_table.lookup(input_string)\n    self.assertAllEqual([-1, 0, 1, 2, -1], self.evaluate(output))\n\n  @test_util.run_v2_only\n  def testSavedModelSaveRestore(self, is_anonymous):\n    save_dir = os.path.join(self.get_temp_dir(), \"save_restore\")\n    save_path = os.path.join(tempfile.mkdtemp(prefix=save_dir), \"hash\")\n\n    root = autotrackable.AutoTrackable()\n\n    default_value = -1\n    empty_key = 0\n    deleted_key = -1\n    keys = constant_op.constant([11, 12, 13], dtypes.int64)\n    values = constant_op.constant([0, 1, 2], dtypes.int64)\n    root.table = lookup_ops.DenseHashTable(\n        dtypes.int64,\n        dtypes.int64,\n        default_value=default_value,\n        empty_key=empty_key,\n        deleted_key=deleted_key,\n        name=\"t1\",\n        checkpoint=True,\n        initial_num_buckets=32,\n        experimental_is_anonymous=is_anonymous)\n\n    @def_function.function(\n        input_signature=[tensor_spec.TensorSpec((), dtypes.int64)])\n    def lookup(key):\n      return root.table.lookup(key)\n\n    @def_function.function(input_signature=[])\n    def size():\n      return root.table.size()\n\n    @def_function.function(input_signature=[])\n    def is_ref_counting():\n      return test_ops.is_resource_handle_ref_counting(\n          root.table.resource_handle)\n\n    root.lookup = lookup\n    root.size = size\n    root.is_ref_counting = is_ref_counting\n\n    self.assertEqual(root.table.size(), 0)\n    root.table.insert(keys, values)\n    self.assertEqual(root.table.size(), 3)\n    self.assertEqual(root.table.lookup(12), 1)\n    self.assertEqual(root.table.lookup(10), -1)\n    self.assertEqual(len(root.table.export()[0]), 32)\n    self.assertEqual(root.is_ref_counting(), is_anonymous)\n\n    saved_model_save.save(root, save_path)\n\n    del root\n    loaded = saved_model_load.load(save_path)\n    self.assertEqual(loaded.size(), 3)\n    self.assertEqual(loaded.lookup(12), 1)\n    self.assertEqual(loaded.lookup(10), -1)\n    self.assertEqual(loaded.is_ref_counting(), is_anonymous)\n\n  @test_util.run_v1_only(\"Saver V1 only\")\n  def testVectorSaveRestore(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    save_dir = os.path.join(self.get_temp_dir(), \"vector_save_restore\")\n    save_path = os.path.join(tempfile.mkdtemp(prefix=save_dir), \"hash\")\n\n    with self.session(graph=ops.Graph()) as sess:\n      empty_key = constant_op.constant([11, 13], dtypes.int64)\n      deleted_key = constant_op.constant([-2, -3], dtypes.int64)\n      default_value = constant_op.constant([-1, -2], dtypes.int64)\n      keys = constant_op.constant([[11, 12], [11, 14], [12, 13], [13, 14]],\n                                  dtypes.int64)\n      values = constant_op.constant([[0, 1], [2, 3], [2, 4], [4, 5]],\n                                    dtypes.int64)\n      table = lookup_ops.DenseHashTable(\n          dtypes.int64,\n          dtypes.int64,\n          default_value=default_value,\n          empty_key=empty_key,\n          deleted_key=deleted_key,\n          name=\"t1\",\n          checkpoint=True,\n          initial_num_buckets=32,\n          experimental_is_anonymous=is_anonymous)\n\n      save = saver.Saver()\n\n      self.assertAllEqual(0, table.size())\n      table.insert(keys, values).run()\n      self.assertAllEqual(4, table.size())\n      self.assertAllEqual(32, len(table.export()[0].eval()))\n\n      keys2 = constant_op.constant([[12, 13], [16, 17]], dtypes.int64)\n      table.remove(keys2).run()\n      self.assertAllEqual(3, table.size())\n      self.assertAllEqual(32, len(table.export()[0].eval()))\n\n      val = save.save(sess, save_path)\n      self.assertIsInstance(val, str)\n      self.assertEqual(save_path, val)\n\n    with self.session(graph=ops.Graph()) as sess:\n      empty_key = constant_op.constant([11, 13], dtypes.int64)\n      deleted_key = constant_op.constant([-2, -3], dtypes.int64)\n      default_value = constant_op.constant([-1, -2], dtypes.int64)\n      table = lookup_ops.DenseHashTable(\n          dtypes.int64,\n          dtypes.int64,\n          default_value=default_value,\n          empty_key=empty_key,\n          deleted_key=deleted_key,\n          name=\"t1\",\n          checkpoint=True,\n          initial_num_buckets=64,\n          experimental_is_anonymous=is_anonymous)\n      table.insert(\n          constant_op.constant([[11, 12], [13, 15]], dtypes.int64),\n          constant_op.constant([[21, 22], [23, 24]], dtypes.int64)).run()\n      self.assertAllEqual(2, table.size())\n      self.assertAllEqual(64, len(table.export()[0].eval()))\n\n      save = saver.Saver()\n\n      # Restore the saved values in the parameter nodes.\n      save.restore(sess, save_path)\n\n      self.assertAllEqual(3, table.size())\n      self.assertAllEqual(32, len(table.export()[0].eval()))\n\n      input_string = constant_op.constant(\n          [[11, 12], [11, 14], [11, 15], [13, 14], [13, 15]], dtypes.int64)\n      output = table.lookup(input_string)\n      self.assertAllEqual([[0, 1], [2, 3], [-1, -2], [4, 5], [-1, -2]],\n                          self.evaluate(output))\n\n  @test_util.run_v1_only(\"Saver V1 only\")\n  def testVectorScalarSaveRestore(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    save_dir = os.path.join(self.get_temp_dir(), \"vector_scalar_save_restore\")\n    save_path = os.path.join(tempfile.mkdtemp(prefix=save_dir), \"hash\")\n\n    with self.session(graph=ops.Graph()) as sess:\n      empty_key = constant_op.constant([11, 13], dtypes.int64)\n      deleted_key = constant_op.constant([-1, -1], dtypes.int64)\n      default_value = constant_op.constant(-1, dtypes.int64)\n      keys = constant_op.constant([[11, 12], [11, 14], [12, 13], [13, 14]],\n                                  dtypes.int64)\n      values = constant_op.constant([0, 1, 2, 3], dtypes.int64)\n      table = lookup_ops.DenseHashTable(\n          dtypes.int64,\n          dtypes.int64,\n          default_value=default_value,\n          empty_key=empty_key,\n          deleted_key=deleted_key,\n          name=\"t2\",\n          checkpoint=True,\n          initial_num_buckets=32,\n          experimental_is_anonymous=is_anonymous)\n\n      save = saver.Saver()\n\n      self.assertAllEqual(0, table.size())\n      table.insert(keys, values).run()\n      self.assertAllEqual(4, table.size())\n      self.assertAllEqual(32, len(table.export()[0].eval()))\n\n      keys2 = constant_op.constant([[12, 13], [15, 16]], dtypes.int64)\n      table.remove(keys2).run()\n      self.assertAllEqual(3, table.size())\n      self.assertAllEqual(32, len(table.export()[0].eval()))\n\n      val = save.save(sess, save_path)\n      self.assertIsInstance(val, str)\n      self.assertEqual(save_path, val)\n\n    with self.session(graph=ops.Graph()) as sess:\n      empty_key = constant_op.constant([11, 13], dtypes.int64)\n      deleted_key = constant_op.constant([-1, -1], dtypes.int64)\n      default_value = constant_op.constant(-1, dtypes.int64)\n      table = lookup_ops.DenseHashTable(\n          dtypes.int64,\n          dtypes.int64,\n          default_value=default_value,\n          empty_key=empty_key,\n          deleted_key=deleted_key,\n          name=\"t2\",\n          checkpoint=True,\n          initial_num_buckets=64,\n          experimental_is_anonymous=is_anonymous)\n      table.insert(\n          constant_op.constant([[11, 12], [13, 15]], dtypes.int64),\n          constant_op.constant([3, 4], dtypes.int64)).run()\n      self.assertAllEqual(2, table.size())\n      self.assertAllEqual(64, len(table.export()[0].eval()))\n\n      save = saver.Saver()\n\n      # Restore the saved values in the parameter nodes.\n      save.restore(sess, save_path)\n\n      self.assertAllEqual(3, table.size())\n      self.assertAllEqual(32, len(table.export()[0].eval()))\n\n      input_string = constant_op.constant(\n          [[11, 12], [11, 14], [11, 15], [13, 14], [13, 15]], dtypes.int64)\n      output = table.lookup(input_string)\n      self.assertAllEqual([0, 1, -1, 3, -1], output)\n\n  def testReprobe(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    # Insert 6 keys into a table with 8 buckets.\n    # The values are chosen to make sure collisions occur when using GCC STL\n    keys = constant_op.constant([11, 12, 13, 19, 20, 21], dtypes.int64)\n    values = constant_op.constant([51, 52, 53, 54, 55, 56], dtypes.int64)\n    table = lookup_ops.DenseHashTable(\n        dtypes.int64,\n        dtypes.int64,\n        default_value=-1,\n        empty_key=0,\n        deleted_key=-1,\n        initial_num_buckets=8,\n        experimental_is_anonymous=is_anonymous)\n    self.assertAllEqual(0, self.evaluate(table.size()))\n\n    self.evaluate(table.insert(keys, values))\n    self.assertAllEqual(6, self.evaluate(table.size()))\n\n    input_string = constant_op.constant([10, 11, 12, 13, 14, 19, 20, 21, 22],\n                                        dtypes.int64)\n    output = table.lookup(input_string)\n    self.assertAllEqual([9], output.get_shape())\n\n    result = self.evaluate(output)\n    self.assertAllEqual([-1, 51, 52, 53, -1, 54, 55, 56, -1], result)\n\n  def testCustomEmptyKey(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    keys = constant_op.constant([11, 0, 13], dtypes.int64)\n    values = constant_op.constant([0, 1, 2], dtypes.int64)\n    table = lookup_ops.DenseHashTable(\n        dtypes.int64,\n        dtypes.int64,\n        default_value=-1,\n        empty_key=12,\n        deleted_key=-1,\n        experimental_is_anonymous=is_anonymous)\n    self.assertAllEqual(0, self.evaluate(table.size()))\n\n    self.evaluate(table.insert(keys, values))\n    self.assertAllEqual(3, self.evaluate(table.size()))\n\n    input_string = constant_op.constant([11, 0, 15], dtypes.int64)\n    output = table.lookup(input_string)\n    self.assertAllEqual([3], output.get_shape())\n\n    result = self.evaluate(output)\n    self.assertAllEqual([0, 1, -1], result)\n\n  def testErrors(self, is_anonymous):\n    table = lookup_ops.DenseHashTable(\n        dtypes.int64,\n        dtypes.int64,\n        default_value=-1,\n        empty_key=0,\n        deleted_key=-1,\n        experimental_is_anonymous=is_anonymous)\n\n    # Inserting the empty key returns an error\n    keys1 = constant_op.constant([11, 0], dtypes.int64)\n    values1 = constant_op.constant([0, 1], dtypes.int64)\n    with self.assertRaisesRegex(errors_impl.InvalidArgumentError,\n                                \"empty_key\"):\n      self.evaluate(table.insert(keys1, values1))\n\n    # Looking up the empty key returns an error\n    with self.assertRaisesRegex(errors_impl.InvalidArgumentError,\n                                \"empty_key\"):\n      self.evaluate(table.lookup(keys1))\n\n    # Inserting the deleted key returns an error\n    keys2 = constant_op.constant([11, -1], dtypes.int64)\n    values2 = constant_op.constant([0, 1], dtypes.int64)\n    with self.assertRaisesRegex(errors_impl.InvalidArgumentError,\n                                \"deleted_key\"):\n      self.evaluate(table.insert(keys2, values2))\n\n    # Looking up the empty key returns an error\n    with self.assertRaisesRegex(errors_impl.InvalidArgumentError,\n                                \"deleted_key\"):\n      self.evaluate(table.lookup(keys2))\n\n    # Arbitrary tensors of keys are not supported\n    keys = constant_op.constant([[11, 0], [12, 1]], dtypes.int64)\n    values = constant_op.constant([[11, 0], [12, 1]], dtypes.int64)\n    with self.assertRaisesRegex(errors_impl.InvalidArgumentError,\n                                \"Expected key shape\"):\n      self.evaluate(table.lookup(keys))\n    with self.assertRaisesRegex(errors_impl.InvalidArgumentError,\n                                \"Expected key shape\"):\n      self.evaluate(table.insert(keys, values))\n\n    with self.assertRaisesRegex(errors_impl.InvalidArgumentError,\n                                \"Number of buckets must be\"):\n      table2 = lookup_ops.DenseHashTable(\n          dtypes.int64,\n          dtypes.int64,\n          default_value=-1,\n          empty_key=17,\n          deleted_key=-1,\n          initial_num_buckets=12,\n          experimental_is_anonymous=is_anonymous)\n      self.assertAllEqual(0, self.evaluate(table2.size()))\n\n    with self.assertRaisesRegex(\n        errors_impl.InvalidArgumentError,\n        \"Empty and deleted keys must have same shape\"):\n      table3 = lookup_ops.DenseHashTable(\n          dtypes.int64,\n          dtypes.int64,\n          default_value=-1,\n          empty_key=42,\n          deleted_key=[1, 2],\n          experimental_is_anonymous=is_anonymous)\n      self.assertAllEqual(0, self.evaluate(table3.size()))\n\n    with self.assertRaisesRegex(errors_impl.InvalidArgumentError,\n                                \"Empty and deleted keys cannot be equal\"):\n      table4 = lookup_ops.DenseHashTable(\n          dtypes.int64,\n          dtypes.int64,\n          default_value=-1,\n          empty_key=42,\n          deleted_key=42,\n          experimental_is_anonymous=is_anonymous)\n      self.assertAllEqual(0, self.evaluate(table4.size()))\n\n    with self.assertRaisesRegex(errors_impl.InvalidArgumentError,\n                                \"Empty and deleted keys cannot be equal\"):\n      table5 = lookup_ops.DenseHashTable(\n          dtypes.int64,\n          dtypes.int64,\n          default_value=-1,\n          empty_key=[1, 2, 3],\n          deleted_key=[1, 2, 3],\n          experimental_is_anonymous=is_anonymous)\n      self.assertAllEqual(0, self.evaluate(table5.size()))\n\n  @test_util.run_in_graph_and_eager_modes\n  def testStringToResource(self, is_anonymous):\n    v = variables.Variable(1.)\n    v1 = variables.Variable(1.)\n    table = lookup_ops.DenseHashTable(\n        dtypes.string,\n        dtypes.resource,\n        default_value=v.handle,\n        empty_key=\"<empty>\",\n        deleted_key=\"<deleted>\",\n        experimental_is_anonymous=is_anonymous)\n    self.assertEqual([], table.lookup(\"not_found\").shape)\n    table.insert(\"v1\", v1.handle)\n    self.assertEqual([], table.lookup(\"v1\").shape)\n\n  def testExportShapeInference(self, is_anonymous):\n    default_value = -1\n    empty_key = 0\n    deleted_key = -1\n    table = lookup_ops.DenseHashTable(\n        dtypes.int64,\n        dtypes.int64,\n        default_value=default_value,\n        empty_key=empty_key,\n        deleted_key=deleted_key,\n        experimental_is_anonymous=is_anonymous)\n    actual_shapes = [t.shape for t in table.export()]\n    inferred_shapes = []\n\n    @def_function.function\n    def f():\n      for t in table.export():\n        inferred_shapes.append(t.shape)\n\n    f()\n    self.assertLen(actual_shapes, 2)\n    self.assertLen(inferred_shapes, 2)\n    self.assertTrue(inferred_shapes[0].is_compatible_with(actual_shapes[0]))\n    self.assertTrue(inferred_shapes[1].is_compatible_with(actual_shapes[1]))\n\n\nclass IndexTableFromFile(test.TestCase):\n\n  def _createVocabFile(self, basename, values=(\"brain\", \"salad\", \"surgery\")):\n    vocabulary_file = os.path.join(self.get_temp_dir(), basename)\n    with open(vocabulary_file, \"w\") as f:\n      f.write(\"\\n\".join(values) + \"\\n\")\n    return vocabulary_file\n\n  def test_string_index_table_from_file(self):\n    vocabulary_file = self._createVocabFile(\"f2i_vocab1.txt\")\n\n    table = lookup_ops.index_table_from_file(\n        vocabulary_file=vocabulary_file, num_oov_buckets=1)\n    ids = table.lookup(constant_op.constant([\"salad\", \"surgery\", \"tarkus\"]))\n\n    if not context.executing_eagerly():\n      with self.assertRaises(errors_impl.OpError):\n        self.evaluate(ids)\n    self.evaluate(lookup_ops.tables_initializer())\n    self.assertAllEqual((1, 2, 3), self.evaluate(ids))\n\n  def test_string_index_table_from_multicolumn_file(self):\n    vocabulary_file = self._createVocabFile(\n        \"f2i_vocab1.txt\", values=(\"brain\\t300\", \"salad\\t20\", \"surgery\\t1\"))\n    table = lookup_ops.index_table_from_file(\n        vocabulary_file=vocabulary_file,\n        num_oov_buckets=1,\n        key_column_index=0,\n        value_column_index=lookup_ops.TextFileIndex.LINE_NUMBER)\n    ids = table.lookup(constant_op.constant([\"salad\", \"surgery\", \"tarkus\"]))\n\n    if not context.executing_eagerly():\n      with self.assertRaises(errors_impl.OpError):\n        self.evaluate(ids)\n    self.evaluate(lookup_ops.tables_initializer())\n    self.assertAllEqual((1, 2, 3), self.evaluate(ids))\n\n  def test_string_index_table_from_multicolumn_file_custom_delimiter(self):\n    vocabulary_file = self._createVocabFile(\n        \"f2i_vocab1.txt\", values=(\"brain 300\", \"salad 20\", \"surgery 1\"))\n    table = lookup_ops.index_table_from_file(\n        vocabulary_file=vocabulary_file,\n        num_oov_buckets=1,\n        key_column_index=0,\n        value_column_index=lookup_ops.TextFileIndex.LINE_NUMBER,\n        delimiter=\" \")\n    ids = table.lookup(constant_op.constant([\"salad\", \"surgery\", \"tarkus\"]))\n\n    if not context.executing_eagerly():\n      with self.assertRaises(errors_impl.OpError):\n        self.evaluate(ids)\n    self.evaluate(lookup_ops.tables_initializer())\n    self.assertAllEqual((1, 2, 3), self.evaluate(ids))\n\n  def test_string_index_table_from_file_tensor_filename(self):\n    vocabulary_file = self._createVocabFile(\"f2i_vocab1.txt\")\n    vocabulary_file = constant_op.constant(vocabulary_file)\n    table = lookup_ops.index_table_from_file(\n        vocabulary_file=vocabulary_file, num_oov_buckets=1)\n    ids = table.lookup(constant_op.constant([\"salad\", \"surgery\", \"tarkus\"]))\n\n    if not context.executing_eagerly():\n      with self.assertRaises(errors_impl.OpError):\n        self.evaluate(ids)\n    self.evaluate(lookup_ops.tables_initializer())\n    self.assertAllEqual((1, 2, 3), self.evaluate(ids))\n    if not context.executing_eagerly():\n      self.assertEqual(1,\n                       len(ops.get_collection(ops.GraphKeys.ASSET_FILEPATHS)))\n\n  @test_util.run_v1_only(\"placeholder usage\")\n  def test_string_index_table_from_file_placeholder_filename(self):\n    vocabulary_file = self._createVocabFile(\"f2i_vocab1.txt\")\n    with self.cached_session():\n      vocabulary_placeholder = array_ops.placeholder(dtypes.string, [])\n      table = lookup_ops.index_table_from_file(\n          vocabulary_file=vocabulary_placeholder, num_oov_buckets=1)\n      ids = table.lookup(constant_op.constant([\"salad\", \"surgery\", \"tarkus\"]))\n\n      with self.assertRaises(errors_impl.OpError):\n        self.evaluate(ids)\n\n      feed_dict = {vocabulary_placeholder.name: vocabulary_file}\n      lookup_ops.tables_initializer().run(feed_dict=feed_dict)\n      self.assertAllEqual((1, 2, 3), self.evaluate(ids))\n      self.assertEqual(0,\n                       len(ops.get_collection(ops.GraphKeys.ASSET_FILEPATHS)))\n\n  def test_int32_index_table_from_file(self):\n    vocabulary_file = self._createVocabFile(\n        \"f2i_vocab2.txt\", values=(\"42\", \"1\", \"-1000\"))\n    table = lookup_ops.index_table_from_file(\n        vocabulary_file=vocabulary_file,\n        num_oov_buckets=1,\n        key_dtype=dtypes.int32)\n    ids = table.lookup(constant_op.constant((1, -1000, 11), dtype=dtypes.int32))\n\n    if not context.executing_eagerly():\n      with self.assertRaises(errors_impl.OpError):\n        self.evaluate(ids)\n    self.evaluate(lookup_ops.tables_initializer())\n    self.assertAllEqual((1, 2, 3), self.evaluate(ids))\n\n  def test_int64_index_table_from_file(self):\n    vocabulary_file = self._createVocabFile(\n        \"f2i_vocab3.txt\", values=(\"42\", \"1\", \"-1000\"))\n    table = lookup_ops.index_table_from_file(\n        vocabulary_file=vocabulary_file,\n        num_oov_buckets=1,\n        key_dtype=dtypes.int64)\n    ids = table.lookup(constant_op.constant((1, -1000, 11), dtype=dtypes.int64))\n\n    if not context.executing_eagerly():\n      with self.assertRaises(errors_impl.OpError):\n        self.evaluate(ids)\n    self.evaluate(lookup_ops.tables_initializer())\n    self.assertAllEqual((1, 2, 3), self.evaluate(ids))\n\n  def test_index_table_from_file_with_default_value(self):\n    default_value = -42\n    vocabulary_file = self._createVocabFile(\"f2i_vocab4.txt\")\n    table = lookup_ops.index_table_from_file(\n        vocabulary_file=vocabulary_file, default_value=default_value)\n    ids = table.lookup(constant_op.constant([\"salad\", \"surgery\", \"tarkus\"]))\n\n    if not context.executing_eagerly():\n      with self.assertRaises(errors_impl.OpError):\n        self.evaluate(ids)\n    self.evaluate(lookup_ops.tables_initializer())\n    self.assertAllEqual((1, 2, default_value), self.evaluate(ids))\n\n  def test_index_table_from_file_with_oov_buckets(self):\n    vocabulary_file = self._createVocabFile(\"f2i_vocab5.txt\")\n    table = lookup_ops.index_table_from_file(\n        vocabulary_file=vocabulary_file, num_oov_buckets=1000)\n    ids = table.lookup(\n        constant_op.constant([\"salad\", \"surgery\", \"tarkus\", \"toccata\"]))\n\n    if not context.executing_eagerly():\n      with self.assertRaises(errors_impl.OpError):\n        self.evaluate(ids)\n    self.evaluate(lookup_ops.tables_initializer())\n    self.assertAllEqual(\n        (\n            1,  # From vocabulary file.\n            2,  # From vocabulary file.\n            867,  # 3 + fingerprint(\"tarkus\") mod 300.\n            860),  # 3 + fingerprint(\"toccata\") mod 300.\n        self.evaluate(ids))\n\n  def test_index_table_from_file_fails_with_empty_vocabulary_file_name(self):\n    self.assertRaises(\n        ValueError, lookup_ops.index_table_from_file, vocabulary_file=\"\")\n\n  def test_index_table_from_file_fails_with_empty_vocabulary(self):\n    self.assertRaises(\n        ValueError, lookup_ops.index_table_from_file, vocabulary_file=None)\n\n  def test_index_table_from_file_str_fails_with_zero_size_vocabulary(self):\n    vocabulary_file = self._createVocabFile(\"zero_vocab_str.txt\")\n    self.assertRaisesRegex(\n        ValueError, \"`vocab_size` must be greater than 0, got 0 for \"\n        \"vocabulary_file: .*zero_vocab_str.txt\",\n        lookup_ops.index_table_from_file,\n        vocabulary_file=vocabulary_file,\n        vocab_size=0)\n\n  def test_index_table_from_file_tensor_fails_with_zero_size_vocabulary(self):\n    vocabulary_file = constant_op.constant(\n        self._createVocabFile(\"zero_vocab_tensor.txt\"))\n    self.assertRaisesRegex(\n        ValueError, \"`vocab_size` must be greater than 0, got 0 for \"\n        \"vocabulary_file: .*zero_vocab_tensor.txt\",\n        lookup_ops.index_table_from_file,\n        vocabulary_file=vocabulary_file,\n        vocab_size=0)\n\n  def test_index_table_from_file_with_vocab_size_too_small(self):\n    vocabulary_file = self._createVocabFile(\"f2i_vocab6.txt\")\n    table = lookup_ops.index_table_from_file(\n        vocabulary_file=vocabulary_file, vocab_size=2)\n    ids = table.lookup(constant_op.constant([\"salad\", \"surgery\", \"tarkus\"]))\n\n    if not context.executing_eagerly():\n      with self.assertRaises(errors_impl.OpError):\n        self.evaluate(ids)\n    self.evaluate(lookup_ops.tables_initializer())\n    self.assertAllEqual((1, -1, -1), self.evaluate(ids))\n    self.assertEqual(2, self.evaluate(table.size()))\n\n  def test_index_table_from_file_with_vocab_size_too_large(self):\n    vocabulary_file = self._createVocabFile(\"f2i_vocab7.txt\")\n    with self.assertRaisesRegex(errors_impl.InvalidArgumentError,\n                                \"Invalid vocab_size\"):\n      table = lookup_ops.index_table_from_file(\n          vocabulary_file=vocabulary_file, vocab_size=4)\n      self.evaluate(table.initializer)\n\n  def test_index_table_from_file_with_vocab_size(self):\n    vocabulary_file = self._createVocabFile(\"f2i_vocab8.txt\")\n\n    self.assertRaises(\n        ValueError,\n        lookup_ops.index_table_from_file,\n        vocabulary_file=vocabulary_file,\n        vocab_size=0)\n\n    table = lookup_ops.index_table_from_file(\n        vocabulary_file=vocabulary_file, vocab_size=3)\n    ids = table.lookup(constant_op.constant([\"salad\", \"surgery\", \"tarkus\"]))\n\n    if not context.executing_eagerly():\n      with self.assertRaises(errors_impl.OpError):\n        self.evaluate(ids)\n    self.evaluate(lookup_ops.tables_initializer())\n    self.assertAllEqual((1, 2, -1), self.evaluate(ids))\n    self.assertEqual(3, self.evaluate(table.size()))\n\n  def test_index_table_from_file_with_invalid_hashers(self):\n    vocabulary_file = self._createVocabFile(\"invalid_hasher.txt\")\n    with self.assertRaises(TypeError):\n      lookup_ops.index_table_from_file(\n          vocabulary_file=vocabulary_file,\n          vocab_size=3,\n          num_oov_buckets=1,\n          hasher_spec=1)\n\n    table = lookup_ops.index_table_from_file(\n        vocabulary_file=vocabulary_file,\n        vocab_size=3,\n        num_oov_buckets=1,\n        hasher_spec=lookup_ops.HasherSpec(\"my-awesome-hash\", None))\n\n    self.assertRaises(ValueError, table.lookup,\n                      constant_op.constant([\"salad\", \"surgery\", \"tarkus\"]))\n\n  def test_index_table_from_file_table_ref_with_oov_buckets(self):\n    vocabulary_file = self._createVocabFile(\"f2i_vocab9.txt\")\n    table = lookup_ops.index_table_from_file(\n        vocabulary_file=vocabulary_file, num_oov_buckets=1)\n    self.assertIsNotNone(table.resource_handle)\n\n  def test_index_table_from_file_table_ref_without_oov_buckets(self):\n    vocabulary_file = self._createVocabFile(\"f2i_vocab10.txt\")\n    table = lookup_ops.index_table_from_file(\n        vocabulary_file=vocabulary_file, num_oov_buckets=0)\n    self.assertIsNotNone(table.resource_handle)\n\n\nclass IndexTableFromTensor(test.TestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_index_table_from_tensor_with_tensor_init(self):\n    table = lookup_ops.index_table_from_tensor(\n        vocabulary_list=(\"brain\", \"salad\", \"surgery\"), num_oov_buckets=1)\n\n    if not context.executing_eagerly():\n      with self.assertRaises(errors_impl.OpError):\n        self.evaluate(\n            table.lookup(constant_op.constant((\"salad\", \"surgery\", \"tarkus\"))))\n    else:\n      # Reinitializing a table in eager should work.\n      table = lookup_ops.index_table_from_tensor(\n          vocabulary_list=(\"brain\", \"salad\", \"surgery\"), num_oov_buckets=1)\n    self.evaluate(lookup_ops.tables_initializer())\n    ids = table.lookup(constant_op.constant((\"salad\", \"surgery\", \"tarkus\")))\n    self.assertAllEqual((1, 2, 3), self.evaluate(ids))\n\n  def test_int32_index_table_from_tensor_with_tensor_init(self):\n    table = lookup_ops.index_table_from_tensor(\n        vocabulary_list=(42, 1, -1000), num_oov_buckets=1, dtype=dtypes.int32)\n    ids = table.lookup(constant_op.constant((1, -1000, 11), dtype=dtypes.int32))\n\n    if not context.executing_eagerly():\n      with self.assertRaises(errors_impl.FailedPreconditionError):\n        self.evaluate(ids)\n    self.evaluate(lookup_ops.tables_initializer())\n    self.assertAllEqual((1, 2, 3), self.evaluate(ids))\n\n  def test_int64_index_table_from_tensor_with_tensor_init(self):\n    table = lookup_ops.index_table_from_tensor(\n        vocabulary_list=(42, 1, -1000), num_oov_buckets=1, dtype=dtypes.int64)\n    ids = table.lookup(constant_op.constant((1, -1000, 11), dtype=dtypes.int64))\n\n    if not context.executing_eagerly():\n      with self.assertRaises(errors_impl.FailedPreconditionError):\n        self.evaluate(ids)\n    self.evaluate(lookup_ops.tables_initializer())\n    self.assertAllEqual((1, 2, 3), self.evaluate(ids))\n\n  def test_index_table_from_tensor_with_default_value(self):\n    default_value = -42\n    table = lookup_ops.index_table_from_tensor(\n        vocabulary_list=[\"brain\", \"salad\", \"surgery\"],\n        default_value=default_value)\n    ids = table.lookup(constant_op.constant([\"salad\", \"surgery\", \"tarkus\"]))\n\n    if not context.executing_eagerly():\n      with self.assertRaises(errors_impl.FailedPreconditionError):\n        self.evaluate(ids)\n    self.evaluate(lookup_ops.tables_initializer())\n    self.assertAllEqual((1, 2, default_value), self.evaluate(ids))\n\n  def test_index_table_from_tensor_missing_vocabulary_list(self):\n    with self.assertRaisesRegex(ValueError,\n                                \"`vocabulary_list` must be specified\"):\n      lookup_ops.index_table_from_tensor(\n          vocabulary_list=None, num_oov_buckets=1)\n\n  def test_index_table_from_tensor_empty_vocabulary_list(self):\n    with self.assertRaisesRegex(errors_impl.OpError,\n                                \"keys and values cannot be empty\"):\n      _ = lookup_ops.index_table_from_tensor(\n          vocabulary_list=np.array([], dtype=np.str_), num_oov_buckets=1)\n      self.evaluate(lookup_ops.tables_initializer())\n\n  def test_index_table_from_tensor_with_invalid_hashers(self):\n    with self.assertRaises(TypeError):\n      lookup_ops.index_table_from_tensor(\n          vocabulary_list=[\"brain\", \"salad\", \"surgery\"],\n          num_oov_buckets=1,\n          hasher_spec=1)\n\n    table = lookup_ops.index_table_from_tensor(\n        vocabulary_list=[\"brain\", \"salad\", \"surgery\"],\n        num_oov_buckets=1,\n        hasher_spec=lookup_ops.HasherSpec(\"my-awesome-hash\", None))\n\n    self.assertRaises(ValueError, table.lookup,\n                      constant_op.constant([\"salad\", \"surgery\", \"tarkus\"]))\n\n\nclass IndexToStringTableFromFileTest(test.TestCase):\n\n  def _createVocabFile(self, basename, values=(\"brain\", \"salad\", \"surgery\")):\n    vocabulary_file = os.path.join(self.get_temp_dir(), basename)\n    with open(vocabulary_file, \"w\") as f:\n      f.write(\"\\n\".join(values) + \"\\n\")\n    return vocabulary_file\n\n  def test_index_to_string_table(self):\n    vocabulary_path = self._createVocabFile(\"i2f_vocab1.txt\")\n    # vocabulary_file supports string and tensor\n    type_funcs = [str, constant_op.constant]\n    for type_func in type_funcs:\n      vocabulary_file = type_func(vocabulary_path)\n      table = lookup_ops.index_to_string_table_from_file(\n          vocabulary_file=vocabulary_file)\n      features = table.lookup(constant_op.constant([0, 1, 2, 3], dtypes.int64))\n      if not context.executing_eagerly():\n        with self.assertRaises(errors_impl.OpError):\n          self.evaluate(features)\n      self.evaluate(lookup_ops.tables_initializer())\n      self.assertAllEqual((b\"brain\", b\"salad\", b\"surgery\", b\"UNK\"),\n                          self.evaluate(features))\n\n  def test_index_to_string_table_from_multicolumn_file(self):\n    vocabulary_file = self._createVocabFile(\n        \"f2i_vocab1.txt\", values=(\"brain\\t300\", \"salad\\t20\", \"surgery\\t1\"))\n    table = lookup_ops.index_to_string_table_from_file(\n        vocabulary_file=vocabulary_file,\n        key_column_index=lookup_ops.TextFileIndex.LINE_NUMBER,\n        value_column_index=0)\n    features = table.lookup(constant_op.constant([0, 1, 2, 3], dtypes.int64))\n    if not context.executing_eagerly():\n      with self.assertRaises(errors_impl.OpError):\n        self.evaluate(features)\n    self.evaluate(lookup_ops.tables_initializer())\n    self.assertAllEqual((b\"brain\", b\"salad\", b\"surgery\", b\"UNK\"),\n                        self.evaluate(features))\n\n  def test_index_to_string_table_from_multicolumn_file_custom_delimiter(self):\n    vocabulary_file = self._createVocabFile(\n        \"f2i_vocab1.txt\", values=(\"brain 300\", \"salad 20\", \"surgery 1\"))\n    table = lookup_ops.index_to_string_table_from_file(\n        vocabulary_file=vocabulary_file,\n        key_column_index=lookup_ops.TextFileIndex.LINE_NUMBER,\n        value_column_index=0,\n        delimiter=\" \")\n    features = table.lookup(constant_op.constant([0, 1, 2, 3], dtypes.int64))\n    if not context.executing_eagerly():\n      with self.assertRaises(errors_impl.OpError):\n        self.evaluate(features)\n    self.evaluate(lookup_ops.tables_initializer())\n    self.assertAllEqual((b\"brain\", b\"salad\", b\"surgery\", b\"UNK\"),\n                        self.evaluate(features))\n\n  def test_index_to_string_table_with_default_value(self):\n    default_value = b\"NONE\"\n    vocabulary_file = self._createVocabFile(\"f2i_vocab2.txt\")\n    table = lookup_ops.index_to_string_table_from_file(\n        vocabulary_file=vocabulary_file, default_value=default_value)\n    features = table.lookup(constant_op.constant([1, 2, 4], dtypes.int64))\n    if not context.executing_eagerly():\n      with self.assertRaises(errors_impl.OpError):\n        self.evaluate(features)\n    self.evaluate(lookup_ops.tables_initializer())\n    self.assertAllEqual((b\"salad\", b\"surgery\", default_value),\n                        self.evaluate(features))\n\n  def test_index_to_string_table_with_vocab_size_too_small(self):\n    default_value = b\"NONE\"\n    vocabulary_file = self._createVocabFile(\"f2i_vocab2.txt\")\n    table = lookup_ops.index_to_string_table_from_file(\n        vocabulary_file=vocabulary_file,\n        vocab_size=2,\n        default_value=default_value)\n    features = table.lookup(constant_op.constant([1, 2, 4], dtypes.int64))\n    if not context.executing_eagerly():\n      with self.assertRaises(errors_impl.OpError):\n        self.evaluate(features)\n    self.evaluate(lookup_ops.tables_initializer())\n    self.assertAllEqual((b\"salad\", default_value, default_value),\n                        self.evaluate(features))\n\n  def test_index_to_string_table_with_vocab_size_too_large(self):\n    vocabulary_file = self._createVocabFile(\"f2i_vocab6.txt\")\n    with self.assertRaisesRegex(errors_impl.InvalidArgumentError,\n                                \"Invalid vocab_size\"):\n      _ = lookup_ops.index_to_string_table_from_file(\n          vocabulary_file=vocabulary_file, vocab_size=4)\n      self.evaluate(lookup_ops.tables_initializer())\n\n  def test_index_to_string_table_with_vocab_size(self):\n    vocabulary_file = self._createVocabFile(\"f2i_vocab7.txt\")\n    table = lookup_ops.index_to_string_table_from_file(\n        vocabulary_file=vocabulary_file, vocab_size=3)\n    features = table.lookup(constant_op.constant([1, 2, 4], dtypes.int64))\n\n    if not context.executing_eagerly():\n      with self.assertRaises(errors_impl.OpError):\n        self.evaluate(features)\n    self.evaluate(lookup_ops.tables_initializer())\n    self.assertAllEqual((b\"salad\", b\"surgery\", b\"UNK\"), self.evaluate(features))\n\n\nclass IndexToStringTableFromTensorTest(test.TestCase):\n\n  def test_index_to_string_table_from_tensor(self):\n    vocabulary_list = constant_op.constant([\"brain\", \"salad\", \"surgery\"])\n    table = lookup_ops.index_to_string_table_from_tensor(\n        vocabulary_list=vocabulary_list)\n\n    indices = constant_op.constant([0, 1, 2, 3], dtypes.int64)\n    features = table.lookup(indices)\n    if not context.executing_eagerly():\n      with self.assertRaises(errors_impl.OpError):\n        self.evaluate(features)\n    self.evaluate(lookup_ops.tables_initializer())\n\n    self.assertAllEqual((b\"brain\", b\"salad\", b\"surgery\", b\"UNK\"),\n                        self.evaluate(features))\n\n  def test_duplicate_entries(self):\n    vocabulary_list = constant_op.constant([\"hello\", \"hello\"])\n    table = lookup_ops.index_to_string_table_from_tensor(\n        vocabulary_list=vocabulary_list)\n    indices = constant_op.constant([0, 1, 4], dtypes.int64)\n    features = table.lookup(indices)\n    self.evaluate(lookup_ops.tables_initializer())\n    self.assertAllEqual((b\"hello\", b\"hello\", b\"UNK\"), self.evaluate(features))\n\n  def test_index_to_string_with_default_value(self):\n    default_value = b\"NONE\"\n    vocabulary_list = constant_op.constant([\"brain\", \"salad\", \"surgery\"])\n    table = lookup_ops.index_to_string_table_from_tensor(\n        vocabulary_list=vocabulary_list, default_value=default_value)\n    indices = constant_op.constant([1, 2, 4], dtypes.int64)\n    features = table.lookup(indices)\n    if not context.executing_eagerly():\n      with self.assertRaises(errors_impl.OpError):\n        self.evaluate(features)\n    self.evaluate(lookup_ops.tables_initializer())\n    self.assertAllEqual((b\"salad\", b\"surgery\", default_value),\n                        self.evaluate(features))\n\n\nclass IdTableWithHashBucketsTest(test.TestCase):\n\n  def _createVocabFile(self, basename, values=(\"brain\", \"salad\", \"surgery\")):\n    vocabulary_file = os.path.join(self.get_temp_dir(), basename)\n    with open(vocabulary_file, \"w\") as f:\n      f.write(\"\\n\".join(values) + \"\\n\")\n    return vocabulary_file\n\n  def testStringIdTableWithHashBuckets(self):\n    vocab_file = self._createVocabFile(\"feat_to_id_1.txt\")\n    default_value = -1\n    vocab_size = 3\n    oov_buckets = 1\n    table = lookup_ops.IdTableWithHashBuckets(\n        lookup_ops.StaticHashTable(\n            lookup_ops.TextFileIdTableInitializer(\n                vocab_file, vocab_size=vocab_size), default_value),\n        oov_buckets)\n\n    self.evaluate(table.initializer)\n\n    input_string = constant_op.constant([\"brain\", \"salad\", \"surgery\", \"UNK\"])\n\n    out = table.lookup(input_string)\n    self.assertAllEqual([0, 1, 2, 3], self.evaluate(out))\n    self.assertEqual(vocab_size + oov_buckets, self.evaluate(table.size()))\n\n  def testInt32IdTableWithHashBuckets(self):\n    vocab_file = self._createVocabFile(\"feat_to_id_2.txt\", (\"42\", \"1\", \"-1000\"))\n    default_value = -1\n    vocab_size = 3\n    oov_buckets = 1\n    table = lookup_ops.IdTableWithHashBuckets(\n        lookup_ops.StaticHashTable(\n            lookup_ops.TextFileIdTableInitializer(\n                vocab_file, vocab_size=vocab_size, key_dtype=dtypes.int64),\n            default_value),\n        oov_buckets,\n        key_dtype=dtypes.int32)\n\n    self.evaluate(table.initializer)\n\n    values = constant_op.constant((42, 1, -1000, 11), dtype=dtypes.int32)\n\n    out = table.lookup(values)\n    self.assertAllEqual([0, 1, 2, 3], self.evaluate(out))\n    self.assertEqual(vocab_size + oov_buckets, self.evaluate(table.size()))\n\n  def testInt64IdTableWithHashBuckets(self):\n    vocab_file = self._createVocabFile(\"feat_to_id_3.txt\", (\"42\", \"1\", \"-1000\"))\n    default_value = -1\n    vocab_size = 3\n    oov_buckets = 1\n    table = lookup_ops.IdTableWithHashBuckets(\n        lookup_ops.StaticHashTable(\n            lookup_ops.TextFileIdTableInitializer(\n                vocab_file, vocab_size=vocab_size, key_dtype=dtypes.int64),\n            default_value), oov_buckets)\n\n    self.evaluate(table.initializer)\n\n    values = constant_op.constant((42, 1, -1000, 11), dtype=dtypes.int64)\n\n    out = table.lookup(values)\n    self.assertAllEqual([0, 1, 2, 3], self.evaluate(out))\n    self.assertEqual(vocab_size + oov_buckets, self.evaluate(table.size()))\n\n  def testStringIdTableWithOnlyHashBucket(self):\n    oov_buckets = 5\n\n    # Set a table that only uses hash buckets, for each input value returns\n    # an id calculated by fingerprint(\"input\") mod oov_buckets.\n    table = lookup_ops.IdTableWithHashBuckets(None, oov_buckets)\n    self.evaluate(table.initializer)\n\n    values = constant_op.constant((\"brain\", \"salad\", \"surgery\"))\n\n    out = table.lookup(values)\n    self.assertAllEqual(\n        [\n            3,  # fingerprint(\"brain\") mod 5.\n            1,  # fingerprint(\"salad\") mod 5.\n            4  # fingerprint(\"surgery\") mod 5\n        ],\n        self.evaluate(out))\n    self.assertEqual(oov_buckets, self.evaluate(table.size()))\n\n  def testInt32IdTableWithOnlyHashBucket(self):\n    oov_buckets = 5\n\n    # Set a table that only uses hash buckets, for each input value returns\n    # an id calculated by fingerprint(\"input\") mod oov_buckets.\n    table = lookup_ops.IdTableWithHashBuckets(\n        None, oov_buckets, key_dtype=dtypes.int32)\n    self.evaluate(table.initializer)\n\n    input_string = constant_op.constant([42, 1, -1000], dtype=dtypes.int32)\n\n    out = table.lookup(input_string)\n    self.assertAllEqual(\n        [\n            1,  # fingerprint(\"42\") mod 5.\n            4,  # fingerprint(\"1\") mod 5.\n            2  # fingerprint(\"-1000\") mod 5\n        ],\n        self.evaluate(out))\n    self.assertEqual(oov_buckets, self.evaluate(table.size()))\n\n  def testFloat64IdTableWithOnlyHashBucket(self):\n    with self.assertRaisesRegex(TypeError, \"Invalid `key_dtype`\"):\n      lookup_ops.IdTableWithHashBuckets(\n          None, num_oov_buckets=5, key_dtype=dtypes.float64)\n\n  def testBoolIdTableWithOnlyHashBucket(self):\n    with self.assertRaisesRegex(TypeError, \"Invalid `key_dtype`\"):\n      lookup_ops.IdTableWithHashBuckets(\n          None, num_oov_buckets=5, key_dtype=dtypes.bool)\n\n  def testIdTableWithHashBucketsWithMultipleInitializers(self):\n    vocab_file = self._createVocabFile(\"feat_to_id_4.txt\")\n    default_value = -1\n    vocab_size = 3\n    oov_buckets = 3\n\n    vocab_table = lookup_ops.StaticHashTable(\n        lookup_ops.TextFileIdTableInitializer(\n            vocab_file, vocab_size=vocab_size), default_value)\n    table1 = lookup_ops.IdTableWithHashBuckets(\n        vocab_table,\n        oov_buckets,\n        hasher_spec=lookup_ops.FastHashSpec,\n        name=\"table1\")\n\n    table2 = lookup_ops.IdTableWithHashBuckets(\n        vocab_table,\n        oov_buckets,\n        hasher_spec=lookup_ops.StrongHashSpec((1, 2)),\n        name=\"table2\")\n\n    self.evaluate(lookup_ops.tables_initializer())\n\n    input_string = constant_op.constant(\n        [\"fruit\", \"brain\", \"salad\", \"surgery\", \"UNK\"])\n\n    out1 = table1.lookup(input_string)\n    out2 = table2.lookup(input_string)\n\n    out1, out2 = self.evaluate([out1, out2])\n    self.assertAllEqual([5, 0, 1, 2, 5], out1)\n    self.assertAllEqual([5, 0, 1, 2, 3], out2)\n    self.assertEqual(vocab_size + oov_buckets, self.evaluate(table1.size()))\n    self.assertEqual(vocab_size + oov_buckets, self.evaluate(table2.size()))\n    if not context.executing_eagerly():\n      test_util.assert_ops_in_graph({\n          \"table1_Lookup/hash_bucket\": \"StringToHashBucketFast\",\n          \"table2_Lookup/hash_bucket\": \"StringToHashBucketStrong\",\n      }, ops.get_default_graph())\n\n  def testIdTableWithHashBucketsInitializationAcrossSessions(self):\n    vocab_file = self._createVocabFile(\"feat_to_id_5.txt\")\n    default_value = -1\n    vocab_size = 3\n    oov_buckets = 1\n    table1 = lookup_ops.IdTableWithHashBuckets(\n        lookup_ops.StaticHashTable(\n            lookup_ops.TextFileIdTableInitializer(\n                vocab_file, vocab_size=vocab_size), default_value), oov_buckets)\n\n    self.evaluate(table1.initializer)\n\n    input_string_1 = constant_op.constant([\"brain\", \"salad\", \"surgery\", \"UNK\"])\n\n    out1 = table1.lookup(input_string_1)\n\n    self.assertAllEqual([0, 1, 2, 3], self.evaluate(out1))\n    self.assertEqual(vocab_size + oov_buckets, self.evaluate(table1.size()))\n\n    default_value = -1\n    vocab_size = 3\n    oov_buckets = 1\n\n    # Underlying lookup table already initialized in previous session.\n    # No need to call self.evaluate(table2.initializer)\n    table2 = lookup_ops.IdTableWithHashBuckets(\n        lookup_ops.StaticHashTable(\n            lookup_ops.TextFileIdTableInitializer(\n                vocab_file, vocab_size=vocab_size), default_value), oov_buckets)\n\n    input_string_2 = constant_op.constant([\"fruit\", \"salad\", \"UNK\"])\n\n    out2 = table2.lookup(input_string_2)\n\n    self.assertAllEqual([3, 1, 3], self.evaluate(out2))\n    self.assertEqual(vocab_size + oov_buckets, self.evaluate(table2.size()))\n\n  def testIdTableWithHashBucketsWithMultipleInitializersDifferentDefault(self):\n    vocab_file = self._createVocabFile(\"feat_to_id_6.txt\")\n    default_value1 = -1\n    vocab_size = 3\n    oov_buckets = 0\n    table1 = lookup_ops.IdTableWithHashBuckets(\n        lookup_ops.StaticHashTable(\n            lookup_ops.TextFileIdTableInitializer(\n                vocab_file, vocab_size=vocab_size), default_value1),\n        oov_buckets)\n\n    default_value2 = -2\n    table2 = lookup_ops.IdTableWithHashBuckets(\n        lookup_ops.StaticHashTable(\n            lookup_ops.TextFileIdTableInitializer(\n                vocab_file, vocab_size=vocab_size), default_value2),\n        oov_buckets)\n\n    self.evaluate(lookup_ops.tables_initializer())\n\n    input_string_1 = constant_op.constant(\n        [\"brain\", \"salad\", \"surgery\", \"UNK\"])\n    input_string_2 = constant_op.constant([\"fruit\", \"salad\", \"UNK\"])\n\n    out1 = table1.lookup(input_string_1)\n    out2 = table2.lookup(input_string_2)\n\n    out1, out2 = self.evaluate([out1, out2])\n    self.assertAllEqual([0, 1, 2, -1], out1)\n    self.assertAllEqual([-2, 1, -2], out2)\n    self.assertEqual(vocab_size + oov_buckets, self.evaluate(table1.size()))\n    self.assertEqual(vocab_size + oov_buckets, self.evaluate(table2.size()))\n\n  def testSparseTensor(self):\n    vocab_file = self._createVocabFile(\"feat_to_id_7.txt\")\n    input_indices = [[0, 0], [0, 1], [2, 0], [2, 2], [3, 0]]\n    input_shape = [4, 4]\n    sp_features = sparse_tensor.SparseTensor(\n        constant_op.constant(input_indices, dtypes.int64),\n        constant_op.constant([\"brain\", \"salad\", \"brain\", \"surgery\", \"tarkus\"],\n                             dtypes.string),\n        constant_op.constant(input_shape, dtypes.int64))\n\n    table = lookup_ops.IdTableWithHashBuckets(\n        lookup_ops.StaticHashTable(\n            lookup_ops.TextFileIdTableInitializer(vocab_file, vocab_size=3),\n            -1), 1)\n    self.evaluate(table.initializer)\n\n    sp_ids = table.lookup(sp_features)\n\n    self.assertAllEqual([5], sp_ids.values._shape_as_list())\n\n    sp_ids_ind, sp_ids_val, sp_ids_shape = self.evaluate(\n        [sp_ids.indices, sp_ids.values, sp_ids.dense_shape])\n\n    self.assertAllEqual(input_indices, sp_ids_ind)\n    self.assertAllEqual([0, 1, 0, 2, 3], sp_ids_val)\n    self.assertAllEqual(input_shape, sp_ids_shape)\n\n  def testRaggedTensor(self):\n    vocab_file = self._createVocabFile(\"feat_to_id_7.txt\")\n    input_row_splits = [0, 2, 4, 5]\n    ragged_features = ragged_tensor.RaggedTensor.from_row_splits(\n        constant_op.constant([\"brain\", \"salad\", \"brain\", \"surgery\", \"tarkus\"],\n                             dtypes.string),\n        constant_op.constant(input_row_splits, dtypes.int64))\n\n    table = lookup_ops.IdTableWithHashBuckets(\n        lookup_ops.StaticHashTable(\n            lookup_ops.TextFileIdTableInitializer(vocab_file, vocab_size=3),\n            -1), 1)\n    self.evaluate(table.initializer)\n\n    ragged_ids = table.lookup(ragged_features)\n    self.assertAllEqual([5], ragged_ids.values._shape_as_list())\n\n    ragged_ids_val, ragged_ids_row_splits = self.evaluate(\n        [ragged_ids.values, ragged_ids.row_splits])\n\n    self.assertAllEqual([0, 1, 0, 2, 3], ragged_ids_val)\n    self.assertAllEqual(input_row_splits, ragged_ids_row_splits)\n\n  def testInt32SparseTensor(self):\n    input_indices = [[0, 0], [0, 1], [2, 0], [2, 2], [3, 0]]\n    input_shape = [4, 4]\n    sp_features = sparse_tensor.SparseTensor(\n        constant_op.constant(input_indices, dtypes.int64),\n        constant_op.constant([42, 1, 42, -1000, 11], dtypes.int32),\n        constant_op.constant(input_shape, dtypes.int64))\n\n    table = lookup_ops.IdTableWithHashBuckets(\n        lookup_ops.StaticHashTable(\n            lookup_ops.KeyValueTensorInitializer(\n                (42, 1, -1000), (0, 1, 2), dtypes.int64, dtypes.int64), -1),\n        1,\n        key_dtype=dtypes.int32)\n    self.evaluate(table.initializer)\n\n    sp_ids = table.lookup(sp_features)\n\n    self.assertAllEqual([5], sp_ids.values._shape_as_list())\n\n    sp_ids_ind, sp_ids_val, sp_ids_shape = self.evaluate(\n        [sp_ids.indices, sp_ids.values, sp_ids.dense_shape])\n\n    self.assertAllEqual(input_indices, sp_ids_ind)\n    self.assertAllEqual([0, 1, 0, 2, 3], sp_ids_val)\n    self.assertAllEqual(input_shape, sp_ids_shape)\n\n  def testInt32RaggedTensor(self):\n    input_row_splits = [0, 2, 4, 5]\n    ragged_features = ragged_tensor.RaggedTensor.from_row_splits(\n        constant_op.constant([42, 1, 42, -1000, 11], dtypes.int32),\n        constant_op.constant(input_row_splits, dtypes.int32))\n\n    table = lookup_ops.IdTableWithHashBuckets(\n        lookup_ops.StaticHashTable(\n            lookup_ops.KeyValueTensorInitializer(\n                (42, 1, -1000), (0, 1, 2), dtypes.int64, dtypes.int64), -1),\n        1,\n        key_dtype=dtypes.int32)\n    self.evaluate(table.initializer)\n\n    ragged_ids = table.lookup(ragged_features)\n\n    self.assertAllEqual([5], ragged_ids.values._shape_as_list())\n\n    ragged_ids_val, ragged_ids_row_splits = self.evaluate(\n        [ragged_ids.values, ragged_ids.row_splits])\n\n    self.assertAllEqual([0, 1, 0, 2, 3], ragged_ids_val)\n    self.assertAllEqual(input_row_splits, ragged_ids_row_splits)\n\n  def testInt64SparseTensor(self):\n    input_indices = [[0, 0], [0, 1], [2, 0], [2, 2], [3, 0]]\n    input_shape = [4, 4]\n    sp_features = sparse_tensor.SparseTensor(\n        constant_op.constant(input_indices, dtypes.int64),\n        constant_op.constant([42, 1, 42, -1000, 11], dtypes.int64),\n        constant_op.constant(input_shape, dtypes.int64))\n\n    table = lookup_ops.IdTableWithHashBuckets(\n        lookup_ops.StaticHashTable(\n            lookup_ops.KeyValueTensorInitializer(\n                (42, 1, -1000), (0, 1, 2), dtypes.int64, dtypes.int64), -1),\n        1,\n        key_dtype=dtypes.int64)\n    self.evaluate(table.initializer)\n\n    sp_ids = table.lookup(sp_features)\n\n    self.assertAllEqual([5], sp_ids.values._shape_as_list())\n\n    sp_ids_ind, sp_ids_val, sp_ids_shape = self.evaluate(\n        [sp_ids.indices, sp_ids.values, sp_ids.dense_shape])\n\n    self.assertAllEqual(input_indices, sp_ids_ind)\n    self.assertAllEqual([0, 1, 0, 2, 3], sp_ids_val)\n    self.assertAllEqual(input_shape, sp_ids_shape)\n\n  def testInt64RaggedTensor(self):\n    input_row_splits = [0, 2, 4, 5]\n    ragged_features = ragged_tensor.RaggedTensor.from_row_splits(\n        constant_op.constant([42, 1, 42, -1000, 11], dtypes.int64),\n        constant_op.constant(input_row_splits, dtypes.int64))\n\n    table = lookup_ops.IdTableWithHashBuckets(\n        lookup_ops.StaticHashTable(\n            lookup_ops.KeyValueTensorInitializer(\n                (42, 1, -1000), (0, 1, 2), dtypes.int64, dtypes.int64), -1),\n        1,\n        key_dtype=dtypes.int64)\n    self.evaluate(table.initializer)\n\n    ragged_ids = table.lookup(ragged_features)\n\n    self.assertAllEqual([5], ragged_ids.values._shape_as_list())\n\n    ragged_ids_val, ragged_ids_row_splits = self.evaluate(\n        [ragged_ids.values, ragged_ids.row_splits])\n\n    self.assertAllEqual([0, 1, 0, 2, 3], ragged_ids_val)\n    self.assertAllEqual(input_row_splits, ragged_ids_row_splits)\n\n  def testIdTableWithHashBucketsWithInvalidHashers(self):\n    vocab_file = self._createVocabFile(\"feat_to_id_4.txt\")\n    default_value = -1\n    vocab_size = 3\n    oov_buckets = 1\n    lookup_table = lookup_ops.StaticHashTable(\n        lookup_ops.TextFileIdTableInitializer(\n            vocab_file, vocab_size=vocab_size), default_value)\n\n    with self.assertRaises(TypeError):\n      lookup_ops.IdTableWithHashBuckets(\n          lookup_table, oov_buckets, hasher_spec=1)\n\n    table = lookup_ops.IdTableWithHashBuckets(\n        lookup_table,\n        oov_buckets,\n        hasher_spec=lookup_ops.HasherSpec(\"my-awesome-hash\", None))\n\n    input_string = constant_op.constant([\"brain\", \"salad\", \"surgery\", \"UNK\"])\n\n    with self.assertRaises(ValueError):\n      table.lookup(input_string)\n\n    with self.assertRaises(ValueError):\n      table = lookup_ops.IdTableWithHashBuckets(\n          lookup_table, oov_buckets, hasher_spec=lookup_ops.StrongHashSpec([]))\n\n    with self.assertRaises(ValueError):\n      table = lookup_ops.IdTableWithHashBuckets(\n          lookup_table,\n          oov_buckets,\n          hasher_spec=lookup_ops.StrongHashSpec([1, 2, 3]))\n\n    with self.assertRaises(TypeError):\n      table = lookup_ops.IdTableWithHashBuckets(\n          lookup_table,\n          oov_buckets,\n          hasher_spec=lookup_ops.StrongHashSpec([None, 2]))\n\n  def testIdTableWithHashBucketsNoInnerTable(self):\n    table = lookup_ops.IdTableWithHashBuckets(None, num_oov_buckets=1)\n    self.assertIsNone(table.resource_handle)\n\n\n@parameterized.named_parameters(\n    (f\"_{is_anonymous}\", is_anonymous) for is_anonymous in [False, True])\nclass MutableHashTableOpTest(test.TestCase):\n\n  def testMutableHashTable(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    default_val = -1\n    keys = constant_op.constant([\"brain\", \"salad\", \"surgery\", \"tarkus\"])\n    values = constant_op.constant([0, 1, 2, 3], dtypes.int64)\n    table = lookup_ops.MutableHashTable(\n        dtypes.string,\n        dtypes.int64,\n        default_val,\n        experimental_is_anonymous=is_anonymous)\n    self.assertAllEqual(0, self.evaluate(table.size()))\n\n    self.evaluate(table.insert(keys, values))\n    self.assertAllEqual(4, self.evaluate(table.size()))\n\n    remove_string = constant_op.constant([\"tarkus\", \"tank\"])\n    self.evaluate(table.remove(remove_string))\n    self.assertAllEqual(3, self.evaluate(table.size()))\n\n    input_string = constant_op.constant([\"brain\", \"salad\", \"tank\"])\n    output = table.lookup(input_string)\n    self.assertAllEqual([3], output.get_shape())\n\n    result = self.evaluate(output)\n    self.assertAllEqual([0, 1, -1], result)\n\n    exported_keys, exported_values = table.export()\n\n    # exported data is in the order of the internal map, i.e. undefined\n    sorted_keys = np.sort(self.evaluate(exported_keys))\n    sorted_values = np.sort(self.evaluate(exported_values))\n    self.assertAllEqual([b\"brain\", b\"salad\", b\"surgery\"], sorted_keys)\n    self.assertAllEqual([0, 1, 2], sorted_values)\n\n  # TODO(https://github.com/tensorflow/tensorflow/issues/24439): remove exepectedFailure when fixed\n  @unittest.expectedFailure\n  @test_util.run_v2_only\n  def testImportedHashTable(self, is_anonymous):\n    g = ops.Graph()\n    with g.as_default():\n      default_val = -1\n      keys = constant_op.constant([\"brain\", \"salad\", \"surgery\", \"tarkus\"])\n      values = constant_op.constant([0, 1, 2, 3], dtypes.int64)\n      table = lookup_ops.MutableHashTable(\n          dtypes.string,\n          dtypes.int64,\n          default_val,\n          experimental_is_anonymous=is_anonymous)\n      self.evaluate(table.insert(keys, values))\n      op = table.lookup(constant_op.constant([\"brain\", \"salad\", \"tank\"]))\n      meta_graph = saver.export_meta_graph()\n\n    def f():\n      saver.import_meta_graph(meta_graph)\n      return ops.get_default_graph().get_tensor_by_name(op.name)\n\n    wrapped = wrap_function.wrap_function(f, [])\n    self.assertAllEqual([0, 1, -1], wrapped())\n\n  @test_util.run_v1_only(\"SaverV1\")\n  def testSaveRestore(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    save_dir = os.path.join(self.get_temp_dir(), \"save_restore\")\n    save_path = os.path.join(tempfile.mkdtemp(prefix=save_dir), \"hash\")\n\n    with self.session(graph=ops.Graph()) as sess:\n      v0 = variables.Variable(10.0, name=\"v0\")\n      v1 = variables.Variable(20.0, name=\"v1\")\n\n      default_val = -1\n      keys = constant_op.constant([\"b\", \"c\", \"d\"], dtypes.string)\n      values = constant_op.constant([0, 1, 2], dtypes.int64)\n      table = lookup_ops.MutableHashTable(\n          dtypes.string,\n          dtypes.int64,\n          default_val,\n          name=\"t1\",\n          checkpoint=True,\n          experimental_is_anonymous=is_anonymous)\n\n      save = saver.Saver()\n      self.evaluate(variables.global_variables_initializer())\n\n      # Check that the parameter nodes have been initialized.\n      self.assertEqual(10.0, self.evaluate(v0))\n      self.assertEqual(20.0, self.evaluate(v1))\n\n      self.assertAllEqual(0, self.evaluate(table.size()))\n      self.evaluate(table.insert(keys, values))\n      self.assertAllEqual(3, self.evaluate(table.size()))\n\n      val = save.save(sess, save_path)\n      self.assertIsInstance(val, str)\n      self.assertEqual(save_path, val)\n\n    with self.session(graph=ops.Graph()) as sess:\n      v0 = variables.Variable(-1.0, name=\"v0\")\n      v1 = variables.Variable(-1.0, name=\"v1\")\n      default_val = -1\n      table = lookup_ops.MutableHashTable(\n          dtypes.string,\n          dtypes.int64,\n          default_val,\n          name=\"t1\",\n          checkpoint=True,\n          experimental_is_anonymous=is_anonymous)\n      self.evaluate(\n          table.insert(\n              constant_op.constant([\"a\", \"c\"], dtypes.string),\n              constant_op.constant([12, 24], dtypes.int64)))\n      self.assertAllEqual(2, self.evaluate(table.size()))\n\n      save = saver.Saver()\n\n      # Restore the saved values in the parameter nodes.\n      save.restore(sess, save_path)\n      # Check that the parameter nodes have been restored.\n      self.assertEqual(10.0, self.evaluate(v0))\n      self.assertEqual(20.0, self.evaluate(v1))\n\n      self.assertAllEqual(3, self.evaluate(table.size()))\n\n      input_string = constant_op.constant([\"a\", \"b\", \"c\", \"d\", \"e\"],\n                                          dtypes.string)\n      output = table.lookup(input_string)\n      self.assertAllEqual([-1, 0, 1, 2, -1], self.evaluate(output))\n\n  @test_util.run_v1_only(\"SaverV1\")\n  def testSaveRestoreOnlyTable(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    save_dir = os.path.join(self.get_temp_dir(), \"save_restore\")\n    save_path = os.path.join(tempfile.mkdtemp(prefix=save_dir), \"hash\")\n\n    with self.session(graph=ops.Graph()) as sess:\n      v0 = variables.Variable(10.0, name=\"v0\")\n      v1 = variables.Variable(20.0, name=\"v1\")\n\n      default_val = -1\n      keys = constant_op.constant([\"b\", \"c\", \"d\"], dtypes.string)\n      values = constant_op.constant([0, 1, 2], dtypes.int64)\n      table = lookup_ops.MutableHashTable(\n          dtypes.string,\n          dtypes.int64,\n          default_val,\n          name=\"t1\",\n          checkpoint=True,\n          experimental_is_anonymous=is_anonymous)\n\n      save = saver.Saver([table])\n      self.evaluate(variables.global_variables_initializer())\n\n      # Check that the parameter nodes have been initialized.\n      self.assertEqual(10.0, self.evaluate(v0))\n      self.assertEqual(20.0, self.evaluate(v1))\n\n      self.assertAllEqual(0, self.evaluate(table.size()))\n      self.evaluate(table.insert(keys, values))\n      self.assertAllEqual(3, self.evaluate(table.size()))\n\n      val = save.save(sess, save_path)\n      self.assertIsInstance(val, str)\n      self.assertEqual(save_path, val)\n\n    with self.session(graph=ops.Graph()) as sess:\n      default_val = -1\n      table = lookup_ops.MutableHashTable(\n          dtypes.string,\n          dtypes.int64,\n          default_val,\n          name=\"t1\",\n          checkpoint=True,\n          experimental_is_anonymous=is_anonymous)\n      self.evaluate(\n          table.insert(\n              constant_op.constant([\"a\", \"c\"], dtypes.string),\n              constant_op.constant([12, 24], dtypes.int64)))\n      self.assertAllEqual(2, self.evaluate(table.size()))\n\n      save = saver.Saver([table])\n\n      # Restore the saved values in the parameter nodes.\n      save.restore(sess, save_path)\n      # Check that the parameter nodes have been restored.\n\n      self.assertAllEqual(3, self.evaluate(table.size()))\n\n      input_string = constant_op.constant([\"a\", \"b\", \"c\", \"d\", \"e\"],\n                                          dtypes.string)\n      output = table.lookup(input_string)\n      self.assertAllEqual([-1, 0, 1, 2, -1], self.evaluate(output))\n\n  @test_util.run_in_graph_and_eager_modes\n  def testObjectSaveRestore(self, is_anonymous):\n    if is_anonymous and not context.executing_eagerly():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    save_dir = os.path.join(self.get_temp_dir(), \"save_restore\")\n    save_prefix = os.path.join(tempfile.mkdtemp(prefix=save_dir), \"hash\")\n\n    v0 = variables.Variable(10.0, name=\"v0\")\n    v1 = variables.Variable(20.0, name=\"v1\")\n\n    default_val = -1\n    keys = constant_op.constant([\"b\", \"c\", \"d\"], dtypes.string)\n    values = constant_op.constant([0, 1, 2], dtypes.int64)\n    table = lookup_ops.MutableHashTable(\n        dtypes.string,\n        dtypes.int64,\n        default_val,\n        name=\"t1\",\n        checkpoint=True,\n        experimental_is_anonymous=is_anonymous)\n\n    checkpoint = trackable.Checkpoint(table=table, v0=v0, v1=v1)\n    self.evaluate([v0.initializer, v1.initializer])\n\n    # Check that the parameter nodes have been initialized.\n    self.assertEqual(10.0, self.evaluate(v0))\n    self.assertEqual(20.0, self.evaluate(v1))\n\n    self.assertAllEqual(0, self.evaluate(table.size()))\n    self.evaluate(table.insert(keys, values))\n    self.assertAllEqual(3, self.evaluate(table.size()))\n\n    save_path = checkpoint.save(save_prefix)\n    del table, checkpoint, v0, v1\n\n    v0 = variables.Variable(-1.0, name=\"v0\")\n    v1 = variables.Variable(-1.0, name=\"v1\")\n    default_val = -1\n    table = lookup_ops.MutableHashTable(\n        dtypes.string,\n        dtypes.int64,\n        default_val,\n        name=\"t1\",\n        checkpoint=True,\n        experimental_is_anonymous=is_anonymous)\n    self.evaluate(\n        table.insert(\n            constant_op.constant([\"a\", \"c\"], dtypes.string),\n            constant_op.constant([12, 24], dtypes.int64)))\n    self.assertAllEqual(2, self.evaluate(table.size()))\n\n    checkpoint = trackable.Checkpoint(table=table, v0=v0, v1=v1)\n\n    # Restore the saved values in the parameter nodes.\n    checkpoint.restore(save_path).run_restore_ops()\n    # Check that the parameter nodes have been restored.\n    self.assertEqual(10.0, self.evaluate(v0))\n    self.assertEqual(20.0, self.evaluate(v1))\n\n    self.assertAllEqual(3, self.evaluate(table.size()))\n\n    input_string = constant_op.constant([\"a\", \"b\", \"c\", \"d\", \"e\"],\n                                        dtypes.string)\n    output = table.lookup(input_string)\n    self.assertAllEqual([-1, 0, 1, 2, -1], self.evaluate(output))\n\n  @test_util.run_v2_only\n  def testSavedModelSaveRestore(self, is_anonymous):\n    save_dir = os.path.join(self.get_temp_dir(), \"save_restore\")\n    save_path = os.path.join(tempfile.mkdtemp(prefix=save_dir), \"hash\")\n\n    root = autotrackable.AutoTrackable()\n\n    default_value = -1\n    keys = constant_op.constant([11, 12, 13], dtypes.int64)\n    values = constant_op.constant([0, 1, 2], dtypes.int64)\n    root.table = lookup_ops.MutableHashTable(\n        dtypes.int64,\n        dtypes.int64,\n        default_value,\n        experimental_is_anonymous=is_anonymous)\n\n    @def_function.function(\n        input_signature=[tensor_spec.TensorSpec((), dtypes.int64)])\n    def lookup(key):\n      return root.table.lookup(key)\n\n    @def_function.function(input_signature=[])\n    def size():\n      return root.table.size()\n\n    @def_function.function(input_signature=[])\n    def is_ref_counting():\n      return test_ops.is_resource_handle_ref_counting(\n          root.table.resource_handle)\n\n    root.lookup = lookup\n    root.size = size\n    root.is_ref_counting = is_ref_counting\n\n    self.assertEqual(root.table.size(), 0)\n    root.table.insert(keys, values)\n    self.assertEqual(root.table.size(), 3)\n    self.assertEqual(root.table.lookup(12), 1)\n    self.assertEqual(root.table.lookup(10), -1)\n    self.assertEqual(len(root.table.export()[0]), 3)\n    self.assertEqual(root.is_ref_counting(), is_anonymous)\n\n    saved_model_save.save(root, save_path)\n\n    del root\n    loaded = saved_model_load.load(save_path)\n    self.assertEqual(loaded.size(), 3)\n    self.assertEqual(loaded.lookup(12), 1)\n    self.assertEqual(loaded.lookup(10), -1)\n    self.assertEqual(loaded.is_ref_counting(), is_anonymous)\n\n  @test_util.run_v1_only(\"Multiple sessions\")\n  def testSharing(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    # Start a server to store the table state\n    server = server_lib.Server({\"local0\": [\"localhost:0\"]},\n                               protocol=\"grpc\",\n                               start=True)\n    # Create two sessions sharing the same state\n    session1 = session.Session(server.target)\n    session2 = session.Session(server.target)\n\n    table = lookup_ops.MutableHashTable(\n        dtypes.int64,\n        dtypes.string,\n        \"-\",\n        name=\"t1\",\n        experimental_is_anonymous=is_anonymous)\n\n    # Populate the table in the first session\n    with session1:\n      self.assertAllEqual(0, table.size())\n\n      keys = constant_op.constant([11, 12], dtypes.int64)\n      values = constant_op.constant([\"a\", \"b\"])\n      table.insert(keys, values).run()\n      self.assertAllEqual(2, table.size())\n\n      output = table.lookup(constant_op.constant([11, 12, 13], dtypes.int64))\n      self.assertAllEqual([b\"a\", b\"b\", b\"-\"], output)\n\n    # Verify that we can access the shared data from the second session\n    with session2:\n      self.assertAllEqual(2, table.size())\n\n      output = table.lookup(constant_op.constant([10, 11, 12], dtypes.int64))\n      self.assertAllEqual([b\"-\", b\"a\", b\"b\"], output)\n\n  def testMutableHashTableOfTensors(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    default_val = constant_op.constant([-1, -1], dtypes.int64)\n    keys = constant_op.constant([\"brain\", \"salad\", \"surgery\", \"tarkus\"])\n    values = constant_op.constant([[0, 1], [2, 3], [4, 5], [6, 7]],\n                                  dtypes.int64)\n    table = lookup_ops.MutableHashTable(\n        dtypes.string,\n        dtypes.int64,\n        default_val,\n        experimental_is_anonymous=is_anonymous)\n    self.assertAllEqual(0, self.evaluate(table.size()))\n\n    self.evaluate(table.insert(keys, values))\n    self.assertAllEqual(4, self.evaluate(table.size()))\n\n    remove_string = constant_op.constant([\"tarkus\", \"tank\"])\n    self.evaluate(table.remove(remove_string))\n    self.assertAllEqual(3, self.evaluate(table.size()))\n\n    input_string = constant_op.constant([\"brain\", \"salad\", \"tank\"])\n    output = table.lookup(input_string)\n    self.assertAllEqual([3, 2], output.get_shape())\n\n    result = self.evaluate(output)\n    self.assertAllEqual([[0, 1], [2, 3], [-1, -1]], result)\n\n    exported_keys, exported_values = table.export()\n    # exported data is in the order of the internal map, i.e. undefined\n    sorted_keys = np.sort(self.evaluate(exported_keys))\n    sorted_values = np.sort(self.evaluate(exported_values), axis=0)\n    self.assertAllEqual([b\"brain\", b\"salad\", b\"surgery\"], sorted_keys)\n    sorted_expected_values = np.sort([[4, 5], [2, 3], [0, 1]], axis=0)\n    self.assertAllEqual(sorted_expected_values, sorted_values)\n\n  def testMutableHashTableExportInsert(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    default_val = constant_op.constant([-1, -1], dtypes.int64)\n    keys = constant_op.constant([\"brain\", \"salad\", \"surgery\"])\n    values = constant_op.constant([[0, 1], [2, 3], [4, 5]], dtypes.int64)\n    table1 = lookup_ops.MutableHashTable(\n        dtypes.string,\n        dtypes.int64,\n        default_val,\n        experimental_is_anonymous=is_anonymous)\n    self.assertAllEqual(0, self.evaluate(table1.size()))\n    self.evaluate(table1.insert(keys, values))\n    self.assertAllEqual(3, self.evaluate(table1.size()))\n\n    input_string = constant_op.constant([\"brain\", \"salad\", \"tank\"])\n    expected_output = [[0, 1], [2, 3], [-1, -1]]\n    output1 = table1.lookup(input_string)\n    self.assertAllEqual(expected_output, self.evaluate(output1))\n\n    exported_keys, exported_values = table1.export()\n    self.assertAllEqual(3, self.evaluate(exported_keys).size)\n    self.assertAllEqual(6, self.evaluate(exported_values).size)\n\n    # Populate a second table from the exported data\n    table2 = lookup_ops.MutableHashTable(\n        dtypes.string,\n        dtypes.int64,\n        default_val,\n        experimental_is_anonymous=is_anonymous)\n    self.assertAllEqual(0, self.evaluate(table2.size()))\n    self.evaluate(table2.insert(exported_keys, exported_values))\n    self.assertAllEqual(3, self.evaluate(table2.size()))\n\n    # Verify lookup result is still the same\n    output2 = table2.lookup(input_string)\n    self.assertAllEqual(expected_output, self.evaluate(output2))\n\n  def testMutableHashTableOfTensorsInvalidShape(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    default_val = constant_op.constant([-1, -1], dtypes.int64)\n    keys = constant_op.constant([\"brain\", \"salad\", \"surgery\"])\n    table = lookup_ops.MutableHashTable(\n        dtypes.string,\n        dtypes.int64,\n        default_val,\n        experimental_is_anonymous=is_anonymous)\n\n    # Shape [6] instead of [3, 2]\n    values = constant_op.constant([0, 1, 2, 3, 4, 5], dtypes.int64)\n    with self.assertRaisesOpError(\"Expected shape\"):\n      self.evaluate(table.insert(keys, values))\n\n    # Shape [2,3] instead of [3, 2]\n    values = constant_op.constant([[0, 1, 2], [3, 4, 5]], dtypes.int64)\n    with self.assertRaisesOpError(\"Expected shape\"):\n      self.evaluate(table.insert(keys, values))\n\n    # Shape [2, 2] instead of [3, 2]\n    values = constant_op.constant([[0, 1], [2, 3]], dtypes.int64)\n    with self.assertRaisesOpError(\"Expected shape\"):\n      self.evaluate(table.insert(keys, values))\n\n    # Shape [3, 1] instead of [3, 2]\n    values = constant_op.constant([[0], [2], [4]], dtypes.int64)\n    with self.assertRaisesOpError(\"Expected shape\"):\n      self.evaluate(table.insert(keys, values))\n\n    # Valid Insert\n    values = constant_op.constant([[0, 1], [2, 3], [4, 5]], dtypes.int64)\n    self.evaluate(table.insert(keys, values))\n    self.assertAllEqual(3, self.evaluate(table.size()))\n\n  def testMutableHashTableInvalidDefaultValue(self, is_anonymous):\n    default_val = constant_op.constant([[-1, -1]], dtypes.int64)\n    with self.assertRaisesOpError(\"Default value must be a vector\"):\n      table = lookup_ops.MutableHashTable(\n          dtypes.string,\n          dtypes.int64,\n          default_val,\n          experimental_is_anonymous=is_anonymous)\n      self.assertAllEqual(0, self.evaluate(table.size()))\n\n  def testMutableHashTableDuplicateInsert(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    default_val = -1\n    keys = constant_op.constant([\"brain\", \"salad\", \"surgery\", \"brain\"])\n    values = constant_op.constant([0, 1, 2, 3], dtypes.int64)\n    table = lookup_ops.MutableHashTable(\n        dtypes.string,\n        dtypes.int64,\n        default_val,\n        experimental_is_anonymous=is_anonymous)\n    self.assertAllEqual(0, self.evaluate(table.size()))\n\n    self.evaluate(table.insert(keys, values))\n    self.assertAllEqual(3, self.evaluate(table.size()))\n\n    input_string = constant_op.constant([\"brain\", \"salad\", \"tank\"])\n    output = table.lookup(input_string)\n\n    result = self.evaluate(output)\n    self.assertAllEqual([3, 1, -1], result)\n\n  def testMutableHashTableFindHighRank(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    default_val = -1\n    keys = constant_op.constant([\"brain\", \"salad\", \"surgery\"])\n    values = constant_op.constant([0, 1, 2], dtypes.int64)\n    table = lookup_ops.MutableHashTable(\n        dtypes.string,\n        dtypes.int64,\n        default_val,\n        experimental_is_anonymous=is_anonymous)\n\n    self.evaluate(table.insert(keys, values))\n    self.assertAllEqual(3, self.evaluate(table.size()))\n\n    input_string = constant_op.constant([[\"brain\", \"salad\"],\n                                         [\"tank\", \"tarkus\"]])\n    output = table.lookup(input_string)\n    self.assertAllEqual([2, 2], output.get_shape())\n\n    result = self.evaluate(output)\n    self.assertAllEqual([[0, 1], [-1, -1]], result)\n\n  def testMutableHashTableFindWithInvalidShapeDefaultValue(self, is_anonymous):\n    default_val = [-1, -1]\n    table = lookup_ops.MutableHashTable(\n        dtypes.string,\n        dtypes.int64,\n        default_val,\n        experimental_is_anonymous=is_anonymous)\n\n    input_string = constant_op.constant([[\"brain\", \"salad\"], [\"tank\",\n                                                              \"tarkus\"]])\n\n    invalid_default_val = constant_op.constant(\n        [[-2, -3], [-4, -5], [-6, -7], [-8, -9]], dtypes.int64)\n\n    with self.assertRaisesRegex(\n        (ValueError, errors_impl.InvalidArgumentError),\n        \"Expected shape \\[2\\] or \\[2,2,2\\] for default value, got \\[4,2]\"):\n      self.evaluate(table.lookup(input_string, invalid_default_val))\n\n    invalid_default_val = constant_op.constant([[[-2, -3], [-4, -5]]],\n                                               dtypes.int64)\n    with self.assertRaisesRegex(\n        (ValueError, errors_impl.InvalidArgumentError),\n        \"Expected shape \\[2\\] or \\[2,2,2\\] for default value, got \\[1,2,2\\]\"):\n      self.evaluate(table.lookup(input_string, invalid_default_val))\n\n  def testMutableHashTableFindHighRankScalarWithDynamicDefaultValue(\n      self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    default_val = -1\n    keys = constant_op.constant([\"brain\", \"salad\", \"surgery\"])\n    values = constant_op.constant([0, 1, 2], dtypes.int64)\n    table = lookup_ops.MutableHashTable(\n        dtypes.string,\n        dtypes.int64,\n        default_val,\n        experimental_is_anonymous=is_anonymous)\n\n    self.evaluate(table.insert(keys, values))\n    self.assertAllEqual(3, self.evaluate(table.size()))\n\n    input_string = constant_op.constant([[\"brain\", \"salad\"], [\"tank\",\n                                                              \"tarkus\"]])\n\n    dynamic_default_val = constant_op.constant([[-2, -3], [-4, -5]],\n                                               dtypes.int64)\n    output = table.lookup(input_string, dynamic_default_val)\n    self.assertAllEqual([2, 2], output.get_shape())\n\n    result = self.evaluate(output)\n    self.assertAllEqual([[0, 1], [-4, -5]], result)\n\n  def testMutableHashTableFindHighRankVectorWithDynamicDefaultValue(\n      self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    default_val = [-1, -1]\n    keys = constant_op.constant([\"brain\", \"salad\", \"surgery\"])\n    values = constant_op.constant([[0, 1], [2, 3], [4, 5]], dtypes.int64)\n    table = lookup_ops.MutableHashTable(\n        dtypes.string,\n        dtypes.int64,\n        default_val,\n        experimental_is_anonymous=is_anonymous)\n\n    self.evaluate(table.insert(keys, values))\n    self.assertAllEqual(3, self.evaluate(table.size()))\n\n    input_string = constant_op.constant([[\"brain\", \"salad\"], [\"tank\",\n                                                              \"tarkus\"]])\n\n    dynamic_default_val = constant_op.constant(\n        [[[-2, -3], [-4, -5]], [[-6, -7], [-8, -9]]], dtypes.int64)\n    output = table.lookup(input_string, dynamic_default_val)\n    self.assertAllEqual([2, 2, 2], output.get_shape())\n\n    result = self.evaluate(output)\n    self.assertAllEqual([[[0, 1], [2, 3]], [[-6, -7], [-8, -9]]], result)\n\n  def testMutableHashTableInsertHighRank(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    default_val = -1\n    keys = constant_op.constant([[\"brain\", \"salad\"], [\"surgery\", \"tank\"]])\n    values = constant_op.constant([[0, 1], [2, 3]], dtypes.int64)\n    table = lookup_ops.MutableHashTable(\n        dtypes.string,\n        dtypes.int64,\n        default_val,\n        experimental_is_anonymous=is_anonymous)\n\n    self.evaluate(table.insert(keys, values))\n    self.assertAllEqual(4, self.evaluate(table.size()))\n\n    input_string = constant_op.constant([\"brain\", \"salad\", \"tank\", \"tarkus\"])\n    output = table.lookup(input_string)\n\n    result = self.evaluate(output)\n    self.assertAllEqual([0, 1, 3, -1], result)\n\n  def testMutableHashTableRemoveHighRank(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    default_val = -1\n    keys = constant_op.constant([[\"brain\", \"salad\"], [\"surgery\", \"tank\"]])\n    values = constant_op.constant([[0, 1], [2, 3]], dtypes.int64)\n    table = lookup_ops.MutableHashTable(\n        dtypes.string,\n        dtypes.int64,\n        default_val,\n        experimental_is_anonymous=is_anonymous)\n\n    self.evaluate(table.insert(keys, values))\n    self.assertAllEqual(4, self.evaluate(table.size()))\n\n    remove_string = constant_op.constant([\"salad\", \"tarkus\"])\n    self.evaluate(table.remove(remove_string))\n    self.assertAllEqual(3, self.evaluate(table.size()))\n\n    input_string = constant_op.constant([\"brain\", \"salad\", \"tank\", \"tarkus\"])\n    output = table.lookup(input_string)\n\n    result = self.evaluate(output)\n    self.assertAllEqual([0, -1, 3, -1], result)\n\n  def testMutableHashTableOfTensorsFindHighRank(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    default_val = constant_op.constant([-1, -1, -1], dtypes.int64)\n    keys = constant_op.constant([\"brain\", \"salad\", \"surgery\"])\n    values = constant_op.constant([[0, 1, 2], [2, 3, 4], [4, 5, 6]],\n                                  dtypes.int64)\n    table = lookup_ops.MutableHashTable(\n        dtypes.string,\n        dtypes.int64,\n        default_val,\n        experimental_is_anonymous=is_anonymous)\n\n    self.evaluate(table.insert(keys, values))\n    self.assertAllEqual(3, self.evaluate(table.size()))\n\n    input_string = constant_op.constant([[\"brain\", \"salad\"],\n                                         [\"tank\", \"tarkus\"]])\n    output = table.lookup(input_string)\n    self.assertAllEqual([2, 2, 3], output.get_shape())\n\n    result = self.evaluate(output)\n    self.assertAllEqual(\n        [[[0, 1, 2], [2, 3, 4]], [[-1, -1, -1], [-1, -1, -1]]], result)\n\n  def testMutableHashTableOfTensorsRemoveHighRank(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    default_val = constant_op.constant([-1, -1, -1], dtypes.int64)\n    keys = constant_op.constant([\"brain\", \"salad\", \"surgery\"])\n    values = constant_op.constant([[0, 1, 2], [2, 3, 4], [4, 5, 6]],\n                                  dtypes.int64)\n    table = lookup_ops.MutableHashTable(\n        dtypes.string,\n        dtypes.int64,\n        default_val,\n        experimental_is_anonymous=is_anonymous)\n\n    self.evaluate(table.insert(keys, values))\n    self.assertAllEqual(3, self.evaluate(table.size()))\n\n    remove_string = constant_op.constant([[\"brain\", \"tank\"]])\n    self.evaluate(table.remove(remove_string))\n    self.assertAllEqual(2, self.evaluate(table.size()))\n\n    input_string = constant_op.constant([[\"brain\", \"salad\"],\n                                         [\"surgery\", \"tank\"]])\n    output = table.lookup(input_string)\n    self.assertAllEqual([2, 2, 3], output.get_shape())\n\n    result = self.evaluate(output)\n    self.assertAllEqual(\n        [[[-1, -1, -1], [2, 3, 4]], [[4, 5, 6], [-1, -1, -1]]], result)\n\n  def testMultipleMutableHashTables(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    default_val = -1\n    keys = constant_op.constant([\"brain\", \"salad\", \"surgery\"])\n    values = constant_op.constant([0, 1, 2], dtypes.int64)\n\n    table1 = lookup_ops.MutableHashTable(\n        dtypes.string,\n        dtypes.int64,\n        default_val,\n        experimental_is_anonymous=is_anonymous)\n    table2 = lookup_ops.MutableHashTable(\n        dtypes.string,\n        dtypes.int64,\n        default_val,\n        experimental_is_anonymous=is_anonymous)\n    table3 = lookup_ops.MutableHashTable(\n        dtypes.string,\n        dtypes.int64,\n        default_val,\n        experimental_is_anonymous=is_anonymous)\n    self.evaluate(table1.insert(keys, values))\n    self.evaluate(table2.insert(keys, values))\n    self.evaluate(table3.insert(keys, values))\n\n    self.assertAllEqual(3, self.evaluate(table1.size()))\n    self.assertAllEqual(3, self.evaluate(table2.size()))\n    self.assertAllEqual(3, self.evaluate(table3.size()))\n\n    input_string = constant_op.constant([\"brain\", \"salad\", \"tank\"])\n    output1 = table1.lookup(input_string)\n    output2 = table2.lookup(input_string)\n    output3 = table3.lookup(input_string)\n\n    out1, out2, out3 = self.evaluate([output1, output2, output3])\n    self.assertAllEqual([0, 1, -1], out1)\n    self.assertAllEqual([0, 1, -1], out2)\n    self.assertAllEqual([0, 1, -1], out3)\n\n  def testMutableHashTableWithTensorDefault(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    default_val = constant_op.constant(-1, dtypes.int64)\n    keys = constant_op.constant([\"brain\", \"salad\", \"surgery\"])\n    values = constant_op.constant([0, 1, 2], dtypes.int64)\n    table = lookup_ops.MutableHashTable(\n        dtypes.string,\n        dtypes.int64,\n        default_val,\n        experimental_is_anonymous=is_anonymous)\n\n    self.evaluate(table.insert(keys, values))\n    self.assertAllEqual(3, self.evaluate(table.size()))\n\n    input_string = constant_op.constant([\"brain\", \"salad\", \"tank\"])\n    output = table.lookup(input_string)\n\n    result = self.evaluate(output)\n    self.assertAllEqual([0, 1, -1], result)\n\n  def testSignatureMismatch(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    default_val = -1\n    keys = constant_op.constant([\"brain\", \"salad\", \"surgery\"])\n    values = constant_op.constant([0, 1, 2], dtypes.int64)\n    table = lookup_ops.MutableHashTable(\n        dtypes.string,\n        dtypes.int64,\n        default_val,\n        experimental_is_anonymous=is_anonymous)\n\n    # insert with keys of the wrong type\n    with self.assertRaises(ValueError):\n      self.evaluate(table.insert(constant_op.constant([4, 5, 6]), values))\n\n    # insert with values of the wrong type\n    with self.assertRaises(ValueError):\n      self.evaluate(table.insert(keys, constant_op.constant([\"a\", \"b\", \"c\"])))\n\n    self.assertAllEqual(0, self.evaluate(table.size()))\n\n    self.evaluate(table.insert(keys, values))\n    self.assertAllEqual(3, self.evaluate(table.size()))\n\n    input_string_ref = variables.Variable(\"brain\")\n    input_int64_ref = variables.Variable(-1, dtype=dtypes.int64)\n    self.evaluate(variables.global_variables_initializer())\n\n    # Ref types do not produce an insert signature mismatch.\n    self.evaluate(table.insert(input_string_ref, input_int64_ref))\n    self.assertAllEqual(3, self.evaluate(table.size()))\n\n    # Ref types do not produce a lookup signature mismatch.\n    self.assertEqual(-1, self.evaluate(table.lookup(input_string_ref)))\n\n    # lookup with keys of the wrong type\n    input_string = constant_op.constant([1, 2, 3], dtypes.int64)\n    with self.assertRaises(ValueError):\n      self.evaluate(table.lookup(input_string))\n\n    # default value of the wrong type\n    with self.assertRaises(TypeError):\n      lookup_ops.MutableHashTable(\n          dtypes.string,\n          dtypes.int64,\n          \"UNK\",\n          experimental_is_anonymous=is_anonymous)\n\n  def testMutableHashTableStringFloat(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    default_val = -1.5\n    keys = constant_op.constant([\"brain\", \"salad\", \"surgery\"])\n    values = constant_op.constant([0, 1.1, 2.2], dtypes.float32)\n    table = lookup_ops.MutableHashTable(\n        dtypes.string,\n        dtypes.float32,\n        default_val,\n        experimental_is_anonymous=is_anonymous)\n    self.assertAllEqual(0, self.evaluate(table.size()))\n\n    self.evaluate(table.insert(keys, values))\n    self.assertAllEqual(3, self.evaluate(table.size()))\n\n    input_string = constant_op.constant([\"brain\", \"salad\", \"tank\"])\n    output = table.lookup(input_string)\n\n    result = self.evaluate(output)\n    self.assertAllClose([0, 1.1, default_val], result)\n\n  def testMutableHashTableIntFloat(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    default_val = -1.0\n    keys = constant_op.constant([3, 7, 0], dtypes.int64)\n    values = constant_op.constant([7.5, -1.2, 9.9], dtypes.float32)\n    table = lookup_ops.MutableHashTable(\n        dtypes.int64,\n        dtypes.float32,\n        default_val,\n        experimental_is_anonymous=is_anonymous)\n    self.assertAllEqual(0, self.evaluate(table.size()))\n\n    self.evaluate(table.insert(keys, values))\n    self.assertAllEqual(3, self.evaluate(table.size()))\n\n    input_string = constant_op.constant([7, 0, 11], dtypes.int64)\n    output = table.lookup(input_string)\n\n    result = self.evaluate(output)\n    self.assertAllClose([-1.2, 9.9, default_val], result)\n\n  def testMutableHashTableInt64String(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    default_val = \"n/a\"\n    keys = constant_op.constant([0, 1, 2], dtypes.int64)\n    values = constant_op.constant([\"brain\", \"salad\", \"surgery\"])\n    table = lookup_ops.MutableHashTable(\n        dtypes.int64,\n        dtypes.string,\n        default_val,\n        experimental_is_anonymous=is_anonymous)\n    self.assertAllEqual(0, self.evaluate(table.size()))\n\n    self.evaluate(table.insert(keys, values))\n    self.assertAllEqual(3, self.evaluate(table.size()))\n\n    input_string = constant_op.constant([0, 1, 3], dtypes.int64)\n    output = table.lookup(input_string)\n\n    result = self.evaluate(output)\n    self.assertAllEqual((b\"brain\", b\"salad\", b\"n/a\"), result)\n\n  def testExportShapeInference(self, is_anonymous):\n    default_value = -1\n    table = lookup_ops.MutableHashTable(\n        dtypes.int64,\n        dtypes.int64,\n        default_value=default_value,\n        experimental_is_anonymous=is_anonymous)\n    actual_shapes = [t.shape for t in table.export()]\n    inferred_shapes = []\n\n    @def_function.function\n    def f():\n      for t in table.export():\n        inferred_shapes.append(t.shape)\n\n    f()\n    self.assertLen(actual_shapes, 2)\n    self.assertLen(inferred_shapes, 2)\n    self.assertTrue(inferred_shapes[0].is_compatible_with(actual_shapes[0]))\n    self.assertTrue(inferred_shapes[1].is_compatible_with(actual_shapes[1]))\n\n\nclass MutableHashTableBenchmark(test.Benchmark):\n\n  def _create_table(self):\n    return lookup_ops.MutableHashTable(dtypes.int64, dtypes.float32, 0.0)\n\n  def benchmark_single_repeated_scalar_insert_scalar(self):\n    table = self._create_table()\n    value = variables.Variable(1.0)\n    insert = table.insert(0, value)\n    size = table.size()\n    with session.Session() as sess:\n      sess.run(value.initializer)\n      self.run_op_benchmark(sess, insert, burn_iters=10, min_iters=10000)\n      assert sess.run(size) == 1\n\n  def benchmark_many_repeated_scalar_insert_scalar(self):\n    table = self._create_table()\n    c = dataset_ops.make_one_shot_iterator(counter.Counter()).get_next()\n    value = variables.Variable(1.0)\n    insert = table.insert(c, value)\n    size = table.size()\n    with session.Session() as sess:\n      sess.run(value.initializer)\n      self.run_op_benchmark(sess, insert, burn_iters=10, min_iters=10000)\n      assert sess.run(size) >= 10000\n\n  def benchmark_single_repeated_batch_32_insert_scalar(self):\n    table = self._create_table()\n    value = variables.Variable([1.0] * 32)\n    insert = table.insert(list(range(32)), value)\n    size = table.size()\n    with session.Session() as sess:\n      sess.run(value.initializer)\n      self.run_op_benchmark(sess, insert, burn_iters=10, min_iters=1000)\n      assert sess.run(size) == 32\n\n  def benchmark_many_repeated_batch_32_insert_scalar(self):\n    table = self._create_table()\n    c = dataset_ops.make_one_shot_iterator(counter.Counter()).get_next()\n    value = variables.Variable([1.0] * 32)\n    insert = table.insert(32 * c + list(range(32)), value)\n    size = table.size()\n    with session.Session() as sess:\n      sess.run(value.initializer)\n      self.run_op_benchmark(sess, insert, burn_iters=10, min_iters=1000)\n      assert sess.run(size) >= 1000 * 32\n\n\nclass DenseHashTableBenchmark(MutableHashTableBenchmark):\n\n  def _create_table(self):\n    return lookup_ops.DenseHashTable(\n        dtypes.int64,\n        dtypes.float32,\n        default_value=0.0,\n        empty_key=-1,\n        deleted_key=-2)\n\n\nif __name__ == \"__main__\":\n  test.main()\n"], "fixing_code": ["/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include \"tensorflow/core/framework/common_shape_fns.h\"\n#include \"tensorflow/core/framework/dataset_stateful_op_allowlist.h\"\n#include \"tensorflow/core/framework/op.h\"\n#include \"tensorflow/core/framework/op_def_builder.h\"\n#include \"tensorflow/core/framework/shape_inference.h\"\n\nnamespace tensorflow {\n\nusing shape_inference::DimensionHandle;\nusing shape_inference::InferenceContext;\nusing shape_inference::ShapeAndType;\nusing shape_inference::ShapeHandle;\n\n// --------------------------------------------------------------------------\n\nnamespace {\nStatus TwoElementVectorInputsAndScalarOutputs(InferenceContext* c) {\n  ShapeHandle handle;\n  DimensionHandle unused_handle;\n  for (int i = 0; i < c->num_inputs(); ++i) {\n    TF_RETURN_IF_ERROR(c->WithRank(c->input(i), 1, &handle));\n    TF_RETURN_IF_ERROR(c->WithValue(c->Dim(handle, 0), 2, &unused_handle));\n  }\n  for (int i = 0; i < c->num_outputs(); ++i) {\n    c->set_output(i, c->Scalar());\n  }\n  return OkStatus();\n}\n\nStatus ScalarAndTwoElementVectorInputsAndScalarOutputs(InferenceContext* c) {\n  ShapeHandle handle;\n  DimensionHandle unused_handle;\n  TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 0, &handle));\n  for (int i = 1; i < c->num_inputs(); ++i) {\n    TF_RETURN_IF_ERROR(c->WithRank(c->input(i), 1, &handle));\n    TF_RETURN_IF_ERROR(c->WithValue(c->Dim(handle, 0), 2, &unused_handle));\n  }\n  for (int i = 0; i < c->num_outputs(); ++i) {\n    c->set_output(i, c->Scalar());\n  }\n  return OkStatus();\n}\n\nStatus TwoElementOutput(InferenceContext* c) {\n  c->set_output(0, c->Vector(2));\n  return OkStatus();\n}\n\nStatus ScalarOutput(InferenceContext* c) {\n  c->set_output(0, c->Scalar());\n  return OkStatus();\n}\n}  // namespace\n\nREGISTER_OP(\"LookupTableFind\")\n    .Input(\"table_handle: Ref(string)\")\n    .Input(\"keys: Tin\")\n    .Input(\"default_value: Tout\")\n    .Output(\"values: Tout\")\n    .Attr(\"Tin: type\")\n    .Attr(\"Tout: type\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle handle;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 1, &handle));\n      DimensionHandle unused_dim;\n      TF_RETURN_IF_ERROR(c->WithValue(c->Dim(handle, 0), 2, &unused_dim));\n\n      // Default value must be scalar or vector.\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(2), 1, &unused));\n      c->set_output(0, c->UnknownShape());\n      return OkStatus();\n    });\n\nStatus ValidateTableType(InferenceContext* c,\n                         const ShapeAndType& key_shape_and_type,\n                         const string& key_dtype_attr,\n                         const ShapeAndType& value_shape_and_type,\n                         const string& value_dtype_attr) {\n  DataType key_dtype;\n  TF_RETURN_IF_ERROR(c->GetAttr(key_dtype_attr, &key_dtype));\n  if (key_shape_and_type.dtype != key_dtype) {\n    return errors::InvalidArgument(\n        \"Trying to read value with wrong dtype. \"\n        \"Expected \",\n        DataTypeString(key_shape_and_type.dtype), \" got \",\n        DataTypeString(key_dtype));\n  }\n  DataType value_dtype;\n  TF_RETURN_IF_ERROR(c->GetAttr(value_dtype_attr, &value_dtype));\n  if (value_shape_and_type.dtype != value_dtype) {\n    return errors::InvalidArgument(\n        \"Trying to read value with wrong dtype. \"\n        \"Expected \",\n        DataTypeString(value_shape_and_type.dtype), \" got \",\n        DataTypeString(value_dtype));\n  }\n  return OkStatus();\n}\n\nStatus ValidateTableResourceHandle(InferenceContext* c, ShapeHandle keys,\n                                   const string& key_dtype_attr,\n                                   const string& value_dtype_attr,\n                                   ShapeAndType* output_shape_and_type) {\n  auto* handle_data = c->input_handle_shapes_and_types(0);\n  if (handle_data == nullptr || handle_data->size() != 2) {\n    output_shape_and_type->shape = c->UnknownShape();\n    output_shape_and_type->dtype = DT_INVALID;\n  } else {\n    const ShapeAndType& key_shape_and_type = (*handle_data)[0];\n    const ShapeAndType& value_shape_and_type = (*handle_data)[1];\n    TF_RETURN_IF_ERROR(ValidateTableType(c, key_shape_and_type, key_dtype_attr,\n                                         value_shape_and_type,\n                                         value_dtype_attr));\n    output_shape_and_type->dtype = value_shape_and_type.dtype;\n    if (c->RankKnown(key_shape_and_type.shape) && c->RankKnown(keys)) {\n      int keys_rank = c->Rank(keys);\n      int key_suffix_rank = c->Rank(key_shape_and_type.shape);\n      if (keys_rank < key_suffix_rank) {\n        return errors::InvalidArgument(\n            \"Expected keys to have suffix \",\n            c->DebugString(key_shape_and_type.shape),\n            \" but saw shape: \", c->DebugString(keys));\n      }\n      for (int d = 0; d < key_suffix_rank; d++) {\n        // Ensure the suffix of keys match what's in the Table.\n        DimensionHandle dim = c->Dim(key_shape_and_type.shape, d);\n        TF_RETURN_IF_ERROR(\n            c->ReplaceDim(keys, keys_rank - key_suffix_rank + d, dim, &keys));\n      }\n      std::vector<DimensionHandle> keys_prefix_vec;\n      keys_prefix_vec.reserve(keys_rank - key_suffix_rank);\n      for (int d = 0; d < keys_rank - key_suffix_rank; ++d) {\n        keys_prefix_vec.push_back(c->Dim(keys, d));\n      }\n      ShapeHandle keys_prefix = c->MakeShape(keys_prefix_vec);\n      TF_RETURN_IF_ERROR(c->Concatenate(keys_prefix, value_shape_and_type.shape,\n                                        &output_shape_and_type->shape));\n    } else {\n      output_shape_and_type->shape = c->UnknownShape();\n    }\n  }\n  return OkStatus();\n}\n\nREGISTER_OP(\"LookupTableFindV2\")\n    .Input(\"table_handle: resource\")\n    .Input(\"keys: Tin\")\n    .Input(\"default_value: Tout\")\n    .Output(\"values: Tout\")\n    .Attr(\"Tin: type\")\n    .Attr(\"Tout: type\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle handle;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 0, &handle));\n\n      ShapeAndType value_shape_and_type;\n      TF_RETURN_IF_ERROR(ValidateTableResourceHandle(\n          c,\n          /*keys=*/c->input(1),\n          /*key_dtype_attr=*/\"Tin\",\n          /*value_dtype_attr=*/\"Tout\", &value_shape_and_type));\n      c->set_output(0, value_shape_and_type.shape);\n\n      return OkStatus();\n    });\nALLOW_STATEFUL_OP_FOR_DATASET_FUNCTIONS(\"LookupTableFindV2\");\n// TODO(b/72710477): Update this.\n\nREGISTER_OP(\"LookupTableInsert\")\n    .Input(\"table_handle: Ref(string)\")\n    .Input(\"keys: Tin\")\n    .Input(\"values: Tout\")\n    .Attr(\"Tin: type\")\n    .Attr(\"Tout: type\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle handle;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 1, &handle));\n      DimensionHandle unused_dim;\n      TF_RETURN_IF_ERROR(c->WithValue(c->Dim(handle, 0), 2, &unused_dim));\n\n      // TODO(ebrevdo): Validate keys and values shape.\n      return OkStatus();\n    });\n\nREGISTER_OP(\"LookupTableInsertV2\")\n    .Input(\"table_handle: resource\")\n    .Input(\"keys: Tin\")\n    .Input(\"values: Tout\")\n    .Attr(\"Tin: type\")\n    .Attr(\"Tout: type\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle handle;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 0, &handle));\n\n      // TODO: Validate keys and values shape.\n      return OkStatus();\n    });\n\nREGISTER_OP(\"LookupTableRemoveV2\")\n    .Input(\"table_handle: resource\")\n    .Input(\"keys: Tin\")\n    .Attr(\"Tin: type\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle handle;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 0, &handle));\n      TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(1), 1, &handle));\n\n      // TODO(turboale): Validate keys shape.\n      return OkStatus();\n    });\n\nREGISTER_OP(\"LookupTableSize\")\n    .Input(\"table_handle: Ref(string)\")\n    .Output(\"size: int64\")\n    .SetShapeFn(TwoElementVectorInputsAndScalarOutputs);\nALLOW_STATEFUL_OP_FOR_DATASET_FUNCTIONS(\"LookupTableSize\");\n\nREGISTER_OP(\"LookupTableSizeV2\")\n    .Input(\"table_handle: resource\")\n    .Output(\"size: int64\")\n    .SetShapeFn(ScalarAndTwoElementVectorInputsAndScalarOutputs);\nALLOW_STATEFUL_OP_FOR_DATASET_FUNCTIONS(\"LookupTableSizeV2\");\n\nREGISTER_OP(\"LookupTableExport\")\n    .Input(\"table_handle: Ref(string)\")\n    .Output(\"keys: Tkeys\")\n    .Output(\"values: Tvalues\")\n    .Attr(\"Tkeys: type\")\n    .Attr(\"Tvalues: type\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle handle;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 1, &handle));\n      DimensionHandle unused_dim;\n      TF_RETURN_IF_ERROR(c->WithValue(c->Dim(handle, 0), 2, &unused_dim));\n\n      ShapeHandle values = c->UnknownShape();\n      TF_RETURN_IF_ERROR(c->WithRankAtLeast(values, 1, &values));\n      ShapeHandle keys = c->Vector(c->Dim(values, 0));\n      c->set_output(0, keys);\n      c->set_output(1, values);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"LookupTableExportV2\")\n    .Input(\"table_handle: resource\")\n    .Output(\"keys: Tkeys\")\n    .Output(\"values: Tvalues\")\n    .Attr(\"Tkeys: type\")\n    .Attr(\"Tvalues: type\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle handle;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 0, &handle));\n      auto* handle_data = c->input_handle_shapes_and_types(0);\n      if (handle_data != nullptr && handle_data->size() == 2) {\n        const ShapeAndType& key_shape_and_type = (*handle_data)[0];\n        const ShapeAndType& value_shape_and_type = (*handle_data)[1];\n        TF_RETURN_IF_ERROR(ValidateTableType(c, key_shape_and_type,\n                                             /*key_dtype_attr*/ \"Tkeys\",\n                                             value_shape_and_type,\n                                             /*value_dtype_attr*/ \"Tvalues\"));\n      }\n      // Different lookup tables have different output shapes.\n      c->set_output(0, c->UnknownShape());\n      c->set_output(1, c->UnknownShape());\n      return OkStatus();\n    });\n\nREGISTER_OP(\"LookupTableImport\")\n    .Input(\"table_handle: Ref(string)\")\n    .Input(\"keys: Tin\")\n    .Input(\"values: Tout\")\n    .Attr(\"Tin: type\")\n    .Attr(\"Tout: type\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle handle;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 1, &handle));\n      DimensionHandle unused_dim;\n      TF_RETURN_IF_ERROR(c->WithValue(c->Dim(handle, 0), 2, &unused_dim));\n\n      // TODO(ebrevdo): Validate keys and values shape.\n      return OkStatus();\n    });\n\nREGISTER_OP(\"LookupTableImportV2\")\n    .Input(\"table_handle: resource\")\n    .Input(\"keys: Tin\")\n    .Input(\"values: Tout\")\n    .Attr(\"Tin: type\")\n    .Attr(\"Tout: type\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle handle;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 0, &handle));\n\n      ShapeHandle keys;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &keys));\n      ShapeHandle values;\n      TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(2), 1, &values));\n      DimensionHandle unused;\n      TF_RETURN_IF_ERROR(c->Merge(c->Dim(keys, 0), c->Dim(values, 0), &unused));\n      return OkStatus();\n    });\n\nStatus MutableHashTableShape(InferenceContext* c, const ShapeHandle& key,\n                             const ShapeHandle& value) {\n  c->set_output(0, c->Scalar());\n\n  ShapeHandle key_s;\n  TF_RETURN_IF_ERROR(c->WithRankAtMost(key, 1, &key_s));\n\n  DataType key_t;\n  TF_RETURN_IF_ERROR(c->GetAttr(\"key_dtype\", &key_t));\n\n  DataType value_t;\n  TF_RETURN_IF_ERROR(c->GetAttr(\"value_dtype\", &value_t));\n\n  // ShapeAndType vector for {key, value}.\n  c->set_output_handle_shapes_and_types(\n      0, std::vector<ShapeAndType>{{key_s, key_t}, {value, value_t}});\n\n  return OkStatus();\n}\n\nStatus MutableHashTableShapeFn(InferenceContext* c) {\n  return MutableHashTableShape(c, /*key=*/c->Scalar(),\n                               /*value=*/c->Scalar());\n}\n\nStatus MutableHashTableOfTensorsShapeFn(InferenceContext* c) {\n  PartialTensorShape value_p;\n  TF_RETURN_IF_ERROR(c->GetAttr(\"value_shape\", &value_p));\n  ShapeHandle value_s;\n  TF_RETURN_IF_ERROR(c->MakeShapeFromPartialTensorShape(value_p, &value_s));\n  return MutableHashTableShape(c, /*key=*/c->Scalar(), /*value=*/value_s);\n}\n\nStatus MutableDenseHashTableShapeFn(InferenceContext* c) {\n  PartialTensorShape value_p;\n  TF_RETURN_IF_ERROR(c->GetAttr(\"value_shape\", &value_p));\n  ShapeHandle value_s;\n  TF_RETURN_IF_ERROR(c->MakeShapeFromPartialTensorShape(value_p, &value_s));\n  return MutableHashTableShape(c, /*key=*/c->input(0), /*value=*/value_s);\n}\n\nREGISTER_OP(\"HashTable\")\n    .Output(\"table_handle: Ref(string)\")\n    .Attr(\"container: string = ''\")\n    .Attr(\"shared_name: string = ''\")\n    .Attr(\"use_node_name_sharing: bool = false\")\n    .Attr(\"key_dtype: type\")\n    .Attr(\"value_dtype: type\")\n    .SetIsStateful()\n    .SetShapeFn(TwoElementOutput);\n\nREGISTER_OP(\"HashTableV2\")\n    .Output(\"table_handle: resource\")\n    .Attr(\"container: string = ''\")\n    .Attr(\"shared_name: string = ''\")\n    .Attr(\"use_node_name_sharing: bool = false\")\n    .Attr(\"key_dtype: type\")\n    .Attr(\"value_dtype: type\")\n    .SetIsStateful()\n    .SetShapeFn(ScalarOutput);\n\nREGISTER_OP(\"AnonymousHashTable\")\n    .Output(\"table_handle: resource\")\n    .Attr(\"key_dtype: type\")\n    .Attr(\"value_dtype: type\")\n    .SetIsStateful()\n    .SetShapeFn(ScalarOutput);\n\nREGISTER_OP(\"MutableHashTable\")\n    .Output(\"table_handle: Ref(string)\")\n    .Attr(\"container: string = ''\")\n    .Attr(\"shared_name: string = ''\")\n    .Attr(\"use_node_name_sharing: bool = false\")\n    .Attr(\"key_dtype: type\")\n    .Attr(\"value_dtype: type\")\n    .SetIsStateful()\n    .SetShapeFn(TwoElementOutput);\n\nREGISTER_OP(\"MutableHashTableV2\")\n    .Output(\"table_handle: resource\")\n    .Attr(\"container: string = ''\")\n    .Attr(\"shared_name: string = ''\")\n    .Attr(\"use_node_name_sharing: bool = false\")\n    .Attr(\"key_dtype: type\")\n    .Attr(\"value_dtype: type\")\n    .SetIsStateful()\n    .SetShapeFn(MutableHashTableShapeFn);\n\nREGISTER_OP(\"AnonymousMutableHashTable\")\n    .Output(\"table_handle: resource\")\n    .Attr(\"key_dtype: type\")\n    .Attr(\"value_dtype: type\")\n    .SetIsStateful()\n    .SetShapeFn(MutableHashTableShapeFn);\n\nREGISTER_OP(\"MutableHashTableOfTensors\")\n    .Output(\"table_handle: Ref(string)\")\n    .Attr(\"container: string = ''\")\n    .Attr(\"shared_name: string = ''\")\n    .Attr(\"use_node_name_sharing: bool = false\")\n    .Attr(\"key_dtype: type\")\n    .Attr(\"value_dtype: type\")\n    .Attr(\"value_shape: shape = {}\")\n    .SetIsStateful()\n    .SetShapeFn(TwoElementOutput);\n\nREGISTER_OP(\"MutableHashTableOfTensorsV2\")\n    .Output(\"table_handle: resource\")\n    .Attr(\"container: string = ''\")\n    .Attr(\"shared_name: string = ''\")\n    .Attr(\"use_node_name_sharing: bool = false\")\n    .Attr(\"key_dtype: type\")\n    .Attr(\"value_dtype: type\")\n    .Attr(\"value_shape: shape = {}\")\n    .SetIsStateful()\n    .SetShapeFn(MutableHashTableOfTensorsShapeFn);\n\nREGISTER_OP(\"AnonymousMutableHashTableOfTensors\")\n    .Output(\"table_handle: resource\")\n    .Attr(\"key_dtype: type\")\n    .Attr(\"value_dtype: type\")\n    .Attr(\"value_shape: shape = {}\")\n    .SetIsStateful()\n    .SetShapeFn(MutableHashTableOfTensorsShapeFn);\n\nREGISTER_OP(\"MutableDenseHashTable\")\n    .Input(\"empty_key: key_dtype\")\n    .Output(\"table_handle: Ref(string)\")\n    .Attr(\"container: string = ''\")\n    .Attr(\"shared_name: string = ''\")\n    .Attr(\"use_node_name_sharing: bool = false\")\n    .Attr(\"key_dtype: type\")\n    .Attr(\"value_dtype: type\")\n    .Attr(\"value_shape: shape = {}\")\n    .Attr(\"initial_num_buckets: int = 131072\")  // 2^17\n    .Attr(\"max_load_factor: float = 0.8\")\n    .SetIsStateful()\n    .SetShapeFn(TwoElementOutput);\n\nREGISTER_OP(\"MutableDenseHashTableV2\")\n    .Input(\"empty_key: key_dtype\")\n    .Input(\"deleted_key: key_dtype\")\n    .Output(\"table_handle: resource\")\n    .Attr(\"container: string = ''\")\n    .Attr(\"shared_name: string = ''\")\n    .Attr(\"use_node_name_sharing: bool = false\")\n    .Attr(\"key_dtype: type\")\n    .Attr(\"value_dtype: type\")\n    .Attr(\"value_shape: shape = {}\")\n    .Attr(\"initial_num_buckets: int = 131072\")  // 2^17\n    .Attr(\"max_load_factor: float = 0.8\")\n    .SetIsStateful()\n    .SetShapeFn(MutableDenseHashTableShapeFn);\n\nREGISTER_OP(\"AnonymousMutableDenseHashTable\")\n    .Input(\"empty_key: key_dtype\")\n    .Input(\"deleted_key: key_dtype\")\n    .Output(\"table_handle: resource\")\n    .Attr(\"key_dtype: type\")\n    .Attr(\"value_dtype: type\")\n    .Attr(\"value_shape: shape = {}\")\n    .Attr(\"initial_num_buckets: int = 131072\")  // 2^17\n    .Attr(\"max_load_factor: float = 0.8\")\n    .SetIsStateful()\n    .SetShapeFn(MutableDenseHashTableShapeFn);\n\nREGISTER_OP(\"InitializeTable\")\n    .Input(\"table_handle: Ref(string)\")\n    .Input(\"keys: Tkey\")\n    .Input(\"values: Tval\")\n    .Attr(\"Tkey: type\")\n    .Attr(\"Tval: type\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle handle;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 1, &handle));\n      DimensionHandle unused_dim;\n      TF_RETURN_IF_ERROR(c->WithValue(c->Dim(handle, 0), 2, &unused_dim));\n\n      ShapeHandle keys;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &keys));\n      TF_RETURN_IF_ERROR(c->Merge(keys, c->input(2), &keys));\n      return OkStatus();\n    });\n\nREGISTER_OP(\"InitializeTableV2\")\n    .Input(\"table_handle: resource\")\n    .Input(\"keys: Tkey\")\n    .Input(\"values: Tval\")\n    .Attr(\"Tkey: type\")\n    .Attr(\"Tval: type\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle handle;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 0, &handle));\n\n      ShapeHandle keys;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &keys));\n      TF_RETURN_IF_ERROR(c->Merge(keys, c->input(2), &keys));\n      return OkStatus();\n    });\n\nREGISTER_OP(\"InitializeTableFromTextFile\")\n    .Input(\"table_handle: Ref(string)\")\n    .Input(\"filename: string\")\n    .Attr(\"key_index: int >= -2\")\n    .Attr(\"value_index: int >= -2\")\n    .Attr(\"vocab_size: int >= -1 = -1\")\n    .Attr(\"delimiter: string = '\\t'\")\n    .Attr(\"offset: int = 0\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle handle;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 1, &handle));\n      DimensionHandle unused_dim;\n      TF_RETURN_IF_ERROR(c->WithValue(c->Dim(handle, 0), 2, &unused_dim));\n\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &handle));\n      return OkStatus();\n    });\n\nREGISTER_OP(\"InitializeTableFromTextFileV2\")\n    .Input(\"table_handle: resource\")\n    .Input(\"filename: string\")\n    .Attr(\"key_index: int >= -2\")\n    .Attr(\"value_index: int >= -2\")\n    .Attr(\"vocab_size: int >= -1 = -1\")\n    .Attr(\"delimiter: string = '\\t'\")\n    .Attr(\"offset: int = 0\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle handle;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 0, &handle));\n\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &handle));\n      return OkStatus();\n    });\n\n}  // namespace tensorflow\n", "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for lookup ops.\"\"\"\nimport os\nimport tempfile\nimport unittest\n\nfrom absl.testing import parameterized\nimport numpy as np\n\nfrom tensorflow.python import tf2\nfrom tensorflow.python.checkpoint import checkpoint as trackable\nfrom tensorflow.python.checkpoint import graph_view\nfrom tensorflow.python.checkpoint import util as checkpoint_util\nfrom tensorflow.python.client import session\nfrom tensorflow.python.data.experimental.ops import counter\nfrom tensorflow.python.data.ops import dataset_ops\nfrom tensorflow.python.eager import backprop\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.eager import def_function\nfrom tensorflow.python.eager import wrap_function\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors_impl\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import sparse_tensor\nfrom tensorflow.python.framework import tensor_spec\nfrom tensorflow.python.framework import test_ops\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import gen_lookup_ops\nfrom tensorflow.python.ops import lookup_ops\nfrom tensorflow.python.ops import map_fn\nfrom tensorflow.python.ops import variables\nfrom tensorflow.python.ops.ragged import ragged_tensor\nfrom tensorflow.python.platform import test\nfrom tensorflow.python.saved_model import load as saved_model_load\nfrom tensorflow.python.saved_model import save as saved_model_save\nfrom tensorflow.python.trackable import asset\nfrom tensorflow.python.trackable import autotrackable\nfrom tensorflow.python.training import saver\nfrom tensorflow.python.training import server_lib\nfrom tensorflow.python.util import compat\n\n\nclass BaseLookupTableTest(test.TestCase):\n\n  def getHashTable(self):\n    if tf2.enabled():\n      return lookup_ops.StaticHashTable\n    else:\n      return lookup_ops.StaticHashTableV1\n\n  def getVocabularyTable(self):\n    if tf2.enabled():\n      return lookup_ops.StaticVocabularyTable\n    else:\n      return lookup_ops.StaticVocabularyTableV1\n\n  def initialize_table(self, table):\n    if not tf2.enabled():\n      self.evaluate(table.initializer)\n\n\nSKIP_ANONYMOUS_IN_TF1_REASON = (\n    \"In v1 graph mode, each self.evaluate call will execute the handle \"\n    \"creation op (e.g. AnonymousHashTable) which will create a new table \"\n    \"resource unrelated to other self.evaluate calls, so we can't test \"\n    \"anonymous resources with self.evaluate .\"\n)\n\n\n@parameterized.named_parameters(\n    (f\"_{is_anonymous}\", is_anonymous) for is_anonymous in [False, True])\nclass StaticHashTableTest(BaseLookupTableTest, parameterized.TestCase):\n\n  def testStaticHashTable(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    default_val = -1\n    keys = constant_op.constant([\"brain\", \"salad\", \"surgery\"])\n    values = constant_op.constant([0, 1, 2], dtypes.int64)\n    table = self.getHashTable()(\n        lookup_ops.KeyValueTensorInitializer(keys, values),\n        default_val,\n        experimental_is_anonymous=is_anonymous)\n    self.assertEqual(table._is_anonymous, is_anonymous)\n    self.initialize_table(table)\n\n    self.assertAllEqual(3, self.evaluate(table.size()))\n\n    input_string = constant_op.constant([\"brain\", \"salad\", \"tank\"])\n    output = table.lookup(input_string)\n    self.assertAllEqual([3], output.get_shape())\n\n    result = self.evaluate(output)\n    self.assertAllEqual([0, 1, -1], result)\n\n    exported_keys_tensor, exported_values_tensor = table.export()\n\n    self.assertItemsEqual([b\"brain\", b\"salad\", b\"surgery\"],\n                          self.evaluate(exported_keys_tensor))\n    self.assertItemsEqual([0, 1, 2], self.evaluate(exported_values_tensor))\n\n  def testStaticHashTableFindHighRank(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    default_val = -1\n    keys = constant_op.constant([\"brain\", \"salad\", \"surgery\"])\n    values = constant_op.constant([0, 1, 2], dtypes.int64)\n    table = self.getHashTable()(\n        lookup_ops.KeyValueTensorInitializer(keys, values),\n        default_val,\n        experimental_is_anonymous=is_anonymous)\n    self.initialize_table(table)\n\n    self.assertAllEqual(3, self.evaluate(table.size()))\n\n    input_string = constant_op.constant([[\"brain\", \"salad\"],\n                                         [\"tank\", \"tarkus\"]])\n    output = table.lookup(input_string)\n\n    result = self.evaluate(output)\n    self.assertAllEqual([[0, 1], [-1, -1]], result)\n\n  def testStaticHashTableInitWithPythonArrays(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    default_val = -1\n    keys = [\"brain\", \"salad\", \"surgery\"]\n    values = [0, 1, 2]\n    table = self.getHashTable()(\n        lookup_ops.KeyValueTensorInitializer(\n            keys, values, value_dtype=dtypes.int64),\n        default_val,\n        experimental_is_anonymous=is_anonymous)\n    self.initialize_table(table)\n\n    self.assertAllEqual(3, self.evaluate(table.size()))\n\n    input_string = constant_op.constant([\"brain\", \"salad\", \"tank\"])\n    output = table.lookup(input_string)\n\n    result = self.evaluate(output)\n    self.assertAllEqual([0, 1, -1], result)\n\n  def testStaticHashTableInitWithNumPyArrays(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    default_val = -1\n    keys = np.array([\"brain\", \"salad\", \"surgery\"], dtype=np.str_)\n    values = np.array([0, 1, 2], dtype=np.int64)\n    table = self.getHashTable()(\n        lookup_ops.KeyValueTensorInitializer(keys, values),\n        default_val,\n        experimental_is_anonymous=is_anonymous)\n    self.initialize_table(table)\n\n    self.assertAllEqual(3, self.evaluate(table.size()))\n\n    input_string = constant_op.constant([\"brain\", \"salad\", \"tank\"])\n    output = table.lookup(input_string)\n\n    result = self.evaluate(output)\n    self.assertAllEqual([0, 1, -1], result)\n\n  def testMultipleStaticHashTables(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    default_val = -1\n    keys = constant_op.constant([\"brain\", \"salad\", \"surgery\"])\n    values = constant_op.constant([0, 1, 2], dtypes.int64)\n\n    table1 = self.getHashTable()(\n        lookup_ops.KeyValueTensorInitializer(keys, values),\n        default_val,\n        experimental_is_anonymous=is_anonymous)\n    table2 = self.getHashTable()(\n        lookup_ops.KeyValueTensorInitializer(keys, values),\n        default_val,\n        experimental_is_anonymous=is_anonymous)\n    table3 = self.getHashTable()(\n        lookup_ops.KeyValueTensorInitializer(keys, values),\n        default_val,\n        experimental_is_anonymous=is_anonymous)\n\n    self.initialize_table(table1)\n    self.initialize_table(table2)\n    self.initialize_table(table3)\n    self.assertAllEqual(3, self.evaluate(table1.size()))\n    self.assertAllEqual(3, self.evaluate(table2.size()))\n    self.assertAllEqual(3, self.evaluate(table3.size()))\n\n    input_string = constant_op.constant([\"brain\", \"salad\", \"tank\"])\n    output1 = table1.lookup(input_string)\n    output2 = table2.lookup(input_string)\n    output3 = table3.lookup(input_string)\n\n    out1, out2, out3 = self.evaluate([output1, output2, output3])\n    self.assertAllEqual([0, 1, -1], out1)\n    self.assertAllEqual([0, 1, -1], out2)\n    self.assertAllEqual([0, 1, -1], out3)\n\n  def testStaticHashTableWithTensorDefault(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    default_val = constant_op.constant(-1, dtypes.int64)\n    keys = constant_op.constant([\"brain\", \"salad\", \"surgery\"])\n    values = constant_op.constant([0, 1, 2], dtypes.int64)\n    table = self.getHashTable()(\n        lookup_ops.KeyValueTensorInitializer(keys, values),\n        default_val,\n        experimental_is_anonymous=is_anonymous)\n    self.initialize_table(table)\n\n    input_string = constant_op.constant([\"brain\", \"salad\", \"tank\"])\n    output = table.lookup(input_string)\n\n    result = self.evaluate(output)\n    self.assertAllEqual([0, 1, -1], result)\n\n  def testStaticHashTableGetItem(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    default_val = constant_op.constant(-1, dtypes.int64)\n    keys = constant_op.constant([\"brain\", \"salad\", \"surgery\"])\n    values = constant_op.constant([0, 1, 2], dtypes.int64)\n    table = self.getHashTable()(\n        lookup_ops.KeyValueTensorInitializer(keys, values),\n        default_val,\n        experimental_is_anonymous=is_anonymous)\n    self.initialize_table(table)\n\n    input_string = constant_op.constant([\"brain\", \"salad\", \"tank\"])\n    output = table[input_string]\n\n    result = self.evaluate(output)\n    self.assertAllEqual([0, 1, -1], result)\n\n  def testStaticHashTableWithSparseTensorInput(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    default_val = constant_op.constant(-1, dtypes.int64)\n    keys = constant_op.constant([\"brain\", \"salad\", \"surgery\"])\n    values = constant_op.constant([0, 1, 2], dtypes.int64)\n    table = self.getHashTable()(\n        lookup_ops.KeyValueTensorInitializer(keys, values),\n        default_val,\n        experimental_is_anonymous=is_anonymous)\n    self.initialize_table(table)\n\n    sp_indices = [[0, 0], [0, 1], [1, 0]]\n    sp_shape = [2, 2]\n    input_tensor = sparse_tensor.SparseTensor(\n        constant_op.constant(sp_indices, dtypes.int64),\n        constant_op.constant([\"brain\", \"salad\", \"tank\"]),\n        constant_op.constant(sp_shape, dtypes.int64))\n    output = table.lookup(input_tensor)\n\n    out_indices, out_values, out_shape = self.evaluate(output)\n\n    self.assertAllEqual([0, 1, -1], out_values)\n    self.assertAllEqual(sp_indices, out_indices)\n    self.assertAllEqual(sp_shape, out_shape)\n\n  def testStaticHashTableWithRaggedTensorInput(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    default_val = constant_op.constant(-1, dtypes.int64)\n    keys = constant_op.constant([\"brain\", \"salad\", \"surgery\"])\n    values = constant_op.constant([0, 1, 2], dtypes.int64)\n    table = self.getHashTable()(\n        lookup_ops.KeyValueTensorInitializer(keys, values),\n        default_val,\n        experimental_is_anonymous=is_anonymous)\n    self.initialize_table(table)\n\n    row_splits = [0, 2, 3]\n    input_tensor = ragged_tensor.RaggedTensor.from_row_splits(\n        constant_op.constant([\"brain\", \"salad\", \"tank\"]),\n        constant_op.constant(row_splits, dtypes.int64))\n    output = table.lookup(input_tensor)\n\n    out = self.evaluate(output)\n\n    self.assertAllEqual([0, 1, -1], out.values)\n    self.assertAllEqual(row_splits, out.row_splits)\n\n  def testSignatureMismatch(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    default_val = -1\n    keys = constant_op.constant([\"brain\", \"salad\", \"surgery\"])\n    values = constant_op.constant([0, 1, 2], dtypes.int64)\n    table = self.getHashTable()(\n        lookup_ops.KeyValueTensorInitializer(keys, values),\n        default_val,\n        experimental_is_anonymous=is_anonymous)\n    self.initialize_table(table)\n\n    # Ref types do not produce a lookup signature mismatch.\n    input_string_ref = variables.Variable(\"brain\")\n    self.evaluate(input_string_ref.initializer)\n    self.assertEqual(0, self.evaluate(table.lookup(input_string_ref)))\n\n    input_string = constant_op.constant([1, 2, 3], dtypes.int64)\n    with self.assertRaises(TypeError):\n      table.lookup(input_string)\n\n    with self.assertRaises(TypeError):\n      self.getHashTable()(\n          lookup_ops.KeyValueTensorInitializer(keys, values),\n          \"UNK\",\n          experimental_is_anonymous=is_anonymous)\n\n  def testDTypes(self, is_anonymous):\n    default_val = -1\n    with self.assertRaises(TypeError):\n      self.getHashTable()(\n          lookup_ops.KeyValueTensorInitializer([\"a\"], [1], [dtypes.string],\n                                               dtypes.int64),\n          default_val,\n          experimental_is_anonymous=is_anonymous)\n\n  @test_util.run_v1_only(\"(Cached) Sessions not available in TF2.0\")\n  def testNotInitialized(self, is_anonymous):\n    with self.cached_session():\n      default_val = -1\n      table = self.getHashTable()(\n          lookup_ops.KeyValueTensorInitializer([\"a\"], [1],\n                                               value_dtype=dtypes.int64),\n          default_val,\n          experimental_is_anonymous=is_anonymous)\n\n      input_string = constant_op.constant([\"brain\", \"salad\", \"surgery\"])\n      output = table.lookup(input_string)\n\n      with self.assertRaisesOpError(\"Table not initialized\"):\n        self.evaluate(output)\n\n  @test_util.run_v1_only(\"(Cached) Sessions not available in TF2.0\")\n  def testInitializeTwice(self, is_anonymous):\n    with self.cached_session():\n      default_val = -1\n      keys = constant_op.constant([\"brain\", \"salad\", \"surgery\"])\n      values = constant_op.constant([0, 1, 2], dtypes.int64)\n      table = self.getHashTable()(\n          lookup_ops.KeyValueTensorInitializer(keys, values),\n          default_val,\n          experimental_is_anonymous=is_anonymous)\n      self.initialize_table(table)\n      # Make sure that initializing twice doesn't throw any errors.\n      self.initialize_table(table)\n\n  def testInitializationWithInvalidDimensions(self, is_anonymous):\n    default_val = -1\n    keys = constant_op.constant([\"brain\", \"salad\", \"surgery\"])\n    values = constant_op.constant([0, 1, 2, 3, 4], dtypes.int64)\n\n    raised_error = ValueError\n    if context.executing_eagerly():\n      raised_error = errors_impl.InvalidArgumentError\n    with self.assertRaises(raised_error):\n      self.getHashTable()(\n          lookup_ops.KeyValueTensorInitializer(keys, values),\n          default_val,\n          experimental_is_anonymous=is_anonymous)\n\n  @test_util.run_v1_only(\"Sessions not available in TF2.0\")\n  def testMultipleSessions(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    # Start a server\n    server = server_lib.Server({\"local0\": [\"localhost:0\"]},\n                               protocol=\"grpc\",\n                               start=True)\n    # Create two sessions sharing the same state\n    session1 = session.Session(server.target)\n    session2 = session.Session(server.target)\n\n    default_val = -1\n    keys = constant_op.constant([\"brain\", \"salad\", \"surgery\"])\n    values = constant_op.constant([0, 1, 2], dtypes.int64)\n    table = self.getHashTable()(\n        lookup_ops.KeyValueTensorInitializer(keys, values),\n        default_val,\n        name=\"t1\",\n        experimental_is_anonymous=is_anonymous)\n\n    # Init the table in the first session.\n    with session1:\n      self.initialize_table(table)\n      self.assertAllEqual(3, self.evaluate(table.size()))\n\n    # Init the table in the second session and verify that we do not get a\n    # \"Table already initialized\" error.\n    with session2:\n      self.evaluate(table.initializer)\n      self.assertAllEqual(3, self.evaluate(table.size()))\n\n  @test_util.run_v2_only\n  def testImportedHashTable(self, is_anonymous):\n    g = ops.Graph()\n    with g.as_default():\n      t = lookup_ops.StaticHashTable(\n          lookup_ops.KeyValueTensorInitializer([\"a\"], [1]),\n          2)\n      init_op = t._init_op\n      op = t.lookup(ops.convert_to_tensor([\"a\"]))\n      meta_graph = saver.export_meta_graph()\n\n    def f():\n      saver.import_meta_graph(meta_graph)\n      return ops.get_default_graph().get_tensor_by_name(op.name)\n\n    wrapped = wrap_function.wrap_function(f, [])\n    pruned_init_fn = wrapped.prune(\n        (), [wrapped.graph.get_operation_by_name(init_op.name)])\n    self.evaluate(pruned_init_fn())\n    self.assertAllEqual([1], wrapped())\n\n  def testStaticHashTableInt32String(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    default_val = \"n/a\"\n    keys = constant_op.constant([0, 1, 2], dtypes.int32)\n    values = constant_op.constant([\"brain\", \"salad\", \"surgery\"])\n    table = self.getHashTable()(\n        lookup_ops.KeyValueTensorInitializer(keys, values),\n        default_val,\n        experimental_is_anonymous=is_anonymous)\n    self.initialize_table(table)\n\n    input_tensor = constant_op.constant([0, 1, -1])\n    output = table.lookup(input_tensor)\n\n    result = self.evaluate(output)\n    self.assertAllEqual([b\"brain\", b\"salad\", b\"n/a\"], result)\n\n  def testTableUseInFunction(self, is_anonymous):\n    if not context.executing_eagerly():\n      self.skipTest(\"Only Eager mode test.\")\n    keys = constant_op.constant([0, 1, 2], dtypes.int32)\n    values = constant_op.constant([\"brain\", \"salad\", \"surgery\"])\n    table = self.getHashTable()(\n        lookup_ops.KeyValueTensorInitializer(keys, values),\n        \"n/a\",\n        experimental_is_anonymous=is_anonymous)\n\n    @def_function.function\n    def lookup_table_func(k):\n      return table.lookup(k)\n\n    result = lookup_table_func(constant_op.constant([0, 1, -1]))\n    self.assertAllEqual([b\"brain\", b\"salad\", b\"n/a\"], result)\n    result = lookup_table_func(constant_op.constant([2, -1, 1]))\n    self.assertAllEqual([b\"surgery\", b\"n/a\", b\"salad\"], result)\n\n  def testTableCreatedInFunction(self, is_anonymous):\n    if not context.executing_eagerly():\n      self.skipTest(\"Only Eager mode test.\")\n    keys = constant_op.constant([0, 1, 2], dtypes.int32)\n    values = constant_op.constant([\"brain\", \"salad\", \"surgery\"])\n\n    @def_function.function\n    def lookup_table_func(k):\n      table = self.getHashTable()(\n          lookup_ops.KeyValueTensorInitializer(keys, values),\n          \"n/a\",\n          experimental_is_anonymous=is_anonymous)\n      return table.lookup(k)\n\n    result = lookup_table_func(constant_op.constant([0, 1, -1]))\n    self.assertAllEqual([b\"brain\", b\"salad\", b\"n/a\"], result)\n    result = lookup_table_func(constant_op.constant([2, -1, 1]))\n    self.assertAllEqual([b\"surgery\", b\"n/a\", b\"salad\"], result)\n\n  def testTwoTablesInControlFlow(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    keys = constant_op.constant([1, 2, 3], dtypes.int32)\n    values = constant_op.constant([5, 10, 15], dtypes.int32)\n\n    def table_func1(x):\n      table = self.getHashTable()(\n          lookup_ops.KeyValueTensorInitializer(keys, values),\n          -1,\n          experimental_is_anonymous=is_anonymous)\n      return table.lookup(x)\n\n    elems = np.array([2, 4, 1], dtype=np.int32)\n    result1 = map_fn.map_fn(table_func1, elems, dtype=dtypes.int32)\n\n    def table_func2(x):\n      table = self.getHashTable()(\n          lookup_ops.KeyValueTensorInitializer(keys, values),\n          -1,\n          experimental_is_anonymous=is_anonymous)\n      return table.lookup(x)\n\n    elems = np.array([2, 4, 1], dtype=np.int32)\n    result2 = map_fn.map_fn(table_func2, elems, dtype=dtypes.int32)\n\n    self.evaluate(lookup_ops.tables_initializer())\n\n    self.assertAllEqual([10, -1, 5], self.evaluate(result1))\n    self.assertAllEqual([10, -1, 5], self.evaluate(result2))\n\n  @test_util.enable_control_flow_v2\n  def testLookupTableInWhileV2(self, is_anonymous):\n    lookup = self.getHashTable()(\n        lookup_ops.KeyValueTensorInitializer(\n            constant_op.constant([2, 5], dtype=dtypes.int64),\n            constant_op.constant([-10.0, 1], dtype=dtypes.float32)),\n        -1,\n        experimental_is_anonymous=is_anonymous)\n\n    beta = variables.Variable(1.0, trainable=True)\n\n    @def_function.function\n    def get_loss(unused_beta):\n      return map_fn.map_fn(\n          lookup.lookup,\n          constant_op.constant([2, 3], dtype=dtypes.int64),\n          dtype=dtypes.float32)\n\n    with backprop.GradientTape() as tape:\n      loss = get_loss(beta)\n\n    self.assertIsNone(tape.gradient(loss, beta))\n\n  @test_util.enable_control_flow_v2\n  def testLookupTableInCondV2(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    lookup = self.getHashTable()(\n        lookup_ops.KeyValueTensorInitializer(\n            constant_op.constant([2, 5], dtype=dtypes.int64),\n            constant_op.constant([-10.0, 1], dtype=dtypes.float32)),\n        -1,\n        experimental_is_anonymous=is_anonymous)\n\n    beta = variables.Variable(1.0, trainable=True)\n\n    @def_function.function\n    def get_loss(beta):\n\n      def true_fn():\n        return lookup.lookup(constant_op.constant(2, dtype=dtypes.int64))\n\n      def false_fn():\n        return constant_op.constant(0, dtype=dtypes.float32)\n\n      return beta * control_flow_ops.cond(\n          constant_op.constant(True), true_fn=true_fn, false_fn=false_fn)\n\n    with backprop.GradientTape() as tape:\n      loss = get_loss(beta)\n    grad = tape.gradient(loss, beta)\n    self.evaluate(variables.global_variables_initializer())\n    self.evaluate(lookup_ops.tables_initializer())\n    self.assertAllEqual(grad, -10.)\n\n  def testImportShapeInference(self, is_anonymous):\n    v = variables.Variable(1)\n\n    @def_function.function(jit_compile=True)\n    def foo():\n      return gen_lookup_ops.lookup_table_import_v2(\n          table_handle=v.handle, keys=[1.1, 2.2], values=1\n      )\n\n    with self.assertRaisesRegex(\n        ValueError, r\"Shape must be at least rank 1 but is rank 0\"\n    ):\n      foo()\n\n  def testExportShapeInference(self, is_anonymous):\n    table = self.getHashTable()(\n        lookup_ops.KeyValueTensorInitializer(\n            constant_op.constant([2, 5], dtype=dtypes.int64),\n            constant_op.constant([-10.0, 1], dtype=dtypes.float32)),\n        -1,\n        experimental_is_anonymous=is_anonymous)\n    actual_shapes = [t.shape for t in table.export()]\n    inferred_shapes = []\n\n    @def_function.function\n    def f():\n      for t in table.export():\n        inferred_shapes.append(t.shape)\n\n    f()\n    self.assertLen(actual_shapes, 2)\n    self.assertLen(inferred_shapes, 2)\n    self.assertTrue(inferred_shapes[0].is_compatible_with(actual_shapes[0]))\n    self.assertTrue(inferred_shapes[1].is_compatible_with(actual_shapes[1]))\n\n  @test_util.run_v2_only\n  def testSavedModelSaveRestore(self, is_anonymous):\n    save_dir = os.path.join(self.get_temp_dir(), \"save_restore\")\n    save_path = os.path.join(tempfile.mkdtemp(prefix=save_dir), \"hash\")\n\n    root = autotrackable.AutoTrackable()\n\n    default_value = -1\n    keys = constant_op.constant([11, 12, 13], dtypes.int64)\n    values = constant_op.constant([0, 1, 2], dtypes.int64)\n    root.table = self.getHashTable()(\n        lookup_ops.KeyValueTensorInitializer(keys, values),\n        default_value,\n        experimental_is_anonymous=is_anonymous)\n\n    @def_function.function(\n        input_signature=[tensor_spec.TensorSpec((), dtypes.int64)])\n    def lookup(key):\n      return root.table.lookup(key)\n\n    @def_function.function(input_signature=[])\n    def size():\n      return root.table.size()\n\n    @def_function.function(input_signature=[])\n    def is_ref_counting():\n      return test_ops.is_resource_handle_ref_counting(\n          root.table.resource_handle)\n\n    root.lookup = lookup\n    root.size = size\n    root.is_ref_counting = is_ref_counting\n\n    self.assertEqual(root.table.size(), 3)\n    self.assertEqual(root.lookup(12), 1)\n    self.assertEqual(root.lookup(10), -1)\n    self.assertLen(root.table.export()[0], 3)\n    self.assertEqual(root.is_ref_counting(), is_anonymous)\n\n    saved_model_save.save(root, save_path)\n\n    del root\n    loaded = saved_model_load.load(save_path)\n    self.assertEqual(loaded.size(), 3)\n    self.assertEqual(loaded.lookup(12), 1)\n    self.assertEqual(loaded.lookup(10), -1)\n    self.assertEqual(loaded.is_ref_counting(), is_anonymous)\n\n\n@parameterized.named_parameters(\n    (f\"_{is_anonymous}\", is_anonymous) for is_anonymous in [False, True])\nclass KeyValueTensorInitializerTest(BaseLookupTableTest):\n\n  def test_string(self, is_anonymous):\n    init = lookup_ops.KeyValueTensorInitializer(\n        (\"brain\", \"salad\", \"surgery\"), (0, 1, 2), dtypes.string, dtypes.int64)\n    table = self.getHashTable()(\n        init, default_value=-1, experimental_is_anonymous=is_anonymous)\n    self.initialize_table(table)\n\n  def test_multiple_tables(self, is_anonymous):\n    with ops.name_scope(\"table_scope\"):\n      init1 = lookup_ops.KeyValueTensorInitializer(\n          (\"brain\", \"salad\", \"surgery\"), (0, 1, 2), dtypes.string, dtypes.int64)\n      table1 = self.getHashTable()(\n          init1, default_value=-1, experimental_is_anonymous=is_anonymous)\n      if not context.executing_eagerly():\n        self.assertEqual(\"hash_table\", table1.name)\n        self.assertEqual(\"table_scope/hash_table\",\n                         table1.resource_handle.op.name)\n      init2 = lookup_ops.KeyValueTensorInitializer(\n          (\"brain\", \"salad\", \"surgery\"), (0, 1, 2), dtypes.string, dtypes.int64)\n      table2 = self.getHashTable()(\n          init2, default_value=-1, experimental_is_anonymous=is_anonymous)\n      if not context.executing_eagerly():\n        self.assertEqual(\"hash_table_1\", table2.name)\n        self.assertEqual(\"table_scope/hash_table_1\",\n                         table2.resource_handle.op.name)\n\n  def test_int64(self, is_anonymous):\n    init = lookup_ops.KeyValueTensorInitializer((42, 1, -1000), (0, 1, 2),\n                                                dtypes.int64, dtypes.int64)\n    table = self.getHashTable()(\n        init, default_value=-1, experimental_is_anonymous=is_anonymous)\n    self.initialize_table(table)\n\n  def test_int32(self, is_anonymous):\n    init = lookup_ops.KeyValueTensorInitializer((42, 1, -1000), (0, 1, 2),\n                                                dtypes.int32, dtypes.int64)\n    with self.assertRaises(errors_impl.OpError):\n      table = self.getHashTable()(\n          init, default_value=-1, experimental_is_anonymous=is_anonymous)\n      self.initialize_table(table)\n\n\n@parameterized.named_parameters(\n    (f\"_{is_anonymous}\", is_anonymous) for is_anonymous in [False, True])\nclass InitializeTableFromFileOpTest(BaseLookupTableTest):\n\n  def _createVocabFile(self, basename, values=(\"brain\", \"salad\", \"surgery\")):\n    vocabulary_file = os.path.join(self.get_temp_dir(), basename)\n    with open(vocabulary_file, \"w\") as f:\n      f.write(\"\\n\".join(values) + \"\\n\")\n    return vocabulary_file\n\n  def testInitializeStringTable(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    vocabulary_file = self._createVocabFile(\"one_column_1.txt\")\n    default_value = -1\n    init = lookup_ops.TextFileInitializer(\n        vocabulary_file, dtypes.string, lookup_ops.TextFileIndex.WHOLE_LINE,\n        dtypes.int64, lookup_ops.TextFileIndex.LINE_NUMBER)\n    self.assertIn(\"one_column_1.txt_-2_-1\", init._shared_name)\n    table = self.getHashTable()(\n        init, default_value, experimental_is_anonymous=is_anonymous)\n    self.initialize_table(table)\n\n    output = table.lookup(constant_op.constant([\"brain\", \"salad\", \"tank\"]))\n\n    result = self.evaluate(output)\n    self.assertAllEqual([0, 1, -1], result)\n\n  def testInitializeInt64Table(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    vocabulary_file = self._createVocabFile(\n        \"one_column_int64.txt\", values=(\"42\", \"1\", \"-1000\"))\n\n    with self.cached_session():\n      default_value = -1\n      init = lookup_ops.TextFileInitializer(\n          vocabulary_file, dtypes.int64, lookup_ops.TextFileIndex.WHOLE_LINE,\n          dtypes.int64, lookup_ops.TextFileIndex.LINE_NUMBER)\n      self.assertIn(\"one_column_int64.txt_-2_-1\", init._shared_name)\n      table = self.getHashTable()(\n          init, default_value, experimental_is_anonymous=is_anonymous)\n      self.initialize_table(table)\n\n      output = table.lookup(\n          constant_op.constant((42, 1, 11), dtype=dtypes.int64))\n\n      result = self.evaluate(output)\n      self.assertAllEqual([0, 1, -1], result)\n\n  def testInitializeIndexTable(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    vocabulary_file = self._createVocabFile(\"one_column_2.txt\")\n\n    with self.cached_session():\n      default_value = \"UNK\"\n      key_index = lookup_ops.TextFileIndex.LINE_NUMBER\n      value_index = lookup_ops.TextFileIndex.WHOLE_LINE\n      init = lookup_ops.TextFileInitializer(\n          vocabulary_file, dtypes.int64, key_index, dtypes.string, value_index)\n      self.assertIn(\"one_column_2.txt_-1_-2\", init._shared_name)\n      table = self.getHashTable()(\n          init, default_value, experimental_is_anonymous=is_anonymous)\n      self.initialize_table(table)\n\n      input_values = constant_op.constant([0, 1, 2, 3], dtypes.int64)\n      output = table.lookup(input_values)\n\n      result = self.evaluate(output)\n      self.assertAllEqual([b\"brain\", b\"salad\", b\"surgery\", b\"UNK\"], result)\n\n  def testMultiColumn(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    vocabulary_file = os.path.join(self.get_temp_dir(), \"three_columns.txt\")\n    with open(vocabulary_file, \"w\") as f:\n      f.write(\"\\n\".join([\"0\\tbrain\\t1\", \"1\\tsalad\\t5\", \"2\\tsurgery\\t6\"]) + \"\\n\")\n\n    with self.cached_session():\n      default_value = -1\n      key_index = 1\n      value_index = 2\n\n      init = lookup_ops.TextFileInitializer(\n          vocabulary_file, dtypes.string, key_index, dtypes.int64, value_index)\n      self.assertIn(\"three_columns.txt_1_2\", init._shared_name)\n      table = self.getHashTable()(\n          init, default_value, experimental_is_anonymous=is_anonymous)\n      self.initialize_table(table)\n\n      input_string = constant_op.constant([\"brain\", \"salad\", \"surgery\"])\n      output = table.lookup(input_string)\n\n      result = self.evaluate(output)\n      self.assertAllEqual([1, 5, 6], result)\n\n  def testInvalidDataTypeInMultiColumn(self, is_anonymous):\n    vocabulary_file = os.path.join(self.get_temp_dir(), \"three_columns.txt\")\n    with open(vocabulary_file, \"w\") as f:\n      f.write(\"\\n\".join([\"0\\tbrain\\t1\", \"1\\tsalad\\t5\", \"2\\tsurgery\\t6\"]) + \"\\n\")\n\n    with self.cached_session():\n      default_value = -1\n      key_index = 2\n      value_index = 1\n      init = lookup_ops.TextFileInitializer(\n          vocabulary_file, dtypes.string, key_index, dtypes.int64, value_index)\n      self.assertIn(\"three_columns.txt_2_1\", init._shared_name)\n      with self.assertRaisesOpError(\"is not a valid\"):\n        table = self.getHashTable()(\n            init, default_value, experimental_is_anonymous=is_anonymous)\n        self.initialize_table(table)\n\n  def testInvalidDataType(self, is_anonymous):\n    vocabulary_file = self._createVocabFile(\"one_column_3.txt\")\n\n    with self.cached_session():\n      default_value = \"UNK\"\n      key_index = lookup_ops.TextFileIndex.WHOLE_LINE\n      value_index = lookup_ops.TextFileIndex.LINE_NUMBER\n\n      with self.assertRaises(ValueError):\n        init = lookup_ops.TextFileInitializer(vocabulary_file, dtypes.int64,\n                                              key_index, dtypes.string,\n                                              value_index)\n        self.assertIn(\"one_column_3.txt_-2_-1\", init._shared_name)\n        self.getHashTable()(\n            init, default_value, experimental_is_anonymous=is_anonymous)\n\n  def testInvalidIndex(self, is_anonymous):\n    vocabulary_file = self._createVocabFile(\"one_column_4.txt\")\n    with self.cached_session():\n      default_value = -1\n      key_index = 1  # second column of the line\n      value_index = lookup_ops.TextFileIndex.LINE_NUMBER\n      init = lookup_ops.TextFileInitializer(\n          vocabulary_file, dtypes.string, key_index, dtypes.int64, value_index)\n      self.assertIn(\"one_column_4.txt_1_-1\", init._shared_name)\n\n      with self.assertRaisesOpError(\"Invalid number of columns\"):\n        table = self.getHashTable()(\n            init, default_value, experimental_is_anonymous=is_anonymous)\n        self.initialize_table(table)\n\n  def testInitializeSameTableWithMultipleNodes(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    vocabulary_file = self._createVocabFile(\"one_column_5.txt\")\n\n    with self.cached_session():\n      default_value = -1\n      init1 = lookup_ops.TextFileInitializer(\n          vocabulary_file, dtypes.string, lookup_ops.TextFileIndex.WHOLE_LINE,\n          dtypes.int64, lookup_ops.TextFileIndex.LINE_NUMBER)\n      self.assertIn(\"one_column_5.txt_-2_-1\", init1._shared_name)\n      table1 = self.getHashTable()(\n          init1, default_value, experimental_is_anonymous=is_anonymous)\n      init2 = lookup_ops.TextFileInitializer(\n          vocabulary_file, dtypes.string, lookup_ops.TextFileIndex.WHOLE_LINE,\n          dtypes.int64, lookup_ops.TextFileIndex.LINE_NUMBER)\n      self.assertIn(\"one_column_5.txt_-2_-1\", init2._shared_name)\n      table2 = self.getHashTable()(\n          init2, default_value, experimental_is_anonymous=is_anonymous)\n      init3 = lookup_ops.TextFileInitializer(\n          vocabulary_file, dtypes.string, lookup_ops.TextFileIndex.WHOLE_LINE,\n          dtypes.int64, lookup_ops.TextFileIndex.LINE_NUMBER)\n      self.assertIn(\"one_column_5.txt_-2_-1\", init3._shared_name)\n      table3 = self.getHashTable()(\n          init3, default_value, experimental_is_anonymous=is_anonymous)\n\n      self.evaluate(lookup_ops.tables_initializer())\n\n      input_string = constant_op.constant([\"brain\", \"salad\", \"tank\"])\n\n      output1 = table1.lookup(input_string)\n      output2 = table2.lookup(input_string)\n      output3 = table3.lookup(input_string)\n\n      out1, out2, out3 = self.evaluate([output1, output2, output3])\n      self.assertAllEqual([0, 1, -1], out1)\n      self.assertAllEqual([0, 1, -1], out2)\n      self.assertAllEqual([0, 1, -1], out3)\n\n  def testInitializeTableWithNoFilename(self, is_anonymous):\n    with self.cached_session():\n      default_value = -1\n      with self.assertRaises(ValueError):\n        self.getHashTable()(\n            lookup_ops.TextFileInitializer(\n                \"\", dtypes.string, lookup_ops.TextFileIndex.WHOLE_LINE,\n                dtypes.int64, lookup_ops.TextFileIndex.LINE_NUMBER),\n            default_value,\n            experimental_is_anonymous=is_anonymous)\n\n  def testInitializeWithVocabSize(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    with self.cached_session():\n      default_value = -1\n      vocab_size = 3\n      vocabulary_file1 = self._createVocabFile(\"one_column6.txt\")\n      init1 = lookup_ops.TextFileInitializer(\n          vocabulary_file1,\n          dtypes.string,\n          lookup_ops.TextFileIndex.WHOLE_LINE,\n          dtypes.int64,\n          lookup_ops.TextFileIndex.LINE_NUMBER,\n          vocab_size=vocab_size)\n      self.assertIn(\"one_column6.txt_3_-2_-1\", init1._shared_name)\n      table1 = self.getHashTable()(\n          init1, default_value, experimental_is_anonymous=is_anonymous)\n\n      # Initialize from file.\n      self.initialize_table(table1)\n      self.assertEqual(vocab_size, self.evaluate(table1.size()))\n\n      vocabulary_file2 = self._createVocabFile(\"one_column7.txt\")\n      vocab_size = 5\n      init2 = lookup_ops.TextFileInitializer(\n          vocabulary_file2,\n          dtypes.string,\n          lookup_ops.TextFileIndex.WHOLE_LINE,\n          dtypes.int64,\n          lookup_ops.TextFileIndex.LINE_NUMBER,\n          vocab_size=vocab_size)\n      self.assertIn(\"one_column7.txt_5_-2_-1\", init2._shared_name)\n      with self.assertRaisesOpError(\"Invalid vocab_size\"):\n        table2 = self.getHashTable()(\n            init2, default_value, experimental_is_anonymous=is_anonymous)\n        self.initialize_table(table2)\n\n      vocab_size = 1\n      vocabulary_file3 = self._createVocabFile(\"one_column3.txt\")\n      init3 = lookup_ops.TextFileInitializer(\n          vocabulary_file3,\n          dtypes.string,\n          lookup_ops.TextFileIndex.WHOLE_LINE,\n          dtypes.int64,\n          lookup_ops.TextFileIndex.LINE_NUMBER,\n          vocab_size=vocab_size)\n      self.assertIn(\"one_column3.txt_1_-2_-1\", init3._shared_name)\n      table3 = self.getHashTable()(\n          init3, default_value, experimental_is_anonymous=is_anonymous)\n\n      # Smaller vocab size reads only vocab_size records.\n      self.initialize_table(table3)\n      self.assertEqual(vocab_size, self.evaluate(table3.size()))\n\n  @test_util.run_v1_only(\"placeholder usage\")\n  def testFeedVocabularyName(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    vocabulary_file = self._createVocabFile(\"feed_vocabulary.txt\")\n\n    with self.cached_session():\n      default_value = -1\n      init = lookup_ops.TextFileInitializer(\n          \"old_file.txt\", dtypes.string, lookup_ops.TextFileIndex.WHOLE_LINE,\n          dtypes.int64, lookup_ops.TextFileIndex.LINE_NUMBER)\n      self.assertIn(\"old_file.txt_-2_-1\", init._shared_name)\n      table = self.getHashTable()(\n          init, default_value, experimental_is_anonymous=is_anonymous)\n\n      # Initialize with non existing file (old_file.txt) should fail.\n      # TODO(yleon): Update message, which might change per FileSystem.\n      with self.assertRaisesOpError(\"old_file.txt\"):\n        self.evaluate(table.initializer)\n\n      # Initialize the model feeding the vocabulary file.\n      filenames = ops.get_collection(ops.GraphKeys.ASSET_FILEPATHS)\n      table.initializer.run(feed_dict={filenames[0]: vocabulary_file})\n\n      input_string = constant_op.constant([\"brain\", \"salad\", \"tank\"])\n      output = table.lookup(input_string)\n\n      result = self.evaluate(output)\n      self.assertAllEqual([0, 1, -1], result)\n\n  def testInvalidFilenames(self, is_anonymous):\n    vocabulary_file = self._createVocabFile(\"filename_shape.txt\")\n\n    with self.cached_session():\n      default_value = -1\n\n      # Invalid data type\n      other_type = constant_op.constant(1)\n      with self.assertRaises(Exception) as cm:\n        self.getHashTable()(\n            lookup_ops.TextFileInitializer(\n                other_type, dtypes.string, lookup_ops.TextFileIndex.WHOLE_LINE,\n                dtypes.int64, lookup_ops.TextFileIndex.LINE_NUMBER),\n            default_value,\n            experimental_is_anonymous=is_anonymous)\n      self.assertIsInstance(cm.exception, (ValueError, TypeError))\n\n      # Non-scalar filename\n      filenames = constant_op.constant([vocabulary_file, vocabulary_file])\n      if not context.executing_eagerly():\n        with self.assertRaises(Exception) as cm:\n          self.getHashTable()(\n              lookup_ops.TextFileInitializer(\n                  filenames, dtypes.string, lookup_ops.TextFileIndex.WHOLE_LINE,\n                  dtypes.int64, lookup_ops.TextFileIndex.LINE_NUMBER),\n              default_value,\n              experimental_is_anonymous=is_anonymous)\n        self.assertIsInstance(cm.exception, (ValueError, TypeError))\n      else:\n        with self.assertRaises(errors_impl.InvalidArgumentError):\n          self.getHashTable()(\n              lookup_ops.TextFileInitializer(\n                  filenames, dtypes.string, lookup_ops.TextFileIndex.WHOLE_LINE,\n                  dtypes.int64, lookup_ops.TextFileIndex.LINE_NUMBER),\n              default_value,\n              experimental_is_anonymous=is_anonymous)\n\n  def testIdToStringTable(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    vocab_file = self._createVocabFile(\"feat_to_id_1.txt\")\n    with self.cached_session():\n      default_value = \"UNK\"\n      vocab_size = 3\n      init = lookup_ops.TextFileStringTableInitializer(\n          vocab_file, vocab_size=vocab_size)\n      self.assertTrue(\"feat_to_id_1.txt_3_-1_-2\", init._shared_name)\n      table = self.getHashTable()(\n          init, default_value, experimental_is_anonymous=is_anonymous)\n\n      self.initialize_table(table)\n\n      input_values = constant_op.constant([0, 1, 2, 3], dtypes.int64)\n\n      out = table.lookup(input_values)\n      self.assertAllEqual([b\"brain\", b\"salad\", b\"surgery\", b\"UNK\"],\n                          self.evaluate(out))\n      self.assertEqual(vocab_size, self.evaluate(table.size()))\n\n  def testStringToIdTable(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    vocab_file = self._createVocabFile(\"feat_to_id_2.txt\")\n    with self.cached_session():\n      default_value = -1\n      vocab_size = 3\n      init = lookup_ops.TextFileIdTableInitializer(\n          vocab_file, vocab_size=vocab_size)\n      self.assertTrue(\"feat_to_id_2.txt_3_-1_-2\", init._shared_name)\n      table = self.getHashTable()(\n          init, default_value, experimental_is_anonymous=is_anonymous)\n      self.initialize_table(table)\n\n      input_string = constant_op.constant([\"brain\", \"salad\", \"surgery\", \"UNK\"])\n\n      out = table.lookup(input_string)\n      self.assertAllEqual([0, 1, 2, -1], self.evaluate(out))\n      self.assertEqual(vocab_size, self.evaluate(table.size()))\n\n  def testInt64ToIdTable(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    vocab_file = self._createVocabFile(\n        \"feat_to_id_3.txt\", values=(\"42\", \"1\", \"-1000\"))\n    with self.cached_session():\n      default_value = -1\n      vocab_size = 3\n      init = lookup_ops.TextFileIdTableInitializer(\n          vocab_file, vocab_size=vocab_size, key_dtype=dtypes.int64)\n      self.assertTrue(\"feat_to_id_3.txt_3_-1_-2\", init._shared_name)\n      table = self.getHashTable()(\n          init, default_value, experimental_is_anonymous=is_anonymous)\n      self.initialize_table(table)\n\n      out = table.lookup(\n          constant_op.constant((42, 1, -1000, 11), dtype=dtypes.int64))\n      self.assertAllEqual((0, 1, 2, -1), self.evaluate(out))\n      self.assertEqual(vocab_size, self.evaluate(table.size()))\n\n\n@parameterized.named_parameters(\n    (f\"_{is_anonymous}\", is_anonymous) for is_anonymous in [False, True])\nclass StaticVocabularyTableTest(BaseLookupTableTest):\n\n  def _createVocabFile(self, basename, values=(\"brain\", \"salad\", \"surgery\")):\n    vocabulary_file = os.path.join(self.get_temp_dir(), basename)\n    with open(vocabulary_file, \"w\") as f:\n      f.write(\"\\n\".join(values) + \"\\n\")\n    return vocabulary_file\n\n  def testStringStaticVocabularyTable(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    vocab_file = self._createVocabFile(\"feat_to_id_1.txt\")\n    vocab_size = 3\n    oov_buckets = 1\n    table = self.getVocabularyTable()(\n        lookup_ops.TextFileIdTableInitializer(\n            vocab_file, vocab_size=vocab_size),\n        oov_buckets,\n        experimental_is_anonymous=is_anonymous)\n\n    self.initialize_table(table)\n\n    input_string = constant_op.constant([\"brain\", \"salad\", \"surgery\", \"UNK\"])\n\n    out = table.lookup(input_string)\n    self.assertAllEqual([0, 1, 2, 3], self.evaluate(out))\n    self.assertEqual(vocab_size + oov_buckets, self.evaluate(table.size()))\n\n  def testStaticVocabularyTableGetItem(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    vocab_file = self._createVocabFile(\"feat_to_id_1.txt\")\n    vocab_size = 3\n    oov_buckets = 1\n    table = self.getVocabularyTable()(\n        lookup_ops.TextFileIdTableInitializer(\n            vocab_file, vocab_size=vocab_size),\n        oov_buckets,\n        experimental_is_anonymous=is_anonymous)\n\n    self.initialize_table(table)\n\n    input_string = constant_op.constant([\"brain\", \"salad\", \"surgery\", \"UNK\"])\n\n    out = table[input_string]\n    self.assertAllEqual([0, 1, 2, 3], self.evaluate(out))\n    self.assertEqual(vocab_size + oov_buckets, self.evaluate(table.size()))\n\n  def testInt32StaticVocabularyTable(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    vocab_file = self._createVocabFile(\"feat_to_id_2.txt\", (\"42\", \"1\", \"-1000\"))\n    vocab_size = 3\n    oov_buckets = 1\n    table = self.getVocabularyTable()(\n        lookup_ops.TextFileIdTableInitializer(\n            vocab_file, vocab_size=vocab_size, key_dtype=dtypes.int64),\n        oov_buckets,\n        lookup_key_dtype=dtypes.int32,\n        experimental_is_anonymous=is_anonymous)\n\n    self.initialize_table(table)\n\n    values = constant_op.constant((42, 1, -1000, 11), dtype=dtypes.int32)\n\n    out = table.lookup(values)\n    self.assertAllEqual([0, 1, 2, 3], self.evaluate(out))\n    self.assertEqual(vocab_size + oov_buckets, self.evaluate(table.size()))\n\n  def testInt64StaticVocabularyTable(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    vocab_file = self._createVocabFile(\"feat_to_id_3.txt\", (\"42\", \"1\", \"-1000\"))\n    vocab_size = 3\n    oov_buckets = 1\n    table = self.getVocabularyTable()(\n        lookup_ops.TextFileIdTableInitializer(\n            vocab_file, vocab_size=vocab_size, key_dtype=dtypes.int64),\n        oov_buckets,\n        experimental_is_anonymous=is_anonymous)\n\n    self.initialize_table(table)\n\n    values = constant_op.constant((42, 1, -1000, 11), dtype=dtypes.int64)\n\n    out = table.lookup(values)\n    self.assertAllEqual([0, 1, 2, 3], self.evaluate(out))\n    self.assertEqual(vocab_size + oov_buckets, self.evaluate(table.size()))\n\n  def testStringStaticVocabularyTableNoInitializer(self, is_anonymous):\n    oov_buckets = 5\n\n    # Set a table that only uses hash buckets, for each input value returns\n    # an id calculated by fingerprint(\"input\") mod oov_buckets.\n    table = self.getVocabularyTable()(\n        None, oov_buckets, experimental_is_anonymous=is_anonymous)\n    self.initialize_table(table)\n\n    values = constant_op.constant((\"brain\", \"salad\", \"surgery\"))\n\n    out = table.lookup(values)\n    self.assertAllEqual(\n        [\n            3,  # fingerprint(\"brain\") mod 5.\n            1,  # fingerprint(\"salad\") mod 5.\n            4  # fingerprint(\"surgery\") mod 5\n        ],\n        self.evaluate(out))\n    self.assertEqual(oov_buckets, self.evaluate(table.size()))\n\n  def testStaticVocabularyTableWithMultipleInitializers(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    vocab_file = self._createVocabFile(\"feat_to_id_4.txt\")\n    vocab_size = 3\n    oov_buckets = 3\n\n    init = lookup_ops.TextFileIdTableInitializer(\n        vocab_file, vocab_size=vocab_size)\n    table1 = self.getVocabularyTable()(\n        init,\n        oov_buckets,\n        name=\"table1\",\n        experimental_is_anonymous=is_anonymous)\n\n    table2 = self.getVocabularyTable()(\n        init,\n        oov_buckets,\n        name=\"table2\",\n        experimental_is_anonymous=is_anonymous)\n\n    self.evaluate(lookup_ops.tables_initializer())\n\n    input_string = constant_op.constant(\n        [\"fruit\", \"brain\", \"salad\", \"surgery\", \"UNK\"])\n\n    out1 = table1.lookup(input_string)\n    out2 = table2.lookup(input_string)\n\n    out1, out2 = self.evaluate([out1, out2])\n    self.assertAllEqual([5, 0, 1, 2, 5], out1)\n    self.assertAllEqual([5, 0, 1, 2, 5], out2)\n    self.assertEqual(vocab_size + oov_buckets, self.evaluate(table1.size()))\n    self.assertEqual(vocab_size + oov_buckets, self.evaluate(table2.size()))\n\n  def testStaticVocabularyTableInitializationAcrossSessions(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    vocab_file = self._createVocabFile(\"feat_to_id_5.txt\")\n    with self.cached_session():\n      vocab_size = 3\n      oov_buckets = 1\n      table1 = self.getVocabularyTable()(\n          lookup_ops.TextFileIdTableInitializer(\n              vocab_file, vocab_size=vocab_size),\n          oov_buckets,\n          experimental_is_anonymous=is_anonymous)\n\n      self.initialize_table(table1)\n\n      input_string_1 = constant_op.constant(\n          [\"brain\", \"salad\", \"surgery\", \"UNK\"])\n\n      out1 = table1.lookup(input_string_1)\n\n      self.assertAllEqual([0, 1, 2, 3], self.evaluate(out1))\n      self.assertEqual(vocab_size + oov_buckets, self.evaluate(table1.size()))\n\n    with self.cached_session():\n      vocab_size = 3\n      oov_buckets = 1\n\n      # Underlying lookup table already initialized in previous session.\n      # No need to initialize table2\n      table2 = self.getVocabularyTable()(\n          lookup_ops.TextFileIdTableInitializer(\n              vocab_file, vocab_size=vocab_size),\n          oov_buckets,\n          experimental_is_anonymous=is_anonymous)\n\n      input_string_2 = constant_op.constant([\"fruit\", \"salad\", \"UNK\"])\n\n      out2 = table2.lookup(input_string_2)\n\n      self.assertAllEqual([3, 1, 3], self.evaluate(out2))\n      self.assertEqual(vocab_size + oov_buckets, self.evaluate(table2.size()))\n\n  def testStaticVocabularyTableAssetTracking(self, is_anonymous):\n    vocab_file = self._createVocabFile(\"vocab.txt\")\n    vocab_size = 3\n    oov_buckets = 1\n    table = self.getVocabularyTable()(\n        lookup_ops.TextFileIdTableInitializer(\n            vocab_file, vocab_size=vocab_size),\n        oov_buckets,\n        experimental_is_anonymous=is_anonymous)\n    objects = checkpoint_util.list_objects(graph_view.ObjectGraphView(table))\n    assets = list(filter(lambda obj: isinstance(obj, asset.Asset), objects))\n    self.assertLen(assets, 1)\n    self.assertEqual(\n        self.evaluate(assets[0].asset_path), compat.as_bytes(vocab_file))\n\n  def testSparseTensor(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    vocab_file = self._createVocabFile(\"feat_to_id_7.txt\")\n    input_indices = [[0, 0], [0, 1], [2, 0], [2, 2], [3, 0]]\n    input_shape = [4, 4]\n    sp_features = sparse_tensor.SparseTensor(\n        constant_op.constant(input_indices, dtypes.int64),\n        constant_op.constant([\"brain\", \"salad\", \"brain\", \"surgery\", \"tarkus\"],\n                             dtypes.string),\n        constant_op.constant(input_shape, dtypes.int64))\n\n    table = self.getVocabularyTable()(\n        lookup_ops.TextFileIdTableInitializer(vocab_file, vocab_size=3),\n        1,\n        experimental_is_anonymous=is_anonymous)\n    self.initialize_table(table)\n\n    sp_ids = table.lookup(sp_features)\n\n    self.assertAllEqual([5], sp_ids.values._shape_as_list())\n\n    sp_ids_ind, sp_ids_val, sp_ids_shape = self.evaluate(\n        [sp_ids.indices, sp_ids.values, sp_ids.dense_shape])\n\n    self.assertAllEqual(input_indices, sp_ids_ind)\n    self.assertAllEqual([0, 1, 0, 2, 3], sp_ids_val)\n    self.assertAllEqual(input_shape, sp_ids_shape)\n\n  def testRaggedTensor(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    vocab_file = self._createVocabFile(\"feat_to_id_7.txt\")\n    input_row_splits = [0, 2, 4, 5]\n    ragged_features = ragged_tensor.RaggedTensor.from_row_splits(\n        constant_op.constant([\"brain\", \"salad\", \"brain\", \"surgery\", \"tarkus\"],\n                             dtypes.string),\n        constant_op.constant(input_row_splits, dtypes.int64))\n\n    table = self.getVocabularyTable()(\n        lookup_ops.TextFileIdTableInitializer(vocab_file, vocab_size=3),\n        1,\n        experimental_is_anonymous=is_anonymous)\n    self.initialize_table(table)\n\n    ragged_ids = table.lookup(ragged_features)\n\n    self.assertAllEqual([5], ragged_ids.values._shape_as_list())\n\n    ragged_ids_val, ragged_ids_row_splits = self.evaluate(\n        [ragged_ids.values, ragged_ids.row_splits])\n\n    self.assertAllEqual([0, 1, 0, 2, 3], ragged_ids_val)\n    self.assertAllEqual(input_row_splits, ragged_ids_row_splits)\n\n  def testInt32SparseTensor(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    input_indices = [[0, 0], [0, 1], [2, 0], [2, 2], [3, 0]]\n    input_shape = [4, 4]\n    sp_features = sparse_tensor.SparseTensor(\n        constant_op.constant(input_indices, dtypes.int64),\n        constant_op.constant([42, 1, 42, -1000, 11], dtypes.int32),\n        constant_op.constant(input_shape, dtypes.int64))\n\n    table = self.getVocabularyTable()(\n        lookup_ops.KeyValueTensorInitializer((42, 1, -1000), (0, 1, 2),\n                                             dtypes.int64, dtypes.int64),\n        1,\n        lookup_key_dtype=dtypes.int32,\n        experimental_is_anonymous=is_anonymous)\n    self.initialize_table(table)\n\n    sp_ids = table.lookup(sp_features)\n\n    self.assertAllEqual([5], sp_ids.values._shape_as_list())\n\n    sp_ids_ind, sp_ids_val, sp_ids_shape = self.evaluate(\n        [sp_ids.indices, sp_ids.values, sp_ids.dense_shape])\n\n    self.assertAllEqual(input_indices, sp_ids_ind)\n    self.assertAllEqual([0, 1, 0, 2, 3], sp_ids_val)\n    self.assertAllEqual(input_shape, sp_ids_shape)\n\n  def testInt32RaggedTensor(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    input_row_splits = [0, 2, 4, 5]\n    ragged_features = ragged_tensor.RaggedTensor.from_row_splits(\n        constant_op.constant([42, 1, 42, -1000, 11], dtypes.int32),\n        constant_op.constant(input_row_splits, dtypes.int64))\n\n    table = self.getVocabularyTable()(\n        lookup_ops.KeyValueTensorInitializer((42, 1, -1000), (0, 1, 2),\n                                             dtypes.int64, dtypes.int64),\n        1,\n        lookup_key_dtype=dtypes.int32,\n        experimental_is_anonymous=is_anonymous)\n    self.initialize_table(table)\n\n    ragged_ids = table.lookup(ragged_features)\n\n    self.assertAllEqual([5], ragged_ids.values._shape_as_list())\n\n    ragged_ids_val, ragged_ids_row_splits = self.evaluate(\n        [ragged_ids.values, ragged_ids.row_splits])\n\n    self.assertAllEqual([0, 1, 0, 2, 3], ragged_ids_val)\n    self.assertAllEqual(input_row_splits, ragged_ids_row_splits)\n\n  def testInt64SparseTensor(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    input_indices = [[0, 0], [0, 1], [2, 0], [2, 2], [3, 0]]\n    input_shape = [4, 4]\n    sp_features = sparse_tensor.SparseTensor(\n        constant_op.constant(input_indices, dtypes.int64),\n        constant_op.constant([42, 1, 42, -1000, 11], dtypes.int64),\n        constant_op.constant(input_shape, dtypes.int64))\n\n    table = self.getVocabularyTable()(\n        lookup_ops.KeyValueTensorInitializer((42, 1, -1000), (0, 1, 2),\n                                             dtypes.int64, dtypes.int64),\n        1,\n        experimental_is_anonymous=is_anonymous)\n    self.initialize_table(table)\n\n    sp_ids = table.lookup(sp_features)\n\n    self.assertAllEqual([5], sp_ids.values._shape_as_list())\n\n    sp_ids_ind, sp_ids_val, sp_ids_shape = self.evaluate(\n        [sp_ids.indices, sp_ids.values, sp_ids.dense_shape])\n\n    self.assertAllEqual(input_indices, sp_ids_ind)\n    self.assertAllEqual([0, 1, 0, 2, 3], sp_ids_val)\n    self.assertAllEqual(input_shape, sp_ids_shape)\n\n  def testInt64RaggedTensor(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    input_row_splits = [0, 2, 4, 5]\n    ragged_features = ragged_tensor.RaggedTensor.from_row_splits(\n        constant_op.constant([42, 1, 42, -1000, 11], dtypes.int64),\n        constant_op.constant(input_row_splits, dtypes.int64))\n\n    table = self.getVocabularyTable()(\n        lookup_ops.KeyValueTensorInitializer((42, 1, -1000), (0, 1, 2),\n                                             dtypes.int64, dtypes.int64),\n        1,\n        experimental_is_anonymous=is_anonymous)\n    self.initialize_table(table)\n\n    ragged_ids = table.lookup(ragged_features)\n\n    self.assertAllEqual([5], ragged_ids.values._shape_as_list())\n\n    ragged_ids_val, ragged_ids_row_splits = self.evaluate(\n        [ragged_ids.values, ragged_ids.row_splits])\n\n    self.assertAllEqual([0, 1, 0, 2, 3], ragged_ids_val)\n    self.assertAllEqual(input_row_splits, ragged_ids_row_splits)\n\n  def testStaticVocabularyTableNoInnerTable(self, is_anonymous):\n    table = self.getVocabularyTable()(\n        None, num_oov_buckets=1, experimental_is_anonymous=is_anonymous)\n    self.assertIsNone(table.resource_handle)\n\n  @test_util.run_v2_only\n  def testSavedModelSaveRestore(self, is_anonymous):\n    save_dir = os.path.join(self.get_temp_dir(), \"save_restore\")\n    save_path = os.path.join(tempfile.mkdtemp(prefix=save_dir), \"hash\")\n\n    root = autotrackable.AutoTrackable()\n\n    vocab_file = self._createVocabFile(\"feat_to_id_3.txt\", (\"11\", \"12\", \"13\"))\n    vocab_size = 3\n    oov_buckets = 1\n    root.table = self.getVocabularyTable()(\n        lookup_ops.TextFileIdTableInitializer(\n            vocab_file, vocab_size=vocab_size, key_dtype=dtypes.int64),\n        oov_buckets,\n        experimental_is_anonymous=is_anonymous)\n\n    @def_function.function(\n        input_signature=[tensor_spec.TensorSpec((), dtypes.int64)])\n    def lookup(key):\n      return root.table.lookup(key)\n\n    @def_function.function(input_signature=[])\n    def size():\n      return root.table.size()\n\n    @def_function.function(input_signature=[])\n    def is_ref_counting():\n      return test_ops.is_resource_handle_ref_counting(\n          root.table.resource_handle)\n\n    root.lookup = lookup\n    root.size = size\n    root.is_ref_counting = is_ref_counting\n\n    self.assertEqual(root.table.size(), 4)\n    self.assertEqual(root.lookup(12), 1)\n    self.assertEqual(root.lookup(10), 3)\n    self.assertEqual(root.is_ref_counting(), is_anonymous)\n\n    saved_model_save.save(root, save_path)\n\n    del root\n    loaded = saved_model_load.load(save_path)\n    self.assertEqual(loaded.size(), 4)\n    self.assertEqual(loaded.lookup(12), 1)\n    self.assertEqual(loaded.lookup(10), 3)\n    self.assertEqual(loaded.is_ref_counting(), is_anonymous)\n\n\n@parameterized.named_parameters(\n    (f\"_{is_anonymous}\", is_anonymous) for is_anonymous in [False, True])\nclass DenseHashTableOpTest(test.TestCase):\n\n  def testBasic(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    keys = constant_op.constant([11, 12, 13, 14], dtypes.int64)\n    values = constant_op.constant([0, 1, 2, 3], dtypes.int64)\n    table = lookup_ops.DenseHashTable(\n        dtypes.int64,\n        dtypes.int64,\n        default_value=-1,\n        empty_key=0,\n        deleted_key=-1,\n        experimental_is_anonymous=is_anonymous)\n    self.assertAllEqual(0, self.evaluate(table.size()))\n\n    self.evaluate(table.insert(keys, values))\n    self.assertAllEqual(4, self.evaluate(table.size()))\n\n    remove_string = constant_op.constant([12, 15], dtypes.int64)\n    self.evaluate(table.remove(remove_string))\n    self.assertAllEqual(3, self.evaluate(table.size()))\n\n    input_string = constant_op.constant([11, 12, 15], dtypes.int64)\n    output = table.lookup(input_string)\n    self.assertAllEqual([3], output.get_shape())\n\n    result = self.evaluate(output)\n    self.assertAllEqual([0, -1, -1], result)\n\n  def testGetItem(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    keys = constant_op.constant([11, 12, 13, 14], dtypes.int64)\n    values = constant_op.constant([0, 1, 2, 3], dtypes.int64)\n    table = lookup_ops.DenseHashTable(\n        dtypes.int64,\n        dtypes.int64,\n        default_value=-1,\n        empty_key=0,\n        deleted_key=-1,\n        experimental_is_anonymous=is_anonymous)\n\n    self.evaluate(table.insert(keys, values))\n\n    input_string = constant_op.constant([11, 12, 15], dtypes.int64)\n    output = table[input_string]\n    self.assertAllEqual([3], output.get_shape())\n\n    result = self.evaluate(output)\n    self.assertAllEqual([0, 1, -1], result)\n\n  def testBasicBool(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    keys = constant_op.constant([11, 12, 13, 14], dtypes.int64)\n    values = constant_op.constant([True, True, True, True], dtypes.bool)\n    table = lookup_ops.DenseHashTable(\n        dtypes.int64,\n        dtypes.bool,\n        default_value=False,\n        empty_key=0,\n        deleted_key=-1,\n        experimental_is_anonymous=is_anonymous)\n    self.assertAllEqual(0, self.evaluate(table.size()))\n\n    self.evaluate(table.insert(keys, values))\n    self.assertAllEqual(4, self.evaluate(table.size()))\n\n    remove_string = constant_op.constant([11, 15], dtypes.int64)\n    self.evaluate(table.remove(remove_string))\n    self.assertAllEqual(3, self.evaluate(table.size()))\n\n    input_string = constant_op.constant([11, 12, 15], dtypes.int64)\n    output = table.lookup(input_string)\n    self.assertAllEqual([3], output.get_shape())\n\n    result = self.evaluate(output)\n    self.assertAllEqual([False, True, False], result)\n\n  def testSameEmptyAndDeletedKey(self, is_anonymous):\n    with self.assertRaisesRegex(errors_impl.InvalidArgumentError,\n                                \"Empty and deleted keys\"):\n      table = lookup_ops.DenseHashTable(\n          dtypes.int64,\n          dtypes.int64,\n          default_value=-1,\n          empty_key=42,\n          deleted_key=42,\n          experimental_is_anonymous=is_anonymous)\n      self.assertAllEqual(0, self.evaluate(table.size()))\n\n  @test_util.run_v1_only(\"uses placeholders\")\n  def testLookupUnknownShape(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    with self.cached_session():\n      keys = constant_op.constant([11, 12, 13], dtypes.int64)\n      values = constant_op.constant([0, 1, 2], dtypes.int64)\n      table = lookup_ops.DenseHashTable(\n          dtypes.int64,\n          dtypes.int64,\n          default_value=-1,\n          empty_key=0,\n          deleted_key=-1,\n          experimental_is_anonymous=is_anonymous)\n\n      self.evaluate(table.insert(keys, values))\n      self.assertAllEqual(3, self.evaluate(table.size()))\n\n      placeholder_keys = array_ops.placeholder(dtypes.int64)\n      output = table.lookup(placeholder_keys)\n      self.assertAllEqual(None, output.get_shape())\n      result = output.eval({placeholder_keys: [11, 12, 15]})\n      self.assertAllEqual([0, 1, -1], result)\n\n  def testMapStringToFloat(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    keys = constant_op.constant([\"a\", \"b\", \"c\", \"d\"], dtypes.string)\n    values = constant_op.constant([0.0, 1.1, 2.2, 3.3], dtypes.float32)\n    default_value = constant_op.constant(-1.5, dtypes.float32)\n    table = lookup_ops.DenseHashTable(\n        dtypes.string,\n        dtypes.float32,\n        default_value=default_value,\n        empty_key=\"\",\n        deleted_key=\"$\",\n        experimental_is_anonymous=is_anonymous)\n    self.assertAllEqual(0, self.evaluate(table.size()))\n\n    self.evaluate(table.insert(keys, values))\n    self.assertAllEqual(4, self.evaluate(table.size()))\n\n    remove_string = constant_op.constant([\"b\", \"e\"])\n    self.evaluate(table.remove(remove_string))\n    self.assertAllEqual(3, self.evaluate(table.size()))\n\n    input_string = constant_op.constant([\"a\", \"b\", \"d\", \"e\"], dtypes.string)\n    output = table.lookup(input_string)\n    self.assertAllEqual([4], output.get_shape())\n\n    result = self.evaluate(output)\n    self.assertAllClose([0, -1.5, 3.3, -1.5], result)\n\n  def testMapInt64ToFloat(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    for float_dtype in [dtypes.float32, dtypes.float64]:\n      keys = constant_op.constant([11, 12, 13, 14], dtypes.int64)\n      values = constant_op.constant([0.0, 1.1, 2.2, 3.3], float_dtype)\n      default_value = constant_op.constant(-1.5, float_dtype)\n      table = lookup_ops.DenseHashTable(\n          dtypes.int64,\n          float_dtype,\n          default_value=default_value,\n          empty_key=0,\n          deleted_key=-1,\n          experimental_is_anonymous=is_anonymous)\n      self.assertAllEqual(0, self.evaluate(table.size()))\n\n      self.evaluate(table.insert(keys, values))\n      self.assertAllEqual(4, self.evaluate(table.size()))\n\n      remove_string = constant_op.constant([12, 15], dtypes.int64)\n      self.evaluate(table.remove(remove_string))\n      self.assertAllEqual(3, self.evaluate(table.size()))\n\n      input_string = constant_op.constant([11, 12, 14, 15], dtypes.int64)\n      output = table.lookup(input_string)\n      self.assertAllEqual([4], output.get_shape())\n\n      result = self.evaluate(output)\n      self.assertAllClose([0, -1.5, 3.3, -1.5], result)\n\n  def testVectorValues(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    keys = constant_op.constant([11, 12, 13], dtypes.int64)\n    values = constant_op.constant([[0, 1, 2, 3], [3, 4, 5, 6], [6, 7, 8, 9]],\n                                  dtypes.int64)\n    default_value = constant_op.constant([-1, -2, -3, -4], dtypes.int64)\n    table = lookup_ops.DenseHashTable(\n        dtypes.int64,\n        dtypes.int64,\n        default_value=default_value,\n        empty_key=0,\n        deleted_key=-1,\n        initial_num_buckets=4,\n        experimental_is_anonymous=is_anonymous)\n    self.assertAllEqual(0, self.evaluate(table.size()))\n\n    self.evaluate(table.insert(keys, values))\n    self.assertAllEqual(3, self.evaluate(table.size()))\n    self.assertAllEqual(4, len(self.evaluate(table.export()[0])))\n\n    self.evaluate(\n        table.insert(\n            constant_op.constant([14], dtypes.int64),\n            constant_op.constant([[2, 3, 4, 5]], dtypes.int64)))\n    self.assertAllEqual(4, self.evaluate(table.size()))\n    self.assertAllEqual(8, len(self.evaluate(table.export()[0])))\n\n    remove_string = constant_op.constant([12, 16], dtypes.int64)\n    self.evaluate(table.remove(remove_string))\n    self.assertAllEqual(3, self.evaluate(table.size()))\n    self.assertAllEqual(8, len(self.evaluate(table.export()[0])))\n\n    input_string = constant_op.constant([11, 12, 14, 15], dtypes.int64)\n    output = table.lookup(input_string)\n    self.assertAllEqual([4, 4],\n                        output.shape,\n                        msg=\"Saw shape: %s\" % output.shape)\n\n    result = self.evaluate(output)\n    self.assertAllEqual(\n        [[0, 1, 2, 3], [-1, -2, -3, -4], [2, 3, 4, 5], [-1, -2, -3, -4]],\n        result)\n\n  def testVectorKeys(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    keys = constant_op.constant([[0, 1], [1, 2], [1, 3]], dtypes.int64)\n    values = constant_op.constant([10, 11, 12], dtypes.int64)\n    empty_key = constant_op.constant([0, 3], dtypes.int64)\n    deleted_key = constant_op.constant([-1, -1], dtypes.int64)\n    default_value = constant_op.constant(-1, dtypes.int64)\n    table = lookup_ops.DenseHashTable(\n        dtypes.int64,\n        dtypes.int64,\n        default_value=default_value,\n        empty_key=empty_key,\n        deleted_key=deleted_key,\n        initial_num_buckets=8,\n        experimental_is_anonymous=is_anonymous)\n    self.assertAllEqual(0, self.evaluate(table.size()))\n\n    self.evaluate(table.insert(keys, values))\n    self.assertAllEqual(3, self.evaluate(table.size()))\n\n    self.evaluate(\n        table.insert(\n            constant_op.constant([[0, 0]], dtypes.int64),\n            constant_op.constant([13], dtypes.int64)))\n    self.assertAllEqual(4, self.evaluate(table.size()))\n    self.assertAllEqual(8, len(self.evaluate(table.export()[0])))\n\n    remove_string = constant_op.constant([[1, 2], [7, 8]], dtypes.int64)\n    self.evaluate(table.remove(remove_string))\n    self.assertAllEqual(3, self.evaluate(table.size()))\n    self.assertAllEqual(8, len(self.evaluate(table.export()[0])))\n\n    input_string = constant_op.constant([[0, 1], [1, 2], [1, 3], [0, 2]],\n                                        dtypes.int64)\n    output = table.lookup(input_string)\n    self.assertAllEqual([4], output.get_shape())\n\n    result = self.evaluate(output)\n    self.assertAllEqual([10, -1, 12, -1], result)\n\n  def testResize(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    keys = constant_op.constant([11, 12, 13], dtypes.int64)\n    values = constant_op.constant([0, 1, 2], dtypes.int64)\n    table = lookup_ops.DenseHashTable(\n        dtypes.int64,\n        dtypes.int64,\n        default_value=-1,\n        empty_key=0,\n        deleted_key=-1,\n        initial_num_buckets=4,\n        experimental_is_anonymous=is_anonymous)\n    self.assertAllEqual(0, self.evaluate(table.size()))\n\n    self.evaluate(table.insert(keys, values))\n    self.assertAllEqual(3, self.evaluate(table.size()))\n    self.assertAllEqual(4, len(self.evaluate(table.export()[0])))\n\n    keys2 = constant_op.constant([12, 99], dtypes.int64)\n    self.evaluate(table.remove(keys2))\n    self.assertAllEqual(2, self.evaluate(table.size()))\n    self.assertAllEqual(4, len(self.evaluate(table.export()[0])))\n\n    keys3 = constant_op.constant([13, 14, 15, 16, 17], dtypes.int64)\n    values3 = constant_op.constant([3, 4, 5, 6, 7], dtypes.int64)\n\n    self.evaluate(table.insert(keys3, values3))\n    self.assertAllEqual(6, self.evaluate(table.size()))\n    self.assertAllEqual(16, len(self.evaluate(table.export()[0])))\n\n    keys4 = constant_op.constant([10, 11, 12, 13, 14, 15, 16, 17, 18],\n                                 dtypes.int64)\n    output = table.lookup(keys4)\n    self.assertAllEqual([-1, 0, -1, 3, 4, 5, 6, 7, -1], self.evaluate(output))\n\n  def testExport(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    keys = constant_op.constant([11, 12, 13, 14], dtypes.int64)\n    values = constant_op.constant([1, 2, 3, 4], dtypes.int64)\n    table = lookup_ops.DenseHashTable(\n        dtypes.int64,\n        dtypes.int64,\n        default_value=-1,\n        empty_key=100,\n        deleted_key=200,\n        initial_num_buckets=8,\n        experimental_is_anonymous=is_anonymous)\n    self.assertAllEqual(0, self.evaluate(table.size()))\n\n    self.evaluate(table.insert(keys, values))\n    self.assertAllEqual(4, self.evaluate(table.size()))\n\n    keys2 = constant_op.constant([12, 15], dtypes.int64)\n    self.evaluate(table.remove(keys2))\n    self.assertAllEqual(3, self.evaluate(table.size()))\n\n    exported_keys, exported_values = table.export()\n\n    np_keys = self.evaluate(exported_keys)\n    np_values = self.evaluate(exported_values)\n\n    self.assertAllEqual(8, len(np_keys))\n    self.assertAllEqual(8, len(np_values))\n\n    # pair up keys and values, drop extra added dimension\n    pairs = np.dstack((np_keys.flatten(), np_values.flatten()))[0]\n    # sort by key\n    pairs = pairs[pairs[:, 0].argsort()]\n    self.assertAllEqual([[11, 1], [13, 3], [14, 4], [100, 0], [100, 0],\n                         [100, 0], [100, 0], [200, 2]], pairs)\n\n  @test_util.run_v1_only(\"Saver V1 only\")\n  def testSaveRestore(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    save_dir = os.path.join(self.get_temp_dir(), \"save_restore\")\n    save_path = os.path.join(tempfile.mkdtemp(prefix=save_dir), \"hash\")\n\n    with self.session(graph=ops.Graph()) as sess:\n      default_value = -1\n      empty_key = 0\n      deleted_key = -1\n      keys = constant_op.constant([11, 12, 13, 14], dtypes.int64)\n      values = constant_op.constant([0, 1, 2, 3], dtypes.int64)\n      table = lookup_ops.DenseHashTable(\n          dtypes.int64,\n          dtypes.int64,\n          default_value=default_value,\n          empty_key=empty_key,\n          deleted_key=deleted_key,\n          name=\"t1\",\n          checkpoint=True,\n          initial_num_buckets=32,\n          experimental_is_anonymous=is_anonymous)\n\n      save = saver.Saver()\n\n      self.assertAllEqual(0, table.size())\n      table.insert(keys, values).run()\n      self.assertAllEqual(4, table.size())\n      self.assertAllEqual(32, len(table.export()[0].eval()))\n\n      keys2 = constant_op.constant([12, 15], dtypes.int64)\n      table.remove(keys2).run()\n      self.assertAllEqual(3, table.size())\n      self.assertAllEqual(32, len(table.export()[0].eval()))\n\n      val = save.save(sess, save_path)\n      self.assertIsInstance(val, str)\n      self.assertEqual(save_path, val)\n\n    with self.session(graph=ops.Graph()) as sess:\n      table = lookup_ops.DenseHashTable(\n          dtypes.int64,\n          dtypes.int64,\n          default_value=default_value,\n          empty_key=empty_key,\n          deleted_key=deleted_key,\n          name=\"t1\",\n          checkpoint=True,\n          initial_num_buckets=64,\n          experimental_is_anonymous=is_anonymous)\n      table.insert(\n          constant_op.constant([11, 14], dtypes.int64),\n          constant_op.constant([12, 24], dtypes.int64)).run()\n      self.assertAllEqual(2, table.size())\n      self.assertAllEqual(64, len(table.export()[0].eval()))\n\n      save = saver.Saver()\n\n      # Restore the saved values in the parameter nodes.\n      save.restore(sess, save_path)\n\n      self.assertAllEqual(3, table.size())\n      self.assertAllEqual(32, len(table.export()[0].eval()))\n\n      input_string = constant_op.constant([10, 11, 12, 13, 14], dtypes.int64)\n      output = table.lookup(input_string)\n      self.assertAllEqual([-1, 0, -1, 2, 3], output)\n\n  @test_util.run_v1_only(\"Saver V1 only\")\n  def testSaveRestoreOnlyTable(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    save_dir = os.path.join(self.get_temp_dir(), \"save_restore\")\n    save_path = os.path.join(tempfile.mkdtemp(prefix=save_dir), \"hash\")\n\n    with self.session(graph=ops.Graph()) as sess:\n      default_value = -1\n      empty_key = 0\n      deleted_key = -1\n      keys = constant_op.constant([11, 12, 13, 14], dtypes.int64)\n      values = constant_op.constant([0, 1, 2, 3], dtypes.int64)\n      table = lookup_ops.DenseHashTable(\n          dtypes.int64,\n          dtypes.int64,\n          default_value=default_value,\n          empty_key=empty_key,\n          deleted_key=deleted_key,\n          name=\"t1\",\n          checkpoint=True,\n          initial_num_buckets=32,\n          experimental_is_anonymous=is_anonymous)\n\n      save = saver.Saver([table])\n\n      self.assertAllEqual(0, table.size())\n      table.insert(keys, values).run()\n      self.assertAllEqual(4, table.size())\n      self.assertAllEqual(32, len(table.export()[0].eval()))\n\n      keys2 = constant_op.constant([12, 15], dtypes.int64)\n      table.remove(keys2).run()\n      self.assertAllEqual(3, table.size())\n      self.assertAllEqual(32, len(table.export()[0].eval()))\n\n      val = save.save(sess, save_path)\n      self.assertIsInstance(val, str)\n      self.assertEqual(save_path, val)\n\n    with self.session(graph=ops.Graph()) as sess:\n      table = lookup_ops.DenseHashTable(\n          dtypes.int64,\n          dtypes.int64,\n          default_value=default_value,\n          empty_key=empty_key,\n          deleted_key=deleted_key,\n          name=\"t1\",\n          checkpoint=True,\n          initial_num_buckets=64,\n          experimental_is_anonymous=is_anonymous)\n      table.insert(\n          constant_op.constant([11, 14], dtypes.int64),\n          constant_op.constant([12, 24], dtypes.int64)).run()\n      self.assertAllEqual(2, table.size())\n      self.assertAllEqual(64, len(table.export()[0].eval()))\n\n      save = saver.Saver([table])\n\n      # Restore the saved values in the parameter nodes.\n      save.restore(sess, save_path)\n\n      self.assertAllEqual(3, table.size())\n      self.assertAllEqual(32, len(table.export()[0].eval()))\n\n      input_string = constant_op.constant([10, 11, 12, 13, 14], dtypes.int64)\n      output = table.lookup(input_string)\n      self.assertAllEqual([-1, 0, -1, 2, 3], output)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testObjectSaveRestore(self, is_anonymous):\n    if is_anonymous and not context.executing_eagerly():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    save_dir = os.path.join(self.get_temp_dir(), \"save_restore\")\n    save_prefix = os.path.join(tempfile.mkdtemp(prefix=save_dir), \"hash\")\n\n    default_value = -1\n    empty_key = 0\n    deleted_key = -1\n    keys = constant_op.constant([11, 12, 13], dtypes.int64)\n    values = constant_op.constant([0, 1, 2], dtypes.int64)\n    save_table = lookup_ops.DenseHashTable(\n        dtypes.int64,\n        dtypes.int64,\n        default_value=default_value,\n        empty_key=empty_key,\n        deleted_key=deleted_key,\n        name=\"t1\",\n        checkpoint=True,\n        initial_num_buckets=32,\n        experimental_is_anonymous=is_anonymous)\n\n    save_checkpoint = trackable.Checkpoint(table=save_table)\n\n    self.assertAllEqual(0, self.evaluate(save_table.size()))\n    self.evaluate(save_table.insert(keys, values))\n    self.assertAllEqual(3, self.evaluate(save_table.size()))\n    self.assertAllEqual(32, len(self.evaluate(save_table.export()[0])))\n\n    save_path = save_checkpoint.save(save_prefix)\n    del save_table, save_checkpoint\n\n    load_table = lookup_ops.DenseHashTable(\n        dtypes.int64,\n        dtypes.int64,\n        default_value=default_value,\n        empty_key=empty_key,\n        deleted_key=deleted_key,\n        name=\"t1\",\n        checkpoint=True,\n        initial_num_buckets=64,\n        experimental_is_anonymous=is_anonymous)\n    self.evaluate(\n        load_table.insert(\n            constant_op.constant([11, 14], dtypes.int64),\n            constant_op.constant([12, 24], dtypes.int64)))\n    self.assertAllEqual(2, self.evaluate(load_table.size()))\n    self.assertAllEqual(64, len(self.evaluate(load_table.export()[0])))\n\n    restore_checkpoint = trackable.Checkpoint(table=load_table)\n\n    # Restore the saved values in the parameter nodes.\n    restore_checkpoint.restore(save_path).run_restore_ops()\n\n    self.assertAllEqual(3, self.evaluate(load_table.size()))\n    self.assertAllEqual(32, len(self.evaluate(load_table.export()[0])))\n\n    input_string = constant_op.constant([10, 11, 12, 13, 14], dtypes.int64)\n    output = load_table.lookup(input_string)\n    self.assertAllEqual([-1, 0, 1, 2, -1], self.evaluate(output))\n\n  @test_util.run_v2_only\n  def testSavedModelSaveRestore(self, is_anonymous):\n    save_dir = os.path.join(self.get_temp_dir(), \"save_restore\")\n    save_path = os.path.join(tempfile.mkdtemp(prefix=save_dir), \"hash\")\n\n    root = autotrackable.AutoTrackable()\n\n    default_value = -1\n    empty_key = 0\n    deleted_key = -1\n    keys = constant_op.constant([11, 12, 13], dtypes.int64)\n    values = constant_op.constant([0, 1, 2], dtypes.int64)\n    root.table = lookup_ops.DenseHashTable(\n        dtypes.int64,\n        dtypes.int64,\n        default_value=default_value,\n        empty_key=empty_key,\n        deleted_key=deleted_key,\n        name=\"t1\",\n        checkpoint=True,\n        initial_num_buckets=32,\n        experimental_is_anonymous=is_anonymous)\n\n    @def_function.function(\n        input_signature=[tensor_spec.TensorSpec((), dtypes.int64)])\n    def lookup(key):\n      return root.table.lookup(key)\n\n    @def_function.function(input_signature=[])\n    def size():\n      return root.table.size()\n\n    @def_function.function(input_signature=[])\n    def is_ref_counting():\n      return test_ops.is_resource_handle_ref_counting(\n          root.table.resource_handle)\n\n    root.lookup = lookup\n    root.size = size\n    root.is_ref_counting = is_ref_counting\n\n    self.assertEqual(root.table.size(), 0)\n    root.table.insert(keys, values)\n    self.assertEqual(root.table.size(), 3)\n    self.assertEqual(root.table.lookup(12), 1)\n    self.assertEqual(root.table.lookup(10), -1)\n    self.assertEqual(len(root.table.export()[0]), 32)\n    self.assertEqual(root.is_ref_counting(), is_anonymous)\n\n    saved_model_save.save(root, save_path)\n\n    del root\n    loaded = saved_model_load.load(save_path)\n    self.assertEqual(loaded.size(), 3)\n    self.assertEqual(loaded.lookup(12), 1)\n    self.assertEqual(loaded.lookup(10), -1)\n    self.assertEqual(loaded.is_ref_counting(), is_anonymous)\n\n  @test_util.run_v1_only(\"Saver V1 only\")\n  def testVectorSaveRestore(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    save_dir = os.path.join(self.get_temp_dir(), \"vector_save_restore\")\n    save_path = os.path.join(tempfile.mkdtemp(prefix=save_dir), \"hash\")\n\n    with self.session(graph=ops.Graph()) as sess:\n      empty_key = constant_op.constant([11, 13], dtypes.int64)\n      deleted_key = constant_op.constant([-2, -3], dtypes.int64)\n      default_value = constant_op.constant([-1, -2], dtypes.int64)\n      keys = constant_op.constant([[11, 12], [11, 14], [12, 13], [13, 14]],\n                                  dtypes.int64)\n      values = constant_op.constant([[0, 1], [2, 3], [2, 4], [4, 5]],\n                                    dtypes.int64)\n      table = lookup_ops.DenseHashTable(\n          dtypes.int64,\n          dtypes.int64,\n          default_value=default_value,\n          empty_key=empty_key,\n          deleted_key=deleted_key,\n          name=\"t1\",\n          checkpoint=True,\n          initial_num_buckets=32,\n          experimental_is_anonymous=is_anonymous)\n\n      save = saver.Saver()\n\n      self.assertAllEqual(0, table.size())\n      table.insert(keys, values).run()\n      self.assertAllEqual(4, table.size())\n      self.assertAllEqual(32, len(table.export()[0].eval()))\n\n      keys2 = constant_op.constant([[12, 13], [16, 17]], dtypes.int64)\n      table.remove(keys2).run()\n      self.assertAllEqual(3, table.size())\n      self.assertAllEqual(32, len(table.export()[0].eval()))\n\n      val = save.save(sess, save_path)\n      self.assertIsInstance(val, str)\n      self.assertEqual(save_path, val)\n\n    with self.session(graph=ops.Graph()) as sess:\n      empty_key = constant_op.constant([11, 13], dtypes.int64)\n      deleted_key = constant_op.constant([-2, -3], dtypes.int64)\n      default_value = constant_op.constant([-1, -2], dtypes.int64)\n      table = lookup_ops.DenseHashTable(\n          dtypes.int64,\n          dtypes.int64,\n          default_value=default_value,\n          empty_key=empty_key,\n          deleted_key=deleted_key,\n          name=\"t1\",\n          checkpoint=True,\n          initial_num_buckets=64,\n          experimental_is_anonymous=is_anonymous)\n      table.insert(\n          constant_op.constant([[11, 12], [13, 15]], dtypes.int64),\n          constant_op.constant([[21, 22], [23, 24]], dtypes.int64)).run()\n      self.assertAllEqual(2, table.size())\n      self.assertAllEqual(64, len(table.export()[0].eval()))\n\n      save = saver.Saver()\n\n      # Restore the saved values in the parameter nodes.\n      save.restore(sess, save_path)\n\n      self.assertAllEqual(3, table.size())\n      self.assertAllEqual(32, len(table.export()[0].eval()))\n\n      input_string = constant_op.constant(\n          [[11, 12], [11, 14], [11, 15], [13, 14], [13, 15]], dtypes.int64)\n      output = table.lookup(input_string)\n      self.assertAllEqual([[0, 1], [2, 3], [-1, -2], [4, 5], [-1, -2]],\n                          self.evaluate(output))\n\n  @test_util.run_v1_only(\"Saver V1 only\")\n  def testVectorScalarSaveRestore(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    save_dir = os.path.join(self.get_temp_dir(), \"vector_scalar_save_restore\")\n    save_path = os.path.join(tempfile.mkdtemp(prefix=save_dir), \"hash\")\n\n    with self.session(graph=ops.Graph()) as sess:\n      empty_key = constant_op.constant([11, 13], dtypes.int64)\n      deleted_key = constant_op.constant([-1, -1], dtypes.int64)\n      default_value = constant_op.constant(-1, dtypes.int64)\n      keys = constant_op.constant([[11, 12], [11, 14], [12, 13], [13, 14]],\n                                  dtypes.int64)\n      values = constant_op.constant([0, 1, 2, 3], dtypes.int64)\n      table = lookup_ops.DenseHashTable(\n          dtypes.int64,\n          dtypes.int64,\n          default_value=default_value,\n          empty_key=empty_key,\n          deleted_key=deleted_key,\n          name=\"t2\",\n          checkpoint=True,\n          initial_num_buckets=32,\n          experimental_is_anonymous=is_anonymous)\n\n      save = saver.Saver()\n\n      self.assertAllEqual(0, table.size())\n      table.insert(keys, values).run()\n      self.assertAllEqual(4, table.size())\n      self.assertAllEqual(32, len(table.export()[0].eval()))\n\n      keys2 = constant_op.constant([[12, 13], [15, 16]], dtypes.int64)\n      table.remove(keys2).run()\n      self.assertAllEqual(3, table.size())\n      self.assertAllEqual(32, len(table.export()[0].eval()))\n\n      val = save.save(sess, save_path)\n      self.assertIsInstance(val, str)\n      self.assertEqual(save_path, val)\n\n    with self.session(graph=ops.Graph()) as sess:\n      empty_key = constant_op.constant([11, 13], dtypes.int64)\n      deleted_key = constant_op.constant([-1, -1], dtypes.int64)\n      default_value = constant_op.constant(-1, dtypes.int64)\n      table = lookup_ops.DenseHashTable(\n          dtypes.int64,\n          dtypes.int64,\n          default_value=default_value,\n          empty_key=empty_key,\n          deleted_key=deleted_key,\n          name=\"t2\",\n          checkpoint=True,\n          initial_num_buckets=64,\n          experimental_is_anonymous=is_anonymous)\n      table.insert(\n          constant_op.constant([[11, 12], [13, 15]], dtypes.int64),\n          constant_op.constant([3, 4], dtypes.int64)).run()\n      self.assertAllEqual(2, table.size())\n      self.assertAllEqual(64, len(table.export()[0].eval()))\n\n      save = saver.Saver()\n\n      # Restore the saved values in the parameter nodes.\n      save.restore(sess, save_path)\n\n      self.assertAllEqual(3, table.size())\n      self.assertAllEqual(32, len(table.export()[0].eval()))\n\n      input_string = constant_op.constant(\n          [[11, 12], [11, 14], [11, 15], [13, 14], [13, 15]], dtypes.int64)\n      output = table.lookup(input_string)\n      self.assertAllEqual([0, 1, -1, 3, -1], output)\n\n  def testReprobe(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    # Insert 6 keys into a table with 8 buckets.\n    # The values are chosen to make sure collisions occur when using GCC STL\n    keys = constant_op.constant([11, 12, 13, 19, 20, 21], dtypes.int64)\n    values = constant_op.constant([51, 52, 53, 54, 55, 56], dtypes.int64)\n    table = lookup_ops.DenseHashTable(\n        dtypes.int64,\n        dtypes.int64,\n        default_value=-1,\n        empty_key=0,\n        deleted_key=-1,\n        initial_num_buckets=8,\n        experimental_is_anonymous=is_anonymous)\n    self.assertAllEqual(0, self.evaluate(table.size()))\n\n    self.evaluate(table.insert(keys, values))\n    self.assertAllEqual(6, self.evaluate(table.size()))\n\n    input_string = constant_op.constant([10, 11, 12, 13, 14, 19, 20, 21, 22],\n                                        dtypes.int64)\n    output = table.lookup(input_string)\n    self.assertAllEqual([9], output.get_shape())\n\n    result = self.evaluate(output)\n    self.assertAllEqual([-1, 51, 52, 53, -1, 54, 55, 56, -1], result)\n\n  def testCustomEmptyKey(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    keys = constant_op.constant([11, 0, 13], dtypes.int64)\n    values = constant_op.constant([0, 1, 2], dtypes.int64)\n    table = lookup_ops.DenseHashTable(\n        dtypes.int64,\n        dtypes.int64,\n        default_value=-1,\n        empty_key=12,\n        deleted_key=-1,\n        experimental_is_anonymous=is_anonymous)\n    self.assertAllEqual(0, self.evaluate(table.size()))\n\n    self.evaluate(table.insert(keys, values))\n    self.assertAllEqual(3, self.evaluate(table.size()))\n\n    input_string = constant_op.constant([11, 0, 15], dtypes.int64)\n    output = table.lookup(input_string)\n    self.assertAllEqual([3], output.get_shape())\n\n    result = self.evaluate(output)\n    self.assertAllEqual([0, 1, -1], result)\n\n  def testErrors(self, is_anonymous):\n    table = lookup_ops.DenseHashTable(\n        dtypes.int64,\n        dtypes.int64,\n        default_value=-1,\n        empty_key=0,\n        deleted_key=-1,\n        experimental_is_anonymous=is_anonymous)\n\n    # Inserting the empty key returns an error\n    keys1 = constant_op.constant([11, 0], dtypes.int64)\n    values1 = constant_op.constant([0, 1], dtypes.int64)\n    with self.assertRaisesRegex(errors_impl.InvalidArgumentError,\n                                \"empty_key\"):\n      self.evaluate(table.insert(keys1, values1))\n\n    # Looking up the empty key returns an error\n    with self.assertRaisesRegex(errors_impl.InvalidArgumentError,\n                                \"empty_key\"):\n      self.evaluate(table.lookup(keys1))\n\n    # Inserting the deleted key returns an error\n    keys2 = constant_op.constant([11, -1], dtypes.int64)\n    values2 = constant_op.constant([0, 1], dtypes.int64)\n    with self.assertRaisesRegex(errors_impl.InvalidArgumentError,\n                                \"deleted_key\"):\n      self.evaluate(table.insert(keys2, values2))\n\n    # Looking up the empty key returns an error\n    with self.assertRaisesRegex(errors_impl.InvalidArgumentError,\n                                \"deleted_key\"):\n      self.evaluate(table.lookup(keys2))\n\n    # Arbitrary tensors of keys are not supported\n    keys = constant_op.constant([[11, 0], [12, 1]], dtypes.int64)\n    values = constant_op.constant([[11, 0], [12, 1]], dtypes.int64)\n    with self.assertRaisesRegex(errors_impl.InvalidArgumentError,\n                                \"Expected key shape\"):\n      self.evaluate(table.lookup(keys))\n    with self.assertRaisesRegex(errors_impl.InvalidArgumentError,\n                                \"Expected key shape\"):\n      self.evaluate(table.insert(keys, values))\n\n    with self.assertRaisesRegex(errors_impl.InvalidArgumentError,\n                                \"Number of buckets must be\"):\n      table2 = lookup_ops.DenseHashTable(\n          dtypes.int64,\n          dtypes.int64,\n          default_value=-1,\n          empty_key=17,\n          deleted_key=-1,\n          initial_num_buckets=12,\n          experimental_is_anonymous=is_anonymous)\n      self.assertAllEqual(0, self.evaluate(table2.size()))\n\n    with self.assertRaisesRegex(\n        errors_impl.InvalidArgumentError,\n        \"Empty and deleted keys must have same shape\"):\n      table3 = lookup_ops.DenseHashTable(\n          dtypes.int64,\n          dtypes.int64,\n          default_value=-1,\n          empty_key=42,\n          deleted_key=[1, 2],\n          experimental_is_anonymous=is_anonymous)\n      self.assertAllEqual(0, self.evaluate(table3.size()))\n\n    with self.assertRaisesRegex(errors_impl.InvalidArgumentError,\n                                \"Empty and deleted keys cannot be equal\"):\n      table4 = lookup_ops.DenseHashTable(\n          dtypes.int64,\n          dtypes.int64,\n          default_value=-1,\n          empty_key=42,\n          deleted_key=42,\n          experimental_is_anonymous=is_anonymous)\n      self.assertAllEqual(0, self.evaluate(table4.size()))\n\n    with self.assertRaisesRegex(errors_impl.InvalidArgumentError,\n                                \"Empty and deleted keys cannot be equal\"):\n      table5 = lookup_ops.DenseHashTable(\n          dtypes.int64,\n          dtypes.int64,\n          default_value=-1,\n          empty_key=[1, 2, 3],\n          deleted_key=[1, 2, 3],\n          experimental_is_anonymous=is_anonymous)\n      self.assertAllEqual(0, self.evaluate(table5.size()))\n\n  @test_util.run_in_graph_and_eager_modes\n  def testStringToResource(self, is_anonymous):\n    v = variables.Variable(1.)\n    v1 = variables.Variable(1.)\n    table = lookup_ops.DenseHashTable(\n        dtypes.string,\n        dtypes.resource,\n        default_value=v.handle,\n        empty_key=\"<empty>\",\n        deleted_key=\"<deleted>\",\n        experimental_is_anonymous=is_anonymous)\n    self.assertEqual([], table.lookup(\"not_found\").shape)\n    table.insert(\"v1\", v1.handle)\n    self.assertEqual([], table.lookup(\"v1\").shape)\n\n  def testExportShapeInference(self, is_anonymous):\n    default_value = -1\n    empty_key = 0\n    deleted_key = -1\n    table = lookup_ops.DenseHashTable(\n        dtypes.int64,\n        dtypes.int64,\n        default_value=default_value,\n        empty_key=empty_key,\n        deleted_key=deleted_key,\n        experimental_is_anonymous=is_anonymous)\n    actual_shapes = [t.shape for t in table.export()]\n    inferred_shapes = []\n\n    @def_function.function\n    def f():\n      for t in table.export():\n        inferred_shapes.append(t.shape)\n\n    f()\n    self.assertLen(actual_shapes, 2)\n    self.assertLen(inferred_shapes, 2)\n    self.assertTrue(inferred_shapes[0].is_compatible_with(actual_shapes[0]))\n    self.assertTrue(inferred_shapes[1].is_compatible_with(actual_shapes[1]))\n\n\nclass IndexTableFromFile(test.TestCase):\n\n  def _createVocabFile(self, basename, values=(\"brain\", \"salad\", \"surgery\")):\n    vocabulary_file = os.path.join(self.get_temp_dir(), basename)\n    with open(vocabulary_file, \"w\") as f:\n      f.write(\"\\n\".join(values) + \"\\n\")\n    return vocabulary_file\n\n  def test_string_index_table_from_file(self):\n    vocabulary_file = self._createVocabFile(\"f2i_vocab1.txt\")\n\n    table = lookup_ops.index_table_from_file(\n        vocabulary_file=vocabulary_file, num_oov_buckets=1)\n    ids = table.lookup(constant_op.constant([\"salad\", \"surgery\", \"tarkus\"]))\n\n    if not context.executing_eagerly():\n      with self.assertRaises(errors_impl.OpError):\n        self.evaluate(ids)\n    self.evaluate(lookup_ops.tables_initializer())\n    self.assertAllEqual((1, 2, 3), self.evaluate(ids))\n\n  def test_string_index_table_from_multicolumn_file(self):\n    vocabulary_file = self._createVocabFile(\n        \"f2i_vocab1.txt\", values=(\"brain\\t300\", \"salad\\t20\", \"surgery\\t1\"))\n    table = lookup_ops.index_table_from_file(\n        vocabulary_file=vocabulary_file,\n        num_oov_buckets=1,\n        key_column_index=0,\n        value_column_index=lookup_ops.TextFileIndex.LINE_NUMBER)\n    ids = table.lookup(constant_op.constant([\"salad\", \"surgery\", \"tarkus\"]))\n\n    if not context.executing_eagerly():\n      with self.assertRaises(errors_impl.OpError):\n        self.evaluate(ids)\n    self.evaluate(lookup_ops.tables_initializer())\n    self.assertAllEqual((1, 2, 3), self.evaluate(ids))\n\n  def test_string_index_table_from_multicolumn_file_custom_delimiter(self):\n    vocabulary_file = self._createVocabFile(\n        \"f2i_vocab1.txt\", values=(\"brain 300\", \"salad 20\", \"surgery 1\"))\n    table = lookup_ops.index_table_from_file(\n        vocabulary_file=vocabulary_file,\n        num_oov_buckets=1,\n        key_column_index=0,\n        value_column_index=lookup_ops.TextFileIndex.LINE_NUMBER,\n        delimiter=\" \")\n    ids = table.lookup(constant_op.constant([\"salad\", \"surgery\", \"tarkus\"]))\n\n    if not context.executing_eagerly():\n      with self.assertRaises(errors_impl.OpError):\n        self.evaluate(ids)\n    self.evaluate(lookup_ops.tables_initializer())\n    self.assertAllEqual((1, 2, 3), self.evaluate(ids))\n\n  def test_string_index_table_from_file_tensor_filename(self):\n    vocabulary_file = self._createVocabFile(\"f2i_vocab1.txt\")\n    vocabulary_file = constant_op.constant(vocabulary_file)\n    table = lookup_ops.index_table_from_file(\n        vocabulary_file=vocabulary_file, num_oov_buckets=1)\n    ids = table.lookup(constant_op.constant([\"salad\", \"surgery\", \"tarkus\"]))\n\n    if not context.executing_eagerly():\n      with self.assertRaises(errors_impl.OpError):\n        self.evaluate(ids)\n    self.evaluate(lookup_ops.tables_initializer())\n    self.assertAllEqual((1, 2, 3), self.evaluate(ids))\n    if not context.executing_eagerly():\n      self.assertEqual(1,\n                       len(ops.get_collection(ops.GraphKeys.ASSET_FILEPATHS)))\n\n  @test_util.run_v1_only(\"placeholder usage\")\n  def test_string_index_table_from_file_placeholder_filename(self):\n    vocabulary_file = self._createVocabFile(\"f2i_vocab1.txt\")\n    with self.cached_session():\n      vocabulary_placeholder = array_ops.placeholder(dtypes.string, [])\n      table = lookup_ops.index_table_from_file(\n          vocabulary_file=vocabulary_placeholder, num_oov_buckets=1)\n      ids = table.lookup(constant_op.constant([\"salad\", \"surgery\", \"tarkus\"]))\n\n      with self.assertRaises(errors_impl.OpError):\n        self.evaluate(ids)\n\n      feed_dict = {vocabulary_placeholder.name: vocabulary_file}\n      lookup_ops.tables_initializer().run(feed_dict=feed_dict)\n      self.assertAllEqual((1, 2, 3), self.evaluate(ids))\n      self.assertEqual(0,\n                       len(ops.get_collection(ops.GraphKeys.ASSET_FILEPATHS)))\n\n  def test_int32_index_table_from_file(self):\n    vocabulary_file = self._createVocabFile(\n        \"f2i_vocab2.txt\", values=(\"42\", \"1\", \"-1000\"))\n    table = lookup_ops.index_table_from_file(\n        vocabulary_file=vocabulary_file,\n        num_oov_buckets=1,\n        key_dtype=dtypes.int32)\n    ids = table.lookup(constant_op.constant((1, -1000, 11), dtype=dtypes.int32))\n\n    if not context.executing_eagerly():\n      with self.assertRaises(errors_impl.OpError):\n        self.evaluate(ids)\n    self.evaluate(lookup_ops.tables_initializer())\n    self.assertAllEqual((1, 2, 3), self.evaluate(ids))\n\n  def test_int64_index_table_from_file(self):\n    vocabulary_file = self._createVocabFile(\n        \"f2i_vocab3.txt\", values=(\"42\", \"1\", \"-1000\"))\n    table = lookup_ops.index_table_from_file(\n        vocabulary_file=vocabulary_file,\n        num_oov_buckets=1,\n        key_dtype=dtypes.int64)\n    ids = table.lookup(constant_op.constant((1, -1000, 11), dtype=dtypes.int64))\n\n    if not context.executing_eagerly():\n      with self.assertRaises(errors_impl.OpError):\n        self.evaluate(ids)\n    self.evaluate(lookup_ops.tables_initializer())\n    self.assertAllEqual((1, 2, 3), self.evaluate(ids))\n\n  def test_index_table_from_file_with_default_value(self):\n    default_value = -42\n    vocabulary_file = self._createVocabFile(\"f2i_vocab4.txt\")\n    table = lookup_ops.index_table_from_file(\n        vocabulary_file=vocabulary_file, default_value=default_value)\n    ids = table.lookup(constant_op.constant([\"salad\", \"surgery\", \"tarkus\"]))\n\n    if not context.executing_eagerly():\n      with self.assertRaises(errors_impl.OpError):\n        self.evaluate(ids)\n    self.evaluate(lookup_ops.tables_initializer())\n    self.assertAllEqual((1, 2, default_value), self.evaluate(ids))\n\n  def test_index_table_from_file_with_oov_buckets(self):\n    vocabulary_file = self._createVocabFile(\"f2i_vocab5.txt\")\n    table = lookup_ops.index_table_from_file(\n        vocabulary_file=vocabulary_file, num_oov_buckets=1000)\n    ids = table.lookup(\n        constant_op.constant([\"salad\", \"surgery\", \"tarkus\", \"toccata\"]))\n\n    if not context.executing_eagerly():\n      with self.assertRaises(errors_impl.OpError):\n        self.evaluate(ids)\n    self.evaluate(lookup_ops.tables_initializer())\n    self.assertAllEqual(\n        (\n            1,  # From vocabulary file.\n            2,  # From vocabulary file.\n            867,  # 3 + fingerprint(\"tarkus\") mod 300.\n            860),  # 3 + fingerprint(\"toccata\") mod 300.\n        self.evaluate(ids))\n\n  def test_index_table_from_file_fails_with_empty_vocabulary_file_name(self):\n    self.assertRaises(\n        ValueError, lookup_ops.index_table_from_file, vocabulary_file=\"\")\n\n  def test_index_table_from_file_fails_with_empty_vocabulary(self):\n    self.assertRaises(\n        ValueError, lookup_ops.index_table_from_file, vocabulary_file=None)\n\n  def test_index_table_from_file_str_fails_with_zero_size_vocabulary(self):\n    vocabulary_file = self._createVocabFile(\"zero_vocab_str.txt\")\n    self.assertRaisesRegex(\n        ValueError, \"`vocab_size` must be greater than 0, got 0 for \"\n        \"vocabulary_file: .*zero_vocab_str.txt\",\n        lookup_ops.index_table_from_file,\n        vocabulary_file=vocabulary_file,\n        vocab_size=0)\n\n  def test_index_table_from_file_tensor_fails_with_zero_size_vocabulary(self):\n    vocabulary_file = constant_op.constant(\n        self._createVocabFile(\"zero_vocab_tensor.txt\"))\n    self.assertRaisesRegex(\n        ValueError, \"`vocab_size` must be greater than 0, got 0 for \"\n        \"vocabulary_file: .*zero_vocab_tensor.txt\",\n        lookup_ops.index_table_from_file,\n        vocabulary_file=vocabulary_file,\n        vocab_size=0)\n\n  def test_index_table_from_file_with_vocab_size_too_small(self):\n    vocabulary_file = self._createVocabFile(\"f2i_vocab6.txt\")\n    table = lookup_ops.index_table_from_file(\n        vocabulary_file=vocabulary_file, vocab_size=2)\n    ids = table.lookup(constant_op.constant([\"salad\", \"surgery\", \"tarkus\"]))\n\n    if not context.executing_eagerly():\n      with self.assertRaises(errors_impl.OpError):\n        self.evaluate(ids)\n    self.evaluate(lookup_ops.tables_initializer())\n    self.assertAllEqual((1, -1, -1), self.evaluate(ids))\n    self.assertEqual(2, self.evaluate(table.size()))\n\n  def test_index_table_from_file_with_vocab_size_too_large(self):\n    vocabulary_file = self._createVocabFile(\"f2i_vocab7.txt\")\n    with self.assertRaisesRegex(errors_impl.InvalidArgumentError,\n                                \"Invalid vocab_size\"):\n      table = lookup_ops.index_table_from_file(\n          vocabulary_file=vocabulary_file, vocab_size=4)\n      self.evaluate(table.initializer)\n\n  def test_index_table_from_file_with_vocab_size(self):\n    vocabulary_file = self._createVocabFile(\"f2i_vocab8.txt\")\n\n    self.assertRaises(\n        ValueError,\n        lookup_ops.index_table_from_file,\n        vocabulary_file=vocabulary_file,\n        vocab_size=0)\n\n    table = lookup_ops.index_table_from_file(\n        vocabulary_file=vocabulary_file, vocab_size=3)\n    ids = table.lookup(constant_op.constant([\"salad\", \"surgery\", \"tarkus\"]))\n\n    if not context.executing_eagerly():\n      with self.assertRaises(errors_impl.OpError):\n        self.evaluate(ids)\n    self.evaluate(lookup_ops.tables_initializer())\n    self.assertAllEqual((1, 2, -1), self.evaluate(ids))\n    self.assertEqual(3, self.evaluate(table.size()))\n\n  def test_index_table_from_file_with_invalid_hashers(self):\n    vocabulary_file = self._createVocabFile(\"invalid_hasher.txt\")\n    with self.assertRaises(TypeError):\n      lookup_ops.index_table_from_file(\n          vocabulary_file=vocabulary_file,\n          vocab_size=3,\n          num_oov_buckets=1,\n          hasher_spec=1)\n\n    table = lookup_ops.index_table_from_file(\n        vocabulary_file=vocabulary_file,\n        vocab_size=3,\n        num_oov_buckets=1,\n        hasher_spec=lookup_ops.HasherSpec(\"my-awesome-hash\", None))\n\n    self.assertRaises(ValueError, table.lookup,\n                      constant_op.constant([\"salad\", \"surgery\", \"tarkus\"]))\n\n  def test_index_table_from_file_table_ref_with_oov_buckets(self):\n    vocabulary_file = self._createVocabFile(\"f2i_vocab9.txt\")\n    table = lookup_ops.index_table_from_file(\n        vocabulary_file=vocabulary_file, num_oov_buckets=1)\n    self.assertIsNotNone(table.resource_handle)\n\n  def test_index_table_from_file_table_ref_without_oov_buckets(self):\n    vocabulary_file = self._createVocabFile(\"f2i_vocab10.txt\")\n    table = lookup_ops.index_table_from_file(\n        vocabulary_file=vocabulary_file, num_oov_buckets=0)\n    self.assertIsNotNone(table.resource_handle)\n\n\nclass IndexTableFromTensor(test.TestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_index_table_from_tensor_with_tensor_init(self):\n    table = lookup_ops.index_table_from_tensor(\n        vocabulary_list=(\"brain\", \"salad\", \"surgery\"), num_oov_buckets=1)\n\n    if not context.executing_eagerly():\n      with self.assertRaises(errors_impl.OpError):\n        self.evaluate(\n            table.lookup(constant_op.constant((\"salad\", \"surgery\", \"tarkus\"))))\n    else:\n      # Reinitializing a table in eager should work.\n      table = lookup_ops.index_table_from_tensor(\n          vocabulary_list=(\"brain\", \"salad\", \"surgery\"), num_oov_buckets=1)\n    self.evaluate(lookup_ops.tables_initializer())\n    ids = table.lookup(constant_op.constant((\"salad\", \"surgery\", \"tarkus\")))\n    self.assertAllEqual((1, 2, 3), self.evaluate(ids))\n\n  def test_int32_index_table_from_tensor_with_tensor_init(self):\n    table = lookup_ops.index_table_from_tensor(\n        vocabulary_list=(42, 1, -1000), num_oov_buckets=1, dtype=dtypes.int32)\n    ids = table.lookup(constant_op.constant((1, -1000, 11), dtype=dtypes.int32))\n\n    if not context.executing_eagerly():\n      with self.assertRaises(errors_impl.FailedPreconditionError):\n        self.evaluate(ids)\n    self.evaluate(lookup_ops.tables_initializer())\n    self.assertAllEqual((1, 2, 3), self.evaluate(ids))\n\n  def test_int64_index_table_from_tensor_with_tensor_init(self):\n    table = lookup_ops.index_table_from_tensor(\n        vocabulary_list=(42, 1, -1000), num_oov_buckets=1, dtype=dtypes.int64)\n    ids = table.lookup(constant_op.constant((1, -1000, 11), dtype=dtypes.int64))\n\n    if not context.executing_eagerly():\n      with self.assertRaises(errors_impl.FailedPreconditionError):\n        self.evaluate(ids)\n    self.evaluate(lookup_ops.tables_initializer())\n    self.assertAllEqual((1, 2, 3), self.evaluate(ids))\n\n  def test_index_table_from_tensor_with_default_value(self):\n    default_value = -42\n    table = lookup_ops.index_table_from_tensor(\n        vocabulary_list=[\"brain\", \"salad\", \"surgery\"],\n        default_value=default_value)\n    ids = table.lookup(constant_op.constant([\"salad\", \"surgery\", \"tarkus\"]))\n\n    if not context.executing_eagerly():\n      with self.assertRaises(errors_impl.FailedPreconditionError):\n        self.evaluate(ids)\n    self.evaluate(lookup_ops.tables_initializer())\n    self.assertAllEqual((1, 2, default_value), self.evaluate(ids))\n\n  def test_index_table_from_tensor_missing_vocabulary_list(self):\n    with self.assertRaisesRegex(ValueError,\n                                \"`vocabulary_list` must be specified\"):\n      lookup_ops.index_table_from_tensor(\n          vocabulary_list=None, num_oov_buckets=1)\n\n  def test_index_table_from_tensor_empty_vocabulary_list(self):\n    with self.assertRaisesRegex(errors_impl.OpError,\n                                \"keys and values cannot be empty\"):\n      _ = lookup_ops.index_table_from_tensor(\n          vocabulary_list=np.array([], dtype=np.str_), num_oov_buckets=1)\n      self.evaluate(lookup_ops.tables_initializer())\n\n  def test_index_table_from_tensor_with_invalid_hashers(self):\n    with self.assertRaises(TypeError):\n      lookup_ops.index_table_from_tensor(\n          vocabulary_list=[\"brain\", \"salad\", \"surgery\"],\n          num_oov_buckets=1,\n          hasher_spec=1)\n\n    table = lookup_ops.index_table_from_tensor(\n        vocabulary_list=[\"brain\", \"salad\", \"surgery\"],\n        num_oov_buckets=1,\n        hasher_spec=lookup_ops.HasherSpec(\"my-awesome-hash\", None))\n\n    self.assertRaises(ValueError, table.lookup,\n                      constant_op.constant([\"salad\", \"surgery\", \"tarkus\"]))\n\n\nclass IndexToStringTableFromFileTest(test.TestCase):\n\n  def _createVocabFile(self, basename, values=(\"brain\", \"salad\", \"surgery\")):\n    vocabulary_file = os.path.join(self.get_temp_dir(), basename)\n    with open(vocabulary_file, \"w\") as f:\n      f.write(\"\\n\".join(values) + \"\\n\")\n    return vocabulary_file\n\n  def test_index_to_string_table(self):\n    vocabulary_path = self._createVocabFile(\"i2f_vocab1.txt\")\n    # vocabulary_file supports string and tensor\n    type_funcs = [str, constant_op.constant]\n    for type_func in type_funcs:\n      vocabulary_file = type_func(vocabulary_path)\n      table = lookup_ops.index_to_string_table_from_file(\n          vocabulary_file=vocabulary_file)\n      features = table.lookup(constant_op.constant([0, 1, 2, 3], dtypes.int64))\n      if not context.executing_eagerly():\n        with self.assertRaises(errors_impl.OpError):\n          self.evaluate(features)\n      self.evaluate(lookup_ops.tables_initializer())\n      self.assertAllEqual((b\"brain\", b\"salad\", b\"surgery\", b\"UNK\"),\n                          self.evaluate(features))\n\n  def test_index_to_string_table_from_multicolumn_file(self):\n    vocabulary_file = self._createVocabFile(\n        \"f2i_vocab1.txt\", values=(\"brain\\t300\", \"salad\\t20\", \"surgery\\t1\"))\n    table = lookup_ops.index_to_string_table_from_file(\n        vocabulary_file=vocabulary_file,\n        key_column_index=lookup_ops.TextFileIndex.LINE_NUMBER,\n        value_column_index=0)\n    features = table.lookup(constant_op.constant([0, 1, 2, 3], dtypes.int64))\n    if not context.executing_eagerly():\n      with self.assertRaises(errors_impl.OpError):\n        self.evaluate(features)\n    self.evaluate(lookup_ops.tables_initializer())\n    self.assertAllEqual((b\"brain\", b\"salad\", b\"surgery\", b\"UNK\"),\n                        self.evaluate(features))\n\n  def test_index_to_string_table_from_multicolumn_file_custom_delimiter(self):\n    vocabulary_file = self._createVocabFile(\n        \"f2i_vocab1.txt\", values=(\"brain 300\", \"salad 20\", \"surgery 1\"))\n    table = lookup_ops.index_to_string_table_from_file(\n        vocabulary_file=vocabulary_file,\n        key_column_index=lookup_ops.TextFileIndex.LINE_NUMBER,\n        value_column_index=0,\n        delimiter=\" \")\n    features = table.lookup(constant_op.constant([0, 1, 2, 3], dtypes.int64))\n    if not context.executing_eagerly():\n      with self.assertRaises(errors_impl.OpError):\n        self.evaluate(features)\n    self.evaluate(lookup_ops.tables_initializer())\n    self.assertAllEqual((b\"brain\", b\"salad\", b\"surgery\", b\"UNK\"),\n                        self.evaluate(features))\n\n  def test_index_to_string_table_with_default_value(self):\n    default_value = b\"NONE\"\n    vocabulary_file = self._createVocabFile(\"f2i_vocab2.txt\")\n    table = lookup_ops.index_to_string_table_from_file(\n        vocabulary_file=vocabulary_file, default_value=default_value)\n    features = table.lookup(constant_op.constant([1, 2, 4], dtypes.int64))\n    if not context.executing_eagerly():\n      with self.assertRaises(errors_impl.OpError):\n        self.evaluate(features)\n    self.evaluate(lookup_ops.tables_initializer())\n    self.assertAllEqual((b\"salad\", b\"surgery\", default_value),\n                        self.evaluate(features))\n\n  def test_index_to_string_table_with_vocab_size_too_small(self):\n    default_value = b\"NONE\"\n    vocabulary_file = self._createVocabFile(\"f2i_vocab2.txt\")\n    table = lookup_ops.index_to_string_table_from_file(\n        vocabulary_file=vocabulary_file,\n        vocab_size=2,\n        default_value=default_value)\n    features = table.lookup(constant_op.constant([1, 2, 4], dtypes.int64))\n    if not context.executing_eagerly():\n      with self.assertRaises(errors_impl.OpError):\n        self.evaluate(features)\n    self.evaluate(lookup_ops.tables_initializer())\n    self.assertAllEqual((b\"salad\", default_value, default_value),\n                        self.evaluate(features))\n\n  def test_index_to_string_table_with_vocab_size_too_large(self):\n    vocabulary_file = self._createVocabFile(\"f2i_vocab6.txt\")\n    with self.assertRaisesRegex(errors_impl.InvalidArgumentError,\n                                \"Invalid vocab_size\"):\n      _ = lookup_ops.index_to_string_table_from_file(\n          vocabulary_file=vocabulary_file, vocab_size=4)\n      self.evaluate(lookup_ops.tables_initializer())\n\n  def test_index_to_string_table_with_vocab_size(self):\n    vocabulary_file = self._createVocabFile(\"f2i_vocab7.txt\")\n    table = lookup_ops.index_to_string_table_from_file(\n        vocabulary_file=vocabulary_file, vocab_size=3)\n    features = table.lookup(constant_op.constant([1, 2, 4], dtypes.int64))\n\n    if not context.executing_eagerly():\n      with self.assertRaises(errors_impl.OpError):\n        self.evaluate(features)\n    self.evaluate(lookup_ops.tables_initializer())\n    self.assertAllEqual((b\"salad\", b\"surgery\", b\"UNK\"), self.evaluate(features))\n\n\nclass IndexToStringTableFromTensorTest(test.TestCase):\n\n  def test_index_to_string_table_from_tensor(self):\n    vocabulary_list = constant_op.constant([\"brain\", \"salad\", \"surgery\"])\n    table = lookup_ops.index_to_string_table_from_tensor(\n        vocabulary_list=vocabulary_list)\n\n    indices = constant_op.constant([0, 1, 2, 3], dtypes.int64)\n    features = table.lookup(indices)\n    if not context.executing_eagerly():\n      with self.assertRaises(errors_impl.OpError):\n        self.evaluate(features)\n    self.evaluate(lookup_ops.tables_initializer())\n\n    self.assertAllEqual((b\"brain\", b\"salad\", b\"surgery\", b\"UNK\"),\n                        self.evaluate(features))\n\n  def test_duplicate_entries(self):\n    vocabulary_list = constant_op.constant([\"hello\", \"hello\"])\n    table = lookup_ops.index_to_string_table_from_tensor(\n        vocabulary_list=vocabulary_list)\n    indices = constant_op.constant([0, 1, 4], dtypes.int64)\n    features = table.lookup(indices)\n    self.evaluate(lookup_ops.tables_initializer())\n    self.assertAllEqual((b\"hello\", b\"hello\", b\"UNK\"), self.evaluate(features))\n\n  def test_index_to_string_with_default_value(self):\n    default_value = b\"NONE\"\n    vocabulary_list = constant_op.constant([\"brain\", \"salad\", \"surgery\"])\n    table = lookup_ops.index_to_string_table_from_tensor(\n        vocabulary_list=vocabulary_list, default_value=default_value)\n    indices = constant_op.constant([1, 2, 4], dtypes.int64)\n    features = table.lookup(indices)\n    if not context.executing_eagerly():\n      with self.assertRaises(errors_impl.OpError):\n        self.evaluate(features)\n    self.evaluate(lookup_ops.tables_initializer())\n    self.assertAllEqual((b\"salad\", b\"surgery\", default_value),\n                        self.evaluate(features))\n\n\nclass IdTableWithHashBucketsTest(test.TestCase):\n\n  def _createVocabFile(self, basename, values=(\"brain\", \"salad\", \"surgery\")):\n    vocabulary_file = os.path.join(self.get_temp_dir(), basename)\n    with open(vocabulary_file, \"w\") as f:\n      f.write(\"\\n\".join(values) + \"\\n\")\n    return vocabulary_file\n\n  def testStringIdTableWithHashBuckets(self):\n    vocab_file = self._createVocabFile(\"feat_to_id_1.txt\")\n    default_value = -1\n    vocab_size = 3\n    oov_buckets = 1\n    table = lookup_ops.IdTableWithHashBuckets(\n        lookup_ops.StaticHashTable(\n            lookup_ops.TextFileIdTableInitializer(\n                vocab_file, vocab_size=vocab_size), default_value),\n        oov_buckets)\n\n    self.evaluate(table.initializer)\n\n    input_string = constant_op.constant([\"brain\", \"salad\", \"surgery\", \"UNK\"])\n\n    out = table.lookup(input_string)\n    self.assertAllEqual([0, 1, 2, 3], self.evaluate(out))\n    self.assertEqual(vocab_size + oov_buckets, self.evaluate(table.size()))\n\n  def testInt32IdTableWithHashBuckets(self):\n    vocab_file = self._createVocabFile(\"feat_to_id_2.txt\", (\"42\", \"1\", \"-1000\"))\n    default_value = -1\n    vocab_size = 3\n    oov_buckets = 1\n    table = lookup_ops.IdTableWithHashBuckets(\n        lookup_ops.StaticHashTable(\n            lookup_ops.TextFileIdTableInitializer(\n                vocab_file, vocab_size=vocab_size, key_dtype=dtypes.int64),\n            default_value),\n        oov_buckets,\n        key_dtype=dtypes.int32)\n\n    self.evaluate(table.initializer)\n\n    values = constant_op.constant((42, 1, -1000, 11), dtype=dtypes.int32)\n\n    out = table.lookup(values)\n    self.assertAllEqual([0, 1, 2, 3], self.evaluate(out))\n    self.assertEqual(vocab_size + oov_buckets, self.evaluate(table.size()))\n\n  def testInt64IdTableWithHashBuckets(self):\n    vocab_file = self._createVocabFile(\"feat_to_id_3.txt\", (\"42\", \"1\", \"-1000\"))\n    default_value = -1\n    vocab_size = 3\n    oov_buckets = 1\n    table = lookup_ops.IdTableWithHashBuckets(\n        lookup_ops.StaticHashTable(\n            lookup_ops.TextFileIdTableInitializer(\n                vocab_file, vocab_size=vocab_size, key_dtype=dtypes.int64),\n            default_value), oov_buckets)\n\n    self.evaluate(table.initializer)\n\n    values = constant_op.constant((42, 1, -1000, 11), dtype=dtypes.int64)\n\n    out = table.lookup(values)\n    self.assertAllEqual([0, 1, 2, 3], self.evaluate(out))\n    self.assertEqual(vocab_size + oov_buckets, self.evaluate(table.size()))\n\n  def testStringIdTableWithOnlyHashBucket(self):\n    oov_buckets = 5\n\n    # Set a table that only uses hash buckets, for each input value returns\n    # an id calculated by fingerprint(\"input\") mod oov_buckets.\n    table = lookup_ops.IdTableWithHashBuckets(None, oov_buckets)\n    self.evaluate(table.initializer)\n\n    values = constant_op.constant((\"brain\", \"salad\", \"surgery\"))\n\n    out = table.lookup(values)\n    self.assertAllEqual(\n        [\n            3,  # fingerprint(\"brain\") mod 5.\n            1,  # fingerprint(\"salad\") mod 5.\n            4  # fingerprint(\"surgery\") mod 5\n        ],\n        self.evaluate(out))\n    self.assertEqual(oov_buckets, self.evaluate(table.size()))\n\n  def testInt32IdTableWithOnlyHashBucket(self):\n    oov_buckets = 5\n\n    # Set a table that only uses hash buckets, for each input value returns\n    # an id calculated by fingerprint(\"input\") mod oov_buckets.\n    table = lookup_ops.IdTableWithHashBuckets(\n        None, oov_buckets, key_dtype=dtypes.int32)\n    self.evaluate(table.initializer)\n\n    input_string = constant_op.constant([42, 1, -1000], dtype=dtypes.int32)\n\n    out = table.lookup(input_string)\n    self.assertAllEqual(\n        [\n            1,  # fingerprint(\"42\") mod 5.\n            4,  # fingerprint(\"1\") mod 5.\n            2  # fingerprint(\"-1000\") mod 5\n        ],\n        self.evaluate(out))\n    self.assertEqual(oov_buckets, self.evaluate(table.size()))\n\n  def testFloat64IdTableWithOnlyHashBucket(self):\n    with self.assertRaisesRegex(TypeError, \"Invalid `key_dtype`\"):\n      lookup_ops.IdTableWithHashBuckets(\n          None, num_oov_buckets=5, key_dtype=dtypes.float64)\n\n  def testBoolIdTableWithOnlyHashBucket(self):\n    with self.assertRaisesRegex(TypeError, \"Invalid `key_dtype`\"):\n      lookup_ops.IdTableWithHashBuckets(\n          None, num_oov_buckets=5, key_dtype=dtypes.bool)\n\n  def testIdTableWithHashBucketsWithMultipleInitializers(self):\n    vocab_file = self._createVocabFile(\"feat_to_id_4.txt\")\n    default_value = -1\n    vocab_size = 3\n    oov_buckets = 3\n\n    vocab_table = lookup_ops.StaticHashTable(\n        lookup_ops.TextFileIdTableInitializer(\n            vocab_file, vocab_size=vocab_size), default_value)\n    table1 = lookup_ops.IdTableWithHashBuckets(\n        vocab_table,\n        oov_buckets,\n        hasher_spec=lookup_ops.FastHashSpec,\n        name=\"table1\")\n\n    table2 = lookup_ops.IdTableWithHashBuckets(\n        vocab_table,\n        oov_buckets,\n        hasher_spec=lookup_ops.StrongHashSpec((1, 2)),\n        name=\"table2\")\n\n    self.evaluate(lookup_ops.tables_initializer())\n\n    input_string = constant_op.constant(\n        [\"fruit\", \"brain\", \"salad\", \"surgery\", \"UNK\"])\n\n    out1 = table1.lookup(input_string)\n    out2 = table2.lookup(input_string)\n\n    out1, out2 = self.evaluate([out1, out2])\n    self.assertAllEqual([5, 0, 1, 2, 5], out1)\n    self.assertAllEqual([5, 0, 1, 2, 3], out2)\n    self.assertEqual(vocab_size + oov_buckets, self.evaluate(table1.size()))\n    self.assertEqual(vocab_size + oov_buckets, self.evaluate(table2.size()))\n    if not context.executing_eagerly():\n      test_util.assert_ops_in_graph({\n          \"table1_Lookup/hash_bucket\": \"StringToHashBucketFast\",\n          \"table2_Lookup/hash_bucket\": \"StringToHashBucketStrong\",\n      }, ops.get_default_graph())\n\n  def testIdTableWithHashBucketsInitializationAcrossSessions(self):\n    vocab_file = self._createVocabFile(\"feat_to_id_5.txt\")\n    default_value = -1\n    vocab_size = 3\n    oov_buckets = 1\n    table1 = lookup_ops.IdTableWithHashBuckets(\n        lookup_ops.StaticHashTable(\n            lookup_ops.TextFileIdTableInitializer(\n                vocab_file, vocab_size=vocab_size), default_value), oov_buckets)\n\n    self.evaluate(table1.initializer)\n\n    input_string_1 = constant_op.constant([\"brain\", \"salad\", \"surgery\", \"UNK\"])\n\n    out1 = table1.lookup(input_string_1)\n\n    self.assertAllEqual([0, 1, 2, 3], self.evaluate(out1))\n    self.assertEqual(vocab_size + oov_buckets, self.evaluate(table1.size()))\n\n    default_value = -1\n    vocab_size = 3\n    oov_buckets = 1\n\n    # Underlying lookup table already initialized in previous session.\n    # No need to call self.evaluate(table2.initializer)\n    table2 = lookup_ops.IdTableWithHashBuckets(\n        lookup_ops.StaticHashTable(\n            lookup_ops.TextFileIdTableInitializer(\n                vocab_file, vocab_size=vocab_size), default_value), oov_buckets)\n\n    input_string_2 = constant_op.constant([\"fruit\", \"salad\", \"UNK\"])\n\n    out2 = table2.lookup(input_string_2)\n\n    self.assertAllEqual([3, 1, 3], self.evaluate(out2))\n    self.assertEqual(vocab_size + oov_buckets, self.evaluate(table2.size()))\n\n  def testIdTableWithHashBucketsWithMultipleInitializersDifferentDefault(self):\n    vocab_file = self._createVocabFile(\"feat_to_id_6.txt\")\n    default_value1 = -1\n    vocab_size = 3\n    oov_buckets = 0\n    table1 = lookup_ops.IdTableWithHashBuckets(\n        lookup_ops.StaticHashTable(\n            lookup_ops.TextFileIdTableInitializer(\n                vocab_file, vocab_size=vocab_size), default_value1),\n        oov_buckets)\n\n    default_value2 = -2\n    table2 = lookup_ops.IdTableWithHashBuckets(\n        lookup_ops.StaticHashTable(\n            lookup_ops.TextFileIdTableInitializer(\n                vocab_file, vocab_size=vocab_size), default_value2),\n        oov_buckets)\n\n    self.evaluate(lookup_ops.tables_initializer())\n\n    input_string_1 = constant_op.constant(\n        [\"brain\", \"salad\", \"surgery\", \"UNK\"])\n    input_string_2 = constant_op.constant([\"fruit\", \"salad\", \"UNK\"])\n\n    out1 = table1.lookup(input_string_1)\n    out2 = table2.lookup(input_string_2)\n\n    out1, out2 = self.evaluate([out1, out2])\n    self.assertAllEqual([0, 1, 2, -1], out1)\n    self.assertAllEqual([-2, 1, -2], out2)\n    self.assertEqual(vocab_size + oov_buckets, self.evaluate(table1.size()))\n    self.assertEqual(vocab_size + oov_buckets, self.evaluate(table2.size()))\n\n  def testSparseTensor(self):\n    vocab_file = self._createVocabFile(\"feat_to_id_7.txt\")\n    input_indices = [[0, 0], [0, 1], [2, 0], [2, 2], [3, 0]]\n    input_shape = [4, 4]\n    sp_features = sparse_tensor.SparseTensor(\n        constant_op.constant(input_indices, dtypes.int64),\n        constant_op.constant([\"brain\", \"salad\", \"brain\", \"surgery\", \"tarkus\"],\n                             dtypes.string),\n        constant_op.constant(input_shape, dtypes.int64))\n\n    table = lookup_ops.IdTableWithHashBuckets(\n        lookup_ops.StaticHashTable(\n            lookup_ops.TextFileIdTableInitializer(vocab_file, vocab_size=3),\n            -1), 1)\n    self.evaluate(table.initializer)\n\n    sp_ids = table.lookup(sp_features)\n\n    self.assertAllEqual([5], sp_ids.values._shape_as_list())\n\n    sp_ids_ind, sp_ids_val, sp_ids_shape = self.evaluate(\n        [sp_ids.indices, sp_ids.values, sp_ids.dense_shape])\n\n    self.assertAllEqual(input_indices, sp_ids_ind)\n    self.assertAllEqual([0, 1, 0, 2, 3], sp_ids_val)\n    self.assertAllEqual(input_shape, sp_ids_shape)\n\n  def testRaggedTensor(self):\n    vocab_file = self._createVocabFile(\"feat_to_id_7.txt\")\n    input_row_splits = [0, 2, 4, 5]\n    ragged_features = ragged_tensor.RaggedTensor.from_row_splits(\n        constant_op.constant([\"brain\", \"salad\", \"brain\", \"surgery\", \"tarkus\"],\n                             dtypes.string),\n        constant_op.constant(input_row_splits, dtypes.int64))\n\n    table = lookup_ops.IdTableWithHashBuckets(\n        lookup_ops.StaticHashTable(\n            lookup_ops.TextFileIdTableInitializer(vocab_file, vocab_size=3),\n            -1), 1)\n    self.evaluate(table.initializer)\n\n    ragged_ids = table.lookup(ragged_features)\n    self.assertAllEqual([5], ragged_ids.values._shape_as_list())\n\n    ragged_ids_val, ragged_ids_row_splits = self.evaluate(\n        [ragged_ids.values, ragged_ids.row_splits])\n\n    self.assertAllEqual([0, 1, 0, 2, 3], ragged_ids_val)\n    self.assertAllEqual(input_row_splits, ragged_ids_row_splits)\n\n  def testInt32SparseTensor(self):\n    input_indices = [[0, 0], [0, 1], [2, 0], [2, 2], [3, 0]]\n    input_shape = [4, 4]\n    sp_features = sparse_tensor.SparseTensor(\n        constant_op.constant(input_indices, dtypes.int64),\n        constant_op.constant([42, 1, 42, -1000, 11], dtypes.int32),\n        constant_op.constant(input_shape, dtypes.int64))\n\n    table = lookup_ops.IdTableWithHashBuckets(\n        lookup_ops.StaticHashTable(\n            lookup_ops.KeyValueTensorInitializer(\n                (42, 1, -1000), (0, 1, 2), dtypes.int64, dtypes.int64), -1),\n        1,\n        key_dtype=dtypes.int32)\n    self.evaluate(table.initializer)\n\n    sp_ids = table.lookup(sp_features)\n\n    self.assertAllEqual([5], sp_ids.values._shape_as_list())\n\n    sp_ids_ind, sp_ids_val, sp_ids_shape = self.evaluate(\n        [sp_ids.indices, sp_ids.values, sp_ids.dense_shape])\n\n    self.assertAllEqual(input_indices, sp_ids_ind)\n    self.assertAllEqual([0, 1, 0, 2, 3], sp_ids_val)\n    self.assertAllEqual(input_shape, sp_ids_shape)\n\n  def testInt32RaggedTensor(self):\n    input_row_splits = [0, 2, 4, 5]\n    ragged_features = ragged_tensor.RaggedTensor.from_row_splits(\n        constant_op.constant([42, 1, 42, -1000, 11], dtypes.int32),\n        constant_op.constant(input_row_splits, dtypes.int32))\n\n    table = lookup_ops.IdTableWithHashBuckets(\n        lookup_ops.StaticHashTable(\n            lookup_ops.KeyValueTensorInitializer(\n                (42, 1, -1000), (0, 1, 2), dtypes.int64, dtypes.int64), -1),\n        1,\n        key_dtype=dtypes.int32)\n    self.evaluate(table.initializer)\n\n    ragged_ids = table.lookup(ragged_features)\n\n    self.assertAllEqual([5], ragged_ids.values._shape_as_list())\n\n    ragged_ids_val, ragged_ids_row_splits = self.evaluate(\n        [ragged_ids.values, ragged_ids.row_splits])\n\n    self.assertAllEqual([0, 1, 0, 2, 3], ragged_ids_val)\n    self.assertAllEqual(input_row_splits, ragged_ids_row_splits)\n\n  def testInt64SparseTensor(self):\n    input_indices = [[0, 0], [0, 1], [2, 0], [2, 2], [3, 0]]\n    input_shape = [4, 4]\n    sp_features = sparse_tensor.SparseTensor(\n        constant_op.constant(input_indices, dtypes.int64),\n        constant_op.constant([42, 1, 42, -1000, 11], dtypes.int64),\n        constant_op.constant(input_shape, dtypes.int64))\n\n    table = lookup_ops.IdTableWithHashBuckets(\n        lookup_ops.StaticHashTable(\n            lookup_ops.KeyValueTensorInitializer(\n                (42, 1, -1000), (0, 1, 2), dtypes.int64, dtypes.int64), -1),\n        1,\n        key_dtype=dtypes.int64)\n    self.evaluate(table.initializer)\n\n    sp_ids = table.lookup(sp_features)\n\n    self.assertAllEqual([5], sp_ids.values._shape_as_list())\n\n    sp_ids_ind, sp_ids_val, sp_ids_shape = self.evaluate(\n        [sp_ids.indices, sp_ids.values, sp_ids.dense_shape])\n\n    self.assertAllEqual(input_indices, sp_ids_ind)\n    self.assertAllEqual([0, 1, 0, 2, 3], sp_ids_val)\n    self.assertAllEqual(input_shape, sp_ids_shape)\n\n  def testInt64RaggedTensor(self):\n    input_row_splits = [0, 2, 4, 5]\n    ragged_features = ragged_tensor.RaggedTensor.from_row_splits(\n        constant_op.constant([42, 1, 42, -1000, 11], dtypes.int64),\n        constant_op.constant(input_row_splits, dtypes.int64))\n\n    table = lookup_ops.IdTableWithHashBuckets(\n        lookup_ops.StaticHashTable(\n            lookup_ops.KeyValueTensorInitializer(\n                (42, 1, -1000), (0, 1, 2), dtypes.int64, dtypes.int64), -1),\n        1,\n        key_dtype=dtypes.int64)\n    self.evaluate(table.initializer)\n\n    ragged_ids = table.lookup(ragged_features)\n\n    self.assertAllEqual([5], ragged_ids.values._shape_as_list())\n\n    ragged_ids_val, ragged_ids_row_splits = self.evaluate(\n        [ragged_ids.values, ragged_ids.row_splits])\n\n    self.assertAllEqual([0, 1, 0, 2, 3], ragged_ids_val)\n    self.assertAllEqual(input_row_splits, ragged_ids_row_splits)\n\n  def testIdTableWithHashBucketsWithInvalidHashers(self):\n    vocab_file = self._createVocabFile(\"feat_to_id_4.txt\")\n    default_value = -1\n    vocab_size = 3\n    oov_buckets = 1\n    lookup_table = lookup_ops.StaticHashTable(\n        lookup_ops.TextFileIdTableInitializer(\n            vocab_file, vocab_size=vocab_size), default_value)\n\n    with self.assertRaises(TypeError):\n      lookup_ops.IdTableWithHashBuckets(\n          lookup_table, oov_buckets, hasher_spec=1)\n\n    table = lookup_ops.IdTableWithHashBuckets(\n        lookup_table,\n        oov_buckets,\n        hasher_spec=lookup_ops.HasherSpec(\"my-awesome-hash\", None))\n\n    input_string = constant_op.constant([\"brain\", \"salad\", \"surgery\", \"UNK\"])\n\n    with self.assertRaises(ValueError):\n      table.lookup(input_string)\n\n    with self.assertRaises(ValueError):\n      table = lookup_ops.IdTableWithHashBuckets(\n          lookup_table, oov_buckets, hasher_spec=lookup_ops.StrongHashSpec([]))\n\n    with self.assertRaises(ValueError):\n      table = lookup_ops.IdTableWithHashBuckets(\n          lookup_table,\n          oov_buckets,\n          hasher_spec=lookup_ops.StrongHashSpec([1, 2, 3]))\n\n    with self.assertRaises(TypeError):\n      table = lookup_ops.IdTableWithHashBuckets(\n          lookup_table,\n          oov_buckets,\n          hasher_spec=lookup_ops.StrongHashSpec([None, 2]))\n\n  def testIdTableWithHashBucketsNoInnerTable(self):\n    table = lookup_ops.IdTableWithHashBuckets(None, num_oov_buckets=1)\n    self.assertIsNone(table.resource_handle)\n\n\n@parameterized.named_parameters(\n    (f\"_{is_anonymous}\", is_anonymous) for is_anonymous in [False, True])\nclass MutableHashTableOpTest(test.TestCase):\n\n  def testMutableHashTable(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    default_val = -1\n    keys = constant_op.constant([\"brain\", \"salad\", \"surgery\", \"tarkus\"])\n    values = constant_op.constant([0, 1, 2, 3], dtypes.int64)\n    table = lookup_ops.MutableHashTable(\n        dtypes.string,\n        dtypes.int64,\n        default_val,\n        experimental_is_anonymous=is_anonymous)\n    self.assertAllEqual(0, self.evaluate(table.size()))\n\n    self.evaluate(table.insert(keys, values))\n    self.assertAllEqual(4, self.evaluate(table.size()))\n\n    remove_string = constant_op.constant([\"tarkus\", \"tank\"])\n    self.evaluate(table.remove(remove_string))\n    self.assertAllEqual(3, self.evaluate(table.size()))\n\n    input_string = constant_op.constant([\"brain\", \"salad\", \"tank\"])\n    output = table.lookup(input_string)\n    self.assertAllEqual([3], output.get_shape())\n\n    result = self.evaluate(output)\n    self.assertAllEqual([0, 1, -1], result)\n\n    exported_keys, exported_values = table.export()\n\n    # exported data is in the order of the internal map, i.e. undefined\n    sorted_keys = np.sort(self.evaluate(exported_keys))\n    sorted_values = np.sort(self.evaluate(exported_values))\n    self.assertAllEqual([b\"brain\", b\"salad\", b\"surgery\"], sorted_keys)\n    self.assertAllEqual([0, 1, 2], sorted_values)\n\n  # TODO(https://github.com/tensorflow/tensorflow/issues/24439): remove exepectedFailure when fixed\n  @unittest.expectedFailure\n  @test_util.run_v2_only\n  def testImportedHashTable(self, is_anonymous):\n    g = ops.Graph()\n    with g.as_default():\n      default_val = -1\n      keys = constant_op.constant([\"brain\", \"salad\", \"surgery\", \"tarkus\"])\n      values = constant_op.constant([0, 1, 2, 3], dtypes.int64)\n      table = lookup_ops.MutableHashTable(\n          dtypes.string,\n          dtypes.int64,\n          default_val,\n          experimental_is_anonymous=is_anonymous)\n      self.evaluate(table.insert(keys, values))\n      op = table.lookup(constant_op.constant([\"brain\", \"salad\", \"tank\"]))\n      meta_graph = saver.export_meta_graph()\n\n    def f():\n      saver.import_meta_graph(meta_graph)\n      return ops.get_default_graph().get_tensor_by_name(op.name)\n\n    wrapped = wrap_function.wrap_function(f, [])\n    self.assertAllEqual([0, 1, -1], wrapped())\n\n  @test_util.run_v1_only(\"SaverV1\")\n  def testSaveRestore(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    save_dir = os.path.join(self.get_temp_dir(), \"save_restore\")\n    save_path = os.path.join(tempfile.mkdtemp(prefix=save_dir), \"hash\")\n\n    with self.session(graph=ops.Graph()) as sess:\n      v0 = variables.Variable(10.0, name=\"v0\")\n      v1 = variables.Variable(20.0, name=\"v1\")\n\n      default_val = -1\n      keys = constant_op.constant([\"b\", \"c\", \"d\"], dtypes.string)\n      values = constant_op.constant([0, 1, 2], dtypes.int64)\n      table = lookup_ops.MutableHashTable(\n          dtypes.string,\n          dtypes.int64,\n          default_val,\n          name=\"t1\",\n          checkpoint=True,\n          experimental_is_anonymous=is_anonymous)\n\n      save = saver.Saver()\n      self.evaluate(variables.global_variables_initializer())\n\n      # Check that the parameter nodes have been initialized.\n      self.assertEqual(10.0, self.evaluate(v0))\n      self.assertEqual(20.0, self.evaluate(v1))\n\n      self.assertAllEqual(0, self.evaluate(table.size()))\n      self.evaluate(table.insert(keys, values))\n      self.assertAllEqual(3, self.evaluate(table.size()))\n\n      val = save.save(sess, save_path)\n      self.assertIsInstance(val, str)\n      self.assertEqual(save_path, val)\n\n    with self.session(graph=ops.Graph()) as sess:\n      v0 = variables.Variable(-1.0, name=\"v0\")\n      v1 = variables.Variable(-1.0, name=\"v1\")\n      default_val = -1\n      table = lookup_ops.MutableHashTable(\n          dtypes.string,\n          dtypes.int64,\n          default_val,\n          name=\"t1\",\n          checkpoint=True,\n          experimental_is_anonymous=is_anonymous)\n      self.evaluate(\n          table.insert(\n              constant_op.constant([\"a\", \"c\"], dtypes.string),\n              constant_op.constant([12, 24], dtypes.int64)))\n      self.assertAllEqual(2, self.evaluate(table.size()))\n\n      save = saver.Saver()\n\n      # Restore the saved values in the parameter nodes.\n      save.restore(sess, save_path)\n      # Check that the parameter nodes have been restored.\n      self.assertEqual(10.0, self.evaluate(v0))\n      self.assertEqual(20.0, self.evaluate(v1))\n\n      self.assertAllEqual(3, self.evaluate(table.size()))\n\n      input_string = constant_op.constant([\"a\", \"b\", \"c\", \"d\", \"e\"],\n                                          dtypes.string)\n      output = table.lookup(input_string)\n      self.assertAllEqual([-1, 0, 1, 2, -1], self.evaluate(output))\n\n  @test_util.run_v1_only(\"SaverV1\")\n  def testSaveRestoreOnlyTable(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    save_dir = os.path.join(self.get_temp_dir(), \"save_restore\")\n    save_path = os.path.join(tempfile.mkdtemp(prefix=save_dir), \"hash\")\n\n    with self.session(graph=ops.Graph()) as sess:\n      v0 = variables.Variable(10.0, name=\"v0\")\n      v1 = variables.Variable(20.0, name=\"v1\")\n\n      default_val = -1\n      keys = constant_op.constant([\"b\", \"c\", \"d\"], dtypes.string)\n      values = constant_op.constant([0, 1, 2], dtypes.int64)\n      table = lookup_ops.MutableHashTable(\n          dtypes.string,\n          dtypes.int64,\n          default_val,\n          name=\"t1\",\n          checkpoint=True,\n          experimental_is_anonymous=is_anonymous)\n\n      save = saver.Saver([table])\n      self.evaluate(variables.global_variables_initializer())\n\n      # Check that the parameter nodes have been initialized.\n      self.assertEqual(10.0, self.evaluate(v0))\n      self.assertEqual(20.0, self.evaluate(v1))\n\n      self.assertAllEqual(0, self.evaluate(table.size()))\n      self.evaluate(table.insert(keys, values))\n      self.assertAllEqual(3, self.evaluate(table.size()))\n\n      val = save.save(sess, save_path)\n      self.assertIsInstance(val, str)\n      self.assertEqual(save_path, val)\n\n    with self.session(graph=ops.Graph()) as sess:\n      default_val = -1\n      table = lookup_ops.MutableHashTable(\n          dtypes.string,\n          dtypes.int64,\n          default_val,\n          name=\"t1\",\n          checkpoint=True,\n          experimental_is_anonymous=is_anonymous)\n      self.evaluate(\n          table.insert(\n              constant_op.constant([\"a\", \"c\"], dtypes.string),\n              constant_op.constant([12, 24], dtypes.int64)))\n      self.assertAllEqual(2, self.evaluate(table.size()))\n\n      save = saver.Saver([table])\n\n      # Restore the saved values in the parameter nodes.\n      save.restore(sess, save_path)\n      # Check that the parameter nodes have been restored.\n\n      self.assertAllEqual(3, self.evaluate(table.size()))\n\n      input_string = constant_op.constant([\"a\", \"b\", \"c\", \"d\", \"e\"],\n                                          dtypes.string)\n      output = table.lookup(input_string)\n      self.assertAllEqual([-1, 0, 1, 2, -1], self.evaluate(output))\n\n  @test_util.run_in_graph_and_eager_modes\n  def testObjectSaveRestore(self, is_anonymous):\n    if is_anonymous and not context.executing_eagerly():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    save_dir = os.path.join(self.get_temp_dir(), \"save_restore\")\n    save_prefix = os.path.join(tempfile.mkdtemp(prefix=save_dir), \"hash\")\n\n    v0 = variables.Variable(10.0, name=\"v0\")\n    v1 = variables.Variable(20.0, name=\"v1\")\n\n    default_val = -1\n    keys = constant_op.constant([\"b\", \"c\", \"d\"], dtypes.string)\n    values = constant_op.constant([0, 1, 2], dtypes.int64)\n    table = lookup_ops.MutableHashTable(\n        dtypes.string,\n        dtypes.int64,\n        default_val,\n        name=\"t1\",\n        checkpoint=True,\n        experimental_is_anonymous=is_anonymous)\n\n    checkpoint = trackable.Checkpoint(table=table, v0=v0, v1=v1)\n    self.evaluate([v0.initializer, v1.initializer])\n\n    # Check that the parameter nodes have been initialized.\n    self.assertEqual(10.0, self.evaluate(v0))\n    self.assertEqual(20.0, self.evaluate(v1))\n\n    self.assertAllEqual(0, self.evaluate(table.size()))\n    self.evaluate(table.insert(keys, values))\n    self.assertAllEqual(3, self.evaluate(table.size()))\n\n    save_path = checkpoint.save(save_prefix)\n    del table, checkpoint, v0, v1\n\n    v0 = variables.Variable(-1.0, name=\"v0\")\n    v1 = variables.Variable(-1.0, name=\"v1\")\n    default_val = -1\n    table = lookup_ops.MutableHashTable(\n        dtypes.string,\n        dtypes.int64,\n        default_val,\n        name=\"t1\",\n        checkpoint=True,\n        experimental_is_anonymous=is_anonymous)\n    self.evaluate(\n        table.insert(\n            constant_op.constant([\"a\", \"c\"], dtypes.string),\n            constant_op.constant([12, 24], dtypes.int64)))\n    self.assertAllEqual(2, self.evaluate(table.size()))\n\n    checkpoint = trackable.Checkpoint(table=table, v0=v0, v1=v1)\n\n    # Restore the saved values in the parameter nodes.\n    checkpoint.restore(save_path).run_restore_ops()\n    # Check that the parameter nodes have been restored.\n    self.assertEqual(10.0, self.evaluate(v0))\n    self.assertEqual(20.0, self.evaluate(v1))\n\n    self.assertAllEqual(3, self.evaluate(table.size()))\n\n    input_string = constant_op.constant([\"a\", \"b\", \"c\", \"d\", \"e\"],\n                                        dtypes.string)\n    output = table.lookup(input_string)\n    self.assertAllEqual([-1, 0, 1, 2, -1], self.evaluate(output))\n\n  @test_util.run_v2_only\n  def testSavedModelSaveRestore(self, is_anonymous):\n    save_dir = os.path.join(self.get_temp_dir(), \"save_restore\")\n    save_path = os.path.join(tempfile.mkdtemp(prefix=save_dir), \"hash\")\n\n    root = autotrackable.AutoTrackable()\n\n    default_value = -1\n    keys = constant_op.constant([11, 12, 13], dtypes.int64)\n    values = constant_op.constant([0, 1, 2], dtypes.int64)\n    root.table = lookup_ops.MutableHashTable(\n        dtypes.int64,\n        dtypes.int64,\n        default_value,\n        experimental_is_anonymous=is_anonymous)\n\n    @def_function.function(\n        input_signature=[tensor_spec.TensorSpec((), dtypes.int64)])\n    def lookup(key):\n      return root.table.lookup(key)\n\n    @def_function.function(input_signature=[])\n    def size():\n      return root.table.size()\n\n    @def_function.function(input_signature=[])\n    def is_ref_counting():\n      return test_ops.is_resource_handle_ref_counting(\n          root.table.resource_handle)\n\n    root.lookup = lookup\n    root.size = size\n    root.is_ref_counting = is_ref_counting\n\n    self.assertEqual(root.table.size(), 0)\n    root.table.insert(keys, values)\n    self.assertEqual(root.table.size(), 3)\n    self.assertEqual(root.table.lookup(12), 1)\n    self.assertEqual(root.table.lookup(10), -1)\n    self.assertEqual(len(root.table.export()[0]), 3)\n    self.assertEqual(root.is_ref_counting(), is_anonymous)\n\n    saved_model_save.save(root, save_path)\n\n    del root\n    loaded = saved_model_load.load(save_path)\n    self.assertEqual(loaded.size(), 3)\n    self.assertEqual(loaded.lookup(12), 1)\n    self.assertEqual(loaded.lookup(10), -1)\n    self.assertEqual(loaded.is_ref_counting(), is_anonymous)\n\n  @test_util.run_v1_only(\"Multiple sessions\")\n  def testSharing(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    # Start a server to store the table state\n    server = server_lib.Server({\"local0\": [\"localhost:0\"]},\n                               protocol=\"grpc\",\n                               start=True)\n    # Create two sessions sharing the same state\n    session1 = session.Session(server.target)\n    session2 = session.Session(server.target)\n\n    table = lookup_ops.MutableHashTable(\n        dtypes.int64,\n        dtypes.string,\n        \"-\",\n        name=\"t1\",\n        experimental_is_anonymous=is_anonymous)\n\n    # Populate the table in the first session\n    with session1:\n      self.assertAllEqual(0, table.size())\n\n      keys = constant_op.constant([11, 12], dtypes.int64)\n      values = constant_op.constant([\"a\", \"b\"])\n      table.insert(keys, values).run()\n      self.assertAllEqual(2, table.size())\n\n      output = table.lookup(constant_op.constant([11, 12, 13], dtypes.int64))\n      self.assertAllEqual([b\"a\", b\"b\", b\"-\"], output)\n\n    # Verify that we can access the shared data from the second session\n    with session2:\n      self.assertAllEqual(2, table.size())\n\n      output = table.lookup(constant_op.constant([10, 11, 12], dtypes.int64))\n      self.assertAllEqual([b\"-\", b\"a\", b\"b\"], output)\n\n  def testMutableHashTableOfTensors(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    default_val = constant_op.constant([-1, -1], dtypes.int64)\n    keys = constant_op.constant([\"brain\", \"salad\", \"surgery\", \"tarkus\"])\n    values = constant_op.constant([[0, 1], [2, 3], [4, 5], [6, 7]],\n                                  dtypes.int64)\n    table = lookup_ops.MutableHashTable(\n        dtypes.string,\n        dtypes.int64,\n        default_val,\n        experimental_is_anonymous=is_anonymous)\n    self.assertAllEqual(0, self.evaluate(table.size()))\n\n    self.evaluate(table.insert(keys, values))\n    self.assertAllEqual(4, self.evaluate(table.size()))\n\n    remove_string = constant_op.constant([\"tarkus\", \"tank\"])\n    self.evaluate(table.remove(remove_string))\n    self.assertAllEqual(3, self.evaluate(table.size()))\n\n    input_string = constant_op.constant([\"brain\", \"salad\", \"tank\"])\n    output = table.lookup(input_string)\n    self.assertAllEqual([3, 2], output.get_shape())\n\n    result = self.evaluate(output)\n    self.assertAllEqual([[0, 1], [2, 3], [-1, -1]], result)\n\n    exported_keys, exported_values = table.export()\n    # exported data is in the order of the internal map, i.e. undefined\n    sorted_keys = np.sort(self.evaluate(exported_keys))\n    sorted_values = np.sort(self.evaluate(exported_values), axis=0)\n    self.assertAllEqual([b\"brain\", b\"salad\", b\"surgery\"], sorted_keys)\n    sorted_expected_values = np.sort([[4, 5], [2, 3], [0, 1]], axis=0)\n    self.assertAllEqual(sorted_expected_values, sorted_values)\n\n  def testMutableHashTableExportInsert(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    default_val = constant_op.constant([-1, -1], dtypes.int64)\n    keys = constant_op.constant([\"brain\", \"salad\", \"surgery\"])\n    values = constant_op.constant([[0, 1], [2, 3], [4, 5]], dtypes.int64)\n    table1 = lookup_ops.MutableHashTable(\n        dtypes.string,\n        dtypes.int64,\n        default_val,\n        experimental_is_anonymous=is_anonymous)\n    self.assertAllEqual(0, self.evaluate(table1.size()))\n    self.evaluate(table1.insert(keys, values))\n    self.assertAllEqual(3, self.evaluate(table1.size()))\n\n    input_string = constant_op.constant([\"brain\", \"salad\", \"tank\"])\n    expected_output = [[0, 1], [2, 3], [-1, -1]]\n    output1 = table1.lookup(input_string)\n    self.assertAllEqual(expected_output, self.evaluate(output1))\n\n    exported_keys, exported_values = table1.export()\n    self.assertAllEqual(3, self.evaluate(exported_keys).size)\n    self.assertAllEqual(6, self.evaluate(exported_values).size)\n\n    # Populate a second table from the exported data\n    table2 = lookup_ops.MutableHashTable(\n        dtypes.string,\n        dtypes.int64,\n        default_val,\n        experimental_is_anonymous=is_anonymous)\n    self.assertAllEqual(0, self.evaluate(table2.size()))\n    self.evaluate(table2.insert(exported_keys, exported_values))\n    self.assertAllEqual(3, self.evaluate(table2.size()))\n\n    # Verify lookup result is still the same\n    output2 = table2.lookup(input_string)\n    self.assertAllEqual(expected_output, self.evaluate(output2))\n\n  def testMutableHashTableOfTensorsInvalidShape(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    default_val = constant_op.constant([-1, -1], dtypes.int64)\n    keys = constant_op.constant([\"brain\", \"salad\", \"surgery\"])\n    table = lookup_ops.MutableHashTable(\n        dtypes.string,\n        dtypes.int64,\n        default_val,\n        experimental_is_anonymous=is_anonymous)\n\n    # Shape [6] instead of [3, 2]\n    values = constant_op.constant([0, 1, 2, 3, 4, 5], dtypes.int64)\n    with self.assertRaisesOpError(\"Expected shape\"):\n      self.evaluate(table.insert(keys, values))\n\n    # Shape [2,3] instead of [3, 2]\n    values = constant_op.constant([[0, 1, 2], [3, 4, 5]], dtypes.int64)\n    with self.assertRaisesOpError(\"Expected shape\"):\n      self.evaluate(table.insert(keys, values))\n\n    # Shape [2, 2] instead of [3, 2]\n    values = constant_op.constant([[0, 1], [2, 3]], dtypes.int64)\n    with self.assertRaisesOpError(\"Expected shape\"):\n      self.evaluate(table.insert(keys, values))\n\n    # Shape [3, 1] instead of [3, 2]\n    values = constant_op.constant([[0], [2], [4]], dtypes.int64)\n    with self.assertRaisesOpError(\"Expected shape\"):\n      self.evaluate(table.insert(keys, values))\n\n    # Valid Insert\n    values = constant_op.constant([[0, 1], [2, 3], [4, 5]], dtypes.int64)\n    self.evaluate(table.insert(keys, values))\n    self.assertAllEqual(3, self.evaluate(table.size()))\n\n  def testMutableHashTableInvalidDefaultValue(self, is_anonymous):\n    default_val = constant_op.constant([[-1, -1]], dtypes.int64)\n    with self.assertRaisesOpError(\"Default value must be a vector\"):\n      table = lookup_ops.MutableHashTable(\n          dtypes.string,\n          dtypes.int64,\n          default_val,\n          experimental_is_anonymous=is_anonymous)\n      self.assertAllEqual(0, self.evaluate(table.size()))\n\n  def testMutableHashTableDuplicateInsert(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    default_val = -1\n    keys = constant_op.constant([\"brain\", \"salad\", \"surgery\", \"brain\"])\n    values = constant_op.constant([0, 1, 2, 3], dtypes.int64)\n    table = lookup_ops.MutableHashTable(\n        dtypes.string,\n        dtypes.int64,\n        default_val,\n        experimental_is_anonymous=is_anonymous)\n    self.assertAllEqual(0, self.evaluate(table.size()))\n\n    self.evaluate(table.insert(keys, values))\n    self.assertAllEqual(3, self.evaluate(table.size()))\n\n    input_string = constant_op.constant([\"brain\", \"salad\", \"tank\"])\n    output = table.lookup(input_string)\n\n    result = self.evaluate(output)\n    self.assertAllEqual([3, 1, -1], result)\n\n  def testMutableHashTableFindHighRank(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    default_val = -1\n    keys = constant_op.constant([\"brain\", \"salad\", \"surgery\"])\n    values = constant_op.constant([0, 1, 2], dtypes.int64)\n    table = lookup_ops.MutableHashTable(\n        dtypes.string,\n        dtypes.int64,\n        default_val,\n        experimental_is_anonymous=is_anonymous)\n\n    self.evaluate(table.insert(keys, values))\n    self.assertAllEqual(3, self.evaluate(table.size()))\n\n    input_string = constant_op.constant([[\"brain\", \"salad\"],\n                                         [\"tank\", \"tarkus\"]])\n    output = table.lookup(input_string)\n    self.assertAllEqual([2, 2], output.get_shape())\n\n    result = self.evaluate(output)\n    self.assertAllEqual([[0, 1], [-1, -1]], result)\n\n  def testMutableHashTableFindWithInvalidShapeDefaultValue(self, is_anonymous):\n    default_val = [-1, -1]\n    table = lookup_ops.MutableHashTable(\n        dtypes.string,\n        dtypes.int64,\n        default_val,\n        experimental_is_anonymous=is_anonymous)\n\n    input_string = constant_op.constant([[\"brain\", \"salad\"], [\"tank\",\n                                                              \"tarkus\"]])\n\n    invalid_default_val = constant_op.constant(\n        [[-2, -3], [-4, -5], [-6, -7], [-8, -9]], dtypes.int64)\n\n    with self.assertRaisesRegex(\n        (ValueError, errors_impl.InvalidArgumentError),\n        \"Expected shape \\[2\\] or \\[2,2,2\\] for default value, got \\[4,2]\"):\n      self.evaluate(table.lookup(input_string, invalid_default_val))\n\n    invalid_default_val = constant_op.constant([[[-2, -3], [-4, -5]]],\n                                               dtypes.int64)\n    with self.assertRaisesRegex(\n        (ValueError, errors_impl.InvalidArgumentError),\n        \"Expected shape \\[2\\] or \\[2,2,2\\] for default value, got \\[1,2,2\\]\"):\n      self.evaluate(table.lookup(input_string, invalid_default_val))\n\n  def testMutableHashTableFindHighRankScalarWithDynamicDefaultValue(\n      self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    default_val = -1\n    keys = constant_op.constant([\"brain\", \"salad\", \"surgery\"])\n    values = constant_op.constant([0, 1, 2], dtypes.int64)\n    table = lookup_ops.MutableHashTable(\n        dtypes.string,\n        dtypes.int64,\n        default_val,\n        experimental_is_anonymous=is_anonymous)\n\n    self.evaluate(table.insert(keys, values))\n    self.assertAllEqual(3, self.evaluate(table.size()))\n\n    input_string = constant_op.constant([[\"brain\", \"salad\"], [\"tank\",\n                                                              \"tarkus\"]])\n\n    dynamic_default_val = constant_op.constant([[-2, -3], [-4, -5]],\n                                               dtypes.int64)\n    output = table.lookup(input_string, dynamic_default_val)\n    self.assertAllEqual([2, 2], output.get_shape())\n\n    result = self.evaluate(output)\n    self.assertAllEqual([[0, 1], [-4, -5]], result)\n\n  def testMutableHashTableFindHighRankVectorWithDynamicDefaultValue(\n      self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    default_val = [-1, -1]\n    keys = constant_op.constant([\"brain\", \"salad\", \"surgery\"])\n    values = constant_op.constant([[0, 1], [2, 3], [4, 5]], dtypes.int64)\n    table = lookup_ops.MutableHashTable(\n        dtypes.string,\n        dtypes.int64,\n        default_val,\n        experimental_is_anonymous=is_anonymous)\n\n    self.evaluate(table.insert(keys, values))\n    self.assertAllEqual(3, self.evaluate(table.size()))\n\n    input_string = constant_op.constant([[\"brain\", \"salad\"], [\"tank\",\n                                                              \"tarkus\"]])\n\n    dynamic_default_val = constant_op.constant(\n        [[[-2, -3], [-4, -5]], [[-6, -7], [-8, -9]]], dtypes.int64)\n    output = table.lookup(input_string, dynamic_default_val)\n    self.assertAllEqual([2, 2, 2], output.get_shape())\n\n    result = self.evaluate(output)\n    self.assertAllEqual([[[0, 1], [2, 3]], [[-6, -7], [-8, -9]]], result)\n\n  def testMutableHashTableInsertHighRank(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    default_val = -1\n    keys = constant_op.constant([[\"brain\", \"salad\"], [\"surgery\", \"tank\"]])\n    values = constant_op.constant([[0, 1], [2, 3]], dtypes.int64)\n    table = lookup_ops.MutableHashTable(\n        dtypes.string,\n        dtypes.int64,\n        default_val,\n        experimental_is_anonymous=is_anonymous)\n\n    self.evaluate(table.insert(keys, values))\n    self.assertAllEqual(4, self.evaluate(table.size()))\n\n    input_string = constant_op.constant([\"brain\", \"salad\", \"tank\", \"tarkus\"])\n    output = table.lookup(input_string)\n\n    result = self.evaluate(output)\n    self.assertAllEqual([0, 1, 3, -1], result)\n\n  def testMutableHashTableRemoveHighRank(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    default_val = -1\n    keys = constant_op.constant([[\"brain\", \"salad\"], [\"surgery\", \"tank\"]])\n    values = constant_op.constant([[0, 1], [2, 3]], dtypes.int64)\n    table = lookup_ops.MutableHashTable(\n        dtypes.string,\n        dtypes.int64,\n        default_val,\n        experimental_is_anonymous=is_anonymous)\n\n    self.evaluate(table.insert(keys, values))\n    self.assertAllEqual(4, self.evaluate(table.size()))\n\n    remove_string = constant_op.constant([\"salad\", \"tarkus\"])\n    self.evaluate(table.remove(remove_string))\n    self.assertAllEqual(3, self.evaluate(table.size()))\n\n    input_string = constant_op.constant([\"brain\", \"salad\", \"tank\", \"tarkus\"])\n    output = table.lookup(input_string)\n\n    result = self.evaluate(output)\n    self.assertAllEqual([0, -1, 3, -1], result)\n\n  def testMutableHashTableOfTensorsFindHighRank(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    default_val = constant_op.constant([-1, -1, -1], dtypes.int64)\n    keys = constant_op.constant([\"brain\", \"salad\", \"surgery\"])\n    values = constant_op.constant([[0, 1, 2], [2, 3, 4], [4, 5, 6]],\n                                  dtypes.int64)\n    table = lookup_ops.MutableHashTable(\n        dtypes.string,\n        dtypes.int64,\n        default_val,\n        experimental_is_anonymous=is_anonymous)\n\n    self.evaluate(table.insert(keys, values))\n    self.assertAllEqual(3, self.evaluate(table.size()))\n\n    input_string = constant_op.constant([[\"brain\", \"salad\"],\n                                         [\"tank\", \"tarkus\"]])\n    output = table.lookup(input_string)\n    self.assertAllEqual([2, 2, 3], output.get_shape())\n\n    result = self.evaluate(output)\n    self.assertAllEqual(\n        [[[0, 1, 2], [2, 3, 4]], [[-1, -1, -1], [-1, -1, -1]]], result)\n\n  def testMutableHashTableOfTensorsRemoveHighRank(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    default_val = constant_op.constant([-1, -1, -1], dtypes.int64)\n    keys = constant_op.constant([\"brain\", \"salad\", \"surgery\"])\n    values = constant_op.constant([[0, 1, 2], [2, 3, 4], [4, 5, 6]],\n                                  dtypes.int64)\n    table = lookup_ops.MutableHashTable(\n        dtypes.string,\n        dtypes.int64,\n        default_val,\n        experimental_is_anonymous=is_anonymous)\n\n    self.evaluate(table.insert(keys, values))\n    self.assertAllEqual(3, self.evaluate(table.size()))\n\n    remove_string = constant_op.constant([[\"brain\", \"tank\"]])\n    self.evaluate(table.remove(remove_string))\n    self.assertAllEqual(2, self.evaluate(table.size()))\n\n    input_string = constant_op.constant([[\"brain\", \"salad\"],\n                                         [\"surgery\", \"tank\"]])\n    output = table.lookup(input_string)\n    self.assertAllEqual([2, 2, 3], output.get_shape())\n\n    result = self.evaluate(output)\n    self.assertAllEqual(\n        [[[-1, -1, -1], [2, 3, 4]], [[4, 5, 6], [-1, -1, -1]]], result)\n\n  def testMultipleMutableHashTables(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    default_val = -1\n    keys = constant_op.constant([\"brain\", \"salad\", \"surgery\"])\n    values = constant_op.constant([0, 1, 2], dtypes.int64)\n\n    table1 = lookup_ops.MutableHashTable(\n        dtypes.string,\n        dtypes.int64,\n        default_val,\n        experimental_is_anonymous=is_anonymous)\n    table2 = lookup_ops.MutableHashTable(\n        dtypes.string,\n        dtypes.int64,\n        default_val,\n        experimental_is_anonymous=is_anonymous)\n    table3 = lookup_ops.MutableHashTable(\n        dtypes.string,\n        dtypes.int64,\n        default_val,\n        experimental_is_anonymous=is_anonymous)\n    self.evaluate(table1.insert(keys, values))\n    self.evaluate(table2.insert(keys, values))\n    self.evaluate(table3.insert(keys, values))\n\n    self.assertAllEqual(3, self.evaluate(table1.size()))\n    self.assertAllEqual(3, self.evaluate(table2.size()))\n    self.assertAllEqual(3, self.evaluate(table3.size()))\n\n    input_string = constant_op.constant([\"brain\", \"salad\", \"tank\"])\n    output1 = table1.lookup(input_string)\n    output2 = table2.lookup(input_string)\n    output3 = table3.lookup(input_string)\n\n    out1, out2, out3 = self.evaluate([output1, output2, output3])\n    self.assertAllEqual([0, 1, -1], out1)\n    self.assertAllEqual([0, 1, -1], out2)\n    self.assertAllEqual([0, 1, -1], out3)\n\n  def testMutableHashTableWithTensorDefault(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    default_val = constant_op.constant(-1, dtypes.int64)\n    keys = constant_op.constant([\"brain\", \"salad\", \"surgery\"])\n    values = constant_op.constant([0, 1, 2], dtypes.int64)\n    table = lookup_ops.MutableHashTable(\n        dtypes.string,\n        dtypes.int64,\n        default_val,\n        experimental_is_anonymous=is_anonymous)\n\n    self.evaluate(table.insert(keys, values))\n    self.assertAllEqual(3, self.evaluate(table.size()))\n\n    input_string = constant_op.constant([\"brain\", \"salad\", \"tank\"])\n    output = table.lookup(input_string)\n\n    result = self.evaluate(output)\n    self.assertAllEqual([0, 1, -1], result)\n\n  def testSignatureMismatch(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    default_val = -1\n    keys = constant_op.constant([\"brain\", \"salad\", \"surgery\"])\n    values = constant_op.constant([0, 1, 2], dtypes.int64)\n    table = lookup_ops.MutableHashTable(\n        dtypes.string,\n        dtypes.int64,\n        default_val,\n        experimental_is_anonymous=is_anonymous)\n\n    # insert with keys of the wrong type\n    with self.assertRaises(ValueError):\n      self.evaluate(table.insert(constant_op.constant([4, 5, 6]), values))\n\n    # insert with values of the wrong type\n    with self.assertRaises(ValueError):\n      self.evaluate(table.insert(keys, constant_op.constant([\"a\", \"b\", \"c\"])))\n\n    self.assertAllEqual(0, self.evaluate(table.size()))\n\n    self.evaluate(table.insert(keys, values))\n    self.assertAllEqual(3, self.evaluate(table.size()))\n\n    input_string_ref = variables.Variable(\"brain\")\n    input_int64_ref = variables.Variable(-1, dtype=dtypes.int64)\n    self.evaluate(variables.global_variables_initializer())\n\n    # Ref types do not produce an insert signature mismatch.\n    self.evaluate(table.insert(input_string_ref, input_int64_ref))\n    self.assertAllEqual(3, self.evaluate(table.size()))\n\n    # Ref types do not produce a lookup signature mismatch.\n    self.assertEqual(-1, self.evaluate(table.lookup(input_string_ref)))\n\n    # lookup with keys of the wrong type\n    input_string = constant_op.constant([1, 2, 3], dtypes.int64)\n    with self.assertRaises(ValueError):\n      self.evaluate(table.lookup(input_string))\n\n    # default value of the wrong type\n    with self.assertRaises(TypeError):\n      lookup_ops.MutableHashTable(\n          dtypes.string,\n          dtypes.int64,\n          \"UNK\",\n          experimental_is_anonymous=is_anonymous)\n\n  def testMutableHashTableStringFloat(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    default_val = -1.5\n    keys = constant_op.constant([\"brain\", \"salad\", \"surgery\"])\n    values = constant_op.constant([0, 1.1, 2.2], dtypes.float32)\n    table = lookup_ops.MutableHashTable(\n        dtypes.string,\n        dtypes.float32,\n        default_val,\n        experimental_is_anonymous=is_anonymous)\n    self.assertAllEqual(0, self.evaluate(table.size()))\n\n    self.evaluate(table.insert(keys, values))\n    self.assertAllEqual(3, self.evaluate(table.size()))\n\n    input_string = constant_op.constant([\"brain\", \"salad\", \"tank\"])\n    output = table.lookup(input_string)\n\n    result = self.evaluate(output)\n    self.assertAllClose([0, 1.1, default_val], result)\n\n  def testMutableHashTableIntFloat(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    default_val = -1.0\n    keys = constant_op.constant([3, 7, 0], dtypes.int64)\n    values = constant_op.constant([7.5, -1.2, 9.9], dtypes.float32)\n    table = lookup_ops.MutableHashTable(\n        dtypes.int64,\n        dtypes.float32,\n        default_val,\n        experimental_is_anonymous=is_anonymous)\n    self.assertAllEqual(0, self.evaluate(table.size()))\n\n    self.evaluate(table.insert(keys, values))\n    self.assertAllEqual(3, self.evaluate(table.size()))\n\n    input_string = constant_op.constant([7, 0, 11], dtypes.int64)\n    output = table.lookup(input_string)\n\n    result = self.evaluate(output)\n    self.assertAllClose([-1.2, 9.9, default_val], result)\n\n  def testMutableHashTableInt64String(self, is_anonymous):\n    if is_anonymous and not tf2.enabled():\n      self.skipTest(SKIP_ANONYMOUS_IN_TF1_REASON)\n    default_val = \"n/a\"\n    keys = constant_op.constant([0, 1, 2], dtypes.int64)\n    values = constant_op.constant([\"brain\", \"salad\", \"surgery\"])\n    table = lookup_ops.MutableHashTable(\n        dtypes.int64,\n        dtypes.string,\n        default_val,\n        experimental_is_anonymous=is_anonymous)\n    self.assertAllEqual(0, self.evaluate(table.size()))\n\n    self.evaluate(table.insert(keys, values))\n    self.assertAllEqual(3, self.evaluate(table.size()))\n\n    input_string = constant_op.constant([0, 1, 3], dtypes.int64)\n    output = table.lookup(input_string)\n\n    result = self.evaluate(output)\n    self.assertAllEqual((b\"brain\", b\"salad\", b\"n/a\"), result)\n\n  def testExportShapeInference(self, is_anonymous):\n    default_value = -1\n    table = lookup_ops.MutableHashTable(\n        dtypes.int64,\n        dtypes.int64,\n        default_value=default_value,\n        experimental_is_anonymous=is_anonymous)\n    actual_shapes = [t.shape for t in table.export()]\n    inferred_shapes = []\n\n    @def_function.function\n    def f():\n      for t in table.export():\n        inferred_shapes.append(t.shape)\n\n    f()\n    self.assertLen(actual_shapes, 2)\n    self.assertLen(inferred_shapes, 2)\n    self.assertTrue(inferred_shapes[0].is_compatible_with(actual_shapes[0]))\n    self.assertTrue(inferred_shapes[1].is_compatible_with(actual_shapes[1]))\n\n\nclass MutableHashTableBenchmark(test.Benchmark):\n\n  def _create_table(self):\n    return lookup_ops.MutableHashTable(dtypes.int64, dtypes.float32, 0.0)\n\n  def benchmark_single_repeated_scalar_insert_scalar(self):\n    table = self._create_table()\n    value = variables.Variable(1.0)\n    insert = table.insert(0, value)\n    size = table.size()\n    with session.Session() as sess:\n      sess.run(value.initializer)\n      self.run_op_benchmark(sess, insert, burn_iters=10, min_iters=10000)\n      assert sess.run(size) == 1\n\n  def benchmark_many_repeated_scalar_insert_scalar(self):\n    table = self._create_table()\n    c = dataset_ops.make_one_shot_iterator(counter.Counter()).get_next()\n    value = variables.Variable(1.0)\n    insert = table.insert(c, value)\n    size = table.size()\n    with session.Session() as sess:\n      sess.run(value.initializer)\n      self.run_op_benchmark(sess, insert, burn_iters=10, min_iters=10000)\n      assert sess.run(size) >= 10000\n\n  def benchmark_single_repeated_batch_32_insert_scalar(self):\n    table = self._create_table()\n    value = variables.Variable([1.0] * 32)\n    insert = table.insert(list(range(32)), value)\n    size = table.size()\n    with session.Session() as sess:\n      sess.run(value.initializer)\n      self.run_op_benchmark(sess, insert, burn_iters=10, min_iters=1000)\n      assert sess.run(size) == 32\n\n  def benchmark_many_repeated_batch_32_insert_scalar(self):\n    table = self._create_table()\n    c = dataset_ops.make_one_shot_iterator(counter.Counter()).get_next()\n    value = variables.Variable([1.0] * 32)\n    insert = table.insert(32 * c + list(range(32)), value)\n    size = table.size()\n    with session.Session() as sess:\n      sess.run(value.initializer)\n      self.run_op_benchmark(sess, insert, burn_iters=10, min_iters=1000)\n      assert sess.run(size) >= 1000 * 32\n\n\nclass DenseHashTableBenchmark(MutableHashTableBenchmark):\n\n  def _create_table(self):\n    return lookup_ops.DenseHashTable(\n        dtypes.int64,\n        dtypes.float32,\n        default_value=0.0,\n        empty_key=-1,\n        deleted_key=-2)\n\n\nif __name__ == \"__main__\":\n  test.main()\n"], "buggy_code_start_loc": [311, 43], "buggy_code_end_loc": [315, 574], "fixing_code_start_loc": [312, 44], "fixing_code_end_loc": [316, 590], "type": "CWE-476", "message": "TensorFlow is an open source platform for machine learning. The function `tf.raw_ops.LookupTableImportV2` cannot handle scalars in the `values` parameter and gives an NPE. A fix is included in TensorFlow version 2.12.0 and version 2.11.1.", "other": {"cve": {"id": "CVE-2023-25672", "sourceIdentifier": "security-advisories@github.com", "published": "2023-03-25T00:15:07.817", "lastModified": "2023-03-30T15:22:56.770", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "TensorFlow is an open source platform for machine learning. The function `tf.raw_ops.LookupTableImportV2` cannot handle scalars in the `values` parameter and gives an NPE. A fix is included in TensorFlow version 2.12.0 and version 2.11.1."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 7.5, "baseSeverity": "HIGH"}, "exploitabilityScore": 3.9, "impactScore": 3.6}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 7.5, "baseSeverity": "HIGH"}, "exploitabilityScore": 3.9, "impactScore": 3.6}]}, "weaknesses": [{"source": "security-advisories@github.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-476"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionEndExcluding": "2.12.0", "matchCriteriaId": "FAC3DE54-93B4-4D6C-9648-B9D416B9770F"}]}]}], "references": [{"url": "https://github.com/tensorflow/tensorflow/commit/980b22536abcbbe1b4a5642fc940af33d8c19b69", "source": "security-advisories@github.com", "tags": ["Patch"]}, {"url": "https://github.com/tensorflow/tensorflow/security/advisories/GHSA-94mm-g2mv-8p7r", "source": "security-advisories@github.com", "tags": ["Patch", "Vendor Advisory"]}]}, "github_commit_url": "https://github.com/tensorflow/tensorflow/commit/980b22536abcbbe1b4a5642fc940af33d8c19b69"}}