{"buggy_code": ["/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n// See docs in ../ops/data_flow_ops.cc.\n\n#define EIGEN_USE_THREADS\n\n#include <limits>\n#include <vector>\n// TODO(b/31496047): Fix non-standard include order.\n#include <numeric>  // clang-format off\n\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n#include \"tensorflow/core/framework/bounds_check.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/register_types.h\"\n#include \"tensorflow/core/framework/resource_mgr.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/tensor_shape.h\"\n#include \"tensorflow/core/framework/tensor_util.h\"\n#include \"tensorflow/core/framework/types.h\"\n#include \"tensorflow/core/kernels/concat_lib.h\"\n#include \"tensorflow/core/kernels/split_lib.h\"\n#include \"tensorflow/core/kernels/tensor_array.h\"\n#include \"tensorflow/core/lib/core/errors.h\"\n#include \"tensorflow/core/lib/core/refcount.h\"\n#include \"tensorflow/core/lib/strings/strcat.h\"\n#include \"tensorflow/core/platform/dynamic_annotations.h\"\n#include \"tensorflow/core/platform/logging.h\"\n#include \"tensorflow/core/platform/thread_annotations.h\"\n#include \"tensorflow/core/platform/types.h\"\n#include \"tensorflow/core/util/ptr_util.h\"\n\ntypedef Eigen::ThreadPoolDevice CPUDevice;\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\ntypedef Eigen::GpuDevice GPUDevice;\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n// clang-format on\n\nnamespace tensorflow {\n\nStatus GetHandle(OpKernelContext* ctx, string* container, string* ta_handle) {\n  {\n    Tensor tensor;\n    // Assuming that handle is the input at index 0.\n    if (IsRefType(ctx->input_dtype(0))) {\n      tensor = ctx->mutable_input(0, false);\n    } else {\n      tensor = ctx->input(0);\n    }\n    if (tensor.NumElements() != 2) {\n      return errors::InvalidArgument(\n          \"Tensor array handle must be 2-element vector, but had shape: \",\n          tensor.shape().DebugString());\n    }\n    auto h = tensor.flat<tstring>();\n    *container = h(0);\n    *ta_handle = h(1);\n  }\n  return OkStatus();\n}\n\nStatus GetTensorArray(OpKernelContext* ctx, TensorArray** tensor_array) {\n  string container;\n  string ta_handle;\n  if (ctx->input_dtype(0) != DT_RESOURCE) {\n    TF_RETURN_IF_ERROR(GetHandle(ctx, &container, &ta_handle));\n    ResourceMgr* rm = ctx->resource_manager();\n    if (rm == nullptr) return errors::Internal(\"No resource manager.\");\n    TF_RETURN_IF_ERROR(\n        ctx->step_container()->Lookup(rm, container + ta_handle, tensor_array));\n    return OkStatus();\n  } else {\n    return LookupResource(ctx, HandleFromInput(ctx, 0), tensor_array);\n  }\n}\n\nStatus SetupFlowControlInputs(OpKernelContext* ctx, bool set_output) {\n  const Tensor* flow_control;\n  TF_RETURN_IF_ERROR(ctx->input(\"flow_in\", &flow_control));\n  if (set_output) {\n    TF_RETURN_IF_ERROR(ctx->set_output(\"flow_out\", *flow_control));\n  }\n  return OkStatus();\n}\n\n// CREATION *******************************************************************\n\n// Virtual class for shared behavior between TensorArrayOp and\n// TensorArrayGradOp.\nclass TensorArrayCreationOp : public OpKernel {\n public:\n  explicit TensorArrayCreationOp(OpKernelConstruction* context)\n      : OpKernel(context), device_type_(context->device_type()) {}\n\n  void Compute(OpKernelContext* ctx) override {\n    Tensor tensor_array_output_handle;\n\n    AllocatorAttributes alloc_attr;\n    alloc_attr.set_on_host(true);\n    OP_REQUIRES_OK(ctx, ctx->allocate_temp(\n                            tensorflow::DT_STRING, tensorflow::TensorShape({2}),\n                            &tensor_array_output_handle, alloc_attr));\n    // Store the handle in a per-step container of the RM.\n    ResourceMgr* rm = ctx->resource_manager();\n    OP_REQUIRES(ctx, rm != nullptr, errors::Internal(\"No resource manager.\"));\n\n    TensorArray* output_tensor_array;\n    OP_REQUIRES_OK(ctx, CreateTensorArray(ctx, rm, &tensor_array_output_handle,\n                                          &output_tensor_array));\n    if (IsRefType(ctx->expected_output_dtype(0))) {\n      ctx->set_output_ref(0, output_tensor_array->mu(),\n                          output_tensor_array->handle());\n    } else if (ctx->expected_output_dtype(0) == DT_STRING) {\n      ctx->set_output(0, *output_tensor_array->handle());\n    } else {\n      Tensor* handle;\n      OP_REQUIRES_OK(ctx, ctx->allocate_output(0, TensorShape({}), &handle));\n      handle->flat<ResourceHandle>()(0) =\n          output_tensor_array->resource_handle(ctx);\n    }\n    if (ctx->num_outputs() == 2) {\n      // Create the flow output.\n      Tensor* flow;\n      OP_REQUIRES_OK(ctx, ctx->allocate_output(1, TensorShape({}), &flow));\n      if (device_type_ == DEVICE_CPU) {\n        // Value doesn't matter, but this makes msan not complaint about\n        // copying an uninitialized value. To do this on GPU would require\n        // a kernel launch or a host->device memcpy, so we avoid that.\n        flow->flat<float>()(0) = 0;\n      }\n    }\n  }\n\n protected:\n  virtual Status CreateTensorArray(OpKernelContext* ctx, ResourceMgr* rm,\n                                   Tensor* tensor_array_output_handle,\n                                   TensorArray** output_tensor_array) = 0;\n\n private:\n  const DeviceType device_type_;\n};\n\n// A per-run local tensor array. The tensor array uses a \"per-step\" resource\n// manager which ensures that correct garbage collection on error or\n// successful completion.\nclass TensorArrayOp : public TensorArrayCreationOp {\n public:\n  explicit TensorArrayOp(OpKernelConstruction* context)\n      : TensorArrayCreationOp(context) {\n    OP_REQUIRES_OK(context, context->GetAttr(\"dtype\", &dtype_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"element_shape\", &element_shape_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"dynamic_size\", &dynamic_size_));\n    // The HasAttr check is for backwards compatibility with older op\n    // versions which do not have this attribute.\n    if (context->HasAttr(\"identical_element_shapes\")) {\n      OP_REQUIRES_OK(context, context->GetAttr(\"identical_element_shapes\",\n                                               &identical_element_shapes_));\n    } else {\n      identical_element_shapes_ = false;\n    }\n    OP_REQUIRES_OK(context,\n                   context->GetAttr(\"clear_after_read\", &clear_after_read_));\n    OP_REQUIRES_OK(context,\n                   context->GetAttr(\"tensor_array_name\", &tensor_array_name_));\n    if (tensor_array_name_.empty()) tensor_array_name_ = name();\n  }\n\n  Status CreateTensorArray(OpKernelContext* ctx, ResourceMgr* rm,\n                           Tensor* tensor_array_output_handle,\n                           TensorArray** output_tensor_array) override {\n    const Tensor* tensor_size;\n    TF_RETURN_IF_ERROR(ctx->input(\"size\", &tensor_size));\n\n    if (!TensorShapeUtils::IsScalar(tensor_size->shape())) {\n      return errors::InvalidArgument(\n          \"TensorArray size must be scalar, but had shape: \",\n          tensor_size->shape().DebugString());\n    }\n    const int32_t size = tensor_size->scalar<int32>()();\n    if (size < 0) {\n      return errors::InvalidArgument(\"Size should be >= 0.\");\n    }\n\n    auto handle = tensor_array_output_handle->flat<tstring>();\n    string unique_tensor_array_name =\n        strings::StrCat(tensor_array_name_, \"_\",\n                        TensorArray::tensor_array_counter.fetch_add(1));\n    handle(0) = \"_tensor_arrays\";\n    handle(1) = unique_tensor_array_name;\n\n    auto key = strings::StrCat(handle(0), unique_tensor_array_name);\n\n    TensorArray* tensor_array = new TensorArray(\n        key, dtype_, *tensor_array_output_handle, size, element_shape_,\n        identical_element_shapes_, dynamic_size_,\n        false /* multiple_writes_aggregate */, false /* is_grad */,\n        -1 /* marked_size */, clear_after_read_);\n\n    TF_RETURN_IF_ERROR(ctx->step_container()->Create(rm, key, tensor_array));\n\n    *output_tensor_array = tensor_array;\n\n    return OkStatus();\n  }\n\n private:\n  DataType dtype_;\n  PartialTensorShape element_shape_;\n  bool identical_element_shapes_;\n  bool dynamic_size_;\n  bool clear_after_read_;\n  string tensor_array_name_;  // The name used to create the TensorArray.\n\n  TF_DISALLOW_COPY_AND_ASSIGN(TensorArrayOp);\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"TensorArray\").Device(DEVICE_CPU), TensorArrayOp);\nREGISTER_KERNEL_BUILDER(Name(\"TensorArrayV2\").Device(DEVICE_CPU),\n                        TensorArrayOp);\nREGISTER_KERNEL_BUILDER(Name(\"TensorArrayV3\").Device(DEVICE_CPU),\n                        TensorArrayOp);\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n#define REGISTER_GPU(type)                                   \\\n  REGISTER_KERNEL_BUILDER(Name(\"TensorArray\")                \\\n                              .Device(DEVICE_GPU)            \\\n                              .TypeConstraint<type>(\"dtype\") \\\n                              .HostMemory(\"size\")            \\\n                              .HostMemory(\"handle\"),         \\\n                          TensorArrayOp);                    \\\n  REGISTER_KERNEL_BUILDER(Name(\"TensorArrayV2\")              \\\n                              .Device(DEVICE_GPU)            \\\n                              .TypeConstraint<type>(\"dtype\") \\\n                              .HostMemory(\"size\")            \\\n                              .HostMemory(\"handle\"),         \\\n                          TensorArrayOp);                    \\\n  REGISTER_KERNEL_BUILDER(Name(\"TensorArrayV3\")              \\\n                              .Device(DEVICE_GPU)            \\\n                              .TypeConstraint<type>(\"dtype\") \\\n                              .HostMemory(\"size\")            \\\n                              .HostMemory(\"handle\"),         \\\n                          TensorArrayOp);\n\nTF_CALL_int64(REGISTER_GPU);\nTF_CALL_GPU_NUMBER_TYPES(REGISTER_GPU);\nTF_CALL_COMPLEX_TYPES(REGISTER_GPU);\n#undef REGISTER_GPU\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n// GRADIENT *******************************************************************\n// Note that this op may have an optional third input. If present, it represents\n// a shape value. It indicates that element shape of this gradient array is that\n// shape value concatenated with the element shape of the original tensor array.\n// See TensorArrayGradWithShape.\nclass TensorArrayGradOp : public TensorArrayCreationOp {\n public:\n  explicit TensorArrayGradOp(OpKernelConstruction* context)\n      : TensorArrayCreationOp(context) {\n    OP_REQUIRES_OK(context, context->GetAttr(\"source\", &source_));\n  }\n\n  Status CreateTensorArray(OpKernelContext* ctx, ResourceMgr* rm,\n                           Tensor* tensor_array_output_handle,\n                           TensorArray** output_tensor_array) override {\n    string container;\n    string tensor_array_name;\n    if (ctx->input_dtype(0) != DT_RESOURCE) {\n      TF_RETURN_IF_ERROR(GetHandle(ctx, &container, &tensor_array_name));\n      if (container != \"_tensor_arrays\") {\n        return errors::InvalidArgument(\n            \"Input container should be '_tensor_arrays',  but received '\",\n            container, \"'\");\n      }\n    } else {\n      container = \"_tensor_arrays\";\n      const auto& resource = ctx->input(0).flat<ResourceHandle>()(0);\n      if (StringPiece(resource.name()).substr(0, container.size()) !=\n          container) {\n        return errors::InvalidArgument(\"Wrong input container. \",\n                                       resource.name());\n      }\n      tensor_array_name =\n          string(StringPiece(resource.name()).substr(container.size()));\n    }\n\n    auto output_handle = tensor_array_output_handle->flat<tstring>();\n    output_handle(0) = \"_tensor_array_grads\";\n    output_handle(1) = strings::StrCat(tensor_array_name, \"@\", source_);\n\n    TensorArray* tensor_array;\n    TF_RETURN_IF_ERROR(ctx->step_container()->Lookup(\n        rm, strings::StrCat(container, tensor_array_name), &tensor_array));\n    core::ScopedUnref unref(tensor_array);\n\n    // Once gradients are being calculated, the forward TensorArray\n    // may no longer be resized by new Writes.\n    tensor_array->DisableDynamicSize();\n\n    int32_t array_size = 0;\n    int32_t marked_size = 0;\n    TF_RETURN_IF_ERROR(tensor_array->Size(&array_size));\n    TF_RETURN_IF_ERROR(tensor_array->MarkedSize(&marked_size));\n\n    if (array_size < 0) {\n      return errors::InvalidArgument(\"ArraySize should be >= 0.\");\n    }\n    if (!tensor_array->GradientsAllowed()) {\n      return errors::InvalidArgument(\n          \"Unable to create a gradients TensorArray for \", tensor_array_name,\n          \".  Perhaps you used the multiple_writes_aggregate flag on a \"\n          \"previous write?  Gradient calculation is impossible when multiple \"\n          \"writes are performed to the same index.\");\n    }\n    TensorShape shape_to_prepend;\n    auto element_shape = PartialTensorShape();\n    if (ctx->num_inputs() > 2) {\n      TF_RETURN_IF_ERROR(tensor::MakeShape(ctx->input(2), &shape_to_prepend));\n      auto ta_element_shape = tensor_array->ElemShape();\n      if (!ta_element_shape.unknown_rank()) {\n        std::vector<int64_t> dims;\n        for (auto dim : shape_to_prepend) {\n          dims.push_back(dim.size);\n        }\n        for (auto dim : ta_element_shape) {\n          dims.push_back(dim.size);\n        }\n        TF_RETURN_IF_ERROR(TensorShapeUtils::MakeShape(\n            gtl::ArraySlice<int64_t>(dims), &element_shape));\n      }\n    } else {\n      element_shape = tensor_array->ElemShape();\n    }\n\n    const auto key = strings::StrCat(output_handle(0), output_handle(1));\n    auto creator = [key, tensor_array, array_size, marked_size, element_shape,\n                    shape_to_prepend,\n                    tensor_array_output_handle](TensorArray** ret) -> Status {\n      *ret = new TensorArray(\n          key, tensor_array->ElemType(), *tensor_array_output_handle,\n          array_size, element_shape, tensor_array->HasIdenticalElementShapes(),\n          false /* dynamic_size */, true /* multiple_writes_aggregate */,\n          true /* is_grad */, marked_size /* marked_size */,\n          true /* close_after_read */);\n      return (*ret)->CopyShapesFrom(tensor_array, &shape_to_prepend);\n    };\n\n    Status s = ctx->step_container()->LookupOrCreate<TensorArray>(\n        rm, key, output_tensor_array, creator);\n    (*output_tensor_array)->Unref();\n\n    return s;\n  }\n\n private:\n  // The gradient source for creating the given\n  // gradient TensorArray.  This should be unique to each gradients\n  // call.  Typical values look like \"gradients\", \"gradients_1\", ...\n  string source_;\n\n  TF_DISALLOW_COPY_AND_ASSIGN(TensorArrayGradOp);\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"TensorArrayGrad\").Device(DEVICE_CPU),\n                        TensorArrayGradOp);\nREGISTER_KERNEL_BUILDER(Name(\"TensorArrayGradV2\").Device(DEVICE_CPU),\n                        TensorArrayGradOp);\nREGISTER_KERNEL_BUILDER(Name(\"TensorArrayGradV3\").Device(DEVICE_CPU),\n                        TensorArrayGradOp);\nREGISTER_KERNEL_BUILDER(Name(\"TensorArrayGradWithShape\").Device(DEVICE_CPU),\n                        TensorArrayGradOp);\nREGISTER_KERNEL_BUILDER(Name(\"TensorArrayGrad\")\n                            .Device(DEVICE_GPU)\n                            .HostMemory(\"handle\")\n                            .HostMemory(\"grad_handle\"),\n                        TensorArrayGradOp);\nREGISTER_KERNEL_BUILDER(Name(\"TensorArrayGradV2\")\n                            .Device(DEVICE_GPU)\n                            .HostMemory(\"handle\")\n                            .HostMemory(\"grad_handle\"),\n                        TensorArrayGradOp);\nREGISTER_KERNEL_BUILDER(Name(\"TensorArrayGradV3\")\n                            .Device(DEVICE_GPU)\n                            .HostMemory(\"handle\")\n                            .HostMemory(\"grad_handle\"),\n                        TensorArrayGradOp);\nREGISTER_KERNEL_BUILDER(Name(\"TensorArrayGradWithShape\")\n                            .Device(DEVICE_GPU)\n                            .HostMemory(\"handle\")\n                            .HostMemory(\"shape_to_prepend\")\n                            .HostMemory(\"grad_handle\"),\n                        TensorArrayGradOp);\n\n// WRITE **********************************************************************\n\ntemplate <typename Device, typename T>\nclass TensorArrayWriteOp : public OpKernel {\n public:\n  explicit TensorArrayWriteOp(OpKernelConstruction* context)\n      : OpKernel(context) {}\n\n  void Compute(OpKernelContext* ctx) override {\n    OP_REQUIRES_OK(ctx, SetupFlowControlInputs(ctx, true));\n\n    const Tensor* tensor_index;\n    const Tensor* tensor_value;\n    OP_REQUIRES_OK(ctx, ctx->input(\"index\", &tensor_index));\n    OP_REQUIRES_OK(ctx, ctx->input(\"value\", &tensor_value));\n\n    OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(tensor_index->shape()),\n                errors::InvalidArgument(\n                    \"TensorArray index must be scalar, but had shape: \",\n                    tensor_index->shape().DebugString()));\n\n    TensorArray* tensor_array = nullptr;\n    OP_REQUIRES_OK(ctx, GetTensorArray(ctx, &tensor_array));\n    core::ScopedUnref unref(tensor_array);\n    const int32_t index = tensor_index->scalar<int32>()();\n    OP_REQUIRES(\n        ctx, tensor_value->dtype() == tensor_array->ElemType(),\n        errors::InvalidArgument(\"TensorArray dtype is \",\n                                DataTypeString(tensor_array->ElemType()),\n                                \" but Op is trying to write dtype \",\n                                DataTypeString(tensor_value->dtype()), \".\"));\n    Status s =\n        tensor_array->WriteOrAggregate<Device, T>(ctx, index, tensor_value);\n    OP_REQUIRES_OK(ctx, s);\n  }\n};\n\n#define REGISTER_WRITE(type)                                                   \\\n  REGISTER_KERNEL_BUILDER(                                                     \\\n      Name(\"TensorArrayWrite\").Device(DEVICE_CPU).TypeConstraint<type>(\"T\"),   \\\n      TensorArrayWriteOp<CPUDevice, type>);                                    \\\n  REGISTER_KERNEL_BUILDER(                                                     \\\n      Name(\"TensorArrayWriteV2\").Device(DEVICE_CPU).TypeConstraint<type>(\"T\"), \\\n      TensorArrayWriteOp<CPUDevice, type>);                                    \\\n  REGISTER_KERNEL_BUILDER(                                                     \\\n      Name(\"TensorArrayWriteV3\").Device(DEVICE_CPU).TypeConstraint<type>(\"T\"), \\\n      TensorArrayWriteOp<CPUDevice, type>);\n\nTF_CALL_ALL_TYPES(REGISTER_WRITE);\n\n#undef REGISTER_WRITE\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n#define REGISTER_GPU(type)                                      \\\n  REGISTER_KERNEL_BUILDER(Name(\"TensorArrayWrite\")              \\\n                              .Device(DEVICE_GPU)               \\\n                              .TypeConstraint<type>(\"T\")        \\\n                              .HostMemory(\"handle\")             \\\n                              .HostMemory(\"index\"),             \\\n                          TensorArrayWriteOp<GPUDevice, type>); \\\n  REGISTER_KERNEL_BUILDER(Name(\"TensorArrayWriteV2\")            \\\n                              .Device(DEVICE_GPU)               \\\n                              .TypeConstraint<type>(\"T\")        \\\n                              .HostMemory(\"handle\")             \\\n                              .HostMemory(\"index\"),             \\\n                          TensorArrayWriteOp<GPUDevice, type>); \\\n  REGISTER_KERNEL_BUILDER(Name(\"TensorArrayWriteV3\")            \\\n                              .Device(DEVICE_GPU)               \\\n                              .TypeConstraint<type>(\"T\")        \\\n                              .HostMemory(\"handle\")             \\\n                              .HostMemory(\"index\"),             \\\n                          TensorArrayWriteOp<GPUDevice, type>);\n\nTF_CALL_GPU_NUMBER_TYPES(REGISTER_GPU);\nTF_CALL_COMPLEX_TYPES(REGISTER_GPU);\n#undef REGISTER_GPU\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n// READ ***********************************************************************\n\ntemplate <typename Device, typename T>\nclass TensorArrayReadOp : public OpKernel {\n public:\n  explicit TensorArrayReadOp(OpKernelConstruction* context)\n      : OpKernel(context) {\n    OP_REQUIRES_OK(context, context->GetAttr(\"dtype\", &dtype_));\n  }\n\n  void Compute(OpKernelContext* ctx) override {\n    OP_REQUIRES_OK(ctx, SetupFlowControlInputs(ctx, false));\n\n    const Tensor* tensor_index;\n    OP_REQUIRES_OK(ctx, ctx->input(\"index\", &tensor_index));\n\n    OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(tensor_index->shape()),\n                errors::InvalidArgument(\n                    \"TensorArray index must be scalar, but had shape: \",\n                    tensor_index->shape().DebugString()));\n\n    TensorArray* tensor_array = nullptr;\n    OP_REQUIRES_OK(ctx, GetTensorArray(ctx, &tensor_array));\n    core::ScopedUnref unref(tensor_array);\n\n    const int32_t index = tensor_index->scalar<int32>()();\n    OP_REQUIRES(\n        ctx, dtype_ == tensor_array->ElemType(),\n        errors::InvalidArgument(\n            \"TensorArray dtype is \", DataTypeString(tensor_array->ElemType()),\n            \" but Op requested dtype \", DataTypeString(dtype_), \".\"));\n    Tensor value;\n    Status s = tensor_array->Read<Device, T>(ctx, index, &value);\n    OP_REQUIRES_OK(ctx, s);\n    ctx->set_output(0, value);\n  }\n\n private:\n  DataType dtype_;\n};\n\n#define REGISTER_READ(type)                                    \\\n  REGISTER_KERNEL_BUILDER(Name(\"TensorArrayRead\")              \\\n                              .Device(DEVICE_CPU)              \\\n                              .TypeConstraint<type>(\"dtype\"),  \\\n                          TensorArrayReadOp<CPUDevice, type>); \\\n  REGISTER_KERNEL_BUILDER(Name(\"TensorArrayReadV2\")            \\\n                              .Device(DEVICE_CPU)              \\\n                              .TypeConstraint<type>(\"dtype\"),  \\\n                          TensorArrayReadOp<CPUDevice, type>); \\\n  REGISTER_KERNEL_BUILDER(Name(\"TensorArrayReadV3\")            \\\n                              .Device(DEVICE_CPU)              \\\n                              .TypeConstraint<type>(\"dtype\"),  \\\n                          TensorArrayReadOp<CPUDevice, type>);\n\nTF_CALL_ALL_TYPES(REGISTER_READ)\n\n#undef REGISTER_READ\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n#define REGISTER_GPU(type)                                     \\\n  REGISTER_KERNEL_BUILDER(Name(\"TensorArrayRead\")              \\\n                              .Device(DEVICE_GPU)              \\\n                              .TypeConstraint<type>(\"dtype\")   \\\n                              .HostMemory(\"handle\")            \\\n                              .HostMemory(\"index\"),            \\\n                          TensorArrayReadOp<GPUDevice, type>); \\\n  REGISTER_KERNEL_BUILDER(Name(\"TensorArrayReadV2\")            \\\n                              .Device(DEVICE_GPU)              \\\n                              .TypeConstraint<type>(\"dtype\")   \\\n                              .HostMemory(\"handle\")            \\\n                              .HostMemory(\"index\"),            \\\n                          TensorArrayReadOp<GPUDevice, type>); \\\n  REGISTER_KERNEL_BUILDER(Name(\"TensorArrayReadV3\")            \\\n                              .Device(DEVICE_GPU)              \\\n                              .TypeConstraint<type>(\"dtype\")   \\\n                              .HostMemory(\"handle\")            \\\n                              .HostMemory(\"index\"),            \\\n                          TensorArrayReadOp<GPUDevice, type>);\n\nTF_CALL_int64(REGISTER_GPU);\nTF_CALL_GPU_NUMBER_TYPES(REGISTER_GPU);\nTF_CALL_COMPLEX_TYPES(REGISTER_GPU);\n#undef REGISTER_GPU\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n// PACK and GATHER ************************************************************\n\n// Concatenate the elements in a TensorArray.  All elements must be\n// defined and have the same shape.\ntemplate <typename Device, typename T, bool LEGACY_PACK>\nclass TensorArrayPackOrGatherOp : public OpKernel {\n public:\n  typedef typename TTypes<T, 2>::ConstMatrix ConstMatrix;\n  typedef std::vector<std::unique_ptr<ConstMatrix> > ConstMatrixVector;\n\n  explicit TensorArrayPackOrGatherOp(OpKernelConstruction* context)\n      : OpKernel(context) {\n    OP_REQUIRES_OK(context, context->GetAttr(\"dtype\", &dtype_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"element_shape\", &element_shape_));\n  }\n\n  void Compute(OpKernelContext* ctx) override {\n    OP_REQUIRES_OK(ctx, SetupFlowControlInputs(ctx, false));\n\n    TensorArray* tensor_array = nullptr;\n    OP_REQUIRES_OK(ctx, GetTensorArray(ctx, &tensor_array));\n\n    core::ScopedUnref unref(tensor_array);\n    OP_REQUIRES(\n        ctx, dtype_ == tensor_array->ElemType(),\n        errors::InvalidArgument(\n            \"TensorArray dtype is \", DataTypeString(tensor_array->ElemType()),\n            \" but Op requested dtype \", DataTypeString(dtype_), \".\"));\n\n    // Ensure new element shape is compatible with the one stored in the\n    // TensorArray.\n    OP_REQUIRES_OK(ctx, tensor_array->SetElemShape(element_shape_));\n\n    int32_t num_indices;\n    std::vector<Tensor> values;\n    std::vector<int32> indices;\n    if (LEGACY_PACK) {\n      OP_REQUIRES_OK(ctx, tensor_array->PackOrConcatSize(&num_indices));\n      indices.resize(num_indices);\n      std::iota(indices.begin(), indices.end(), 0);\n    } else {\n      const Tensor* tensor_indices;\n      OP_REQUIRES_OK(ctx, ctx->input(\"indices\", &tensor_indices));\n      OP_REQUIRES(ctx, TensorShapeUtils::IsVector(tensor_indices->shape()),\n                  errors::InvalidArgument(\n                      \"Expected indices to be a vector, but received shape: \",\n                      tensor_indices->shape().DebugString()));\n      const auto indices_t = tensor_indices->vec<int32>();\n      num_indices = tensor_indices->NumElements();\n      indices.resize(num_indices);\n      std::copy(indices_t.data(), indices_t.data() + num_indices,\n                indices.begin());\n    }\n\n    // If there are no elements to return, return a zero-element Tensor with\n    // shape [0] + element_shape_\n    if (num_indices == 0) {\n      OP_REQUIRES(ctx, element_shape_.IsFullyDefined(),\n                  errors::Unimplemented(\n                      \"TensorArray has size zero, but element shape \",\n                      element_shape_.DebugString(),\n                      \" is not fully defined. \"\n                      \"Currently only static shapes are supported when packing \"\n                      \"zero-size TensorArrays.\"));\n      TensorShape empty_shape;\n      element_shape_.AsTensorShape(&empty_shape);\n      empty_shape.InsertDim(0, 0);\n      Tensor* empty_unused;\n      OP_REQUIRES_OK(ctx, ctx->allocate_output(0, empty_shape, &empty_unused));\n      return;\n    }\n\n    // Read all the Tensors into a vector to keep track of their memory.\n    Status s = tensor_array->ReadMany<Device, T>(ctx, indices, &values);\n    OP_REQUIRES_OK(ctx, s);\n\n    const Tensor* value_0_t = &values[0];\n\n    OP_REQUIRES(\n        ctx, element_shape_.IsCompatibleWith(value_0_t->shape()),\n        errors::InvalidArgument(\"TensorArray was passed element_shape \",\n                                element_shape_.DebugString(),\n                                \" which does not match the Tensor at index 0: \",\n                                value_0_t->shape().DebugString()));\n\n    TensorShape output_shape(value_0_t->shape());\n    output_shape.InsertDim(0, num_indices);\n\n    Tensor* output_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, output_shape, &output_tensor));\n\n    // If output_tensor is empty, there is nothing to concatenate so return it.\n    if (output_shape.num_elements() == 0) {\n      return;\n    }\n\n    ConstMatrixVector input_tensors_flat;\n    input_tensors_flat.reserve(num_indices);\n    auto output_flat =\n        output_tensor->shaped<T, 2>({1, output_shape.num_elements()});\n\n    // Insert the first value\n    input_tensors_flat.push_back(MakeUnique<ConstMatrix>(\n        value_0_t->shaped<T, 2>({1, value_0_t->NumElements()})));\n\n    for (int i = 1; i < num_indices; ++i) {\n      const Tensor* value_t = &values[i];\n      OP_REQUIRES(\n          ctx, value_0_t->shape() == value_t->shape(),\n          errors::InvalidArgument(\n              \"TensorArray has inconsistent shapes.  Index 0 has shape: \",\n              value_0_t->shape().DebugString(), \" but index \", i,\n              \" has shape: \", value_t->shape().DebugString()));\n      input_tensors_flat.push_back(MakeUnique<ConstMatrix>(\n          value_t->shaped<T, 2>({1, value_t->NumElements()})));\n    }\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n    if (std::is_same<Device, GPUDevice>::value) {\n      ConcatGPU<T>(ctx, input_tensors_flat, output_tensor, &output_flat);\n      return;\n    }\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n    ConcatCPU<T>(ctx->device(), input_tensors_flat, &output_flat);\n  }\n\n private:\n  DataType dtype_;\n  PartialTensorShape element_shape_;\n};\n\n#define REGISTER_GATHER_AND_PACK(type)                                      \\\n  REGISTER_KERNEL_BUILDER(                                                  \\\n      Name(\"TensorArrayPack\")                                               \\\n          .Device(DEVICE_CPU)                                               \\\n          .TypeConstraint<type>(\"dtype\"),                                   \\\n      TensorArrayPackOrGatherOp<CPUDevice, type, true /* LEGACY_PACK */>);  \\\n  REGISTER_KERNEL_BUILDER(                                                  \\\n      Name(\"TensorArrayGather\")                                             \\\n          .Device(DEVICE_CPU)                                               \\\n          .TypeConstraint<type>(\"dtype\"),                                   \\\n      TensorArrayPackOrGatherOp<CPUDevice, type, false /* LEGACY_PACK */>); \\\n  REGISTER_KERNEL_BUILDER(                                                  \\\n      Name(\"TensorArrayGatherV2\")                                           \\\n          .Device(DEVICE_CPU)                                               \\\n          .TypeConstraint<type>(\"dtype\"),                                   \\\n      TensorArrayPackOrGatherOp<CPUDevice, type, false /* LEGACY_PACK */>); \\\n  REGISTER_KERNEL_BUILDER(                                                  \\\n      Name(\"TensorArrayGatherV3\")                                           \\\n          .Device(DEVICE_CPU)                                               \\\n          .TypeConstraint<type>(\"dtype\"),                                   \\\n      TensorArrayPackOrGatherOp<CPUDevice, type, false /* LEGACY_PACK */>);\n\nTF_CALL_POD_STRING_TYPES(REGISTER_GATHER_AND_PACK);\nTF_CALL_variant(REGISTER_GATHER_AND_PACK);\nREGISTER_GATHER_AND_PACK(quint8);\nREGISTER_GATHER_AND_PACK(qint8);\nREGISTER_GATHER_AND_PACK(qint32);\n\n#undef REGISTER_GATHER_AND_PACK\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n#define REGISTER_GPU(type)                                                  \\\n  REGISTER_KERNEL_BUILDER(                                                  \\\n      Name(\"TensorArrayPack\")                                               \\\n          .Device(DEVICE_GPU)                                               \\\n          .TypeConstraint<type>(\"dtype\")                                    \\\n          .HostMemory(\"handle\"),                                            \\\n      TensorArrayPackOrGatherOp<GPUDevice, type, true /* LEGACY_PACK */>);  \\\n  REGISTER_KERNEL_BUILDER(                                                  \\\n      Name(\"TensorArrayGather\")                                             \\\n          .Device(DEVICE_GPU)                                               \\\n          .TypeConstraint<type>(\"dtype\")                                    \\\n          .HostMemory(\"indices\")                                            \\\n          .HostMemory(\"handle\"),                                            \\\n      TensorArrayPackOrGatherOp<GPUDevice, type, false /* LEGACY_PACK */>); \\\n  REGISTER_KERNEL_BUILDER(                                                  \\\n      Name(\"TensorArrayGatherV2\")                                           \\\n          .Device(DEVICE_GPU)                                               \\\n          .TypeConstraint<type>(\"dtype\")                                    \\\n          .HostMemory(\"indices\")                                            \\\n          .HostMemory(\"handle\"),                                            \\\n      TensorArrayPackOrGatherOp<GPUDevice, type, false /* LEGACY_PACK */>); \\\n  REGISTER_KERNEL_BUILDER(                                                  \\\n      Name(\"TensorArrayGatherV3\")                                           \\\n          .Device(DEVICE_GPU)                                               \\\n          .TypeConstraint<type>(\"dtype\")                                    \\\n          .HostMemory(\"indices\")                                            \\\n          .HostMemory(\"handle\"),                                            \\\n      TensorArrayPackOrGatherOp<GPUDevice, type, false /* LEGACY_PACK */>);\n\nTF_CALL_GPU_NUMBER_TYPES(REGISTER_GPU);\nTF_CALL_COMPLEX_TYPES(REGISTER_GPU);\n#undef REGISTER_GPU\n\n// A special GPU kernel for int32.\n// TODO(b/25387198): Also enable int32 in device memory. This kernel\n// registration requires all int32 inputs and outputs to be in host memory.\nREGISTER_KERNEL_BUILDER(\n    Name(\"TensorArrayGather\")\n        .Device(DEVICE_GPU)\n        .TypeConstraint<int32>(\"dtype\")\n        .HostMemory(\"indices\")\n        .HostMemory(\"handle\"),\n    TensorArrayPackOrGatherOp<CPUDevice, int32, false /* LEGACY_PACK */>);\nREGISTER_KERNEL_BUILDER(\n    Name(\"TensorArrayGatherV2\")\n        .Device(DEVICE_GPU)\n        .TypeConstraint<int32>(\"dtype\")\n        .HostMemory(\"indices\")\n        .HostMemory(\"handle\"),\n    TensorArrayPackOrGatherOp<CPUDevice, int32, false /* LEGACY_PACK */>);\nREGISTER_KERNEL_BUILDER(\n    Name(\"TensorArrayGatherV3\")\n        .Device(DEVICE_GPU)\n        .TypeConstraint<int32>(\"dtype\")\n        .HostMemory(\"indices\")\n        .HostMemory(\"handle\"),\n    TensorArrayPackOrGatherOp<CPUDevice, int32, false /* LEGACY_PACK */>);\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n// CONCAT *********************************************************************\n\n// Concatenate the elements in a TensorArray.  All elements must be\n// defined and (excepting the first dimension) have the same shape.\ntemplate <typename Device, typename T>\nclass TensorArrayConcatOp : public OpKernel {\n public:\n  typedef typename TTypes<T, 2>::ConstMatrix ConstMatrix;\n  typedef std::vector<std::unique_ptr<ConstMatrix> > ConstMatrixVector;\n\n  explicit TensorArrayConcatOp(OpKernelConstruction* context)\n      : OpKernel(context) {\n    OP_REQUIRES_OK(context, context->GetAttr(\"dtype\", &dtype_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"element_shape_except0\",\n                                             &element_shape_except0_));\n  }\n\n  void Compute(OpKernelContext* ctx) override {\n    OP_REQUIRES_OK(ctx, SetupFlowControlInputs(ctx, false));\n\n    TensorArray* tensor_array = nullptr;\n    OP_REQUIRES_OK(ctx, GetTensorArray(ctx, &tensor_array));\n    core::ScopedUnref unref(tensor_array);\n    OP_REQUIRES(\n        ctx, dtype_ == tensor_array->ElemType(),\n        errors::InvalidArgument(\n            \"TensorArray dtype is \", DataTypeString(tensor_array->ElemType()),\n            \" but Op requested dtype \", DataTypeString(dtype_), \".\"));\n\n    int32_t array_size;\n    OP_REQUIRES_OK(ctx, tensor_array->PackOrConcatSize(&array_size));\n\n    // If there are no elements, return a zero-element Tensor with\n    // shape [0] + element_shape_except0_\n    if (array_size == 0) {\n      OP_REQUIRES(\n          ctx, element_shape_except0_.IsFullyDefined(),\n          errors::Unimplemented(\n              \"TensorArray has size zero, but element_shape_except0 \",\n              element_shape_except0_.DebugString(),\n              \" is not fully defined. \"\n              \"Currently only static shapes are supported when concatenating \"\n              \"zero-size TensorArrays.\"));\n      TensorShape empty_shape;\n      element_shape_except0_.AsTensorShape(&empty_shape);\n      empty_shape.InsertDim(0, 0);\n      Tensor* empty_unused;\n      OP_REQUIRES_OK(ctx, ctx->allocate_output(0, empty_shape, &empty_unused));\n      OP_REQUIRES_OK(ctx, ctx->allocate_output(1, {0}, &empty_unused));\n      return;\n    }\n\n    // Read all the Tensors into a vector to keep track of their memory.\n    std::vector<Tensor> values;\n    std::vector<int32> indices(array_size);\n    std::iota(indices.begin(), indices.end(), 0);\n    Status s = tensor_array->ReadMany<Device, T>(ctx, indices, &values);\n    OP_REQUIRES_OK(ctx, s);\n\n    Tensor* lengths_tensor = nullptr;\n    OP_REQUIRES_OK(ctx,\n                   ctx->allocate_output(\n                       1, TensorShape({static_cast<int64_t>(values.size())}),\n                       &lengths_tensor));\n    auto lengths_tensor_t = lengths_tensor->vec<int64_t>();\n\n    TensorShape output_shape;\n    TensorShape output_shape_except0;\n    for (std::size_t i = 0; i < values.size(); ++i) {\n      TensorShape value_shape_t = values[i].shape();\n\n      OP_REQUIRES(\n          ctx, TensorShapeUtils::IsVectorOrHigher(value_shape_t),\n          errors::InvalidArgument(\n              \"Concat saw a scalar shape at index \", i,\n              \" but requires at least vectors.  Did you mean to call pack?\"));\n\n      lengths_tensor_t(i) = value_shape_t.dim_size(0);\n\n      TensorShape value_shape_t_except0 = value_shape_t;\n      value_shape_t_except0.RemoveDim(0);\n      if (i == 0) {\n        output_shape = value_shape_t;\n        output_shape_except0 = value_shape_t_except0;\n        OP_REQUIRES(\n            ctx, element_shape_except0_.IsCompatibleWith(output_shape_except0),\n            errors::InvalidArgument(\n                \"TensorArray was passed element_shape_except0 \",\n                element_shape_except0_.DebugString(),\n                \" but index 0 has (excepting dimension 0) shape: \",\n                value_shape_t_except0.DebugString(), \" which does not match.\"));\n      } else {\n        OP_REQUIRES(ctx, output_shape_except0 == value_shape_t_except0,\n                    errors::InvalidArgument(\n                        \"TensorArray has inconsistent shapes.  Index 0 has \"\n                        \"(excepting dimension 0) shape: \",\n                        output_shape_except0.DebugString(), \" but index \", i,\n                        \" has (excepting dimension 0) shape: \",\n                        value_shape_t_except0.DebugString()));\n        // Store the previous maximum length as the offset for this tensor.\n        output_shape.set_dim(\n            0, output_shape.dim_size(0) + value_shape_t.dim_size(0));\n      }\n    }\n\n    Tensor* output_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, output_shape, &output_tensor));\n    ConstMatrixVector input_tensors_flat;\n    input_tensors_flat.reserve(values.size());\n    for (size_t i = 0; i < values.size(); ++i) {\n      const Tensor* value_t = &values[i];\n      if (value_t->NumElements() > 0) {\n        input_tensors_flat.push_back(MakeUnique<ConstMatrix>(\n            value_t->shaped<T, 2>({1, value_t->NumElements()})));\n      }\n    }\n\n    if (output_shape.num_elements() > 0) {\n      auto output_flat =\n          output_tensor->shaped<T, 2>({1, output_shape.num_elements()});\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n      if (std::is_same<Device, GPUDevice>::value) {\n        ConcatGPU<T>(ctx, input_tensors_flat, output_tensor, &output_flat);\n        return;\n      }\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n      ConcatCPU<T>(ctx->device(), input_tensors_flat, &output_flat);\n    }\n  }\n\n private:\n  DataType dtype_;\n  PartialTensorShape element_shape_except0_;\n};\n\n#define REGISTER_CONCAT(type)                                    \\\n  REGISTER_KERNEL_BUILDER(Name(\"TensorArrayConcat\")              \\\n                              .Device(DEVICE_CPU)                \\\n                              .TypeConstraint<type>(\"dtype\")     \\\n                              .HostMemory(\"lengths\")             \\\n                              .HostMemory(\"handle\"),             \\\n                          TensorArrayConcatOp<CPUDevice, type>); \\\n  REGISTER_KERNEL_BUILDER(Name(\"TensorArrayConcatV2\")            \\\n                              .Device(DEVICE_CPU)                \\\n                              .TypeConstraint<type>(\"dtype\")     \\\n                              .HostMemory(\"lengths\")             \\\n                              .HostMemory(\"handle\"),             \\\n                          TensorArrayConcatOp<CPUDevice, type>)  \\\n  REGISTER_KERNEL_BUILDER(Name(\"TensorArrayConcatV3\")            \\\n                              .Device(DEVICE_CPU)                \\\n                              .TypeConstraint<type>(\"dtype\")     \\\n                              .HostMemory(\"lengths\")             \\\n                              .HostMemory(\"handle\"),             \\\n                          TensorArrayConcatOp<CPUDevice, type>)\n\nTF_CALL_POD_STRING_TYPES(REGISTER_CONCAT);\nREGISTER_CONCAT(quint8);\nREGISTER_CONCAT(qint8);\nREGISTER_CONCAT(qint32);\n\n#undef REGISTER_CONCAT\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n#define REGISTER_GPU(type)                                       \\\n  REGISTER_KERNEL_BUILDER(Name(\"TensorArrayConcat\")              \\\n                              .Device(DEVICE_GPU)                \\\n                              .TypeConstraint<type>(\"dtype\")     \\\n                              .HostMemory(\"lengths\")             \\\n                              .HostMemory(\"handle\"),             \\\n                          TensorArrayConcatOp<GPUDevice, type>); \\\n  REGISTER_KERNEL_BUILDER(Name(\"TensorArrayConcatV2\")            \\\n                              .Device(DEVICE_GPU)                \\\n                              .TypeConstraint<type>(\"dtype\")     \\\n                              .HostMemory(\"lengths\")             \\\n                              .HostMemory(\"handle\"),             \\\n                          TensorArrayConcatOp<GPUDevice, type>)  \\\n  REGISTER_KERNEL_BUILDER(Name(\"TensorArrayConcatV3\")            \\\n                              .Device(DEVICE_GPU)                \\\n                              .TypeConstraint<type>(\"dtype\")     \\\n                              .HostMemory(\"lengths\")             \\\n                              .HostMemory(\"handle\"),             \\\n                          TensorArrayConcatOp<GPUDevice, type>)\n\nTF_CALL_GPU_NUMBER_TYPES(REGISTER_GPU);\nTF_CALL_COMPLEX_TYPES(REGISTER_GPU);\n#undef REGISTER_GPU\n\n// A special GPU kernel for int32.\n// TODO(b/25387198): Also enable int32 in device memory. This kernel\n// registration requires all int32 inputs and outputs to be in host memory.\nREGISTER_KERNEL_BUILDER(Name(\"TensorArrayConcat\")\n                            .Device(DEVICE_GPU)\n                            .TypeConstraint<int32>(\"dtype\")\n                            .HostMemory(\"lengths\")\n                            .HostMemory(\"handle\"),\n                        TensorArrayConcatOp<CPUDevice, int32>);\nREGISTER_KERNEL_BUILDER(Name(\"TensorArrayConcatV2\")\n                            .Device(DEVICE_GPU)\n                            .TypeConstraint<int32>(\"dtype\")\n                            .HostMemory(\"lengths\")\n                            .HostMemory(\"handle\"),\n                        TensorArrayConcatOp<CPUDevice, int32>);\nREGISTER_KERNEL_BUILDER(Name(\"TensorArrayConcatV3\")\n                            .Device(DEVICE_GPU)\n                            .TypeConstraint<int32>(\"dtype\")\n                            .HostMemory(\"lengths\")\n                            .HostMemory(\"handle\"),\n                        TensorArrayConcatOp<CPUDevice, int32>);\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n// UNPACK and SCATTER *********************************************************\n\ntemplate <typename Device, typename T, bool LEGACY_UNPACK>\nclass TensorArrayUnpackOrScatterOp : public OpKernel {\n public:\n  explicit TensorArrayUnpackOrScatterOp(OpKernelConstruction* context)\n      : OpKernel(context) {}\n\n  void Compute(OpKernelContext* ctx) override {\n    OP_REQUIRES_OK(ctx, SetupFlowControlInputs(ctx, true));\n\n    TensorArray* tensor_array = nullptr;\n    OP_REQUIRES_OK(ctx, GetTensorArray(ctx, &tensor_array));\n    core::ScopedUnref unref(tensor_array);\n    const Tensor* tensor_value;\n    OP_REQUIRES_OK(ctx, ctx->input(\"value\", &tensor_value));\n    TensorShape element_shape(tensor_value->shape());\n\n    OP_REQUIRES(ctx,\n                FastBoundsCheck(element_shape.dim_size(0),\n                                std::numeric_limits<int32>::max()),\n                errors::InvalidArgument(\"tensor dim0 too large to unpack\"));\n\n    OP_REQUIRES(\n        ctx, tensor_value->dtype() == tensor_array->ElemType(),\n        errors::InvalidArgument(\"TensorArray dtype is \",\n                                DataTypeString(tensor_array->ElemType()),\n                                \" but Op is trying to write dtype \",\n                                DataTypeString(tensor_value->dtype()), \".\"));\n    OP_REQUIRES(ctx, element_shape.dims() > 0,\n                errors::InvalidArgument(\"Input value for unpack must be at \"\n                                        \"least a vector but received shape: \",\n                                        element_shape.DebugString()));\n    int32_t array_size;\n    OP_REQUIRES_OK(ctx, tensor_array->Size(&array_size));\n\n    int32_t max_index;\n    int32_t num_values;\n    std::vector<int32> write_indices;\n    if (LEGACY_UNPACK) {\n      num_values = element_shape.dim_size(0);\n      max_index = num_values - 1;\n      write_indices.resize(num_values);\n      std::iota(write_indices.begin(), write_indices.end(), 0);\n    } else {\n      const Tensor* tensor_indices;\n      OP_REQUIRES_OK(ctx, ctx->input(\"indices\", &tensor_indices));\n      OP_REQUIRES(ctx, TensorShapeUtils::IsVector(tensor_indices->shape()),\n                  errors::InvalidArgument(\n                      \"Expected indices to be a vector, but received shape: \",\n                      tensor_indices->shape().DebugString()));\n      OP_REQUIRES(ctx,\n                  tensor_indices->NumElements() == element_shape.dim_size(0),\n                  errors::InvalidArgument(\n                      \"Expected len(indices) == values.shape[0], but saw: \",\n                      tensor_indices->NumElements(), \" vs. \",\n                      element_shape.dim_size(0)));\n      const auto indices_t = tensor_indices->vec<int32>();\n      num_values = tensor_indices->NumElements();\n      max_index = (num_values == 0)\n                      ? -1\n                      : *std::max_element(indices_t.data(),\n                                          indices_t.data() + num_values);\n      write_indices.resize(num_values);\n      // Copy into write_indices.\n      std::copy(indices_t.data(), indices_t.data() + num_values,\n                write_indices.begin());\n    }\n\n    bool dynamic_size = tensor_array->HasDynamicSize();\n\n    // If dynamic size, we may have to resize the TensorArray to fit.\n    if (dynamic_size && array_size < max_index + 1) {\n      array_size = static_cast<int32>(max_index + 1);\n    }\n\n    if (LEGACY_UNPACK) {\n      OP_REQUIRES(\n          ctx, element_shape.dim_size(0) == array_size,\n          errors::InvalidArgument(\n              \"Input value must have first dimension equal to the array size (\",\n              element_shape.dim_size(0), \" vs. \", array_size, \")\"));\n    } else {\n      OP_REQUIRES(\n          ctx, max_index < array_size,\n          errors::InvalidArgument(\"Max scatter index must be < array size (\",\n                                  max_index, \" vs. \", array_size, \")\"));\n    }\n    element_shape.RemoveDim(0);\n\n    auto tensor_value_t = tensor_value->shaped<T, 3>(\n        {1, num_values, element_shape.num_elements()});\n\n    Eigen::DSizes<Eigen::DenseIndex, 3> indices{0, 0, 0};\n    Eigen::DSizes<Eigen::DenseIndex, 3> sizes{\n        1, 1, static_cast<Eigen::DenseIndex>(element_shape.num_elements())};\n\n    std::vector<Tensor> write_values;\n    write_values.reserve(num_values);\n\n    for (int i = 0; i < num_values; ++i) {\n      Tensor tensor_value_i;\n      OP_REQUIRES_OK(ctx, ctx->allocate_temp(tensor_array->ElemType(),\n                                             element_shape, &tensor_value_i));\n      auto tensor_value_i_t =\n          tensor_value_i.shaped<T, 3>({1, 1, element_shape.num_elements()});\n      indices[1] = i;\n\n      if (element_shape.num_elements() > 0) {\n        functor::Split<Device, T, 3>()(ctx->eigen_device<Device>(),\n                                       tensor_value_i_t, tensor_value_t,\n                                       indices, sizes);\n      }\n\n      write_values.push_back(tensor_value_i);\n    }\n\n    // Record the pack size of the TensorArray.\n    if (LEGACY_UNPACK) {\n      OP_REQUIRES_OK(ctx, tensor_array->SetMarkedSize(array_size));\n    }\n\n    Status s = tensor_array->WriteOrAggregateMany<Device, T>(ctx, write_indices,\n                                                             &write_values);\n    OP_REQUIRES_OK(ctx, s);\n  }\n};\n\n#define REGISTER_SCATTER_AND_UNPACK(type)                                      \\\n  REGISTER_KERNEL_BUILDER(                                                     \\\n      Name(\"TensorArrayUnpack\").Device(DEVICE_CPU).TypeConstraint<type>(\"T\"),  \\\n      TensorArrayUnpackOrScatterOp<CPUDevice, type,                            \\\n                                   true /* LEGACY_UNPACK */>);                 \\\n  REGISTER_KERNEL_BUILDER(                                                     \\\n      Name(\"TensorArrayScatter\").Device(DEVICE_CPU).TypeConstraint<type>(\"T\"), \\\n      TensorArrayUnpackOrScatterOp<CPUDevice, type,                            \\\n                                   false /* LEGACY_UNPACK */>);                \\\n  REGISTER_KERNEL_BUILDER(                                                     \\\n      Name(\"TensorArrayScatterV2\")                                             \\\n          .Device(DEVICE_CPU)                                                  \\\n          .TypeConstraint<type>(\"T\"),                                          \\\n      TensorArrayUnpackOrScatterOp<CPUDevice, type,                            \\\n                                   false /* LEGACY_UNPACK */>);                \\\n  REGISTER_KERNEL_BUILDER(                                                     \\\n      Name(\"TensorArrayScatterV3\")                                             \\\n          .Device(DEVICE_CPU)                                                  \\\n          .TypeConstraint<type>(\"T\"),                                          \\\n      TensorArrayUnpackOrScatterOp<CPUDevice, type,                            \\\n                                   false /* LEGACY_UNPACK */>);\n\nTF_CALL_ALL_TYPES(REGISTER_SCATTER_AND_UNPACK);\n#undef REGISTER_SCATTER_AND_UNPACK\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n#define REGISTER_GPU(type)                                      \\\n  REGISTER_KERNEL_BUILDER(                                      \\\n      Name(\"TensorArrayUnpack\")                                 \\\n          .Device(DEVICE_GPU)                                   \\\n          .TypeConstraint<type>(\"T\")                            \\\n          .HostMemory(\"handle\"),                                \\\n      TensorArrayUnpackOrScatterOp<GPUDevice, type,             \\\n                                   true /* LEGACY_UNPACK */>);  \\\n  REGISTER_KERNEL_BUILDER(                                      \\\n      Name(\"TensorArrayScatter\")                                \\\n          .Device(DEVICE_GPU)                                   \\\n          .TypeConstraint<type>(\"T\")                            \\\n          .HostMemory(\"indices\")                                \\\n          .HostMemory(\"handle\"),                                \\\n      TensorArrayUnpackOrScatterOp<GPUDevice, type,             \\\n                                   false /* LEGACY_UNPACK */>); \\\n  REGISTER_KERNEL_BUILDER(                                      \\\n      Name(\"TensorArrayScatterV2\")                              \\\n          .Device(DEVICE_GPU)                                   \\\n          .TypeConstraint<type>(\"T\")                            \\\n          .HostMemory(\"indices\")                                \\\n          .HostMemory(\"handle\"),                                \\\n      TensorArrayUnpackOrScatterOp<GPUDevice, type,             \\\n                                   false /* LEGACY_UNPACK */>); \\\n  REGISTER_KERNEL_BUILDER(                                      \\\n      Name(\"TensorArrayScatterV3\")                              \\\n          .Device(DEVICE_GPU)                                   \\\n          .TypeConstraint<type>(\"T\")                            \\\n          .HostMemory(\"indices\")                                \\\n          .HostMemory(\"handle\"),                                \\\n      TensorArrayUnpackOrScatterOp<GPUDevice, type,             \\\n                                   false /* LEGACY_UNPACK */>);\n\nTF_CALL_int64(REGISTER_GPU);\nTF_CALL_GPU_NUMBER_TYPES(REGISTER_GPU);\nTF_CALL_COMPLEX_TYPES(REGISTER_GPU);\n#undef REGISTER_GPU\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n// SPLIT *********************************************************************\n\ntemplate <typename Device, typename T>\nclass TensorArraySplitOp : public OpKernel {\n public:\n  explicit TensorArraySplitOp(OpKernelConstruction* context)\n      : OpKernel(context) {}\n\n  void Compute(OpKernelContext* ctx) override {\n    OP_REQUIRES_OK(ctx, SetupFlowControlInputs(ctx, true));\n\n    TensorArray* tensor_array = nullptr;\n    OP_REQUIRES_OK(ctx, GetTensorArray(ctx, &tensor_array));\n    core::ScopedUnref unref(tensor_array);\n    const Tensor* tensor_value;\n    OP_REQUIRES_OK(ctx, ctx->input(\"value\", &tensor_value));\n    const Tensor* tensor_lengths;\n    OP_REQUIRES_OK(ctx, ctx->input(\"lengths\", &tensor_lengths));\n\n    OP_REQUIRES(ctx, TensorShapeUtils::IsVector(tensor_lengths->shape()),\n                errors::InvalidArgument(\n                    \"Expected lengths to be a vector, received shape: \",\n                    tensor_lengths->shape().DebugString()));\n    OP_REQUIRES(ctx,\n                FastBoundsCheck(tensor_lengths->NumElements(),\n                                std::numeric_limits<int32>::max()),\n                errors::InvalidArgument(\n                    \"Expected lengths to have < max int32 entries\"));\n\n    int32_t num_tensors = static_cast<int32>(tensor_lengths->NumElements());\n    auto tensor_lengths_t = tensor_lengths->vec<int64_t>();\n    std::vector<int64_t> cumulative_lengths;\n    cumulative_lengths.reserve(num_tensors);\n    int64_t total_length = 0;\n    for (int i = 0; i < num_tensors; ++i) {\n      total_length += tensor_lengths_t(i);\n      cumulative_lengths.push_back(total_length);\n    }\n\n    OP_REQUIRES(\n        ctx, TensorShapeUtils::IsVectorOrHigher(tensor_value->shape()),\n        errors::InvalidArgument(\n            \"Expected value to be at least a vector, but received shape: \",\n            tensor_value->shape().DebugString()));\n\n    OP_REQUIRES(\n        ctx, total_length == tensor_value->shape().dim_size(0),\n        errors::InvalidArgument(\"Expected sum of lengths to be equal to \"\n                                \"values.shape[0], but sum of lengths is \",\n                                total_length, \" and value's shape is: \",\n                                tensor_value->shape().DebugString()));\n    int64_t elements_per_row =\n        (total_length == 0) ? 0 : (tensor_value->NumElements() / total_length);\n\n    int32_t array_size;\n    OP_REQUIRES_OK(ctx, tensor_array->Size(&array_size));\n    bool dynamic_size = tensor_array->HasDynamicSize();\n\n    std::vector<TensorShape> element_shapes(num_tensors, tensor_value->shape());\n    for (int32_t i = 0; i < num_tensors; ++i) {\n      element_shapes[i].set_dim(0, tensor_lengths_t(i));\n    }\n\n    // If dynamic size, we may have to resize the TensorArray to fit.\n    if (dynamic_size && array_size < num_tensors) {\n      array_size = num_tensors;\n    }\n\n    OP_REQUIRES(\n        ctx, array_size == num_tensors,\n        errors::InvalidArgument(\n            \"TensorArray's size is not equal to the size of lengths (\",\n            array_size, \" vs. \", num_tensors, \"), and the TensorArray is not \",\n            \"marked as dynamically resizeable\"));\n\n    OP_REQUIRES(\n        ctx, tensor_value->dtype() == tensor_array->ElemType(),\n        errors::InvalidArgument(\"TensorArray dtype is \",\n                                DataTypeString(tensor_array->ElemType()),\n                                \" but Op is trying to write dtype \",\n                                DataTypeString(tensor_value->dtype()), \".\"));\n\n    auto tensor_value_t =\n        tensor_value->shaped<T, 3>({1, total_length, elements_per_row});\n\n    std::vector<Tensor> write_values;\n    write_values.reserve(array_size);\n\n    for (int i = 0; i < array_size; ++i) {\n      Tensor tensor_value_i;\n\n      int64_t previous_length = (i == 0) ? 0 : cumulative_lengths[i - 1];\n      Eigen::DSizes<Eigen::DenseIndex, 3> indices{\n          0, static_cast<Eigen::DenseIndex>(previous_length), 0};\n      Eigen::DSizes<Eigen::DenseIndex, 3> sizes{\n          1, static_cast<Eigen::DenseIndex>(tensor_lengths_t(i)),\n          static_cast<Eigen::DenseIndex>(elements_per_row)};\n\n      OP_REQUIRES_OK(\n          ctx, ctx->allocate_temp(tensor_array->ElemType(), element_shapes[i],\n                                  &tensor_value_i));\n\n      if (tensor_lengths_t(i) > 0) {\n        auto tensor_value_i_t = tensor_value_i.shaped<T, 3>(\n            {1, tensor_lengths_t(i), elements_per_row});\n\n        functor::Split<Device, T, 3>()(ctx->eigen_device<Device>(),\n                                       tensor_value_i_t, tensor_value_t,\n                                       indices, sizes);\n      }\n\n      write_values.push_back(tensor_value_i);\n    }\n\n    // Record the concat size of the TensorArray.\n    OP_REQUIRES_OK(ctx, tensor_array->SetMarkedSize(array_size));\n\n    std::vector<int32> indices(array_size);\n    std::iota(indices.begin(), indices.end(), 0);\n\n    Status s = tensor_array->WriteOrAggregateMany<Device, T>(ctx, indices,\n                                                             &write_values);\n    OP_REQUIRES_OK(ctx, s);\n  }\n};\n\n#define REGISTER_SPLIT(type)                                                   \\\n  REGISTER_KERNEL_BUILDER(                                                     \\\n      Name(\"TensorArraySplit\").Device(DEVICE_CPU).TypeConstraint<type>(\"T\"),   \\\n      TensorArraySplitOp<CPUDevice, type>);                                    \\\n  REGISTER_KERNEL_BUILDER(                                                     \\\n      Name(\"TensorArraySplitV2\").Device(DEVICE_CPU).TypeConstraint<type>(\"T\"), \\\n      TensorArraySplitOp<CPUDevice, type>);                                    \\\n  REGISTER_KERNEL_BUILDER(                                                     \\\n      Name(\"TensorArraySplitV3\").Device(DEVICE_CPU).TypeConstraint<type>(\"T\"), \\\n      TensorArraySplitOp<CPUDevice, type>);\n\nTF_CALL_ALL_TYPES(REGISTER_SPLIT);\n#undef REGISTER_SPLIT\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n#define REGISTER_GPU(type)                                      \\\n  REGISTER_KERNEL_BUILDER(Name(\"TensorArraySplit\")              \\\n                              .Device(DEVICE_GPU)               \\\n                              .TypeConstraint<type>(\"T\")        \\\n                              .HostMemory(\"lengths\")            \\\n                              .HostMemory(\"handle\"),            \\\n                          TensorArraySplitOp<GPUDevice, type>); \\\n  REGISTER_KERNEL_BUILDER(Name(\"TensorArraySplitV2\")            \\\n                              .Device(DEVICE_GPU)               \\\n                              .TypeConstraint<type>(\"T\")        \\\n                              .HostMemory(\"lengths\")            \\\n                              .HostMemory(\"handle\"),            \\\n                          TensorArraySplitOp<GPUDevice, type>); \\\n  REGISTER_KERNEL_BUILDER(Name(\"TensorArraySplitV3\")            \\\n                              .Device(DEVICE_GPU)               \\\n                              .TypeConstraint<type>(\"T\")        \\\n                              .HostMemory(\"lengths\")            \\\n                              .HostMemory(\"handle\"),            \\\n                          TensorArraySplitOp<GPUDevice, type>);\n\nTF_CALL_GPU_NUMBER_TYPES(REGISTER_GPU);\nTF_CALL_COMPLEX_TYPES(REGISTER_GPU);\n#undef REGISTER_GPU\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n// SIZE ***********************************************************************\n\n// Get the size of the TensorArray\nclass TensorArraySizeOp : public OpKernel {\n public:\n  explicit TensorArraySizeOp(OpKernelConstruction* context)\n      : OpKernel(context) {}\n\n  void Compute(OpKernelContext* ctx) override {\n    TensorArray* tensor_array;\n    OP_REQUIRES_OK(ctx, GetTensorArray(ctx, &tensor_array));\n    core::ScopedUnref unref(tensor_array);\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, TensorShape({}), &output));\n    OP_REQUIRES_OK(ctx, tensor_array->Size(&(output->scalar<int32>()())));\n  }\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"TensorArraySize\").Device(DEVICE_CPU),\n                        TensorArraySizeOp);\nREGISTER_KERNEL_BUILDER(Name(\"TensorArraySizeV2\").Device(DEVICE_CPU),\n                        TensorArraySizeOp);\nREGISTER_KERNEL_BUILDER(Name(\"TensorArraySizeV3\").Device(DEVICE_CPU),\n                        TensorArraySizeOp);\n\nREGISTER_KERNEL_BUILDER(Name(\"TensorArraySize\")\n                            .Device(DEVICE_GPU)\n                            .HostMemory(\"handle\")\n                            .HostMemory(\"size\"),\n                        TensorArraySizeOp);\nREGISTER_KERNEL_BUILDER(Name(\"TensorArraySizeV2\")\n                            .Device(DEVICE_GPU)\n                            .HostMemory(\"handle\")\n                            .HostMemory(\"size\"),\n                        TensorArraySizeOp);\nREGISTER_KERNEL_BUILDER(Name(\"TensorArraySizeV3\")\n                            .Device(DEVICE_GPU)\n                            .HostMemory(\"handle\")\n                            .HostMemory(\"size\"),\n                        TensorArraySizeOp);\n\n// CLOSE\n// **********************************************************************\n\n// Delete the TensorArray from its resource container.  This enables\n// the user to close and release the resource in the middle of a step/run.\n// TODO(ebrevdo): decide whether closing the grad op should happen\n// here or on the python side.\nclass TensorArrayCloseOp : public OpKernel {\n public:\n  explicit TensorArrayCloseOp(OpKernelConstruction* context)\n      : OpKernel(context) {}\n\n  void Compute(OpKernelContext* ctx) override {\n    TensorArray* tensor_array;\n    OP_REQUIRES_OK(ctx, GetTensorArray(ctx, &tensor_array));\n    core::ScopedUnref unref(tensor_array);\n    // Instead of deleting this TA from the ResourceManager, we just\n    // clear it away and mark it as closed.  The remaining memory\n    // consumed store its mutex and handle Tensor.  This will be\n    // cleared out at the end of the step anyway, so it's fine to keep\n    // it around until the end of the step.  Further calls to the\n    // TensorArray will fail because TensorArray checks internally to\n    // see if it is closed or not.\n    tensor_array->ClearAndMarkClosed();\n  }\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"TensorArrayClose\").Device(DEVICE_CPU),\n                        TensorArrayCloseOp);\nREGISTER_KERNEL_BUILDER(Name(\"TensorArrayCloseV2\").Device(DEVICE_CPU),\n                        TensorArrayCloseOp);\nREGISTER_KERNEL_BUILDER(Name(\"TensorArrayCloseV3\").Device(DEVICE_CPU),\n                        TensorArrayCloseOp);\n\nREGISTER_KERNEL_BUILDER(\n    Name(\"TensorArrayClose\").Device(DEVICE_GPU).HostMemory(\"handle\"),\n    TensorArrayCloseOp);\nREGISTER_KERNEL_BUILDER(\n    Name(\"TensorArrayCloseV2\").Device(DEVICE_GPU).HostMemory(\"handle\"),\n    TensorArrayCloseOp);\nREGISTER_KERNEL_BUILDER(\n    Name(\"TensorArrayCloseV3\").Device(DEVICE_GPU).HostMemory(\"handle\"),\n    TensorArrayCloseOp);\n\n}  // namespace tensorflow\n", "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for tensorflow.ops.tensor_array_ops.\"\"\"\n\nimport numpy as np\n\nfrom tensorflow.core.protobuf import config_pb2\nfrom tensorflow.python.client import session as session_lib\nfrom tensorflow.python.data.ops import dataset_ops\nfrom tensorflow.python.eager import backprop\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.eager import def_function\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.framework import tensor_spec\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import control_flow_util\nfrom tensorflow.python.ops import data_flow_ops\nfrom tensorflow.python.ops import gen_data_flow_ops\nfrom tensorflow.python.ops import gradients_impl\nfrom tensorflow.python.ops import init_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import tensor_array_grad\nfrom tensorflow.python.ops import tensor_array_ops\nfrom tensorflow.python.ops import variable_scope\nfrom tensorflow.python.ops import variables\nimport tensorflow.python.ops.nn_grad  # pylint: disable=unused-import\nfrom tensorflow.python.platform import test\n\n\ndef _make_converter(tf_dtype):\n  def _converter(x):\n    if tf_dtype == dtypes.string:\n      # In Python3, np.str_ is unicode, while we always want bytes\n      return np.asarray(x).astype(\"|S\")\n    x = np.asarray(x).astype(tf_dtype.as_numpy_dtype)\n    if tf_dtype.is_complex:\n      # Add a non-zero imaginary component to x.\n      x -= 1j * x\n    return x\n  return _converter\n\n\ndef _make_ta(size, name, dtype=dtypes.float32, infer_shape=False):\n  return tensor_array_ops.TensorArray(\n      dtype=dtype, tensor_array_name=name, size=size, infer_shape=infer_shape)\n\n\n@test_util.run_all_in_graph_and_eager_modes\n@test_util.with_control_flow_v2\nclass TensorArrayTest(test.TestCase):\n\n  @classmethod\n  def setUpClass(cls):\n    super(TensorArrayTest, cls).setUpClass()\n    cls._workers, _ = test.create_local_cluster(num_workers=3, num_ps=0)\n\n  @classmethod\n  def tearDownClass(cls):\n    super(TensorArrayTest, cls).tearDownClass()\n    session_lib.Session.reset(cls._workers[0].target)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testTensorArrayWriteRead(self):\n    with self.session():\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32,\n          tensor_array_name=\"foo\",\n          size=3,\n          infer_shape=False)\n\n      w0 = ta.write(0, [[4.0, 5.0]])\n      w1 = w0.write(1, [[1.0]])\n      w2 = w1.write(2, -3.0)\n\n      r0 = w2.read(0)\n      r1 = w2.read(1)\n      r2 = w2.read(2)\n\n      d0, d1, d2 = self.evaluate([r0, r1, r2])\n      self.assertAllEqual([[4.0, 5.0]], d0)\n      self.assertAllEqual([[1.0]], d1)\n      self.assertAllEqual(-3.0, d2)\n\n  def _testTensorArrayWritePack(self, tf_dtype):\n    with self.cached_session():\n      ta = tensor_array_ops.TensorArray(\n          dtype=tf_dtype, tensor_array_name=\"foo\", size=3)\n\n      convert = _make_converter(tf_dtype)\n\n      w0 = ta.write(0, convert([[4.0, 5.0]]))\n      w1 = w0.write(1, convert([[6.0, 7.0]]))\n      w2 = w1.write(2, convert([[8.0, 9.0]]))\n\n      c0 = w2.stack()\n\n      c0 = self.evaluate(c0)\n      self.assertAllEqual(\n          convert([[[4.0, 5.0]], [[6.0, 7.0]], [[8.0, 9.0]]]), c0)\n\n  def _testTensorArrayWritePackMaybeLegacy(self):\n    self._testTensorArrayWritePack(dtypes.float32)\n    self._testTensorArrayWritePack(dtypes.float64)\n    self._testTensorArrayWritePack(dtypes.int32)\n    self._testTensorArrayWritePack(dtypes.int64)\n    self._testTensorArrayWritePack(dtypes.complex64)\n    self._testTensorArrayWritePack(dtypes.complex128)\n    self._testTensorArrayWritePack(dtypes.string)\n\n  def testTensorArrayWritePack(self):\n    self._testTensorArrayWritePackMaybeLegacy()\n\n  def testEmptyTensorArrayPack(self):\n    with self.session():\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32, tensor_array_name=\"foo\", size=3)\n\n      empty_element = np.zeros((0, 1), dtype=np.float32)\n      w0 = ta.write(0, empty_element)\n      w1 = w0.write(1, empty_element)\n      w2 = w1.write(2, empty_element)\n\n      c0 = w2.stack()\n\n      c0 = self.evaluate(c0)\n      self.assertAllEqual([3, 0, 1], c0.shape)\n\n  def testTensorArrayWriteConcatInParallel(self):\n    with self.session():\n\n      def _concat_1():\n        ta = tensor_array_ops.TensorArray(\n            dtype=dtypes.int32, size=2, infer_shape=False)\n        w0 = ta.write(0, constant_op.constant([1]))\n        w1 = w0.write(1, constant_op.constant([],\n                                              shape=(0,),\n                                              dtype=dtypes.int32))\n        return w1.concat()\n\n      def _concat_2():\n        ta = tensor_array_ops.TensorArray(\n            dtype=dtypes.int32, size=3, infer_shape=False)\n        w0 = ta.write(0, constant_op.constant([8]))\n        w1 = w0.write(1, constant_op.constant([],\n                                              shape=(0,),\n                                              dtype=dtypes.int32))\n        w2 = w1.write(2, constant_op.constant([9]))\n        return w2.concat()\n\n      def _write(index, output):\n        elements = control_flow_ops.cond(\n            math_ops.less(index, 3), _concat_1, _concat_2)\n        return (index + 1, output.write(index, elements))\n\n      num_iterations = 6\n      init_state = (0,\n                    tensor_array_ops.TensorArray(\n                        dtype=dtypes.int32,\n                        size=num_iterations,\n                        infer_shape=False))\n      _, final_state = control_flow_ops.while_loop(\n          lambda i, _: i < num_iterations, _write, init_state)\n\n      c0 = final_state.concat()\n\n      c0 = self.evaluate(c0)\n      self.assertAllEqual([1, 1, 1, 8, 9, 8, 9, 8, 9], c0)\n\n  def _testTensorArrayWriteConcat(self, tf_dtype):\n    with self.cached_session():\n      ta = tensor_array_ops.TensorArray(\n          dtype=tf_dtype, tensor_array_name=\"foo\", size=3, infer_shape=False)\n\n      convert = _make_converter(tf_dtype)\n\n      w0 = ta.write(0, convert([[4.0, 5.0], [104.0, 105.0], [204.0, 205.0]]))\n      w1 = w0.write(1, convert([[6.0, 7.0], [106.0, 107.0]]))\n      w2 = w1.write(2, convert([[8.0, 9.0]]))\n\n      c0 = w2.concat()\n\n      c0 = self.evaluate(c0)\n      self.assertAllEqual(\n          convert([[4.0, 5.0], [104.0, 105.0], [204.0, 205.0], [6.0, 7.0],\n                   [106.0, 107.0], [8.0, 9.0]]), c0)\n\n  @test_util.deprecated_graph_mode_only\n  def testTensorArrayWriteConcat(self):\n    self._testTensorArrayWriteConcat(dtypes.float32)\n    self._testTensorArrayWriteConcat(dtypes.float64)\n    self._testTensorArrayWriteConcat(dtypes.int32)\n    self._testTensorArrayWriteConcat(dtypes.int64)\n    self._testTensorArrayWriteConcat(dtypes.complex64)\n    self._testTensorArrayWriteConcat(dtypes.complex128)\n    self._testTensorArrayWriteConcat(dtypes.string)\n\n  def _testTensorArrayReadOrPackNotAllValuesAvailableFillsZeros(self):\n    with self.cached_session():\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32,\n          tensor_array_name=\"foo\",\n          size=3,\n          element_shape=tensor_shape.TensorShape([1, 2]))\n      self.assertAllEqual([[0.0, 0.0]], self.evaluate(ta.read(0)))\n      self.assertAllEqual([[[0.0, 0.0]], [[4.0, 5.0]], [[0.0, 0.0]]],\n                          self.evaluate(ta.write(1, [[4.0, 5.0]]).stack()))\n      self.assertAllEqual([[0.0, 0.0], [4.0, 5.0], [0.0, 0.0]],\n                          self.evaluate(ta.write(1, [[4.0, 5.0]]).concat()))\n\n  @test_util.run_v1_only(\"b/122324791\")\n  def testTensorArrayReadOrPackNotAllValuesAvailableFillsZeros(self):\n    self._testTensorArrayReadOrPackNotAllValuesAvailableFillsZeros()\n\n  def _testTensorArrayReadOrPackNotAllValuesAvailableInferShapeFillsZeros(self):\n    ta = tensor_array_ops.TensorArray(\n        dtype=dtypes.float32,\n        tensor_array_name=\"foo\",\n        size=3)\n    self.assertAllEqual(\n        [[0.0, 0.0]], self.evaluate(ta.write(1, [[4.0, 5.0]]).read(0)))\n    self.assertAllEqual([[[0.0, 0.0]], [[4.0, 5.0]], [[0.0, 0.0]]],\n                        self.evaluate(ta.write(1, [[4.0, 5.0]]).stack()))\n    self.assertAllEqual([[0.0, 0.0], [4.0, 5.0], [0.0, 0.0]],\n                        self.evaluate(ta.write(1, [[4.0, 5.0]]).concat()))\n\n  @test_util.run_v1_only(\"b/122324791\")\n  def testTensorArrayReadOrPackNotAllValuesAvailableInferShapeFillsZeros(self):\n    self._testTensorArrayReadOrPackNotAllValuesAvailableInferShapeFillsZeros()\n\n  @test_util.run_v1_only(\"Uses placeholders\")\n  def testSkipEagerTensorArrayReadUninitializedInferShapeFillsZeros(self):\n    with self.cached_session() as sess:\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32,\n          tensor_array_name=\"foo\",\n          size=3)\n      val = array_ops.placeholder(dtypes.float32)\n      self.assertAllEqual(\n          [[0.0, 0.0]], sess.run(ta.write(1, val).read(0), {val: [[4.0, 5.0]]}))\n\n  def _testTensorArrayUnpackRead(self, tf_dtype):\n    with self.cached_session():\n      convert = _make_converter(tf_dtype)\n\n      ta = _make_ta(3, \"foo\", dtype=tf_dtype)\n      # Unpack a vector into scalars\n      w0 = ta.unstack(convert([1.0, 2.0, 3.0]))\n      r0 = w0.read(0)\n      r1 = w0.read(1)\n      r2 = w0.read(2)\n\n      d0, d1, d2 = self.evaluate([r0, r1, r2])\n      self.assertAllEqual(convert(1.0), d0)\n      self.assertAllEqual(convert(2.0), d1)\n      self.assertAllEqual(convert(3.0), d2)\n\n      # Unpack a matrix into vectors\n      w1 = ta.unstack(convert([[1.0, 1.1], [2.0, 2.1], [3.0, 3.1]]))\n      r0 = w1.read(0)\n      r1 = w1.read(1)\n      r2 = w1.read(2)\n\n      d0, d1, d2 = self.evaluate([r0, r1, r2])\n      self.assertAllEqual(convert([1.0, 1.1]), d0)\n      self.assertAllEqual(convert([2.0, 2.1]), d1)\n      self.assertAllEqual(convert([3.0, 3.1]), d2)\n\n      # Try unpacking an empty matrix, which should not cause an error.\n      w2 = ta.unstack(convert([[], [], []]))\n      r0 = w2.read(0)\n      r1 = w2.read(1)\n      r2 = w2.read(2)\n\n      d0, d1, d2 = self.evaluate([r0, r1, r2])\n      self.assertAllEqual(convert([]), d0)\n      self.assertAllEqual(convert([]), d1)\n      self.assertAllEqual(convert([]), d2)\n\n  def _testTensorArrayUnpackReadMaybeLegacy(self):\n    self._testTensorArrayUnpackRead(dtypes.float32)\n    self._testTensorArrayUnpackRead(dtypes.float64)\n    self._testTensorArrayUnpackRead(dtypes.int32)\n    self._testTensorArrayUnpackRead(dtypes.int64)\n    self._testTensorArrayUnpackRead(dtypes.complex64)\n    self._testTensorArrayUnpackRead(dtypes.complex128)\n    self._testTensorArrayUnpackRead(dtypes.string)\n    self._testTensorArrayUnpackRead(dtypes.bfloat16)\n\n  def testTensorArrayUnpackRead(self):\n    self._testTensorArrayUnpackReadMaybeLegacy()\n\n  def _testTensorArraySplitRead(self, tf_dtype):\n    with self.cached_session():\n      convert = _make_converter(tf_dtype)\n\n      # Split an empty vector\n      ta = _make_ta(3, \"foo\", dtype=tf_dtype)\n      lengths = constant_op.constant([0, 0, 0])\n      w0 = ta.split(convert([]), lengths=lengths)\n      r0 = w0.read(0)\n      r1 = w0.read(1)\n      r2 = w0.read(2)\n\n      d0, d1, d2 = self.evaluate([r0, r1, r2])\n      self.assertAllEqual(convert([]), d0)\n      self.assertAllEqual(convert([]), d1)\n      self.assertAllEqual(convert([]), d2)\n\n      # Split a vector\n      lengths = constant_op.constant([2, 0, 1])\n      w0 = ta.split(convert([1.0, 2.0, 3.0]), lengths=lengths)\n      r0 = w0.read(0)\n      r1 = w0.read(1)\n      r2 = w0.read(2)\n\n      d0, d1, d2 = self.evaluate([r0, r1, r2])\n      self.assertAllEqual(convert([1.0, 2.0]), d0)\n      self.assertAllEqual(convert([]), d1)\n      self.assertAllEqual(convert([3.0]), d2)\n\n      # Split a matrix\n      lengths = constant_op.constant([2, 0, 1])\n      w0 = ta.split(\n          convert([[1.0, 101.0], [2.0, 201.0], [3.0, 301.0]]), lengths=lengths)\n      r0 = w0.read(0)\n      r1 = w0.read(1)\n      r2 = w0.read(2)\n\n      d0, d1, d2 = self.evaluate([r0, r1, r2])\n      self.assertAllEqual(convert([[1.0, 101.0], [2.0, 201.0]]), d0)\n      self.assertAllEqual(convert([]).reshape(0, 2), d1)\n      self.assertAllEqual(convert([[3.0, 301.0]]), d2)\n\n  @test_util.deprecated_graph_mode_only\n  def testTensorArraySplitRead(self):\n    self._testTensorArraySplitRead(dtypes.float32)\n    self._testTensorArraySplitRead(dtypes.float64)\n    self._testTensorArraySplitRead(dtypes.int32)\n    self._testTensorArraySplitRead(dtypes.int64)\n    self._testTensorArraySplitRead(dtypes.complex64)\n    self._testTensorArraySplitRead(dtypes.complex128)\n    self._testTensorArraySplitRead(dtypes.string)\n    self._testTensorArraySplitRead(dtypes.bfloat16)\n\n  @test_util.disable_control_flow_v2(\"v2 does not support TensorArray.grad.\")\n  @test_util.run_v1_only(\"v2 does not support TensorArray.grad.\")\n  def testSkipEagerTensorGradArrayWriteRead(self):\n    with self.session() as session:\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32,\n          tensor_array_name=\"foo\",\n          size=3,\n          infer_shape=False)\n      g_ta = ta.grad(\"grad\")\n\n      w0 = ta.write(0, [[4.0, 5.0]])\n      w1 = w0.write(1, [[1.0]])\n      w2 = w1.write(2, -3.0)\n\n      g_w0 = g_ta.write(0, [[5.0, 6.0]])\n      g_w1 = g_w0.write(1, [[2.0]])\n      g_w2 = g_w1.write(2, -2.0)\n\n      r0 = w2.read(0)\n      r1 = w2.read(1)\n      r2 = w2.read(2)\n\n      g_r0 = g_w2.read(0)\n      g_r1 = g_w2.read(1)\n      g_r2 = g_w2.read(2)\n\n      d0, d1, d2, g_d0, g_d1, g_d2 = session.run([r0, r1, r2, g_r0, g_r1, g_r2])\n      self.assertAllEqual([[4.0, 5.0]], d0)\n      self.assertAllEqual([[1.0]], d1)\n      self.assertAllEqual(-3.0, d2)\n      self.assertAllEqual([[5.0, 6.0]], g_d0)\n      self.assertAllEqual([[2.0]], g_d1)\n      self.assertAllEqual(-2.0, g_d2)\n\n  @test_util.deprecated_graph_mode_only\n  def testSkipEagerTensorArrayGradGrad(self):\n    if not control_flow_util.ENABLE_CONTROL_FLOW_V2:\n      self.skipTest(\"Legacy TensorArray does not support double derivatives.\")\n    with self.test_session() as session:\n      x = constant_op.constant(4.0)\n\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32,\n          tensor_array_name=\"foo\",\n          size=1,\n          infer_shape=False)\n      w0 = ta.write(0, x)\n      r0 = w0.read(0)\n      y = r0 * r0\n\n      g1 = gradients_impl.gradients(ys=[y], xs=[x])\n      g2 = gradients_impl.gradients(ys=[g1], xs=[x])\n      self.assertAllEqual([2.0], session.run(g2))\n\n  @test_util.disable_control_flow_v2(\"v2 does not support TensorArray.grad.\")\n  @test_util.run_v1_only(\"v2 does not support TensorArray.grad.\")\n  def testSkipEagerTensorGradArrayDynamicWriteRead(self):\n    with self.session() as session:\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32,\n          tensor_array_name=\"foo\",\n          size=0,\n          dynamic_size=True,\n          infer_shape=False)\n\n      w0 = ta.write(0, [[4.0, 5.0]])\n      w1 = w0.write(1, [[1.0]])\n      w2 = w1.write(2, -3.0)\n\n      g_ta = w2.grad(\"grad\")  # Get gradient array here so we know the shape\n\n      s = w2.size()\n      g_s = g_ta.size()\n\n      g_w0 = g_ta.write(0, [[5.0, 6.0]])\n      g_w1 = g_w0.write(1, [[2.0]])\n      g_w2 = g_w1.write(2, -2.0)\n\n      r0 = w2.read(0)\n      r1 = w2.read(1)\n      r2 = w2.read(2)\n\n      g_r0 = g_w2.read(0)\n      g_r1 = g_w2.read(1)\n      g_r2 = g_w2.read(2)\n\n      d0, d1, d2, g_d0, g_d1, g_d2, vs, g_vs = session.run(\n          [r0, r1, r2, g_r0, g_r1, g_r2, s, g_s])\n      self.assertAllEqual([[4.0, 5.0]], d0)\n      self.assertAllEqual([[1.0]], d1)\n      self.assertAllEqual(-3.0, d2)\n      self.assertAllEqual([[5.0, 6.0]], g_d0)\n      self.assertAllEqual([[2.0]], g_d1)\n      self.assertAllEqual(-2.0, g_d2)\n      self.assertAllEqual(3, vs)\n      self.assertAllEqual(3, g_vs)\n\n  @test_util.disable_control_flow_v2(\"v2 does not support TensorArray.grad.\")\n  @test_util.run_v1_only(\"v2 does not support TensorArray.grad.\")\n  def testSkipEagerTensorGradAccessTwiceReceiveSameObject(self):\n    with self.session() as session:\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32, tensor_array_name=\"foo\", size=3)\n      g_ta_0 = ta.grad(\"grad\")\n      g_ta_1 = ta.grad(\"grad\")\n\n      with ops.control_dependencies([g_ta_0.write(0, [[4.0, 5.0]]).flow]):\n        # Write with one gradient handle, read with another copy of it\n        r1_0 = g_ta_1.read(0)\n\n      t_g_ta_0, t_g_ta_1, d_r1_0 = session.run(\n          [g_ta_0.handle.op, g_ta_1.handle.op, r1_0])\n      self.assertAllEqual(t_g_ta_0, t_g_ta_1)\n      self.assertAllEqual([[4.0, 5.0]], d_r1_0)\n\n  def testTensorArrayWriteWrongIndexOrDataTypeFails(self):\n    with self.session():\n      ta = _make_ta(3, \"foo\", dtype=dtypes.float32)\n      # TODO(b/129870929): Remove the last 2 checks (runtime checks) after\n      # back back from preferred_dtype= to dtype= in convert_to_tensor.  Also\n      # restrict error check to only TypeError.\n      error_msg_regex = (\n          \"(\"\n          \"Expected float32, got 'wrong_type_scalar' of type 'str' instead.\"\n          \"|\"\n          \"Cannot convert provided value to EagerTensor. Provided value: \"\n          \"wrong_type_scalar Requested dtype: float\"\n          \"|\"\n          \"TensorArray dtype is float.* but Op is trying to write dtype string\"\n          \"|\"\n          \"Invalid data types; op elements string but list elements float\"\n          \")\")\n      with self.assertRaisesRegex((TypeError, errors.InvalidArgumentError),\n                                  error_msg_regex):\n        self.evaluate(ta.write(0, \"wrong_type_scalar\").flow)\n\n      if (control_flow_util.ENABLE_CONTROL_FLOW_V2 and\n          not context.executing_eagerly()):\n        error_msg = \"Trying to modify element -1 in a list with 3 elements.\"\n      else:\n        error_msg = \"index -1\"\n      with self.assertRaisesOpError(error_msg):\n        self.evaluate(ta.write(-1, 3.0).flow)\n\n      if (control_flow_util.ENABLE_CONTROL_FLOW_V2 and\n          not context.executing_eagerly()):\n        error_msg = \"Trying to modify element 3 in a list with 3 elements\"\n      else:\n        error_msg = (\"Tried to write to index 3 but array is not \"\n                     \"resizeable and size is: 3\")\n      # Test reading from too large an index\n      with self.assertRaisesOpError(error_msg):\n        self.evaluate(ta.write(3, 3.0).flow)\n\n  def testTensorArrayReadWrongIndexOrDataTypeFails(self):\n    with self.session():\n      ta = _make_ta(3, \"foo\", dtype=dtypes.float32)\n\n      w0 = ta.write(0, [[4.0, 5.0]])\n\n      # Test reading wrong datatype (only possible when constructing graphs).\n      if (not context.executing_eagerly() and\n          not control_flow_util.ENABLE_CONTROL_FLOW_V2):\n        r0_bad = gen_data_flow_ops.tensor_array_read_v3(\n            handle=w0.handle, index=0, dtype=dtypes.float64, flow_in=w0.flow)\n        with self.assertRaisesOpError(\n            \"TensorArray dtype is float but Op requested dtype double.\"):\n          self.evaluate(r0_bad)\n\n      if (control_flow_util.ENABLE_CONTROL_FLOW_V2 and\n          not context.executing_eagerly()):\n        error_msg = \"Trying to access element -1 in a list with 3 elements.\"\n      else:\n        error_msg = \"index -1\"\n      # Test reading from a negative index, which is not allowed\n      with self.assertRaisesOpError(error_msg):\n        self.evaluate(ta.read(-1))\n\n      if (control_flow_util.ENABLE_CONTROL_FLOW_V2 and\n          not context.executing_eagerly()):\n        error_msg = \"Trying to access element 3 in a list with 3 elements.\"\n      else:\n        error_msg = \"Tried to read from index 3 but array size is: 3\"\n      # Test reading from too large an index\n      with self.assertRaisesOpError(error_msg):\n        self.evaluate(ta.read(3))\n\n  @test_util.disable_control_flow_v2(\"v2 allows multiple writes.\")\n  @test_util.run_v1_only(\"v2 allows multiple writes.\")\n  def testSkipEagerTensorArrayWriteMultipleFails(self):\n    with self.session():\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32, tensor_array_name=\"foo\", size=3)\n\n      with self.assertRaisesOpError(\n          \"Could not write to TensorArray index 2 because \"\n          \"it has already been written to.\"):\n        self.evaluate(ta.write(2, 3.0).write(2, 3.0).flow)\n\n  def testTensorArrayConcatIncompatibleShapesFails(self):\n    with self.session():\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32,\n          tensor_array_name=\"foo\",\n          size=3,\n          infer_shape=False)\n\n      w1 = ta.write(0, 3.0)\n      w2 = w1.write(1, 4.0)\n      w3 = w2.write(2, [3.0])\n\n      with self.assertRaisesOpError(\n          \"Concat saw a scalar shape at index 0 but requires at least vectors\"):\n        self.evaluate(w3.concat())\n\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32,\n          tensor_array_name=\"foo\",\n          size=3,\n          infer_shape=False)\n\n      w1 = ta.write(0, [3.0])\n      w2 = w1.write(1, [4.0])\n      w3 = w2.write(2, [[3.0]])\n\n      # The exact error messages differ between eager execution and graph\n      # construction as the former bubbles up the error from array_op.concat.\n      error_msg = (\"Incompatible ranks\"\n                   if control_flow_util.ENABLE_CONTROL_FLOW_V2 and\n                   not context.executing_eagerly() else \"shape\")\n      with self.assertRaisesRegex(errors.InvalidArgumentError, error_msg):\n        self.evaluate(w3.concat())\n\n  def testTensorArraySplitIncompatibleShapesFails(self):\n    with self.session():\n      in_eager_mode = context.executing_eagerly()\n      ta = _make_ta(3, \"foo\")\n      with self.assertRaisesOpError(\n          r\"Expected lengths to be a vector, received shape: \\[\\]\"):\n        if in_eager_mode:\n          self.evaluate(ta.split([1.0, 2.0, 3.0], 1))\n        else:\n          lengths = array_ops.placeholder(dtypes.int64)\n          ta.split([1.0, 2.0, 3.0], lengths).flow.eval(feed_dict={lengths: 1})\n\n      error_msg = (\"Unused values in tensor. Length of tensor: 3 Values used: 1\"\n                   if control_flow_util.ENABLE_CONTROL_FLOW_V2 and\n                   not in_eager_mode else\n                   r\"Expected sum of lengths to be equal to values.shape\\[0\\], \"\n                   r\"but sum of lengths is 1 and value's shape is: \\[3\\]\")\n      with self.assertRaisesOpError(error_msg):\n        self.evaluate(ta.split([1.0, 2.0, 3.0], [1]).flow)\n\n      ta = _make_ta(1, \"baz\")\n      if control_flow_util.ENABLE_CONTROL_FLOW_V2 and not in_eager_mode:\n        with self.assertRaisesRegex(\n            ValueError, \"Shape must be at least rank 1 but is rank 0\"):\n          self.evaluate(ta.split(1.0, [1]).flow)\n      else:\n        with self.assertRaisesOpError(\n            r\"Expected value to be at least a vector, but received shape: \\[\\]\"\n        ):\n          self.evaluate(ta.split(1.0, [1]).flow)\n\n      if not control_flow_util.ENABLE_CONTROL_FLOW_V2 or in_eager_mode:\n        ta = _make_ta(2, \"buz\")\n        with self.assertRaisesOpError(\n            r\"TensorArray's size is not equal to the size of lengths \"\n            r\"\\(2 vs. 1\\), and the TensorArray is not marked as \"\n            r\"dynamically resizeable\"):\n          self.evaluate(ta.split([1.0], [1]).flow)\n\n  def _testTensorArrayWriteGradientAddMultipleAdds(self, dtype):\n    with self.cached_session():\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtype, tensor_array_name=\"foo\", size=3, infer_shape=False)\n      ta_grad = ta.grad(\"grad\")\n\n      c = lambda x: np.asarray(x, dtype=dtype.as_numpy_dtype)\n\n      w0 = ta.write(2, c(3.0))\n      w1 = w0.write(2, c(4.0))\n\n      w0_grad = ta_grad.write(2, c(3.0))\n      w1_grad = w0_grad.write(2, c(4.0))\n      w2_grad = w1_grad.write(2, c(5.0))\n\n      # Assert that aggregation works correctly\n      self.assertAllEqual(c(12.00), w2_grad.read(2))\n\n      # Assert that if multiple_writes_aggregate is not enabled,\n      # multiple writes raise an exception.\n      with self.assertRaisesOpError(\n          r\"TensorArray foo_.*: Could not write to TensorArray index 2 because \"\n          r\"it has already been written to.\"):\n        self.evaluate(w1.flow)\n\n      # Using differing shapes causes an exception\n      wb0_grad = ta_grad.write(1, c(1.0))\n      wb1_grad = wb0_grad.write(1, c([1.0]))\n\n      with self.assertRaisesOpError(\n          r\"Could not aggregate to TensorArray index 1 because the \"\n          r\"existing shape is \\[\\] but the new input shape is \\[1\\]\"):\n        self.evaluate(wb1_grad.flow)\n\n  @test_util.disable_control_flow_v2(\"v2 does not support TensorArray.grad.\")\n  @test_util.run_v1_only(\"v2 does not support TensorArray.grad.\")\n  def testSkipEagerTensorArrayWriteGradientAddMultipleAdds(self):\n    for dtype in (dtypes.int32, dtypes.int64, dtypes.float32, dtypes.float64,\n                  dtypes.complex64, dtypes.complex128):\n      self._testTensorArrayWriteGradientAddMultipleAdds(dtype)\n\n  @test_util.disable_control_flow_v2(\"Low level legacy TA op test.\")\n  @test_util.run_v1_only(\"Low level legacy TA op test.\")\n  def testSkipEagerTensorArrayGradWithShapeKnownElementShape(self):\n    with self.session() as sess:\n      ta = tensor_array_ops.TensorArray(\n          size=3,\n          dtype=dtypes.float32,\n          element_shape=tensor_shape.TensorShape([2, 3]))\n      handle, flow = data_flow_ops.tensor_array_grad_with_shape(\n          handle=ta.handle,\n          flow_in=ta.flow,\n          shape_to_prepend=tensor_shape.TensorShape([4, 5]),\n          source=\"source\")\n      ta_grad = tensor_array_ops.TensorArray(\n          dtypes.float32, handle=handle, flow=flow)\n      value = array_ops.placeholder(dtypes.float32)\n      ta_grad = ta_grad.write(0, value)\n      read_value = ta_grad.read(0)\n\n      # Make sure shape inference worked.\n      self.assertAllEqual([None, None, 2, 3], read_value.shape.as_list())\n      # Writing with wrong shape should not work.\n      with self.assertRaisesRegex(errors.InvalidArgumentError,\n                                  \"Could not write to TensorArray\"):\n        fed_value = np.random.random([2, 3])\n        sess.run(read_value, feed_dict={value: fed_value})\n      # Writing with correct shape should work.\n      fed_value = np.random.random([4, 5, 2, 3])\n      self.assertAllClose(fed_value,\n                          sess.run(read_value, feed_dict={value: fed_value}))\n\n  @test_util.disable_control_flow_v2(\"Low level legacy TA op test.\")\n  @test_util.run_v1_only(\"Low level legacy TA op test.\")\n  def testSkipEagerTensorArrayGradWithShapeUnknownElementShape(self):\n    with self.session() as sess:\n      ta = tensor_array_ops.TensorArray(\n          size=3, dtype=dtypes.float32,\n          element_shape=None)  # Note that element_shape is unknown\n      handle, flow = data_flow_ops.tensor_array_grad_with_shape(\n          handle=ta.handle,\n          flow_in=ta.flow,\n          shape_to_prepend=tensor_shape.TensorShape([4, 5]),\n          source=\"source\")\n      ta_grad = tensor_array_ops.TensorArray(\n          dtypes.float32, handle=handle, flow=flow)\n      value = array_ops.placeholder(dtypes.float32)\n      ta_grad = ta_grad.write(0, value)\n      read_value = ta_grad.read(0)\n\n      # Make sure shape inference worked.\n      self.assertIsNone(read_value.shape.ndims)\n      # Write with some shape and check read value.\n      fed_value = np.random.random([4, 5, 7])\n      self.assertAllClose(fed_value,\n                          sess.run(read_value, feed_dict={value: fed_value}))\n\n  def testMultiTensorArray(self):\n    with self.session():\n      h1 = tensor_array_ops.TensorArray(\n          size=1, dtype=dtypes.float32, tensor_array_name=\"foo\")\n      w1 = h1.write(0, 4.0)\n      r1 = w1.read(0)\n\n      h2 = tensor_array_ops.TensorArray(\n          size=1, dtype=dtypes.float32, tensor_array_name=\"bar\")\n\n      w2 = h2.write(0, 5.0)\n      r2 = w2.read(0)\n      r = r1 + r2\n      val = self.evaluate(r)\n      self.assertAllClose(9.0, val)\n\n  def _testTensorArrayGradientWriteReadType(self, dtype):\n    with self.cached_session() as session:\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.as_dtype(dtype),\n          tensor_array_name=\"foo\",\n          size=3,\n          infer_shape=False)\n\n      c = lambda x: np.array(x, dtype=dtype)\n\n      value_0 = constant_op.constant(c([[4.0, 5.0]]))\n      value_1 = constant_op.constant(c(3.0))\n\n      w0 = ta.write(0, value_0)\n      w1 = w0.write(1, value_1)\n      r0 = w1.read(0)\n      r1 = w1.read(1)\n      r0_2 = w1.read(0)\n\n      # Test individual components' gradients\n      grad_just_r0 = gradients_impl.gradients(\n          ys=[r0], xs=[value_0], grad_ys=[c([[2.0, 3.0]])])\n      grad_just_r0_vals = session.run(grad_just_r0)\n      self.assertAllEqual(c([[2.0, 3.0]]), grad_just_r0_vals[0])\n\n      grad_r0_r0_2 = gradients_impl.gradients(\n          ys=[r0, r0_2],\n          xs=[value_0],\n          grad_ys=[c([[2.0, 3.0]]), c([[1.0, -1.0]])])\n      grad_r0_r0_2_vals = session.run(grad_r0_r0_2)\n      self.assertAllEqual(c([[3.0, 2.0]]), grad_r0_r0_2_vals[0])\n\n      grad_just_r1 = gradients_impl.gradients(\n          ys=[r1], xs=[value_1], grad_ys=[c(-2.0)])\n      grad_just_r1_vals = session.run(grad_just_r1)\n      self.assertAllEqual(c(-2.0), grad_just_r1_vals[0])\n\n      # Test combined gradients\n      grad = gradients_impl.gradients(\n          ys=[r0, r0_2, r1],\n          xs=[value_0, value_1],\n          grad_ys=[c([[2.0, 3.0]]), c([[1.0, -1.0]]), c(-2.0)])\n      grad_vals = session.run(grad)\n      self.assertEqual(len(grad_vals), 2)\n      self.assertAllEqual(c([[3.0, 2.0]]), grad_vals[0])\n      self.assertAllEqual(c(-2.0), grad_vals[1])\n\n  @test_util.deprecated_graph_mode_only\n  def testSkipEagerTensorArrayGradientWriteRead(self):\n    for dtype in (np.float32, np.float64, np.complex64, np.complex128):\n      self._testTensorArrayGradientWriteReadType(dtype)\n\n  def _testTensorArrayGradientWritePackConcatAndRead(self):\n    with self.cached_session():\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32,\n          tensor_array_name=\"foo\",\n          size=2,\n          clear_after_read=False)\n\n      value_0 = constant_op.constant([-1.0, 1.0])\n      value_1 = constant_op.constant([-10.0, 10.0])\n\n      w0 = ta.write(0, value_0)\n      w1 = w0.write(1, value_1)\n      p0 = w1.stack()\n      r0 = w1.read(0)\n      s0 = w1.concat()\n\n      # Test gradient accumulation between read(0), pack(), and concat()\n      with ops.control_dependencies([p0, r0, s0]):\n        grad_r = gradients_impl.gradients(\n            ys=[p0, r0, s0],\n            xs=[value_0, value_1],\n            grad_ys=[\n                [[2.0, 3.0], [4.0, 5.0]],  # pack gradient\n                [-0.5, 1.5],  # read(0) gradient\n                [20.0, 30.0, 40.0, 50.0]\n            ])  # concat gradient\n      grad_vals = self.evaluate(grad_r)  # 2 + 2 entries\n\n      self.assertAllClose([2.0 - 0.5 + 20.0, 3.0 + 1.5 + 30.0], grad_vals[0])\n      self.assertAllEqual([4.0 + 40.0, 5.0 + 50.0], grad_vals[1])\n\n  @test_util.deprecated_graph_mode_only\n  def testSkipEagerTensorArrayGradientWritePackConcatAndRead(self):\n    self._testTensorArrayGradientWritePackConcatAndRead()\n\n  @test_util.disable_control_flow_v2(\"v2 does not support clear_after_read.\")\n  @test_util.run_v1_only(\"v2 does not support clear_after_read.\")\n  def testTensorArrayReadTwice(self):\n    with self.session():\n      value = constant_op.constant([[1.0, -1.0], [10.0, -10.0]])\n\n      ta_readonce = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32, tensor_array_name=\"foo\", size=2)\n\n      w_readonce = ta_readonce.unstack(value)\n      r0_readonce = w_readonce.read(0)\n\n      with self.assertRaisesOpError(\n          r\"Could not read index 0 twice because it was cleared after a \"\n          r\"previous read \\(perhaps try setting clear_after_read = false\\?\\)\"):\n        with ops.control_dependencies([r0_readonce]):\n          self.evaluate(w_readonce.read(0))\n\n      ta_readtwice = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32,\n          tensor_array_name=\"foo\",\n          size=2,\n          clear_after_read=False)\n      w_readtwice = ta_readtwice.unstack(value)\n      r0_readtwice = w_readtwice.read(0)\n      with ops.control_dependencies([r0_readtwice]):\n        r1_readtwice = w_readtwice.read(0)\n\n      self.assertAllEqual([1.0, -1.0], self.evaluate(r1_readtwice))\n\n  def _testTensorArrayGradientUnpackRead(self):\n    with self.cached_session() as session:\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32,\n          tensor_array_name=\"foo\",\n          size=2,\n          clear_after_read=False)\n\n      value = constant_op.constant([[1.0, -1.0], [10.0, -10.0]])\n\n      w = ta.unstack(value)\n      r0 = w.read(0)\n      r0_1 = w.read(0)\n      r1 = w.read(1)\n\n      # Test combined gradients + aggregation of read(0)\n      grad = gradients_impl.gradients(\n          ys=[r0, r0_1, r1],\n          xs=[value],\n          grad_ys=[[2.0, 3.0], [-1.5, 1.5], [4.0, 5.0]])\n      grad_vals = session.run(grad)\n\n      self.assertEqual(len(grad_vals), 1)\n      self.assertAllEqual([[2.0 - 1.5, 3.0 + 1.5], [4.0, 5.0]], grad_vals[0])\n\n  @test_util.deprecated_graph_mode_only\n  def testSkipEagerTensorArrayGradientUnpackRead(self):\n    self._testTensorArrayGradientUnpackRead()\n\n  @test_util.deprecated_graph_mode_only\n  def testSkipEagerTensorArrayGradientSplitConcat(self):\n    with self.session() as session:\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32, tensor_array_name=\"foo\", size=2,\n          infer_shape=False)\n\n      value = constant_op.constant(\n          [[1.0, -1.0], [10.0, -10.0], [100.0, -100.0]])\n\n      w = ta.split(value, [2, 1])\n      r = w.concat()\n\n      # Test combined gradients\n      grad = gradients_impl.gradients(\n          ys=[r],\n          xs=[value],\n          grad_ys=[[[2.0, -2.0], [20.0, -20.0], [200.0, -200.0]]])\n      grad_vals = session.run(grad)\n\n      self.assertEqual(len(grad_vals), 1)\n      self.assertAllEqual([[2.0, -2.0], [20.0, -20.0], [200.0, -200.0]],\n                          grad_vals[0])\n\n  def _testTensorArrayGradientDynamicUnpackRead(self):\n    with self.cached_session() as session:\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32,\n          tensor_array_name=\"foo\",\n          size=0,\n          dynamic_size=True)\n\n      value = constant_op.constant([[1.0, -1.0], [10.0, -10.0]])\n\n      w = ta.unstack(value)\n      r0 = w.read(0)\n      r1 = w.read(1)\n\n      # Test combined gradients + aggregation of read(0)\n      grad = gradients_impl.gradients(\n          ys=[r0, r1], xs=[value], grad_ys=[[2.0, 3.0], [4.0, 5.0]])\n      grad_vals = session.run(grad)\n\n      self.assertEqual(len(grad_vals), 1)\n      self.assertAllEqual([[2.0, 3.0], [4.0, 5.0]], grad_vals[0])\n\n  @test_util.deprecated_graph_mode_only\n  def testSkipEagerTensorArrayGradientDynamicUnpackRead(self):\n    self._testTensorArrayGradientDynamicUnpackRead()\n\n  def testCloseTensorArray(self):\n    with self.session():\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32, tensor_array_name=\"foo\", size=3)\n      self.evaluate(ta.close())\n\n  def testSizeTensorArray(self):\n    with self.session():\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32, tensor_array_name=\"foo\", size=3)\n      s = ta.size()\n      self.assertAllEqual(3, self.evaluate(s))\n\n  def testWriteCloseTensorArray(self):\n    with self.session():\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32,\n          tensor_array_name=\"foo\",\n          size=3,\n          infer_shape=False)\n      w0 = ta.write(0, [[4.0, 5.0]])\n      w1 = w0.write(1, [3.0])\n      self.evaluate(w1.close())  # Expected to run without problems\n\n  def _testWhileLoopWritePackGradients(self, dynamic_size, dtype):\n    np_dtype = dtype.as_numpy_dtype\n    with self.cached_session():\n\n      def func(v0, state0, var):\n        ta = tensor_array_ops.TensorArray(\n            dtype=dtype,\n            tensor_array_name=\"foo\",\n            size=0 if dynamic_size else 3,\n            dynamic_size=dynamic_size)\n        time_0 = array_ops.identity(0)\n\n        def body(time, ta_t, state):\n          sliced = array_ops.slice(\n              v0, begin=array_ops.stack([time, 0]), size=[1, -1])\n          sliced = array_ops.squeeze(sliced)\n          out = sliced + var + state\n          state += sliced\n          ta_t = ta_t.write(time, out)\n          return (time + 1, ta_t, state)\n\n        (unused_0, h_final, unused_2) = control_flow_ops.while_loop(\n            cond=lambda time, unused_1, unused_2: time < 3,\n            body=body,\n            loop_vars=(time_0, ta, state0),\n            shape_invariants=(time_0.get_shape(), tensor_shape.unknown_shape(),\n                              tensor_shape.unknown_shape()),\n            parallel_iterations=3)\n        vout = h_final.stack()\n        return vout\n\n      v0 = array_ops.identity(np.arange(3 * 5, dtype=np_dtype).reshape(3, 5))\n      state0 = array_ops.identity(np.array([1] * 5, dtype=np_dtype))\n      init_val = np.arange(100, 105, dtype=np_dtype)\n      var = variable_scope.get_variable(\n          \"var\",\n          shape=init_val.shape,\n          dtype=np_dtype,\n          initializer=init_ops.constant_initializer(init_val))\n\n      vout = func(v0, state0, var)\n      grad_val = -np.arange(3 * 5, dtype=np_dtype).reshape(3, 5)\n      if context.executing_eagerly():\n        grad_fn = backprop.gradients_function(func)\n        v0_grad, state0_grad, var_grad = grad_fn(v0, state0, var, dy=grad_val)\n      else:\n        v0_grad = gradients_impl.gradients([vout], [v0], [grad_val])[0]\n        state0_grad = gradients_impl.gradients([vout], [state0], [grad_val])[0]\n        var_grad = gradients_impl.gradients([vout], [var], [grad_val])[0]\n        self.evaluate(variables.global_variables_initializer())\n\n      state0_t, var_t, v0_t, vout_t, v0_grad_t, var_grad_t, state0_grad_t = (\n          self.evaluate(\n              ([state0, var, v0, vout, v0_grad, var_grad, state0_grad])))\n      just_v0_grad_t = self.evaluate(v0_grad)\n\n      # state = [ state0 | state0 + v0[0] | state0 + v0[0] + v0[1] ]\n      # vout = [ v0[0] + var + state[0] |\n      #          v0[1] + var + state[1] |\n      #          v0[2] + var + state[2] ]\n      #      = [ v0[0] + var + state0 |\n      #          v0[1] + var + state0 + v0[0] |\n      #          v0[2] + var + state0 + v0[0] + v0[1] ]\n      #\n      # d(vout[0])/d(v0) = [1 | 0 | 0 ]\n      # d(vout[1])/d(v0) = [1 | 1 | 0 ]\n      # d(vout[2])/d(v0) = [1 | 1 | 1 ]\n      # d(vout)/d(var) = [1 | 1 | 1]\n      # d(vout)/d(state0) = [ 1 | 1 | 1 ]\n\n      state_per_time = np.array(\n          [state0_t, state0_t + v0_t[0, :], state0_t + v0_t[0, :] + v0_t[1, :]])\n\n      # Compare forward prop\n      self.assertAllClose(v0_t + var_t + state_per_time, vout_t)\n\n      # Compare backward prop\n      expected_v0_grad_t = np.array([\n          grad_val[0, :] + grad_val[1, :] + grad_val[2, :],\n          grad_val[1, :] + grad_val[2, :], grad_val[2, :]\n      ])\n\n      self.assertAllEqual(expected_v0_grad_t, v0_grad_t)\n      self.assertAllEqual(expected_v0_grad_t, just_v0_grad_t)\n      self.assertAllClose(grad_val.sum(axis=0), var_grad_t)\n      self.assertAllClose(grad_val.sum(axis=0), state0_grad_t)\n\n  def testWhileLoopWritePackGradients(self):\n    self._testWhileLoopWritePackGradients(\n        dynamic_size=False, dtype=dtypes.float32)\n    # TODO(ebrevdo): re-enable when While supports non-float32 gradients.\n    # self._testWhileLoopWritePackGradients(\n    #     dynamic_size=False, dtype=tf.int64)\n\n  @test_util.run_deprecated_v1\n  def testSkipEagerWhileLoopDynamicWritePackGradients(self):\n    self._testWhileLoopWritePackGradients(\n        dynamic_size=True, dtype=dtypes.float32)\n\n  def testGradSerialTwoLoops(self):\n    with self.session():\n\n      def loop(x):\n        num_steps = 100\n        acc = tensor_array_ops.TensorArray(\n            dtype=dtypes.float32,\n            size=num_steps,\n            clear_after_read=False,\n            element_shape=tensor_shape.TensorShape([]))\n        i = constant_op.constant(0, name=\"i\")\n\n        c = lambda i, acc: i < 5\n\n        def b(i, acc):\n          x1 = control_flow_ops.cond(\n              math_ops.equal(i, 0), lambda: x,\n              lambda: math_ops.multiply(acc.read(i - 1), 2.0))\n          return i + 1, acc.write(i, x1)\n\n        i1, acc1 = control_flow_ops.while_loop(c, b, [i, acc])\n\n        z = constant_op.constant(0.0)\n\n        def fn(i, acc):\n          return i + 1, acc.write(i, z)\n\n        _, acc2 = control_flow_ops.while_loop(lambda i, acc: i < num_steps, fn,\n                                              [i1, acc1])\n\n        r = acc2.stack()\n        return r\n\n      x = constant_op.constant(2.0, name=\"x\")\n      if context.executing_eagerly():\n        grad = backprop.gradients_function(loop)(x)[0]\n      else:\n        grad = gradients_impl.gradients(loop(x), [x])[0]\n      self.assertAllClose(31.0, self.evaluate(grad))\n\n  def testShapeAfterWhileLoop(self):\n    size = 10\n    ta = tensor_array_ops.TensorArray(dtype=dtypes.float32, size=size)\n    _, ta = control_flow_ops.while_loop(\n        lambda i, _: i < size,\n        lambda i, ta: (i + 1, ta.write(i, [[0.]])), [0, ta],\n        parallel_iterations=1)\n    self.assertIsNotNone(ta.element_shape.dims)\n\n  @test_util.deprecated_graph_mode_only\n  def testSkipEagerSumOfTwoReadVariablesWithoutRepeatGrad(self):\n    with self.session() as session:\n      a = array_ops.identity(\n          np.arange(\n              3 * 5, dtype=np.float32).reshape(3, 5) + 1)\n      b = array_ops.identity(\n          np.arange(\n              3 * 5, dtype=np.float32).reshape(3, 5) + 1 + 3 * 5)\n      ta = tensor_array_ops.TensorArray(dtype=dtypes.float32, size=2)\n      ta = ta.write(0, a, name=\"write_a\")\n      ta = ta.write(1, b, name=\"write_b\")\n      c = (\n          ta.read(\n              0, name=\"read_a_0\") +  # a + b\n          ta.read(\n              1, name=\"read_b_0\"))\n      g0 = -(np.arange(3 * 5, dtype=np.float32).reshape(3, 5) + 1)\n      grad_a = gradients_impl.gradients([c], [a], [g0])[0]  # d(a+b)/da = 1\n      grad_b = gradients_impl.gradients([c], [b], [g0])[0]  # d(a+b)/db = 1\n\n      # Test gradients calculated individually\n      grad_a_t, = session.run([grad_a])\n      self.assertAllEqual(grad_a_t, g0)\n\n      grad_b_t, = session.run([grad_b])\n      self.assertAllEqual(grad_b_t, g0)\n\n      # Test gradients calculated jointly\n      joint_grad_a_t, joint_grad_b_t = session.run([grad_a, grad_b])\n      self.assertAllEqual(joint_grad_a_t, g0)\n      self.assertAllEqual(joint_grad_b_t, g0)\n\n  def _grad_source_for_name(self, name):\n    return tensor_array_grad._GetGradSource(constant_op.constant(0, name=name))\n\n  @test_util.deprecated_graph_mode_only\n  def testSkipEagerGetGradSource_Invalid(self):\n    with self.assertRaises(ValueError):\n      self._grad_source_for_name(\"\")\n    with self.assertRaises(ValueError):\n      self._grad_source_for_name(\"foo\")\n    with self.assertRaises(ValueError):\n      self._grad_source_for_name(\"foo/bar\")\n\n  @test_util.deprecated_graph_mode_only\n  def testSkipEagerGetGradSource_NoEnclosingScope(self):\n    self.assertEqual(\"gradients:0\", self._grad_source_for_name(\"gradients\"))\n    self.assertEqual(\"gradients_0:0\", self._grad_source_for_name(\"gradients_0\"))\n    self.assertEqual(\"gradients\", self._grad_source_for_name(\"gradients/foo\"))\n    self.assertEqual(\"gradients_0\",\n                     self._grad_source_for_name(\"gradients_0/foo\"))\n    self.assertEqual(\"gradients\",\n                     self._grad_source_for_name(\"gradients/foo/bar\"))\n    self.assertEqual(\"gradients_0\",\n                     self._grad_source_for_name(\"gradients_0/foo/bar\"))\n\n  @test_util.deprecated_graph_mode_only\n  def testSkipEagerGetGradSource_EnclosingScope(self):\n    self.assertEqual(\"foo/gradients:0\",\n                     self._grad_source_for_name(\"foo/gradients\"))\n    self.assertEqual(\"foo/gradients_0:0\",\n                     self._grad_source_for_name(\"foo/gradients_0\"))\n    self.assertEqual(\"foo/gradients\",\n                     self._grad_source_for_name(\"foo/gradients/bar\"))\n    self.assertEqual(\"foo/gradients_0\",\n                     self._grad_source_for_name(\"foo/gradients_0/bar\"))\n    self.assertEqual(\"foo/bar/gradients\",\n                     self._grad_source_for_name(\"foo/bar/gradients/baz\"))\n    self.assertEqual(\"foo/bar/gradients_0\",\n                     self._grad_source_for_name(\"foo/bar/gradients_0/baz\"))\n\n  @test_util.deprecated_graph_mode_only\n  def testSkipEagerGetGradSource_NestedUsesInnermost(self):\n    self.assertEqual(\n        \"foo/gradients/bar/gradients_0\",\n        self._grad_source_for_name(\"foo/gradients/bar/gradients_0/baz\"))\n\n  @test_util.deprecated_graph_mode_only\n  def testSkipEagerWriteShape(self):\n    with self.session():\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32, tensor_array_name=\"foo\", size=3)\n      c0 = constant_op.constant([4.0, 5.0])\n      w0 = ta.write(0, c0)\n      r0 = w0.read(0)\n      self.assertAllEqual(c0.get_shape(), r0.get_shape())\n\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32, tensor_array_name=\"foo\", size=3)\n      c1 = constant_op.constant([6.0, 7.0])\n      w1 = w0.write(1, c1)\n      r0 = w1.read(0)\n      r1 = w1.read(1)\n      self.assertAllEqual(c0.get_shape(), r0.get_shape())\n      self.assertAllEqual(c1.get_shape(), r1.get_shape())\n\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32, tensor_array_name=\"foo\", size=3)\n      c2 = constant_op.constant([4.0, 5.0, 6.0])\n      with self.assertRaises(ValueError):\n        w0.write(0, c2)\n\n  @test_util.deprecated_graph_mode_only\n  def testSkipEagerPartlyUnknownShape(self):\n    with self.session():\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32, tensor_array_name=\"foo\", size=6)\n\n      c0 = array_ops.placeholder(dtypes.float32, [None, None, None, 3])\n      w0 = ta.write(0, c0)\n      r0 = w0.read(0)\n      self.assertAllEqual([None, None, None, 3], r0.get_shape().as_list())\n\n      c1 = array_ops.placeholder(dtypes.float32, [None, None, None, 3])\n      w1 = w0.write(1, c1)\n      r1 = w1.read(0)\n      self.assertAllEqual([None, None, None, 3], r1.get_shape().as_list())\n\n      # Writing less specific shape (doesn't change type.)\n      c2 = array_ops.placeholder(dtypes.float32, [None, None, None, None])\n      w2 = w1.write(2, c2)\n      r2 = w2.read(0)\n      self.assertAllEqual([None, None, None, 3], r2.get_shape().as_list())\n\n      # Writing more specific shape in one dimension and less specific in\n      # another.\n      c3 = array_ops.placeholder(dtypes.float32, [None, None, 2, None])\n      w3 = w2.write(3, c3)\n      r3 = w3.read(0)\n      self.assertAllEqual([None, None, 2, 3], r3.get_shape().as_list())\n\n      # Writing partly defined shape using TensorArray.scatter.\n      c4 = array_ops.placeholder(dtypes.float32, [2, None, 4, 2, 3])\n      w4 = w3.scatter([4, 5], c4)\n      r4 = w4.read(0)\n      self.assertAllEqual([None, 4, 2, 3], r4.get_shape().as_list())\n\n      # Writing fully defined shape using TensorArray.split.\n      c5 = array_ops.placeholder(dtypes.float32, [10, 4, 2, 3])\n      w5 = w4.split(c5, constant_op.constant([5, 5]))\n      r5 = w5.read(0)\n      self.assertAllEqual([5, 4, 2, 3], r5.get_shape().as_list())\n\n  def _testUnpackShape(self):\n    with self.cached_session():\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32,\n          tensor_array_name=\"foo\",\n          size=0,\n          dynamic_size=True,\n          infer_shape=True)\n      value = constant_op.constant(\n          [[1.0, -1.0], [10.0, -10.0], [100.0, -100.0]])\n      w0 = ta.unstack(value)\n      r0 = w0.read(0)\n      self.assertAllEqual((2,), r0.get_shape())\n\n      c1 = constant_op.constant([4.0, 5.0])\n      w1 = w0.write(3, c1)\n\n      if not control_flow_util.ENABLE_CONTROL_FLOW_V2:\n        # TensorArray v2 does not support clear_after_read.\n        with self.assertRaisesOpError(\n            r\"Could not read index 0 twice because it was cleared after a \"\n            r\"previous read \\(perhaps try setting clear_after_read = false\\?\\)\"\n        ):\n          with ops.control_dependencies([r0]):\n            self.evaluate(w1.read(0))\n\n      r1 = w1.read(1)\n      self.assertAllEqual(c1.get_shape(), r1.shape)\n\n      c2 = constant_op.constant([4.0, 5.0, 6.0])\n      with self.assertRaises(ValueError):\n        w1.write(4, c2)\n\n  def testUnpackShape(self):\n    self._testUnpackShape()\n\n  @test_util.deprecated_graph_mode_only\n  def testSplitShape(self):\n    with self.session():\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32,\n          tensor_array_name=\"foo\",\n          size=0,\n          dynamic_size=True,\n          infer_shape=True)\n      value = constant_op.constant([[1.0, -1.0], [2.0, -2.0], [3.0, -3.0]])\n      w0 = ta.split(value, [1, 1, 1])\n      r0 = w0.read(0)\n      self.assertAllEqual((1, 2), r0.get_shape())\n\n      ta1 = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32,\n          tensor_array_name=\"foo1\",\n          size=0,\n          dynamic_size=True,\n          infer_shape=True)\n      w0 = ta1.split(value, [1, 2])\n      r0 = w0.read(0)\n      if context.executing_eagerly():\n        self.assertEqual((1, 2), r0.get_shape())\n        self.assertEqual((2, 2), w0.read(1).get_shape())\n      else:\n        self.assertEqual(r0.get_shape().ndims, None)\n        if not control_flow_util.ENABLE_CONTROL_FLOW_V2:\n          self.assertEqual(\n              tensor_shape.TensorShape(\n                  ta1.handle.op.get_attr(\"element_shape\")).ndims, None)\n\n  @test_util.deprecated_graph_mode_only\n  def testSkipEagerWriteUnknownShape(self):\n    with self.session():\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32,\n          tensor_array_name=\"foo\",\n          size=3,\n          infer_shape=True)\n      c0 = array_ops.placeholder(dtypes.float32)\n      w0 = ta.write(0, c0)\n      r0 = w0.read(0)\n      self.assertAllEqual(r0.get_shape(), tensor_shape.unknown_shape())\n\n  def _testGradientWhenNotAllComponentsRead(self):\n    with self.cached_session() as session:\n      ta = tensor_array_ops.TensorArray(dtype=dtypes.float32, size=2)\n      x = constant_op.constant([2.0, 3.0])\n      w = ta.unstack(x)\n      r0 = w.read(0)\n      # calculate (dr0/dx0, dr0/dx1).  since r0 = x0, gradients are (1, 0).\n      grad_r0 = gradients_impl.gradients(ys=[r0], xs=[x], grad_ys=[1.0])\n      grad_r0_vals = session.run(grad_r0)[0]\n      self.assertAllEqual(grad_r0_vals, [1.0, 0.0])\n\n  @test_util.deprecated_graph_mode_only\n  def testSkipEagerGradientWhenNotAllComponentsRead(self):\n    self._testGradientWhenNotAllComponentsRead()\n\n  @test_util.deprecated_graph_mode_only\n  def testSkipEagerWriteButNotAllComponentsReadGrad(self):\n    with self.cached_session() as session:\n      x0 = constant_op.constant(5.0)\n      x1 = constant_op.constant(10.0)\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32, size=2).write(0, x0).write(1, x1)\n      r0 = ta.read(0)\n      # calculate (dr0/dx0, dr0/dx1).  since r0 = x0, gradients are (1, 0).\n      grad_r0_x1 = gradients_impl.gradients(ys=[r0], xs=[x0, x1], grad_ys=[1.0])\n      grad_r0_x1_vals = session.run(grad_r0_x1)\n      self.assertAllEqual(grad_r0_x1_vals, [1.0, 0.0])\n\n  def _testTensorArrayUnpackDynamic(self):\n    with self.cached_session():\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32, size=3, dynamic_size=True)\n      x = constant_op.constant([1.0, 2.0, 3.0])\n      w0 = ta.unstack(x)\n      w1 = w0.write(3, 4.0)\n      r = w1.stack()\n      self.assertAllEqual(np.array([1.0, 2.0, 3.0, 4.0]), self.evaluate(r))\n      grad = gradients_impl.gradients(ys=[r], xs=[x])\n      self.assertAllEqual(np.array([1.0, 1.0, 1.0]), self.evaluate(grad)[0])\n\n  @test_util.run_deprecated_v1\n  def testSkipEagerTensorArrayUnpackDynamic(self):\n    self._testTensorArrayUnpackDynamic()\n\n  @test_util.run_deprecated_v1\n  def testSkipEagerTensorArraySplitDynamic(self):\n    with self.session():\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32, size=3, dynamic_size=True)\n      x = constant_op.constant([1.0, 2.0, 3.0])\n      w0 = ta.split(x, [1, 1, 1])\n      w1 = w0.write(3, [4.0])\n      r = w1.concat()\n      self.assertAllEqual(np.array([1.0, 2.0, 3.0, 4.0]), self.evaluate(r))\n      grad = gradients_impl.gradients(ys=[r], xs=[x])\n      self.assertAllEqual(np.array([1.0, 1.0, 1.0]), self.evaluate(grad)[0])\n\n  def testStackShape(self):\n\n    @def_function.function\n    def ta_stack():\n      ta = tensor_array_ops.TensorArray(dtype=dtypes.float32, size=3)\n      x = constant_op.constant([1.0, 2.0, 3.0])\n      ta = ta.write(0, x)\n      t = ta.stack()\n      self.assertEqual(t.shape.as_list(), [3, 3])\n      return t\n\n    ta_stack()\n\n  def testReadShape(self):\n\n    @def_function.function\n    def ta_read():\n      ta = tensor_array_ops.TensorArray(dtype=dtypes.float32, size=3)\n      x = constant_op.constant([1.0, 2.0, 3.0])\n      ta = ta.write(0, x)\n      t = ta.read(0)\n      self.assertEqual(t.shape.as_list(), [3])\n      return t\n\n    ta_read()\n\n  def testGatherShape(self):\n\n    def ta_gather(indices):\n      ta = tensor_array_ops.TensorArray(dtype=dtypes.float32, size=3)\n      x = constant_op.constant([1.0, 2.0, 3.0])\n      ta = ta.write(0, x)\n      t = ta.gather(indices)\n      self.assertEqual(t.shape.as_list(), [first_dim, 3])\n      return t\n\n    # This propagates shape of `indices` when compiling ta_gather.\n    ta_gather_with_known_indices_shape = def_function.function(ta_gather)\n    first_dim = 1\n    ta_gather_with_known_indices_shape([0])\n\n    # Here were force the shape of `indices` to be [None] during ta_gather's\n    # compilation.\n    ta_gather_with_unknown_indices_shape = def_function.function(\n        ta_gather,\n        input_signature=[\n            tensor_spec.TensorSpec(dtype=dtypes.int32, shape=[None])\n        ])\n    first_dim = None\n    ta_gather_with_unknown_indices_shape([0])\n\n  def _testTensorArrayEvalEmpty(self):\n    with self.cached_session():\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32, size=0, dynamic_size=False, infer_shape=False)\n      v2_msg = (\"Tried to stack elements of an empty list with \"\n                \"non-fully-defined element_shape\")\n      v1_msg = (\n          \"TensorArray has size zero, but element shape <unknown> is not \"\n          \"fully defined. Currently only static shapes are supported when \"\n          \"packing zero-size TensorArrays.\")\n      with self.assertRaisesOpError(\n          v2_msg if control_flow_util.ENABLE_CONTROL_FLOW_V2 else v1_msg):\n        ta.stack().eval()\n\n  @test_util.run_deprecated_v1\n  def testSkipEagerTensorArrayEvalEmpty(self):\n    self._testTensorArrayEvalEmpty()\n\n  # this test is ill-defined for Eager mode --- unpacking an empty tensor\n  # gives an empty list / there is not equivalent of \"mark_used\" in Eager\n  def _testTensorArrayEvalEmptyWithDefault(self):\n    with self.cached_session():\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32, size=0, dynamic_size=False, infer_shape=True)\n      self.assertEqual(0, ta.size().eval())\n      # Don't actually perform the pack.  This stores the static shape.\n      if control_flow_util.ENABLE_CONTROL_FLOW_V2:\n        ta = ta.unstack(array_ops.zeros([0, 3, 5]))\n      else:\n        ta.unstack(array_ops.zeros([0, 3, 5])).mark_used()\n      packed = ta.stack()\n      concatenated = ta.concat()\n      self.assertAllEqual([0, 3, 5], self.evaluate(packed).shape)\n      # Concatenating zero tensors along their first dimension gives a\n      # first dimension of zero\n      self.assertAllEqual([0, 5], self.evaluate(concatenated).shape)\n\n  @test_util.run_deprecated_v1\n  def testSkipEagerTensorArrayEvalEmptyWithDefault(self):\n    self._testTensorArrayEvalEmptyWithDefault()\n\n  @test_util.run_deprecated_v1\n  def testSkipEagerTensorArrayScatterReadAndGradients(self):\n    with self.session() as session:\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32,\n          tensor_array_name=\"foo\",\n          size=0,\n          dynamic_size=True)\n\n      indices = constant_op.constant([1, 8])\n      value = constant_op.constant([[1.0, -1.0], [10.0, -10.0]])\n\n      w = ta.scatter(indices, value)\n      r0 = w.read(1)\n      r1 = w.read(8)\n\n      # Test combined gradients + aggregation of read(0)\n      grad = gradients_impl.gradients(\n          ys=[r0, r1], xs=[value], grad_ys=[[2.0, 3.0], [4.0, 5.0]])\n      read_vals, grad_vals = session.run([[r0, r1], grad])\n\n      self.assertEqual(len(read_vals), 2)\n      self.assertEqual(len(grad_vals), 1)\n      self.assertAllEqual([1.0, -1.0], read_vals[0])\n      self.assertAllEqual([10.0, -10.0], read_vals[1])\n      self.assertAllEqual([[2.0, 3.0], [4.0, 5.0]], grad_vals[0])\n\n  @test_util.run_deprecated_v1\n  def testSkipEagerTensorArrayScatterPartialReadAndGradients(self):\n    with self.session() as session:\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32,\n          tensor_array_name=\"foo\",\n          size=0,\n          dynamic_size=True)\n\n      indices = constant_op.constant([1, 8])\n      value = constant_op.constant([[1.0, -1.0], [10.0, -10.0]])\n\n      w = ta.scatter(indices, value)\n      r0 = w.read(1)\n\n      # Test combined gradients + aggregation of read(0)\n      grad = gradients_impl.gradients(\n          ys=[r0], xs=[value], grad_ys=[[2.0, 3.0]])[0]\n      read_val, grad_val = session.run([r0, grad])\n\n      self.assertAllEqual([1.0, -1.0], read_val)\n      self.assertAllEqual([[2.0, 3.0], [0.0, 0.0]], grad_val)\n\n  def testScatterIntoExistingList(self):\n    ta = tensor_array_ops.TensorArray(\n        dtype=dtypes.float32, tensor_array_name=\"foo\", size=5)\n\n    ta = ta.scatter(indices=[3, 4], value=array_ops.ones([2]))\n    self.assertAllEqual(ta.stack(), [0., 0., 0., 1., 1.])\n\n    ta = ta.scatter(indices=[1], value=array_ops.ones([1]))\n    self.assertAllEqual(ta.stack(), [0., 1., 0., 1., 1.])\n\n    ta = ta.scatter(indices=[0, 2], value=[5., 6.])\n    self.assertAllEqual(ta.stack(), [5., 1., 6., 1., 1.])\n\n  @test_util.run_v1_only(\"b/118890905\")\n  def testTensorArrayWriteGatherAndGradients(self):\n    with self.session() as session:\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32,\n          tensor_array_name=\"foo\",\n          size=0,\n          dynamic_size=True)\n\n      def func(values):\n        indices = constant_op.constant([1, 8])\n        w = ta.unstack(values)\n        g = w.gather(indices)\n        return g\n\n      values = constant_op.constant([[1.0 * x, -1.0 * x] for x in range(10)])\n      g = func(values)\n      grad_ys = [[[2.0, 3.0], [4.0, 5.0]]]\n      # Test combined gradients + aggregation of read(0)\n      if context.executing_eagerly():\n        g_vals = [g]\n        grad_vals = backprop.gradients_function(func)(\n            values, dy=constant_op.constant(grad_ys[0], dtype=dtypes.float32))\n      else:\n        grad = gradients_impl.gradients(ys=[g], xs=[values], grad_ys=grad_ys)\n        g_vals, grad_vals = session.run([[g], grad])\n\n      # Gradients for 8 of the 10 unread components are zero.\n      expected_grad = np.zeros((10, 2))\n      expected_grad[1] = [2.0, 3.0]\n      expected_grad[8] = [4.0, 5.0]\n\n      self.assertEqual(len(g_vals), 1)\n      self.assertEqual(len(grad_vals), 1)\n      self.assertAllEqual([[1.0, -1.0], [8.0, -8.0]], g_vals[0])\n      self.assertAllEqual(expected_grad, grad_vals[0])\n\n  @test_util.disable_control_flow_v2(\"colocate_with not supported in v2.\")\n  @test_util.run_v1_only(\"b/120545219\")\n  def testSkipEagerTensorArrayGetsDeviceFromFirstWrite(self):\n    with ops.device(\"/job:worker/task:0/cpu:0\"):\n      # this initial device will be ignored.\n      ta = tensor_array_ops.TensorArray(dtype=dtypes.float32, size=2)\n    with ops.device(\"/job:worker/task:1/cpu:0\"):\n      # the first write sets the op's device.\n      ta = ta.write(0, 1.0)\n    with ops.device(\"/job:worker/task:2/cpu:0\"):\n      # subsequent writes do not modify the op's device.\n      ta = ta.write(1, 1.0)\n\n    # The gradient TA will sit on the same device as the forward TA.\n    ta_grad = ta.grad(\"grad\")\n    flows = [ta.flow, ta_grad.flow]\n\n    # Similar tests for unpack and split\n    with ops.device(\"/job:worker/task:0/cpu:0\"):\n      ta = tensor_array_ops.TensorArray(dtype=dtypes.float32, size=3)\n    with ops.device(\"/job:worker/task:1/cpu:0\"):\n      ta = ta.unstack([1.0, 2.0])\n    with ops.device(\"/job:worker/task:2/cpu:0\"):\n      ta = ta.write(2, 3.0)\n    flows.append(ta.flow)\n\n    with ops.device(\"/job:worker/task:0/cpu:0\"):\n      ta = tensor_array_ops.TensorArray(dtype=dtypes.float32, size=2)\n    with ops.device(\"/job:worker/task:1/cpu:0\"):\n      ta = ta.split([1.0, 2.0], [1, 1])\n    flows.append(ta.flow)\n\n    session = session_lib.Session(self._workers[0].target)\n\n    run_options = config_pb2.RunOptions(\n        trace_level=config_pb2.RunOptions.FULL_TRACE)\n    run_metadata = config_pb2.RunMetadata()\n\n    session.run(flows, options=run_options, run_metadata=run_metadata)\n    self.assertTrue(run_metadata.HasField(\"step_stats\"))\n    dev_stats = {d.device: d.node_stats\n                 for d in run_metadata.step_stats.dev_stats}\n    for d in dev_stats:\n      if \"/task:1/\" in d:\n        self.assertTrue(\n            [s for s in dev_stats[d] if \"/TensorArray\" in s.node_name])\n      elif \"/host:CPU\" not in d:\n        self.assertFalse(\n            [s for s in dev_stats[d] if \"/TensorArray\" in s.node_name])\n\n  @test_util.disable_control_flow_v2(\"colocate_with not supported in v2.\")\n  @test_util.run_v1_only(\"b/120545219\")\n  def testSkipEagerTensorArrayGetsDeviceFromFirstWriteInWhileLoop(self):\n    with ops.device(\"/job:worker/task:0/cpu:0\"):\n      ta = tensor_array_ops.TensorArray(dtype=dtypes.float32, size=2)\n\n    def _body(i, ta_i):\n      with ops.device(\"/job:worker/task:1/cpu:0\"):\n        return i + 1, ta_i.write(i, constant_op.constant(0.0))\n\n    _, ta_out = control_flow_ops.while_loop(\n        lambda i, ta: i < 2, _body, loop_vars=[0, ta])\n\n    session = session_lib.Session(self._workers[0].target)\n\n    run_options = config_pb2.RunOptions(\n        trace_level=config_pb2.RunOptions.FULL_TRACE)\n    run_metadata = config_pb2.RunMetadata()\n\n    session.run(ta_out.flow, options=run_options, run_metadata=run_metadata)\n    self.assertTrue(run_metadata.HasField(\"step_stats\"))\n    dev_stats = {d.device: d.node_stats\n                 for d in run_metadata.step_stats.dev_stats}\n    for d in dev_stats:\n      if \"/task:1/\" in d:\n        self.assertTrue(\n            [s for s in dev_stats[d] if \"TensorArray\" == s.node_name])\n      else:\n        self.assertFalse(\n            [s for s in dev_stats[d] if \"TensorArray\" == s.node_name])\n\n  @test_util.disable_control_flow_v2(\"colocate_with not supported in v2.\")\n  @test_util.run_v1_only(\"b/120545219\")\n  def testSkipEagerTensorArrayDisabledColocateWithFirstWriteCall(self):\n    with ops.device(\"/job:worker/task:0/cpu:0\"):\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32, size=2, colocate_with_first_write_call=False)\n\n    def _body(i, ta_i):\n      with ops.device(\"/job:worker/task:1/cpu:0\"):\n        return i + 1, ta_i.write(i, constant_op.constant(0.0))\n\n    _, ta_out = control_flow_ops.while_loop(\n        lambda i, ta: i < 2, _body, loop_vars=[0, ta])\n\n    session = session_lib.Session(self._workers[0].target)\n\n    run_options = config_pb2.RunOptions(\n        trace_level=config_pb2.RunOptions.FULL_TRACE)\n    run_metadata = config_pb2.RunMetadata()\n\n    session.run(ta_out.flow, options=run_options, run_metadata=run_metadata)\n    self.assertTrue(run_metadata.HasField(\"step_stats\"))\n    dev_stats = {d.device: list(d.node_stats)\n                 for d in run_metadata.step_stats.dev_stats}\n    for d in dev_stats:\n      if \"/task:0/\" in d and \"CPU\" in d:  # Skip any GPU node stats\n        self.assertTrue(\n            [s for s in dev_stats[d] if \"TensorArray\" == s.node_name])\n      else:\n        self.assertFalse(\n            [s for s in dev_stats[d] if \"TensorArray\" == s.node_name])\n\n  def testTensorArrayIdentity(self):\n    with self.session():\n      ta0 = tensor_array_ops.TensorArray(dtype=dtypes.float32, size=2,\n                                         infer_shape=False)\n      ta1 = tensor_array_ops.TensorArray(dtype=dtypes.int32, size=4,\n                                         infer_shape=True)\n\n      ta0 = ta0.write(0, 0.)\n      ta1 = ta1.write(0, 1)\n\n      v0 = variable_scope.get_variable(\n          \"v0\", shape=(), initializer=init_ops.zeros_initializer())\n      v1 = variable_scope.get_variable(\n          \"v1\", shape=(), initializer=init_ops.zeros_initializer())\n\n      with ops.control_dependencies([v0.assign_add(1)]):\n        ta0 = ta0.identity()\n\n      with ops.control_dependencies([v1.assign_add(1)]):\n        ta1 = ta1.identity()\n\n      read0 = ta0.read(0)\n      read1 = ta1.read(0)\n\n      size0 = ta0.size()\n      size1 = ta1.size()\n\n      # Tests correct properties on new TensorArrays.\n      self.assertEqual(dtypes.float32, ta0.dtype)\n      self.assertEqual(dtypes.int32, ta1.dtype)\n      if context.executing_eagerly():\n        self.assertEqual(tensor_shape.TensorShape([]), read0.get_shape())\n      else:\n        self.assertEqual(tensor_shape.unknown_shape(), read0.get_shape())\n      self.assertEqual(tensor_shape.TensorShape([]), read1.get_shape())\n\n      if not context.executing_eagerly():\n        self.evaluate(variables.global_variables_initializer())\n\n      read0_v, read1_v, size0_v, size1_v = self.evaluate((read0, read1, size0,\n                                                          size1))\n\n      # Tests that the control dependencies was added and executed.\n      self.assertEqual(1, self.evaluate(v0))\n      self.assertEqual(1, self.evaluate(v1))\n\n      # Tests correct TensorArray.\n      self.assertEqual(read0_v, 0)\n      self.assertEqual(read1_v, 1)\n      self.assertEqual(size0_v, 2)\n      self.assertEqual(size1_v, 4)\n\n  @test_util.deprecated_graph_mode_only\n  def testSkipEagerTensorArrayGradYsInCorrectScope(self):\n    n_time = 1\n    n_dim = 1\n    x = constant_op.constant([[1.42]])\n    dy = constant_op.constant([[2.42]])\n\n    ta = tensor_array_ops.TensorArray(\n        dtypes.float32, size=n_time, element_shape=[n_dim])\n    for t in range(n_time):\n      ta = ta.write(index=t, value=x[t])\n      y = ta.stack()\n      # dy is outside of the gradients name scope; tf.gradients must\n      # wrap it in the correct name scope.\n      dx, = gradients_impl.gradients(ys=[y], xs=[x], grad_ys=[dy])\n      with self.cached_session():\n        vdx, vdy = self.evaluate([dx, dy])\n      self.assertAllClose(vdx, vdy)\n\n  @test_util.deprecated_graph_mode_only\n  def testSkipEagerTensorArrayInt64GPU(self):\n    if not test.is_gpu_available():\n      return\n    with self.session(force_gpu=True) as sess:\n      value = array_ops.placeholder(dtypes.int64)\n      ta = tensor_array_ops.TensorArray(dtype=dtypes.int64, size=2)\n      ta = ta.scatter([0, 1], value)\n      r0 = ta.read(0)\n      r1 = ta.read(1)\n      v0, v1 = sess.run([r0, r1], feed_dict={value: [-3, 100]})\n      self.assertAllEqual(v0, -3)\n      self.assertAllEqual(v1, 100)\n\n  @test_util.deprecated_graph_mode_only\n  def testTensorArrayScatterBfloat16GPU(self):\n    if not test.is_gpu_available():\n      return\n    with self.session(force_gpu=True) as sess:\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.bfloat16, tensor_array_name=\"foo\", size=5)\n      ta = ta.scatter(\n          indices=[3, 4], value=array_ops.ones([2], dtype=dtypes.bfloat16))\n      self.assertAllEqual(ta.stack(), [0., 0., 0., 1., 1.])\n\n  def testInferShapeFalseValid(self):\n    ta = tensor_array_ops.TensorArray(\n        dtypes.float32, size=3, infer_shape=False, element_shape=[None, 10, 20])\n    ta = ta.write(0, array_ops.ones([50, 10, 20]))\n    ta = ta.write(1, array_ops.ones([50, 10, 20]))\n    ta = ta.write(2, array_ops.ones([1, 10, 20]))\n    ta = ta.concat()\n\n    correct = np.ones([101, 10, 20])\n\n    self.assertAllEqual(ta, correct)\n\n  def testInferShapeFalseInvalid(self):\n    ta = tensor_array_ops.TensorArray(\n        dtypes.float32, size=2, infer_shape=False, element_shape=[None, 10, 20])\n    ta = ta.write(0, array_ops.ones([50, 10, 20]))\n\n    with self.assertRaises(ValueError):\n      ta = ta.write(1, array_ops.ones([1, 20, 20]))\n\n  def testInferShapeTrue(self):\n    ta = tensor_array_ops.TensorArray(\n        dtypes.float32, size=3, infer_shape=True, element_shape=[None, 10, 20])\n    self.assertAllEqual((None, 10, 20), ta.element_shape.as_list())\n    ta = ta.write(0, array_ops.ones([50, 10, 20]))\n    self.assertAllEqual((50, 10, 20), ta.element_shape.as_list())\n    ta = ta.write(1, array_ops.ones([50, 10, 20]))\n    with self.assertRaises(ValueError):\n      ta = ta.write(\n          2, array_ops.ones([1, 10, 20])\n      )  # Inconsistent shapes: saw (1, 10, 20) but expected (50, 10, 20)\n\n  def testStackShapeOnEmpty(self):\n    ta = tensor_array_ops.TensorArray(\n        dtypes.float32, size=0, element_shape=(5, 10), dynamic_size=True)\n    self.assertAllEqual([0, 5, 10], self.evaluate(ta.stack()).shape)\n\n  @test_util.run_deprecated_v1\n  def testSkipEagerStackOnPartiallyDefinedShape(self):\n    ta = tensor_array_ops.TensorArray(\n        dtypes.float32, size=0, element_shape=(5, None), dynamic_size=True)\n    self.assertEqual([None, 5, None], ta.stack().shape.as_list())\n\n  def testStackShapeOnStaticSize(self):\n    ta = tensor_array_ops.TensorArray(dtypes.float32, size=42)\n    ta = ta.write(0, [0])\n    self.assertEqual([42, 1], ta.stack().shape.as_list())\n\n\nclass TensorArrayBenchmark(test.Benchmark):\n\n  def _tensorArrayWriteInWhile(self):\n    size = 10000\n    ta = tensor_array_ops.TensorArray(dtype=dtypes.float32, size=size)\n    (_, ta) = control_flow_ops.while_loop(\n        lambda i, _: i < size,\n        lambda i, ta: (i + 1, ta.write(i, 0.)), [0, ta],\n        parallel_iterations=1)\n    return ta.stack()\n\n  def _benchmarkWriteInWhile(self):\n    ops.reset_default_graph()\n    op = self._tensorArrayWriteInWhile()\n    self.run_op_benchmark(session_lib.Session(), op)\n\n  def benchmarkWriteInWhile(self):\n    self._benchmarkWriteInWhile()\n\n  @test_util.enable_control_flow_v2\n  def benchmarkWriteInWhileWithControlFlowV2(self):\n    self._benchmarkWriteInWhile()\n\n  def benchmarkWriteInDatasetMapFn(self):\n    ds = dataset_ops.Dataset.from_tensors(array_ops.zeros([10])).repeat()\n    ds = ds.map(lambda _: self._tensorArrayWriteInWhile())\n    op = ds.make_one_shot_iterator().get_next()\n    self.run_op_benchmark(session_lib.Session(), op)\n\n  def benchmarkWriteInDatasetParallelMapFn(self):\n    ds = dataset_ops.Dataset.from_tensors(array_ops.zeros([10])).repeat()\n    ds = ds.map(lambda _: self._tensorArrayWriteInWhile(), num_parallel_calls=2)\n    op = ds.make_one_shot_iterator().get_next()\n    self.run_op_benchmark(session_lib.Session(), op)\n\n\nif __name__ == \"__main__\":\n  test.main()\n"], "fixing_code": ["/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n// See docs in ../ops/data_flow_ops.cc.\n\n#define EIGEN_USE_THREADS\n\n#include <limits>\n#include <vector>\n// TODO(b/31496047): Fix non-standard include order.\n#include <numeric>  // clang-format off\n\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n#include \"tensorflow/core/framework/bounds_check.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/register_types.h\"\n#include \"tensorflow/core/framework/resource_mgr.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/tensor_shape.h\"\n#include \"tensorflow/core/framework/tensor_util.h\"\n#include \"tensorflow/core/framework/types.h\"\n#include \"tensorflow/core/kernels/concat_lib.h\"\n#include \"tensorflow/core/kernels/split_lib.h\"\n#include \"tensorflow/core/kernels/tensor_array.h\"\n#include \"tensorflow/core/lib/core/errors.h\"\n#include \"tensorflow/core/lib/core/refcount.h\"\n#include \"tensorflow/core/lib/strings/strcat.h\"\n#include \"tensorflow/core/platform/dynamic_annotations.h\"\n#include \"tensorflow/core/platform/logging.h\"\n#include \"tensorflow/core/platform/thread_annotations.h\"\n#include \"tensorflow/core/platform/types.h\"\n#include \"tensorflow/core/util/ptr_util.h\"\n\ntypedef Eigen::ThreadPoolDevice CPUDevice;\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\ntypedef Eigen::GpuDevice GPUDevice;\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n// clang-format on\n\nnamespace tensorflow {\n\nStatus GetHandle(OpKernelContext* ctx, string* container, string* ta_handle) {\n  {\n    Tensor tensor;\n    // Assuming that handle is the input at index 0.\n    if (IsRefType(ctx->input_dtype(0))) {\n      tensor = ctx->mutable_input(0, false);\n    } else {\n      tensor = ctx->input(0);\n    }\n    if (tensor.NumElements() != 2) {\n      return errors::InvalidArgument(\n          \"Tensor array handle must be 2-element vector, but had shape: \",\n          tensor.shape().DebugString());\n    }\n    auto h = tensor.flat<tstring>();\n    *container = h(0);\n    *ta_handle = h(1);\n  }\n  return OkStatus();\n}\n\nStatus GetTensorArray(OpKernelContext* ctx, TensorArray** tensor_array) {\n  string container;\n  string ta_handle;\n  if (ctx->input_dtype(0) != DT_RESOURCE) {\n    TF_RETURN_IF_ERROR(GetHandle(ctx, &container, &ta_handle));\n    ResourceMgr* rm = ctx->resource_manager();\n    if (rm == nullptr) return errors::Internal(\"No resource manager.\");\n    ScopedStepContainer* sc = ctx->step_container();\n    if (sc == nullptr) return errors::Internal(\"No step container.\");\n    TF_RETURN_IF_ERROR(sc->Lookup(rm, container + ta_handle, tensor_array));\n    return OkStatus();\n  } else {\n    return LookupResource(ctx, HandleFromInput(ctx, 0), tensor_array);\n  }\n}\n\nStatus SetupFlowControlInputs(OpKernelContext* ctx, bool set_output) {\n  const Tensor* flow_control;\n  TF_RETURN_IF_ERROR(ctx->input(\"flow_in\", &flow_control));\n  if (set_output) {\n    TF_RETURN_IF_ERROR(ctx->set_output(\"flow_out\", *flow_control));\n  }\n  return OkStatus();\n}\n\n// CREATION *******************************************************************\n\n// Virtual class for shared behavior between TensorArrayOp and\n// TensorArrayGradOp.\nclass TensorArrayCreationOp : public OpKernel {\n public:\n  explicit TensorArrayCreationOp(OpKernelConstruction* context)\n      : OpKernel(context), device_type_(context->device_type()) {}\n\n  void Compute(OpKernelContext* ctx) override {\n    Tensor tensor_array_output_handle;\n\n    AllocatorAttributes alloc_attr;\n    alloc_attr.set_on_host(true);\n    OP_REQUIRES_OK(ctx, ctx->allocate_temp(\n                            tensorflow::DT_STRING, tensorflow::TensorShape({2}),\n                            &tensor_array_output_handle, alloc_attr));\n    // Store the handle in a per-step container of the RM.\n    ResourceMgr* rm = ctx->resource_manager();\n    OP_REQUIRES(ctx, rm != nullptr, errors::Internal(\"No resource manager.\"));\n\n    TensorArray* output_tensor_array;\n    OP_REQUIRES_OK(ctx, CreateTensorArray(ctx, rm, &tensor_array_output_handle,\n                                          &output_tensor_array));\n    if (IsRefType(ctx->expected_output_dtype(0))) {\n      ctx->set_output_ref(0, output_tensor_array->mu(),\n                          output_tensor_array->handle());\n    } else if (ctx->expected_output_dtype(0) == DT_STRING) {\n      ctx->set_output(0, *output_tensor_array->handle());\n    } else {\n      Tensor* handle;\n      OP_REQUIRES_OK(ctx, ctx->allocate_output(0, TensorShape({}), &handle));\n      handle->flat<ResourceHandle>()(0) =\n          output_tensor_array->resource_handle(ctx);\n    }\n    if (ctx->num_outputs() == 2) {\n      // Create the flow output.\n      Tensor* flow;\n      OP_REQUIRES_OK(ctx, ctx->allocate_output(1, TensorShape({}), &flow));\n      if (device_type_ == DEVICE_CPU) {\n        // Value doesn't matter, but this makes msan not complaint about\n        // copying an uninitialized value. To do this on GPU would require\n        // a kernel launch or a host->device memcpy, so we avoid that.\n        flow->flat<float>()(0) = 0;\n      }\n    }\n  }\n\n protected:\n  virtual Status CreateTensorArray(OpKernelContext* ctx, ResourceMgr* rm,\n                                   Tensor* tensor_array_output_handle,\n                                   TensorArray** output_tensor_array) = 0;\n\n private:\n  const DeviceType device_type_;\n};\n\n// A per-run local tensor array. The tensor array uses a \"per-step\" resource\n// manager which ensures that correct garbage collection on error or\n// successful completion.\nclass TensorArrayOp : public TensorArrayCreationOp {\n public:\n  explicit TensorArrayOp(OpKernelConstruction* context)\n      : TensorArrayCreationOp(context) {\n    OP_REQUIRES_OK(context, context->GetAttr(\"dtype\", &dtype_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"element_shape\", &element_shape_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"dynamic_size\", &dynamic_size_));\n    // The HasAttr check is for backwards compatibility with older op\n    // versions which do not have this attribute.\n    if (context->HasAttr(\"identical_element_shapes\")) {\n      OP_REQUIRES_OK(context, context->GetAttr(\"identical_element_shapes\",\n                                               &identical_element_shapes_));\n    } else {\n      identical_element_shapes_ = false;\n    }\n    OP_REQUIRES_OK(context,\n                   context->GetAttr(\"clear_after_read\", &clear_after_read_));\n    OP_REQUIRES_OK(context,\n                   context->GetAttr(\"tensor_array_name\", &tensor_array_name_));\n    if (tensor_array_name_.empty()) tensor_array_name_ = name();\n  }\n\n  Status CreateTensorArray(OpKernelContext* ctx, ResourceMgr* rm,\n                           Tensor* tensor_array_output_handle,\n                           TensorArray** output_tensor_array) override {\n    const Tensor* tensor_size;\n    TF_RETURN_IF_ERROR(ctx->input(\"size\", &tensor_size));\n\n    if (!TensorShapeUtils::IsScalar(tensor_size->shape())) {\n      return errors::InvalidArgument(\n          \"TensorArray size must be scalar, but had shape: \",\n          tensor_size->shape().DebugString());\n    }\n    const int32_t size = tensor_size->scalar<int32>()();\n    if (size < 0) {\n      return errors::InvalidArgument(\"Size should be >= 0.\");\n    }\n\n    auto handle = tensor_array_output_handle->flat<tstring>();\n    string unique_tensor_array_name =\n        strings::StrCat(tensor_array_name_, \"_\",\n                        TensorArray::tensor_array_counter.fetch_add(1));\n    handle(0) = \"_tensor_arrays\";\n    handle(1) = unique_tensor_array_name;\n\n    auto key = strings::StrCat(handle(0), unique_tensor_array_name);\n\n    TensorArray* tensor_array = new TensorArray(\n        key, dtype_, *tensor_array_output_handle, size, element_shape_,\n        identical_element_shapes_, dynamic_size_,\n        false /* multiple_writes_aggregate */, false /* is_grad */,\n        -1 /* marked_size */, clear_after_read_);\n\n    TF_RETURN_IF_ERROR(ctx->step_container()->Create(rm, key, tensor_array));\n\n    *output_tensor_array = tensor_array;\n\n    return OkStatus();\n  }\n\n private:\n  DataType dtype_;\n  PartialTensorShape element_shape_;\n  bool identical_element_shapes_;\n  bool dynamic_size_;\n  bool clear_after_read_;\n  string tensor_array_name_;  // The name used to create the TensorArray.\n\n  TF_DISALLOW_COPY_AND_ASSIGN(TensorArrayOp);\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"TensorArray\").Device(DEVICE_CPU), TensorArrayOp);\nREGISTER_KERNEL_BUILDER(Name(\"TensorArrayV2\").Device(DEVICE_CPU),\n                        TensorArrayOp);\nREGISTER_KERNEL_BUILDER(Name(\"TensorArrayV3\").Device(DEVICE_CPU),\n                        TensorArrayOp);\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n#define REGISTER_GPU(type)                                   \\\n  REGISTER_KERNEL_BUILDER(Name(\"TensorArray\")                \\\n                              .Device(DEVICE_GPU)            \\\n                              .TypeConstraint<type>(\"dtype\") \\\n                              .HostMemory(\"size\")            \\\n                              .HostMemory(\"handle\"),         \\\n                          TensorArrayOp);                    \\\n  REGISTER_KERNEL_BUILDER(Name(\"TensorArrayV2\")              \\\n                              .Device(DEVICE_GPU)            \\\n                              .TypeConstraint<type>(\"dtype\") \\\n                              .HostMemory(\"size\")            \\\n                              .HostMemory(\"handle\"),         \\\n                          TensorArrayOp);                    \\\n  REGISTER_KERNEL_BUILDER(Name(\"TensorArrayV3\")              \\\n                              .Device(DEVICE_GPU)            \\\n                              .TypeConstraint<type>(\"dtype\") \\\n                              .HostMemory(\"size\")            \\\n                              .HostMemory(\"handle\"),         \\\n                          TensorArrayOp);\n\nTF_CALL_int64(REGISTER_GPU);\nTF_CALL_GPU_NUMBER_TYPES(REGISTER_GPU);\nTF_CALL_COMPLEX_TYPES(REGISTER_GPU);\n#undef REGISTER_GPU\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n// GRADIENT *******************************************************************\n// Note that this op may have an optional third input. If present, it represents\n// a shape value. It indicates that element shape of this gradient array is that\n// shape value concatenated with the element shape of the original tensor array.\n// See TensorArrayGradWithShape.\nclass TensorArrayGradOp : public TensorArrayCreationOp {\n public:\n  explicit TensorArrayGradOp(OpKernelConstruction* context)\n      : TensorArrayCreationOp(context) {\n    OP_REQUIRES_OK(context, context->GetAttr(\"source\", &source_));\n  }\n\n  Status CreateTensorArray(OpKernelContext* ctx, ResourceMgr* rm,\n                           Tensor* tensor_array_output_handle,\n                           TensorArray** output_tensor_array) override {\n    string container;\n    string tensor_array_name;\n    if (ctx->input_dtype(0) != DT_RESOURCE) {\n      TF_RETURN_IF_ERROR(GetHandle(ctx, &container, &tensor_array_name));\n      if (container != \"_tensor_arrays\") {\n        return errors::InvalidArgument(\n            \"Input container should be '_tensor_arrays',  but received '\",\n            container, \"'\");\n      }\n    } else {\n      container = \"_tensor_arrays\";\n      const auto& resource = ctx->input(0).flat<ResourceHandle>()(0);\n      if (StringPiece(resource.name()).substr(0, container.size()) !=\n          container) {\n        return errors::InvalidArgument(\"Wrong input container. \",\n                                       resource.name());\n      }\n      tensor_array_name =\n          string(StringPiece(resource.name()).substr(container.size()));\n    }\n\n    auto output_handle = tensor_array_output_handle->flat<tstring>();\n    output_handle(0) = \"_tensor_array_grads\";\n    output_handle(1) = strings::StrCat(tensor_array_name, \"@\", source_);\n\n    TensorArray* tensor_array;\n    TF_RETURN_IF_ERROR(ctx->step_container()->Lookup(\n        rm, strings::StrCat(container, tensor_array_name), &tensor_array));\n    core::ScopedUnref unref(tensor_array);\n\n    // Once gradients are being calculated, the forward TensorArray\n    // may no longer be resized by new Writes.\n    tensor_array->DisableDynamicSize();\n\n    int32_t array_size = 0;\n    int32_t marked_size = 0;\n    TF_RETURN_IF_ERROR(tensor_array->Size(&array_size));\n    TF_RETURN_IF_ERROR(tensor_array->MarkedSize(&marked_size));\n\n    if (array_size < 0) {\n      return errors::InvalidArgument(\"ArraySize should be >= 0.\");\n    }\n    if (!tensor_array->GradientsAllowed()) {\n      return errors::InvalidArgument(\n          \"Unable to create a gradients TensorArray for \", tensor_array_name,\n          \".  Perhaps you used the multiple_writes_aggregate flag on a \"\n          \"previous write?  Gradient calculation is impossible when multiple \"\n          \"writes are performed to the same index.\");\n    }\n    TensorShape shape_to_prepend;\n    auto element_shape = PartialTensorShape();\n    if (ctx->num_inputs() > 2) {\n      TF_RETURN_IF_ERROR(tensor::MakeShape(ctx->input(2), &shape_to_prepend));\n      auto ta_element_shape = tensor_array->ElemShape();\n      if (!ta_element_shape.unknown_rank()) {\n        std::vector<int64_t> dims;\n        for (auto dim : shape_to_prepend) {\n          dims.push_back(dim.size);\n        }\n        for (auto dim : ta_element_shape) {\n          dims.push_back(dim.size);\n        }\n        TF_RETURN_IF_ERROR(TensorShapeUtils::MakeShape(\n            gtl::ArraySlice<int64_t>(dims), &element_shape));\n      }\n    } else {\n      element_shape = tensor_array->ElemShape();\n    }\n\n    const auto key = strings::StrCat(output_handle(0), output_handle(1));\n    auto creator = [key, tensor_array, array_size, marked_size, element_shape,\n                    shape_to_prepend,\n                    tensor_array_output_handle](TensorArray** ret) -> Status {\n      *ret = new TensorArray(\n          key, tensor_array->ElemType(), *tensor_array_output_handle,\n          array_size, element_shape, tensor_array->HasIdenticalElementShapes(),\n          false /* dynamic_size */, true /* multiple_writes_aggregate */,\n          true /* is_grad */, marked_size /* marked_size */,\n          true /* close_after_read */);\n      return (*ret)->CopyShapesFrom(tensor_array, &shape_to_prepend);\n    };\n\n    Status s = ctx->step_container()->LookupOrCreate<TensorArray>(\n        rm, key, output_tensor_array, creator);\n    (*output_tensor_array)->Unref();\n\n    return s;\n  }\n\n private:\n  // The gradient source for creating the given\n  // gradient TensorArray.  This should be unique to each gradients\n  // call.  Typical values look like \"gradients\", \"gradients_1\", ...\n  string source_;\n\n  TF_DISALLOW_COPY_AND_ASSIGN(TensorArrayGradOp);\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"TensorArrayGrad\").Device(DEVICE_CPU),\n                        TensorArrayGradOp);\nREGISTER_KERNEL_BUILDER(Name(\"TensorArrayGradV2\").Device(DEVICE_CPU),\n                        TensorArrayGradOp);\nREGISTER_KERNEL_BUILDER(Name(\"TensorArrayGradV3\").Device(DEVICE_CPU),\n                        TensorArrayGradOp);\nREGISTER_KERNEL_BUILDER(Name(\"TensorArrayGradWithShape\").Device(DEVICE_CPU),\n                        TensorArrayGradOp);\nREGISTER_KERNEL_BUILDER(Name(\"TensorArrayGrad\")\n                            .Device(DEVICE_GPU)\n                            .HostMemory(\"handle\")\n                            .HostMemory(\"grad_handle\"),\n                        TensorArrayGradOp);\nREGISTER_KERNEL_BUILDER(Name(\"TensorArrayGradV2\")\n                            .Device(DEVICE_GPU)\n                            .HostMemory(\"handle\")\n                            .HostMemory(\"grad_handle\"),\n                        TensorArrayGradOp);\nREGISTER_KERNEL_BUILDER(Name(\"TensorArrayGradV3\")\n                            .Device(DEVICE_GPU)\n                            .HostMemory(\"handle\")\n                            .HostMemory(\"grad_handle\"),\n                        TensorArrayGradOp);\nREGISTER_KERNEL_BUILDER(Name(\"TensorArrayGradWithShape\")\n                            .Device(DEVICE_GPU)\n                            .HostMemory(\"handle\")\n                            .HostMemory(\"shape_to_prepend\")\n                            .HostMemory(\"grad_handle\"),\n                        TensorArrayGradOp);\n\n// WRITE **********************************************************************\n\ntemplate <typename Device, typename T>\nclass TensorArrayWriteOp : public OpKernel {\n public:\n  explicit TensorArrayWriteOp(OpKernelConstruction* context)\n      : OpKernel(context) {}\n\n  void Compute(OpKernelContext* ctx) override {\n    OP_REQUIRES_OK(ctx, SetupFlowControlInputs(ctx, true));\n\n    const Tensor* tensor_index;\n    const Tensor* tensor_value;\n    OP_REQUIRES_OK(ctx, ctx->input(\"index\", &tensor_index));\n    OP_REQUIRES_OK(ctx, ctx->input(\"value\", &tensor_value));\n\n    OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(tensor_index->shape()),\n                errors::InvalidArgument(\n                    \"TensorArray index must be scalar, but had shape: \",\n                    tensor_index->shape().DebugString()));\n\n    TensorArray* tensor_array = nullptr;\n    OP_REQUIRES_OK(ctx, GetTensorArray(ctx, &tensor_array));\n    core::ScopedUnref unref(tensor_array);\n    const int32_t index = tensor_index->scalar<int32>()();\n    OP_REQUIRES(\n        ctx, tensor_value->dtype() == tensor_array->ElemType(),\n        errors::InvalidArgument(\"TensorArray dtype is \",\n                                DataTypeString(tensor_array->ElemType()),\n                                \" but Op is trying to write dtype \",\n                                DataTypeString(tensor_value->dtype()), \".\"));\n    Status s =\n        tensor_array->WriteOrAggregate<Device, T>(ctx, index, tensor_value);\n    OP_REQUIRES_OK(ctx, s);\n  }\n};\n\n#define REGISTER_WRITE(type)                                                   \\\n  REGISTER_KERNEL_BUILDER(                                                     \\\n      Name(\"TensorArrayWrite\").Device(DEVICE_CPU).TypeConstraint<type>(\"T\"),   \\\n      TensorArrayWriteOp<CPUDevice, type>);                                    \\\n  REGISTER_KERNEL_BUILDER(                                                     \\\n      Name(\"TensorArrayWriteV2\").Device(DEVICE_CPU).TypeConstraint<type>(\"T\"), \\\n      TensorArrayWriteOp<CPUDevice, type>);                                    \\\n  REGISTER_KERNEL_BUILDER(                                                     \\\n      Name(\"TensorArrayWriteV3\").Device(DEVICE_CPU).TypeConstraint<type>(\"T\"), \\\n      TensorArrayWriteOp<CPUDevice, type>);\n\nTF_CALL_ALL_TYPES(REGISTER_WRITE);\n\n#undef REGISTER_WRITE\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n#define REGISTER_GPU(type)                                      \\\n  REGISTER_KERNEL_BUILDER(Name(\"TensorArrayWrite\")              \\\n                              .Device(DEVICE_GPU)               \\\n                              .TypeConstraint<type>(\"T\")        \\\n                              .HostMemory(\"handle\")             \\\n                              .HostMemory(\"index\"),             \\\n                          TensorArrayWriteOp<GPUDevice, type>); \\\n  REGISTER_KERNEL_BUILDER(Name(\"TensorArrayWriteV2\")            \\\n                              .Device(DEVICE_GPU)               \\\n                              .TypeConstraint<type>(\"T\")        \\\n                              .HostMemory(\"handle\")             \\\n                              .HostMemory(\"index\"),             \\\n                          TensorArrayWriteOp<GPUDevice, type>); \\\n  REGISTER_KERNEL_BUILDER(Name(\"TensorArrayWriteV3\")            \\\n                              .Device(DEVICE_GPU)               \\\n                              .TypeConstraint<type>(\"T\")        \\\n                              .HostMemory(\"handle\")             \\\n                              .HostMemory(\"index\"),             \\\n                          TensorArrayWriteOp<GPUDevice, type>);\n\nTF_CALL_GPU_NUMBER_TYPES(REGISTER_GPU);\nTF_CALL_COMPLEX_TYPES(REGISTER_GPU);\n#undef REGISTER_GPU\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n// READ ***********************************************************************\n\ntemplate <typename Device, typename T>\nclass TensorArrayReadOp : public OpKernel {\n public:\n  explicit TensorArrayReadOp(OpKernelConstruction* context)\n      : OpKernel(context) {\n    OP_REQUIRES_OK(context, context->GetAttr(\"dtype\", &dtype_));\n  }\n\n  void Compute(OpKernelContext* ctx) override {\n    OP_REQUIRES_OK(ctx, SetupFlowControlInputs(ctx, false));\n\n    const Tensor* tensor_index;\n    OP_REQUIRES_OK(ctx, ctx->input(\"index\", &tensor_index));\n\n    OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(tensor_index->shape()),\n                errors::InvalidArgument(\n                    \"TensorArray index must be scalar, but had shape: \",\n                    tensor_index->shape().DebugString()));\n\n    TensorArray* tensor_array = nullptr;\n    OP_REQUIRES_OK(ctx, GetTensorArray(ctx, &tensor_array));\n    core::ScopedUnref unref(tensor_array);\n\n    const int32_t index = tensor_index->scalar<int32>()();\n    OP_REQUIRES(\n        ctx, dtype_ == tensor_array->ElemType(),\n        errors::InvalidArgument(\n            \"TensorArray dtype is \", DataTypeString(tensor_array->ElemType()),\n            \" but Op requested dtype \", DataTypeString(dtype_), \".\"));\n    Tensor value;\n    Status s = tensor_array->Read<Device, T>(ctx, index, &value);\n    OP_REQUIRES_OK(ctx, s);\n    ctx->set_output(0, value);\n  }\n\n private:\n  DataType dtype_;\n};\n\n#define REGISTER_READ(type)                                    \\\n  REGISTER_KERNEL_BUILDER(Name(\"TensorArrayRead\")              \\\n                              .Device(DEVICE_CPU)              \\\n                              .TypeConstraint<type>(\"dtype\"),  \\\n                          TensorArrayReadOp<CPUDevice, type>); \\\n  REGISTER_KERNEL_BUILDER(Name(\"TensorArrayReadV2\")            \\\n                              .Device(DEVICE_CPU)              \\\n                              .TypeConstraint<type>(\"dtype\"),  \\\n                          TensorArrayReadOp<CPUDevice, type>); \\\n  REGISTER_KERNEL_BUILDER(Name(\"TensorArrayReadV3\")            \\\n                              .Device(DEVICE_CPU)              \\\n                              .TypeConstraint<type>(\"dtype\"),  \\\n                          TensorArrayReadOp<CPUDevice, type>);\n\nTF_CALL_ALL_TYPES(REGISTER_READ)\n\n#undef REGISTER_READ\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n#define REGISTER_GPU(type)                                     \\\n  REGISTER_KERNEL_BUILDER(Name(\"TensorArrayRead\")              \\\n                              .Device(DEVICE_GPU)              \\\n                              .TypeConstraint<type>(\"dtype\")   \\\n                              .HostMemory(\"handle\")            \\\n                              .HostMemory(\"index\"),            \\\n                          TensorArrayReadOp<GPUDevice, type>); \\\n  REGISTER_KERNEL_BUILDER(Name(\"TensorArrayReadV2\")            \\\n                              .Device(DEVICE_GPU)              \\\n                              .TypeConstraint<type>(\"dtype\")   \\\n                              .HostMemory(\"handle\")            \\\n                              .HostMemory(\"index\"),            \\\n                          TensorArrayReadOp<GPUDevice, type>); \\\n  REGISTER_KERNEL_BUILDER(Name(\"TensorArrayReadV3\")            \\\n                              .Device(DEVICE_GPU)              \\\n                              .TypeConstraint<type>(\"dtype\")   \\\n                              .HostMemory(\"handle\")            \\\n                              .HostMemory(\"index\"),            \\\n                          TensorArrayReadOp<GPUDevice, type>);\n\nTF_CALL_int64(REGISTER_GPU);\nTF_CALL_GPU_NUMBER_TYPES(REGISTER_GPU);\nTF_CALL_COMPLEX_TYPES(REGISTER_GPU);\n#undef REGISTER_GPU\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n// PACK and GATHER ************************************************************\n\n// Concatenate the elements in a TensorArray.  All elements must be\n// defined and have the same shape.\ntemplate <typename Device, typename T, bool LEGACY_PACK>\nclass TensorArrayPackOrGatherOp : public OpKernel {\n public:\n  typedef typename TTypes<T, 2>::ConstMatrix ConstMatrix;\n  typedef std::vector<std::unique_ptr<ConstMatrix> > ConstMatrixVector;\n\n  explicit TensorArrayPackOrGatherOp(OpKernelConstruction* context)\n      : OpKernel(context) {\n    OP_REQUIRES_OK(context, context->GetAttr(\"dtype\", &dtype_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"element_shape\", &element_shape_));\n  }\n\n  void Compute(OpKernelContext* ctx) override {\n    OP_REQUIRES_OK(ctx, SetupFlowControlInputs(ctx, false));\n\n    TensorArray* tensor_array = nullptr;\n    OP_REQUIRES_OK(ctx, GetTensorArray(ctx, &tensor_array));\n\n    core::ScopedUnref unref(tensor_array);\n    OP_REQUIRES(\n        ctx, dtype_ == tensor_array->ElemType(),\n        errors::InvalidArgument(\n            \"TensorArray dtype is \", DataTypeString(tensor_array->ElemType()),\n            \" but Op requested dtype \", DataTypeString(dtype_), \".\"));\n\n    // Ensure new element shape is compatible with the one stored in the\n    // TensorArray.\n    OP_REQUIRES_OK(ctx, tensor_array->SetElemShape(element_shape_));\n\n    int32_t num_indices;\n    std::vector<Tensor> values;\n    std::vector<int32> indices;\n    if (LEGACY_PACK) {\n      OP_REQUIRES_OK(ctx, tensor_array->PackOrConcatSize(&num_indices));\n      indices.resize(num_indices);\n      std::iota(indices.begin(), indices.end(), 0);\n    } else {\n      const Tensor* tensor_indices;\n      OP_REQUIRES_OK(ctx, ctx->input(\"indices\", &tensor_indices));\n      OP_REQUIRES(ctx, TensorShapeUtils::IsVector(tensor_indices->shape()),\n                  errors::InvalidArgument(\n                      \"Expected indices to be a vector, but received shape: \",\n                      tensor_indices->shape().DebugString()));\n      const auto indices_t = tensor_indices->vec<int32>();\n      num_indices = tensor_indices->NumElements();\n      indices.resize(num_indices);\n      std::copy(indices_t.data(), indices_t.data() + num_indices,\n                indices.begin());\n    }\n\n    // If there are no elements to return, return a zero-element Tensor with\n    // shape [0] + element_shape_\n    if (num_indices == 0) {\n      OP_REQUIRES(ctx, element_shape_.IsFullyDefined(),\n                  errors::Unimplemented(\n                      \"TensorArray has size zero, but element shape \",\n                      element_shape_.DebugString(),\n                      \" is not fully defined. \"\n                      \"Currently only static shapes are supported when packing \"\n                      \"zero-size TensorArrays.\"));\n      TensorShape empty_shape;\n      element_shape_.AsTensorShape(&empty_shape);\n      empty_shape.InsertDim(0, 0);\n      Tensor* empty_unused;\n      OP_REQUIRES_OK(ctx, ctx->allocate_output(0, empty_shape, &empty_unused));\n      return;\n    }\n\n    // Read all the Tensors into a vector to keep track of their memory.\n    Status s = tensor_array->ReadMany<Device, T>(ctx, indices, &values);\n    OP_REQUIRES_OK(ctx, s);\n\n    const Tensor* value_0_t = &values[0];\n\n    OP_REQUIRES(\n        ctx, element_shape_.IsCompatibleWith(value_0_t->shape()),\n        errors::InvalidArgument(\"TensorArray was passed element_shape \",\n                                element_shape_.DebugString(),\n                                \" which does not match the Tensor at index 0: \",\n                                value_0_t->shape().DebugString()));\n\n    TensorShape output_shape(value_0_t->shape());\n    output_shape.InsertDim(0, num_indices);\n\n    Tensor* output_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, output_shape, &output_tensor));\n\n    // If output_tensor is empty, there is nothing to concatenate so return it.\n    if (output_shape.num_elements() == 0) {\n      return;\n    }\n\n    ConstMatrixVector input_tensors_flat;\n    input_tensors_flat.reserve(num_indices);\n    auto output_flat =\n        output_tensor->shaped<T, 2>({1, output_shape.num_elements()});\n\n    // Insert the first value\n    input_tensors_flat.push_back(MakeUnique<ConstMatrix>(\n        value_0_t->shaped<T, 2>({1, value_0_t->NumElements()})));\n\n    for (int i = 1; i < num_indices; ++i) {\n      const Tensor* value_t = &values[i];\n      OP_REQUIRES(\n          ctx, value_0_t->shape() == value_t->shape(),\n          errors::InvalidArgument(\n              \"TensorArray has inconsistent shapes.  Index 0 has shape: \",\n              value_0_t->shape().DebugString(), \" but index \", i,\n              \" has shape: \", value_t->shape().DebugString()));\n      input_tensors_flat.push_back(MakeUnique<ConstMatrix>(\n          value_t->shaped<T, 2>({1, value_t->NumElements()})));\n    }\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n    if (std::is_same<Device, GPUDevice>::value) {\n      ConcatGPU<T>(ctx, input_tensors_flat, output_tensor, &output_flat);\n      return;\n    }\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n    ConcatCPU<T>(ctx->device(), input_tensors_flat, &output_flat);\n  }\n\n private:\n  DataType dtype_;\n  PartialTensorShape element_shape_;\n};\n\n#define REGISTER_GATHER_AND_PACK(type)                                      \\\n  REGISTER_KERNEL_BUILDER(                                                  \\\n      Name(\"TensorArrayPack\")                                               \\\n          .Device(DEVICE_CPU)                                               \\\n          .TypeConstraint<type>(\"dtype\"),                                   \\\n      TensorArrayPackOrGatherOp<CPUDevice, type, true /* LEGACY_PACK */>);  \\\n  REGISTER_KERNEL_BUILDER(                                                  \\\n      Name(\"TensorArrayGather\")                                             \\\n          .Device(DEVICE_CPU)                                               \\\n          .TypeConstraint<type>(\"dtype\"),                                   \\\n      TensorArrayPackOrGatherOp<CPUDevice, type, false /* LEGACY_PACK */>); \\\n  REGISTER_KERNEL_BUILDER(                                                  \\\n      Name(\"TensorArrayGatherV2\")                                           \\\n          .Device(DEVICE_CPU)                                               \\\n          .TypeConstraint<type>(\"dtype\"),                                   \\\n      TensorArrayPackOrGatherOp<CPUDevice, type, false /* LEGACY_PACK */>); \\\n  REGISTER_KERNEL_BUILDER(                                                  \\\n      Name(\"TensorArrayGatherV3\")                                           \\\n          .Device(DEVICE_CPU)                                               \\\n          .TypeConstraint<type>(\"dtype\"),                                   \\\n      TensorArrayPackOrGatherOp<CPUDevice, type, false /* LEGACY_PACK */>);\n\nTF_CALL_POD_STRING_TYPES(REGISTER_GATHER_AND_PACK);\nTF_CALL_variant(REGISTER_GATHER_AND_PACK);\nREGISTER_GATHER_AND_PACK(quint8);\nREGISTER_GATHER_AND_PACK(qint8);\nREGISTER_GATHER_AND_PACK(qint32);\n\n#undef REGISTER_GATHER_AND_PACK\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n#define REGISTER_GPU(type)                                                  \\\n  REGISTER_KERNEL_BUILDER(                                                  \\\n      Name(\"TensorArrayPack\")                                               \\\n          .Device(DEVICE_GPU)                                               \\\n          .TypeConstraint<type>(\"dtype\")                                    \\\n          .HostMemory(\"handle\"),                                            \\\n      TensorArrayPackOrGatherOp<GPUDevice, type, true /* LEGACY_PACK */>);  \\\n  REGISTER_KERNEL_BUILDER(                                                  \\\n      Name(\"TensorArrayGather\")                                             \\\n          .Device(DEVICE_GPU)                                               \\\n          .TypeConstraint<type>(\"dtype\")                                    \\\n          .HostMemory(\"indices\")                                            \\\n          .HostMemory(\"handle\"),                                            \\\n      TensorArrayPackOrGatherOp<GPUDevice, type, false /* LEGACY_PACK */>); \\\n  REGISTER_KERNEL_BUILDER(                                                  \\\n      Name(\"TensorArrayGatherV2\")                                           \\\n          .Device(DEVICE_GPU)                                               \\\n          .TypeConstraint<type>(\"dtype\")                                    \\\n          .HostMemory(\"indices\")                                            \\\n          .HostMemory(\"handle\"),                                            \\\n      TensorArrayPackOrGatherOp<GPUDevice, type, false /* LEGACY_PACK */>); \\\n  REGISTER_KERNEL_BUILDER(                                                  \\\n      Name(\"TensorArrayGatherV3\")                                           \\\n          .Device(DEVICE_GPU)                                               \\\n          .TypeConstraint<type>(\"dtype\")                                    \\\n          .HostMemory(\"indices\")                                            \\\n          .HostMemory(\"handle\"),                                            \\\n      TensorArrayPackOrGatherOp<GPUDevice, type, false /* LEGACY_PACK */>);\n\nTF_CALL_GPU_NUMBER_TYPES(REGISTER_GPU);\nTF_CALL_COMPLEX_TYPES(REGISTER_GPU);\n#undef REGISTER_GPU\n\n// A special GPU kernel for int32.\n// TODO(b/25387198): Also enable int32 in device memory. This kernel\n// registration requires all int32 inputs and outputs to be in host memory.\nREGISTER_KERNEL_BUILDER(\n    Name(\"TensorArrayGather\")\n        .Device(DEVICE_GPU)\n        .TypeConstraint<int32>(\"dtype\")\n        .HostMemory(\"indices\")\n        .HostMemory(\"handle\"),\n    TensorArrayPackOrGatherOp<CPUDevice, int32, false /* LEGACY_PACK */>);\nREGISTER_KERNEL_BUILDER(\n    Name(\"TensorArrayGatherV2\")\n        .Device(DEVICE_GPU)\n        .TypeConstraint<int32>(\"dtype\")\n        .HostMemory(\"indices\")\n        .HostMemory(\"handle\"),\n    TensorArrayPackOrGatherOp<CPUDevice, int32, false /* LEGACY_PACK */>);\nREGISTER_KERNEL_BUILDER(\n    Name(\"TensorArrayGatherV3\")\n        .Device(DEVICE_GPU)\n        .TypeConstraint<int32>(\"dtype\")\n        .HostMemory(\"indices\")\n        .HostMemory(\"handle\"),\n    TensorArrayPackOrGatherOp<CPUDevice, int32, false /* LEGACY_PACK */>);\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n// CONCAT *********************************************************************\n\n// Concatenate the elements in a TensorArray.  All elements must be\n// defined and (excepting the first dimension) have the same shape.\ntemplate <typename Device, typename T>\nclass TensorArrayConcatOp : public OpKernel {\n public:\n  typedef typename TTypes<T, 2>::ConstMatrix ConstMatrix;\n  typedef std::vector<std::unique_ptr<ConstMatrix> > ConstMatrixVector;\n\n  explicit TensorArrayConcatOp(OpKernelConstruction* context)\n      : OpKernel(context) {\n    OP_REQUIRES_OK(context, context->GetAttr(\"dtype\", &dtype_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"element_shape_except0\",\n                                             &element_shape_except0_));\n  }\n\n  void Compute(OpKernelContext* ctx) override {\n    OP_REQUIRES_OK(ctx, SetupFlowControlInputs(ctx, false));\n\n    TensorArray* tensor_array = nullptr;\n    OP_REQUIRES_OK(ctx, GetTensorArray(ctx, &tensor_array));\n    core::ScopedUnref unref(tensor_array);\n    OP_REQUIRES(\n        ctx, dtype_ == tensor_array->ElemType(),\n        errors::InvalidArgument(\n            \"TensorArray dtype is \", DataTypeString(tensor_array->ElemType()),\n            \" but Op requested dtype \", DataTypeString(dtype_), \".\"));\n\n    int32_t array_size;\n    OP_REQUIRES_OK(ctx, tensor_array->PackOrConcatSize(&array_size));\n\n    // If there are no elements, return a zero-element Tensor with\n    // shape [0] + element_shape_except0_\n    if (array_size == 0) {\n      OP_REQUIRES(\n          ctx, element_shape_except0_.IsFullyDefined(),\n          errors::Unimplemented(\n              \"TensorArray has size zero, but element_shape_except0 \",\n              element_shape_except0_.DebugString(),\n              \" is not fully defined. \"\n              \"Currently only static shapes are supported when concatenating \"\n              \"zero-size TensorArrays.\"));\n      TensorShape empty_shape;\n      element_shape_except0_.AsTensorShape(&empty_shape);\n      empty_shape.InsertDim(0, 0);\n      Tensor* empty_unused;\n      OP_REQUIRES_OK(ctx, ctx->allocate_output(0, empty_shape, &empty_unused));\n      OP_REQUIRES_OK(ctx, ctx->allocate_output(1, {0}, &empty_unused));\n      return;\n    }\n\n    // Read all the Tensors into a vector to keep track of their memory.\n    std::vector<Tensor> values;\n    std::vector<int32> indices(array_size);\n    std::iota(indices.begin(), indices.end(), 0);\n    Status s = tensor_array->ReadMany<Device, T>(ctx, indices, &values);\n    OP_REQUIRES_OK(ctx, s);\n\n    Tensor* lengths_tensor = nullptr;\n    OP_REQUIRES_OK(ctx,\n                   ctx->allocate_output(\n                       1, TensorShape({static_cast<int64_t>(values.size())}),\n                       &lengths_tensor));\n    auto lengths_tensor_t = lengths_tensor->vec<int64_t>();\n\n    TensorShape output_shape;\n    TensorShape output_shape_except0;\n    for (std::size_t i = 0; i < values.size(); ++i) {\n      TensorShape value_shape_t = values[i].shape();\n\n      OP_REQUIRES(\n          ctx, TensorShapeUtils::IsVectorOrHigher(value_shape_t),\n          errors::InvalidArgument(\n              \"Concat saw a scalar shape at index \", i,\n              \" but requires at least vectors.  Did you mean to call pack?\"));\n\n      lengths_tensor_t(i) = value_shape_t.dim_size(0);\n\n      TensorShape value_shape_t_except0 = value_shape_t;\n      value_shape_t_except0.RemoveDim(0);\n      if (i == 0) {\n        output_shape = value_shape_t;\n        output_shape_except0 = value_shape_t_except0;\n        OP_REQUIRES(\n            ctx, element_shape_except0_.IsCompatibleWith(output_shape_except0),\n            errors::InvalidArgument(\n                \"TensorArray was passed element_shape_except0 \",\n                element_shape_except0_.DebugString(),\n                \" but index 0 has (excepting dimension 0) shape: \",\n                value_shape_t_except0.DebugString(), \" which does not match.\"));\n      } else {\n        OP_REQUIRES(ctx, output_shape_except0 == value_shape_t_except0,\n                    errors::InvalidArgument(\n                        \"TensorArray has inconsistent shapes.  Index 0 has \"\n                        \"(excepting dimension 0) shape: \",\n                        output_shape_except0.DebugString(), \" but index \", i,\n                        \" has (excepting dimension 0) shape: \",\n                        value_shape_t_except0.DebugString()));\n        // Store the previous maximum length as the offset for this tensor.\n        output_shape.set_dim(\n            0, output_shape.dim_size(0) + value_shape_t.dim_size(0));\n      }\n    }\n\n    Tensor* output_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, output_shape, &output_tensor));\n    ConstMatrixVector input_tensors_flat;\n    input_tensors_flat.reserve(values.size());\n    for (size_t i = 0; i < values.size(); ++i) {\n      const Tensor* value_t = &values[i];\n      if (value_t->NumElements() > 0) {\n        input_tensors_flat.push_back(MakeUnique<ConstMatrix>(\n            value_t->shaped<T, 2>({1, value_t->NumElements()})));\n      }\n    }\n\n    if (output_shape.num_elements() > 0) {\n      auto output_flat =\n          output_tensor->shaped<T, 2>({1, output_shape.num_elements()});\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n      if (std::is_same<Device, GPUDevice>::value) {\n        ConcatGPU<T>(ctx, input_tensors_flat, output_tensor, &output_flat);\n        return;\n      }\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n      ConcatCPU<T>(ctx->device(), input_tensors_flat, &output_flat);\n    }\n  }\n\n private:\n  DataType dtype_;\n  PartialTensorShape element_shape_except0_;\n};\n\n#define REGISTER_CONCAT(type)                                    \\\n  REGISTER_KERNEL_BUILDER(Name(\"TensorArrayConcat\")              \\\n                              .Device(DEVICE_CPU)                \\\n                              .TypeConstraint<type>(\"dtype\")     \\\n                              .HostMemory(\"lengths\")             \\\n                              .HostMemory(\"handle\"),             \\\n                          TensorArrayConcatOp<CPUDevice, type>); \\\n  REGISTER_KERNEL_BUILDER(Name(\"TensorArrayConcatV2\")            \\\n                              .Device(DEVICE_CPU)                \\\n                              .TypeConstraint<type>(\"dtype\")     \\\n                              .HostMemory(\"lengths\")             \\\n                              .HostMemory(\"handle\"),             \\\n                          TensorArrayConcatOp<CPUDevice, type>)  \\\n  REGISTER_KERNEL_BUILDER(Name(\"TensorArrayConcatV3\")            \\\n                              .Device(DEVICE_CPU)                \\\n                              .TypeConstraint<type>(\"dtype\")     \\\n                              .HostMemory(\"lengths\")             \\\n                              .HostMemory(\"handle\"),             \\\n                          TensorArrayConcatOp<CPUDevice, type>)\n\nTF_CALL_POD_STRING_TYPES(REGISTER_CONCAT);\nREGISTER_CONCAT(quint8);\nREGISTER_CONCAT(qint8);\nREGISTER_CONCAT(qint32);\n\n#undef REGISTER_CONCAT\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n#define REGISTER_GPU(type)                                       \\\n  REGISTER_KERNEL_BUILDER(Name(\"TensorArrayConcat\")              \\\n                              .Device(DEVICE_GPU)                \\\n                              .TypeConstraint<type>(\"dtype\")     \\\n                              .HostMemory(\"lengths\")             \\\n                              .HostMemory(\"handle\"),             \\\n                          TensorArrayConcatOp<GPUDevice, type>); \\\n  REGISTER_KERNEL_BUILDER(Name(\"TensorArrayConcatV2\")            \\\n                              .Device(DEVICE_GPU)                \\\n                              .TypeConstraint<type>(\"dtype\")     \\\n                              .HostMemory(\"lengths\")             \\\n                              .HostMemory(\"handle\"),             \\\n                          TensorArrayConcatOp<GPUDevice, type>)  \\\n  REGISTER_KERNEL_BUILDER(Name(\"TensorArrayConcatV3\")            \\\n                              .Device(DEVICE_GPU)                \\\n                              .TypeConstraint<type>(\"dtype\")     \\\n                              .HostMemory(\"lengths\")             \\\n                              .HostMemory(\"handle\"),             \\\n                          TensorArrayConcatOp<GPUDevice, type>)\n\nTF_CALL_GPU_NUMBER_TYPES(REGISTER_GPU);\nTF_CALL_COMPLEX_TYPES(REGISTER_GPU);\n#undef REGISTER_GPU\n\n// A special GPU kernel for int32.\n// TODO(b/25387198): Also enable int32 in device memory. This kernel\n// registration requires all int32 inputs and outputs to be in host memory.\nREGISTER_KERNEL_BUILDER(Name(\"TensorArrayConcat\")\n                            .Device(DEVICE_GPU)\n                            .TypeConstraint<int32>(\"dtype\")\n                            .HostMemory(\"lengths\")\n                            .HostMemory(\"handle\"),\n                        TensorArrayConcatOp<CPUDevice, int32>);\nREGISTER_KERNEL_BUILDER(Name(\"TensorArrayConcatV2\")\n                            .Device(DEVICE_GPU)\n                            .TypeConstraint<int32>(\"dtype\")\n                            .HostMemory(\"lengths\")\n                            .HostMemory(\"handle\"),\n                        TensorArrayConcatOp<CPUDevice, int32>);\nREGISTER_KERNEL_BUILDER(Name(\"TensorArrayConcatV3\")\n                            .Device(DEVICE_GPU)\n                            .TypeConstraint<int32>(\"dtype\")\n                            .HostMemory(\"lengths\")\n                            .HostMemory(\"handle\"),\n                        TensorArrayConcatOp<CPUDevice, int32>);\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n// UNPACK and SCATTER *********************************************************\n\ntemplate <typename Device, typename T, bool LEGACY_UNPACK>\nclass TensorArrayUnpackOrScatterOp : public OpKernel {\n public:\n  explicit TensorArrayUnpackOrScatterOp(OpKernelConstruction* context)\n      : OpKernel(context) {}\n\n  void Compute(OpKernelContext* ctx) override {\n    OP_REQUIRES_OK(ctx, SetupFlowControlInputs(ctx, true));\n\n    TensorArray* tensor_array = nullptr;\n    OP_REQUIRES_OK(ctx, GetTensorArray(ctx, &tensor_array));\n    core::ScopedUnref unref(tensor_array);\n    const Tensor* tensor_value;\n    OP_REQUIRES_OK(ctx, ctx->input(\"value\", &tensor_value));\n    TensorShape element_shape(tensor_value->shape());\n\n    OP_REQUIRES(ctx,\n                FastBoundsCheck(element_shape.dim_size(0),\n                                std::numeric_limits<int32>::max()),\n                errors::InvalidArgument(\"tensor dim0 too large to unpack\"));\n\n    OP_REQUIRES(\n        ctx, tensor_value->dtype() == tensor_array->ElemType(),\n        errors::InvalidArgument(\"TensorArray dtype is \",\n                                DataTypeString(tensor_array->ElemType()),\n                                \" but Op is trying to write dtype \",\n                                DataTypeString(tensor_value->dtype()), \".\"));\n    OP_REQUIRES(ctx, element_shape.dims() > 0,\n                errors::InvalidArgument(\"Input value for unpack must be at \"\n                                        \"least a vector but received shape: \",\n                                        element_shape.DebugString()));\n    int32_t array_size;\n    OP_REQUIRES_OK(ctx, tensor_array->Size(&array_size));\n\n    int32_t max_index;\n    int32_t num_values;\n    std::vector<int32> write_indices;\n    if (LEGACY_UNPACK) {\n      num_values = element_shape.dim_size(0);\n      max_index = num_values - 1;\n      write_indices.resize(num_values);\n      std::iota(write_indices.begin(), write_indices.end(), 0);\n    } else {\n      const Tensor* tensor_indices;\n      OP_REQUIRES_OK(ctx, ctx->input(\"indices\", &tensor_indices));\n      OP_REQUIRES(ctx, TensorShapeUtils::IsVector(tensor_indices->shape()),\n                  errors::InvalidArgument(\n                      \"Expected indices to be a vector, but received shape: \",\n                      tensor_indices->shape().DebugString()));\n      OP_REQUIRES(ctx,\n                  tensor_indices->NumElements() == element_shape.dim_size(0),\n                  errors::InvalidArgument(\n                      \"Expected len(indices) == values.shape[0], but saw: \",\n                      tensor_indices->NumElements(), \" vs. \",\n                      element_shape.dim_size(0)));\n      const auto indices_t = tensor_indices->vec<int32>();\n      num_values = tensor_indices->NumElements();\n      max_index = (num_values == 0)\n                      ? -1\n                      : *std::max_element(indices_t.data(),\n                                          indices_t.data() + num_values);\n      write_indices.resize(num_values);\n      // Copy into write_indices.\n      std::copy(indices_t.data(), indices_t.data() + num_values,\n                write_indices.begin());\n    }\n\n    bool dynamic_size = tensor_array->HasDynamicSize();\n\n    // If dynamic size, we may have to resize the TensorArray to fit.\n    if (dynamic_size && array_size < max_index + 1) {\n      array_size = static_cast<int32>(max_index + 1);\n    }\n\n    if (LEGACY_UNPACK) {\n      OP_REQUIRES(\n          ctx, element_shape.dim_size(0) == array_size,\n          errors::InvalidArgument(\n              \"Input value must have first dimension equal to the array size (\",\n              element_shape.dim_size(0), \" vs. \", array_size, \")\"));\n    } else {\n      OP_REQUIRES(\n          ctx, max_index < array_size,\n          errors::InvalidArgument(\"Max scatter index must be < array size (\",\n                                  max_index, \" vs. \", array_size, \")\"));\n    }\n    element_shape.RemoveDim(0);\n\n    auto tensor_value_t = tensor_value->shaped<T, 3>(\n        {1, num_values, element_shape.num_elements()});\n\n    Eigen::DSizes<Eigen::DenseIndex, 3> indices{0, 0, 0};\n    Eigen::DSizes<Eigen::DenseIndex, 3> sizes{\n        1, 1, static_cast<Eigen::DenseIndex>(element_shape.num_elements())};\n\n    std::vector<Tensor> write_values;\n    write_values.reserve(num_values);\n\n    for (int i = 0; i < num_values; ++i) {\n      Tensor tensor_value_i;\n      OP_REQUIRES_OK(ctx, ctx->allocate_temp(tensor_array->ElemType(),\n                                             element_shape, &tensor_value_i));\n      auto tensor_value_i_t =\n          tensor_value_i.shaped<T, 3>({1, 1, element_shape.num_elements()});\n      indices[1] = i;\n\n      if (element_shape.num_elements() > 0) {\n        functor::Split<Device, T, 3>()(ctx->eigen_device<Device>(),\n                                       tensor_value_i_t, tensor_value_t,\n                                       indices, sizes);\n      }\n\n      write_values.push_back(tensor_value_i);\n    }\n\n    // Record the pack size of the TensorArray.\n    if (LEGACY_UNPACK) {\n      OP_REQUIRES_OK(ctx, tensor_array->SetMarkedSize(array_size));\n    }\n\n    Status s = tensor_array->WriteOrAggregateMany<Device, T>(ctx, write_indices,\n                                                             &write_values);\n    OP_REQUIRES_OK(ctx, s);\n  }\n};\n\n#define REGISTER_SCATTER_AND_UNPACK(type)                                      \\\n  REGISTER_KERNEL_BUILDER(                                                     \\\n      Name(\"TensorArrayUnpack\").Device(DEVICE_CPU).TypeConstraint<type>(\"T\"),  \\\n      TensorArrayUnpackOrScatterOp<CPUDevice, type,                            \\\n                                   true /* LEGACY_UNPACK */>);                 \\\n  REGISTER_KERNEL_BUILDER(                                                     \\\n      Name(\"TensorArrayScatter\").Device(DEVICE_CPU).TypeConstraint<type>(\"T\"), \\\n      TensorArrayUnpackOrScatterOp<CPUDevice, type,                            \\\n                                   false /* LEGACY_UNPACK */>);                \\\n  REGISTER_KERNEL_BUILDER(                                                     \\\n      Name(\"TensorArrayScatterV2\")                                             \\\n          .Device(DEVICE_CPU)                                                  \\\n          .TypeConstraint<type>(\"T\"),                                          \\\n      TensorArrayUnpackOrScatterOp<CPUDevice, type,                            \\\n                                   false /* LEGACY_UNPACK */>);                \\\n  REGISTER_KERNEL_BUILDER(                                                     \\\n      Name(\"TensorArrayScatterV3\")                                             \\\n          .Device(DEVICE_CPU)                                                  \\\n          .TypeConstraint<type>(\"T\"),                                          \\\n      TensorArrayUnpackOrScatterOp<CPUDevice, type,                            \\\n                                   false /* LEGACY_UNPACK */>);\n\nTF_CALL_ALL_TYPES(REGISTER_SCATTER_AND_UNPACK);\n#undef REGISTER_SCATTER_AND_UNPACK\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n#define REGISTER_GPU(type)                                      \\\n  REGISTER_KERNEL_BUILDER(                                      \\\n      Name(\"TensorArrayUnpack\")                                 \\\n          .Device(DEVICE_GPU)                                   \\\n          .TypeConstraint<type>(\"T\")                            \\\n          .HostMemory(\"handle\"),                                \\\n      TensorArrayUnpackOrScatterOp<GPUDevice, type,             \\\n                                   true /* LEGACY_UNPACK */>);  \\\n  REGISTER_KERNEL_BUILDER(                                      \\\n      Name(\"TensorArrayScatter\")                                \\\n          .Device(DEVICE_GPU)                                   \\\n          .TypeConstraint<type>(\"T\")                            \\\n          .HostMemory(\"indices\")                                \\\n          .HostMemory(\"handle\"),                                \\\n      TensorArrayUnpackOrScatterOp<GPUDevice, type,             \\\n                                   false /* LEGACY_UNPACK */>); \\\n  REGISTER_KERNEL_BUILDER(                                      \\\n      Name(\"TensorArrayScatterV2\")                              \\\n          .Device(DEVICE_GPU)                                   \\\n          .TypeConstraint<type>(\"T\")                            \\\n          .HostMemory(\"indices\")                                \\\n          .HostMemory(\"handle\"),                                \\\n      TensorArrayUnpackOrScatterOp<GPUDevice, type,             \\\n                                   false /* LEGACY_UNPACK */>); \\\n  REGISTER_KERNEL_BUILDER(                                      \\\n      Name(\"TensorArrayScatterV3\")                              \\\n          .Device(DEVICE_GPU)                                   \\\n          .TypeConstraint<type>(\"T\")                            \\\n          .HostMemory(\"indices\")                                \\\n          .HostMemory(\"handle\"),                                \\\n      TensorArrayUnpackOrScatterOp<GPUDevice, type,             \\\n                                   false /* LEGACY_UNPACK */>);\n\nTF_CALL_int64(REGISTER_GPU);\nTF_CALL_GPU_NUMBER_TYPES(REGISTER_GPU);\nTF_CALL_COMPLEX_TYPES(REGISTER_GPU);\n#undef REGISTER_GPU\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n// SPLIT *********************************************************************\n\ntemplate <typename Device, typename T>\nclass TensorArraySplitOp : public OpKernel {\n public:\n  explicit TensorArraySplitOp(OpKernelConstruction* context)\n      : OpKernel(context) {}\n\n  void Compute(OpKernelContext* ctx) override {\n    OP_REQUIRES_OK(ctx, SetupFlowControlInputs(ctx, true));\n\n    TensorArray* tensor_array = nullptr;\n    OP_REQUIRES_OK(ctx, GetTensorArray(ctx, &tensor_array));\n    core::ScopedUnref unref(tensor_array);\n    const Tensor* tensor_value;\n    OP_REQUIRES_OK(ctx, ctx->input(\"value\", &tensor_value));\n    const Tensor* tensor_lengths;\n    OP_REQUIRES_OK(ctx, ctx->input(\"lengths\", &tensor_lengths));\n\n    OP_REQUIRES(ctx, TensorShapeUtils::IsVector(tensor_lengths->shape()),\n                errors::InvalidArgument(\n                    \"Expected lengths to be a vector, received shape: \",\n                    tensor_lengths->shape().DebugString()));\n    OP_REQUIRES(ctx,\n                FastBoundsCheck(tensor_lengths->NumElements(),\n                                std::numeric_limits<int32>::max()),\n                errors::InvalidArgument(\n                    \"Expected lengths to have < max int32 entries\"));\n\n    int32_t num_tensors = static_cast<int32>(tensor_lengths->NumElements());\n    auto tensor_lengths_t = tensor_lengths->vec<int64_t>();\n    std::vector<int64_t> cumulative_lengths;\n    cumulative_lengths.reserve(num_tensors);\n    int64_t total_length = 0;\n    for (int i = 0; i < num_tensors; ++i) {\n      total_length += tensor_lengths_t(i);\n      cumulative_lengths.push_back(total_length);\n    }\n\n    OP_REQUIRES(\n        ctx, TensorShapeUtils::IsVectorOrHigher(tensor_value->shape()),\n        errors::InvalidArgument(\n            \"Expected value to be at least a vector, but received shape: \",\n            tensor_value->shape().DebugString()));\n\n    OP_REQUIRES(\n        ctx, total_length == tensor_value->shape().dim_size(0),\n        errors::InvalidArgument(\"Expected sum of lengths to be equal to \"\n                                \"values.shape[0], but sum of lengths is \",\n                                total_length, \" and value's shape is: \",\n                                tensor_value->shape().DebugString()));\n    int64_t elements_per_row =\n        (total_length == 0) ? 0 : (tensor_value->NumElements() / total_length);\n\n    int32_t array_size;\n    OP_REQUIRES_OK(ctx, tensor_array->Size(&array_size));\n    bool dynamic_size = tensor_array->HasDynamicSize();\n\n    std::vector<TensorShape> element_shapes(num_tensors, tensor_value->shape());\n    for (int32_t i = 0; i < num_tensors; ++i) {\n      element_shapes[i].set_dim(0, tensor_lengths_t(i));\n    }\n\n    // If dynamic size, we may have to resize the TensorArray to fit.\n    if (dynamic_size && array_size < num_tensors) {\n      array_size = num_tensors;\n    }\n\n    OP_REQUIRES(\n        ctx, array_size == num_tensors,\n        errors::InvalidArgument(\n            \"TensorArray's size is not equal to the size of lengths (\",\n            array_size, \" vs. \", num_tensors, \"), and the TensorArray is not \",\n            \"marked as dynamically resizeable\"));\n\n    OP_REQUIRES(\n        ctx, tensor_value->dtype() == tensor_array->ElemType(),\n        errors::InvalidArgument(\"TensorArray dtype is \",\n                                DataTypeString(tensor_array->ElemType()),\n                                \" but Op is trying to write dtype \",\n                                DataTypeString(tensor_value->dtype()), \".\"));\n\n    auto tensor_value_t =\n        tensor_value->shaped<T, 3>({1, total_length, elements_per_row});\n\n    std::vector<Tensor> write_values;\n    write_values.reserve(array_size);\n\n    for (int i = 0; i < array_size; ++i) {\n      Tensor tensor_value_i;\n\n      int64_t previous_length = (i == 0) ? 0 : cumulative_lengths[i - 1];\n      Eigen::DSizes<Eigen::DenseIndex, 3> indices{\n          0, static_cast<Eigen::DenseIndex>(previous_length), 0};\n      Eigen::DSizes<Eigen::DenseIndex, 3> sizes{\n          1, static_cast<Eigen::DenseIndex>(tensor_lengths_t(i)),\n          static_cast<Eigen::DenseIndex>(elements_per_row)};\n\n      OP_REQUIRES_OK(\n          ctx, ctx->allocate_temp(tensor_array->ElemType(), element_shapes[i],\n                                  &tensor_value_i));\n\n      if (tensor_lengths_t(i) > 0) {\n        auto tensor_value_i_t = tensor_value_i.shaped<T, 3>(\n            {1, tensor_lengths_t(i), elements_per_row});\n\n        functor::Split<Device, T, 3>()(ctx->eigen_device<Device>(),\n                                       tensor_value_i_t, tensor_value_t,\n                                       indices, sizes);\n      }\n\n      write_values.push_back(tensor_value_i);\n    }\n\n    // Record the concat size of the TensorArray.\n    OP_REQUIRES_OK(ctx, tensor_array->SetMarkedSize(array_size));\n\n    std::vector<int32> indices(array_size);\n    std::iota(indices.begin(), indices.end(), 0);\n\n    Status s = tensor_array->WriteOrAggregateMany<Device, T>(ctx, indices,\n                                                             &write_values);\n    OP_REQUIRES_OK(ctx, s);\n  }\n};\n\n#define REGISTER_SPLIT(type)                                                   \\\n  REGISTER_KERNEL_BUILDER(                                                     \\\n      Name(\"TensorArraySplit\").Device(DEVICE_CPU).TypeConstraint<type>(\"T\"),   \\\n      TensorArraySplitOp<CPUDevice, type>);                                    \\\n  REGISTER_KERNEL_BUILDER(                                                     \\\n      Name(\"TensorArraySplitV2\").Device(DEVICE_CPU).TypeConstraint<type>(\"T\"), \\\n      TensorArraySplitOp<CPUDevice, type>);                                    \\\n  REGISTER_KERNEL_BUILDER(                                                     \\\n      Name(\"TensorArraySplitV3\").Device(DEVICE_CPU).TypeConstraint<type>(\"T\"), \\\n      TensorArraySplitOp<CPUDevice, type>);\n\nTF_CALL_ALL_TYPES(REGISTER_SPLIT);\n#undef REGISTER_SPLIT\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n#define REGISTER_GPU(type)                                      \\\n  REGISTER_KERNEL_BUILDER(Name(\"TensorArraySplit\")              \\\n                              .Device(DEVICE_GPU)               \\\n                              .TypeConstraint<type>(\"T\")        \\\n                              .HostMemory(\"lengths\")            \\\n                              .HostMemory(\"handle\"),            \\\n                          TensorArraySplitOp<GPUDevice, type>); \\\n  REGISTER_KERNEL_BUILDER(Name(\"TensorArraySplitV2\")            \\\n                              .Device(DEVICE_GPU)               \\\n                              .TypeConstraint<type>(\"T\")        \\\n                              .HostMemory(\"lengths\")            \\\n                              .HostMemory(\"handle\"),            \\\n                          TensorArraySplitOp<GPUDevice, type>); \\\n  REGISTER_KERNEL_BUILDER(Name(\"TensorArraySplitV3\")            \\\n                              .Device(DEVICE_GPU)               \\\n                              .TypeConstraint<type>(\"T\")        \\\n                              .HostMemory(\"lengths\")            \\\n                              .HostMemory(\"handle\"),            \\\n                          TensorArraySplitOp<GPUDevice, type>);\n\nTF_CALL_GPU_NUMBER_TYPES(REGISTER_GPU);\nTF_CALL_COMPLEX_TYPES(REGISTER_GPU);\n#undef REGISTER_GPU\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n// SIZE ***********************************************************************\n\n// Get the size of the TensorArray\nclass TensorArraySizeOp : public OpKernel {\n public:\n  explicit TensorArraySizeOp(OpKernelConstruction* context)\n      : OpKernel(context) {}\n\n  void Compute(OpKernelContext* ctx) override {\n    TensorArray* tensor_array;\n    OP_REQUIRES_OK(ctx, GetTensorArray(ctx, &tensor_array));\n    core::ScopedUnref unref(tensor_array);\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, TensorShape({}), &output));\n    OP_REQUIRES_OK(ctx, tensor_array->Size(&(output->scalar<int32>()())));\n  }\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"TensorArraySize\").Device(DEVICE_CPU),\n                        TensorArraySizeOp);\nREGISTER_KERNEL_BUILDER(Name(\"TensorArraySizeV2\").Device(DEVICE_CPU),\n                        TensorArraySizeOp);\nREGISTER_KERNEL_BUILDER(Name(\"TensorArraySizeV3\").Device(DEVICE_CPU),\n                        TensorArraySizeOp);\n\nREGISTER_KERNEL_BUILDER(Name(\"TensorArraySize\")\n                            .Device(DEVICE_GPU)\n                            .HostMemory(\"handle\")\n                            .HostMemory(\"size\"),\n                        TensorArraySizeOp);\nREGISTER_KERNEL_BUILDER(Name(\"TensorArraySizeV2\")\n                            .Device(DEVICE_GPU)\n                            .HostMemory(\"handle\")\n                            .HostMemory(\"size\"),\n                        TensorArraySizeOp);\nREGISTER_KERNEL_BUILDER(Name(\"TensorArraySizeV3\")\n                            .Device(DEVICE_GPU)\n                            .HostMemory(\"handle\")\n                            .HostMemory(\"size\"),\n                        TensorArraySizeOp);\n\n// CLOSE\n// **********************************************************************\n\n// Delete the TensorArray from its resource container.  This enables\n// the user to close and release the resource in the middle of a step/run.\n// TODO(ebrevdo): decide whether closing the grad op should happen\n// here or on the python side.\nclass TensorArrayCloseOp : public OpKernel {\n public:\n  explicit TensorArrayCloseOp(OpKernelConstruction* context)\n      : OpKernel(context) {}\n\n  void Compute(OpKernelContext* ctx) override {\n    TensorArray* tensor_array;\n    OP_REQUIRES_OK(ctx, GetTensorArray(ctx, &tensor_array));\n    core::ScopedUnref unref(tensor_array);\n    // Instead of deleting this TA from the ResourceManager, we just\n    // clear it away and mark it as closed.  The remaining memory\n    // consumed store its mutex and handle Tensor.  This will be\n    // cleared out at the end of the step anyway, so it's fine to keep\n    // it around until the end of the step.  Further calls to the\n    // TensorArray will fail because TensorArray checks internally to\n    // see if it is closed or not.\n    tensor_array->ClearAndMarkClosed();\n  }\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"TensorArrayClose\").Device(DEVICE_CPU),\n                        TensorArrayCloseOp);\nREGISTER_KERNEL_BUILDER(Name(\"TensorArrayCloseV2\").Device(DEVICE_CPU),\n                        TensorArrayCloseOp);\nREGISTER_KERNEL_BUILDER(Name(\"TensorArrayCloseV3\").Device(DEVICE_CPU),\n                        TensorArrayCloseOp);\n\nREGISTER_KERNEL_BUILDER(\n    Name(\"TensorArrayClose\").Device(DEVICE_GPU).HostMemory(\"handle\"),\n    TensorArrayCloseOp);\nREGISTER_KERNEL_BUILDER(\n    Name(\"TensorArrayCloseV2\").Device(DEVICE_GPU).HostMemory(\"handle\"),\n    TensorArrayCloseOp);\nREGISTER_KERNEL_BUILDER(\n    Name(\"TensorArrayCloseV3\").Device(DEVICE_GPU).HostMemory(\"handle\"),\n    TensorArrayCloseOp);\n\n}  // namespace tensorflow\n", "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for tensorflow.ops.tensor_array_ops.\"\"\"\n\nimport numpy as np\n\nfrom tensorflow.core.protobuf import config_pb2\nfrom tensorflow.python.client import session as session_lib\nfrom tensorflow.python.data.ops import dataset_ops\nfrom tensorflow.python.eager import backprop\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.eager import def_function\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.framework import tensor_spec\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import control_flow_util\nfrom tensorflow.python.ops import data_flow_ops\nfrom tensorflow.python.ops import gen_data_flow_ops\nfrom tensorflow.python.ops import gradients_impl\nfrom tensorflow.python.ops import init_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import tensor_array_grad\nfrom tensorflow.python.ops import tensor_array_ops\nfrom tensorflow.python.ops import variable_scope\nfrom tensorflow.python.ops import variables\nimport tensorflow.python.ops.nn_grad  # pylint: disable=unused-import\nfrom tensorflow.python.platform import test\n\n\ndef _make_converter(tf_dtype):\n  def _converter(x):\n    if tf_dtype == dtypes.string:\n      # In Python3, np.str_ is unicode, while we always want bytes\n      return np.asarray(x).astype(\"|S\")\n    x = np.asarray(x).astype(tf_dtype.as_numpy_dtype)\n    if tf_dtype.is_complex:\n      # Add a non-zero imaginary component to x.\n      x -= 1j * x\n    return x\n  return _converter\n\n\ndef _make_ta(size, name, dtype=dtypes.float32, infer_shape=False):\n  return tensor_array_ops.TensorArray(\n      dtype=dtype, tensor_array_name=name, size=size, infer_shape=infer_shape)\n\n\n@test_util.run_all_in_graph_and_eager_modes\n@test_util.with_control_flow_v2\nclass TensorArrayTest(test.TestCase):\n\n  @classmethod\n  def setUpClass(cls):\n    super(TensorArrayTest, cls).setUpClass()\n    cls._workers, _ = test.create_local_cluster(num_workers=3, num_ps=0)\n\n  @classmethod\n  def tearDownClass(cls):\n    super(TensorArrayTest, cls).tearDownClass()\n    session_lib.Session.reset(cls._workers[0].target)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testTensorArrayWriteRead(self):\n    with self.session():\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32,\n          tensor_array_name=\"foo\",\n          size=3,\n          infer_shape=False)\n\n      w0 = ta.write(0, [[4.0, 5.0]])\n      w1 = w0.write(1, [[1.0]])\n      w2 = w1.write(2, -3.0)\n\n      r0 = w2.read(0)\n      r1 = w2.read(1)\n      r2 = w2.read(2)\n\n      d0, d1, d2 = self.evaluate([r0, r1, r2])\n      self.assertAllEqual([[4.0, 5.0]], d0)\n      self.assertAllEqual([[1.0]], d1)\n      self.assertAllEqual(-3.0, d2)\n\n  def _testTensorArrayWritePack(self, tf_dtype):\n    with self.cached_session():\n      ta = tensor_array_ops.TensorArray(\n          dtype=tf_dtype, tensor_array_name=\"foo\", size=3)\n\n      convert = _make_converter(tf_dtype)\n\n      w0 = ta.write(0, convert([[4.0, 5.0]]))\n      w1 = w0.write(1, convert([[6.0, 7.0]]))\n      w2 = w1.write(2, convert([[8.0, 9.0]]))\n\n      c0 = w2.stack()\n\n      c0 = self.evaluate(c0)\n      self.assertAllEqual(\n          convert([[[4.0, 5.0]], [[6.0, 7.0]], [[8.0, 9.0]]]), c0)\n\n  def _testTensorArrayWritePackMaybeLegacy(self):\n    self._testTensorArrayWritePack(dtypes.float32)\n    self._testTensorArrayWritePack(dtypes.float64)\n    self._testTensorArrayWritePack(dtypes.int32)\n    self._testTensorArrayWritePack(dtypes.int64)\n    self._testTensorArrayWritePack(dtypes.complex64)\n    self._testTensorArrayWritePack(dtypes.complex128)\n    self._testTensorArrayWritePack(dtypes.string)\n\n  def testTensorArrayWritePack(self):\n    self._testTensorArrayWritePackMaybeLegacy()\n\n  def testEmptyTensorArrayPack(self):\n    with self.session():\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32, tensor_array_name=\"foo\", size=3)\n\n      empty_element = np.zeros((0, 1), dtype=np.float32)\n      w0 = ta.write(0, empty_element)\n      w1 = w0.write(1, empty_element)\n      w2 = w1.write(2, empty_element)\n\n      c0 = w2.stack()\n\n      c0 = self.evaluate(c0)\n      self.assertAllEqual([3, 0, 1], c0.shape)\n\n  def testTensorArrayWriteConcatInParallel(self):\n    with self.session():\n\n      def _concat_1():\n        ta = tensor_array_ops.TensorArray(\n            dtype=dtypes.int32, size=2, infer_shape=False)\n        w0 = ta.write(0, constant_op.constant([1]))\n        w1 = w0.write(1, constant_op.constant([],\n                                              shape=(0,),\n                                              dtype=dtypes.int32))\n        return w1.concat()\n\n      def _concat_2():\n        ta = tensor_array_ops.TensorArray(\n            dtype=dtypes.int32, size=3, infer_shape=False)\n        w0 = ta.write(0, constant_op.constant([8]))\n        w1 = w0.write(1, constant_op.constant([],\n                                              shape=(0,),\n                                              dtype=dtypes.int32))\n        w2 = w1.write(2, constant_op.constant([9]))\n        return w2.concat()\n\n      def _write(index, output):\n        elements = control_flow_ops.cond(\n            math_ops.less(index, 3), _concat_1, _concat_2)\n        return (index + 1, output.write(index, elements))\n\n      num_iterations = 6\n      init_state = (0,\n                    tensor_array_ops.TensorArray(\n                        dtype=dtypes.int32,\n                        size=num_iterations,\n                        infer_shape=False))\n      _, final_state = control_flow_ops.while_loop(\n          lambda i, _: i < num_iterations, _write, init_state)\n\n      c0 = final_state.concat()\n\n      c0 = self.evaluate(c0)\n      self.assertAllEqual([1, 1, 1, 8, 9, 8, 9, 8, 9], c0)\n\n  def _testTensorArrayWriteConcat(self, tf_dtype):\n    with self.cached_session():\n      ta = tensor_array_ops.TensorArray(\n          dtype=tf_dtype, tensor_array_name=\"foo\", size=3, infer_shape=False)\n\n      convert = _make_converter(tf_dtype)\n\n      w0 = ta.write(0, convert([[4.0, 5.0], [104.0, 105.0], [204.0, 205.0]]))\n      w1 = w0.write(1, convert([[6.0, 7.0], [106.0, 107.0]]))\n      w2 = w1.write(2, convert([[8.0, 9.0]]))\n\n      c0 = w2.concat()\n\n      c0 = self.evaluate(c0)\n      self.assertAllEqual(\n          convert([[4.0, 5.0], [104.0, 105.0], [204.0, 205.0], [6.0, 7.0],\n                   [106.0, 107.0], [8.0, 9.0]]), c0)\n\n  @test_util.deprecated_graph_mode_only\n  def testTensorArrayWriteConcat(self):\n    self._testTensorArrayWriteConcat(dtypes.float32)\n    self._testTensorArrayWriteConcat(dtypes.float64)\n    self._testTensorArrayWriteConcat(dtypes.int32)\n    self._testTensorArrayWriteConcat(dtypes.int64)\n    self._testTensorArrayWriteConcat(dtypes.complex64)\n    self._testTensorArrayWriteConcat(dtypes.complex128)\n    self._testTensorArrayWriteConcat(dtypes.string)\n\n  def _testTensorArrayReadOrPackNotAllValuesAvailableFillsZeros(self):\n    with self.cached_session():\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32,\n          tensor_array_name=\"foo\",\n          size=3,\n          element_shape=tensor_shape.TensorShape([1, 2]))\n      self.assertAllEqual([[0.0, 0.0]], self.evaluate(ta.read(0)))\n      self.assertAllEqual([[[0.0, 0.0]], [[4.0, 5.0]], [[0.0, 0.0]]],\n                          self.evaluate(ta.write(1, [[4.0, 5.0]]).stack()))\n      self.assertAllEqual([[0.0, 0.0], [4.0, 5.0], [0.0, 0.0]],\n                          self.evaluate(ta.write(1, [[4.0, 5.0]]).concat()))\n\n  @test_util.run_v1_only(\"b/122324791\")\n  def testTensorArrayReadOrPackNotAllValuesAvailableFillsZeros(self):\n    self._testTensorArrayReadOrPackNotAllValuesAvailableFillsZeros()\n\n  def _testTensorArrayReadOrPackNotAllValuesAvailableInferShapeFillsZeros(self):\n    ta = tensor_array_ops.TensorArray(\n        dtype=dtypes.float32,\n        tensor_array_name=\"foo\",\n        size=3)\n    self.assertAllEqual(\n        [[0.0, 0.0]], self.evaluate(ta.write(1, [[4.0, 5.0]]).read(0)))\n    self.assertAllEqual([[[0.0, 0.0]], [[4.0, 5.0]], [[0.0, 0.0]]],\n                        self.evaluate(ta.write(1, [[4.0, 5.0]]).stack()))\n    self.assertAllEqual([[0.0, 0.0], [4.0, 5.0], [0.0, 0.0]],\n                        self.evaluate(ta.write(1, [[4.0, 5.0]]).concat()))\n\n  @test_util.run_v1_only(\"b/122324791\")\n  def testTensorArrayReadOrPackNotAllValuesAvailableInferShapeFillsZeros(self):\n    self._testTensorArrayReadOrPackNotAllValuesAvailableInferShapeFillsZeros()\n\n  @test_util.run_v1_only(\"Uses placeholders\")\n  def testSkipEagerTensorArrayReadUninitializedInferShapeFillsZeros(self):\n    with self.cached_session() as sess:\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32,\n          tensor_array_name=\"foo\",\n          size=3)\n      val = array_ops.placeholder(dtypes.float32)\n      self.assertAllEqual(\n          [[0.0, 0.0]], sess.run(ta.write(1, val).read(0), {val: [[4.0, 5.0]]}))\n\n  def _testTensorArrayUnpackRead(self, tf_dtype):\n    with self.cached_session():\n      convert = _make_converter(tf_dtype)\n\n      ta = _make_ta(3, \"foo\", dtype=tf_dtype)\n      # Unpack a vector into scalars\n      w0 = ta.unstack(convert([1.0, 2.0, 3.0]))\n      r0 = w0.read(0)\n      r1 = w0.read(1)\n      r2 = w0.read(2)\n\n      d0, d1, d2 = self.evaluate([r0, r1, r2])\n      self.assertAllEqual(convert(1.0), d0)\n      self.assertAllEqual(convert(2.0), d1)\n      self.assertAllEqual(convert(3.0), d2)\n\n      # Unpack a matrix into vectors\n      w1 = ta.unstack(convert([[1.0, 1.1], [2.0, 2.1], [3.0, 3.1]]))\n      r0 = w1.read(0)\n      r1 = w1.read(1)\n      r2 = w1.read(2)\n\n      d0, d1, d2 = self.evaluate([r0, r1, r2])\n      self.assertAllEqual(convert([1.0, 1.1]), d0)\n      self.assertAllEqual(convert([2.0, 2.1]), d1)\n      self.assertAllEqual(convert([3.0, 3.1]), d2)\n\n      # Try unpacking an empty matrix, which should not cause an error.\n      w2 = ta.unstack(convert([[], [], []]))\n      r0 = w2.read(0)\n      r1 = w2.read(1)\n      r2 = w2.read(2)\n\n      d0, d1, d2 = self.evaluate([r0, r1, r2])\n      self.assertAllEqual(convert([]), d0)\n      self.assertAllEqual(convert([]), d1)\n      self.assertAllEqual(convert([]), d2)\n\n  def _testTensorArrayUnpackReadMaybeLegacy(self):\n    self._testTensorArrayUnpackRead(dtypes.float32)\n    self._testTensorArrayUnpackRead(dtypes.float64)\n    self._testTensorArrayUnpackRead(dtypes.int32)\n    self._testTensorArrayUnpackRead(dtypes.int64)\n    self._testTensorArrayUnpackRead(dtypes.complex64)\n    self._testTensorArrayUnpackRead(dtypes.complex128)\n    self._testTensorArrayUnpackRead(dtypes.string)\n    self._testTensorArrayUnpackRead(dtypes.bfloat16)\n\n  def testTensorArrayUnpackRead(self):\n    self._testTensorArrayUnpackReadMaybeLegacy()\n\n  def _testTensorArraySplitRead(self, tf_dtype):\n    with self.cached_session():\n      convert = _make_converter(tf_dtype)\n\n      # Split an empty vector\n      ta = _make_ta(3, \"foo\", dtype=tf_dtype)\n      lengths = constant_op.constant([0, 0, 0])\n      w0 = ta.split(convert([]), lengths=lengths)\n      r0 = w0.read(0)\n      r1 = w0.read(1)\n      r2 = w0.read(2)\n\n      d0, d1, d2 = self.evaluate([r0, r1, r2])\n      self.assertAllEqual(convert([]), d0)\n      self.assertAllEqual(convert([]), d1)\n      self.assertAllEqual(convert([]), d2)\n\n      # Split a vector\n      lengths = constant_op.constant([2, 0, 1])\n      w0 = ta.split(convert([1.0, 2.0, 3.0]), lengths=lengths)\n      r0 = w0.read(0)\n      r1 = w0.read(1)\n      r2 = w0.read(2)\n\n      d0, d1, d2 = self.evaluate([r0, r1, r2])\n      self.assertAllEqual(convert([1.0, 2.0]), d0)\n      self.assertAllEqual(convert([]), d1)\n      self.assertAllEqual(convert([3.0]), d2)\n\n      # Split a matrix\n      lengths = constant_op.constant([2, 0, 1])\n      w0 = ta.split(\n          convert([[1.0, 101.0], [2.0, 201.0], [3.0, 301.0]]), lengths=lengths)\n      r0 = w0.read(0)\n      r1 = w0.read(1)\n      r2 = w0.read(2)\n\n      d0, d1, d2 = self.evaluate([r0, r1, r2])\n      self.assertAllEqual(convert([[1.0, 101.0], [2.0, 201.0]]), d0)\n      self.assertAllEqual(convert([]).reshape(0, 2), d1)\n      self.assertAllEqual(convert([[3.0, 301.0]]), d2)\n\n  @test_util.deprecated_graph_mode_only\n  def testTensorArraySplitRead(self):\n    self._testTensorArraySplitRead(dtypes.float32)\n    self._testTensorArraySplitRead(dtypes.float64)\n    self._testTensorArraySplitRead(dtypes.int32)\n    self._testTensorArraySplitRead(dtypes.int64)\n    self._testTensorArraySplitRead(dtypes.complex64)\n    self._testTensorArraySplitRead(dtypes.complex128)\n    self._testTensorArraySplitRead(dtypes.string)\n    self._testTensorArraySplitRead(dtypes.bfloat16)\n\n  @test_util.disable_control_flow_v2(\"v2 does not support TensorArray.grad.\")\n  @test_util.run_v1_only(\"v2 does not support TensorArray.grad.\")\n  def testSkipEagerTensorGradArrayWriteRead(self):\n    with self.session() as session:\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32,\n          tensor_array_name=\"foo\",\n          size=3,\n          infer_shape=False)\n      g_ta = ta.grad(\"grad\")\n\n      w0 = ta.write(0, [[4.0, 5.0]])\n      w1 = w0.write(1, [[1.0]])\n      w2 = w1.write(2, -3.0)\n\n      g_w0 = g_ta.write(0, [[5.0, 6.0]])\n      g_w1 = g_w0.write(1, [[2.0]])\n      g_w2 = g_w1.write(2, -2.0)\n\n      r0 = w2.read(0)\n      r1 = w2.read(1)\n      r2 = w2.read(2)\n\n      g_r0 = g_w2.read(0)\n      g_r1 = g_w2.read(1)\n      g_r2 = g_w2.read(2)\n\n      d0, d1, d2, g_d0, g_d1, g_d2 = session.run([r0, r1, r2, g_r0, g_r1, g_r2])\n      self.assertAllEqual([[4.0, 5.0]], d0)\n      self.assertAllEqual([[1.0]], d1)\n      self.assertAllEqual(-3.0, d2)\n      self.assertAllEqual([[5.0, 6.0]], g_d0)\n      self.assertAllEqual([[2.0]], g_d1)\n      self.assertAllEqual(-2.0, g_d2)\n\n  @test_util.deprecated_graph_mode_only\n  def testSkipEagerTensorArrayGradGrad(self):\n    if not control_flow_util.ENABLE_CONTROL_FLOW_V2:\n      self.skipTest(\"Legacy TensorArray does not support double derivatives.\")\n    with self.test_session() as session:\n      x = constant_op.constant(4.0)\n\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32,\n          tensor_array_name=\"foo\",\n          size=1,\n          infer_shape=False)\n      w0 = ta.write(0, x)\n      r0 = w0.read(0)\n      y = r0 * r0\n\n      g1 = gradients_impl.gradients(ys=[y], xs=[x])\n      g2 = gradients_impl.gradients(ys=[g1], xs=[x])\n      self.assertAllEqual([2.0], session.run(g2))\n\n  @test_util.disable_control_flow_v2(\"v2 does not support TensorArray.grad.\")\n  @test_util.run_v1_only(\"v2 does not support TensorArray.grad.\")\n  def testSkipEagerTensorGradArrayDynamicWriteRead(self):\n    with self.session() as session:\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32,\n          tensor_array_name=\"foo\",\n          size=0,\n          dynamic_size=True,\n          infer_shape=False)\n\n      w0 = ta.write(0, [[4.0, 5.0]])\n      w1 = w0.write(1, [[1.0]])\n      w2 = w1.write(2, -3.0)\n\n      g_ta = w2.grad(\"grad\")  # Get gradient array here so we know the shape\n\n      s = w2.size()\n      g_s = g_ta.size()\n\n      g_w0 = g_ta.write(0, [[5.0, 6.0]])\n      g_w1 = g_w0.write(1, [[2.0]])\n      g_w2 = g_w1.write(2, -2.0)\n\n      r0 = w2.read(0)\n      r1 = w2.read(1)\n      r2 = w2.read(2)\n\n      g_r0 = g_w2.read(0)\n      g_r1 = g_w2.read(1)\n      g_r2 = g_w2.read(2)\n\n      d0, d1, d2, g_d0, g_d1, g_d2, vs, g_vs = session.run(\n          [r0, r1, r2, g_r0, g_r1, g_r2, s, g_s])\n      self.assertAllEqual([[4.0, 5.0]], d0)\n      self.assertAllEqual([[1.0]], d1)\n      self.assertAllEqual(-3.0, d2)\n      self.assertAllEqual([[5.0, 6.0]], g_d0)\n      self.assertAllEqual([[2.0]], g_d1)\n      self.assertAllEqual(-2.0, g_d2)\n      self.assertAllEqual(3, vs)\n      self.assertAllEqual(3, g_vs)\n\n  @test_util.disable_control_flow_v2(\"v2 does not support TensorArray.grad.\")\n  @test_util.run_v1_only(\"v2 does not support TensorArray.grad.\")\n  def testSkipEagerTensorGradAccessTwiceReceiveSameObject(self):\n    with self.session() as session:\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32, tensor_array_name=\"foo\", size=3)\n      g_ta_0 = ta.grad(\"grad\")\n      g_ta_1 = ta.grad(\"grad\")\n\n      with ops.control_dependencies([g_ta_0.write(0, [[4.0, 5.0]]).flow]):\n        # Write with one gradient handle, read with another copy of it\n        r1_0 = g_ta_1.read(0)\n\n      t_g_ta_0, t_g_ta_1, d_r1_0 = session.run(\n          [g_ta_0.handle.op, g_ta_1.handle.op, r1_0])\n      self.assertAllEqual(t_g_ta_0, t_g_ta_1)\n      self.assertAllEqual([[4.0, 5.0]], d_r1_0)\n\n  def testTensorArrayWriteWrongIndexOrDataTypeFails(self):\n    with self.session():\n      ta = _make_ta(3, \"foo\", dtype=dtypes.float32)\n      # TODO(b/129870929): Remove the last 2 checks (runtime checks) after\n      # back back from preferred_dtype= to dtype= in convert_to_tensor.  Also\n      # restrict error check to only TypeError.\n      error_msg_regex = (\n          \"(\"\n          \"Expected float32, got 'wrong_type_scalar' of type 'str' instead.\"\n          \"|\"\n          \"Cannot convert provided value to EagerTensor. Provided value: \"\n          \"wrong_type_scalar Requested dtype: float\"\n          \"|\"\n          \"TensorArray dtype is float.* but Op is trying to write dtype string\"\n          \"|\"\n          \"Invalid data types; op elements string but list elements float\"\n          \")\")\n      with self.assertRaisesRegex((TypeError, errors.InvalidArgumentError),\n                                  error_msg_regex):\n        self.evaluate(ta.write(0, \"wrong_type_scalar\").flow)\n\n      if (control_flow_util.ENABLE_CONTROL_FLOW_V2 and\n          not context.executing_eagerly()):\n        error_msg = \"Trying to modify element -1 in a list with 3 elements.\"\n      else:\n        error_msg = \"index -1\"\n      with self.assertRaisesOpError(error_msg):\n        self.evaluate(ta.write(-1, 3.0).flow)\n\n      if (control_flow_util.ENABLE_CONTROL_FLOW_V2 and\n          not context.executing_eagerly()):\n        error_msg = \"Trying to modify element 3 in a list with 3 elements\"\n      else:\n        error_msg = (\"Tried to write to index 3 but array is not \"\n                     \"resizeable and size is: 3\")\n      # Test reading from too large an index\n      with self.assertRaisesOpError(error_msg):\n        self.evaluate(ta.write(3, 3.0).flow)\n\n  def testTensorArrayReadWrongIndexOrDataTypeFails(self):\n    with self.session():\n      ta = _make_ta(3, \"foo\", dtype=dtypes.float32)\n\n      w0 = ta.write(0, [[4.0, 5.0]])\n\n      # Test reading wrong datatype (only possible when constructing graphs).\n      if (not context.executing_eagerly() and\n          not control_flow_util.ENABLE_CONTROL_FLOW_V2):\n        r0_bad = gen_data_flow_ops.tensor_array_read_v3(\n            handle=w0.handle, index=0, dtype=dtypes.float64, flow_in=w0.flow)\n        with self.assertRaisesOpError(\n            \"TensorArray dtype is float but Op requested dtype double.\"):\n          self.evaluate(r0_bad)\n\n      if (control_flow_util.ENABLE_CONTROL_FLOW_V2 and\n          not context.executing_eagerly()):\n        error_msg = \"Trying to access element -1 in a list with 3 elements.\"\n      else:\n        error_msg = \"index -1\"\n      # Test reading from a negative index, which is not allowed\n      with self.assertRaisesOpError(error_msg):\n        self.evaluate(ta.read(-1))\n\n      if (control_flow_util.ENABLE_CONTROL_FLOW_V2 and\n          not context.executing_eagerly()):\n        error_msg = \"Trying to access element 3 in a list with 3 elements.\"\n      else:\n        error_msg = \"Tried to read from index 3 but array size is: 3\"\n      # Test reading from too large an index\n      with self.assertRaisesOpError(error_msg):\n        self.evaluate(ta.read(3))\n\n  @test_util.disable_control_flow_v2(\"v2 allows multiple writes.\")\n  @test_util.run_v1_only(\"v2 allows multiple writes.\")\n  def testSkipEagerTensorArrayWriteMultipleFails(self):\n    with self.session():\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32, tensor_array_name=\"foo\", size=3)\n\n      with self.assertRaisesOpError(\n          \"Could not write to TensorArray index 2 because \"\n          \"it has already been written to.\"):\n        self.evaluate(ta.write(2, 3.0).write(2, 3.0).flow)\n\n  def testTensorArrayConcatIncompatibleShapesFails(self):\n    with self.session():\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32,\n          tensor_array_name=\"foo\",\n          size=3,\n          infer_shape=False)\n\n      w1 = ta.write(0, 3.0)\n      w2 = w1.write(1, 4.0)\n      w3 = w2.write(2, [3.0])\n\n      with self.assertRaisesOpError(\n          \"Concat saw a scalar shape at index 0 but requires at least vectors\"):\n        self.evaluate(w3.concat())\n\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32,\n          tensor_array_name=\"foo\",\n          size=3,\n          infer_shape=False)\n\n      w1 = ta.write(0, [3.0])\n      w2 = w1.write(1, [4.0])\n      w3 = w2.write(2, [[3.0]])\n\n      # The exact error messages differ between eager execution and graph\n      # construction as the former bubbles up the error from array_op.concat.\n      error_msg = (\"Incompatible ranks\"\n                   if control_flow_util.ENABLE_CONTROL_FLOW_V2 and\n                   not context.executing_eagerly() else \"shape\")\n      with self.assertRaisesRegex(errors.InvalidArgumentError, error_msg):\n        self.evaluate(w3.concat())\n\n  def testTensorArraySplitIncompatibleShapesFails(self):\n    with self.session():\n      in_eager_mode = context.executing_eagerly()\n      ta = _make_ta(3, \"foo\")\n      with self.assertRaisesOpError(\n          r\"Expected lengths to be a vector, received shape: \\[\\]\"):\n        if in_eager_mode:\n          self.evaluate(ta.split([1.0, 2.0, 3.0], 1))\n        else:\n          lengths = array_ops.placeholder(dtypes.int64)\n          ta.split([1.0, 2.0, 3.0], lengths).flow.eval(feed_dict={lengths: 1})\n\n      error_msg = (\"Unused values in tensor. Length of tensor: 3 Values used: 1\"\n                   if control_flow_util.ENABLE_CONTROL_FLOW_V2 and\n                   not in_eager_mode else\n                   r\"Expected sum of lengths to be equal to values.shape\\[0\\], \"\n                   r\"but sum of lengths is 1 and value's shape is: \\[3\\]\")\n      with self.assertRaisesOpError(error_msg):\n        self.evaluate(ta.split([1.0, 2.0, 3.0], [1]).flow)\n\n      ta = _make_ta(1, \"baz\")\n      if control_flow_util.ENABLE_CONTROL_FLOW_V2 and not in_eager_mode:\n        with self.assertRaisesRegex(\n            ValueError, \"Shape must be at least rank 1 but is rank 0\"):\n          self.evaluate(ta.split(1.0, [1]).flow)\n      else:\n        with self.assertRaisesOpError(\n            r\"Expected value to be at least a vector, but received shape: \\[\\]\"\n        ):\n          self.evaluate(ta.split(1.0, [1]).flow)\n\n      if not control_flow_util.ENABLE_CONTROL_FLOW_V2 or in_eager_mode:\n        ta = _make_ta(2, \"buz\")\n        with self.assertRaisesOpError(\n            r\"TensorArray's size is not equal to the size of lengths \"\n            r\"\\(2 vs. 1\\), and the TensorArray is not marked as \"\n            r\"dynamically resizeable\"):\n          self.evaluate(ta.split([1.0], [1]).flow)\n\n  def _testTensorArrayWriteGradientAddMultipleAdds(self, dtype):\n    with self.cached_session():\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtype, tensor_array_name=\"foo\", size=3, infer_shape=False)\n      ta_grad = ta.grad(\"grad\")\n\n      c = lambda x: np.asarray(x, dtype=dtype.as_numpy_dtype)\n\n      w0 = ta.write(2, c(3.0))\n      w1 = w0.write(2, c(4.0))\n\n      w0_grad = ta_grad.write(2, c(3.0))\n      w1_grad = w0_grad.write(2, c(4.0))\n      w2_grad = w1_grad.write(2, c(5.0))\n\n      # Assert that aggregation works correctly\n      self.assertAllEqual(c(12.00), w2_grad.read(2))\n\n      # Assert that if multiple_writes_aggregate is not enabled,\n      # multiple writes raise an exception.\n      with self.assertRaisesOpError(\n          r\"TensorArray foo_.*: Could not write to TensorArray index 2 because \"\n          r\"it has already been written to.\"):\n        self.evaluate(w1.flow)\n\n      # Using differing shapes causes an exception\n      wb0_grad = ta_grad.write(1, c(1.0))\n      wb1_grad = wb0_grad.write(1, c([1.0]))\n\n      with self.assertRaisesOpError(\n          r\"Could not aggregate to TensorArray index 1 because the \"\n          r\"existing shape is \\[\\] but the new input shape is \\[1\\]\"):\n        self.evaluate(wb1_grad.flow)\n\n  @test_util.disable_control_flow_v2(\"v2 does not support TensorArray.grad.\")\n  @test_util.run_v1_only(\"v2 does not support TensorArray.grad.\")\n  def testSkipEagerTensorArrayWriteGradientAddMultipleAdds(self):\n    for dtype in (dtypes.int32, dtypes.int64, dtypes.float32, dtypes.float64,\n                  dtypes.complex64, dtypes.complex128):\n      self._testTensorArrayWriteGradientAddMultipleAdds(dtype)\n\n  @test_util.disable_control_flow_v2(\"Low level legacy TA op test.\")\n  @test_util.run_v1_only(\"Low level legacy TA op test.\")\n  def testSkipEagerTensorArrayGradWithShapeKnownElementShape(self):\n    with self.session() as sess:\n      ta = tensor_array_ops.TensorArray(\n          size=3,\n          dtype=dtypes.float32,\n          element_shape=tensor_shape.TensorShape([2, 3]))\n      handle, flow = data_flow_ops.tensor_array_grad_with_shape(\n          handle=ta.handle,\n          flow_in=ta.flow,\n          shape_to_prepend=tensor_shape.TensorShape([4, 5]),\n          source=\"source\")\n      ta_grad = tensor_array_ops.TensorArray(\n          dtypes.float32, handle=handle, flow=flow)\n      value = array_ops.placeholder(dtypes.float32)\n      ta_grad = ta_grad.write(0, value)\n      read_value = ta_grad.read(0)\n\n      # Make sure shape inference worked.\n      self.assertAllEqual([None, None, 2, 3], read_value.shape.as_list())\n      # Writing with wrong shape should not work.\n      with self.assertRaisesRegex(errors.InvalidArgumentError,\n                                  \"Could not write to TensorArray\"):\n        fed_value = np.random.random([2, 3])\n        sess.run(read_value, feed_dict={value: fed_value})\n      # Writing with correct shape should work.\n      fed_value = np.random.random([4, 5, 2, 3])\n      self.assertAllClose(fed_value,\n                          sess.run(read_value, feed_dict={value: fed_value}))\n\n  @test_util.disable_control_flow_v2(\"Low level legacy TA op test.\")\n  @test_util.run_v1_only(\"Low level legacy TA op test.\")\n  def testSkipEagerTensorArrayGradWithShapeUnknownElementShape(self):\n    with self.session() as sess:\n      ta = tensor_array_ops.TensorArray(\n          size=3, dtype=dtypes.float32,\n          element_shape=None)  # Note that element_shape is unknown\n      handle, flow = data_flow_ops.tensor_array_grad_with_shape(\n          handle=ta.handle,\n          flow_in=ta.flow,\n          shape_to_prepend=tensor_shape.TensorShape([4, 5]),\n          source=\"source\")\n      ta_grad = tensor_array_ops.TensorArray(\n          dtypes.float32, handle=handle, flow=flow)\n      value = array_ops.placeholder(dtypes.float32)\n      ta_grad = ta_grad.write(0, value)\n      read_value = ta_grad.read(0)\n\n      # Make sure shape inference worked.\n      self.assertIsNone(read_value.shape.ndims)\n      # Write with some shape and check read value.\n      fed_value = np.random.random([4, 5, 7])\n      self.assertAllClose(fed_value,\n                          sess.run(read_value, feed_dict={value: fed_value}))\n\n  def testMultiTensorArray(self):\n    with self.session():\n      h1 = tensor_array_ops.TensorArray(\n          size=1, dtype=dtypes.float32, tensor_array_name=\"foo\")\n      w1 = h1.write(0, 4.0)\n      r1 = w1.read(0)\n\n      h2 = tensor_array_ops.TensorArray(\n          size=1, dtype=dtypes.float32, tensor_array_name=\"bar\")\n\n      w2 = h2.write(0, 5.0)\n      r2 = w2.read(0)\n      r = r1 + r2\n      val = self.evaluate(r)\n      self.assertAllClose(9.0, val)\n\n  def _testTensorArrayGradientWriteReadType(self, dtype):\n    with self.cached_session() as session:\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.as_dtype(dtype),\n          tensor_array_name=\"foo\",\n          size=3,\n          infer_shape=False)\n\n      c = lambda x: np.array(x, dtype=dtype)\n\n      value_0 = constant_op.constant(c([[4.0, 5.0]]))\n      value_1 = constant_op.constant(c(3.0))\n\n      w0 = ta.write(0, value_0)\n      w1 = w0.write(1, value_1)\n      r0 = w1.read(0)\n      r1 = w1.read(1)\n      r0_2 = w1.read(0)\n\n      # Test individual components' gradients\n      grad_just_r0 = gradients_impl.gradients(\n          ys=[r0], xs=[value_0], grad_ys=[c([[2.0, 3.0]])])\n      grad_just_r0_vals = session.run(grad_just_r0)\n      self.assertAllEqual(c([[2.0, 3.0]]), grad_just_r0_vals[0])\n\n      grad_r0_r0_2 = gradients_impl.gradients(\n          ys=[r0, r0_2],\n          xs=[value_0],\n          grad_ys=[c([[2.0, 3.0]]), c([[1.0, -1.0]])])\n      grad_r0_r0_2_vals = session.run(grad_r0_r0_2)\n      self.assertAllEqual(c([[3.0, 2.0]]), grad_r0_r0_2_vals[0])\n\n      grad_just_r1 = gradients_impl.gradients(\n          ys=[r1], xs=[value_1], grad_ys=[c(-2.0)])\n      grad_just_r1_vals = session.run(grad_just_r1)\n      self.assertAllEqual(c(-2.0), grad_just_r1_vals[0])\n\n      # Test combined gradients\n      grad = gradients_impl.gradients(\n          ys=[r0, r0_2, r1],\n          xs=[value_0, value_1],\n          grad_ys=[c([[2.0, 3.0]]), c([[1.0, -1.0]]), c(-2.0)])\n      grad_vals = session.run(grad)\n      self.assertEqual(len(grad_vals), 2)\n      self.assertAllEqual(c([[3.0, 2.0]]), grad_vals[0])\n      self.assertAllEqual(c(-2.0), grad_vals[1])\n\n  @test_util.deprecated_graph_mode_only\n  def testSkipEagerTensorArrayGradientWriteRead(self):\n    for dtype in (np.float32, np.float64, np.complex64, np.complex128):\n      self._testTensorArrayGradientWriteReadType(dtype)\n\n  def _testTensorArrayGradientWritePackConcatAndRead(self):\n    with self.cached_session():\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32,\n          tensor_array_name=\"foo\",\n          size=2,\n          clear_after_read=False)\n\n      value_0 = constant_op.constant([-1.0, 1.0])\n      value_1 = constant_op.constant([-10.0, 10.0])\n\n      w0 = ta.write(0, value_0)\n      w1 = w0.write(1, value_1)\n      p0 = w1.stack()\n      r0 = w1.read(0)\n      s0 = w1.concat()\n\n      # Test gradient accumulation between read(0), pack(), and concat()\n      with ops.control_dependencies([p0, r0, s0]):\n        grad_r = gradients_impl.gradients(\n            ys=[p0, r0, s0],\n            xs=[value_0, value_1],\n            grad_ys=[\n                [[2.0, 3.0], [4.0, 5.0]],  # pack gradient\n                [-0.5, 1.5],  # read(0) gradient\n                [20.0, 30.0, 40.0, 50.0]\n            ])  # concat gradient\n      grad_vals = self.evaluate(grad_r)  # 2 + 2 entries\n\n      self.assertAllClose([2.0 - 0.5 + 20.0, 3.0 + 1.5 + 30.0], grad_vals[0])\n      self.assertAllEqual([4.0 + 40.0, 5.0 + 50.0], grad_vals[1])\n\n  @test_util.deprecated_graph_mode_only\n  def testSkipEagerTensorArrayGradientWritePackConcatAndRead(self):\n    self._testTensorArrayGradientWritePackConcatAndRead()\n\n  @test_util.disable_control_flow_v2(\"v2 does not support clear_after_read.\")\n  @test_util.run_v1_only(\"v2 does not support clear_after_read.\")\n  def testTensorArrayReadTwice(self):\n    with self.session():\n      value = constant_op.constant([[1.0, -1.0], [10.0, -10.0]])\n\n      ta_readonce = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32, tensor_array_name=\"foo\", size=2)\n\n      w_readonce = ta_readonce.unstack(value)\n      r0_readonce = w_readonce.read(0)\n\n      with self.assertRaisesOpError(\n          r\"Could not read index 0 twice because it was cleared after a \"\n          r\"previous read \\(perhaps try setting clear_after_read = false\\?\\)\"):\n        with ops.control_dependencies([r0_readonce]):\n          self.evaluate(w_readonce.read(0))\n\n      ta_readtwice = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32,\n          tensor_array_name=\"foo\",\n          size=2,\n          clear_after_read=False)\n      w_readtwice = ta_readtwice.unstack(value)\n      r0_readtwice = w_readtwice.read(0)\n      with ops.control_dependencies([r0_readtwice]):\n        r1_readtwice = w_readtwice.read(0)\n\n      self.assertAllEqual([1.0, -1.0], self.evaluate(r1_readtwice))\n\n  def _testTensorArrayGradientUnpackRead(self):\n    with self.cached_session() as session:\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32,\n          tensor_array_name=\"foo\",\n          size=2,\n          clear_after_read=False)\n\n      value = constant_op.constant([[1.0, -1.0], [10.0, -10.0]])\n\n      w = ta.unstack(value)\n      r0 = w.read(0)\n      r0_1 = w.read(0)\n      r1 = w.read(1)\n\n      # Test combined gradients + aggregation of read(0)\n      grad = gradients_impl.gradients(\n          ys=[r0, r0_1, r1],\n          xs=[value],\n          grad_ys=[[2.0, 3.0], [-1.5, 1.5], [4.0, 5.0]])\n      grad_vals = session.run(grad)\n\n      self.assertEqual(len(grad_vals), 1)\n      self.assertAllEqual([[2.0 - 1.5, 3.0 + 1.5], [4.0, 5.0]], grad_vals[0])\n\n  @test_util.deprecated_graph_mode_only\n  def testSkipEagerTensorArrayGradientUnpackRead(self):\n    self._testTensorArrayGradientUnpackRead()\n\n  @test_util.deprecated_graph_mode_only\n  def testSkipEagerTensorArrayGradientSplitConcat(self):\n    with self.session() as session:\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32, tensor_array_name=\"foo\", size=2,\n          infer_shape=False)\n\n      value = constant_op.constant(\n          [[1.0, -1.0], [10.0, -10.0], [100.0, -100.0]])\n\n      w = ta.split(value, [2, 1])\n      r = w.concat()\n\n      # Test combined gradients\n      grad = gradients_impl.gradients(\n          ys=[r],\n          xs=[value],\n          grad_ys=[[[2.0, -2.0], [20.0, -20.0], [200.0, -200.0]]])\n      grad_vals = session.run(grad)\n\n      self.assertEqual(len(grad_vals), 1)\n      self.assertAllEqual([[2.0, -2.0], [20.0, -20.0], [200.0, -200.0]],\n                          grad_vals[0])\n\n  def _testTensorArrayGradientDynamicUnpackRead(self):\n    with self.cached_session() as session:\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32,\n          tensor_array_name=\"foo\",\n          size=0,\n          dynamic_size=True)\n\n      value = constant_op.constant([[1.0, -1.0], [10.0, -10.0]])\n\n      w = ta.unstack(value)\n      r0 = w.read(0)\n      r1 = w.read(1)\n\n      # Test combined gradients + aggregation of read(0)\n      grad = gradients_impl.gradients(\n          ys=[r0, r1], xs=[value], grad_ys=[[2.0, 3.0], [4.0, 5.0]])\n      grad_vals = session.run(grad)\n\n      self.assertEqual(len(grad_vals), 1)\n      self.assertAllEqual([[2.0, 3.0], [4.0, 5.0]], grad_vals[0])\n\n  @test_util.deprecated_graph_mode_only\n  def testSkipEagerTensorArrayGradientDynamicUnpackRead(self):\n    self._testTensorArrayGradientDynamicUnpackRead()\n\n  def testCloseTensorArray(self):\n    with self.session():\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32, tensor_array_name=\"foo\", size=3)\n      self.evaluate(ta.close())\n\n  def testSizeTensorArray(self):\n    with self.session():\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32, tensor_array_name=\"foo\", size=3)\n      s = ta.size()\n      self.assertAllEqual(3, self.evaluate(s))\n\n  def testWriteCloseTensorArray(self):\n    with self.session():\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32,\n          tensor_array_name=\"foo\",\n          size=3,\n          infer_shape=False)\n      w0 = ta.write(0, [[4.0, 5.0]])\n      w1 = w0.write(1, [3.0])\n      self.evaluate(w1.close())  # Expected to run without problems\n\n  def _testWhileLoopWritePackGradients(self, dynamic_size, dtype):\n    np_dtype = dtype.as_numpy_dtype\n    with self.cached_session():\n\n      def func(v0, state0, var):\n        ta = tensor_array_ops.TensorArray(\n            dtype=dtype,\n            tensor_array_name=\"foo\",\n            size=0 if dynamic_size else 3,\n            dynamic_size=dynamic_size)\n        time_0 = array_ops.identity(0)\n\n        def body(time, ta_t, state):\n          sliced = array_ops.slice(\n              v0, begin=array_ops.stack([time, 0]), size=[1, -1])\n          sliced = array_ops.squeeze(sliced)\n          out = sliced + var + state\n          state += sliced\n          ta_t = ta_t.write(time, out)\n          return (time + 1, ta_t, state)\n\n        (unused_0, h_final, unused_2) = control_flow_ops.while_loop(\n            cond=lambda time, unused_1, unused_2: time < 3,\n            body=body,\n            loop_vars=(time_0, ta, state0),\n            shape_invariants=(time_0.get_shape(), tensor_shape.unknown_shape(),\n                              tensor_shape.unknown_shape()),\n            parallel_iterations=3)\n        vout = h_final.stack()\n        return vout\n\n      v0 = array_ops.identity(np.arange(3 * 5, dtype=np_dtype).reshape(3, 5))\n      state0 = array_ops.identity(np.array([1] * 5, dtype=np_dtype))\n      init_val = np.arange(100, 105, dtype=np_dtype)\n      var = variable_scope.get_variable(\n          \"var\",\n          shape=init_val.shape,\n          dtype=np_dtype,\n          initializer=init_ops.constant_initializer(init_val))\n\n      vout = func(v0, state0, var)\n      grad_val = -np.arange(3 * 5, dtype=np_dtype).reshape(3, 5)\n      if context.executing_eagerly():\n        grad_fn = backprop.gradients_function(func)\n        v0_grad, state0_grad, var_grad = grad_fn(v0, state0, var, dy=grad_val)\n      else:\n        v0_grad = gradients_impl.gradients([vout], [v0], [grad_val])[0]\n        state0_grad = gradients_impl.gradients([vout], [state0], [grad_val])[0]\n        var_grad = gradients_impl.gradients([vout], [var], [grad_val])[0]\n        self.evaluate(variables.global_variables_initializer())\n\n      state0_t, var_t, v0_t, vout_t, v0_grad_t, var_grad_t, state0_grad_t = (\n          self.evaluate(\n              ([state0, var, v0, vout, v0_grad, var_grad, state0_grad])))\n      just_v0_grad_t = self.evaluate(v0_grad)\n\n      # state = [ state0 | state0 + v0[0] | state0 + v0[0] + v0[1] ]\n      # vout = [ v0[0] + var + state[0] |\n      #          v0[1] + var + state[1] |\n      #          v0[2] + var + state[2] ]\n      #      = [ v0[0] + var + state0 |\n      #          v0[1] + var + state0 + v0[0] |\n      #          v0[2] + var + state0 + v0[0] + v0[1] ]\n      #\n      # d(vout[0])/d(v0) = [1 | 0 | 0 ]\n      # d(vout[1])/d(v0) = [1 | 1 | 0 ]\n      # d(vout[2])/d(v0) = [1 | 1 | 1 ]\n      # d(vout)/d(var) = [1 | 1 | 1]\n      # d(vout)/d(state0) = [ 1 | 1 | 1 ]\n\n      state_per_time = np.array(\n          [state0_t, state0_t + v0_t[0, :], state0_t + v0_t[0, :] + v0_t[1, :]])\n\n      # Compare forward prop\n      self.assertAllClose(v0_t + var_t + state_per_time, vout_t)\n\n      # Compare backward prop\n      expected_v0_grad_t = np.array([\n          grad_val[0, :] + grad_val[1, :] + grad_val[2, :],\n          grad_val[1, :] + grad_val[2, :], grad_val[2, :]\n      ])\n\n      self.assertAllEqual(expected_v0_grad_t, v0_grad_t)\n      self.assertAllEqual(expected_v0_grad_t, just_v0_grad_t)\n      self.assertAllClose(grad_val.sum(axis=0), var_grad_t)\n      self.assertAllClose(grad_val.sum(axis=0), state0_grad_t)\n\n  def testWhileLoopWritePackGradients(self):\n    self._testWhileLoopWritePackGradients(\n        dynamic_size=False, dtype=dtypes.float32)\n    # TODO(ebrevdo): re-enable when While supports non-float32 gradients.\n    # self._testWhileLoopWritePackGradients(\n    #     dynamic_size=False, dtype=tf.int64)\n\n  @test_util.run_deprecated_v1\n  def testSkipEagerWhileLoopDynamicWritePackGradients(self):\n    self._testWhileLoopWritePackGradients(\n        dynamic_size=True, dtype=dtypes.float32)\n\n  def testGradSerialTwoLoops(self):\n    with self.session():\n\n      def loop(x):\n        num_steps = 100\n        acc = tensor_array_ops.TensorArray(\n            dtype=dtypes.float32,\n            size=num_steps,\n            clear_after_read=False,\n            element_shape=tensor_shape.TensorShape([]))\n        i = constant_op.constant(0, name=\"i\")\n\n        c = lambda i, acc: i < 5\n\n        def b(i, acc):\n          x1 = control_flow_ops.cond(\n              math_ops.equal(i, 0), lambda: x,\n              lambda: math_ops.multiply(acc.read(i - 1), 2.0))\n          return i + 1, acc.write(i, x1)\n\n        i1, acc1 = control_flow_ops.while_loop(c, b, [i, acc])\n\n        z = constant_op.constant(0.0)\n\n        def fn(i, acc):\n          return i + 1, acc.write(i, z)\n\n        _, acc2 = control_flow_ops.while_loop(lambda i, acc: i < num_steps, fn,\n                                              [i1, acc1])\n\n        r = acc2.stack()\n        return r\n\n      x = constant_op.constant(2.0, name=\"x\")\n      if context.executing_eagerly():\n        grad = backprop.gradients_function(loop)(x)[0]\n      else:\n        grad = gradients_impl.gradients(loop(x), [x])[0]\n      self.assertAllClose(31.0, self.evaluate(grad))\n\n  def testShapeAfterWhileLoop(self):\n    size = 10\n    ta = tensor_array_ops.TensorArray(dtype=dtypes.float32, size=size)\n    _, ta = control_flow_ops.while_loop(\n        lambda i, _: i < size,\n        lambda i, ta: (i + 1, ta.write(i, [[0.]])), [0, ta],\n        parallel_iterations=1)\n    self.assertIsNotNone(ta.element_shape.dims)\n\n  @test_util.deprecated_graph_mode_only\n  def testSkipEagerSumOfTwoReadVariablesWithoutRepeatGrad(self):\n    with self.session() as session:\n      a = array_ops.identity(\n          np.arange(\n              3 * 5, dtype=np.float32).reshape(3, 5) + 1)\n      b = array_ops.identity(\n          np.arange(\n              3 * 5, dtype=np.float32).reshape(3, 5) + 1 + 3 * 5)\n      ta = tensor_array_ops.TensorArray(dtype=dtypes.float32, size=2)\n      ta = ta.write(0, a, name=\"write_a\")\n      ta = ta.write(1, b, name=\"write_b\")\n      c = (\n          ta.read(\n              0, name=\"read_a_0\") +  # a + b\n          ta.read(\n              1, name=\"read_b_0\"))\n      g0 = -(np.arange(3 * 5, dtype=np.float32).reshape(3, 5) + 1)\n      grad_a = gradients_impl.gradients([c], [a], [g0])[0]  # d(a+b)/da = 1\n      grad_b = gradients_impl.gradients([c], [b], [g0])[0]  # d(a+b)/db = 1\n\n      # Test gradients calculated individually\n      grad_a_t, = session.run([grad_a])\n      self.assertAllEqual(grad_a_t, g0)\n\n      grad_b_t, = session.run([grad_b])\n      self.assertAllEqual(grad_b_t, g0)\n\n      # Test gradients calculated jointly\n      joint_grad_a_t, joint_grad_b_t = session.run([grad_a, grad_b])\n      self.assertAllEqual(joint_grad_a_t, g0)\n      self.assertAllEqual(joint_grad_b_t, g0)\n\n  def _grad_source_for_name(self, name):\n    return tensor_array_grad._GetGradSource(constant_op.constant(0, name=name))\n\n  @test_util.deprecated_graph_mode_only\n  def testSkipEagerGetGradSource_Invalid(self):\n    with self.assertRaises(ValueError):\n      self._grad_source_for_name(\"\")\n    with self.assertRaises(ValueError):\n      self._grad_source_for_name(\"foo\")\n    with self.assertRaises(ValueError):\n      self._grad_source_for_name(\"foo/bar\")\n\n  @test_util.deprecated_graph_mode_only\n  def testSkipEagerGetGradSource_NoEnclosingScope(self):\n    self.assertEqual(\"gradients:0\", self._grad_source_for_name(\"gradients\"))\n    self.assertEqual(\"gradients_0:0\", self._grad_source_for_name(\"gradients_0\"))\n    self.assertEqual(\"gradients\", self._grad_source_for_name(\"gradients/foo\"))\n    self.assertEqual(\"gradients_0\",\n                     self._grad_source_for_name(\"gradients_0/foo\"))\n    self.assertEqual(\"gradients\",\n                     self._grad_source_for_name(\"gradients/foo/bar\"))\n    self.assertEqual(\"gradients_0\",\n                     self._grad_source_for_name(\"gradients_0/foo/bar\"))\n\n  @test_util.deprecated_graph_mode_only\n  def testSkipEagerGetGradSource_EnclosingScope(self):\n    self.assertEqual(\"foo/gradients:0\",\n                     self._grad_source_for_name(\"foo/gradients\"))\n    self.assertEqual(\"foo/gradients_0:0\",\n                     self._grad_source_for_name(\"foo/gradients_0\"))\n    self.assertEqual(\"foo/gradients\",\n                     self._grad_source_for_name(\"foo/gradients/bar\"))\n    self.assertEqual(\"foo/gradients_0\",\n                     self._grad_source_for_name(\"foo/gradients_0/bar\"))\n    self.assertEqual(\"foo/bar/gradients\",\n                     self._grad_source_for_name(\"foo/bar/gradients/baz\"))\n    self.assertEqual(\"foo/bar/gradients_0\",\n                     self._grad_source_for_name(\"foo/bar/gradients_0/baz\"))\n\n  @test_util.deprecated_graph_mode_only\n  def testSkipEagerGetGradSource_NestedUsesInnermost(self):\n    self.assertEqual(\n        \"foo/gradients/bar/gradients_0\",\n        self._grad_source_for_name(\"foo/gradients/bar/gradients_0/baz\"))\n\n  @test_util.deprecated_graph_mode_only\n  def testSkipEagerWriteShape(self):\n    with self.session():\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32, tensor_array_name=\"foo\", size=3)\n      c0 = constant_op.constant([4.0, 5.0])\n      w0 = ta.write(0, c0)\n      r0 = w0.read(0)\n      self.assertAllEqual(c0.get_shape(), r0.get_shape())\n\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32, tensor_array_name=\"foo\", size=3)\n      c1 = constant_op.constant([6.0, 7.0])\n      w1 = w0.write(1, c1)\n      r0 = w1.read(0)\n      r1 = w1.read(1)\n      self.assertAllEqual(c0.get_shape(), r0.get_shape())\n      self.assertAllEqual(c1.get_shape(), r1.get_shape())\n\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32, tensor_array_name=\"foo\", size=3)\n      c2 = constant_op.constant([4.0, 5.0, 6.0])\n      with self.assertRaises(ValueError):\n        w0.write(0, c2)\n\n  @test_util.deprecated_graph_mode_only\n  def testSkipEagerPartlyUnknownShape(self):\n    with self.session():\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32, tensor_array_name=\"foo\", size=6)\n\n      c0 = array_ops.placeholder(dtypes.float32, [None, None, None, 3])\n      w0 = ta.write(0, c0)\n      r0 = w0.read(0)\n      self.assertAllEqual([None, None, None, 3], r0.get_shape().as_list())\n\n      c1 = array_ops.placeholder(dtypes.float32, [None, None, None, 3])\n      w1 = w0.write(1, c1)\n      r1 = w1.read(0)\n      self.assertAllEqual([None, None, None, 3], r1.get_shape().as_list())\n\n      # Writing less specific shape (doesn't change type.)\n      c2 = array_ops.placeholder(dtypes.float32, [None, None, None, None])\n      w2 = w1.write(2, c2)\n      r2 = w2.read(0)\n      self.assertAllEqual([None, None, None, 3], r2.get_shape().as_list())\n\n      # Writing more specific shape in one dimension and less specific in\n      # another.\n      c3 = array_ops.placeholder(dtypes.float32, [None, None, 2, None])\n      w3 = w2.write(3, c3)\n      r3 = w3.read(0)\n      self.assertAllEqual([None, None, 2, 3], r3.get_shape().as_list())\n\n      # Writing partly defined shape using TensorArray.scatter.\n      c4 = array_ops.placeholder(dtypes.float32, [2, None, 4, 2, 3])\n      w4 = w3.scatter([4, 5], c4)\n      r4 = w4.read(0)\n      self.assertAllEqual([None, 4, 2, 3], r4.get_shape().as_list())\n\n      # Writing fully defined shape using TensorArray.split.\n      c5 = array_ops.placeholder(dtypes.float32, [10, 4, 2, 3])\n      w5 = w4.split(c5, constant_op.constant([5, 5]))\n      r5 = w5.read(0)\n      self.assertAllEqual([5, 4, 2, 3], r5.get_shape().as_list())\n\n  def _testUnpackShape(self):\n    with self.cached_session():\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32,\n          tensor_array_name=\"foo\",\n          size=0,\n          dynamic_size=True,\n          infer_shape=True)\n      value = constant_op.constant(\n          [[1.0, -1.0], [10.0, -10.0], [100.0, -100.0]])\n      w0 = ta.unstack(value)\n      r0 = w0.read(0)\n      self.assertAllEqual((2,), r0.get_shape())\n\n      c1 = constant_op.constant([4.0, 5.0])\n      w1 = w0.write(3, c1)\n\n      if not control_flow_util.ENABLE_CONTROL_FLOW_V2:\n        # TensorArray v2 does not support clear_after_read.\n        with self.assertRaisesOpError(\n            r\"Could not read index 0 twice because it was cleared after a \"\n            r\"previous read \\(perhaps try setting clear_after_read = false\\?\\)\"\n        ):\n          with ops.control_dependencies([r0]):\n            self.evaluate(w1.read(0))\n\n      r1 = w1.read(1)\n      self.assertAllEqual(c1.get_shape(), r1.shape)\n\n      c2 = constant_op.constant([4.0, 5.0, 6.0])\n      with self.assertRaises(ValueError):\n        w1.write(4, c2)\n\n  def testUnpackShape(self):\n    self._testUnpackShape()\n\n  @test_util.deprecated_graph_mode_only\n  def testSplitShape(self):\n    with self.session():\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32,\n          tensor_array_name=\"foo\",\n          size=0,\n          dynamic_size=True,\n          infer_shape=True)\n      value = constant_op.constant([[1.0, -1.0], [2.0, -2.0], [3.0, -3.0]])\n      w0 = ta.split(value, [1, 1, 1])\n      r0 = w0.read(0)\n      self.assertAllEqual((1, 2), r0.get_shape())\n\n      ta1 = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32,\n          tensor_array_name=\"foo1\",\n          size=0,\n          dynamic_size=True,\n          infer_shape=True)\n      w0 = ta1.split(value, [1, 2])\n      r0 = w0.read(0)\n      if context.executing_eagerly():\n        self.assertEqual((1, 2), r0.get_shape())\n        self.assertEqual((2, 2), w0.read(1).get_shape())\n      else:\n        self.assertEqual(r0.get_shape().ndims, None)\n        if not control_flow_util.ENABLE_CONTROL_FLOW_V2:\n          self.assertEqual(\n              tensor_shape.TensorShape(\n                  ta1.handle.op.get_attr(\"element_shape\")).ndims, None)\n\n  @test_util.deprecated_graph_mode_only\n  def testSkipEagerWriteUnknownShape(self):\n    with self.session():\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32,\n          tensor_array_name=\"foo\",\n          size=3,\n          infer_shape=True)\n      c0 = array_ops.placeholder(dtypes.float32)\n      w0 = ta.write(0, c0)\n      r0 = w0.read(0)\n      self.assertAllEqual(r0.get_shape(), tensor_shape.unknown_shape())\n\n  def _testGradientWhenNotAllComponentsRead(self):\n    with self.cached_session() as session:\n      ta = tensor_array_ops.TensorArray(dtype=dtypes.float32, size=2)\n      x = constant_op.constant([2.0, 3.0])\n      w = ta.unstack(x)\n      r0 = w.read(0)\n      # calculate (dr0/dx0, dr0/dx1).  since r0 = x0, gradients are (1, 0).\n      grad_r0 = gradients_impl.gradients(ys=[r0], xs=[x], grad_ys=[1.0])\n      grad_r0_vals = session.run(grad_r0)[0]\n      self.assertAllEqual(grad_r0_vals, [1.0, 0.0])\n\n  @test_util.deprecated_graph_mode_only\n  def testSkipEagerGradientWhenNotAllComponentsRead(self):\n    self._testGradientWhenNotAllComponentsRead()\n\n  @test_util.deprecated_graph_mode_only\n  def testSkipEagerWriteButNotAllComponentsReadGrad(self):\n    with self.cached_session() as session:\n      x0 = constant_op.constant(5.0)\n      x1 = constant_op.constant(10.0)\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32, size=2).write(0, x0).write(1, x1)\n      r0 = ta.read(0)\n      # calculate (dr0/dx0, dr0/dx1).  since r0 = x0, gradients are (1, 0).\n      grad_r0_x1 = gradients_impl.gradients(ys=[r0], xs=[x0, x1], grad_ys=[1.0])\n      grad_r0_x1_vals = session.run(grad_r0_x1)\n      self.assertAllEqual(grad_r0_x1_vals, [1.0, 0.0])\n\n  def _testTensorArrayUnpackDynamic(self):\n    with self.cached_session():\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32, size=3, dynamic_size=True)\n      x = constant_op.constant([1.0, 2.0, 3.0])\n      w0 = ta.unstack(x)\n      w1 = w0.write(3, 4.0)\n      r = w1.stack()\n      self.assertAllEqual(np.array([1.0, 2.0, 3.0, 4.0]), self.evaluate(r))\n      grad = gradients_impl.gradients(ys=[r], xs=[x])\n      self.assertAllEqual(np.array([1.0, 1.0, 1.0]), self.evaluate(grad)[0])\n\n  @test_util.run_deprecated_v1\n  def testSkipEagerTensorArrayUnpackDynamic(self):\n    self._testTensorArrayUnpackDynamic()\n\n  @test_util.run_deprecated_v1\n  def testSkipEagerTensorArraySplitDynamic(self):\n    with self.session():\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32, size=3, dynamic_size=True)\n      x = constant_op.constant([1.0, 2.0, 3.0])\n      w0 = ta.split(x, [1, 1, 1])\n      w1 = w0.write(3, [4.0])\n      r = w1.concat()\n      self.assertAllEqual(np.array([1.0, 2.0, 3.0, 4.0]), self.evaluate(r))\n      grad = gradients_impl.gradients(ys=[r], xs=[x])\n      self.assertAllEqual(np.array([1.0, 1.0, 1.0]), self.evaluate(grad)[0])\n\n  def testStackShape(self):\n\n    @def_function.function\n    def ta_stack():\n      ta = tensor_array_ops.TensorArray(dtype=dtypes.float32, size=3)\n      x = constant_op.constant([1.0, 2.0, 3.0])\n      ta = ta.write(0, x)\n      t = ta.stack()\n      self.assertEqual(t.shape.as_list(), [3, 3])\n      return t\n\n    ta_stack()\n\n  def testReadShape(self):\n\n    @def_function.function\n    def ta_read():\n      ta = tensor_array_ops.TensorArray(dtype=dtypes.float32, size=3)\n      x = constant_op.constant([1.0, 2.0, 3.0])\n      ta = ta.write(0, x)\n      t = ta.read(0)\n      self.assertEqual(t.shape.as_list(), [3])\n      return t\n\n    ta_read()\n\n  def testGatherShape(self):\n\n    def ta_gather(indices):\n      ta = tensor_array_ops.TensorArray(dtype=dtypes.float32, size=3)\n      x = constant_op.constant([1.0, 2.0, 3.0])\n      ta = ta.write(0, x)\n      t = ta.gather(indices)\n      self.assertEqual(t.shape.as_list(), [first_dim, 3])\n      return t\n\n    # This propagates shape of `indices` when compiling ta_gather.\n    ta_gather_with_known_indices_shape = def_function.function(ta_gather)\n    first_dim = 1\n    ta_gather_with_known_indices_shape([0])\n\n    # Here were force the shape of `indices` to be [None] during ta_gather's\n    # compilation.\n    ta_gather_with_unknown_indices_shape = def_function.function(\n        ta_gather,\n        input_signature=[\n            tensor_spec.TensorSpec(dtype=dtypes.int32, shape=[None])\n        ])\n    first_dim = None\n    ta_gather_with_unknown_indices_shape([0])\n\n  def _testTensorArrayEvalEmpty(self):\n    with self.cached_session():\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32, size=0, dynamic_size=False, infer_shape=False)\n      v2_msg = (\"Tried to stack elements of an empty list with \"\n                \"non-fully-defined element_shape\")\n      v1_msg = (\n          \"TensorArray has size zero, but element shape <unknown> is not \"\n          \"fully defined. Currently only static shapes are supported when \"\n          \"packing zero-size TensorArrays.\")\n      with self.assertRaisesOpError(\n          v2_msg if control_flow_util.ENABLE_CONTROL_FLOW_V2 else v1_msg):\n        ta.stack().eval()\n\n  @test_util.run_deprecated_v1\n  def testSkipEagerTensorArrayEvalEmpty(self):\n    self._testTensorArrayEvalEmpty()\n\n  # this test is ill-defined for Eager mode --- unpacking an empty tensor\n  # gives an empty list / there is not equivalent of \"mark_used\" in Eager\n  def _testTensorArrayEvalEmptyWithDefault(self):\n    with self.cached_session():\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32, size=0, dynamic_size=False, infer_shape=True)\n      self.assertEqual(0, ta.size().eval())\n      # Don't actually perform the pack.  This stores the static shape.\n      if control_flow_util.ENABLE_CONTROL_FLOW_V2:\n        ta = ta.unstack(array_ops.zeros([0, 3, 5]))\n      else:\n        ta.unstack(array_ops.zeros([0, 3, 5])).mark_used()\n      packed = ta.stack()\n      concatenated = ta.concat()\n      self.assertAllEqual([0, 3, 5], self.evaluate(packed).shape)\n      # Concatenating zero tensors along their first dimension gives a\n      # first dimension of zero\n      self.assertAllEqual([0, 5], self.evaluate(concatenated).shape)\n\n  @test_util.run_deprecated_v1\n  def testSkipEagerTensorArrayEvalEmptyWithDefault(self):\n    self._testTensorArrayEvalEmptyWithDefault()\n\n  @test_util.run_deprecated_v1\n  def testSkipEagerTensorArrayScatterReadAndGradients(self):\n    with self.session() as session:\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32,\n          tensor_array_name=\"foo\",\n          size=0,\n          dynamic_size=True)\n\n      indices = constant_op.constant([1, 8])\n      value = constant_op.constant([[1.0, -1.0], [10.0, -10.0]])\n\n      w = ta.scatter(indices, value)\n      r0 = w.read(1)\n      r1 = w.read(8)\n\n      # Test combined gradients + aggregation of read(0)\n      grad = gradients_impl.gradients(\n          ys=[r0, r1], xs=[value], grad_ys=[[2.0, 3.0], [4.0, 5.0]])\n      read_vals, grad_vals = session.run([[r0, r1], grad])\n\n      self.assertEqual(len(read_vals), 2)\n      self.assertEqual(len(grad_vals), 1)\n      self.assertAllEqual([1.0, -1.0], read_vals[0])\n      self.assertAllEqual([10.0, -10.0], read_vals[1])\n      self.assertAllEqual([[2.0, 3.0], [4.0, 5.0]], grad_vals[0])\n\n  @test_util.run_deprecated_v1\n  def testSkipEagerTensorArrayScatterPartialReadAndGradients(self):\n    with self.session() as session:\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32,\n          tensor_array_name=\"foo\",\n          size=0,\n          dynamic_size=True)\n\n      indices = constant_op.constant([1, 8])\n      value = constant_op.constant([[1.0, -1.0], [10.0, -10.0]])\n\n      w = ta.scatter(indices, value)\n      r0 = w.read(1)\n\n      # Test combined gradients + aggregation of read(0)\n      grad = gradients_impl.gradients(\n          ys=[r0], xs=[value], grad_ys=[[2.0, 3.0]])[0]\n      read_val, grad_val = session.run([r0, grad])\n\n      self.assertAllEqual([1.0, -1.0], read_val)\n      self.assertAllEqual([[2.0, 3.0], [0.0, 0.0]], grad_val)\n\n  def testScatterIntoExistingList(self):\n    ta = tensor_array_ops.TensorArray(\n        dtype=dtypes.float32, tensor_array_name=\"foo\", size=5)\n\n    ta = ta.scatter(indices=[3, 4], value=array_ops.ones([2]))\n    self.assertAllEqual(ta.stack(), [0., 0., 0., 1., 1.])\n\n    ta = ta.scatter(indices=[1], value=array_ops.ones([1]))\n    self.assertAllEqual(ta.stack(), [0., 1., 0., 1., 1.])\n\n    ta = ta.scatter(indices=[0, 2], value=[5., 6.])\n    self.assertAllEqual(ta.stack(), [5., 1., 6., 1., 1.])\n\n  @test_util.run_v1_only(\"b/118890905\")\n  def testTensorArrayWriteGatherAndGradients(self):\n    with self.session() as session:\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32,\n          tensor_array_name=\"foo\",\n          size=0,\n          dynamic_size=True)\n\n      def func(values):\n        indices = constant_op.constant([1, 8])\n        w = ta.unstack(values)\n        g = w.gather(indices)\n        return g\n\n      values = constant_op.constant([[1.0 * x, -1.0 * x] for x in range(10)])\n      g = func(values)\n      grad_ys = [[[2.0, 3.0], [4.0, 5.0]]]\n      # Test combined gradients + aggregation of read(0)\n      if context.executing_eagerly():\n        g_vals = [g]\n        grad_vals = backprop.gradients_function(func)(\n            values, dy=constant_op.constant(grad_ys[0], dtype=dtypes.float32))\n      else:\n        grad = gradients_impl.gradients(ys=[g], xs=[values], grad_ys=grad_ys)\n        g_vals, grad_vals = session.run([[g], grad])\n\n      # Gradients for 8 of the 10 unread components are zero.\n      expected_grad = np.zeros((10, 2))\n      expected_grad[1] = [2.0, 3.0]\n      expected_grad[8] = [4.0, 5.0]\n\n      self.assertEqual(len(g_vals), 1)\n      self.assertEqual(len(grad_vals), 1)\n      self.assertAllEqual([[1.0, -1.0], [8.0, -8.0]], g_vals[0])\n      self.assertAllEqual(expected_grad, grad_vals[0])\n\n  @test_util.disable_control_flow_v2(\"colocate_with not supported in v2.\")\n  @test_util.run_v1_only(\"b/120545219\")\n  def testSkipEagerTensorArrayGetsDeviceFromFirstWrite(self):\n    with ops.device(\"/job:worker/task:0/cpu:0\"):\n      # this initial device will be ignored.\n      ta = tensor_array_ops.TensorArray(dtype=dtypes.float32, size=2)\n    with ops.device(\"/job:worker/task:1/cpu:0\"):\n      # the first write sets the op's device.\n      ta = ta.write(0, 1.0)\n    with ops.device(\"/job:worker/task:2/cpu:0\"):\n      # subsequent writes do not modify the op's device.\n      ta = ta.write(1, 1.0)\n\n    # The gradient TA will sit on the same device as the forward TA.\n    ta_grad = ta.grad(\"grad\")\n    flows = [ta.flow, ta_grad.flow]\n\n    # Similar tests for unpack and split\n    with ops.device(\"/job:worker/task:0/cpu:0\"):\n      ta = tensor_array_ops.TensorArray(dtype=dtypes.float32, size=3)\n    with ops.device(\"/job:worker/task:1/cpu:0\"):\n      ta = ta.unstack([1.0, 2.0])\n    with ops.device(\"/job:worker/task:2/cpu:0\"):\n      ta = ta.write(2, 3.0)\n    flows.append(ta.flow)\n\n    with ops.device(\"/job:worker/task:0/cpu:0\"):\n      ta = tensor_array_ops.TensorArray(dtype=dtypes.float32, size=2)\n    with ops.device(\"/job:worker/task:1/cpu:0\"):\n      ta = ta.split([1.0, 2.0], [1, 1])\n    flows.append(ta.flow)\n\n    session = session_lib.Session(self._workers[0].target)\n\n    run_options = config_pb2.RunOptions(\n        trace_level=config_pb2.RunOptions.FULL_TRACE)\n    run_metadata = config_pb2.RunMetadata()\n\n    session.run(flows, options=run_options, run_metadata=run_metadata)\n    self.assertTrue(run_metadata.HasField(\"step_stats\"))\n    dev_stats = {d.device: d.node_stats\n                 for d in run_metadata.step_stats.dev_stats}\n    for d in dev_stats:\n      if \"/task:1/\" in d:\n        self.assertTrue(\n            [s for s in dev_stats[d] if \"/TensorArray\" in s.node_name])\n      elif \"/host:CPU\" not in d:\n        self.assertFalse(\n            [s for s in dev_stats[d] if \"/TensorArray\" in s.node_name])\n\n  @test_util.disable_control_flow_v2(\"colocate_with not supported in v2.\")\n  @test_util.run_v1_only(\"b/120545219\")\n  def testSkipEagerTensorArrayGetsDeviceFromFirstWriteInWhileLoop(self):\n    with ops.device(\"/job:worker/task:0/cpu:0\"):\n      ta = tensor_array_ops.TensorArray(dtype=dtypes.float32, size=2)\n\n    def _body(i, ta_i):\n      with ops.device(\"/job:worker/task:1/cpu:0\"):\n        return i + 1, ta_i.write(i, constant_op.constant(0.0))\n\n    _, ta_out = control_flow_ops.while_loop(\n        lambda i, ta: i < 2, _body, loop_vars=[0, ta])\n\n    session = session_lib.Session(self._workers[0].target)\n\n    run_options = config_pb2.RunOptions(\n        trace_level=config_pb2.RunOptions.FULL_TRACE)\n    run_metadata = config_pb2.RunMetadata()\n\n    session.run(ta_out.flow, options=run_options, run_metadata=run_metadata)\n    self.assertTrue(run_metadata.HasField(\"step_stats\"))\n    dev_stats = {d.device: d.node_stats\n                 for d in run_metadata.step_stats.dev_stats}\n    for d in dev_stats:\n      if \"/task:1/\" in d:\n        self.assertTrue(\n            [s for s in dev_stats[d] if \"TensorArray\" == s.node_name])\n      else:\n        self.assertFalse(\n            [s for s in dev_stats[d] if \"TensorArray\" == s.node_name])\n\n  @test_util.disable_control_flow_v2(\"colocate_with not supported in v2.\")\n  @test_util.run_v1_only(\"b/120545219\")\n  def testSkipEagerTensorArrayDisabledColocateWithFirstWriteCall(self):\n    with ops.device(\"/job:worker/task:0/cpu:0\"):\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.float32, size=2, colocate_with_first_write_call=False)\n\n    def _body(i, ta_i):\n      with ops.device(\"/job:worker/task:1/cpu:0\"):\n        return i + 1, ta_i.write(i, constant_op.constant(0.0))\n\n    _, ta_out = control_flow_ops.while_loop(\n        lambda i, ta: i < 2, _body, loop_vars=[0, ta])\n\n    session = session_lib.Session(self._workers[0].target)\n\n    run_options = config_pb2.RunOptions(\n        trace_level=config_pb2.RunOptions.FULL_TRACE)\n    run_metadata = config_pb2.RunMetadata()\n\n    session.run(ta_out.flow, options=run_options, run_metadata=run_metadata)\n    self.assertTrue(run_metadata.HasField(\"step_stats\"))\n    dev_stats = {d.device: list(d.node_stats)\n                 for d in run_metadata.step_stats.dev_stats}\n    for d in dev_stats:\n      if \"/task:0/\" in d and \"CPU\" in d:  # Skip any GPU node stats\n        self.assertTrue(\n            [s for s in dev_stats[d] if \"TensorArray\" == s.node_name])\n      else:\n        self.assertFalse(\n            [s for s in dev_stats[d] if \"TensorArray\" == s.node_name])\n\n  def testTensorArrayIdentity(self):\n    with self.session():\n      ta0 = tensor_array_ops.TensorArray(dtype=dtypes.float32, size=2,\n                                         infer_shape=False)\n      ta1 = tensor_array_ops.TensorArray(dtype=dtypes.int32, size=4,\n                                         infer_shape=True)\n\n      ta0 = ta0.write(0, 0.)\n      ta1 = ta1.write(0, 1)\n\n      v0 = variable_scope.get_variable(\n          \"v0\", shape=(), initializer=init_ops.zeros_initializer())\n      v1 = variable_scope.get_variable(\n          \"v1\", shape=(), initializer=init_ops.zeros_initializer())\n\n      with ops.control_dependencies([v0.assign_add(1)]):\n        ta0 = ta0.identity()\n\n      with ops.control_dependencies([v1.assign_add(1)]):\n        ta1 = ta1.identity()\n\n      read0 = ta0.read(0)\n      read1 = ta1.read(0)\n\n      size0 = ta0.size()\n      size1 = ta1.size()\n\n      # Tests correct properties on new TensorArrays.\n      self.assertEqual(dtypes.float32, ta0.dtype)\n      self.assertEqual(dtypes.int32, ta1.dtype)\n      if context.executing_eagerly():\n        self.assertEqual(tensor_shape.TensorShape([]), read0.get_shape())\n      else:\n        self.assertEqual(tensor_shape.unknown_shape(), read0.get_shape())\n      self.assertEqual(tensor_shape.TensorShape([]), read1.get_shape())\n\n      if not context.executing_eagerly():\n        self.evaluate(variables.global_variables_initializer())\n\n      read0_v, read1_v, size0_v, size1_v = self.evaluate((read0, read1, size0,\n                                                          size1))\n\n      # Tests that the control dependencies was added and executed.\n      self.assertEqual(1, self.evaluate(v0))\n      self.assertEqual(1, self.evaluate(v1))\n\n      # Tests correct TensorArray.\n      self.assertEqual(read0_v, 0)\n      self.assertEqual(read1_v, 1)\n      self.assertEqual(size0_v, 2)\n      self.assertEqual(size1_v, 4)\n\n  @test_util.deprecated_graph_mode_only\n  def testSkipEagerTensorArrayGradYsInCorrectScope(self):\n    n_time = 1\n    n_dim = 1\n    x = constant_op.constant([[1.42]])\n    dy = constant_op.constant([[2.42]])\n\n    ta = tensor_array_ops.TensorArray(\n        dtypes.float32, size=n_time, element_shape=[n_dim])\n    for t in range(n_time):\n      ta = ta.write(index=t, value=x[t])\n      y = ta.stack()\n      # dy is outside of the gradients name scope; tf.gradients must\n      # wrap it in the correct name scope.\n      dx, = gradients_impl.gradients(ys=[y], xs=[x], grad_ys=[dy])\n      with self.cached_session():\n        vdx, vdy = self.evaluate([dx, dy])\n      self.assertAllClose(vdx, vdy)\n\n  @test_util.deprecated_graph_mode_only\n  def testSkipEagerTensorArrayInt64GPU(self):\n    if not test.is_gpu_available():\n      return\n    with self.session(force_gpu=True) as sess:\n      value = array_ops.placeholder(dtypes.int64)\n      ta = tensor_array_ops.TensorArray(dtype=dtypes.int64, size=2)\n      ta = ta.scatter([0, 1], value)\n      r0 = ta.read(0)\n      r1 = ta.read(1)\n      v0, v1 = sess.run([r0, r1], feed_dict={value: [-3, 100]})\n      self.assertAllEqual(v0, -3)\n      self.assertAllEqual(v1, 100)\n\n  @test_util.deprecated_graph_mode_only\n  def testTensorArrayScatterBfloat16GPU(self):\n    if not test.is_gpu_available():\n      return\n    with self.session(force_gpu=True) as sess:\n      ta = tensor_array_ops.TensorArray(\n          dtype=dtypes.bfloat16, tensor_array_name=\"foo\", size=5)\n      ta = ta.scatter(\n          indices=[3, 4], value=array_ops.ones([2], dtype=dtypes.bfloat16))\n      self.assertAllEqual(ta.stack(), [0., 0., 0., 1., 1.])\n\n  def testInferShapeFalseValid(self):\n    ta = tensor_array_ops.TensorArray(\n        dtypes.float32, size=3, infer_shape=False, element_shape=[None, 10, 20])\n    ta = ta.write(0, array_ops.ones([50, 10, 20]))\n    ta = ta.write(1, array_ops.ones([50, 10, 20]))\n    ta = ta.write(2, array_ops.ones([1, 10, 20]))\n    ta = ta.concat()\n\n    correct = np.ones([101, 10, 20])\n\n    self.assertAllEqual(ta, correct)\n\n  def testInferShapeFalseInvalid(self):\n    ta = tensor_array_ops.TensorArray(\n        dtypes.float32, size=2, infer_shape=False, element_shape=[None, 10, 20])\n    ta = ta.write(0, array_ops.ones([50, 10, 20]))\n\n    with self.assertRaises(ValueError):\n      ta = ta.write(1, array_ops.ones([1, 20, 20]))\n\n  def testInferShapeTrue(self):\n    ta = tensor_array_ops.TensorArray(\n        dtypes.float32, size=3, infer_shape=True, element_shape=[None, 10, 20])\n    self.assertAllEqual((None, 10, 20), ta.element_shape.as_list())\n    ta = ta.write(0, array_ops.ones([50, 10, 20]))\n    self.assertAllEqual((50, 10, 20), ta.element_shape.as_list())\n    ta = ta.write(1, array_ops.ones([50, 10, 20]))\n    with self.assertRaises(ValueError):\n      ta = ta.write(\n          2, array_ops.ones([1, 10, 20])\n      )  # Inconsistent shapes: saw (1, 10, 20) but expected (50, 10, 20)\n\n  def testStackShapeOnEmpty(self):\n    ta = tensor_array_ops.TensorArray(\n        dtypes.float32, size=0, element_shape=(5, 10), dynamic_size=True)\n    self.assertAllEqual([0, 5, 10], self.evaluate(ta.stack()).shape)\n\n  @test_util.run_deprecated_v1\n  def testSkipEagerStackOnPartiallyDefinedShape(self):\n    ta = tensor_array_ops.TensorArray(\n        dtypes.float32, size=0, element_shape=(5, None), dynamic_size=True)\n    self.assertEqual([None, 5, None], ta.stack().shape.as_list())\n\n  def testStackShapeOnStaticSize(self):\n    ta = tensor_array_ops.TensorArray(dtypes.float32, size=42)\n    ta = ta.write(0, [0])\n    self.assertEqual([42, 1], ta.stack().shape.as_list())\n\n  def testTensorArrayConcatFailsWhenMissingStepContainer(self):\n    @def_function.function\n    def func():\n      y = data_flow_ops.TensorArrayConcatV2(\n          handle=[\"a\", \"b\"],\n          flow_in=0.1,\n          dtype=dtypes.int32,\n          element_shape_except0=1,\n      )\n      return y\n\n    with self.assertRaisesRegex(\n        errors.NotFoundError, \"Container .* does not exist\"\n    ):\n      self.evaluate(func())\n\n\nclass TensorArrayBenchmark(test.Benchmark):\n\n  def _tensorArrayWriteInWhile(self):\n    size = 10000\n    ta = tensor_array_ops.TensorArray(dtype=dtypes.float32, size=size)\n    (_, ta) = control_flow_ops.while_loop(\n        lambda i, _: i < size,\n        lambda i, ta: (i + 1, ta.write(i, 0.)), [0, ta],\n        parallel_iterations=1)\n    return ta.stack()\n\n  def _benchmarkWriteInWhile(self):\n    ops.reset_default_graph()\n    op = self._tensorArrayWriteInWhile()\n    self.run_op_benchmark(session_lib.Session(), op)\n\n  def benchmarkWriteInWhile(self):\n    self._benchmarkWriteInWhile()\n\n  @test_util.enable_control_flow_v2\n  def benchmarkWriteInWhileWithControlFlowV2(self):\n    self._benchmarkWriteInWhile()\n\n  def benchmarkWriteInDatasetMapFn(self):\n    ds = dataset_ops.Dataset.from_tensors(array_ops.zeros([10])).repeat()\n    ds = ds.map(lambda _: self._tensorArrayWriteInWhile())\n    op = ds.make_one_shot_iterator().get_next()\n    self.run_op_benchmark(session_lib.Session(), op)\n\n  def benchmarkWriteInDatasetParallelMapFn(self):\n    ds = dataset_ops.Dataset.from_tensors(array_ops.zeros([10])).repeat()\n    ds = ds.map(lambda _: self._tensorArrayWriteInWhile(), num_parallel_calls=2)\n    op = ds.make_one_shot_iterator().get_next()\n    self.run_op_benchmark(session_lib.Session(), op)\n\n\nif __name__ == \"__main__\":\n  test.main()\n"], "buggy_code_start_loc": [83, 1848], "buggy_code_end_loc": [85, 1848], "fixing_code_start_loc": [83, 1849], "fixing_code_end_loc": [86, 1865], "type": "CWE-476", "message": "TensorFlow is an open source platform for machine learning. Prior to versions 2.12.0 and 2.11.1, when `ctx->step_containter()` is a null ptr, the Lookup function will be executed with a null pointer. A fix is included in TensorFlow 2.12.0 and 2.11.1.", "other": {"cve": {"id": "CVE-2023-25663", "sourceIdentifier": "security-advisories@github.com", "published": "2023-03-25T00:15:07.313", "lastModified": "2023-03-30T17:42:24.500", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "TensorFlow is an open source platform for machine learning. Prior to versions 2.12.0 and 2.11.1, when `ctx->step_containter()` is a null ptr, the Lookup function will be executed with a null pointer. A fix is included in TensorFlow 2.12.0 and 2.11.1."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 7.5, "baseSeverity": "HIGH"}, "exploitabilityScore": 3.9, "impactScore": 3.6}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 7.5, "baseSeverity": "HIGH"}, "exploitabilityScore": 3.9, "impactScore": 3.6}]}, "weaknesses": [{"source": "security-advisories@github.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-476"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionEndExcluding": "2.12.0", "matchCriteriaId": "FAC3DE54-93B4-4D6C-9648-B9D416B9770F"}]}]}], "references": [{"url": "https://github.com/tensorflow/tensorflow/commit/239139d2ae6a81ae9ba499ad78b56d9b2931538a", "source": "security-advisories@github.com", "tags": ["Patch"]}, {"url": "https://github.com/tensorflow/tensorflow/security/advisories/GHSA-64jg-wjww-7c5w", "source": "security-advisories@github.com", "tags": ["Patch", "Vendor Advisory"]}]}, "github_commit_url": "https://github.com/tensorflow/tensorflow/commit/239139d2ae6a81ae9ba499ad78b56d9b2931538a"}}