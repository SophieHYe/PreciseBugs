{"buggy_code": ["/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include \"tensorflow/core/kernels/linalg/linalg_ops_common.h\"\n\n#include <utility>\n\n#include \"third_party/eigen3/Eigen/Core\"\n#include \"tensorflow/core/framework/device_base.h\"\n#include \"tensorflow/core/framework/kernel_def_builder.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/tensor_shape.h\"\n#include \"tensorflow/core/lib/core/errors.h\"\n#include \"tensorflow/core/platform/logging.h\"\n#include \"tensorflow/core/platform/types.h\"\n\nnamespace tensorflow {\n\n// static\ntemplate <class InputScalar, class OutputScalar>\nvoid LinearAlgebraOp<InputScalar, OutputScalar>::ValidateSingleMatrix(\n    OpKernelContext* context, const TensorShapes& input_matrix_shapes) {\n  OP_REQUIRES(context, input_matrix_shapes.size() == 1,\n              errors::InvalidArgument(\"Expected a single input matrix, got %d.\",\n                                      input_matrix_shapes.size()));\n  OP_REQUIRES(context, TensorShapeUtils::IsMatrix(input_matrix_shapes[0]),\n              errors::InvalidArgument(\"Input must be a matrix.\"));\n}\n\n// static\ntemplate <class InputScalar, class OutputScalar>\nvoid LinearAlgebraOp<InputScalar, OutputScalar>::ValidateSingleSquareMatrix(\n    OpKernelContext* context, const TensorShapes& input_matrix_shapes) {\n  OP_REQUIRES(context, input_matrix_shapes.size() == 1,\n              errors::InvalidArgument(\"Expected a single input matrix, got %d.\",\n                                      input_matrix_shapes.size()));\n  OP_REQUIRES(context, TensorShapeUtils::IsSquareMatrix(input_matrix_shapes[0]),\n              errors::InvalidArgument(\"Input matrix must be square.\"));\n}\n\n// static\ntemplate <class InputScalar, class OutputScalar>\nvoid LinearAlgebraOp<InputScalar, OutputScalar>::ValidateSolver(\n    OpKernelContext* context, const TensorShapes& input_matrix_shapes) {\n  OP_REQUIRES(context, input_matrix_shapes.size() == 2,\n              errors::InvalidArgument(\"Expected two input matrices, got %d.\",\n                                      input_matrix_shapes.size()));\n  OP_REQUIRES(context, TensorShapeUtils::IsMatrix(input_matrix_shapes[0]),\n              errors::InvalidArgument(\"First input (lhs) must be a matrix.\"));\n  OP_REQUIRES(context, TensorShapeUtils::IsMatrix(input_matrix_shapes[1]),\n              errors::InvalidArgument(\"Second input (rhs) must be a matrix.\"));\n  OP_REQUIRES(\n      context,\n      input_matrix_shapes[0].dim_size(0) == input_matrix_shapes[1].dim_size(0),\n      errors::InvalidArgument(\"Input matrix and rhs are incompatible.\"));\n}\n\n// static\ntemplate <class InputScalar, class OutputScalar>\nvoid LinearAlgebraOp<InputScalar, OutputScalar>::ValidateSquareSolver(\n    OpKernelContext* context, const TensorShapes& input_matrix_shapes) {\n  OP_REQUIRES(context, input_matrix_shapes.size() == 2,\n              errors::InvalidArgument(\"Expected two input matrices, got %d.\",\n                                      input_matrix_shapes.size()));\n  OP_REQUIRES(\n      context, TensorShapeUtils::IsSquareMatrix(input_matrix_shapes[0]),\n      errors::InvalidArgument(\"First input (lhs) must be a square matrix.\"));\n  OP_REQUIRES(context, TensorShapeUtils::IsMatrix(input_matrix_shapes[1]),\n              errors::InvalidArgument(\"Second input (rhs) must be a matrix.\"));\n  OP_REQUIRES(\n      context,\n      input_matrix_shapes[0].dim_size(0) == input_matrix_shapes[1].dim_size(0),\n      errors::InvalidArgument(\"Input matrix and rhs are incompatible.\"));\n}\n\ntemplate <class InputScalar, class OutputScalar>\nvoid LinearAlgebraOp<InputScalar, OutputScalar>::Compute(\n    OpKernelContext* context) {\n  TensorInputs inputs;\n  TensorShapes input_matrix_shapes;\n  TensorShape batch_shape;\n  AnalyzeInputs(context, &inputs, &input_matrix_shapes, &batch_shape);\n  if (!context->status().ok()) return;\n\n  TensorShapes output_matrix_shapes;\n  TensorOutputs outputs;\n  PrepareOutputs(context, input_matrix_shapes, batch_shape, &outputs,\n                 &output_matrix_shapes);\n  if (!context->status().ok()) return;\n\n  // Process the individual matrix problems in parallel using a threadpool.\n  auto shard = [this, &inputs, &input_matrix_shapes, &outputs,\n                &output_matrix_shapes, context](int64_t begin, int64_t end) {\n    for (int64_t i = begin; i < end; ++i) {\n      ComputeTensorSlice(context, i, inputs, input_matrix_shapes, outputs,\n                         output_matrix_shapes);\n    }\n  };\n  auto worker_threads = *(context->device()->tensorflow_cpu_worker_threads());\n  Shard(worker_threads.num_threads, worker_threads.workers,\n        batch_shape.num_elements(), GetCostPerUnit(input_matrix_shapes), shard);\n}\n\ntemplate <class InputScalar, class OutputScalar>\nvoid LinearAlgebraOp<InputScalar, OutputScalar>::AnalyzeInputs(\n    OpKernelContext* context, TensorInputs* inputs,\n    TensorShapes* input_matrix_shapes, TensorShape* batch_shape) {\n  int input_rank = -1;\n  for (int i = 0; i < NumMatrixInputs(context); ++i) {\n    const Tensor& in = context->input(i);\n    if (i == 0) {\n      input_rank = in.dims();\n      OP_REQUIRES(\n          context, input_rank >= 2,\n          errors::InvalidArgument(\"Input tensor \", i,\n                                  \" must have rank >= 2, got \", input_rank));\n      // If the tensor rank is greater than 2, we consider the inner-most\n      // dimensions as matrices, and loop over all the other outer (\"batch\")\n      // dimensions to compute the results.\n      for (int dim = 0; dim < input_rank - 2; ++dim) {\n        batch_shape->AddDim(in.dim_size(dim));\n      }\n    } else {\n      // Make sure that all inputs have the same rank and outer dimensions.\n      OP_REQUIRES(context, input_rank == in.dims(),\n                  errors::InvalidArgument(\n                      \"All input tensors must have the same rank.\"));\n      for (int dim = 0; dim < input_rank - 2; ++dim) {\n        OP_REQUIRES(\n            context, in.dim_size(dim) == batch_shape->dim_size(dim),\n            errors::InvalidArgument(\n                \"All input tensors must have the same outer dimensions.\"));\n      }\n    }\n\n    const int row_dimension = input_rank - 2;\n    const int col_dimension = input_rank - 1;\n    const int64_t num_rows = in.dim_size(row_dimension);\n    const int64_t num_cols = in.dim_size(col_dimension);\n    input_matrix_shapes->emplace_back(\n        std::initializer_list<int64_t>({num_rows, num_cols}));\n    inputs->emplace_back(&in);\n  }\n  // Have the derived class validate that the inputs are as expected.\n  ValidateInputMatrixShapes(context, *input_matrix_shapes);\n}\n\ntemplate <class InputScalar, class OutputScalar>\nvoid LinearAlgebraOp<InputScalar, OutputScalar>::PrepareOutputs(\n    OpKernelContext* context, const TensorShapes& input_matrix_shapes,\n    const TensorShape& batch_shape, TensorOutputs* outputs,\n    TensorShapes* output_matrix_shapes) {\n  // Get shape for each of the matrix outputs produced by the derived class.\n  *output_matrix_shapes = GetOutputMatrixShapes(input_matrix_shapes);\n  const int num_outputs = output_matrix_shapes->size();\n\n  // Make sure the number of op outputs is what the derived class expects.\n  OP_REQUIRES(\n      context, num_outputs <= context->num_outputs(),\n      errors::Internal(\n          \"Derived class expected more outputs (%d) that the op has (%d).\",\n          num_outputs, context->num_outputs()));\n\n  // Allocate outputs.\n  std::set<int> unused_inputs;\n  for (int input_idx = 0; input_idx < context->num_inputs(); ++input_idx) {\n    unused_inputs.insert(input_idx);\n  }\n  for (int output_idx = 0; output_idx < context->num_outputs(); ++output_idx) {\n    TensorShape output_tensor_shape({});\n    if (output_idx < num_outputs) {\n      // This output is used, set up output shape and allocate it.\n      const TensorShape& output_matrix_shape =\n          output_matrix_shapes->at(output_idx);\n      OP_REQUIRES(context, output_matrix_shape.dims() <= 2,\n                  errors::InvalidArgument(\n                      \"Rank of matrix output no. %d must be 0, 1 or 2, got %d.\",\n                      output_idx, output_matrix_shape.dims()));\n\n      // The final output has the shape of the outer batch dimensions\n      // concatenated with the output_matrix_shape (if the output is not\n      // scalar).\n      output_tensor_shape = batch_shape;\n      output_tensor_shape.AppendShape(output_matrix_shape);\n    }\n    Tensor* out = nullptr;\n    // See if there is an input buffer we can reuse for this output.\n    bool reused_input = false;\n    if (EnableInputForwarding()) {\n      for (int input_idx : unused_inputs) {\n        if (context->forward_input_to_output_with_shape(\n                input_idx, output_idx, output_tensor_shape, &out)) {\n          reused_input = true;\n          unused_inputs.erase(input_idx);\n          break;\n        }\n      }\n    }\n    if (!reused_input) {\n      OP_REQUIRES_OK(context, context->allocate_output(\n                                  output_idx, output_tensor_shape, &out));\n    }\n    outputs->emplace_back(out);\n  }\n}\n\ntemplate <class InputScalar, class OutputScalar>\nvoid LinearAlgebraOp<InputScalar, OutputScalar>::ComputeTensorSlice(\n    OpKernelContext* context, int64_t matrix_index, const TensorInputs& inputs,\n    const TensorShapes& input_matrix_shapes, const TensorOutputs& outputs,\n    const TensorShapes& output_matrix_shapes) {\n  InputConstMatrixMaps matrix_inputs;\n  for (size_t i = 0; i < inputs.size(); ++i) {\n    // TODO(kalakris): Handle alignment if possible. Eigen::Map is\n    // unaligned by default.\n    matrix_inputs.emplace_back(\n        inputs[i]->flat<InputScalar>().data() +\n            matrix_index * input_matrix_shapes[i].num_elements(),\n        input_matrix_shapes[i].dim_size(0), input_matrix_shapes[i].dim_size(1));\n  }\n\n  OutputMatrixMaps matrix_outputs;\n  for (size_t i = 0; i < output_matrix_shapes.size(); ++i) {\n    // The output matrix shape may not be a matrix.\n    int num_output_rows = output_matrix_shapes[i].dims() >= 1\n                              ? output_matrix_shapes[i].dim_size(0)\n                              : 1;\n    int num_output_cols = output_matrix_shapes[i].dims() == 2\n                              ? output_matrix_shapes[i].dim_size(1)\n                              : 1;\n    matrix_outputs.emplace_back(\n        outputs[i]->flat<OutputScalar>().data() +\n            matrix_index * output_matrix_shapes[i].num_elements(),\n        num_output_rows, num_output_cols);\n  }\n  ComputeMatrix(context, matrix_inputs, &matrix_outputs);\n}\n\n// Explicitly instantiate LinearAlgebraOp for the scalar types we expect to use.\ntemplate class LinearAlgebraOp<Eigen::half>;\ntemplate class LinearAlgebraOp<float>;\ntemplate class LinearAlgebraOp<double>;\ntemplate class LinearAlgebraOp<complex64>;\ntemplate class LinearAlgebraOp<complex128>;\ntemplate class LinearAlgebraOp<float, complex64>;\ntemplate class LinearAlgebraOp<double, complex128>;\n\n}  // namespace tensorflow\n", "# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for tensorflow.ops.linalg_ops.eig.\"\"\"\n\nimport numpy as np\n\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes as dtypes_lib\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import gradient_checker_v2\nfrom tensorflow.python.ops import linalg_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import random_ops\nfrom tensorflow.python.ops import sort_ops\nfrom tensorflow.python.platform import test\n\n\ndef _AddTest(test_class, op_name, testcase_name, fn):\n  test_name = \"_\".join([\"test\", op_name, testcase_name])\n  if hasattr(test_class, test_name):\n    raise RuntimeError(\"Test %s defined more than once\" % test_name)\n  setattr(test_class, test_name, fn)\n\n\nclass EigTest(test.TestCase):\n\n  @test_util.run_deprecated_v1\n  def testWrongDimensions(self):\n    # The input to self_adjoint_eig should be a tensor of\n    # at least rank 2.\n    scalar = constant_op.constant(1.)\n    with self.assertRaises(ValueError):\n      linalg_ops.eig(scalar)\n    vector = constant_op.constant([1., 2.])\n    with self.assertRaises(ValueError):\n      linalg_ops.eig(vector)\n\n  @test_util.run_deprecated_v1\n  def testConcurrentExecutesWithoutError(self):\n    all_ops = []\n    with self.session():\n      for compute_v_ in True, False:\n        matrix1 = random_ops.random_normal([5, 5], seed=42)\n        matrix2 = random_ops.random_normal([5, 5], seed=42)\n        if compute_v_:\n          e1, v1 = linalg_ops.eig(matrix1)\n          e2, v2 = linalg_ops.eig(matrix2)\n          all_ops += [e1, v1, e2, v2]\n        else:\n          e1 = linalg_ops.eigvals(matrix1)\n          e2 = linalg_ops.eigvals(matrix2)\n          all_ops += [e1, e2]\n      val = self.evaluate(all_ops)\n      self.assertAllEqual(val[0], val[2])\n      # The algorithm is slightly different for compute_v being True and False,\n      # so require approximate equality only here.\n      self.assertAllClose(val[2], val[4])\n      self.assertAllEqual(val[4], val[5])\n      self.assertAllEqual(val[1], val[3])\n\n  def testMatrixThatFailsWhenFlushingDenormsToZero(self):\n    # Test a 32x32 matrix which is known to fail if denorm floats are flushed to\n    # zero.\n    matrix = np.genfromtxt(\n        test.test_src_dir_path(\n            \"python/kernel_tests/linalg/testdata/\"\n            \"self_adjoint_eig_fail_if_denorms_flushed.txt\")).astype(np.float32)\n    self.assertEqual(matrix.shape, (32, 32))\n    matrix_tensor = constant_op.constant(matrix)\n    with self.session() as _:\n      (e, v) = self.evaluate(linalg_ops.self_adjoint_eig(matrix_tensor))\n      self.assertEqual(e.size, 32)\n      self.assertAllClose(\n          np.matmul(v, v.transpose()), np.eye(32, dtype=np.float32), atol=2e-3)\n      self.assertAllClose(matrix,\n                          np.matmul(np.matmul(v, np.diag(e)), v.transpose()))\n\n\ndef SortEigenValues(e):\n  perm = np.argsort(e.real + e.imag, -1)\n  return np.take(e, perm, -1)\n\n\ndef SortEigenDecomposition(e, v):\n  if v.ndim < 2:\n    return e, v\n  perm = np.argsort(e.real + e.imag, -1)\n  return np.take(e, perm, -1), np.take(v, perm, -1)\n\n\ndef EquilibrateEigenVectorPhases(x, y):\n  \"\"\"Equilibrate the phase of the Eigenvectors in the columns of `x` and `y`.\n\n  Eigenvectors are only unique up to an arbitrary phase. This function rotates x\n  such that it matches y. Precondition: The columns of x and y differ by a\n  multiplicative complex phase factor only.\n\n  Args:\n    x: `np.ndarray` with Eigenvectors\n    y: `np.ndarray` with Eigenvectors\n\n  Returns:\n    `np.ndarray` containing an equilibrated version of x.\n  \"\"\"\n  phases = np.sum(np.conj(x) * y, -2, keepdims=True)\n  phases /= np.abs(phases)\n  return phases * x\n\n\ndef _GetEigTest(dtype_, shape_, compute_v_):\n\n  def CompareEigenVectors(self, x, y, tol):\n    x = EquilibrateEigenVectorPhases(x, y)\n    self.assertAllClose(x, y, atol=tol)\n\n  def CompareEigenDecompositions(self, x_e, x_v, y_e, y_v, tol):\n    num_batches = int(np.prod(x_e.shape[:-1]))\n    n = x_e.shape[-1]\n    x_e = np.reshape(x_e, [num_batches] + [n])\n    x_v = np.reshape(x_v, [num_batches] + [n, n])\n    y_e = np.reshape(y_e, [num_batches] + [n])\n    y_v = np.reshape(y_v, [num_batches] + [n, n])\n    for i in range(num_batches):\n      x_ei, x_vi = SortEigenDecomposition(x_e[i, :], x_v[i, :, :])\n      y_ei, y_vi = SortEigenDecomposition(y_e[i, :], y_v[i, :, :])\n      self.assertAllClose(x_ei, y_ei, atol=tol, rtol=tol)\n      CompareEigenVectors(self, x_vi, y_vi, tol)\n\n  def Test(self):\n    np.random.seed(1)\n    n = shape_[-1]\n    batch_shape = shape_[:-2]\n    np_dtype = dtype_.as_numpy_dtype\n\n    def RandomInput():\n      # Most matrices are diagonalizable\n      a = np.random.uniform(\n          low=-1.0, high=1.0, size=n * n).reshape([n, n]).astype(np_dtype)\n      if dtype_.is_complex:\n        a += 1j * np.random.uniform(\n            low=-1.0, high=1.0, size=n * n).reshape([n, n]).astype(np_dtype)\n      a = np.tile(a, batch_shape + (1, 1))\n      return a\n\n    if dtype_ in (dtypes_lib.float32, dtypes_lib.complex64):\n      atol = 1e-4\n    else:\n      atol = 1e-12\n\n    a = RandomInput()\n    np_e, np_v = np.linalg.eig(a)\n    with self.session():\n      if compute_v_:\n        tf_e, tf_v = linalg_ops.eig(constant_op.constant(a))\n\n        # Check that V*diag(E)*V^(-1) is close to A.\n        a_ev = math_ops.matmul(\n            math_ops.matmul(tf_v, array_ops.matrix_diag(tf_e)),\n            linalg_ops.matrix_inverse(tf_v))\n        self.assertAllClose(self.evaluate(a_ev), a, atol=atol)\n\n        # Compare to numpy.linalg.eig.\n        CompareEigenDecompositions(self, np_e, np_v, self.evaluate(tf_e),\n                                   self.evaluate(tf_v), atol)\n      else:\n        tf_e = linalg_ops.eigvals(constant_op.constant(a))\n        self.assertAllClose(\n            SortEigenValues(np_e),\n            SortEigenValues(self.evaluate(tf_e)),\n            atol=atol)\n\n  return Test\n\n\nclass EigGradTest(test.TestCase):\n  pass  # Filled in below\n\n\ndef _GetEigGradTest(dtype_, shape_, compute_v_):\n\n  def Test(self):\n    np.random.seed(1)\n    n = shape_[-1]\n    batch_shape = shape_[:-2]\n    np_dtype = dtype_.as_numpy_dtype\n\n    def RandomInput():\n      # Most matrices are diagonalizable\n      a = np.random.uniform(\n          low=-1.0, high=1.0, size=n * n).reshape([n, n]).astype(np_dtype)\n      if dtype_.is_complex:\n        a += 1j * np.random.uniform(\n            low=-1.0, high=1.0, size=n * n).reshape([n, n]).astype(np_dtype)\n      a = np.tile(a, batch_shape + (1, 1))\n      return a\n\n    # Optimal stepsize for central difference is O(epsilon^{1/3}).\n    epsilon = np.finfo(np_dtype).eps\n    delta = 0.1 * epsilon**(1.0 / 3.0)\n    # tolerance obtained by looking at actual differences using\n    # np.linalg.norm(theoretical-numerical, np.inf) on -mavx build\n    # after discarding one random input sample\n    _ = RandomInput()\n    if dtype_ in (dtypes_lib.float32, dtypes_lib.complex64):\n      tol = 1e-2\n    else:\n      tol = 1e-7\n    with self.session():\n\n      def Compute(x):\n        e, v = linalg_ops.eig(x)\n\n        # We sort eigenvalues by e.real+e.imag to have consistent\n        # order between runs\n        b_dims = len(e.shape) - 1\n        idx = sort_ops.argsort(math_ops.real(e) + math_ops.imag(e), axis=-1)\n        e = array_ops.gather(e, idx, batch_dims=b_dims)\n        v = array_ops.gather(v, idx, batch_dims=b_dims)\n\n        # (complex) Eigenvectors are only unique up to an arbitrary phase\n        # We normalize the vectors such that the first component has phase 0.\n        top_rows = v[..., 0:1, :]\n        angle = -math_ops.angle(top_rows)\n        phase = math_ops.complex(math_ops.cos(angle), math_ops.sin(angle))\n        v *= phase\n        return e, v\n\n      if compute_v_:\n        funcs = [lambda x: Compute(x)[0], lambda x: Compute(x)[1]]\n      else:\n        funcs = [linalg_ops.eigvals]\n\n      for f in funcs:\n        theoretical, numerical = gradient_checker_v2.compute_gradient(\n            f, [RandomInput()], delta=delta)\n        self.assertAllClose(theoretical, numerical, atol=tol, rtol=tol)\n\n  return Test\n\n\nif __name__ == \"__main__\":\n  dtypes_to_test = [\n      dtypes_lib.float32, dtypes_lib.float64, dtypes_lib.complex64,\n      dtypes_lib.complex128\n  ]\n  for compute_v in True, False:\n    for dtype in dtypes_to_test:\n      for size in 1, 2, 5, 10:\n        for batch_dims in [(), (3,)] + [(3, 2)] * (max(size, size) < 10):\n          shape = batch_dims + (size, size)\n          name = \"%s_%s_%s\" % (dtype.name, \"_\".join(map(str, shape)), compute_v)\n          _AddTest(EigTest, \"Eig\", name, _GetEigTest(dtype, shape, compute_v))\n\n          if dtype not in [dtypes_lib.float32, dtypes_lib.float64]:\n            _AddTest(EigGradTest, \"EigGrad\", name,\n                     _GetEigGradTest(dtype, shape, compute_v))\n  test.main()\n"], "fixing_code": ["/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include \"tensorflow/core/kernels/linalg/linalg_ops_common.h\"\n\n#include <initializer_list>\n#include <utility>\n\n#include \"third_party/eigen3/Eigen/Core\"\n#include \"tensorflow/core/framework/device_base.h\"\n#include \"tensorflow/core/framework/kernel_def_builder.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/tensor_shape.h\"\n#include \"tensorflow/core/framework/types.h\"\n#include \"tensorflow/core/lib/core/errors.h\"\n#include \"tensorflow/core/platform/errors.h\"\n#include \"tensorflow/core/platform/logging.h\"\n#include \"tensorflow/core/platform/types.h\"\n\nnamespace tensorflow {\n\n// static\ntemplate <class InputScalar, class OutputScalar>\nvoid LinearAlgebraOp<InputScalar, OutputScalar>::ValidateSingleMatrix(\n    OpKernelContext* context, const TensorShapes& input_matrix_shapes) {\n  OP_REQUIRES(context, input_matrix_shapes.size() == 1,\n              errors::InvalidArgument(\"Expected a single input matrix, got %d.\",\n                                      input_matrix_shapes.size()));\n  OP_REQUIRES(context, TensorShapeUtils::IsMatrix(input_matrix_shapes[0]),\n              errors::InvalidArgument(\"Input must be a matrix.\"));\n}\n\n// static\ntemplate <class InputScalar, class OutputScalar>\nvoid LinearAlgebraOp<InputScalar, OutputScalar>::ValidateSingleSquareMatrix(\n    OpKernelContext* context, const TensorShapes& input_matrix_shapes) {\n  OP_REQUIRES(context, input_matrix_shapes.size() == 1,\n              errors::InvalidArgument(\"Expected a single input matrix, got %d.\",\n                                      input_matrix_shapes.size()));\n  OP_REQUIRES(context, TensorShapeUtils::IsSquareMatrix(input_matrix_shapes[0]),\n              errors::InvalidArgument(\"Input matrix must be square.\"));\n}\n\n// static\ntemplate <class InputScalar, class OutputScalar>\nvoid LinearAlgebraOp<InputScalar, OutputScalar>::ValidateSolver(\n    OpKernelContext* context, const TensorShapes& input_matrix_shapes) {\n  OP_REQUIRES(context, input_matrix_shapes.size() == 2,\n              errors::InvalidArgument(\"Expected two input matrices, got %d.\",\n                                      input_matrix_shapes.size()));\n  OP_REQUIRES(context, TensorShapeUtils::IsMatrix(input_matrix_shapes[0]),\n              errors::InvalidArgument(\"First input (lhs) must be a matrix.\"));\n  OP_REQUIRES(context, TensorShapeUtils::IsMatrix(input_matrix_shapes[1]),\n              errors::InvalidArgument(\"Second input (rhs) must be a matrix.\"));\n  OP_REQUIRES(\n      context,\n      input_matrix_shapes[0].dim_size(0) == input_matrix_shapes[1].dim_size(0),\n      errors::InvalidArgument(\"Input matrix and rhs are incompatible.\"));\n}\n\n// static\ntemplate <class InputScalar, class OutputScalar>\nvoid LinearAlgebraOp<InputScalar, OutputScalar>::ValidateSquareSolver(\n    OpKernelContext* context, const TensorShapes& input_matrix_shapes) {\n  OP_REQUIRES(context, input_matrix_shapes.size() == 2,\n              errors::InvalidArgument(\"Expected two input matrices, got %d.\",\n                                      input_matrix_shapes.size()));\n  OP_REQUIRES(\n      context, TensorShapeUtils::IsSquareMatrix(input_matrix_shapes[0]),\n      errors::InvalidArgument(\"First input (lhs) must be a square matrix.\"));\n  OP_REQUIRES(context, TensorShapeUtils::IsMatrix(input_matrix_shapes[1]),\n              errors::InvalidArgument(\"Second input (rhs) must be a matrix.\"));\n  OP_REQUIRES(\n      context,\n      input_matrix_shapes[0].dim_size(0) == input_matrix_shapes[1].dim_size(0),\n      errors::InvalidArgument(\"Input matrix and rhs are incompatible.\"));\n}\n\ntemplate <class InputScalar, class OutputScalar>\nvoid LinearAlgebraOp<InputScalar, OutputScalar>::Compute(\n    OpKernelContext* context) {\n  TensorInputs inputs;\n  TensorShapes input_matrix_shapes;\n  TensorShape batch_shape;\n  AnalyzeInputs(context, &inputs, &input_matrix_shapes, &batch_shape);\n  if (!context->status().ok()) return;\n\n  TensorShapes output_matrix_shapes;\n  TensorOutputs outputs;\n  PrepareOutputs(context, input_matrix_shapes, batch_shape, &outputs,\n                 &output_matrix_shapes);\n  if (!context->status().ok()) return;\n\n  // Process the individual matrix problems in parallel using a threadpool.\n  auto shard = [this, &inputs, &input_matrix_shapes, &outputs,\n                &output_matrix_shapes, context](int64_t begin, int64_t end) {\n    for (int64_t i = begin; i < end; ++i) {\n      ComputeTensorSlice(context, i, inputs, input_matrix_shapes, outputs,\n                         output_matrix_shapes);\n    }\n  };\n  auto worker_threads = *(context->device()->tensorflow_cpu_worker_threads());\n  Shard(worker_threads.num_threads, worker_threads.workers,\n        batch_shape.num_elements(), GetCostPerUnit(input_matrix_shapes), shard);\n}\n\ntemplate <class InputScalar, class OutputScalar>\nvoid LinearAlgebraOp<InputScalar, OutputScalar>::AnalyzeInputs(\n    OpKernelContext* context, TensorInputs* inputs,\n    TensorShapes* input_matrix_shapes, TensorShape* batch_shape) {\n  int input_rank = -1;\n  for (int i = 0; i < NumMatrixInputs(context); ++i) {\n    const Tensor& in = context->input(i);\n    if (i == 0) {\n      input_rank = in.dims();\n      OP_REQUIRES(\n          context, input_rank >= 2,\n          errors::InvalidArgument(\"Input tensor \", i,\n                                  \" must have rank >= 2, got \", input_rank));\n      // If the tensor rank is greater than 2, we consider the inner-most\n      // dimensions as matrices, and loop over all the other outer (\"batch\")\n      // dimensions to compute the results.\n      for (int dim = 0; dim < input_rank - 2; ++dim) {\n        batch_shape->AddDim(in.dim_size(dim));\n      }\n    } else {\n      // Make sure that all inputs have the same rank and outer dimensions.\n      OP_REQUIRES(context, input_rank == in.dims(),\n                  errors::InvalidArgument(\n                      \"All input tensors must have the same rank.\"));\n      for (int dim = 0; dim < input_rank - 2; ++dim) {\n        OP_REQUIRES(\n            context, in.dim_size(dim) == batch_shape->dim_size(dim),\n            errors::InvalidArgument(\n                \"All input tensors must have the same outer dimensions.\"));\n      }\n    }\n\n    const int row_dimension = input_rank - 2;\n    const int col_dimension = input_rank - 1;\n    const int64_t num_rows = in.dim_size(row_dimension);\n    const int64_t num_cols = in.dim_size(col_dimension);\n    input_matrix_shapes->emplace_back(\n        std::initializer_list<int64_t>({num_rows, num_cols}));\n    inputs->emplace_back(&in);\n    OP_REQUIRES(\n        context, in.dtype() == DataTypeToEnum<InputScalar>::v(),\n        errors::InvalidArgument(\"Invalid input dtype \", in.dtype(), \" vs \",\n                                DataTypeToEnum<InputScalar>::v()));\n  }\n  // Have the derived class validate that the inputs are as expected.\n  ValidateInputMatrixShapes(context, *input_matrix_shapes);\n}\n\ntemplate <class InputScalar, class OutputScalar>\nvoid LinearAlgebraOp<InputScalar, OutputScalar>::PrepareOutputs(\n    OpKernelContext* context, const TensorShapes& input_matrix_shapes,\n    const TensorShape& batch_shape, TensorOutputs* outputs,\n    TensorShapes* output_matrix_shapes) {\n  // Get shape for each of the matrix outputs produced by the derived class.\n  *output_matrix_shapes = GetOutputMatrixShapes(input_matrix_shapes);\n  const int num_outputs = output_matrix_shapes->size();\n\n  // Make sure the number of op outputs is what the derived class expects.\n  OP_REQUIRES(\n      context, num_outputs <= context->num_outputs(),\n      errors::Internal(\n          \"Derived class expected more outputs (%d) that the op has (%d).\",\n          num_outputs, context->num_outputs()));\n\n  // Allocate outputs.\n  std::set<int> unused_inputs;\n  for (int input_idx = 0; input_idx < context->num_inputs(); ++input_idx) {\n    unused_inputs.insert(input_idx);\n  }\n  for (int output_idx = 0; output_idx < context->num_outputs(); ++output_idx) {\n    TensorShape output_tensor_shape({});\n    if (output_idx < num_outputs) {\n      // This output is used, set up output shape and allocate it.\n      const TensorShape& output_matrix_shape =\n          output_matrix_shapes->at(output_idx);\n      OP_REQUIRES(context, output_matrix_shape.dims() <= 2,\n                  errors::InvalidArgument(\n                      \"Rank of matrix output no. %d must be 0, 1 or 2, got %d.\",\n                      output_idx, output_matrix_shape.dims()));\n\n      // The final output has the shape of the outer batch dimensions\n      // concatenated with the output_matrix_shape (if the output is not\n      // scalar).\n      output_tensor_shape = batch_shape;\n      output_tensor_shape.AppendShape(output_matrix_shape);\n    }\n    Tensor* out = nullptr;\n    // See if there is an input buffer we can reuse for this output.\n    bool reused_input = false;\n    if (EnableInputForwarding()) {\n      for (int input_idx : unused_inputs) {\n        if (context->forward_input_to_output_with_shape(\n                input_idx, output_idx, output_tensor_shape, &out)) {\n          reused_input = true;\n          unused_inputs.erase(input_idx);\n          break;\n        }\n      }\n    }\n    if (!reused_input) {\n      OP_REQUIRES_OK(context, context->allocate_output(\n                                  output_idx, output_tensor_shape, &out));\n    }\n    OP_REQUIRES(\n        context, out->dtype() == DataTypeToEnum<OutputScalar>::v(),\n        errors::InvalidArgument(\"Invalid output dtype \", out->dtype(), \" vs \",\n                                DataTypeToEnum<OutputScalar>::v()));\n\n    outputs->emplace_back(out);\n  }\n}\n\ntemplate <class InputScalar, class OutputScalar>\nvoid LinearAlgebraOp<InputScalar, OutputScalar>::ComputeTensorSlice(\n    OpKernelContext* context, int64_t matrix_index, const TensorInputs& inputs,\n    const TensorShapes& input_matrix_shapes, const TensorOutputs& outputs,\n    const TensorShapes& output_matrix_shapes) {\n  InputConstMatrixMaps matrix_inputs;\n  for (size_t i = 0; i < inputs.size(); ++i) {\n    // TODO(kalakris): Handle alignment if possible. Eigen::Map is\n    // unaligned by default.\n    matrix_inputs.emplace_back(\n        inputs[i]->flat<InputScalar>().data() +\n            matrix_index * input_matrix_shapes[i].num_elements(),\n        input_matrix_shapes[i].dim_size(0), input_matrix_shapes[i].dim_size(1));\n  }\n\n  OutputMatrixMaps matrix_outputs;\n  for (size_t i = 0; i < output_matrix_shapes.size(); ++i) {\n    // The output matrix shape may not be a matrix.\n    int num_output_rows = output_matrix_shapes[i].dims() >= 1\n                              ? output_matrix_shapes[i].dim_size(0)\n                              : 1;\n    int num_output_cols = output_matrix_shapes[i].dims() == 2\n                              ? output_matrix_shapes[i].dim_size(1)\n                              : 1;\n    matrix_outputs.emplace_back(\n        outputs[i]->flat<OutputScalar>().data() +\n            matrix_index * output_matrix_shapes[i].num_elements(),\n        num_output_rows, num_output_cols);\n  }\n  ComputeMatrix(context, matrix_inputs, &matrix_outputs);\n}\n\n// Explicitly instantiate LinearAlgebraOp for the scalar types we expect to use.\ntemplate class LinearAlgebraOp<Eigen::half>;\ntemplate class LinearAlgebraOp<float>;\ntemplate class LinearAlgebraOp<double>;\ntemplate class LinearAlgebraOp<complex64>;\ntemplate class LinearAlgebraOp<complex128>;\ntemplate class LinearAlgebraOp<float, complex64>;\ntemplate class LinearAlgebraOp<double, complex128>;\n\n}  // namespace tensorflow\n", "# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for tensorflow.ops.linalg_ops.eig.\"\"\"\n\nimport numpy as np\n\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes as dtypes_lib\nfrom tensorflow.python.framework import errors\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import gen_linalg_ops\nfrom tensorflow.python.ops import gradient_checker_v2\nfrom tensorflow.python.ops import linalg_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import random_ops\nfrom tensorflow.python.ops import sort_ops\nfrom tensorflow.python.platform import test\n\n\ndef _AddTest(test_class, op_name, testcase_name, fn):\n  test_name = \"_\".join([\"test\", op_name, testcase_name])\n  if hasattr(test_class, test_name):\n    raise RuntimeError(\"Test %s defined more than once\" % test_name)\n  setattr(test_class, test_name, fn)\n\n\nclass EigTest(test.TestCase):\n\n  @test_util.run_deprecated_v1\n  def testWrongDimensions(self):\n    # The input to self_adjoint_eig should be a tensor of\n    # at least rank 2.\n    scalar = constant_op.constant(1.)\n    with self.assertRaises(ValueError):\n      linalg_ops.eig(scalar)\n    vector = constant_op.constant([1., 2.])\n    with self.assertRaises(ValueError):\n      linalg_ops.eig(vector)\n\n  @test_util.run_deprecated_v1\n  def testConcurrentExecutesWithoutError(self):\n    all_ops = []\n    with self.session():\n      for compute_v_ in True, False:\n        matrix1 = random_ops.random_normal([5, 5], seed=42)\n        matrix2 = random_ops.random_normal([5, 5], seed=42)\n        if compute_v_:\n          e1, v1 = linalg_ops.eig(matrix1)\n          e2, v2 = linalg_ops.eig(matrix2)\n          all_ops += [e1, v1, e2, v2]\n        else:\n          e1 = linalg_ops.eigvals(matrix1)\n          e2 = linalg_ops.eigvals(matrix2)\n          all_ops += [e1, e2]\n      val = self.evaluate(all_ops)\n      self.assertAllEqual(val[0], val[2])\n      # The algorithm is slightly different for compute_v being True and False,\n      # so require approximate equality only here.\n      self.assertAllClose(val[2], val[4])\n      self.assertAllEqual(val[4], val[5])\n      self.assertAllEqual(val[1], val[3])\n\n  def testMatrixThatFailsWhenFlushingDenormsToZero(self):\n    # Test a 32x32 matrix which is known to fail if denorm floats are flushed to\n    # zero.\n    matrix = np.genfromtxt(\n        test.test_src_dir_path(\n            \"python/kernel_tests/linalg/testdata/\"\n            \"self_adjoint_eig_fail_if_denorms_flushed.txt\")).astype(np.float32)\n    self.assertEqual(matrix.shape, (32, 32))\n    matrix_tensor = constant_op.constant(matrix)\n    with self.session() as _:\n      (e, v) = self.evaluate(linalg_ops.self_adjoint_eig(matrix_tensor))\n      self.assertEqual(e.size, 32)\n      self.assertAllClose(\n          np.matmul(v, v.transpose()), np.eye(32, dtype=np.float32), atol=2e-3)\n      self.assertAllClose(matrix,\n                          np.matmul(np.matmul(v, np.diag(e)), v.transpose()))\n\n  def testMismatchedDtypes(self):\n    tensor = constant_op.constant([[0, 1], [2, 3]], dtype=dtypes_lib.float32)\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"Invalid output dtype\"):\n      self.evaluate(\n          gen_linalg_ops.eig(\n              input=tensor,\n              Tout=dtypes_lib.complex128,  # Expected dtype: complex64.\n              compute_v=True))\n\n\ndef SortEigenValues(e):\n  perm = np.argsort(e.real + e.imag, -1)\n  return np.take(e, perm, -1)\n\n\ndef SortEigenDecomposition(e, v):\n  if v.ndim < 2:\n    return e, v\n  perm = np.argsort(e.real + e.imag, -1)\n  return np.take(e, perm, -1), np.take(v, perm, -1)\n\n\ndef EquilibrateEigenVectorPhases(x, y):\n  \"\"\"Equilibrate the phase of the Eigenvectors in the columns of `x` and `y`.\n\n  Eigenvectors are only unique up to an arbitrary phase. This function rotates x\n  such that it matches y. Precondition: The columns of x and y differ by a\n  multiplicative complex phase factor only.\n\n  Args:\n    x: `np.ndarray` with Eigenvectors\n    y: `np.ndarray` with Eigenvectors\n\n  Returns:\n    `np.ndarray` containing an equilibrated version of x.\n  \"\"\"\n  phases = np.sum(np.conj(x) * y, -2, keepdims=True)\n  phases /= np.abs(phases)\n  return phases * x\n\n\ndef _GetEigTest(dtype_, shape_, compute_v_):\n\n  def CompareEigenVectors(self, x, y, tol):\n    x = EquilibrateEigenVectorPhases(x, y)\n    self.assertAllClose(x, y, atol=tol)\n\n  def CompareEigenDecompositions(self, x_e, x_v, y_e, y_v, tol):\n    num_batches = int(np.prod(x_e.shape[:-1]))\n    n = x_e.shape[-1]\n    x_e = np.reshape(x_e, [num_batches] + [n])\n    x_v = np.reshape(x_v, [num_batches] + [n, n])\n    y_e = np.reshape(y_e, [num_batches] + [n])\n    y_v = np.reshape(y_v, [num_batches] + [n, n])\n    for i in range(num_batches):\n      x_ei, x_vi = SortEigenDecomposition(x_e[i, :], x_v[i, :, :])\n      y_ei, y_vi = SortEigenDecomposition(y_e[i, :], y_v[i, :, :])\n      self.assertAllClose(x_ei, y_ei, atol=tol, rtol=tol)\n      CompareEigenVectors(self, x_vi, y_vi, tol)\n\n  def Test(self):\n    np.random.seed(1)\n    n = shape_[-1]\n    batch_shape = shape_[:-2]\n    np_dtype = dtype_.as_numpy_dtype\n\n    def RandomInput():\n      # Most matrices are diagonalizable\n      a = np.random.uniform(\n          low=-1.0, high=1.0, size=n * n).reshape([n, n]).astype(np_dtype)\n      if dtype_.is_complex:\n        a += 1j * np.random.uniform(\n            low=-1.0, high=1.0, size=n * n).reshape([n, n]).astype(np_dtype)\n      a = np.tile(a, batch_shape + (1, 1))\n      return a\n\n    if dtype_ in (dtypes_lib.float32, dtypes_lib.complex64):\n      atol = 1e-4\n    else:\n      atol = 1e-12\n\n    a = RandomInput()\n    np_e, np_v = np.linalg.eig(a)\n    with self.session():\n      if compute_v_:\n        tf_e, tf_v = linalg_ops.eig(constant_op.constant(a))\n\n        # Check that V*diag(E)*V^(-1) is close to A.\n        a_ev = math_ops.matmul(\n            math_ops.matmul(tf_v, array_ops.matrix_diag(tf_e)),\n            linalg_ops.matrix_inverse(tf_v))\n        self.assertAllClose(self.evaluate(a_ev), a, atol=atol)\n\n        # Compare to numpy.linalg.eig.\n        CompareEigenDecompositions(self, np_e, np_v, self.evaluate(tf_e),\n                                   self.evaluate(tf_v), atol)\n      else:\n        tf_e = linalg_ops.eigvals(constant_op.constant(a))\n        self.assertAllClose(\n            SortEigenValues(np_e),\n            SortEigenValues(self.evaluate(tf_e)),\n            atol=atol)\n\n  return Test\n\n\nclass EigGradTest(test.TestCase):\n  pass  # Filled in below\n\n\ndef _GetEigGradTest(dtype_, shape_, compute_v_):\n\n  def Test(self):\n    np.random.seed(1)\n    n = shape_[-1]\n    batch_shape = shape_[:-2]\n    np_dtype = dtype_.as_numpy_dtype\n\n    def RandomInput():\n      # Most matrices are diagonalizable\n      a = np.random.uniform(\n          low=-1.0, high=1.0, size=n * n).reshape([n, n]).astype(np_dtype)\n      if dtype_.is_complex:\n        a += 1j * np.random.uniform(\n            low=-1.0, high=1.0, size=n * n).reshape([n, n]).astype(np_dtype)\n      a = np.tile(a, batch_shape + (1, 1))\n      return a\n\n    # Optimal stepsize for central difference is O(epsilon^{1/3}).\n    epsilon = np.finfo(np_dtype).eps\n    delta = 0.1 * epsilon**(1.0 / 3.0)\n    # tolerance obtained by looking at actual differences using\n    # np.linalg.norm(theoretical-numerical, np.inf) on -mavx build\n    # after discarding one random input sample\n    _ = RandomInput()\n    if dtype_ in (dtypes_lib.float32, dtypes_lib.complex64):\n      tol = 1e-2\n    else:\n      tol = 1e-7\n    with self.session():\n\n      def Compute(x):\n        e, v = linalg_ops.eig(x)\n\n        # We sort eigenvalues by e.real+e.imag to have consistent\n        # order between runs\n        b_dims = len(e.shape) - 1\n        idx = sort_ops.argsort(math_ops.real(e) + math_ops.imag(e), axis=-1)\n        e = array_ops.gather(e, idx, batch_dims=b_dims)\n        v = array_ops.gather(v, idx, batch_dims=b_dims)\n\n        # (complex) Eigenvectors are only unique up to an arbitrary phase\n        # We normalize the vectors such that the first component has phase 0.\n        top_rows = v[..., 0:1, :]\n        angle = -math_ops.angle(top_rows)\n        phase = math_ops.complex(math_ops.cos(angle), math_ops.sin(angle))\n        v *= phase\n        return e, v\n\n      if compute_v_:\n        funcs = [lambda x: Compute(x)[0], lambda x: Compute(x)[1]]\n      else:\n        funcs = [linalg_ops.eigvals]\n\n      for f in funcs:\n        theoretical, numerical = gradient_checker_v2.compute_gradient(\n            f, [RandomInput()], delta=delta)\n        self.assertAllClose(theoretical, numerical, atol=tol, rtol=tol)\n\n  return Test\n\n\nif __name__ == \"__main__\":\n  dtypes_to_test = [\n      dtypes_lib.float32, dtypes_lib.float64, dtypes_lib.complex64,\n      dtypes_lib.complex128\n  ]\n  for compute_v in True, False:\n    for dtype in dtypes_to_test:\n      for size in 1, 2, 5, 10:\n        for batch_dims in [(), (3,)] + [(3, 2)] * (max(size, size) < 10):\n          shape = batch_dims + (size, size)\n          name = \"%s_%s_%s\" % (dtype.name, \"_\".join(map(str, shape)), compute_v)\n          _AddTest(EigTest, \"Eig\", name, _GetEigTest(dtype, shape, compute_v))\n\n          if dtype not in [dtypes_lib.float32, dtypes_lib.float64]:\n            _AddTest(EigGradTest, \"EigGrad\", name,\n                     _GetEigGradTest(dtype, shape, compute_v))\n  test.main()\n"], "buggy_code_start_loc": [17, 20], "buggy_code_end_loc": [214, 90], "fixing_code_start_loc": [18, 21], "fixing_code_end_loc": [227, 103], "type": "CWE-476", "message": "TensorFlow is an open source platform for machine learning. When `mlir::tfg::ConvertGenericFunctionToFunctionDef` is given empty function attributes, it gives a null dereference. We have patched the issue in GitHub commit aed36912609fc07229b4d0a7b44f3f48efc00fd0. The fix will be included in TensorFlow 2.10.0. We will also cherrypick this commit on TensorFlow 2.9.1, TensorFlow 2.8.1, and TensorFlow 2.7.2, as these are also affected and still in supported range. There are no known workarounds for this issue.", "other": {"cve": {"id": "CVE-2022-36000", "sourceIdentifier": "security-advisories@github.com", "published": "2022-09-16T23:15:10.647", "lastModified": "2022-09-20T14:43:24.050", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "TensorFlow is an open source platform for machine learning. When `mlir::tfg::ConvertGenericFunctionToFunctionDef` is given empty function attributes, it gives a null dereference. We have patched the issue in GitHub commit aed36912609fc07229b4d0a7b44f3f48efc00fd0. The fix will be included in TensorFlow 2.10.0. We will also cherrypick this commit on TensorFlow 2.9.1, TensorFlow 2.8.1, and TensorFlow 2.7.2, as these are also affected and still in supported range. There are no known workarounds for this issue."}, {"lang": "es", "value": "TensorFlow es una plataforma de c\u00f3digo abierto para el aprendizaje autom\u00e1tico. Cuando \"mlir::tfg::ConvertGenericFunctionToFunctionDef\" recibe atributos de funci\u00f3n vac\u00edos, da una derivaci\u00f3n nula. Hemos parcheado el problema en el commit de GitHub aed36912609fc07229b4d0a7b44f3f48efc00fd0. La correcci\u00f3n ser\u00e1 incluida en TensorFlow versi\u00f3n 2.10.0. Tambi\u00e9n seleccionaremos este compromiso en TensorFlow versi\u00f3n 2.9.1, TensorFlow versi\u00f3n 2.8.1, y TensorFlow versi\u00f3n 2.7.2, ya que estos tambi\u00e9n est\u00e1n afectados y todav\u00eda est\u00e1n en el rango admitido. No se presentan mitigaciones conocidas para este problema"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 7.5, "baseSeverity": "HIGH"}, "exploitabilityScore": 3.9, "impactScore": 3.6}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:H/PR:N/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "HIGH", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.9, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 2.2, "impactScore": 3.6}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-476"}]}, {"source": "security-advisories@github.com", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-476"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionEndExcluding": "2.7.2", "matchCriteriaId": "C6622D95-1C86-45C5-AB55-E6EEEA0996DF"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.8.0", "versionEndExcluding": "2.8.1", "matchCriteriaId": "0F9D273D-02DC-441E-AA91-EAC8DEAA4B44"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.9.0", "versionEndExcluding": "2.9.1", "matchCriteriaId": "FE4F8A81-6CC2-4F7F-9602-C170FDD926E7"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.10:rc0:*:*:*:*:*:*", "matchCriteriaId": "1DBFBCE2-0A01-4575-BE45-6775ABFB8B28"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.10:rc1:*:*:*:*:*:*", "matchCriteriaId": "89806CF9-E423-4CA6-A01A-8175C260CB24"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.10:rc2:*:*:*:*:*:*", "matchCriteriaId": "F2B80690-A257-4E16-BD27-9AE045BC56ED"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.10:rc3:*:*:*:*:*:*", "matchCriteriaId": "F335F9A4-5AB8-4E53-BC18-E01F7C653E5E"}]}]}], "references": [{"url": "https://github.com/tensorflow/tensorflow/commit/aed36912609fc07229b4d0a7b44f3f48efc00fd0", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/security/advisories/GHSA-fqxc-pvf8-2w9v", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/tensorflow/tensorflow/commit/aed36912609fc07229b4d0a7b44f3f48efc00fd0"}}