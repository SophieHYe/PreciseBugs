{"buggy_code": ["/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#define EIGEN_USE_THREADS\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n#define EIGEN_USE_GPU\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n#include \"tensorflow/core/kernels/list_kernels.h\"\n\n#include <limits>\n\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n#include \"tensorflow/core/framework/allocator.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/register_types.h\"\n#include \"tensorflow/core/framework/tensor_types.h\"\n#include \"tensorflow/core/framework/variant.h\"\n#include \"tensorflow/core/framework/variant_op_registry.h\"\n#include \"tensorflow/core/kernels/concat_lib.h\"\n#include \"tensorflow/core/lib/core/coding.h\"\n#include \"tensorflow/core/lib/core/errors.h\"\n#include \"tensorflow/core/util/util.h\"\n\nnamespace tensorflow {\n\ntypedef Eigen::ThreadPoolDevice CPUDevice;\n\nStatus TensorShapeFromTensor(const Tensor& t, PartialTensorShape* out) {\n  if (t.shape() == TensorShape({})) {\n    if ((t.dtype() == DT_INT32 && t.scalar<int32>()() == -1) ||\n        (t.dtype() == DT_INT64 && t.scalar<int64>()() == -1)) {\n      *out = PartialTensorShape();\n      return Status::OK();\n    }\n    return errors::InvalidArgument(\n        \"The only valid scalar shape tensor is the fully unknown shape \"\n        \"specified as -1.\");\n  }\n  if (t.dtype() == DT_INT32) {\n    return PartialTensorShape::MakePartialShape(t.vec<int32>().data(),\n                                                t.NumElements(), out);\n  } else if (t.dtype() == DT_INT64) {\n    return PartialTensorShape::MakePartialShape(t.vec<int64>().data(),\n                                                t.NumElements(), out);\n  }\n  return errors::InvalidArgument(\n      \"Expected an int32 or int64 shape tensor; found \",\n      DataTypeString(t.dtype()));\n}\n\nStatus GetElementShapeFromInput(OpKernelContext* c,\n                                const TensorList& tensor_list, int index,\n                                PartialTensorShape* element_shape) {\n  TF_RETURN_IF_ERROR(TensorShapeFromTensor(c->input(index), element_shape));\n  // Check that `element_shape` and `tensor_list.element_shape` are\n  // compatible and store the merged shape in `element_shape`.\n  PartialTensorShape tmp = *element_shape;\n  TF_RETURN_IF_ERROR(tmp.MergeWith(tensor_list.element_shape, element_shape));\n  return Status::OK();\n}\n\nStatus GetInputList(OpKernelContext* c, int index, const TensorList** list) {\n  if (!TensorShapeUtils::IsScalar(c->input(index).shape())) {\n    return errors::InvalidArgument(\"Input list must be a scalar saw: \",\n                                   c->input(index).shape().DebugString());\n  }\n  const TensorList* l = c->input(index).scalar<Variant>()().get<TensorList>();\n  if (l == nullptr) {\n    return errors::InvalidArgument(\n        \"Input handle is not a list. Saw: '\",\n        c->input(index).scalar<Variant>()().DebugString(), \"'\");\n  }\n  *list = l;\n  return Status::OK();\n}\n\nStatus ForwardInputOrCreateNewList(OpKernelContext* c, int32 input_index,\n                                   int32 output_index,\n                                   const TensorList& input_list,\n                                   TensorList** output_list) {\n  // Attempt to forward the input tensor to the output if possible.\n  std::unique_ptr<Tensor> maybe_output = c->forward_input(\n      input_index, output_index, DT_VARIANT, TensorShape{},\n      c->input_memory_type(input_index), AllocatorAttributes());\n  Tensor* output_tensor;\n  if (maybe_output != nullptr && maybe_output->dtype() == DT_VARIANT &&\n      maybe_output->NumElements() == 1) {\n    output_tensor = maybe_output.get();\n    TensorList* tmp_out = output_tensor->scalar<Variant>()().get<TensorList>();\n    if (tmp_out == nullptr) {\n      return errors::InvalidArgument(\n          \"Expected input \", input_index, \" to be a TensorList but saw \",\n          output_tensor->scalar<Variant>()().TypeName());\n    }\n    if (tmp_out->RefCountIsOne()) {\n      // Woohoo, forwarding succeeded!\n      c->set_output(output_index, *output_tensor);\n      *output_list = tmp_out;\n      return Status::OK();\n    }\n  }\n\n  // If forwarding is not possible allocate a new output tensor and copy\n  // the `input_list` to it.\n  AllocatorAttributes attr;\n  attr.set_on_host(true);\n  TF_RETURN_IF_ERROR(\n      c->allocate_output(output_index, {}, &output_tensor, attr));\n  output_tensor->scalar<Variant>()() = input_list.Copy();\n\n  *output_list = output_tensor->scalar<Variant>()().get<TensorList>();\n  return Status::OK();\n}\n\nclass EmptyTensorList : public OpKernel {\n public:\n  explicit EmptyTensorList(OpKernelConstruction* ctx) : OpKernel(ctx) {\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"element_dtype\", &element_dtype_));\n  }\n\n  void Compute(OpKernelContext* ctx) override {\n    const Tensor& max_num_elements_t = ctx->input(1);\n    OP_REQUIRES(\n        ctx, TensorShapeUtils::IsScalar(max_num_elements_t.shape()),\n        errors::InvalidArgument(\n            \"max_num_elements expected to be a scalar \",\n            \"but got shape: \", max_num_elements_t.shape().DebugString()));\n    Tensor* result;\n    AllocatorAttributes attr;\n    attr.set_on_host(true);\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, TensorShape{}, &result, attr));\n    TensorList empty;\n    empty.element_dtype = element_dtype_;\n    empty.max_num_elements = max_num_elements_t.scalar<int32>()();\n    PartialTensorShape element_shape;\n    OP_REQUIRES_OK(ctx, TensorShapeFromTensor(ctx->input(0), &element_shape));\n    empty.element_shape = element_shape;\n    result->scalar<Variant>()() = std::move(empty);\n  }\n\n private:\n  DataType element_dtype_;\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"EmptyTensorList\").Device(DEVICE_CPU),\n                        EmptyTensorList);\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\nREGISTER_KERNEL_BUILDER(Name(\"EmptyTensorList\")\n                            .Device(DEVICE_GPU)\n                            .HostMemory(\"element_shape\")\n                            .HostMemory(\"max_num_elements\"),\n                        EmptyTensorList);\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\nclass TensorListPushBack : public OpKernel {\n public:\n  explicit TensorListPushBack(OpKernelConstruction* c) : OpKernel(c) {\n    OP_REQUIRES_OK(c, c->GetAttr(\"element_dtype\", &element_dtype_));\n  }\n\n  ~TensorListPushBack() override {}\n\n  void Compute(OpKernelContext* c) override {\n    const Tensor& input = c->input(1);\n    OP_REQUIRES(c, element_dtype_ == input.dtype(),\n                errors::InvalidArgument(\"Invalid data types; list elements \",\n                                        DataTypeString(element_dtype_),\n                                        \" but tried to append \",\n                                        DataTypeString(input.dtype())));\n\n    const TensorList* l = nullptr;\n    OP_REQUIRES_OK(c, GetInputList(c, 0, &l));\n    OP_REQUIRES(c, l->element_shape.IsCompatibleWith(input.shape()),\n                errors::InvalidArgument(\n                    \"Tried to append a tensor with incompatible shape to a \"\n                    \"list. Op element shape: \",\n                    input.shape().DebugString(),\n                    \" list shape: \", l->element_shape.DebugString()));\n    OP_REQUIRES(c, element_dtype_ == l->element_dtype,\n                errors::InvalidArgument(\"Invalid data types; op elements \",\n                                        DataTypeString(element_dtype_),\n                                        \" but list elements \",\n                                        DataTypeString(l->element_dtype)));\n\n    if (l->max_num_elements != -1) {\n      OP_REQUIRES(\n          c, l->tensors().size() < l->max_num_elements,\n          errors::InvalidArgument(\"Tried to push item into a full list\",\n                                  \" list size: \", l->tensors().size(),\n                                  \" max_num_elements: \", l->max_num_elements));\n    }\n\n    TensorList* output_list = nullptr;\n    OP_REQUIRES_OK(c, ForwardInputOrCreateNewList(c, 0, 0, *l, &output_list));\n    output_list->tensors().push_back(input);\n  }\n\n private:\n  DataType element_dtype_;\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"TensorListPushBack\").Device(DEVICE_CPU),\n                        TensorListPushBack);\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\nREGISTER_KERNEL_BUILDER(Name(\"TensorListPushBack\").Device(DEVICE_GPU),\n                        TensorListPushBack);\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\nclass TensorListLength : public OpKernel {\n public:\n  explicit TensorListLength(OpKernelConstruction* c) : OpKernel(c) {}\n  ~TensorListLength() override {}\n\n  void Compute(OpKernelContext* c) override {\n    const TensorList* l = nullptr;\n    OP_REQUIRES_OK(c, GetInputList(c, 0, &l));\n    Tensor* result;\n    OP_REQUIRES_OK(c, c->allocate_output(0, TensorShape{}, &result));\n    result->scalar<int32>()() = l->tensors().size();\n  }\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"TensorListLength\").Device(DEVICE_CPU),\n                        TensorListLength);\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\nREGISTER_KERNEL_BUILDER(\n    Name(\"TensorListLength\").Device(DEVICE_GPU).HostMemory(\"length\"),\n    TensorListLength);\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\nclass TensorListElementShape : public OpKernel {\n public:\n  explicit TensorListElementShape(OpKernelConstruction* c) : OpKernel(c) {}\n\n  void Compute(OpKernelContext* c) override {\n    const TensorList* l = nullptr;\n    OP_REQUIRES_OK(c, GetInputList(c, 0, &l));\n    Tensor* result;\n    if (l->element_shape.unknown_rank()) {\n      OP_REQUIRES_OK(c, c->allocate_output(0, TensorShape({}), &result));\n      if (result->dtype() == DT_INT32) {\n        result->scalar<int32>()() = -1;\n      } else {\n        result->scalar<int64>()() = -1;\n      }\n    } else {\n      OP_REQUIRES_OK(c, c->allocate_output(\n                            0, TensorShape{l->element_shape.dims()}, &result));\n      for (int i = 0; i < l->element_shape.dims(); ++i) {\n        if (result->dtype() == DT_INT32) {\n          result->flat<int32>()(i) = l->element_shape.dim_size(i);\n        } else {\n          result->flat<int64>()(i) = l->element_shape.dim_size(i);\n        }\n      }\n    }\n  }\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"TensorListElementShape\").Device(DEVICE_CPU),\n                        TensorListElementShape);\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\nREGISTER_KERNEL_BUILDER(Name(\"TensorListElementShape\")\n                            .Device(DEVICE_GPU)\n                            .HostMemory(\"element_shape\"),\n                        TensorListElementShape);\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\nclass TensorListReserve : public OpKernel {\n public:\n  explicit TensorListReserve(OpKernelConstruction* c) : OpKernel(c) {\n    OP_REQUIRES_OK(c, c->GetAttr(\"element_dtype\", &element_dtype_));\n  }\n\n  void Compute(OpKernelContext* c) override {\n    PartialTensorShape element_shape;\n    OP_REQUIRES_OK(c, TensorShapeFromTensor(c->input(0), &element_shape));\n    int32 num_elements = c->input(1).scalar<int32>()();\n    TensorList output;\n    output.element_shape = element_shape;\n    output.element_dtype = element_dtype_;\n    output.tensors().resize(num_elements, Tensor(DT_INVALID));\n    Tensor* result;\n    AllocatorAttributes attr;\n    attr.set_on_host(true);\n    OP_REQUIRES_OK(c, c->allocate_output(0, TensorShape{}, &result, attr));\n    result->scalar<Variant>()() = std::move(output);\n  }\n\n private:\n  DataType element_dtype_;\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"TensorListReserve\").Device(DEVICE_CPU),\n                        TensorListReserve);\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\nREGISTER_KERNEL_BUILDER(Name(\"TensorListReserve\")\n                            .Device(DEVICE_GPU)\n                            .HostMemory(\"element_shape\")\n                            .HostMemory(\"num_elements\"),\n                        TensorListReserve);\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\nclass TensorListResize : public OpKernel {\n public:\n  explicit TensorListResize(OpKernelConstruction* c) : OpKernel(c) {}\n\n  void Compute(OpKernelContext* c) override {\n    const TensorList* input_list = nullptr;\n    OP_REQUIRES_OK(c, GetInputList(c, 0, &input_list));\n    int32 size = c->input(1).scalar<int32>()();\n    OP_REQUIRES(\n        c, size >= 0,\n        errors::InvalidArgument(\n            \"TensorListSlice expects size to be non-negative. Got: \", size));\n\n    std::unique_ptr<Tensor> maybe_result =\n        c->forward_input(0, 0, DT_VARIANT, TensorShape{},\n                         c->input_memory_type(0), AllocatorAttributes());\n    if (maybe_result != nullptr) {\n      TensorList* out = maybe_result->scalar<Variant>()().get<TensorList>();\n      if (out->RefCountIsOne()) {\n        // We are able to forward the input.\n        out->tensors().resize(size, Tensor(DT_INVALID));\n        c->set_output(0, *maybe_result);\n        return;\n      }\n    }\n\n    // We were not able to forward the input.  Will have to resize from scratch.\n    Tensor* result;\n    AllocatorAttributes attr;\n    attr.set_on_host(true);\n    OP_REQUIRES_OK(c, c->allocate_output(0, TensorShape{}, &result, attr));\n    TensorList output_list;\n    output_list.element_shape = input_list->element_shape;\n    output_list.element_dtype = input_list->element_dtype;\n    output_list.max_num_elements = input_list->max_num_elements;\n    if (size > input_list->tensors().size()) {\n      output_list.tensors().insert(output_list.tensors().begin(),\n                                   input_list->tensors().begin(),\n                                   input_list->tensors().end());\n      // Add DT_INVALID tensors to the end of the list if the requested size\n      // is larger than the list length.\n      output_list.tensors().resize(size, Tensor(DT_INVALID));\n    } else {\n      output_list.tensors().insert(output_list.tensors().begin(),\n                                   input_list->tensors().begin(),\n                                   input_list->tensors().begin() + size);\n    }\n    result->scalar<Variant>()() = std::move(output_list);\n  }\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"TensorListResize\").Device(DEVICE_CPU),\n                        TensorListResize);\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\nREGISTER_KERNEL_BUILDER(\n    Name(\"TensorListResize\").Device(DEVICE_GPU).HostMemory(\"size\"),\n    TensorListResize);\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\nclass TensorListSetItem : public OpKernel {\n public:\n  explicit TensorListSetItem(OpKernelConstruction* c) : OpKernel(c) {\n    OP_REQUIRES_OK(c, c->GetAttr(\"element_dtype\", &element_dtype_));\n  }\n\n  void Compute(OpKernelContext* c) override {\n    const TensorList* l = nullptr;\n    OP_REQUIRES_OK(c, GetInputList(c, 0, &l));\n    OP_REQUIRES(c, element_dtype_ == l->element_dtype,\n                errors::InvalidArgument(\"Invalid data types; op elements \",\n                                        DataTypeString(element_dtype_),\n                                        \" but list elements \",\n                                        DataTypeString(l->element_dtype)));\n    int32 index = c->input(1).scalar<int32>()();\n    OP_REQUIRES(c, index < l->tensors().size(),\n                errors::InvalidArgument(\"Trying to modify element \", index,\n                                        \" in a list with \", l->tensors().size(),\n                                        \" elements.\"));\n    const Tensor& value = c->input(2);\n    OP_REQUIRES(c, l->element_shape.IsCompatibleWith(value.shape()),\n                errors::InvalidArgument(\n                    \"Tried to set a tensor with incompatible shape at a \"\n                    \"list index. Item element shape: \",\n                    value.shape().DebugString(),\n                    \" list shape: \", l->element_shape.DebugString()));\n    TensorList* output_list = nullptr;\n    OP_REQUIRES_OK(c, ForwardInputOrCreateNewList(c, 0, 0, *l, &output_list));\n    output_list->tensors()[index] = value;\n  }\n\n private:\n  DataType element_dtype_;\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"TensorListSetItem\").Device(DEVICE_CPU),\n                        TensorListSetItem);\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n#define REGISTER_TENSOR_LIST_SET_ITEM_GPU(T)                      \\\n  REGISTER_KERNEL_BUILDER(Name(\"TensorListSetItem\")               \\\n                              .TypeConstraint<T>(\"element_dtype\") \\\n                              .Device(DEVICE_GPU)                 \\\n                              .HostMemory(\"index\"),               \\\n                          TensorListSetItem);\n\nTF_CALL_GPU_ALL_TYPES(REGISTER_TENSOR_LIST_SET_ITEM_GPU);\nTF_CALL_int32(REGISTER_TENSOR_LIST_SET_ITEM_GPU);\nTF_CALL_int64(REGISTER_TENSOR_LIST_SET_ITEM_GPU);\nREGISTER_TENSOR_LIST_SET_ITEM_GPU(bfloat16)\n#undef REGISTER_TENSOR_LIST_SET_ITEM_GPU\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\nclass TensorListConcatLists : public OpKernel {\n public:\n  explicit TensorListConcatLists(OpKernelConstruction* c) : OpKernel(c) {\n    OP_REQUIRES_OK(c, c->GetAttr(\"element_dtype\", &element_dtype_));\n  }\n\n  void Compute(OpKernelContext* c) override {\n    const TensorShape& tl_a_shape = c->input(0).shape();\n    const TensorShape& tl_b_shape = c->input(1).shape();\n    OP_REQUIRES(\n        c, tl_a_shape == tl_b_shape,\n        errors::InvalidArgument(\"Incompatible input TensorList tensor shapes: \",\n                                tl_a_shape.DebugString(), \" vs. \",\n                                tl_b_shape.DebugString()));\n    AllocatorAttributes attr;\n    std::unique_ptr<Tensor> tl_alias = c->forward_input(\n        0 /*input_index*/, 0 /*output_index*/, DT_VARIANT, tl_a_shape,\n        DEVICE_MEMORY /* input is always on DEVICE_MEMORY */, attr);\n\n    // tl_a may be aliased by tl_alias.\n    const Tensor& tl_a = c->input(0);\n    const Tensor& tl_b = c->input(1);\n\n    Tensor* output = nullptr;\n    bool ok_to_alias = tl_alias != nullptr;\n    if (tl_alias && tl_alias->dtype() == DT_VARIANT &&\n        tl_alias->NumElements() > 0) {\n      auto tl_a_t = tl_alias->flat<Variant>();\n      for (int64 i = 0; i < tl_alias->NumElements(); ++i) {\n        TensorList* aliased = tl_a_t(i).get<TensorList>();\n        if (aliased == nullptr || !aliased->RefCountIsOne()) {\n          ok_to_alias = false;\n          break;\n        }\n      }\n      if (ok_to_alias) {\n        c->set_output(0, *tl_alias);\n        output = tl_alias.get();\n      }\n    }\n    if (!ok_to_alias) {\n      // Couldn't alias the entire Tensor.  We'll be conservative and not try\n      // to alias individual batch entries.\n      attr.set_on_host(true);\n      OP_REQUIRES_OK(c, c->allocate_output(0, tl_a_shape, &output, attr));\n    }\n\n    auto output_t = output->flat<Variant>();\n    auto tl_a_t = tl_a.flat<Variant>();\n    auto tl_b_t = tl_b.flat<Variant>();\n\n    for (int64 i = 0; i < tl_a.NumElements(); ++i) {\n      const TensorList* l_a = tl_a_t(i).get<TensorList>();\n      const TensorList* l_b = tl_b_t(i).get<TensorList>();\n      OP_REQUIRES(\n          c, l_a != nullptr,\n          errors::InvalidArgument(\"input_a is not a TensorList at index \", i,\n                                  \".  Saw: '\", tl_a_t(i).DebugString(), \"'\"));\n      OP_REQUIRES(\n          c, l_b != nullptr,\n          errors::InvalidArgument(\"input_b is not a TensorList at index \", i,\n                                  \".  Saw: '\", tl_b_t(i).DebugString(), \"'\"));\n      OP_REQUIRES(c, l_a->element_dtype == element_dtype_,\n                  errors::InvalidArgument(\n                      \"input_a[\", i, \"].dtype != element_dtype.  Saw: \",\n                      DataTypeString(l_a->element_dtype), \" vs. \",\n                      DataTypeString(element_dtype_)));\n      OP_REQUIRES(c, l_b->element_dtype == element_dtype_,\n                  errors::InvalidArgument(\n                      \"input_b[\", i, \"].dtype != element_dtype.  Saw: \",\n                      DataTypeString(l_b->element_dtype), \" vs. \",\n                      DataTypeString(element_dtype_)));\n      OP_REQUIRES(c, l_a->element_shape.IsIdenticalTo(l_b->element_shape),\n                  errors::InvalidArgument(\n                      \"input_a and input_b TensorList element shapes are not \"\n                      \"identical at index \",\n                      i, \".  Saw \", l_a->element_shape.DebugString(), \" vs. \",\n                      l_b->element_shape.DebugString()));\n      if (ok_to_alias) {\n        TensorList* out = output_t(i).get<TensorList>();\n        std::copy(l_b->tensors().begin(), l_b->tensors().end(),\n                  std::back_inserter(out->tensors()));\n      } else {\n        TensorList out = l_a->Copy();\n        std::copy(l_b->tensors().begin(), l_b->tensors().end(),\n                  std::back_inserter(out.tensors()));\n        output_t(i) = std::move(out);\n      }\n    }\n  }\n\n private:\n  DataType element_dtype_;\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"TensorListConcatLists\").Device(DEVICE_CPU),\n                        TensorListConcatLists);\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\nREGISTER_KERNEL_BUILDER(Name(\"TensorListConcatLists\").Device(DEVICE_GPU),\n                        TensorListConcatLists);\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n#define REGISTER_TENSOR_LIST_OPS_CPU(T)                                    \\\n  REGISTER_KERNEL_BUILDER(Name(\"TensorListStack\")                          \\\n                              .TypeConstraint<T>(\"element_dtype\")          \\\n                              .Device(DEVICE_CPU),                         \\\n                          TensorListStack<CPUDevice, T>)                   \\\n  REGISTER_KERNEL_BUILDER(Name(\"TensorListGather\")                         \\\n                              .TypeConstraint<T>(\"element_dtype\")          \\\n                              .Device(DEVICE_CPU),                         \\\n                          TensorListGather<CPUDevice, T>)                  \\\n  REGISTER_KERNEL_BUILDER(Name(\"TensorListConcat\")                         \\\n                              .TypeConstraint<T>(\"element_dtype\")          \\\n                              .Device(DEVICE_CPU),                         \\\n                          TensorListConcat<CPUDevice, T>)                  \\\n  REGISTER_KERNEL_BUILDER(Name(\"TensorListConcatV2\")                       \\\n                              .TypeConstraint<T>(\"element_dtype\")          \\\n                              .Device(DEVICE_CPU),                         \\\n                          TensorListConcat<CPUDevice, T>)                  \\\n  REGISTER_KERNEL_BUILDER(Name(\"TensorListGetItem\")                        \\\n                              .TypeConstraint<T>(\"element_dtype\")          \\\n                              .Device(DEVICE_CPU),                         \\\n                          TensorListGetItem<CPUDevice, T>)                 \\\n  REGISTER_KERNEL_BUILDER(Name(\"TensorListPopBack\")                        \\\n                              .TypeConstraint<T>(\"element_dtype\")          \\\n                              .Device(DEVICE_CPU),                         \\\n                          TensorListPopBack<CPUDevice, T>)                 \\\n  REGISTER_KERNEL_BUILDER(Name(\"TensorListFromTensor\")                     \\\n                              .TypeConstraint<T>(\"element_dtype\")          \\\n                              .Device(DEVICE_CPU),                         \\\n                          TensorListFromTensor<CPUDevice, T>)              \\\n  REGISTER_KERNEL_BUILDER(Name(\"TensorListScatter\")                        \\\n                              .TypeConstraint<T>(\"element_dtype\")          \\\n                              .Device(DEVICE_CPU),                         \\\n                          TensorListScatter<CPUDevice, T>)                 \\\n  REGISTER_KERNEL_BUILDER(Name(\"TensorListScatterV2\")                      \\\n                              .TypeConstraint<T>(\"element_dtype\")          \\\n                              .Device(DEVICE_CPU),                         \\\n                          TensorListScatter<CPUDevice, T>)                 \\\n  REGISTER_KERNEL_BUILDER(Name(\"TensorListScatterIntoExistingList\")        \\\n                              .TypeConstraint<T>(\"element_dtype\")          \\\n                              .Device(DEVICE_CPU),                         \\\n                          TensorListScatterIntoExistingList<CPUDevice, T>) \\\n  REGISTER_KERNEL_BUILDER(Name(\"TensorListSplit\")                          \\\n                              .TypeConstraint<T>(\"element_dtype\")          \\\n                              .Device(DEVICE_CPU),                         \\\n                          TensorListSplit<CPUDevice, T>)                   \\\n  REGISTER_KERNEL_BUILDER(Name(\"TensorListPushBackBatch\")                  \\\n                              .TypeConstraint<T>(\"element_dtype\")          \\\n                              .Device(DEVICE_CPU),                         \\\n                          TensorListPushBackBatch<CPUDevice, T>)\n\nTF_CALL_POD_STRING_TYPES(REGISTER_TENSOR_LIST_OPS_CPU);\nREGISTER_TENSOR_LIST_OPS_CPU(quint8);\nREGISTER_TENSOR_LIST_OPS_CPU(qint8);\nREGISTER_TENSOR_LIST_OPS_CPU(quint16);\nREGISTER_TENSOR_LIST_OPS_CPU(qint16);\nREGISTER_TENSOR_LIST_OPS_CPU(qint32);\nREGISTER_TENSOR_LIST_OPS_CPU(Variant);\n\n#undef REGISTER_TENSOR_LIST_OPS_CPU\n\n#define REGISTER_TENSOR_LIST_OPS_CPU(T)\n\nREGISTER_UNARY_VARIANT_BINARY_OP_FUNCTION(ADD_VARIANT_BINARY_OP, DEVICE_CPU,\n                                          TensorList,\n                                          TensorListBinaryAdd<CPUDevice>);\n\nREGISTER_UNARY_VARIANT_UNARY_OP_FUNCTION(ZEROS_LIKE_VARIANT_UNARY_OP,\n                                         DEVICE_CPU, TensorList,\n                                         TensorListZerosLike<CPUDevice>);\n\n}  // namespace tensorflow\n"], "fixing_code": ["/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#define EIGEN_USE_THREADS\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n#define EIGEN_USE_GPU\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n#include \"tensorflow/core/kernels/list_kernels.h\"\n\n#include <limits>\n\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n#include \"tensorflow/core/framework/allocator.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/register_types.h\"\n#include \"tensorflow/core/framework/tensor_types.h\"\n#include \"tensorflow/core/framework/variant.h\"\n#include \"tensorflow/core/framework/variant_op_registry.h\"\n#include \"tensorflow/core/kernels/concat_lib.h\"\n#include \"tensorflow/core/lib/core/coding.h\"\n#include \"tensorflow/core/lib/core/errors.h\"\n#include \"tensorflow/core/util/util.h\"\n\nnamespace tensorflow {\n\ntypedef Eigen::ThreadPoolDevice CPUDevice;\n\nStatus TensorShapeFromTensor(const Tensor& t, PartialTensorShape* out) {\n  if (t.shape() == TensorShape({})) {\n    if ((t.dtype() == DT_INT32 && t.scalar<int32>()() == -1) ||\n        (t.dtype() == DT_INT64 && t.scalar<int64>()() == -1)) {\n      *out = PartialTensorShape();\n      return Status::OK();\n    }\n    return errors::InvalidArgument(\n        \"The only valid scalar shape tensor is the fully unknown shape \"\n        \"specified as -1.\");\n  }\n  if (t.dtype() == DT_INT32) {\n    return PartialTensorShape::MakePartialShape(t.vec<int32>().data(),\n                                                t.NumElements(), out);\n  } else if (t.dtype() == DT_INT64) {\n    return PartialTensorShape::MakePartialShape(t.vec<int64>().data(),\n                                                t.NumElements(), out);\n  }\n  return errors::InvalidArgument(\n      \"Expected an int32 or int64 shape tensor; found \",\n      DataTypeString(t.dtype()));\n}\n\nStatus GetElementShapeFromInput(OpKernelContext* c,\n                                const TensorList& tensor_list, int index,\n                                PartialTensorShape* element_shape) {\n  TF_RETURN_IF_ERROR(TensorShapeFromTensor(c->input(index), element_shape));\n  // Check that `element_shape` and `tensor_list.element_shape` are\n  // compatible and store the merged shape in `element_shape`.\n  PartialTensorShape tmp = *element_shape;\n  TF_RETURN_IF_ERROR(tmp.MergeWith(tensor_list.element_shape, element_shape));\n  return Status::OK();\n}\n\nStatus GetInputList(OpKernelContext* c, int index, const TensorList** list) {\n  if (!TensorShapeUtils::IsScalar(c->input(index).shape())) {\n    return errors::InvalidArgument(\"Input list must be a scalar saw: \",\n                                   c->input(index).shape().DebugString());\n  }\n  const TensorList* l = c->input(index).scalar<Variant>()().get<TensorList>();\n  if (l == nullptr) {\n    return errors::InvalidArgument(\n        \"Input handle is not a list. Saw: '\",\n        c->input(index).scalar<Variant>()().DebugString(), \"'\");\n  }\n  *list = l;\n  return Status::OK();\n}\n\nStatus ForwardInputOrCreateNewList(OpKernelContext* c, int32 input_index,\n                                   int32 output_index,\n                                   const TensorList& input_list,\n                                   TensorList** output_list) {\n  // Attempt to forward the input tensor to the output if possible.\n  std::unique_ptr<Tensor> maybe_output = c->forward_input(\n      input_index, output_index, DT_VARIANT, TensorShape{},\n      c->input_memory_type(input_index), AllocatorAttributes());\n  Tensor* output_tensor;\n  if (maybe_output != nullptr && maybe_output->dtype() == DT_VARIANT &&\n      maybe_output->NumElements() == 1) {\n    output_tensor = maybe_output.get();\n    TensorList* tmp_out = output_tensor->scalar<Variant>()().get<TensorList>();\n    if (tmp_out == nullptr) {\n      return errors::InvalidArgument(\n          \"Expected input \", input_index, \" to be a TensorList but saw \",\n          output_tensor->scalar<Variant>()().TypeName());\n    }\n    if (tmp_out->RefCountIsOne()) {\n      // Woohoo, forwarding succeeded!\n      c->set_output(output_index, *output_tensor);\n      *output_list = tmp_out;\n      return Status::OK();\n    }\n  }\n\n  // If forwarding is not possible allocate a new output tensor and copy\n  // the `input_list` to it.\n  AllocatorAttributes attr;\n  attr.set_on_host(true);\n  TF_RETURN_IF_ERROR(\n      c->allocate_output(output_index, {}, &output_tensor, attr));\n  output_tensor->scalar<Variant>()() = input_list.Copy();\n\n  *output_list = output_tensor->scalar<Variant>()().get<TensorList>();\n  return Status::OK();\n}\n\nclass EmptyTensorList : public OpKernel {\n public:\n  explicit EmptyTensorList(OpKernelConstruction* ctx) : OpKernel(ctx) {\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"element_dtype\", &element_dtype_));\n  }\n\n  void Compute(OpKernelContext* ctx) override {\n    const Tensor& max_num_elements_t = ctx->input(1);\n    OP_REQUIRES(\n        ctx, TensorShapeUtils::IsScalar(max_num_elements_t.shape()),\n        errors::InvalidArgument(\n            \"max_num_elements expected to be a scalar \",\n            \"but got shape: \", max_num_elements_t.shape().DebugString()));\n    Tensor* result;\n    AllocatorAttributes attr;\n    attr.set_on_host(true);\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, TensorShape{}, &result, attr));\n    TensorList empty;\n    empty.element_dtype = element_dtype_;\n    empty.max_num_elements = max_num_elements_t.scalar<int32>()();\n    PartialTensorShape element_shape;\n    OP_REQUIRES_OK(ctx, TensorShapeFromTensor(ctx->input(0), &element_shape));\n    empty.element_shape = element_shape;\n    result->scalar<Variant>()() = std::move(empty);\n  }\n\n private:\n  DataType element_dtype_;\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"EmptyTensorList\").Device(DEVICE_CPU),\n                        EmptyTensorList);\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\nREGISTER_KERNEL_BUILDER(Name(\"EmptyTensorList\")\n                            .Device(DEVICE_GPU)\n                            .HostMemory(\"element_shape\")\n                            .HostMemory(\"max_num_elements\"),\n                        EmptyTensorList);\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\nclass TensorListPushBack : public OpKernel {\n public:\n  explicit TensorListPushBack(OpKernelConstruction* c) : OpKernel(c) {\n    OP_REQUIRES_OK(c, c->GetAttr(\"element_dtype\", &element_dtype_));\n  }\n\n  ~TensorListPushBack() override {}\n\n  void Compute(OpKernelContext* c) override {\n    const Tensor& input = c->input(1);\n    OP_REQUIRES(c, element_dtype_ == input.dtype(),\n                errors::InvalidArgument(\"Invalid data types; list elements \",\n                                        DataTypeString(element_dtype_),\n                                        \" but tried to append \",\n                                        DataTypeString(input.dtype())));\n\n    const TensorList* l = nullptr;\n    OP_REQUIRES_OK(c, GetInputList(c, 0, &l));\n    OP_REQUIRES(c, l->element_shape.IsCompatibleWith(input.shape()),\n                errors::InvalidArgument(\n                    \"Tried to append a tensor with incompatible shape to a \"\n                    \"list. Op element shape: \",\n                    input.shape().DebugString(),\n                    \" list shape: \", l->element_shape.DebugString()));\n    OP_REQUIRES(c, element_dtype_ == l->element_dtype,\n                errors::InvalidArgument(\"Invalid data types; op elements \",\n                                        DataTypeString(element_dtype_),\n                                        \" but list elements \",\n                                        DataTypeString(l->element_dtype)));\n\n    if (l->max_num_elements != -1) {\n      OP_REQUIRES(\n          c, l->tensors().size() < l->max_num_elements,\n          errors::InvalidArgument(\"Tried to push item into a full list\",\n                                  \" list size: \", l->tensors().size(),\n                                  \" max_num_elements: \", l->max_num_elements));\n    }\n\n    TensorList* output_list = nullptr;\n    OP_REQUIRES_OK(c, ForwardInputOrCreateNewList(c, 0, 0, *l, &output_list));\n    output_list->tensors().push_back(input);\n  }\n\n private:\n  DataType element_dtype_;\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"TensorListPushBack\").Device(DEVICE_CPU),\n                        TensorListPushBack);\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\nREGISTER_KERNEL_BUILDER(Name(\"TensorListPushBack\").Device(DEVICE_GPU),\n                        TensorListPushBack);\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\nclass TensorListLength : public OpKernel {\n public:\n  explicit TensorListLength(OpKernelConstruction* c) : OpKernel(c) {}\n  ~TensorListLength() override {}\n\n  void Compute(OpKernelContext* c) override {\n    const TensorList* l = nullptr;\n    OP_REQUIRES_OK(c, GetInputList(c, 0, &l));\n    Tensor* result;\n    OP_REQUIRES_OK(c, c->allocate_output(0, TensorShape{}, &result));\n    result->scalar<int32>()() = l->tensors().size();\n  }\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"TensorListLength\").Device(DEVICE_CPU),\n                        TensorListLength);\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\nREGISTER_KERNEL_BUILDER(\n    Name(\"TensorListLength\").Device(DEVICE_GPU).HostMemory(\"length\"),\n    TensorListLength);\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\nclass TensorListElementShape : public OpKernel {\n public:\n  explicit TensorListElementShape(OpKernelConstruction* c) : OpKernel(c) {}\n\n  void Compute(OpKernelContext* c) override {\n    const TensorList* l = nullptr;\n    OP_REQUIRES_OK(c, GetInputList(c, 0, &l));\n    Tensor* result;\n    if (l->element_shape.unknown_rank()) {\n      OP_REQUIRES_OK(c, c->allocate_output(0, TensorShape({}), &result));\n      if (result->dtype() == DT_INT32) {\n        result->scalar<int32>()() = -1;\n      } else {\n        result->scalar<int64>()() = -1;\n      }\n    } else {\n      OP_REQUIRES_OK(c, c->allocate_output(\n                            0, TensorShape{l->element_shape.dims()}, &result));\n      for (int i = 0; i < l->element_shape.dims(); ++i) {\n        if (result->dtype() == DT_INT32) {\n          result->flat<int32>()(i) = l->element_shape.dim_size(i);\n        } else {\n          result->flat<int64>()(i) = l->element_shape.dim_size(i);\n        }\n      }\n    }\n  }\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"TensorListElementShape\").Device(DEVICE_CPU),\n                        TensorListElementShape);\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\nREGISTER_KERNEL_BUILDER(Name(\"TensorListElementShape\")\n                            .Device(DEVICE_GPU)\n                            .HostMemory(\"element_shape\"),\n                        TensorListElementShape);\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\nclass TensorListReserve : public OpKernel {\n public:\n  explicit TensorListReserve(OpKernelConstruction* c) : OpKernel(c) {\n    OP_REQUIRES_OK(c, c->GetAttr(\"element_dtype\", &element_dtype_));\n  }\n\n  void Compute(OpKernelContext* c) override {\n    PartialTensorShape element_shape;\n    OP_REQUIRES_OK(c, TensorShapeFromTensor(c->input(0), &element_shape));\n    int32 num_elements = c->input(1).scalar<int32>()();\n    OP_REQUIRES(c, num_elements >= 0,\n                errors::InvalidArgument(\"The num_elements to reserve must be a \"\n                                        \"non negative number, but got \",\n                                        num_elements));\n    TensorList output;\n    output.element_shape = element_shape;\n    output.element_dtype = element_dtype_;\n    output.tensors().resize(num_elements, Tensor(DT_INVALID));\n    Tensor* result;\n    AllocatorAttributes attr;\n    attr.set_on_host(true);\n    OP_REQUIRES_OK(c, c->allocate_output(0, TensorShape{}, &result, attr));\n    result->scalar<Variant>()() = std::move(output);\n  }\n\n private:\n  DataType element_dtype_;\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"TensorListReserve\").Device(DEVICE_CPU),\n                        TensorListReserve);\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\nREGISTER_KERNEL_BUILDER(Name(\"TensorListReserve\")\n                            .Device(DEVICE_GPU)\n                            .HostMemory(\"element_shape\")\n                            .HostMemory(\"num_elements\"),\n                        TensorListReserve);\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\nclass TensorListResize : public OpKernel {\n public:\n  explicit TensorListResize(OpKernelConstruction* c) : OpKernel(c) {}\n\n  void Compute(OpKernelContext* c) override {\n    const TensorList* input_list = nullptr;\n    OP_REQUIRES_OK(c, GetInputList(c, 0, &input_list));\n    int32 size = c->input(1).scalar<int32>()();\n    OP_REQUIRES(\n        c, size >= 0,\n        errors::InvalidArgument(\n            \"TensorListSlice expects size to be non-negative. Got: \", size));\n\n    std::unique_ptr<Tensor> maybe_result =\n        c->forward_input(0, 0, DT_VARIANT, TensorShape{},\n                         c->input_memory_type(0), AllocatorAttributes());\n    if (maybe_result != nullptr) {\n      TensorList* out = maybe_result->scalar<Variant>()().get<TensorList>();\n      if (out->RefCountIsOne()) {\n        // We are able to forward the input.\n        out->tensors().resize(size, Tensor(DT_INVALID));\n        c->set_output(0, *maybe_result);\n        return;\n      }\n    }\n\n    // We were not able to forward the input.  Will have to resize from scratch.\n    Tensor* result;\n    AllocatorAttributes attr;\n    attr.set_on_host(true);\n    OP_REQUIRES_OK(c, c->allocate_output(0, TensorShape{}, &result, attr));\n    TensorList output_list;\n    output_list.element_shape = input_list->element_shape;\n    output_list.element_dtype = input_list->element_dtype;\n    output_list.max_num_elements = input_list->max_num_elements;\n    if (size > input_list->tensors().size()) {\n      output_list.tensors().insert(output_list.tensors().begin(),\n                                   input_list->tensors().begin(),\n                                   input_list->tensors().end());\n      // Add DT_INVALID tensors to the end of the list if the requested size\n      // is larger than the list length.\n      output_list.tensors().resize(size, Tensor(DT_INVALID));\n    } else {\n      output_list.tensors().insert(output_list.tensors().begin(),\n                                   input_list->tensors().begin(),\n                                   input_list->tensors().begin() + size);\n    }\n    result->scalar<Variant>()() = std::move(output_list);\n  }\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"TensorListResize\").Device(DEVICE_CPU),\n                        TensorListResize);\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\nREGISTER_KERNEL_BUILDER(\n    Name(\"TensorListResize\").Device(DEVICE_GPU).HostMemory(\"size\"),\n    TensorListResize);\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\nclass TensorListSetItem : public OpKernel {\n public:\n  explicit TensorListSetItem(OpKernelConstruction* c) : OpKernel(c) {\n    OP_REQUIRES_OK(c, c->GetAttr(\"element_dtype\", &element_dtype_));\n  }\n\n  void Compute(OpKernelContext* c) override {\n    const TensorList* l = nullptr;\n    OP_REQUIRES_OK(c, GetInputList(c, 0, &l));\n    OP_REQUIRES(c, element_dtype_ == l->element_dtype,\n                errors::InvalidArgument(\"Invalid data types; op elements \",\n                                        DataTypeString(element_dtype_),\n                                        \" but list elements \",\n                                        DataTypeString(l->element_dtype)));\n    int32 index = c->input(1).scalar<int32>()();\n    OP_REQUIRES(c, index < l->tensors().size(),\n                errors::InvalidArgument(\"Trying to modify element \", index,\n                                        \" in a list with \", l->tensors().size(),\n                                        \" elements.\"));\n    const Tensor& value = c->input(2);\n    OP_REQUIRES(c, l->element_shape.IsCompatibleWith(value.shape()),\n                errors::InvalidArgument(\n                    \"Tried to set a tensor with incompatible shape at a \"\n                    \"list index. Item element shape: \",\n                    value.shape().DebugString(),\n                    \" list shape: \", l->element_shape.DebugString()));\n    TensorList* output_list = nullptr;\n    OP_REQUIRES_OK(c, ForwardInputOrCreateNewList(c, 0, 0, *l, &output_list));\n    output_list->tensors()[index] = value;\n  }\n\n private:\n  DataType element_dtype_;\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"TensorListSetItem\").Device(DEVICE_CPU),\n                        TensorListSetItem);\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n#define REGISTER_TENSOR_LIST_SET_ITEM_GPU(T)                      \\\n  REGISTER_KERNEL_BUILDER(Name(\"TensorListSetItem\")               \\\n                              .TypeConstraint<T>(\"element_dtype\") \\\n                              .Device(DEVICE_GPU)                 \\\n                              .HostMemory(\"index\"),               \\\n                          TensorListSetItem);\n\nTF_CALL_GPU_ALL_TYPES(REGISTER_TENSOR_LIST_SET_ITEM_GPU);\nTF_CALL_int32(REGISTER_TENSOR_LIST_SET_ITEM_GPU);\nTF_CALL_int64(REGISTER_TENSOR_LIST_SET_ITEM_GPU);\nREGISTER_TENSOR_LIST_SET_ITEM_GPU(bfloat16)\n#undef REGISTER_TENSOR_LIST_SET_ITEM_GPU\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\nclass TensorListConcatLists : public OpKernel {\n public:\n  explicit TensorListConcatLists(OpKernelConstruction* c) : OpKernel(c) {\n    OP_REQUIRES_OK(c, c->GetAttr(\"element_dtype\", &element_dtype_));\n  }\n\n  void Compute(OpKernelContext* c) override {\n    const TensorShape& tl_a_shape = c->input(0).shape();\n    const TensorShape& tl_b_shape = c->input(1).shape();\n    OP_REQUIRES(\n        c, tl_a_shape == tl_b_shape,\n        errors::InvalidArgument(\"Incompatible input TensorList tensor shapes: \",\n                                tl_a_shape.DebugString(), \" vs. \",\n                                tl_b_shape.DebugString()));\n    AllocatorAttributes attr;\n    std::unique_ptr<Tensor> tl_alias = c->forward_input(\n        0 /*input_index*/, 0 /*output_index*/, DT_VARIANT, tl_a_shape,\n        DEVICE_MEMORY /* input is always on DEVICE_MEMORY */, attr);\n\n    // tl_a may be aliased by tl_alias.\n    const Tensor& tl_a = c->input(0);\n    const Tensor& tl_b = c->input(1);\n\n    Tensor* output = nullptr;\n    bool ok_to_alias = tl_alias != nullptr;\n    if (tl_alias && tl_alias->dtype() == DT_VARIANT &&\n        tl_alias->NumElements() > 0) {\n      auto tl_a_t = tl_alias->flat<Variant>();\n      for (int64 i = 0; i < tl_alias->NumElements(); ++i) {\n        TensorList* aliased = tl_a_t(i).get<TensorList>();\n        if (aliased == nullptr || !aliased->RefCountIsOne()) {\n          ok_to_alias = false;\n          break;\n        }\n      }\n      if (ok_to_alias) {\n        c->set_output(0, *tl_alias);\n        output = tl_alias.get();\n      }\n    }\n    if (!ok_to_alias) {\n      // Couldn't alias the entire Tensor.  We'll be conservative and not try\n      // to alias individual batch entries.\n      attr.set_on_host(true);\n      OP_REQUIRES_OK(c, c->allocate_output(0, tl_a_shape, &output, attr));\n    }\n\n    auto output_t = output->flat<Variant>();\n    auto tl_a_t = tl_a.flat<Variant>();\n    auto tl_b_t = tl_b.flat<Variant>();\n\n    for (int64 i = 0; i < tl_a.NumElements(); ++i) {\n      const TensorList* l_a = tl_a_t(i).get<TensorList>();\n      const TensorList* l_b = tl_b_t(i).get<TensorList>();\n      OP_REQUIRES(\n          c, l_a != nullptr,\n          errors::InvalidArgument(\"input_a is not a TensorList at index \", i,\n                                  \".  Saw: '\", tl_a_t(i).DebugString(), \"'\"));\n      OP_REQUIRES(\n          c, l_b != nullptr,\n          errors::InvalidArgument(\"input_b is not a TensorList at index \", i,\n                                  \".  Saw: '\", tl_b_t(i).DebugString(), \"'\"));\n      OP_REQUIRES(c, l_a->element_dtype == element_dtype_,\n                  errors::InvalidArgument(\n                      \"input_a[\", i, \"].dtype != element_dtype.  Saw: \",\n                      DataTypeString(l_a->element_dtype), \" vs. \",\n                      DataTypeString(element_dtype_)));\n      OP_REQUIRES(c, l_b->element_dtype == element_dtype_,\n                  errors::InvalidArgument(\n                      \"input_b[\", i, \"].dtype != element_dtype.  Saw: \",\n                      DataTypeString(l_b->element_dtype), \" vs. \",\n                      DataTypeString(element_dtype_)));\n      OP_REQUIRES(c, l_a->element_shape.IsIdenticalTo(l_b->element_shape),\n                  errors::InvalidArgument(\n                      \"input_a and input_b TensorList element shapes are not \"\n                      \"identical at index \",\n                      i, \".  Saw \", l_a->element_shape.DebugString(), \" vs. \",\n                      l_b->element_shape.DebugString()));\n      if (ok_to_alias) {\n        TensorList* out = output_t(i).get<TensorList>();\n        std::copy(l_b->tensors().begin(), l_b->tensors().end(),\n                  std::back_inserter(out->tensors()));\n      } else {\n        TensorList out = l_a->Copy();\n        std::copy(l_b->tensors().begin(), l_b->tensors().end(),\n                  std::back_inserter(out.tensors()));\n        output_t(i) = std::move(out);\n      }\n    }\n  }\n\n private:\n  DataType element_dtype_;\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"TensorListConcatLists\").Device(DEVICE_CPU),\n                        TensorListConcatLists);\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\nREGISTER_KERNEL_BUILDER(Name(\"TensorListConcatLists\").Device(DEVICE_GPU),\n                        TensorListConcatLists);\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n#define REGISTER_TENSOR_LIST_OPS_CPU(T)                                    \\\n  REGISTER_KERNEL_BUILDER(Name(\"TensorListStack\")                          \\\n                              .TypeConstraint<T>(\"element_dtype\")          \\\n                              .Device(DEVICE_CPU),                         \\\n                          TensorListStack<CPUDevice, T>)                   \\\n  REGISTER_KERNEL_BUILDER(Name(\"TensorListGather\")                         \\\n                              .TypeConstraint<T>(\"element_dtype\")          \\\n                              .Device(DEVICE_CPU),                         \\\n                          TensorListGather<CPUDevice, T>)                  \\\n  REGISTER_KERNEL_BUILDER(Name(\"TensorListConcat\")                         \\\n                              .TypeConstraint<T>(\"element_dtype\")          \\\n                              .Device(DEVICE_CPU),                         \\\n                          TensorListConcat<CPUDevice, T>)                  \\\n  REGISTER_KERNEL_BUILDER(Name(\"TensorListConcatV2\")                       \\\n                              .TypeConstraint<T>(\"element_dtype\")          \\\n                              .Device(DEVICE_CPU),                         \\\n                          TensorListConcat<CPUDevice, T>)                  \\\n  REGISTER_KERNEL_BUILDER(Name(\"TensorListGetItem\")                        \\\n                              .TypeConstraint<T>(\"element_dtype\")          \\\n                              .Device(DEVICE_CPU),                         \\\n                          TensorListGetItem<CPUDevice, T>)                 \\\n  REGISTER_KERNEL_BUILDER(Name(\"TensorListPopBack\")                        \\\n                              .TypeConstraint<T>(\"element_dtype\")          \\\n                              .Device(DEVICE_CPU),                         \\\n                          TensorListPopBack<CPUDevice, T>)                 \\\n  REGISTER_KERNEL_BUILDER(Name(\"TensorListFromTensor\")                     \\\n                              .TypeConstraint<T>(\"element_dtype\")          \\\n                              .Device(DEVICE_CPU),                         \\\n                          TensorListFromTensor<CPUDevice, T>)              \\\n  REGISTER_KERNEL_BUILDER(Name(\"TensorListScatter\")                        \\\n                              .TypeConstraint<T>(\"element_dtype\")          \\\n                              .Device(DEVICE_CPU),                         \\\n                          TensorListScatter<CPUDevice, T>)                 \\\n  REGISTER_KERNEL_BUILDER(Name(\"TensorListScatterV2\")                      \\\n                              .TypeConstraint<T>(\"element_dtype\")          \\\n                              .Device(DEVICE_CPU),                         \\\n                          TensorListScatter<CPUDevice, T>)                 \\\n  REGISTER_KERNEL_BUILDER(Name(\"TensorListScatterIntoExistingList\")        \\\n                              .TypeConstraint<T>(\"element_dtype\")          \\\n                              .Device(DEVICE_CPU),                         \\\n                          TensorListScatterIntoExistingList<CPUDevice, T>) \\\n  REGISTER_KERNEL_BUILDER(Name(\"TensorListSplit\")                          \\\n                              .TypeConstraint<T>(\"element_dtype\")          \\\n                              .Device(DEVICE_CPU),                         \\\n                          TensorListSplit<CPUDevice, T>)                   \\\n  REGISTER_KERNEL_BUILDER(Name(\"TensorListPushBackBatch\")                  \\\n                              .TypeConstraint<T>(\"element_dtype\")          \\\n                              .Device(DEVICE_CPU),                         \\\n                          TensorListPushBackBatch<CPUDevice, T>)\n\nTF_CALL_POD_STRING_TYPES(REGISTER_TENSOR_LIST_OPS_CPU);\nREGISTER_TENSOR_LIST_OPS_CPU(quint8);\nREGISTER_TENSOR_LIST_OPS_CPU(qint8);\nREGISTER_TENSOR_LIST_OPS_CPU(quint16);\nREGISTER_TENSOR_LIST_OPS_CPU(qint16);\nREGISTER_TENSOR_LIST_OPS_CPU(qint32);\nREGISTER_TENSOR_LIST_OPS_CPU(Variant);\n\n#undef REGISTER_TENSOR_LIST_OPS_CPU\n\n#define REGISTER_TENSOR_LIST_OPS_CPU(T)\n\nREGISTER_UNARY_VARIANT_BINARY_OP_FUNCTION(ADD_VARIANT_BINARY_OP, DEVICE_CPU,\n                                          TensorList,\n                                          TensorListBinaryAdd<CPUDevice>);\n\nREGISTER_UNARY_VARIANT_UNARY_OP_FUNCTION(ZEROS_LIKE_VARIANT_UNARY_OP,\n                                         DEVICE_CPU, TensorList,\n                                         TensorListZerosLike<CPUDevice>);\n\n}  // namespace tensorflow\n"], "buggy_code_start_loc": [304], "buggy_code_end_loc": [304], "fixing_code_start_loc": [305], "fixing_code_end_loc": [309], "type": "CWE-617", "message": "TensorFlow is an end-to-end open source platform for machine learning. In affected versions providing a negative element to `num_elements` list argument of `tf.raw_ops.TensorListReserve` causes the runtime to abort the process due to reallocating a `std::vector` to have a negative number of elements. The [implementation](https://github.com/tensorflow/tensorflow/blob/8d72537c6abf5a44103b57b9c2e22c14f5f49698/tensorflow/core/kernels/list_kernels.cc#L312) calls `std::vector.resize()` with the new size controlled by input given by the user, without checking that this input is valid. We have patched the issue in GitHub commit 8a6e874437670045e6c7dc6154c7412b4a2135e2. The fix will be included in TensorFlow 2.6.0. We will also cherrypick this commit on TensorFlow 2.5.1, TensorFlow 2.4.3, and TensorFlow 2.3.4, as these are also affected and still in supported range.", "other": {"cve": {"id": "CVE-2021-37644", "sourceIdentifier": "security-advisories@github.com", "published": "2021-08-12T21:15:07.770", "lastModified": "2021-08-18T15:38:10.463", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "TensorFlow is an end-to-end open source platform for machine learning. In affected versions providing a negative element to `num_elements` list argument of `tf.raw_ops.TensorListReserve` causes the runtime to abort the process due to reallocating a `std::vector` to have a negative number of elements. The [implementation](https://github.com/tensorflow/tensorflow/blob/8d72537c6abf5a44103b57b9c2e22c14f5f49698/tensorflow/core/kernels/list_kernels.cc#L312) calls `std::vector.resize()` with the new size controlled by input given by the user, without checking that this input is valid. We have patched the issue in GitHub commit 8a6e874437670045e6c7dc6154c7412b4a2135e2. The fix will be included in TensorFlow 2.6.0. We will also cherrypick this commit on TensorFlow 2.5.1, TensorFlow 2.4.3, and TensorFlow 2.3.4, as these are also affected and still in supported range."}, {"lang": "es", "value": "TensorFlow es una plataforma de c\u00f3digo abierto de extremo a extremo para el aprendizaje autom\u00e1tico. En las versiones afectadas, proporcionar un elemento negativo al argumento de lista \"num_elements\" de \"tf.raw_ops.TensorListReserve\" hace que el tiempo de ejecuci\u00f3n aborte el proceso debido a la reasignaci\u00f3n de un \"std::vector\" para tener un n\u00famero negativo de elementos. La [implementaci\u00f3n](https://github.com/tensorflow/tensorflow/blob/8d72537c6abf5a44103b57b9c2e22c14f5f49698/tensorflow/core/kernels/list_kernels.cc#L312) llama a \"std::vector.resize()\" con el nuevo tama\u00f1o controlado por la entrada dada por el usuario, sin comprobar que esta entrada es v\u00e1lida. Hemos parcheado el problema en el commit 8a6e874437670045e6c7dc6154c7412b4a2135e2 de GitHub. La correcci\u00f3n ser\u00e1 incluida en TensorFlow versi\u00f3n 2.6.0. Tambi\u00e9n seleccionaremos este commit en TensorFlow 2.5.1, TensorFlow 2.4.3 y TensorFlow 2.3.4, ya que estos tambi\u00e9n est\u00e1n afectados y todav\u00eda est\u00e1n en el rango de soporte."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:N/I:N/A:P", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "PARTIAL", "baseScore": 2.1}, "baseSeverity": "LOW", "exploitabilityScore": 3.9, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "security-advisories@github.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-617"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.3.0", "versionEndExcluding": "2.3.4", "matchCriteriaId": "0F83C081-51CC-415F-A8C0-0A44C75E2CD6"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.4.0", "versionEndExcluding": "2.4.3", "matchCriteriaId": "BD3F2BF8-EBA9-42BF-8F9B-D918B880B15A"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.5.0:*:*:*:*:*:*:*", "matchCriteriaId": "D03E99A7-4E3D-427D-A156-C0713E9FB02A"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.6.0:rc0:*:*:*:*:*:*", "matchCriteriaId": "70FA6E48-6C57-40CA-809F-4E3D07CBF348"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.6.0:rc1:*:*:*:*:*:*", "matchCriteriaId": "42187561-E491-434D-828C-F36701446634"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.6.0:rc2:*:*:*:*:*:*", "matchCriteriaId": "C66B61C8-450A-4C5E-9174-F970D6DEE778"}]}]}], "references": [{"url": "https://github.com/tensorflow/tensorflow/commit/8a6e874437670045e6c7dc6154c7412b4a2135e2", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/security/advisories/GHSA-27j5-4p9v-pp67", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/tensorflow/tensorflow/commit/8a6e874437670045e6c7dc6154c7412b4a2135e2"}}