{"buggy_code": ["/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n// This transformation pass takes operations in TensorFlowLite dialect and\n// optimizes them to resulting operations in TensorFlowLite dialect.\n\n#include <algorithm>\n#include <climits>\n#include <cstdint>\n#include <functional>\n#include <iterator>\n#include <map>\n#include <numeric>\n#include <utility>\n\n#include \"llvm/ADT/APFloat.h\"\n#include \"llvm/ADT/APInt.h\"\n#include \"llvm/ADT/ArrayRef.h\"\n#include \"llvm/ADT/None.h\"\n#include \"llvm/ADT/Optional.h\"\n#include \"llvm/ADT/STLExtras.h\"\n#include \"llvm/ADT/SmallSet.h\"\n#include \"llvm/ADT/SmallVector.h\"\n#include \"llvm/ADT/StringRef.h\"\n#include \"llvm/ADT/StringSwitch.h\"\n#include \"llvm/Support/Casting.h\"\n#include \"llvm/Support/raw_ostream.h\"\n#include \"mlir/Dialect/StandardOps/IR/Ops.h\"  // from @llvm-project\n#include \"mlir/IR/Attributes.h\"  // from @llvm-project\n#include \"mlir/IR/BuiltinAttributes.h\"  // from @llvm-project\n#include \"mlir/IR/BuiltinTypes.h\"  // from @llvm-project\n#include \"mlir/IR/MLIRContext.h\"  // from @llvm-project\n#include \"mlir/IR/Matchers.h\"  // from @llvm-project\n#include \"mlir/IR/TypeUtilities.h\"  // from @llvm-project\n#include \"mlir/IR/Value.h\"  // from @llvm-project\n#include \"mlir/Pass/Pass.h\"  // from @llvm-project\n#include \"mlir/Support/LLVM.h\"  // from @llvm-project\n#include \"mlir/Support/LogicalResult.h\"  // from @llvm-project\n#include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"  // from @llvm-project\n#include \"tensorflow/compiler/mlir/lite/ir/tfl_ops.h\"\n#include \"tensorflow/compiler/mlir/lite/quantization/quantization_utils.h\"\n#include \"tensorflow/compiler/mlir/lite/transforms/passes.h\"\n#include \"tensorflow/compiler/mlir/lite/utils/attribute_utils.h\"\n#include \"tensorflow/compiler/mlir/lite/utils/convert_type.h\"\n#include \"tensorflow/compiler/mlir/lite/utils/validators.h\"\n#include \"tensorflow/compiler/mlir/tensorflow/ir/tf_ops.h\"\n\nnamespace mlir {\nnamespace TFL {\n\n//===----------------------------------------------------------------------===//\n// The actual Optimize Pass.\nnamespace {\nconstexpr char kRelu[] = \"RELU\";\nconstexpr char kRelu6[] = \"RELU6\";\nconstexpr char kRelu1[] = \"RELU_N1_TO_1\";\n\nbool L2NormalizeReduceAxis(Value sq_op, DenseElementsAttr axis) {\n  if (sq_op.getType().cast<ShapedType>().getRank() - 1 ==\n          *axis.getValues<int>().begin() ||\n      *axis.getValues<int>().begin() == -1) {\n    return true;\n  }\n  if (sq_op.getType().cast<ShapedType>().getRank() != axis.getNumElements()) {\n    return false;\n  }\n  auto shape = sq_op.getType().cast<ShapedType>();\n  SmallVector<int, 4> elems{axis.getValues<int>().begin(),\n                            axis.getValues<int>().end()};\n  for (int i = 0; i < shape.getRank(); ++i) {\n    if (i != elems[i]) return false;\n  }\n  return true;\n}\n\nusing ::llvm::cast;\n\n// Optimize TFLite operations in functions.\nclass OptimizePass : public PassWrapper<OptimizePass, FunctionPass> {\n public:\n  OptimizePass() = default;\n  OptimizePass(const OptimizePass &) {}\n  explicit OptimizePass(bool enable_canonicalization) {\n    enable_canonicalization_ = enable_canonicalization;\n  }\n\n  StringRef getArgument() const final {\n    // This is the argument used to refer to the pass in\n    // the textual format (on the commandline for example).\n    return \"tfl-optimize\";\n  }\n  StringRef getDescription() const final {\n    // This is a brief description of the pass.\n    return \"Optimize within the TensorFlow Lite dialect\";\n  }\n\n  void runOnFunction() override;\n\n private:\n  Option<bool> enable_canonicalization_{\n      *this, \"enable-canonicalization\",\n      llvm::cl::desc(\"Enable canonicalization during optimization pass.\"),\n      llvm::cl::init(false)};\n};\n\n// Returns whether the given type `a` is broadcast-compatible with `b`.\nbool IsBroadcastableElementsAttrAndType(Type a, Type b) {\n  return OpTrait::util::getBroadcastedType(a, b) != Type();\n}\n\n// Returns whether the resultant type of any broadcastable operation with\n// operands `a` and `b` matches `expected_output`. Returns false if `a` is not\n// broadcast-compatible with `b`.\nbool OperandsBroadcastToOutputType(Type a, Type b, Type expected_output) {\n  Type output_element_type =\n      expected_output.cast<ShapedType>().getElementType();\n  Type broadcasted_type =\n      OpTrait::util::getBroadcastedType(a, b, output_element_type);\n  return broadcasted_type != Type() && broadcasted_type == expected_output;\n}\n\n// Returns whether if `type1` dimensions are the same as the ending dimensions\n// of `type2`. This is more restricted than broadcastable.\nbool IsTailOfShape(Type type1, Type type2) {\n  auto tail_type = type1.dyn_cast<ShapedType>();\n  auto full_type = type2.dyn_cast<ShapedType>();\n  if (!tail_type || !full_type || !tail_type.hasRank() ||\n      !full_type.hasRank() || tail_type.getRank() > full_type.getRank())\n    return false;\n  auto i1 = tail_type.getShape().rbegin(), e1 = tail_type.getShape().rend();\n  auto i2 = full_type.getShape().rbegin();\n  return std::equal(i1, e1, i2);\n}\n\nbool CanFuseConvOrDepthwiseConvShapes(const ArrayRef<int64_t> filter_shape,\n                                      const ArrayRef<int64_t> elements_shape,\n                                      bool is_depthwise) {\n  // Make sure the val tensor has shape where all dimensions are 1 except\n  // last one.\n  // Also, val tensor must be of rank 1 or 4 or 0 (scalar).\n  const auto elements_rank = elements_shape.size();\n  for (int i = 0; i < static_cast<int>(elements_shape.size()) - 1; ++i) {\n    if (elements_shape[i] != 1) return false;\n  }\n  if (elements_rank != 1 && elements_rank != 0 && elements_rank != 4) {\n    return false;\n  }\n  auto elements_depth = elements_shape.empty() ? 1 : elements_shape.back();\n  // If elements depth equals 1 (i.e., scalar or tensor with 1 element), then we\n  // can let binary op to broadcast elements.\n  if (elements_depth == 1) {\n    return true;\n  }\n\n  // In TFLite Conv2D uses OHWI format for filter, and 1HWO for Depthwise Conv.\n  // For conv:\n  // Check if last dimension in filter equals the first dimension\n  // For depthwise conv:\n  // Check if the first in filter dimension equals the first dimension.\n  if (filter_shape.empty() ||\n      (is_depthwise ? filter_shape.back() != elements_depth\n                    : filter_shape[0] != elements_depth))\n    return false;\n  return true;\n}\n\nbool CanFuseConvOrDepthwiseConv(Value filter, Attribute val,\n                                bool is_depthwise) {\n  const auto elements = val.dyn_cast<DenseElementsAttr>();\n  if (!elements) {\n    return false;\n  }\n  const auto elements_shape = elements.getType().getShape();\n  const auto filter_shape = filter.getType().cast<ShapedType>().getShape();\n  return CanFuseConvOrDepthwiseConvShapes(filter_shape, elements_shape,\n                                          is_depthwise);\n}\n\nbool CanFuseConvOrDepthwiseConv(Attribute filter, Attribute val,\n                                bool is_depthwise) {\n  if (const auto elements = val.dyn_cast<DenseElementsAttr>()) {\n    if (const auto filter_elements = filter.dyn_cast<DenseElementsAttr>()) {\n      return CanFuseConvOrDepthwiseConvShapes(\n          filter_elements.getType().getShape(), elements.getType().getShape(),\n          is_depthwise);\n    }\n  }\n  return false;\n}\n\n// Retuns true if we can eliminate the GatherNdOp or ScatterNdOp. When the value\n// of `indices` are from 0 to n-1, the output tensor are identical to the\n// `params`.\nbool CanOptimizeIdentityGatherNdOrScatterNdOp(Value params,\n                                              DenseIntElementsAttr indices) {\n  auto params_type = params.getType().dyn_cast<RankedTensorType>();\n  auto indices_type = indices.getType().dyn_cast<RankedTensorType>();\n  // Checks the shape of `params` is [n, ...], shape of `indices` is [n, 1]. 2D\n  // `indices` means it gets the first row of `params`. As long as indices\n  // iterate the first row of `params`, the output is identical to input.\n  if (!params_type || !indices_type || indices_type.getRank() != 2 ||\n      indices_type.getDimSize(0) != params_type.getDimSize(0) ||\n      indices_type.getDimSize(1) != 1)\n    return false;\n\n  // Checks the value in `indices` is from 0 to n-1.\n  int cur_value = 0;\n  for (const auto &v : indices.getValues<APInt>()) {\n    if (v.getSExtValue() != cur_value) return false;\n    ++cur_value;\n  }\n\n  return true;\n}\n\n// Returns true if we can eliminate the SliceOp. When the values of `begin` are\n// all 0s and `size[i]` is equal to either -1 or `input.shape[i]`\n// for each dim i, the output tensor is identical to `input`.\nbool CanOptimizeIdentitySliceOp(Value input, Attribute begin, Attribute size) {\n  // Checks if `begin` and `size` are i32 or i64.\n  auto begin_attr = begin.dyn_cast<DenseIntElementsAttr>();\n  auto size_attr = size.dyn_cast<DenseIntElementsAttr>();\n  if (!begin_attr || !size_attr) {\n    return false;\n  }\n\n  auto begin_elem_ty = begin_attr.getType().getElementType();\n  if (!begin_elem_ty.isInteger(32) && !begin_elem_ty.isInteger(64)) {\n    return false;\n  }\n  auto size_elem_ty = size_attr.getType().getElementType();\n  if (!size_elem_ty.isInteger(32) && !size_elem_ty.isInteger(64)) {\n    return false;\n  }\n\n  // Checks if `input` is ranked and its rank is equal to number of elements in\n  // `begin` and `size`.\n  auto input_ty = input.getType().cast<ShapedType>();\n  if (!input_ty.hasRank()) {\n    return false;\n  }\n\n  int64_t rank = input_ty.getRank();\n  if (rank != begin_attr.getNumElements() ||\n      rank != size_attr.getNumElements()) {\n    return false;\n  }\n\n  // Checks if `begin` is all 0s, and `size[i]` is equal to either -1 or\n  // `input.shape[i]`.\n  for (uint64_t i = 0; i < rank; ++i) {\n    if (begin_attr.getValue<APInt>({i}).getSExtValue() != 0) return false;\n    int64_t si = size_attr.getValue<APInt>({i}).getSExtValue();\n    if (si != -1 && si != input_ty.getDimSize(i)) return false;\n  }\n\n  return true;\n}\n\n// Expand Attribute 'a' to 4D with all 1s except 1 dimension.\n// Which dimension depends on 'is_depthwise' is true or false.\nElementsAttr ExpandTo4DForConvImpl(Attribute a, bool is_depthwise) {\n  auto elements = a.dyn_cast<DenseElementsAttr>();\n  auto shape = elements.getType().getShape();\n  if (!shape.empty()) {\n    // Checks that elements are essentially 1d.\n    assert(elements.getNumElements() == shape.back());\n  }\n  std::vector<int64_t> shape_data = {1, 1, 1, 1};\n  const int vector_length = elements.getNumElements();\n  if (is_depthwise)\n    shape_data[3] = vector_length;\n  else\n    shape_data[0] = vector_length;\n  auto new_shape =\n      RankedTensorType::get(shape_data, elements.getType().getElementType());\n  return elements.reshape(new_shape);\n}\n\nElementsAttr ExpandTo4DForConv(Attribute a) {\n  return ExpandTo4DForConvImpl(a, false);\n}\n\nElementsAttr ExpandTo4DForDepthwiseConv(Attribute a) {\n  return ExpandTo4DForConvImpl(a, true);\n}\n\nTypeAttr RescaleQtype(Type input, Attribute factor) {\n  return quant::RescaleQuantizedType(input, factor);\n}\n\n// Returns shape of a ranked tensor.\n// Precondition: output_val's is ranked tensor.\nDenseElementsAttr GetShape(Value output_val) {\n  auto output_type = output_val.getType().cast<RankedTensorType>();\n  auto shape_vector = output_type.getShape();\n  std::vector<int32_t> shape;\n  shape.reserve(shape_vector.size());\n  for (auto shape_object : shape_vector) {\n    shape.push_back(shape_object);\n  }\n  return mlir::DenseElementsAttr::get(\n      RankedTensorType::get(\n          {static_cast<int>(shape.size())},\n          mlir::IntegerType::get(output_val.getContext(), 32)),\n      llvm::makeArrayRef(shape));\n}\n\nstatic Type GetShapeStrippedType(TypeAttr type_attr) {\n  auto type = type_attr.getValue();\n  auto shaped_type = type.dyn_cast<ShapedType>();\n  if (shaped_type) {\n    return shaped_type.getElementType();\n  } else {\n    return type;\n  }\n}\n\n// Returns `true` if reducing `axes` in `input` with `keep_dims=true` results in\n// the specified `shape` and `false` otherwise.\nstatic bool ShapeMatchesReduceWithKeepAxes(Value input,\n                                           const mlir::Attribute &axes,\n                                           const mlir::Attribute &shape) {\n  RankedTensorType type = input.getType().dyn_cast_or_null<RankedTensorType>();\n  if (!type) return false;\n\n  DenseIntElementsAttr axes_attr =\n      axes.dyn_cast_or_null<DenseIntElementsAttr>();\n  DenseIntElementsAttr shape_attr =\n      shape.dyn_cast_or_null<DenseIntElementsAttr>();\n  if (!axes_attr || !shape_attr) return false;\n\n  if (shape_attr.getNumElements() != type.getRank()) return false;\n\n  llvm::SmallSet<uint64_t, 4> axes_set;\n  for (auto a : axes_attr.getIntValues()) {\n    axes_set.insert(a.getZExtValue());\n  }\n\n  auto type_shape = type.getShape();\n  for (uint64_t i = 0; i < type.getRank(); ++i) {\n    if (axes_set.contains(i)) {\n      if (shape_attr.getValue<APInt>({i}) != 1) return false;\n    } else {\n      if (shape_attr.getValue<APInt>({i}) != type_shape[i]) return false;\n    }\n  }\n  return true;\n}\n\nstatic bool FloatValueEquals(const Attribute &attr, double value) {\n  auto fp_attr = attr.dyn_cast_or_null<DenseFPElementsAttr>();\n  if (!fp_attr) return false;\n\n  if (fp_attr.isSplat()) {\n    return fp_attr.getSplatValue<APFloat>().isExactlyValue(value);\n  }\n  return llvm::all_of(fp_attr.getFloatValues(), [value](const APFloat &f) {\n    return f.isExactlyValue(value);\n  });\n}\n\n// Returns true if the value's element type is F32.\nbool IsF32Value(Value value) {\n  return value.getType().cast<ShapedType>().getElementType().isF32();\n}\n\n// Returns the number of elements in attr if it is a DenseElementsAttr, 1\n// otherwise, as an unranked int32 Attribute.\nAttribute GetNumElementsOrOne(Attribute attr) {\n  const auto dense_attr = attr.dyn_cast_or_null<DenseElementsAttr>();\n  int32_t num_elements = dense_attr ? dense_attr.getNumElements() : 1;\n\n  OpBuilder builder(attr.getContext());\n\n  return DenseIntElementsAttr::get(\n      RankedTensorType::get({}, builder.getI32Type()),\n      {llvm::APInt(32, num_elements, true)});\n}\n\n// Returns true if attr is a DenseIntElementsAttr with the last element equal 1.\nbool IsLastElementEqualsOne(Attribute attr) {\n  const auto ints = attr.dyn_cast_or_null<DenseIntElementsAttr>();\n  if (!ints) return false;\n  if (ints.empty()) return false;\n  const auto last_element_index = ints.getNumElements() - 1;\n  const auto iterator = ints.getIntValues().begin();\n  const APInt last_element = iterator[last_element_index];\n  return last_element == 1;\n}\n\n// Returns true if attr is a DenseIntElementsAttr of int32 or int64 values or an\n// incrementing sequence from 0 to N-1.\n//\n// If such a value is used in an Equal operator, it can be replaced with OneHot.\nbool IsOneHotIndexAttribute(Attribute attr) {\n  const auto dense_attr = attr.dyn_cast_or_null<DenseIntElementsAttr>();\n  if (!dense_attr) {\n    return false;\n  }\n  auto index_type = dense_attr.getType();\n  const auto index_elem_bits = index_type.getElementTypeBitWidth();\n  if (index_elem_bits != 32 && index_elem_bits != 64) {\n    return false;\n  }\n  if (index_type.getRank() != 1) {\n    return false;\n  }\n  const auto elems = dense_attr.getIntValues().begin();\n  for (int i = 0; i < dense_attr.getNumElements(); ++i) {\n    if (i != elems[i]) {\n      return false;\n    }\n  }\n  return true;\n}\n\n// Converts an Attribute with a single value of float or integral type to an\n// Attribute holding a single value of float type. If attr has no elements, the\n// result is 0.0f.\nAttribute ConvertSingleElementAttrToFloatAttr(Attribute attr) {\n  const auto dense_fp_attr = attr.dyn_cast_or_null<DenseFPElementsAttr>();\n  if (dense_fp_attr) {\n    // Already float => return\n    return dense_fp_attr;\n  }\n\n  OpBuilder builder(attr.getContext());\n\n  const auto dense_int_attr = attr.dyn_cast<DenseIntElementsAttr>();\n  const auto int_values = dense_int_attr.getIntValues();\n  float float_val = 0.0f;\n  if (!int_values.empty()) {\n    const APInt apint_val = *int_values.begin();\n    if (dense_int_attr.getType().getElementType().isSignedInteger()) {\n      // Get the sign-extended value (=>int64) if the type is signed.\n      float_val = apint_val.getSExtValue();\n    } else {\n      // Get the zero-extended value (=>uint64) if unsigned or signless.\n      float_val = apint_val.getZExtValue();\n    }\n  }\n  return DenseFPElementsAttr::get(\n      RankedTensorType::get({}, builder.getF32Type()),\n      {llvm::APFloat(float_val)});\n}\n\n#include \"tensorflow/compiler/mlir/lite/transforms/generated_optimize.inc\"\n\n// Fuse Add with proceeding FullyConnected.\n// TODO(b/136285429): Move to tablegen when variadic is supported\nstruct FuseFullyConnectedAndAdd : public OpRewritePattern<TFL::AddOp> {\n  using OpRewritePattern<TFL::AddOp>::OpRewritePattern;\n\n  LogicalResult matchAndRewrite(TFL::AddOp add_op,\n                                PatternRewriter &rewriter) const override {\n    // Match Add.\n    DenseElementsAttr added_value;\n    Value constant_val = add_op.rhs();\n    if (!matchPattern(constant_val, m_Constant(&added_value))) return failure();\n\n    // Match Fully Connected.\n    auto fc_op =\n        dyn_cast_or_null<TFL::FullyConnectedOp>(add_op.lhs().getDefiningOp());\n    if (!fc_op) return failure();\n\n    // Check if the constant RHS is either 0D (scalar), or a 1D with\n    // `{num_channels}` shape.\n    auto constant_val_type = constant_val.getType().cast<TensorType>();\n\n    // In TFLite FullyConnect definition, bias must be a 1D tensor where\n    // the number of elements is equal to the number of channels.\n    // If it's not 1D or 0D (which can be broadcasted to 1D), reject the\n    // matching.\n    bool is_scalar_rhs = false;\n    if (constant_val_type.getRank() == 0) {\n      is_scalar_rhs = true;\n    } else if (constant_val_type.getRank() != 1) {\n      return failure();\n    }\n\n    Value filter = fc_op.filter();\n    Value bias = fc_op.bias();\n    ElementsAttr bias_value;\n    const bool is_none_bias = bias.getType().isa<NoneType>();\n    if (fc_op.fused_activation_function() != \"NONE\") return failure();\n\n    if (!is_none_bias && !matchPattern(bias, m_Constant(&bias_value)))\n      return failure();\n\n    // Rewrite\n    if (is_none_bias) {\n      if (is_scalar_rhs) {\n        // If the `constant_val` is scalar, we must the shape of filter\n        // to properly broadcast the scalar to `{num_channels}` shape.\n\n        // Get the number of channels if possible.\n        auto filter_type = filter.getType().dyn_cast<RankedTensorType>();\n        // Filter must be a `2D` tensor with `{num_channels, num_features}`\n        // shape. The following check is rejecting unknown rank (-1).\n        if (filter_type == nullptr || filter_type.getRank() != 2) {\n          return failure();\n        }\n        int num_channels = filter_type.getShape()[0];\n\n        // Create a zero tensor with shape {num_channels}, and the type need to\n        // be the same as constant_val.\n        // This is a way to gracefully handle scalar tensor. The Add will always\n        // be constant-folded away regardless if `constant_val` is a scalar or\n        // not.\n        RankedTensorType type = RankedTensorType::get(\n            {num_channels}, constant_val_type.getElementType());\n        auto attr = rewriter.getZeroAttr(type);\n        bias = rewriter.create<ConstantOp>(add_op.getLoc(), type, attr);\n        auto none_af = rewriter.getStringAttr(\"NONE\");\n        bias =\n            rewriter.create<AddOp>(add_op.getLoc(), bias, constant_val, none_af)\n                .output();\n      } else {\n        // If there no pre-existing bias and the `constant_val` is 1D, simply\n        // use `constant_val` as bias.\n        bias = constant_val;\n      }\n    } else {\n      auto none_af = rewriter.getStringAttr(\"NONE\");\n      bias =\n          rewriter.create<AddOp>(add_op.getLoc(), bias, constant_val, none_af)\n              .output();\n    }\n\n    auto fc = rewriter.create<TFL::FullyConnectedOp>(\n        FusedLoc::get(fc_op.getContext(), {fc_op.getLoc(), add_op.getLoc()}),\n        add_op.getType(),\n        /*input=*/fc_op.input(),\n        /*filter=*/filter,\n        /*bias=*/bias,\n        /*fused_activation_function=*/\n        rewriter.getStringAttr(add_op.fused_activation_function()),\n        /*weights_format=*/rewriter.getStringAttr(fc_op.weights_format()),\n        /*keep_num_dims=*/rewriter.getBoolAttr(fc_op.keep_num_dims()));\n    rewriter.replaceOp(add_op, fc.output());\n\n    return success();\n  }\n};\n\n// Replace ..\n// FC(Add(lhs, rhs), filter, bias)\n// .. with ..\n// FC(lhs, filter, FC(rhs, filter, bias))\n// .. if rhs, filter, and bias are all constants.\n// The second FC will be constant folded to a single vector.\n// TODO(b/136285429): Move to tablegen when variadic is supported\nstruct FuseAddAndFullyConnected\n    : public OpRewritePattern<TFL::FullyConnectedOp> {\n  using OpRewritePattern<TFL::FullyConnectedOp>::OpRewritePattern;\n\n  LogicalResult matchAndRewrite(TFL::FullyConnectedOp fc_op,\n                                PatternRewriter &rewriter) const override {\n    // This only works with default format.\n    if (fc_op.weights_format() != \"DEFAULT\") return failure();\n\n    // Match Add.\n    auto add_op = dyn_cast_or_null<TFL::AddOp>(fc_op.input().getDefiningOp());\n    if (!add_op) return failure();\n    if (add_op.fused_activation_function() != \"NONE\") return failure();\n\n    // Don't match adds where the added constant is not 1D.\n    {\n      auto addend_shape = add_op.rhs().getType().cast<ShapedType>();\n      if (!addend_shape.hasStaticShape()) return failure();\n      if (addend_shape.getShape().size() != 1) return failure();\n    }\n\n    // Calculate new bias.  Generate a new FC; it will be constant folded.\n    auto old_bias = fc_op.bias();\n    if (!old_bias || old_bias.getType().isa<NoneType>()) {\n      // TODO(b/180752069): Figure out new bias' type when old bias is empty.\n      return failure();\n    }\n\n    // The FC relies on constant folding, which is implemented on F32. Checks\n    // types to be F32.\n    {\n      if (!IsF32Value(add_op.rhs()) || !IsF32Value(fc_op.filter()) ||\n          !IsF32Value(old_bias))\n        return failure();\n    }\n\n    auto new_bias = rewriter.create<TFL::FullyConnectedOp>(\n        fc_op.getLoc(), old_bias.getType(),\n        /*input=*/add_op.rhs(),\n        /*filter=*/fc_op.filter(),\n        /*bias=*/old_bias,\n        /*fused_activation_function=*/rewriter.getStringAttr(\"NONE\"),\n        /*weights_format=*/rewriter.getStringAttr(\"DEFAULT\"),\n        /*keep_num_dims=*/rewriter.getBoolAttr(true));\n\n    // Create the updated FC.\n    auto new_fc = rewriter.create<TFL::FullyConnectedOp>(\n        FusedLoc::get(add_op.getContext(), {add_op.getLoc(), fc_op.getLoc()}),\n        fc_op.output().getTypes(),\n        /*input=*/add_op.lhs(),\n        /*filter=*/fc_op.filter(),\n        /*bias=*/*new_bias.output().begin(),\n        /*fused_activation_function=*/\n        rewriter.getStringAttr(fc_op.fused_activation_function()),\n        /*weights_format=*/rewriter.getStringAttr(\"DEFAULT\"),\n        /*keep_num_dims=*/rewriter.getBoolAttr(fc_op.keep_num_dims()));\n    rewriter.replaceOp(fc_op.getOperation(), new_fc.output());\n\n    return success();\n  }\n};\n\n// Replace ..\n// FC(Mul(lhs, rhs), filter, bias)\n// .. with ..\n// FC(lhs, Mul(filter, rhs), bias)\n// .. if rhs, filter, and bias are all constants.\n// The generated Mul will be constant folded to a single matrix.\nstruct FuseMulAndFullyConnected\n    : public OpRewritePattern<TFL::FullyConnectedOp> {\n  using OpRewritePattern<TFL::FullyConnectedOp>::OpRewritePattern;\n\n  LogicalResult matchAndRewrite(TFL::FullyConnectedOp fc_op,\n                                PatternRewriter &rewriter) const override {\n    // This only works with default format.\n    if (fc_op.weights_format() != \"DEFAULT\") return failure();\n\n    // Match Mul.\n    auto mul_op = dyn_cast_or_null<TFL::MulOp>(fc_op.input().getDefiningOp());\n    if (!mul_op) return failure();\n    if (mul_op.fused_activation_function() != \"NONE\") return failure();\n\n    // Don't match muls where the multiplier constant is not 1D.\n    {\n      auto multiplier_shape = mul_op.rhs().getType().cast<ShapedType>();\n      if (!multiplier_shape.hasStaticShape()) return failure();\n      if (multiplier_shape.getShape().size() != 1) return failure();\n    }\n\n    // We rely on constant folding, implemented only for F32. Check types.\n    if (!IsF32Value(mul_op.rhs()) || !IsF32Value(fc_op.filter())) {\n      return failure();\n    }\n\n    auto location =\n        FusedLoc::get(mul_op.getContext(), {mul_op.getLoc(), fc_op.getLoc()});\n\n    auto new_filter = rewriter.create<TFL::MulOp>(\n        location,\n        /*lhs=*/fc_op.filter(),\n        /*rhs=*/mul_op.rhs(),\n        /*fused_activation_function=*/rewriter.getStringAttr(\"NONE\"));\n    // Create the updated FC.\n    auto new_fc = rewriter.create<TFL::FullyConnectedOp>(\n        location, fc_op.output().getTypes(),\n        /*input=*/mul_op.lhs(),\n        /*filter=*/new_filter,\n        /*bias=*/fc_op.bias(),\n        /*fused_activation_function=*/\n        rewriter.getStringAttr(fc_op.fused_activation_function()),\n        /*weights_format=*/rewriter.getStringAttr(\"DEFAULT\"),\n        /*keep_num_dims=*/rewriter.getBoolAttr(fc_op.keep_num_dims()));\n    rewriter.replaceOp(fc_op.getOperation(), new_fc.output());\n\n    return success();\n  }\n};\n\n// TODO(b/136285429): Move to tablegen when variadic is supported.\ntemplate <typename ReluXOp, char const *Act>\nstruct FuseFullyConnectedAndReluX : public OpRewritePattern<ReluXOp> {\n  using OpRewritePattern<ReluXOp>::OpRewritePattern;\n\n  LogicalResult matchAndRewrite(ReluXOp relu_op,\n                                PatternRewriter &rewriter) const override {\n    Operation *input = relu_op.getOperand().getDefiningOp();\n    if (!isa_and_nonnull<FullyConnectedOp>(input)) return failure();\n    auto fully_connected_op = cast<FullyConnectedOp>(input);\n    if (fully_connected_op.fused_activation_function() != \"NONE\")\n      return failure();\n\n    auto new_activation_func = rewriter.getStringAttr(Act);\n    auto new_weights_format =\n        rewriter.getStringAttr(fully_connected_op.weights_format());\n    auto new_keep_num_dims =\n        rewriter.getBoolAttr(fully_connected_op.keep_num_dims());\n    auto fc = rewriter.create<FullyConnectedOp>(\n        FusedLoc::get(relu_op.getContext(),\n                      {fully_connected_op.getLoc(), relu_op.getLoc()}),\n        relu_op.getType(), fully_connected_op.input(),\n        fully_connected_op.filter(), fully_connected_op.bias(),\n        new_activation_func, new_weights_format, new_keep_num_dims);\n    rewriter.replaceOp(relu_op, fc.output());\n\n    return success();\n  }\n};\n\n// Fuse Mul with proceeding FullyConnected.\n// TODO(b/136285429): Move to tablegen when variadic is supported\nstruct FuseFullyConnectedAndMul : public OpRewritePattern<TFL::MulOp> {\n  using OpRewritePattern<TFL::MulOp>::OpRewritePattern;\n\n  LogicalResult matchAndRewrite(TFL::MulOp mul_op,\n                                PatternRewriter &rewriter) const override {\n    // If we are broadcasting on the lhs then don't fold the multiply as it\n    // would increase the amount of compute done by the fully connected op.\n    if (mul_op.lhs().getType() != mul_op.getType()) return failure();\n\n    // Mul.\n    DenseElementsAttr cst;\n    Value constant_val = mul_op.rhs();\n    if (!matchPattern(constant_val, m_Constant(&cst))) return failure();\n\n    // Fully Connected.\n    auto fc_op =\n        dyn_cast_or_null<TFL::FullyConnectedOp>(mul_op.lhs().getDefiningOp());\n    if (!fc_op) return failure();\n    Value filter = fc_op.filter();\n    Value bias = fc_op.bias();\n    ElementsAttr cst_tmp;\n    if (!matchPattern(filter, m_Constant(&cst_tmp))) return failure();\n    if (!bias.getType().isa<NoneType>() &&\n        !matchPattern(bias, m_Constant(&cst_tmp)))\n      return failure();\n    if (fc_op.fused_activation_function() != \"NONE\") return failure();\n\n    // Only fuse multiplier if all dimensions other than the depth dimension\n    // are equal to 1 since otherwise\n    // `matmul(x, filter) * cst != matmul(x, filter * cst)`\n    // even if `filter` and `cst` are be broadcastable.\n    auto shape = cst.getType().getShape();\n    if (!IsDimensionsDegenerateExceptLastOne(shape)) return failure();\n\n    int64_t element_size = shape.empty() ? 1 : shape[shape.size() - 1];\n    // Expand and transpose the multiplier since weights are using the\n    // OHWI data format in TFLite.\n    int64_t normalized_shape[2] = {element_size, 1};\n    auto new_cst = cst.reshape(RankedTensorType::get(\n        normalized_shape, cst.getType().getElementType()));\n    Type new_type = new_cst.getType();\n    if (!IsBroadcastableElementsAttrAndType(new_type, filter.getType())) {\n      return failure();\n    }\n\n    auto new_op =\n        rewriter.create<ConstantOp>(mul_op.getLoc(), new_type, new_cst);\n    Value new_const_val = new_op.getResult();\n\n    // Rewrite. Since the folder of TFL::MulOp couldn't broadcast the operands,\n    // TF::MulOp is used to fold the constant.\n    // TODO(b/139192933): switch to the TFL constant folding\n    auto new_filter =\n        rewriter.create<TF::MulOp>(mul_op.getLoc(), filter, new_const_val).z();\n    // If bias isn't None, it needs to be multiplied as well.\n    if (!bias.getType().isa<NoneType>()) {\n      bias =\n          rewriter.create<TF::MulOp>(mul_op.getLoc(), bias, constant_val).z();\n    }\n\n    auto fc = rewriter.create<TFL::FullyConnectedOp>(\n        FusedLoc::get(fc_op.getContext(), {fc_op.getLoc(), mul_op.getLoc()}),\n        mul_op.getType(),\n        /*input=*/fc_op.input(),\n        /*filter=*/new_filter,\n        /*bias=*/bias,\n        /*fused_activation_function=*/\n        rewriter.getStringAttr(mul_op.fused_activation_function()),\n        /*weights_format=*/rewriter.getStringAttr(fc_op.weights_format()),\n        /*keep_num_dims=*/rewriter.getBoolAttr(fc_op.keep_num_dims()));\n    rewriter.replaceOp(mul_op, fc.output());\n\n    return success();\n  }\n};\n\n// Fuse Mul with proceeding Affine ops. This is an C++ implementation of the\n// following table gen implementation, which doesn't derived the result type of\n// the TFL_DequantizeOp.\n// def : Pat<(TFL_MulOp (TFL_Conv2DOp:$conv_output $input,\n//                          (TFL_DequantizeOp (TFL_QuantizeOp\n//                              (ConstantOp F32ElementsAttr:$filter), $qtype)),\n//                          (ConstantOp F32ElementsAttr:$bias),\n//                          $h_factor, $w_factor, TFL_AF_None,\n//                          $padding, $stride_h, $stride_w),\n//                      (ConstantOp F32ElementsAttr:$value), $act_fn),\n//           (TFL_Conv2DOp $input,\n//                      (TFL_DequantizeOp (TFL_QuantizeOp\n//                          (TFL_MulOp (ConstantOp $filter),\n//                                     (ConstantOp (ExpandTo4DForConv $value)),\n//                                      TFL_AF_None),\n//                          (RescaleQtype $qtype, $value))),\n//                      (TFL_MulOp (ConstantOp $bias), (ConstantOp $value),\n//                          TFL_AF_None),\n//                      $h_factor, $w_factor, $act_fn,\n//                      $padding, $stride_h, $stride_w),\n//         [(CanFuseConvOrDepthwiseConv<\"false\"> $filter, $value),\n//          (HasOneUse $conv_output),\n//          (IsPerAxisQuantization $qtype), // per-axis quantization\n//         ]>;\ntemplate <typename AffineOpType>\nstruct FuseAffinOpAndMulWithQDQs : public OpRewritePattern<TFL::MulOp> {\n  using OpRewritePattern<TFL::MulOp>::OpRewritePattern;\n\n  LogicalResult matchAndRewrite(TFL::MulOp mul_op,\n                                PatternRewriter &rewriter) const override {\n    // Mul. Required 1-D rhs for batch normalization.\n    DenseElementsAttr gamma_cst;\n    Value gamma = mul_op.rhs();\n    if (!matchPattern(gamma, m_Constant(&gamma_cst))) return failure();\n    if (gamma_cst.getType().getRank() != 1) return failure();\n\n    // Affine op\n    Operation *mul_op_lhs = mul_op.lhs().getDefiningOp();\n    auto fc_op = dyn_cast_or_null<AffineOpType>(mul_op_lhs);\n    if (!fc_op) return failure();\n    Value filter = fc_op.filter();\n    Value bias = fc_op.bias();\n\n    // QDQs\n    auto dq_op = dyn_cast_or_null<TFL::DequantizeOp>(filter.getDefiningOp());\n    if (!dq_op) return failure();\n    auto q_op =\n        dyn_cast_or_null<TFL::QuantizeOp>(dq_op.input().getDefiningOp());\n    if (!q_op) return failure();\n    filter = q_op.input();\n\n    // weight constant\n    ElementsAttr cst_tmp;\n    if (!matchPattern(filter, m_Constant(&cst_tmp))) return failure();\n    if (!bias.getType().isa<NoneType>() &&\n        !matchPattern(bias, m_Constant(&cst_tmp)))\n      return failure();\n    if (fc_op.fused_activation_function() != \"NONE\") return failure();\n\n    // Broadcast the constant operand of Mul if it isn't compatible to the\n    // filter input. We only support broadcasting the operand along the depth\n    // dimension, when the operand's depth is 1.\n    rewriter.setInsertionPoint(q_op);\n    Location loc = fc_op.getLoc();\n    Value broadcasted_gamma;\n    if (isa<TFL::Conv2DOp>(mul_op_lhs)) {\n      auto mul_rhs = ExpandTo4DForConv(gamma_cst);\n      broadcasted_gamma = rewriter.create<ConstOp>(loc, mul_rhs);\n    } else if (isa<TFL::DepthwiseConv2DOp>(mul_op_lhs)) {\n      auto mul_rhs = ExpandTo4DForDepthwiseConv(gamma_cst);\n      broadcasted_gamma = rewriter.create<ConstOp>(loc, mul_rhs);\n    } else {\n      return failure();\n    }\n\n    // Rewrite filter constant. Since the folder of TFL::MulOp couldn't\n    // broadcast the operands, TF::MulOp is used to fold the constant.\n    auto new_filter =\n        rewriter.create<TF::MulOp>(loc, filter, broadcasted_gamma).z();\n    // Update the scale in the quantize op.\n    auto new_qtype = RescaleQtype(q_op.qtype(), gamma_cst);\n    if (!new_qtype) return failure();\n    rewriter.replaceOpWithNewOp<TFL::QuantizeOp>(q_op, new_qtype.getValue(),\n                                                 new_filter, new_qtype);\n\n    // If bias isn't None, it needs to be multiplied as well.\n    if (!bias.getType().isa<NoneType>()) {\n      rewriter.setInsertionPoint(fc_op);\n      auto new_bias = rewriter.create<TF::MulOp>(loc, bias, gamma);\n      fc_op.getOperation()->replaceUsesOfWith(bias, new_bias);\n    }\n\n    // Remove the tailing mul op.\n    mul_op.replaceAllUsesWith(fc_op.getResult());\n    return success();\n  }\n};\n\nusing FuseConv2DAndMulWithQDQs = FuseAffinOpAndMulWithQDQs<TFL::Conv2DOp>;\nusing FuseDepthwiseConv2DAndMulWithQDQs =\n    FuseAffinOpAndMulWithQDQs<TFL::DepthwiseConv2DOp>;\n\n// Fuse Binary Op with following Affine operation.\ntemplate <typename AffineOpType>\nstruct FuseBinaryOpToFollowingAffineOp : public OpRewritePattern<AffineOpType> {\n  using OpRewritePattern<AffineOpType>::OpRewritePattern;\n\n  LogicalResult matchAndRewrite(AffineOpType fc_op,\n                                PatternRewriter &rewriter) const override {\n    // Binary op.\n    Operation *binary_op = fc_op.input().getDefiningOp();\n    if (!binary_op || binary_op->getNumOperands() != 2) return failure();\n    // We only handle the cases the RHS is a scalar.\n    // TODO(fengliuai): Currently the canonicalizer pass couldn't guarantee that\n    // the constant operands are on the RHS, we need to consider LHS constant\n    // operand if necessary.\n    DenseFPElementsAttr cst;\n    if (!matchPattern(binary_op->getOperand(1), m_Constant(&cst)))\n      return failure();\n    if (cst.getNumElements() != 1) return failure();\n    APFloat cst_value = *cst.float_value_begin();\n\n    // Affine op.\n    Value filter = fc_op.filter();\n    Value bias = fc_op.bias();\n    DenseFPElementsAttr filter_cst, bias_cst;\n    if (!matchPattern(filter, m_Constant(&filter_cst))) {\n      // The filter maybe quantized, then we should set it to the real constant.\n      auto dq = llvm::dyn_cast_or_null<DequantizeOp>(filter.getDefiningOp());\n      if (!dq) return failure();\n      auto q = llvm::dyn_cast_or_null<QuantizeOp>(dq.input().getDefiningOp());\n      if (!q || !matchPattern(q.input(), m_Constant(&filter_cst))) {\n        return failure();\n      }\n      filter = q.input();\n    }\n    if (!bias.getType().isa<NoneType>() &&\n        !matchPattern(bias, m_Constant(&bias_cst)))\n      return failure();\n    auto binary_op_activation_func =\n        binary_op->template getAttrOfType<StringAttr>(\n            \"fused_activation_function\");\n    if (!binary_op_activation_func ||\n        binary_op_activation_func.getValue() != \"NONE\")\n      return failure();\n    ShapedType filter_type = filter_cst.getType();\n\n    if (llvm::isa<AddOp, SubOp>(binary_op)) {\n      auto padding = fc_op->template getAttrOfType<StringAttr>(\"padding\");\n      if (padding && padding.getValue() != \"VALID\") return failure();\n\n      // The fusion of add/sub is actually applying the following\n      // transformation:\n      // w * (x + c) + b => w * x + (w * c + b)\n      // so we have to update the bias.\n      if (llvm::isa<SubOp>(binary_op)) cst_value.changeSign();\n\n      auto bias_and_slice =\n          GetBiasDimAndSliceSize(filter_type.getShape(), fc_op);\n      int64_t bias_size = bias_and_slice.first;\n      int64_t slice_size = bias_and_slice.second;\n      ShapedType new_bias_type =\n          RankedTensorType::get({bias_size}, filter_type.getElementType());\n\n      // The new bias should be a 1-D tensor with length equals to the bias\n      // dimension of the weight.\n      SmallVector<APFloat, 4> new_bias_values;\n      if (bias.getType().isa<NoneType>()) {  // none bias, a list of zeros\n        new_bias_values.resize(bias_size,\n                               APFloat::getZero(cst_value.getSemantics()));\n      } else if (bias_cst.getNumElements() == 1) {  // scalar bias, broadcast it\n        new_bias_values.resize(bias_size, *bias_cst.float_value_begin());\n      } else if (bias_cst.getNumElements() == bias_size) {  // 1-d bias, copy it\n        new_bias_values.insert(new_bias_values.begin(),\n                               bias_cst.float_value_begin(),\n                               bias_cst.float_value_end());\n      } else {\n        return failure();\n      }\n\n      int64_t flatten_index = 0;\n      for (auto fp_it = filter_cst.float_value_begin(),\n                fp_end = filter_cst.float_value_end();\n           fp_it != fp_end; ++fp_it) {\n        int bias_index = (flatten_index++ / slice_size) % bias_size;\n\n        new_bias_values[bias_index] =\n            new_bias_values[bias_index] + *fp_it * cst_value;\n      }\n      auto new_bias = DenseFPElementsAttr::get(new_bias_type, new_bias_values);\n      auto new_bias_op =\n          rewriter.create<ConstOp>(fc_op.getLoc(), new_bias_type, new_bias);\n      fc_op.setOperand(0, binary_op->getOperand(0));\n      fc_op.setOperand(2, new_bias_op);\n    } else if (llvm::isa<MulOp, DivOp>(binary_op)) {\n      // The fusion of mul/div is actually applying the following\n      // transformation:\n      // w * (x ' c) + b => (w ' c) x + b\n      // so we have to update the weight.\n      bool is_mul = llvm::isa<MulOp>(binary_op);\n      auto new_filter =\n          filter_cst.mapValues(filter_type.getElementType(), [&](APFloat it) {\n            return (is_mul ? it * cst_value : it / cst_value).bitcastToAPInt();\n          });\n      // We recreate the constant op in case it is shared by the other ops. This\n      // might increase the model size.\n      auto new_filter_op = rewriter.create<ConstOp>(\n          fc_op.getLoc(), filter.getType(), new_filter);\n      fc_op.setOperand(0, binary_op->getOperand(0));\n      if (fc_op.filter() != filter) {\n        // This filter goes through quantize and dequantize ops. Then we just\n        // need to update the weight to the quantize op.\n        filter.replaceAllUsesWith(new_filter_op);\n      } else {\n        // This filter doesn't go through quantize and dequantize ops, Then\n        // we update the weight of the affine op directly.\n        fc_op.setOperand(1, new_filter_op);\n      }\n    } else {\n      return failure();\n    }\n    return success();\n  }\n\n private:\n  // Returns the dimension length of the channel dimension and also the slide\n  // size by each position in the channel dimension accordingly. tfl.conv2d and\n  // tfl.fully_connected has heading channel dimension, but tfl.depthwise_conv2d\n  // has tailing channel dimension. This function is to provide a utility to\n  // create the above information from the op property.\n  static std::pair<int64_t, int64_t> GetBiasDimAndSliceSize(\n      ArrayRef<int64_t> filter_shape, AffineOpType op) {\n    // Channel dimension index is specified as op property\n    auto channel_index_iter = filter_shape.begin();\n    std::advance(channel_index_iter, op.GetChannelDimIndex());\n    // The slide size is the size of the data in higher dimensions.\n    int64_t slice_size =\n        std::accumulate(std::next(channel_index_iter), filter_shape.end(), 1,\n                        std::multiplies<int64_t>());\n    return {*channel_index_iter, slice_size};\n  }\n};\n\n// If the operand to a broadcastable op is a splat constant, try to replace it\n// with a 0-d constant, e.g. before this optimization,\n//   %cst = constant dense<1.0> : tensor<16x16x4xf32>\n//   %0 = \"tfl.conv_2d\"...\n//   %1 = \"tfl.add\"(%0, %cst) : (tensor<16x16x4xf32>, tensor<16x16x4xf32>)\n// After this optimization:\n//   %cst = constant dense<1.0> : tensor<f32>\n//   %0 = \"tfl.conv_2d\"...\n//   %1 = \"tfl.add\"(%0, %cst) : (tensor<16x16x4xf32>, tensor<f32>)\n// This pattern can enable more fusing opportunities when the binary op is\n// following conv ops.\ntemplate <typename BinaryOpType>\nstruct ScalarizeSplatConstantForBroadcastableOps\n    : public OpRewritePattern<BinaryOpType> {\n  using OpRewritePattern<BinaryOpType>::OpRewritePattern;\n\n  LogicalResult matchAndRewrite(BinaryOpType binary_op,\n                                PatternRewriter &rewriter) const override {\n    DenseElementsAttr splat_elements_attr;\n    if (!IsScalarizableSplatConstant(binary_op.rhs(), &splat_elements_attr)) {\n      return failure();\n    }\n\n    constexpr int kSplatOperandIndex = 1;\n    auto result_type =\n        binary_op.getResult().getType().template cast<ShapedType>();\n    mlir::Value non_splat_operand =\n        binary_op.getOperand(1 - kSplatOperandIndex);\n    auto non_splat_operand_type =\n        non_splat_operand.getType().cast<ShapedType>();\n    // If the other operand's shape does not equal to the result shape, then we\n    // cannot scalarize the splat constant because the result shape relies on\n    // the splat constant op's shape for broadcasting.\n    if (!non_splat_operand_type.hasStaticShape() ||\n        non_splat_operand_type.getShape() != result_type.getShape() ||\n        non_splat_operand_type.getRank() > 4) {\n      return failure();\n    }\n\n    // If non-splat operand is not fusable affine ops, then no need to apply\n    // this transformation.\n    if (!CanFuseAffineOp(non_splat_operand.getDefiningOp(), binary_op)) {\n      return failure();\n    }\n\n    // Creates a new scalar constant op using the splat value.\n    mlir::Value splat_operand = binary_op.getOperand(kSplatOperandIndex);\n    auto scalar_elements_attr = DenseElementsAttr::get(\n        RankedTensorType::get({},\n                              splat_elements_attr.getType().getElementType()),\n        splat_elements_attr.getSplatValue());\n\n    auto scalar_constant_op = rewriter.create<ConstantOp>(\n        splat_operand.getLoc(), scalar_elements_attr.getType(),\n        scalar_elements_attr);\n\n    binary_op.setOperand(kSplatOperandIndex, scalar_constant_op);\n    return success();\n  }\n\n private:\n  // Returns true if this value is a splat constant op which can be scalarized.\n  // Also returns the elements attr if this value is indeed a splat constant.\n  bool IsScalarizableSplatConstant(mlir::Value value,\n                                   DenseElementsAttr *elements_attr) const {\n    if (!matchPattern(value, m_Constant(elements_attr))) {\n      return false;\n    }\n    auto element_type = value.getType().cast<ShapedType>().getElementType();\n    // Ignore per-axis quantized constants because after converting to scalar,\n    // we will lose per-axis qantization parameter.\n    if (element_type.isa<quant::UniformQuantizedPerAxisType>()) {\n      return false;\n    }\n    if (IsScalar(value)) {\n      return false;\n    }\n    return elements_attr->isSplat();\n  }\n\n  // If this type is a scalar shaped type.\n  bool IsScalar(mlir::Value value) const {\n    auto type = value.getType().dyn_cast<ShapedType>();\n    if (!type) {\n      return false;\n    }\n    if (!type.hasStaticShape()) {\n      return false;\n    }\n    return type.getNumElements() == 1;\n  }\n\n  // Returns true if we can fuse an affine op with consuming binary op.\n  bool CanFuseAffineOp(Operation *affine_op, Operation *binary_op) const {\n    if (!isa_and_nonnull<TFL::Conv2DOp, TFL::DepthwiseConv2DOp,\n                         TFL::FullyConnectedOp>(affine_op)) {\n      return false;\n    }\n    DenseElementsAttr value;\n    // Check that bias are constants if not none.\n    Value bias = affine_op->getOperand(2);\n    if (!bias.getType().isa<NoneType>() &&\n        !matchPattern(bias, m_Constant(&value))) {\n      return false;\n    }\n    // If the binary op is mul/div, also check that filter is constant.\n    if (isa<TFL::MulOp, TFL::DivOp>(binary_op) &&\n        !matchPattern(affine_op->getOperand(1), m_Constant(&value))) {\n      return false;\n    }\n\n    // We can only fuse F32/BF16.\n    auto is_fusable_type = [](Type t) {\n      Type element_type = t;\n      if (auto shaped_type = t.dyn_cast<ShapedType>()) {\n        element_type = shaped_type.getElementType();\n      }\n      return element_type.isBF16() || element_type.isF32();\n    };\n    for (Type t : binary_op->getOperandTypes()) {\n      if (!is_fusable_type(t)) {\n        return false;\n      }\n    }\n\n    return true;\n  }\n};\n\nusing ScalarizeSplatConstantForSub =\n    ScalarizeSplatConstantForBroadcastableOps<TFL::SubOp>;\nusing ScalarizeSplatConstantForAdd =\n    ScalarizeSplatConstantForBroadcastableOps<TFL::AddOp>;\nusing ScalarizeSplatConstantForMul =\n    ScalarizeSplatConstantForBroadcastableOps<TFL::MulOp>;\nusing ScalarizeSplatConstantForDiv =\n    ScalarizeSplatConstantForBroadcastableOps<TFL::DivOp>;\n\nstruct ConvertTrivialTransposeOpToReshapeOp\n    : public OpRewritePattern<TFL::TransposeOp> {\n  using OpRewritePattern<TFL::TransposeOp>::OpRewritePattern;\n\n  LogicalResult matchAndRewrite(TFL::TransposeOp transpose_op,\n                                PatternRewriter &rewriter) const override {\n    auto input_type = transpose_op.input().getType().cast<ShapedType>();\n    auto output_type = transpose_op.output().getType().cast<ShapedType>();\n    // It's possible to know if the transformation is safe only if the input\n    // & output shapes are fully known and permutation is a constant.\n    if (!input_type.hasStaticShape() || !output_type.hasStaticShape())\n      return failure();\n    Value perm = transpose_op.perm();\n    DenseElementsAttr perm_values_attr;\n    if (!matchPattern(perm, m_Constant(&perm_values_attr))) return failure();\n\n    auto input_shape = input_type.getShape();\n    SmallVector<int64_t, 8> perm_values;\n    for (const auto &dim : perm_values_attr.getIntValues())\n      perm_values.push_back(dim.getSExtValue());\n\n    // This should never happen unless the input graph is malformed.\n    if (input_shape.size() != perm_values.size()) {\n      transpose_op.emitError(\n          \"TransposeOP has inconsistent input and perm values.\");\n    }\n\n    SmallVector<int, 8> old_major_index_ordering;\n    SmallVector<int, 8> new_major_index_ordering;\n    for (int i = 0, end = input_shape.size(); i < end; i++) {\n      if (input_shape[i] != 1) {\n        old_major_index_ordering.push_back(i);\n      }\n\n      if (input_shape[perm_values[i]] != 1) {\n        new_major_index_ordering.push_back(perm_values[i]);\n      }\n    }\n    if (old_major_index_ordering != new_major_index_ordering) {\n      return failure();\n    }\n\n    // Rewrite.\n    Location loc = transpose_op.getLoc();\n\n    SmallVector<int32_t, 8> output_shape_values;\n    for (auto dim : output_type.getShape()) {\n      output_shape_values.push_back(dim);\n    }\n    auto type = mlir::RankedTensorType::get(output_shape_values.size(),\n                                            rewriter.getIntegerType(32));\n    auto new_shape_attr =\n        mlir::DenseIntElementsAttr::get(type, output_shape_values);\n    auto new_shape = rewriter.create<TF::ConstOp>(loc, new_shape_attr);\n\n    rewriter.replaceOpWithNewOp<TFL::ReshapeOp>(\n        transpose_op, transpose_op.output().getType(), transpose_op.input(),\n        new_shape);\n\n    return success();\n  }\n};\n\n// Remove Reshape before FullyConnected when `keep_num_dims=false` and Reshape\n// does not alter the last dimension as FullyConnected will collapse all other\n// dimensions into a single dimension. For example,\n//\n//   %shape = constant dense<[1, 128, 64]> : tensor<3xi32>\n//   %reshape = tfl.reshape(%input, %shape) // %input: tensor<128x64xf32>\n//   %fc = tfl.fully_connected(%reshape, %filter, %bias)\n//           {keep_num_dims = false, weights_format = \"DEFAULT\"}\n//\n// can be canonicalized to\n//\n//   %fc = tfl.fully_connected(%input, %filter, %bias)\n//           {keep_num_dims = false, weights_format = \"DEFAULT\"}\nstruct RemoveReshapeBeforeFullyConnected\n    : public OpRewritePattern<TFL::FullyConnectedOp> {\n  using OpRewritePattern<TFL::FullyConnectedOp>::OpRewritePattern;\n\n  LogicalResult matchAndRewrite(TFL::FullyConnectedOp fully_connected_op,\n                                PatternRewriter &) const override {\n    auto input = fully_connected_op.input();\n    auto input_ty = input.getType().dyn_cast<ShapedType>();\n    auto output_ty = fully_connected_op.output()[0]\n                         .getType()\n                         .template dyn_cast<ShapedType>();\n    if (!input_ty.hasStaticShape() ||\n        fully_connected_op.weights_format() != \"DEFAULT\" ||\n        fully_connected_op.keep_num_dims() || !output_ty.hasStaticShape() ||\n        output_ty.getRank() != 2) {\n      return failure();\n    }\n\n    auto reshape_op = input.getDefiningOp<TFL::ReshapeOp>();\n    if (!reshape_op) return failure();\n\n    // Check if the last dimension does not change after reshape.\n    auto reshape_input = reshape_op.input();\n    auto reshape_input_ty = reshape_input.getType().dyn_cast<ShapedType>();\n    if (!reshape_input_ty.hasStaticShape() || input_ty.getRank() == 0 ||\n        reshape_input_ty.getRank() == 0 ||\n        input_ty.getDimSize(input_ty.getRank() - 1) !=\n            reshape_input_ty.getDimSize(reshape_input_ty.getRank() - 1)) {\n      return failure();\n    }\n\n    // Connect the input to the one of reshape.\n    fully_connected_op.setOperand(0, reshape_input);\n    return success();\n  }\n};\n\n// Remove Reshape after FullyConnected when `keep_num_dims=false`, the Reshaoe\n// does not alter the last dimension and it restores the batch dimensions\n// collapsed by the FullyConnected op due to `keep_num_dims=false`. For example,\n//\n//   // %input: tensor<4x16x32xf32>\n//   %fc = tfl.fully_connected(%input, %filter, %bias)\n//           {keep_num_dims = false, weights_format = \"DEFAULT\"}\n//   %shape = constant dense<[4, 16, 32]> : tensor<3xi32>\n//   %rs = tfl.reshape(%fc, %shape)\n//\n// can be canonicalized to\n//\n//   %fc = tfl.fully_connected(%input, %filter, %bias)\n//           {keep_num_dims = true, weights_format = \"DEFAULT\"}\nstruct RemoveReshapeAfterFullyConnected\n    : public OpRewritePattern<TFL::ReshapeOp> {\n  using OpRewritePattern::OpRewritePattern;\n\n  LogicalResult matchAndRewrite(TFL::ReshapeOp reshape_op,\n                                PatternRewriter &rewriter) const override {\n    auto fully_connected_op = llvm::dyn_cast_or_null<TFL::FullyConnectedOp>(\n        reshape_op.input().getDefiningOp());\n    if (!fully_connected_op || fully_connected_op.getNumResults() != 1 ||\n        fully_connected_op.weights_format() != \"DEFAULT\" ||\n        fully_connected_op.keep_num_dims())\n      return failure();\n    if (!reshape_op.input().hasOneUse()) return failure();\n\n    auto input_shape = fully_connected_op.input().getType().cast<ShapedType>();\n    auto output_shape = fully_connected_op.getType(0).cast<ShapedType>();\n    auto reshape_shape = reshape_op.getType().cast<ShapedType>();\n    if (!input_shape.hasStaticShape() || !output_shape.hasStaticShape() ||\n        !reshape_shape.hasStaticShape())\n      return failure();\n\n    // Check that the reshape doesn't modify the last dimension and it restores\n    // the input (batch) dimension with the exception of the feature (last)\n    // dimension.\n    if (output_shape.getShape().empty() || reshape_shape.getShape().empty() ||\n        output_shape.getShape().back() != reshape_shape.getShape().back() ||\n        input_shape.getShape().drop_back() !=\n            reshape_shape.getShape().drop_back())\n      return failure();\n\n    llvm::SmallVector<Type, 1> output_type{reshape_op.getType()};\n    rewriter.replaceOpWithNewOp<TFL::FullyConnectedOp>(\n        reshape_op, output_type, fully_connected_op.input(),\n        fully_connected_op.filter(), fully_connected_op.bias(),\n        fully_connected_op.fused_activation_function(),\n        fully_connected_op.weights_format(), /*keep_num_dims=*/true);\n    return success();\n  }\n};\n\n// Fuses Unpack with proceeding Concatenation to Reshape if output type has\n// static shape and activation function is none. For example:\n//\n//   // %input: tensor<1x3x2xf32>\n//   %unpack:3 = \"tfl.unpack\"(%input) {axis = 1 : i32, num = 3 : i32}\n//   %res = \"tfl.concatenation\"(%unpack#0, %unpack#1, %unpack#2)\n//        {axis = -1 : i32, fused_activation_function = \"NONE\"}\n//\n// can be optimized to\n//\n//   %cst = constant dense<[1, 6]> : tensor<2xi32>\n//   %res = \"tfl.reshape\"(%input, %cst)\nstruct FuseUnpackAndConcatToReshape\n    : public OpRewritePattern<TFL::ConcatenationOp> {\n  using OpRewritePattern::OpRewritePattern;\n\n  LogicalResult matchAndRewrite(TFL::ConcatenationOp concat_op,\n                                PatternRewriter &rewriter) const override {\n    if (concat_op.fused_activation_function() != \"NONE\") {\n      return failure();\n    }\n\n    // Checks all operands come from the same unpack op.\n    auto first_operand = concat_op.values().front();\n    auto unpack_op =\n        dyn_cast_or_null<TFL::UnpackOp>(first_operand.getDefiningOp());\n    if (!unpack_op || unpack_op.getNumResults() != concat_op.getNumOperands()) {\n      return failure();\n    }\n    for (auto &index_and_value : llvm::enumerate(concat_op.values())) {\n      if (index_and_value.value() !=\n          unpack_op.getResult(index_and_value.index())) {\n        return failure();\n      }\n    }\n\n    auto output_type = concat_op.getType().cast<ShapedType>();\n    if (!output_type.hasStaticShape()) {\n      return failure();\n    }\n\n    auto new_shape_array = output_type.getShape();\n    // This is to workaround the unnecessary cast i64 -> i32.\n    SmallVector<int32_t, 4> new_shape_array_i32;\n    for (auto size : new_shape_array) {\n      new_shape_array_i32.push_back(static_cast<int32_t>(size));\n    }\n    auto new_shape = rewriter.create<TFL::ConstOp>(\n        concat_op.getLoc(),\n        DenseIntElementsAttr::get(\n            RankedTensorType::get(new_shape_array_i32.size(),\n                                  rewriter.getIntegerType(32)),\n            new_shape_array_i32));\n\n    rewriter.replaceOpWithNewOp<TFL::ReshapeOp>(concat_op, output_type,\n                                                unpack_op.input(), new_shape);\n    return success();\n  }\n};\n\nusing FuseBinaryOpToFollowingFullyConnected =\n    FuseBinaryOpToFollowingAffineOp<FullyConnectedOp>;\nusing FuseBinaryOpToFollowingDepthwiseConv2D =\n    FuseBinaryOpToFollowingAffineOp<DepthwiseConv2DOp>;\nusing FuseBinaryOpToFollowingConv2D = FuseBinaryOpToFollowingAffineOp<Conv2DOp>;\n\n// Adds canonicalization patterns to the list of patterns.\nvoid AddCanonicalizationPatterns(MLIRContext *context,\n                                 OwningRewritePatternList *patterns) {\n  for (auto *op : context->getRegisteredOperations())\n    op->getCanonicalizationPatterns(*patterns, context);\n}\n\nvoid OptimizePass::runOnFunction() {\n  OwningRewritePatternList patterns(&getContext());\n  auto *ctx = &getContext();\n  auto func = getFunction();\n\n  // Merge reshapes into fully connected ops before we start moving them past\n  // binary ops.\n  OwningRewritePatternList phase_0_patterns(&getContext());\n  phase_0_patterns.insert<RemoveReshapeAfterFullyConnected,\n                          RemoveReshapeBeforeFullyConnected>(ctx);\n  (void)applyPatternsAndFoldGreedily(func, std::move(phase_0_patterns));\n\n  // Potentially the binary ops might be fused together, like hard_swish, thus\n  // we explore these potentially first and then fuse the binary ops with the\n  // following ops in a second pattern match.\n  TFL::populateWithGenerated(patterns);\n  patterns.insert<FuseFullyConnectedAndAdd, FuseAddAndFullyConnected,\n                  FuseFullyConnectedAndMul, FuseMulAndFullyConnected,\n                  FuseFullyConnectedAndReluX<TFL::ReluOp, kRelu>,\n                  FuseFullyConnectedAndReluX<TFL::Relu6Op, kRelu6>,\n                  FuseFullyConnectedAndReluX<TFL::Relu1Op, kRelu1>>(ctx);\n  if (enable_canonicalization_) AddCanonicalizationPatterns(ctx, &patterns);\n  (void)applyPatternsAndFoldGreedily(func, std::move(patterns));\n\n  // Fuse the binary ops with the following ops.\n  OwningRewritePatternList phase_2_patterns(&getContext());\n  TFL::populateWithGenerated(phase_2_patterns);\n  phase_2_patterns.insert<\n      ScalarizeSplatConstantForAdd, ScalarizeSplatConstantForSub,\n      ScalarizeSplatConstantForMul, ScalarizeSplatConstantForDiv,\n      FuseFullyConnectedAndAdd, FuseAddAndFullyConnected,\n      FuseFullyConnectedAndMul, FuseMulAndFullyConnected,\n      FuseFullyConnectedAndReluX<TFL::ReluOp, kRelu>,\n      FuseFullyConnectedAndReluX<TFL::Relu6Op, kRelu6>,\n      FuseFullyConnectedAndReluX<TFL::Relu1Op, kRelu1>,\n      FuseBinaryOpToFollowingConv2D, FuseBinaryOpToFollowingDepthwiseConv2D,\n      FuseBinaryOpToFollowingFullyConnected, FuseConv2DAndMulWithQDQs,\n      FuseDepthwiseConv2DAndMulWithQDQs, ConvertTrivialTransposeOpToReshapeOp,\n      RemoveReshapeAfterFullyConnected, RemoveReshapeBeforeFullyConnected,\n      FuseUnpackAndConcatToReshape>(ctx);\n  if (enable_canonicalization_)\n    AddCanonicalizationPatterns(ctx, &phase_2_patterns);\n  (void)applyPatternsAndFoldGreedily(func, std::move(phase_2_patterns));\n}\n}  // namespace\n\n// Creates an instance of the TensorFlow Lite dialect Optimize pass.\nstd::unique_ptr<OperationPass<FuncOp>> CreateOptimizePass(\n    bool enable_canonicalization) {\n  return std::make_unique<OptimizePass>(enable_canonicalization);\n}\n\nstatic PassRegistration<OptimizePass> pass;\n\n}  // namespace TFL\n}  // namespace mlir\n"], "fixing_code": ["/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n// This transformation pass takes operations in TensorFlowLite dialect and\n// optimizes them to resulting operations in TensorFlowLite dialect.\n\n#include <algorithm>\n#include <climits>\n#include <cstdint>\n#include <functional>\n#include <iterator>\n#include <map>\n#include <numeric>\n#include <utility>\n\n#include \"llvm/ADT/APFloat.h\"\n#include \"llvm/ADT/APInt.h\"\n#include \"llvm/ADT/ArrayRef.h\"\n#include \"llvm/ADT/None.h\"\n#include \"llvm/ADT/Optional.h\"\n#include \"llvm/ADT/STLExtras.h\"\n#include \"llvm/ADT/SmallSet.h\"\n#include \"llvm/ADT/SmallVector.h\"\n#include \"llvm/ADT/StringRef.h\"\n#include \"llvm/ADT/StringSwitch.h\"\n#include \"llvm/Support/Casting.h\"\n#include \"llvm/Support/raw_ostream.h\"\n#include \"mlir/Dialect/StandardOps/IR/Ops.h\"  // from @llvm-project\n#include \"mlir/IR/Attributes.h\"  // from @llvm-project\n#include \"mlir/IR/BuiltinAttributes.h\"  // from @llvm-project\n#include \"mlir/IR/BuiltinTypes.h\"  // from @llvm-project\n#include \"mlir/IR/MLIRContext.h\"  // from @llvm-project\n#include \"mlir/IR/Matchers.h\"  // from @llvm-project\n#include \"mlir/IR/TypeUtilities.h\"  // from @llvm-project\n#include \"mlir/IR/Value.h\"  // from @llvm-project\n#include \"mlir/Pass/Pass.h\"  // from @llvm-project\n#include \"mlir/Support/LLVM.h\"  // from @llvm-project\n#include \"mlir/Support/LogicalResult.h\"  // from @llvm-project\n#include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"  // from @llvm-project\n#include \"tensorflow/compiler/mlir/lite/ir/tfl_ops.h\"\n#include \"tensorflow/compiler/mlir/lite/quantization/quantization_utils.h\"\n#include \"tensorflow/compiler/mlir/lite/transforms/passes.h\"\n#include \"tensorflow/compiler/mlir/lite/utils/attribute_utils.h\"\n#include \"tensorflow/compiler/mlir/lite/utils/convert_type.h\"\n#include \"tensorflow/compiler/mlir/lite/utils/validators.h\"\n#include \"tensorflow/compiler/mlir/tensorflow/ir/tf_ops.h\"\n\nnamespace mlir {\nnamespace TFL {\n\n//===----------------------------------------------------------------------===//\n// The actual Optimize Pass.\nnamespace {\nconstexpr char kRelu[] = \"RELU\";\nconstexpr char kRelu6[] = \"RELU6\";\nconstexpr char kRelu1[] = \"RELU_N1_TO_1\";\n\nbool L2NormalizeReduceAxis(Value sq_op, DenseElementsAttr axis) {\n  if (axis.getNumElements() == 0) {\n    return false;\n  }\n  if (sq_op.getType().cast<ShapedType>().getRank() - 1 ==\n          *axis.getValues<int>().begin() ||\n      *axis.getValues<int>().begin() == -1) {\n    return true;\n  }\n  if (sq_op.getType().cast<ShapedType>().getRank() != axis.getNumElements()) {\n    return false;\n  }\n  auto shape = sq_op.getType().cast<ShapedType>();\n  SmallVector<int, 4> elems{axis.getValues<int>().begin(),\n                            axis.getValues<int>().end()};\n  for (int i = 0; i < shape.getRank(); ++i) {\n    if (i != elems[i]) return false;\n  }\n  return true;\n}\n\nusing ::llvm::cast;\n\n// Optimize TFLite operations in functions.\nclass OptimizePass : public PassWrapper<OptimizePass, FunctionPass> {\n public:\n  OptimizePass() = default;\n  OptimizePass(const OptimizePass &) {}\n  explicit OptimizePass(bool enable_canonicalization) {\n    enable_canonicalization_ = enable_canonicalization;\n  }\n\n  StringRef getArgument() const final {\n    // This is the argument used to refer to the pass in\n    // the textual format (on the commandline for example).\n    return \"tfl-optimize\";\n  }\n  StringRef getDescription() const final {\n    // This is a brief description of the pass.\n    return \"Optimize within the TensorFlow Lite dialect\";\n  }\n\n  void runOnFunction() override;\n\n private:\n  Option<bool> enable_canonicalization_{\n      *this, \"enable-canonicalization\",\n      llvm::cl::desc(\"Enable canonicalization during optimization pass.\"),\n      llvm::cl::init(false)};\n};\n\n// Returns whether the given type `a` is broadcast-compatible with `b`.\nbool IsBroadcastableElementsAttrAndType(Type a, Type b) {\n  return OpTrait::util::getBroadcastedType(a, b) != Type();\n}\n\n// Returns whether the resultant type of any broadcastable operation with\n// operands `a` and `b` matches `expected_output`. Returns false if `a` is not\n// broadcast-compatible with `b`.\nbool OperandsBroadcastToOutputType(Type a, Type b, Type expected_output) {\n  Type output_element_type =\n      expected_output.cast<ShapedType>().getElementType();\n  Type broadcasted_type =\n      OpTrait::util::getBroadcastedType(a, b, output_element_type);\n  return broadcasted_type != Type() && broadcasted_type == expected_output;\n}\n\n// Returns whether if `type1` dimensions are the same as the ending dimensions\n// of `type2`. This is more restricted than broadcastable.\nbool IsTailOfShape(Type type1, Type type2) {\n  auto tail_type = type1.dyn_cast<ShapedType>();\n  auto full_type = type2.dyn_cast<ShapedType>();\n  if (!tail_type || !full_type || !tail_type.hasRank() ||\n      !full_type.hasRank() || tail_type.getRank() > full_type.getRank())\n    return false;\n  auto i1 = tail_type.getShape().rbegin(), e1 = tail_type.getShape().rend();\n  auto i2 = full_type.getShape().rbegin();\n  return std::equal(i1, e1, i2);\n}\n\nbool CanFuseConvOrDepthwiseConvShapes(const ArrayRef<int64_t> filter_shape,\n                                      const ArrayRef<int64_t> elements_shape,\n                                      bool is_depthwise) {\n  // Make sure the val tensor has shape where all dimensions are 1 except\n  // last one.\n  // Also, val tensor must be of rank 1 or 4 or 0 (scalar).\n  const auto elements_rank = elements_shape.size();\n  for (int i = 0; i < static_cast<int>(elements_shape.size()) - 1; ++i) {\n    if (elements_shape[i] != 1) return false;\n  }\n  if (elements_rank != 1 && elements_rank != 0 && elements_rank != 4) {\n    return false;\n  }\n  auto elements_depth = elements_shape.empty() ? 1 : elements_shape.back();\n  // If elements depth equals 1 (i.e., scalar or tensor with 1 element), then we\n  // can let binary op to broadcast elements.\n  if (elements_depth == 1) {\n    return true;\n  }\n\n  // In TFLite Conv2D uses OHWI format for filter, and 1HWO for Depthwise Conv.\n  // For conv:\n  // Check if last dimension in filter equals the first dimension\n  // For depthwise conv:\n  // Check if the first in filter dimension equals the first dimension.\n  if (filter_shape.empty() ||\n      (is_depthwise ? filter_shape.back() != elements_depth\n                    : filter_shape[0] != elements_depth))\n    return false;\n  return true;\n}\n\nbool CanFuseConvOrDepthwiseConv(Value filter, Attribute val,\n                                bool is_depthwise) {\n  const auto elements = val.dyn_cast<DenseElementsAttr>();\n  if (!elements) {\n    return false;\n  }\n  const auto elements_shape = elements.getType().getShape();\n  const auto filter_shape = filter.getType().cast<ShapedType>().getShape();\n  return CanFuseConvOrDepthwiseConvShapes(filter_shape, elements_shape,\n                                          is_depthwise);\n}\n\nbool CanFuseConvOrDepthwiseConv(Attribute filter, Attribute val,\n                                bool is_depthwise) {\n  if (const auto elements = val.dyn_cast<DenseElementsAttr>()) {\n    if (const auto filter_elements = filter.dyn_cast<DenseElementsAttr>()) {\n      return CanFuseConvOrDepthwiseConvShapes(\n          filter_elements.getType().getShape(), elements.getType().getShape(),\n          is_depthwise);\n    }\n  }\n  return false;\n}\n\n// Retuns true if we can eliminate the GatherNdOp or ScatterNdOp. When the value\n// of `indices` are from 0 to n-1, the output tensor are identical to the\n// `params`.\nbool CanOptimizeIdentityGatherNdOrScatterNdOp(Value params,\n                                              DenseIntElementsAttr indices) {\n  auto params_type = params.getType().dyn_cast<RankedTensorType>();\n  auto indices_type = indices.getType().dyn_cast<RankedTensorType>();\n  // Checks the shape of `params` is [n, ...], shape of `indices` is [n, 1]. 2D\n  // `indices` means it gets the first row of `params`. As long as indices\n  // iterate the first row of `params`, the output is identical to input.\n  if (!params_type || !indices_type || indices_type.getRank() != 2 ||\n      indices_type.getDimSize(0) != params_type.getDimSize(0) ||\n      indices_type.getDimSize(1) != 1)\n    return false;\n\n  // Checks the value in `indices` is from 0 to n-1.\n  int cur_value = 0;\n  for (const auto &v : indices.getValues<APInt>()) {\n    if (v.getSExtValue() != cur_value) return false;\n    ++cur_value;\n  }\n\n  return true;\n}\n\n// Returns true if we can eliminate the SliceOp. When the values of `begin` are\n// all 0s and `size[i]` is equal to either -1 or `input.shape[i]`\n// for each dim i, the output tensor is identical to `input`.\nbool CanOptimizeIdentitySliceOp(Value input, Attribute begin, Attribute size) {\n  // Checks if `begin` and `size` are i32 or i64.\n  auto begin_attr = begin.dyn_cast<DenseIntElementsAttr>();\n  auto size_attr = size.dyn_cast<DenseIntElementsAttr>();\n  if (!begin_attr || !size_attr) {\n    return false;\n  }\n\n  auto begin_elem_ty = begin_attr.getType().getElementType();\n  if (!begin_elem_ty.isInteger(32) && !begin_elem_ty.isInteger(64)) {\n    return false;\n  }\n  auto size_elem_ty = size_attr.getType().getElementType();\n  if (!size_elem_ty.isInteger(32) && !size_elem_ty.isInteger(64)) {\n    return false;\n  }\n\n  // Checks if `input` is ranked and its rank is equal to number of elements in\n  // `begin` and `size`.\n  auto input_ty = input.getType().cast<ShapedType>();\n  if (!input_ty.hasRank()) {\n    return false;\n  }\n\n  int64_t rank = input_ty.getRank();\n  if (rank != begin_attr.getNumElements() ||\n      rank != size_attr.getNumElements()) {\n    return false;\n  }\n\n  // Checks if `begin` is all 0s, and `size[i]` is equal to either -1 or\n  // `input.shape[i]`.\n  for (uint64_t i = 0; i < rank; ++i) {\n    if (begin_attr.getValue<APInt>({i}).getSExtValue() != 0) return false;\n    int64_t si = size_attr.getValue<APInt>({i}).getSExtValue();\n    if (si != -1 && si != input_ty.getDimSize(i)) return false;\n  }\n\n  return true;\n}\n\n// Expand Attribute 'a' to 4D with all 1s except 1 dimension.\n// Which dimension depends on 'is_depthwise' is true or false.\nElementsAttr ExpandTo4DForConvImpl(Attribute a, bool is_depthwise) {\n  auto elements = a.dyn_cast<DenseElementsAttr>();\n  auto shape = elements.getType().getShape();\n  if (!shape.empty()) {\n    // Checks that elements are essentially 1d.\n    assert(elements.getNumElements() == shape.back());\n  }\n  std::vector<int64_t> shape_data = {1, 1, 1, 1};\n  const int vector_length = elements.getNumElements();\n  if (is_depthwise)\n    shape_data[3] = vector_length;\n  else\n    shape_data[0] = vector_length;\n  auto new_shape =\n      RankedTensorType::get(shape_data, elements.getType().getElementType());\n  return elements.reshape(new_shape);\n}\n\nElementsAttr ExpandTo4DForConv(Attribute a) {\n  return ExpandTo4DForConvImpl(a, false);\n}\n\nElementsAttr ExpandTo4DForDepthwiseConv(Attribute a) {\n  return ExpandTo4DForConvImpl(a, true);\n}\n\nTypeAttr RescaleQtype(Type input, Attribute factor) {\n  return quant::RescaleQuantizedType(input, factor);\n}\n\n// Returns shape of a ranked tensor.\n// Precondition: output_val's is ranked tensor.\nDenseElementsAttr GetShape(Value output_val) {\n  auto output_type = output_val.getType().cast<RankedTensorType>();\n  auto shape_vector = output_type.getShape();\n  std::vector<int32_t> shape;\n  shape.reserve(shape_vector.size());\n  for (auto shape_object : shape_vector) {\n    shape.push_back(shape_object);\n  }\n  return mlir::DenseElementsAttr::get(\n      RankedTensorType::get(\n          {static_cast<int>(shape.size())},\n          mlir::IntegerType::get(output_val.getContext(), 32)),\n      llvm::makeArrayRef(shape));\n}\n\nstatic Type GetShapeStrippedType(TypeAttr type_attr) {\n  auto type = type_attr.getValue();\n  auto shaped_type = type.dyn_cast<ShapedType>();\n  if (shaped_type) {\n    return shaped_type.getElementType();\n  } else {\n    return type;\n  }\n}\n\n// Returns `true` if reducing `axes` in `input` with `keep_dims=true` results in\n// the specified `shape` and `false` otherwise.\nstatic bool ShapeMatchesReduceWithKeepAxes(Value input,\n                                           const mlir::Attribute &axes,\n                                           const mlir::Attribute &shape) {\n  RankedTensorType type = input.getType().dyn_cast_or_null<RankedTensorType>();\n  if (!type) return false;\n\n  DenseIntElementsAttr axes_attr =\n      axes.dyn_cast_or_null<DenseIntElementsAttr>();\n  DenseIntElementsAttr shape_attr =\n      shape.dyn_cast_or_null<DenseIntElementsAttr>();\n  if (!axes_attr || !shape_attr) return false;\n\n  if (shape_attr.getNumElements() != type.getRank()) return false;\n\n  llvm::SmallSet<uint64_t, 4> axes_set;\n  for (auto a : axes_attr.getIntValues()) {\n    axes_set.insert(a.getZExtValue());\n  }\n\n  auto type_shape = type.getShape();\n  for (uint64_t i = 0; i < type.getRank(); ++i) {\n    if (axes_set.contains(i)) {\n      if (shape_attr.getValue<APInt>({i}) != 1) return false;\n    } else {\n      if (shape_attr.getValue<APInt>({i}) != type_shape[i]) return false;\n    }\n  }\n  return true;\n}\n\nstatic bool FloatValueEquals(const Attribute &attr, double value) {\n  auto fp_attr = attr.dyn_cast_or_null<DenseFPElementsAttr>();\n  if (!fp_attr) return false;\n\n  if (fp_attr.isSplat()) {\n    return fp_attr.getSplatValue<APFloat>().isExactlyValue(value);\n  }\n  return llvm::all_of(fp_attr.getFloatValues(), [value](const APFloat &f) {\n    return f.isExactlyValue(value);\n  });\n}\n\n// Returns true if the value's element type is F32.\nbool IsF32Value(Value value) {\n  return value.getType().cast<ShapedType>().getElementType().isF32();\n}\n\n// Returns the number of elements in attr if it is a DenseElementsAttr, 1\n// otherwise, as an unranked int32 Attribute.\nAttribute GetNumElementsOrOne(Attribute attr) {\n  const auto dense_attr = attr.dyn_cast_or_null<DenseElementsAttr>();\n  int32_t num_elements = dense_attr ? dense_attr.getNumElements() : 1;\n\n  OpBuilder builder(attr.getContext());\n\n  return DenseIntElementsAttr::get(\n      RankedTensorType::get({}, builder.getI32Type()),\n      {llvm::APInt(32, num_elements, true)});\n}\n\n// Returns true if attr is a DenseIntElementsAttr with the last element equal 1.\nbool IsLastElementEqualsOne(Attribute attr) {\n  const auto ints = attr.dyn_cast_or_null<DenseIntElementsAttr>();\n  if (!ints) return false;\n  if (ints.empty()) return false;\n  const auto last_element_index = ints.getNumElements() - 1;\n  const auto iterator = ints.getIntValues().begin();\n  const APInt last_element = iterator[last_element_index];\n  return last_element == 1;\n}\n\n// Returns true if attr is a DenseIntElementsAttr of int32 or int64 values or an\n// incrementing sequence from 0 to N-1.\n//\n// If such a value is used in an Equal operator, it can be replaced with OneHot.\nbool IsOneHotIndexAttribute(Attribute attr) {\n  const auto dense_attr = attr.dyn_cast_or_null<DenseIntElementsAttr>();\n  if (!dense_attr) {\n    return false;\n  }\n  auto index_type = dense_attr.getType();\n  const auto index_elem_bits = index_type.getElementTypeBitWidth();\n  if (index_elem_bits != 32 && index_elem_bits != 64) {\n    return false;\n  }\n  if (index_type.getRank() != 1) {\n    return false;\n  }\n  const auto elems = dense_attr.getIntValues().begin();\n  for (int i = 0; i < dense_attr.getNumElements(); ++i) {\n    if (i != elems[i]) {\n      return false;\n    }\n  }\n  return true;\n}\n\n// Converts an Attribute with a single value of float or integral type to an\n// Attribute holding a single value of float type. If attr has no elements, the\n// result is 0.0f.\nAttribute ConvertSingleElementAttrToFloatAttr(Attribute attr) {\n  const auto dense_fp_attr = attr.dyn_cast_or_null<DenseFPElementsAttr>();\n  if (dense_fp_attr) {\n    // Already float => return\n    return dense_fp_attr;\n  }\n\n  OpBuilder builder(attr.getContext());\n\n  const auto dense_int_attr = attr.dyn_cast<DenseIntElementsAttr>();\n  const auto int_values = dense_int_attr.getIntValues();\n  float float_val = 0.0f;\n  if (!int_values.empty()) {\n    const APInt apint_val = *int_values.begin();\n    if (dense_int_attr.getType().getElementType().isSignedInteger()) {\n      // Get the sign-extended value (=>int64) if the type is signed.\n      float_val = apint_val.getSExtValue();\n    } else {\n      // Get the zero-extended value (=>uint64) if unsigned or signless.\n      float_val = apint_val.getZExtValue();\n    }\n  }\n  return DenseFPElementsAttr::get(\n      RankedTensorType::get({}, builder.getF32Type()),\n      {llvm::APFloat(float_val)});\n}\n\n#include \"tensorflow/compiler/mlir/lite/transforms/generated_optimize.inc\"\n\n// Fuse Add with proceeding FullyConnected.\n// TODO(b/136285429): Move to tablegen when variadic is supported\nstruct FuseFullyConnectedAndAdd : public OpRewritePattern<TFL::AddOp> {\n  using OpRewritePattern<TFL::AddOp>::OpRewritePattern;\n\n  LogicalResult matchAndRewrite(TFL::AddOp add_op,\n                                PatternRewriter &rewriter) const override {\n    // Match Add.\n    DenseElementsAttr added_value;\n    Value constant_val = add_op.rhs();\n    if (!matchPattern(constant_val, m_Constant(&added_value))) return failure();\n\n    // Match Fully Connected.\n    auto fc_op =\n        dyn_cast_or_null<TFL::FullyConnectedOp>(add_op.lhs().getDefiningOp());\n    if (!fc_op) return failure();\n\n    // Check if the constant RHS is either 0D (scalar), or a 1D with\n    // `{num_channels}` shape.\n    auto constant_val_type = constant_val.getType().cast<TensorType>();\n\n    // In TFLite FullyConnect definition, bias must be a 1D tensor where\n    // the number of elements is equal to the number of channels.\n    // If it's not 1D or 0D (which can be broadcasted to 1D), reject the\n    // matching.\n    bool is_scalar_rhs = false;\n    if (constant_val_type.getRank() == 0) {\n      is_scalar_rhs = true;\n    } else if (constant_val_type.getRank() != 1) {\n      return failure();\n    }\n\n    Value filter = fc_op.filter();\n    Value bias = fc_op.bias();\n    ElementsAttr bias_value;\n    const bool is_none_bias = bias.getType().isa<NoneType>();\n    if (fc_op.fused_activation_function() != \"NONE\") return failure();\n\n    if (!is_none_bias && !matchPattern(bias, m_Constant(&bias_value)))\n      return failure();\n\n    // Rewrite\n    if (is_none_bias) {\n      if (is_scalar_rhs) {\n        // If the `constant_val` is scalar, we must the shape of filter\n        // to properly broadcast the scalar to `{num_channels}` shape.\n\n        // Get the number of channels if possible.\n        auto filter_type = filter.getType().dyn_cast<RankedTensorType>();\n        // Filter must be a `2D` tensor with `{num_channels, num_features}`\n        // shape. The following check is rejecting unknown rank (-1).\n        if (filter_type == nullptr || filter_type.getRank() != 2) {\n          return failure();\n        }\n        int num_channels = filter_type.getShape()[0];\n\n        // Create a zero tensor with shape {num_channels}, and the type need to\n        // be the same as constant_val.\n        // This is a way to gracefully handle scalar tensor. The Add will always\n        // be constant-folded away regardless if `constant_val` is a scalar or\n        // not.\n        RankedTensorType type = RankedTensorType::get(\n            {num_channels}, constant_val_type.getElementType());\n        auto attr = rewriter.getZeroAttr(type);\n        bias = rewriter.create<ConstantOp>(add_op.getLoc(), type, attr);\n        auto none_af = rewriter.getStringAttr(\"NONE\");\n        bias =\n            rewriter.create<AddOp>(add_op.getLoc(), bias, constant_val, none_af)\n                .output();\n      } else {\n        // If there no pre-existing bias and the `constant_val` is 1D, simply\n        // use `constant_val` as bias.\n        bias = constant_val;\n      }\n    } else {\n      auto none_af = rewriter.getStringAttr(\"NONE\");\n      bias =\n          rewriter.create<AddOp>(add_op.getLoc(), bias, constant_val, none_af)\n              .output();\n    }\n\n    auto fc = rewriter.create<TFL::FullyConnectedOp>(\n        FusedLoc::get(fc_op.getContext(), {fc_op.getLoc(), add_op.getLoc()}),\n        add_op.getType(),\n        /*input=*/fc_op.input(),\n        /*filter=*/filter,\n        /*bias=*/bias,\n        /*fused_activation_function=*/\n        rewriter.getStringAttr(add_op.fused_activation_function()),\n        /*weights_format=*/rewriter.getStringAttr(fc_op.weights_format()),\n        /*keep_num_dims=*/rewriter.getBoolAttr(fc_op.keep_num_dims()));\n    rewriter.replaceOp(add_op, fc.output());\n\n    return success();\n  }\n};\n\n// Replace ..\n// FC(Add(lhs, rhs), filter, bias)\n// .. with ..\n// FC(lhs, filter, FC(rhs, filter, bias))\n// .. if rhs, filter, and bias are all constants.\n// The second FC will be constant folded to a single vector.\n// TODO(b/136285429): Move to tablegen when variadic is supported\nstruct FuseAddAndFullyConnected\n    : public OpRewritePattern<TFL::FullyConnectedOp> {\n  using OpRewritePattern<TFL::FullyConnectedOp>::OpRewritePattern;\n\n  LogicalResult matchAndRewrite(TFL::FullyConnectedOp fc_op,\n                                PatternRewriter &rewriter) const override {\n    // This only works with default format.\n    if (fc_op.weights_format() != \"DEFAULT\") return failure();\n\n    // Match Add.\n    auto add_op = dyn_cast_or_null<TFL::AddOp>(fc_op.input().getDefiningOp());\n    if (!add_op) return failure();\n    if (add_op.fused_activation_function() != \"NONE\") return failure();\n\n    // Don't match adds where the added constant is not 1D.\n    {\n      auto addend_shape = add_op.rhs().getType().cast<ShapedType>();\n      if (!addend_shape.hasStaticShape()) return failure();\n      if (addend_shape.getShape().size() != 1) return failure();\n    }\n\n    // Calculate new bias.  Generate a new FC; it will be constant folded.\n    auto old_bias = fc_op.bias();\n    if (!old_bias || old_bias.getType().isa<NoneType>()) {\n      // TODO(b/180752069): Figure out new bias' type when old bias is empty.\n      return failure();\n    }\n\n    // The FC relies on constant folding, which is implemented on F32. Checks\n    // types to be F32.\n    {\n      if (!IsF32Value(add_op.rhs()) || !IsF32Value(fc_op.filter()) ||\n          !IsF32Value(old_bias))\n        return failure();\n    }\n\n    auto new_bias = rewriter.create<TFL::FullyConnectedOp>(\n        fc_op.getLoc(), old_bias.getType(),\n        /*input=*/add_op.rhs(),\n        /*filter=*/fc_op.filter(),\n        /*bias=*/old_bias,\n        /*fused_activation_function=*/rewriter.getStringAttr(\"NONE\"),\n        /*weights_format=*/rewriter.getStringAttr(\"DEFAULT\"),\n        /*keep_num_dims=*/rewriter.getBoolAttr(true));\n\n    // Create the updated FC.\n    auto new_fc = rewriter.create<TFL::FullyConnectedOp>(\n        FusedLoc::get(add_op.getContext(), {add_op.getLoc(), fc_op.getLoc()}),\n        fc_op.output().getTypes(),\n        /*input=*/add_op.lhs(),\n        /*filter=*/fc_op.filter(),\n        /*bias=*/*new_bias.output().begin(),\n        /*fused_activation_function=*/\n        rewriter.getStringAttr(fc_op.fused_activation_function()),\n        /*weights_format=*/rewriter.getStringAttr(\"DEFAULT\"),\n        /*keep_num_dims=*/rewriter.getBoolAttr(fc_op.keep_num_dims()));\n    rewriter.replaceOp(fc_op.getOperation(), new_fc.output());\n\n    return success();\n  }\n};\n\n// Replace ..\n// FC(Mul(lhs, rhs), filter, bias)\n// .. with ..\n// FC(lhs, Mul(filter, rhs), bias)\n// .. if rhs, filter, and bias are all constants.\n// The generated Mul will be constant folded to a single matrix.\nstruct FuseMulAndFullyConnected\n    : public OpRewritePattern<TFL::FullyConnectedOp> {\n  using OpRewritePattern<TFL::FullyConnectedOp>::OpRewritePattern;\n\n  LogicalResult matchAndRewrite(TFL::FullyConnectedOp fc_op,\n                                PatternRewriter &rewriter) const override {\n    // This only works with default format.\n    if (fc_op.weights_format() != \"DEFAULT\") return failure();\n\n    // Match Mul.\n    auto mul_op = dyn_cast_or_null<TFL::MulOp>(fc_op.input().getDefiningOp());\n    if (!mul_op) return failure();\n    if (mul_op.fused_activation_function() != \"NONE\") return failure();\n\n    // Don't match muls where the multiplier constant is not 1D.\n    {\n      auto multiplier_shape = mul_op.rhs().getType().cast<ShapedType>();\n      if (!multiplier_shape.hasStaticShape()) return failure();\n      if (multiplier_shape.getShape().size() != 1) return failure();\n    }\n\n    // We rely on constant folding, implemented only for F32. Check types.\n    if (!IsF32Value(mul_op.rhs()) || !IsF32Value(fc_op.filter())) {\n      return failure();\n    }\n\n    auto location =\n        FusedLoc::get(mul_op.getContext(), {mul_op.getLoc(), fc_op.getLoc()});\n\n    auto new_filter = rewriter.create<TFL::MulOp>(\n        location,\n        /*lhs=*/fc_op.filter(),\n        /*rhs=*/mul_op.rhs(),\n        /*fused_activation_function=*/rewriter.getStringAttr(\"NONE\"));\n    // Create the updated FC.\n    auto new_fc = rewriter.create<TFL::FullyConnectedOp>(\n        location, fc_op.output().getTypes(),\n        /*input=*/mul_op.lhs(),\n        /*filter=*/new_filter,\n        /*bias=*/fc_op.bias(),\n        /*fused_activation_function=*/\n        rewriter.getStringAttr(fc_op.fused_activation_function()),\n        /*weights_format=*/rewriter.getStringAttr(\"DEFAULT\"),\n        /*keep_num_dims=*/rewriter.getBoolAttr(fc_op.keep_num_dims()));\n    rewriter.replaceOp(fc_op.getOperation(), new_fc.output());\n\n    return success();\n  }\n};\n\n// TODO(b/136285429): Move to tablegen when variadic is supported.\ntemplate <typename ReluXOp, char const *Act>\nstruct FuseFullyConnectedAndReluX : public OpRewritePattern<ReluXOp> {\n  using OpRewritePattern<ReluXOp>::OpRewritePattern;\n\n  LogicalResult matchAndRewrite(ReluXOp relu_op,\n                                PatternRewriter &rewriter) const override {\n    Operation *input = relu_op.getOperand().getDefiningOp();\n    if (!isa_and_nonnull<FullyConnectedOp>(input)) return failure();\n    auto fully_connected_op = cast<FullyConnectedOp>(input);\n    if (fully_connected_op.fused_activation_function() != \"NONE\")\n      return failure();\n\n    auto new_activation_func = rewriter.getStringAttr(Act);\n    auto new_weights_format =\n        rewriter.getStringAttr(fully_connected_op.weights_format());\n    auto new_keep_num_dims =\n        rewriter.getBoolAttr(fully_connected_op.keep_num_dims());\n    auto fc = rewriter.create<FullyConnectedOp>(\n        FusedLoc::get(relu_op.getContext(),\n                      {fully_connected_op.getLoc(), relu_op.getLoc()}),\n        relu_op.getType(), fully_connected_op.input(),\n        fully_connected_op.filter(), fully_connected_op.bias(),\n        new_activation_func, new_weights_format, new_keep_num_dims);\n    rewriter.replaceOp(relu_op, fc.output());\n\n    return success();\n  }\n};\n\n// Fuse Mul with proceeding FullyConnected.\n// TODO(b/136285429): Move to tablegen when variadic is supported\nstruct FuseFullyConnectedAndMul : public OpRewritePattern<TFL::MulOp> {\n  using OpRewritePattern<TFL::MulOp>::OpRewritePattern;\n\n  LogicalResult matchAndRewrite(TFL::MulOp mul_op,\n                                PatternRewriter &rewriter) const override {\n    // If we are broadcasting on the lhs then don't fold the multiply as it\n    // would increase the amount of compute done by the fully connected op.\n    if (mul_op.lhs().getType() != mul_op.getType()) return failure();\n\n    // Mul.\n    DenseElementsAttr cst;\n    Value constant_val = mul_op.rhs();\n    if (!matchPattern(constant_val, m_Constant(&cst))) return failure();\n\n    // Fully Connected.\n    auto fc_op =\n        dyn_cast_or_null<TFL::FullyConnectedOp>(mul_op.lhs().getDefiningOp());\n    if (!fc_op) return failure();\n    Value filter = fc_op.filter();\n    Value bias = fc_op.bias();\n    ElementsAttr cst_tmp;\n    if (!matchPattern(filter, m_Constant(&cst_tmp))) return failure();\n    if (!bias.getType().isa<NoneType>() &&\n        !matchPattern(bias, m_Constant(&cst_tmp)))\n      return failure();\n    if (fc_op.fused_activation_function() != \"NONE\") return failure();\n\n    // Only fuse multiplier if all dimensions other than the depth dimension\n    // are equal to 1 since otherwise\n    // `matmul(x, filter) * cst != matmul(x, filter * cst)`\n    // even if `filter` and `cst` are be broadcastable.\n    auto shape = cst.getType().getShape();\n    if (!IsDimensionsDegenerateExceptLastOne(shape)) return failure();\n\n    int64_t element_size = shape.empty() ? 1 : shape[shape.size() - 1];\n    // Expand and transpose the multiplier since weights are using the\n    // OHWI data format in TFLite.\n    int64_t normalized_shape[2] = {element_size, 1};\n    auto new_cst = cst.reshape(RankedTensorType::get(\n        normalized_shape, cst.getType().getElementType()));\n    Type new_type = new_cst.getType();\n    if (!IsBroadcastableElementsAttrAndType(new_type, filter.getType())) {\n      return failure();\n    }\n\n    auto new_op =\n        rewriter.create<ConstantOp>(mul_op.getLoc(), new_type, new_cst);\n    Value new_const_val = new_op.getResult();\n\n    // Rewrite. Since the folder of TFL::MulOp couldn't broadcast the operands,\n    // TF::MulOp is used to fold the constant.\n    // TODO(b/139192933): switch to the TFL constant folding\n    auto new_filter =\n        rewriter.create<TF::MulOp>(mul_op.getLoc(), filter, new_const_val).z();\n    // If bias isn't None, it needs to be multiplied as well.\n    if (!bias.getType().isa<NoneType>()) {\n      bias =\n          rewriter.create<TF::MulOp>(mul_op.getLoc(), bias, constant_val).z();\n    }\n\n    auto fc = rewriter.create<TFL::FullyConnectedOp>(\n        FusedLoc::get(fc_op.getContext(), {fc_op.getLoc(), mul_op.getLoc()}),\n        mul_op.getType(),\n        /*input=*/fc_op.input(),\n        /*filter=*/new_filter,\n        /*bias=*/bias,\n        /*fused_activation_function=*/\n        rewriter.getStringAttr(mul_op.fused_activation_function()),\n        /*weights_format=*/rewriter.getStringAttr(fc_op.weights_format()),\n        /*keep_num_dims=*/rewriter.getBoolAttr(fc_op.keep_num_dims()));\n    rewriter.replaceOp(mul_op, fc.output());\n\n    return success();\n  }\n};\n\n// Fuse Mul with proceeding Affine ops. This is an C++ implementation of the\n// following table gen implementation, which doesn't derived the result type of\n// the TFL_DequantizeOp.\n// def : Pat<(TFL_MulOp (TFL_Conv2DOp:$conv_output $input,\n//                          (TFL_DequantizeOp (TFL_QuantizeOp\n//                              (ConstantOp F32ElementsAttr:$filter), $qtype)),\n//                          (ConstantOp F32ElementsAttr:$bias),\n//                          $h_factor, $w_factor, TFL_AF_None,\n//                          $padding, $stride_h, $stride_w),\n//                      (ConstantOp F32ElementsAttr:$value), $act_fn),\n//           (TFL_Conv2DOp $input,\n//                      (TFL_DequantizeOp (TFL_QuantizeOp\n//                          (TFL_MulOp (ConstantOp $filter),\n//                                     (ConstantOp (ExpandTo4DForConv $value)),\n//                                      TFL_AF_None),\n//                          (RescaleQtype $qtype, $value))),\n//                      (TFL_MulOp (ConstantOp $bias), (ConstantOp $value),\n//                          TFL_AF_None),\n//                      $h_factor, $w_factor, $act_fn,\n//                      $padding, $stride_h, $stride_w),\n//         [(CanFuseConvOrDepthwiseConv<\"false\"> $filter, $value),\n//          (HasOneUse $conv_output),\n//          (IsPerAxisQuantization $qtype), // per-axis quantization\n//         ]>;\ntemplate <typename AffineOpType>\nstruct FuseAffinOpAndMulWithQDQs : public OpRewritePattern<TFL::MulOp> {\n  using OpRewritePattern<TFL::MulOp>::OpRewritePattern;\n\n  LogicalResult matchAndRewrite(TFL::MulOp mul_op,\n                                PatternRewriter &rewriter) const override {\n    // Mul. Required 1-D rhs for batch normalization.\n    DenseElementsAttr gamma_cst;\n    Value gamma = mul_op.rhs();\n    if (!matchPattern(gamma, m_Constant(&gamma_cst))) return failure();\n    if (gamma_cst.getType().getRank() != 1) return failure();\n\n    // Affine op\n    Operation *mul_op_lhs = mul_op.lhs().getDefiningOp();\n    auto fc_op = dyn_cast_or_null<AffineOpType>(mul_op_lhs);\n    if (!fc_op) return failure();\n    Value filter = fc_op.filter();\n    Value bias = fc_op.bias();\n\n    // QDQs\n    auto dq_op = dyn_cast_or_null<TFL::DequantizeOp>(filter.getDefiningOp());\n    if (!dq_op) return failure();\n    auto q_op =\n        dyn_cast_or_null<TFL::QuantizeOp>(dq_op.input().getDefiningOp());\n    if (!q_op) return failure();\n    filter = q_op.input();\n\n    // weight constant\n    ElementsAttr cst_tmp;\n    if (!matchPattern(filter, m_Constant(&cst_tmp))) return failure();\n    if (!bias.getType().isa<NoneType>() &&\n        !matchPattern(bias, m_Constant(&cst_tmp)))\n      return failure();\n    if (fc_op.fused_activation_function() != \"NONE\") return failure();\n\n    // Broadcast the constant operand of Mul if it isn't compatible to the\n    // filter input. We only support broadcasting the operand along the depth\n    // dimension, when the operand's depth is 1.\n    rewriter.setInsertionPoint(q_op);\n    Location loc = fc_op.getLoc();\n    Value broadcasted_gamma;\n    if (isa<TFL::Conv2DOp>(mul_op_lhs)) {\n      auto mul_rhs = ExpandTo4DForConv(gamma_cst);\n      broadcasted_gamma = rewriter.create<ConstOp>(loc, mul_rhs);\n    } else if (isa<TFL::DepthwiseConv2DOp>(mul_op_lhs)) {\n      auto mul_rhs = ExpandTo4DForDepthwiseConv(gamma_cst);\n      broadcasted_gamma = rewriter.create<ConstOp>(loc, mul_rhs);\n    } else {\n      return failure();\n    }\n\n    // Rewrite filter constant. Since the folder of TFL::MulOp couldn't\n    // broadcast the operands, TF::MulOp is used to fold the constant.\n    auto new_filter =\n        rewriter.create<TF::MulOp>(loc, filter, broadcasted_gamma).z();\n    // Update the scale in the quantize op.\n    auto new_qtype = RescaleQtype(q_op.qtype(), gamma_cst);\n    if (!new_qtype) return failure();\n    rewriter.replaceOpWithNewOp<TFL::QuantizeOp>(q_op, new_qtype.getValue(),\n                                                 new_filter, new_qtype);\n\n    // If bias isn't None, it needs to be multiplied as well.\n    if (!bias.getType().isa<NoneType>()) {\n      rewriter.setInsertionPoint(fc_op);\n      auto new_bias = rewriter.create<TF::MulOp>(loc, bias, gamma);\n      fc_op.getOperation()->replaceUsesOfWith(bias, new_bias);\n    }\n\n    // Remove the tailing mul op.\n    mul_op.replaceAllUsesWith(fc_op.getResult());\n    return success();\n  }\n};\n\nusing FuseConv2DAndMulWithQDQs = FuseAffinOpAndMulWithQDQs<TFL::Conv2DOp>;\nusing FuseDepthwiseConv2DAndMulWithQDQs =\n    FuseAffinOpAndMulWithQDQs<TFL::DepthwiseConv2DOp>;\n\n// Fuse Binary Op with following Affine operation.\ntemplate <typename AffineOpType>\nstruct FuseBinaryOpToFollowingAffineOp : public OpRewritePattern<AffineOpType> {\n  using OpRewritePattern<AffineOpType>::OpRewritePattern;\n\n  LogicalResult matchAndRewrite(AffineOpType fc_op,\n                                PatternRewriter &rewriter) const override {\n    // Binary op.\n    Operation *binary_op = fc_op.input().getDefiningOp();\n    if (!binary_op || binary_op->getNumOperands() != 2) return failure();\n    // We only handle the cases the RHS is a scalar.\n    // TODO(fengliuai): Currently the canonicalizer pass couldn't guarantee that\n    // the constant operands are on the RHS, we need to consider LHS constant\n    // operand if necessary.\n    DenseFPElementsAttr cst;\n    if (!matchPattern(binary_op->getOperand(1), m_Constant(&cst)))\n      return failure();\n    if (cst.getNumElements() != 1) return failure();\n    APFloat cst_value = *cst.float_value_begin();\n\n    // Affine op.\n    Value filter = fc_op.filter();\n    Value bias = fc_op.bias();\n    DenseFPElementsAttr filter_cst, bias_cst;\n    if (!matchPattern(filter, m_Constant(&filter_cst))) {\n      // The filter maybe quantized, then we should set it to the real constant.\n      auto dq = llvm::dyn_cast_or_null<DequantizeOp>(filter.getDefiningOp());\n      if (!dq) return failure();\n      auto q = llvm::dyn_cast_or_null<QuantizeOp>(dq.input().getDefiningOp());\n      if (!q || !matchPattern(q.input(), m_Constant(&filter_cst))) {\n        return failure();\n      }\n      filter = q.input();\n    }\n    if (!bias.getType().isa<NoneType>() &&\n        !matchPattern(bias, m_Constant(&bias_cst)))\n      return failure();\n    auto binary_op_activation_func =\n        binary_op->template getAttrOfType<StringAttr>(\n            \"fused_activation_function\");\n    if (!binary_op_activation_func ||\n        binary_op_activation_func.getValue() != \"NONE\")\n      return failure();\n    ShapedType filter_type = filter_cst.getType();\n\n    if (llvm::isa<AddOp, SubOp>(binary_op)) {\n      auto padding = fc_op->template getAttrOfType<StringAttr>(\"padding\");\n      if (padding && padding.getValue() != \"VALID\") return failure();\n\n      // The fusion of add/sub is actually applying the following\n      // transformation:\n      // w * (x + c) + b => w * x + (w * c + b)\n      // so we have to update the bias.\n      if (llvm::isa<SubOp>(binary_op)) cst_value.changeSign();\n\n      auto bias_and_slice =\n          GetBiasDimAndSliceSize(filter_type.getShape(), fc_op);\n      int64_t bias_size = bias_and_slice.first;\n      int64_t slice_size = bias_and_slice.second;\n      ShapedType new_bias_type =\n          RankedTensorType::get({bias_size}, filter_type.getElementType());\n\n      // The new bias should be a 1-D tensor with length equals to the bias\n      // dimension of the weight.\n      SmallVector<APFloat, 4> new_bias_values;\n      if (bias.getType().isa<NoneType>()) {  // none bias, a list of zeros\n        new_bias_values.resize(bias_size,\n                               APFloat::getZero(cst_value.getSemantics()));\n      } else if (bias_cst.getNumElements() == 1) {  // scalar bias, broadcast it\n        new_bias_values.resize(bias_size, *bias_cst.float_value_begin());\n      } else if (bias_cst.getNumElements() == bias_size) {  // 1-d bias, copy it\n        new_bias_values.insert(new_bias_values.begin(),\n                               bias_cst.float_value_begin(),\n                               bias_cst.float_value_end());\n      } else {\n        return failure();\n      }\n\n      int64_t flatten_index = 0;\n      for (auto fp_it = filter_cst.float_value_begin(),\n                fp_end = filter_cst.float_value_end();\n           fp_it != fp_end; ++fp_it) {\n        int bias_index = (flatten_index++ / slice_size) % bias_size;\n\n        new_bias_values[bias_index] =\n            new_bias_values[bias_index] + *fp_it * cst_value;\n      }\n      auto new_bias = DenseFPElementsAttr::get(new_bias_type, new_bias_values);\n      auto new_bias_op =\n          rewriter.create<ConstOp>(fc_op.getLoc(), new_bias_type, new_bias);\n      fc_op.setOperand(0, binary_op->getOperand(0));\n      fc_op.setOperand(2, new_bias_op);\n    } else if (llvm::isa<MulOp, DivOp>(binary_op)) {\n      // The fusion of mul/div is actually applying the following\n      // transformation:\n      // w * (x ' c) + b => (w ' c) x + b\n      // so we have to update the weight.\n      bool is_mul = llvm::isa<MulOp>(binary_op);\n      auto new_filter =\n          filter_cst.mapValues(filter_type.getElementType(), [&](APFloat it) {\n            return (is_mul ? it * cst_value : it / cst_value).bitcastToAPInt();\n          });\n      // We recreate the constant op in case it is shared by the other ops. This\n      // might increase the model size.\n      auto new_filter_op = rewriter.create<ConstOp>(\n          fc_op.getLoc(), filter.getType(), new_filter);\n      fc_op.setOperand(0, binary_op->getOperand(0));\n      if (fc_op.filter() != filter) {\n        // This filter goes through quantize and dequantize ops. Then we just\n        // need to update the weight to the quantize op.\n        filter.replaceAllUsesWith(new_filter_op);\n      } else {\n        // This filter doesn't go through quantize and dequantize ops, Then\n        // we update the weight of the affine op directly.\n        fc_op.setOperand(1, new_filter_op);\n      }\n    } else {\n      return failure();\n    }\n    return success();\n  }\n\n private:\n  // Returns the dimension length of the channel dimension and also the slide\n  // size by each position in the channel dimension accordingly. tfl.conv2d and\n  // tfl.fully_connected has heading channel dimension, but tfl.depthwise_conv2d\n  // has tailing channel dimension. This function is to provide a utility to\n  // create the above information from the op property.\n  static std::pair<int64_t, int64_t> GetBiasDimAndSliceSize(\n      ArrayRef<int64_t> filter_shape, AffineOpType op) {\n    // Channel dimension index is specified as op property\n    auto channel_index_iter = filter_shape.begin();\n    std::advance(channel_index_iter, op.GetChannelDimIndex());\n    // The slide size is the size of the data in higher dimensions.\n    int64_t slice_size =\n        std::accumulate(std::next(channel_index_iter), filter_shape.end(), 1,\n                        std::multiplies<int64_t>());\n    return {*channel_index_iter, slice_size};\n  }\n};\n\n// If the operand to a broadcastable op is a splat constant, try to replace it\n// with a 0-d constant, e.g. before this optimization,\n//   %cst = constant dense<1.0> : tensor<16x16x4xf32>\n//   %0 = \"tfl.conv_2d\"...\n//   %1 = \"tfl.add\"(%0, %cst) : (tensor<16x16x4xf32>, tensor<16x16x4xf32>)\n// After this optimization:\n//   %cst = constant dense<1.0> : tensor<f32>\n//   %0 = \"tfl.conv_2d\"...\n//   %1 = \"tfl.add\"(%0, %cst) : (tensor<16x16x4xf32>, tensor<f32>)\n// This pattern can enable more fusing opportunities when the binary op is\n// following conv ops.\ntemplate <typename BinaryOpType>\nstruct ScalarizeSplatConstantForBroadcastableOps\n    : public OpRewritePattern<BinaryOpType> {\n  using OpRewritePattern<BinaryOpType>::OpRewritePattern;\n\n  LogicalResult matchAndRewrite(BinaryOpType binary_op,\n                                PatternRewriter &rewriter) const override {\n    DenseElementsAttr splat_elements_attr;\n    if (!IsScalarizableSplatConstant(binary_op.rhs(), &splat_elements_attr)) {\n      return failure();\n    }\n\n    constexpr int kSplatOperandIndex = 1;\n    auto result_type =\n        binary_op.getResult().getType().template cast<ShapedType>();\n    mlir::Value non_splat_operand =\n        binary_op.getOperand(1 - kSplatOperandIndex);\n    auto non_splat_operand_type =\n        non_splat_operand.getType().cast<ShapedType>();\n    // If the other operand's shape does not equal to the result shape, then we\n    // cannot scalarize the splat constant because the result shape relies on\n    // the splat constant op's shape for broadcasting.\n    if (!non_splat_operand_type.hasStaticShape() ||\n        non_splat_operand_type.getShape() != result_type.getShape() ||\n        non_splat_operand_type.getRank() > 4) {\n      return failure();\n    }\n\n    // If non-splat operand is not fusable affine ops, then no need to apply\n    // this transformation.\n    if (!CanFuseAffineOp(non_splat_operand.getDefiningOp(), binary_op)) {\n      return failure();\n    }\n\n    // Creates a new scalar constant op using the splat value.\n    mlir::Value splat_operand = binary_op.getOperand(kSplatOperandIndex);\n    auto scalar_elements_attr = DenseElementsAttr::get(\n        RankedTensorType::get({},\n                              splat_elements_attr.getType().getElementType()),\n        splat_elements_attr.getSplatValue());\n\n    auto scalar_constant_op = rewriter.create<ConstantOp>(\n        splat_operand.getLoc(), scalar_elements_attr.getType(),\n        scalar_elements_attr);\n\n    binary_op.setOperand(kSplatOperandIndex, scalar_constant_op);\n    return success();\n  }\n\n private:\n  // Returns true if this value is a splat constant op which can be scalarized.\n  // Also returns the elements attr if this value is indeed a splat constant.\n  bool IsScalarizableSplatConstant(mlir::Value value,\n                                   DenseElementsAttr *elements_attr) const {\n    if (!matchPattern(value, m_Constant(elements_attr))) {\n      return false;\n    }\n    auto element_type = value.getType().cast<ShapedType>().getElementType();\n    // Ignore per-axis quantized constants because after converting to scalar,\n    // we will lose per-axis qantization parameter.\n    if (element_type.isa<quant::UniformQuantizedPerAxisType>()) {\n      return false;\n    }\n    if (IsScalar(value)) {\n      return false;\n    }\n    return elements_attr->isSplat();\n  }\n\n  // If this type is a scalar shaped type.\n  bool IsScalar(mlir::Value value) const {\n    auto type = value.getType().dyn_cast<ShapedType>();\n    if (!type) {\n      return false;\n    }\n    if (!type.hasStaticShape()) {\n      return false;\n    }\n    return type.getNumElements() == 1;\n  }\n\n  // Returns true if we can fuse an affine op with consuming binary op.\n  bool CanFuseAffineOp(Operation *affine_op, Operation *binary_op) const {\n    if (!isa_and_nonnull<TFL::Conv2DOp, TFL::DepthwiseConv2DOp,\n                         TFL::FullyConnectedOp>(affine_op)) {\n      return false;\n    }\n    DenseElementsAttr value;\n    // Check that bias are constants if not none.\n    Value bias = affine_op->getOperand(2);\n    if (!bias.getType().isa<NoneType>() &&\n        !matchPattern(bias, m_Constant(&value))) {\n      return false;\n    }\n    // If the binary op is mul/div, also check that filter is constant.\n    if (isa<TFL::MulOp, TFL::DivOp>(binary_op) &&\n        !matchPattern(affine_op->getOperand(1), m_Constant(&value))) {\n      return false;\n    }\n\n    // We can only fuse F32/BF16.\n    auto is_fusable_type = [](Type t) {\n      Type element_type = t;\n      if (auto shaped_type = t.dyn_cast<ShapedType>()) {\n        element_type = shaped_type.getElementType();\n      }\n      return element_type.isBF16() || element_type.isF32();\n    };\n    for (Type t : binary_op->getOperandTypes()) {\n      if (!is_fusable_type(t)) {\n        return false;\n      }\n    }\n\n    return true;\n  }\n};\n\nusing ScalarizeSplatConstantForSub =\n    ScalarizeSplatConstantForBroadcastableOps<TFL::SubOp>;\nusing ScalarizeSplatConstantForAdd =\n    ScalarizeSplatConstantForBroadcastableOps<TFL::AddOp>;\nusing ScalarizeSplatConstantForMul =\n    ScalarizeSplatConstantForBroadcastableOps<TFL::MulOp>;\nusing ScalarizeSplatConstantForDiv =\n    ScalarizeSplatConstantForBroadcastableOps<TFL::DivOp>;\n\nstruct ConvertTrivialTransposeOpToReshapeOp\n    : public OpRewritePattern<TFL::TransposeOp> {\n  using OpRewritePattern<TFL::TransposeOp>::OpRewritePattern;\n\n  LogicalResult matchAndRewrite(TFL::TransposeOp transpose_op,\n                                PatternRewriter &rewriter) const override {\n    auto input_type = transpose_op.input().getType().cast<ShapedType>();\n    auto output_type = transpose_op.output().getType().cast<ShapedType>();\n    // It's possible to know if the transformation is safe only if the input\n    // & output shapes are fully known and permutation is a constant.\n    if (!input_type.hasStaticShape() || !output_type.hasStaticShape())\n      return failure();\n    Value perm = transpose_op.perm();\n    DenseElementsAttr perm_values_attr;\n    if (!matchPattern(perm, m_Constant(&perm_values_attr))) return failure();\n\n    auto input_shape = input_type.getShape();\n    SmallVector<int64_t, 8> perm_values;\n    for (const auto &dim : perm_values_attr.getIntValues())\n      perm_values.push_back(dim.getSExtValue());\n\n    // This should never happen unless the input graph is malformed.\n    if (input_shape.size() != perm_values.size()) {\n      transpose_op.emitError(\n          \"TransposeOP has inconsistent input and perm values.\");\n    }\n\n    SmallVector<int, 8> old_major_index_ordering;\n    SmallVector<int, 8> new_major_index_ordering;\n    for (int i = 0, end = input_shape.size(); i < end; i++) {\n      if (input_shape[i] != 1) {\n        old_major_index_ordering.push_back(i);\n      }\n\n      if (input_shape[perm_values[i]] != 1) {\n        new_major_index_ordering.push_back(perm_values[i]);\n      }\n    }\n    if (old_major_index_ordering != new_major_index_ordering) {\n      return failure();\n    }\n\n    // Rewrite.\n    Location loc = transpose_op.getLoc();\n\n    SmallVector<int32_t, 8> output_shape_values;\n    for (auto dim : output_type.getShape()) {\n      output_shape_values.push_back(dim);\n    }\n    auto type = mlir::RankedTensorType::get(output_shape_values.size(),\n                                            rewriter.getIntegerType(32));\n    auto new_shape_attr =\n        mlir::DenseIntElementsAttr::get(type, output_shape_values);\n    auto new_shape = rewriter.create<TF::ConstOp>(loc, new_shape_attr);\n\n    rewriter.replaceOpWithNewOp<TFL::ReshapeOp>(\n        transpose_op, transpose_op.output().getType(), transpose_op.input(),\n        new_shape);\n\n    return success();\n  }\n};\n\n// Remove Reshape before FullyConnected when `keep_num_dims=false` and Reshape\n// does not alter the last dimension as FullyConnected will collapse all other\n// dimensions into a single dimension. For example,\n//\n//   %shape = constant dense<[1, 128, 64]> : tensor<3xi32>\n//   %reshape = tfl.reshape(%input, %shape) // %input: tensor<128x64xf32>\n//   %fc = tfl.fully_connected(%reshape, %filter, %bias)\n//           {keep_num_dims = false, weights_format = \"DEFAULT\"}\n//\n// can be canonicalized to\n//\n//   %fc = tfl.fully_connected(%input, %filter, %bias)\n//           {keep_num_dims = false, weights_format = \"DEFAULT\"}\nstruct RemoveReshapeBeforeFullyConnected\n    : public OpRewritePattern<TFL::FullyConnectedOp> {\n  using OpRewritePattern<TFL::FullyConnectedOp>::OpRewritePattern;\n\n  LogicalResult matchAndRewrite(TFL::FullyConnectedOp fully_connected_op,\n                                PatternRewriter &) const override {\n    auto input = fully_connected_op.input();\n    auto input_ty = input.getType().dyn_cast<ShapedType>();\n    auto output_ty = fully_connected_op.output()[0]\n                         .getType()\n                         .template dyn_cast<ShapedType>();\n    if (!input_ty.hasStaticShape() ||\n        fully_connected_op.weights_format() != \"DEFAULT\" ||\n        fully_connected_op.keep_num_dims() || !output_ty.hasStaticShape() ||\n        output_ty.getRank() != 2) {\n      return failure();\n    }\n\n    auto reshape_op = input.getDefiningOp<TFL::ReshapeOp>();\n    if (!reshape_op) return failure();\n\n    // Check if the last dimension does not change after reshape.\n    auto reshape_input = reshape_op.input();\n    auto reshape_input_ty = reshape_input.getType().dyn_cast<ShapedType>();\n    if (!reshape_input_ty.hasStaticShape() || input_ty.getRank() == 0 ||\n        reshape_input_ty.getRank() == 0 ||\n        input_ty.getDimSize(input_ty.getRank() - 1) !=\n            reshape_input_ty.getDimSize(reshape_input_ty.getRank() - 1)) {\n      return failure();\n    }\n\n    // Connect the input to the one of reshape.\n    fully_connected_op.setOperand(0, reshape_input);\n    return success();\n  }\n};\n\n// Remove Reshape after FullyConnected when `keep_num_dims=false`, the Reshaoe\n// does not alter the last dimension and it restores the batch dimensions\n// collapsed by the FullyConnected op due to `keep_num_dims=false`. For example,\n//\n//   // %input: tensor<4x16x32xf32>\n//   %fc = tfl.fully_connected(%input, %filter, %bias)\n//           {keep_num_dims = false, weights_format = \"DEFAULT\"}\n//   %shape = constant dense<[4, 16, 32]> : tensor<3xi32>\n//   %rs = tfl.reshape(%fc, %shape)\n//\n// can be canonicalized to\n//\n//   %fc = tfl.fully_connected(%input, %filter, %bias)\n//           {keep_num_dims = true, weights_format = \"DEFAULT\"}\nstruct RemoveReshapeAfterFullyConnected\n    : public OpRewritePattern<TFL::ReshapeOp> {\n  using OpRewritePattern::OpRewritePattern;\n\n  LogicalResult matchAndRewrite(TFL::ReshapeOp reshape_op,\n                                PatternRewriter &rewriter) const override {\n    auto fully_connected_op = llvm::dyn_cast_or_null<TFL::FullyConnectedOp>(\n        reshape_op.input().getDefiningOp());\n    if (!fully_connected_op || fully_connected_op.getNumResults() != 1 ||\n        fully_connected_op.weights_format() != \"DEFAULT\" ||\n        fully_connected_op.keep_num_dims())\n      return failure();\n    if (!reshape_op.input().hasOneUse()) return failure();\n\n    auto input_shape = fully_connected_op.input().getType().cast<ShapedType>();\n    auto output_shape = fully_connected_op.getType(0).cast<ShapedType>();\n    auto reshape_shape = reshape_op.getType().cast<ShapedType>();\n    if (!input_shape.hasStaticShape() || !output_shape.hasStaticShape() ||\n        !reshape_shape.hasStaticShape())\n      return failure();\n\n    // Check that the reshape doesn't modify the last dimension and it restores\n    // the input (batch) dimension with the exception of the feature (last)\n    // dimension.\n    if (output_shape.getShape().empty() || reshape_shape.getShape().empty() ||\n        output_shape.getShape().back() != reshape_shape.getShape().back() ||\n        input_shape.getShape().drop_back() !=\n            reshape_shape.getShape().drop_back())\n      return failure();\n\n    llvm::SmallVector<Type, 1> output_type{reshape_op.getType()};\n    rewriter.replaceOpWithNewOp<TFL::FullyConnectedOp>(\n        reshape_op, output_type, fully_connected_op.input(),\n        fully_connected_op.filter(), fully_connected_op.bias(),\n        fully_connected_op.fused_activation_function(),\n        fully_connected_op.weights_format(), /*keep_num_dims=*/true);\n    return success();\n  }\n};\n\n// Fuses Unpack with proceeding Concatenation to Reshape if output type has\n// static shape and activation function is none. For example:\n//\n//   // %input: tensor<1x3x2xf32>\n//   %unpack:3 = \"tfl.unpack\"(%input) {axis = 1 : i32, num = 3 : i32}\n//   %res = \"tfl.concatenation\"(%unpack#0, %unpack#1, %unpack#2)\n//        {axis = -1 : i32, fused_activation_function = \"NONE\"}\n//\n// can be optimized to\n//\n//   %cst = constant dense<[1, 6]> : tensor<2xi32>\n//   %res = \"tfl.reshape\"(%input, %cst)\nstruct FuseUnpackAndConcatToReshape\n    : public OpRewritePattern<TFL::ConcatenationOp> {\n  using OpRewritePattern::OpRewritePattern;\n\n  LogicalResult matchAndRewrite(TFL::ConcatenationOp concat_op,\n                                PatternRewriter &rewriter) const override {\n    if (concat_op.fused_activation_function() != \"NONE\") {\n      return failure();\n    }\n\n    // Checks all operands come from the same unpack op.\n    auto first_operand = concat_op.values().front();\n    auto unpack_op =\n        dyn_cast_or_null<TFL::UnpackOp>(first_operand.getDefiningOp());\n    if (!unpack_op || unpack_op.getNumResults() != concat_op.getNumOperands()) {\n      return failure();\n    }\n    for (auto &index_and_value : llvm::enumerate(concat_op.values())) {\n      if (index_and_value.value() !=\n          unpack_op.getResult(index_and_value.index())) {\n        return failure();\n      }\n    }\n\n    auto output_type = concat_op.getType().cast<ShapedType>();\n    if (!output_type.hasStaticShape()) {\n      return failure();\n    }\n\n    auto new_shape_array = output_type.getShape();\n    // This is to workaround the unnecessary cast i64 -> i32.\n    SmallVector<int32_t, 4> new_shape_array_i32;\n    for (auto size : new_shape_array) {\n      new_shape_array_i32.push_back(static_cast<int32_t>(size));\n    }\n    auto new_shape = rewriter.create<TFL::ConstOp>(\n        concat_op.getLoc(),\n        DenseIntElementsAttr::get(\n            RankedTensorType::get(new_shape_array_i32.size(),\n                                  rewriter.getIntegerType(32)),\n            new_shape_array_i32));\n\n    rewriter.replaceOpWithNewOp<TFL::ReshapeOp>(concat_op, output_type,\n                                                unpack_op.input(), new_shape);\n    return success();\n  }\n};\n\nusing FuseBinaryOpToFollowingFullyConnected =\n    FuseBinaryOpToFollowingAffineOp<FullyConnectedOp>;\nusing FuseBinaryOpToFollowingDepthwiseConv2D =\n    FuseBinaryOpToFollowingAffineOp<DepthwiseConv2DOp>;\nusing FuseBinaryOpToFollowingConv2D = FuseBinaryOpToFollowingAffineOp<Conv2DOp>;\n\n// Adds canonicalization patterns to the list of patterns.\nvoid AddCanonicalizationPatterns(MLIRContext *context,\n                                 OwningRewritePatternList *patterns) {\n  for (auto *op : context->getRegisteredOperations())\n    op->getCanonicalizationPatterns(*patterns, context);\n}\n\nvoid OptimizePass::runOnFunction() {\n  OwningRewritePatternList patterns(&getContext());\n  auto *ctx = &getContext();\n  auto func = getFunction();\n\n  // Merge reshapes into fully connected ops before we start moving them past\n  // binary ops.\n  OwningRewritePatternList phase_0_patterns(&getContext());\n  phase_0_patterns.insert<RemoveReshapeAfterFullyConnected,\n                          RemoveReshapeBeforeFullyConnected>(ctx);\n  (void)applyPatternsAndFoldGreedily(func, std::move(phase_0_patterns));\n\n  // Potentially the binary ops might be fused together, like hard_swish, thus\n  // we explore these potentially first and then fuse the binary ops with the\n  // following ops in a second pattern match.\n  TFL::populateWithGenerated(patterns);\n  patterns.insert<FuseFullyConnectedAndAdd, FuseAddAndFullyConnected,\n                  FuseFullyConnectedAndMul, FuseMulAndFullyConnected,\n                  FuseFullyConnectedAndReluX<TFL::ReluOp, kRelu>,\n                  FuseFullyConnectedAndReluX<TFL::Relu6Op, kRelu6>,\n                  FuseFullyConnectedAndReluX<TFL::Relu1Op, kRelu1>>(ctx);\n  if (enable_canonicalization_) AddCanonicalizationPatterns(ctx, &patterns);\n  (void)applyPatternsAndFoldGreedily(func, std::move(patterns));\n\n  // Fuse the binary ops with the following ops.\n  OwningRewritePatternList phase_2_patterns(&getContext());\n  TFL::populateWithGenerated(phase_2_patterns);\n  phase_2_patterns.insert<\n      ScalarizeSplatConstantForAdd, ScalarizeSplatConstantForSub,\n      ScalarizeSplatConstantForMul, ScalarizeSplatConstantForDiv,\n      FuseFullyConnectedAndAdd, FuseAddAndFullyConnected,\n      FuseFullyConnectedAndMul, FuseMulAndFullyConnected,\n      FuseFullyConnectedAndReluX<TFL::ReluOp, kRelu>,\n      FuseFullyConnectedAndReluX<TFL::Relu6Op, kRelu6>,\n      FuseFullyConnectedAndReluX<TFL::Relu1Op, kRelu1>,\n      FuseBinaryOpToFollowingConv2D, FuseBinaryOpToFollowingDepthwiseConv2D,\n      FuseBinaryOpToFollowingFullyConnected, FuseConv2DAndMulWithQDQs,\n      FuseDepthwiseConv2DAndMulWithQDQs, ConvertTrivialTransposeOpToReshapeOp,\n      RemoveReshapeAfterFullyConnected, RemoveReshapeBeforeFullyConnected,\n      FuseUnpackAndConcatToReshape>(ctx);\n  if (enable_canonicalization_)\n    AddCanonicalizationPatterns(ctx, &phase_2_patterns);\n  (void)applyPatternsAndFoldGreedily(func, std::move(phase_2_patterns));\n}\n}  // namespace\n\n// Creates an instance of the TensorFlow Lite dialect Optimize pass.\nstd::unique_ptr<OperationPass<FuncOp>> CreateOptimizePass(\n    bool enable_canonicalization) {\n  return std::make_unique<OptimizePass>(enable_canonicalization);\n}\n\nstatic PassRegistration<OptimizePass> pass;\n\n}  // namespace TFL\n}  // namespace mlir\n"], "filenames": ["tensorflow/compiler/mlir/lite/transforms/optimize.cc"], "buggy_code_start_loc": [70], "buggy_code_end_loc": [70], "fixing_code_start_loc": [71], "fixing_code_end_loc": [74], "type": "CWE-476", "message": "TensorFlow is an end-to-end open source platform for machine learning. In affected versions an attacker can craft a TFLite model that would trigger a null pointer dereference, which would result in a crash and denial of service. This is caused by the MLIR optimization of `L2NormalizeReduceAxis` operator. The [implementation](https://github.com/tensorflow/tensorflow/blob/149562d49faa709ea80df1d99fc41d005b81082a/tensorflow/compiler/mlir/lite/transforms/optimize.cc#L67-L70) unconditionally dereferences a pointer to an iterator to a vector without checking that the vector has elements. We have patched the issue in GitHub commit d6b57f461b39fd1aa8c1b870f1b974aac3554955. The fix will be included in TensorFlow 2.6.0. We will also cherrypick this commit on TensorFlow 2.5.1, TensorFlow 2.4.3, and TensorFlow 2.3.4, as these are also affected and still in supported range.", "other": {"cve": {"id": "CVE-2021-37689", "sourceIdentifier": "security-advisories@github.com", "published": "2021-08-12T22:15:09.190", "lastModified": "2021-08-18T21:15:25.927", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "TensorFlow is an end-to-end open source platform for machine learning. In affected versions an attacker can craft a TFLite model that would trigger a null pointer dereference, which would result in a crash and denial of service. This is caused by the MLIR optimization of `L2NormalizeReduceAxis` operator. The [implementation](https://github.com/tensorflow/tensorflow/blob/149562d49faa709ea80df1d99fc41d005b81082a/tensorflow/compiler/mlir/lite/transforms/optimize.cc#L67-L70) unconditionally dereferences a pointer to an iterator to a vector without checking that the vector has elements. We have patched the issue in GitHub commit d6b57f461b39fd1aa8c1b870f1b974aac3554955. The fix will be included in TensorFlow 2.6.0. We will also cherrypick this commit on TensorFlow 2.5.1, TensorFlow 2.4.3, and TensorFlow 2.3.4, as these are also affected and still in supported range."}, {"lang": "es", "value": "TensorFlow es una plataforma de c\u00f3digo abierto de extremo a extremo para el aprendizaje autom\u00e1tico. En las versiones afectadas, un atacante puede dise\u00f1ar un modelo TFLite que podr\u00eda desencadenar una desreferencia de puntero null, que resultar\u00eda en un bloqueo y una denegaci\u00f3n de servicio. Esto es causado por la optimizaci\u00f3n MLIR del operador \"L2NormalizeReduceAxis\". La [implementaci\u00f3n](https://github.com/tensorflow/tensorflow/blob/149562d49faa709ea80df1d99fc41d005b81082a/tensorflow/compiler/mlir/lite/transforms/optimize.cc#L67-L70) hace desreferencia incondicional a un puntero a un iterador a un vector sin comprobar que el vector presenta elementos. Hemos parcheado el problema en el commit d6b57f461b39fd1aa8c1b870f1b974aac3554955 de GitHub. La correcci\u00f3n ser\u00e1 incluida en TensorFlow versi\u00f3n 2.6.0. Tambi\u00e9n seleccionaremos este commit en TensorFlow versi\u00f3n 2.5.1, TensorFlow versi\u00f3n 2.4.3, y TensorFlow versi\u00f3n 2.3.4, ya que estos tambi\u00e9n est\u00e1n afectados y todav\u00eda est\u00e1n en el rango de soporte."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:N/I:N/A:P", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "PARTIAL", "baseScore": 2.1}, "baseSeverity": "LOW", "exploitabilityScore": 3.9, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "security-advisories@github.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-476"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.3.0", "versionEndExcluding": "2.3.4", "matchCriteriaId": "0F83C081-51CC-415F-A8C0-0A44C75E2CD6"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.4.0", "versionEndExcluding": "2.4.3", "matchCriteriaId": "BD3F2BF8-EBA9-42BF-8F9B-D918B880B15A"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.5.0:*:*:*:*:*:*:*", "matchCriteriaId": "D03E99A7-4E3D-427D-A156-C0713E9FB02A"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.6.0:rc0:*:*:*:*:*:*", "matchCriteriaId": "70FA6E48-6C57-40CA-809F-4E3D07CBF348"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.6.0:rc1:*:*:*:*:*:*", "matchCriteriaId": "42187561-E491-434D-828C-F36701446634"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.6.0:rc2:*:*:*:*:*:*", "matchCriteriaId": "C66B61C8-450A-4C5E-9174-F970D6DEE778"}]}]}], "references": [{"url": "https://github.com/tensorflow/tensorflow/commit/d6b57f461b39fd1aa8c1b870f1b974aac3554955", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/security/advisories/GHSA-wf5p-c75w-w3wh", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/tensorflow/tensorflow/commit/d6b57f461b39fd1aa8c1b870f1b974aac3554955"}}