{"buggy_code": ["/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n// See docs in ../ops/nn_ops.cc.\n\n#define USE_EIGEN_TENSOR\n#define EIGEN_USE_THREADS\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n#define EIGEN_USE_GPU\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n#include \"tensorflow/core/kernels/conv_ops.h\"\n\n#include <string.h>\n\n#include <atomic>\n#include <map>\n#include <utility>\n#include <vector>\n\n#include \"absl/synchronization/blocking_counter.h\"\n#include \"tensorflow/core/framework/allocator.h\"\n#include \"tensorflow/core/framework/bounds_check.h\"\n#include \"tensorflow/core/framework/kernel_shape_util.h\"\n#include \"tensorflow/core/framework/numeric_op.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/register_types.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/tensor_shape.h\"\n#include \"tensorflow/core/framework/tensor_slice.h\"\n#include \"tensorflow/core/framework/types.h\"\n#include \"tensorflow/core/kernels/conv_2d.h\"\n#include \"tensorflow/core/kernels/deep_conv2d.h\"\n#include \"tensorflow/core/kernels/ops_util.h\"\n#include \"tensorflow/core/lib/core/errors.h\"\n#include \"tensorflow/core/lib/gtl/array_slice.h\"\n#include \"tensorflow/core/lib/strings/numbers.h\"\n#include \"tensorflow/core/lib/strings/str_util.h\"\n#include \"tensorflow/core/platform/logging.h\"\n#include \"tensorflow/core/platform/macros.h\"\n#include \"tensorflow/core/profiler/lib/scoped_annotation.h\"\n#include \"tensorflow/core/util/padding.h\"\n#include \"tensorflow/core/util/tensor_format.h\"\n#include \"tensorflow/core/util/use_cudnn.h\"\n\n#ifdef TENSORFLOW_USE_LIBXSMM_CONVOLUTIONS\n#include \"tensorflow/core/kernels/xsmm_conv2d.h\"\n#endif\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n#include \"tensorflow/core/kernels/conv_ops_gpu.h\"\n#include \"tensorflow/core/platform/stream_executor.h\"\n#include \"tensorflow/core/protobuf/autotuning.pb.h\"\n#include \"tensorflow/core/util/autotune_maps/conv_autotune_maps.h\"\n#include \"tensorflow/core/util/autotune_maps/conv_parameters.h\"\n#include \"tensorflow/core/util/proto/proto_utils.h\"\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n#if GOOGLE_CUDA\n#include \"tensorflow/stream_executor/gpu/gpu_asm_opts.h\"\n#include \"tensorflow/stream_executor/gpu/redzone_allocator.h\"\n#include \"tensorflow/stream_executor/tf_allocator_adapter.h\"\n#endif  // GOOGLE_CUDA\n\nnamespace tensorflow {\n\ntypedef Eigen::ThreadPoolDevice CPUDevice;\ntypedef Eigen::GpuDevice GPUDevice;\n\nnamespace {\ntemplate <typename Device, typename T>\nstruct LaunchGeneric {\n  void operator()(OpKernelContext* ctx, const Tensor& input,\n                  const Tensor& filter, int row_stride, int col_stride,\n                  int row_dilation, int col_dilation, const Padding& padding,\n                  const std::vector<int64_t>& explicit_paddings, Tensor* output,\n                  TensorFormat data_format) {\n    CHECK(data_format == FORMAT_NHWC) << \"Generic conv implementation only \"\n                                         \"supports NHWC tensor format for now.\";\n    if (filter.dim_size(0) == 1 && filter.dim_size(1) == 1 && row_stride == 1 &&\n        col_stride == 1 && (padding == SAME || padding == VALID)) {\n      // For 1x1 kernel, the 2D convolution is reduced to matrix\n      // multiplication.\n      //\n      // TODO(vrv): We should be able to call SpatialConvolution\n      // and it will produce the same result, but doing so\n      // led to NaNs during training.  Using matmul instead for now.\n      int conv_width = 1;  // Width for the convolution step.\n      for (int i = 0; i < 3; ++i) {\n        conv_width *= output->dim_size(i);\n      }\n\n      Eigen::array<Eigen::IndexPair<Eigen::DenseIndex>, 1> dim_pair;\n      dim_pair[0] = Eigen::IndexPair<Eigen::DenseIndex>(1, 0);\n      functor::MatMulConvFunctor<Device, T>()(\n          ctx->eigen_device<Device>(),\n          output->shaped<T, 2>({conv_width, filter.dim_size(3)}),\n          input.shaped<T, 2>({conv_width, filter.dim_size(2)}),\n          filter.shaped<T, 2>({filter.dim_size(2), filter.dim_size(3)}),\n          dim_pair);\n    } else if (filter.dim_size(0) == input.dim_size(1) &&\n               filter.dim_size(1) == input.dim_size(2) && row_dilation == 1 &&\n               col_dilation == 1 && padding == VALID) {\n      // If the input data and filter have the same height/width,\n      // the 2D convolution is reduced to matrix multiplication.\n      const int k =  // Length of reduction dimension.\n          filter.dim_size(0) * filter.dim_size(1) * filter.dim_size(2);\n\n      Eigen::array<Eigen::IndexPair<Eigen::DenseIndex>, 1> dim_pair;\n      dim_pair[0] = Eigen::IndexPair<Eigen::DenseIndex>(1, 0);\n      functor::MatMulConvFunctor<Device, T>()(\n          ctx->eigen_device<Device>(),\n          output->shaped<T, 2>({input.dim_size(0), filter.dim_size(3)}),\n          input.shaped<T, 2>({input.dim_size(0), k}),\n          filter.shaped<T, 2>({k, filter.dim_size(3)}), dim_pair);\n    } else {\n      if (padding == EXPLICIT) {\n        functor::SpatialConvolution<Device, T>()(\n            ctx->eigen_device<Device>(), output->tensor<T, 4>(),\n            input.tensor<T, 4>(), filter.tensor<T, 4>(), row_stride, col_stride,\n            row_dilation, col_dilation, static_cast<int>(explicit_paddings[2]),\n            static_cast<int>(explicit_paddings[3]),\n            static_cast<int>(explicit_paddings[4]),\n            static_cast<int>(explicit_paddings[5]));\n      } else {\n        functor::SpatialConvolution<Device, T>()(\n            ctx->eigen_device<Device>(), output->tensor<T, 4>(),\n            input.tensor<T, 4>(), filter.tensor<T, 4>(), row_stride, col_stride,\n            row_dilation, col_dilation, BrainPadding2EigenPadding(padding));\n      }\n    }\n  }\n};\n\n// Compute grouped 2D convolutions on CPU. Unlike grouped convolution\n// implementation in cuDNN this is faaaaaar from optimal and needs more work\n// to deliver competitive performance. Currently it exists to close the feature\n// parity gap between convolution operations on different devices.\ntemplate <typename T>\nstruct LaunchGrouped {\n  void operator()(OpKernelContext* ctx, const Tensor& input,\n                  const Tensor& filter, int row_stride, int col_stride,\n                  int row_dilation, int col_dilation, const Padding& padding,\n                  const std::vector<int64_t>& explicit_paddings, Tensor* output,\n                  TensorFormat data_format) {\n    DCHECK(data_format == FORMAT_NHWC)\n        << \"Grouped conv implementation only \"\n           \"supports NHWC tensor format for now.\";\n\n    const int64_t in_depth = input.dim_size(3);\n    const int64_t patch_depth = filter.dim_size(2);\n    const int64_t num_groups = in_depth / patch_depth;\n\n    // Shuffle input/filter tensors to have group as a leading dimension.\n    std::array<int64_t, 5> shuffle({3, 0, 1, 2, 4});\n\n    // Compute pre shuffle dimemnsions.\n    auto pre_shuffle = [&](const Tensor& tensor) -> std::array<int64, 5> {\n      return {tensor.dim_size(0), tensor.dim_size(1), tensor.dim_size(2),\n              num_groups, tensor.dim_size(3) / num_groups};\n    };\n\n    // Compute post shuffle dimemnsions.\n    auto post_shuffle = [&](const Tensor& tensor) -> std::array<int64, 5> {\n      return {num_groups, tensor.dim_size(0), tensor.dim_size(1),\n              tensor.dim_size(2), tensor.dim_size(3) / num_groups};\n    };\n\n    auto& device = ctx->eigen_device<CPUDevice>();\n\n    absl::BlockingCounter shuffles_completed(2);\n    auto on_shuffled = [&]() { shuffles_completed.DecrementCount(); };\n\n    // Shuffle input into temporary tensor.\n    Tensor input_shuffled;\n    OP_REQUIRES_OK(\n        ctx, ctx->allocate_temp(input.dtype(), TensorShape(post_shuffle(input)),\n                                &input_shuffled));\n    input_shuffled.tensor<T, 5>().device(device, on_shuffled) =\n        input.shaped<T, 5>(pre_shuffle(input)).shuffle(shuffle);\n\n    // Shuffle filter into temporary tensor.\n    Tensor filter_shuffled;\n    OP_REQUIRES_OK(ctx, ctx->allocate_temp(filter.dtype(),\n                                           TensorShape(post_shuffle(filter)),\n                                           &filter_shuffled));\n    filter_shuffled.tensor<T, 5>().device(device, on_shuffled) =\n        filter.shaped<T, 5>(pre_shuffle(filter)).shuffle(shuffle);\n\n    // Wait for the completion of input/filter shuffles.\n    shuffles_completed.Wait();\n\n    // Write group convolution results into temporary output tensor.\n    Tensor output_shuffled;\n    OP_REQUIRES_OK(ctx, ctx->allocate_temp(output->dtype(),\n                                           TensorShape(post_shuffle(*output)),\n                                           &output_shuffled));\n\n    for (int64_t i = 0; i < num_groups; ++i) {\n      // TODO(ezhulenev): Run this loop using `parallelFor` (regular parallelFor\n      // will lead to deadlock, SpatialConvolution has to use async Eigen\n      // assignment). This requires small changes to Eigen to support async\n      // exeuction for tensor chipping operation.\n\n      // TODO(ezhulenev): Grouped convolution should also support 1x1 filter\n      // optimization.\n\n      auto input_slice = input_shuffled.tensor<T, 5>().template chip<0>(i);\n      auto filter_slice = filter_shuffled.tensor<T, 5>().template chip<0>(i);\n      auto output_slice = output_shuffled.tensor<T, 5>().template chip<0>(i);\n\n      if (padding == EXPLICIT) {\n        functor::SpatialConvolution<CPUDevice, T>()(\n            ctx->eigen_device<CPUDevice>(), output_slice, input_slice,\n            filter_slice, row_stride, col_stride, row_dilation, col_dilation,\n            static_cast<int>(explicit_paddings[2]),\n            static_cast<int>(explicit_paddings[3]),\n            static_cast<int>(explicit_paddings[4]),\n            static_cast<int>(explicit_paddings[5]));\n      } else {\n        functor::SpatialConvolution<CPUDevice, T>()(\n            ctx->eigen_device<CPUDevice>(), output_slice, input_slice,\n            filter_slice, row_stride, col_stride, row_dilation, col_dilation,\n            BrainPadding2EigenPadding(padding));\n      }\n    }\n\n    // Shuffle temporary output back into pre-shuffled shape.\n    std::array<int64_t, 5> rev_shuffle({1, 2, 3, 0, 4});\n    output->shaped<T, 5>(pre_shuffle(*output)).device(device) =\n        output_shuffled.tensor<T, 5>().shuffle(rev_shuffle);\n  }\n};\n\n}  // namespace\n\ntemplate <typename T>\nstruct LaunchConv2DOp<CPUDevice, T> {\n  void operator()(OpKernelContext* ctx, bool use_cudnn, bool cudnn_use_autotune,\n                  const Tensor& input, const Tensor& filter, int row_dilation,\n                  int col_dilation, int row_stride, int col_stride,\n                  const Padding& padding,\n                  const std::vector<int64_t>& explicit_paddings, Tensor* output,\n                  TensorFormat data_format) {\n    if (data_format != FORMAT_NHWC) {\n      ctx->SetStatus(errors::Unimplemented(\n          \"The Conv2D op currently only supports the NHWC tensor format on the \"\n          \"CPU. The op was given the format: \",\n          ToString(data_format)));\n      return;\n    }\n\n    for (int64_t explicit_padding : explicit_paddings) {\n      if (!FastBoundsCheck(explicit_padding, std::numeric_limits<int>::max())) {\n        ctx->SetStatus(errors::InvalidArgument(\"filter too large\"));\n        return;\n      }\n    }\n\n    const int64_t in_depth = input.dim_size(3);\n    const int64_t out_depth = output->dim_size(3);\n    const int64_t patch_depth = filter.dim_size(2);\n\n    if (patch_depth <= 0) {\n      ctx->SetStatus(errors::InvalidArgument(\n          \"filter depth must be stricly positive, got \", patch_depth));\n      return;\n    }\n    if (in_depth % patch_depth != 0) {\n      ctx->SetStatus(errors::InvalidArgument(\n          \"input depth must be evenly divisible by filter depth: \", in_depth,\n          \" vs \", patch_depth));\n      return;\n    }\n    if (filter.NumElements() <= 0) {\n      ctx->SetStatus(\n          errors::InvalidArgument(\"filter must not have zero elements \"\n                                  \"(i.e. all dimensions must be non-zero)\"));\n      return;\n    }\n\n    const int64_t num_groups = in_depth / patch_depth;\n    if (num_groups <= 0) {\n      ctx->SetStatus(errors::InvalidArgument(\n          \"number of groups must be stricly positive, got \", num_groups));\n      return;\n    }\n    if (out_depth % num_groups != 0 || out_depth < num_groups) {\n      ctx->SetStatus(errors::InvalidArgument(\n          \"output depth must be evenly divisible by number of groups: \",\n          out_depth, \" vs \", num_groups));\n      return;\n    }\n\n    if (in_depth != patch_depth) {\n      LaunchGrouped<T>()(ctx, input, filter, row_stride, col_stride,\n                         row_dilation, col_dilation, padding, explicit_paddings,\n                         output, data_format);\n    } else {\n      LaunchGeneric<CPUDevice, T>()(ctx, input, filter, row_stride, col_stride,\n                                    row_dilation, col_dilation, padding,\n                                    explicit_paddings, output, data_format);\n    }\n  }\n};\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\ntemplate <>\nstruct LaunchConv2DOp<GPUDevice, int32> {\n  void operator()(OpKernelContext* ctx, bool use_cudnn, bool cudnn_use_autotune,\n                  const Tensor& input, const Tensor& filter, int row_dilation,\n                  int col_dilation, int row_stride, int col_stride,\n                  const Padding& padding,\n                  const std::vector<int64_t>& explicit_paddings, Tensor* output,\n                  TensorFormat data_format) {\n    if (data_format != FORMAT_NHWC) {\n      ctx->SetStatus(\n          errors::Unimplemented(\"The Conv2D op currently only supports the \"\n                                \"NHWC tensor format for integer types. \"\n                                \"The op was given the format: \",\n                                ToString(data_format)));\n      return;\n    }\n    const int64_t in_depth = GetTensorDim(input, data_format, 'C');\n    OP_REQUIRES(ctx, in_depth == filter.dim_size(2),\n                errors::Unimplemented(\n                    \"The Conv2D op currently does not support grouped \"\n                    \"convolutions for integer types. A grouped convolution was \"\n                    \"attempted to be run because the input depth of \",\n                    in_depth, \" does not match the filter input depth of \",\n                    filter.dim_size(2)));\n    OP_REQUIRES(\n        ctx, filter.NumElements() > 0,\n        errors::InvalidArgument(\"filter must not have zero elements \"\n                                \"(i.e. all dimensions must be non-zero)\"));\n\n    for (int64_t explicit_padding : explicit_paddings) {\n      if (!FastBoundsCheck(explicit_padding, std::numeric_limits<int>::max())) {\n        ctx->SetStatus(errors::InvalidArgument(\"filter too large\"));\n        return;\n      }\n    }\n    LaunchGeneric<GPUDevice, int32>()(\n        ctx, input, filter, row_stride, col_stride, row_dilation, col_dilation,\n        padding, explicit_paddings, output, data_format);\n  }\n};\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\ntemplate <typename Device, typename T>\nclass LaunchDeepConvOp {\n public:\n  static bool Run(OpKernelContext* ctx, const Tensor& input,\n                  const Tensor& filter, int batch, int input_rows,\n                  int input_cols, int in_depth, int filter_rows,\n                  int filter_cols, int pad_rows, int pad_cols, int out_rows,\n                  int /*out_cols*/, int /*out_depth*/, int /*dilation_rows*/,\n                  int /*dilation_cols*/, int /*stride_rows*/,\n                  int /*stride_cols*/, Tensor* /*output*/,\n                  TensorFormat /*data_format*/) {\n    return false;\n  }\n};\n\n// Conditionally launches DeepConv operation based on convolution parameters.\ntemplate <>\nclass LaunchDeepConvOp<CPUDevice, float> {\n public:\n  static bool Run(OpKernelContext* ctx, const Tensor& input,\n                  const Tensor& filter, int batch, int input_rows,\n                  int input_cols, int in_depth, int filter_rows,\n                  int filter_cols, int pad_rows, int pad_cols, int out_rows,\n                  int out_cols, int out_depth, int dilation_rows,\n                  int dilation_cols, int stride_rows, int stride_cols,\n                  Tensor* output, TensorFormat data_format) {\n    if (data_format != FORMAT_NHWC || dilation_rows != 1 ||\n        dilation_cols != 1 ||\n        !CanUseDeepConv2D(stride_rows, stride_cols, filter_rows, filter_cols,\n                          in_depth, out_depth, out_rows, out_cols)) {\n      return false;\n    }\n\n    Conv2DArgs args;\n    args.batch = batch;\n    args.in_rows = input_rows;\n    args.in_cols = input_cols;\n    args.in_depth = in_depth;\n    args.filter_rows = filter_rows;\n    args.filter_cols = filter_cols;\n    args.pad_rows = pad_rows;\n    args.pad_cols = pad_cols;\n    args.out_rows = out_rows;\n    args.out_cols = out_cols;\n    args.out_depth = out_depth;\n\n    auto input_ptr = input.template flat<float>().data();\n    auto filter_ptr = filter.template flat<float>().data();\n    auto output_ptr = output->template flat<float>().data();\n\n    functor::DeepConv2D<CPUDevice, float>()(ctx, args, input_ptr, filter_ptr,\n                                            output_ptr);\n    return true;\n  }\n};\n\n#ifdef TENSORFLOW_USE_LIBXSMM_CONVOLUTIONS\ntemplate <typename Device, typename T>\nclass LaunchXsmmConvOp {\n public:\n  static bool Run(OpKernelContext* ctx, const Tensor& input,\n                  const Tensor& filter, int batch, int input_rows,\n                  int input_cols, int in_depth, int filter_rows,\n                  int filter_cols, int pad_rows, int pad_cols, int out_rows,\n                  int out_cols, int out_depth, int stride_rows, int stride_cols,\n                  int dilation_rows, int dilation_cols, Tensor* output,\n                  TensorFormat data_format) {\n    return false;\n  }\n};\n\ntemplate <>\nclass LaunchXsmmConvOp<CPUDevice, float> {\n public:\n  static bool Run(OpKernelContext* ctx, const Tensor& input,\n                  const Tensor& filter, int batch, int input_rows,\n                  int input_cols, int in_depth, int filter_rows,\n                  int filter_cols, int pad_rows, int pad_cols, int out_rows,\n                  int out_cols, int out_depth, int dilation_rows,\n                  int dilation_cols, int stride_rows, int stride_cols,\n                  Tensor* output, TensorFormat data_format) {\n    auto num_threads =\n        ctx->device()->tensorflow_cpu_worker_threads()->num_threads;\n    // See libxsmm_dnn.h for this struct definition.\n    libxsmm_dnn_conv_desc desc;\n    desc.N = batch;\n    desc.C = in_depth;\n    desc.H = input_rows;\n    desc.W = input_cols;\n    desc.K = out_depth;\n    desc.R = filter_rows;\n    desc.S = filter_cols;\n    desc.u = stride_rows;\n    desc.v = stride_cols;\n    desc.pad_h = pad_rows;\n    desc.pad_w = pad_cols;\n    desc.pad_h_in = 0;\n    desc.pad_w_in = 0;\n    desc.pad_h_out = 0;\n    desc.pad_w_out = 0;\n    desc.threads = num_threads;\n    desc.algo = LIBXSMM_DNN_CONV_ALGO_DIRECT;\n    desc.buffer_format = LIBXSMM_DNN_TENSOR_FORMAT_NHWC;\n    desc.filter_format = LIBXSMM_DNN_TENSOR_FORMAT_LIBXSMM;\n    desc.fuse_ops = LIBXSMM_DNN_CONV_FUSE_NONE;\n    desc.options = LIBXSMM_DNN_CONV_OPTION_OVERWRITE;\n    desc.datatype_out = LIBXSMM_DNN_DATATYPE_F32;\n    desc.datatype_in = LIBXSMM_DNN_DATATYPE_F32;\n    if (dilation_rows != 1 || dilation_cols != 1 ||\n        !CanUseXsmmConv2D(desc, data_format)) {\n      return false;\n    }\n\n    auto input_ptr = input.template flat<float>().data();\n    auto filter_ptr = filter.template flat<float>().data();\n    auto output_ptr = output->template flat<float>().data();\n\n    bool success = functor::XsmmFwdConv2D<CPUDevice, float>()(\n        ctx, desc, input_ptr, filter_ptr, output_ptr);\n    return success;\n  }\n};\n#endif\n\n#define TF_REQUIRES(EXP, STATUS)                \\\n  do {                                          \\\n    if (!TF_PREDICT_TRUE(EXP)) return (STATUS); \\\n  } while (false)\n\nStatus InitConv2DParameters(const OpKernelConstruction* context,\n                            Conv2DParameters* params) {\n  TF_RETURN_IF_ERROR(context->GetAttr(\"dilations\", &params->dilations));\n  TF_RETURN_IF_ERROR(context->GetAttr(\"strides\", &params->strides));\n  TF_RETURN_IF_ERROR(context->GetAttr(\"padding\", &params->padding));\n  if (context->HasAttr(\"explicit_paddings\")) {\n    TF_RETURN_IF_ERROR(\n        context->GetAttr(\"explicit_paddings\", &params->explicit_paddings));\n  }\n  string data_format_string;\n  TF_RETURN_IF_ERROR(context->GetAttr(\"data_format\", &data_format_string));\n  TF_REQUIRES(FormatFromString(data_format_string, &params->data_format),\n              errors::InvalidArgument(\"Invalid data format\"));\n\n  const auto& strides = params->strides;\n  const auto& dilations = params->dilations;\n  const auto& data_format = params->data_format;\n\n  TF_REQUIRES(dilations.size() == 4,\n              errors::InvalidArgument(\"Sliding window dilations field must \"\n                                      \"specify 4 dimensions\"));\n  TF_REQUIRES(strides.size() == 4,\n              errors::InvalidArgument(\"Sliding window strides field must \"\n                                      \"specify 4 dimensions\"));\n  const int64_t stride_n = GetTensorDim(strides, data_format, 'N');\n  const int64_t stride_c = GetTensorDim(strides, data_format, 'C');\n  const int64_t stride_h = GetTensorDim(strides, data_format, 'H');\n  const int64_t stride_w = GetTensorDim(strides, data_format, 'W');\n  TF_REQUIRES(\n      stride_n == 1 && stride_c == 1,\n      errors::Unimplemented(\"Current implementation does not yet support \"\n                            \"strides in the batch and depth dimensions.\"));\n  TF_REQUIRES(stride_h > 0 && stride_w > 0,\n              errors::InvalidArgument(\n                  \"Row and column strides should be larger than 0.\"));\n\n  const int64_t dilation_n = GetTensorDim(dilations, data_format, 'N');\n  const int64_t dilation_c = GetTensorDim(dilations, data_format, 'C');\n  const int64_t dilation_h = GetTensorDim(dilations, data_format, 'H');\n  const int64_t dilation_w = GetTensorDim(dilations, data_format, 'W');\n  TF_REQUIRES(\n      dilation_n == 1 && dilation_c == 1,\n      errors::Unimplemented(\"Current implementation does not yet support \"\n                            \"dilations in the batch and depth dimensions.\"));\n  TF_REQUIRES(\n      dilation_h > 0 && dilation_w > 0,\n      errors::InvalidArgument(\"Dilated rates should be larger than 0.\"));\n\n  TF_RETURN_IF_ERROR(CheckValidPadding(params->padding,\n                                       params->explicit_paddings,\n                                       /*num_dims=*/4, data_format));\n\n  return OkStatus();\n}\n\nStatus ComputeConv2DDimension(const Conv2DParameters& params,\n                              const Tensor& input, const Tensor& filter,\n                              Conv2DDimensions* dimensions) {\n  // Check that 2D convolution input and filter have exactly 4 dimensions.\n  TF_REQUIRES(input.dims() == 4,\n              errors::InvalidArgument(\"input must be 4-dimensional\",\n                                      input.shape().DebugString()));\n  TF_REQUIRES(filter.dims() == 4,\n              errors::InvalidArgument(\"filter must be 4-dimensional: \",\n                                      filter.shape().DebugString()));\n  for (int i = 0; i < 3; i++) {\n    TF_REQUIRES(\n        FastBoundsCheck(filter.dim_size(i), std::numeric_limits<int>::max()),\n        errors::InvalidArgument(\"filter too large\"));\n  }\n\n  // The last dimension for input is in_depth. Check that it is the same as the\n  // filter's in_depth or it is evenly divisible by filter's in_depth.\n  const int64_t in_depth_raw = GetTensorDim(input, params.data_format, 'C');\n  const int64_t patch_depth_raw = filter.dim_size(2);\n  TF_REQUIRES(FastBoundsCheck(in_depth_raw, std::numeric_limits<int>::max()),\n              errors::InvalidArgument(\"Input depth too large\"));\n  TF_REQUIRES(FastBoundsCheck(patch_depth_raw, std::numeric_limits<int>::max()),\n              errors::InvalidArgument(\"Patch depth too large\"));\n  const int in_depth = static_cast<int>(in_depth_raw);\n  const int patch_depth = static_cast<int>(patch_depth_raw);\n  TF_REQUIRES(patch_depth > 0,\n              errors::InvalidArgument(\n                  \"filter depth must be stricly positive, got \", patch_depth));\n  TF_REQUIRES(in_depth % patch_depth == 0,\n              errors::InvalidArgument(\n                  \"input depth must be evenly divisible by filter depth: \",\n                  in_depth, \" vs \", patch_depth));\n\n  // The last dimension for filter is out_depth.\n  const int out_depth = static_cast<int>(filter.dim_size(3));\n\n  // The second dimension for input is rows/height.\n  // The first dimension for filter is rows/height.\n  const int64_t input_rows_raw = GetTensorDim(input, params.data_format, 'H');\n  TF_REQUIRES(FastBoundsCheck(input_rows_raw, std::numeric_limits<int>::max()),\n              errors::InvalidArgument(\"Input rows too large\"));\n  const int input_rows = static_cast<int>(input_rows_raw);\n  const int filter_rows = static_cast<int>(filter.dim_size(0));\n\n  // The third dimension for input is columns/width.\n  // The second dimension for filter is columns/width.\n  const int64_t input_cols_raw = GetTensorDim(input, params.data_format, 'W');\n  TF_REQUIRES(FastBoundsCheck(input_cols_raw, std::numeric_limits<int>::max()),\n              errors::InvalidArgument(\"Input cols too large\"));\n  const int input_cols = static_cast<int>(input_cols_raw);\n  const int filter_cols = static_cast<int>(filter.dim_size(1));\n\n  // The first dimension for input is batch.\n  const int64_t batch_raw = GetTensorDim(input, params.data_format, 'N');\n  TF_REQUIRES(FastBoundsCheck(batch_raw, std::numeric_limits<int>::max()),\n              errors::InvalidArgument(\"batch is too large\"));\n  const int batch = static_cast<int>(batch_raw);\n\n  // Take the stride and dilation from the second and third dimensions only (we\n  // do not support striding or dilation on the batch or depth dimension).\n  const int stride_rows = GetTensorDim(params.strides, params.data_format, 'H');\n  const int stride_cols = GetTensorDim(params.strides, params.data_format, 'W');\n  const int dilation_rows =\n      GetTensorDim(params.dilations, params.data_format, 'H');\n  const int dilation_cols =\n      GetTensorDim(params.dilations, params.data_format, 'W');\n\n  int64_t pad_rows_before, pad_rows_after, pad_cols_before, pad_cols_after;\n  if (params.padding == Padding::EXPLICIT) {\n    GetExplicitPaddingForDim(params.explicit_paddings, params.data_format, 'H',\n                             &pad_rows_before, &pad_rows_after);\n    GetExplicitPaddingForDim(params.explicit_paddings, params.data_format, 'W',\n                             &pad_cols_before, &pad_cols_after);\n  }\n\n  // Compute windowed output sizes for rows and columns.\n  int64_t out_rows = 0, out_cols = 0;\n  TF_RETURN_IF_ERROR(GetWindowedOutputSizeVerboseV2(\n      input_rows, filter_rows, dilation_rows, stride_rows, params.padding,\n      &out_rows, &pad_rows_before, &pad_rows_after));\n  TF_RETURN_IF_ERROR(GetWindowedOutputSizeVerboseV2(\n      input_cols, filter_cols, dilation_cols, stride_cols, params.padding,\n      &out_cols, &pad_cols_before, &pad_cols_after));\n\n  dimensions->batch = batch;\n  dimensions->input_rows = input_rows;\n  dimensions->input_cols = input_cols;\n  dimensions->in_depth = in_depth;\n  dimensions->filter_rows = filter_rows;\n  dimensions->filter_cols = filter_cols;\n  dimensions->patch_depth = patch_depth;\n  dimensions->out_depth = out_depth;\n  dimensions->stride_rows = stride_rows;\n  dimensions->stride_cols = stride_cols;\n  dimensions->dilation_rows = dilation_rows;\n  dimensions->dilation_cols = dilation_cols;\n  dimensions->out_rows = out_rows;\n  dimensions->out_cols = out_cols;\n  dimensions->pad_rows_before = pad_rows_before;\n  dimensions->pad_rows_after = pad_rows_after;\n  dimensions->pad_cols_before = pad_cols_before;\n  dimensions->pad_cols_after = pad_cols_after;\n\n  return OkStatus();\n}\n\n#undef TF_REQUIRES\n\ntemplate <typename Device, typename T>\nclass Conv2DOp : public BinaryOp<T> {\n public:\n  explicit Conv2DOp(OpKernelConstruction* context) : BinaryOp<T>(context) {\n    OP_REQUIRES_OK(context, InitConv2DParameters(context, &params_));\n\n    OP_REQUIRES_OK(context, context->GetAttr(\"use_cudnn_on_gpu\", &use_cudnn_));\n    cudnn_use_autotune_ = CudnnUseAutotune();\n  }\n\n  void Compute(OpKernelContext* context) override {\n    // Input tensor is of the following dimensions:\n    // [ batch, in_rows, in_cols, in_depth ]\n    const Tensor& input = context->input(0);\n\n    // Input filter is of the following dimensions:\n    // [ filter_rows, filter_cols, in_depth, out_depth]\n    const Tensor& filter = context->input(1);\n\n    Conv2DDimensions dimensions;\n    OP_REQUIRES_OK(context,\n                   ComputeConv2DDimension(params_, input, filter, &dimensions));\n\n    TensorShape out_shape = ShapeFromFormat(\n        params_.data_format, dimensions.batch, dimensions.out_rows,\n        dimensions.out_cols, dimensions.out_depth);\n\n    // Output tensor is of the following dimensions:\n    // [ in_batch, out_rows, out_cols, out_depth ]\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(0, out_shape, &output));\n\n    VLOG(2) << \"Conv2D: in_depth = \" << dimensions.in_depth\n            << \", patch_depth = \" << dimensions.patch_depth\n            << \", input_cols = \" << dimensions.input_cols\n            << \", filter_cols = \" << dimensions.filter_cols\n            << \", input_rows = \" << dimensions.input_rows\n            << \", filter_rows = \" << dimensions.filter_rows\n            << \", stride_rows = \" << dimensions.stride_rows\n            << \", stride_cols = \" << dimensions.stride_cols\n            << \", dilation_rows = \" << dimensions.dilation_rows\n            << \", dilation_cols = \" << dimensions.dilation_cols\n            << \", out_depth = \" << dimensions.out_depth;\n\n    // If there is nothing to compute, return.\n    if (out_shape.num_elements() == 0) {\n      return;\n    }\n\n#ifdef TENSORFLOW_USE_LIBXSMM_CONVOLUTIONS\n    if (params_.padding != EXPLICIT &&\n        LaunchXsmmConvOp<Device, T>::Run(\n            context, input, filter, dimensions.batch, dimensions.input_rows,\n            dimensions.input_cols, dimensions.in_depth, dimensions.filter_rows,\n            dimensions.filter_cols, dimensions.pad_rows_before,\n            dimensions.pad_cols_before, dimensions.out_rows,\n            dimensions.out_cols, dimensions.out_depth, dimensions.dilation_rows,\n            dimensions.dilation_cols, dimensions.stride_rows,\n            dimensions.stride_cols, output, params_.data_format)) {\n      return;\n    }\n#endif\n\n    if (params_.padding != EXPLICIT &&\n        LaunchDeepConvOp<Device, T>::Run(\n            context, input, filter, dimensions.batch, dimensions.input_rows,\n            dimensions.input_cols, dimensions.in_depth, dimensions.filter_rows,\n            dimensions.filter_cols, dimensions.pad_rows_before,\n            dimensions.pad_cols_before, dimensions.out_rows,\n            dimensions.out_cols, dimensions.out_depth, dimensions.dilation_rows,\n            dimensions.dilation_cols, dimensions.stride_rows,\n            dimensions.stride_cols, output, params_.data_format)) {\n      return;\n    }\n\n    launcher_(context, use_cudnn_, cudnn_use_autotune_, input, filter,\n              dimensions.dilation_rows, dimensions.dilation_cols,\n              dimensions.stride_rows, dimensions.stride_cols, params_.padding,\n              params_.explicit_paddings, output, params_.data_format);\n  }\n\n private:\n  Conv2DParameters params_;\n  bool use_cudnn_;\n  bool cudnn_use_autotune_;\n\n  LaunchConv2DOp<Device, T> launcher_;\n\n  TF_DISALLOW_COPY_AND_ASSIGN(Conv2DOp);\n};\n\n#define REGISTER_CPU(T)                                         \\\n  REGISTER_KERNEL_BUILDER(                                      \\\n      Name(\"Conv2D\").Device(DEVICE_CPU).TypeConstraint<T>(\"T\"), \\\n      Conv2DOp<CPUDevice, T>);\n\n// If we're using the alternative GEMM-based implementation of Conv2D for the\n// CPU implementation, don't register this EigenTensor-based version.\n#if !defined(USE_GEMM_FOR_CONV)\nTF_CALL_half(REGISTER_CPU);\nTF_CALL_float(REGISTER_CPU);\nTF_CALL_double(REGISTER_CPU);\nTF_CALL_int32(REGISTER_CPU);\n#endif  // USE_GEMM_FOR_CONV\n\n// To be used inside depthwise_conv_op.cc.\ntemplate struct LaunchConv2DOp<CPUDevice, Eigen::bfloat16>;\ntemplate struct LaunchConv2DOp<CPUDevice, Eigen::half>;\ntemplate struct LaunchConv2DOp<CPUDevice, float>;\ntemplate struct LaunchConv2DOp<CPUDevice, double>;\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\nint64_t GetDnnWorkspaceLimit(const string& envvar_in_mb,\n                             int64_t default_value_in_bytes) {\n  const char* workspace_limit_in_mb_str = getenv(envvar_in_mb.c_str());\n  if (workspace_limit_in_mb_str != nullptr &&\n      strcmp(workspace_limit_in_mb_str, \"\") != 0) {\n    int64_t scratch_limit_in_mb = -1;\n    if (strings::safe_strto64(workspace_limit_in_mb_str,\n                              &scratch_limit_in_mb)) {\n      return scratch_limit_in_mb * (1 << 20);\n    } else {\n      LOG(WARNING) << \"Invalid value for env-var \" << envvar_in_mb << \": \"\n                   << workspace_limit_in_mb_str;\n    }\n  }\n  return default_value_in_bytes;\n}\n\nint64_t GetDnnWorkspaceLimitOrDefault() {\n  return GetDnnWorkspaceLimit(\"TF_CUDNN_WORKSPACE_LIMIT_IN_MB\",\n                              1LL << 33);  // 8GB by default\n}\n\ntemplate <typename T>\nvoid LaunchConv2DOp<GPUDevice, T>::operator()(\n    OpKernelContext* ctx, bool use_cudnn, bool cudnn_use_autotune,\n    const Tensor& input_param, const Tensor& filter, int row_dilation,\n    int col_dilation, int row_stride, int col_stride, const Padding& padding,\n    const std::vector<int64_t>& explicit_paddings, Tensor* output,\n    TensorFormat data_format) {\n  using se::dnn::AlgorithmConfig;\n  using se::dnn::AlgorithmDesc;\n  using se::dnn::ProfileResult;\n  auto* stream = ctx->op_device_context()->stream();\n  OP_REQUIRES(ctx, stream, errors::Internal(\"No GPU stream available.\"));\n\n  if (!use_cudnn) {\n    ctx->SetStatus(\n        errors::Unimplemented(\"Conv2D for GPU is not currently supported \"\n                              \"without cudnn\"));\n    return;\n  }\n\n  Tensor input = input_param;\n  const int64_t in_batch = GetTensorDim(input, data_format, 'N');\n  int64_t in_rows = GetTensorDim(input, data_format, 'H');\n  int64_t in_cols = GetTensorDim(input, data_format, 'W');\n  const int64_t in_depths = GetTensorDim(input, data_format, 'C');\n  const int64_t patch_rows = filter.dim_size(0);\n  const int64_t patch_cols = filter.dim_size(1);\n  const int64_t patch_depths = filter.dim_size(2);\n\n  OP_REQUIRES(\n      ctx, filter.NumElements() > 0,\n      errors::InvalidArgument(\"filter must not have zero elements \"\n                              \"(i.e. all dimensions must be non-zero)\"));\n\n  // If the filter in-depth (patch_depths) is 1 and smaller than the input\n  // depth, it's a depthwise convolution. More generally, if the filter in-depth\n  // divides but is smaller than the input depth, it is a grouped convolution.\n  bool is_grouped_convolution = patch_depths != in_depths;\n  if (patch_rows == 1 && patch_cols == 1 && !is_grouped_convolution &&\n      row_dilation == 1 && col_dilation == 1 && row_stride == 1 &&\n      col_stride == 1 && data_format == FORMAT_NHWC &&\n      (padding == VALID || padding == SAME)) {\n    // 1x1 filter, so call cublas directly.\n    const uint64 m = in_batch * in_rows * in_cols;\n    const uint64 k = patch_depths;\n    const uint64 n = filter.dim_size(3);\n\n    auto a_ptr = AsDeviceMemory(input.template flat<T>().data(),\n                                input.template flat<T>().size());\n    auto b_ptr = AsDeviceMemory(filter.template flat<T>().data(),\n                                filter.template flat<T>().size());\n    auto c_ptr = AsDeviceMemory(output->template flat<T>().data(),\n                                output->template flat<T>().size());\n\n    auto no_transpose = se::blas::Transpose::kNoTranspose;\n    OP_REQUIRES_OK(\n        ctx, stream->ThenBlasGemm(no_transpose, no_transpose, n, m, k, b_ptr, n,\n                                  a_ptr, k, &c_ptr, n,\n                                  se::blas::kDefaultComputePrecision));\n    return;\n  } else if (patch_rows == in_rows && patch_cols == in_cols &&\n             !is_grouped_convolution && row_dilation == 1 &&\n             col_dilation == 1 && padding == VALID &&\n             data_format == FORMAT_NHWC) {\n    // The input data and filter have the same height/width, so call cublas\n    // directly.\n    const uint64 m = in_batch;\n    const uint64 k = patch_rows * patch_cols * patch_depths;\n    const uint64 n = filter.dim_size(3);\n\n    auto a_ptr = AsDeviceMemory(input.template flat<T>().data(),\n                                input.template flat<T>().size());\n    auto b_ptr = AsDeviceMemory(filter.template flat<T>().data(),\n                                filter.template flat<T>().size());\n    auto c_ptr = AsDeviceMemory(output->template flat<T>().data(),\n                                output->template flat<T>().size());\n\n    auto no_transpose = se::blas::Transpose::kNoTranspose;\n    OP_REQUIRES_OK(\n        ctx, stream->ThenBlasGemm(no_transpose, no_transpose, n, m, k, b_ptr, n,\n                                  a_ptr, k, &c_ptr, n,\n                                  se::blas::kDefaultComputePrecision));\n    return;\n  }\n\n#if GOOGLE_CUDA\n  const bool compute_in_nhwc = ComputeInNhwcEnabled(DataTypeToEnum<T>::value,\n                                                    stream, /*is_conv2d=*/true);\n#else\n  // fast NHWC implementation is a CUDA only feature\n  const bool compute_in_nhwc = false;\n#endif\n\n  // We only do one directional conversion: NHWC->NCHW. We never convert in the\n  // other direction. Grappler layout optimizer selects preferred layout and\n  // adds necessary annotations to the graph.\n  // TODO(ezhulenev): Convert in other direction for fp16?\n  const TensorFormat compute_data_format =\n      (compute_in_nhwc && data_format == FORMAT_NHWC) ? FORMAT_NHWC\n                                                      : FORMAT_NCHW;\n\n  VLOG(3) << \"Compute Conv2D with cuDNN:\"\n          << \" data_format=\" << ToString(data_format)\n          << \" compute_data_format=\" << ToString(compute_data_format);\n\n  const int64_t out_batch = GetTensorDim(*output, data_format, 'N');\n  const int64_t out_rows = GetTensorDim(*output, data_format, 'H');\n  const int64_t out_cols = GetTensorDim(*output, data_format, 'W');\n  const int64_t out_depths = GetTensorDim(*output, data_format, 'C');\n  int64_t padding_top = -1, padding_bottom = -1;\n  int64_t padding_left = -1, padding_right = -1;\n  if (padding == EXPLICIT) {\n    GetExplicitPaddingForDim(explicit_paddings, data_format, 'H', &padding_top,\n                             &padding_bottom);\n    GetExplicitPaddingForDim(explicit_paddings, data_format, 'W', &padding_left,\n                             &padding_right);\n  }\n  int64_t out_rows_check, out_cols_check;\n  Status status = GetWindowedOutputSizeVerboseV2(\n      in_rows, patch_rows, row_dilation, row_stride, padding, &out_rows_check,\n      &padding_top, &padding_bottom);\n  // The status is guaranteed to be OK because we checked the output and padding\n  // was valid earlier.\n  TF_CHECK_OK(status);\n  DCHECK_EQ(out_rows, out_rows_check);\n  status = GetWindowedOutputSizeVerboseV2(in_cols, patch_cols, col_dilation,\n                                          col_stride, padding, &out_cols_check,\n                                          &padding_left, &padding_right);\n  TF_CHECK_OK(status);\n  DCHECK_EQ(out_cols, out_cols_check);\n\n  const int64_t common_padding_rows = std::min(padding_top, padding_bottom);\n  const int64_t common_padding_cols = std::min(padding_left, padding_right);\n  if (padding_top != padding_bottom || padding_left != padding_right) {\n    // cuDNN only supports padding the same amount on the left and right sides,\n    // and on the top and bottom sides. So we manually create a new padded\n    // input tensor such that we can pass it to cuDNN.\n    VLOG(4) << \"Pad input tensor:\"\n            << \" padding_top=\" << padding_top\n            << \" padding_bottom=\" << padding_bottom\n            << \" padding_left=\" << padding_left\n            << \" padding_right=\" << padding_right;\n\n    // TODO(reedwm): In some cases, we can avoid an allocation even if the two\n    // padding sides are different. For example, if the input is 2x2, the filter\n    // is 1x1, the stride is 2, and the padding is (1, 0, 1, 0), the result is\n    // equivalent to as if the padding is (1, 1, 1, 1). Changing the padding in\n    // such a way would allow us to avoid the allocation.\n    Tensor transformed_input;\n    const int64_t padding_rows_diff = std::abs(padding_bottom - padding_top);\n    const int64_t padding_cols_diff = std::abs(padding_right - padding_left);\n    const int64_t new_in_rows = in_rows + padding_rows_diff;\n    const int64_t new_in_cols = in_cols + padding_cols_diff;\n    OP_REQUIRES_OK(ctx, ctx->allocate_temp(\n                            DataTypeToEnum<T>::value,\n                            ShapeFromFormat(data_format, in_batch, new_in_rows,\n                                            new_in_cols, in_depths),\n                            &transformed_input));\n\n    const int64_t input_pad_top = padding_top - common_padding_rows;\n    const int64_t input_pad_bottom = padding_bottom - common_padding_rows;\n    const int64_t input_pad_left = padding_left - common_padding_cols;\n    const int64_t input_pad_right = padding_right - common_padding_cols;\n    bool in_bounds =\n        FastBoundsCheck(input_pad_top, std::numeric_limits<int>::max()) &&\n        FastBoundsCheck(input_pad_bottom, std::numeric_limits<int>::max()) &&\n        FastBoundsCheck(input_pad_left, std::numeric_limits<int>::max()) &&\n        FastBoundsCheck(input_pad_right, std::numeric_limits<int>::max());\n    if (!in_bounds) {\n      ctx->SetStatus(errors::InvalidArgument(\"Padding is too large.\"));\n      return;\n    }\n    functor::PadInput<GPUDevice, T, int, 4>()(\n        ctx->eigen_device<GPUDevice>(), To32Bit(input_param.tensor<T, 4>()),\n        {{static_cast<int>(input_pad_top), static_cast<int>(input_pad_left)}},\n        {{static_cast<int>(input_pad_bottom),\n          static_cast<int>(input_pad_right)}},\n        To32Bit(transformed_input.tensor<T, 4>()), data_format, T{});\n\n    input = transformed_input;\n    in_rows = new_in_rows;\n    in_cols = new_in_cols;\n  }\n\n  if (data_format == FORMAT_NHWC && compute_data_format == FORMAT_NCHW) {\n    VLOG(4) << \"Convert the input tensor from NHWC to NCHW.\";\n\n    TensorShape nchw_shape =\n        ShapeFromFormat(FORMAT_NCHW, in_batch, in_rows, in_cols, in_depths);\n    if (in_depths > 1) {\n      Tensor transformed_input;\n      OP_REQUIRES_OK(ctx, ctx->allocate_temp(DataTypeToEnum<T>::value,\n                                             nchw_shape, &transformed_input));\n      functor::NHWCToNCHW<GPUDevice, T, 4>()(\n          ctx->eigen_device<GPUDevice>(),\n          const_cast<const Tensor&>(input).tensor<T, 4>(),\n          transformed_input.tensor<T, 4>());\n      input = transformed_input;\n    } else {\n      // If depth <= 1, then just reshape.\n      CHECK(input.CopyFrom(input, nchw_shape));\n    }\n  } else {\n    CHECK(data_format == compute_data_format)  // Crash OK\n        << \"Illegal data and compute format pair:\"\n        << \" data_format=\" << ToString(data_format)\n        << \" compute_data_format=\" << ToString(compute_data_format);\n  }\n\n  CHECK(common_padding_rows >= 0 && common_padding_cols >= 0)  // Crash OK\n      << \"Negative row or col paddings: (\" << common_padding_rows << \", \"\n      << common_padding_cols << \")\";\n\n  constexpr auto kComputeInNHWC =\n      std::make_tuple(se::dnn::DataLayout::kBatchYXDepth,\n                      se::dnn::FilterLayout::kOutputYXInput);\n  constexpr auto kComputeInNCHW =\n      std::make_tuple(se::dnn::DataLayout::kBatchDepthYX,\n                      se::dnn::FilterLayout::kOutputInputYX);\n\n  se::dnn::DataLayout compute_data_layout;\n  se::dnn::FilterLayout filter_layout;\n\n  std::tie(compute_data_layout, filter_layout) =\n      compute_data_format == FORMAT_NHWC ? kComputeInNHWC : kComputeInNCHW;\n\n  se::dnn::BatchDescriptor input_desc;\n  input_desc.set_count(in_batch)\n      .set_feature_map_count(in_depths)\n      .set_height(in_rows)\n      .set_width(in_cols)\n      .set_layout(compute_data_layout);\n  se::dnn::BatchDescriptor output_desc;\n  output_desc.set_count(out_batch)\n      .set_height(out_rows)\n      .set_width(out_cols)\n      .set_feature_map_count(out_depths)\n      .set_layout(compute_data_layout);\n  se::dnn::FilterDescriptor filter_desc;\n  filter_desc.set_input_filter_height(patch_rows)\n      .set_input_filter_width(patch_cols)\n      .set_input_feature_map_count(patch_depths)\n      .set_output_feature_map_count(filter.dim_size(3))\n      .set_layout(filter_layout);\n  se::dnn::ConvolutionDescriptor conv_desc;\n  conv_desc.set_vertical_dilation_rate(row_dilation)\n      .set_horizontal_dilation_rate(col_dilation)\n      .set_vertical_filter_stride(row_stride)\n      .set_horizontal_filter_stride(col_stride)\n      .set_zero_padding_height(common_padding_rows)\n      .set_zero_padding_width(common_padding_cols)\n      .set_group_count(in_depths / patch_depths);\n\n  Tensor transformed_filter;\n\n  const auto transform_filter = [&](FilterTensorFormat dst_format) -> Status {\n    VLOG(4) << \"Transform filter tensor from \" << ToString(FORMAT_HWIO)\n            << \" to \" << ToString(dst_format);\n\n    TensorShape dst_shape =\n        dst_format == FORMAT_OIHW\n            ? TensorShape({filter.dim_size(3), filter.dim_size(2),\n                           filter.dim_size(0), filter.dim_size(1)})\n            : TensorShape({filter.dim_size(3), filter.dim_size(0),\n                           filter.dim_size(1), filter.dim_size(2)});\n\n    TF_RETURN_IF_ERROR(ctx->allocate_temp(DataTypeToEnum<T>::value, dst_shape,\n                                          &transformed_filter));\n    functor::TransformFilter<GPUDevice, T, int, 4>()(\n        ctx->eigen_device<GPUDevice>(), dst_format,\n        To32Bit(filter.tensor<T, 4>()),\n        To32Bit(transformed_filter.tensor<T, 4>()));\n\n    return OkStatus();\n  };\n\n  if (compute_data_format == FORMAT_NCHW) {\n    OP_REQUIRES_OK(ctx, transform_filter(FORMAT_OIHW));\n  } else if (compute_data_format == FORMAT_NHWC) {\n    OP_REQUIRES_OK(ctx, transform_filter(FORMAT_OHWI));\n  } else {\n    ctx->SetStatus(errors::InvalidArgument(\"Invalid compute data format: \",\n                                           ToString(compute_data_format)));\n    return;\n  }\n\n  Tensor transformed_output;\n  if (data_format != compute_data_format) {\n    VLOG(4) << \"Allocate temporary memory for output in compute data format\";\n    OP_REQUIRES_OK(\n        ctx, ctx->allocate_temp(DataTypeToEnum<T>::value,\n                                ShapeFromFormat(compute_data_format, out_batch,\n                                                out_rows, out_cols, out_depths),\n                                &transformed_output));\n  } else {\n    transformed_output = *output;\n  }\n\n  auto input_ptr = AsDeviceMemory(input.template flat<T>().data(),\n                                  input.template flat<T>().size());\n  auto filter_ptr =\n      AsDeviceMemory(transformed_filter.template flat<T>().data(),\n                     transformed_filter.template flat<T>().size());\n  auto output_ptr =\n      AsDeviceMemory(transformed_output.template flat<T>().data(),\n                     transformed_output.template flat<T>().size());\n\n  static int64_t ConvolveScratchSize = GetDnnWorkspaceLimitOrDefault();\n\n  int device_id = stream->parent()->device_ordinal();\n  DataType dtype = input.dtype();\n  ConvParameters conv_parameters = {in_batch,             // batch\n                                    in_depths,            // in_depths\n                                    {{in_rows,            // in_rows\n                                      in_cols}},          // in_cols\n                                    compute_data_format,  // compute_data_format\n                                    out_depths,           // out_depths\n                                    {{patch_rows,         // filter_rows\n                                      patch_cols,         // filter_cols\n                                      patch_depths}},     // filter_depths\n                                    {{row_dilation,       // dilation_rows\n                                      col_dilation}},     // dilation_cols\n                                    {{row_stride,         // stride_rows\n                                      col_stride}},       // stride_cols\n                                    {{common_padding_rows,    // padding_rows\n                                      common_padding_cols}},  // padding_cols\n                                    dtype,                    // tensor datatype\n                                    device_id,                // device_id\n                                    conv_desc.group_count()};\n\n  auto entry_or = AutotuneUnfusedConv(\n      cudnn_use_autotune, ConvAutotuneMap::GetInstance(), conv_parameters, ctx,\n      se::dnn::ConvolutionKind::FORWARD, input_desc, input_ptr, filter_desc,\n      filter_ptr, conv_desc, output_desc, output_ptr, ConvolveScratchSize);\n  OP_REQUIRES_OK(ctx, entry_or.status());\n  auto autotune_entry = std::move(entry_or).value();\n\n  DnnScratchAllocator scratch_allocator(ConvolveScratchSize, ctx);\n  Status cudnn_launch_status = LaunchAutotunedConv(\n      autotune_entry, &scratch_allocator, se::dnn::ConvolutionKind::FORWARD,\n      stream, input_desc, input_ptr, filter_desc, filter_ptr, conv_desc,\n      output_desc, output_ptr);\n  if (!cudnn_launch_status.ok()) {\n    ctx->SetStatus(cudnn_launch_status);\n    return;\n  }\n\n  if (data_format == FORMAT_NHWC && compute_data_format == FORMAT_NCHW) {\n    VLOG(4) << \"Convert the output tensor back from NCHW to NHWC.\";\n    functor::NCHWToNHWC<GPUDevice, T, 4>()(\n        ctx->eigen_device<GPUDevice>(),\n        const_cast<const Tensor&>(transformed_output).tensor<T, 4>(),\n        output->tensor<T, 4>());\n  }\n}\n\n// Forward declarations of the functor specializations for GPU.\nnamespace functor {\n#define DECLARE_GPU_SPEC(T)                                                 \\\n  template <>                                                               \\\n  void SpatialConvolution<GPUDevice, T>::operator()(                        \\\n      const GPUDevice& d, typename TTypes<T, 4>::Tensor output,             \\\n      typename TTypes<T, 4>::ConstTensor input,                             \\\n      typename TTypes<T, 4>::ConstTensor filter, int row_stride,            \\\n      int col_stride, int row_dilation, int col_dilation,                   \\\n      const Eigen::PaddingType& padding,                                    \\\n      const Eigen::NoOpOutputKernel& output_kernel);                        \\\n  template <>                                                               \\\n  void SpatialConvolution<GPUDevice, T>::operator()(                        \\\n      const GPUDevice& d, typename TTypes<T, 4>::Tensor output,             \\\n      typename TTypes<T, 4>::ConstTensor input,                             \\\n      typename TTypes<T, 4>::ConstTensor filter, int row_stride,            \\\n      int col_stride, int row_dilation, int col_dilation, int padding_top,  \\\n      int padding_bottom, int padding_left, int padding_right,              \\\n      const Eigen::NoOpOutputKernel& output_kernel);                        \\\n  extern template struct SpatialConvolution<GPUDevice, T>;                  \\\n  template <>                                                               \\\n  void MatMulConvFunctor<GPUDevice, T>::operator()(                         \\\n      const GPUDevice& d, typename TTypes<T, 2>::Tensor out,                \\\n      typename TTypes<T, 2>::ConstTensor in0,                               \\\n      typename TTypes<T, 2>::ConstTensor in1,                               \\\n      const Eigen::array<Eigen::IndexPair<Eigen::DenseIndex>, 1>& dim_pair, \\\n      const Eigen::NoOpOutputKernel& output_kernel);                        \\\n  extern template struct MatMulConvFunctor<GPUDevice, T>;                   \\\n  template <>                                                               \\\n  void TransformFilter<GPUDevice, T, int, 4>::operator()(                   \\\n      const GPUDevice& d, FilterTensorFormat dst_filter_format,             \\\n      typename TTypes<T, 4, int>::ConstTensor in,                           \\\n      typename TTypes<T, 4, int>::Tensor out);                              \\\n  extern template struct TransformFilter<GPUDevice, T, int, 4>;             \\\n  template <>                                                               \\\n  void PadInput<GPUDevice, T, int, 4>::operator()(                          \\\n      const GPUDevice& d, typename TTypes<T, 4, int>::ConstTensor in,       \\\n      const std::array<int, 2>& padding_left,                               \\\n      const std::array<int, 2>& padding_right,                              \\\n      typename TTypes<T, 4, int>::Tensor out, TensorFormat data_format,     \\\n      const T& padding_value);                                              \\\n  extern template struct PadInput<GPUDevice, T, int, 4>\n\nDECLARE_GPU_SPEC(float);\nDECLARE_GPU_SPEC(Eigen::half);\nDECLARE_GPU_SPEC(double);\nDECLARE_GPU_SPEC(int32);\n#undef DECLARE_GPU_SPEC\n\n}  // namespace functor\n\n// Registration of the GPU implementations.\nREGISTER_KERNEL_BUILDER(\n    Name(\"Conv2D\").Device(DEVICE_GPU).TypeConstraint<Eigen::half>(\"T\"),\n    Conv2DOp<GPUDevice, Eigen::half>);\nREGISTER_KERNEL_BUILDER(\n    Name(\"Conv2D\").Device(DEVICE_GPU).TypeConstraint<float>(\"T\"),\n    Conv2DOp<GPUDevice, float>);\nREGISTER_KERNEL_BUILDER(\n    Name(\"Conv2D\").Device(DEVICE_GPU).TypeConstraint<double>(\"T\"),\n    Conv2DOp<GPUDevice, double>);\nREGISTER_KERNEL_BUILDER(\n    Name(\"Conv2D\").Device(DEVICE_GPU).TypeConstraint<int32>(\"T\"),\n    Conv2DOp<GPUDevice, int32>);\n\n// To be used inside depthwise_conv_op.cc.\ntemplate struct LaunchConv2DOp<GPUDevice, float>;\ntemplate struct LaunchConv2DOp<GPUDevice, Eigen::half>;\ntemplate struct LaunchConv2DOp<GPUDevice, double>;\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n}  // namespace tensorflow\n", "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Functional tests for convolutional operations.\"\"\"\n\nimport os\nimport time\n\nimport numpy as np\n\nfrom tensorflow.core.protobuf import config_pb2\nfrom tensorflow.core.protobuf import rewriter_config_pb2\nfrom tensorflow.python.client import session as session_lib\nfrom tensorflow.python.eager import backprop\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors_impl\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.layers import convolutional\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import gradient_checker\nfrom tensorflow.python.ops import gradients_impl\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import nn_impl\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.ops import random_ops\nfrom tensorflow.python.ops import variables\nimport tensorflow.python.ops.nn_grad  # pylint: disable=unused-import\nfrom tensorflow.python.platform import test\nfrom tensorflow.python.platform import tf_logging\nfrom tensorflow.python.util.compat import collections_abc\n\n\ndef GetShrunkInceptionShapes(shrink=10):\n  \"\"\"Iterator for smaller versions of convolution shapes in 2015 Inception.\n\n  Relative to inception, each depth value is `depth // shrink`.\n\n  Args:\n    shrink: Factor to shrink each depth value by relative to Inception.\n\n  Yields:\n    Tuple (input_size, filter_size, out_size, stride, padding), the convolution\n    parameters of Inception layers.\n  \"\"\"\n  input_sizes = [[4, 5, 5, 1248], [4, 8, 8, 384], [4, 8, 8, 384],\n                 [4, 8, 8, 2048], [4, 8, 8, 448], [4, 8, 8, 2048],\n                 [4, 8, 8, 2048], [4, 8, 8, 2048], [4, 8, 8, 1760],\n                 [4, 8, 8, 1760], [4, 8, 8, 1760], [4, 8, 8, 1760],\n                 [4, 17, 17, 192], [4, 17, 17, 192], [4, 17, 17, 1248],\n                 [4, 17, 17, 128], [4, 17, 17, 1248], [4, 17, 17, 224],\n                 [4, 17, 17, 192], [4, 17, 17, 192], [4, 17, 17, 1216],\n                 [4, 17, 17, 1216], [4, 17, 17, 224], [4, 17, 17, 192],\n                 [4, 17, 17, 192], [4, 17, 17, 1152], [4, 17, 17, 1152],\n                 [4, 17, 17, 192], [4, 17, 17, 160], [4, 17, 17, 1152],\n                 [4, 17, 17, 1024], [4, 17, 17, 128], [4, 17, 17, 1024],\n                 [4, 17, 17, 128], [4, 17, 17, 1024], [4, 17, 17, 128],\n                 [4, 17, 17, 768], [4, 17, 17, 128], [4, 17, 17, 128],\n                 [4, 17, 17, 768], [4, 17, 17, 768], [4, 35, 35, 96],\n                 [4, 35, 35, 288], [4, 35, 35, 64], [4, 35, 35, 288],\n                 [4, 35, 35, 256], [4, 35, 35, 48], [4, 35, 35, 256],\n                 [4, 35, 35, 96], [4, 35, 35, 192], [4, 35, 35, 192],\n                 [4, 35, 35, 192], [4, 73, 73, 64], [4, 73, 73, 64],\n                 [4, 147, 147, 24]]\n  filter_sizes = [[1, 1, 1248, 128], [1, 3, 384, 384], [3, 1, 384, 384],\n                  [1, 1, 2048, 192], [3, 3, 448, 384], [1, 1, 2048, 320],\n                  [1, 1, 2048, 448], [1, 1, 2048, 384], [1, 1, 1760, 384],\n                  [1, 1, 1760, 192], [1, 1, 1760, 448], [1, 1, 1760, 320],\n                  [3, 3, 192, 192], [3, 3, 192, 192], [1, 1, 1248, 192],\n                  [3, 3, 128, 320], [1, 1, 1248, 128], [1, 3, 224, 224],\n                  [3, 1, 192, 256], [1, 3, 192, 256], [1, 1, 1216, 192],\n                  [1, 1, 1216, 96], [3, 1, 224, 224], [3, 3, 192, 224],\n                  [1, 3, 192, 192], [1, 1, 1152, 192], [1, 1, 1152, 128],\n                  [3, 1, 192, 192], [3, 3, 160, 192], [1, 1, 1152, 160],\n                  [1, 1, 1024, 128], [1, 3, 128, 192], [1, 1, 1024, 160],\n                  [3, 1, 128, 192], [1, 1, 1024, 256], [3, 1, 128, 128],\n                  [1, 1, 768, 192], [1, 3, 128, 128], [3, 3, 128, 128],\n                  [1, 1, 768, 128], [1, 1, 768, 320], [3, 3, 96, 96],\n                  [3, 3, 288, 384], [3, 3, 64, 96], [1, 1, 288, 64],\n                  [1, 1, 256, 64], [5, 5, 48, 64], [1, 1, 256, 48],\n                  [3, 3, 96, 96], [1, 1, 192, 32], [1, 1, 192, 64],\n                  [1, 1, 192, 48], [3, 3, 64, 192], [1, 1, 64, 64],\n                  [1, 1, 24, 64]]\n  out_sizes = [[4, 5, 5, 128], [4, 8, 8, 384], [4, 8, 8, 384],\n               [4, 8, 8, 192], [4, 8, 8, 384], [4, 8, 8, 320],\n               [4, 8, 8, 448], [4, 8, 8, 384], [4, 8, 8, 384],\n               [4, 8, 8, 192], [4, 8, 8, 448], [4, 8, 8, 320],\n               [4, 8, 8, 192], [4, 17, 17, 192], [4, 17, 17, 192],\n               [4, 8, 8, 320], [4, 17, 17, 128], [4, 17, 17, 224],\n               [4, 17, 17, 256], [4, 17, 17, 256], [4, 17, 17, 192],\n               [4, 17, 17, 96], [4, 17, 17, 224], [4, 17, 17, 224],\n               [4, 17, 17, 192], [4, 17, 17, 192], [4, 17, 17, 128],\n               [4, 17, 17, 192], [4, 17, 17, 192], [4, 17, 17, 160],\n               [4, 17, 17, 128], [4, 17, 17, 192], [4, 17, 17, 160],\n               [4, 17, 17, 192], [4, 17, 17, 256], [4, 17, 17, 128],\n               [4, 17, 17, 192], [4, 17, 17, 128], [4, 17, 17, 128],\n               [4, 17, 17, 128], [4, 17, 17, 320], [4, 17, 17, 96],\n               [4, 17, 17, 384], [4, 35, 35, 96], [4, 35, 35, 64],\n               [4, 35, 35, 64], [4, 35, 35, 64], [4, 35, 35, 48],\n               [4, 35, 35, 96], [4, 35, 35, 32], [4, 35, 35, 64],\n               [4, 35, 35, 48], [4, 71, 71, 192], [4, 73, 73, 64],\n               [4, 147, 147, 64]]\n  strides = [\n      1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n      1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1,\n      1, 1, 1, 1, 1\n  ]\n  # Shrink sizes to make the test faster\n  for i in input_sizes:\n    i[3] //= shrink\n  for f in filter_sizes:\n    f[2] //= shrink\n    f[3] //= shrink\n  for o in out_sizes:\n    o[3] //= shrink\n  # pylint: disable=invalid-name\n  VALID = \"VALID\"\n  SAME = \"SAME\"\n  # pylint: enable=invalid-name\n  paddings = [\n      SAME, SAME, SAME, SAME, SAME, SAME, SAME, SAME, SAME, SAME, SAME, SAME,\n      VALID, SAME, SAME, VALID, SAME, SAME, SAME, SAME, SAME, SAME, SAME, SAME,\n      SAME, SAME, SAME, SAME, SAME, SAME, SAME, SAME, SAME, SAME, SAME, SAME,\n      SAME, SAME, SAME, SAME, SAME, VALID, VALID, SAME, SAME, SAME, SAME, SAME,\n      SAME, SAME, SAME, SAME, VALID, VALID, VALID\n  ]\n  for i, f, o, s, p in zip(input_sizes, filter_sizes, out_sizes, strides,\n                           paddings):\n    yield i, f, o, s, p\n\n\ndef GetTestConfigs():\n  \"\"\"Get all the valid tests configs to run.\n\n  Returns:\n    all the valid test configs as tuples of data_format and use_gpu.\n  \"\"\"\n  test_configs = [(\"NHWC\", False), (\"NHWC\", True)]\n  if test.is_gpu_available(cuda_only=True):\n    # \"NCHW\" format is only supported on CUDA.\n    test_configs += [(\"NCHW\", True)]\n  return test_configs\n\n\n@test_util.run_all_without_tensor_float_32(\"Avoid TF32 conv on GPU\")\nclass Conv2DTest(test.TestCase):\n\n  def _DtypesToTest(self, use_gpu):\n    if test_util.IsMklEnabled():\n      return [dtypes.float32]\n    # double datatype is currently not supported for convolution ops\n    # on the ROCm platform\n    optional_float64 = [] if test.is_built_with_rocm() else [dtypes.float64]\n    if use_gpu and not test_util.GpuSupportsHalfMatMulAndConv():\n      return [dtypes.float32] + optional_float64\n    else:\n      # It is important that float32 comes before float16 here,\n      # as we will be using its gradients as reference for fp16 gradients.\n      return [dtypes.float32, dtypes.float16] + optional_float64\n\n  def _CreateNumpyTensor(self, shape):\n    total_size = 1\n    for s in shape:\n      total_size *= s\n    return np.arange(1, total_size + 1, dtype=np.float32).reshape(shape)\n\n  def _SetupValuesForDevice(self, tensor_in_sizes, filter_in_sizes, dilations,\n                            strides, padding, data_format, dtype, use_gpu):\n    \"\"\"Verifies the output values of the convolution function.\n\n    Args:\n      tensor_in_sizes: Input tensor dimensions in\n        [batch, input_rows, input_cols, input_depth].\n      filter_in_sizes: Filter tensor dimensions in\n        [kernel_rows, kernel_cols, input_depth, output_depth].\n      dilations: Dilated rate: [col_dilation, row_dilation]\n      strides: Stride: [col_stride, row_stride]\n      padding: Padding type.\n      data_format: Format of the data tensors.\n      dtype: Data type for inputs and outputs.\n      use_gpu: True if the operations should be run on GPU\n    Returns:\n      Symbolic tensor value that can be used to execute the computation\n    \"\"\"\n    x1 = self._CreateNumpyTensor(tensor_in_sizes)\n    x2 = self._CreateNumpyTensor(filter_in_sizes)\n\n    with test_util.device(use_gpu):\n      t1 = constant_op.constant(x1, shape=tensor_in_sizes, dtype=dtype)\n      t2 = constant_op.constant(x2, shape=filter_in_sizes, dtype=dtype)\n      strides = [1] + strides + [1]\n      dilations = [1] + dilations + [1]\n      if isinstance(padding, (list, tuple)):\n        padding = [(0, 0)] + padding + [(0, 0)]\n      if data_format == \"NCHW\":\n        t1 = test_util.NHWCToNCHW(t1)\n        strides = test_util.NHWCToNCHW(strides)\n        dilations = test_util.NHWCToNCHW(dilations)\n        if isinstance(padding, (list, tuple)):\n          padding = test_util.NHWCToNCHW(padding)\n      conv = nn_ops.conv2d(\n          t1,\n          t2,\n          dilations=dilations,\n          strides=strides,\n          padding=padding,\n          data_format=data_format)\n      self.assertEqual(conv.dtype, dtype)\n      if data_format == \"NCHW\":\n        conv = test_util.NCHWToNHWC(conv)\n\n      return conv\n\n  def _CompareFwdValues(self, tensor_in_sizes, filter_in_sizes, conv_strides,\n                        padding):\n    \"\"\"Verifies that CPU and GPU produce the same values.\n\n    Args:\n      tensor_in_sizes: Input tensor dimensions in\n        [batch, input_rows, input_cols, input_depth].\n      filter_in_sizes: Filter tensor dimensions in\n        [kernel_rows, kernel_cols, input_depth, output_depth].\n      conv_strides: [row_stride, col_stride] for the convolution;\n      padding: Padding type.\n    \"\"\"\n    x1 = np.random.rand(*tensor_in_sizes).astype(np.float32)\n    x2 = np.random.rand(*filter_in_sizes).astype(np.float32)\n\n    def _SetupVal(data_format, use_gpu):\n      with test_util.device(use_gpu):\n        t1 = constant_op.constant(x1, shape=tensor_in_sizes)\n        t2 = constant_op.constant(x2, shape=filter_in_sizes)\n        strides = [1] + conv_strides + [1]\n        if data_format == \"NCHW\":\n          t1 = test_util.NHWCToNCHW(t1)\n          strides = test_util.NHWCToNCHW(strides)\n        conv = nn_ops.conv2d(\n            t1, t2, strides=strides, padding=padding, data_format=data_format)\n        if data_format == \"NCHW\":\n          conv = test_util.NCHWToNHWC(conv)\n        return conv\n\n    tensors = []\n    for (data_format, use_gpu) in GetTestConfigs():\n      tensors.append(_SetupVal(data_format, use_gpu))\n    values = self.evaluate(tensors)\n    for i in range(1, len(values)):\n      self.assertAllClose(values[0], values[i], rtol=1e-3, atol=1e-3)\n\n  def _ComputeReferenceDilatedConv(self, tensor_in_sizes, filter_in_sizes,\n                                   stride, dilation, padding, data_format,\n                                   use_gpu):\n    x1 = self._CreateNumpyTensor(tensor_in_sizes)\n    x2 = self._CreateNumpyTensor(filter_in_sizes)\n    with test_util.device(use_gpu):\n      t1 = constant_op.constant(x1, shape=tensor_in_sizes)\n      t2 = constant_op.constant(x2, shape=filter_in_sizes)\n      if isinstance(stride, collections_abc.Iterable):\n        strides = list(stride)\n      else:\n        strides = [stride, stride]\n      if data_format == \"NCHW\":\n        t1 = test_util.NHWCToNCHW(t1)\n        full_strides = [1, 1] + strides\n        full_dilation = [1, 1] + dilation\n      else:\n        full_strides = [1] + strides + [1]\n        full_dilation = [1] + dilation + [1]\n      expected = nn_ops.convolution(\n          t1,\n          t2,\n          padding=padding,\n          strides=strides,\n          dilation_rate=dilation,\n          data_format=data_format)\n      computed = nn_ops.conv2d(\n          t1,\n          t2,\n          strides=full_strides,\n          dilations=full_dilation,\n          padding=padding,\n          data_format=data_format)\n      if data_format == \"NCHW\":\n        expected = test_util.NCHWToNHWC(expected)\n        computed = test_util.NCHWToNHWC(computed)\n    return expected, computed\n\n  def _VerifyDilatedConvValues(self, tensor_in_sizes, filter_in_sizes, strides,\n                               padding, dilations, rtol=1e-4):\n    expected_results = []\n    computed_results = []\n    for data_format, use_gpu in GetTestConfigs():\n      expected, computed = self._ComputeReferenceDilatedConv(\n          tensor_in_sizes, filter_in_sizes, strides, dilations, padding,\n          data_format, use_gpu)\n      expected_results.append(expected)\n      computed_results.append(computed)\n    tolerance = 1e-2 if use_gpu else 1e-5\n    expected_values = self.evaluate(expected_results)\n    computed_values = self.evaluate(computed_results)\n    for e_value, c_value in zip(expected_values, computed_values):\n      tf_logging.debug(\"expected = %s\", e_value)\n      tf_logging.debug(\"actual = %s\", c_value)\n      self.assertAllClose(\n          e_value.flatten(), c_value.flatten(), atol=tolerance, rtol=rtol)\n\n  def _VerifyValues(self,\n                    tensor_in_sizes,\n                    filter_in_sizes,\n                    strides,\n                    padding,\n                    expected,\n                    dilations=(1, 1),\n                    gpu_only=False,\n                    test_grappler_layout_optimizer=False,\n                    tol=1e-5,\n                    fp16_tol=1e-3):\n    if gpu_only and not test.is_gpu_available(cuda_only=True):\n      return\n    tensors = []\n    dilations = list(dilations)\n    for (data_format, use_gpu) in GetTestConfigs():\n      if gpu_only and not use_gpu:\n        continue\n      dtypes_to_test = self._DtypesToTest(use_gpu)\n      if not test_grappler_layout_optimizer and data_format == \"NHWC\":\n        dtypes_to_test.append(dtypes.int32)\n      for dtype in dtypes_to_test:\n        result = self._SetupValuesForDevice(\n            tensor_in_sizes,\n            filter_in_sizes,\n            dilations,\n            strides,\n            padding,\n            data_format,\n            dtype,\n            use_gpu=use_gpu)\n        if test_grappler_layout_optimizer and data_format == \"NHWC\" and use_gpu:\n          # Grappler's layout optimizer will not optimize a fetch node, so\n          # this identity allows Grappler to optimize the Conv2D node.\n          result = array_ops.identity(result)\n        tensors.append(result)\n      values = self.evaluate(tensors)\n      for i in range(len(tensors)):\n        conv = tensors[i]\n        value = values[i]\n        tf_logging.debug(\"expected = %s\", expected)\n        tf_logging.debug(\"actual = %s\", value)\n        tol_to_use = fp16_tol if value.dtype == np.float16 else tol\n        if np.issubdtype(value.dtype, np.integer):\n          self.assertAllEqual(np.rint(expected), np.ravel(value))\n        else:\n          self.assertAllClose(expected, np.ravel(value), atol=tol_to_use,\n                              rtol=tol_to_use)\n        self.assertShapeEqual(value, conv)\n        self.assertEqual(value.dtype, conv.dtype.as_numpy_dtype)\n\n  def _VerifyExplicitPaddings(self,\n                              tensor_in_sizes,\n                              filter_in_sizes,\n                              strides,\n                              padding,\n                              dilations=(1, 1),\n                              test_grappler_layout_optimizer=False,\n                              tol=1e-5,\n                              fp16_tol=1e-3):\n    \"\"\"Verifies Conv2D with explicit padding generates correct values.\n\n    It does this by comparing with Conv2D without explicit padding. This\n    function assumes Conv2D without explicit padding works correctly.\n\n    Args:\n      tensor_in_sizes: Input tensor dimensions in [batch, input_rows,\n        input_cols, input_depth].\n      filter_in_sizes: Filter tensor dimensions in [kernel_rows, kernel_cols,\n        input_depth, output_depth].\n      strides: [row_stride, col_stride] for the convolution;\n      padding: Explicit padding amounts.\n      dilations: Dilation values\n      test_grappler_layout_optimizer: If True, allow the Grappler layout\n        optimizer to run, which turns NHWC Conv2Ds on the GPU to NCHW Conv2Ds.\n      tol: The absolute and relative tolerance for non-fp16 dtypes.\n      fp16_tol: The absolute and relative tolerance for fp16.\n    \"\"\"\n    input_tensor = self._CreateNumpyTensor(tensor_in_sizes)\n    filter_tensor = self._CreateNumpyTensor(filter_in_sizes)\n    input_tensor = array_ops.pad(input_tensor, [(0, 0)] + padding + [(0, 0)])\n    dilations = list(dilations)\n    conv2d_result = nn_ops.conv2d(\n        input_tensor,\n        filter_tensor, [1] + list(strides) + [1],\n        \"VALID\",\n        dilations=[1] + dilations + [1])\n    expected = list(self.evaluate(array_ops.reshape(conv2d_result, [-1])))\n    self._VerifyValues(\n        tensor_in_sizes,\n        filter_in_sizes,\n        strides,\n        padding,\n        expected,\n        dilations,\n        test_grappler_layout_optimizer=test_grappler_layout_optimizer,\n        tol=tol,\n        fp16_tol=fp16_tol)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D1x1Filter(self):\n    expected_output = [\n        30.0, 36.0, 42.0, 66.0, 81.0, 96.0, 102.0, 126.0, 150.0, 138.0, 171.0,\n        204.0, 174.0, 216.0, 258.0, 210.0, 261.0, 312.0\n    ]\n    self._VerifyValues(\n        tensor_in_sizes=[1, 2, 3, 3],\n        filter_in_sizes=[1, 1, 3, 3],\n        strides=[1, 1],\n        padding=\"VALID\",\n        expected=expected_output)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DExpandedBatch(self):\n    tensor_in_sizes_batch = [10, 2, 3, 3]\n    tensor_in_sizes_expanded_batch = [2, 5, 2, 3, 3]\n    filter_in_sizes = [1, 1, 3, 3]\n    filter_in = self._CreateNumpyTensor(filter_in_sizes)\n    x1 = self._CreateNumpyTensor(tensor_in_sizes_batch)\n    x2 = x1.reshape(tensor_in_sizes_expanded_batch)\n    conv1 = nn_ops.conv2d(\n        x1,\n        filter_in,\n        strides=[1, 1],\n        padding=\"VALID\")\n    conv2 = nn_ops.conv2d(\n        x2,\n        filter_in,\n        strides=[1, 1],\n        padding=\"VALID\")\n    self.assertEqual(conv1.shape, tensor_in_sizes_batch)\n    self.assertEqual(conv2.shape, tensor_in_sizes_expanded_batch)\n    self.assertAllEqual(\n        conv1,\n        self.evaluate(conv2).reshape(conv1.shape))\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConvolutionClass2DExpandedBatch(self):\n    tensor_in_sizes_batch = [10, 2, 3, 3]\n    tensor_in_sizes_expanded_batch = [2, 5, 2, 3, 3]\n    filter_in_sizes = [1, 1, 3, 3]\n    filter_in = self._CreateNumpyTensor(filter_in_sizes)\n    x1 = self._CreateNumpyTensor(tensor_in_sizes_batch)\n    x2 = x1.reshape(tensor_in_sizes_expanded_batch)\n    convolver1 = nn_ops.Convolution(\n        input_shape=x1.shape,\n        filter_shape=filter_in.shape,\n        strides=[1, 1],\n        padding=\"VALID\")\n    self.assertEqual(convolver1.num_batch_dims, 1)\n    convolver2 = nn_ops.Convolution(\n        input_shape=x2.shape,\n        filter_shape=filter_in.shape,\n        strides=[1, 1],\n        padding=\"VALID\")\n    self.assertEqual(convolver2.num_batch_dims, 2)\n    conv1 = convolver1(x1, filter_in)\n    conv2 = convolver2(x2, filter_in)\n    self.assertEqual(conv1.shape, tensor_in_sizes_batch)\n    self.assertEqual(conv2.shape, tensor_in_sizes_expanded_batch)\n    self.assertAllEqual(\n        conv1,\n        self.evaluate(conv2).reshape(conv1.shape))\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConvolutionWith2SpatialDimensionsAndExpandedBatch(self):\n    tensor_in_sizes_batch = [10, 2, 3, 3]\n    tensor_in_sizes_expanded_batch = [2, 5, 2, 3, 3]\n    filter_in_sizes = [1, 1, 3, 3]\n    filter_in = self._CreateNumpyTensor(filter_in_sizes)\n    x1 = self._CreateNumpyTensor(tensor_in_sizes_batch)\n    x2 = x1.reshape(tensor_in_sizes_expanded_batch)\n    conv1 = nn_ops.convolution(\n        x1,\n        filter_in,\n        strides=[1, 1],\n        padding=\"VALID\")\n    conv2 = nn_ops.convolution(\n        x2,\n        filter_in,\n        strides=[1, 1],\n        padding=\"VALID\")\n    self.assertEqual(conv1.shape, tensor_in_sizes_batch)\n    self.assertEqual(conv2.shape, tensor_in_sizes_expanded_batch)\n    self.assertAllEqual(\n        conv1,\n        self.evaluate(conv2).reshape(conv1.shape))\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D2x2Filter2x1Dilation(self):\n    self._VerifyDilatedConvValues(\n        tensor_in_sizes=[1, 4, 4, 1],\n        filter_in_sizes=[2, 2, 1, 1],\n        strides=[1, 1],\n        dilations=[2, 1],\n        padding=\"VALID\")\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DEmpty(self):\n    expected_output = []\n    self._VerifyValues(\n        tensor_in_sizes=[0, 2, 3, 3],\n        filter_in_sizes=[1, 1, 3, 3],\n        strides=[1, 1],\n        padding=\"VALID\",\n        expected=expected_output)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DEmptyDilation(self):\n    self._VerifyDilatedConvValues(\n        tensor_in_sizes=[0, 2, 3, 3],\n        filter_in_sizes=[1, 1, 3, 3],\n        strides=[1, 1],\n        dilations=[2, 1],\n        padding=\"VALID\")\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D2x2Filter(self):\n    # The outputs are computed using third_party/py/IPython/notebook.\n    expected_output = [2271.0, 2367.0, 2463.0, 2901.0, 3033.0, 3165.0]\n    self._VerifyValues(\n        tensor_in_sizes=[1, 2, 3, 3],\n        filter_in_sizes=[2, 2, 3, 3],\n        strides=[1, 1],\n        padding=\"VALID\",\n        expected=expected_output)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D2x2FilterDilation(self):\n    self._VerifyDilatedConvValues(\n        tensor_in_sizes=[1, 2, 3, 3],\n        filter_in_sizes=[2, 2, 3, 3],\n        strides=[1, 1],\n        dilations=[1, 2],\n        padding=\"VALID\")\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D1x2Filter(self):\n    # The outputs are computed using third_party/py/IPython/notebook.\n    expected_output = [\n        231.0, 252.0, 273.0, 384.0, 423.0, 462.0, 690.0, 765.0, 840.0, 843.0,\n        936.0, 1029.0\n    ]\n    self._VerifyValues(\n        tensor_in_sizes=[1, 2, 3, 3],\n        filter_in_sizes=[1, 2, 3, 3],\n        strides=[1, 1],\n        padding=\"VALID\",\n        expected=expected_output)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D1x2FilterDilation(self):\n    self._VerifyDilatedConvValues(\n        tensor_in_sizes=[1, 2, 3, 3],\n        filter_in_sizes=[1, 2, 3, 3],\n        strides=[1, 1],\n        dilations=[2, 1],\n        padding=\"VALID\")\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D2x2FilterStride2(self):\n    expected_output = [2271.0, 2367.0, 2463.0]\n    self._VerifyValues(\n        tensor_in_sizes=[1, 2, 3, 3],\n        filter_in_sizes=[2, 2, 3, 3],\n        strides=[2, 2],\n        padding=\"VALID\",\n        expected=expected_output)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D2x2FilterStride2Same(self):\n    expected_output = [2271.0, 2367.0, 2463.0, 1230.0, 1305.0, 1380.0]\n    self._VerifyValues(\n        tensor_in_sizes=[1, 2, 3, 3],\n        filter_in_sizes=[2, 2, 3, 3],\n        strides=[2, 2],\n        padding=\"SAME\",\n        expected=expected_output)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D2x2FilterStride1x2(self):\n    expected_output = [58.0, 78.0, 98.0, 118.0, 138.0, 158.0]\n    self._VerifyValues(\n        tensor_in_sizes=[1, 3, 6, 1],\n        filter_in_sizes=[2, 2, 1, 1],\n        strides=[1, 2],\n        padding=\"VALID\",\n        expected=expected_output)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DKernelSmallerThanStrideValid(self):\n    expected_output = [65, 95, 275, 305]\n    self._VerifyValues(\n        tensor_in_sizes=[1, 7, 7, 1],\n        filter_in_sizes=[2, 2, 1, 1],\n        strides=[3, 3],\n        padding=\"VALID\",\n        expected=expected_output)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DKernelSmallerThanStrideSame(self):\n    self._VerifyValues(\n        tensor_in_sizes=[1, 3, 3, 1],\n        filter_in_sizes=[1, 1, 1, 1],\n        strides=[2, 2],\n        padding=\"SAME\",\n        expected=[1, 3, 7, 9])\n\n    self._VerifyValues(\n        tensor_in_sizes=[1, 4, 4, 1],\n        filter_in_sizes=[1, 1, 1, 1],\n        strides=[2, 2],\n        padding=\"SAME\",\n        expected=[1, 3, 9, 11])\n\n    self._VerifyValues(\n        tensor_in_sizes=[1, 4, 4, 1],\n        filter_in_sizes=[2, 2, 1, 1],\n        strides=[3, 3],\n        padding=\"SAME\",\n        expected=[44, 28, 41, 16])\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DKernelSizeMatchesInputSize(self):\n    self._VerifyValues(\n        tensor_in_sizes=[1, 2, 2, 1],\n        filter_in_sizes=[2, 2, 1, 2],\n        strides=[1, 1],\n        padding=\"VALID\",\n        expected=[50, 60])\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DKernelSizeMatchesInputSizeDilation(self):\n    self._VerifyDilatedConvValues(\n        tensor_in_sizes=[1, 3, 3, 1],\n        filter_in_sizes=[2, 2, 1, 2],\n        strides=[1, 1],\n        dilations=[2, 2],\n        padding=\"VALID\")\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2D0x0Padding(self):\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[1, 2, 3, 3],\n        filter_in_sizes=[2, 2, 3, 3],\n        strides=[1, 1],\n        padding=[[0, 0], [0, 0]])\n\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[3, 4, 3, 2],\n        filter_in_sizes=[1, 1, 2, 1],\n        strides=[2, 2],\n        padding=[[0, 0], [0, 0]])\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2D1x1Padding(self):\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[1, 2, 3, 2],\n        filter_in_sizes=[2, 2, 2, 2],\n        strides=[1, 1],\n        padding=[[1, 1], [1, 1]])\n\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[1, 2, 2, 1],\n        filter_in_sizes=[1, 1, 1, 2],\n        strides=[1, 1],\n        padding=[[1, 1], [1, 1]])\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2D2x2Padding(self):\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[1, 2, 1, 2],\n        filter_in_sizes=[2, 1, 2, 1],\n        strides=[1, 1],\n        padding=[[2, 2], [2, 2]])\n\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[1, 2, 1, 2],\n        filter_in_sizes=[1, 1, 2, 1],\n        strides=[2, 1],\n        padding=[[2, 2], [2, 2]])\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2DOnlyBottomPadding(self):\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[1, 2, 3, 3],\n        filter_in_sizes=[2, 2, 3, 2],\n        strides=[1, 1],\n        padding=[[0, 3], [0, 0]], tol=2e-5)\n\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[2, 2, 4, 3],\n        filter_in_sizes=[1, 2, 3, 2],\n        strides=[2, 2],\n        padding=[[0, 3], [0, 0]])\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2DOnlyTopRightPadding(self):\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[1, 2, 3, 3],\n        filter_in_sizes=[2, 2, 3, 2],\n        strides=[1, 1],\n        padding=[[1, 0], [0, 2]],\n        tol=5e-5)\n\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[1, 2, 4, 2],\n        filter_in_sizes=[2, 2, 2, 2],\n        strides=[1, 3],\n        padding=[[1, 0], [0, 2]])\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2DLotsPadding(self):\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[1, 1, 1, 3],\n        filter_in_sizes=[2, 2, 3, 3],\n        strides=[1, 1],\n        padding=[[3, 4], [4, 2]])\n\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[1, 2, 1, 1],\n        filter_in_sizes=[2, 2, 1, 3],\n        strides=[2, 1],\n        padding=[[3, 4], [4, 2]])\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2DExplicitPaddingWithDilations(self):\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[1, 3, 2, 1],\n        filter_in_sizes=[1, 2, 1, 2],\n        strides=[1, 1],\n        padding=[[1, 0], [0, 1]],\n        dilations=[2, 1])\n\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[1, 2, 3, 2],\n        filter_in_sizes=[3, 2, 2, 1],\n        strides=[1, 1],\n        padding=[[2, 1], [1, 2]],\n        dilations=[2, 3])\n\n  def testConv2DExplicitPaddingWithLayoutOptimizer(self):\n    # Test with Grappler's layout optimizer, to ensure the layout optimizer\n    # handles explicit padding correctly.\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[1, 3, 2, 1],\n        filter_in_sizes=[1, 2, 1, 2],\n        strides=[1, 1],\n        padding=[[1, 0], [0, 1]],\n        dilations=[2, 1],\n        test_grappler_layout_optimizer=True)\n\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[1, 2, 3, 2],\n        filter_in_sizes=[3, 2, 2, 1],\n        strides=[1, 1],\n        padding=[[2, 1], [1, 2]],\n        dilations=[2, 3],\n        test_grappler_layout_optimizer=True)\n\n  def _VerifyGroupConvFwd(self, tensor_in_sizes, filter_in_sizes, dilations,\n                          strides, padding, data_format, dtype):\n    \"\"\"Verify the output of group convolution is equal to a for-loop implementation.\n\n    Args:\n      tensor_in_sizes: Input tensor dimensions in [batch, input_rows,\n        input_cols, input_depth].\n      filter_in_sizes: Filter tensor dimensions in [kernel_rows, kernel_cols,\n        input_depth, output_depth].\n      dilations: Dilated rate: [col_dilation, row_dilation]\n      strides: Stride: [col_stride, row_stride]\n      padding: Padding type.\n      data_format: Format of the data tensors.\n      dtype: Data type for inputs and outputs.\n    \"\"\"\n    tensor_in = self._CreateNumpyTensor(tensor_in_sizes)\n    filter_in = self._CreateNumpyTensor(filter_in_sizes)\n    num_groups = tensor_in_sizes[3] // filter_in_sizes[2]\n    assert num_groups > 1 and \\\n        filter_in_sizes[2] * num_groups == tensor_in_sizes[3]\n    with test_util.device(True):\n      t1 = constant_op.constant(tensor_in, dtype=dtype)\n      t2 = constant_op.constant(filter_in, dtype=dtype)\n      strides = [1] + strides + [1]\n      dilations = [1] + dilations + [1]\n      if data_format == \"NCHW\":\n        t1 = test_util.NHWCToNCHW(t1)\n        strides = test_util.NHWCToNCHW(strides)\n        dilations = test_util.NHWCToNCHW(dilations)\n        t1_splits = array_ops.split(t1, num_groups, axis=1)\n      else:\n        t1_splits = array_ops.split(t1, num_groups, axis=3)\n      t2_splits = array_ops.split(t2, num_groups, axis=3)\n\n      def MakeConv2d(inputs, filters):\n        return nn_ops.conv2d(\n            inputs,\n            filters,\n            strides,\n            padding,\n            dilations=dilations,\n            data_format=data_format)\n\n      group_conv = MakeConv2d(t1, t2)\n      group_conv_loop = array_ops.concat(\n          [MakeConv2d(t1s, t2s) for t1s, t2s in zip(t1_splits, t2_splits)],\n          axis=1 if data_format == \"NCHW\" else 3)\n\n      results = self.evaluate([group_conv, group_conv_loop])\n      tol_to_use = 1e-5\n      self.assertAllClose(\n          results[0], results[1], atol=tol_to_use, rtol=tol_to_use)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DGroupConvFwd(self):\n    if test.is_gpu_available(cuda_only=True) or test_util.IsMklEnabled():\n      data_formats = [\"NHWC\", \"NCHW\"]\n    else:\n      data_formats = [\"NHWC\"]\n    for data_format in data_formats:\n      for dilation in [1, 2]:\n        for stride in [1, 2]:\n          for filter_dims in [[3, 3, 4, 8], [1, 1, 2, 16]]:\n            self._VerifyGroupConvFwd([10, 32, 32, 16], filter_dims,\n                                     dilations=[dilation, dilation],\n                                     strides=[stride, stride],\n                                     padding=\"SAME\",\n                                     data_format=data_format,\n                                     dtype=dtypes.float32)\n\n  @test_util.deprecated_graph_mode_only\n  @test_util.run_cuda_only\n  def testInputGradientGroupConv(self):\n    for data_format in [\"NCHW\", \"NHWC\"]:\n      for test_input in [True, False]:\n        self.ConstructAndTestGradient(\n            batch=2,\n            input_rows=5,\n            input_cols=4,\n            filter_rows=3,\n            filter_cols=3,\n            num_groups=2,\n            padding=\"VALID\",\n            in_depth=4,\n            out_depth=6,\n            stride_rows=1,\n            stride_cols=1,\n            test_input=test_input,\n            data_format=data_format,\n            use_gpu=True,\n            max_err=0.005)\n\n  @test_util.deprecated_graph_mode_only\n  @test_util.run_cuda_only\n  def testFilterGradientGroupConv(self):\n    for data_format in [\"NCHW\", \"NHWC\"]:\n      for test_input in [True, False]:\n        self.ConstructAndTestGradient(\n            batch=2,\n            input_rows=5,\n            input_cols=4,\n            filter_rows=3,\n            filter_cols=3,\n            num_groups=2,\n            padding=\"VALID\",\n            in_depth=4,\n            out_depth=6,\n            stride_rows=1,\n            stride_cols=1,\n            test_input=test_input,\n            data_format=data_format,\n            use_gpu=True,\n            max_err=0.005)\n  # TODO(yzhwang): this currently fails.\n  # self._VerifyValues(tensor_in_sizes=[1, 8, 8, 1],\n  #                   filter_in_sizes=[2, 2, 1, 1],\n  #                   strides=[4, 4], padding=\"SAME\",\n  #                   expected=[72, 112, 392, 432])\n\n  # Testing for backprops\n  def _RunAndVerifyBackpropInput(self,\n                                 input_sizes,\n                                 filter_sizes,\n                                 output_sizes,\n                                 strides,\n                                 padding,\n                                 expected,\n                                 data_format,\n                                 use_gpu,\n                                 err,\n                                 dilations=(1, 1)):\n    if use_gpu and not test.is_gpu_available(cuda_only=True):\n      return\n    x1 = self._CreateNumpyTensor(filter_sizes)\n    x2 = self._CreateNumpyTensor(output_sizes)\n    dilations = list(dilations)\n    with test_util.device(use_gpu):\n      if len(input_sizes) == 4:\n        if data_format == \"NCHW\":\n          input_sizes = test_util.NHWCToNCHW(input_sizes)\n      t0 = constant_op.constant(input_sizes, shape=[len(input_sizes)])\n      t1 = constant_op.constant(x1, shape=filter_sizes)\n      t2 = constant_op.constant(x2, shape=output_sizes)\n      strides = [1] + strides + [1]\n      dilations = [1] + dilations + [1]\n      if isinstance(padding, (list, tuple)):\n        padding = [(0, 0)] + padding + [(0, 0)]\n      if data_format == \"NCHW\":\n        t2 = test_util.NHWCToNCHW(t2)\n        strides = test_util.NHWCToNCHW(strides)\n        dilations = test_util.NHWCToNCHW(dilations)\n        if isinstance(padding, (list, tuple)):\n          padding = test_util.NHWCToNCHW((padding))\n      conv = nn_ops.conv2d_backprop_input(\n          t0,\n          t1,\n          t2,\n          strides=strides,\n          padding=padding,\n          data_format=data_format,\n          dilations=dilations)\n      if data_format == \"NCHW\":\n        conv = test_util.NCHWToNHWC(conv)\n      # \"values\" consists of two tensors for two backprops\n      value = self.evaluate(conv)\n      self.assertShapeEqual(value, conv)\n    tf_logging.debug(\"expected = %s\", expected)\n    tf_logging.debug(\"actual = %s\", value)\n    self.assertAllCloseAccordingToType(expected, value.flatten(), atol=1e-5)\n\n  def _CompareBackpropInput(self, input_sizes, filter_sizes, output_sizes,\n                            conv_strides, padding):\n    x1 = np.random.rand(*filter_sizes).astype(np.float32)\n    x2 = np.random.rand(*output_sizes).astype(np.float32)\n\n    def _GetVal(data_format, use_gpu):\n      with test_util.device(use_gpu):\n        if data_format == \"NCHW\":\n          new_input_sizes = test_util.NHWCToNCHW(input_sizes)\n        else:\n          new_input_sizes = input_sizes\n        t0 = constant_op.constant(new_input_sizes, shape=[len(new_input_sizes)])\n        t1 = constant_op.constant(x1, shape=filter_sizes)\n        t2 = constant_op.constant(x2, shape=output_sizes)\n        strides = [1] + conv_strides + [1]\n        if data_format == \"NCHW\":\n          t2 = test_util.NHWCToNCHW(t2)\n          strides = test_util.NHWCToNCHW(strides)\n        conv = nn_ops.conv2d_backprop_input(\n            t0,\n            t1,\n            t2,\n            strides=strides,\n            padding=padding,\n            data_format=data_format)\n        if data_format == \"NCHW\":\n          conv = test_util.NCHWToNHWC(conv)\n        ret = self.evaluate(conv)\n        self.assertShapeEqual(ret, conv)\n        return ret\n\n    values = []\n    for (data_format, use_gpu) in GetTestConfigs():\n      values.append(_GetVal(data_format, use_gpu))\n\n    for i in range(1, len(values)):\n      self.assertAllClose(values[0], values[i], rtol=1e-2, atol=1e-2)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D2x2Depth1ValidBackpropInput(self):\n    expected_output = [1.0, 4.0, 4.0, 3.0, 10.0, 8.0]\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropInput(\n          input_sizes=[1, 2, 3, 1],\n          filter_sizes=[2, 2, 1, 1],\n          output_sizes=[1, 1, 2, 1],\n          strides=[1, 1],\n          padding=\"VALID\",\n          expected=expected_output,\n          data_format=data_format,\n          use_gpu=use_gpu,\n          err=1e-5)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DEmptyBackpropInput(self):\n    expected_output = []\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropInput(\n          input_sizes=[0, 2, 3, 1],\n          filter_sizes=[2, 2, 1, 1],\n          output_sizes=[0, 1, 2, 1],\n          strides=[1, 1],\n          padding=\"VALID\",\n          expected=expected_output,\n          data_format=data_format,\n          use_gpu=use_gpu,\n          err=1e-5)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D2x2Depth3ValidBackpropInput(self):\n    expected_output = [\n        14.0, 32.0, 50.0, 100.0, 163.0, 226.0, 167.0, 212.0, 257.0, 122.0,\n        140.0, 158.0, 478.0, 541.0, 604.0, 437.0, 482.0, 527.0\n    ]\n    for (data_format, use_gpu) in GetTestConfigs():\n      # The GPU version of this test is not very stable. So adjusting the\n      # error threshold to 1e-4.\n      self._RunAndVerifyBackpropInput(\n          input_sizes=[1, 2, 3, 3],\n          filter_sizes=[2, 2, 3, 3],\n          output_sizes=[1, 1, 2, 3],\n          strides=[1, 1],\n          padding=\"VALID\",\n          expected=expected_output,\n          data_format=data_format,\n          use_gpu=use_gpu,\n          err=1e-4)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D2x2Depth3ValidBackpropInputStride1x2(self):\n    expected_output = [\n        1.0, 2.0, 2.0, 4.0, 3.0, 6.0, 7.0, 12.0, 11.0, 18.0, 15.0, 24.0, 12.0,\n        16.0, 15.0, 20.0, 18.0, 24.0\n    ]\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropInput(\n          input_sizes=[1, 3, 6, 1],\n          filter_sizes=[2, 2, 1, 1],\n          output_sizes=[1, 2, 3, 1],\n          strides=[1, 2],\n          padding=\"VALID\",\n          expected=expected_output,\n          data_format=data_format,\n          use_gpu=use_gpu,\n          err=1e-5)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DStrideTwoFilterOneSameBackpropInput(self):\n    expected_output = [\n        1.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 0.0, 4.0, 0.0, 0.0, 0.0,\n        0.0, 0.0\n    ]\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropInput(\n          input_sizes=[1, 4, 4, 1],\n          filter_sizes=[1, 1, 1, 1],\n          output_sizes=[1, 2, 2, 1],\n          strides=[2, 2],\n          padding=\"SAME\",\n          expected=expected_output,\n          data_format=data_format,\n          use_gpu=use_gpu,\n          err=1e-5)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DKernelSizeMatchesInputSizeBackpropInput(self):\n    expected_output = [5.0, 11.0, 17.0, 23.0]\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropInput(\n          input_sizes=[1, 2, 2, 1],\n          filter_sizes=[2, 2, 1, 2],\n          output_sizes=[1, 1, 1, 2],\n          strides=[1, 1],\n          padding=\"VALID\",\n          expected=expected_output,\n          data_format=data_format,\n          use_gpu=use_gpu,\n          err=1e-5)\n\n  @test_util.run_in_graph_and_eager_modes\n  @test_util.disable_xla(\"XLA requires input_sizes to be a 4D shape.\")\n  def testConv2DInputSizesContainsOnlySpatialDimensionsBackpropInput(self):\n    expected_output = [5.0, 11.0, 17.0, 23.0]\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropInput(\n          input_sizes=[2, 2],\n          filter_sizes=[2, 2, 1, 2],\n          output_sizes=[1, 1, 1, 2],\n          strides=[1, 1],\n          padding=\"VALID\",\n          expected=expected_output,\n          data_format=data_format,\n          use_gpu=use_gpu,\n          err=1e-5)\n\n  @test_util.run_in_graph_and_eager_modes\n  @test_util.disable_xla(\"b/239598470\")\n  def testConv2DBackpropInputDegenerateBackpropInput(self):\n    input_sizes = [3, 1, 1, 2]\n    expected_output = np.zeros(input_sizes).flatten()\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropInput(\n          input_sizes=input_sizes,\n          filter_sizes=[1, 3, 2, 3],\n          output_sizes=[3, 1, 0, 3],\n          strides=[1, 2],\n          padding=\"VALID\",\n          expected=expected_output,\n          data_format=data_format,\n          use_gpu=use_gpu,\n          err=1e-5)\n\n  # Testing for backprops\n  def _RunAndVerifyBackpropFilter(self,\n                                  input_sizes,\n                                  filter_sizes,\n                                  output_sizes,\n                                  strides,\n                                  padding,\n                                  expected,\n                                  data_format,\n                                  use_gpu,\n                                  dilations=(1, 1),\n                                  err=1e-5):\n    x0 = self._CreateNumpyTensor(input_sizes)\n    x2 = self._CreateNumpyTensor(output_sizes)\n    dilations = list(dilations)\n    explicit_strides = [1] + strides + [1]\n    new_padding = padding\n    new_dilations = [1] + dilations + [1]\n    if isinstance(new_padding, (list, tuple)):\n      new_padding = [(0, 0)] + new_padding + [(0, 0)]\n    if data_format == \"NCHW\":\n      explicit_strides = test_util.NHWCToNCHW(explicit_strides)\n      new_dilations = test_util.NHWCToNCHW(new_dilations)\n      if isinstance(padding, (list, tuple)):\n        new_padding = test_util.NHWCToNCHW(new_padding)\n    for dtype in self._DtypesToTest(use_gpu=use_gpu):\n      with test_util.device(use_gpu):\n        t0 = constant_op.constant(x0, shape=input_sizes, dtype=dtype)\n        t1 = constant_op.constant(filter_sizes, shape=[len(filter_sizes)])\n        t2 = constant_op.constant(x2, shape=output_sizes, dtype=dtype)\n        if data_format == \"NCHW\":\n          t0 = test_util.NHWCToNCHW(t0)\n          t2 = test_util.NHWCToNCHW(t2)\n        conv = nn_ops.conv2d_backprop_filter(\n            t0,\n            t1,\n            t2,\n            strides=explicit_strides,\n            padding=new_padding,\n            dilations=new_dilations,\n            data_format=data_format)\n        value = self.evaluate(conv)\n        self.assertShapeEqual(value, conv)\n      tf_logging.debug(\"expected = %s\", expected)\n      tf_logging.debug(\"actual = %s\", value)\n      self.assertArrayNear(expected, value.flatten(), err)\n\n  def _CompareBackFilter(self, input_sizes, filter_sizes, output_sizes,\n                         conv_strides, padding):\n    x0 = np.random.rand(*input_sizes).astype(np.float32)\n    x2 = np.random.rand(*output_sizes).astype(np.float32)\n\n    def _GetVal(data_format, use_gpu):\n      with test_util.device(use_gpu):\n        t0 = constant_op.constant(x0, shape=input_sizes)\n        t1 = constant_op.constant(filter_sizes, shape=[len(filter_sizes)])\n        t2 = constant_op.constant(x2, shape=output_sizes)\n        strides = [1] + conv_strides + [1]\n        if data_format == \"NCHW\":\n          t0 = test_util.NHWCToNCHW(t0)\n          t2 = test_util.NHWCToNCHW(t2)\n          strides = test_util.NHWCToNCHW(strides)\n        conv = nn_ops.conv2d_backprop_filter(\n            t0,\n            t1,\n            t2,\n            strides=strides,\n            padding=padding,\n            data_format=data_format)\n        ret = self.evaluate(conv)\n        self.assertShapeEqual(ret, conv)\n        return ret\n\n    values = []\n    for (data_format, use_gpu) in GetTestConfigs():\n      values.append(_GetVal(data_format, use_gpu))\n    for i in range(1, len(values)):\n      self.assertAllClose(values[0], values[i], rtol=1e-4, atol=1e-4)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D2x2Depth1ValidBackpropFilter(self):\n    expected = [5.0, 8.0, 14.0, 17.0]\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropFilter(\n          input_sizes=[1, 2, 3, 1],\n          filter_sizes=[2, 2, 1, 1],\n          output_sizes=[1, 1, 2, 1],\n          strides=[1, 1],\n          padding=\"VALID\",\n          expected=expected,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DEmptyBackpropFilter(self):\n    expected = []\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropFilter(\n          input_sizes=[1, 2, 3, 1],\n          filter_sizes=[2, 2, 1, 0],\n          output_sizes=[1, 1, 2, 0],\n          strides=[1, 1],\n          padding=\"VALID\",\n          expected=expected,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DBackpropFilterWithEmptyInput(self):\n    expected = [0, 0, 0, 0]\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropFilter(\n          input_sizes=[0, 2, 3, 1],\n          filter_sizes=[2, 2, 1, 1],\n          output_sizes=[0, 1, 2, 1],\n          strides=[1, 1],\n          padding=\"VALID\",\n          expected=expected,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D2x2Depth3ValidBackpropFilter(self):\n    expected = [\n        17.0, 22.0, 27.0, 22.0, 29.0, 36.0, 27.0, 36.0, 45.0, 32.0, 43.0, 54.0,\n        37.0, 50.0, 63.0, 42.0, 57.0, 72.0, 62.0, 85.0, 108.0, 67.0, 92.0,\n        117.0, 72.0, 99.0, 126.0, 77.0, 106.0, 135.0, 82.0, 113.0, 144.0, 87.0,\n        120.0, 153.0\n    ]\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropFilter(\n          input_sizes=[1, 2, 3, 3],\n          filter_sizes=[2, 2, 3, 3],\n          output_sizes=[1, 1, 2, 3],\n          strides=[1, 1],\n          padding=\"VALID\",\n          expected=expected,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D2x2Depth3ValidBackpropFilterStride1x2(self):\n    expected = [161.0, 182.0, 287.0, 308.0]\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropFilter(\n          input_sizes=[1, 3, 6, 1],\n          filter_sizes=[2, 2, 1, 1],\n          output_sizes=[1, 2, 3, 1],\n          strides=[1, 2],\n          padding=\"VALID\",\n          expected=expected,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DStrideTwoFilterOneSameBackpropFilter(self):\n    expected_output = [78.]\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropFilter(\n          input_sizes=[1, 4, 4, 1],\n          filter_sizes=[1, 1, 1, 1],\n          output_sizes=[1, 2, 2, 1],\n          strides=[2, 2],\n          padding=\"SAME\",\n          expected=expected_output,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DKernelSizeMatchesInputSizeBackpropFilter(self):\n    expected_output = [1.0, 2.0, 2.0, 4.0, 3.0, 6.0, 4.0, 8.0]\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropFilter(\n          input_sizes=[1, 2, 2, 1],\n          filter_sizes=[2, 2, 1, 2],\n          output_sizes=[1, 1, 1, 2],\n          strides=[1, 1],\n          padding=\"VALID\",\n          expected=expected_output,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  # Testing for backprops\n  def _RunAndVerifyBackpropInputDilation(self, input_sizes, filter_sizes,\n                                         output_sizes, strides, dilations,\n                                         padding, data_format, use_gpu, err):\n    x1 = self._CreateNumpyTensor(input_sizes)\n    x2 = self._CreateNumpyTensor(filter_sizes)\n    default_dilations = (dilations[0] == 1 and dilations[1] == 1)\n    if default_dilations or use_gpu:\n      with self.cached_session(use_gpu=use_gpu) as sess:\n        if data_format == \"NCHW\":\n          input_sizes = test_util.NHWCToNCHW(input_sizes)\n        t1 = constant_op.constant(x1, shape=input_sizes)\n        t2 = constant_op.constant(x2, shape=filter_sizes)\n        full_strides = [1] + strides + [1]\n        full_dilations = [1] + dilations + [1]\n        if data_format == \"NCHW\":\n          full_strides = test_util.NHWCToNCHW(full_strides)\n          full_dilations = test_util.NHWCToNCHW(full_dilations)\n        conv_forward = nn_ops.conv2d(\n            t1,\n            t2,\n            strides=full_strides,\n            dilations=full_dilations,\n            padding=padding,\n            data_format=data_format)\n        conv_forward_2 = nn_ops.convolution(\n            t1,\n            t2,\n            padding=padding,\n            strides=strides,\n            dilation_rate=dilations,\n            data_format=data_format)\n        if data_format == \"NCHW\":\n          conv_forward = test_util.NCHWToNHWC(conv_forward)\n          conv_forward_2 = test_util.NCHWToNHWC(conv_forward_2)\n        conv = gradients_impl.gradients(conv_forward, t1)[0]\n        conv_2 = gradients_impl.gradients(conv_forward_2, t1)[0]\n        # \"values\" consists of two tensors for two backprops\n        value = self.evaluate(conv)\n        value_2 = self.evaluate(conv_2)\n        self.assertShapeEqual(value, conv)\n        self.assertShapeEqual(value_2, conv_2)\n      tf_logging.debug(\"expected = %s\", value_2)\n      tf_logging.debug(\"actual = %s\", value)\n      self.assertArrayNear(value_2.flatten(), value.flatten(), err)\n\n  # Testing for backprops\n  def _RunAndVerifyBackpropFilterDilation(self, input_sizes, filter_sizes,\n                                          output_sizes, strides, dilations,\n                                          padding, data_format, use_gpu, err):\n    x1 = self._CreateNumpyTensor(input_sizes)\n    x2 = self._CreateNumpyTensor(filter_sizes)\n    default_dilations = (dilations[0] == 1 and dilations[1] == 1)\n    if default_dilations or use_gpu:\n      with self.cached_session(use_gpu=use_gpu) as sess:\n        if data_format == \"NCHW\":\n          input_sizes = test_util.NHWCToNCHW(input_sizes)\n        t1 = constant_op.constant(x1, shape=input_sizes)\n        t2 = constant_op.constant(x2, shape=filter_sizes)\n        full_strides = [1] + strides + [1]\n        full_dilations = [1] + dilations + [1]\n        if data_format == \"NCHW\":\n          full_strides = test_util.NHWCToNCHW(full_strides)\n          full_dilations = test_util.NHWCToNCHW(full_dilations)\n        conv_forward = nn_ops.conv2d(\n            t1,\n            t2,\n            strides=full_strides,\n            dilations=full_dilations,\n            padding=padding,\n            data_format=data_format)\n        conv_forward_2 = nn_ops.convolution(\n            t1,\n            t2,\n            padding=padding,\n            strides=strides,\n            dilation_rate=dilations,\n            data_format=data_format)\n        if data_format == \"NCHW\":\n          conv_forward = test_util.NCHWToNHWC(conv_forward)\n          conv_forward_2 = test_util.NCHWToNHWC(conv_forward_2)\n        conv = gradients_impl.gradients(conv_forward, t2)[0]\n        conv_2 = gradients_impl.gradients(conv_forward, t2)[0]\n        value = self.evaluate(conv)\n        value_2 = self.evaluate(conv_2)\n        self.assertShapeEqual(value, conv)\n        self.assertShapeEqual(value_2, conv_2)\n      tf_logging.debug(\"expected = %s\", value_2)\n      tf_logging.debug(\"actual = %s\", value)\n      self.assertArrayNear(value_2.flatten(), value.flatten(), err)\n\n  @test_util.deprecated_graph_mode_only\n  def testConv2D2x2Depth3ValidBackpropFilterStride1x1Dilation2x1(self):\n    if test.is_gpu_available(cuda_only=True) or test_util.IsMklEnabled():\n      for (data_format, use_gpu) in GetTestConfigs():\n        self._RunAndVerifyBackpropFilterDilation(\n            input_sizes=[1, 3, 6, 1],\n            filter_sizes=[2, 2, 1, 1],\n            output_sizes=[1, 1, 5, 1],\n            strides=[1, 1],\n            dilations=[2, 1],\n            padding=\"VALID\",\n            data_format=data_format,\n            use_gpu=use_gpu,\n            err=1e-5)\n\n  @test_util.deprecated_graph_mode_only\n  def testConv2D2x2Depth1ValidBackpropFilterDilation1x2(self):\n    if test.is_gpu_available(cuda_only=True) or test_util.IsMklEnabled():\n      for (data_format, use_gpu) in GetTestConfigs():\n        self._RunAndVerifyBackpropFilterDilation(\n            input_sizes=[1, 2, 3, 1],\n            filter_sizes=[2, 2, 1, 1],\n            output_sizes=[1, 1, 2, 1],\n            strides=[1, 1],\n            dilations=[1, 2],\n            padding=\"VALID\",\n            data_format=data_format,\n            use_gpu=use_gpu,\n            err=1e-5)\n\n  @test_util.deprecated_graph_mode_only\n  def testConv2DEmptyBackpropFilterDilation1x2(self):\n    if test.is_gpu_available(cuda_only=True) or test_util.IsMklEnabled():\n      for (data_format, use_gpu) in GetTestConfigs():\n        self._RunAndVerifyBackpropFilterDilation(\n            input_sizes=[1, 2, 3, 1],\n            filter_sizes=[2, 2, 1, 0],\n            output_sizes=[1, 1, 2, 0],\n            strides=[1, 1],\n            dilations=[1, 2],\n            padding=\"VALID\",\n            data_format=data_format,\n            use_gpu=use_gpu,\n            err=1e-5)\n\n  @test_util.deprecated_graph_mode_only\n  def testConv2D2x2Depth3ValidBackpropFilterDilation2x2(self):\n    if test.is_gpu_available(cuda_only=True) or test_util.IsMklEnabled():\n      for (data_format, use_gpu) in GetTestConfigs():\n        self._RunAndVerifyBackpropFilterDilation(\n            input_sizes=[1, 3, 4, 3],\n            filter_sizes=[2, 2, 3, 3],\n            output_sizes=[1, 1, 2, 3],\n            strides=[1, 1],\n            dilations=[2, 2],\n            padding=\"VALID\",\n            data_format=data_format,\n            use_gpu=use_gpu,\n            err=1e-5)\n\n  @test_util.deprecated_graph_mode_only\n  def testConv2DKernelSizeMatchesInputSizeBackpropFilterDilation2x2(self):\n    if test.is_gpu_available(cuda_only=True) or test_util.IsMklEnabled():\n      for (data_format, use_gpu) in GetTestConfigs():\n        self._RunAndVerifyBackpropFilterDilation(\n            input_sizes=[1, 3, 3, 1],\n            filter_sizes=[2, 2, 1, 2],\n            output_sizes=[1, 1, 1, 2],\n            strides=[1, 1],\n            dilations=[2, 2],\n            padding=\"VALID\",\n            data_format=data_format,\n            use_gpu=use_gpu,\n            err=1e-5)\n\n  @test_util.deprecated_graph_mode_only\n  def testConv2D2x2Depth3ValidBackpropInputStride1x1Dilation2x1(self):\n    if test.is_gpu_available(cuda_only=True) or test_util.IsMklEnabled():\n      for (data_format, use_gpu) in GetTestConfigs():\n        self._RunAndVerifyBackpropInputDilation(\n            input_sizes=[1, 3, 6, 1],\n            filter_sizes=[2, 2, 1, 1],\n            output_sizes=[1, 1, 5, 1],\n            strides=[1, 1],\n            dilations=[2, 1],\n            padding=\"VALID\",\n            data_format=data_format,\n            use_gpu=use_gpu,\n            err=1e-5)\n\n  @test_util.deprecated_graph_mode_only\n  def testConv2D2x2Depth1ValidBackpropInputDilation1x2(self):\n    if test.is_gpu_available(cuda_only=True) or test_util.IsMklEnabled():\n      for (data_format, use_gpu) in GetTestConfigs():\n        self._RunAndVerifyBackpropInputDilation(\n            input_sizes=[1, 2, 3, 1],\n            filter_sizes=[2, 2, 1, 1],\n            output_sizes=[1, 1, 2, 1],\n            strides=[1, 1],\n            dilations=[1, 2],\n            padding=\"VALID\",\n            data_format=data_format,\n            use_gpu=use_gpu,\n            err=1e-5)\n\n  @test_util.deprecated_graph_mode_only\n  def testConv2DEmptyBackpropInputDilation1x2(self):\n    if test.is_gpu_available(cuda_only=True) or test_util.IsMklEnabled():\n      for (data_format, use_gpu) in GetTestConfigs():\n        self._RunAndVerifyBackpropInputDilation(\n            input_sizes=[0, 2, 3, 1],\n            filter_sizes=[2, 2, 1, 1],\n            output_sizes=[0, 1, 2, 1],\n            strides=[1, 1],\n            dilations=[1, 2],\n            padding=\"VALID\",\n            data_format=data_format,\n            use_gpu=use_gpu,\n            err=1e-5)\n\n  @test_util.deprecated_graph_mode_only\n  def testConv2D2x2Depth3ValidBackpropInputDilation2x1(self):\n    if test.is_gpu_available(cuda_only=True) or test_util.IsMklEnabled():\n      for (data_format, use_gpu) in GetTestConfigs():\n        # The GPU version of this test is not very stable. So adjusting the\n        # error threshold to 1e-4.\n        self._RunAndVerifyBackpropInputDilation(\n            input_sizes=[1, 3, 2, 3],\n            filter_sizes=[2, 2, 3, 3],\n            output_sizes=[1, 1, 2, 3],\n            strides=[1, 1],\n            dilations=[2, 1],\n            padding=\"VALID\",\n            data_format=data_format,\n            use_gpu=use_gpu,\n            err=1e-4)\n\n  @test_util.deprecated_graph_mode_only\n  def testConv2DKernelSizeMatchesInputSizeBackpropInputDilation2x2(self):\n    if test.is_gpu_available(cuda_only=True) or test_util.IsMklEnabled():\n      for (data_format, use_gpu) in GetTestConfigs():\n        self._RunAndVerifyBackpropInputDilation(\n            input_sizes=[1, 3, 3, 1],\n            filter_sizes=[2, 2, 1, 2],\n            output_sizes=[1, 1, 1, 2],\n            strides=[1, 1],\n            dilations=[2, 2],\n            padding=\"VALID\",\n            data_format=data_format,\n            use_gpu=use_gpu,\n            err=1e-5)\n\n  def _RunAndVerifyBackpropInputExplicitPadding(self,\n                                                input_sizes,\n                                                filter_sizes,\n                                                output_sizes,\n                                                strides,\n                                                padding,\n                                                data_format,\n                                                use_gpu,\n                                                dilations=(1, 1),\n                                                err=2e-5):\n    if use_gpu and not test.is_gpu_available(cuda_only=True):\n      return\n    if not use_gpu and dilations != (1, 1):\n      return  # Non-default dilations is currently not supported on the CPU.\n\n    x1 = self._CreateNumpyTensor(filter_sizes)\n    x2 = self._CreateNumpyTensor(output_sizes)\n    dilations = list(dilations)\n    padded_input_sizes = input_sizes[:]\n    padded_input_sizes[1] += padding[0][0] + padding[0][1]\n    padded_input_sizes[2] += padding[1][0] + padding[1][1]\n    c = nn_ops.conv2d_backprop_input(\n        padded_input_sizes,\n        x1,\n        x2,\n        strides=[1] + strides + [1],\n        padding=\"VALID\",\n        dilations=[1] + dilations + [1])\n    c = c[:, padding[0][0]:(c.shape[1] - padding[0][1]), padding[1][0]:(\n        c.shape[2] - padding[1][1]), :]\n    expected = list(self.evaluate(array_ops.reshape(c, [-1])))\n    self._RunAndVerifyBackpropInput(\n        input_sizes,\n        filter_sizes,\n        output_sizes,\n        strides,\n        padding,\n        expected,\n        data_format,\n        use_gpu=use_gpu,\n        err=err,\n        dilations=dilations)\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2D2x2Depth1Padding0x0BackpropInput(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropInputExplicitPadding(\n          input_sizes=[1, 2, 3, 1],\n          filter_sizes=[2, 2, 1, 1],\n          output_sizes=[1, 1, 2, 1],\n          strides=[1, 1],\n          padding=[[0, 0], [0, 0]],\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n      self._RunAndVerifyBackpropInputExplicitPadding(\n          input_sizes=[1, 3, 4, 2],\n          filter_sizes=[2, 2, 2, 3],\n          output_sizes=[1, 1, 2, 3],\n          strides=[2, 2],\n          padding=[[0, 0], [0, 0]],\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2D2x2Depth1Padding1x1BackpropInput(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropInputExplicitPadding(\n          input_sizes=[1, 2, 3, 1],\n          filter_sizes=[2, 2, 1, 2],\n          output_sizes=[1, 3, 4, 2],\n          strides=[1, 1],\n          padding=[[1, 1], [1, 1]],\n          data_format=data_format,\n          use_gpu=use_gpu,\n          err=1e-4)\n\n      self._RunAndVerifyBackpropInputExplicitPadding(\n          input_sizes=[1, 2, 3, 2],\n          filter_sizes=[1, 1, 2, 1],\n          output_sizes=[1, 4, 3, 1],\n          strides=[1, 2],\n          padding=[[1, 1], [1, 1]],\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n      self._RunAndVerifyBackpropInputExplicitPadding(\n          input_sizes=[1, 4, 3, 1],\n          filter_sizes=[2, 2, 1, 1],\n          output_sizes=[1, 4, 2, 1],\n          strides=[1, 2],\n          padding=[[1, 1], [1, 1]],\n          data_format=data_format,\n          dilations=[2, 2], use_gpu=use_gpu)\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2D2x2Depth1Padding2x2BackpropInput(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropInputExplicitPadding(\n          input_sizes=[2, 3, 1, 1],\n          filter_sizes=[2, 1, 1, 1],\n          output_sizes=[2, 2, 5, 1],\n          strides=[3, 1],\n          padding=[[2, 2], [2, 2]],\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n      self._RunAndVerifyBackpropInputExplicitPadding(\n          input_sizes=[1, 3, 6, 1],\n          filter_sizes=[3, 2, 1, 1],\n          output_sizes=[1, 3, 4, 1],\n          strides=[1, 2],\n          padding=[[2, 2], [2, 2]],\n          data_format=data_format,\n          dilations=[2, 3],\n          use_gpu=use_gpu)\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2D2x2Depth1Padding_1_8_4_1_BackpropInput(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropInputExplicitPadding(\n          input_sizes=[1, 2, 3, 1],\n          filter_sizes=[2, 2, 1, 1],\n          output_sizes=[1, 10, 8, 1],\n          strides=[1, 1],\n          padding=[[1, 8], [4, 2]],\n          data_format=data_format,\n          use_gpu=use_gpu,\n          err=5e-5)\n\n      self._RunAndVerifyBackpropInputExplicitPadding(\n          input_sizes=[1, 5, 3, 1],\n          filter_sizes=[3, 2, 1, 1],\n          output_sizes=[1, 4, 8, 1],\n          strides=[3, 1],\n          padding=[[1, 8], [4, 2]],\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2D2x2Depth1Padding_5_0_2_2_BackpropInput(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropInputExplicitPadding(\n          input_sizes=[1, 3, 3, 1],\n          filter_sizes=[2, 1, 1, 1],\n          output_sizes=[1, 7, 7, 1],\n          strides=[1, 1],\n          padding=[[5, 0], [2, 2]],\n          data_format=data_format,\n          err=5e-5,\n          use_gpu=use_gpu)\n\n      self._RunAndVerifyBackpropInputExplicitPadding(\n          input_sizes=[1, 4, 2, 1],\n          filter_sizes=[3, 3, 1, 1],\n          output_sizes=[1, 5, 2, 1],\n          strides=[1, 2],\n          padding=[[5, 0], [2, 2]],\n          data_format=data_format,\n          dilations=[2, 1],\n          use_gpu=use_gpu)\n\n  def _RunAndVerifyBackpropFilterExplicitPadding(self,\n                                                 input_sizes,\n                                                 filter_sizes,\n                                                 output_sizes,\n                                                 strides,\n                                                 padding,\n                                                 data_format,\n                                                 use_gpu,\n                                                 dilations=(1, 1),\n                                                 err=1e-5):\n    if use_gpu and not test.is_gpu_available(cuda_only=True):\n      return\n    if not use_gpu and dilations != (1, 1):\n      return  # Non-default dilations is currently not supported on the CPU.\n\n    x0 = self._CreateNumpyTensor(input_sizes)\n    x2 = self._CreateNumpyTensor(output_sizes)\n    dilations = list(dilations)\n\n    x0 = np.pad(x0, [(0, 0)] + padding + [(0, 0)], \"constant\")\n    c = nn_ops.conv2d_backprop_filter(\n        x0,\n        filter_sizes,\n        x2,\n        strides=[1] + strides + [1],\n        padding=\"VALID\",\n        dilations=[1] + dilations + [1])\n    expected = list(self.evaluate(array_ops.reshape(c, [-1])))\n    self._RunAndVerifyBackpropFilter(\n        input_sizes,\n        filter_sizes,\n        output_sizes,\n        strides,\n        padding,\n        expected,\n        data_format,\n        use_gpu=use_gpu,\n        dilations=dilations,\n        err=err)\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2D2x2Depth1Padding0x0BackpropFilter(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropFilterExplicitPadding(\n          input_sizes=[1, 2, 3, 1],\n          filter_sizes=[2, 2, 1, 1],\n          output_sizes=[1, 1, 2, 1],\n          strides=[1, 1],\n          padding=[[0, 0], [0, 0]],\n          data_format=data_format, use_gpu=use_gpu)\n\n      self._RunAndVerifyBackpropFilterExplicitPadding(\n          input_sizes=[1, 3, 4, 2],\n          filter_sizes=[2, 2, 2, 3],\n          output_sizes=[1, 1, 2, 3],\n          strides=[2, 2],\n          padding=[[0, 0], [0, 0]],\n          data_format=data_format, use_gpu=use_gpu)\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2D2x2Depth1Padding1x1BackpropFilter(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropFilterExplicitPadding(\n          input_sizes=[1, 2, 3, 1],\n          filter_sizes=[2, 2, 1, 2],\n          output_sizes=[1, 3, 4, 2],\n          strides=[1, 1],\n          padding=[[1, 1], [1, 1]],\n          data_format=data_format,\n          use_gpu=use_gpu,\n          err=5e-5)\n\n      self._RunAndVerifyBackpropFilterExplicitPadding(\n          input_sizes=[1, 2, 3, 2],\n          filter_sizes=[1, 1, 2, 1],\n          output_sizes=[1, 4, 3, 1],\n          strides=[1, 2],\n          padding=[[1, 1], [1, 1]],\n          use_gpu=use_gpu,\n          data_format=data_format)\n\n      self._RunAndVerifyBackpropFilterExplicitPadding(\n          input_sizes=[1, 4, 3, 1],\n          filter_sizes=[2, 2, 1, 1],\n          output_sizes=[1, 4, 2, 1],\n          strides=[1, 2],\n          padding=[[1, 1], [1, 1]],\n          data_format=data_format,\n          use_gpu=use_gpu,\n          dilations=[2, 2])\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2D2x2Depth1Padding2x2BackpropFilter(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropFilterExplicitPadding(\n          input_sizes=[2, 3, 1, 1],\n          filter_sizes=[2, 1, 1, 1],\n          output_sizes=[2, 2, 5, 1],\n          strides=[3, 1],\n          padding=[[2, 2], [2, 2]],\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n      self._RunAndVerifyBackpropFilterExplicitPadding(\n          input_sizes=[1, 3, 6, 1],\n          filter_sizes=[3, 2, 1, 1],\n          output_sizes=[1, 3, 4, 1],\n          strides=[1, 2],\n          padding=[[2, 2], [2, 2]],\n          data_format=data_format,\n          use_gpu=use_gpu,\n          dilations=[2, 3])\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2D2x2Depth1Padding_1_8_4_1_BackpropFilter(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropFilterExplicitPadding(\n          input_sizes=[1, 2, 3, 1],\n          filter_sizes=[2, 2, 1, 1],\n          output_sizes=[1, 10, 8, 1],\n          strides=[1, 1],\n          padding=[[1, 8], [4, 2]],\n          data_format=data_format,\n          use_gpu=use_gpu,\n          err=1e-4)\n\n      self._RunAndVerifyBackpropFilterExplicitPadding(\n          input_sizes=[1, 5, 3, 1],\n          filter_sizes=[3, 2, 1, 1],\n          output_sizes=[1, 4, 8, 1],\n          strides=[3, 1],\n          padding=[[1, 8], [4, 2]],\n          use_gpu=use_gpu,\n          data_format=data_format)\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2D2x2Depth1Padding_5_0_2_2_BackpropFilter(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropFilterExplicitPadding(\n          input_sizes=[1, 3, 3, 1],\n          filter_sizes=[2, 1, 1, 1],\n          output_sizes=[1, 7, 7, 1],\n          strides=[1, 1],\n          padding=[[5, 0], [2, 2]],\n          data_format=data_format,\n          use_gpu=use_gpu,\n          err=1e-4)\n\n      self._RunAndVerifyBackpropFilterExplicitPadding(\n          input_sizes=[1, 4, 2, 1],\n          filter_sizes=[3, 3, 1, 1],\n          output_sizes=[1, 5, 2, 1],\n          strides=[1, 2],\n          padding=[[5, 0], [2, 2]],\n          data_format=data_format,\n          use_gpu=use_gpu,\n          dilations=[2, 1])\n\n  # Gradient checkers\n  def ConstructAndTestGradient(self,\n                               batch,\n                               input_rows,\n                               input_cols,\n                               filter_rows,\n                               filter_cols,\n                               in_depth,\n                               out_depth,\n                               stride_rows,\n                               stride_cols,\n                               padding,\n                               test_input,\n                               data_format,\n                               use_gpu,\n                               num_groups=1,\n                               max_err=0.003):\n    assert in_depth % num_groups == 0 and out_depth % num_groups == 0\n    input_shape = [batch, input_rows, input_cols, in_depth]\n    filter_shape = [filter_rows, filter_cols, in_depth // num_groups, out_depth]\n    # TODO(yangke): re-factor the computation of output shape.\n    if padding == \"VALID\":\n      output_rows = (input_rows - filter_rows + stride_rows) // stride_rows\n      output_cols = (input_cols - filter_cols + stride_cols) // stride_cols\n    elif padding == \"SAME\":\n      output_rows = (input_rows + stride_rows - 1) // stride_rows\n      output_cols = (input_cols + stride_cols - 1) // stride_cols\n    else:\n      self.assertIsInstance(padding, (list, tuple))\n      output_rows = (input_rows + padding[1][0] + padding[1][1] - filter_rows +\n                     stride_rows) // stride_rows\n      output_cols = (input_cols + padding[2][0] + padding[2][1] - filter_cols +\n                     stride_cols) // stride_cols\n    output_shape = [batch, output_rows, output_cols, out_depth]\n    input_size = 1\n    for x in input_shape:\n      input_size *= x\n    filter_size = 1\n    for x in filter_shape:\n      filter_size *= x\n    input_data = [x * 1.0 / input_size for x in range(0, input_size)]\n    filter_data = [x * 1.0 / filter_size for x in range(0, filter_size)]\n    # Conv2DGrad functions are not compiled for double due to\n    # a problem in the way Eigen's Conv2DGrad works for double.\n    # So we disable the DOUBLE path.  We should re-enable this\n    # when double support returns for CPU and/or GPU.\n    for dtype in self._DtypesToTest(use_gpu=use_gpu):\n      with self.cached_session(use_gpu=use_gpu):\n        input_tensor = constant_op.constant(\n            input_data, shape=input_shape, dtype=dtype, name=\"input\")\n        filter_tensor = constant_op.constant(\n            filter_data, shape=filter_shape, dtype=dtype, name=\"filter\")\n        strides = [1, stride_rows, stride_cols, 1]\n        new_padding = padding\n        if data_format == \"NCHW\":\n          new_input_tensor = test_util.NHWCToNCHW(input_tensor)\n          strides = test_util.NHWCToNCHW(strides)\n          if isinstance(padding, (list, tuple)):\n            new_padding = test_util.NHWCToNCHW(padding)\n        else:\n          new_input_tensor = input_tensor\n        conv = nn_ops.conv2d(\n            new_input_tensor,\n            filter_tensor,\n            strides,\n            new_padding,\n            data_format=data_format,\n            name=\"conv\")\n        if data_format == \"NCHW\":\n          conv = test_util.NCHWToNHWC(conv)\n        self.assertEqual(output_shape, conv.get_shape())\n        if test_input:\n          jacob_t, jacob_n = gradient_checker.compute_gradient(input_tensor,\n                                                               input_shape,\n                                                               conv,\n                                                               output_shape)\n        else:\n          jacob_t, jacob_n = gradient_checker.compute_gradient(filter_tensor,\n                                                               filter_shape,\n                                                               conv,\n                                                               output_shape)\n        if dtype == dtypes.float32:\n          reference_jacob_t = jacob_t\n          err = np.fabs(jacob_t - jacob_n).max()\n        else:\n          # Compare fp16 theoretical gradients to fp32 theoretical gradients,\n          # since fp16 numerical gradients are too imprecise.\n          err = np.fabs(jacob_t - reference_jacob_t).max()\n\n        tf_logging.debug(\"conv_2d gradient error = %s\", err)\n        self.assertLess(err, max_err)\n\n  @test_util.deprecated_graph_mode_only\n  def testInputGradientValidPaddingStrideOne(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=5,\n          input_cols=4,\n          filter_rows=3,\n          filter_cols=3,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=1,\n          stride_cols=1,\n          padding=\"VALID\",\n          test_input=True,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testFilterGradientValidPaddingStrideOne(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=4,\n          input_rows=6,\n          input_cols=5,\n          filter_rows=2,\n          filter_cols=2,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=1,\n          stride_cols=1,\n          padding=\"VALID\",\n          test_input=False,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testInputGradientValidPaddingStrideTwo(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=4,\n          input_cols=5,\n          filter_rows=3,\n          filter_cols=3,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=2,\n          stride_cols=2,\n          padding=\"VALID\",\n          test_input=True,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testFilterGradientValidPaddingStrideTwo(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=4,\n          input_rows=6,\n          input_cols=5,\n          filter_rows=2,\n          filter_cols=2,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=2,\n          stride_cols=2,\n          padding=\"VALID\",\n          test_input=False,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testInputGradientValidPaddingStrideThree(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=7,\n          input_cols=6,\n          filter_rows=3,\n          filter_cols=3,\n          in_depth=4,\n          out_depth=5,\n          stride_rows=3,\n          stride_cols=3,\n          padding=\"VALID\",\n          test_input=True,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testFilterGradientValidPaddingStrideThree(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=8,\n          input_cols=7,\n          filter_rows=4,\n          filter_cols=4,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=3,\n          stride_cols=3,\n          padding=\"VALID\",\n          test_input=False,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testInputGradientSamePaddingStrideOne(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=7,\n          input_cols=6,\n          filter_rows=3,\n          filter_cols=3,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=1,\n          stride_cols=1,\n          padding=\"SAME\",\n          test_input=True,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testFilterGradientSamePaddingStrideOne(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=4,\n          input_rows=6,\n          input_cols=5,\n          filter_rows=2,\n          filter_cols=2,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=1,\n          stride_cols=1,\n          padding=\"SAME\",\n          test_input=False,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testInputGradientSamePaddingStrideTwo(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=5,\n          input_cols=4,\n          filter_rows=3,\n          filter_cols=3,\n          in_depth=3,\n          out_depth=3,\n          stride_rows=2,\n          stride_cols=2,\n          padding=\"SAME\",\n          test_input=True,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testFilterGradientSamePaddingStrideTwo(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=4,\n          input_rows=6,\n          input_cols=5,\n          filter_rows=2,\n          filter_cols=2,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=2,\n          stride_cols=2,\n          padding=\"SAME\",\n          test_input=False,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testInputGradientSamePaddingStrideThree(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=7,\n          input_cols=6,\n          filter_rows=3,\n          filter_cols=3,\n          in_depth=4,\n          out_depth=5,\n          stride_rows=3,\n          stride_cols=3,\n          padding=\"SAME\",\n          test_input=True,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testFilterGradientSamePaddingStrideThree(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=8,\n          input_cols=7,\n          filter_rows=4,\n          filter_cols=4,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=3,\n          stride_cols=3,\n          padding=\"SAME\",\n          test_input=False,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testFilterGradientSamePaddingStride2x1(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=8,\n          input_cols=7,\n          filter_rows=4,\n          filter_cols=4,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=2,\n          stride_cols=1,\n          padding=\"SAME\",\n          test_input=False,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testInputGradientKernelSizeMatchesInputSize(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=4,\n          input_cols=3,\n          filter_rows=4,\n          filter_cols=3,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=1,\n          stride_cols=1,\n          padding=\"VALID\",\n          test_input=True,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testFilterGradientKernelSizeMatchesInputSize(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=4,\n          input_cols=3,\n          filter_rows=4,\n          filter_cols=3,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=1,\n          stride_cols=1,\n          padding=\"VALID\",\n          test_input=False,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testInputGradient1x1PaddingStrideOne(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=5,\n          input_cols=4,\n          filter_rows=3,\n          filter_cols=3,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=1,\n          stride_cols=1,\n          padding=[[0, 0], [1, 1], [1, 1], [0, 0]],\n          test_input=True,\n          data_format=data_format,\n          use_gpu=use_gpu,\n          max_err=0.0025)\n\n  @test_util.deprecated_graph_mode_only\n  def testFilterGradient1x1PaddingStrideOne(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=5,\n          input_cols=4,\n          filter_rows=3,\n          filter_cols=3,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=1,\n          stride_cols=1,\n          padding=[[0, 0], [1, 1], [1, 1], [0, 0]],\n          test_input=False,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testInputGradient1x1PaddingStrideTwo(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=4,\n          input_cols=5,\n          filter_rows=3,\n          filter_cols=3,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=2,\n          stride_cols=2,\n          padding=[[0, 0], [1, 1], [1, 1], [0, 0]],\n          test_input=True,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testFilterGradient1x1PaddingStrideTwo(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=4,\n          input_cols=5,\n          filter_rows=3,\n          filter_cols=3,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=2,\n          stride_cols=2,\n          padding=[[0, 0], [1, 1], [1, 1], [0, 0]],\n          test_input=False,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testInputGradient2x2PaddingStrideOne(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=5,\n          input_cols=4,\n          filter_rows=3,\n          filter_cols=3,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=1,\n          stride_cols=1,\n          padding=[[0, 0], [2, 2], [2, 2], [0, 0]],\n          test_input=True,\n          data_format=data_format,\n          use_gpu=use_gpu,\n          max_err=0.003)\n\n  @test_util.deprecated_graph_mode_only\n  def testFilterGradient2x2PaddingStrideOne(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=5,\n          input_cols=4,\n          filter_rows=3,\n          filter_cols=3,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=1,\n          stride_cols=1,\n          padding=[[0, 0], [2, 2], [2, 2], [0, 0]],\n          test_input=False,\n          data_format=data_format,\n          use_gpu=use_gpu,\n          max_err=0.005)\n\n  @test_util.deprecated_graph_mode_only\n  def testInputGradient1_2_3_4PaddingStride3x2(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=8,\n          input_cols=5,\n          filter_rows=4,\n          filter_cols=2,\n          in_depth=3,\n          out_depth=2,\n          stride_rows=3,\n          stride_cols=2,\n          padding=[[0, 0], [1, 2], [3, 4], [0, 0]],\n          test_input=True,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testFilterGradient1_2_3_4PaddingStride3x2(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=8,\n          input_cols=5,\n          filter_rows=4,\n          filter_cols=2,\n          in_depth=3,\n          out_depth=2,\n          stride_rows=3,\n          stride_cols=2,\n          padding=[[0, 0], [1, 2], [3, 4], [0, 0]],\n          test_input=False,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testInputGradient4_3_2_1PaddingStride2x1(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=3,\n          input_rows=5,\n          input_cols=7,\n          filter_rows=3,\n          filter_cols=2,\n          in_depth=1,\n          out_depth=2,\n          stride_rows=2,\n          stride_cols=1,\n          padding=[[0, 0], [4, 3], [2, 1], [0, 0]],\n          test_input=True,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testFilterGradient4_3_2_1PaddingStride2x1(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=3,\n          input_rows=5,\n          input_cols=7,\n          filter_rows=3,\n          filter_cols=2,\n          in_depth=1,\n          out_depth=2,\n          stride_rows=2,\n          stride_cols=1,\n          padding=[[0, 0], [4, 3], [2, 1], [0, 0]],\n          test_input=False,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testInputGradient0_0_0_5PaddingStride1x2(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=6,\n          input_cols=7,\n          filter_rows=3,\n          filter_cols=4,\n          in_depth=3,\n          out_depth=2,\n          stride_rows=1,\n          stride_cols=2,\n          padding=[[0, 0], [0, 0], [0, 5], [0, 0]],\n          test_input=True,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testFilterGradient0_0_0_5PaddingStride1x2(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=6,\n          input_cols=7,\n          filter_rows=3,\n          filter_cols=4,\n          in_depth=3,\n          out_depth=2,\n          stride_rows=1,\n          stride_cols=2,\n          padding=[[0, 0], [0, 0], [0, 5], [0, 0]],\n          test_input=False,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testShapeFunctionEdgeCases(self):\n    # All shapes unknown.\n    c1 = nn_ops.conv2d(\n        array_ops.placeholder(dtypes.float32),\n        array_ops.placeholder(dtypes.float32),\n        strides=[1, 1, 1, 1],\n        padding=\"SAME\")\n    self.assertEqual([None, None, None, None], c1.get_shape().as_list())\n\n    # Incorrect input shape.\n    with self.assertRaises(ValueError):\n      nn_ops.conv2d(\n          array_ops.placeholder(\n              dtypes.float32, shape=[1, 3]),\n          array_ops.placeholder(dtypes.float32),\n          strides=[1, 1, 1, 1],\n          padding=\"SAME\")\n\n    # Incorrect filter shape.\n    with self.assertRaises(ValueError):\n      nn_ops.conv2d(\n          array_ops.placeholder(dtypes.float32),\n          array_ops.placeholder(\n              dtypes.float32, shape=[1, 3]),\n          strides=[1, 1, 1, 1],\n          padding=\"SAME\")\n\n    # Depth mismatch.\n    with self.assertRaises(ValueError):\n      nn_ops.conv2d(\n          array_ops.placeholder(\n              dtypes.float32, shape=[32, 20, 20, 3]),\n          array_ops.placeholder(\n              dtypes.float32, shape=[4, 4, 2, 2]),\n          strides=[1, 1, 1, 1],\n          padding=\"SAME\")\n\n    # Input depth divisible by filter depth (group convolution).\n    # No exceptions should appear.\n    nn_ops.conv2d(\n        array_ops.placeholder(dtypes.float32, shape=[32, 20, 20, 8]),\n        array_ops.placeholder(dtypes.float32, shape=[4, 4, 2, 16]),\n        strides=[1, 1, 1, 1],\n        padding=\"SAME\")\n\n    # Negative padding.\n    with self.assertRaises(ValueError):\n      nn_ops.conv2d(\n          array_ops.placeholder(dtypes.float32),\n          array_ops.placeholder(dtypes.float32),\n          strides=[1, 1, 1, 1],\n          padding=[[0, 0], [0, -1], [1, 2], [0, 0]])\n\n    # Nonzero padding in nonspatial dimension.\n    with self.assertRaises(ValueError):\n      nn_ops.conv2d(\n          array_ops.placeholder(dtypes.float32),\n          array_ops.placeholder(dtypes.float32),\n          strides=[1, 1, 1, 1],\n          padding=[[1, 0], [0, 0], [0, 0], [0, 0]])\n\n    # Nonzero NCHW padding in nonspatial dimension.\n    with self.assertRaises(ValueError):\n      nn_ops.conv2d(\n          array_ops.placeholder(dtypes.float32),\n          array_ops.placeholder(dtypes.float32),\n          strides=[1, 1, 1, 1],\n          padding=[[0, 0], [0, 1], [0, 0], [0, 0]],\n          data_format=\"NCHW\")\n\n    # Wrong amount of padding\n    with self.assertRaises(ValueError):\n      nn_ops.conv2d(\n          array_ops.placeholder(dtypes.float32),\n          array_ops.placeholder(dtypes.float32),\n          strides=[1, 1, 1, 1],\n          padding=[[0, 0], [0, 0], [0, 0]])\n\n    # Only specify one padding amount per dimension\n    with self.assertRaises(ValueError):\n      nn_ops.conv2d(\n          array_ops.placeholder(dtypes.float32),\n          array_ops.placeholder(dtypes.float32),\n          strides=[1, 1, 1, 1],\n          padding=[[0], [0], [0], [0]])\n\n    # Explicit padding elements are not lists\n    with self.assertRaises(ValueError):\n      nn_ops.conv2d(\n          array_ops.placeholder(dtypes.float32),\n          array_ops.placeholder(dtypes.float32),\n          strides=[1, 1, 1, 1],\n          padding=[0, 0, 0, 0])\n\n  def testOpEdgeCases(self):\n    # Illegal strides.\n    with self.assertRaisesRegex((ValueError, errors_impl.UnimplementedError),\n                                \"strides in the batch and depth\"):\n      input_val = np.ones([2, 4, 10, 10])\n      filter_val = np.ones([2, 4, 10, 10])\n      self.evaluate(\n          nn_ops.conv2d(\n              input_val, filter_val, strides=[2, 1, 1, 1], padding=\"SAME\"))\n    with self.assertRaisesRegex((ValueError, errors_impl.UnimplementedError),\n                                \"strides in the batch and depth\"):\n      input_val = np.ones([2, 4, 10, 10])\n      filter_val = np.ones([2, 4, 10, 10])\n      self.evaluate(\n          nn_ops.conv2d(\n              input_val, filter_val, strides=[1, 1, 1, 2], padding=\"SAME\"))\n\n    # TODO(b/195689143): Will enable when fixed for V2 behavior\n    # # Filter larger than input.\n    # with self.assertRaisesRegex(ValueError, \"Negative dimension size\"):\n    #   input_val = np.ones([32, 20, 20, 3])\n    #   filter_val = np.ones([20, 21, 3, 2])\n    #   self.evaluate(\n    #       nn_ops.conv2d(\n    #           input_val, filter_val, strides=[1, 1, 1, 1], padding=\"VALID\"))\n    # with self.assertRaisesRegex(ValueError, \"Negative dimension size\"):\n    #   input_val = np.ones([32, 20, 20, 3])\n    #   filter_val = np.ones([21, 20, 3, 2])\n    #   self.evaluate(\n    #       nn_ops.conv2d(\n    #           input_val, filter_val, strides=[1, 1, 1, 1], padding=\"VALID\"))\n    #\n    # # Filter larger than input + padding.\n    # with self.assertRaisesRegex(ValueError, \"Negative dimension size\"):\n    #   input_val = np.ones([32, 20, 20, 3])\n    # filter_val = np.ones([24, 25, 3, 2])\n    #   self.evaluate(\n    #       nn_ops.conv2d(\n    #           input_val,\n    #           filter_val,\n    #           strides=[1, 1, 1, 1],\n    #           padding=[[0, 0], [2, 2], [2, 2], [0, 0]]))\n\n    # Filter dimensions must be greater than 0.\n    with self.assertRaisesRegex(\n        errors_impl.InvalidArgumentError, \"filter must not have zero elements\"\n        \"|has a non-positive dimension\"):\n      input_val = np.ones([1, 1, 1, 1])\n      filter_val = np.ones([1, 0, 1, 1])\n      self.evaluate(\n          nn_ops.conv2d(\n              input_val, filter_val, strides=[1, 1, 1, 1], padding=\"SAME\"))\n\n    # Negative padding during backprop.\n    with self.assertRaisesRegex(\n        errors_impl.InvalidArgumentError,\n        \"All elements of explicit_paddings must be nonnegative\"):\n      filter_val = np.ones([18, 18, 3, 2])\n      out_backprop_val = np.ones([32, 3, 2, 2])\n      self.evaluate(\n          nn_ops.conv2d_backprop_input([32, 20, 20, 3],\n                                       filter_val,\n                                       out_backprop_val,\n                                       strides=[1, 1, 1, 1],\n                                       padding=[[0, 0], [-1, 0], [0, 0], [0,\n                                                                          0]]))\n    with self.assertRaisesRegex(\n        errors_impl.InvalidArgumentError,\n        \"All elements of explicit_paddings must be nonnegative\"):\n      input_val = np.ones([32, 20, 20, 3])\n      out_backprop_val = np.ones([32, 3, 2, 2])\n      self.evaluate(\n          nn_ops.conv2d_backprop_filter(\n              input_val, [18, 18, 3, 2],\n              out_backprop_val,\n              strides=[1, 1, 1, 1],\n              padding=[[0, 0], [-1, 0], [0, 0], [0, 0]]))\n\n\n@test_util.run_all_without_tensor_float_32(\"Avoid TF32 conv on GPU\")\nclass DepthwiseConv2DTest(test.TestCase):\n\n  def _VerifyValues(self, tensor_in_sizes, filter_in_sizes, stride, padding,\n                    expected):\n    \"\"\"Verifies the output values of the convolution function.\n\n    Args:\n      tensor_in_sizes: Input tensor dimensions in [batch, input_rows,\n        input_cols, input_depth].\n      filter_in_sizes: Filter tensor dimensions in [filter_rows, filter_cols,\n        input_depth, depth_multiplier].\n      stride: Stride.\n      padding: Padding type.\n      expected: An array containing the expected operation outputs.\n    \"\"\"\n    total_size_1 = 1\n    total_size_2 = 1\n    for s in tensor_in_sizes:\n      total_size_1 *= s\n    for s in filter_in_sizes:\n      total_size_2 *= s\n    # Initializes the input tensor with array containing incrementing\n    # numbers from 1.\n    x1 = [f * 1.0 for f in range(1, total_size_1 + 1)]\n    x2 = [f * 1.0 for f in range(1, total_size_2 + 1)]\n    with self.cached_session() as sess:\n      t1 = constant_op.constant(x1, shape=tensor_in_sizes)\n      t1.set_shape(tensor_in_sizes)\n      t2 = constant_op.constant(x2, shape=filter_in_sizes)\n      conv = nn_impl.depthwise_conv2d(\n          t1, t2, strides=[1, stride, stride, 1], padding=padding)\n      value = self.evaluate(conv)\n    tf_logging.debug(\"value = %s\", value)\n    self.assertArrayNear(expected, np.ravel(value), 1e-5)\n    self.assertShapeEqual(value, conv)\n\n  def testConv2D2x2Filter(self):\n    # The inputs look like this (it's a 3 x 2 matrix, each of depth 2):\n    #\n    # [ (1.0, 2.0), (3.0,  4.0), ( 5.0,  6.0) ]\n    # [ (7.0, 8.0), (9.0, 10.0), (11.0, 12.0) ]\n    #  We can view this as two inputs\n    #\n    #  input depth 0:\n    #\n    #  [ 1.0,  3.0,  5.0 ]\n    #  [ 7.0,  9.0, 11.0 ]\n    #\n    #  input depth 1:\n    #\n    #  [ 2.0,  4.0,  6.0 ]\n    #  [ 8.0, 10.0, 12.0 ]\n    #\n    # The filter looks like this (it has two 2 x 2 patches, each generating 2\n    # depths):\n    #\n    #  filter #0:\n    #\n    #  [ (1.0,  3.0), ( 5.0,  7.0)]\n    #  [ (9.0, 11.0), (13.0, 15.0)]\n    #\n    #  filter #1:\n    #\n    #  [ ( 2.0,  4.0), ( 6.0,  8.0)]\n    #  [ (10.0, 12.0), (14.0, 16.0)]\n    #\n    # So the outputs are:\n    #\n    # (position 0, 0: in_depth 0, output_depth 0 -- using filter #0)\n    #  1.0 * 1.0 + 7.0 * 9.0 + 3.0 * 5.0 + 9.0 * 13.0 = 196\n    # (position 0, 0: in_depth 0, output_depth 1 -- using filter #1)\n    #  1.0 * 2.0 + 7.0 * 10.0 + 3.0 * 6.0 + 9.0 * 14.0 = 216\n    # (position 0, 0: in_depth 1, output_depth 2 -- using filter #0)\n    #  2.0 * 3.0 + 8.0 * 11.0 + 4.0 * 7.0 + 10.0 * 15.0 = 272\n    # (position 0, 0: in_depth 1, output_depth 3 -- using filter #1)\n    #  2.0 * 4.0 + 8.0 * 12.0 + 4.0 * 8.0 + 10.0 * 16.0 = 296\n    #\n    # (position 1, 0: in_depth 0, output_depth 0 -- using filter #0)\n    #  3.0 * 1.0 + 9.0 * 9.0 + 5.0 * 5.0 + 11.0 * 13.0 = 252\n    # (position 1, 0: in_depth 0, output_depth 1 -- using filter #1)\n    #  3.0 * 2.0 + 9.0 * 10.0 + 5.0 * 6.0 + 11.0 * 14.0 = 280\n    # (position 1, 0: in_depth 1, output_depth 2 -- using filter #0)\n    #  4.0 * 3.0 + 10.0 * 11.0 + 6.0 * 7.0 + 12.0 * 15.0 = 344\n    # (position 1, 0: in_depth 1, output_depth 3 -- using filter #1)\n    #  4.0 * 4.0 + 10.0 * 12.0 + 6.0 * 8.0 + 12.0 * 16.0 = 376\n    expected_output = [196, 216, 272, 296, 252, 280, 344, 376]\n    self._VerifyValues(\n        tensor_in_sizes=[1, 2, 3, 2],\n        filter_in_sizes=[2, 2, 2, 2],\n        stride=1,\n        padding=\"VALID\",\n        expected=expected_output)\n\n\n@test_util.run_all_without_tensor_float_32(\"Avoid TF32 conv on GPU\")\nclass SeparableConv2DTest(test.TestCase):\n\n  def _InitValues(self, sizes):\n    \"\"\"Initializes values for input tensors.\n\n    Args:\n      sizes: Tensor dimensions.\n\n    Returns:\n      Tensor initialized to values.\n    \"\"\"\n    total_size = 1\n    for s in sizes:\n      total_size *= s\n    x = [f * 0.5 for f in range(1, total_size + 1)]\n    return constant_op.constant(x, shape=sizes)\n\n  def _VerifyValues(self,\n                    tensor_in_sizes,\n                    depthwise_filter_in_sizes,\n                    pointwise_filter_in_sizes,\n                    stride,\n                    padding,\n                    expected,\n                    data_format=\"NHWC\"):\n    \"\"\"Verifies the output values of the separable convolution function.\n\n    Args:\n      tensor_in_sizes: Input tensor dimensions.\n      depthwise_filter_in_sizes: Depthwise filter tensor dimensions.\n      pointwise_filter_in_sizes: Pointwise filter tensor dimensions.\n      stride: Stride.\n      padding: Padding type.\n      expected: An array containing the expected operation outputs.\n      data_format: string data format for input tensor.\n    \"\"\"\n    with self.cached_session():\n      t1 = self._InitValues(tensor_in_sizes)\n      f1 = self._InitValues(depthwise_filter_in_sizes)\n      f1.set_shape(depthwise_filter_in_sizes)\n      f2 = self._InitValues(pointwise_filter_in_sizes)\n\n      real_t1 = t1\n      strides = [1, stride, stride, 1]\n      if data_format == \"NCHW\":\n        real_t1 = array_ops.transpose(t1, [0, 3, 1, 2])\n        strides = [1, 1, stride, stride]\n        if isinstance(padding, list):\n          padding = [padding[0], padding[3], padding[1], padding[2]]\n\n      conv = nn_impl.separable_conv2d(\n          real_t1,\n          f1,\n          f2,\n          strides=strides,\n          padding=padding,\n          data_format=data_format)\n\n      if data_format == \"NCHW\":\n        conv = array_ops.transpose(conv, [0, 2, 3, 1])\n\n      value = self.evaluate(conv)\n    tf_logging.debug(\"value = %s\", value)\n    self.assertArrayNear(expected, np.ravel(value), 2e-3)\n    self.assertShapeEqual(value, conv)\n\n  def _testSeparableConv2D(self, data_format):\n    # The output is the result of two convolutions:\n    # First with tensor_in[1, 4, 4, 2] * filter1[2, 2, 2, 3].\n    # Second with intermediate_out[1, 4, 4, 6] * filter2[1, 1, 6, 7].\n    # Complexity is O(2*3*2*2 + 6*7*1*1) as opposed to O(2*7*2*2).\n    expected_output = [\n        6644.5, 6971.5, 7298.5, 7625.5, 7952.5, 8279.5, 8606.5, 8154.5, 8556.5,\n        8958.5, 9360.5, 9762.5, 10164.5, 10566.5, 9664.5, 10141.5, 10618.5,\n        11095.5, 11572.5, 12049.5, 12526.5, 4145.5, 4346.5, 4547.5, 4748.5,\n        4949.5, 5150.5, 5351.5, 12684.5, 13311.5, 13938.5, 14565.5, 15192.5,\n        15819.5, 16446.5, 14194.5, 14896.5, 15598.5, 16300.5, 17002.5, 17704.5,\n        18406.5, 15704.5, 16481.5, 17258.5, 18035.5, 18812.5, 19589.5, 20366.5,\n        6499.5, 6814.5, 7129.5, 7444.5, 7759.5, 8074.5, 8389.5, 18724.5,\n        19651.5, 20578.5, 21505.5, 22432.5, 23359.5, 24286.5, 20234.5, 21236.5,\n        22238.5, 23240.5, 24242.5, 25244.5, 26246.5, 21744.5, 22821.5, 23898.5,\n        24975.5, 26052.5, 27129.5, 28206.5, 8853.5, 9282.5, 9711.5, 10140.5,\n        10569.5, 10998.5, 11427.5, 5746.75, 6010.75, 6274.75, 6538.75, 6802.75,\n        7066.75, 7330.75, 6168.75, 6452.25, 6735.75, 7019.25, 7302.75, 7586.25,\n        7869.75, 6590.75, 6893.75, 7196.75, 7499.75, 7802.75, 8105.75, 8408.75,\n        2036.25, 2119.5, 2202.75, 2286.0, 2369.25, 2452.5, 2535.75\n    ]\n\n    self._VerifyValues(\n        tensor_in_sizes=[1, 4, 4, 2],\n        depthwise_filter_in_sizes=[2, 2, 2, 3],\n        pointwise_filter_in_sizes=[1, 1, 6, 7],\n        stride=1,\n        padding=\"SAME\",\n        expected=expected_output,\n        data_format=data_format)\n\n  def testSeparableConv2D(self):\n    self._testSeparableConv2D(\"NHWC\")\n\n  def disabledtestSeparableConv2DNCHW(self):\n    if not test.is_gpu_available():\n      return\n    self._testSeparableConv2D(\"NCHW\")\n\n  def _testSeparableConv2DEqualInputOutputDepth(self, data_format):\n    # The output is the result of two convolutions:\n    # First with tensor_in[1, 4, 4, 2] * filter1[2, 2, 3, 3].\n    # Second with intermediate_out[1, 4, 4, 6] * filter2[1, 1, 6, 6].\n    # Complexity is O(2*3*2*2 + 6*6*1*1) as opposed to O(2*6*2*2).\n    expected_output = [\n        5742.0, 6069.0, 6396.0, 6723.0, 7050.0, 7377.0, 7047.0, 7449.0, 7851.0,\n        8253.0, 8655.0, 9057.0, 8352.0, 8829.0, 9306.0, 9783.0, 10260.0,\n        10737.0, 3582.0, 3783.0, 3984.0, 4185.0, 4386.0, 4587.0, 10962.0,\n        11589.0, 12216.0, 12843.0, 13470.0, 14097.0, 12267.0, 12969.0, 13671.0,\n        14373.0, 15075.0, 15777.0, 13572.0, 14349.0, 15126.0, 15903.0, 16680.0,\n        17457.0, 5616.0, 5931.0, 6246.0, 6561.0, 6876.0, 7191.0, 16182.0,\n        17109.0, 18036.0, 18963.0, 19890.0, 20817.0, 17487.0, 18489.0, 19491.0,\n        20493.0, 21495.0, 22497.0, 18792.0, 19869.0, 20946.0, 22023.0, 23100.0,\n        24177.0, 7650.0, 8079.0, 8508.0, 8937.0, 9366.0, 9795.0, 4963.5, 5227.5,\n        5491.5, 5755.5, 6019.5, 6283.5, 5328.0, 5611.5, 5895.0, 6178.5, 6462.0,\n        6745.5, 5692.5, 5995.5, 6298.5, 6601.5, 6904.5, 7207.5, 1757.25, 1840.5,\n        1923.75, 2007.0, 2090.25, 2173.5\n    ]\n\n    self._VerifyValues(\n        tensor_in_sizes=[1, 4, 4, 2],\n        depthwise_filter_in_sizes=[2, 2, 2, 3],\n        pointwise_filter_in_sizes=[1, 1, 6, 6],\n        stride=1,\n        padding=\"SAME\",\n        expected=expected_output,\n        data_format=data_format)\n\n  @test_util.deprecated_graph_mode_only\n  def testSeparableConv2DEqualInputOutputDepth(self):\n    self._testSeparableConv2DEqualInputOutputDepth(\"NHWC\")\n\n  def testSeparableConv2DEqualInputOutputDepthNCHW(self):\n    if not test.is_gpu_available():\n      return\n    self._testSeparableConv2DEqualInputOutputDepth(\"NCHW\")\n\n  def _testSeparableConv2dExplicitPadding(self, data_format):\n    tensor_in_sizes = [1, 4, 4, 2]\n    depthwise_filter_in_sizes = [2, 2, 2, 3]\n    pointwise_filter_in_sizes = [1, 1, 6, 7]\n    padding = [[0, 0], [1, 2], [3, 4], [0, 0]]\n    with self.cached_session():\n      # Compute the 'expected' values by manually padding before calling\n      # separable_conv2d\n      t1 = self._InitValues(tensor_in_sizes)\n      t1 = array_ops.pad(t1, padding)\n      f1 = self._InitValues(depthwise_filter_in_sizes)\n      f1.set_shape(depthwise_filter_in_sizes)\n      f2 = self._InitValues(pointwise_filter_in_sizes)\n      conv = nn_impl.separable_conv2d(\n          t1,\n          f1,\n          f2,\n          strides=[1, 1, 1, 1],\n          padding=\"VALID\",\n          data_format=\"NHWC\")\n      expected = self.evaluate(conv)\n      expected = np.ravel(expected)\n    self._VerifyValues(\n        tensor_in_sizes=tensor_in_sizes,\n        depthwise_filter_in_sizes=depthwise_filter_in_sizes,\n        pointwise_filter_in_sizes=pointwise_filter_in_sizes,\n        stride=1,\n        padding=padding,\n        expected=expected,\n        data_format=data_format)\n\n  def testSeparableConv2dExplicitPadding(self):\n    self._testSeparableConv2dExplicitPadding(\"NHWC\")\n\n  def testSeparableConv2dExplicitPaddingNCHW(self):\n    if not test.is_gpu_available():\n      return\n    self._testSeparableConv2dExplicitPadding(\"NCHW\")\n\n\n@test_util.run_all_without_tensor_float_32(\"Avoid TF32 conv on GPU\")\nclass DeepConv2DTest(test.TestCase):\n\n  def _CompareFwdConv2D(self, tensor_in_sizes, filter_in_sizes, conv_strides,\n                        padding):\n    \"\"\"Verifies that DeepConv2D and Conv2D produce the same values.\n\n    Args:\n      tensor_in_sizes: Input tensor dimensions in\n        [batch, input_rows, input_cols, input_depth].\n      filter_in_sizes: Filter tensor dimensions in\n        [kernel_rows, kernel_cols, input_depth, output_depth].\n      conv_strides: [row_stride, col_stride] for the convolution;\n      padding: Padding type.\n    \"\"\"\n    x1 = np.random.rand(*tensor_in_sizes).astype(np.float32)\n    x2 = np.random.rand(*filter_in_sizes).astype(np.float32)\n\n    with self.cached_session(use_gpu=False) as sess:\n      t1 = constant_op.constant(x1, shape=tensor_in_sizes)\n      t2 = constant_op.constant(x2, shape=filter_in_sizes)\n      strides = [1] + conv_strides + [1]\n\n      conv = nn_ops.conv2d(t1, t2, strides=strides, padding=padding)\n\n      os.environ[\"TF_USE_DEEP_CONV2D\"] = \"0\"\n      values_expect = self.evaluate([conv])\n\n      os.environ[\"TF_USE_DEEP_CONV2D\"] = \"1\"\n      values_test = self.evaluate([conv])\n\n      self.assertAllClose(values_expect, values_test, rtol=1e-5, atol=1e-5)\n\n  def _RunTestCases(self, conv_strides, padding):\n    input_sizes = [[5, 5, 5, 1248], [3, 17, 17, 192], [2, 35, 35, 288],\n                   [2, 6, 8, 517], [2, 7, 4, 81], [3, 11, 3, 77]]\n    filter_sizes = [[3, 3, 1248, 128], [3, 3, 192, 192], [3, 3, 288, 384],\n                    [3, 3, 517, 64], [3, 3, 81, 77], [3, 3, 77, 181]]\n    for input_shape, filter_shape in zip(input_sizes, filter_sizes):\n      self._CompareFwdConv2D(input_shape, filter_shape, conv_strides, padding)\n\n  def testConv2D3x3FilterStride1x1Valid(self):\n    self._RunTestCases([1, 1], \"VALID\")\n\n  def testConv2D3x3FilterStride1x1Same(self):\n    self._RunTestCases([1, 1], \"SAME\")\n\n\nclass Conv2DBenchmark(test.Benchmark):\n\n  def benchmarkGPUConvStackFirst(self):\n    # Benchmark the first iteration of a conv-net with many identical conv\n    # operations.\n    if not test.is_gpu_available():\n      return\n\n    with ops.Graph().as_default(), session_lib.Session() as session:\n      batch_size = 1\n      timesteps = 600\n      features = 1\n\n      inputs = random_ops.random_uniform(\n          [batch_size, 1, timesteps, features], seed=1234)\n      num_outputs_list = [512] * 40 + [1]\n      kernel_w = 3\n      x = inputs\n      for num_outputs in num_outputs_list:\n        x = convolutional.conv2d(x, num_outputs, [1, kernel_w])\n      outputs = x\n\n      self.evaluate(variables.global_variables_initializer())\n      num_iterations = 4\n      for iter_index in range(num_iterations):\n        start = time.time()\n        session.run(outputs)\n        wall_time = time.time() - start\n        self.report_benchmark(\n            name=\"conv_stack_iter_%d\" % iter_index, wall_time=wall_time)\n        tf_logging.info(\"conv_stack_iter_%d: %.4f\" % (iter_index, wall_time))\n\n  def _bench_op(self, name, op, burn_iters, num_iters):\n    config = config_pb2.ConfigProto()\n    # Prevent Grappler from optimizing away the entire graph.\n    config.graph_options.rewrite_options.dependency_optimization = (\n        rewriter_config_pb2.RewriterConfig.OFF)\n    with session_lib.Session(config=config) as session:\n      self.evaluate(variables.global_variables_initializer())\n      self.run_op_benchmark(\n          session, op, burn_iters=burn_iters, min_iters=num_iters, name=name)\n\n  def benchmarkExplicitVsManualPadding(self):\n    \"\"\"Compare performance of EXPLICIT padding and calling tf.pad.\n\n    A Conv2D op with EXPLICIT padding is benchmarked, and a tf.pad with the same\n    padding followed by an equivalent Conv2D op is benchmarked.\n    \"\"\"\n    if not test.is_gpu_available():\n      return\n\n    with ops.Graph().as_default():\n      burn_iters = 15\n      num_iters = 300\n      batch_size = 64\n      # The input and filter correspond to the first layer of Resnet50.\n      input = variables.Variable(  # pylint: disable=redefined-builtin\n          random_ops.random_uniform([\n              batch_size,\n              3,\n              224,\n              224\n          ]))\n      filter = variables.Variable(random_ops.random_uniform([7, 7, 3, 64]))  # pylint: disable=redefined-builtin\n      strides = [1, 1, 2, 2]\n      padding = [(0, 0), (0, 0), (3, 3), (3, 3)]\n      output_explicit_pad = nn_ops.conv2d(\n          input, filter, strides, padding=padding, data_format=\"NCHW\")\n      input_padded = array_ops.pad(input, padding)\n      output_manual_pad = nn_ops.conv2d(\n          input_padded, filter, strides, padding=\"VALID\", data_format=\"NCHW\")\n      # Benchmark just the forward pass.\n      self._bench_op(\"explicit_pad_forward\", output_explicit_pad.op, burn_iters,\n                     num_iters)\n      self._bench_op(\"manual_pad_forward\", output_manual_pad.op, burn_iters,\n                     num_iters)\n\n      # Benchmark both the forward and backwards passes.\n      input_grad_explicit_pad, filter_grad_explicit_pad = (\n          gradients_impl.gradients(output_explicit_pad, [input, filter]))\n      self._bench_op(\n          \"explicit_pad_backward\",\n          control_flow_ops.group(input_grad_explicit_pad,\n                                 filter_grad_explicit_pad), burn_iters,\n          num_iters)\n      input_grad_manual_pad, filter_grad_manual_pad = gradients_impl.gradients(\n          output_manual_pad, [input, filter])\n      self._bench_op(\n          \"manual_pad_backward\",\n          control_flow_ops.group(input_grad_manual_pad, filter_grad_manual_pad),\n          burn_iters, num_iters)\n\n  def benchmarkExplicitVsSamePaddingGraph(self):\n    \"\"\"Compare performance of EXPLICIT and SAME padding in graph mode.\n\n    A Conv2D op with SAME padding is benchmarked, and an equivalent Conv2D op\n    with explicit padding is benchmarked, where the padding is the same as in\n    the SAME case. The purpose is to ensure EXPLICIT padding is just as\n    efficient as the SAME case\n    \"\"\"\n    if not test.is_gpu_available():\n      return\n\n    with ops.Graph().as_default():\n      burn_iters = 15\n      num_convs = 20\n      num_iters = 50\n      batch_size = 64\n      # The input and filter correspond to a middle layer of Resnet50.\n      input = variables.Variable(  # pylint: disable=redefined-builtin\n          random_ops.random_uniform([\n              batch_size,\n              256,\n              14,\n              14\n          ]))\n      filter = variables.Variable(random_ops.random_uniform([3, 3, 256, 256]))  # pylint: disable=redefined-builtin\n      strides = [1, 1, 1, 1]\n      padding = [(0, 0), (0, 0), (1, 1), (1, 1)]\n      output_explicit_pad = input\n      output_same_pad = input\n\n      for _ in range(num_convs):\n        output_explicit_pad = nn_ops.conv2d(\n            output_explicit_pad,\n            filter,\n            strides,\n            padding=padding,\n            data_format=\"NCHW\")\n        output_same_pad = nn_ops.conv2d(\n            output_same_pad,\n            filter,\n            strides,\n            padding=\"SAME\",\n            data_format=\"NCHW\")\n      grad_explicit_pad, = gradients_impl.gradients(output_explicit_pad, filter)\n      grad_same_pad, = gradients_impl.gradients(output_same_pad, filter)\n      self._bench_op(\"graph_explicit_pad\", grad_explicit_pad.op, burn_iters,\n                     num_iters)\n      self._bench_op(\"graph_same_pad\", grad_same_pad.op, burn_iters, num_iters)\n\n  def benchmarkExplicitVsSamePaddingEager(self):\n    \"\"\"Compare performance of EXPLICIT and SAME padding in eager mode.\n\n    A Conv2D op with SAME padding is benchmarked, and an equivalent Conv2D op\n    with explicit padding is benchmarked, where the padding is the same as in\n    the SAME case. Currently, EXPLICIT padding is slightly slower, due to the\n    fact the Python padding list must be checked and processed before the Conv2D\n    op can run.\n    \"\"\"\n    # TODO(reedwm): Make EXPLICIT padding as fast as SAME padding.\n    if not test.is_gpu_available():\n      return\n\n    with context.eager_mode():\n      burn_iters = 15\n      num_convs = 20\n      num_iters = 50\n      batch_size = 64\n      # The input and filter correspond to a middle layer of Resnet50.\n      input = variables.Variable(  # pylint: disable=redefined-builtin\n          random_ops.random_uniform([\n              batch_size,\n              256,\n              14,\n              14\n          ]))\n      filter = variables.Variable(random_ops.random_uniform([3, 3, 256, 256]))  # pylint: disable=redefined-builtin\n      strides = [1, 1, 1, 1]\n      padding = [(0, 0), (0, 0), (1, 1), (1, 1)]\n      output_explicit_pad = input\n      output_same_pad = input\n      for _ in range(burn_iters):\n        output_explicit_pad = nn_ops.conv2d(\n            output_explicit_pad,\n            filter,\n            strides,\n            padding=padding,\n            data_format=\"NCHW\")\n        output_same_pad = nn_ops.conv2d(\n            output_same_pad,\n            filter,\n            strides,\n            padding=\"SAME\",\n            data_format=\"NCHW\")\n\n      start = time.time()\n      for _ in range(num_iters):\n        with backprop.GradientTape() as tape:\n          for _ in range(num_convs):\n            output_explicit_pad = nn_ops.conv2d(\n                output_explicit_pad,\n                filter,\n                strides,\n                padding=padding,\n                data_format=\"NCHW\")\n          tape.gradient(output_explicit_pad, filter)\n      end = time.time()\n      self.report_benchmark(\n          name=\"eager_explicit_pad\",\n          wall_time=(end - start) / num_iters,\n          iters=num_iters)\n\n      start = time.time()\n      for _ in range(num_iters):\n        with backprop.GradientTape() as tape:\n          for _ in range(num_convs):\n            output_same_pad = nn_ops.conv2d(\n                output_same_pad,\n                filter,\n                strides,\n                padding=\"SAME\",\n                data_format=\"NCHW\")\n          tape.gradient(output_same_pad, filter)\n      end = time.time()\n      self.report_benchmark(\n          name=\"eager_same_pad\",\n          wall_time=(end - start) / num_iters,\n          iters=num_iters)\n\n\ndef GetInceptionFwdTest(input_size, filter_size, stride, padding,\n                        gpu_only=False):\n\n  def Test(self):\n    if gpu_only and not test.is_gpu_available():\n      tf_logging.info(\"Skipping InceptionFwd %s\", (input_size, filter_size,\n                                                   stride, padding))\n      return\n    tf_logging.info(\"Testing InceptionFwd %s\", (input_size, filter_size, stride,\n                                                padding))\n    self._CompareFwdValues(input_size, filter_size, [stride, stride], padding)\n\n  return Test\n\n\ndef GetInceptionFwdDilatedConvTest(input_size, filter_size, stride, padding):\n\n  def Test(self):\n    if stride == 1:\n      tf_logging.info(\"Testing InceptionFwd with dilations %s\",\n                      (input_size, filter_size, stride, padding))\n      self._VerifyDilatedConvValues(\n          tensor_in_sizes=input_size,\n          filter_in_sizes=filter_size,\n          strides=[stride, stride],\n          dilations=[2, 2],\n          padding=padding,\n          rtol=5e-4)\n\n  return Test\n\n\ndef GetInceptionBackInputTest(input_size, filter_size, output_size, stride,\n                              padding,\n                              gpu_only=False):\n\n  def Test(self):\n    if gpu_only and not test.is_gpu_available():\n      tf_logging.info(\"Skipping InceptionBackInput %s\",\n                      (input_size, filter_size, output_size, stride, padding))\n      return\n    tf_logging.info(\"Testing InceptionBackInput %s\",\n                    (input_size, filter_size, output_size, stride, padding))\n    self._CompareBackpropInput(input_size, filter_size, output_size,\n                               [stride, stride], padding)\n\n  return Test\n\n\ndef GetInceptionBackFilterTest(input_size, filter_size, output_size, strides,\n                               padding, gpu_only=False):\n\n  def Test(self):\n    if gpu_only and not test.is_gpu_available():\n      tf_logging.info(\"Skipping InceptionBackFilter %s\",\n                      (input_size, filter_size, output_size, strides, padding))\n      return\n    tf_logging.info(\"Testing InceptionBackFilter %s\",\n                    (input_size, filter_size, output_size, strides, padding))\n    self._CompareBackFilter(input_size, filter_size, output_size, strides,\n                            padding)\n\n  return Test\n\n\n@test_util.run_all_without_tensor_float_32(\"Avoid TF32 conv on GPU\")\nclass FusedConv2DTest(test.TestCase):\n\n  def _CreateNumpyTensor(self, shape):\n    total_size = np.prod(shape)\n    return np.arange(1, total_size + 1, dtype=np.float32).reshape(shape)\n\n  def _CreateConv2D(self,\n                    input_values,\n                    filters,\n                    strides=[1, 1],\n                    padding=\"SAME\"):\n    return nn_ops.convolution(\n        input_values, filters, strides=strides, padding=padding)\n\n  # Tests tensor forwarding of a fused Conv2D+BiasAdd+Add op when the input to\n  # Add has refcount 1.\n  @test_util.run_in_graph_and_eager_modes(use_gpu=False)\n  def testAddWithRefCountOne(self):\n    expected_output = [\n        113377, 125570, 77305, 86738, 19433, 22226, 60681, 70722, 36291, 43718,\n        7143, 9206, 9785, 12098, 4783, 6366, 779, 1134\n    ]\n    tensor_in_sizes = [1, 3, 3, 2]\n    filter_in_sizes = [2, 2, 2, 2]\n    bias_in_sizes = [2]\n\n    x = self._CreateNumpyTensor(tensor_in_sizes)\n    filter_in = self._CreateNumpyTensor(filter_in_sizes)\n    bias_in = self._CreateNumpyTensor(bias_in_sizes)\n    # To get different weights for filter\n    offset = 1\n\n    conv1 = self._CreateConv2D(x, filter_in)\n    conv2 = self._CreateConv2D(conv1, filter_in + offset)\n\n    conv = self._CreateConv2D(conv1, filter_in - offset)\n    bias_add = nn_ops.bias_add(conv, bias_in)\n    add = math_ops.add_n([bias_add, conv2])\n\n    self.assertAllEqual(\n        np.rint(expected_output),\n        self.evaluate(add).reshape(-1))\n\n  # Tests tensor forwarding of a fused Conv2D+BiasAdd+Add op when the input to\n  # Add has a total refcount of 2, and Add is its last consumer.\n  @test_util.run_in_graph_and_eager_modes(use_gpu=False)\n  def testAddWithRefCountTwoAndRunAddLast(self):\n    expected_output = [\n        1.907175e+06, 2.253505e+06, 7.809210e+05, 9.537180e+05, 1.184170e+05,\n        1.523070e+05, 5.367010e+05, 6.803700e+05, 1.867090e+05, 2.529460e+05,\n        2.362300e+04, 3.522600e+04, 5.121700e+04, 7.168300e+04, 1.494300e+04,\n        2.347400e+04, 1.558000e+03, 2.903000e+03\n    ]\n    tensor_in_sizes = [1, 3, 3, 2]\n    filter_in_sizes = [2, 2, 2, 2]\n    bias_in_sizes = [2]\n\n    x = self._CreateNumpyTensor(tensor_in_sizes)\n    filter_in = self._CreateNumpyTensor(filter_in_sizes)\n    bias_in = self._CreateNumpyTensor(bias_in_sizes)\n    # To get different weights for filter\n    offset = 1\n\n    conv1 = self._CreateConv2D(x, filter_in)\n    conv2 = self._CreateConv2D(conv1, filter_in + offset)\n\n    conv = self._CreateConv2D(conv2, filter_in - offset)\n    bias_add = nn_ops.bias_add(conv, bias_in)\n    add = math_ops.add_n([bias_add, conv1])\n\n    self.assertAllEqual(\n        np.rint(expected_output),\n        self.evaluate(add).reshape(-1))\n\n  # Tests tensor forwarding of a fused Conv2D+BiasAdd+Add op when the input to\n  # Add has refcount 2 and Add (in the fused Conv2D op) is its first consumer.\n  @test_util.run_in_graph_and_eager_modes(use_gpu=False)\n  def testAddWithRefCountTwoAndRunAddFirst(self):\n    expected_output = [\n        176161, 194450, 120673, 134822, 30545, 34734, 96041, 111102, 58149,\n        69289, 11745, 14839, 15833, 19302, 7965, 10339, 1345, 1877\n    ]\n    tensor_in_sizes = [1, 3, 3, 2]\n    filter_in_sizes = [2, 2, 2, 2]\n    bias_in_sizes = [2]\n\n    x = self._CreateNumpyTensor(tensor_in_sizes)\n    filter_in = self._CreateNumpyTensor(filter_in_sizes)\n    bias_in = self._CreateNumpyTensor(bias_in_sizes)\n    # To get different weights for filter\n    offset = 1\n\n    conv1 = self._CreateConv2D(x, filter_in)\n    conv2 = self._CreateConv2D(conv1, filter_in + offset)\n\n    conv = self._CreateConv2D(conv1, filter_in - offset)\n    bias_add = nn_ops.bias_add(conv, bias_in)\n    add = math_ops.add_n([bias_add, conv2])\n\n    relu = nn_ops.relu(add)\n    output = math_ops.add_n([relu, conv2])\n\n    self.assertAllEqual(\n        np.rint(expected_output),\n        self.evaluate(output).reshape(-1))\n\n  # Tests tensor forwarding of a fused Conv2D+BiasAdd+Add op when the input to\n  # Add has refcount 2, and there is no dependency between its two consumers.\n  @test_util.run_in_graph_and_eager_modes(use_gpu=False)\n  def testAddWithRefCountTwoAndNoDependence(self):\n    expected_output = [\n        176161, 194450, 120673, 134822, 30545, 34734, 96041, 111102, 58149,\n        69289, 11745, 14839, 15833, 19302, 7965, 10339, 1345, 1877\n    ]\n    tensor_in_sizes = [1, 3, 3, 2]\n    filter_in_sizes = [2, 2, 2, 2]\n    bias_in_sizes = [2]\n\n    x = self._CreateNumpyTensor(tensor_in_sizes)\n    filter_in = self._CreateNumpyTensor(filter_in_sizes)\n    bias_in = self._CreateNumpyTensor(bias_in_sizes)\n    # To get different weights for filter\n    offset = 1\n\n    conv1 = self._CreateConv2D(x, filter_in)\n    conv2 = self._CreateConv2D(conv1, filter_in + offset)\n\n    conv = self._CreateConv2D(conv1, filter_in - offset)\n    bias_add = nn_ops.bias_add(conv, bias_in)\n    add = math_ops.add_n([bias_add, conv2])\n\n    relu1 = nn_ops.relu(add)\n    relu2 = nn_ops.relu(conv2)\n    output = math_ops.add_n([relu1, relu2])\n\n    self.assertAllEqual(\n        np.rint(expected_output),\n        self.evaluate(output).reshape(-1))\n\n  # Tests tensor forwarding of a fused Conv2D+BiasAdd+Add op when the input to\n  # Add is the same as the input to the fused Conv2D op and needs a tensor\n  # buffer.\n  @test_util.run_in_graph_and_eager_modes(use_gpu=False)\n  def testAddWithSameSrcAndAddTensorBuffer(self):\n    expected_output = [\n        57157, 63298, 39249, 44026, 9971, 11402, 31193, 36306, 19126, 22948,\n        3970, 5060, 5135, 6350, 2666, 3524, 461, 674\n    ]\n    tensor_in_sizes = [1, 3, 3, 2]\n    filter_in_sizes = [2, 2, 2, 2]\n    bias_in_sizes = [2]\n\n    x = self._CreateNumpyTensor(tensor_in_sizes)\n    filter_in = self._CreateNumpyTensor(filter_in_sizes)\n    bias_in = self._CreateNumpyTensor(bias_in_sizes)\n\n    conv1 = self._CreateConv2D(x, filter_in)\n\n    conv = self._CreateConv2D(conv1, filter_in)\n    bias_add = nn_ops.bias_add(conv, bias_in)\n    add = math_ops.add_n([bias_add, conv1])\n\n    self.assertAllEqual(\n        np.rint(expected_output),\n        self.evaluate(add).reshape(-1))\n\n\nif __name__ == \"__main__\":\n  for index, (input_size_, filter_size_, output_size_, stride_,\n              padding_) in enumerate(GetShrunkInceptionShapes()):\n    setattr(Conv2DTest, \"testInceptionFwd_\" + str(index),\n            test_util.run_in_graph_and_eager_modes(\n                GetInceptionFwdTest(input_size_, filter_size_, stride_,\n                                    padding_)))\n    setattr(\n        Conv2DTest, \"testInceptionFwdDilatedConv_\" + str(index),\n        test_util.run_in_graph_and_eager_modes(GetInceptionFwdDilatedConvTest(\n            input_size_, filter_size_, stride_, padding_)))\n    setattr(Conv2DTest, \"testInceptionBackInput_\" + str(index),\n            test_util.run_in_graph_and_eager_modes(\n                GetInceptionBackInputTest(input_size_, filter_size_,\n                                          output_size_, stride_, padding_)))\n    setattr(Conv2DTest, \"testInceptionBackFilter_\" + str(index),\n            test_util.run_in_graph_and_eager_modes(\n                GetInceptionBackFilterTest(input_size_, filter_size_,\n                                           output_size_, [stride_, stride_],\n                                           padding_)))\n\n  # TODO(b/35359731)\n  # Fwd, BckInput, and BackFilter to test that for certain input parameter\n  # set, winograd nonfused algorithm will be excluded from conv autotune. If\n  # in such case, winograd nonfused algorithm is added as one option of the\n  # conv autotune, and cuDNN version is smaller than 7, the following tests\n  # will fail.\n  ishape = [1, 400, 400, 1]\n  fshape = [1, 1, 1, 256]\n  oshape = [1, 400, 400, 256]\n  setattr(Conv2DTest, \"testInceptionFwd_No_Winograd_Nonfused\",\n          test_util.run_in_graph_and_eager_modes(\n              GetInceptionFwdTest(ishape, fshape, 1, \"SAME\", gpu_only=True)))\n  setattr(Conv2DTest, \"testInceptionFwdDilatedConv_No_Winograd_Nonfused\",\n          test_util.run_in_graph_and_eager_modes(\n              GetInceptionFwdDilatedConvTest(ishape, fshape, 1, \"SAME\")))\n  setattr(Conv2DTest, \"testInceptionBackInput_No_Winograd_Nonfused\",\n          test_util.run_in_graph_and_eager_modes(\n              GetInceptionBackInputTest(ishape, fshape, oshape, 1, \"SAME\",\n                                        gpu_only=True)))\n  setattr(Conv2DTest, \"testInceptionBackFilter_No_Winograd_Nonfused\",\n          test_util.run_in_graph_and_eager_modes(\n              GetInceptionBackFilterTest(ishape, fshape, oshape, [1, 1], \"SAME\",\n                                         gpu_only=True)))\n  test.main()\n"], "fixing_code": ["/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n// See docs in ../ops/nn_ops.cc.\n\n#define USE_EIGEN_TENSOR\n#define EIGEN_USE_THREADS\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n#define EIGEN_USE_GPU\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n#include \"tensorflow/core/kernels/conv_ops.h\"\n\n#include <string.h>\n\n#include <atomic>\n#include <map>\n#include <utility>\n#include <vector>\n\n#include \"absl/synchronization/blocking_counter.h\"\n#include \"tensorflow/core/framework/allocator.h\"\n#include \"tensorflow/core/framework/bounds_check.h\"\n#include \"tensorflow/core/framework/kernel_shape_util.h\"\n#include \"tensorflow/core/framework/numeric_op.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/register_types.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/tensor_shape.h\"\n#include \"tensorflow/core/framework/tensor_slice.h\"\n#include \"tensorflow/core/framework/types.h\"\n#include \"tensorflow/core/kernels/conv_2d.h\"\n#include \"tensorflow/core/kernels/deep_conv2d.h\"\n#include \"tensorflow/core/kernels/fill_functor.h\"\n#include \"tensorflow/core/kernels/ops_util.h\"\n#include \"tensorflow/core/lib/core/errors.h\"\n#include \"tensorflow/core/lib/gtl/array_slice.h\"\n#include \"tensorflow/core/lib/strings/numbers.h\"\n#include \"tensorflow/core/lib/strings/str_util.h\"\n#include \"tensorflow/core/platform/logging.h\"\n#include \"tensorflow/core/platform/macros.h\"\n#include \"tensorflow/core/profiler/lib/scoped_annotation.h\"\n#include \"tensorflow/core/util/padding.h\"\n#include \"tensorflow/core/util/tensor_format.h\"\n#include \"tensorflow/core/util/use_cudnn.h\"\n\n#ifdef TENSORFLOW_USE_LIBXSMM_CONVOLUTIONS\n#include \"tensorflow/core/kernels/xsmm_conv2d.h\"\n#endif\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n#include \"tensorflow/core/kernels/conv_ops_gpu.h\"\n#include \"tensorflow/core/platform/stream_executor.h\"\n#include \"tensorflow/core/protobuf/autotuning.pb.h\"\n#include \"tensorflow/core/util/autotune_maps/conv_autotune_maps.h\"\n#include \"tensorflow/core/util/autotune_maps/conv_parameters.h\"\n#include \"tensorflow/core/util/proto/proto_utils.h\"\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n#if GOOGLE_CUDA\n#include \"tensorflow/stream_executor/gpu/gpu_asm_opts.h\"\n#include \"tensorflow/stream_executor/gpu/redzone_allocator.h\"\n#include \"tensorflow/stream_executor/tf_allocator_adapter.h\"\n#endif  // GOOGLE_CUDA\n\nnamespace tensorflow {\n\ntypedef Eigen::ThreadPoolDevice CPUDevice;\ntypedef Eigen::GpuDevice GPUDevice;\n\nnamespace {\ntemplate <typename Device, typename T>\nstruct LaunchGeneric {\n  void operator()(OpKernelContext* ctx, const Tensor& input,\n                  const Tensor& filter, int row_stride, int col_stride,\n                  int row_dilation, int col_dilation, const Padding& padding,\n                  const std::vector<int64_t>& explicit_paddings, Tensor* output,\n                  TensorFormat data_format) {\n    CHECK(data_format == FORMAT_NHWC) << \"Generic conv implementation only \"\n                                         \"supports NHWC tensor format for now.\";\n    if (filter.dim_size(0) == 1 && filter.dim_size(1) == 1 && row_stride == 1 &&\n        col_stride == 1 && (padding == SAME || padding == VALID)) {\n      // For 1x1 kernel, the 2D convolution is reduced to matrix\n      // multiplication.\n      //\n      // TODO(vrv): We should be able to call SpatialConvolution\n      // and it will produce the same result, but doing so\n      // led to NaNs during training.  Using matmul instead for now.\n      int conv_width = 1;  // Width for the convolution step.\n      for (int i = 0; i < 3; ++i) {\n        conv_width *= output->dim_size(i);\n      }\n\n      Eigen::array<Eigen::IndexPair<Eigen::DenseIndex>, 1> dim_pair;\n      dim_pair[0] = Eigen::IndexPair<Eigen::DenseIndex>(1, 0);\n      functor::MatMulConvFunctor<Device, T>()(\n          ctx->eigen_device<Device>(),\n          output->shaped<T, 2>({conv_width, filter.dim_size(3)}),\n          input.shaped<T, 2>({conv_width, filter.dim_size(2)}),\n          filter.shaped<T, 2>({filter.dim_size(2), filter.dim_size(3)}),\n          dim_pair);\n    } else if (filter.dim_size(0) == input.dim_size(1) &&\n               filter.dim_size(1) == input.dim_size(2) && row_dilation == 1 &&\n               col_dilation == 1 && padding == VALID) {\n      // If the input data and filter have the same height/width,\n      // the 2D convolution is reduced to matrix multiplication.\n      const int k =  // Length of reduction dimension.\n          filter.dim_size(0) * filter.dim_size(1) * filter.dim_size(2);\n\n      Eigen::array<Eigen::IndexPair<Eigen::DenseIndex>, 1> dim_pair;\n      dim_pair[0] = Eigen::IndexPair<Eigen::DenseIndex>(1, 0);\n      functor::MatMulConvFunctor<Device, T>()(\n          ctx->eigen_device<Device>(),\n          output->shaped<T, 2>({input.dim_size(0), filter.dim_size(3)}),\n          input.shaped<T, 2>({input.dim_size(0), k}),\n          filter.shaped<T, 2>({k, filter.dim_size(3)}), dim_pair);\n    } else {\n      if (padding == EXPLICIT) {\n        functor::SpatialConvolution<Device, T>()(\n            ctx->eigen_device<Device>(), output->tensor<T, 4>(),\n            input.tensor<T, 4>(), filter.tensor<T, 4>(), row_stride, col_stride,\n            row_dilation, col_dilation, static_cast<int>(explicit_paddings[2]),\n            static_cast<int>(explicit_paddings[3]),\n            static_cast<int>(explicit_paddings[4]),\n            static_cast<int>(explicit_paddings[5]));\n      } else {\n        functor::SpatialConvolution<Device, T>()(\n            ctx->eigen_device<Device>(), output->tensor<T, 4>(),\n            input.tensor<T, 4>(), filter.tensor<T, 4>(), row_stride, col_stride,\n            row_dilation, col_dilation, BrainPadding2EigenPadding(padding));\n      }\n    }\n  }\n};\n\n// Compute grouped 2D convolutions on CPU. Unlike grouped convolution\n// implementation in cuDNN this is faaaaaar from optimal and needs more work\n// to deliver competitive performance. Currently it exists to close the feature\n// parity gap between convolution operations on different devices.\ntemplate <typename T>\nstruct LaunchGrouped {\n  void operator()(OpKernelContext* ctx, const Tensor& input,\n                  const Tensor& filter, int row_stride, int col_stride,\n                  int row_dilation, int col_dilation, const Padding& padding,\n                  const std::vector<int64_t>& explicit_paddings, Tensor* output,\n                  TensorFormat data_format) {\n    DCHECK(data_format == FORMAT_NHWC)\n        << \"Grouped conv implementation only \"\n           \"supports NHWC tensor format for now.\";\n\n    const int64_t in_depth = input.dim_size(3);\n    const int64_t patch_depth = filter.dim_size(2);\n    const int64_t num_groups = in_depth / patch_depth;\n\n    // Shuffle input/filter tensors to have group as a leading dimension.\n    std::array<int64_t, 5> shuffle({3, 0, 1, 2, 4});\n\n    // Compute pre shuffle dimemnsions.\n    auto pre_shuffle = [&](const Tensor& tensor) -> std::array<int64, 5> {\n      return {tensor.dim_size(0), tensor.dim_size(1), tensor.dim_size(2),\n              num_groups, tensor.dim_size(3) / num_groups};\n    };\n\n    // Compute post shuffle dimemnsions.\n    auto post_shuffle = [&](const Tensor& tensor) -> std::array<int64, 5> {\n      return {num_groups, tensor.dim_size(0), tensor.dim_size(1),\n              tensor.dim_size(2), tensor.dim_size(3) / num_groups};\n    };\n\n    auto& device = ctx->eigen_device<CPUDevice>();\n\n    absl::BlockingCounter shuffles_completed(2);\n    auto on_shuffled = [&]() { shuffles_completed.DecrementCount(); };\n\n    // Shuffle input into temporary tensor.\n    Tensor input_shuffled;\n    OP_REQUIRES_OK(\n        ctx, ctx->allocate_temp(input.dtype(), TensorShape(post_shuffle(input)),\n                                &input_shuffled));\n    input_shuffled.tensor<T, 5>().device(device, on_shuffled) =\n        input.shaped<T, 5>(pre_shuffle(input)).shuffle(shuffle);\n\n    // Shuffle filter into temporary tensor.\n    Tensor filter_shuffled;\n    OP_REQUIRES_OK(ctx, ctx->allocate_temp(filter.dtype(),\n                                           TensorShape(post_shuffle(filter)),\n                                           &filter_shuffled));\n    filter_shuffled.tensor<T, 5>().device(device, on_shuffled) =\n        filter.shaped<T, 5>(pre_shuffle(filter)).shuffle(shuffle);\n\n    // Wait for the completion of input/filter shuffles.\n    shuffles_completed.Wait();\n\n    // Write group convolution results into temporary output tensor.\n    Tensor output_shuffled;\n    OP_REQUIRES_OK(ctx, ctx->allocate_temp(output->dtype(),\n                                           TensorShape(post_shuffle(*output)),\n                                           &output_shuffled));\n\n    for (int64_t i = 0; i < num_groups; ++i) {\n      // TODO(ezhulenev): Run this loop using `parallelFor` (regular parallelFor\n      // will lead to deadlock, SpatialConvolution has to use async Eigen\n      // assignment). This requires small changes to Eigen to support async\n      // exeuction for tensor chipping operation.\n\n      // TODO(ezhulenev): Grouped convolution should also support 1x1 filter\n      // optimization.\n\n      auto input_slice = input_shuffled.tensor<T, 5>().template chip<0>(i);\n      auto filter_slice = filter_shuffled.tensor<T, 5>().template chip<0>(i);\n      auto output_slice = output_shuffled.tensor<T, 5>().template chip<0>(i);\n\n      if (padding == EXPLICIT) {\n        functor::SpatialConvolution<CPUDevice, T>()(\n            ctx->eigen_device<CPUDevice>(), output_slice, input_slice,\n            filter_slice, row_stride, col_stride, row_dilation, col_dilation,\n            static_cast<int>(explicit_paddings[2]),\n            static_cast<int>(explicit_paddings[3]),\n            static_cast<int>(explicit_paddings[4]),\n            static_cast<int>(explicit_paddings[5]));\n      } else {\n        functor::SpatialConvolution<CPUDevice, T>()(\n            ctx->eigen_device<CPUDevice>(), output_slice, input_slice,\n            filter_slice, row_stride, col_stride, row_dilation, col_dilation,\n            BrainPadding2EigenPadding(padding));\n      }\n    }\n\n    // Shuffle temporary output back into pre-shuffled shape.\n    std::array<int64_t, 5> rev_shuffle({1, 2, 3, 0, 4});\n    output->shaped<T, 5>(pre_shuffle(*output)).device(device) =\n        output_shuffled.tensor<T, 5>().shuffle(rev_shuffle);\n  }\n};\n\n}  // namespace\n\ntemplate <typename T>\nstruct LaunchConv2DOp<CPUDevice, T> {\n  void operator()(OpKernelContext* ctx, bool use_cudnn, bool cudnn_use_autotune,\n                  const Tensor& input, const Tensor& filter, int row_dilation,\n                  int col_dilation, int row_stride, int col_stride,\n                  const Padding& padding,\n                  const std::vector<int64_t>& explicit_paddings, Tensor* output,\n                  TensorFormat data_format) {\n    if (data_format != FORMAT_NHWC) {\n      ctx->SetStatus(errors::Unimplemented(\n          \"The Conv2D op currently only supports the NHWC tensor format on the \"\n          \"CPU. The op was given the format: \",\n          ToString(data_format)));\n      return;\n    }\n\n    for (int64_t explicit_padding : explicit_paddings) {\n      if (!FastBoundsCheck(explicit_padding, std::numeric_limits<int>::max())) {\n        ctx->SetStatus(errors::InvalidArgument(\"filter too large\"));\n        return;\n      }\n    }\n\n    const int64_t in_depth = input.dim_size(3);\n    const int64_t out_depth = output->dim_size(3);\n    const int64_t patch_depth = filter.dim_size(2);\n\n    if (patch_depth <= 0) {\n      ctx->SetStatus(errors::InvalidArgument(\n          \"filter depth must be stricly positive, got \", patch_depth));\n      return;\n    }\n    if (in_depth % patch_depth != 0) {\n      ctx->SetStatus(errors::InvalidArgument(\n          \"input depth must be evenly divisible by filter depth: \", in_depth,\n          \" vs \", patch_depth));\n      return;\n    }\n    if (filter.NumElements() <= 0) {\n      ctx->SetStatus(\n          errors::InvalidArgument(\"filter must not have zero elements \"\n                                  \"(i.e. all dimensions must be non-zero)\"));\n      return;\n    }\n\n    const int64_t num_groups = in_depth / patch_depth;\n    if (num_groups <= 0) {\n      ctx->SetStatus(errors::InvalidArgument(\n          \"number of groups must be stricly positive, got \", num_groups));\n      return;\n    }\n    if (out_depth % num_groups != 0 || out_depth < num_groups) {\n      ctx->SetStatus(errors::InvalidArgument(\n          \"output depth must be evenly divisible by number of groups: \",\n          out_depth, \" vs \", num_groups));\n      return;\n    }\n\n    if (in_depth != patch_depth) {\n      LaunchGrouped<T>()(ctx, input, filter, row_stride, col_stride,\n                         row_dilation, col_dilation, padding, explicit_paddings,\n                         output, data_format);\n    } else {\n      LaunchGeneric<CPUDevice, T>()(ctx, input, filter, row_stride, col_stride,\n                                    row_dilation, col_dilation, padding,\n                                    explicit_paddings, output, data_format);\n    }\n  }\n};\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\ntemplate <>\nstruct LaunchConv2DOp<GPUDevice, int32> {\n  void operator()(OpKernelContext* ctx, bool use_cudnn, bool cudnn_use_autotune,\n                  const Tensor& input, const Tensor& filter, int row_dilation,\n                  int col_dilation, int row_stride, int col_stride,\n                  const Padding& padding,\n                  const std::vector<int64_t>& explicit_paddings, Tensor* output,\n                  TensorFormat data_format) {\n    if (data_format != FORMAT_NHWC) {\n      ctx->SetStatus(\n          errors::Unimplemented(\"The Conv2D op currently only supports the \"\n                                \"NHWC tensor format for integer types. \"\n                                \"The op was given the format: \",\n                                ToString(data_format)));\n      return;\n    }\n    const int64_t in_depth = GetTensorDim(input, data_format, 'C');\n    OP_REQUIRES(ctx, in_depth == filter.dim_size(2),\n                errors::Unimplemented(\n                    \"The Conv2D op currently does not support grouped \"\n                    \"convolutions for integer types. A grouped convolution was \"\n                    \"attempted to be run because the input depth of \",\n                    in_depth, \" does not match the filter input depth of \",\n                    filter.dim_size(2)));\n    OP_REQUIRES(\n        ctx, filter.NumElements() > 0,\n        errors::InvalidArgument(\"filter must not have zero elements \"\n                                \"(i.e. all dimensions must be non-zero)\"));\n\n    for (int64_t explicit_padding : explicit_paddings) {\n      if (!FastBoundsCheck(explicit_padding, std::numeric_limits<int>::max())) {\n        ctx->SetStatus(errors::InvalidArgument(\"filter too large\"));\n        return;\n      }\n    }\n    LaunchGeneric<GPUDevice, int32>()(\n        ctx, input, filter, row_stride, col_stride, row_dilation, col_dilation,\n        padding, explicit_paddings, output, data_format);\n  }\n};\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\ntemplate <typename Device, typename T>\nclass LaunchDeepConvOp {\n public:\n  static bool Run(OpKernelContext* ctx, const Tensor& input,\n                  const Tensor& filter, int batch, int input_rows,\n                  int input_cols, int in_depth, int filter_rows,\n                  int filter_cols, int pad_rows, int pad_cols, int out_rows,\n                  int /*out_cols*/, int /*out_depth*/, int /*dilation_rows*/,\n                  int /*dilation_cols*/, int /*stride_rows*/,\n                  int /*stride_cols*/, Tensor* /*output*/,\n                  TensorFormat /*data_format*/) {\n    return false;\n  }\n};\n\n// Conditionally launches DeepConv operation based on convolution parameters.\ntemplate <>\nclass LaunchDeepConvOp<CPUDevice, float> {\n public:\n  static bool Run(OpKernelContext* ctx, const Tensor& input,\n                  const Tensor& filter, int batch, int input_rows,\n                  int input_cols, int in_depth, int filter_rows,\n                  int filter_cols, int pad_rows, int pad_cols, int out_rows,\n                  int out_cols, int out_depth, int dilation_rows,\n                  int dilation_cols, int stride_rows, int stride_cols,\n                  Tensor* output, TensorFormat data_format) {\n    if (data_format != FORMAT_NHWC || dilation_rows != 1 ||\n        dilation_cols != 1 ||\n        !CanUseDeepConv2D(stride_rows, stride_cols, filter_rows, filter_cols,\n                          in_depth, out_depth, out_rows, out_cols)) {\n      return false;\n    }\n\n    Conv2DArgs args;\n    args.batch = batch;\n    args.in_rows = input_rows;\n    args.in_cols = input_cols;\n    args.in_depth = in_depth;\n    args.filter_rows = filter_rows;\n    args.filter_cols = filter_cols;\n    args.pad_rows = pad_rows;\n    args.pad_cols = pad_cols;\n    args.out_rows = out_rows;\n    args.out_cols = out_cols;\n    args.out_depth = out_depth;\n\n    auto input_ptr = input.template flat<float>().data();\n    auto filter_ptr = filter.template flat<float>().data();\n    auto output_ptr = output->template flat<float>().data();\n\n    functor::DeepConv2D<CPUDevice, float>()(ctx, args, input_ptr, filter_ptr,\n                                            output_ptr);\n    return true;\n  }\n};\n\n#ifdef TENSORFLOW_USE_LIBXSMM_CONVOLUTIONS\ntemplate <typename Device, typename T>\nclass LaunchXsmmConvOp {\n public:\n  static bool Run(OpKernelContext* ctx, const Tensor& input,\n                  const Tensor& filter, int batch, int input_rows,\n                  int input_cols, int in_depth, int filter_rows,\n                  int filter_cols, int pad_rows, int pad_cols, int out_rows,\n                  int out_cols, int out_depth, int stride_rows, int stride_cols,\n                  int dilation_rows, int dilation_cols, Tensor* output,\n                  TensorFormat data_format) {\n    return false;\n  }\n};\n\ntemplate <>\nclass LaunchXsmmConvOp<CPUDevice, float> {\n public:\n  static bool Run(OpKernelContext* ctx, const Tensor& input,\n                  const Tensor& filter, int batch, int input_rows,\n                  int input_cols, int in_depth, int filter_rows,\n                  int filter_cols, int pad_rows, int pad_cols, int out_rows,\n                  int out_cols, int out_depth, int dilation_rows,\n                  int dilation_cols, int stride_rows, int stride_cols,\n                  Tensor* output, TensorFormat data_format) {\n    auto num_threads =\n        ctx->device()->tensorflow_cpu_worker_threads()->num_threads;\n    // See libxsmm_dnn.h for this struct definition.\n    libxsmm_dnn_conv_desc desc;\n    desc.N = batch;\n    desc.C = in_depth;\n    desc.H = input_rows;\n    desc.W = input_cols;\n    desc.K = out_depth;\n    desc.R = filter_rows;\n    desc.S = filter_cols;\n    desc.u = stride_rows;\n    desc.v = stride_cols;\n    desc.pad_h = pad_rows;\n    desc.pad_w = pad_cols;\n    desc.pad_h_in = 0;\n    desc.pad_w_in = 0;\n    desc.pad_h_out = 0;\n    desc.pad_w_out = 0;\n    desc.threads = num_threads;\n    desc.algo = LIBXSMM_DNN_CONV_ALGO_DIRECT;\n    desc.buffer_format = LIBXSMM_DNN_TENSOR_FORMAT_NHWC;\n    desc.filter_format = LIBXSMM_DNN_TENSOR_FORMAT_LIBXSMM;\n    desc.fuse_ops = LIBXSMM_DNN_CONV_FUSE_NONE;\n    desc.options = LIBXSMM_DNN_CONV_OPTION_OVERWRITE;\n    desc.datatype_out = LIBXSMM_DNN_DATATYPE_F32;\n    desc.datatype_in = LIBXSMM_DNN_DATATYPE_F32;\n    if (dilation_rows != 1 || dilation_cols != 1 ||\n        !CanUseXsmmConv2D(desc, data_format)) {\n      return false;\n    }\n\n    auto input_ptr = input.template flat<float>().data();\n    auto filter_ptr = filter.template flat<float>().data();\n    auto output_ptr = output->template flat<float>().data();\n\n    bool success = functor::XsmmFwdConv2D<CPUDevice, float>()(\n        ctx, desc, input_ptr, filter_ptr, output_ptr);\n    return success;\n  }\n};\n#endif\n\n#define TF_REQUIRES(EXP, STATUS)                \\\n  do {                                          \\\n    if (!TF_PREDICT_TRUE(EXP)) return (STATUS); \\\n  } while (false)\n\nStatus InitConv2DParameters(const OpKernelConstruction* context,\n                            Conv2DParameters* params) {\n  TF_RETURN_IF_ERROR(context->GetAttr(\"dilations\", &params->dilations));\n  TF_RETURN_IF_ERROR(context->GetAttr(\"strides\", &params->strides));\n  TF_RETURN_IF_ERROR(context->GetAttr(\"padding\", &params->padding));\n  if (context->HasAttr(\"explicit_paddings\")) {\n    TF_RETURN_IF_ERROR(\n        context->GetAttr(\"explicit_paddings\", &params->explicit_paddings));\n  }\n  string data_format_string;\n  TF_RETURN_IF_ERROR(context->GetAttr(\"data_format\", &data_format_string));\n  TF_REQUIRES(FormatFromString(data_format_string, &params->data_format),\n              errors::InvalidArgument(\"Invalid data format\"));\n\n  const auto& strides = params->strides;\n  const auto& dilations = params->dilations;\n  const auto& data_format = params->data_format;\n\n  TF_REQUIRES(dilations.size() == 4,\n              errors::InvalidArgument(\"Sliding window dilations field must \"\n                                      \"specify 4 dimensions\"));\n  TF_REQUIRES(strides.size() == 4,\n              errors::InvalidArgument(\"Sliding window strides field must \"\n                                      \"specify 4 dimensions\"));\n  const int64_t stride_n = GetTensorDim(strides, data_format, 'N');\n  const int64_t stride_c = GetTensorDim(strides, data_format, 'C');\n  const int64_t stride_h = GetTensorDim(strides, data_format, 'H');\n  const int64_t stride_w = GetTensorDim(strides, data_format, 'W');\n  TF_REQUIRES(\n      stride_n == 1 && stride_c == 1,\n      errors::Unimplemented(\"Current implementation does not yet support \"\n                            \"strides in the batch and depth dimensions.\"));\n  TF_REQUIRES(stride_h > 0 && stride_w > 0,\n              errors::InvalidArgument(\n                  \"Row and column strides should be larger than 0.\"));\n\n  const int64_t dilation_n = GetTensorDim(dilations, data_format, 'N');\n  const int64_t dilation_c = GetTensorDim(dilations, data_format, 'C');\n  const int64_t dilation_h = GetTensorDim(dilations, data_format, 'H');\n  const int64_t dilation_w = GetTensorDim(dilations, data_format, 'W');\n  TF_REQUIRES(\n      dilation_n == 1 && dilation_c == 1,\n      errors::Unimplemented(\"Current implementation does not yet support \"\n                            \"dilations in the batch and depth dimensions.\"));\n  TF_REQUIRES(\n      dilation_h > 0 && dilation_w > 0,\n      errors::InvalidArgument(\"Dilated rates should be larger than 0.\"));\n\n  TF_RETURN_IF_ERROR(CheckValidPadding(params->padding,\n                                       params->explicit_paddings,\n                                       /*num_dims=*/4, data_format));\n\n  return OkStatus();\n}\n\nStatus ComputeConv2DDimension(const Conv2DParameters& params,\n                              const Tensor& input, const Tensor& filter,\n                              Conv2DDimensions* dimensions) {\n  // Check that 2D convolution input and filter have exactly 4 dimensions.\n  TF_REQUIRES(input.dims() == 4,\n              errors::InvalidArgument(\"input must be 4-dimensional\",\n                                      input.shape().DebugString()));\n  TF_REQUIRES(filter.dims() == 4,\n              errors::InvalidArgument(\"filter must be 4-dimensional: \",\n                                      filter.shape().DebugString()));\n  for (int i = 0; i < 3; i++) {\n    TF_REQUIRES(\n        FastBoundsCheck(filter.dim_size(i), std::numeric_limits<int>::max()),\n        errors::InvalidArgument(\"filter too large\"));\n  }\n\n  // The last dimension for input is in_depth. Check that it is the same as the\n  // filter's in_depth or it is evenly divisible by filter's in_depth.\n  const int64_t in_depth_raw = GetTensorDim(input, params.data_format, 'C');\n  const int64_t patch_depth_raw = filter.dim_size(2);\n  TF_REQUIRES(FastBoundsCheck(in_depth_raw, std::numeric_limits<int>::max()),\n              errors::InvalidArgument(\"Input depth too large\"));\n  TF_REQUIRES(FastBoundsCheck(patch_depth_raw, std::numeric_limits<int>::max()),\n              errors::InvalidArgument(\"Patch depth too large\"));\n  const int in_depth = static_cast<int>(in_depth_raw);\n  const int patch_depth = static_cast<int>(patch_depth_raw);\n  TF_REQUIRES(patch_depth > 0,\n              errors::InvalidArgument(\n                  \"filter depth must be stricly positive, got \", patch_depth));\n  TF_REQUIRES(in_depth % patch_depth == 0,\n              errors::InvalidArgument(\n                  \"input depth must be evenly divisible by filter depth: \",\n                  in_depth, \" vs \", patch_depth));\n\n  // The last dimension for filter is out_depth.\n  const int out_depth = static_cast<int>(filter.dim_size(3));\n\n  // The second dimension for input is rows/height.\n  // The first dimension for filter is rows/height.\n  const int64_t input_rows_raw = GetTensorDim(input, params.data_format, 'H');\n  TF_REQUIRES(FastBoundsCheck(input_rows_raw, std::numeric_limits<int>::max()),\n              errors::InvalidArgument(\"Input rows too large\"));\n  const int input_rows = static_cast<int>(input_rows_raw);\n  const int filter_rows = static_cast<int>(filter.dim_size(0));\n\n  // The third dimension for input is columns/width.\n  // The second dimension for filter is columns/width.\n  const int64_t input_cols_raw = GetTensorDim(input, params.data_format, 'W');\n  TF_REQUIRES(FastBoundsCheck(input_cols_raw, std::numeric_limits<int>::max()),\n              errors::InvalidArgument(\"Input cols too large\"));\n  const int input_cols = static_cast<int>(input_cols_raw);\n  const int filter_cols = static_cast<int>(filter.dim_size(1));\n\n  // The first dimension for input is batch.\n  const int64_t batch_raw = GetTensorDim(input, params.data_format, 'N');\n  TF_REQUIRES(FastBoundsCheck(batch_raw, std::numeric_limits<int>::max()),\n              errors::InvalidArgument(\"batch is too large\"));\n  const int batch = static_cast<int>(batch_raw);\n\n  // Take the stride and dilation from the second and third dimensions only (we\n  // do not support striding or dilation on the batch or depth dimension).\n  const int stride_rows = GetTensorDim(params.strides, params.data_format, 'H');\n  const int stride_cols = GetTensorDim(params.strides, params.data_format, 'W');\n  const int dilation_rows =\n      GetTensorDim(params.dilations, params.data_format, 'H');\n  const int dilation_cols =\n      GetTensorDim(params.dilations, params.data_format, 'W');\n\n  int64_t pad_rows_before, pad_rows_after, pad_cols_before, pad_cols_after;\n  if (params.padding == Padding::EXPLICIT) {\n    GetExplicitPaddingForDim(params.explicit_paddings, params.data_format, 'H',\n                             &pad_rows_before, &pad_rows_after);\n    GetExplicitPaddingForDim(params.explicit_paddings, params.data_format, 'W',\n                             &pad_cols_before, &pad_cols_after);\n  }\n\n  // Compute windowed output sizes for rows and columns.\n  int64_t out_rows = 0, out_cols = 0;\n  TF_RETURN_IF_ERROR(GetWindowedOutputSizeVerboseV2(\n      input_rows, filter_rows, dilation_rows, stride_rows, params.padding,\n      &out_rows, &pad_rows_before, &pad_rows_after));\n  TF_RETURN_IF_ERROR(GetWindowedOutputSizeVerboseV2(\n      input_cols, filter_cols, dilation_cols, stride_cols, params.padding,\n      &out_cols, &pad_cols_before, &pad_cols_after));\n\n  dimensions->batch = batch;\n  dimensions->input_rows = input_rows;\n  dimensions->input_cols = input_cols;\n  dimensions->in_depth = in_depth;\n  dimensions->filter_rows = filter_rows;\n  dimensions->filter_cols = filter_cols;\n  dimensions->patch_depth = patch_depth;\n  dimensions->out_depth = out_depth;\n  dimensions->stride_rows = stride_rows;\n  dimensions->stride_cols = stride_cols;\n  dimensions->dilation_rows = dilation_rows;\n  dimensions->dilation_cols = dilation_cols;\n  dimensions->out_rows = out_rows;\n  dimensions->out_cols = out_cols;\n  dimensions->pad_rows_before = pad_rows_before;\n  dimensions->pad_rows_after = pad_rows_after;\n  dimensions->pad_cols_before = pad_cols_before;\n  dimensions->pad_cols_after = pad_cols_after;\n\n  return OkStatus();\n}\n\n#undef TF_REQUIRES\n\ntemplate <typename Device, typename T>\nclass Conv2DOp : public BinaryOp<T> {\n public:\n  explicit Conv2DOp(OpKernelConstruction* context) : BinaryOp<T>(context) {\n    OP_REQUIRES_OK(context, InitConv2DParameters(context, &params_));\n\n    OP_REQUIRES_OK(context, context->GetAttr(\"use_cudnn_on_gpu\", &use_cudnn_));\n    cudnn_use_autotune_ = CudnnUseAutotune();\n  }\n\n  void Compute(OpKernelContext* context) override {\n    // Input tensor is of the following dimensions:\n    // [ batch, in_rows, in_cols, in_depth ]\n    const Tensor& input = context->input(0);\n\n    // Input filter is of the following dimensions:\n    // [ filter_rows, filter_cols, in_depth, out_depth]\n    const Tensor& filter = context->input(1);\n\n    Conv2DDimensions dimensions;\n    OP_REQUIRES_OK(context,\n                   ComputeConv2DDimension(params_, input, filter, &dimensions));\n\n    TensorShape out_shape = ShapeFromFormat(\n        params_.data_format, dimensions.batch, dimensions.out_rows,\n        dimensions.out_cols, dimensions.out_depth);\n\n    // Output tensor is of the following dimensions:\n    // [ in_batch, out_rows, out_cols, out_depth ]\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(0, out_shape, &output));\n\n    VLOG(2) << \"Conv2D: in_depth = \" << dimensions.in_depth\n            << \", patch_depth = \" << dimensions.patch_depth\n            << \", input_cols = \" << dimensions.input_cols\n            << \", filter_cols = \" << dimensions.filter_cols\n            << \", input_rows = \" << dimensions.input_rows\n            << \", filter_rows = \" << dimensions.filter_rows\n            << \", stride_rows = \" << dimensions.stride_rows\n            << \", stride_cols = \" << dimensions.stride_cols\n            << \", dilation_rows = \" << dimensions.dilation_rows\n            << \", dilation_cols = \" << dimensions.dilation_cols\n            << \", out_depth = \" << dimensions.out_depth;\n\n    // If there is nothing to compute, return.\n    if (out_shape.num_elements() == 0) {\n      return;\n    }\n\n    // If the input is empty, result can only be due to padding.\n    if (input.NumElements() == 0) {\n      // Zero-out output and return.\n      functor::SetZeroFunctor<Device, T>()(context->eigen_device<Device>(),\n                                           output->template flat<T>());\n\n      return;\n    }\n\n#ifdef TENSORFLOW_USE_LIBXSMM_CONVOLUTIONS\n    if (params_.padding != EXPLICIT &&\n        LaunchXsmmConvOp<Device, T>::Run(\n            context, input, filter, dimensions.batch, dimensions.input_rows,\n            dimensions.input_cols, dimensions.in_depth, dimensions.filter_rows,\n            dimensions.filter_cols, dimensions.pad_rows_before,\n            dimensions.pad_cols_before, dimensions.out_rows,\n            dimensions.out_cols, dimensions.out_depth, dimensions.dilation_rows,\n            dimensions.dilation_cols, dimensions.stride_rows,\n            dimensions.stride_cols, output, params_.data_format)) {\n      return;\n    }\n#endif\n\n    if (params_.padding != EXPLICIT &&\n        LaunchDeepConvOp<Device, T>::Run(\n            context, input, filter, dimensions.batch, dimensions.input_rows,\n            dimensions.input_cols, dimensions.in_depth, dimensions.filter_rows,\n            dimensions.filter_cols, dimensions.pad_rows_before,\n            dimensions.pad_cols_before, dimensions.out_rows,\n            dimensions.out_cols, dimensions.out_depth, dimensions.dilation_rows,\n            dimensions.dilation_cols, dimensions.stride_rows,\n            dimensions.stride_cols, output, params_.data_format)) {\n      return;\n    }\n\n    launcher_(context, use_cudnn_, cudnn_use_autotune_, input, filter,\n              dimensions.dilation_rows, dimensions.dilation_cols,\n              dimensions.stride_rows, dimensions.stride_cols, params_.padding,\n              params_.explicit_paddings, output, params_.data_format);\n  }\n\n private:\n  Conv2DParameters params_;\n  bool use_cudnn_;\n  bool cudnn_use_autotune_;\n\n  LaunchConv2DOp<Device, T> launcher_;\n\n  TF_DISALLOW_COPY_AND_ASSIGN(Conv2DOp);\n};\n\n#define REGISTER_CPU(T)                                         \\\n  REGISTER_KERNEL_BUILDER(                                      \\\n      Name(\"Conv2D\").Device(DEVICE_CPU).TypeConstraint<T>(\"T\"), \\\n      Conv2DOp<CPUDevice, T>);\n\n// If we're using the alternative GEMM-based implementation of Conv2D for the\n// CPU implementation, don't register this EigenTensor-based version.\n#if !defined(USE_GEMM_FOR_CONV)\nTF_CALL_half(REGISTER_CPU);\nTF_CALL_float(REGISTER_CPU);\nTF_CALL_double(REGISTER_CPU);\nTF_CALL_int32(REGISTER_CPU);\n#endif  // USE_GEMM_FOR_CONV\n\n// To be used inside depthwise_conv_op.cc.\ntemplate struct LaunchConv2DOp<CPUDevice, Eigen::bfloat16>;\ntemplate struct LaunchConv2DOp<CPUDevice, Eigen::half>;\ntemplate struct LaunchConv2DOp<CPUDevice, float>;\ntemplate struct LaunchConv2DOp<CPUDevice, double>;\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\nint64_t GetDnnWorkspaceLimit(const string& envvar_in_mb,\n                             int64_t default_value_in_bytes) {\n  const char* workspace_limit_in_mb_str = getenv(envvar_in_mb.c_str());\n  if (workspace_limit_in_mb_str != nullptr &&\n      strcmp(workspace_limit_in_mb_str, \"\") != 0) {\n    int64_t scratch_limit_in_mb = -1;\n    if (strings::safe_strto64(workspace_limit_in_mb_str,\n                              &scratch_limit_in_mb)) {\n      return scratch_limit_in_mb * (1 << 20);\n    } else {\n      LOG(WARNING) << \"Invalid value for env-var \" << envvar_in_mb << \": \"\n                   << workspace_limit_in_mb_str;\n    }\n  }\n  return default_value_in_bytes;\n}\n\nint64_t GetDnnWorkspaceLimitOrDefault() {\n  return GetDnnWorkspaceLimit(\"TF_CUDNN_WORKSPACE_LIMIT_IN_MB\",\n                              1LL << 33);  // 8GB by default\n}\n\ntemplate <typename T>\nvoid LaunchConv2DOp<GPUDevice, T>::operator()(\n    OpKernelContext* ctx, bool use_cudnn, bool cudnn_use_autotune,\n    const Tensor& input_param, const Tensor& filter, int row_dilation,\n    int col_dilation, int row_stride, int col_stride, const Padding& padding,\n    const std::vector<int64_t>& explicit_paddings, Tensor* output,\n    TensorFormat data_format) {\n  using se::dnn::AlgorithmConfig;\n  using se::dnn::AlgorithmDesc;\n  using se::dnn::ProfileResult;\n  auto* stream = ctx->op_device_context()->stream();\n  OP_REQUIRES(ctx, stream, errors::Internal(\"No GPU stream available.\"));\n\n  if (!use_cudnn) {\n    ctx->SetStatus(\n        errors::Unimplemented(\"Conv2D for GPU is not currently supported \"\n                              \"without cudnn\"));\n    return;\n  }\n\n  Tensor input = input_param;\n  const int64_t in_batch = GetTensorDim(input, data_format, 'N');\n  int64_t in_rows = GetTensorDim(input, data_format, 'H');\n  int64_t in_cols = GetTensorDim(input, data_format, 'W');\n  const int64_t in_depths = GetTensorDim(input, data_format, 'C');\n  const int64_t patch_rows = filter.dim_size(0);\n  const int64_t patch_cols = filter.dim_size(1);\n  const int64_t patch_depths = filter.dim_size(2);\n\n  OP_REQUIRES(\n      ctx, filter.NumElements() > 0,\n      errors::InvalidArgument(\"filter must not have zero elements \"\n                              \"(i.e. all dimensions must be non-zero)\"));\n\n  // If the filter in-depth (patch_depths) is 1 and smaller than the input\n  // depth, it's a depthwise convolution. More generally, if the filter in-depth\n  // divides but is smaller than the input depth, it is a grouped convolution.\n  bool is_grouped_convolution = patch_depths != in_depths;\n  if (patch_rows == 1 && patch_cols == 1 && !is_grouped_convolution &&\n      row_dilation == 1 && col_dilation == 1 && row_stride == 1 &&\n      col_stride == 1 && data_format == FORMAT_NHWC &&\n      (padding == VALID || padding == SAME)) {\n    // 1x1 filter, so call cublas directly.\n    const uint64 m = in_batch * in_rows * in_cols;\n    const uint64 k = patch_depths;\n    const uint64 n = filter.dim_size(3);\n\n    auto a_ptr = AsDeviceMemory(input.template flat<T>().data(),\n                                input.template flat<T>().size());\n    auto b_ptr = AsDeviceMemory(filter.template flat<T>().data(),\n                                filter.template flat<T>().size());\n    auto c_ptr = AsDeviceMemory(output->template flat<T>().data(),\n                                output->template flat<T>().size());\n\n    auto no_transpose = se::blas::Transpose::kNoTranspose;\n    OP_REQUIRES_OK(\n        ctx, stream->ThenBlasGemm(no_transpose, no_transpose, n, m, k, b_ptr, n,\n                                  a_ptr, k, &c_ptr, n,\n                                  se::blas::kDefaultComputePrecision));\n    return;\n  } else if (patch_rows == in_rows && patch_cols == in_cols &&\n             !is_grouped_convolution && row_dilation == 1 &&\n             col_dilation == 1 && padding == VALID &&\n             data_format == FORMAT_NHWC) {\n    // The input data and filter have the same height/width, so call cublas\n    // directly.\n    const uint64 m = in_batch;\n    const uint64 k = patch_rows * patch_cols * patch_depths;\n    const uint64 n = filter.dim_size(3);\n\n    auto a_ptr = AsDeviceMemory(input.template flat<T>().data(),\n                                input.template flat<T>().size());\n    auto b_ptr = AsDeviceMemory(filter.template flat<T>().data(),\n                                filter.template flat<T>().size());\n    auto c_ptr = AsDeviceMemory(output->template flat<T>().data(),\n                                output->template flat<T>().size());\n\n    auto no_transpose = se::blas::Transpose::kNoTranspose;\n    OP_REQUIRES_OK(\n        ctx, stream->ThenBlasGemm(no_transpose, no_transpose, n, m, k, b_ptr, n,\n                                  a_ptr, k, &c_ptr, n,\n                                  se::blas::kDefaultComputePrecision));\n    return;\n  }\n\n#if GOOGLE_CUDA\n  const bool compute_in_nhwc = ComputeInNhwcEnabled(DataTypeToEnum<T>::value,\n                                                    stream, /*is_conv2d=*/true);\n#else\n  // fast NHWC implementation is a CUDA only feature\n  const bool compute_in_nhwc = false;\n#endif\n\n  // We only do one directional conversion: NHWC->NCHW. We never convert in the\n  // other direction. Grappler layout optimizer selects preferred layout and\n  // adds necessary annotations to the graph.\n  // TODO(ezhulenev): Convert in other direction for fp16?\n  const TensorFormat compute_data_format =\n      (compute_in_nhwc && data_format == FORMAT_NHWC) ? FORMAT_NHWC\n                                                      : FORMAT_NCHW;\n\n  VLOG(3) << \"Compute Conv2D with cuDNN:\"\n          << \" data_format=\" << ToString(data_format)\n          << \" compute_data_format=\" << ToString(compute_data_format);\n\n  const int64_t out_batch = GetTensorDim(*output, data_format, 'N');\n  const int64_t out_rows = GetTensorDim(*output, data_format, 'H');\n  const int64_t out_cols = GetTensorDim(*output, data_format, 'W');\n  const int64_t out_depths = GetTensorDim(*output, data_format, 'C');\n  int64_t padding_top = -1, padding_bottom = -1;\n  int64_t padding_left = -1, padding_right = -1;\n  if (padding == EXPLICIT) {\n    GetExplicitPaddingForDim(explicit_paddings, data_format, 'H', &padding_top,\n                             &padding_bottom);\n    GetExplicitPaddingForDim(explicit_paddings, data_format, 'W', &padding_left,\n                             &padding_right);\n  }\n  int64_t out_rows_check, out_cols_check;\n  Status status = GetWindowedOutputSizeVerboseV2(\n      in_rows, patch_rows, row_dilation, row_stride, padding, &out_rows_check,\n      &padding_top, &padding_bottom);\n  // The status is guaranteed to be OK because we checked the output and padding\n  // was valid earlier.\n  TF_CHECK_OK(status);\n  DCHECK_EQ(out_rows, out_rows_check);\n  status = GetWindowedOutputSizeVerboseV2(in_cols, patch_cols, col_dilation,\n                                          col_stride, padding, &out_cols_check,\n                                          &padding_left, &padding_right);\n  TF_CHECK_OK(status);\n  DCHECK_EQ(out_cols, out_cols_check);\n\n  const int64_t common_padding_rows = std::min(padding_top, padding_bottom);\n  const int64_t common_padding_cols = std::min(padding_left, padding_right);\n  if (padding_top != padding_bottom || padding_left != padding_right) {\n    // cuDNN only supports padding the same amount on the left and right sides,\n    // and on the top and bottom sides. So we manually create a new padded\n    // input tensor such that we can pass it to cuDNN.\n    VLOG(4) << \"Pad input tensor:\"\n            << \" padding_top=\" << padding_top\n            << \" padding_bottom=\" << padding_bottom\n            << \" padding_left=\" << padding_left\n            << \" padding_right=\" << padding_right;\n\n    // TODO(reedwm): In some cases, we can avoid an allocation even if the two\n    // padding sides are different. For example, if the input is 2x2, the filter\n    // is 1x1, the stride is 2, and the padding is (1, 0, 1, 0), the result is\n    // equivalent to as if the padding is (1, 1, 1, 1). Changing the padding in\n    // such a way would allow us to avoid the allocation.\n    Tensor transformed_input;\n    const int64_t padding_rows_diff = std::abs(padding_bottom - padding_top);\n    const int64_t padding_cols_diff = std::abs(padding_right - padding_left);\n    const int64_t new_in_rows = in_rows + padding_rows_diff;\n    const int64_t new_in_cols = in_cols + padding_cols_diff;\n    OP_REQUIRES_OK(ctx, ctx->allocate_temp(\n                            DataTypeToEnum<T>::value,\n                            ShapeFromFormat(data_format, in_batch, new_in_rows,\n                                            new_in_cols, in_depths),\n                            &transformed_input));\n\n    const int64_t input_pad_top = padding_top - common_padding_rows;\n    const int64_t input_pad_bottom = padding_bottom - common_padding_rows;\n    const int64_t input_pad_left = padding_left - common_padding_cols;\n    const int64_t input_pad_right = padding_right - common_padding_cols;\n    bool in_bounds =\n        FastBoundsCheck(input_pad_top, std::numeric_limits<int>::max()) &&\n        FastBoundsCheck(input_pad_bottom, std::numeric_limits<int>::max()) &&\n        FastBoundsCheck(input_pad_left, std::numeric_limits<int>::max()) &&\n        FastBoundsCheck(input_pad_right, std::numeric_limits<int>::max());\n    if (!in_bounds) {\n      ctx->SetStatus(errors::InvalidArgument(\"Padding is too large.\"));\n      return;\n    }\n    functor::PadInput<GPUDevice, T, int, 4>()(\n        ctx->eigen_device<GPUDevice>(), To32Bit(input_param.tensor<T, 4>()),\n        {{static_cast<int>(input_pad_top), static_cast<int>(input_pad_left)}},\n        {{static_cast<int>(input_pad_bottom),\n          static_cast<int>(input_pad_right)}},\n        To32Bit(transformed_input.tensor<T, 4>()), data_format, T{});\n\n    input = transformed_input;\n    in_rows = new_in_rows;\n    in_cols = new_in_cols;\n  }\n\n  if (data_format == FORMAT_NHWC && compute_data_format == FORMAT_NCHW) {\n    VLOG(4) << \"Convert the input tensor from NHWC to NCHW.\";\n\n    TensorShape nchw_shape =\n        ShapeFromFormat(FORMAT_NCHW, in_batch, in_rows, in_cols, in_depths);\n    if (in_depths > 1) {\n      Tensor transformed_input;\n      OP_REQUIRES_OK(ctx, ctx->allocate_temp(DataTypeToEnum<T>::value,\n                                             nchw_shape, &transformed_input));\n      functor::NHWCToNCHW<GPUDevice, T, 4>()(\n          ctx->eigen_device<GPUDevice>(),\n          const_cast<const Tensor&>(input).tensor<T, 4>(),\n          transformed_input.tensor<T, 4>());\n      input = transformed_input;\n    } else {\n      // If depth <= 1, then just reshape.\n      CHECK(input.CopyFrom(input, nchw_shape));\n    }\n  } else {\n    CHECK(data_format == compute_data_format)  // Crash OK\n        << \"Illegal data and compute format pair:\"\n        << \" data_format=\" << ToString(data_format)\n        << \" compute_data_format=\" << ToString(compute_data_format);\n  }\n\n  CHECK(common_padding_rows >= 0 && common_padding_cols >= 0)  // Crash OK\n      << \"Negative row or col paddings: (\" << common_padding_rows << \", \"\n      << common_padding_cols << \")\";\n\n  constexpr auto kComputeInNHWC =\n      std::make_tuple(se::dnn::DataLayout::kBatchYXDepth,\n                      se::dnn::FilterLayout::kOutputYXInput);\n  constexpr auto kComputeInNCHW =\n      std::make_tuple(se::dnn::DataLayout::kBatchDepthYX,\n                      se::dnn::FilterLayout::kOutputInputYX);\n\n  se::dnn::DataLayout compute_data_layout;\n  se::dnn::FilterLayout filter_layout;\n\n  std::tie(compute_data_layout, filter_layout) =\n      compute_data_format == FORMAT_NHWC ? kComputeInNHWC : kComputeInNCHW;\n\n  se::dnn::BatchDescriptor input_desc;\n  input_desc.set_count(in_batch)\n      .set_feature_map_count(in_depths)\n      .set_height(in_rows)\n      .set_width(in_cols)\n      .set_layout(compute_data_layout);\n  se::dnn::BatchDescriptor output_desc;\n  output_desc.set_count(out_batch)\n      .set_height(out_rows)\n      .set_width(out_cols)\n      .set_feature_map_count(out_depths)\n      .set_layout(compute_data_layout);\n  se::dnn::FilterDescriptor filter_desc;\n  filter_desc.set_input_filter_height(patch_rows)\n      .set_input_filter_width(patch_cols)\n      .set_input_feature_map_count(patch_depths)\n      .set_output_feature_map_count(filter.dim_size(3))\n      .set_layout(filter_layout);\n  se::dnn::ConvolutionDescriptor conv_desc;\n  conv_desc.set_vertical_dilation_rate(row_dilation)\n      .set_horizontal_dilation_rate(col_dilation)\n      .set_vertical_filter_stride(row_stride)\n      .set_horizontal_filter_stride(col_stride)\n      .set_zero_padding_height(common_padding_rows)\n      .set_zero_padding_width(common_padding_cols)\n      .set_group_count(in_depths / patch_depths);\n\n  Tensor transformed_filter;\n\n  const auto transform_filter = [&](FilterTensorFormat dst_format) -> Status {\n    VLOG(4) << \"Transform filter tensor from \" << ToString(FORMAT_HWIO)\n            << \" to \" << ToString(dst_format);\n\n    TensorShape dst_shape =\n        dst_format == FORMAT_OIHW\n            ? TensorShape({filter.dim_size(3), filter.dim_size(2),\n                           filter.dim_size(0), filter.dim_size(1)})\n            : TensorShape({filter.dim_size(3), filter.dim_size(0),\n                           filter.dim_size(1), filter.dim_size(2)});\n\n    TF_RETURN_IF_ERROR(ctx->allocate_temp(DataTypeToEnum<T>::value, dst_shape,\n                                          &transformed_filter));\n    functor::TransformFilter<GPUDevice, T, int, 4>()(\n        ctx->eigen_device<GPUDevice>(), dst_format,\n        To32Bit(filter.tensor<T, 4>()),\n        To32Bit(transformed_filter.tensor<T, 4>()));\n\n    return OkStatus();\n  };\n\n  if (compute_data_format == FORMAT_NCHW) {\n    OP_REQUIRES_OK(ctx, transform_filter(FORMAT_OIHW));\n  } else if (compute_data_format == FORMAT_NHWC) {\n    OP_REQUIRES_OK(ctx, transform_filter(FORMAT_OHWI));\n  } else {\n    ctx->SetStatus(errors::InvalidArgument(\"Invalid compute data format: \",\n                                           ToString(compute_data_format)));\n    return;\n  }\n\n  Tensor transformed_output;\n  if (data_format != compute_data_format) {\n    VLOG(4) << \"Allocate temporary memory for output in compute data format\";\n    OP_REQUIRES_OK(\n        ctx, ctx->allocate_temp(DataTypeToEnum<T>::value,\n                                ShapeFromFormat(compute_data_format, out_batch,\n                                                out_rows, out_cols, out_depths),\n                                &transformed_output));\n  } else {\n    transformed_output = *output;\n  }\n\n  auto input_ptr = AsDeviceMemory(input.template flat<T>().data(),\n                                  input.template flat<T>().size());\n  auto filter_ptr =\n      AsDeviceMemory(transformed_filter.template flat<T>().data(),\n                     transformed_filter.template flat<T>().size());\n  auto output_ptr =\n      AsDeviceMemory(transformed_output.template flat<T>().data(),\n                     transformed_output.template flat<T>().size());\n\n  static int64_t ConvolveScratchSize = GetDnnWorkspaceLimitOrDefault();\n\n  int device_id = stream->parent()->device_ordinal();\n  DataType dtype = input.dtype();\n  ConvParameters conv_parameters = {in_batch,             // batch\n                                    in_depths,            // in_depths\n                                    {{in_rows,            // in_rows\n                                      in_cols}},          // in_cols\n                                    compute_data_format,  // compute_data_format\n                                    out_depths,           // out_depths\n                                    {{patch_rows,         // filter_rows\n                                      patch_cols,         // filter_cols\n                                      patch_depths}},     // filter_depths\n                                    {{row_dilation,       // dilation_rows\n                                      col_dilation}},     // dilation_cols\n                                    {{row_stride,         // stride_rows\n                                      col_stride}},       // stride_cols\n                                    {{common_padding_rows,    // padding_rows\n                                      common_padding_cols}},  // padding_cols\n                                    dtype,                    // tensor datatype\n                                    device_id,                // device_id\n                                    conv_desc.group_count()};\n\n  auto entry_or = AutotuneUnfusedConv(\n      cudnn_use_autotune, ConvAutotuneMap::GetInstance(), conv_parameters, ctx,\n      se::dnn::ConvolutionKind::FORWARD, input_desc, input_ptr, filter_desc,\n      filter_ptr, conv_desc, output_desc, output_ptr, ConvolveScratchSize);\n  OP_REQUIRES_OK(ctx, entry_or.status());\n  auto autotune_entry = std::move(entry_or).value();\n\n  DnnScratchAllocator scratch_allocator(ConvolveScratchSize, ctx);\n  Status cudnn_launch_status = LaunchAutotunedConv(\n      autotune_entry, &scratch_allocator, se::dnn::ConvolutionKind::FORWARD,\n      stream, input_desc, input_ptr, filter_desc, filter_ptr, conv_desc,\n      output_desc, output_ptr);\n  if (!cudnn_launch_status.ok()) {\n    ctx->SetStatus(cudnn_launch_status);\n    return;\n  }\n\n  if (data_format == FORMAT_NHWC && compute_data_format == FORMAT_NCHW) {\n    VLOG(4) << \"Convert the output tensor back from NCHW to NHWC.\";\n    functor::NCHWToNHWC<GPUDevice, T, 4>()(\n        ctx->eigen_device<GPUDevice>(),\n        const_cast<const Tensor&>(transformed_output).tensor<T, 4>(),\n        output->tensor<T, 4>());\n  }\n}\n\n// Forward declarations of the functor specializations for GPU.\nnamespace functor {\n#define DECLARE_GPU_SPEC(T)                                                 \\\n  template <>                                                               \\\n  void SpatialConvolution<GPUDevice, T>::operator()(                        \\\n      const GPUDevice& d, typename TTypes<T, 4>::Tensor output,             \\\n      typename TTypes<T, 4>::ConstTensor input,                             \\\n      typename TTypes<T, 4>::ConstTensor filter, int row_stride,            \\\n      int col_stride, int row_dilation, int col_dilation,                   \\\n      const Eigen::PaddingType& padding,                                    \\\n      const Eigen::NoOpOutputKernel& output_kernel);                        \\\n  template <>                                                               \\\n  void SpatialConvolution<GPUDevice, T>::operator()(                        \\\n      const GPUDevice& d, typename TTypes<T, 4>::Tensor output,             \\\n      typename TTypes<T, 4>::ConstTensor input,                             \\\n      typename TTypes<T, 4>::ConstTensor filter, int row_stride,            \\\n      int col_stride, int row_dilation, int col_dilation, int padding_top,  \\\n      int padding_bottom, int padding_left, int padding_right,              \\\n      const Eigen::NoOpOutputKernel& output_kernel);                        \\\n  extern template struct SpatialConvolution<GPUDevice, T>;                  \\\n  template <>                                                               \\\n  void MatMulConvFunctor<GPUDevice, T>::operator()(                         \\\n      const GPUDevice& d, typename TTypes<T, 2>::Tensor out,                \\\n      typename TTypes<T, 2>::ConstTensor in0,                               \\\n      typename TTypes<T, 2>::ConstTensor in1,                               \\\n      const Eigen::array<Eigen::IndexPair<Eigen::DenseIndex>, 1>& dim_pair, \\\n      const Eigen::NoOpOutputKernel& output_kernel);                        \\\n  extern template struct MatMulConvFunctor<GPUDevice, T>;                   \\\n  template <>                                                               \\\n  void TransformFilter<GPUDevice, T, int, 4>::operator()(                   \\\n      const GPUDevice& d, FilterTensorFormat dst_filter_format,             \\\n      typename TTypes<T, 4, int>::ConstTensor in,                           \\\n      typename TTypes<T, 4, int>::Tensor out);                              \\\n  extern template struct TransformFilter<GPUDevice, T, int, 4>;             \\\n  template <>                                                               \\\n  void PadInput<GPUDevice, T, int, 4>::operator()(                          \\\n      const GPUDevice& d, typename TTypes<T, 4, int>::ConstTensor in,       \\\n      const std::array<int, 2>& padding_left,                               \\\n      const std::array<int, 2>& padding_right,                              \\\n      typename TTypes<T, 4, int>::Tensor out, TensorFormat data_format,     \\\n      const T& padding_value);                                              \\\n  extern template struct PadInput<GPUDevice, T, int, 4>\n\nDECLARE_GPU_SPEC(float);\nDECLARE_GPU_SPEC(Eigen::half);\nDECLARE_GPU_SPEC(double);\nDECLARE_GPU_SPEC(int32);\n#undef DECLARE_GPU_SPEC\n\n}  // namespace functor\n\n// Registration of the GPU implementations.\nREGISTER_KERNEL_BUILDER(\n    Name(\"Conv2D\").Device(DEVICE_GPU).TypeConstraint<Eigen::half>(\"T\"),\n    Conv2DOp<GPUDevice, Eigen::half>);\nREGISTER_KERNEL_BUILDER(\n    Name(\"Conv2D\").Device(DEVICE_GPU).TypeConstraint<float>(\"T\"),\n    Conv2DOp<GPUDevice, float>);\nREGISTER_KERNEL_BUILDER(\n    Name(\"Conv2D\").Device(DEVICE_GPU).TypeConstraint<double>(\"T\"),\n    Conv2DOp<GPUDevice, double>);\nREGISTER_KERNEL_BUILDER(\n    Name(\"Conv2D\").Device(DEVICE_GPU).TypeConstraint<int32>(\"T\"),\n    Conv2DOp<GPUDevice, int32>);\n\n// To be used inside depthwise_conv_op.cc.\ntemplate struct LaunchConv2DOp<GPUDevice, float>;\ntemplate struct LaunchConv2DOp<GPUDevice, Eigen::half>;\ntemplate struct LaunchConv2DOp<GPUDevice, double>;\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n}  // namespace tensorflow\n", "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Functional tests for convolutional operations.\"\"\"\n\nimport os\nimport time\n\nimport numpy as np\n\nfrom tensorflow.core.protobuf import config_pb2\nfrom tensorflow.core.protobuf import rewriter_config_pb2\nfrom tensorflow.python.client import session as session_lib\nfrom tensorflow.python.eager import backprop\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors_impl\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.layers import convolutional\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import gradient_checker\nfrom tensorflow.python.ops import gradients_impl\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import nn_impl\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.ops import random_ops\nfrom tensorflow.python.ops import variables\nimport tensorflow.python.ops.nn_grad  # pylint: disable=unused-import\nfrom tensorflow.python.platform import test\nfrom tensorflow.python.platform import tf_logging\nfrom tensorflow.python.util.compat import collections_abc\n\n\ndef GetShrunkInceptionShapes(shrink=10):\n  \"\"\"Iterator for smaller versions of convolution shapes in 2015 Inception.\n\n  Relative to inception, each depth value is `depth // shrink`.\n\n  Args:\n    shrink: Factor to shrink each depth value by relative to Inception.\n\n  Yields:\n    Tuple (input_size, filter_size, out_size, stride, padding), the convolution\n    parameters of Inception layers.\n  \"\"\"\n  input_sizes = [[4, 5, 5, 1248], [4, 8, 8, 384], [4, 8, 8, 384],\n                 [4, 8, 8, 2048], [4, 8, 8, 448], [4, 8, 8, 2048],\n                 [4, 8, 8, 2048], [4, 8, 8, 2048], [4, 8, 8, 1760],\n                 [4, 8, 8, 1760], [4, 8, 8, 1760], [4, 8, 8, 1760],\n                 [4, 17, 17, 192], [4, 17, 17, 192], [4, 17, 17, 1248],\n                 [4, 17, 17, 128], [4, 17, 17, 1248], [4, 17, 17, 224],\n                 [4, 17, 17, 192], [4, 17, 17, 192], [4, 17, 17, 1216],\n                 [4, 17, 17, 1216], [4, 17, 17, 224], [4, 17, 17, 192],\n                 [4, 17, 17, 192], [4, 17, 17, 1152], [4, 17, 17, 1152],\n                 [4, 17, 17, 192], [4, 17, 17, 160], [4, 17, 17, 1152],\n                 [4, 17, 17, 1024], [4, 17, 17, 128], [4, 17, 17, 1024],\n                 [4, 17, 17, 128], [4, 17, 17, 1024], [4, 17, 17, 128],\n                 [4, 17, 17, 768], [4, 17, 17, 128], [4, 17, 17, 128],\n                 [4, 17, 17, 768], [4, 17, 17, 768], [4, 35, 35, 96],\n                 [4, 35, 35, 288], [4, 35, 35, 64], [4, 35, 35, 288],\n                 [4, 35, 35, 256], [4, 35, 35, 48], [4, 35, 35, 256],\n                 [4, 35, 35, 96], [4, 35, 35, 192], [4, 35, 35, 192],\n                 [4, 35, 35, 192], [4, 73, 73, 64], [4, 73, 73, 64],\n                 [4, 147, 147, 24]]\n  filter_sizes = [[1, 1, 1248, 128], [1, 3, 384, 384], [3, 1, 384, 384],\n                  [1, 1, 2048, 192], [3, 3, 448, 384], [1, 1, 2048, 320],\n                  [1, 1, 2048, 448], [1, 1, 2048, 384], [1, 1, 1760, 384],\n                  [1, 1, 1760, 192], [1, 1, 1760, 448], [1, 1, 1760, 320],\n                  [3, 3, 192, 192], [3, 3, 192, 192], [1, 1, 1248, 192],\n                  [3, 3, 128, 320], [1, 1, 1248, 128], [1, 3, 224, 224],\n                  [3, 1, 192, 256], [1, 3, 192, 256], [1, 1, 1216, 192],\n                  [1, 1, 1216, 96], [3, 1, 224, 224], [3, 3, 192, 224],\n                  [1, 3, 192, 192], [1, 1, 1152, 192], [1, 1, 1152, 128],\n                  [3, 1, 192, 192], [3, 3, 160, 192], [1, 1, 1152, 160],\n                  [1, 1, 1024, 128], [1, 3, 128, 192], [1, 1, 1024, 160],\n                  [3, 1, 128, 192], [1, 1, 1024, 256], [3, 1, 128, 128],\n                  [1, 1, 768, 192], [1, 3, 128, 128], [3, 3, 128, 128],\n                  [1, 1, 768, 128], [1, 1, 768, 320], [3, 3, 96, 96],\n                  [3, 3, 288, 384], [3, 3, 64, 96], [1, 1, 288, 64],\n                  [1, 1, 256, 64], [5, 5, 48, 64], [1, 1, 256, 48],\n                  [3, 3, 96, 96], [1, 1, 192, 32], [1, 1, 192, 64],\n                  [1, 1, 192, 48], [3, 3, 64, 192], [1, 1, 64, 64],\n                  [1, 1, 24, 64]]\n  out_sizes = [[4, 5, 5, 128], [4, 8, 8, 384], [4, 8, 8, 384],\n               [4, 8, 8, 192], [4, 8, 8, 384], [4, 8, 8, 320],\n               [4, 8, 8, 448], [4, 8, 8, 384], [4, 8, 8, 384],\n               [4, 8, 8, 192], [4, 8, 8, 448], [4, 8, 8, 320],\n               [4, 8, 8, 192], [4, 17, 17, 192], [4, 17, 17, 192],\n               [4, 8, 8, 320], [4, 17, 17, 128], [4, 17, 17, 224],\n               [4, 17, 17, 256], [4, 17, 17, 256], [4, 17, 17, 192],\n               [4, 17, 17, 96], [4, 17, 17, 224], [4, 17, 17, 224],\n               [4, 17, 17, 192], [4, 17, 17, 192], [4, 17, 17, 128],\n               [4, 17, 17, 192], [4, 17, 17, 192], [4, 17, 17, 160],\n               [4, 17, 17, 128], [4, 17, 17, 192], [4, 17, 17, 160],\n               [4, 17, 17, 192], [4, 17, 17, 256], [4, 17, 17, 128],\n               [4, 17, 17, 192], [4, 17, 17, 128], [4, 17, 17, 128],\n               [4, 17, 17, 128], [4, 17, 17, 320], [4, 17, 17, 96],\n               [4, 17, 17, 384], [4, 35, 35, 96], [4, 35, 35, 64],\n               [4, 35, 35, 64], [4, 35, 35, 64], [4, 35, 35, 48],\n               [4, 35, 35, 96], [4, 35, 35, 32], [4, 35, 35, 64],\n               [4, 35, 35, 48], [4, 71, 71, 192], [4, 73, 73, 64],\n               [4, 147, 147, 64]]\n  strides = [\n      1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n      1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1,\n      1, 1, 1, 1, 1\n  ]\n  # Shrink sizes to make the test faster\n  for i in input_sizes:\n    i[3] //= shrink\n  for f in filter_sizes:\n    f[2] //= shrink\n    f[3] //= shrink\n  for o in out_sizes:\n    o[3] //= shrink\n  # pylint: disable=invalid-name\n  VALID = \"VALID\"\n  SAME = \"SAME\"\n  # pylint: enable=invalid-name\n  paddings = [\n      SAME, SAME, SAME, SAME, SAME, SAME, SAME, SAME, SAME, SAME, SAME, SAME,\n      VALID, SAME, SAME, VALID, SAME, SAME, SAME, SAME, SAME, SAME, SAME, SAME,\n      SAME, SAME, SAME, SAME, SAME, SAME, SAME, SAME, SAME, SAME, SAME, SAME,\n      SAME, SAME, SAME, SAME, SAME, VALID, VALID, SAME, SAME, SAME, SAME, SAME,\n      SAME, SAME, SAME, SAME, VALID, VALID, VALID\n  ]\n  for i, f, o, s, p in zip(input_sizes, filter_sizes, out_sizes, strides,\n                           paddings):\n    yield i, f, o, s, p\n\n\ndef GetTestConfigs():\n  \"\"\"Get all the valid tests configs to run.\n\n  Returns:\n    all the valid test configs as tuples of data_format and use_gpu.\n  \"\"\"\n  test_configs = [(\"NHWC\", False), (\"NHWC\", True)]\n  if test.is_gpu_available(cuda_only=True):\n    # \"NCHW\" format is only supported on CUDA.\n    test_configs += [(\"NCHW\", True)]\n  return test_configs\n\n\n@test_util.run_all_without_tensor_float_32(\"Avoid TF32 conv on GPU\")\nclass Conv2DTest(test.TestCase):\n\n  def _DtypesToTest(self, use_gpu):\n    if test_util.IsMklEnabled():\n      return [dtypes.float32]\n    # double datatype is currently not supported for convolution ops\n    # on the ROCm platform\n    optional_float64 = [] if test.is_built_with_rocm() else [dtypes.float64]\n    if use_gpu and not test_util.GpuSupportsHalfMatMulAndConv():\n      return [dtypes.float32] + optional_float64\n    else:\n      # It is important that float32 comes before float16 here,\n      # as we will be using its gradients as reference for fp16 gradients.\n      return [dtypes.float32, dtypes.float16] + optional_float64\n\n  def _CreateNumpyTensor(self, shape):\n    total_size = 1\n    for s in shape:\n      total_size *= s\n    return np.arange(1, total_size + 1, dtype=np.float32).reshape(shape)\n\n  def _SetupValuesForDevice(self, tensor_in_sizes, filter_in_sizes, dilations,\n                            strides, padding, data_format, dtype, use_gpu):\n    \"\"\"Verifies the output values of the convolution function.\n\n    Args:\n      tensor_in_sizes: Input tensor dimensions in\n        [batch, input_rows, input_cols, input_depth].\n      filter_in_sizes: Filter tensor dimensions in\n        [kernel_rows, kernel_cols, input_depth, output_depth].\n      dilations: Dilated rate: [col_dilation, row_dilation]\n      strides: Stride: [col_stride, row_stride]\n      padding: Padding type.\n      data_format: Format of the data tensors.\n      dtype: Data type for inputs and outputs.\n      use_gpu: True if the operations should be run on GPU\n    Returns:\n      Symbolic tensor value that can be used to execute the computation\n    \"\"\"\n    x1 = self._CreateNumpyTensor(tensor_in_sizes)\n    x2 = self._CreateNumpyTensor(filter_in_sizes)\n\n    with test_util.device(use_gpu):\n      t1 = constant_op.constant(x1, shape=tensor_in_sizes, dtype=dtype)\n      t2 = constant_op.constant(x2, shape=filter_in_sizes, dtype=dtype)\n      strides = [1] + strides + [1]\n      dilations = [1] + dilations + [1]\n      if isinstance(padding, (list, tuple)):\n        padding = [(0, 0)] + padding + [(0, 0)]\n      if data_format == \"NCHW\":\n        t1 = test_util.NHWCToNCHW(t1)\n        strides = test_util.NHWCToNCHW(strides)\n        dilations = test_util.NHWCToNCHW(dilations)\n        if isinstance(padding, (list, tuple)):\n          padding = test_util.NHWCToNCHW(padding)\n      conv = nn_ops.conv2d(\n          t1,\n          t2,\n          dilations=dilations,\n          strides=strides,\n          padding=padding,\n          data_format=data_format)\n      self.assertEqual(conv.dtype, dtype)\n      if data_format == \"NCHW\":\n        conv = test_util.NCHWToNHWC(conv)\n\n      return conv\n\n  def _CompareFwdValues(self, tensor_in_sizes, filter_in_sizes, conv_strides,\n                        padding):\n    \"\"\"Verifies that CPU and GPU produce the same values.\n\n    Args:\n      tensor_in_sizes: Input tensor dimensions in\n        [batch, input_rows, input_cols, input_depth].\n      filter_in_sizes: Filter tensor dimensions in\n        [kernel_rows, kernel_cols, input_depth, output_depth].\n      conv_strides: [row_stride, col_stride] for the convolution;\n      padding: Padding type.\n    \"\"\"\n    x1 = np.random.rand(*tensor_in_sizes).astype(np.float32)\n    x2 = np.random.rand(*filter_in_sizes).astype(np.float32)\n\n    def _SetupVal(data_format, use_gpu):\n      with test_util.device(use_gpu):\n        t1 = constant_op.constant(x1, shape=tensor_in_sizes)\n        t2 = constant_op.constant(x2, shape=filter_in_sizes)\n        strides = [1] + conv_strides + [1]\n        if data_format == \"NCHW\":\n          t1 = test_util.NHWCToNCHW(t1)\n          strides = test_util.NHWCToNCHW(strides)\n        conv = nn_ops.conv2d(\n            t1, t2, strides=strides, padding=padding, data_format=data_format)\n        if data_format == \"NCHW\":\n          conv = test_util.NCHWToNHWC(conv)\n        return conv\n\n    tensors = []\n    for (data_format, use_gpu) in GetTestConfigs():\n      tensors.append(_SetupVal(data_format, use_gpu))\n    values = self.evaluate(tensors)\n    for i in range(1, len(values)):\n      self.assertAllClose(values[0], values[i], rtol=1e-3, atol=1e-3)\n\n  def _ComputeReferenceDilatedConv(self, tensor_in_sizes, filter_in_sizes,\n                                   stride, dilation, padding, data_format,\n                                   use_gpu):\n    x1 = self._CreateNumpyTensor(tensor_in_sizes)\n    x2 = self._CreateNumpyTensor(filter_in_sizes)\n    with test_util.device(use_gpu):\n      t1 = constant_op.constant(x1, shape=tensor_in_sizes)\n      t2 = constant_op.constant(x2, shape=filter_in_sizes)\n      if isinstance(stride, collections_abc.Iterable):\n        strides = list(stride)\n      else:\n        strides = [stride, stride]\n      if data_format == \"NCHW\":\n        t1 = test_util.NHWCToNCHW(t1)\n        full_strides = [1, 1] + strides\n        full_dilation = [1, 1] + dilation\n      else:\n        full_strides = [1] + strides + [1]\n        full_dilation = [1] + dilation + [1]\n      expected = nn_ops.convolution(\n          t1,\n          t2,\n          padding=padding,\n          strides=strides,\n          dilation_rate=dilation,\n          data_format=data_format)\n      computed = nn_ops.conv2d(\n          t1,\n          t2,\n          strides=full_strides,\n          dilations=full_dilation,\n          padding=padding,\n          data_format=data_format)\n      if data_format == \"NCHW\":\n        expected = test_util.NCHWToNHWC(expected)\n        computed = test_util.NCHWToNHWC(computed)\n    return expected, computed\n\n  def _VerifyDilatedConvValues(self, tensor_in_sizes, filter_in_sizes, strides,\n                               padding, dilations, rtol=1e-4):\n    expected_results = []\n    computed_results = []\n    for data_format, use_gpu in GetTestConfigs():\n      expected, computed = self._ComputeReferenceDilatedConv(\n          tensor_in_sizes, filter_in_sizes, strides, dilations, padding,\n          data_format, use_gpu)\n      expected_results.append(expected)\n      computed_results.append(computed)\n    tolerance = 1e-2 if use_gpu else 1e-5\n    expected_values = self.evaluate(expected_results)\n    computed_values = self.evaluate(computed_results)\n    for e_value, c_value in zip(expected_values, computed_values):\n      tf_logging.debug(\"expected = %s\", e_value)\n      tf_logging.debug(\"actual = %s\", c_value)\n      self.assertAllClose(\n          e_value.flatten(), c_value.flatten(), atol=tolerance, rtol=rtol)\n\n  def _VerifyValues(self,\n                    tensor_in_sizes,\n                    filter_in_sizes,\n                    strides,\n                    padding,\n                    expected,\n                    dilations=(1, 1),\n                    gpu_only=False,\n                    test_grappler_layout_optimizer=False,\n                    tol=1e-5,\n                    fp16_tol=1e-3):\n    if gpu_only and not test.is_gpu_available(cuda_only=True):\n      return\n    tensors = []\n    dilations = list(dilations)\n    for (data_format, use_gpu) in GetTestConfigs():\n      if gpu_only and not use_gpu:\n        continue\n      dtypes_to_test = self._DtypesToTest(use_gpu)\n      if not test_grappler_layout_optimizer and data_format == \"NHWC\":\n        dtypes_to_test.append(dtypes.int32)\n      for dtype in dtypes_to_test:\n        result = self._SetupValuesForDevice(\n            tensor_in_sizes,\n            filter_in_sizes,\n            dilations,\n            strides,\n            padding,\n            data_format,\n            dtype,\n            use_gpu=use_gpu)\n        if test_grappler_layout_optimizer and data_format == \"NHWC\" and use_gpu:\n          # Grappler's layout optimizer will not optimize a fetch node, so\n          # this identity allows Grappler to optimize the Conv2D node.\n          result = array_ops.identity(result)\n        tensors.append(result)\n      values = self.evaluate(tensors)\n      for i in range(len(tensors)):\n        conv = tensors[i]\n        value = values[i]\n        tf_logging.debug(\"expected = %s\", expected)\n        tf_logging.debug(\"actual = %s\", value)\n        tol_to_use = fp16_tol if value.dtype == np.float16 else tol\n        if np.issubdtype(value.dtype, np.integer):\n          self.assertAllEqual(np.rint(expected), np.ravel(value))\n        else:\n          self.assertAllClose(expected, np.ravel(value), atol=tol_to_use,\n                              rtol=tol_to_use)\n        self.assertShapeEqual(value, conv)\n        self.assertEqual(value.dtype, conv.dtype.as_numpy_dtype)\n\n  def _VerifyExplicitPaddings(self,\n                              tensor_in_sizes,\n                              filter_in_sizes,\n                              strides,\n                              padding,\n                              dilations=(1, 1),\n                              test_grappler_layout_optimizer=False,\n                              tol=1e-5,\n                              fp16_tol=1e-3):\n    \"\"\"Verifies Conv2D with explicit padding generates correct values.\n\n    It does this by comparing with Conv2D without explicit padding. This\n    function assumes Conv2D without explicit padding works correctly.\n\n    Args:\n      tensor_in_sizes: Input tensor dimensions in [batch, input_rows,\n        input_cols, input_depth].\n      filter_in_sizes: Filter tensor dimensions in [kernel_rows, kernel_cols,\n        input_depth, output_depth].\n      strides: [row_stride, col_stride] for the convolution;\n      padding: Explicit padding amounts.\n      dilations: Dilation values\n      test_grappler_layout_optimizer: If True, allow the Grappler layout\n        optimizer to run, which turns NHWC Conv2Ds on the GPU to NCHW Conv2Ds.\n      tol: The absolute and relative tolerance for non-fp16 dtypes.\n      fp16_tol: The absolute and relative tolerance for fp16.\n    \"\"\"\n    input_tensor = self._CreateNumpyTensor(tensor_in_sizes)\n    filter_tensor = self._CreateNumpyTensor(filter_in_sizes)\n    input_tensor = array_ops.pad(input_tensor, [(0, 0)] + padding + [(0, 0)])\n    dilations = list(dilations)\n    conv2d_result = nn_ops.conv2d(\n        input_tensor,\n        filter_tensor, [1] + list(strides) + [1],\n        \"VALID\",\n        dilations=[1] + dilations + [1])\n    expected = list(self.evaluate(array_ops.reshape(conv2d_result, [-1])))\n    self._VerifyValues(\n        tensor_in_sizes,\n        filter_in_sizes,\n        strides,\n        padding,\n        expected,\n        dilations,\n        test_grappler_layout_optimizer=test_grappler_layout_optimizer,\n        tol=tol,\n        fp16_tol=fp16_tol)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D1x1Filter(self):\n    expected_output = [\n        30.0, 36.0, 42.0, 66.0, 81.0, 96.0, 102.0, 126.0, 150.0, 138.0, 171.0,\n        204.0, 174.0, 216.0, 258.0, 210.0, 261.0, 312.0\n    ]\n    self._VerifyValues(\n        tensor_in_sizes=[1, 2, 3, 3],\n        filter_in_sizes=[1, 1, 3, 3],\n        strides=[1, 1],\n        padding=\"VALID\",\n        expected=expected_output)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DExpandedBatch(self):\n    tensor_in_sizes_batch = [10, 2, 3, 3]\n    tensor_in_sizes_expanded_batch = [2, 5, 2, 3, 3]\n    filter_in_sizes = [1, 1, 3, 3]\n    filter_in = self._CreateNumpyTensor(filter_in_sizes)\n    x1 = self._CreateNumpyTensor(tensor_in_sizes_batch)\n    x2 = x1.reshape(tensor_in_sizes_expanded_batch)\n    conv1 = nn_ops.conv2d(\n        x1,\n        filter_in,\n        strides=[1, 1],\n        padding=\"VALID\")\n    conv2 = nn_ops.conv2d(\n        x2,\n        filter_in,\n        strides=[1, 1],\n        padding=\"VALID\")\n    self.assertEqual(conv1.shape, tensor_in_sizes_batch)\n    self.assertEqual(conv2.shape, tensor_in_sizes_expanded_batch)\n    self.assertAllEqual(\n        conv1,\n        self.evaluate(conv2).reshape(conv1.shape))\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConvolutionClass2DExpandedBatch(self):\n    tensor_in_sizes_batch = [10, 2, 3, 3]\n    tensor_in_sizes_expanded_batch = [2, 5, 2, 3, 3]\n    filter_in_sizes = [1, 1, 3, 3]\n    filter_in = self._CreateNumpyTensor(filter_in_sizes)\n    x1 = self._CreateNumpyTensor(tensor_in_sizes_batch)\n    x2 = x1.reshape(tensor_in_sizes_expanded_batch)\n    convolver1 = nn_ops.Convolution(\n        input_shape=x1.shape,\n        filter_shape=filter_in.shape,\n        strides=[1, 1],\n        padding=\"VALID\")\n    self.assertEqual(convolver1.num_batch_dims, 1)\n    convolver2 = nn_ops.Convolution(\n        input_shape=x2.shape,\n        filter_shape=filter_in.shape,\n        strides=[1, 1],\n        padding=\"VALID\")\n    self.assertEqual(convolver2.num_batch_dims, 2)\n    conv1 = convolver1(x1, filter_in)\n    conv2 = convolver2(x2, filter_in)\n    self.assertEqual(conv1.shape, tensor_in_sizes_batch)\n    self.assertEqual(conv2.shape, tensor_in_sizes_expanded_batch)\n    self.assertAllEqual(\n        conv1,\n        self.evaluate(conv2).reshape(conv1.shape))\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConvolutionWith2SpatialDimensionsAndExpandedBatch(self):\n    tensor_in_sizes_batch = [10, 2, 3, 3]\n    tensor_in_sizes_expanded_batch = [2, 5, 2, 3, 3]\n    filter_in_sizes = [1, 1, 3, 3]\n    filter_in = self._CreateNumpyTensor(filter_in_sizes)\n    x1 = self._CreateNumpyTensor(tensor_in_sizes_batch)\n    x2 = x1.reshape(tensor_in_sizes_expanded_batch)\n    conv1 = nn_ops.convolution(\n        x1,\n        filter_in,\n        strides=[1, 1],\n        padding=\"VALID\")\n    conv2 = nn_ops.convolution(\n        x2,\n        filter_in,\n        strides=[1, 1],\n        padding=\"VALID\")\n    self.assertEqual(conv1.shape, tensor_in_sizes_batch)\n    self.assertEqual(conv2.shape, tensor_in_sizes_expanded_batch)\n    self.assertAllEqual(\n        conv1,\n        self.evaluate(conv2).reshape(conv1.shape))\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D2x2Filter2x1Dilation(self):\n    self._VerifyDilatedConvValues(\n        tensor_in_sizes=[1, 4, 4, 1],\n        filter_in_sizes=[2, 2, 1, 1],\n        strides=[1, 1],\n        dilations=[2, 1],\n        padding=\"VALID\")\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DEmpty(self):\n    expected_output = []\n    self._VerifyValues(\n        tensor_in_sizes=[0, 2, 3, 3],\n        filter_in_sizes=[1, 1, 3, 3],\n        strides=[1, 1],\n        padding=\"VALID\",\n        expected=expected_output)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DEmptyDilation(self):\n    self._VerifyDilatedConvValues(\n        tensor_in_sizes=[0, 2, 3, 3],\n        filter_in_sizes=[1, 1, 3, 3],\n        strides=[1, 1],\n        dilations=[2, 1],\n        padding=\"VALID\")\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D2x2Filter(self):\n    # The outputs are computed using third_party/py/IPython/notebook.\n    expected_output = [2271.0, 2367.0, 2463.0, 2901.0, 3033.0, 3165.0]\n    self._VerifyValues(\n        tensor_in_sizes=[1, 2, 3, 3],\n        filter_in_sizes=[2, 2, 3, 3],\n        strides=[1, 1],\n        padding=\"VALID\",\n        expected=expected_output)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D2x2FilterDilation(self):\n    self._VerifyDilatedConvValues(\n        tensor_in_sizes=[1, 2, 3, 3],\n        filter_in_sizes=[2, 2, 3, 3],\n        strides=[1, 1],\n        dilations=[1, 2],\n        padding=\"VALID\")\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D1x2Filter(self):\n    # The outputs are computed using third_party/py/IPython/notebook.\n    expected_output = [\n        231.0, 252.0, 273.0, 384.0, 423.0, 462.0, 690.0, 765.0, 840.0, 843.0,\n        936.0, 1029.0\n    ]\n    self._VerifyValues(\n        tensor_in_sizes=[1, 2, 3, 3],\n        filter_in_sizes=[1, 2, 3, 3],\n        strides=[1, 1],\n        padding=\"VALID\",\n        expected=expected_output)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D1x2FilterDilation(self):\n    self._VerifyDilatedConvValues(\n        tensor_in_sizes=[1, 2, 3, 3],\n        filter_in_sizes=[1, 2, 3, 3],\n        strides=[1, 1],\n        dilations=[2, 1],\n        padding=\"VALID\")\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D2x2FilterStride2(self):\n    expected_output = [2271.0, 2367.0, 2463.0]\n    self._VerifyValues(\n        tensor_in_sizes=[1, 2, 3, 3],\n        filter_in_sizes=[2, 2, 3, 3],\n        strides=[2, 2],\n        padding=\"VALID\",\n        expected=expected_output)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D2x2FilterStride2Same(self):\n    expected_output = [2271.0, 2367.0, 2463.0, 1230.0, 1305.0, 1380.0]\n    self._VerifyValues(\n        tensor_in_sizes=[1, 2, 3, 3],\n        filter_in_sizes=[2, 2, 3, 3],\n        strides=[2, 2],\n        padding=\"SAME\",\n        expected=expected_output)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D2x2FilterStride1x2(self):\n    expected_output = [58.0, 78.0, 98.0, 118.0, 138.0, 158.0]\n    self._VerifyValues(\n        tensor_in_sizes=[1, 3, 6, 1],\n        filter_in_sizes=[2, 2, 1, 1],\n        strides=[1, 2],\n        padding=\"VALID\",\n        expected=expected_output)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DKernelSmallerThanStrideValid(self):\n    expected_output = [65, 95, 275, 305]\n    self._VerifyValues(\n        tensor_in_sizes=[1, 7, 7, 1],\n        filter_in_sizes=[2, 2, 1, 1],\n        strides=[3, 3],\n        padding=\"VALID\",\n        expected=expected_output)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DKernelSmallerThanStrideSame(self):\n    self._VerifyValues(\n        tensor_in_sizes=[1, 3, 3, 1],\n        filter_in_sizes=[1, 1, 1, 1],\n        strides=[2, 2],\n        padding=\"SAME\",\n        expected=[1, 3, 7, 9])\n\n    self._VerifyValues(\n        tensor_in_sizes=[1, 4, 4, 1],\n        filter_in_sizes=[1, 1, 1, 1],\n        strides=[2, 2],\n        padding=\"SAME\",\n        expected=[1, 3, 9, 11])\n\n    self._VerifyValues(\n        tensor_in_sizes=[1, 4, 4, 1],\n        filter_in_sizes=[2, 2, 1, 1],\n        strides=[3, 3],\n        padding=\"SAME\",\n        expected=[44, 28, 41, 16])\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DKernelSizeMatchesInputSize(self):\n    self._VerifyValues(\n        tensor_in_sizes=[1, 2, 2, 1],\n        filter_in_sizes=[2, 2, 1, 2],\n        strides=[1, 1],\n        padding=\"VALID\",\n        expected=[50, 60])\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DKernelSizeMatchesInputSizeDilation(self):\n    self._VerifyDilatedConvValues(\n        tensor_in_sizes=[1, 3, 3, 1],\n        filter_in_sizes=[2, 2, 1, 2],\n        strides=[1, 1],\n        dilations=[2, 2],\n        padding=\"VALID\")\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2D0x0Padding(self):\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[1, 2, 3, 3],\n        filter_in_sizes=[2, 2, 3, 3],\n        strides=[1, 1],\n        padding=[[0, 0], [0, 0]])\n\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[3, 4, 3, 2],\n        filter_in_sizes=[1, 1, 2, 1],\n        strides=[2, 2],\n        padding=[[0, 0], [0, 0]])\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2D1x1Padding(self):\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[1, 2, 3, 2],\n        filter_in_sizes=[2, 2, 2, 2],\n        strides=[1, 1],\n        padding=[[1, 1], [1, 1]])\n\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[1, 2, 2, 1],\n        filter_in_sizes=[1, 1, 1, 2],\n        strides=[1, 1],\n        padding=[[1, 1], [1, 1]])\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2D2x2Padding(self):\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[1, 2, 1, 2],\n        filter_in_sizes=[2, 1, 2, 1],\n        strides=[1, 1],\n        padding=[[2, 2], [2, 2]])\n\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[1, 2, 1, 2],\n        filter_in_sizes=[1, 1, 2, 1],\n        strides=[2, 1],\n        padding=[[2, 2], [2, 2]])\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2DOnlyBottomPadding(self):\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[1, 2, 3, 3],\n        filter_in_sizes=[2, 2, 3, 2],\n        strides=[1, 1],\n        padding=[[0, 3], [0, 0]], tol=2e-5)\n\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[2, 2, 4, 3],\n        filter_in_sizes=[1, 2, 3, 2],\n        strides=[2, 2],\n        padding=[[0, 3], [0, 0]])\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2DOnlyTopRightPadding(self):\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[1, 2, 3, 3],\n        filter_in_sizes=[2, 2, 3, 2],\n        strides=[1, 1],\n        padding=[[1, 0], [0, 2]],\n        tol=5e-5)\n\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[1, 2, 4, 2],\n        filter_in_sizes=[2, 2, 2, 2],\n        strides=[1, 3],\n        padding=[[1, 0], [0, 2]])\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2DLotsPadding(self):\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[1, 1, 1, 3],\n        filter_in_sizes=[2, 2, 3, 3],\n        strides=[1, 1],\n        padding=[[3, 4], [4, 2]])\n\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[1, 2, 1, 1],\n        filter_in_sizes=[2, 2, 1, 3],\n        strides=[2, 1],\n        padding=[[3, 4], [4, 2]])\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2DExplicitPaddingWithDilations(self):\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[1, 3, 2, 1],\n        filter_in_sizes=[1, 2, 1, 2],\n        strides=[1, 1],\n        padding=[[1, 0], [0, 1]],\n        dilations=[2, 1])\n\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[1, 2, 3, 2],\n        filter_in_sizes=[3, 2, 2, 1],\n        strides=[1, 1],\n        padding=[[2, 1], [1, 2]],\n        dilations=[2, 3])\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2dOnlyPaddingReturnsZeros(self):\n    self._VerifyValues(\n        tensor_in_sizes=[1, 0, 2, 1],\n        filter_in_sizes=[1, 1, 1, 1],\n        strides=[1, 1],\n        padding=[[1, 1], [1, 1]],\n        expected=[0, 0, 0, 0, 0, 0, 0, 0])\n\n  def testConv2DExplicitPaddingWithLayoutOptimizer(self):\n    # Test with Grappler's layout optimizer, to ensure the layout optimizer\n    # handles explicit padding correctly.\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[1, 3, 2, 1],\n        filter_in_sizes=[1, 2, 1, 2],\n        strides=[1, 1],\n        padding=[[1, 0], [0, 1]],\n        dilations=[2, 1],\n        test_grappler_layout_optimizer=True)\n\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[1, 2, 3, 2],\n        filter_in_sizes=[3, 2, 2, 1],\n        strides=[1, 1],\n        padding=[[2, 1], [1, 2]],\n        dilations=[2, 3],\n        test_grappler_layout_optimizer=True)\n\n  def _VerifyGroupConvFwd(self, tensor_in_sizes, filter_in_sizes, dilations,\n                          strides, padding, data_format, dtype):\n    \"\"\"Verify the output of group convolution is equal to a for-loop implementation.\n\n    Args:\n      tensor_in_sizes: Input tensor dimensions in [batch, input_rows,\n        input_cols, input_depth].\n      filter_in_sizes: Filter tensor dimensions in [kernel_rows, kernel_cols,\n        input_depth, output_depth].\n      dilations: Dilated rate: [col_dilation, row_dilation]\n      strides: Stride: [col_stride, row_stride]\n      padding: Padding type.\n      data_format: Format of the data tensors.\n      dtype: Data type for inputs and outputs.\n    \"\"\"\n    tensor_in = self._CreateNumpyTensor(tensor_in_sizes)\n    filter_in = self._CreateNumpyTensor(filter_in_sizes)\n    num_groups = tensor_in_sizes[3] // filter_in_sizes[2]\n    assert num_groups > 1 and \\\n        filter_in_sizes[2] * num_groups == tensor_in_sizes[3]\n    with test_util.device(True):\n      t1 = constant_op.constant(tensor_in, dtype=dtype)\n      t2 = constant_op.constant(filter_in, dtype=dtype)\n      strides = [1] + strides + [1]\n      dilations = [1] + dilations + [1]\n      if data_format == \"NCHW\":\n        t1 = test_util.NHWCToNCHW(t1)\n        strides = test_util.NHWCToNCHW(strides)\n        dilations = test_util.NHWCToNCHW(dilations)\n        t1_splits = array_ops.split(t1, num_groups, axis=1)\n      else:\n        t1_splits = array_ops.split(t1, num_groups, axis=3)\n      t2_splits = array_ops.split(t2, num_groups, axis=3)\n\n      def MakeConv2d(inputs, filters):\n        return nn_ops.conv2d(\n            inputs,\n            filters,\n            strides,\n            padding,\n            dilations=dilations,\n            data_format=data_format)\n\n      group_conv = MakeConv2d(t1, t2)\n      group_conv_loop = array_ops.concat(\n          [MakeConv2d(t1s, t2s) for t1s, t2s in zip(t1_splits, t2_splits)],\n          axis=1 if data_format == \"NCHW\" else 3)\n\n      results = self.evaluate([group_conv, group_conv_loop])\n      tol_to_use = 1e-5\n      self.assertAllClose(\n          results[0], results[1], atol=tol_to_use, rtol=tol_to_use)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DGroupConvFwd(self):\n    if test.is_gpu_available(cuda_only=True) or test_util.IsMklEnabled():\n      data_formats = [\"NHWC\", \"NCHW\"]\n    else:\n      data_formats = [\"NHWC\"]\n    for data_format in data_formats:\n      for dilation in [1, 2]:\n        for stride in [1, 2]:\n          for filter_dims in [[3, 3, 4, 8], [1, 1, 2, 16]]:\n            self._VerifyGroupConvFwd([10, 32, 32, 16], filter_dims,\n                                     dilations=[dilation, dilation],\n                                     strides=[stride, stride],\n                                     padding=\"SAME\",\n                                     data_format=data_format,\n                                     dtype=dtypes.float32)\n\n  @test_util.deprecated_graph_mode_only\n  @test_util.run_cuda_only\n  def testInputGradientGroupConv(self):\n    for data_format in [\"NCHW\", \"NHWC\"]:\n      for test_input in [True, False]:\n        self.ConstructAndTestGradient(\n            batch=2,\n            input_rows=5,\n            input_cols=4,\n            filter_rows=3,\n            filter_cols=3,\n            num_groups=2,\n            padding=\"VALID\",\n            in_depth=4,\n            out_depth=6,\n            stride_rows=1,\n            stride_cols=1,\n            test_input=test_input,\n            data_format=data_format,\n            use_gpu=True,\n            max_err=0.005)\n\n  @test_util.deprecated_graph_mode_only\n  @test_util.run_cuda_only\n  def testFilterGradientGroupConv(self):\n    for data_format in [\"NCHW\", \"NHWC\"]:\n      for test_input in [True, False]:\n        self.ConstructAndTestGradient(\n            batch=2,\n            input_rows=5,\n            input_cols=4,\n            filter_rows=3,\n            filter_cols=3,\n            num_groups=2,\n            padding=\"VALID\",\n            in_depth=4,\n            out_depth=6,\n            stride_rows=1,\n            stride_cols=1,\n            test_input=test_input,\n            data_format=data_format,\n            use_gpu=True,\n            max_err=0.005)\n  # TODO(yzhwang): this currently fails.\n  # self._VerifyValues(tensor_in_sizes=[1, 8, 8, 1],\n  #                   filter_in_sizes=[2, 2, 1, 1],\n  #                   strides=[4, 4], padding=\"SAME\",\n  #                   expected=[72, 112, 392, 432])\n\n  # Testing for backprops\n  def _RunAndVerifyBackpropInput(self,\n                                 input_sizes,\n                                 filter_sizes,\n                                 output_sizes,\n                                 strides,\n                                 padding,\n                                 expected,\n                                 data_format,\n                                 use_gpu,\n                                 err,\n                                 dilations=(1, 1)):\n    if use_gpu and not test.is_gpu_available(cuda_only=True):\n      return\n    x1 = self._CreateNumpyTensor(filter_sizes)\n    x2 = self._CreateNumpyTensor(output_sizes)\n    dilations = list(dilations)\n    with test_util.device(use_gpu):\n      if len(input_sizes) == 4:\n        if data_format == \"NCHW\":\n          input_sizes = test_util.NHWCToNCHW(input_sizes)\n      t0 = constant_op.constant(input_sizes, shape=[len(input_sizes)])\n      t1 = constant_op.constant(x1, shape=filter_sizes)\n      t2 = constant_op.constant(x2, shape=output_sizes)\n      strides = [1] + strides + [1]\n      dilations = [1] + dilations + [1]\n      if isinstance(padding, (list, tuple)):\n        padding = [(0, 0)] + padding + [(0, 0)]\n      if data_format == \"NCHW\":\n        t2 = test_util.NHWCToNCHW(t2)\n        strides = test_util.NHWCToNCHW(strides)\n        dilations = test_util.NHWCToNCHW(dilations)\n        if isinstance(padding, (list, tuple)):\n          padding = test_util.NHWCToNCHW((padding))\n      conv = nn_ops.conv2d_backprop_input(\n          t0,\n          t1,\n          t2,\n          strides=strides,\n          padding=padding,\n          data_format=data_format,\n          dilations=dilations)\n      if data_format == \"NCHW\":\n        conv = test_util.NCHWToNHWC(conv)\n      # \"values\" consists of two tensors for two backprops\n      value = self.evaluate(conv)\n      self.assertShapeEqual(value, conv)\n    tf_logging.debug(\"expected = %s\", expected)\n    tf_logging.debug(\"actual = %s\", value)\n    self.assertAllCloseAccordingToType(expected, value.flatten(), atol=1e-5)\n\n  def _CompareBackpropInput(self, input_sizes, filter_sizes, output_sizes,\n                            conv_strides, padding):\n    x1 = np.random.rand(*filter_sizes).astype(np.float32)\n    x2 = np.random.rand(*output_sizes).astype(np.float32)\n\n    def _GetVal(data_format, use_gpu):\n      with test_util.device(use_gpu):\n        if data_format == \"NCHW\":\n          new_input_sizes = test_util.NHWCToNCHW(input_sizes)\n        else:\n          new_input_sizes = input_sizes\n        t0 = constant_op.constant(new_input_sizes, shape=[len(new_input_sizes)])\n        t1 = constant_op.constant(x1, shape=filter_sizes)\n        t2 = constant_op.constant(x2, shape=output_sizes)\n        strides = [1] + conv_strides + [1]\n        if data_format == \"NCHW\":\n          t2 = test_util.NHWCToNCHW(t2)\n          strides = test_util.NHWCToNCHW(strides)\n        conv = nn_ops.conv2d_backprop_input(\n            t0,\n            t1,\n            t2,\n            strides=strides,\n            padding=padding,\n            data_format=data_format)\n        if data_format == \"NCHW\":\n          conv = test_util.NCHWToNHWC(conv)\n        ret = self.evaluate(conv)\n        self.assertShapeEqual(ret, conv)\n        return ret\n\n    values = []\n    for (data_format, use_gpu) in GetTestConfigs():\n      values.append(_GetVal(data_format, use_gpu))\n\n    for i in range(1, len(values)):\n      self.assertAllClose(values[0], values[i], rtol=1e-2, atol=1e-2)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D2x2Depth1ValidBackpropInput(self):\n    expected_output = [1.0, 4.0, 4.0, 3.0, 10.0, 8.0]\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropInput(\n          input_sizes=[1, 2, 3, 1],\n          filter_sizes=[2, 2, 1, 1],\n          output_sizes=[1, 1, 2, 1],\n          strides=[1, 1],\n          padding=\"VALID\",\n          expected=expected_output,\n          data_format=data_format,\n          use_gpu=use_gpu,\n          err=1e-5)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DEmptyBackpropInput(self):\n    expected_output = []\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropInput(\n          input_sizes=[0, 2, 3, 1],\n          filter_sizes=[2, 2, 1, 1],\n          output_sizes=[0, 1, 2, 1],\n          strides=[1, 1],\n          padding=\"VALID\",\n          expected=expected_output,\n          data_format=data_format,\n          use_gpu=use_gpu,\n          err=1e-5)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D2x2Depth3ValidBackpropInput(self):\n    expected_output = [\n        14.0, 32.0, 50.0, 100.0, 163.0, 226.0, 167.0, 212.0, 257.0, 122.0,\n        140.0, 158.0, 478.0, 541.0, 604.0, 437.0, 482.0, 527.0\n    ]\n    for (data_format, use_gpu) in GetTestConfigs():\n      # The GPU version of this test is not very stable. So adjusting the\n      # error threshold to 1e-4.\n      self._RunAndVerifyBackpropInput(\n          input_sizes=[1, 2, 3, 3],\n          filter_sizes=[2, 2, 3, 3],\n          output_sizes=[1, 1, 2, 3],\n          strides=[1, 1],\n          padding=\"VALID\",\n          expected=expected_output,\n          data_format=data_format,\n          use_gpu=use_gpu,\n          err=1e-4)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D2x2Depth3ValidBackpropInputStride1x2(self):\n    expected_output = [\n        1.0, 2.0, 2.0, 4.0, 3.0, 6.0, 7.0, 12.0, 11.0, 18.0, 15.0, 24.0, 12.0,\n        16.0, 15.0, 20.0, 18.0, 24.0\n    ]\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropInput(\n          input_sizes=[1, 3, 6, 1],\n          filter_sizes=[2, 2, 1, 1],\n          output_sizes=[1, 2, 3, 1],\n          strides=[1, 2],\n          padding=\"VALID\",\n          expected=expected_output,\n          data_format=data_format,\n          use_gpu=use_gpu,\n          err=1e-5)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DStrideTwoFilterOneSameBackpropInput(self):\n    expected_output = [\n        1.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 0.0, 4.0, 0.0, 0.0, 0.0,\n        0.0, 0.0\n    ]\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropInput(\n          input_sizes=[1, 4, 4, 1],\n          filter_sizes=[1, 1, 1, 1],\n          output_sizes=[1, 2, 2, 1],\n          strides=[2, 2],\n          padding=\"SAME\",\n          expected=expected_output,\n          data_format=data_format,\n          use_gpu=use_gpu,\n          err=1e-5)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DKernelSizeMatchesInputSizeBackpropInput(self):\n    expected_output = [5.0, 11.0, 17.0, 23.0]\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropInput(\n          input_sizes=[1, 2, 2, 1],\n          filter_sizes=[2, 2, 1, 2],\n          output_sizes=[1, 1, 1, 2],\n          strides=[1, 1],\n          padding=\"VALID\",\n          expected=expected_output,\n          data_format=data_format,\n          use_gpu=use_gpu,\n          err=1e-5)\n\n  @test_util.run_in_graph_and_eager_modes\n  @test_util.disable_xla(\"XLA requires input_sizes to be a 4D shape.\")\n  def testConv2DInputSizesContainsOnlySpatialDimensionsBackpropInput(self):\n    expected_output = [5.0, 11.0, 17.0, 23.0]\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropInput(\n          input_sizes=[2, 2],\n          filter_sizes=[2, 2, 1, 2],\n          output_sizes=[1, 1, 1, 2],\n          strides=[1, 1],\n          padding=\"VALID\",\n          expected=expected_output,\n          data_format=data_format,\n          use_gpu=use_gpu,\n          err=1e-5)\n\n  @test_util.run_in_graph_and_eager_modes\n  @test_util.disable_xla(\"b/239598470\")\n  def testConv2DBackpropInputDegenerateBackpropInput(self):\n    input_sizes = [3, 1, 1, 2]\n    expected_output = np.zeros(input_sizes).flatten()\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropInput(\n          input_sizes=input_sizes,\n          filter_sizes=[1, 3, 2, 3],\n          output_sizes=[3, 1, 0, 3],\n          strides=[1, 2],\n          padding=\"VALID\",\n          expected=expected_output,\n          data_format=data_format,\n          use_gpu=use_gpu,\n          err=1e-5)\n\n  # Testing for backprops\n  def _RunAndVerifyBackpropFilter(self,\n                                  input_sizes,\n                                  filter_sizes,\n                                  output_sizes,\n                                  strides,\n                                  padding,\n                                  expected,\n                                  data_format,\n                                  use_gpu,\n                                  dilations=(1, 1),\n                                  err=1e-5):\n    x0 = self._CreateNumpyTensor(input_sizes)\n    x2 = self._CreateNumpyTensor(output_sizes)\n    dilations = list(dilations)\n    explicit_strides = [1] + strides + [1]\n    new_padding = padding\n    new_dilations = [1] + dilations + [1]\n    if isinstance(new_padding, (list, tuple)):\n      new_padding = [(0, 0)] + new_padding + [(0, 0)]\n    if data_format == \"NCHW\":\n      explicit_strides = test_util.NHWCToNCHW(explicit_strides)\n      new_dilations = test_util.NHWCToNCHW(new_dilations)\n      if isinstance(padding, (list, tuple)):\n        new_padding = test_util.NHWCToNCHW(new_padding)\n    for dtype in self._DtypesToTest(use_gpu=use_gpu):\n      with test_util.device(use_gpu):\n        t0 = constant_op.constant(x0, shape=input_sizes, dtype=dtype)\n        t1 = constant_op.constant(filter_sizes, shape=[len(filter_sizes)])\n        t2 = constant_op.constant(x2, shape=output_sizes, dtype=dtype)\n        if data_format == \"NCHW\":\n          t0 = test_util.NHWCToNCHW(t0)\n          t2 = test_util.NHWCToNCHW(t2)\n        conv = nn_ops.conv2d_backprop_filter(\n            t0,\n            t1,\n            t2,\n            strides=explicit_strides,\n            padding=new_padding,\n            dilations=new_dilations,\n            data_format=data_format)\n        value = self.evaluate(conv)\n        self.assertShapeEqual(value, conv)\n      tf_logging.debug(\"expected = %s\", expected)\n      tf_logging.debug(\"actual = %s\", value)\n      self.assertArrayNear(expected, value.flatten(), err)\n\n  def _CompareBackFilter(self, input_sizes, filter_sizes, output_sizes,\n                         conv_strides, padding):\n    x0 = np.random.rand(*input_sizes).astype(np.float32)\n    x2 = np.random.rand(*output_sizes).astype(np.float32)\n\n    def _GetVal(data_format, use_gpu):\n      with test_util.device(use_gpu):\n        t0 = constant_op.constant(x0, shape=input_sizes)\n        t1 = constant_op.constant(filter_sizes, shape=[len(filter_sizes)])\n        t2 = constant_op.constant(x2, shape=output_sizes)\n        strides = [1] + conv_strides + [1]\n        if data_format == \"NCHW\":\n          t0 = test_util.NHWCToNCHW(t0)\n          t2 = test_util.NHWCToNCHW(t2)\n          strides = test_util.NHWCToNCHW(strides)\n        conv = nn_ops.conv2d_backprop_filter(\n            t0,\n            t1,\n            t2,\n            strides=strides,\n            padding=padding,\n            data_format=data_format)\n        ret = self.evaluate(conv)\n        self.assertShapeEqual(ret, conv)\n        return ret\n\n    values = []\n    for (data_format, use_gpu) in GetTestConfigs():\n      values.append(_GetVal(data_format, use_gpu))\n    for i in range(1, len(values)):\n      self.assertAllClose(values[0], values[i], rtol=1e-4, atol=1e-4)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D2x2Depth1ValidBackpropFilter(self):\n    expected = [5.0, 8.0, 14.0, 17.0]\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropFilter(\n          input_sizes=[1, 2, 3, 1],\n          filter_sizes=[2, 2, 1, 1],\n          output_sizes=[1, 1, 2, 1],\n          strides=[1, 1],\n          padding=\"VALID\",\n          expected=expected,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DEmptyBackpropFilter(self):\n    expected = []\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropFilter(\n          input_sizes=[1, 2, 3, 1],\n          filter_sizes=[2, 2, 1, 0],\n          output_sizes=[1, 1, 2, 0],\n          strides=[1, 1],\n          padding=\"VALID\",\n          expected=expected,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DBackpropFilterWithEmptyInput(self):\n    expected = [0, 0, 0, 0]\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropFilter(\n          input_sizes=[0, 2, 3, 1],\n          filter_sizes=[2, 2, 1, 1],\n          output_sizes=[0, 1, 2, 1],\n          strides=[1, 1],\n          padding=\"VALID\",\n          expected=expected,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D2x2Depth3ValidBackpropFilter(self):\n    expected = [\n        17.0, 22.0, 27.0, 22.0, 29.0, 36.0, 27.0, 36.0, 45.0, 32.0, 43.0, 54.0,\n        37.0, 50.0, 63.0, 42.0, 57.0, 72.0, 62.0, 85.0, 108.0, 67.0, 92.0,\n        117.0, 72.0, 99.0, 126.0, 77.0, 106.0, 135.0, 82.0, 113.0, 144.0, 87.0,\n        120.0, 153.0\n    ]\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropFilter(\n          input_sizes=[1, 2, 3, 3],\n          filter_sizes=[2, 2, 3, 3],\n          output_sizes=[1, 1, 2, 3],\n          strides=[1, 1],\n          padding=\"VALID\",\n          expected=expected,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D2x2Depth3ValidBackpropFilterStride1x2(self):\n    expected = [161.0, 182.0, 287.0, 308.0]\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropFilter(\n          input_sizes=[1, 3, 6, 1],\n          filter_sizes=[2, 2, 1, 1],\n          output_sizes=[1, 2, 3, 1],\n          strides=[1, 2],\n          padding=\"VALID\",\n          expected=expected,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DStrideTwoFilterOneSameBackpropFilter(self):\n    expected_output = [78.]\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropFilter(\n          input_sizes=[1, 4, 4, 1],\n          filter_sizes=[1, 1, 1, 1],\n          output_sizes=[1, 2, 2, 1],\n          strides=[2, 2],\n          padding=\"SAME\",\n          expected=expected_output,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DKernelSizeMatchesInputSizeBackpropFilter(self):\n    expected_output = [1.0, 2.0, 2.0, 4.0, 3.0, 6.0, 4.0, 8.0]\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropFilter(\n          input_sizes=[1, 2, 2, 1],\n          filter_sizes=[2, 2, 1, 2],\n          output_sizes=[1, 1, 1, 2],\n          strides=[1, 1],\n          padding=\"VALID\",\n          expected=expected_output,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  # Testing for backprops\n  def _RunAndVerifyBackpropInputDilation(self, input_sizes, filter_sizes,\n                                         output_sizes, strides, dilations,\n                                         padding, data_format, use_gpu, err):\n    x1 = self._CreateNumpyTensor(input_sizes)\n    x2 = self._CreateNumpyTensor(filter_sizes)\n    default_dilations = (dilations[0] == 1 and dilations[1] == 1)\n    if default_dilations or use_gpu:\n      with self.cached_session(use_gpu=use_gpu) as sess:\n        if data_format == \"NCHW\":\n          input_sizes = test_util.NHWCToNCHW(input_sizes)\n        t1 = constant_op.constant(x1, shape=input_sizes)\n        t2 = constant_op.constant(x2, shape=filter_sizes)\n        full_strides = [1] + strides + [1]\n        full_dilations = [1] + dilations + [1]\n        if data_format == \"NCHW\":\n          full_strides = test_util.NHWCToNCHW(full_strides)\n          full_dilations = test_util.NHWCToNCHW(full_dilations)\n        conv_forward = nn_ops.conv2d(\n            t1,\n            t2,\n            strides=full_strides,\n            dilations=full_dilations,\n            padding=padding,\n            data_format=data_format)\n        conv_forward_2 = nn_ops.convolution(\n            t1,\n            t2,\n            padding=padding,\n            strides=strides,\n            dilation_rate=dilations,\n            data_format=data_format)\n        if data_format == \"NCHW\":\n          conv_forward = test_util.NCHWToNHWC(conv_forward)\n          conv_forward_2 = test_util.NCHWToNHWC(conv_forward_2)\n        conv = gradients_impl.gradients(conv_forward, t1)[0]\n        conv_2 = gradients_impl.gradients(conv_forward_2, t1)[0]\n        # \"values\" consists of two tensors for two backprops\n        value = self.evaluate(conv)\n        value_2 = self.evaluate(conv_2)\n        self.assertShapeEqual(value, conv)\n        self.assertShapeEqual(value_2, conv_2)\n      tf_logging.debug(\"expected = %s\", value_2)\n      tf_logging.debug(\"actual = %s\", value)\n      self.assertArrayNear(value_2.flatten(), value.flatten(), err)\n\n  # Testing for backprops\n  def _RunAndVerifyBackpropFilterDilation(self, input_sizes, filter_sizes,\n                                          output_sizes, strides, dilations,\n                                          padding, data_format, use_gpu, err):\n    x1 = self._CreateNumpyTensor(input_sizes)\n    x2 = self._CreateNumpyTensor(filter_sizes)\n    default_dilations = (dilations[0] == 1 and dilations[1] == 1)\n    if default_dilations or use_gpu:\n      with self.cached_session(use_gpu=use_gpu) as sess:\n        if data_format == \"NCHW\":\n          input_sizes = test_util.NHWCToNCHW(input_sizes)\n        t1 = constant_op.constant(x1, shape=input_sizes)\n        t2 = constant_op.constant(x2, shape=filter_sizes)\n        full_strides = [1] + strides + [1]\n        full_dilations = [1] + dilations + [1]\n        if data_format == \"NCHW\":\n          full_strides = test_util.NHWCToNCHW(full_strides)\n          full_dilations = test_util.NHWCToNCHW(full_dilations)\n        conv_forward = nn_ops.conv2d(\n            t1,\n            t2,\n            strides=full_strides,\n            dilations=full_dilations,\n            padding=padding,\n            data_format=data_format)\n        conv_forward_2 = nn_ops.convolution(\n            t1,\n            t2,\n            padding=padding,\n            strides=strides,\n            dilation_rate=dilations,\n            data_format=data_format)\n        if data_format == \"NCHW\":\n          conv_forward = test_util.NCHWToNHWC(conv_forward)\n          conv_forward_2 = test_util.NCHWToNHWC(conv_forward_2)\n        conv = gradients_impl.gradients(conv_forward, t2)[0]\n        conv_2 = gradients_impl.gradients(conv_forward, t2)[0]\n        value = self.evaluate(conv)\n        value_2 = self.evaluate(conv_2)\n        self.assertShapeEqual(value, conv)\n        self.assertShapeEqual(value_2, conv_2)\n      tf_logging.debug(\"expected = %s\", value_2)\n      tf_logging.debug(\"actual = %s\", value)\n      self.assertArrayNear(value_2.flatten(), value.flatten(), err)\n\n  @test_util.deprecated_graph_mode_only\n  def testConv2D2x2Depth3ValidBackpropFilterStride1x1Dilation2x1(self):\n    if test.is_gpu_available(cuda_only=True) or test_util.IsMklEnabled():\n      for (data_format, use_gpu) in GetTestConfigs():\n        self._RunAndVerifyBackpropFilterDilation(\n            input_sizes=[1, 3, 6, 1],\n            filter_sizes=[2, 2, 1, 1],\n            output_sizes=[1, 1, 5, 1],\n            strides=[1, 1],\n            dilations=[2, 1],\n            padding=\"VALID\",\n            data_format=data_format,\n            use_gpu=use_gpu,\n            err=1e-5)\n\n  @test_util.deprecated_graph_mode_only\n  def testConv2D2x2Depth1ValidBackpropFilterDilation1x2(self):\n    if test.is_gpu_available(cuda_only=True) or test_util.IsMklEnabled():\n      for (data_format, use_gpu) in GetTestConfigs():\n        self._RunAndVerifyBackpropFilterDilation(\n            input_sizes=[1, 2, 3, 1],\n            filter_sizes=[2, 2, 1, 1],\n            output_sizes=[1, 1, 2, 1],\n            strides=[1, 1],\n            dilations=[1, 2],\n            padding=\"VALID\",\n            data_format=data_format,\n            use_gpu=use_gpu,\n            err=1e-5)\n\n  @test_util.deprecated_graph_mode_only\n  def testConv2DEmptyBackpropFilterDilation1x2(self):\n    if test.is_gpu_available(cuda_only=True) or test_util.IsMklEnabled():\n      for (data_format, use_gpu) in GetTestConfigs():\n        self._RunAndVerifyBackpropFilterDilation(\n            input_sizes=[1, 2, 3, 1],\n            filter_sizes=[2, 2, 1, 0],\n            output_sizes=[1, 1, 2, 0],\n            strides=[1, 1],\n            dilations=[1, 2],\n            padding=\"VALID\",\n            data_format=data_format,\n            use_gpu=use_gpu,\n            err=1e-5)\n\n  @test_util.deprecated_graph_mode_only\n  def testConv2D2x2Depth3ValidBackpropFilterDilation2x2(self):\n    if test.is_gpu_available(cuda_only=True) or test_util.IsMklEnabled():\n      for (data_format, use_gpu) in GetTestConfigs():\n        self._RunAndVerifyBackpropFilterDilation(\n            input_sizes=[1, 3, 4, 3],\n            filter_sizes=[2, 2, 3, 3],\n            output_sizes=[1, 1, 2, 3],\n            strides=[1, 1],\n            dilations=[2, 2],\n            padding=\"VALID\",\n            data_format=data_format,\n            use_gpu=use_gpu,\n            err=1e-5)\n\n  @test_util.deprecated_graph_mode_only\n  def testConv2DKernelSizeMatchesInputSizeBackpropFilterDilation2x2(self):\n    if test.is_gpu_available(cuda_only=True) or test_util.IsMklEnabled():\n      for (data_format, use_gpu) in GetTestConfigs():\n        self._RunAndVerifyBackpropFilterDilation(\n            input_sizes=[1, 3, 3, 1],\n            filter_sizes=[2, 2, 1, 2],\n            output_sizes=[1, 1, 1, 2],\n            strides=[1, 1],\n            dilations=[2, 2],\n            padding=\"VALID\",\n            data_format=data_format,\n            use_gpu=use_gpu,\n            err=1e-5)\n\n  @test_util.deprecated_graph_mode_only\n  def testConv2D2x2Depth3ValidBackpropInputStride1x1Dilation2x1(self):\n    if test.is_gpu_available(cuda_only=True) or test_util.IsMklEnabled():\n      for (data_format, use_gpu) in GetTestConfigs():\n        self._RunAndVerifyBackpropInputDilation(\n            input_sizes=[1, 3, 6, 1],\n            filter_sizes=[2, 2, 1, 1],\n            output_sizes=[1, 1, 5, 1],\n            strides=[1, 1],\n            dilations=[2, 1],\n            padding=\"VALID\",\n            data_format=data_format,\n            use_gpu=use_gpu,\n            err=1e-5)\n\n  @test_util.deprecated_graph_mode_only\n  def testConv2D2x2Depth1ValidBackpropInputDilation1x2(self):\n    if test.is_gpu_available(cuda_only=True) or test_util.IsMklEnabled():\n      for (data_format, use_gpu) in GetTestConfigs():\n        self._RunAndVerifyBackpropInputDilation(\n            input_sizes=[1, 2, 3, 1],\n            filter_sizes=[2, 2, 1, 1],\n            output_sizes=[1, 1, 2, 1],\n            strides=[1, 1],\n            dilations=[1, 2],\n            padding=\"VALID\",\n            data_format=data_format,\n            use_gpu=use_gpu,\n            err=1e-5)\n\n  @test_util.deprecated_graph_mode_only\n  def testConv2DEmptyBackpropInputDilation1x2(self):\n    if test.is_gpu_available(cuda_only=True) or test_util.IsMklEnabled():\n      for (data_format, use_gpu) in GetTestConfigs():\n        self._RunAndVerifyBackpropInputDilation(\n            input_sizes=[0, 2, 3, 1],\n            filter_sizes=[2, 2, 1, 1],\n            output_sizes=[0, 1, 2, 1],\n            strides=[1, 1],\n            dilations=[1, 2],\n            padding=\"VALID\",\n            data_format=data_format,\n            use_gpu=use_gpu,\n            err=1e-5)\n\n  @test_util.deprecated_graph_mode_only\n  def testConv2D2x2Depth3ValidBackpropInputDilation2x1(self):\n    if test.is_gpu_available(cuda_only=True) or test_util.IsMklEnabled():\n      for (data_format, use_gpu) in GetTestConfigs():\n        # The GPU version of this test is not very stable. So adjusting the\n        # error threshold to 1e-4.\n        self._RunAndVerifyBackpropInputDilation(\n            input_sizes=[1, 3, 2, 3],\n            filter_sizes=[2, 2, 3, 3],\n            output_sizes=[1, 1, 2, 3],\n            strides=[1, 1],\n            dilations=[2, 1],\n            padding=\"VALID\",\n            data_format=data_format,\n            use_gpu=use_gpu,\n            err=1e-4)\n\n  @test_util.deprecated_graph_mode_only\n  def testConv2DKernelSizeMatchesInputSizeBackpropInputDilation2x2(self):\n    if test.is_gpu_available(cuda_only=True) or test_util.IsMklEnabled():\n      for (data_format, use_gpu) in GetTestConfigs():\n        self._RunAndVerifyBackpropInputDilation(\n            input_sizes=[1, 3, 3, 1],\n            filter_sizes=[2, 2, 1, 2],\n            output_sizes=[1, 1, 1, 2],\n            strides=[1, 1],\n            dilations=[2, 2],\n            padding=\"VALID\",\n            data_format=data_format,\n            use_gpu=use_gpu,\n            err=1e-5)\n\n  def _RunAndVerifyBackpropInputExplicitPadding(self,\n                                                input_sizes,\n                                                filter_sizes,\n                                                output_sizes,\n                                                strides,\n                                                padding,\n                                                data_format,\n                                                use_gpu,\n                                                dilations=(1, 1),\n                                                err=2e-5):\n    if use_gpu and not test.is_gpu_available(cuda_only=True):\n      return\n    if not use_gpu and dilations != (1, 1):\n      return  # Non-default dilations is currently not supported on the CPU.\n\n    x1 = self._CreateNumpyTensor(filter_sizes)\n    x2 = self._CreateNumpyTensor(output_sizes)\n    dilations = list(dilations)\n    padded_input_sizes = input_sizes[:]\n    padded_input_sizes[1] += padding[0][0] + padding[0][1]\n    padded_input_sizes[2] += padding[1][0] + padding[1][1]\n    c = nn_ops.conv2d_backprop_input(\n        padded_input_sizes,\n        x1,\n        x2,\n        strides=[1] + strides + [1],\n        padding=\"VALID\",\n        dilations=[1] + dilations + [1])\n    c = c[:, padding[0][0]:(c.shape[1] - padding[0][1]), padding[1][0]:(\n        c.shape[2] - padding[1][1]), :]\n    expected = list(self.evaluate(array_ops.reshape(c, [-1])))\n    self._RunAndVerifyBackpropInput(\n        input_sizes,\n        filter_sizes,\n        output_sizes,\n        strides,\n        padding,\n        expected,\n        data_format,\n        use_gpu=use_gpu,\n        err=err,\n        dilations=dilations)\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2D2x2Depth1Padding0x0BackpropInput(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropInputExplicitPadding(\n          input_sizes=[1, 2, 3, 1],\n          filter_sizes=[2, 2, 1, 1],\n          output_sizes=[1, 1, 2, 1],\n          strides=[1, 1],\n          padding=[[0, 0], [0, 0]],\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n      self._RunAndVerifyBackpropInputExplicitPadding(\n          input_sizes=[1, 3, 4, 2],\n          filter_sizes=[2, 2, 2, 3],\n          output_sizes=[1, 1, 2, 3],\n          strides=[2, 2],\n          padding=[[0, 0], [0, 0]],\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2D2x2Depth1Padding1x1BackpropInput(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropInputExplicitPadding(\n          input_sizes=[1, 2, 3, 1],\n          filter_sizes=[2, 2, 1, 2],\n          output_sizes=[1, 3, 4, 2],\n          strides=[1, 1],\n          padding=[[1, 1], [1, 1]],\n          data_format=data_format,\n          use_gpu=use_gpu,\n          err=1e-4)\n\n      self._RunAndVerifyBackpropInputExplicitPadding(\n          input_sizes=[1, 2, 3, 2],\n          filter_sizes=[1, 1, 2, 1],\n          output_sizes=[1, 4, 3, 1],\n          strides=[1, 2],\n          padding=[[1, 1], [1, 1]],\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n      self._RunAndVerifyBackpropInputExplicitPadding(\n          input_sizes=[1, 4, 3, 1],\n          filter_sizes=[2, 2, 1, 1],\n          output_sizes=[1, 4, 2, 1],\n          strides=[1, 2],\n          padding=[[1, 1], [1, 1]],\n          data_format=data_format,\n          dilations=[2, 2], use_gpu=use_gpu)\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2D2x2Depth1Padding2x2BackpropInput(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropInputExplicitPadding(\n          input_sizes=[2, 3, 1, 1],\n          filter_sizes=[2, 1, 1, 1],\n          output_sizes=[2, 2, 5, 1],\n          strides=[3, 1],\n          padding=[[2, 2], [2, 2]],\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n      self._RunAndVerifyBackpropInputExplicitPadding(\n          input_sizes=[1, 3, 6, 1],\n          filter_sizes=[3, 2, 1, 1],\n          output_sizes=[1, 3, 4, 1],\n          strides=[1, 2],\n          padding=[[2, 2], [2, 2]],\n          data_format=data_format,\n          dilations=[2, 3],\n          use_gpu=use_gpu)\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2D2x2Depth1Padding_1_8_4_1_BackpropInput(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropInputExplicitPadding(\n          input_sizes=[1, 2, 3, 1],\n          filter_sizes=[2, 2, 1, 1],\n          output_sizes=[1, 10, 8, 1],\n          strides=[1, 1],\n          padding=[[1, 8], [4, 2]],\n          data_format=data_format,\n          use_gpu=use_gpu,\n          err=5e-5)\n\n      self._RunAndVerifyBackpropInputExplicitPadding(\n          input_sizes=[1, 5, 3, 1],\n          filter_sizes=[3, 2, 1, 1],\n          output_sizes=[1, 4, 8, 1],\n          strides=[3, 1],\n          padding=[[1, 8], [4, 2]],\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2D2x2Depth1Padding_5_0_2_2_BackpropInput(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropInputExplicitPadding(\n          input_sizes=[1, 3, 3, 1],\n          filter_sizes=[2, 1, 1, 1],\n          output_sizes=[1, 7, 7, 1],\n          strides=[1, 1],\n          padding=[[5, 0], [2, 2]],\n          data_format=data_format,\n          err=5e-5,\n          use_gpu=use_gpu)\n\n      self._RunAndVerifyBackpropInputExplicitPadding(\n          input_sizes=[1, 4, 2, 1],\n          filter_sizes=[3, 3, 1, 1],\n          output_sizes=[1, 5, 2, 1],\n          strides=[1, 2],\n          padding=[[5, 0], [2, 2]],\n          data_format=data_format,\n          dilations=[2, 1],\n          use_gpu=use_gpu)\n\n  def _RunAndVerifyBackpropFilterExplicitPadding(self,\n                                                 input_sizes,\n                                                 filter_sizes,\n                                                 output_sizes,\n                                                 strides,\n                                                 padding,\n                                                 data_format,\n                                                 use_gpu,\n                                                 dilations=(1, 1),\n                                                 err=1e-5):\n    if use_gpu and not test.is_gpu_available(cuda_only=True):\n      return\n    if not use_gpu and dilations != (1, 1):\n      return  # Non-default dilations is currently not supported on the CPU.\n\n    x0 = self._CreateNumpyTensor(input_sizes)\n    x2 = self._CreateNumpyTensor(output_sizes)\n    dilations = list(dilations)\n\n    x0 = np.pad(x0, [(0, 0)] + padding + [(0, 0)], \"constant\")\n    c = nn_ops.conv2d_backprop_filter(\n        x0,\n        filter_sizes,\n        x2,\n        strides=[1] + strides + [1],\n        padding=\"VALID\",\n        dilations=[1] + dilations + [1])\n    expected = list(self.evaluate(array_ops.reshape(c, [-1])))\n    self._RunAndVerifyBackpropFilter(\n        input_sizes,\n        filter_sizes,\n        output_sizes,\n        strides,\n        padding,\n        expected,\n        data_format,\n        use_gpu=use_gpu,\n        dilations=dilations,\n        err=err)\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2D2x2Depth1Padding0x0BackpropFilter(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropFilterExplicitPadding(\n          input_sizes=[1, 2, 3, 1],\n          filter_sizes=[2, 2, 1, 1],\n          output_sizes=[1, 1, 2, 1],\n          strides=[1, 1],\n          padding=[[0, 0], [0, 0]],\n          data_format=data_format, use_gpu=use_gpu)\n\n      self._RunAndVerifyBackpropFilterExplicitPadding(\n          input_sizes=[1, 3, 4, 2],\n          filter_sizes=[2, 2, 2, 3],\n          output_sizes=[1, 1, 2, 3],\n          strides=[2, 2],\n          padding=[[0, 0], [0, 0]],\n          data_format=data_format, use_gpu=use_gpu)\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2D2x2Depth1Padding1x1BackpropFilter(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropFilterExplicitPadding(\n          input_sizes=[1, 2, 3, 1],\n          filter_sizes=[2, 2, 1, 2],\n          output_sizes=[1, 3, 4, 2],\n          strides=[1, 1],\n          padding=[[1, 1], [1, 1]],\n          data_format=data_format,\n          use_gpu=use_gpu,\n          err=5e-5)\n\n      self._RunAndVerifyBackpropFilterExplicitPadding(\n          input_sizes=[1, 2, 3, 2],\n          filter_sizes=[1, 1, 2, 1],\n          output_sizes=[1, 4, 3, 1],\n          strides=[1, 2],\n          padding=[[1, 1], [1, 1]],\n          use_gpu=use_gpu,\n          data_format=data_format)\n\n      self._RunAndVerifyBackpropFilterExplicitPadding(\n          input_sizes=[1, 4, 3, 1],\n          filter_sizes=[2, 2, 1, 1],\n          output_sizes=[1, 4, 2, 1],\n          strides=[1, 2],\n          padding=[[1, 1], [1, 1]],\n          data_format=data_format,\n          use_gpu=use_gpu,\n          dilations=[2, 2])\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2D2x2Depth1Padding2x2BackpropFilter(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropFilterExplicitPadding(\n          input_sizes=[2, 3, 1, 1],\n          filter_sizes=[2, 1, 1, 1],\n          output_sizes=[2, 2, 5, 1],\n          strides=[3, 1],\n          padding=[[2, 2], [2, 2]],\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n      self._RunAndVerifyBackpropFilterExplicitPadding(\n          input_sizes=[1, 3, 6, 1],\n          filter_sizes=[3, 2, 1, 1],\n          output_sizes=[1, 3, 4, 1],\n          strides=[1, 2],\n          padding=[[2, 2], [2, 2]],\n          data_format=data_format,\n          use_gpu=use_gpu,\n          dilations=[2, 3])\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2D2x2Depth1Padding_1_8_4_1_BackpropFilter(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropFilterExplicitPadding(\n          input_sizes=[1, 2, 3, 1],\n          filter_sizes=[2, 2, 1, 1],\n          output_sizes=[1, 10, 8, 1],\n          strides=[1, 1],\n          padding=[[1, 8], [4, 2]],\n          data_format=data_format,\n          use_gpu=use_gpu,\n          err=1e-4)\n\n      self._RunAndVerifyBackpropFilterExplicitPadding(\n          input_sizes=[1, 5, 3, 1],\n          filter_sizes=[3, 2, 1, 1],\n          output_sizes=[1, 4, 8, 1],\n          strides=[3, 1],\n          padding=[[1, 8], [4, 2]],\n          use_gpu=use_gpu,\n          data_format=data_format)\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2D2x2Depth1Padding_5_0_2_2_BackpropFilter(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropFilterExplicitPadding(\n          input_sizes=[1, 3, 3, 1],\n          filter_sizes=[2, 1, 1, 1],\n          output_sizes=[1, 7, 7, 1],\n          strides=[1, 1],\n          padding=[[5, 0], [2, 2]],\n          data_format=data_format,\n          use_gpu=use_gpu,\n          err=1e-4)\n\n      self._RunAndVerifyBackpropFilterExplicitPadding(\n          input_sizes=[1, 4, 2, 1],\n          filter_sizes=[3, 3, 1, 1],\n          output_sizes=[1, 5, 2, 1],\n          strides=[1, 2],\n          padding=[[5, 0], [2, 2]],\n          data_format=data_format,\n          use_gpu=use_gpu,\n          dilations=[2, 1])\n\n  # Gradient checkers\n  def ConstructAndTestGradient(self,\n                               batch,\n                               input_rows,\n                               input_cols,\n                               filter_rows,\n                               filter_cols,\n                               in_depth,\n                               out_depth,\n                               stride_rows,\n                               stride_cols,\n                               padding,\n                               test_input,\n                               data_format,\n                               use_gpu,\n                               num_groups=1,\n                               max_err=0.003):\n    assert in_depth % num_groups == 0 and out_depth % num_groups == 0\n    input_shape = [batch, input_rows, input_cols, in_depth]\n    filter_shape = [filter_rows, filter_cols, in_depth // num_groups, out_depth]\n    # TODO(yangke): re-factor the computation of output shape.\n    if padding == \"VALID\":\n      output_rows = (input_rows - filter_rows + stride_rows) // stride_rows\n      output_cols = (input_cols - filter_cols + stride_cols) // stride_cols\n    elif padding == \"SAME\":\n      output_rows = (input_rows + stride_rows - 1) // stride_rows\n      output_cols = (input_cols + stride_cols - 1) // stride_cols\n    else:\n      self.assertIsInstance(padding, (list, tuple))\n      output_rows = (input_rows + padding[1][0] + padding[1][1] - filter_rows +\n                     stride_rows) // stride_rows\n      output_cols = (input_cols + padding[2][0] + padding[2][1] - filter_cols +\n                     stride_cols) // stride_cols\n    output_shape = [batch, output_rows, output_cols, out_depth]\n    input_size = 1\n    for x in input_shape:\n      input_size *= x\n    filter_size = 1\n    for x in filter_shape:\n      filter_size *= x\n    input_data = [x * 1.0 / input_size for x in range(0, input_size)]\n    filter_data = [x * 1.0 / filter_size for x in range(0, filter_size)]\n    # Conv2DGrad functions are not compiled for double due to\n    # a problem in the way Eigen's Conv2DGrad works for double.\n    # So we disable the DOUBLE path.  We should re-enable this\n    # when double support returns for CPU and/or GPU.\n    for dtype in self._DtypesToTest(use_gpu=use_gpu):\n      with self.cached_session(use_gpu=use_gpu):\n        input_tensor = constant_op.constant(\n            input_data, shape=input_shape, dtype=dtype, name=\"input\")\n        filter_tensor = constant_op.constant(\n            filter_data, shape=filter_shape, dtype=dtype, name=\"filter\")\n        strides = [1, stride_rows, stride_cols, 1]\n        new_padding = padding\n        if data_format == \"NCHW\":\n          new_input_tensor = test_util.NHWCToNCHW(input_tensor)\n          strides = test_util.NHWCToNCHW(strides)\n          if isinstance(padding, (list, tuple)):\n            new_padding = test_util.NHWCToNCHW(padding)\n        else:\n          new_input_tensor = input_tensor\n        conv = nn_ops.conv2d(\n            new_input_tensor,\n            filter_tensor,\n            strides,\n            new_padding,\n            data_format=data_format,\n            name=\"conv\")\n        if data_format == \"NCHW\":\n          conv = test_util.NCHWToNHWC(conv)\n        self.assertEqual(output_shape, conv.get_shape())\n        if test_input:\n          jacob_t, jacob_n = gradient_checker.compute_gradient(input_tensor,\n                                                               input_shape,\n                                                               conv,\n                                                               output_shape)\n        else:\n          jacob_t, jacob_n = gradient_checker.compute_gradient(filter_tensor,\n                                                               filter_shape,\n                                                               conv,\n                                                               output_shape)\n        if dtype == dtypes.float32:\n          reference_jacob_t = jacob_t\n          err = np.fabs(jacob_t - jacob_n).max()\n        else:\n          # Compare fp16 theoretical gradients to fp32 theoretical gradients,\n          # since fp16 numerical gradients are too imprecise.\n          err = np.fabs(jacob_t - reference_jacob_t).max()\n\n        tf_logging.debug(\"conv_2d gradient error = %s\", err)\n        self.assertLess(err, max_err)\n\n  @test_util.deprecated_graph_mode_only\n  def testInputGradientValidPaddingStrideOne(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=5,\n          input_cols=4,\n          filter_rows=3,\n          filter_cols=3,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=1,\n          stride_cols=1,\n          padding=\"VALID\",\n          test_input=True,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testFilterGradientValidPaddingStrideOne(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=4,\n          input_rows=6,\n          input_cols=5,\n          filter_rows=2,\n          filter_cols=2,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=1,\n          stride_cols=1,\n          padding=\"VALID\",\n          test_input=False,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testInputGradientValidPaddingStrideTwo(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=4,\n          input_cols=5,\n          filter_rows=3,\n          filter_cols=3,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=2,\n          stride_cols=2,\n          padding=\"VALID\",\n          test_input=True,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testFilterGradientValidPaddingStrideTwo(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=4,\n          input_rows=6,\n          input_cols=5,\n          filter_rows=2,\n          filter_cols=2,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=2,\n          stride_cols=2,\n          padding=\"VALID\",\n          test_input=False,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testInputGradientValidPaddingStrideThree(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=7,\n          input_cols=6,\n          filter_rows=3,\n          filter_cols=3,\n          in_depth=4,\n          out_depth=5,\n          stride_rows=3,\n          stride_cols=3,\n          padding=\"VALID\",\n          test_input=True,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testFilterGradientValidPaddingStrideThree(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=8,\n          input_cols=7,\n          filter_rows=4,\n          filter_cols=4,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=3,\n          stride_cols=3,\n          padding=\"VALID\",\n          test_input=False,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testInputGradientSamePaddingStrideOne(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=7,\n          input_cols=6,\n          filter_rows=3,\n          filter_cols=3,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=1,\n          stride_cols=1,\n          padding=\"SAME\",\n          test_input=True,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testFilterGradientSamePaddingStrideOne(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=4,\n          input_rows=6,\n          input_cols=5,\n          filter_rows=2,\n          filter_cols=2,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=1,\n          stride_cols=1,\n          padding=\"SAME\",\n          test_input=False,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testInputGradientSamePaddingStrideTwo(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=5,\n          input_cols=4,\n          filter_rows=3,\n          filter_cols=3,\n          in_depth=3,\n          out_depth=3,\n          stride_rows=2,\n          stride_cols=2,\n          padding=\"SAME\",\n          test_input=True,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testFilterGradientSamePaddingStrideTwo(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=4,\n          input_rows=6,\n          input_cols=5,\n          filter_rows=2,\n          filter_cols=2,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=2,\n          stride_cols=2,\n          padding=\"SAME\",\n          test_input=False,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testInputGradientSamePaddingStrideThree(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=7,\n          input_cols=6,\n          filter_rows=3,\n          filter_cols=3,\n          in_depth=4,\n          out_depth=5,\n          stride_rows=3,\n          stride_cols=3,\n          padding=\"SAME\",\n          test_input=True,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testFilterGradientSamePaddingStrideThree(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=8,\n          input_cols=7,\n          filter_rows=4,\n          filter_cols=4,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=3,\n          stride_cols=3,\n          padding=\"SAME\",\n          test_input=False,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testFilterGradientSamePaddingStride2x1(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=8,\n          input_cols=7,\n          filter_rows=4,\n          filter_cols=4,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=2,\n          stride_cols=1,\n          padding=\"SAME\",\n          test_input=False,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testInputGradientKernelSizeMatchesInputSize(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=4,\n          input_cols=3,\n          filter_rows=4,\n          filter_cols=3,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=1,\n          stride_cols=1,\n          padding=\"VALID\",\n          test_input=True,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testFilterGradientKernelSizeMatchesInputSize(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=4,\n          input_cols=3,\n          filter_rows=4,\n          filter_cols=3,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=1,\n          stride_cols=1,\n          padding=\"VALID\",\n          test_input=False,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testInputGradient1x1PaddingStrideOne(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=5,\n          input_cols=4,\n          filter_rows=3,\n          filter_cols=3,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=1,\n          stride_cols=1,\n          padding=[[0, 0], [1, 1], [1, 1], [0, 0]],\n          test_input=True,\n          data_format=data_format,\n          use_gpu=use_gpu,\n          max_err=0.0025)\n\n  @test_util.deprecated_graph_mode_only\n  def testFilterGradient1x1PaddingStrideOne(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=5,\n          input_cols=4,\n          filter_rows=3,\n          filter_cols=3,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=1,\n          stride_cols=1,\n          padding=[[0, 0], [1, 1], [1, 1], [0, 0]],\n          test_input=False,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testInputGradient1x1PaddingStrideTwo(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=4,\n          input_cols=5,\n          filter_rows=3,\n          filter_cols=3,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=2,\n          stride_cols=2,\n          padding=[[0, 0], [1, 1], [1, 1], [0, 0]],\n          test_input=True,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testFilterGradient1x1PaddingStrideTwo(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=4,\n          input_cols=5,\n          filter_rows=3,\n          filter_cols=3,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=2,\n          stride_cols=2,\n          padding=[[0, 0], [1, 1], [1, 1], [0, 0]],\n          test_input=False,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testInputGradient2x2PaddingStrideOne(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=5,\n          input_cols=4,\n          filter_rows=3,\n          filter_cols=3,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=1,\n          stride_cols=1,\n          padding=[[0, 0], [2, 2], [2, 2], [0, 0]],\n          test_input=True,\n          data_format=data_format,\n          use_gpu=use_gpu,\n          max_err=0.003)\n\n  @test_util.deprecated_graph_mode_only\n  def testFilterGradient2x2PaddingStrideOne(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=5,\n          input_cols=4,\n          filter_rows=3,\n          filter_cols=3,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=1,\n          stride_cols=1,\n          padding=[[0, 0], [2, 2], [2, 2], [0, 0]],\n          test_input=False,\n          data_format=data_format,\n          use_gpu=use_gpu,\n          max_err=0.005)\n\n  @test_util.deprecated_graph_mode_only\n  def testInputGradient1_2_3_4PaddingStride3x2(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=8,\n          input_cols=5,\n          filter_rows=4,\n          filter_cols=2,\n          in_depth=3,\n          out_depth=2,\n          stride_rows=3,\n          stride_cols=2,\n          padding=[[0, 0], [1, 2], [3, 4], [0, 0]],\n          test_input=True,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testFilterGradient1_2_3_4PaddingStride3x2(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=8,\n          input_cols=5,\n          filter_rows=4,\n          filter_cols=2,\n          in_depth=3,\n          out_depth=2,\n          stride_rows=3,\n          stride_cols=2,\n          padding=[[0, 0], [1, 2], [3, 4], [0, 0]],\n          test_input=False,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testInputGradient4_3_2_1PaddingStride2x1(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=3,\n          input_rows=5,\n          input_cols=7,\n          filter_rows=3,\n          filter_cols=2,\n          in_depth=1,\n          out_depth=2,\n          stride_rows=2,\n          stride_cols=1,\n          padding=[[0, 0], [4, 3], [2, 1], [0, 0]],\n          test_input=True,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testFilterGradient4_3_2_1PaddingStride2x1(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=3,\n          input_rows=5,\n          input_cols=7,\n          filter_rows=3,\n          filter_cols=2,\n          in_depth=1,\n          out_depth=2,\n          stride_rows=2,\n          stride_cols=1,\n          padding=[[0, 0], [4, 3], [2, 1], [0, 0]],\n          test_input=False,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testInputGradient0_0_0_5PaddingStride1x2(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=6,\n          input_cols=7,\n          filter_rows=3,\n          filter_cols=4,\n          in_depth=3,\n          out_depth=2,\n          stride_rows=1,\n          stride_cols=2,\n          padding=[[0, 0], [0, 0], [0, 5], [0, 0]],\n          test_input=True,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testFilterGradient0_0_0_5PaddingStride1x2(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=6,\n          input_cols=7,\n          filter_rows=3,\n          filter_cols=4,\n          in_depth=3,\n          out_depth=2,\n          stride_rows=1,\n          stride_cols=2,\n          padding=[[0, 0], [0, 0], [0, 5], [0, 0]],\n          test_input=False,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testShapeFunctionEdgeCases(self):\n    # All shapes unknown.\n    c1 = nn_ops.conv2d(\n        array_ops.placeholder(dtypes.float32),\n        array_ops.placeholder(dtypes.float32),\n        strides=[1, 1, 1, 1],\n        padding=\"SAME\")\n    self.assertEqual([None, None, None, None], c1.get_shape().as_list())\n\n    # Incorrect input shape.\n    with self.assertRaises(ValueError):\n      nn_ops.conv2d(\n          array_ops.placeholder(\n              dtypes.float32, shape=[1, 3]),\n          array_ops.placeholder(dtypes.float32),\n          strides=[1, 1, 1, 1],\n          padding=\"SAME\")\n\n    # Incorrect filter shape.\n    with self.assertRaises(ValueError):\n      nn_ops.conv2d(\n          array_ops.placeholder(dtypes.float32),\n          array_ops.placeholder(\n              dtypes.float32, shape=[1, 3]),\n          strides=[1, 1, 1, 1],\n          padding=\"SAME\")\n\n    # Depth mismatch.\n    with self.assertRaises(ValueError):\n      nn_ops.conv2d(\n          array_ops.placeholder(\n              dtypes.float32, shape=[32, 20, 20, 3]),\n          array_ops.placeholder(\n              dtypes.float32, shape=[4, 4, 2, 2]),\n          strides=[1, 1, 1, 1],\n          padding=\"SAME\")\n\n    # Input depth divisible by filter depth (group convolution).\n    # No exceptions should appear.\n    nn_ops.conv2d(\n        array_ops.placeholder(dtypes.float32, shape=[32, 20, 20, 8]),\n        array_ops.placeholder(dtypes.float32, shape=[4, 4, 2, 16]),\n        strides=[1, 1, 1, 1],\n        padding=\"SAME\")\n\n    # Negative padding.\n    with self.assertRaises(ValueError):\n      nn_ops.conv2d(\n          array_ops.placeholder(dtypes.float32),\n          array_ops.placeholder(dtypes.float32),\n          strides=[1, 1, 1, 1],\n          padding=[[0, 0], [0, -1], [1, 2], [0, 0]])\n\n    # Nonzero padding in nonspatial dimension.\n    with self.assertRaises(ValueError):\n      nn_ops.conv2d(\n          array_ops.placeholder(dtypes.float32),\n          array_ops.placeholder(dtypes.float32),\n          strides=[1, 1, 1, 1],\n          padding=[[1, 0], [0, 0], [0, 0], [0, 0]])\n\n    # Nonzero NCHW padding in nonspatial dimension.\n    with self.assertRaises(ValueError):\n      nn_ops.conv2d(\n          array_ops.placeholder(dtypes.float32),\n          array_ops.placeholder(dtypes.float32),\n          strides=[1, 1, 1, 1],\n          padding=[[0, 0], [0, 1], [0, 0], [0, 0]],\n          data_format=\"NCHW\")\n\n    # Wrong amount of padding\n    with self.assertRaises(ValueError):\n      nn_ops.conv2d(\n          array_ops.placeholder(dtypes.float32),\n          array_ops.placeholder(dtypes.float32),\n          strides=[1, 1, 1, 1],\n          padding=[[0, 0], [0, 0], [0, 0]])\n\n    # Only specify one padding amount per dimension\n    with self.assertRaises(ValueError):\n      nn_ops.conv2d(\n          array_ops.placeholder(dtypes.float32),\n          array_ops.placeholder(dtypes.float32),\n          strides=[1, 1, 1, 1],\n          padding=[[0], [0], [0], [0]])\n\n    # Explicit padding elements are not lists\n    with self.assertRaises(ValueError):\n      nn_ops.conv2d(\n          array_ops.placeholder(dtypes.float32),\n          array_ops.placeholder(dtypes.float32),\n          strides=[1, 1, 1, 1],\n          padding=[0, 0, 0, 0])\n\n  def testOpEdgeCases(self):\n    # Illegal strides.\n    with self.assertRaisesRegex((ValueError, errors_impl.UnimplementedError),\n                                \"strides in the batch and depth\"):\n      input_val = np.ones([2, 4, 10, 10])\n      filter_val = np.ones([2, 4, 10, 10])\n      self.evaluate(\n          nn_ops.conv2d(\n              input_val, filter_val, strides=[2, 1, 1, 1], padding=\"SAME\"))\n    with self.assertRaisesRegex((ValueError, errors_impl.UnimplementedError),\n                                \"strides in the batch and depth\"):\n      input_val = np.ones([2, 4, 10, 10])\n      filter_val = np.ones([2, 4, 10, 10])\n      self.evaluate(\n          nn_ops.conv2d(\n              input_val, filter_val, strides=[1, 1, 1, 2], padding=\"SAME\"))\n\n    # TODO(b/195689143): Will enable when fixed for V2 behavior\n    # # Filter larger than input.\n    # with self.assertRaisesRegex(ValueError, \"Negative dimension size\"):\n    #   input_val = np.ones([32, 20, 20, 3])\n    #   filter_val = np.ones([20, 21, 3, 2])\n    #   self.evaluate(\n    #       nn_ops.conv2d(\n    #           input_val, filter_val, strides=[1, 1, 1, 1], padding=\"VALID\"))\n    # with self.assertRaisesRegex(ValueError, \"Negative dimension size\"):\n    #   input_val = np.ones([32, 20, 20, 3])\n    #   filter_val = np.ones([21, 20, 3, 2])\n    #   self.evaluate(\n    #       nn_ops.conv2d(\n    #           input_val, filter_val, strides=[1, 1, 1, 1], padding=\"VALID\"))\n    #\n    # # Filter larger than input + padding.\n    # with self.assertRaisesRegex(ValueError, \"Negative dimension size\"):\n    #   input_val = np.ones([32, 20, 20, 3])\n    # filter_val = np.ones([24, 25, 3, 2])\n    #   self.evaluate(\n    #       nn_ops.conv2d(\n    #           input_val,\n    #           filter_val,\n    #           strides=[1, 1, 1, 1],\n    #           padding=[[0, 0], [2, 2], [2, 2], [0, 0]]))\n\n    # Filter dimensions must be greater than 0.\n    with self.assertRaisesRegex(\n        errors_impl.InvalidArgumentError, \"filter must not have zero elements\"\n        \"|has a non-positive dimension\"):\n      input_val = np.ones([1, 1, 1, 1])\n      filter_val = np.ones([1, 0, 1, 1])\n      self.evaluate(\n          nn_ops.conv2d(\n              input_val, filter_val, strides=[1, 1, 1, 1], padding=\"SAME\"))\n\n    # Negative padding during backprop.\n    with self.assertRaisesRegex(\n        errors_impl.InvalidArgumentError,\n        \"All elements of explicit_paddings must be nonnegative\"):\n      filter_val = np.ones([18, 18, 3, 2])\n      out_backprop_val = np.ones([32, 3, 2, 2])\n      self.evaluate(\n          nn_ops.conv2d_backprop_input([32, 20, 20, 3],\n                                       filter_val,\n                                       out_backprop_val,\n                                       strides=[1, 1, 1, 1],\n                                       padding=[[0, 0], [-1, 0], [0, 0], [0,\n                                                                          0]]))\n    with self.assertRaisesRegex(\n        errors_impl.InvalidArgumentError,\n        \"All elements of explicit_paddings must be nonnegative\"):\n      input_val = np.ones([32, 20, 20, 3])\n      out_backprop_val = np.ones([32, 3, 2, 2])\n      self.evaluate(\n          nn_ops.conv2d_backprop_filter(\n              input_val, [18, 18, 3, 2],\n              out_backprop_val,\n              strides=[1, 1, 1, 1],\n              padding=[[0, 0], [-1, 0], [0, 0], [0, 0]]))\n\n\n@test_util.run_all_without_tensor_float_32(\"Avoid TF32 conv on GPU\")\nclass DepthwiseConv2DTest(test.TestCase):\n\n  def _VerifyValues(self, tensor_in_sizes, filter_in_sizes, stride, padding,\n                    expected):\n    \"\"\"Verifies the output values of the convolution function.\n\n    Args:\n      tensor_in_sizes: Input tensor dimensions in [batch, input_rows,\n        input_cols, input_depth].\n      filter_in_sizes: Filter tensor dimensions in [filter_rows, filter_cols,\n        input_depth, depth_multiplier].\n      stride: Stride.\n      padding: Padding type.\n      expected: An array containing the expected operation outputs.\n    \"\"\"\n    total_size_1 = 1\n    total_size_2 = 1\n    for s in tensor_in_sizes:\n      total_size_1 *= s\n    for s in filter_in_sizes:\n      total_size_2 *= s\n    # Initializes the input tensor with array containing incrementing\n    # numbers from 1.\n    x1 = [f * 1.0 for f in range(1, total_size_1 + 1)]\n    x2 = [f * 1.0 for f in range(1, total_size_2 + 1)]\n    with self.cached_session() as sess:\n      t1 = constant_op.constant(x1, shape=tensor_in_sizes)\n      t1.set_shape(tensor_in_sizes)\n      t2 = constant_op.constant(x2, shape=filter_in_sizes)\n      conv = nn_impl.depthwise_conv2d(\n          t1, t2, strides=[1, stride, stride, 1], padding=padding)\n      value = self.evaluate(conv)\n    tf_logging.debug(\"value = %s\", value)\n    self.assertArrayNear(expected, np.ravel(value), 1e-5)\n    self.assertShapeEqual(value, conv)\n\n  def testConv2D2x2Filter(self):\n    # The inputs look like this (it's a 3 x 2 matrix, each of depth 2):\n    #\n    # [ (1.0, 2.0), (3.0,  4.0), ( 5.0,  6.0) ]\n    # [ (7.0, 8.0), (9.0, 10.0), (11.0, 12.0) ]\n    #  We can view this as two inputs\n    #\n    #  input depth 0:\n    #\n    #  [ 1.0,  3.0,  5.0 ]\n    #  [ 7.0,  9.0, 11.0 ]\n    #\n    #  input depth 1:\n    #\n    #  [ 2.0,  4.0,  6.0 ]\n    #  [ 8.0, 10.0, 12.0 ]\n    #\n    # The filter looks like this (it has two 2 x 2 patches, each generating 2\n    # depths):\n    #\n    #  filter #0:\n    #\n    #  [ (1.0,  3.0), ( 5.0,  7.0)]\n    #  [ (9.0, 11.0), (13.0, 15.0)]\n    #\n    #  filter #1:\n    #\n    #  [ ( 2.0,  4.0), ( 6.0,  8.0)]\n    #  [ (10.0, 12.0), (14.0, 16.0)]\n    #\n    # So the outputs are:\n    #\n    # (position 0, 0: in_depth 0, output_depth 0 -- using filter #0)\n    #  1.0 * 1.0 + 7.0 * 9.0 + 3.0 * 5.0 + 9.0 * 13.0 = 196\n    # (position 0, 0: in_depth 0, output_depth 1 -- using filter #1)\n    #  1.0 * 2.0 + 7.0 * 10.0 + 3.0 * 6.0 + 9.0 * 14.0 = 216\n    # (position 0, 0: in_depth 1, output_depth 2 -- using filter #0)\n    #  2.0 * 3.0 + 8.0 * 11.0 + 4.0 * 7.0 + 10.0 * 15.0 = 272\n    # (position 0, 0: in_depth 1, output_depth 3 -- using filter #1)\n    #  2.0 * 4.0 + 8.0 * 12.0 + 4.0 * 8.0 + 10.0 * 16.0 = 296\n    #\n    # (position 1, 0: in_depth 0, output_depth 0 -- using filter #0)\n    #  3.0 * 1.0 + 9.0 * 9.0 + 5.0 * 5.0 + 11.0 * 13.0 = 252\n    # (position 1, 0: in_depth 0, output_depth 1 -- using filter #1)\n    #  3.0 * 2.0 + 9.0 * 10.0 + 5.0 * 6.0 + 11.0 * 14.0 = 280\n    # (position 1, 0: in_depth 1, output_depth 2 -- using filter #0)\n    #  4.0 * 3.0 + 10.0 * 11.0 + 6.0 * 7.0 + 12.0 * 15.0 = 344\n    # (position 1, 0: in_depth 1, output_depth 3 -- using filter #1)\n    #  4.0 * 4.0 + 10.0 * 12.0 + 6.0 * 8.0 + 12.0 * 16.0 = 376\n    expected_output = [196, 216, 272, 296, 252, 280, 344, 376]\n    self._VerifyValues(\n        tensor_in_sizes=[1, 2, 3, 2],\n        filter_in_sizes=[2, 2, 2, 2],\n        stride=1,\n        padding=\"VALID\",\n        expected=expected_output)\n\n\n@test_util.run_all_without_tensor_float_32(\"Avoid TF32 conv on GPU\")\nclass SeparableConv2DTest(test.TestCase):\n\n  def _InitValues(self, sizes):\n    \"\"\"Initializes values for input tensors.\n\n    Args:\n      sizes: Tensor dimensions.\n\n    Returns:\n      Tensor initialized to values.\n    \"\"\"\n    total_size = 1\n    for s in sizes:\n      total_size *= s\n    x = [f * 0.5 for f in range(1, total_size + 1)]\n    return constant_op.constant(x, shape=sizes)\n\n  def _VerifyValues(self,\n                    tensor_in_sizes,\n                    depthwise_filter_in_sizes,\n                    pointwise_filter_in_sizes,\n                    stride,\n                    padding,\n                    expected,\n                    data_format=\"NHWC\"):\n    \"\"\"Verifies the output values of the separable convolution function.\n\n    Args:\n      tensor_in_sizes: Input tensor dimensions.\n      depthwise_filter_in_sizes: Depthwise filter tensor dimensions.\n      pointwise_filter_in_sizes: Pointwise filter tensor dimensions.\n      stride: Stride.\n      padding: Padding type.\n      expected: An array containing the expected operation outputs.\n      data_format: string data format for input tensor.\n    \"\"\"\n    with self.cached_session():\n      t1 = self._InitValues(tensor_in_sizes)\n      f1 = self._InitValues(depthwise_filter_in_sizes)\n      f1.set_shape(depthwise_filter_in_sizes)\n      f2 = self._InitValues(pointwise_filter_in_sizes)\n\n      real_t1 = t1\n      strides = [1, stride, stride, 1]\n      if data_format == \"NCHW\":\n        real_t1 = array_ops.transpose(t1, [0, 3, 1, 2])\n        strides = [1, 1, stride, stride]\n        if isinstance(padding, list):\n          padding = [padding[0], padding[3], padding[1], padding[2]]\n\n      conv = nn_impl.separable_conv2d(\n          real_t1,\n          f1,\n          f2,\n          strides=strides,\n          padding=padding,\n          data_format=data_format)\n\n      if data_format == \"NCHW\":\n        conv = array_ops.transpose(conv, [0, 2, 3, 1])\n\n      value = self.evaluate(conv)\n    tf_logging.debug(\"value = %s\", value)\n    self.assertArrayNear(expected, np.ravel(value), 2e-3)\n    self.assertShapeEqual(value, conv)\n\n  def _testSeparableConv2D(self, data_format):\n    # The output is the result of two convolutions:\n    # First with tensor_in[1, 4, 4, 2] * filter1[2, 2, 2, 3].\n    # Second with intermediate_out[1, 4, 4, 6] * filter2[1, 1, 6, 7].\n    # Complexity is O(2*3*2*2 + 6*7*1*1) as opposed to O(2*7*2*2).\n    expected_output = [\n        6644.5, 6971.5, 7298.5, 7625.5, 7952.5, 8279.5, 8606.5, 8154.5, 8556.5,\n        8958.5, 9360.5, 9762.5, 10164.5, 10566.5, 9664.5, 10141.5, 10618.5,\n        11095.5, 11572.5, 12049.5, 12526.5, 4145.5, 4346.5, 4547.5, 4748.5,\n        4949.5, 5150.5, 5351.5, 12684.5, 13311.5, 13938.5, 14565.5, 15192.5,\n        15819.5, 16446.5, 14194.5, 14896.5, 15598.5, 16300.5, 17002.5, 17704.5,\n        18406.5, 15704.5, 16481.5, 17258.5, 18035.5, 18812.5, 19589.5, 20366.5,\n        6499.5, 6814.5, 7129.5, 7444.5, 7759.5, 8074.5, 8389.5, 18724.5,\n        19651.5, 20578.5, 21505.5, 22432.5, 23359.5, 24286.5, 20234.5, 21236.5,\n        22238.5, 23240.5, 24242.5, 25244.5, 26246.5, 21744.5, 22821.5, 23898.5,\n        24975.5, 26052.5, 27129.5, 28206.5, 8853.5, 9282.5, 9711.5, 10140.5,\n        10569.5, 10998.5, 11427.5, 5746.75, 6010.75, 6274.75, 6538.75, 6802.75,\n        7066.75, 7330.75, 6168.75, 6452.25, 6735.75, 7019.25, 7302.75, 7586.25,\n        7869.75, 6590.75, 6893.75, 7196.75, 7499.75, 7802.75, 8105.75, 8408.75,\n        2036.25, 2119.5, 2202.75, 2286.0, 2369.25, 2452.5, 2535.75\n    ]\n\n    self._VerifyValues(\n        tensor_in_sizes=[1, 4, 4, 2],\n        depthwise_filter_in_sizes=[2, 2, 2, 3],\n        pointwise_filter_in_sizes=[1, 1, 6, 7],\n        stride=1,\n        padding=\"SAME\",\n        expected=expected_output,\n        data_format=data_format)\n\n  def testSeparableConv2D(self):\n    self._testSeparableConv2D(\"NHWC\")\n\n  def disabledtestSeparableConv2DNCHW(self):\n    if not test.is_gpu_available():\n      return\n    self._testSeparableConv2D(\"NCHW\")\n\n  def _testSeparableConv2DEqualInputOutputDepth(self, data_format):\n    # The output is the result of two convolutions:\n    # First with tensor_in[1, 4, 4, 2] * filter1[2, 2, 3, 3].\n    # Second with intermediate_out[1, 4, 4, 6] * filter2[1, 1, 6, 6].\n    # Complexity is O(2*3*2*2 + 6*6*1*1) as opposed to O(2*6*2*2).\n    expected_output = [\n        5742.0, 6069.0, 6396.0, 6723.0, 7050.0, 7377.0, 7047.0, 7449.0, 7851.0,\n        8253.0, 8655.0, 9057.0, 8352.0, 8829.0, 9306.0, 9783.0, 10260.0,\n        10737.0, 3582.0, 3783.0, 3984.0, 4185.0, 4386.0, 4587.0, 10962.0,\n        11589.0, 12216.0, 12843.0, 13470.0, 14097.0, 12267.0, 12969.0, 13671.0,\n        14373.0, 15075.0, 15777.0, 13572.0, 14349.0, 15126.0, 15903.0, 16680.0,\n        17457.0, 5616.0, 5931.0, 6246.0, 6561.0, 6876.0, 7191.0, 16182.0,\n        17109.0, 18036.0, 18963.0, 19890.0, 20817.0, 17487.0, 18489.0, 19491.0,\n        20493.0, 21495.0, 22497.0, 18792.0, 19869.0, 20946.0, 22023.0, 23100.0,\n        24177.0, 7650.0, 8079.0, 8508.0, 8937.0, 9366.0, 9795.0, 4963.5, 5227.5,\n        5491.5, 5755.5, 6019.5, 6283.5, 5328.0, 5611.5, 5895.0, 6178.5, 6462.0,\n        6745.5, 5692.5, 5995.5, 6298.5, 6601.5, 6904.5, 7207.5, 1757.25, 1840.5,\n        1923.75, 2007.0, 2090.25, 2173.5\n    ]\n\n    self._VerifyValues(\n        tensor_in_sizes=[1, 4, 4, 2],\n        depthwise_filter_in_sizes=[2, 2, 2, 3],\n        pointwise_filter_in_sizes=[1, 1, 6, 6],\n        stride=1,\n        padding=\"SAME\",\n        expected=expected_output,\n        data_format=data_format)\n\n  @test_util.deprecated_graph_mode_only\n  def testSeparableConv2DEqualInputOutputDepth(self):\n    self._testSeparableConv2DEqualInputOutputDepth(\"NHWC\")\n\n  def testSeparableConv2DEqualInputOutputDepthNCHW(self):\n    if not test.is_gpu_available():\n      return\n    self._testSeparableConv2DEqualInputOutputDepth(\"NCHW\")\n\n  def _testSeparableConv2dExplicitPadding(self, data_format):\n    tensor_in_sizes = [1, 4, 4, 2]\n    depthwise_filter_in_sizes = [2, 2, 2, 3]\n    pointwise_filter_in_sizes = [1, 1, 6, 7]\n    padding = [[0, 0], [1, 2], [3, 4], [0, 0]]\n    with self.cached_session():\n      # Compute the 'expected' values by manually padding before calling\n      # separable_conv2d\n      t1 = self._InitValues(tensor_in_sizes)\n      t1 = array_ops.pad(t1, padding)\n      f1 = self._InitValues(depthwise_filter_in_sizes)\n      f1.set_shape(depthwise_filter_in_sizes)\n      f2 = self._InitValues(pointwise_filter_in_sizes)\n      conv = nn_impl.separable_conv2d(\n          t1,\n          f1,\n          f2,\n          strides=[1, 1, 1, 1],\n          padding=\"VALID\",\n          data_format=\"NHWC\")\n      expected = self.evaluate(conv)\n      expected = np.ravel(expected)\n    self._VerifyValues(\n        tensor_in_sizes=tensor_in_sizes,\n        depthwise_filter_in_sizes=depthwise_filter_in_sizes,\n        pointwise_filter_in_sizes=pointwise_filter_in_sizes,\n        stride=1,\n        padding=padding,\n        expected=expected,\n        data_format=data_format)\n\n  def testSeparableConv2dExplicitPadding(self):\n    self._testSeparableConv2dExplicitPadding(\"NHWC\")\n\n  def testSeparableConv2dExplicitPaddingNCHW(self):\n    if not test.is_gpu_available():\n      return\n    self._testSeparableConv2dExplicitPadding(\"NCHW\")\n\n\n@test_util.run_all_without_tensor_float_32(\"Avoid TF32 conv on GPU\")\nclass DeepConv2DTest(test.TestCase):\n\n  def _CompareFwdConv2D(self, tensor_in_sizes, filter_in_sizes, conv_strides,\n                        padding):\n    \"\"\"Verifies that DeepConv2D and Conv2D produce the same values.\n\n    Args:\n      tensor_in_sizes: Input tensor dimensions in\n        [batch, input_rows, input_cols, input_depth].\n      filter_in_sizes: Filter tensor dimensions in\n        [kernel_rows, kernel_cols, input_depth, output_depth].\n      conv_strides: [row_stride, col_stride] for the convolution;\n      padding: Padding type.\n    \"\"\"\n    x1 = np.random.rand(*tensor_in_sizes).astype(np.float32)\n    x2 = np.random.rand(*filter_in_sizes).astype(np.float32)\n\n    with self.cached_session(use_gpu=False) as sess:\n      t1 = constant_op.constant(x1, shape=tensor_in_sizes)\n      t2 = constant_op.constant(x2, shape=filter_in_sizes)\n      strides = [1] + conv_strides + [1]\n\n      conv = nn_ops.conv2d(t1, t2, strides=strides, padding=padding)\n\n      os.environ[\"TF_USE_DEEP_CONV2D\"] = \"0\"\n      values_expect = self.evaluate([conv])\n\n      os.environ[\"TF_USE_DEEP_CONV2D\"] = \"1\"\n      values_test = self.evaluate([conv])\n\n      self.assertAllClose(values_expect, values_test, rtol=1e-5, atol=1e-5)\n\n  def _RunTestCases(self, conv_strides, padding):\n    input_sizes = [[5, 5, 5, 1248], [3, 17, 17, 192], [2, 35, 35, 288],\n                   [2, 6, 8, 517], [2, 7, 4, 81], [3, 11, 3, 77]]\n    filter_sizes = [[3, 3, 1248, 128], [3, 3, 192, 192], [3, 3, 288, 384],\n                    [3, 3, 517, 64], [3, 3, 81, 77], [3, 3, 77, 181]]\n    for input_shape, filter_shape in zip(input_sizes, filter_sizes):\n      self._CompareFwdConv2D(input_shape, filter_shape, conv_strides, padding)\n\n  def testConv2D3x3FilterStride1x1Valid(self):\n    self._RunTestCases([1, 1], \"VALID\")\n\n  def testConv2D3x3FilterStride1x1Same(self):\n    self._RunTestCases([1, 1], \"SAME\")\n\n\nclass Conv2DBenchmark(test.Benchmark):\n\n  def benchmarkGPUConvStackFirst(self):\n    # Benchmark the first iteration of a conv-net with many identical conv\n    # operations.\n    if not test.is_gpu_available():\n      return\n\n    with ops.Graph().as_default(), session_lib.Session() as session:\n      batch_size = 1\n      timesteps = 600\n      features = 1\n\n      inputs = random_ops.random_uniform(\n          [batch_size, 1, timesteps, features], seed=1234)\n      num_outputs_list = [512] * 40 + [1]\n      kernel_w = 3\n      x = inputs\n      for num_outputs in num_outputs_list:\n        x = convolutional.conv2d(x, num_outputs, [1, kernel_w])\n      outputs = x\n\n      self.evaluate(variables.global_variables_initializer())\n      num_iterations = 4\n      for iter_index in range(num_iterations):\n        start = time.time()\n        session.run(outputs)\n        wall_time = time.time() - start\n        self.report_benchmark(\n            name=\"conv_stack_iter_%d\" % iter_index, wall_time=wall_time)\n        tf_logging.info(\"conv_stack_iter_%d: %.4f\" % (iter_index, wall_time))\n\n  def _bench_op(self, name, op, burn_iters, num_iters):\n    config = config_pb2.ConfigProto()\n    # Prevent Grappler from optimizing away the entire graph.\n    config.graph_options.rewrite_options.dependency_optimization = (\n        rewriter_config_pb2.RewriterConfig.OFF)\n    with session_lib.Session(config=config) as session:\n      self.evaluate(variables.global_variables_initializer())\n      self.run_op_benchmark(\n          session, op, burn_iters=burn_iters, min_iters=num_iters, name=name)\n\n  def benchmarkExplicitVsManualPadding(self):\n    \"\"\"Compare performance of EXPLICIT padding and calling tf.pad.\n\n    A Conv2D op with EXPLICIT padding is benchmarked, and a tf.pad with the same\n    padding followed by an equivalent Conv2D op is benchmarked.\n    \"\"\"\n    if not test.is_gpu_available():\n      return\n\n    with ops.Graph().as_default():\n      burn_iters = 15\n      num_iters = 300\n      batch_size = 64\n      # The input and filter correspond to the first layer of Resnet50.\n      input = variables.Variable(  # pylint: disable=redefined-builtin\n          random_ops.random_uniform([\n              batch_size,\n              3,\n              224,\n              224\n          ]))\n      filter = variables.Variable(random_ops.random_uniform([7, 7, 3, 64]))  # pylint: disable=redefined-builtin\n      strides = [1, 1, 2, 2]\n      padding = [(0, 0), (0, 0), (3, 3), (3, 3)]\n      output_explicit_pad = nn_ops.conv2d(\n          input, filter, strides, padding=padding, data_format=\"NCHW\")\n      input_padded = array_ops.pad(input, padding)\n      output_manual_pad = nn_ops.conv2d(\n          input_padded, filter, strides, padding=\"VALID\", data_format=\"NCHW\")\n      # Benchmark just the forward pass.\n      self._bench_op(\"explicit_pad_forward\", output_explicit_pad.op, burn_iters,\n                     num_iters)\n      self._bench_op(\"manual_pad_forward\", output_manual_pad.op, burn_iters,\n                     num_iters)\n\n      # Benchmark both the forward and backwards passes.\n      input_grad_explicit_pad, filter_grad_explicit_pad = (\n          gradients_impl.gradients(output_explicit_pad, [input, filter]))\n      self._bench_op(\n          \"explicit_pad_backward\",\n          control_flow_ops.group(input_grad_explicit_pad,\n                                 filter_grad_explicit_pad), burn_iters,\n          num_iters)\n      input_grad_manual_pad, filter_grad_manual_pad = gradients_impl.gradients(\n          output_manual_pad, [input, filter])\n      self._bench_op(\n          \"manual_pad_backward\",\n          control_flow_ops.group(input_grad_manual_pad, filter_grad_manual_pad),\n          burn_iters, num_iters)\n\n  def benchmarkExplicitVsSamePaddingGraph(self):\n    \"\"\"Compare performance of EXPLICIT and SAME padding in graph mode.\n\n    A Conv2D op with SAME padding is benchmarked, and an equivalent Conv2D op\n    with explicit padding is benchmarked, where the padding is the same as in\n    the SAME case. The purpose is to ensure EXPLICIT padding is just as\n    efficient as the SAME case\n    \"\"\"\n    if not test.is_gpu_available():\n      return\n\n    with ops.Graph().as_default():\n      burn_iters = 15\n      num_convs = 20\n      num_iters = 50\n      batch_size = 64\n      # The input and filter correspond to a middle layer of Resnet50.\n      input = variables.Variable(  # pylint: disable=redefined-builtin\n          random_ops.random_uniform([\n              batch_size,\n              256,\n              14,\n              14\n          ]))\n      filter = variables.Variable(random_ops.random_uniform([3, 3, 256, 256]))  # pylint: disable=redefined-builtin\n      strides = [1, 1, 1, 1]\n      padding = [(0, 0), (0, 0), (1, 1), (1, 1)]\n      output_explicit_pad = input\n      output_same_pad = input\n\n      for _ in range(num_convs):\n        output_explicit_pad = nn_ops.conv2d(\n            output_explicit_pad,\n            filter,\n            strides,\n            padding=padding,\n            data_format=\"NCHW\")\n        output_same_pad = nn_ops.conv2d(\n            output_same_pad,\n            filter,\n            strides,\n            padding=\"SAME\",\n            data_format=\"NCHW\")\n      grad_explicit_pad, = gradients_impl.gradients(output_explicit_pad, filter)\n      grad_same_pad, = gradients_impl.gradients(output_same_pad, filter)\n      self._bench_op(\"graph_explicit_pad\", grad_explicit_pad.op, burn_iters,\n                     num_iters)\n      self._bench_op(\"graph_same_pad\", grad_same_pad.op, burn_iters, num_iters)\n\n  def benchmarkExplicitVsSamePaddingEager(self):\n    \"\"\"Compare performance of EXPLICIT and SAME padding in eager mode.\n\n    A Conv2D op with SAME padding is benchmarked, and an equivalent Conv2D op\n    with explicit padding is benchmarked, where the padding is the same as in\n    the SAME case. Currently, EXPLICIT padding is slightly slower, due to the\n    fact the Python padding list must be checked and processed before the Conv2D\n    op can run.\n    \"\"\"\n    # TODO(reedwm): Make EXPLICIT padding as fast as SAME padding.\n    if not test.is_gpu_available():\n      return\n\n    with context.eager_mode():\n      burn_iters = 15\n      num_convs = 20\n      num_iters = 50\n      batch_size = 64\n      # The input and filter correspond to a middle layer of Resnet50.\n      input = variables.Variable(  # pylint: disable=redefined-builtin\n          random_ops.random_uniform([\n              batch_size,\n              256,\n              14,\n              14\n          ]))\n      filter = variables.Variable(random_ops.random_uniform([3, 3, 256, 256]))  # pylint: disable=redefined-builtin\n      strides = [1, 1, 1, 1]\n      padding = [(0, 0), (0, 0), (1, 1), (1, 1)]\n      output_explicit_pad = input\n      output_same_pad = input\n      for _ in range(burn_iters):\n        output_explicit_pad = nn_ops.conv2d(\n            output_explicit_pad,\n            filter,\n            strides,\n            padding=padding,\n            data_format=\"NCHW\")\n        output_same_pad = nn_ops.conv2d(\n            output_same_pad,\n            filter,\n            strides,\n            padding=\"SAME\",\n            data_format=\"NCHW\")\n\n      start = time.time()\n      for _ in range(num_iters):\n        with backprop.GradientTape() as tape:\n          for _ in range(num_convs):\n            output_explicit_pad = nn_ops.conv2d(\n                output_explicit_pad,\n                filter,\n                strides,\n                padding=padding,\n                data_format=\"NCHW\")\n          tape.gradient(output_explicit_pad, filter)\n      end = time.time()\n      self.report_benchmark(\n          name=\"eager_explicit_pad\",\n          wall_time=(end - start) / num_iters,\n          iters=num_iters)\n\n      start = time.time()\n      for _ in range(num_iters):\n        with backprop.GradientTape() as tape:\n          for _ in range(num_convs):\n            output_same_pad = nn_ops.conv2d(\n                output_same_pad,\n                filter,\n                strides,\n                padding=\"SAME\",\n                data_format=\"NCHW\")\n          tape.gradient(output_same_pad, filter)\n      end = time.time()\n      self.report_benchmark(\n          name=\"eager_same_pad\",\n          wall_time=(end - start) / num_iters,\n          iters=num_iters)\n\n\ndef GetInceptionFwdTest(input_size, filter_size, stride, padding,\n                        gpu_only=False):\n\n  def Test(self):\n    if gpu_only and not test.is_gpu_available():\n      tf_logging.info(\"Skipping InceptionFwd %s\", (input_size, filter_size,\n                                                   stride, padding))\n      return\n    tf_logging.info(\"Testing InceptionFwd %s\", (input_size, filter_size, stride,\n                                                padding))\n    self._CompareFwdValues(input_size, filter_size, [stride, stride], padding)\n\n  return Test\n\n\ndef GetInceptionFwdDilatedConvTest(input_size, filter_size, stride, padding):\n\n  def Test(self):\n    if stride == 1:\n      tf_logging.info(\"Testing InceptionFwd with dilations %s\",\n                      (input_size, filter_size, stride, padding))\n      self._VerifyDilatedConvValues(\n          tensor_in_sizes=input_size,\n          filter_in_sizes=filter_size,\n          strides=[stride, stride],\n          dilations=[2, 2],\n          padding=padding,\n          rtol=5e-4)\n\n  return Test\n\n\ndef GetInceptionBackInputTest(input_size, filter_size, output_size, stride,\n                              padding,\n                              gpu_only=False):\n\n  def Test(self):\n    if gpu_only and not test.is_gpu_available():\n      tf_logging.info(\"Skipping InceptionBackInput %s\",\n                      (input_size, filter_size, output_size, stride, padding))\n      return\n    tf_logging.info(\"Testing InceptionBackInput %s\",\n                    (input_size, filter_size, output_size, stride, padding))\n    self._CompareBackpropInput(input_size, filter_size, output_size,\n                               [stride, stride], padding)\n\n  return Test\n\n\ndef GetInceptionBackFilterTest(input_size, filter_size, output_size, strides,\n                               padding, gpu_only=False):\n\n  def Test(self):\n    if gpu_only and not test.is_gpu_available():\n      tf_logging.info(\"Skipping InceptionBackFilter %s\",\n                      (input_size, filter_size, output_size, strides, padding))\n      return\n    tf_logging.info(\"Testing InceptionBackFilter %s\",\n                    (input_size, filter_size, output_size, strides, padding))\n    self._CompareBackFilter(input_size, filter_size, output_size, strides,\n                            padding)\n\n  return Test\n\n\n@test_util.run_all_without_tensor_float_32(\"Avoid TF32 conv on GPU\")\nclass FusedConv2DTest(test.TestCase):\n\n  def _CreateNumpyTensor(self, shape):\n    total_size = np.prod(shape)\n    return np.arange(1, total_size + 1, dtype=np.float32).reshape(shape)\n\n  def _CreateConv2D(self,\n                    input_values,\n                    filters,\n                    strides=[1, 1],\n                    padding=\"SAME\"):\n    return nn_ops.convolution(\n        input_values, filters, strides=strides, padding=padding)\n\n  # Tests tensor forwarding of a fused Conv2D+BiasAdd+Add op when the input to\n  # Add has refcount 1.\n  @test_util.run_in_graph_and_eager_modes(use_gpu=False)\n  def testAddWithRefCountOne(self):\n    expected_output = [\n        113377, 125570, 77305, 86738, 19433, 22226, 60681, 70722, 36291, 43718,\n        7143, 9206, 9785, 12098, 4783, 6366, 779, 1134\n    ]\n    tensor_in_sizes = [1, 3, 3, 2]\n    filter_in_sizes = [2, 2, 2, 2]\n    bias_in_sizes = [2]\n\n    x = self._CreateNumpyTensor(tensor_in_sizes)\n    filter_in = self._CreateNumpyTensor(filter_in_sizes)\n    bias_in = self._CreateNumpyTensor(bias_in_sizes)\n    # To get different weights for filter\n    offset = 1\n\n    conv1 = self._CreateConv2D(x, filter_in)\n    conv2 = self._CreateConv2D(conv1, filter_in + offset)\n\n    conv = self._CreateConv2D(conv1, filter_in - offset)\n    bias_add = nn_ops.bias_add(conv, bias_in)\n    add = math_ops.add_n([bias_add, conv2])\n\n    self.assertAllEqual(\n        np.rint(expected_output),\n        self.evaluate(add).reshape(-1))\n\n  # Tests tensor forwarding of a fused Conv2D+BiasAdd+Add op when the input to\n  # Add has a total refcount of 2, and Add is its last consumer.\n  @test_util.run_in_graph_and_eager_modes(use_gpu=False)\n  def testAddWithRefCountTwoAndRunAddLast(self):\n    expected_output = [\n        1.907175e+06, 2.253505e+06, 7.809210e+05, 9.537180e+05, 1.184170e+05,\n        1.523070e+05, 5.367010e+05, 6.803700e+05, 1.867090e+05, 2.529460e+05,\n        2.362300e+04, 3.522600e+04, 5.121700e+04, 7.168300e+04, 1.494300e+04,\n        2.347400e+04, 1.558000e+03, 2.903000e+03\n    ]\n    tensor_in_sizes = [1, 3, 3, 2]\n    filter_in_sizes = [2, 2, 2, 2]\n    bias_in_sizes = [2]\n\n    x = self._CreateNumpyTensor(tensor_in_sizes)\n    filter_in = self._CreateNumpyTensor(filter_in_sizes)\n    bias_in = self._CreateNumpyTensor(bias_in_sizes)\n    # To get different weights for filter\n    offset = 1\n\n    conv1 = self._CreateConv2D(x, filter_in)\n    conv2 = self._CreateConv2D(conv1, filter_in + offset)\n\n    conv = self._CreateConv2D(conv2, filter_in - offset)\n    bias_add = nn_ops.bias_add(conv, bias_in)\n    add = math_ops.add_n([bias_add, conv1])\n\n    self.assertAllEqual(\n        np.rint(expected_output),\n        self.evaluate(add).reshape(-1))\n\n  # Tests tensor forwarding of a fused Conv2D+BiasAdd+Add op when the input to\n  # Add has refcount 2 and Add (in the fused Conv2D op) is its first consumer.\n  @test_util.run_in_graph_and_eager_modes(use_gpu=False)\n  def testAddWithRefCountTwoAndRunAddFirst(self):\n    expected_output = [\n        176161, 194450, 120673, 134822, 30545, 34734, 96041, 111102, 58149,\n        69289, 11745, 14839, 15833, 19302, 7965, 10339, 1345, 1877\n    ]\n    tensor_in_sizes = [1, 3, 3, 2]\n    filter_in_sizes = [2, 2, 2, 2]\n    bias_in_sizes = [2]\n\n    x = self._CreateNumpyTensor(tensor_in_sizes)\n    filter_in = self._CreateNumpyTensor(filter_in_sizes)\n    bias_in = self._CreateNumpyTensor(bias_in_sizes)\n    # To get different weights for filter\n    offset = 1\n\n    conv1 = self._CreateConv2D(x, filter_in)\n    conv2 = self._CreateConv2D(conv1, filter_in + offset)\n\n    conv = self._CreateConv2D(conv1, filter_in - offset)\n    bias_add = nn_ops.bias_add(conv, bias_in)\n    add = math_ops.add_n([bias_add, conv2])\n\n    relu = nn_ops.relu(add)\n    output = math_ops.add_n([relu, conv2])\n\n    self.assertAllEqual(\n        np.rint(expected_output),\n        self.evaluate(output).reshape(-1))\n\n  # Tests tensor forwarding of a fused Conv2D+BiasAdd+Add op when the input to\n  # Add has refcount 2, and there is no dependency between its two consumers.\n  @test_util.run_in_graph_and_eager_modes(use_gpu=False)\n  def testAddWithRefCountTwoAndNoDependence(self):\n    expected_output = [\n        176161, 194450, 120673, 134822, 30545, 34734, 96041, 111102, 58149,\n        69289, 11745, 14839, 15833, 19302, 7965, 10339, 1345, 1877\n    ]\n    tensor_in_sizes = [1, 3, 3, 2]\n    filter_in_sizes = [2, 2, 2, 2]\n    bias_in_sizes = [2]\n\n    x = self._CreateNumpyTensor(tensor_in_sizes)\n    filter_in = self._CreateNumpyTensor(filter_in_sizes)\n    bias_in = self._CreateNumpyTensor(bias_in_sizes)\n    # To get different weights for filter\n    offset = 1\n\n    conv1 = self._CreateConv2D(x, filter_in)\n    conv2 = self._CreateConv2D(conv1, filter_in + offset)\n\n    conv = self._CreateConv2D(conv1, filter_in - offset)\n    bias_add = nn_ops.bias_add(conv, bias_in)\n    add = math_ops.add_n([bias_add, conv2])\n\n    relu1 = nn_ops.relu(add)\n    relu2 = nn_ops.relu(conv2)\n    output = math_ops.add_n([relu1, relu2])\n\n    self.assertAllEqual(\n        np.rint(expected_output),\n        self.evaluate(output).reshape(-1))\n\n  # Tests tensor forwarding of a fused Conv2D+BiasAdd+Add op when the input to\n  # Add is the same as the input to the fused Conv2D op and needs a tensor\n  # buffer.\n  @test_util.run_in_graph_and_eager_modes(use_gpu=False)\n  def testAddWithSameSrcAndAddTensorBuffer(self):\n    expected_output = [\n        57157, 63298, 39249, 44026, 9971, 11402, 31193, 36306, 19126, 22948,\n        3970, 5060, 5135, 6350, 2666, 3524, 461, 674\n    ]\n    tensor_in_sizes = [1, 3, 3, 2]\n    filter_in_sizes = [2, 2, 2, 2]\n    bias_in_sizes = [2]\n\n    x = self._CreateNumpyTensor(tensor_in_sizes)\n    filter_in = self._CreateNumpyTensor(filter_in_sizes)\n    bias_in = self._CreateNumpyTensor(bias_in_sizes)\n\n    conv1 = self._CreateConv2D(x, filter_in)\n\n    conv = self._CreateConv2D(conv1, filter_in)\n    bias_add = nn_ops.bias_add(conv, bias_in)\n    add = math_ops.add_n([bias_add, conv1])\n\n    self.assertAllEqual(\n        np.rint(expected_output),\n        self.evaluate(add).reshape(-1))\n\n\nif __name__ == \"__main__\":\n  for index, (input_size_, filter_size_, output_size_, stride_,\n              padding_) in enumerate(GetShrunkInceptionShapes()):\n    setattr(Conv2DTest, \"testInceptionFwd_\" + str(index),\n            test_util.run_in_graph_and_eager_modes(\n                GetInceptionFwdTest(input_size_, filter_size_, stride_,\n                                    padding_)))\n    setattr(\n        Conv2DTest, \"testInceptionFwdDilatedConv_\" + str(index),\n        test_util.run_in_graph_and_eager_modes(GetInceptionFwdDilatedConvTest(\n            input_size_, filter_size_, stride_, padding_)))\n    setattr(Conv2DTest, \"testInceptionBackInput_\" + str(index),\n            test_util.run_in_graph_and_eager_modes(\n                GetInceptionBackInputTest(input_size_, filter_size_,\n                                          output_size_, stride_, padding_)))\n    setattr(Conv2DTest, \"testInceptionBackFilter_\" + str(index),\n            test_util.run_in_graph_and_eager_modes(\n                GetInceptionBackFilterTest(input_size_, filter_size_,\n                                           output_size_, [stride_, stride_],\n                                           padding_)))\n\n  # TODO(b/35359731)\n  # Fwd, BckInput, and BackFilter to test that for certain input parameter\n  # set, winograd nonfused algorithm will be excluded from conv autotune. If\n  # in such case, winograd nonfused algorithm is added as one option of the\n  # conv autotune, and cuDNN version is smaller than 7, the following tests\n  # will fail.\n  ishape = [1, 400, 400, 1]\n  fshape = [1, 1, 1, 256]\n  oshape = [1, 400, 400, 256]\n  setattr(Conv2DTest, \"testInceptionFwd_No_Winograd_Nonfused\",\n          test_util.run_in_graph_and_eager_modes(\n              GetInceptionFwdTest(ishape, fshape, 1, \"SAME\", gpu_only=True)))\n  setattr(Conv2DTest, \"testInceptionFwdDilatedConv_No_Winograd_Nonfused\",\n          test_util.run_in_graph_and_eager_modes(\n              GetInceptionFwdDilatedConvTest(ishape, fshape, 1, \"SAME\")))\n  setattr(Conv2DTest, \"testInceptionBackInput_No_Winograd_Nonfused\",\n          test_util.run_in_graph_and_eager_modes(\n              GetInceptionBackInputTest(ishape, fshape, oshape, 1, \"SAME\",\n                                        gpu_only=True)))\n  setattr(Conv2DTest, \"testInceptionBackFilter_No_Winograd_Nonfused\",\n          test_util.run_in_graph_and_eager_modes(\n              GetInceptionBackFilterTest(ishape, fshape, oshape, [1, 1], \"SAME\",\n                                         gpu_only=True)))\n  test.main()\n"], "filenames": ["tensorflow/core/kernels/conv_ops.cc", "tensorflow/python/kernel_tests/nn_ops/conv_ops_test.py"], "buggy_code_start_loc": [46, 761], "buggy_code_end_loc": [700, 761], "fixing_code_start_loc": [47, 762], "fixing_code_end_loc": [711, 771], "type": "CWE-369", "message": "TensorFlow is an open source platform for machine learning. If `Conv2D` is given empty `input` and the `filter` and `padding` sizes are valid, the output is all-zeros. This causes division-by-zero floating point exceptions that can be used to trigger a denial of service attack. We have patched the issue in GitHub commit 611d80db29dd7b0cfb755772c69d60ae5bca05f9. The fix will be included in TensorFlow 2.10.0. We will also cherrypick this commit on TensorFlow 2.9.1, TensorFlow 2.8.1, and TensorFlow 2.7.2, as these are also affected and still in supported range. There are no known workarounds for this issue.", "other": {"cve": {"id": "CVE-2022-35996", "sourceIdentifier": "security-advisories@github.com", "published": "2022-09-16T23:15:10.407", "lastModified": "2022-09-20T14:49:57.820", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "TensorFlow is an open source platform for machine learning. If `Conv2D` is given empty `input` and the `filter` and `padding` sizes are valid, the output is all-zeros. This causes division-by-zero floating point exceptions that can be used to trigger a denial of service attack. We have patched the issue in GitHub commit 611d80db29dd7b0cfb755772c69d60ae5bca05f9. The fix will be included in TensorFlow 2.10.0. We will also cherrypick this commit on TensorFlow 2.9.1, TensorFlow 2.8.1, and TensorFlow 2.7.2, as these are also affected and still in supported range. There are no known workarounds for this issue."}, {"lang": "es", "value": "TensorFlow es una plataforma de c\u00f3digo abierto para el aprendizaje autom\u00e1tico. Si \"Conv2D\" recibe una \"entrada\" vac\u00eda y los tama\u00f1os de \"filtro\" y \"acolchado\" son v\u00e1lidos, la salida es todo ceros. Esto causa excepciones de divisi\u00f3n por cero en coma flotante que pueden ser usadas para desencadenar un ataque de denegaci\u00f3n de servicio. Hemos parcheado el problema en el commit 611d80db29dd7b0cfb755772c69d60ae5bca05f9 de GitHub. La correcci\u00f3n ser\u00e1 incluida en TensorFlow versi\u00f3n 2.10.0. Tambi\u00e9n seleccionaremos este compromiso en TensorFlow versi\u00f3n 2.9.1, TensorFlow versi\u00f3n 2.8.1, y TensorFlow versi\u00f3n 2.7.2, ya que estos tambi\u00e9n est\u00e1n afectados y todav\u00eda est\u00e1n en el rango admitido. No se presentan mitigaciones conocidas para este problema"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 7.5, "baseSeverity": "HIGH"}, "exploitabilityScore": 3.9, "impactScore": 3.6}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:H/PR:N/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "HIGH", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.9, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 2.2, "impactScore": 3.6}]}, "weaknesses": [{"source": "security-advisories@github.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-369"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionEndExcluding": "2.7.2", "matchCriteriaId": "C6622D95-1C86-45C5-AB55-E6EEEA0996DF"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.8.0", "versionEndExcluding": "2.8.1", "matchCriteriaId": "0F9D273D-02DC-441E-AA91-EAC8DEAA4B44"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.9.0", "versionEndExcluding": "2.9.1", "matchCriteriaId": "FE4F8A81-6CC2-4F7F-9602-C170FDD926E7"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.10:rc0:*:*:*:*:*:*", "matchCriteriaId": "1DBFBCE2-0A01-4575-BE45-6775ABFB8B28"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.10:rc1:*:*:*:*:*:*", "matchCriteriaId": "89806CF9-E423-4CA6-A01A-8175C260CB24"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.10:rc2:*:*:*:*:*:*", "matchCriteriaId": "F2B80690-A257-4E16-BD27-9AE045BC56ED"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.10:rc3:*:*:*:*:*:*", "matchCriteriaId": "F335F9A4-5AB8-4E53-BC18-E01F7C653E5E"}]}]}], "references": [{"url": "https://github.com/tensorflow/tensorflow/commit/611d80db29dd7b0cfb755772c69d60ae5bca05f9", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/security/advisories/GHSA-q5jv-m6qw-5g37", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/tensorflow/tensorflow/commit/611d80db29dd7b0cfb755772c69d60ae5bca05f9"}}