{"buggy_code": ["/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include \"tensorflow/lite/kernels/internal/optimized/integer_ops/depthwise_conv.h\"\n\n#include <stddef.h>\n#include <stdint.h>\n\n#include <vector>\n\n#include \"tensorflow/lite/c/builtin_op_data.h\"\n#include \"tensorflow/lite/c/common.h\"\n#include \"tensorflow/lite/kernels/cpu_backend_context.h\"\n#include \"tensorflow/lite/kernels/internal/compatibility.h\"\n#include \"tensorflow/lite/kernels/internal/optimized/cpu_check.h\"\n#include \"tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h\"\n#include \"tensorflow/lite/kernels/internal/optimized/integer_ops/depthwise_conv_hybrid.h\"\n#include \"tensorflow/lite/kernels/internal/optimized/neon_check.h\"\n#include \"tensorflow/lite/kernels/internal/quantization_util.h\"\n#include \"tensorflow/lite/kernels/internal/reference/depthwiseconv_float.h\"\n#include \"tensorflow/lite/kernels/internal/reference/depthwiseconv_uint8.h\"\n#include \"tensorflow/lite/kernels/internal/reference/integer_ops/depthwise_conv.h\"\n#include \"tensorflow/lite/kernels/internal/tensor.h\"\n#include \"tensorflow/lite/kernels/internal/tensor_ctypes.h\"\n#include \"tensorflow/lite/kernels/internal/tensor_utils.h\"\n#include \"tensorflow/lite/kernels/internal/types.h\"\n#include \"tensorflow/lite/kernels/kernel_util.h\"\n#include \"tensorflow/lite/kernels/padding.h\"\n\nnamespace tflite {\nnamespace ops {\nnamespace builtin {\nnamespace depthwise_conv {\n\nconstexpr int kInputTensor = 0;\nconstexpr int kFilterTensor = 1;\nconstexpr int kBiasTensor = 2;\nconstexpr int kOutputTensor = 0;\n\n// This file has three implementation of DepthwiseConv.\nenum KernelType {\n  kReference,\n  kGenericOptimized,  // Neon-free\n  kNeonOptimized,\n};\n\nconst int kTensorNotAllocated = -1;\n\nstruct OpData {\n  TfLitePaddingValues padding;\n  // The scaling factor from input to output (aka the 'real multiplier') can\n  // be represented as a fixed point multiplier plus a left shift.\n  int32_t output_multiplier;\n  int output_shift;\n  // The range of the fused activation layer. For example for kNone and\n  // uint8_t these would be 0 and 255.\n  int32_t output_activation_min;\n  int32_t output_activation_max;\n\n  // Per channel output multiplier and shift.\n  std::vector<int32_t> per_channel_output_multiplier;\n  std::vector<int> per_channel_output_shift;\n\n  // Hybrid per channel temporary tensors.\n  int input_quantized_id = kTensorNotAllocated;\n  int scaling_factors_id = kTensorNotAllocated;\n  int input_offset_id = kTensorNotAllocated;\n  int32_t input_quantized_index;\n  int32_t scaling_factors_index;\n  int32_t input_offset_index;\n};\n\nvoid* Init(TfLiteContext* context, const char* buffer, size_t length) {\n  // This is a builtin op, so we don't use the contents in 'buffer', if any.\n  // Instead, we allocate a new object to carry information from Prepare() to\n  // Eval().\n  return new OpData;\n}\n\nvoid Free(TfLiteContext* context, void* buffer) {\n  delete reinterpret_cast<OpData*>(buffer);\n}\n\nTfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n  auto* params =\n      reinterpret_cast<TfLiteDepthwiseConvParams*>(node->builtin_data);\n  OpData* data = reinterpret_cast<OpData*>(node->user_data);\n\n  bool has_bias = NumInputs(node) == 3;\n\n  TF_LITE_ENSURE(context, has_bias || NumInputs(node) == 2);\n  const TfLiteTensor* input;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInputTensor, &input));\n  const TfLiteTensor* filter;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kFilterTensor, &filter));\n  const TfLiteTensor* bias = nullptr;\n\n  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context,\n                    GetOutputSafe(context, node, kOutputTensor, &output));\n\n  TF_LITE_ENSURE_EQ(context, NumDimensions(input), 4);\n  TF_LITE_ENSURE_EQ(context, NumDimensions(filter), 4);\n\n  const TfLiteType data_type = input->type;\n\n  const TfLiteType filter_type = filter->type;\n  const bool is_hybrid =\n      data_type == kTfLiteFloat32 && filter_type == kTfLiteInt8;\n  TF_LITE_ENSURE(context,\n                 data_type == kTfLiteFloat32 || data_type == kTfLiteUInt8 ||\n                     data_type == kTfLiteInt8 || data_type == kTfLiteInt16);\n  TF_LITE_ENSURE_TYPES_EQ(context, output->type, data_type);\n  if (!is_hybrid) {\n    TF_LITE_ENSURE(context,\n                   filter->type == data_type || data_type == kTfLiteInt16);\n  }\n\n  if (data_type == kTfLiteInt16) {\n    TF_LITE_ENSURE_EQ(context, input->params.zero_point, 0);\n    TF_LITE_ENSURE_EQ(context, output->params.zero_point, 0);\n  }\n\n  // Filter in DepthwiseConv is expected to be [1, H, W, O].\n  TF_LITE_ENSURE_EQ(context, SizeOfDimension(filter, 0), 1);\n\n  if (has_bias) {\n    TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kBiasTensor, &bias));\n    if (data_type == kTfLiteUInt8 || data_type == kTfLiteInt8) {\n      TF_LITE_ENSURE_TYPES_EQ(context, bias->type, kTfLiteInt32);\n      TF_LITE_ENSURE_EQ(context, bias->params.zero_point, 0);\n    } else if (data_type == kTfLiteInt16) {\n      TF_LITE_ENSURE_TYPES_EQ(context, bias->type, kTfLiteInt64);\n      TF_LITE_ENSURE_EQ(context, bias->params.zero_point, 0);\n    } else {\n      TF_LITE_ENSURE_TYPES_EQ(context, bias->type, data_type);\n    }\n    TF_LITE_ENSURE_EQ(context, NumDimensions(bias), 1);\n    TF_LITE_ENSURE_EQ(context, SizeOfDimension(filter, 3),\n                      SizeOfDimension(bias, 0));\n  }\n\n  int channels_out = SizeOfDimension(filter, 3);\n  int width = SizeOfDimension(input, 2);\n  int height = SizeOfDimension(input, 1);\n  int filter_width = SizeOfDimension(filter, 2);\n  int filter_height = SizeOfDimension(filter, 1);\n  int batches = SizeOfDimension(input, 0);\n\n  // Matching GetWindowedOutputSize in TensorFlow.\n  auto padding = params->padding;\n  int out_width, out_height;\n\n  data->padding = ComputePaddingHeightWidth(\n      params->stride_height, params->stride_width,\n      params->dilation_height_factor, params->dilation_width_factor, height,\n      width, filter_height, filter_width, padding, &out_height, &out_width);\n\n  // Note that quantized inference requires that all tensors have their\n  // parameters set. This is usually done during quantized training or\n  // calibration.\n  if (data_type != kTfLiteFloat32) {\n    TF_LITE_ENSURE_EQ(context, filter->quantization.type,\n                      kTfLiteAffineQuantization);\n    TF_LITE_ENSURE(context, filter->quantization.type != kTfLiteNoQuantization);\n    const auto* affine_quantization =\n        reinterpret_cast<TfLiteAffineQuantization*>(\n            filter->quantization.params);\n    TF_LITE_ENSURE(context, affine_quantization);\n    TF_LITE_ENSURE(context, affine_quantization->scale);\n    TF_LITE_ENSURE(context, (affine_quantization->scale->size == 1 ||\n                             affine_quantization->scale->size == channels_out));\n\n    data->per_channel_output_multiplier.resize(channels_out);\n    data->per_channel_output_shift.resize(channels_out);\n    TF_LITE_ENSURE_STATUS(tflite::PopulateConvolutionQuantizationParams(\n        context, input, filter, bias, output, params->activation,\n        &data->output_multiplier, &data->output_shift,\n        &data->output_activation_min, &data->output_activation_max,\n        data->per_channel_output_multiplier.data(),\n        data->per_channel_output_shift.data(), channels_out));\n  }\n\n  if (is_hybrid) {\n    TF_LITE_ENSURE(context, filter->quantization.type != kTfLiteNoQuantization);\n    const auto* affine_quantization =\n        reinterpret_cast<TfLiteAffineQuantization*>(\n            filter->quantization.params);\n    TF_LITE_ENSURE(context, affine_quantization);\n    TF_LITE_ENSURE(context, affine_quantization->scale);\n    TF_LITE_ENSURE_EQ(\n        context, affine_quantization->scale->size,\n        filter->dims->data[affine_quantization->quantized_dimension]);\n\n    int temporaries_count = 0;\n    data->input_quantized_index = temporaries_count;\n    if (data->input_quantized_id == kTensorNotAllocated) {\n      TF_LITE_ENSURE_OK(\n          context, context->AddTensors(context, 1, &data->input_quantized_id));\n    }\n    ++temporaries_count;\n    data->scaling_factors_index = temporaries_count;\n    if (data->scaling_factors_id == kTensorNotAllocated) {\n      TF_LITE_ENSURE_OK(\n          context, context->AddTensors(context, 1, &data->scaling_factors_id));\n    }\n    ++temporaries_count;\n    data->input_offset_index = temporaries_count;\n    if (data->input_offset_id == kTensorNotAllocated) {\n      TF_LITE_ENSURE_OK(\n          context, context->AddTensors(context, 1, &data->input_offset_id));\n    }\n    ++temporaries_count;\n\n    TfLiteIntArrayFree(node->temporaries);\n    node->temporaries = TfLiteIntArrayCreate(temporaries_count);\n\n    node->temporaries->data[data->input_quantized_index] =\n        data->input_quantized_id;\n    TfLiteTensor* input_quantized;\n    TF_LITE_ENSURE_OK(\n        context, GetTemporarySafe(context, node, data->input_quantized_index,\n                                  &input_quantized));\n    input_quantized->type = kTfLiteInt8;\n    input_quantized->allocation_type = kTfLiteArenaRw;\n    if (!TfLiteIntArrayEqual(input_quantized->dims, input->dims)) {\n      TfLiteIntArray* input_quantized_size = TfLiteIntArrayCopy(input->dims);\n      TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, input_quantized,\n                                                       input_quantized_size));\n    }\n    node->temporaries->data[data->scaling_factors_index] =\n        data->scaling_factors_id;\n    TfLiteTensor* scaling_factors;\n    TF_LITE_ENSURE_OK(\n        context, GetTemporarySafe(context, node, data->scaling_factors_index,\n                                  &scaling_factors));\n    scaling_factors->type = kTfLiteFloat32;\n    scaling_factors->allocation_type = kTfLiteArenaRw;\n    const int batch_size = SizeOfDimension(input, 0);\n    int scaling_dims[1] = {batch_size};\n    if (!TfLiteIntArrayEqualsArray(scaling_factors->dims, 1, scaling_dims)) {\n      TfLiteIntArray* scaling_factors_size = TfLiteIntArrayCreate(1);\n      scaling_factors_size->data[0] = batch_size;\n      TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, scaling_factors,\n                                                       scaling_factors_size));\n    }\n    node->temporaries->data[data->input_offset_index] = data->input_offset_id;\n    TfLiteTensor* input_offsets;\n    TF_LITE_ENSURE_OK(context,\n                      GetTemporarySafe(context, node, data->input_offset_index,\n                                       &input_offsets));\n    input_offsets->type = kTfLiteInt32;\n    input_offsets->allocation_type = kTfLiteArenaRw;\n    if (!TfLiteIntArrayEqualsArray(input_offsets->dims, 1, scaling_dims)) {\n      TfLiteIntArray* input_offsets_size = TfLiteIntArrayCreate(1);\n      input_offsets_size->data[0] = batch_size;\n      TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, input_offsets,\n                                                       input_offsets_size));\n    }\n  }\n\n  TfLiteIntArray* outputSize = TfLiteIntArrayCreate(4);\n  outputSize->data[0] = batches;\n  outputSize->data[1] = out_height;\n  outputSize->data[2] = out_width;\n  outputSize->data[3] = channels_out;\n  return context->ResizeTensor(context, output, outputSize);\n}\n\nTfLiteStatus ComputeDepthMultiplier(TfLiteContext* context,\n                                    const TfLiteTensor* input,\n                                    const TfLiteTensor* filter,\n                                    int16* depth_multiplier) {\n  int num_filter_channels = SizeOfDimension(filter, 3);\n  int num_input_channels = SizeOfDimension(input, 3);\n  TF_LITE_ENSURE(context, num_input_channels != 0);\n  TF_LITE_ENSURE_EQ(context, num_filter_channels % num_input_channels, 0);\n  *depth_multiplier = num_filter_channels / num_input_channels;\n  return kTfLiteOk;\n}\n\ntemplate <KernelType kernel_type>\nTfLiteStatus EvalFloat(TfLiteContext* context, TfLiteNode* node,\n                       TfLiteDepthwiseConvParams* params, OpData* data,\n                       const TfLiteTensor* input, const TfLiteTensor* filter,\n                       const TfLiteTensor* bias, TfLiteTensor* output) {\n  float output_activation_min, output_activation_max;\n  CalculateActivationRange(params->activation, &output_activation_min,\n                           &output_activation_max);\n\n  DepthwiseParams op_params;\n  op_params.padding_type = PaddingType::kSame;\n  op_params.padding_values.width = data->padding.width;\n  op_params.padding_values.height = data->padding.height;\n  op_params.stride_width = params->stride_width;\n  op_params.stride_height = params->stride_height;\n  op_params.dilation_width_factor = params->dilation_width_factor;\n  op_params.dilation_height_factor = params->dilation_height_factor;\n  op_params.float_activation_min = output_activation_min;\n  op_params.float_activation_max = output_activation_max;\n  TF_LITE_ENSURE_STATUS(ComputeDepthMultiplier(context, input, filter,\n                                               &op_params.depth_multiplier));\n  if (kernel_type == kReference) {\n    reference_ops::DepthwiseConv(\n        op_params, GetTensorShape(input), GetTensorData<float>(input),\n        GetTensorShape(filter), GetTensorData<float>(filter),\n        GetTensorShape(bias), GetTensorData<float>(bias),\n        GetTensorShape(output), GetTensorData<float>(output));\n  } else {\n    optimized_ops::DepthwiseConv<float, float>(\n        op_params, GetTensorShape(input), GetTensorData<float>(input),\n        GetTensorShape(filter), GetTensorData<float>(filter),\n        GetTensorShape(bias), GetTensorData<float>(bias),\n        GetTensorShape(output), GetTensorData<float>(output),\n        CpuBackendContext::GetFromContext(context));\n  }\n  return kTfLiteOk;\n}\n\ntemplate <KernelType kernel_type>\nTfLiteStatus EvalQuantized(TfLiteContext* context, TfLiteNode* node,\n                           TfLiteDepthwiseConvParams* params, OpData* data,\n                           const TfLiteTensor* input,\n                           const TfLiteTensor* filter, const TfLiteTensor* bias,\n                           TfLiteTensor* output) {\n  auto input_offset = -input->params.zero_point;\n  auto filter_offset = -filter->params.zero_point;\n  auto output_offset = output->params.zero_point;\n\n  DepthwiseParams op_params;\n  op_params.padding_type = PaddingType::kSame;\n  op_params.padding_values.width = data->padding.width;\n  op_params.padding_values.height = data->padding.height;\n  op_params.stride_width = params->stride_width;\n  op_params.stride_height = params->stride_height;\n  op_params.dilation_width_factor = params->dilation_width_factor;\n  op_params.dilation_height_factor = params->dilation_height_factor;\n  op_params.input_offset = input_offset;\n  op_params.weights_offset = filter_offset;\n  op_params.output_offset = output_offset;\n  op_params.output_multiplier = data->output_multiplier;\n  op_params.output_shift = -data->output_shift;\n  op_params.quantized_activation_min = data->output_activation_min;\n  op_params.quantized_activation_max = data->output_activation_max;\n  TF_LITE_ENSURE_STATUS(ComputeDepthMultiplier(context, input, filter,\n                                               &op_params.depth_multiplier));\n  if (kernel_type == kReference) {\n    reference_ops::DepthwiseConv(\n        op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),\n        GetTensorShape(filter), GetTensorData<uint8_t>(filter),\n        GetTensorShape(bias), GetTensorData<int32_t>(bias),\n        GetTensorShape(output), GetTensorData<uint8_t>(output));\n  } else {\n    optimized_ops::DepthwiseConv<uint8, int32>(\n        op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),\n        GetTensorShape(filter), GetTensorData<uint8_t>(filter),\n        GetTensorShape(bias), GetTensorData<int32_t>(bias),\n        GetTensorShape(output), GetTensorData<uint8_t>(output),\n        CpuBackendContext::GetFromContext(context));\n  }\n  return kTfLiteOk;\n}\n\ntemplate <KernelType kernel_type>\nTfLiteStatus EvalQuantizedPerChannel(TfLiteContext* context, TfLiteNode* node,\n                                     TfLiteDepthwiseConvParams* params,\n                                     OpData* data, const TfLiteTensor* input,\n                                     const TfLiteTensor* filter,\n                                     const TfLiteTensor* bias,\n                                     TfLiteTensor* output) {\n  DepthwiseParams op_params;\n  op_params.padding_type = PaddingType::kSame;\n  op_params.padding_values.width = data->padding.width;\n  op_params.padding_values.height = data->padding.height;\n  op_params.stride_width = params->stride_width;\n  op_params.stride_height = params->stride_height;\n  op_params.dilation_width_factor = params->dilation_width_factor;\n  op_params.dilation_height_factor = params->dilation_height_factor;\n  op_params.input_offset = -input->params.zero_point;\n  op_params.weights_offset = 0;\n  op_params.output_offset = output->params.zero_point;\n  op_params.quantized_activation_min = data->output_activation_min;\n  op_params.quantized_activation_max = data->output_activation_max;\n  TF_LITE_ENSURE_STATUS(ComputeDepthMultiplier(context, input, filter,\n                                               &op_params.depth_multiplier));\n\n  if (kernel_type == kReference) {\n    reference_integer_ops::DepthwiseConvPerChannel(\n        op_params, data->per_channel_output_multiplier.data(),\n        data->per_channel_output_shift.data(), GetTensorShape(input),\n        GetTensorData<int8>(input), GetTensorShape(filter),\n        GetTensorData<int8>(filter), GetTensorShape(bias),\n        GetTensorData<int32>(bias), GetTensorShape(output),\n        GetTensorData<int8>(output));\n  } else {\n    optimized_integer_ops::DepthwiseConvPerChannel(\n        op_params, data->per_channel_output_multiplier.data(),\n        data->per_channel_output_shift.data(), GetTensorShape(input),\n        GetTensorData<int8>(input), GetTensorShape(filter),\n        GetTensorData<int8>(filter), GetTensorShape(bias),\n        GetTensorData<int32>(bias), GetTensorShape(output),\n        GetTensorData<int8>(output),\n        CpuBackendContext::GetFromContext(context));\n  }\n  return kTfLiteOk;\n}\n\nTfLiteStatus EvalQuantizedPerChannel16x8(\n    const TfLiteDepthwiseConvParams* params, const OpData* data,\n    const TfLiteTensor* input, const TfLiteTensor* filter,\n    const TfLiteTensor* bias, TfLiteTensor* output) {\n  DepthwiseParams op_params;\n  op_params.padding_type = PaddingType::kSame;\n  op_params.padding_values.width = data->padding.width;\n  op_params.padding_values.height = data->padding.height;\n  op_params.stride_width = params->stride_width;\n  op_params.stride_height = params->stride_height;\n  op_params.dilation_width_factor = params->dilation_width_factor;\n  op_params.dilation_height_factor = params->dilation_height_factor;\n  op_params.depth_multiplier = params->depth_multiplier;\n  op_params.weights_offset = 0;\n  op_params.quantized_activation_min = data->output_activation_min;\n  op_params.quantized_activation_max = data->output_activation_max;\n\n  reference_integer_ops::DepthwiseConvPerChannel(\n      op_params, data->per_channel_output_multiplier.data(),\n      data->per_channel_output_shift.data(), GetTensorShape(input),\n      GetTensorData<int16>(input), GetTensorShape(filter),\n      GetTensorData<int8>(filter), GetTensorShape(bias),\n      GetTensorData<std::int64_t>(bias), GetTensorShape(output),\n      GetTensorData<int16>(output));\n\n  return kTfLiteOk;\n}\n\ntemplate <KernelType kernel_type>\nTfLiteStatus EvalHybridPerChannel(TfLiteContext* context, TfLiteNode* node,\n                                  TfLiteDepthwiseConvParams* params,\n                                  OpData* data, const TfLiteTensor* input,\n                                  const TfLiteTensor* filter,\n                                  const TfLiteTensor* bias,\n                                  TfLiteTensor* output) {\n  float output_activation_min, output_activation_max;\n  CalculateActivationRange(params->activation, &output_activation_min,\n                           &output_activation_max);\n  const int batch_size = SizeOfDimension(input, 0);\n  TF_LITE_ENSURE(context, batch_size != 0);\n  const int input_size = NumElements(input) / batch_size;\n  TfLiteTensor* input_quantized;\n  TF_LITE_ENSURE_OK(context,\n                    GetTemporarySafe(context, node, data->input_quantized_index,\n                                     &input_quantized));\n  int8_t* quantized_input_ptr_batch = input_quantized->data.int8;\n  TfLiteTensor* scaling_factors_tensor;\n  TF_LITE_ENSURE_OK(context,\n                    GetTemporarySafe(context, node, data->scaling_factors_index,\n                                     &scaling_factors_tensor));\n  float* scaling_factors_ptr = GetTensorData<float>(scaling_factors_tensor);\n  TfLiteTensor* input_offset_tensor;\n  TF_LITE_ENSURE_OK(context,\n                    GetTemporarySafe(context, node, data->input_offset_index,\n                                     &input_offset_tensor));\n  int32_t* input_offset_ptr = GetTensorData<int32_t>(input_offset_tensor);\n\n  for (int b = 0; b < batch_size; ++b) {\n    const int offset = b * input_size;\n    tensor_utils::AsymmetricQuantizeFloats(\n        GetTensorData<float>(input) + offset, input_size,\n        quantized_input_ptr_batch + offset, &scaling_factors_ptr[b],\n        &input_offset_ptr[b]);\n  }\n\n  DepthwiseParams op_params;\n  op_params.padding_type = PaddingType::kSame;\n  op_params.padding_values.width = data->padding.width;\n  op_params.padding_values.height = data->padding.height;\n  op_params.stride_width = params->stride_width;\n  op_params.stride_height = params->stride_height;\n  op_params.dilation_width_factor = params->dilation_width_factor;\n  op_params.dilation_height_factor = params->dilation_height_factor;\n  op_params.depth_multiplier = params->depth_multiplier;\n\n  op_params.weights_offset = 0;\n  op_params.float_activation_min = output_activation_min;\n  op_params.float_activation_max = output_activation_max;\n  TF_LITE_ENSURE(context, filter->quantization.type != kTfLiteNoQuantization);\n  const auto* affine_quantization =\n      reinterpret_cast<TfLiteAffineQuantization*>(filter->quantization.params);\n  if (kernel_type == kReference) {\n    reference_integer_ops::DepthwiseConvHybridPerChannel(\n        op_params, scaling_factors_ptr, GetTensorShape(input),\n        quantized_input_ptr_batch, GetTensorShape(filter),\n        GetTensorData<int8>(filter), GetTensorShape(bias),\n        GetTensorData<float>(bias), GetTensorShape(output),\n        GetTensorData<float>(output), affine_quantization->scale->data,\n        input_offset_ptr);\n  } else {\n    optimized_integer_ops::DepthwiseConvHybridPerChannel(\n        op_params, scaling_factors_ptr, GetTensorShape(input),\n        quantized_input_ptr_batch, GetTensorShape(filter),\n        GetTensorData<int8>(filter), GetTensorShape(bias),\n        GetTensorData<float>(bias), GetTensorShape(output),\n        GetTensorData<float>(output), affine_quantization->scale->data,\n        input_offset_ptr, CpuBackendContext::GetFromContext(context));\n  }\n\n  return kTfLiteOk;\n}\n\ntemplate <KernelType kernel_type, TfLiteType input_type>\nTfLiteStatus EvalImpl(TfLiteContext* context, TfLiteNode* node) {\n  auto* params =\n      reinterpret_cast<TfLiteDepthwiseConvParams*>(node->builtin_data);\n  OpData* data = reinterpret_cast<OpData*>(node->user_data);\n\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context,\n                    GetOutputSafe(context, node, kOutputTensor, &output));\n  const TfLiteTensor* input;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInputTensor, &input));\n  const TfLiteTensor* filter;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kFilterTensor, &filter));\n  const TfLiteTensor* bias =\n      (NumInputs(node) == 3) ? GetInput(context, node, kBiasTensor) : nullptr;\n  TFLITE_DCHECK_EQ(input_type, input->type);\n\n  switch (input_type) {  // Already know in/out types are same.\n    case kTfLiteFloat32:\n      if (filter->type == kTfLiteFloat32) {\n        return EvalFloat<kernel_type>(context, node, params, data, input,\n                                      filter, bias, output);\n      } else if (filter->type == kTfLiteInt8) {\n        return EvalHybridPerChannel<kernel_type>(context, node, params, data,\n                                                 input, filter, bias, output);\n      } else {\n        TF_LITE_KERNEL_LOG(\n            context, \"Type %s with filter type %s not currently supported.\",\n            TfLiteTypeGetName(input->type), TfLiteTypeGetName(filter->type));\n        return kTfLiteError;\n      }\n      break;\n    case kTfLiteUInt8:\n      return EvalQuantized<kernel_type>(context, node, params, data, input,\n                                        filter, bias, output);\n      break;\n    case kTfLiteInt8:\n      return EvalQuantizedPerChannel<kernel_type>(context, node, params, data,\n                                                  input, filter, bias, output);\n      break;\n    case kTfLiteInt16:\n      return EvalQuantizedPerChannel16x8(params, data, input, filter, bias,\n                                         output);\n      break;\n    default:\n      context->ReportError(context, \"Type %d not currently supported.\",\n                           input->type);\n      return kTfLiteError;\n  }\n}\n\ntemplate <KernelType kernel_type>\nTfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n  const TfLiteTensor* input;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInputTensor, &input));\n\n  switch (input->type) {  // Already know in/out types are same.\n    case kTfLiteFloat32:\n      return EvalImpl<kernel_type, kTfLiteFloat32>(context, node);\n    case kTfLiteUInt8:\n      return EvalImpl<kernel_type, kTfLiteUInt8>(context, node);\n    case kTfLiteInt8:\n      return EvalImpl<kernel_type, kTfLiteInt8>(context, node);\n    case kTfLiteInt16:\n      return EvalImpl<kernel_type, kTfLiteInt16>(context, node);\n    default:\n      context->ReportError(context, \"Type %d not currently supported.\",\n                           input->type);\n      return kTfLiteError;\n  }\n}\n\n}  // namespace depthwise_conv\n\nTfLiteRegistration* Register_DEPTHWISE_CONVOLUTION_REF() {\n  static TfLiteRegistration r = {\n      depthwise_conv::Init, depthwise_conv::Free, depthwise_conv::Prepare,\n      depthwise_conv::Eval<depthwise_conv::kReference>};\n  return &r;\n}\n\nTfLiteRegistration* Register_DEPTHWISE_CONVOLUTION_GENERIC_OPT() {\n  static TfLiteRegistration r = {\n      depthwise_conv::Init, depthwise_conv::Free, depthwise_conv::Prepare,\n      depthwise_conv::Eval<depthwise_conv::kGenericOptimized>};\n  return &r;\n}\n\nTfLiteRegistration* Register_DEPTHWISE_CONVOLUTION_NEON_OPT() {\n  static TfLiteRegistration r = {\n      depthwise_conv::Init, depthwise_conv::Free, depthwise_conv::Prepare,\n      depthwise_conv::Eval<depthwise_conv::kNeonOptimized>};\n  return &r;\n}\n\nTfLiteRegistration* Register_DEPTHWISE_CONVOLUTION_NEON_OPT_UINT8() {\n  static TfLiteRegistration r = {\n      depthwise_conv::Init, depthwise_conv::Free, depthwise_conv::Prepare,\n      depthwise_conv::EvalImpl<depthwise_conv::kNeonOptimized, kTfLiteUInt8>};\n  return &r;\n}\n\nTfLiteRegistration* Register_DEPTHWISE_CONV_2D() {\n#ifdef USE_NEON\n  return Register_DEPTHWISE_CONVOLUTION_NEON_OPT();\n#else\n  return Register_DEPTHWISE_CONVOLUTION_GENERIC_OPT();\n#endif\n}\n\n// Warning: Clients using this variant are responsible for ensuring that their\n// models only need the UINT8 type. TFLite's op registration mechanism doesn't\n// yet allow for more nuanced registration mechanisms.\nTfLiteRegistration* Register_DEPTHWISE_CONV_2D_UINT8() {\n#ifdef USE_NEON\n  return Register_DEPTHWISE_CONVOLUTION_NEON_OPT_UINT8();\n#else\n  return Register_DEPTHWISE_CONV_2D();\n#endif\n}\n\n}  // namespace builtin\n}  // namespace ops\n}  // namespace tflite\n"], "fixing_code": ["/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include \"tensorflow/lite/kernels/internal/optimized/integer_ops/depthwise_conv.h\"\n\n#include <stddef.h>\n#include <stdint.h>\n\n#include <vector>\n\n#include \"tensorflow/lite/c/builtin_op_data.h\"\n#include \"tensorflow/lite/c/common.h\"\n#include \"tensorflow/lite/kernels/cpu_backend_context.h\"\n#include \"tensorflow/lite/kernels/internal/compatibility.h\"\n#include \"tensorflow/lite/kernels/internal/optimized/cpu_check.h\"\n#include \"tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h\"\n#include \"tensorflow/lite/kernels/internal/optimized/integer_ops/depthwise_conv_hybrid.h\"\n#include \"tensorflow/lite/kernels/internal/optimized/neon_check.h\"\n#include \"tensorflow/lite/kernels/internal/quantization_util.h\"\n#include \"tensorflow/lite/kernels/internal/reference/depthwiseconv_float.h\"\n#include \"tensorflow/lite/kernels/internal/reference/depthwiseconv_uint8.h\"\n#include \"tensorflow/lite/kernels/internal/reference/integer_ops/depthwise_conv.h\"\n#include \"tensorflow/lite/kernels/internal/tensor.h\"\n#include \"tensorflow/lite/kernels/internal/tensor_ctypes.h\"\n#include \"tensorflow/lite/kernels/internal/tensor_utils.h\"\n#include \"tensorflow/lite/kernels/internal/types.h\"\n#include \"tensorflow/lite/kernels/kernel_util.h\"\n#include \"tensorflow/lite/kernels/padding.h\"\n\nnamespace tflite {\nnamespace ops {\nnamespace builtin {\nnamespace depthwise_conv {\n\nconstexpr int kInputTensor = 0;\nconstexpr int kFilterTensor = 1;\nconstexpr int kBiasTensor = 2;\nconstexpr int kOutputTensor = 0;\n\n// This file has three implementation of DepthwiseConv.\nenum KernelType {\n  kReference,\n  kGenericOptimized,  // Neon-free\n  kNeonOptimized,\n};\n\nconst int kTensorNotAllocated = -1;\n\nstruct OpData {\n  TfLitePaddingValues padding;\n  // The scaling factor from input to output (aka the 'real multiplier') can\n  // be represented as a fixed point multiplier plus a left shift.\n  int32_t output_multiplier;\n  int output_shift;\n  // The range of the fused activation layer. For example for kNone and\n  // uint8_t these would be 0 and 255.\n  int32_t output_activation_min;\n  int32_t output_activation_max;\n\n  // Per channel output multiplier and shift.\n  std::vector<int32_t> per_channel_output_multiplier;\n  std::vector<int> per_channel_output_shift;\n\n  // Hybrid per channel temporary tensors.\n  int input_quantized_id = kTensorNotAllocated;\n  int scaling_factors_id = kTensorNotAllocated;\n  int input_offset_id = kTensorNotAllocated;\n  int32_t input_quantized_index;\n  int32_t scaling_factors_index;\n  int32_t input_offset_index;\n};\n\nvoid* Init(TfLiteContext* context, const char* buffer, size_t length) {\n  // This is a builtin op, so we don't use the contents in 'buffer', if any.\n  // Instead, we allocate a new object to carry information from Prepare() to\n  // Eval().\n  return new OpData;\n}\n\nvoid Free(TfLiteContext* context, void* buffer) {\n  delete reinterpret_cast<OpData*>(buffer);\n}\n\nTfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n  auto* params =\n      reinterpret_cast<TfLiteDepthwiseConvParams*>(node->builtin_data);\n  OpData* data = reinterpret_cast<OpData*>(node->user_data);\n\n  bool has_bias = NumInputs(node) == 3;\n\n  TF_LITE_ENSURE(context, has_bias || NumInputs(node) == 2);\n  const TfLiteTensor* input;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInputTensor, &input));\n  const TfLiteTensor* filter;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kFilterTensor, &filter));\n  const TfLiteTensor* bias = nullptr;\n\n  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context,\n                    GetOutputSafe(context, node, kOutputTensor, &output));\n\n  TF_LITE_ENSURE_EQ(context, NumDimensions(input), 4);\n  TF_LITE_ENSURE_EQ(context, NumDimensions(filter), 4);\n  TF_LITE_ENSURE(context, params->dilation_height_factor > 0);\n  TF_LITE_ENSURE(context, params->dilation_width_factor > 0);\n\n  const TfLiteType data_type = input->type;\n\n  const TfLiteType filter_type = filter->type;\n  const bool is_hybrid =\n      data_type == kTfLiteFloat32 && filter_type == kTfLiteInt8;\n  TF_LITE_ENSURE(context,\n                 data_type == kTfLiteFloat32 || data_type == kTfLiteUInt8 ||\n                     data_type == kTfLiteInt8 || data_type == kTfLiteInt16);\n  TF_LITE_ENSURE_TYPES_EQ(context, output->type, data_type);\n  if (!is_hybrid) {\n    TF_LITE_ENSURE(context,\n                   filter->type == data_type || data_type == kTfLiteInt16);\n  }\n\n  if (data_type == kTfLiteInt16) {\n    TF_LITE_ENSURE_EQ(context, input->params.zero_point, 0);\n    TF_LITE_ENSURE_EQ(context, output->params.zero_point, 0);\n  }\n\n  // Filter in DepthwiseConv is expected to be [1, H, W, O].\n  TF_LITE_ENSURE_EQ(context, SizeOfDimension(filter, 0), 1);\n\n  if (has_bias) {\n    TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kBiasTensor, &bias));\n    if (data_type == kTfLiteUInt8 || data_type == kTfLiteInt8) {\n      TF_LITE_ENSURE_TYPES_EQ(context, bias->type, kTfLiteInt32);\n      TF_LITE_ENSURE_EQ(context, bias->params.zero_point, 0);\n    } else if (data_type == kTfLiteInt16) {\n      TF_LITE_ENSURE_TYPES_EQ(context, bias->type, kTfLiteInt64);\n      TF_LITE_ENSURE_EQ(context, bias->params.zero_point, 0);\n    } else {\n      TF_LITE_ENSURE_TYPES_EQ(context, bias->type, data_type);\n    }\n    TF_LITE_ENSURE_EQ(context, NumDimensions(bias), 1);\n    TF_LITE_ENSURE_EQ(context, SizeOfDimension(filter, 3),\n                      SizeOfDimension(bias, 0));\n  }\n\n  int channels_out = SizeOfDimension(filter, 3);\n  int width = SizeOfDimension(input, 2);\n  int height = SizeOfDimension(input, 1);\n  int filter_width = SizeOfDimension(filter, 2);\n  int filter_height = SizeOfDimension(filter, 1);\n  int batches = SizeOfDimension(input, 0);\n\n  // Matching GetWindowedOutputSize in TensorFlow.\n  auto padding = params->padding;\n  int out_width, out_height;\n\n  data->padding = ComputePaddingHeightWidth(\n      params->stride_height, params->stride_width,\n      params->dilation_height_factor, params->dilation_width_factor, height,\n      width, filter_height, filter_width, padding, &out_height, &out_width);\n\n  // Note that quantized inference requires that all tensors have their\n  // parameters set. This is usually done during quantized training or\n  // calibration.\n  if (data_type != kTfLiteFloat32) {\n    TF_LITE_ENSURE_EQ(context, filter->quantization.type,\n                      kTfLiteAffineQuantization);\n    TF_LITE_ENSURE(context, filter->quantization.type != kTfLiteNoQuantization);\n    const auto* affine_quantization =\n        reinterpret_cast<TfLiteAffineQuantization*>(\n            filter->quantization.params);\n    TF_LITE_ENSURE(context, affine_quantization);\n    TF_LITE_ENSURE(context, affine_quantization->scale);\n    TF_LITE_ENSURE(context, (affine_quantization->scale->size == 1 ||\n                             affine_quantization->scale->size == channels_out));\n\n    data->per_channel_output_multiplier.resize(channels_out);\n    data->per_channel_output_shift.resize(channels_out);\n    TF_LITE_ENSURE_STATUS(tflite::PopulateConvolutionQuantizationParams(\n        context, input, filter, bias, output, params->activation,\n        &data->output_multiplier, &data->output_shift,\n        &data->output_activation_min, &data->output_activation_max,\n        data->per_channel_output_multiplier.data(),\n        data->per_channel_output_shift.data(), channels_out));\n  }\n\n  if (is_hybrid) {\n    TF_LITE_ENSURE(context, filter->quantization.type != kTfLiteNoQuantization);\n    const auto* affine_quantization =\n        reinterpret_cast<TfLiteAffineQuantization*>(\n            filter->quantization.params);\n    TF_LITE_ENSURE(context, affine_quantization);\n    TF_LITE_ENSURE(context, affine_quantization->scale);\n    TF_LITE_ENSURE_EQ(\n        context, affine_quantization->scale->size,\n        filter->dims->data[affine_quantization->quantized_dimension]);\n\n    int temporaries_count = 0;\n    data->input_quantized_index = temporaries_count;\n    if (data->input_quantized_id == kTensorNotAllocated) {\n      TF_LITE_ENSURE_OK(\n          context, context->AddTensors(context, 1, &data->input_quantized_id));\n    }\n    ++temporaries_count;\n    data->scaling_factors_index = temporaries_count;\n    if (data->scaling_factors_id == kTensorNotAllocated) {\n      TF_LITE_ENSURE_OK(\n          context, context->AddTensors(context, 1, &data->scaling_factors_id));\n    }\n    ++temporaries_count;\n    data->input_offset_index = temporaries_count;\n    if (data->input_offset_id == kTensorNotAllocated) {\n      TF_LITE_ENSURE_OK(\n          context, context->AddTensors(context, 1, &data->input_offset_id));\n    }\n    ++temporaries_count;\n\n    TfLiteIntArrayFree(node->temporaries);\n    node->temporaries = TfLiteIntArrayCreate(temporaries_count);\n\n    node->temporaries->data[data->input_quantized_index] =\n        data->input_quantized_id;\n    TfLiteTensor* input_quantized;\n    TF_LITE_ENSURE_OK(\n        context, GetTemporarySafe(context, node, data->input_quantized_index,\n                                  &input_quantized));\n    input_quantized->type = kTfLiteInt8;\n    input_quantized->allocation_type = kTfLiteArenaRw;\n    if (!TfLiteIntArrayEqual(input_quantized->dims, input->dims)) {\n      TfLiteIntArray* input_quantized_size = TfLiteIntArrayCopy(input->dims);\n      TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, input_quantized,\n                                                       input_quantized_size));\n    }\n    node->temporaries->data[data->scaling_factors_index] =\n        data->scaling_factors_id;\n    TfLiteTensor* scaling_factors;\n    TF_LITE_ENSURE_OK(\n        context, GetTemporarySafe(context, node, data->scaling_factors_index,\n                                  &scaling_factors));\n    scaling_factors->type = kTfLiteFloat32;\n    scaling_factors->allocation_type = kTfLiteArenaRw;\n    const int batch_size = SizeOfDimension(input, 0);\n    int scaling_dims[1] = {batch_size};\n    if (!TfLiteIntArrayEqualsArray(scaling_factors->dims, 1, scaling_dims)) {\n      TfLiteIntArray* scaling_factors_size = TfLiteIntArrayCreate(1);\n      scaling_factors_size->data[0] = batch_size;\n      TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, scaling_factors,\n                                                       scaling_factors_size));\n    }\n    node->temporaries->data[data->input_offset_index] = data->input_offset_id;\n    TfLiteTensor* input_offsets;\n    TF_LITE_ENSURE_OK(context,\n                      GetTemporarySafe(context, node, data->input_offset_index,\n                                       &input_offsets));\n    input_offsets->type = kTfLiteInt32;\n    input_offsets->allocation_type = kTfLiteArenaRw;\n    if (!TfLiteIntArrayEqualsArray(input_offsets->dims, 1, scaling_dims)) {\n      TfLiteIntArray* input_offsets_size = TfLiteIntArrayCreate(1);\n      input_offsets_size->data[0] = batch_size;\n      TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, input_offsets,\n                                                       input_offsets_size));\n    }\n  }\n\n  TfLiteIntArray* outputSize = TfLiteIntArrayCreate(4);\n  outputSize->data[0] = batches;\n  outputSize->data[1] = out_height;\n  outputSize->data[2] = out_width;\n  outputSize->data[3] = channels_out;\n  return context->ResizeTensor(context, output, outputSize);\n}\n\nTfLiteStatus ComputeDepthMultiplier(TfLiteContext* context,\n                                    const TfLiteTensor* input,\n                                    const TfLiteTensor* filter,\n                                    int16* depth_multiplier) {\n  int num_filter_channels = SizeOfDimension(filter, 3);\n  int num_input_channels = SizeOfDimension(input, 3);\n  TF_LITE_ENSURE(context, num_input_channels != 0);\n  TF_LITE_ENSURE_EQ(context, num_filter_channels % num_input_channels, 0);\n  *depth_multiplier = num_filter_channels / num_input_channels;\n  return kTfLiteOk;\n}\n\ntemplate <KernelType kernel_type>\nTfLiteStatus EvalFloat(TfLiteContext* context, TfLiteNode* node,\n                       TfLiteDepthwiseConvParams* params, OpData* data,\n                       const TfLiteTensor* input, const TfLiteTensor* filter,\n                       const TfLiteTensor* bias, TfLiteTensor* output) {\n  float output_activation_min, output_activation_max;\n  CalculateActivationRange(params->activation, &output_activation_min,\n                           &output_activation_max);\n\n  DepthwiseParams op_params;\n  op_params.padding_type = PaddingType::kSame;\n  op_params.padding_values.width = data->padding.width;\n  op_params.padding_values.height = data->padding.height;\n  op_params.stride_width = params->stride_width;\n  op_params.stride_height = params->stride_height;\n  op_params.dilation_width_factor = params->dilation_width_factor;\n  op_params.dilation_height_factor = params->dilation_height_factor;\n  op_params.float_activation_min = output_activation_min;\n  op_params.float_activation_max = output_activation_max;\n  TF_LITE_ENSURE_STATUS(ComputeDepthMultiplier(context, input, filter,\n                                               &op_params.depth_multiplier));\n  if (kernel_type == kReference) {\n    reference_ops::DepthwiseConv(\n        op_params, GetTensorShape(input), GetTensorData<float>(input),\n        GetTensorShape(filter), GetTensorData<float>(filter),\n        GetTensorShape(bias), GetTensorData<float>(bias),\n        GetTensorShape(output), GetTensorData<float>(output));\n  } else {\n    optimized_ops::DepthwiseConv<float, float>(\n        op_params, GetTensorShape(input), GetTensorData<float>(input),\n        GetTensorShape(filter), GetTensorData<float>(filter),\n        GetTensorShape(bias), GetTensorData<float>(bias),\n        GetTensorShape(output), GetTensorData<float>(output),\n        CpuBackendContext::GetFromContext(context));\n  }\n  return kTfLiteOk;\n}\n\ntemplate <KernelType kernel_type>\nTfLiteStatus EvalQuantized(TfLiteContext* context, TfLiteNode* node,\n                           TfLiteDepthwiseConvParams* params, OpData* data,\n                           const TfLiteTensor* input,\n                           const TfLiteTensor* filter, const TfLiteTensor* bias,\n                           TfLiteTensor* output) {\n  auto input_offset = -input->params.zero_point;\n  auto filter_offset = -filter->params.zero_point;\n  auto output_offset = output->params.zero_point;\n\n  DepthwiseParams op_params;\n  op_params.padding_type = PaddingType::kSame;\n  op_params.padding_values.width = data->padding.width;\n  op_params.padding_values.height = data->padding.height;\n  op_params.stride_width = params->stride_width;\n  op_params.stride_height = params->stride_height;\n  op_params.dilation_width_factor = params->dilation_width_factor;\n  op_params.dilation_height_factor = params->dilation_height_factor;\n  op_params.input_offset = input_offset;\n  op_params.weights_offset = filter_offset;\n  op_params.output_offset = output_offset;\n  op_params.output_multiplier = data->output_multiplier;\n  op_params.output_shift = -data->output_shift;\n  op_params.quantized_activation_min = data->output_activation_min;\n  op_params.quantized_activation_max = data->output_activation_max;\n  TF_LITE_ENSURE_STATUS(ComputeDepthMultiplier(context, input, filter,\n                                               &op_params.depth_multiplier));\n  if (kernel_type == kReference) {\n    reference_ops::DepthwiseConv(\n        op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),\n        GetTensorShape(filter), GetTensorData<uint8_t>(filter),\n        GetTensorShape(bias), GetTensorData<int32_t>(bias),\n        GetTensorShape(output), GetTensorData<uint8_t>(output));\n  } else {\n    optimized_ops::DepthwiseConv<uint8, int32>(\n        op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),\n        GetTensorShape(filter), GetTensorData<uint8_t>(filter),\n        GetTensorShape(bias), GetTensorData<int32_t>(bias),\n        GetTensorShape(output), GetTensorData<uint8_t>(output),\n        CpuBackendContext::GetFromContext(context));\n  }\n  return kTfLiteOk;\n}\n\ntemplate <KernelType kernel_type>\nTfLiteStatus EvalQuantizedPerChannel(TfLiteContext* context, TfLiteNode* node,\n                                     TfLiteDepthwiseConvParams* params,\n                                     OpData* data, const TfLiteTensor* input,\n                                     const TfLiteTensor* filter,\n                                     const TfLiteTensor* bias,\n                                     TfLiteTensor* output) {\n  DepthwiseParams op_params;\n  op_params.padding_type = PaddingType::kSame;\n  op_params.padding_values.width = data->padding.width;\n  op_params.padding_values.height = data->padding.height;\n  op_params.stride_width = params->stride_width;\n  op_params.stride_height = params->stride_height;\n  op_params.dilation_width_factor = params->dilation_width_factor;\n  op_params.dilation_height_factor = params->dilation_height_factor;\n  op_params.input_offset = -input->params.zero_point;\n  op_params.weights_offset = 0;\n  op_params.output_offset = output->params.zero_point;\n  op_params.quantized_activation_min = data->output_activation_min;\n  op_params.quantized_activation_max = data->output_activation_max;\n  TF_LITE_ENSURE_STATUS(ComputeDepthMultiplier(context, input, filter,\n                                               &op_params.depth_multiplier));\n\n  if (kernel_type == kReference) {\n    reference_integer_ops::DepthwiseConvPerChannel(\n        op_params, data->per_channel_output_multiplier.data(),\n        data->per_channel_output_shift.data(), GetTensorShape(input),\n        GetTensorData<int8>(input), GetTensorShape(filter),\n        GetTensorData<int8>(filter), GetTensorShape(bias),\n        GetTensorData<int32>(bias), GetTensorShape(output),\n        GetTensorData<int8>(output));\n  } else {\n    optimized_integer_ops::DepthwiseConvPerChannel(\n        op_params, data->per_channel_output_multiplier.data(),\n        data->per_channel_output_shift.data(), GetTensorShape(input),\n        GetTensorData<int8>(input), GetTensorShape(filter),\n        GetTensorData<int8>(filter), GetTensorShape(bias),\n        GetTensorData<int32>(bias), GetTensorShape(output),\n        GetTensorData<int8>(output),\n        CpuBackendContext::GetFromContext(context));\n  }\n  return kTfLiteOk;\n}\n\nTfLiteStatus EvalQuantizedPerChannel16x8(\n    const TfLiteDepthwiseConvParams* params, const OpData* data,\n    const TfLiteTensor* input, const TfLiteTensor* filter,\n    const TfLiteTensor* bias, TfLiteTensor* output) {\n  DepthwiseParams op_params;\n  op_params.padding_type = PaddingType::kSame;\n  op_params.padding_values.width = data->padding.width;\n  op_params.padding_values.height = data->padding.height;\n  op_params.stride_width = params->stride_width;\n  op_params.stride_height = params->stride_height;\n  op_params.dilation_width_factor = params->dilation_width_factor;\n  op_params.dilation_height_factor = params->dilation_height_factor;\n  op_params.depth_multiplier = params->depth_multiplier;\n  op_params.weights_offset = 0;\n  op_params.quantized_activation_min = data->output_activation_min;\n  op_params.quantized_activation_max = data->output_activation_max;\n\n  reference_integer_ops::DepthwiseConvPerChannel(\n      op_params, data->per_channel_output_multiplier.data(),\n      data->per_channel_output_shift.data(), GetTensorShape(input),\n      GetTensorData<int16>(input), GetTensorShape(filter),\n      GetTensorData<int8>(filter), GetTensorShape(bias),\n      GetTensorData<std::int64_t>(bias), GetTensorShape(output),\n      GetTensorData<int16>(output));\n\n  return kTfLiteOk;\n}\n\ntemplate <KernelType kernel_type>\nTfLiteStatus EvalHybridPerChannel(TfLiteContext* context, TfLiteNode* node,\n                                  TfLiteDepthwiseConvParams* params,\n                                  OpData* data, const TfLiteTensor* input,\n                                  const TfLiteTensor* filter,\n                                  const TfLiteTensor* bias,\n                                  TfLiteTensor* output) {\n  float output_activation_min, output_activation_max;\n  CalculateActivationRange(params->activation, &output_activation_min,\n                           &output_activation_max);\n  const int batch_size = SizeOfDimension(input, 0);\n  TF_LITE_ENSURE(context, batch_size != 0);\n  const int input_size = NumElements(input) / batch_size;\n  TfLiteTensor* input_quantized;\n  TF_LITE_ENSURE_OK(context,\n                    GetTemporarySafe(context, node, data->input_quantized_index,\n                                     &input_quantized));\n  int8_t* quantized_input_ptr_batch = input_quantized->data.int8;\n  TfLiteTensor* scaling_factors_tensor;\n  TF_LITE_ENSURE_OK(context,\n                    GetTemporarySafe(context, node, data->scaling_factors_index,\n                                     &scaling_factors_tensor));\n  float* scaling_factors_ptr = GetTensorData<float>(scaling_factors_tensor);\n  TfLiteTensor* input_offset_tensor;\n  TF_LITE_ENSURE_OK(context,\n                    GetTemporarySafe(context, node, data->input_offset_index,\n                                     &input_offset_tensor));\n  int32_t* input_offset_ptr = GetTensorData<int32_t>(input_offset_tensor);\n\n  for (int b = 0; b < batch_size; ++b) {\n    const int offset = b * input_size;\n    tensor_utils::AsymmetricQuantizeFloats(\n        GetTensorData<float>(input) + offset, input_size,\n        quantized_input_ptr_batch + offset, &scaling_factors_ptr[b],\n        &input_offset_ptr[b]);\n  }\n\n  DepthwiseParams op_params;\n  op_params.padding_type = PaddingType::kSame;\n  op_params.padding_values.width = data->padding.width;\n  op_params.padding_values.height = data->padding.height;\n  op_params.stride_width = params->stride_width;\n  op_params.stride_height = params->stride_height;\n  op_params.dilation_width_factor = params->dilation_width_factor;\n  op_params.dilation_height_factor = params->dilation_height_factor;\n  op_params.depth_multiplier = params->depth_multiplier;\n\n  op_params.weights_offset = 0;\n  op_params.float_activation_min = output_activation_min;\n  op_params.float_activation_max = output_activation_max;\n  TF_LITE_ENSURE(context, filter->quantization.type != kTfLiteNoQuantization);\n  const auto* affine_quantization =\n      reinterpret_cast<TfLiteAffineQuantization*>(filter->quantization.params);\n  if (kernel_type == kReference) {\n    reference_integer_ops::DepthwiseConvHybridPerChannel(\n        op_params, scaling_factors_ptr, GetTensorShape(input),\n        quantized_input_ptr_batch, GetTensorShape(filter),\n        GetTensorData<int8>(filter), GetTensorShape(bias),\n        GetTensorData<float>(bias), GetTensorShape(output),\n        GetTensorData<float>(output), affine_quantization->scale->data,\n        input_offset_ptr);\n  } else {\n    optimized_integer_ops::DepthwiseConvHybridPerChannel(\n        op_params, scaling_factors_ptr, GetTensorShape(input),\n        quantized_input_ptr_batch, GetTensorShape(filter),\n        GetTensorData<int8>(filter), GetTensorShape(bias),\n        GetTensorData<float>(bias), GetTensorShape(output),\n        GetTensorData<float>(output), affine_quantization->scale->data,\n        input_offset_ptr, CpuBackendContext::GetFromContext(context));\n  }\n\n  return kTfLiteOk;\n}\n\ntemplate <KernelType kernel_type, TfLiteType input_type>\nTfLiteStatus EvalImpl(TfLiteContext* context, TfLiteNode* node) {\n  auto* params =\n      reinterpret_cast<TfLiteDepthwiseConvParams*>(node->builtin_data);\n  OpData* data = reinterpret_cast<OpData*>(node->user_data);\n\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context,\n                    GetOutputSafe(context, node, kOutputTensor, &output));\n  const TfLiteTensor* input;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInputTensor, &input));\n  const TfLiteTensor* filter;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kFilterTensor, &filter));\n  const TfLiteTensor* bias =\n      (NumInputs(node) == 3) ? GetInput(context, node, kBiasTensor) : nullptr;\n  TFLITE_DCHECK_EQ(input_type, input->type);\n\n  switch (input_type) {  // Already know in/out types are same.\n    case kTfLiteFloat32:\n      if (filter->type == kTfLiteFloat32) {\n        return EvalFloat<kernel_type>(context, node, params, data, input,\n                                      filter, bias, output);\n      } else if (filter->type == kTfLiteInt8) {\n        return EvalHybridPerChannel<kernel_type>(context, node, params, data,\n                                                 input, filter, bias, output);\n      } else {\n        TF_LITE_KERNEL_LOG(\n            context, \"Type %s with filter type %s not currently supported.\",\n            TfLiteTypeGetName(input->type), TfLiteTypeGetName(filter->type));\n        return kTfLiteError;\n      }\n      break;\n    case kTfLiteUInt8:\n      return EvalQuantized<kernel_type>(context, node, params, data, input,\n                                        filter, bias, output);\n      break;\n    case kTfLiteInt8:\n      return EvalQuantizedPerChannel<kernel_type>(context, node, params, data,\n                                                  input, filter, bias, output);\n      break;\n    case kTfLiteInt16:\n      return EvalQuantizedPerChannel16x8(params, data, input, filter, bias,\n                                         output);\n      break;\n    default:\n      context->ReportError(context, \"Type %d not currently supported.\",\n                           input->type);\n      return kTfLiteError;\n  }\n}\n\ntemplate <KernelType kernel_type>\nTfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n  const TfLiteTensor* input;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInputTensor, &input));\n\n  switch (input->type) {  // Already know in/out types are same.\n    case kTfLiteFloat32:\n      return EvalImpl<kernel_type, kTfLiteFloat32>(context, node);\n    case kTfLiteUInt8:\n      return EvalImpl<kernel_type, kTfLiteUInt8>(context, node);\n    case kTfLiteInt8:\n      return EvalImpl<kernel_type, kTfLiteInt8>(context, node);\n    case kTfLiteInt16:\n      return EvalImpl<kernel_type, kTfLiteInt16>(context, node);\n    default:\n      context->ReportError(context, \"Type %d not currently supported.\",\n                           input->type);\n      return kTfLiteError;\n  }\n}\n\n}  // namespace depthwise_conv\n\nTfLiteRegistration* Register_DEPTHWISE_CONVOLUTION_REF() {\n  static TfLiteRegistration r = {\n      depthwise_conv::Init, depthwise_conv::Free, depthwise_conv::Prepare,\n      depthwise_conv::Eval<depthwise_conv::kReference>};\n  return &r;\n}\n\nTfLiteRegistration* Register_DEPTHWISE_CONVOLUTION_GENERIC_OPT() {\n  static TfLiteRegistration r = {\n      depthwise_conv::Init, depthwise_conv::Free, depthwise_conv::Prepare,\n      depthwise_conv::Eval<depthwise_conv::kGenericOptimized>};\n  return &r;\n}\n\nTfLiteRegistration* Register_DEPTHWISE_CONVOLUTION_NEON_OPT() {\n  static TfLiteRegistration r = {\n      depthwise_conv::Init, depthwise_conv::Free, depthwise_conv::Prepare,\n      depthwise_conv::Eval<depthwise_conv::kNeonOptimized>};\n  return &r;\n}\n\nTfLiteRegistration* Register_DEPTHWISE_CONVOLUTION_NEON_OPT_UINT8() {\n  static TfLiteRegistration r = {\n      depthwise_conv::Init, depthwise_conv::Free, depthwise_conv::Prepare,\n      depthwise_conv::EvalImpl<depthwise_conv::kNeonOptimized, kTfLiteUInt8>};\n  return &r;\n}\n\nTfLiteRegistration* Register_DEPTHWISE_CONV_2D() {\n#ifdef USE_NEON\n  return Register_DEPTHWISE_CONVOLUTION_NEON_OPT();\n#else\n  return Register_DEPTHWISE_CONVOLUTION_GENERIC_OPT();\n#endif\n}\n\n// Warning: Clients using this variant are responsible for ensuring that their\n// models only need the UINT8 type. TFLite's op registration mechanism doesn't\n// yet allow for more nuanced registration mechanisms.\nTfLiteRegistration* Register_DEPTHWISE_CONV_2D_UINT8() {\n#ifdef USE_NEON\n  return Register_DEPTHWISE_CONVOLUTION_NEON_OPT_UINT8();\n#else\n  return Register_DEPTHWISE_CONV_2D();\n#endif\n}\n\n}  // namespace builtin\n}  // namespace ops\n}  // namespace tflite\n"], "filenames": ["tensorflow/lite/kernels/depthwise_conv.cc"], "buggy_code_start_loc": [117], "buggy_code_end_loc": [117], "fixing_code_start_loc": [118], "fixing_code_end_loc": [120], "type": "CWE-369", "message": "Tensorflow is an Open Source Machine Learning Framework. ### Impact An attacker can craft a TFLite model that would trigger a division by zero in the implementation of depthwise convolutions. The parameters of the convolution can be user controlled and are also used within a division operation to determine the size of the padding that needs to be added before applying the convolution. There is no check before this division that the divisor is strictly positive. The fix will be included in TensorFlow 2.8.0. We will also cherrypick this commit on TensorFlow 2.7.1, TensorFlow 2.6.3, and TensorFlow 2.5.3, as these are also affected and still in supported range.", "other": {"cve": {"id": "CVE-2022-21741", "sourceIdentifier": "security-advisories@github.com", "published": "2022-02-03T15:15:08.077", "lastModified": "2022-02-09T05:36:34.843", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "Tensorflow is an Open Source Machine Learning Framework. ### Impact An attacker can craft a TFLite model that would trigger a division by zero in the implementation of depthwise convolutions. The parameters of the convolution can be user controlled and are also used within a division operation to determine the size of the padding that needs to be added before applying the convolution. There is no check before this division that the divisor is strictly positive. The fix will be included in TensorFlow 2.8.0. We will also cherrypick this commit on TensorFlow 2.7.1, TensorFlow 2.6.3, and TensorFlow 2.5.3, as these are also affected and still in supported range."}, {"lang": "es", "value": "Tensorflow es un marco de aprendizaje autom\u00e1tico de c\u00f3digo abierto. ### Impacto Un atacante puede dise\u00f1ar un modelo TFLite que desencadene una divisi\u00f3n por cero en la implementaci\u00f3n de convoluciones en profundidad. Los par\u00e1metros de la convoluci\u00f3n pueden ser controlados por el usuario y tambi\u00e9n son usados dentro de una operaci\u00f3n de divisi\u00f3n para determinar el tama\u00f1o del relleno que necesita ser a\u00f1adido antes de aplicar la convoluci\u00f3n. Antes de esta divisi\u00f3n no es comprobado que el divisor sea estrictamente positivo. La correcci\u00f3n ser\u00e1 incluida en TensorFlow versi\u00f3n 2.8.0. Tambi\u00e9n seleccionaremos este commit enTensorFlow versi\u00f3n 2.7.1, TensorFlow versi\u00f3n 2.6.3, y TensorFlow versi\u00f3n 2.5.3, ya que estos tambi\u00e9n est\u00e1n afectados y a\u00fan est\u00e1n en el rango admitido"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 6.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 2.8, "impactScore": 3.6}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 6.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 2.8, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:L/Au:N/C:N/I:N/A:P", "accessVector": "NETWORK", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "PARTIAL", "baseScore": 5.0}, "baseSeverity": "MEDIUM", "exploitabilityScore": 10.0, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-369"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionEndIncluding": "2.5.2", "matchCriteriaId": "688150BF-477C-48FC-9AEF-A79AC57A6DDC"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.6.0", "versionEndIncluding": "2.6.2", "matchCriteriaId": "C9E69B60-8C97-47E2-9027-9598B8392E5D"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.7.0:*:*:*:*:*:*:*", "matchCriteriaId": "2EDFAAB8-799C-4259-9102-944D4760DA2C"}]}]}], "references": [{"url": "https://github.com/tensorflow/tensorflow/blob/5100e359aef5c8021f2e71c7b986420b85ce7b3d/tensorflow/lite/kernels/depthwise_conv.cc#L96", "source": "security-advisories@github.com", "tags": ["Exploit", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/commit/e5b0eec199c2d03de54fd6a7fd9275692218e2bc", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/security/advisories/GHSA-428x-9xc2-m8mj", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/tensorflow/tensorflow/commit/e5b0eec199c2d03de54fd6a7fd9275692218e2bc"}}