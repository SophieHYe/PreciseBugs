{"buggy_code": ["/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#ifndef TENSORFLOW_CORE_KERNELS_SPARSE_SPARSE_MATRIX_H_\n#define TENSORFLOW_CORE_KERNELS_SPARSE_SPARSE_MATRIX_H_\n\n#define EIGEN_USE_THREADS\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n#define EIGEN_USE_GPU\n#endif\n\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/tensor_types.h\"\n#include \"tensorflow/core/framework/variant.h\"\n#include \"tensorflow/core/framework/variant_encode_decode.h\"\n#include \"tensorflow/core/framework/variant_op_registry.h\"\n\nnamespace tensorflow {\n\nclass CSRSparseMatrix {\n  // CreateCSRSparseMatrix is the main method used to construct a\n  // CSRSparseMatrix.  The representations for both 2D and 3D\n  // (batched) CSR Sparse Matrices are the same:\n  //\n  // dtype: The datatype of the values.\n  // dense_shape: The dense shape of the matrix.\n  //   * Host int64 vector, size 2 or 3.\n  //   * Takes on values: (rows, cols) or (batch_size, rows, cols).\n  // batch_pointers: Batch offset pointers into col_indices and values.\n  //   * Host int32 vector, size (batch_size + 1).\n  //   * Takes on values: (0, nnz[0], nnz[0] + nnz[1], ..., total_nnz).\n  // row_pointers: Row offset pointers into col_indices and values.\n  //   * Device int32 vector, size ((rows + 1) * batch_size).\n  //   * Each block of size (rows + 1) takes on values:\n  //     (0, num_rows{b}[0], num_rows{b}[0] + num_rows{b}[1], ..., nnz[b]).\n  //     for b = 0 .. batch_size - 1.\n  // col_indices: Column values for the given row and column index.\n  //   * Device int32 vector, size total_nnz.\n  // values: Actual values for the given row and column index.\n  //   * Device dtype vector, size total_nnz.\n  //\n  // The storage agreement is such that for a given (batch, row, ix):\n  //   offset = batch_pointers(batch) + row_pointers(batch * (rows + 1) + row)\n  //   col = col_indices(offset + ix)\n  //   val = values(offset + ix)\n  // where ix < #nnz columns in (batch, row).\n  // Then:\n  //   matrix(batch, row, col) = val.\n  //\n  // All other elements in the dense representation are treated as 0 / empty.\n  //\n  // For example, for a 2D sparse matrix m shaped (3, 4) such that:\n  //\n  //   m[0, 0] = 1.0\n  //   m[0, 1] = 2.0\n  //   m[0, 2] = 3.0\n  //   m[2, 2] = 4.0\n  //   m[2, 3] = 5.0\n  //\n  // The corresponding representation is:\n  //\n  //   dtype: DT_FLOAT\n  //   dense_shape: (3, 4)\n  //   batch_pointers: (0, 5)\n  //   row_pointers: (0, 3, 3, 5)\n  //   col_indices: concat((0, 1, 2), (), (2, 3))\n  //   values: concat((1.0, 2.0, 3.0), (), (4.0, 5.0))\n  //\n  // For a 3D sparse matrix m shaped (2, 3, 4) such that:\n  //\n  //   m[0, 0, 0] = 1.0\n  //   m[0, 0, 2] = 2.0\n  //   m[0, 2, 3] = 3.0\n  //   m[1, 0, 3] = 4.0\n  //   m[1, 1, 0] = 5.0\n  //\n  // The corresponding representation is:\n  //   dtype: DT_FLOAT\n  //   dense_shape: (2, 3, 4)\n  //   batch_pointers: (0, 3, 5)\n  //   row_pointers: concat((0, 2, 2, 3), (0, 1, 2, 2))\n  //   col_indices: concat(concat((0, 2), (), (3,)),\n  //                       concat((3,),   (), (0,)))\n  //   values: concat(concat((1.0, 2.0), (3.0,), ()),\n  ///                 concat((4.0,),     (5.0,), ()))\n  //\n public:\n  static constexpr const char kTypeName[] = \"tensorflow::CSRSparseMatrix\";\n\n  CSRSparseMatrix() : metadata_{false, DT_INVALID} {}\n\n  CSRSparseMatrix(const CSRSparseMatrix& rhs)\n      : metadata_(rhs.metadata_),\n        dense_shape_(rhs.dense_shape_),\n        batch_pointers_(rhs.batch_pointers_),\n        row_pointers_(rhs.row_pointers_),\n        col_indices_(rhs.col_indices_),\n        values_(rhs.values_) {\n    SetupVecs();\n  }\n\n  CSRSparseMatrix(CSRSparseMatrix&& rhs)\n      : metadata_(rhs.metadata_),\n        dense_shape_(std::move(rhs.dense_shape_)),\n        batch_pointers_(std::move(rhs.batch_pointers_)),\n        row_pointers_(std::move(rhs.row_pointers_)),\n        col_indices_(std::move(rhs.col_indices_)),\n        values_(std::move(rhs.values_)) {\n    SetupVecs();\n    rhs.metadata_.validated = false;\n    rhs.metadata_.dtype = DT_INVALID;\n    rhs.ClearVecs();\n  }\n\n  CSRSparseMatrix& operator=(CSRSparseMatrix&& rhs) {\n    if (this == &rhs) return *this;\n    metadata_ = rhs.metadata_;\n    metadata_.validated = rhs.metadata_.validated;\n    dense_shape_ = std::move(rhs.dense_shape_);\n    batch_pointers_ = std::move(rhs.batch_pointers_);\n    row_pointers_ = std::move(rhs.row_pointers_);\n    col_indices_ = std::move(rhs.col_indices_);\n    values_ = std::move(rhs.values_);\n    SetupVecs();\n    rhs.metadata_ = {false, DT_INVALID};\n    rhs.ClearVecs();\n    return *this;\n  }\n\n  static Status CreateCSRSparseMatrix(DataType dtype,\n                                      const Tensor& dense_shape,     // on host\n                                      const Tensor& batch_pointers,  // on host\n                                      const Tensor& row_pointers,\n                                      const Tensor& col_indices,\n                                      const Tensor& values,\n                                      CSRSparseMatrix* matrix) {\n    *matrix = CSRSparseMatrix(dtype, dense_shape, batch_pointers, row_pointers,\n                              col_indices, values);\n    Status s = matrix->Validate();\n    matrix->metadata_.validated = s.ok();\n    matrix->SetupVecs();\n    return s;\n  }\n\n  Status Validate() const {\n    return ValidateTypesAndShapes(metadata_.dtype, dense_shape_,\n                                  batch_pointers_, row_pointers_, col_indices_,\n                                  values_);\n  }\n\n  void Clear() {\n    metadata_ = {false, DT_INVALID};\n    dense_shape_ = Tensor();\n    batch_pointers_ = Tensor();\n    row_pointers_ = Tensor();\n    col_indices_ = Tensor();\n    values_ = Tensor();\n    ClearVecs();\n  }\n\n  bool valid() const {\n    return metadata_.validated && dense_shape_.IsInitialized() &&\n           batch_pointers_.IsInitialized() && row_pointers_.IsInitialized() &&\n           col_indices_.IsInitialized() && values_.IsInitialized() &&\n           dense_shape_.NumElements() > 1 &&\n           batch_pointers_.NumElements() > 0 && row_pointers_.NumElements() > 0;\n  }\n\n  DataType dtype() const {\n    DCHECK(valid());\n    return metadata_.dtype;\n  }\n\n  inline int dims() const {\n    DCHECK(valid());\n    return dense_shape_.NumElements();\n  }\n\n  inline int nnz(int batch) const {\n    DCHECK_LT(batch, batch_size());\n    return (*batch_pointers_vec_)(batch + 1) - (*batch_pointers_vec_)(batch);\n  }\n\n  inline int batch_offset(int batch) const {\n    DCHECK_LT(batch, batch_size());\n    return (*batch_pointers_vec_)(batch);\n  }\n\n  inline int total_nnz() const {\n    DCHECK(valid());\n    return (*batch_pointers_vec_)(batch_size());\n  }\n\n  inline Tensor& dense_shape() {\n    DCHECK(valid());\n    return dense_shape_;\n  }\n\n  inline const Tensor& dense_shape() const {\n    DCHECK(valid());\n    return dense_shape_;\n  }\n\n  inline TTypes<int32>::UnalignedVec row_pointers_vec(int batch) {\n    DCHECK(valid());\n    DCHECK_LT(batch, batch_size());\n    const int64_t rows = dense_shape().vec<int64_t>()((dims() == 2) ? 0 : 1);\n    const int offset = batch * (rows + 1);\n    return TTypes<int32>::UnalignedVec(row_pointers_vec_->data() + offset,\n                                       rows + 1);\n  }\n\n  inline TTypes<int32>::UnalignedConstVec row_pointers_vec(int batch) const {\n    DCHECK(valid());\n    DCHECK_LT(batch, batch_size());\n    const int64_t rows = dense_shape().vec<int64_t>()((dims() == 2) ? 0 : 1);\n    const int offset = batch * (rows + 1);\n    return TTypes<int32>::UnalignedConstVec(row_pointers_vec_->data() + offset,\n                                            rows + 1);\n  }\n\n  inline TTypes<int32>::UnalignedVec col_indices_vec(int batch) {\n    DCHECK(valid());\n    DCHECK_LT(batch, batch_size());\n    const int offset = (*batch_pointers_vec_)(batch);\n    const int nnz_in_batch = nnz(batch);\n    return TTypes<int32>::UnalignedVec(col_indices_vec_->data() + offset,\n                                       nnz_in_batch);\n  }\n\n  inline TTypes<int32>::UnalignedConstVec col_indices_vec(int batch) const {\n    DCHECK(valid());\n    DCHECK_LT(batch, batch_size());\n    const int offset = (*batch_pointers_vec_)(batch);\n    const int nnz_in_batch = nnz(batch);\n    return TTypes<int32>::UnalignedConstVec(col_indices_vec_->data() + offset,\n                                            nnz_in_batch);\n  }\n\n  template <typename T>\n  inline typename TTypes<T>::UnalignedVec values_vec(int batch) {\n    DCHECK(valid());\n    DCHECK_LT(batch, batch_size());\n    const int offset = (*batch_pointers_vec_)(batch);\n    const int nnz_in_batch = nnz(batch);\n    return typename TTypes<T>::UnalignedVec(values().vec<T>().data() + offset,\n                                            nnz_in_batch);\n  }\n\n  template <typename T>\n  inline typename TTypes<T>::UnalignedConstVec values_vec(int batch) const {\n    DCHECK(valid());\n    DCHECK_LT(batch, batch_size());\n    const int offset = (*batch_pointers_vec_)(batch);\n    const int nnz_in_batch = nnz(batch);\n    return typename TTypes<T>::UnalignedConstVec(\n        values().vec<T>().data() + offset, nnz_in_batch);\n  }\n\n  inline Tensor& row_pointers() {\n    DCHECK(valid());\n    return row_pointers_;\n  }\n\n  inline const Tensor& row_pointers() const {\n    DCHECK(valid());\n    return row_pointers_;\n  }\n\n  inline Tensor& col_indices() {\n    DCHECK(valid());\n    return col_indices_;\n  }\n\n  inline const Tensor& col_indices() const {\n    DCHECK(valid());\n    return col_indices_;\n  }\n\n  inline Tensor& values() {\n    DCHECK(valid());\n    return values_;\n  }\n\n  inline const Tensor& values() const {\n    DCHECK(valid());\n    return values_;\n  }\n\n  inline Tensor& batch_pointers() {\n    DCHECK(valid());\n    return batch_pointers_;\n  }\n\n  inline const Tensor& batch_pointers() const {\n    DCHECK(valid());\n    return batch_pointers_;\n  }\n\n  std::string TypeName() const { return kTypeName; }\n\n  // TODO(ebrevdo): A better debug string.\n  std::string DebugString() const { return dense_shape_.DebugString(); }\n\n  // Returns the number of elements.  This is equal to 1 if the\n  // CSRSparseMatrix is a singleton matrix (dense_shape is length 2).\n  int batch_size() const {\n    DCHECK(valid());\n    return batch_pointers_.NumElements() - 1;\n  }\n\n  bool Decode(const VariantTensorData& p) {\n    if (p.tensors_.empty()) return false;\n    Metadata metadata;\n    if (!p.get_metadata(&metadata)) return false;\n    const bool validated = metadata.validated;\n    const DataType dtype = metadata.dtype;\n\n    // p.tensors_ should contain tensors {dense_shape, batch_pointers,\n    // row_pointers, col_indices, values}.\n    if (p.tensors_.size() != 5) return false;\n\n    Tensor dense_shape = p.tensors_[0];\n    if (dense_shape.dtype() != DT_INT64) return false;\n    if (dense_shape.dims() != 1) return false;\n    int rank = dense_shape.dim_size(0);\n    if (rank < 2 || rank > 3) return false;\n\n    Tensor batch_pointers(p.tensors_[1]);\n    Tensor row_pointers(p.tensors_[2]);\n    Tensor col_indices(p.tensors_[3]);\n    Tensor values(p.tensors_[4]);\n\n    // Check that the validated bool is consistent with the data.\n    Status s = ValidateTypesAndShapes(dtype, dense_shape, batch_pointers,\n                                      row_pointers, col_indices, values);\n    if (s.ok() != validated) return false;\n\n    // Save to this object.\n    metadata_ = metadata;\n    dense_shape_ = std::move(dense_shape);\n    batch_pointers_ = std::move(batch_pointers);\n    row_pointers_ = std::move(row_pointers);\n    col_indices_ = std::move(col_indices);\n    values_ = std::move(values);\n    SetupVecs();\n    return true;\n  }\n\n  void Encode(VariantTensorData* p) const {\n    DCHECK(valid());\n\n    // Store metadata_ to p's metadata\n    p->set_metadata(metadata_);\n\n    // Store dense_shape, row_pointers, col_indices, and values to p->tensors_.\n    p->tensors_.reserve(5);\n    p->tensors_.push_back(dense_shape_);\n    p->tensors_.push_back(batch_pointers_);\n    p->tensors_.push_back(row_pointers_);\n    p->tensors_.push_back(col_indices_);\n    p->tensors_.push_back(values_);\n  }\n\n  // This static method copies CSRSparseMatrices in all directions:\n  //   Host->Device, Device->Host, and Device->Device.\n  static Status DeviceCopy(\n      const CSRSparseMatrix& from, CSRSparseMatrix* to,\n      const UnaryVariantOpRegistry::AsyncTensorDeviceCopyFn& copy) {\n    VLOG(2) << \"DeviceCopy from type: \" << DataTypeString(from.dtype())\n            << \" and shape: \" << from.dense_shape().DebugString();\n    Tensor to_row_ptr(DT_INT32);\n    Tensor to_col_ind(DT_INT32);\n    Tensor to_values(from.dtype());\n    TF_RETURN_IF_ERROR(copy(from.row_pointers(), &to_row_ptr));\n    TF_RETURN_IF_ERROR(copy(from.col_indices(), &to_col_ind));\n    TF_RETURN_IF_ERROR(copy(from.values(), &to_values));\n    return CreateCSRSparseMatrix(from.dtype(),\n                                 from.dense_shape(),     // Always on host.\n                                 from.batch_pointers(),  // Always on host.\n                                 to_row_ptr, to_col_ind, to_values, to);\n  }\n\n private:\n  CSRSparseMatrix(DataType dtype, const Tensor& dense_shape,\n                  const Tensor& batch_pointers, const Tensor& row_pointers,\n                  const Tensor& col_indices, const Tensor& values)\n      : metadata_{false, dtype},\n        dense_shape_(dense_shape),\n        batch_pointers_(batch_pointers),\n        row_pointers_(row_pointers),\n        col_indices_(col_indices),\n        values_(values) {}\n\n  void SetupVecs() {\n    if (!metadata_.validated) return;\n    batch_pointers_vec_.reset(\n        new TTypes<int32>::Vec(batch_pointers_.vec<int32>()));\n    row_pointers_vec_.reset(new TTypes<int32>::Vec(row_pointers_.vec<int32>()));\n    col_indices_vec_.reset(new TTypes<int32>::Vec(col_indices_.vec<int32>()));\n  }\n\n  void ClearVecs() {\n    batch_pointers_vec_.reset();\n    row_pointers_vec_.reset();\n    col_indices_vec_.reset();\n  }\n\n  static Status ValidateTypesAndShapes(DataType dtype,\n                                       const Tensor& dense_shape,\n                                       const Tensor& batch_pointers,\n                                       const Tensor& row_pointers,\n                                       const Tensor& col_indices,\n                                       const Tensor& values) {\n    // TODO(ebrevdo): Consider adding support for other floating point types\n    // (namely, float16).\n    if (dtype != DT_FLOAT && dtype != DT_DOUBLE && dtype != DT_COMPLEX64 &&\n        dtype != DT_COMPLEX128) {\n      return errors::InvalidArgument(\n          \"CSRSparseMatrix::Validate: dtype = \", DataTypeString(dtype),\n          \" not in {float32, float64, complex64, complex128}\");\n    }\n    // dense_shape checks\n    if (dense_shape.dtype() != DT_INT64) {\n      return errors::InvalidArgument(\n          \"CSRSparseMatrix::Validate: dense_shape.dtype() = \",\n          DataTypeString(dense_shape.dtype()), \" != int64\");\n    }\n    if (dense_shape.dims() != 1) {\n      return errors::InvalidArgument(\n          \"CSRSparseMatrix::Validate: dense_shape should be a vector, but saw \"\n          \"tensor: \",\n          dense_shape.DebugString());\n    }\n    int rank = dense_shape.dim_size(0);\n    if (rank < 2 || rank > 3) {\n      return errors::InvalidArgument(\n          \"CSRSparseMatrix::Validate: dense_shape should be a 2- or 3- vector, \"\n          \"but saw: \",\n          dense_shape.SummarizeValue(5));\n    }\n    auto dense_shape_t = dense_shape.vec<int64_t>();\n    const int64_t batch_size = (rank == 2) ? 1 : dense_shape_t(0);\n    const int64_t num_rows = (rank == 2) ? dense_shape_t(0) : dense_shape_t(1);\n\n    if (batch_pointers.dtype() != DT_INT32) {\n      return errors::InvalidArgument(\n          \"CSRSparseMatrix::Validate: batch_pointers.dtype() = \",\n          DataTypeString(batch_pointers.dtype()), \" != int32\");\n    }\n    if (batch_pointers.dims() != 1) {\n      return errors::InvalidArgument(\n          \"CSRSparseMatrix::Validate: batch_indices is not a vector, saw \"\n          \"shape: \",\n          batch_pointers.shape().DebugString());\n    }\n\n    // batch size checks\n    if (batch_size != batch_pointers.NumElements() - 1) {\n      return errors::InvalidArgument(\n          \"CSRSparseMatrix::Validate: dense_shape is \",\n          dense_shape.SummarizeValue(5),\n          \" but batch pointers implies batch size is \",\n          batch_pointers.NumElements() - 1);\n    }\n\n    if (row_pointers.dtype() != DT_INT32) {\n      return errors::InvalidArgument(\n          \"CSRSparseMatrix::Validate: row_pointers.dtype() = \",\n          DataTypeString(row_pointers.dtype()), \" != int32\");\n    }\n    if (row_pointers.dims() != 1) {\n      return errors::InvalidArgument(\n          \"CSRSparseMatrix::Validate: row_pointers is not a vector, saw \"\n          \"shape: \",\n          row_pointers.shape().DebugString());\n    }\n    if (row_pointers.dim_size(0) != batch_size * (num_rows + 1)) {\n      return errors::InvalidArgument(\n          \"CSRSparseMatrix::Validate: row_pointers should have size batch_size \"\n          \"* (num_rows + 1), saw shapes: \",\n          dense_shape.DebugString(), \" vs. \",\n          row_pointers.shape().DebugString());\n    }\n    if (col_indices.dtype() != DT_INT32) {\n      return errors::InvalidArgument(\n          \"CSRSparseMatrix::Validate: col_indices.dtype() = \",\n          DataTypeString(col_indices.dtype()), \" != int32\");\n    }\n    if (col_indices.dims() != 1) {\n      return errors::InvalidArgument(\n          \"CSRSparseMatrix::Validate: col_indices is not a vector, saw shape: \",\n          col_indices.shape().DebugString());\n    }\n    if (values.dtype() != dtype) {\n      return errors::InvalidArgument(\n          \"CSRSparseMatrix::Validate: values.dtype() = \",\n          DataTypeString(values.dtype()),\n          \" != dtype = \", DataTypeString(dtype));\n    }\n    if (values.dims() != 1) {\n      return errors::InvalidArgument(\n          \"CSRSparseMatrix::Validate: values is not a vector, saw shape: \",\n          values.shape().DebugString());\n    }\n    if (col_indices.dim_size(0) != values.dim_size(0)) {\n      return errors::InvalidArgument(\n          \"CSRSparseMatrix::Validate: size(col_indices) = \",\n          col_indices.dim_size(0), \" != size(values) = \", values.dim_size(0));\n    }\n    return OkStatus();\n  }\n\n  struct Metadata {\n    bool validated;\n    DataType dtype;\n  };\n  Metadata metadata_;\n  Tensor dense_shape_;\n  Tensor batch_pointers_;\n  Tensor row_pointers_;\n  Tensor col_indices_;\n  Tensor values_;\n  std::unique_ptr<TTypes<int32>::Vec> batch_pointers_vec_;\n  std::unique_ptr<TTypes<int32>::Vec> row_pointers_vec_;\n  std::unique_ptr<TTypes<int32>::Vec> col_indices_vec_;\n};\n\n// Call BinaryFunctor<Device, T>()(ctx, a, b, c)\n// where T depends on a.dtype().  T will be one of: float, double,\n// complex64, complex128.\ntemplate <typename Device, template <typename, typename> class BinaryFunctor>\nStatus CSRSparseMatrixBinaryHelper(OpKernelContext* ctx,\n                                   const CSRSparseMatrix& a,\n                                   const CSRSparseMatrix& b,\n                                   CSRSparseMatrix* c) {\n  DataType dt = a.dtype();\n  if (dt != b.dtype()) {\n    return errors::InvalidArgument(\n        \"CSRSparseMatrixBinaryHelper: Inconsistent dtypes for input matrices, \"\n        \"a \"\n        \"dtype: \",\n        DataTypeString(dt), \", b dtype: \", DataTypeString(b.dtype()));\n  }\n  switch (dt) {\n    case DT_FLOAT: {\n      BinaryFunctor<Device, float> functor(ctx);\n      return functor(a, b, c);\n    }\n    case DT_DOUBLE: {\n      BinaryFunctor<Device, double> functor(ctx);\n      return functor(a, b, c);\n    }\n    case DT_COMPLEX64: {\n      BinaryFunctor<Device, complex64> functor(ctx);\n      return functor(a, b, c);\n    }\n    case DT_COMPLEX128: {\n      BinaryFunctor<Device, complex128> functor(ctx);\n      return functor(a, b, c);\n    }\n    default:\n      return errors::InvalidArgument(\n          \"CSRSparseMatrixBinaryHelper: a.dtype (\", DataTypeString(dt),\n          \") is not one of: float, double, complex64, complex128\");\n  }\n}\n\n// Call UnaryFunctor<Device, T>()(ctx, a, b)\n// where T depends on a.dtype().  T will be one of: float, double,\n// complex64, complex128.\ntemplate <typename Device, template <typename, typename> class UnaryFunctor>\nStatus CSRSparseMatrixUnaryHelper(OpKernelContext* ctx,\n                                  const CSRSparseMatrix& a,\n                                  CSRSparseMatrix* b) {\n  DataType dt = a.dtype();\n  switch (dt) {\n    case DT_FLOAT: {\n      UnaryFunctor<Device, float> functor(ctx);\n      return functor(a, b);\n    }\n    case DT_DOUBLE: {\n      UnaryFunctor<Device, double> functor(ctx);\n      return functor(a, b);\n    }\n    case DT_COMPLEX64: {\n      UnaryFunctor<Device, complex64> functor(ctx);\n      return functor(a, b);\n    }\n    case DT_COMPLEX128: {\n      UnaryFunctor<Device, complex128> functor(ctx);\n      return functor(a, b);\n    }\n    default:\n      return errors::InvalidArgument(\n          \"CSRSparseMatrixUnaryHelper: a.dtype (\", DataTypeString(dt),\n          \") is not one of: float, double, complex64, complex128\");\n  }\n}\n\ntemplate <typename T>\nstruct ConstCSRComponent {\n  TTypes<int32>::UnalignedConstVec row_ptr;\n  TTypes<int32>::UnalignedConstVec col_ind;\n  typename TTypes<T>::UnalignedConstVec values;\n  TTypes<int64_t>::ConstVec dense_shape_host;\n};\n\ntemplate <typename T>\nstruct CSRComponent {\n  TTypes<int32>::UnalignedVec row_ptr;\n  TTypes<int32>::UnalignedVec col_ind;\n  typename TTypes<T>::UnalignedVec values;\n  TTypes<int64_t>::Vec dense_shape_host;\n};\n\ntemplate <typename T>\nStatus ExtractVariantFromInput(OpKernelContext* ctx, int index,\n                               const T** value) {\n  const Tensor& input_t = ctx->input(index);\n  const Variant& input_variant = input_t.scalar<Variant>()();\n  *value = input_variant.get<T>();\n  if (*value == nullptr) {\n    return errors::InvalidArgument(\"Could not retrieve Variant input \", index);\n  }\n  if (!(*value)->valid()) {\n    return errors::InvalidArgument(\"Variant input \", index, \" is not valid.\");\n  }\n  return OkStatus();\n}\n\n}  // namespace tensorflow\n\n#endif  // TENSORFLOW_CORE_KERNELS_SPARSE_SPARSE_MATRIX_H_\n", "# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"CSR sparse matrix tests.\"\"\"\n\nimport numpy as np\nfrom scipy import sparse\n\nfrom tensorflow.core.framework import tensor_pb2\nfrom tensorflow.core.protobuf import config_pb2\nfrom tensorflow.python.client import session\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import random_seed\nfrom tensorflow.python.framework import sparse_tensor\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.framework import tensor_util\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import linalg_ops\nfrom tensorflow.python.ops import map_fn\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.ops import random_ops\nfrom tensorflow.python.ops import sparse_ops\nfrom tensorflow.python.ops import state_ops\nfrom tensorflow.python.ops import variable_scope\nfrom tensorflow.python.ops.linalg.sparse import sparse_csr_matrix_ops\nfrom tensorflow.python.platform import test\nfrom tensorflow.python.platform import tf_logging\n\nCPU = \"/device:CPU:0\"\nGPU = \"/device:GPU:0\"\n\n\ndef dense_to_csr_sparse_matrix(dense):\n  dense_t = ops.convert_to_tensor(dense)\n  locs = array_ops.stop_gradient(array_ops.where(math_ops.abs(dense_t) > 0))\n  return sparse_csr_matrix_ops.dense_to_csr_sparse_matrix(dense_t, locs)\n\n\ndef _swap(a, i, j):\n  a[i], a[j] = a[j], a[i]\n\n\ndef twist_matrix(matrix, permutation_indices):\n  \"\"\"Permute the rows and columns of a 2D or (batched) 3D Tensor.\"\"\"\n  # Shuffle the rows and columns with the same permutation.\n  if matrix.shape.ndims == 2:\n    # Invert the permutation since `tf.gather` and `tf.gather_nd` need the\n    # mapping from each index `i` to the index that maps to `i`.\n    permutation_indices_inv = array_ops.invert_permutation(permutation_indices)\n    matrix = array_ops.gather(matrix, permutation_indices_inv, axis=0)\n    matrix = array_ops.gather(matrix, permutation_indices_inv, axis=1)\n  elif matrix.shape.ndims == 3:\n    permutation_indices_inv = map_fn.map_fn(array_ops.invert_permutation,\n                                            permutation_indices)\n    # For 3D Tensors, it's easy to shuffle the rows but not the columns. We\n    # permute the rows, transpose, permute the rows again, and transpose back.\n    batch_size = matrix.shape[0]\n    batch_indices = array_ops.broadcast_to(\n        math_ops.range(batch_size)[:, None], permutation_indices.shape)\n    for _ in range(2):\n      matrix = array_ops.gather_nd(\n          matrix,\n          array_ops.stack([batch_indices, permutation_indices_inv], axis=-1))\n      # Transpose the matrix, or equivalently, swap dimensions 1 and 2.\n      matrix = array_ops.transpose(matrix, perm=[0, 2, 1])\n  else:\n    raise ValueError(\"Input matrix must have rank 2 or 3. Got: {}\".format(\n        matrix.shape.ndims))\n\n  return matrix\n\n\nclass CSRSparseMatrixOpsTest(test.TestCase):\n\n  @classmethod\n  def setUpClass(cls):  # pylint: disable=g-missing-super-call\n    cls._gpu_available = test_util.is_gpu_available()\n\n  # TODO(ebrevdo): This will work once we find a way to get rendezvous\n  # working for CSRSparseMatrix and can remove the HostMemory\n  # annotations for the other ops.\n  @test_util.run_in_graph_and_eager_modes\n  def DISABLEDtestFromProto(self):\n    if not self._gpu_available:\n      return\n\n    a_indices = np.array([[0, 0], [2, 3]])\n    a_values = np.asarray([1.0, 5.0], dtype=np.float32)\n    a_dense_shape = np.asarray([5, 6], dtype=np.int64)\n    a_sparse_mat = sparse.coo_matrix(\n        (a_values, (a_indices[:, 0], a_indices[:, 1])), shape=a_dense_shape)\n    a_csr_mat = a_sparse_mat.tocsr()\n    a_col_inds = a_csr_mat.indices\n    a_row_ptrs = a_csr_mat.indptr\n\n    # Format of SparseMatrix:\n    #  type_name == \"tensorflow::CSRSparseMatrix\"\n    #  metadata == b (validated)\n    #  tensors == [dense_shape, row_ptrs, col_indices, values]\n    dense_shape_proto = tensor_util.make_tensor_proto(a_dense_shape)\n    row_ptrs_proto = tensor_util.make_tensor_proto(a_row_ptrs)\n    col_inds_proto = tensor_util.make_tensor_proto(a_col_inds)\n    values_proto = tensor_util.make_tensor_proto(a_values)\n    variant_tensor_data = tensor_pb2.VariantTensorDataProto(\n        type_name=\"tensorflow::CSRSparseMatrix\",\n        metadata=np.asarray(True).tobytes(),\n        tensors=[\n            dense_shape_proto, row_ptrs_proto, col_inds_proto, values_proto\n        ])\n    tensor_proto = tensor_pb2.TensorProto(\n        dtype=dtypes.variant.as_datatype_enum,\n        tensor_shape=tensor_shape.TensorShape([]).as_proto())\n    tensor_proto.variant_val.extend([variant_tensor_data])\n    a_sm = constant_op.constant(tensor_proto)\n    a_rt = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n        a_sm, type=dtypes.float32)\n    self.evaluate(a_rt)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testSparseTensorConversion(self):\n    a_indices = np.array([[0, 0], [2, 3], [2, 4], [3, 0]])\n    a_values = [1.0, 5.0, -1.0, -2.0]\n    a_dense_shape = [5, 6]\n    a_sparse_mat = sparse.coo_matrix(\n        (a_values, (a_indices[:, 0], a_indices[:, 1])), shape=a_dense_shape)\n    a_csr_mat = a_sparse_mat.tocsr()\n\n    # Convert 2D SparseTensor to CSR Matrix\n    a_st = sparse_tensor.SparseTensor(a_indices, a_values, a_dense_shape)\n    a_st = math_ops.cast(a_st, dtypes.float32)\n    a_sm = sparse_csr_matrix_ops.sparse_tensor_to_csr_sparse_matrix(\n        a_st.indices, a_st.values, a_st.dense_shape)\n\n    # Get row indices and columns for batch 0.\n    a_sm_row_ptrs, a_sm_col_inds, a_sm_values = (\n        sparse_csr_matrix_ops.csr_sparse_matrix_components(\n            a_sm, 0, type=a_st.dtype))\n\n    a_sm_row_ptrs_values, a_sm_col_inds_values, a_sm_values_values = (\n        self.evaluate((a_sm_row_ptrs, a_sm_col_inds, a_sm_values)))\n\n    self.assertAllEqual(a_csr_mat.indices, a_sm_col_inds_values)\n    self.assertAllEqual(a_csr_mat.indptr, a_sm_row_ptrs_values)\n    self.assertAllClose(a_values, a_sm_values_values)\n\n    # Convert CSR Matrix to 2D SparseTensor\n    a_st_rt = sparse_csr_matrix_ops.csr_sparse_matrix_to_sparse_tensor(\n        a_sm, type=a_st.dtype)\n    a_st_rt_value = self.evaluate(a_st_rt)\n\n    self.assertAllEqual(a_indices, a_st_rt_value.indices)\n    self.assertAllClose(a_values, a_st_rt_value.values)\n    self.assertAllEqual(a_dense_shape, a_st_rt_value.dense_shape)\n\n  def testSparseTensorConversionInvalidInputShapes(self):\n    values = constant_op.constant(\n        0.554979503, shape=[5], dtype=dtypes.float32)\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 1\"):\n      indices = constant_op.constant(0, shape=[5, 2], dtype=dtypes.int64)\n      dense_shape = constant_op.constant(53, shape=[], dtype=dtypes.int64)\n      csr = sparse_csr_matrix_ops.sparse_tensor_to_csr_sparse_matrix(\n          indices=indices, values=values, dense_shape=dense_shape)\n      self.evaluate(csr)\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 2\"):\n      indices = constant_op.constant(0, shape=[5], dtype=dtypes.int64)\n      dense_shape = constant_op.constant(53, shape=[1], dtype=dtypes.int64)\n      csr = sparse_csr_matrix_ops.sparse_tensor_to_csr_sparse_matrix(\n          indices=indices, values=values, dense_shape=dense_shape)\n      self.evaluate(csr)\n\n  # TODO(b/139491352): Add handle_data propagation to array_ops.identity.\n  @test_util.run_deprecated_v1\n  def testCSRSparseMatrixResourceVariable(self):\n    if not self._gpu_available:\n      return\n\n    sparsify = lambda m: m * (m > 0)\n    dense_shape = [53, 65, 127]\n    a_mats = sparsify(np.random.randn(*dense_shape)).astype(np.float32)\n\n    a_sm = dense_to_csr_sparse_matrix(a_mats)\n    with ops.device(\"/gpu:0\"):\n      v = variable_scope.get_variable(\"sm\", initializer=a_sm, use_resource=True)\n      v_id = array_ops.identity(v)\n      self.assertEqual(\n          sparse_csr_matrix_ops.dense_shape_and_type(v_id).shape, a_mats.shape)\n      a_rt = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n          v, type=dtypes.float32)\n    v_reassign = state_ops.assign(v, v_id).op\n    with self.assertRaisesOpError(\"uninitialized\"):\n      self.evaluate(a_rt)\n    self.evaluate(v.initializer)\n    a_rt_value = self.evaluate(a_rt)\n    self.assertAllClose(a_mats, a_rt_value)\n    self.evaluate(v_reassign)\n    a_rt_reassigned_value = self.evaluate(a_rt)\n    self.assertAllClose(a_mats, a_rt_reassigned_value)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testBatchSparseTensorConversion(self):\n    a_indices = np.array([[0, 0, 0], [0, 2, 3], [2, 0, 1]])\n    a_values = [1.0, 5.0, 6.0]\n    a_dense_shape = [3, 5, 6]\n    a_sparse_mats = [\n        sparse.coo_matrix(([1.0, 5.0], ([0, 2], [0, 3])),\n                          shape=a_dense_shape[1:]),\n        sparse.coo_matrix(([], ([], [])), shape=a_dense_shape[1:]),\n        sparse.coo_matrix(([6.0], ([0], [1])), shape=a_dense_shape[1:])\n    ]\n    a_csr_mats = [m.tocsr() for m in a_sparse_mats]\n\n    # Convert 3D SparseTensor to CSR Matrix\n    a_st = sparse_tensor.SparseTensor(a_indices, a_values, a_dense_shape)\n    a_st = math_ops.cast(a_st, dtypes.float32)\n    a_sm = sparse_csr_matrix_ops.sparse_tensor_to_csr_sparse_matrix(\n        a_st.indices, a_st.values, a_st.dense_shape)\n\n    # Get row indices and columns for batches.\n    a_sm_components = [\n        sparse_csr_matrix_ops.csr_sparse_matrix_components(\n            a_sm, i, type=a_st.dtype) for i in range(3)\n    ]\n\n    a_sm_values = self.evaluate(a_sm_components)\n\n    for i, (a_sm_val, a_csr_mat) in enumerate(zip(a_sm_values, a_csr_mats)):\n      tf_logging.info(\"Comparing batch %d\" % i)\n      self.assertAllEqual(a_csr_mat.indptr, a_sm_val.row_ptrs)\n      self.assertAllEqual(a_csr_mat.indices, a_sm_val.col_inds)\n      self.assertAllClose(a_csr_mat.data, a_sm_val.values)\n\n    # Convert CSR batched Matrix to 3D SparseTensor\n    a_st_rt = sparse_csr_matrix_ops.csr_sparse_matrix_to_sparse_tensor(\n        a_sm, type=a_st.dtype)\n    a_st_rt_value = self.evaluate(a_st_rt)\n\n    self.assertAllEqual(a_indices, a_st_rt_value.indices)\n    self.assertAllClose(a_values, a_st_rt_value.values)\n    self.assertAllEqual(a_dense_shape, a_st_rt_value.dense_shape)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testLargeBatchSparseTensorConversion(self):\n    # Test two sets of conversions to check behavior of the ops in a\n    # concurrent environment (parallel executions of the ST -> SM ops).\n\n    sparsify = lambda m: m * (m > 0)\n    dense_shape = [53, 65, 127]\n\n    mats = [\n        sparsify(np.random.randn(*dense_shape)).astype(np.float32)\n        for _ in range(2)\n    ]\n    csr_mats = [list(map(sparse.csr_matrix, mat)) for mat in mats]\n    mats_t = [ops.convert_to_tensor(mat) for mat in mats]\n    mats_locs = [array_ops.where(mat_t > 0) for mat_t in mats_t]\n    sparse_tensors = list()\n    for mat_t, mat_loc in zip(mats_t, mats_locs):\n      sparse_tensors.append(\n          sparse_tensor.SparseTensor(mat_loc,\n                                     array_ops.gather_nd(mat_t,\n                                                         mat_loc), dense_shape))\n    sparse_matrices = [\n        sparse_csr_matrix_ops.sparse_tensor_to_csr_sparse_matrix(\n            st.indices, st.values, st.dense_shape) for st in sparse_tensors\n    ]\n    sm_nnz = [\n        sparse_csr_matrix_ops.sparse_matrix_nnz(sm) for sm in sparse_matrices\n    ]\n\n    # Get row indices and columns for batches.\n    sm_components = list()\n    for sm in sparse_matrices:\n      sm_components.append([\n          sparse_csr_matrix_ops.csr_sparse_matrix_components(\n              sm, i, type=dtypes.float32) for i in range(dense_shape[0])\n      ])\n\n    sm_nnz_values, sm_values = self.evaluate((sm_nnz, sm_components))\n\n    for i, (sm_values_i, csr_mats_i) in enumerate(zip(sm_values, csr_mats)):\n      for b, (sm_val, csr_mat) in enumerate(zip(sm_values_i, csr_mats_i)):\n        tf_logging.info(\"Comparing matrix %d batch %d\" % (i, b))\n        self.assertEqual(csr_mat.nnz, sm_nnz_values[i][b])\n        self.assertAllEqual(csr_mat.indptr, sm_val.row_ptrs)\n        self.assertAllEqual(csr_mat.indices, sm_val.col_inds)\n        self.assertAllClose(csr_mat.data, sm_val.values)\n\n    # Convert CSR batched Matrix to 3D SparseTensor\n    st_rt = [\n        sparse_csr_matrix_ops.csr_sparse_matrix_to_sparse_tensor(\n            sm, type=dtypes.float32) for sm in sparse_matrices\n    ]\n\n    st_values, st_rt_values = self.evaluate((sparse_tensors, st_rt))\n\n    for (st_value, st_rt_value) in zip(st_values, st_rt_values):\n      self.assertAllEqual(st_value.indices, st_rt_value.indices)\n      self.assertAllClose(st_value.values, st_rt_value.values)\n      self.assertAllEqual(dense_shape, st_rt_value.dense_shape)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testDenseConversion(self):\n    a_indices = np.array([[0, 0], [2, 3], [2, 4], [3, 0]])\n    a_values = np.array([1.0, 5.0, -1.0, -2.0]).astype(np.float32)\n    a_dense_shape = [5, 6]\n    a_sparse_mat = sparse.coo_matrix(\n        (a_values, (a_indices[:, 0], a_indices[:, 1])), shape=a_dense_shape)\n    a_csr_mat = a_sparse_mat.tocsr()\n    a_dense = a_sparse_mat.todense()\n\n    # Convert 2D SparseTensor to CSR Matrix\n    a_sm = dense_to_csr_sparse_matrix(a_dense)\n\n    # Get row indices and columns for batch 0.\n    a_sm_row_ptrs, a_sm_col_inds, a_sm_values = (\n        sparse_csr_matrix_ops.csr_sparse_matrix_components(\n            a_sm, 0, type=dtypes.float32))\n\n    a_sm_row_ptrs_values, a_sm_col_inds_values, a_sm_values_values = (\n        self.evaluate((a_sm_row_ptrs, a_sm_col_inds, a_sm_values)))\n\n    self.assertAllEqual(a_csr_mat.indices, a_sm_col_inds_values)\n    self.assertAllEqual(a_csr_mat.indptr, a_sm_row_ptrs_values)\n    self.assertAllClose(a_values, a_sm_values_values)\n\n    # Convert CSR Matrix to 2D dense matrix\n    a_rt = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n        a_sm, dtypes.float32)\n    a_rt_value = self.evaluate(a_rt)\n\n    self.assertAllEqual(a_dense, a_rt_value)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testBatchDenseConversion(self):\n    a_dense_shape = [4, 5, 6]\n    a_sparse_mats = [\n        sparse.coo_matrix(([1.0, 5.0], ([0, 2], [0, 3])),\n                          shape=a_dense_shape[1:]),\n        sparse.coo_matrix(([], ([], [])), shape=a_dense_shape[1:]),\n        sparse.coo_matrix(([6.0], ([0], [1])), shape=a_dense_shape[1:]),\n        sparse.coo_matrix(([], ([], [])), shape=a_dense_shape[1:]),\n    ]\n    a_csr_mats = [m.tocsr() for m in a_sparse_mats]\n    a_dense = np.asarray([m.todense() for m in a_sparse_mats], dtype=np.float32)\n\n    # Convert 3D SparseTensor to CSR Matrix\n    a_sm = dense_to_csr_sparse_matrix(a_dense)\n\n    # Get row indices and columns for batches.\n    a_sm_components = [\n        sparse_csr_matrix_ops.csr_sparse_matrix_components(\n            a_sm, i, type=dtypes.float32) for i in range(3)\n    ]\n\n    a_sm_values = self.evaluate(a_sm_components)\n\n    for i, (a_sm_val, a_csr_mat) in enumerate(zip(a_sm_values, a_csr_mats)):\n      tf_logging.info(\"Comparing batch %d\" % i)\n      self.assertAllEqual(a_csr_mat.indptr, a_sm_val.row_ptrs)\n      self.assertAllEqual(a_csr_mat.indices, a_sm_val.col_inds)\n      self.assertAllClose(a_csr_mat.data, a_sm_val.values)\n\n    # Convert CSR batched Matrix to 3D SparseTensor\n    a_rt = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n        a_sm, type=dtypes.float32)\n    a_rt_value = self.evaluate(a_rt)\n\n    self.assertAllEqual(a_dense, a_rt_value)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testLargeBatchDenseConversion(self):\n    # Test two sets of conversions to check behavior of the ops in a\n    # concurrent environment (parallel executions of the ST -> SM\n    # ops).\n\n    sparsify = lambda m: m * (m > 0)\n    dense_shape = [53, 65, 127]\n\n    mats = [\n        sparsify(np.random.randn(*dense_shape)).astype(np.float32)\n        for _ in range(2)\n    ]\n    csr_mats = [[sparse.csr_matrix(m) for m in mat] for mat in mats]\n    mats_t = [ops.convert_to_tensor(mat) for mat in mats]\n    mats_locs = [array_ops.where(mat_t > 0) for mat_t in mats_t]\n    sparse_matrices = [\n        sparse_csr_matrix_ops.dense_to_csr_sparse_matrix(mat, mat_loc)\n        for (mat, mat_loc) in zip(mats_t, mats_locs)\n    ]\n    sm_nnz = [\n        sparse_csr_matrix_ops.sparse_matrix_nnz(sm) for sm in sparse_matrices\n    ]\n\n    # Get row indices and columns for batches.\n    sm_components = []\n    for sm in sparse_matrices:\n      sm_components.append([\n          sparse_csr_matrix_ops.csr_sparse_matrix_components(\n              sm, i, type=dtypes.float32) for i in range(dense_shape[0])\n      ])\n\n    sm_nnz_values, sm_values = self.evaluate((sm_nnz, sm_components))\n\n    for i, (sm_values_i, csr_mats_i) in enumerate(zip(sm_values, csr_mats)):\n      for b, (sm_val, csr_mat) in enumerate(zip(sm_values_i, csr_mats_i)):\n        tf_logging.info(\"Comparing matrix %d batch %d\" % (i, b))\n        self.assertEqual(csr_mat.nnz, sm_nnz_values[i][b])\n        self.assertAllEqual(csr_mat.indptr, sm_val.row_ptrs)\n        self.assertAllEqual(csr_mat.indices, sm_val.col_inds)\n        self.assertAllClose(csr_mat.data, sm_val.values)\n\n    # Convert CSR batched Matrix to 3D dense tensor\n    sm_rt = [\n        sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n            sm, type=dtypes.float32) for sm in sparse_matrices\n    ]\n\n    sm_rt_values = self.evaluate(sm_rt)\n\n    for (mat, sm_rt_value) in zip(mats, sm_rt_values):\n      self.assertAllEqual(mat, sm_rt_value)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testSparseMatrixAdd(self):\n    if not self._gpu_available:\n      return\n\n    a_indices = np.array([[0, 0], [2, 3]])\n    a_values = np.array([1.0, 5.0]).astype(np.float32)\n    a_dense_shape = [5, 6]\n    a_sparse_mat = sparse.coo_matrix(\n        (a_values, (a_indices[:, 0], a_indices[:, 1])), shape=a_dense_shape)\n    a_dense = a_sparse_mat.todense()\n\n    b_indices = np.array([[1, 0], [1, 4], [2, 3], [4, 1]])\n    b_values = np.array([1.0, 0.5, -5.0, 2.0]).astype(np.float32)\n    b_dense_shape = [5, 6]\n    b_sparse_mat = sparse.coo_matrix(\n        (b_values, (b_indices[:, 0], b_indices[:, 1])), shape=b_dense_shape)\n    b_dense = b_sparse_mat.todense()\n\n    for (alpha, beta) in [(1.0, 1.0), (1.0, -1.0), (0.25, 0.5)]:\n      a_sum_b_sparse_mat = alpha * a_sparse_mat + beta * b_sparse_mat\n\n      # Convert 2D SparseTensor to CSR Matrix\n      a_sm = dense_to_csr_sparse_matrix(a_dense)\n      b_sm = dense_to_csr_sparse_matrix(b_dense)\n      alpha = np.float32(alpha)\n      beta = np.float32(beta)\n      c_sm = sparse_csr_matrix_ops.sparse_matrix_add(\n          a_sm, b_sm, alpha=alpha, beta=beta)\n      c_dense = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n          c_sm, dtypes.float32)\n      c_dense_value = self.evaluate(c_dense)\n\n      self.assertAllClose(a_sum_b_sparse_mat.todense(), c_dense_value)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testLargeBatchSparseMatrixAdd(self):\n    if not self._gpu_available:\n      return\n\n    sparsify = lambda m: m * (m > 0)\n    dense_shape = [53, 65, 127]\n    a_mats = sparsify(np.random.randn(*dense_shape)).astype(np.float32)\n    b_mats = sparsify(np.random.randn(*dense_shape)).astype(np.float32)\n    for (alpha, beta) in [(1.0, 1.0), (1.0, -1.0), (0.25, 0.5)]:\n      tf_logging.info(\"testLargeBatchSparseMatrixAdd, comparing \"\n                      \"alpha, beta (%d, %d)\" % (alpha, beta))\n      a_sm = dense_to_csr_sparse_matrix(a_mats)\n      b_sm = dense_to_csr_sparse_matrix(b_mats)\n      alpha = np.float32(alpha)\n      beta = np.float32(beta)\n      c_sm = sparse_csr_matrix_ops.sparse_matrix_add(\n          a_sm, b_sm, alpha=alpha, beta=beta)\n      c_dense = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n          c_sm, dtypes.float32)\n      c_dense_value = self.evaluate(c_dense)\n\n      self.assertAllClose(c_dense_value, alpha * a_mats + beta * b_mats)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testSparseMatrixMatMul(self):\n    for shapes in [[(5, 6), (6, 1)], [(5, 6), (6, 2)]]:\n      a_indices = np.array([[0, 0], [2, 3]])\n      a_values = np.array([1.0, 5.0]).astype(np.float32)\n      a_dense_shape = shapes[0]\n      a_sparse_mat = sparse.coo_matrix(\n          (a_values, (a_indices[:, 0], a_indices[:, 1])), shape=a_dense_shape)\n      a_dense = a_sparse_mat.todense()\n\n      # Will multiply sparse a (shape=shapes[0]) by dense b (shape=shapes[1]).\n      b = np.random.randn(*shapes[1]).astype(np.float32)\n\n      a_sm = dense_to_csr_sparse_matrix(a_dense)\n      c = sparse_csr_matrix_ops.sparse_matrix_mat_mul(a=a_sm, b=b)\n      c_value = self.evaluate(c)\n\n      expected_c_value = a_sparse_mat.dot(b)\n      self.assertAllClose(expected_c_value, c_value)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testSparseMatrixMatMulConjugateOutput(self):\n    for shapes in [[(5, 6), (6, 1)], [(5, 6), (6, 2)]]:\n      a_indices = np.array([[0, 0], [2, 3]])\n      a_values = np.array([1.0 + 1.j, 5.0 - 2.j]).astype(np.complex64)\n      a_dense_shape = shapes[0]\n      a_sparse_mat = sparse.coo_matrix(\n          (a_values, (a_indices[:, 0], a_indices[:, 1])), shape=a_dense_shape)\n      a_dense = a_sparse_mat.todense()\n\n      # Will multiply sparse a (shape=shapes[0]) by dense b (shape=shapes[1]).\n      b = np.random.randn(*shapes[1]).astype(np.complex64)\n\n      a_sm = dense_to_csr_sparse_matrix(a_dense)\n      c = sparse_csr_matrix_ops.sparse_matrix_mat_mul(\n          a=a_sm, b=b, conjugate_output=True)\n      c_value = self.evaluate(c)\n\n      expected_c_value = self.evaluate(\n          math_ops.conj(test_util.matmul_without_tf32(a_dense, b)))\n      self.assertAllClose(expected_c_value, c_value)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testLargeBatchSparseMatrixMatMul(self):\n    dtypes_to_test = [np.float32, np.complex64]\n    sparsify = lambda m: m * (m > 0)\n    for dtype in dtypes_to_test:\n      for (transpose_a, transpose_b) in ((False, False), (False, True),\n                                         (True, False), (True, True)):\n        for (adjoint_a, adjoint_b) in ((False, False), (False, True),\n                                       (True, False), (True, True)):\n          if (transpose_a and adjoint_a) or (transpose_b and adjoint_b):\n            continue\n          for shapes in [[[53, 127, 65], [53, 65, 1]],\n                         [[53, 127, 1], [53, 1, 65]],\n                         [[53, 127, 65], [53, 65, 127]]]:\n            a_dense_shape = shapes[0]\n            b_dense_shape = shapes[1]\n            if transpose_a or adjoint_a:\n              _swap(a_dense_shape, -2, -1)\n            if transpose_b or adjoint_b:\n              _swap(b_dense_shape, -2, -1)\n            a_mats = sparsify(\n                (np.random.randn(*a_dense_shape) +\n                 1.j * np.random.randn(*a_dense_shape))).astype(dtype)\n            b_mats = (np.random.randn(*b_dense_shape) +\n                      1.j * np.random.randn(*b_dense_shape)).astype(dtype)\n            tf_logging.info(\n                \"testLargeBatchSparseMatrixMatMul transpose_a %s transpose_b \"\n                \"%s adjoint_a %s adjoint_b %s\" %\n                (transpose_a, transpose_b, adjoint_a, adjoint_b))\n            a_sm = dense_to_csr_sparse_matrix(a_mats)\n            c_t = sparse_csr_matrix_ops.sparse_matrix_mat_mul(\n                a_sm,\n                b_mats,\n                transpose_output=False,\n                conjugate_output=False,\n                transpose_a=transpose_a,\n                transpose_b=transpose_b,\n                adjoint_a=adjoint_a,\n                adjoint_b=adjoint_b)\n            c_dense_t = test_util.matmul_without_tf32(\n                a_mats,\n                b_mats,\n                transpose_a=transpose_a,\n                transpose_b=transpose_b,\n                adjoint_a=adjoint_a,\n                adjoint_b=adjoint_b)\n            self.assertAllEqual(c_dense_t.shape, c_t.shape)\n            c_t_value, c_dense_t_value = self.evaluate((c_t, c_dense_t))\n\n            self.assertAllClose(\n                c_t_value, c_dense_t_value, rtol=1e-6, atol=2e-5)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testLargeBatchSparseMatrixMatMulTransposed(self):\n    dtypes_to_test = [np.float32, np.complex64]\n\n    sparsify = lambda m: m * (m > 0)\n    for dtype in dtypes_to_test:\n      for (transpose_a, transpose_b) in ((False, False), (False, True),\n                                         (True, False), (True, True)):\n        for (adjoint_a, adjoint_b) in ((False, False), (False, True),\n                                       (True, False), (True, True)):\n          if (transpose_a and adjoint_a) or (transpose_b and adjoint_b):\n            continue\n          for shapes in [[[53, 127, 65], [53, 65, 1]],\n                         [[53, 127, 1], [53, 1, 65]],\n                         [[53, 127, 65], [53, 65, 127]]]:\n            a_dense_shape = shapes[0]\n            b_dense_shape = shapes[1]\n            if transpose_a or adjoint_a:\n              _swap(a_dense_shape, -2, -1)\n            if transpose_b or adjoint_b:\n              _swap(b_dense_shape, -2, -1)\n            a_mats = sparsify(\n                (np.random.randn(*a_dense_shape) +\n                 1.j * np.random.randn(*a_dense_shape))).astype(dtype)\n            b_mats = (np.random.randn(*b_dense_shape) +\n                      1.j * np.random.randn(*b_dense_shape)).astype(dtype)\n            tf_logging.info(\n                \"testLargeBatchSparseMatrixMatMul transpose_a %s transpose_b \"\n                \"%s adjoint_a %s adjoint_b %s\" %\n                (transpose_a, transpose_b, adjoint_a, adjoint_b))\n            a_sm = dense_to_csr_sparse_matrix(a_mats)\n            c_t = sparse_csr_matrix_ops.sparse_matrix_mat_mul(\n                a_sm,\n                b_mats,\n                transpose_output=True,\n                conjugate_output=False,\n                transpose_a=transpose_a,\n                transpose_b=transpose_b,\n                adjoint_a=adjoint_a,\n                adjoint_b=adjoint_b)\n\n            # Example: t(adj(a) . b) = t(b) . conj(a)\n            c_dense_t = test_util.matmul_without_tf32(\n                math_ops.conj(b_mats) if adjoint_b else b_mats,\n                math_ops.conj(a_mats) if adjoint_a else a_mats,\n                transpose_a=not (transpose_b or adjoint_b),\n                transpose_b=not (transpose_a or adjoint_a),\n                adjoint_a=False,\n                adjoint_b=False)\n            self.assertAllEqual(c_t.shape, c_dense_t.shape)\n            c_t_value, c_dense_t_value = self.evaluate((c_t, c_dense_t))\n            self.assertAllClose(\n                c_t_value, c_dense_t_value, rtol=1e-6, atol=2e-5)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testLargeBatchSparseMatrixMatMulConjugate(self):\n    sparsify = lambda m: m * (m > 0)\n    a_dense_shape = [53, 65, 127]\n    b_dense_shape = [53, 127, 67]\n    a_mats = sparsify(\n        (np.random.randn(*a_dense_shape) +\n         1.j * np.random.randn(*a_dense_shape))).astype(np.complex64)\n    b_mats = (np.random.randn(*b_dense_shape) +\n              1.j * np.random.randn(*b_dense_shape)).astype(np.complex64)\n    a_sm = dense_to_csr_sparse_matrix(a_mats)\n    c_t = sparse_csr_matrix_ops.sparse_matrix_mat_mul(\n        a_sm, b_mats, conjugate_output=True)\n\n    c_dense_t = math_ops.conj(test_util.matmul_without_tf32(a_mats, b_mats))\n    self.assertAllEqual(c_t.shape, c_dense_t.shape)\n    c_t_value, c_dense_t_value = self.evaluate((c_t, c_dense_t))\n\n    self.assertAllClose(c_t_value, c_dense_t_value, atol=1e-5, rtol=1e-5)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testSparseMatrixSparseMatMul(self):\n    a_indices = np.array([[0, 0], [2, 3]])\n    a_values = np.array([1.0, 5.0]).astype(np.float32)\n    a_dense_shape = [5, 6]\n    a_sparse_mat = sparse.coo_matrix(\n        (a_values, (a_indices[:, 0], a_indices[:, 1])), shape=a_dense_shape)\n    a_dense = a_sparse_mat.todense()\n\n    b_indices = np.array([[0, 0], [3, 0], [3, 1]])\n    b_values = np.array([2.0, 7.0, 8.0]).astype(np.float32)\n    b_dense_shape = [6, 7]\n    b_sparse_mat = sparse.coo_matrix(\n        (b_values, (b_indices[:, 0], b_indices[:, 1])), shape=b_dense_shape)\n    b_dense = b_sparse_mat.todense()\n\n    a_sm = dense_to_csr_sparse_matrix(a_dense)\n    b_sm = dense_to_csr_sparse_matrix(b_dense)\n    c_sm = sparse_csr_matrix_ops.sparse_matrix_sparse_mat_mul(\n        a=a_sm, b=b_sm, type=dtypes.float32)\n\n    c_sm_dense = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n        c_sm, dtypes.float32)\n    c_sm_dense_value = self.evaluate(c_sm_dense)\n\n    expected_c_value = a_sparse_mat.dot(b_sparse_mat).todense()\n    self.assertAllClose(expected_c_value, c_sm_dense_value)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testSparseMatrixSparseMatMul_NumericZerosNotPruned(self):\n    # Tests that numeric zeros appearing from the sparse-sparse matrix\n    # multiplication are not pruned from the sparse structural\n    a_indices = np.array([[0, 0], [0, 2]])\n    a_values = np.array([2.0, -1.0]).astype(np.float32)\n    a_dense_shape = [2, 3]\n    a_sparse_mat = sparse.coo_matrix(\n        (a_values, (a_indices[:, 0], a_indices[:, 1])), shape=a_dense_shape)\n    a_dense = a_sparse_mat.todense()\n\n    b_indices = np.array([[0, 1], [2, 1]])\n    b_values = np.array([3.0, 6.0]).astype(np.float32)\n    b_dense_shape = [3, 2]\n    b_sparse_mat = sparse.coo_matrix(\n        (b_values, (b_indices[:, 0], b_indices[:, 1])), shape=b_dense_shape)\n    b_dense = b_sparse_mat.todense()\n\n    # Convert to CSRSparseMatrix while removing numeric zeros from the\n    # structural representation.\n    a_sm = dense_to_csr_sparse_matrix(a_dense)\n    b_sm = dense_to_csr_sparse_matrix(b_dense)\n\n    # Compute the matmul.\n    c_sm = sparse_csr_matrix_ops.sparse_matrix_sparse_mat_mul(\n        a=a_sm, b=b_sm, type=dtypes.float32)\n    c_nnz = sparse_csr_matrix_ops.sparse_matrix_nnz(c_sm)\n    c_nnz_value = self.evaluate(c_nnz)\n\n    # Expect that there is a single numeric zero at index (0, 1) if zeros are\n    # not pruned, since 2.0 * 3.0 + (-1.0) * 6.0 = 0.0.\n    self.assertAllClose(1, c_nnz_value)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testLargeBatchSparseMatrixSparseMatMul(self):\n    sparsify = lambda m: m * (m > 0)\n\n    for (transpose_a, transpose_b) in ((False, False), (False, True),\n                                       (True, False), (True, True)):\n      for (adjoint_a, adjoint_b) in ((False, False), (False, True),\n                                     (True, False), (True, True)):\n        if (transpose_a and adjoint_a) or (transpose_b and adjoint_b):\n          continue\n\n        a_dense_shape = ([53, 127, 65]\n                         if transpose_a or adjoint_a else [53, 65, 127])\n        b_dense_shape = ([53, 67, 127]\n                         if transpose_b or adjoint_b else [53, 127, 67])\n\n        a_mats = sparsify(np.random.randn(*a_dense_shape)).astype(np.float32)\n        b_mats = sparsify(np.random.randn(*b_dense_shape).astype(np.float32))\n\n        a_sm = dense_to_csr_sparse_matrix(a_mats)\n        b_sm = dense_to_csr_sparse_matrix(b_mats)\n        c_sm = sparse_csr_matrix_ops.sparse_matrix_sparse_mat_mul(\n            a_sm,\n            b_sm,\n            type=dtypes.float32,\n            transpose_a=transpose_a,\n            adjoint_a=adjoint_a,\n            transpose_b=transpose_b,\n            adjoint_b=adjoint_b)\n        c_sm_dense = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n            c_sm, dtypes.float32)\n        c_dense_t = test_util.matmul_without_tf32(\n            a_mats,\n            b_mats,\n            transpose_a=transpose_a,\n            adjoint_a=adjoint_a,\n            transpose_b=transpose_b,\n            adjoint_b=adjoint_b)\n        c_dense_t_value, c_sm_dense_value = self.evaluate(\n            (c_dense_t, c_sm_dense))\n\n        self.assertAllClose(c_sm_dense_value, c_dense_t_value)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testLargeBatchRegisteredAddN(self):\n    if not self._gpu_available:\n      return\n\n    sparsify = lambda m: m * (m > 0)\n    dense_shape = [53, 65, 127]\n    matrices = [\n        sparsify(np.random.randn(*dense_shape)).astype(np.float32)\n        for _ in range(16)\n    ]\n    sparse_matrices = [dense_to_csr_sparse_matrix(mat) for mat in matrices]\n    sparse_matrices_sum = math_ops.add_n(sparse_matrices)\n    sparse_matrices_sum_dense = \\\n        sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n            sparse_matrices_sum, dtypes.float32)\n    sparse_matrices_sum_dense_value = self.evaluate(sparse_matrices_sum_dense)\n\n    # Ensure that the dense (numpy) sum across all batches matches the result\n    # of add_n converted back to dense.\n    expected_sum = np.sum(matrices, axis=0)\n    self.assertAllClose(expected_sum, sparse_matrices_sum_dense_value)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testCSRZeros(self):\n    if not self._gpu_available:\n      return\n    a_dense_shape = [65, 127]\n    b_dense_shape = [53, 127, 67]\n    data_types = [\n        dtypes.float32, dtypes.float64, dtypes.complex64, dtypes.complex128\n    ]\n    for dtype in data_types:\n      # Check both rank-2 and rank-3 tensors.\n      a_sm = sparse_csr_matrix_ops.sparse_matrix_zeros(\n          a_dense_shape, type=dtype)\n      b_sm = sparse_csr_matrix_ops.sparse_matrix_zeros(\n          b_dense_shape, type=dtype)\n      a_rt = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(a_sm, type=dtype)\n      b_rt = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(b_sm, type=dtype)\n      a_rt_value, b_rt_value = self.evaluate((a_rt, b_rt))\n\n      self.assertAllEqual(a_rt_value, np.zeros(a_dense_shape))\n      self.assertAllEqual(b_rt_value, np.zeros(b_dense_shape))\n\n  @test_util.run_in_graph_and_eager_modes\n  def testLargeBatchZerosLike(self):\n    if not self._gpu_available:\n      return\n\n    batch_size = 53\n    rows = 128\n    cols = 67\n    dense_shape = [batch_size, rows, cols]\n    data_types = [\n        dtypes.float32, dtypes.float64, dtypes.complex64, dtypes.complex128\n    ]\n    for dtype in data_types:\n      sparse_matrices = sparse_csr_matrix_ops.sparse_matrix_zeros(\n          dense_shape, type=dtype)\n      zeros_like_sparse_matrices = array_ops.zeros_like(sparse_matrices)\n      zeros_like_components = [\n          sparse_csr_matrix_ops.csr_sparse_matrix_components(\n              zeros_like_sparse_matrices, i, type=dtype)\n          for i in range(batch_size)\n      ]\n      zeros_like_components_values = self.evaluate(zeros_like_components)\n      for component in zeros_like_components_values:\n        self.assertAllEqual(component.row_ptrs, np.zeros(rows + 1, np.int32))\n        self.assertAllEqual(component.col_inds, np.empty([0], np.int32))\n        self.assertAllEqual(component.values, np.empty([0],\n                                                       dtype.as_numpy_dtype))\n\n  @test_util.run_in_graph_and_eager_modes\n  def testTranspose(self):\n    sparsify = lambda m: m * (m > 0)\n    dense_shape = [127, 65]\n    data_types = [\n        dtypes.float32, dtypes.float64, dtypes.complex64, dtypes.complex128\n    ]\n    for dtype in data_types:\n      mats = sparsify(\n          (np.random.randn(*dense_shape) +\n           1.j * np.random.randn(*dense_shape))).astype(dtype.as_numpy_dtype)\n      for conjugate in False, True:\n        expected = np.transpose(mats)\n        if conjugate:\n          expected = np.conj(expected)\n        matrices = math_ops.cast(mats, dtype)\n        sparse_matrices = dense_to_csr_sparse_matrix(matrices)\n        transpose_sparse_matrices = \\\n            sparse_csr_matrix_ops.sparse_matrix_transpose(\n                sparse_matrices, conjugate=conjugate, type=dtype)\n        dense_transposed = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n            transpose_sparse_matrices, dtype)\n        dense_transposed_values = self.evaluate(dense_transposed)\n        self.assertAllClose(expected, dense_transposed_values)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testLargeBatchTranspose(self):\n    sparsify = lambda m: m * (m > 0)\n    dense_shape = [53, 65, 127]\n    data_types = [\n        dtypes.float32, dtypes.float64, dtypes.complex64, dtypes.complex128\n    ]\n    for dtype in data_types:\n      mats = sparsify(\n          (np.random.randn(*dense_shape) +\n           1.j * np.random.randn(*dense_shape))).astype(dtype.as_numpy_dtype)\n      expected = np.transpose(mats, (0, 2, 1))\n      for conjugate in False, True:\n        if conjugate:\n          expected = np.conj(expected)\n        matrices = math_ops.cast(mats, dtype)\n        sparse_matrices = dense_to_csr_sparse_matrix(matrices)\n        transpose_sparse_matrices = \\\n            sparse_csr_matrix_ops.sparse_matrix_transpose(\n                sparse_matrices, conjugate=conjugate, type=dtype)\n        dense_transposed = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n            transpose_sparse_matrices, dtype)\n        dense_transposed_values = self.evaluate(dense_transposed)\n        self.assertAllClose(expected, dense_transposed_values)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testSoftmax(self):\n    if not self._gpu_available:\n      return\n\n    sparsify = lambda m: m * (m > 0)\n    dense_shape = [127, 65]\n    logits = sparsify(np.random.randn(*dense_shape))\n    logits_with_ninf = np.copy(logits)\n    logits_with_ninf[logits == 0] = -np.inf\n    data_types = [dtypes.float32, dtypes.float64]\n    for dtype in data_types:\n      logits_t = math_ops.cast(logits, dtype)\n      logits_t_with_ninf = math_ops.cast(logits_with_ninf, dtype)\n      expected = nn_ops.softmax(logits_t_with_ninf)\n      sparse_logits_t = dense_to_csr_sparse_matrix(logits_t)\n      softmax_sparse_logits_t = sparse_csr_matrix_ops.sparse_matrix_softmax(\n          sparse_logits_t, type=dtype)\n      dense_softmax = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n          softmax_sparse_logits_t, dtype)\n      dense_softmax_values, expected_values = self.evaluate(\n          (dense_softmax, expected))\n      self.assertAllClose(expected_values, dense_softmax_values)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testLargeBatchSoftmax(self):\n    if not self._gpu_available:\n      return\n\n    sparsify = lambda m: m * (m > 0)\n    dense_shape = [53, 65, 127]\n    logits = sparsify(np.random.randn(*dense_shape))\n    logits_with_ninf = np.copy(logits)\n    logits_with_ninf[logits == 0] = -np.inf\n    data_types = [dtypes.float32, dtypes.float64]\n    for dtype in data_types:\n      logits_t = math_ops.cast(logits, dtype)\n      logits_t_with_ninf = math_ops.cast(logits_with_ninf, dtype)\n      expected = nn_ops.softmax(logits_t_with_ninf)\n      sparse_logits_t = dense_to_csr_sparse_matrix(logits_t)\n      softmax_sparse_logits_t = sparse_csr_matrix_ops.sparse_matrix_softmax(\n          sparse_logits_t, type=dtype)\n      dense_softmax = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n          softmax_sparse_logits_t, dtype)\n      dense_softmax_values, expected_values = self.evaluate(\n          (dense_softmax, expected))\n      self.assertAllClose(expected_values, dense_softmax_values)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testLargeBatchSoftmaxEmpty(self):\n    if not self._gpu_available:\n      return\n\n    dense_shape = [53, 65, 127]\n    sparse_logits_t = sparse_csr_matrix_ops.sparse_matrix_zeros(\n        dense_shape, type=dtypes.float32)\n    softmax_sparse_logits_t = sparse_csr_matrix_ops.sparse_matrix_softmax(\n        sparse_logits_t, type=dtypes.float32)\n    dense_softmax = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n        softmax_sparse_logits_t, dtypes.float32)\n    dense_softmax_values = self.evaluate(dense_softmax)\n    self.assertAllEqual(\n        np.zeros_like(dense_softmax_values), dense_softmax_values)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testSoftmaxGrad(self):\n    if not self._gpu_available:\n      return\n\n    sparsify = lambda m: m * (m > 0)\n    dense_shape = [127, 65]\n    softmax = sparsify(np.random.randn(*dense_shape))\n    grad_softmax = sparsify(np.random.randn(*dense_shape))\n    expected = (\n        (grad_softmax - np.sum(grad_softmax * softmax, -1, keepdims=True)) *\n        softmax)\n    data_types = [dtypes.float32, dtypes.float64]\n    for dtype in data_types:\n      softmax_t = math_ops.cast(softmax, dtype)\n      grad_softmax_t = math_ops.cast(grad_softmax, dtype)\n      softmax_sparse = dense_to_csr_sparse_matrix(softmax_t)\n      grad_softmax_sparse = dense_to_csr_sparse_matrix(grad_softmax_t)\n      gradients_sparse = sparse_csr_matrix_ops.sparse_matrix_softmax_grad(\n          softmax_sparse, grad_softmax_sparse, dtype)\n      dense_gradients = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n          gradients_sparse, dtype)\n      dense_gradients_values = self.evaluate((dense_gradients))\n      self.assertAllClose(expected, dense_gradients_values)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testLargeBatchSoftmaxGrad(self):\n    if not self._gpu_available:\n      return\n\n    sparsify = lambda m: m * (m > 0)\n    dense_shape = [53, 65, 127]\n    softmax = sparsify(np.random.randn(*dense_shape))\n    grad_softmax = sparsify(np.random.randn(*dense_shape))\n    expected = (\n        (grad_softmax - np.sum(grad_softmax * softmax, -1, keepdims=True)) *\n        softmax)\n    data_types = [dtypes.float32, dtypes.float64]\n    for dtype in data_types:\n      softmax_t = math_ops.cast(softmax, dtype)\n      grad_softmax_t = math_ops.cast(grad_softmax, dtype)\n      softmax_sparse = dense_to_csr_sparse_matrix(softmax_t)\n      grad_softmax_sparse = dense_to_csr_sparse_matrix(grad_softmax_t)\n      gradients_sparse = sparse_csr_matrix_ops.sparse_matrix_softmax_grad(\n          softmax_sparse, grad_softmax_sparse, dtype)\n      dense_gradients = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n          gradients_sparse, dtype)\n      dense_gradients_values = self.evaluate((dense_gradients))\n      self.assertAllClose(expected, dense_gradients_values)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testLargeBatchSoftmaxGradEmpty(self):\n    if not self._gpu_available:\n      return\n\n    sparsify = lambda m: m * (m > 0)\n    dense_shape = [53, 65, 127]\n    not_empty = sparsify(np.random.randn(*dense_shape)).astype(np.float32)\n    sparse_empty = sparse_csr_matrix_ops.sparse_matrix_zeros(\n        dense_shape, type=dtypes.float32)\n    sparse_not_empty = dense_to_csr_sparse_matrix(not_empty)\n    gradients_empty_softmax = sparse_csr_matrix_ops.sparse_matrix_softmax_grad(\n        sparse_empty, sparse_not_empty, dtypes.float32)\n    gradients_empty_grad_softmax = (\n        sparse_csr_matrix_ops.sparse_matrix_softmax_grad(\n            sparse_not_empty, sparse_empty, dtypes.float32))\n    gradients_empty_both = sparse_csr_matrix_ops.sparse_matrix_softmax_grad(\n        sparse_empty, sparse_empty, dtypes.float32)\n    ges = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n        gradients_empty_softmax, dtypes.float32)\n    gegs = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n        gradients_empty_grad_softmax, dtypes.float32)\n    geb = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n        gradients_empty_both, dtypes.float32)\n    ges_v, gegs_v, geb_v = self.evaluate((ges, gegs, geb))\n    for v in (ges_v, gegs_v, geb_v):\n      self.assertAllEqual(np.zeros(dense_shape), v)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testLargeBatchConj(self):\n    if not self._gpu_available:\n      return\n\n    sparsify = lambda m: m * (np.real(m) > 0)\n    dense_shape = [53, 65, 127]\n    matrices = (\n        sparsify(np.random.randn(*dense_shape)) +\n        1j * np.random.randn(*dense_shape))\n    data_types = [\n        dtypes.float32, dtypes.float64, dtypes.complex64, dtypes.complex128\n    ]\n    for dtype in data_types:\n      matrices_t = matrices.astype(dtype.as_numpy_dtype)\n      expected = np.conj(matrices_t)\n      sparse_matrices = dense_to_csr_sparse_matrix(matrices_t)\n      conj_sparse_matrices = math_ops.conj(sparse_matrices)\n      dense_conj_matrices = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n          conj_sparse_matrices, dtype)\n      conj_values = self.evaluate(dense_conj_matrices)\n      self.assertAllClose(expected, conj_values)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testLargeBatchSparseMatrixMulScalar(self):\n    if not self._gpu_available:\n      return\n\n    sparsify = lambda m: m * (m > 0)\n    a_dense_shape = [53, 65, 127]\n    a_mats = sparsify(np.random.randn(*a_dense_shape)).astype(np.float32)\n    b = np.float32(3.5)\n    expected = a_mats * b\n    a_sm = dense_to_csr_sparse_matrix(a_mats)\n    c_t = sparse_csr_matrix_ops.sparse_matrix_mul(a_sm, b)\n    c_dense_t = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n        c_t, dtypes.float32)\n    c_dense_t_value = self.evaluate(c_dense_t)\n\n    self.assertAllClose(expected, c_dense_t_value)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testLargeBatchSparseMatrixMulVec(self):\n    if not self._gpu_available:\n      return\n\n    sparsify = lambda m: m * (m > 0)\n    a_dense_shape = [53, 65, 127]\n    a_mats = sparsify(np.random.randn(*a_dense_shape)).astype(np.float32)\n    b = np.random.randn(53, 1, 1).astype(np.float32)\n    expected = a_mats * b\n    a_sm = dense_to_csr_sparse_matrix(a_mats)\n    c_t = sparse_csr_matrix_ops.sparse_matrix_mul(a_sm, b)\n    c_dense_t = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n        c_t, dtypes.float32)\n    c_dense_t_value = self.evaluate(c_dense_t)\n\n    self.assertAllClose(expected, c_dense_t_value)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testSparseCholesky(self):\n    dense_matrix = np.array([\n        [2, 0, 0, 0, 0, 0],\n        [0, 3, 0, 0, 0, 0],\n        [1, 1, 7, 0, 0, 0],\n        [0, 0, 0, 4, 0, 0],\n        [0, 0, 1, 0, 5, 0],\n        [0, 0, 2, 0, 1, 6],\n    ]).astype(np.complex128)\n\n    data_types = [\n        dtypes.float32, dtypes.float64, dtypes.complex64, dtypes.complex128\n    ]\n    for dtype in data_types:\n      with test_util.force_cpu():\n        if dtype.is_complex:\n          dense_matrix += 0.5j * np.tril(dense_matrix, -1)\n\n        sparse_matrix = dense_to_csr_sparse_matrix(\n            math_ops.cast(dense_matrix, dtype))\n        # Obtain the Sparse Cholesky factor using AMD Ordering for reducing\n        # fill-in.\n        ordering_amd = sparse_csr_matrix_ops.sparse_matrix_ordering_amd(\n            sparse_matrix)\n        cholesky_sparse_matrices = (\n            sparse_csr_matrix_ops.sparse_matrix_sparse_cholesky(\n                sparse_matrix, ordering_amd, type=dtype))\n        dense_cholesky = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n            cholesky_sparse_matrices, dtype)\n        # Compute L * Lh where L is the Sparse Cholesky factor.\n        verification = test_util.matmul_without_tf32(\n            dense_cholesky, array_ops.transpose(dense_cholesky, conjugate=True))\n        verification = twist_matrix(verification, ordering_amd)\n        # Assert that input matrix A satisfies A = L * Lh.\n        verification_values = self.evaluate(verification)\n        full_dense_matrix = (\n            dense_matrix +\n            np.conjugate(np.transpose(np.tril(dense_matrix, -1))))\n        self.assertAllClose(full_dense_matrix, verification_values)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testBatchSparseCholesky(self):\n    dense_mat = np.array([\n        # A diagonal matrix.\n        [\n            [1, 0, 0, 0],  #\n            [0, 2, 0, 0],  #\n            [0, 0, 3, 0],  #\n            [0, 0, 0, 4],\n        ],  #\n        # A tridiagonal hermitian matrix.\n        [\n            [5 + 0j, 1 + 0j, 0 + 0j, 0 + 0j],  #\n            [1 + 0j, 4 + 0j, 1 + 2j, 0 + 0j],  #\n            [0 + 0j, 1 - 2j, 9 + 0j, 3 - 3j],  #\n            [0 + 0j, 0 + 0j, 3 + 3j, 7 + 0j],\n        ],  #\n        # A diagonal matrix with a corner element; for which\n        # OrderingAMD returns a non-identity permutation.\n        [\n            [1, 0, 0, 1.],  #\n            [0, 2, 0, 0.],  #\n            [0, 0, 3, 0.],  #\n            [1, 0, 0, 4.],\n        ]  #\n    ]).astype(np.complex128)\n\n    data_types = [\n        dtypes.float32, dtypes.float64, dtypes.complex64, dtypes.complex128\n    ]\n    for dtype in data_types:\n      sparse_matrix = dense_to_csr_sparse_matrix(\n          math_ops.cast(dense_mat, dtype))\n      ordering_amd = sparse_csr_matrix_ops.sparse_matrix_ordering_amd(\n          sparse_matrix)\n\n      cholesky_sparse_matrix = (\n          sparse_csr_matrix_ops.sparse_matrix_sparse_cholesky(\n              sparse_matrix, ordering_amd, type=dtype))\n      dense_cholesky = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n          cholesky_sparse_matrix, dtype)\n\n      # Compute L * Lh.\n      verification = test_util.matmul_without_tf32(\n          dense_cholesky,\n          array_ops.transpose(dense_cholesky, perm=[0, 2, 1], conjugate=True))\n      verification = twist_matrix(verification, ordering_amd)\n\n      verification_values = self.evaluate(verification)\n      self.assertAllClose(\n          dense_mat.astype(dtype.as_numpy_dtype), verification_values)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testLargeBatchSparseCholesky(self):\n    sparsity = 0.1\n    sparsify = lambda m: m * (m > 1 - sparsity)\n\n    batch_size = 53\n    num_rows = 147\n    dense_shape = [batch_size, num_rows, num_rows]\n\n    dense_matrix = sparsify(np.random.uniform(size=dense_shape)).astype(\n        np.float32)\n\n    # Create a \"random\" SPD matrix, by choosing each entry of A between\n    # 0 and 1 at the specified density, and computing 0.5(A + At) + n*I.\n    # This ensures diagonal dominance which implies positive-definiteness.\n    dense_matrix = (\n        0.5 *\n        (dense_matrix + array_ops.transpose(dense_matrix, perm=[0, 2, 1])) +\n        num_rows * linalg_ops.eye(dense_shape[-1], batch_shape=[batch_size]))\n    # Compute the fill-in reducing permutation and use it to perform\n    # the Sparse Cholesky factorization.\n    sparse_matrix = dense_to_csr_sparse_matrix(dense_matrix)\n    ordering_amd = sparse_csr_matrix_ops.sparse_matrix_ordering_amd(\n        sparse_matrix)\n\n    cholesky_sparse_matrix = \\\n        sparse_csr_matrix_ops.sparse_matrix_sparse_cholesky(\n            sparse_matrix, ordering_amd, type=dtypes.float32)\n    dense_cholesky = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n        cholesky_sparse_matrix, dtypes.float32)\n\n    # Compute L * Lh.\n    verification = test_util.matmul_without_tf32(\n        dense_cholesky, array_ops.transpose(dense_cholesky, perm=[0, 2, 1]))\n    verification = twist_matrix(verification, ordering_amd)\n    verification_values = self.evaluate(verification)\n    self.assertAllClose(dense_matrix, verification_values, atol=1e-5, rtol=1e-5)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testSparseCholesky_InvalidMatrix(self):\n    # Verify that non-SPD matrices result in an Invalid Argument error.\n    invalid_matrices = [\n        # zero matrix.\n        np.array([\n            [0., 0., 0., 0.],  #\n            [0., 0., 0., 0.],  #\n            [0., 0., 0., 0.],  #\n            [0., 0., 0., 0.]  #\n        ]),\n        # zero diagonal entry.\n        np.array([\n            [9., 0., 5., 0.],  #\n            [0., 0., 0., 1.],  #\n            [5., 0., 8., 0.],  #\n            [0., 1., 0., 7.]  #\n        ]),\n        # not positive definite.\n        np.array([\n            [2., -2., 0., 0.],  #\n            [-2., 2., 0., 0.],  #\n            [0., 0., 3., -3.],  #\n            [0., 0., -3., 3.]  #\n        ]),\n    ]\n\n    with test_util.force_cpu():\n      for invalid_matrix in invalid_matrices:\n        with self.assertRaises(errors.InvalidArgumentError):\n          sparse_matrix = dense_to_csr_sparse_matrix(\n              invalid_matrix.astype(np.float32))\n          # Compute the fill-in reducing permutation and use it to perform\n          # the Sparse Cholesky factorization.\n          ordering_amd = sparse_csr_matrix_ops.sparse_matrix_ordering_amd(\n              sparse_matrix)\n          cholesky_sparse_matrices = (\n              sparse_csr_matrix_ops.sparse_matrix_sparse_cholesky(\n                  sparse_matrix, ordering_amd, type=dtypes.float32))\n          # Convert the Cholesky factor to a dense matrix to be evaluated.\n          dense_cholesky = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n              cholesky_sparse_matrices, type=dtypes.float32)\n          self.evaluate(dense_cholesky)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testOrderingAMD(self):\n    num_rows = 6\n    # An SPD matrix where AMD ordering can reduce fill-in for Cholesky factor.\n    dense_matrix = np.array([\n        [7, 0, 0, 0, 0, 0],\n        [1, 4, 0, 0, 0, 0],\n        [1, 1, 3, 0, 0, 0],\n        [0, 0, 0, 4, 0, 0],\n        [2, 0, 0, 0, 5, 0],\n        [1, 2, 2, 0, 0, 6],\n    ]).astype(np.float32)\n\n    with test_util.force_cpu():\n      sparse_matrix = dense_to_csr_sparse_matrix(dense_matrix)\n\n      # Obtain the Sparse Cholesky factor with the identity permutation as the\n      # fill-in reducing ordering.\n      cholesky_without_ordering = (\n          sparse_csr_matrix_ops.sparse_matrix_sparse_cholesky(\n              sparse_matrix, math_ops.range(num_rows), type=dtypes.float32))\n      cholesky_without_ordering_nnz = sparse_csr_matrix_ops.sparse_matrix_nnz(\n          cholesky_without_ordering)\n\n      # Obtain the Sparse Cholesky factor using AMD Ordering for reducing\n      # fill-in.\n      ordering_amd = sparse_csr_matrix_ops.sparse_matrix_ordering_amd(\n          sparse_matrix)\n      cholesky_with_amd = sparse_csr_matrix_ops.sparse_matrix_sparse_cholesky(\n          sparse_matrix, ordering_amd, type=dtypes.float32)\n      cholesky_with_amd_nnz = sparse_csr_matrix_ops.sparse_matrix_nnz(\n          cholesky_with_amd)\n\n      (ordering_amd_value, cholesky_with_amd_nnz_value,\n       cholesky_without_ordering_nnz_value) = self.evaluate(\n           [ordering_amd, cholesky_with_amd_nnz, cholesky_without_ordering_nnz])\n\n      # AMD ordering should return a valid permutation.\n      self.assertAllClose(np.arange(num_rows), np.sort(ordering_amd_value))\n      # Check that cholesky with AMD ordering has a strictly lower nonzero count\n      # for this matrix.\n      self.assertLess(cholesky_with_amd_nnz_value,\n                      cholesky_without_ordering_nnz_value)\n\n\nclass CSRSparseMatrixOpsBenchmark(test.Benchmark):\n\n  def benchmark_sparse_matrix_mat_mul_gpu(self):\n    if not test_util.is_gpu_available():\n      return\n\n    sparsify = lambda m: array_ops.where(m > 2, m, array_ops.zeros_like(m))\n\n    # XW, X dense and W sparse\n    # X is shaped [{1, 8, 16}, 2000]\n    # W is shaped [2000, 4000]\n\n    for batch_size in [1, 8, 16]:\n      x_dense_shape = [batch_size, 2000]\n      w_dense_shape = [2000, 4000]\n\n      with ops.Graph().as_default(), ops.device(\"/gpu:0\"):\n        x_mats = random_ops.random_normal(x_dense_shape, dtype=dtypes.float32)\n        w_mats = sparsify(\n            random_ops.random_normal(w_dense_shape, dtype=dtypes.float32))\n        nnz = array_ops.shape(array_ops.where(w_mats))[0]\n        ratio = math_ops.cast(nnz, dtypes.float32) / np.prod(w_dense_shape)\n        w_sm = dense_to_csr_sparse_matrix(w_mats)\n        with ops.name_scope(\"w_sm_var\"):\n          w_sm_var = variable_scope.get_variable(\n              \"sm\", initializer=w_sm, use_resource=True)\n          w_sm_var_v = w_sm_var.read_value()\n        with ops.name_scope(\"w_var\"):\n          w_var = variable_scope.get_variable(\n              \"sm_dense\", initializer=w_mats, use_resource=True)\n          w_var_v = w_var.read_value()\n        with ops.name_scope(\"b\"):\n          x = variable_scope.get_variable(\n              \"b\", initializer=x_mats, use_resource=True)\n          x_v = x.read_value()\n        # X*W = (W'*X')'\n        xw_sparse = sparse_csr_matrix_ops.sparse_matrix_mat_mul(\n            w_sm_var_v,\n            x_v,\n            transpose_a=True,\n            transpose_b=True,\n            transpose_output=True)\n        xw_dense = math_ops.matmul(x_v, w_var_v)\n\n        with session.Session() as sess:\n          self.evaluate(\n              [w_var.initializer, w_sm_var.initializer, x.initializer])\n          nnz_value, ratio_value = self.evaluate((nnz, ratio))\n          name_template = (\n              \"sparse_matrix_mat_mul_gpu_%s_W_2000x4000_batch_size_%d\")\n          self.run_op_benchmark(\n              sess,\n              xw_sparse.op,\n              name=name_template % (\"sparse\", batch_size),\n              extras={\n                  \"percentage_nonzero\": ratio_value,\n                  \"num_nonzero\": nnz_value\n              },\n              min_iters=50)\n          self.run_op_benchmark(\n              sess,\n              xw_dense.op,\n              name=name_template % (\"dense\", batch_size),\n              extras={\n                  \"percentage_nonzero\": ratio_value,\n                  \"num_nonzero\": nnz_value\n              },\n              min_iters=50)\n\n  def benchmark_sparse_matrix_mat_vec_mul(self):\n    # num_rows, device, transpose.\n    cases = [\n        [2000, CPU, False],\n        [8000, CPU, False],\n        [12000, CPU, False],\n        [2000, CPU, True],\n        [8000, CPU, True],\n        [12000, CPU, True],\n    ]\n    seed = 42\n\n    for num_rows, device, transpose in cases:\n      if device == GPU and not test_util.is_gpu_available():\n        continue\n      for num_threads in [1, 2, 4, 6, 8, 10]:\n        device_str = \"cpu\" if device == CPU else \"gpu\"\n        w_dense_shape = [num_rows, num_rows]\n        x_dense_shape = [num_rows, 1]\n\n        with ops.Graph().as_default(), ops.device(device):\n          random_seed.set_random_seed(seed)\n          x = random_ops.random_normal(x_dense_shape, dtype=dtypes.float32)\n          w_np = sparse.rand(\n              w_dense_shape[0],\n              w_dense_shape[1],\n              density=0.01,\n              dtype=np.float32,\n              random_state=np.random.RandomState(seed))\n          w_st = sparse_tensor.SparseTensor(\n              zip(w_np.row, w_np.col), w_np.data, w_np.shape)\n          w_st = sparse_ops.sparse_reorder(w_st)\n\n          nnz = array_ops.shape(w_st.values)[0]\n          ratio = math_ops.cast(nnz, dtypes.float32) / np.prod(w_np.shape)\n\n          w_sm = sparse_csr_matrix_ops.sparse_tensor_to_csr_sparse_matrix(\n              w_st.indices, w_st.values, w_st.dense_shape)\n          xw_sparse_matrix = sparse_csr_matrix_ops.sparse_matrix_mat_mul(\n              w_sm,\n              x,\n              transpose_a=transpose,\n              transpose_b=False,\n              transpose_output=False)\n          xw_sparse_tensor = sparse_ops.sparse_tensor_dense_matmul(\n              w_st, x, adjoint_a=transpose, adjoint_b=False)\n\n          with session.Session(\n              config=config_pb2.ConfigProto(\n                  intra_op_parallelism_threads=num_threads)) as sess:\n            nnz_value, ratio_value = sess.run((nnz, ratio))\n            name_template = (\"mat_vec_mul_%s_%s_W_%d_transpose_%s_threads_%d\")\n            self.run_op_benchmark(\n                sess,\n                xw_sparse_matrix.op,\n                name=name_template %\n                (device_str, \"sparse_matrix\", num_rows, transpose, num_threads),\n                extras={\n                    \"percentage_nonzero\": ratio_value,\n                    \"num_nonzero\": nnz_value,\n                },\n                min_iters=10)\n            self.run_op_benchmark(\n                sess,\n                xw_sparse_tensor.op,\n                name=name_template %\n                (device_str, \"sparse_tensor\", num_rows, transpose, num_threads),\n                extras={\n                    \"percentage_nonzero\": ratio_value,\n                    \"num_nonzero\": nnz_value,\n                },\n                min_iters=10)\n\n  def benchmark_sparse_matrix_sparse_matmul(self):\n    density = 0.05\n    # pylint: disable=g-long-lambda\n    sparsify = lambda m: array_ops.where(m > 1. - density, m,\n                                         array_ops.zeros_like(m))\n    # pylint: enable=g-long-lambda\n\n    for batch_size in [1, 16]:\n      for num_threads in [1, 4, 12]:\n        dense_shape = [batch_size, 250, 250]\n\n        for device in [CPU, GPU]:\n          if device == GPU and not test_util.is_gpu_available():\n            continue\n\n          with ops.Graph().as_default(), ops.device(device):\n            x_mats = sparsify(\n                random_ops.random_uniform(dense_shape, dtype=dtypes.float32))\n            y_mats = sparsify(\n                random_ops.random_uniform(dense_shape, dtype=dtypes.float32))\n\n            nnz = array_ops.shape(array_ops.where(x_mats))[0] + array_ops.shape(\n                array_ops.where(y_mats))[0]\n            ratio = math_ops.cast(nnz,\n                                  dtypes.float32) / (2 * np.prod(dense_shape))\n\n            x_sm = dense_to_csr_sparse_matrix(x_mats)\n            y_sm = dense_to_csr_sparse_matrix(y_mats)\n\n            xy_sparse = sparse_csr_matrix_ops.sparse_matrix_sparse_mat_mul(\n                x_sm, y_sm, type=dtypes.float32)\n\n            with session.Session(\n                config=config_pb2.ConfigProto(\n                    intra_op_parallelism_threads=num_threads)) as sess:\n              nnz_value, ratio_value = self.evaluate((nnz, ratio))\n              name_template = (\n                  \"sparse_matrix_sparse_matmul_%s_N_%d_batch_size_%d_threads_%d\"\n              )\n              device_str = \"cpu\" if device == CPU else \"gpu\"\n              self.run_op_benchmark(\n                  sess,\n                  xy_sparse.op,\n                  name=name_template %\n                  (device_str, dense_shape[-1], batch_size, num_threads),\n                  extras={\n                      \"percentage_nonzero\": ratio_value,\n                      \"num_nonzero\": nnz_value\n                  },\n                  min_iters=50)\n\n  def benchmark_sparse_dense_conversion(self):\n    sparsity = 0.05\n\n    for batch_size in [1, 16]:\n      for num_threads in [1, 4, 12]:\n        dense_shape = [batch_size, 750, 750]\n\n        for device in [CPU, GPU]:\n          if device == GPU and not test_util.is_gpu_available():\n            continue\n\n          with ops.Graph().as_default(), ops.device(device):\n            mats = random_ops.random_uniform(dense_shape, dtype=dtypes.float32)\n            mats_locs = array_ops.where(mats > 1.0 - sparsity)\n\n            sparse_matrices = sparse_csr_matrix_ops.dense_to_csr_sparse_matrix(\n                mats, mats_locs)\n            dense_matrices = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n                sparse_matrices, type=dtypes.float32)\n            nnz = math_ops.reduce_sum(\n                sparse_csr_matrix_ops.sparse_matrix_nnz(sparse_matrices))\n            ratio = math_ops.cast(nnz, dtypes.float32) / np.prod(dense_shape)\n\n            with session.Session(\n                config=config_pb2.ConfigProto(\n                    intra_op_parallelism_threads=num_threads)) as sess:\n              nnz_value, ratio_value = self.evaluate((nnz, ratio))\n              device_str = \"cpu\" if device == CPU else \"gpu\"\n              name_template = (\n                  \"dense_to_sparse_matrix_%s_N_%d_batch_size_%d_num_threads_%d\")\n              self.run_op_benchmark(\n                  sess,\n                  sparse_matrices.op,\n                  name=name_template %\n                  (device_str, dense_shape[-1], batch_size, num_threads),\n                  extras={\n                      \"percentage_nonzero\": ratio_value,\n                      \"num_nonzero\": nnz_value,\n                  },\n                  min_iters=50)\n              name_template = (\n                  \"sparse_matrix_to_dense_%s_N_%d_batch_size_%d_num_threads_%d\")\n              self.run_op_benchmark(\n                  sess,\n                  dense_matrices.op,\n                  name=name_template %\n                  (device_str, dense_shape[-1], batch_size, num_threads),\n                  extras={\n                      \"percentage_nonzero\": ratio_value,\n                      \"num_nonzero\": nnz_value,\n                  },\n                  min_iters=50)\n\n  def benchmark_sparse_cholesky(self):\n    # TODO(anudhyan): Use conversions from SparseTensor instead of to get this\n    # benchmark working for larger matrices. For this to work without GPU, we\n    # need to write CPU kernels for SparseTensor conversions.\n    num_rows = 500\n    density = 0.01\n    # pylint: disable=g-long-lambda\n    sparsify = lambda m: array_ops.where(m > 1. - density, m,\n                                         array_ops.zeros_like(m))\n    # pylint: enable=g-long-lambda\n\n    for batch_size in [1, 16]:\n      for num_threads in [1, 4, 12]:\n        dense_shape = [batch_size, num_rows, num_rows]\n\n        with ops.Graph().as_default(), ops.device(CPU):\n          # Create a \"random\" SPD matrix, by choosing each entry of A between\n          # 0 and 1 at the specified density, and computing 0.5(A + At) + n*I.\n          # This ensures diagonal dominance which implies positive-definiteness.\n          dense_matrix = sparsify(\n              random_ops.random_uniform(dense_shape, dtype=dtypes.float32))\n          spd_dense_matrix = (\n              0.5 *\n              (dense_matrix + array_ops.transpose(dense_matrix, perm=[0, 2, 1]))\n              + num_rows *\n              linalg_ops.eye(dense_shape[-1], batch_shape=[batch_size]))\n\n          # Convert to SparseMatrix and invoke Sparse Cholesky factorization\n          # with AMD Ordering.\n          sparse_matrix = dense_to_csr_sparse_matrix(spd_dense_matrix)\n          ordering_amd = sparse_csr_matrix_ops.sparse_matrix_ordering_amd(\n              sparse_matrix)\n          cholesky_sparse_matrix = (\n              sparse_csr_matrix_ops.sparse_matrix_sparse_cholesky(\n                  sparse_matrix, ordering_amd, type=dtypes.float32))\n\n          nnz = math_ops.reduce_sum(\n              sparse_csr_matrix_ops.sparse_matrix_nnz(sparse_matrix))\n          ratio = math_ops.cast(nnz, dtypes.float32) / np.prod(dense_shape)\n          ordering_amd_name_template = (\n              \"sparse_matrix_ordering_amd_cpu_N_%d_batch_size_%d_threads_%d\")\n          sparse_cholesky_name_template = (\n              \"sparse_matrix_sparse_cholesky_cpu_N_%d_batch_size_%d_threads_%d\")\n          with session.Session(\n              config=config_pb2.ConfigProto(\n                  intra_op_parallelism_threads=num_threads)) as sess:\n            nnz_value, ratio_value = self.evaluate((nnz, ratio))\n            self.run_op_benchmark(\n                sess,\n                ordering_amd.op,\n                name=ordering_amd_name_template %\n                (dense_shape[-1], batch_size, num_threads),\n                extras={\n                    \"percentage_nonzero\": ratio_value,\n                    \"num_nonzero\": nnz_value\n                },\n                min_iters=25)\n            self.run_op_benchmark(\n                sess,\n                cholesky_sparse_matrix.op,\n                name=sparse_cholesky_name_template %\n                (dense_shape[-1], batch_size, num_threads),\n                extras={\n                    \"percentage_nonzero\": ratio_value,\n                    \"num_nonzero\": nnz_value\n                },\n                min_iters=25)\n\n\nif __name__ == \"__main__\":\n  test.main()\n"], "fixing_code": ["/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#ifndef TENSORFLOW_CORE_KERNELS_SPARSE_SPARSE_MATRIX_H_\n#define TENSORFLOW_CORE_KERNELS_SPARSE_SPARSE_MATRIX_H_\n\n#define EIGEN_USE_THREADS\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n#define EIGEN_USE_GPU\n#endif\n\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/tensor_shape.h\"\n#include \"tensorflow/core/framework/tensor_types.h\"\n#include \"tensorflow/core/framework/variant.h\"\n#include \"tensorflow/core/framework/variant_encode_decode.h\"\n#include \"tensorflow/core/framework/variant_op_registry.h\"\n#include \"tensorflow/core/platform/errors.h\"\n\nnamespace tensorflow {\n\nclass CSRSparseMatrix {\n  // CreateCSRSparseMatrix is the main method used to construct a\n  // CSRSparseMatrix.  The representations for both 2D and 3D\n  // (batched) CSR Sparse Matrices are the same:\n  //\n  // dtype: The datatype of the values.\n  // dense_shape: The dense shape of the matrix.\n  //   * Host int64 vector, size 2 or 3.\n  //   * Takes on values: (rows, cols) or (batch_size, rows, cols).\n  // batch_pointers: Batch offset pointers into col_indices and values.\n  //   * Host int32 vector, size (batch_size + 1).\n  //   * Takes on values: (0, nnz[0], nnz[0] + nnz[1], ..., total_nnz).\n  // row_pointers: Row offset pointers into col_indices and values.\n  //   * Device int32 vector, size ((rows + 1) * batch_size).\n  //   * Each block of size (rows + 1) takes on values:\n  //     (0, num_rows{b}[0], num_rows{b}[0] + num_rows{b}[1], ..., nnz[b]).\n  //     for b = 0 .. batch_size - 1.\n  // col_indices: Column values for the given row and column index.\n  //   * Device int32 vector, size total_nnz.\n  // values: Actual values for the given row and column index.\n  //   * Device dtype vector, size total_nnz.\n  //\n  // The storage agreement is such that for a given (batch, row, ix):\n  //   offset = batch_pointers(batch) + row_pointers(batch * (rows + 1) + row)\n  //   col = col_indices(offset + ix)\n  //   val = values(offset + ix)\n  // where ix < #nnz columns in (batch, row).\n  // Then:\n  //   matrix(batch, row, col) = val.\n  //\n  // All other elements in the dense representation are treated as 0 / empty.\n  //\n  // For example, for a 2D sparse matrix m shaped (3, 4) such that:\n  //\n  //   m[0, 0] = 1.0\n  //   m[0, 1] = 2.0\n  //   m[0, 2] = 3.0\n  //   m[2, 2] = 4.0\n  //   m[2, 3] = 5.0\n  //\n  // The corresponding representation is:\n  //\n  //   dtype: DT_FLOAT\n  //   dense_shape: (3, 4)\n  //   batch_pointers: (0, 5)\n  //   row_pointers: (0, 3, 3, 5)\n  //   col_indices: concat((0, 1, 2), (), (2, 3))\n  //   values: concat((1.0, 2.0, 3.0), (), (4.0, 5.0))\n  //\n  // For a 3D sparse matrix m shaped (2, 3, 4) such that:\n  //\n  //   m[0, 0, 0] = 1.0\n  //   m[0, 0, 2] = 2.0\n  //   m[0, 2, 3] = 3.0\n  //   m[1, 0, 3] = 4.0\n  //   m[1, 1, 0] = 5.0\n  //\n  // The corresponding representation is:\n  //   dtype: DT_FLOAT\n  //   dense_shape: (2, 3, 4)\n  //   batch_pointers: (0, 3, 5)\n  //   row_pointers: concat((0, 2, 2, 3), (0, 1, 2, 2))\n  //   col_indices: concat(concat((0, 2), (), (3,)),\n  //                       concat((3,),   (), (0,)))\n  //   values: concat(concat((1.0, 2.0), (3.0,), ()),\n  ///                 concat((4.0,),     (5.0,), ()))\n  //\n public:\n  static constexpr const char kTypeName[] = \"tensorflow::CSRSparseMatrix\";\n\n  CSRSparseMatrix() : metadata_{false, DT_INVALID} {}\n\n  CSRSparseMatrix(const CSRSparseMatrix& rhs)\n      : metadata_(rhs.metadata_),\n        dense_shape_(rhs.dense_shape_),\n        batch_pointers_(rhs.batch_pointers_),\n        row_pointers_(rhs.row_pointers_),\n        col_indices_(rhs.col_indices_),\n        values_(rhs.values_) {\n    SetupVecs();\n  }\n\n  CSRSparseMatrix(CSRSparseMatrix&& rhs)\n      : metadata_(rhs.metadata_),\n        dense_shape_(std::move(rhs.dense_shape_)),\n        batch_pointers_(std::move(rhs.batch_pointers_)),\n        row_pointers_(std::move(rhs.row_pointers_)),\n        col_indices_(std::move(rhs.col_indices_)),\n        values_(std::move(rhs.values_)) {\n    SetupVecs();\n    rhs.metadata_.validated = false;\n    rhs.metadata_.dtype = DT_INVALID;\n    rhs.ClearVecs();\n  }\n\n  CSRSparseMatrix& operator=(CSRSparseMatrix&& rhs) {\n    if (this == &rhs) return *this;\n    metadata_ = rhs.metadata_;\n    metadata_.validated = rhs.metadata_.validated;\n    dense_shape_ = std::move(rhs.dense_shape_);\n    batch_pointers_ = std::move(rhs.batch_pointers_);\n    row_pointers_ = std::move(rhs.row_pointers_);\n    col_indices_ = std::move(rhs.col_indices_);\n    values_ = std::move(rhs.values_);\n    SetupVecs();\n    rhs.metadata_ = {false, DT_INVALID};\n    rhs.ClearVecs();\n    return *this;\n  }\n\n  static Status CreateCSRSparseMatrix(DataType dtype,\n                                      const Tensor& dense_shape,     // on host\n                                      const Tensor& batch_pointers,  // on host\n                                      const Tensor& row_pointers,\n                                      const Tensor& col_indices,\n                                      const Tensor& values,\n                                      CSRSparseMatrix* matrix) {\n    *matrix = CSRSparseMatrix(dtype, dense_shape, batch_pointers, row_pointers,\n                              col_indices, values);\n    Status s = matrix->Validate();\n    matrix->metadata_.validated = s.ok();\n    matrix->SetupVecs();\n    return s;\n  }\n\n  Status Validate() const {\n    return ValidateTypesAndShapes(metadata_.dtype, dense_shape_,\n                                  batch_pointers_, row_pointers_, col_indices_,\n                                  values_);\n  }\n\n  void Clear() {\n    metadata_ = {false, DT_INVALID};\n    dense_shape_ = Tensor();\n    batch_pointers_ = Tensor();\n    row_pointers_ = Tensor();\n    col_indices_ = Tensor();\n    values_ = Tensor();\n    ClearVecs();\n  }\n\n  bool valid() const {\n    return metadata_.validated && dense_shape_.IsInitialized() &&\n           batch_pointers_.IsInitialized() && row_pointers_.IsInitialized() &&\n           col_indices_.IsInitialized() && values_.IsInitialized() &&\n           dense_shape_.NumElements() > 1 &&\n           batch_pointers_.NumElements() > 0 && row_pointers_.NumElements() > 0;\n  }\n\n  DataType dtype() const {\n    DCHECK(valid());\n    return metadata_.dtype;\n  }\n\n  inline int dims() const {\n    DCHECK(valid());\n    return dense_shape_.NumElements();\n  }\n\n  inline int nnz(int batch) const {\n    DCHECK_LT(batch, batch_size());\n    return (*batch_pointers_vec_)(batch + 1) - (*batch_pointers_vec_)(batch);\n  }\n\n  inline int batch_offset(int batch) const {\n    DCHECK_LT(batch, batch_size());\n    return (*batch_pointers_vec_)(batch);\n  }\n\n  inline int total_nnz() const {\n    DCHECK(valid());\n    return (*batch_pointers_vec_)(batch_size());\n  }\n\n  inline Tensor& dense_shape() {\n    DCHECK(valid());\n    return dense_shape_;\n  }\n\n  inline const Tensor& dense_shape() const {\n    DCHECK(valid());\n    return dense_shape_;\n  }\n\n  inline TTypes<int32>::UnalignedVec row_pointers_vec(int batch) {\n    DCHECK(valid());\n    DCHECK_LT(batch, batch_size());\n    const int64_t rows = dense_shape().vec<int64_t>()((dims() == 2) ? 0 : 1);\n    const int offset = batch * (rows + 1);\n    return TTypes<int32>::UnalignedVec(row_pointers_vec_->data() + offset,\n                                       rows + 1);\n  }\n\n  inline TTypes<int32>::UnalignedConstVec row_pointers_vec(int batch) const {\n    DCHECK(valid());\n    DCHECK_LT(batch, batch_size());\n    const int64_t rows = dense_shape().vec<int64_t>()((dims() == 2) ? 0 : 1);\n    const int offset = batch * (rows + 1);\n    return TTypes<int32>::UnalignedConstVec(row_pointers_vec_->data() + offset,\n                                            rows + 1);\n  }\n\n  inline TTypes<int32>::UnalignedVec col_indices_vec(int batch) {\n    DCHECK(valid());\n    DCHECK_LT(batch, batch_size());\n    const int offset = (*batch_pointers_vec_)(batch);\n    const int nnz_in_batch = nnz(batch);\n    return TTypes<int32>::UnalignedVec(col_indices_vec_->data() + offset,\n                                       nnz_in_batch);\n  }\n\n  inline TTypes<int32>::UnalignedConstVec col_indices_vec(int batch) const {\n    DCHECK(valid());\n    DCHECK_LT(batch, batch_size());\n    const int offset = (*batch_pointers_vec_)(batch);\n    const int nnz_in_batch = nnz(batch);\n    return TTypes<int32>::UnalignedConstVec(col_indices_vec_->data() + offset,\n                                            nnz_in_batch);\n  }\n\n  template <typename T>\n  inline typename TTypes<T>::UnalignedVec values_vec(int batch) {\n    DCHECK(valid());\n    DCHECK_LT(batch, batch_size());\n    const int offset = (*batch_pointers_vec_)(batch);\n    const int nnz_in_batch = nnz(batch);\n    return typename TTypes<T>::UnalignedVec(values().vec<T>().data() + offset,\n                                            nnz_in_batch);\n  }\n\n  template <typename T>\n  inline typename TTypes<T>::UnalignedConstVec values_vec(int batch) const {\n    DCHECK(valid());\n    DCHECK_LT(batch, batch_size());\n    const int offset = (*batch_pointers_vec_)(batch);\n    const int nnz_in_batch = nnz(batch);\n    return typename TTypes<T>::UnalignedConstVec(\n        values().vec<T>().data() + offset, nnz_in_batch);\n  }\n\n  inline Tensor& row_pointers() {\n    DCHECK(valid());\n    return row_pointers_;\n  }\n\n  inline const Tensor& row_pointers() const {\n    DCHECK(valid());\n    return row_pointers_;\n  }\n\n  inline Tensor& col_indices() {\n    DCHECK(valid());\n    return col_indices_;\n  }\n\n  inline const Tensor& col_indices() const {\n    DCHECK(valid());\n    return col_indices_;\n  }\n\n  inline Tensor& values() {\n    DCHECK(valid());\n    return values_;\n  }\n\n  inline const Tensor& values() const {\n    DCHECK(valid());\n    return values_;\n  }\n\n  inline Tensor& batch_pointers() {\n    DCHECK(valid());\n    return batch_pointers_;\n  }\n\n  inline const Tensor& batch_pointers() const {\n    DCHECK(valid());\n    return batch_pointers_;\n  }\n\n  std::string TypeName() const { return kTypeName; }\n\n  // TODO(ebrevdo): A better debug string.\n  std::string DebugString() const { return dense_shape_.DebugString(); }\n\n  // Returns the number of elements.  This is equal to 1 if the\n  // CSRSparseMatrix is a singleton matrix (dense_shape is length 2).\n  int batch_size() const {\n    DCHECK(valid());\n    return batch_pointers_.NumElements() - 1;\n  }\n\n  bool Decode(const VariantTensorData& p) {\n    if (p.tensors_.empty()) return false;\n    Metadata metadata;\n    if (!p.get_metadata(&metadata)) return false;\n    const bool validated = metadata.validated;\n    const DataType dtype = metadata.dtype;\n\n    // p.tensors_ should contain tensors {dense_shape, batch_pointers,\n    // row_pointers, col_indices, values}.\n    if (p.tensors_.size() != 5) return false;\n\n    Tensor dense_shape = p.tensors_[0];\n    if (dense_shape.dtype() != DT_INT64) return false;\n    if (dense_shape.dims() != 1) return false;\n    int rank = dense_shape.dim_size(0);\n    if (rank < 2 || rank > 3) return false;\n\n    Tensor batch_pointers(p.tensors_[1]);\n    Tensor row_pointers(p.tensors_[2]);\n    Tensor col_indices(p.tensors_[3]);\n    Tensor values(p.tensors_[4]);\n\n    // Check that the validated bool is consistent with the data.\n    Status s = ValidateTypesAndShapes(dtype, dense_shape, batch_pointers,\n                                      row_pointers, col_indices, values);\n    if (s.ok() != validated) return false;\n\n    // Save to this object.\n    metadata_ = metadata;\n    dense_shape_ = std::move(dense_shape);\n    batch_pointers_ = std::move(batch_pointers);\n    row_pointers_ = std::move(row_pointers);\n    col_indices_ = std::move(col_indices);\n    values_ = std::move(values);\n    SetupVecs();\n    return true;\n  }\n\n  void Encode(VariantTensorData* p) const {\n    DCHECK(valid());\n\n    // Store metadata_ to p's metadata\n    p->set_metadata(metadata_);\n\n    // Store dense_shape, row_pointers, col_indices, and values to p->tensors_.\n    p->tensors_.reserve(5);\n    p->tensors_.push_back(dense_shape_);\n    p->tensors_.push_back(batch_pointers_);\n    p->tensors_.push_back(row_pointers_);\n    p->tensors_.push_back(col_indices_);\n    p->tensors_.push_back(values_);\n  }\n\n  // This static method copies CSRSparseMatrices in all directions:\n  //   Host->Device, Device->Host, and Device->Device.\n  static Status DeviceCopy(\n      const CSRSparseMatrix& from, CSRSparseMatrix* to,\n      const UnaryVariantOpRegistry::AsyncTensorDeviceCopyFn& copy) {\n    VLOG(2) << \"DeviceCopy from type: \" << DataTypeString(from.dtype())\n            << \" and shape: \" << from.dense_shape().DebugString();\n    Tensor to_row_ptr(DT_INT32);\n    Tensor to_col_ind(DT_INT32);\n    Tensor to_values(from.dtype());\n    TF_RETURN_IF_ERROR(copy(from.row_pointers(), &to_row_ptr));\n    TF_RETURN_IF_ERROR(copy(from.col_indices(), &to_col_ind));\n    TF_RETURN_IF_ERROR(copy(from.values(), &to_values));\n    return CreateCSRSparseMatrix(from.dtype(),\n                                 from.dense_shape(),     // Always on host.\n                                 from.batch_pointers(),  // Always on host.\n                                 to_row_ptr, to_col_ind, to_values, to);\n  }\n\n private:\n  CSRSparseMatrix(DataType dtype, const Tensor& dense_shape,\n                  const Tensor& batch_pointers, const Tensor& row_pointers,\n                  const Tensor& col_indices, const Tensor& values)\n      : metadata_{false, dtype},\n        dense_shape_(dense_shape),\n        batch_pointers_(batch_pointers),\n        row_pointers_(row_pointers),\n        col_indices_(col_indices),\n        values_(values) {}\n\n  void SetupVecs() {\n    if (!metadata_.validated) return;\n    batch_pointers_vec_.reset(\n        new TTypes<int32>::Vec(batch_pointers_.vec<int32>()));\n    row_pointers_vec_.reset(new TTypes<int32>::Vec(row_pointers_.vec<int32>()));\n    col_indices_vec_.reset(new TTypes<int32>::Vec(col_indices_.vec<int32>()));\n  }\n\n  void ClearVecs() {\n    batch_pointers_vec_.reset();\n    row_pointers_vec_.reset();\n    col_indices_vec_.reset();\n  }\n\n  static Status ValidateTypesAndShapes(DataType dtype,\n                                       const Tensor& dense_shape,\n                                       const Tensor& batch_pointers,\n                                       const Tensor& row_pointers,\n                                       const Tensor& col_indices,\n                                       const Tensor& values) {\n    // TODO(ebrevdo): Consider adding support for other floating point types\n    // (namely, float16).\n    if (dtype != DT_FLOAT && dtype != DT_DOUBLE && dtype != DT_COMPLEX64 &&\n        dtype != DT_COMPLEX128) {\n      return errors::InvalidArgument(\n          \"CSRSparseMatrix::Validate: dtype = \", DataTypeString(dtype),\n          \" not in {float32, float64, complex64, complex128}\");\n    }\n    // dense_shape checks\n    if (dense_shape.dtype() != DT_INT64) {\n      return errors::InvalidArgument(\n          \"CSRSparseMatrix::Validate: dense_shape.dtype() = \",\n          DataTypeString(dense_shape.dtype()), \" != int64\");\n    }\n    if (dense_shape.dims() != 1) {\n      return errors::InvalidArgument(\n          \"CSRSparseMatrix::Validate: dense_shape should be a vector, but saw \"\n          \"tensor: \",\n          dense_shape.DebugString());\n    }\n    int rank = dense_shape.dim_size(0);\n    if (rank < 2 || rank > 3) {\n      return errors::InvalidArgument(\n          \"CSRSparseMatrix::Validate: dense_shape should be a 2- or 3- vector, \"\n          \"but saw: \",\n          dense_shape.SummarizeValue(5));\n    }\n    auto dense_shape_t = dense_shape.vec<int64_t>();\n    const int64_t batch_size = (rank == 2) ? 1 : dense_shape_t(0);\n    const int64_t num_rows = (rank == 2) ? dense_shape_t(0) : dense_shape_t(1);\n\n    if (batch_pointers.dtype() != DT_INT32) {\n      return errors::InvalidArgument(\n          \"CSRSparseMatrix::Validate: batch_pointers.dtype() = \",\n          DataTypeString(batch_pointers.dtype()), \" != int32\");\n    }\n    if (batch_pointers.dims() != 1) {\n      return errors::InvalidArgument(\n          \"CSRSparseMatrix::Validate: batch_indices is not a vector, saw \"\n          \"shape: \",\n          batch_pointers.shape().DebugString());\n    }\n\n    // batch size checks\n    if (batch_size != batch_pointers.NumElements() - 1) {\n      return errors::InvalidArgument(\n          \"CSRSparseMatrix::Validate: dense_shape is \",\n          dense_shape.SummarizeValue(5),\n          \" but batch pointers implies batch size is \",\n          batch_pointers.NumElements() - 1);\n    }\n\n    if (row_pointers.dtype() != DT_INT32) {\n      return errors::InvalidArgument(\n          \"CSRSparseMatrix::Validate: row_pointers.dtype() = \",\n          DataTypeString(row_pointers.dtype()), \" != int32\");\n    }\n    if (row_pointers.dims() != 1) {\n      return errors::InvalidArgument(\n          \"CSRSparseMatrix::Validate: row_pointers is not a vector, saw \"\n          \"shape: \",\n          row_pointers.shape().DebugString());\n    }\n    if (row_pointers.dim_size(0) != batch_size * (num_rows + 1)) {\n      return errors::InvalidArgument(\n          \"CSRSparseMatrix::Validate: row_pointers should have size batch_size \"\n          \"* (num_rows + 1), saw shapes: \",\n          dense_shape.DebugString(), \" vs. \",\n          row_pointers.shape().DebugString());\n    }\n    if (col_indices.dtype() != DT_INT32) {\n      return errors::InvalidArgument(\n          \"CSRSparseMatrix::Validate: col_indices.dtype() = \",\n          DataTypeString(col_indices.dtype()), \" != int32\");\n    }\n    if (col_indices.dims() != 1) {\n      return errors::InvalidArgument(\n          \"CSRSparseMatrix::Validate: col_indices is not a vector, saw shape: \",\n          col_indices.shape().DebugString());\n    }\n    if (values.dtype() != dtype) {\n      return errors::InvalidArgument(\n          \"CSRSparseMatrix::Validate: values.dtype() = \",\n          DataTypeString(values.dtype()),\n          \" != dtype = \", DataTypeString(dtype));\n    }\n    if (values.dims() != 1) {\n      return errors::InvalidArgument(\n          \"CSRSparseMatrix::Validate: values is not a vector, saw shape: \",\n          values.shape().DebugString());\n    }\n    if (col_indices.dim_size(0) != values.dim_size(0)) {\n      return errors::InvalidArgument(\n          \"CSRSparseMatrix::Validate: size(col_indices) = \",\n          col_indices.dim_size(0), \" != size(values) = \", values.dim_size(0));\n    }\n    return OkStatus();\n  }\n\n  struct Metadata {\n    bool validated;\n    DataType dtype;\n  };\n  Metadata metadata_;\n  Tensor dense_shape_;\n  Tensor batch_pointers_;\n  Tensor row_pointers_;\n  Tensor col_indices_;\n  Tensor values_;\n  std::unique_ptr<TTypes<int32>::Vec> batch_pointers_vec_;\n  std::unique_ptr<TTypes<int32>::Vec> row_pointers_vec_;\n  std::unique_ptr<TTypes<int32>::Vec> col_indices_vec_;\n};\n\n// Call BinaryFunctor<Device, T>()(ctx, a, b, c)\n// where T depends on a.dtype().  T will be one of: float, double,\n// complex64, complex128.\ntemplate <typename Device, template <typename, typename> class BinaryFunctor>\nStatus CSRSparseMatrixBinaryHelper(OpKernelContext* ctx,\n                                   const CSRSparseMatrix& a,\n                                   const CSRSparseMatrix& b,\n                                   CSRSparseMatrix* c) {\n  DataType dt = a.dtype();\n  if (dt != b.dtype()) {\n    return errors::InvalidArgument(\n        \"CSRSparseMatrixBinaryHelper: Inconsistent dtypes for input matrices, \"\n        \"a \"\n        \"dtype: \",\n        DataTypeString(dt), \", b dtype: \", DataTypeString(b.dtype()));\n  }\n  switch (dt) {\n    case DT_FLOAT: {\n      BinaryFunctor<Device, float> functor(ctx);\n      return functor(a, b, c);\n    }\n    case DT_DOUBLE: {\n      BinaryFunctor<Device, double> functor(ctx);\n      return functor(a, b, c);\n    }\n    case DT_COMPLEX64: {\n      BinaryFunctor<Device, complex64> functor(ctx);\n      return functor(a, b, c);\n    }\n    case DT_COMPLEX128: {\n      BinaryFunctor<Device, complex128> functor(ctx);\n      return functor(a, b, c);\n    }\n    default:\n      return errors::InvalidArgument(\n          \"CSRSparseMatrixBinaryHelper: a.dtype (\", DataTypeString(dt),\n          \") is not one of: float, double, complex64, complex128\");\n  }\n}\n\n// Call UnaryFunctor<Device, T>()(ctx, a, b)\n// where T depends on a.dtype().  T will be one of: float, double,\n// complex64, complex128.\ntemplate <typename Device, template <typename, typename> class UnaryFunctor>\nStatus CSRSparseMatrixUnaryHelper(OpKernelContext* ctx,\n                                  const CSRSparseMatrix& a,\n                                  CSRSparseMatrix* b) {\n  DataType dt = a.dtype();\n  switch (dt) {\n    case DT_FLOAT: {\n      UnaryFunctor<Device, float> functor(ctx);\n      return functor(a, b);\n    }\n    case DT_DOUBLE: {\n      UnaryFunctor<Device, double> functor(ctx);\n      return functor(a, b);\n    }\n    case DT_COMPLEX64: {\n      UnaryFunctor<Device, complex64> functor(ctx);\n      return functor(a, b);\n    }\n    case DT_COMPLEX128: {\n      UnaryFunctor<Device, complex128> functor(ctx);\n      return functor(a, b);\n    }\n    default:\n      return errors::InvalidArgument(\n          \"CSRSparseMatrixUnaryHelper: a.dtype (\", DataTypeString(dt),\n          \") is not one of: float, double, complex64, complex128\");\n  }\n}\n\ntemplate <typename T>\nstruct ConstCSRComponent {\n  TTypes<int32>::UnalignedConstVec row_ptr;\n  TTypes<int32>::UnalignedConstVec col_ind;\n  typename TTypes<T>::UnalignedConstVec values;\n  TTypes<int64_t>::ConstVec dense_shape_host;\n};\n\ntemplate <typename T>\nstruct CSRComponent {\n  TTypes<int32>::UnalignedVec row_ptr;\n  TTypes<int32>::UnalignedVec col_ind;\n  typename TTypes<T>::UnalignedVec values;\n  TTypes<int64_t>::Vec dense_shape_host;\n};\n\ntemplate <typename T>\nStatus ExtractVariantFromInput(OpKernelContext* ctx, int index,\n                               const T** value) {\n  const Tensor& input_t = ctx->input(index);\n  if (!TensorShapeUtils::IsScalar(input_t.shape())) {\n    return errors::InvalidArgument(\n        \"Invalid input matrix: Shape must be rank 0 but is rank \",\n        input_t.dims());\n  }\n  const Variant& input_variant = input_t.scalar<Variant>()();\n  *value = input_variant.get<T>();\n  if (*value == nullptr) {\n    return errors::InvalidArgument(\"Could not retrieve Variant input \", index);\n  }\n  if (!(*value)->valid()) {\n    return errors::InvalidArgument(\"Variant input \", index, \" is not valid.\");\n  }\n  return OkStatus();\n}\n\n}  // namespace tensorflow\n\n#endif  // TENSORFLOW_CORE_KERNELS_SPARSE_SPARSE_MATRIX_H_\n", "# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"CSR sparse matrix tests.\"\"\"\n\nimport numpy as np\nfrom scipy import sparse\n\nfrom tensorflow.core.framework import tensor_pb2\nfrom tensorflow.core.protobuf import config_pb2\nfrom tensorflow.python.client import session\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import random_seed\nfrom tensorflow.python.framework import sparse_tensor\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.framework import tensor_util\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import linalg_ops\nfrom tensorflow.python.ops import map_fn\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.ops import random_ops\nfrom tensorflow.python.ops import sparse_ops\nfrom tensorflow.python.ops import state_ops\nfrom tensorflow.python.ops import variable_scope\nfrom tensorflow.python.ops.linalg.sparse import sparse_csr_matrix_ops\nfrom tensorflow.python.platform import test\nfrom tensorflow.python.platform import tf_logging\n\nCPU = \"/device:CPU:0\"\nGPU = \"/device:GPU:0\"\n\n\ndef dense_to_csr_sparse_matrix(dense):\n  dense_t = ops.convert_to_tensor(dense)\n  locs = array_ops.stop_gradient(array_ops.where(math_ops.abs(dense_t) > 0))\n  return sparse_csr_matrix_ops.dense_to_csr_sparse_matrix(dense_t, locs)\n\n\ndef _swap(a, i, j):\n  a[i], a[j] = a[j], a[i]\n\n\ndef twist_matrix(matrix, permutation_indices):\n  \"\"\"Permute the rows and columns of a 2D or (batched) 3D Tensor.\"\"\"\n  # Shuffle the rows and columns with the same permutation.\n  if matrix.shape.ndims == 2:\n    # Invert the permutation since `tf.gather` and `tf.gather_nd` need the\n    # mapping from each index `i` to the index that maps to `i`.\n    permutation_indices_inv = array_ops.invert_permutation(permutation_indices)\n    matrix = array_ops.gather(matrix, permutation_indices_inv, axis=0)\n    matrix = array_ops.gather(matrix, permutation_indices_inv, axis=1)\n  elif matrix.shape.ndims == 3:\n    permutation_indices_inv = map_fn.map_fn(array_ops.invert_permutation,\n                                            permutation_indices)\n    # For 3D Tensors, it's easy to shuffle the rows but not the columns. We\n    # permute the rows, transpose, permute the rows again, and transpose back.\n    batch_size = matrix.shape[0]\n    batch_indices = array_ops.broadcast_to(\n        math_ops.range(batch_size)[:, None], permutation_indices.shape)\n    for _ in range(2):\n      matrix = array_ops.gather_nd(\n          matrix,\n          array_ops.stack([batch_indices, permutation_indices_inv], axis=-1))\n      # Transpose the matrix, or equivalently, swap dimensions 1 and 2.\n      matrix = array_ops.transpose(matrix, perm=[0, 2, 1])\n  else:\n    raise ValueError(\"Input matrix must have rank 2 or 3. Got: {}\".format(\n        matrix.shape.ndims))\n\n  return matrix\n\n\nclass CSRSparseMatrixOpsTest(test.TestCase):\n\n  @classmethod\n  def setUpClass(cls):  # pylint: disable=g-missing-super-call\n    cls._gpu_available = test_util.is_gpu_available()\n\n  # TODO(ebrevdo): This will work once we find a way to get rendezvous\n  # working for CSRSparseMatrix and can remove the HostMemory\n  # annotations for the other ops.\n  @test_util.run_in_graph_and_eager_modes\n  def DISABLEDtestFromProto(self):\n    if not self._gpu_available:\n      return\n\n    a_indices = np.array([[0, 0], [2, 3]])\n    a_values = np.asarray([1.0, 5.0], dtype=np.float32)\n    a_dense_shape = np.asarray([5, 6], dtype=np.int64)\n    a_sparse_mat = sparse.coo_matrix(\n        (a_values, (a_indices[:, 0], a_indices[:, 1])), shape=a_dense_shape)\n    a_csr_mat = a_sparse_mat.tocsr()\n    a_col_inds = a_csr_mat.indices\n    a_row_ptrs = a_csr_mat.indptr\n\n    # Format of SparseMatrix:\n    #  type_name == \"tensorflow::CSRSparseMatrix\"\n    #  metadata == b (validated)\n    #  tensors == [dense_shape, row_ptrs, col_indices, values]\n    dense_shape_proto = tensor_util.make_tensor_proto(a_dense_shape)\n    row_ptrs_proto = tensor_util.make_tensor_proto(a_row_ptrs)\n    col_inds_proto = tensor_util.make_tensor_proto(a_col_inds)\n    values_proto = tensor_util.make_tensor_proto(a_values)\n    variant_tensor_data = tensor_pb2.VariantTensorDataProto(\n        type_name=\"tensorflow::CSRSparseMatrix\",\n        metadata=np.asarray(True).tobytes(),\n        tensors=[\n            dense_shape_proto, row_ptrs_proto, col_inds_proto, values_proto\n        ])\n    tensor_proto = tensor_pb2.TensorProto(\n        dtype=dtypes.variant.as_datatype_enum,\n        tensor_shape=tensor_shape.TensorShape([]).as_proto())\n    tensor_proto.variant_val.extend([variant_tensor_data])\n    a_sm = constant_op.constant(tensor_proto)\n    a_rt = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n        a_sm, type=dtypes.float32)\n    self.evaluate(a_rt)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testSparseTensorConversion(self):\n    a_indices = np.array([[0, 0], [2, 3], [2, 4], [3, 0]])\n    a_values = [1.0, 5.0, -1.0, -2.0]\n    a_dense_shape = [5, 6]\n    a_sparse_mat = sparse.coo_matrix(\n        (a_values, (a_indices[:, 0], a_indices[:, 1])), shape=a_dense_shape)\n    a_csr_mat = a_sparse_mat.tocsr()\n\n    # Convert 2D SparseTensor to CSR Matrix\n    a_st = sparse_tensor.SparseTensor(a_indices, a_values, a_dense_shape)\n    a_st = math_ops.cast(a_st, dtypes.float32)\n    a_sm = sparse_csr_matrix_ops.sparse_tensor_to_csr_sparse_matrix(\n        a_st.indices, a_st.values, a_st.dense_shape)\n\n    # Get row indices and columns for batch 0.\n    a_sm_row_ptrs, a_sm_col_inds, a_sm_values = (\n        sparse_csr_matrix_ops.csr_sparse_matrix_components(\n            a_sm, 0, type=a_st.dtype))\n\n    a_sm_row_ptrs_values, a_sm_col_inds_values, a_sm_values_values = (\n        self.evaluate((a_sm_row_ptrs, a_sm_col_inds, a_sm_values)))\n\n    self.assertAllEqual(a_csr_mat.indices, a_sm_col_inds_values)\n    self.assertAllEqual(a_csr_mat.indptr, a_sm_row_ptrs_values)\n    self.assertAllClose(a_values, a_sm_values_values)\n\n    # Convert CSR Matrix to 2D SparseTensor\n    a_st_rt = sparse_csr_matrix_ops.csr_sparse_matrix_to_sparse_tensor(\n        a_sm, type=a_st.dtype)\n    a_st_rt_value = self.evaluate(a_st_rt)\n\n    self.assertAllEqual(a_indices, a_st_rt_value.indices)\n    self.assertAllClose(a_values, a_st_rt_value.values)\n    self.assertAllEqual(a_dense_shape, a_st_rt_value.dense_shape)\n\n  def testSparseTensorConversionInvalidInputShapes(self):\n    values = constant_op.constant(\n        0.554979503, shape=[5], dtype=dtypes.float32)\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 1\"):\n      indices = constant_op.constant(0, shape=[5, 2], dtype=dtypes.int64)\n      dense_shape = constant_op.constant(53, shape=[], dtype=dtypes.int64)\n      csr = sparse_csr_matrix_ops.sparse_tensor_to_csr_sparse_matrix(\n          indices=indices, values=values, dense_shape=dense_shape)\n      self.evaluate(csr)\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 2\"):\n      indices = constant_op.constant(0, shape=[5], dtype=dtypes.int64)\n      dense_shape = constant_op.constant(53, shape=[1], dtype=dtypes.int64)\n      csr = sparse_csr_matrix_ops.sparse_tensor_to_csr_sparse_matrix(\n          indices=indices, values=values, dense_shape=dense_shape)\n      self.evaluate(csr)\n\n  # TODO(b/139491352): Add handle_data propagation to array_ops.identity.\n  @test_util.run_deprecated_v1\n  def testCSRSparseMatrixResourceVariable(self):\n    if not self._gpu_available:\n      return\n\n    sparsify = lambda m: m * (m > 0)\n    dense_shape = [53, 65, 127]\n    a_mats = sparsify(np.random.randn(*dense_shape)).astype(np.float32)\n\n    a_sm = dense_to_csr_sparse_matrix(a_mats)\n    with ops.device(\"/gpu:0\"):\n      v = variable_scope.get_variable(\"sm\", initializer=a_sm, use_resource=True)\n      v_id = array_ops.identity(v)\n      self.assertEqual(\n          sparse_csr_matrix_ops.dense_shape_and_type(v_id).shape, a_mats.shape)\n      a_rt = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n          v, type=dtypes.float32)\n    v_reassign = state_ops.assign(v, v_id).op\n    with self.assertRaisesOpError(\"uninitialized\"):\n      self.evaluate(a_rt)\n    self.evaluate(v.initializer)\n    a_rt_value = self.evaluate(a_rt)\n    self.assertAllClose(a_mats, a_rt_value)\n    self.evaluate(v_reassign)\n    a_rt_reassigned_value = self.evaluate(a_rt)\n    self.assertAllClose(a_mats, a_rt_reassigned_value)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testBatchSparseTensorConversion(self):\n    a_indices = np.array([[0, 0, 0], [0, 2, 3], [2, 0, 1]])\n    a_values = [1.0, 5.0, 6.0]\n    a_dense_shape = [3, 5, 6]\n    a_sparse_mats = [\n        sparse.coo_matrix(([1.0, 5.0], ([0, 2], [0, 3])),\n                          shape=a_dense_shape[1:]),\n        sparse.coo_matrix(([], ([], [])), shape=a_dense_shape[1:]),\n        sparse.coo_matrix(([6.0], ([0], [1])), shape=a_dense_shape[1:])\n    ]\n    a_csr_mats = [m.tocsr() for m in a_sparse_mats]\n\n    # Convert 3D SparseTensor to CSR Matrix\n    a_st = sparse_tensor.SparseTensor(a_indices, a_values, a_dense_shape)\n    a_st = math_ops.cast(a_st, dtypes.float32)\n    a_sm = sparse_csr_matrix_ops.sparse_tensor_to_csr_sparse_matrix(\n        a_st.indices, a_st.values, a_st.dense_shape)\n\n    # Get row indices and columns for batches.\n    a_sm_components = [\n        sparse_csr_matrix_ops.csr_sparse_matrix_components(\n            a_sm, i, type=a_st.dtype) for i in range(3)\n    ]\n\n    a_sm_values = self.evaluate(a_sm_components)\n\n    for i, (a_sm_val, a_csr_mat) in enumerate(zip(a_sm_values, a_csr_mats)):\n      tf_logging.info(\"Comparing batch %d\" % i)\n      self.assertAllEqual(a_csr_mat.indptr, a_sm_val.row_ptrs)\n      self.assertAllEqual(a_csr_mat.indices, a_sm_val.col_inds)\n      self.assertAllClose(a_csr_mat.data, a_sm_val.values)\n\n    # Convert CSR batched Matrix to 3D SparseTensor\n    a_st_rt = sparse_csr_matrix_ops.csr_sparse_matrix_to_sparse_tensor(\n        a_sm, type=a_st.dtype)\n    a_st_rt_value = self.evaluate(a_st_rt)\n\n    self.assertAllEqual(a_indices, a_st_rt_value.indices)\n    self.assertAllClose(a_values, a_st_rt_value.values)\n    self.assertAllEqual(a_dense_shape, a_st_rt_value.dense_shape)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testLargeBatchSparseTensorConversion(self):\n    # Test two sets of conversions to check behavior of the ops in a\n    # concurrent environment (parallel executions of the ST -> SM ops).\n\n    sparsify = lambda m: m * (m > 0)\n    dense_shape = [53, 65, 127]\n\n    mats = [\n        sparsify(np.random.randn(*dense_shape)).astype(np.float32)\n        for _ in range(2)\n    ]\n    csr_mats = [list(map(sparse.csr_matrix, mat)) for mat in mats]\n    mats_t = [ops.convert_to_tensor(mat) for mat in mats]\n    mats_locs = [array_ops.where(mat_t > 0) for mat_t in mats_t]\n    sparse_tensors = list()\n    for mat_t, mat_loc in zip(mats_t, mats_locs):\n      sparse_tensors.append(\n          sparse_tensor.SparseTensor(mat_loc,\n                                     array_ops.gather_nd(mat_t,\n                                                         mat_loc), dense_shape))\n    sparse_matrices = [\n        sparse_csr_matrix_ops.sparse_tensor_to_csr_sparse_matrix(\n            st.indices, st.values, st.dense_shape) for st in sparse_tensors\n    ]\n    sm_nnz = [\n        sparse_csr_matrix_ops.sparse_matrix_nnz(sm) for sm in sparse_matrices\n    ]\n\n    # Get row indices and columns for batches.\n    sm_components = list()\n    for sm in sparse_matrices:\n      sm_components.append([\n          sparse_csr_matrix_ops.csr_sparse_matrix_components(\n              sm, i, type=dtypes.float32) for i in range(dense_shape[0])\n      ])\n\n    sm_nnz_values, sm_values = self.evaluate((sm_nnz, sm_components))\n\n    for i, (sm_values_i, csr_mats_i) in enumerate(zip(sm_values, csr_mats)):\n      for b, (sm_val, csr_mat) in enumerate(zip(sm_values_i, csr_mats_i)):\n        tf_logging.info(\"Comparing matrix %d batch %d\" % (i, b))\n        self.assertEqual(csr_mat.nnz, sm_nnz_values[i][b])\n        self.assertAllEqual(csr_mat.indptr, sm_val.row_ptrs)\n        self.assertAllEqual(csr_mat.indices, sm_val.col_inds)\n        self.assertAllClose(csr_mat.data, sm_val.values)\n\n    # Convert CSR batched Matrix to 3D SparseTensor\n    st_rt = [\n        sparse_csr_matrix_ops.csr_sparse_matrix_to_sparse_tensor(\n            sm, type=dtypes.float32) for sm in sparse_matrices\n    ]\n\n    st_values, st_rt_values = self.evaluate((sparse_tensors, st_rt))\n\n    for (st_value, st_rt_value) in zip(st_values, st_rt_values):\n      self.assertAllEqual(st_value.indices, st_rt_value.indices)\n      self.assertAllClose(st_value.values, st_rt_value.values)\n      self.assertAllEqual(dense_shape, st_rt_value.dense_shape)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testDenseConversion(self):\n    a_indices = np.array([[0, 0], [2, 3], [2, 4], [3, 0]])\n    a_values = np.array([1.0, 5.0, -1.0, -2.0]).astype(np.float32)\n    a_dense_shape = [5, 6]\n    a_sparse_mat = sparse.coo_matrix(\n        (a_values, (a_indices[:, 0], a_indices[:, 1])), shape=a_dense_shape)\n    a_csr_mat = a_sparse_mat.tocsr()\n    a_dense = a_sparse_mat.todense()\n\n    # Convert 2D SparseTensor to CSR Matrix\n    a_sm = dense_to_csr_sparse_matrix(a_dense)\n\n    # Get row indices and columns for batch 0.\n    a_sm_row_ptrs, a_sm_col_inds, a_sm_values = (\n        sparse_csr_matrix_ops.csr_sparse_matrix_components(\n            a_sm, 0, type=dtypes.float32))\n\n    a_sm_row_ptrs_values, a_sm_col_inds_values, a_sm_values_values = (\n        self.evaluate((a_sm_row_ptrs, a_sm_col_inds, a_sm_values)))\n\n    self.assertAllEqual(a_csr_mat.indices, a_sm_col_inds_values)\n    self.assertAllEqual(a_csr_mat.indptr, a_sm_row_ptrs_values)\n    self.assertAllClose(a_values, a_sm_values_values)\n\n    # Convert CSR Matrix to 2D dense matrix\n    a_rt = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n        a_sm, dtypes.float32)\n    a_rt_value = self.evaluate(a_rt)\n\n    self.assertAllEqual(a_dense, a_rt_value)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testBatchDenseConversion(self):\n    a_dense_shape = [4, 5, 6]\n    a_sparse_mats = [\n        sparse.coo_matrix(([1.0, 5.0], ([0, 2], [0, 3])),\n                          shape=a_dense_shape[1:]),\n        sparse.coo_matrix(([], ([], [])), shape=a_dense_shape[1:]),\n        sparse.coo_matrix(([6.0], ([0], [1])), shape=a_dense_shape[1:]),\n        sparse.coo_matrix(([], ([], [])), shape=a_dense_shape[1:]),\n    ]\n    a_csr_mats = [m.tocsr() for m in a_sparse_mats]\n    a_dense = np.asarray([m.todense() for m in a_sparse_mats], dtype=np.float32)\n\n    # Convert 3D SparseTensor to CSR Matrix\n    a_sm = dense_to_csr_sparse_matrix(a_dense)\n\n    # Get row indices and columns for batches.\n    a_sm_components = [\n        sparse_csr_matrix_ops.csr_sparse_matrix_components(\n            a_sm, i, type=dtypes.float32) for i in range(3)\n    ]\n\n    a_sm_values = self.evaluate(a_sm_components)\n\n    for i, (a_sm_val, a_csr_mat) in enumerate(zip(a_sm_values, a_csr_mats)):\n      tf_logging.info(\"Comparing batch %d\" % i)\n      self.assertAllEqual(a_csr_mat.indptr, a_sm_val.row_ptrs)\n      self.assertAllEqual(a_csr_mat.indices, a_sm_val.col_inds)\n      self.assertAllClose(a_csr_mat.data, a_sm_val.values)\n\n    # Convert CSR batched Matrix to 3D SparseTensor\n    a_rt = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n        a_sm, type=dtypes.float32)\n    a_rt_value = self.evaluate(a_rt)\n\n    self.assertAllEqual(a_dense, a_rt_value)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testLargeBatchDenseConversion(self):\n    # Test two sets of conversions to check behavior of the ops in a\n    # concurrent environment (parallel executions of the ST -> SM\n    # ops).\n\n    sparsify = lambda m: m * (m > 0)\n    dense_shape = [53, 65, 127]\n\n    mats = [\n        sparsify(np.random.randn(*dense_shape)).astype(np.float32)\n        for _ in range(2)\n    ]\n    csr_mats = [[sparse.csr_matrix(m) for m in mat] for mat in mats]\n    mats_t = [ops.convert_to_tensor(mat) for mat in mats]\n    mats_locs = [array_ops.where(mat_t > 0) for mat_t in mats_t]\n    sparse_matrices = [\n        sparse_csr_matrix_ops.dense_to_csr_sparse_matrix(mat, mat_loc)\n        for (mat, mat_loc) in zip(mats_t, mats_locs)\n    ]\n    sm_nnz = [\n        sparse_csr_matrix_ops.sparse_matrix_nnz(sm) for sm in sparse_matrices\n    ]\n\n    # Get row indices and columns for batches.\n    sm_components = []\n    for sm in sparse_matrices:\n      sm_components.append([\n          sparse_csr_matrix_ops.csr_sparse_matrix_components(\n              sm, i, type=dtypes.float32) for i in range(dense_shape[0])\n      ])\n\n    sm_nnz_values, sm_values = self.evaluate((sm_nnz, sm_components))\n\n    for i, (sm_values_i, csr_mats_i) in enumerate(zip(sm_values, csr_mats)):\n      for b, (sm_val, csr_mat) in enumerate(zip(sm_values_i, csr_mats_i)):\n        tf_logging.info(\"Comparing matrix %d batch %d\" % (i, b))\n        self.assertEqual(csr_mat.nnz, sm_nnz_values[i][b])\n        self.assertAllEqual(csr_mat.indptr, sm_val.row_ptrs)\n        self.assertAllEqual(csr_mat.indices, sm_val.col_inds)\n        self.assertAllClose(csr_mat.data, sm_val.values)\n\n    # Convert CSR batched Matrix to 3D dense tensor\n    sm_rt = [\n        sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n            sm, type=dtypes.float32) for sm in sparse_matrices\n    ]\n\n    sm_rt_values = self.evaluate(sm_rt)\n\n    for (mat, sm_rt_value) in zip(mats, sm_rt_values):\n      self.assertAllEqual(mat, sm_rt_value)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testSparseMatrixAdd(self):\n    if not self._gpu_available:\n      return\n\n    a_indices = np.array([[0, 0], [2, 3]])\n    a_values = np.array([1.0, 5.0]).astype(np.float32)\n    a_dense_shape = [5, 6]\n    a_sparse_mat = sparse.coo_matrix(\n        (a_values, (a_indices[:, 0], a_indices[:, 1])), shape=a_dense_shape)\n    a_dense = a_sparse_mat.todense()\n\n    b_indices = np.array([[1, 0], [1, 4], [2, 3], [4, 1]])\n    b_values = np.array([1.0, 0.5, -5.0, 2.0]).astype(np.float32)\n    b_dense_shape = [5, 6]\n    b_sparse_mat = sparse.coo_matrix(\n        (b_values, (b_indices[:, 0], b_indices[:, 1])), shape=b_dense_shape)\n    b_dense = b_sparse_mat.todense()\n\n    for (alpha, beta) in [(1.0, 1.0), (1.0, -1.0), (0.25, 0.5)]:\n      a_sum_b_sparse_mat = alpha * a_sparse_mat + beta * b_sparse_mat\n\n      # Convert 2D SparseTensor to CSR Matrix\n      a_sm = dense_to_csr_sparse_matrix(a_dense)\n      b_sm = dense_to_csr_sparse_matrix(b_dense)\n      alpha = np.float32(alpha)\n      beta = np.float32(beta)\n      c_sm = sparse_csr_matrix_ops.sparse_matrix_add(\n          a_sm, b_sm, alpha=alpha, beta=beta)\n      c_dense = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n          c_sm, dtypes.float32)\n      c_dense_value = self.evaluate(c_dense)\n\n      self.assertAllClose(a_sum_b_sparse_mat.todense(), c_dense_value)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testLargeBatchSparseMatrixAdd(self):\n    if not self._gpu_available:\n      return\n\n    sparsify = lambda m: m * (m > 0)\n    dense_shape = [53, 65, 127]\n    a_mats = sparsify(np.random.randn(*dense_shape)).astype(np.float32)\n    b_mats = sparsify(np.random.randn(*dense_shape)).astype(np.float32)\n    for (alpha, beta) in [(1.0, 1.0), (1.0, -1.0), (0.25, 0.5)]:\n      tf_logging.info(\"testLargeBatchSparseMatrixAdd, comparing \"\n                      \"alpha, beta (%d, %d)\" % (alpha, beta))\n      a_sm = dense_to_csr_sparse_matrix(a_mats)\n      b_sm = dense_to_csr_sparse_matrix(b_mats)\n      alpha = np.float32(alpha)\n      beta = np.float32(beta)\n      c_sm = sparse_csr_matrix_ops.sparse_matrix_add(\n          a_sm, b_sm, alpha=alpha, beta=beta)\n      c_dense = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n          c_sm, dtypes.float32)\n      c_dense_value = self.evaluate(c_dense)\n\n      self.assertAllClose(c_dense_value, alpha * a_mats + beta * b_mats)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testSparseMatrixMatMul(self):\n    for shapes in [[(5, 6), (6, 1)], [(5, 6), (6, 2)]]:\n      a_indices = np.array([[0, 0], [2, 3]])\n      a_values = np.array([1.0, 5.0]).astype(np.float32)\n      a_dense_shape = shapes[0]\n      a_sparse_mat = sparse.coo_matrix(\n          (a_values, (a_indices[:, 0], a_indices[:, 1])), shape=a_dense_shape)\n      a_dense = a_sparse_mat.todense()\n\n      # Will multiply sparse a (shape=shapes[0]) by dense b (shape=shapes[1]).\n      b = np.random.randn(*shapes[1]).astype(np.float32)\n\n      a_sm = dense_to_csr_sparse_matrix(a_dense)\n      c = sparse_csr_matrix_ops.sparse_matrix_mat_mul(a=a_sm, b=b)\n      c_value = self.evaluate(c)\n\n      expected_c_value = a_sparse_mat.dot(b)\n      self.assertAllClose(expected_c_value, c_value)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testSparseMatrixMatMulConjugateOutput(self):\n    for shapes in [[(5, 6), (6, 1)], [(5, 6), (6, 2)]]:\n      a_indices = np.array([[0, 0], [2, 3]])\n      a_values = np.array([1.0 + 1.j, 5.0 - 2.j]).astype(np.complex64)\n      a_dense_shape = shapes[0]\n      a_sparse_mat = sparse.coo_matrix(\n          (a_values, (a_indices[:, 0], a_indices[:, 1])), shape=a_dense_shape)\n      a_dense = a_sparse_mat.todense()\n\n      # Will multiply sparse a (shape=shapes[0]) by dense b (shape=shapes[1]).\n      b = np.random.randn(*shapes[1]).astype(np.complex64)\n\n      a_sm = dense_to_csr_sparse_matrix(a_dense)\n      c = sparse_csr_matrix_ops.sparse_matrix_mat_mul(\n          a=a_sm, b=b, conjugate_output=True)\n      c_value = self.evaluate(c)\n\n      expected_c_value = self.evaluate(\n          math_ops.conj(test_util.matmul_without_tf32(a_dense, b)))\n      self.assertAllClose(expected_c_value, c_value)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testLargeBatchSparseMatrixMatMul(self):\n    dtypes_to_test = [np.float32, np.complex64]\n    sparsify = lambda m: m * (m > 0)\n    for dtype in dtypes_to_test:\n      for (transpose_a, transpose_b) in ((False, False), (False, True),\n                                         (True, False), (True, True)):\n        for (adjoint_a, adjoint_b) in ((False, False), (False, True),\n                                       (True, False), (True, True)):\n          if (transpose_a and adjoint_a) or (transpose_b and adjoint_b):\n            continue\n          for shapes in [[[53, 127, 65], [53, 65, 1]],\n                         [[53, 127, 1], [53, 1, 65]],\n                         [[53, 127, 65], [53, 65, 127]]]:\n            a_dense_shape = shapes[0]\n            b_dense_shape = shapes[1]\n            if transpose_a or adjoint_a:\n              _swap(a_dense_shape, -2, -1)\n            if transpose_b or adjoint_b:\n              _swap(b_dense_shape, -2, -1)\n            a_mats = sparsify(\n                (np.random.randn(*a_dense_shape) +\n                 1.j * np.random.randn(*a_dense_shape))).astype(dtype)\n            b_mats = (np.random.randn(*b_dense_shape) +\n                      1.j * np.random.randn(*b_dense_shape)).astype(dtype)\n            tf_logging.info(\n                \"testLargeBatchSparseMatrixMatMul transpose_a %s transpose_b \"\n                \"%s adjoint_a %s adjoint_b %s\" %\n                (transpose_a, transpose_b, adjoint_a, adjoint_b))\n            a_sm = dense_to_csr_sparse_matrix(a_mats)\n            c_t = sparse_csr_matrix_ops.sparse_matrix_mat_mul(\n                a_sm,\n                b_mats,\n                transpose_output=False,\n                conjugate_output=False,\n                transpose_a=transpose_a,\n                transpose_b=transpose_b,\n                adjoint_a=adjoint_a,\n                adjoint_b=adjoint_b)\n            c_dense_t = test_util.matmul_without_tf32(\n                a_mats,\n                b_mats,\n                transpose_a=transpose_a,\n                transpose_b=transpose_b,\n                adjoint_a=adjoint_a,\n                adjoint_b=adjoint_b)\n            self.assertAllEqual(c_dense_t.shape, c_t.shape)\n            c_t_value, c_dense_t_value = self.evaluate((c_t, c_dense_t))\n\n            self.assertAllClose(\n                c_t_value, c_dense_t_value, rtol=1e-6, atol=2e-5)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testLargeBatchSparseMatrixMatMulTransposed(self):\n    dtypes_to_test = [np.float32, np.complex64]\n\n    sparsify = lambda m: m * (m > 0)\n    for dtype in dtypes_to_test:\n      for (transpose_a, transpose_b) in ((False, False), (False, True),\n                                         (True, False), (True, True)):\n        for (adjoint_a, adjoint_b) in ((False, False), (False, True),\n                                       (True, False), (True, True)):\n          if (transpose_a and adjoint_a) or (transpose_b and adjoint_b):\n            continue\n          for shapes in [[[53, 127, 65], [53, 65, 1]],\n                         [[53, 127, 1], [53, 1, 65]],\n                         [[53, 127, 65], [53, 65, 127]]]:\n            a_dense_shape = shapes[0]\n            b_dense_shape = shapes[1]\n            if transpose_a or adjoint_a:\n              _swap(a_dense_shape, -2, -1)\n            if transpose_b or adjoint_b:\n              _swap(b_dense_shape, -2, -1)\n            a_mats = sparsify(\n                (np.random.randn(*a_dense_shape) +\n                 1.j * np.random.randn(*a_dense_shape))).astype(dtype)\n            b_mats = (np.random.randn(*b_dense_shape) +\n                      1.j * np.random.randn(*b_dense_shape)).astype(dtype)\n            tf_logging.info(\n                \"testLargeBatchSparseMatrixMatMul transpose_a %s transpose_b \"\n                \"%s adjoint_a %s adjoint_b %s\" %\n                (transpose_a, transpose_b, adjoint_a, adjoint_b))\n            a_sm = dense_to_csr_sparse_matrix(a_mats)\n            c_t = sparse_csr_matrix_ops.sparse_matrix_mat_mul(\n                a_sm,\n                b_mats,\n                transpose_output=True,\n                conjugate_output=False,\n                transpose_a=transpose_a,\n                transpose_b=transpose_b,\n                adjoint_a=adjoint_a,\n                adjoint_b=adjoint_b)\n\n            # Example: t(adj(a) . b) = t(b) . conj(a)\n            c_dense_t = test_util.matmul_without_tf32(\n                math_ops.conj(b_mats) if adjoint_b else b_mats,\n                math_ops.conj(a_mats) if adjoint_a else a_mats,\n                transpose_a=not (transpose_b or adjoint_b),\n                transpose_b=not (transpose_a or adjoint_a),\n                adjoint_a=False,\n                adjoint_b=False)\n            self.assertAllEqual(c_t.shape, c_dense_t.shape)\n            c_t_value, c_dense_t_value = self.evaluate((c_t, c_dense_t))\n            self.assertAllClose(\n                c_t_value, c_dense_t_value, rtol=1e-6, atol=2e-5)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testLargeBatchSparseMatrixMatMulConjugate(self):\n    sparsify = lambda m: m * (m > 0)\n    a_dense_shape = [53, 65, 127]\n    b_dense_shape = [53, 127, 67]\n    a_mats = sparsify(\n        (np.random.randn(*a_dense_shape) +\n         1.j * np.random.randn(*a_dense_shape))).astype(np.complex64)\n    b_mats = (np.random.randn(*b_dense_shape) +\n              1.j * np.random.randn(*b_dense_shape)).astype(np.complex64)\n    a_sm = dense_to_csr_sparse_matrix(a_mats)\n    c_t = sparse_csr_matrix_ops.sparse_matrix_mat_mul(\n        a_sm, b_mats, conjugate_output=True)\n\n    c_dense_t = math_ops.conj(test_util.matmul_without_tf32(a_mats, b_mats))\n    self.assertAllEqual(c_t.shape, c_dense_t.shape)\n    c_t_value, c_dense_t_value = self.evaluate((c_t, c_dense_t))\n\n    self.assertAllClose(c_t_value, c_dense_t_value, atol=1e-5, rtol=1e-5)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testSparseMatrixSparseMatMul(self):\n    a_indices = np.array([[0, 0], [2, 3]])\n    a_values = np.array([1.0, 5.0]).astype(np.float32)\n    a_dense_shape = [5, 6]\n    a_sparse_mat = sparse.coo_matrix(\n        (a_values, (a_indices[:, 0], a_indices[:, 1])), shape=a_dense_shape)\n    a_dense = a_sparse_mat.todense()\n\n    b_indices = np.array([[0, 0], [3, 0], [3, 1]])\n    b_values = np.array([2.0, 7.0, 8.0]).astype(np.float32)\n    b_dense_shape = [6, 7]\n    b_sparse_mat = sparse.coo_matrix(\n        (b_values, (b_indices[:, 0], b_indices[:, 1])), shape=b_dense_shape)\n    b_dense = b_sparse_mat.todense()\n\n    a_sm = dense_to_csr_sparse_matrix(a_dense)\n    b_sm = dense_to_csr_sparse_matrix(b_dense)\n    c_sm = sparse_csr_matrix_ops.sparse_matrix_sparse_mat_mul(\n        a=a_sm, b=b_sm, type=dtypes.float32)\n\n    c_sm_dense = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n        c_sm, dtypes.float32)\n    c_sm_dense_value = self.evaluate(c_sm_dense)\n\n    expected_c_value = a_sparse_mat.dot(b_sparse_mat).todense()\n    self.assertAllClose(expected_c_value, c_sm_dense_value)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testSparseMatrixSparseMatMul_NumericZerosNotPruned(self):\n    # Tests that numeric zeros appearing from the sparse-sparse matrix\n    # multiplication are not pruned from the sparse structural\n    a_indices = np.array([[0, 0], [0, 2]])\n    a_values = np.array([2.0, -1.0]).astype(np.float32)\n    a_dense_shape = [2, 3]\n    a_sparse_mat = sparse.coo_matrix(\n        (a_values, (a_indices[:, 0], a_indices[:, 1])), shape=a_dense_shape)\n    a_dense = a_sparse_mat.todense()\n\n    b_indices = np.array([[0, 1], [2, 1]])\n    b_values = np.array([3.0, 6.0]).astype(np.float32)\n    b_dense_shape = [3, 2]\n    b_sparse_mat = sparse.coo_matrix(\n        (b_values, (b_indices[:, 0], b_indices[:, 1])), shape=b_dense_shape)\n    b_dense = b_sparse_mat.todense()\n\n    # Convert to CSRSparseMatrix while removing numeric zeros from the\n    # structural representation.\n    a_sm = dense_to_csr_sparse_matrix(a_dense)\n    b_sm = dense_to_csr_sparse_matrix(b_dense)\n\n    # Compute the matmul.\n    c_sm = sparse_csr_matrix_ops.sparse_matrix_sparse_mat_mul(\n        a=a_sm, b=b_sm, type=dtypes.float32)\n    c_nnz = sparse_csr_matrix_ops.sparse_matrix_nnz(c_sm)\n    c_nnz_value = self.evaluate(c_nnz)\n\n    # Expect that there is a single numeric zero at index (0, 1) if zeros are\n    # not pruned, since 2.0 * 3.0 + (-1.0) * 6.0 = 0.0.\n    self.assertAllClose(1, c_nnz_value)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testLargeBatchSparseMatrixSparseMatMul(self):\n    sparsify = lambda m: m * (m > 0)\n\n    for (transpose_a, transpose_b) in ((False, False), (False, True),\n                                       (True, False), (True, True)):\n      for (adjoint_a, adjoint_b) in ((False, False), (False, True),\n                                     (True, False), (True, True)):\n        if (transpose_a and adjoint_a) or (transpose_b and adjoint_b):\n          continue\n\n        a_dense_shape = ([53, 127, 65]\n                         if transpose_a or adjoint_a else [53, 65, 127])\n        b_dense_shape = ([53, 67, 127]\n                         if transpose_b or adjoint_b else [53, 127, 67])\n\n        a_mats = sparsify(np.random.randn(*a_dense_shape)).astype(np.float32)\n        b_mats = sparsify(np.random.randn(*b_dense_shape).astype(np.float32))\n\n        a_sm = dense_to_csr_sparse_matrix(a_mats)\n        b_sm = dense_to_csr_sparse_matrix(b_mats)\n        c_sm = sparse_csr_matrix_ops.sparse_matrix_sparse_mat_mul(\n            a_sm,\n            b_sm,\n            type=dtypes.float32,\n            transpose_a=transpose_a,\n            adjoint_a=adjoint_a,\n            transpose_b=transpose_b,\n            adjoint_b=adjoint_b)\n        c_sm_dense = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n            c_sm, dtypes.float32)\n        c_dense_t = test_util.matmul_without_tf32(\n            a_mats,\n            b_mats,\n            transpose_a=transpose_a,\n            adjoint_a=adjoint_a,\n            transpose_b=transpose_b,\n            adjoint_b=adjoint_b)\n        c_dense_t_value, c_sm_dense_value = self.evaluate(\n            (c_dense_t, c_sm_dense))\n\n        self.assertAllClose(c_sm_dense_value, c_dense_t_value)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testLargeBatchRegisteredAddN(self):\n    if not self._gpu_available:\n      return\n\n    sparsify = lambda m: m * (m > 0)\n    dense_shape = [53, 65, 127]\n    matrices = [\n        sparsify(np.random.randn(*dense_shape)).astype(np.float32)\n        for _ in range(16)\n    ]\n    sparse_matrices = [dense_to_csr_sparse_matrix(mat) for mat in matrices]\n    sparse_matrices_sum = math_ops.add_n(sparse_matrices)\n    sparse_matrices_sum_dense = \\\n        sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n            sparse_matrices_sum, dtypes.float32)\n    sparse_matrices_sum_dense_value = self.evaluate(sparse_matrices_sum_dense)\n\n    # Ensure that the dense (numpy) sum across all batches matches the result\n    # of add_n converted back to dense.\n    expected_sum = np.sum(matrices, axis=0)\n    self.assertAllClose(expected_sum, sparse_matrices_sum_dense_value)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testCSRZeros(self):\n    if not self._gpu_available:\n      return\n    a_dense_shape = [65, 127]\n    b_dense_shape = [53, 127, 67]\n    data_types = [\n        dtypes.float32, dtypes.float64, dtypes.complex64, dtypes.complex128\n    ]\n    for dtype in data_types:\n      # Check both rank-2 and rank-3 tensors.\n      a_sm = sparse_csr_matrix_ops.sparse_matrix_zeros(\n          a_dense_shape, type=dtype)\n      b_sm = sparse_csr_matrix_ops.sparse_matrix_zeros(\n          b_dense_shape, type=dtype)\n      a_rt = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(a_sm, type=dtype)\n      b_rt = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(b_sm, type=dtype)\n      a_rt_value, b_rt_value = self.evaluate((a_rt, b_rt))\n\n      self.assertAllEqual(a_rt_value, np.zeros(a_dense_shape))\n      self.assertAllEqual(b_rt_value, np.zeros(b_dense_shape))\n\n  @test_util.run_in_graph_and_eager_modes\n  def testLargeBatchZerosLike(self):\n    if not self._gpu_available:\n      return\n\n    batch_size = 53\n    rows = 128\n    cols = 67\n    dense_shape = [batch_size, rows, cols]\n    data_types = [\n        dtypes.float32, dtypes.float64, dtypes.complex64, dtypes.complex128\n    ]\n    for dtype in data_types:\n      sparse_matrices = sparse_csr_matrix_ops.sparse_matrix_zeros(\n          dense_shape, type=dtype)\n      zeros_like_sparse_matrices = array_ops.zeros_like(sparse_matrices)\n      zeros_like_components = [\n          sparse_csr_matrix_ops.csr_sparse_matrix_components(\n              zeros_like_sparse_matrices, i, type=dtype)\n          for i in range(batch_size)\n      ]\n      zeros_like_components_values = self.evaluate(zeros_like_components)\n      for component in zeros_like_components_values:\n        self.assertAllEqual(component.row_ptrs, np.zeros(rows + 1, np.int32))\n        self.assertAllEqual(component.col_inds, np.empty([0], np.int32))\n        self.assertAllEqual(component.values, np.empty([0],\n                                                       dtype.as_numpy_dtype))\n\n  @test_util.run_in_graph_and_eager_modes\n  def testTranspose(self):\n    sparsify = lambda m: m * (m > 0)\n    dense_shape = [127, 65]\n    data_types = [\n        dtypes.float32, dtypes.float64, dtypes.complex64, dtypes.complex128\n    ]\n    for dtype in data_types:\n      mats = sparsify(\n          (np.random.randn(*dense_shape) +\n           1.j * np.random.randn(*dense_shape))).astype(dtype.as_numpy_dtype)\n      for conjugate in False, True:\n        expected = np.transpose(mats)\n        if conjugate:\n          expected = np.conj(expected)\n        matrices = math_ops.cast(mats, dtype)\n        sparse_matrices = dense_to_csr_sparse_matrix(matrices)\n        transpose_sparse_matrices = \\\n            sparse_csr_matrix_ops.sparse_matrix_transpose(\n                sparse_matrices, conjugate=conjugate, type=dtype)\n        dense_transposed = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n            transpose_sparse_matrices, dtype)\n        dense_transposed_values = self.evaluate(dense_transposed)\n        self.assertAllClose(expected, dense_transposed_values)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testLargeBatchTranspose(self):\n    sparsify = lambda m: m * (m > 0)\n    dense_shape = [53, 65, 127]\n    data_types = [\n        dtypes.float32, dtypes.float64, dtypes.complex64, dtypes.complex128\n    ]\n    for dtype in data_types:\n      mats = sparsify(\n          (np.random.randn(*dense_shape) +\n           1.j * np.random.randn(*dense_shape))).astype(dtype.as_numpy_dtype)\n      expected = np.transpose(mats, (0, 2, 1))\n      for conjugate in False, True:\n        if conjugate:\n          expected = np.conj(expected)\n        matrices = math_ops.cast(mats, dtype)\n        sparse_matrices = dense_to_csr_sparse_matrix(matrices)\n        transpose_sparse_matrices = \\\n            sparse_csr_matrix_ops.sparse_matrix_transpose(\n                sparse_matrices, conjugate=conjugate, type=dtype)\n        dense_transposed = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n            transpose_sparse_matrices, dtype)\n        dense_transposed_values = self.evaluate(dense_transposed)\n        self.assertAllClose(expected, dense_transposed_values)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testSoftmax(self):\n    if not self._gpu_available:\n      return\n\n    sparsify = lambda m: m * (m > 0)\n    dense_shape = [127, 65]\n    logits = sparsify(np.random.randn(*dense_shape))\n    logits_with_ninf = np.copy(logits)\n    logits_with_ninf[logits == 0] = -np.inf\n    data_types = [dtypes.float32, dtypes.float64]\n    for dtype in data_types:\n      logits_t = math_ops.cast(logits, dtype)\n      logits_t_with_ninf = math_ops.cast(logits_with_ninf, dtype)\n      expected = nn_ops.softmax(logits_t_with_ninf)\n      sparse_logits_t = dense_to_csr_sparse_matrix(logits_t)\n      softmax_sparse_logits_t = sparse_csr_matrix_ops.sparse_matrix_softmax(\n          sparse_logits_t, type=dtype)\n      dense_softmax = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n          softmax_sparse_logits_t, dtype)\n      dense_softmax_values, expected_values = self.evaluate(\n          (dense_softmax, expected))\n      self.assertAllClose(expected_values, dense_softmax_values)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testLargeBatchSoftmax(self):\n    if not self._gpu_available:\n      return\n\n    sparsify = lambda m: m * (m > 0)\n    dense_shape = [53, 65, 127]\n    logits = sparsify(np.random.randn(*dense_shape))\n    logits_with_ninf = np.copy(logits)\n    logits_with_ninf[logits == 0] = -np.inf\n    data_types = [dtypes.float32, dtypes.float64]\n    for dtype in data_types:\n      logits_t = math_ops.cast(logits, dtype)\n      logits_t_with_ninf = math_ops.cast(logits_with_ninf, dtype)\n      expected = nn_ops.softmax(logits_t_with_ninf)\n      sparse_logits_t = dense_to_csr_sparse_matrix(logits_t)\n      softmax_sparse_logits_t = sparse_csr_matrix_ops.sparse_matrix_softmax(\n          sparse_logits_t, type=dtype)\n      dense_softmax = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n          softmax_sparse_logits_t, dtype)\n      dense_softmax_values, expected_values = self.evaluate(\n          (dense_softmax, expected))\n      self.assertAllClose(expected_values, dense_softmax_values)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testLargeBatchSoftmaxEmpty(self):\n    if not self._gpu_available:\n      return\n\n    dense_shape = [53, 65, 127]\n    sparse_logits_t = sparse_csr_matrix_ops.sparse_matrix_zeros(\n        dense_shape, type=dtypes.float32)\n    softmax_sparse_logits_t = sparse_csr_matrix_ops.sparse_matrix_softmax(\n        sparse_logits_t, type=dtypes.float32)\n    dense_softmax = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n        softmax_sparse_logits_t, dtypes.float32)\n    dense_softmax_values = self.evaluate(dense_softmax)\n    self.assertAllEqual(\n        np.zeros_like(dense_softmax_values), dense_softmax_values)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testSoftmaxGrad(self):\n    if not self._gpu_available:\n      return\n\n    sparsify = lambda m: m * (m > 0)\n    dense_shape = [127, 65]\n    softmax = sparsify(np.random.randn(*dense_shape))\n    grad_softmax = sparsify(np.random.randn(*dense_shape))\n    expected = (\n        (grad_softmax - np.sum(grad_softmax * softmax, -1, keepdims=True)) *\n        softmax)\n    data_types = [dtypes.float32, dtypes.float64]\n    for dtype in data_types:\n      softmax_t = math_ops.cast(softmax, dtype)\n      grad_softmax_t = math_ops.cast(grad_softmax, dtype)\n      softmax_sparse = dense_to_csr_sparse_matrix(softmax_t)\n      grad_softmax_sparse = dense_to_csr_sparse_matrix(grad_softmax_t)\n      gradients_sparse = sparse_csr_matrix_ops.sparse_matrix_softmax_grad(\n          softmax_sparse, grad_softmax_sparse, dtype)\n      dense_gradients = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n          gradients_sparse, dtype)\n      dense_gradients_values = self.evaluate((dense_gradients))\n      self.assertAllClose(expected, dense_gradients_values)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testLargeBatchSoftmaxGrad(self):\n    if not self._gpu_available:\n      return\n\n    sparsify = lambda m: m * (m > 0)\n    dense_shape = [53, 65, 127]\n    softmax = sparsify(np.random.randn(*dense_shape))\n    grad_softmax = sparsify(np.random.randn(*dense_shape))\n    expected = (\n        (grad_softmax - np.sum(grad_softmax * softmax, -1, keepdims=True)) *\n        softmax)\n    data_types = [dtypes.float32, dtypes.float64]\n    for dtype in data_types:\n      softmax_t = math_ops.cast(softmax, dtype)\n      grad_softmax_t = math_ops.cast(grad_softmax, dtype)\n      softmax_sparse = dense_to_csr_sparse_matrix(softmax_t)\n      grad_softmax_sparse = dense_to_csr_sparse_matrix(grad_softmax_t)\n      gradients_sparse = sparse_csr_matrix_ops.sparse_matrix_softmax_grad(\n          softmax_sparse, grad_softmax_sparse, dtype)\n      dense_gradients = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n          gradients_sparse, dtype)\n      dense_gradients_values = self.evaluate((dense_gradients))\n      self.assertAllClose(expected, dense_gradients_values)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testLargeBatchSoftmaxGradEmpty(self):\n    if not self._gpu_available:\n      return\n\n    sparsify = lambda m: m * (m > 0)\n    dense_shape = [53, 65, 127]\n    not_empty = sparsify(np.random.randn(*dense_shape)).astype(np.float32)\n    sparse_empty = sparse_csr_matrix_ops.sparse_matrix_zeros(\n        dense_shape, type=dtypes.float32)\n    sparse_not_empty = dense_to_csr_sparse_matrix(not_empty)\n    gradients_empty_softmax = sparse_csr_matrix_ops.sparse_matrix_softmax_grad(\n        sparse_empty, sparse_not_empty, dtypes.float32)\n    gradients_empty_grad_softmax = (\n        sparse_csr_matrix_ops.sparse_matrix_softmax_grad(\n            sparse_not_empty, sparse_empty, dtypes.float32))\n    gradients_empty_both = sparse_csr_matrix_ops.sparse_matrix_softmax_grad(\n        sparse_empty, sparse_empty, dtypes.float32)\n    ges = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n        gradients_empty_softmax, dtypes.float32)\n    gegs = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n        gradients_empty_grad_softmax, dtypes.float32)\n    geb = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n        gradients_empty_both, dtypes.float32)\n    ges_v, gegs_v, geb_v = self.evaluate((ges, gegs, geb))\n    for v in (ges_v, gegs_v, geb_v):\n      self.assertAllEqual(np.zeros(dense_shape), v)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testLargeBatchConj(self):\n    if not self._gpu_available:\n      return\n\n    sparsify = lambda m: m * (np.real(m) > 0)\n    dense_shape = [53, 65, 127]\n    matrices = (\n        sparsify(np.random.randn(*dense_shape)) +\n        1j * np.random.randn(*dense_shape))\n    data_types = [\n        dtypes.float32, dtypes.float64, dtypes.complex64, dtypes.complex128\n    ]\n    for dtype in data_types:\n      matrices_t = matrices.astype(dtype.as_numpy_dtype)\n      expected = np.conj(matrices_t)\n      sparse_matrices = dense_to_csr_sparse_matrix(matrices_t)\n      conj_sparse_matrices = math_ops.conj(sparse_matrices)\n      dense_conj_matrices = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n          conj_sparse_matrices, dtype)\n      conj_values = self.evaluate(dense_conj_matrices)\n      self.assertAllClose(expected, conj_values)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testLargeBatchSparseMatrixMulScalar(self):\n    if not self._gpu_available:\n      return\n\n    sparsify = lambda m: m * (m > 0)\n    a_dense_shape = [53, 65, 127]\n    a_mats = sparsify(np.random.randn(*a_dense_shape)).astype(np.float32)\n    b = np.float32(3.5)\n    expected = a_mats * b\n    a_sm = dense_to_csr_sparse_matrix(a_mats)\n    c_t = sparse_csr_matrix_ops.sparse_matrix_mul(a_sm, b)\n    c_dense_t = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n        c_t, dtypes.float32)\n    c_dense_t_value = self.evaluate(c_dense_t)\n\n    self.assertAllClose(expected, c_dense_t_value)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testLargeBatchSparseMatrixMulVec(self):\n    if not self._gpu_available:\n      return\n\n    sparsify = lambda m: m * (m > 0)\n    a_dense_shape = [53, 65, 127]\n    a_mats = sparsify(np.random.randn(*a_dense_shape)).astype(np.float32)\n    b = np.random.randn(53, 1, 1).astype(np.float32)\n    expected = a_mats * b\n    a_sm = dense_to_csr_sparse_matrix(a_mats)\n    c_t = sparse_csr_matrix_ops.sparse_matrix_mul(a_sm, b)\n    c_dense_t = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n        c_t, dtypes.float32)\n    c_dense_t_value = self.evaluate(c_dense_t)\n\n    self.assertAllClose(expected, c_dense_t_value)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testSparseCholesky(self):\n    dense_matrix = np.array([\n        [2, 0, 0, 0, 0, 0],\n        [0, 3, 0, 0, 0, 0],\n        [1, 1, 7, 0, 0, 0],\n        [0, 0, 0, 4, 0, 0],\n        [0, 0, 1, 0, 5, 0],\n        [0, 0, 2, 0, 1, 6],\n    ]).astype(np.complex128)\n\n    data_types = [\n        dtypes.float32, dtypes.float64, dtypes.complex64, dtypes.complex128\n    ]\n    for dtype in data_types:\n      with test_util.force_cpu():\n        if dtype.is_complex:\n          dense_matrix += 0.5j * np.tril(dense_matrix, -1)\n\n        sparse_matrix = dense_to_csr_sparse_matrix(\n            math_ops.cast(dense_matrix, dtype))\n        # Obtain the Sparse Cholesky factor using AMD Ordering for reducing\n        # fill-in.\n        ordering_amd = sparse_csr_matrix_ops.sparse_matrix_ordering_amd(\n            sparse_matrix)\n        cholesky_sparse_matrices = (\n            sparse_csr_matrix_ops.sparse_matrix_sparse_cholesky(\n                sparse_matrix, ordering_amd, type=dtype))\n        dense_cholesky = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n            cholesky_sparse_matrices, dtype)\n        # Compute L * Lh where L is the Sparse Cholesky factor.\n        verification = test_util.matmul_without_tf32(\n            dense_cholesky, array_ops.transpose(dense_cholesky, conjugate=True))\n        verification = twist_matrix(verification, ordering_amd)\n        # Assert that input matrix A satisfies A = L * Lh.\n        verification_values = self.evaluate(verification)\n        full_dense_matrix = (\n            dense_matrix +\n            np.conjugate(np.transpose(np.tril(dense_matrix, -1))))\n        self.assertAllClose(full_dense_matrix, verification_values)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testBatchSparseCholesky(self):\n    dense_mat = np.array([\n        # A diagonal matrix.\n        [\n            [1, 0, 0, 0],  #\n            [0, 2, 0, 0],  #\n            [0, 0, 3, 0],  #\n            [0, 0, 0, 4],\n        ],  #\n        # A tridiagonal hermitian matrix.\n        [\n            [5 + 0j, 1 + 0j, 0 + 0j, 0 + 0j],  #\n            [1 + 0j, 4 + 0j, 1 + 2j, 0 + 0j],  #\n            [0 + 0j, 1 - 2j, 9 + 0j, 3 - 3j],  #\n            [0 + 0j, 0 + 0j, 3 + 3j, 7 + 0j],\n        ],  #\n        # A diagonal matrix with a corner element; for which\n        # OrderingAMD returns a non-identity permutation.\n        [\n            [1, 0, 0, 1.],  #\n            [0, 2, 0, 0.],  #\n            [0, 0, 3, 0.],  #\n            [1, 0, 0, 4.],\n        ]  #\n    ]).astype(np.complex128)\n\n    data_types = [\n        dtypes.float32, dtypes.float64, dtypes.complex64, dtypes.complex128\n    ]\n    for dtype in data_types:\n      sparse_matrix = dense_to_csr_sparse_matrix(\n          math_ops.cast(dense_mat, dtype))\n      ordering_amd = sparse_csr_matrix_ops.sparse_matrix_ordering_amd(\n          sparse_matrix)\n\n      cholesky_sparse_matrix = (\n          sparse_csr_matrix_ops.sparse_matrix_sparse_cholesky(\n              sparse_matrix, ordering_amd, type=dtype))\n      dense_cholesky = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n          cholesky_sparse_matrix, dtype)\n\n      # Compute L * Lh.\n      verification = test_util.matmul_without_tf32(\n          dense_cholesky,\n          array_ops.transpose(dense_cholesky, perm=[0, 2, 1], conjugate=True))\n      verification = twist_matrix(verification, ordering_amd)\n\n      verification_values = self.evaluate(verification)\n      self.assertAllClose(\n          dense_mat.astype(dtype.as_numpy_dtype), verification_values)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testLargeBatchSparseCholesky(self):\n    sparsity = 0.1\n    sparsify = lambda m: m * (m > 1 - sparsity)\n\n    batch_size = 53\n    num_rows = 147\n    dense_shape = [batch_size, num_rows, num_rows]\n\n    dense_matrix = sparsify(np.random.uniform(size=dense_shape)).astype(\n        np.float32)\n\n    # Create a \"random\" SPD matrix, by choosing each entry of A between\n    # 0 and 1 at the specified density, and computing 0.5(A + At) + n*I.\n    # This ensures diagonal dominance which implies positive-definiteness.\n    dense_matrix = (\n        0.5 *\n        (dense_matrix + array_ops.transpose(dense_matrix, perm=[0, 2, 1])) +\n        num_rows * linalg_ops.eye(dense_shape[-1], batch_shape=[batch_size]))\n    # Compute the fill-in reducing permutation and use it to perform\n    # the Sparse Cholesky factorization.\n    sparse_matrix = dense_to_csr_sparse_matrix(dense_matrix)\n    ordering_amd = sparse_csr_matrix_ops.sparse_matrix_ordering_amd(\n        sparse_matrix)\n\n    cholesky_sparse_matrix = \\\n        sparse_csr_matrix_ops.sparse_matrix_sparse_cholesky(\n            sparse_matrix, ordering_amd, type=dtypes.float32)\n    dense_cholesky = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n        cholesky_sparse_matrix, dtypes.float32)\n\n    # Compute L * Lh.\n    verification = test_util.matmul_without_tf32(\n        dense_cholesky, array_ops.transpose(dense_cholesky, perm=[0, 2, 1]))\n    verification = twist_matrix(verification, ordering_amd)\n    verification_values = self.evaluate(verification)\n    self.assertAllClose(dense_matrix, verification_values, atol=1e-5, rtol=1e-5)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testSparseCholesky_InvalidMatrix(self):\n    # Verify that non-SPD matrices result in an Invalid Argument error.\n    invalid_matrices = [\n        # zero matrix.\n        np.array([\n            [0., 0., 0., 0.],  #\n            [0., 0., 0., 0.],  #\n            [0., 0., 0., 0.],  #\n            [0., 0., 0., 0.]  #\n        ]),\n        # zero diagonal entry.\n        np.array([\n            [9., 0., 5., 0.],  #\n            [0., 0., 0., 1.],  #\n            [5., 0., 8., 0.],  #\n            [0., 1., 0., 7.]  #\n        ]),\n        # not positive definite.\n        np.array([\n            [2., -2., 0., 0.],  #\n            [-2., 2., 0., 0.],  #\n            [0., 0., 3., -3.],  #\n            [0., 0., -3., 3.]  #\n        ]),\n    ]\n\n    with test_util.force_cpu():\n      for invalid_matrix in invalid_matrices:\n        with self.assertRaises(errors.InvalidArgumentError):\n          sparse_matrix = dense_to_csr_sparse_matrix(\n              invalid_matrix.astype(np.float32))\n          # Compute the fill-in reducing permutation and use it to perform\n          # the Sparse Cholesky factorization.\n          ordering_amd = sparse_csr_matrix_ops.sparse_matrix_ordering_amd(\n              sparse_matrix)\n          cholesky_sparse_matrices = (\n              sparse_csr_matrix_ops.sparse_matrix_sparse_cholesky(\n                  sparse_matrix, ordering_amd, type=dtypes.float32))\n          # Convert the Cholesky factor to a dense matrix to be evaluated.\n          dense_cholesky = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n              cholesky_sparse_matrices, type=dtypes.float32)\n          self.evaluate(dense_cholesky)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testOrderingAMD(self):\n    num_rows = 6\n    # An SPD matrix where AMD ordering can reduce fill-in for Cholesky factor.\n    dense_matrix = np.array([\n        [7, 0, 0, 0, 0, 0],\n        [1, 4, 0, 0, 0, 0],\n        [1, 1, 3, 0, 0, 0],\n        [0, 0, 0, 4, 0, 0],\n        [2, 0, 0, 0, 5, 0],\n        [1, 2, 2, 0, 0, 6],\n    ]).astype(np.float32)\n\n    with test_util.force_cpu():\n      sparse_matrix = dense_to_csr_sparse_matrix(dense_matrix)\n\n      # Obtain the Sparse Cholesky factor with the identity permutation as the\n      # fill-in reducing ordering.\n      cholesky_without_ordering = (\n          sparse_csr_matrix_ops.sparse_matrix_sparse_cholesky(\n              sparse_matrix, math_ops.range(num_rows), type=dtypes.float32))\n      cholesky_without_ordering_nnz = sparse_csr_matrix_ops.sparse_matrix_nnz(\n          cholesky_without_ordering)\n\n      # Obtain the Sparse Cholesky factor using AMD Ordering for reducing\n      # fill-in.\n      ordering_amd = sparse_csr_matrix_ops.sparse_matrix_ordering_amd(\n          sparse_matrix)\n      cholesky_with_amd = sparse_csr_matrix_ops.sparse_matrix_sparse_cholesky(\n          sparse_matrix, ordering_amd, type=dtypes.float32)\n      cholesky_with_amd_nnz = sparse_csr_matrix_ops.sparse_matrix_nnz(\n          cholesky_with_amd)\n\n      (ordering_amd_value, cholesky_with_amd_nnz_value,\n       cholesky_without_ordering_nnz_value) = self.evaluate(\n           [ordering_amd, cholesky_with_amd_nnz, cholesky_without_ordering_nnz])\n\n      # AMD ordering should return a valid permutation.\n      self.assertAllClose(np.arange(num_rows), np.sort(ordering_amd_value))\n      # Check that cholesky with AMD ordering has a strictly lower nonzero count\n      # for this matrix.\n      self.assertLess(cholesky_with_amd_nnz_value,\n                      cholesky_without_ordering_nnz_value)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testNoMatrixNoCrash(self):\n    # Round-about way of creating an empty variant tensor that works in both\n    # graph and eager modes.\n    no_matrix = array_ops.reshape(dense_to_csr_sparse_matrix([[0.0]]), [1])[0:0]\n    with self.assertRaisesRegex(\n        (ValueError, errors.InvalidArgumentError),\n        \"(Invalid input matrix)|(Shape must be rank 0)\"):\n      sparse_csr_matrix_ops.sparse_matrix_nnz(no_matrix)\n\n\nclass CSRSparseMatrixOpsBenchmark(test.Benchmark):\n\n  def benchmark_sparse_matrix_mat_mul_gpu(self):\n    if not test_util.is_gpu_available():\n      return\n\n    sparsify = lambda m: array_ops.where(m > 2, m, array_ops.zeros_like(m))\n\n    # XW, X dense and W sparse\n    # X is shaped [{1, 8, 16}, 2000]\n    # W is shaped [2000, 4000]\n\n    for batch_size in [1, 8, 16]:\n      x_dense_shape = [batch_size, 2000]\n      w_dense_shape = [2000, 4000]\n\n      with ops.Graph().as_default(), ops.device(\"/gpu:0\"):\n        x_mats = random_ops.random_normal(x_dense_shape, dtype=dtypes.float32)\n        w_mats = sparsify(\n            random_ops.random_normal(w_dense_shape, dtype=dtypes.float32))\n        nnz = array_ops.shape(array_ops.where(w_mats))[0]\n        ratio = math_ops.cast(nnz, dtypes.float32) / np.prod(w_dense_shape)\n        w_sm = dense_to_csr_sparse_matrix(w_mats)\n        with ops.name_scope(\"w_sm_var\"):\n          w_sm_var = variable_scope.get_variable(\n              \"sm\", initializer=w_sm, use_resource=True)\n          w_sm_var_v = w_sm_var.read_value()\n        with ops.name_scope(\"w_var\"):\n          w_var = variable_scope.get_variable(\n              \"sm_dense\", initializer=w_mats, use_resource=True)\n          w_var_v = w_var.read_value()\n        with ops.name_scope(\"b\"):\n          x = variable_scope.get_variable(\n              \"b\", initializer=x_mats, use_resource=True)\n          x_v = x.read_value()\n        # X*W = (W'*X')'\n        xw_sparse = sparse_csr_matrix_ops.sparse_matrix_mat_mul(\n            w_sm_var_v,\n            x_v,\n            transpose_a=True,\n            transpose_b=True,\n            transpose_output=True)\n        xw_dense = math_ops.matmul(x_v, w_var_v)\n\n        with session.Session() as sess:\n          self.evaluate(\n              [w_var.initializer, w_sm_var.initializer, x.initializer])\n          nnz_value, ratio_value = self.evaluate((nnz, ratio))\n          name_template = (\n              \"sparse_matrix_mat_mul_gpu_%s_W_2000x4000_batch_size_%d\")\n          self.run_op_benchmark(\n              sess,\n              xw_sparse.op,\n              name=name_template % (\"sparse\", batch_size),\n              extras={\n                  \"percentage_nonzero\": ratio_value,\n                  \"num_nonzero\": nnz_value\n              },\n              min_iters=50)\n          self.run_op_benchmark(\n              sess,\n              xw_dense.op,\n              name=name_template % (\"dense\", batch_size),\n              extras={\n                  \"percentage_nonzero\": ratio_value,\n                  \"num_nonzero\": nnz_value\n              },\n              min_iters=50)\n\n  def benchmark_sparse_matrix_mat_vec_mul(self):\n    # num_rows, device, transpose.\n    cases = [\n        [2000, CPU, False],\n        [8000, CPU, False],\n        [12000, CPU, False],\n        [2000, CPU, True],\n        [8000, CPU, True],\n        [12000, CPU, True],\n    ]\n    seed = 42\n\n    for num_rows, device, transpose in cases:\n      if device == GPU and not test_util.is_gpu_available():\n        continue\n      for num_threads in [1, 2, 4, 6, 8, 10]:\n        device_str = \"cpu\" if device == CPU else \"gpu\"\n        w_dense_shape = [num_rows, num_rows]\n        x_dense_shape = [num_rows, 1]\n\n        with ops.Graph().as_default(), ops.device(device):\n          random_seed.set_random_seed(seed)\n          x = random_ops.random_normal(x_dense_shape, dtype=dtypes.float32)\n          w_np = sparse.rand(\n              w_dense_shape[0],\n              w_dense_shape[1],\n              density=0.01,\n              dtype=np.float32,\n              random_state=np.random.RandomState(seed))\n          w_st = sparse_tensor.SparseTensor(\n              zip(w_np.row, w_np.col), w_np.data, w_np.shape)\n          w_st = sparse_ops.sparse_reorder(w_st)\n\n          nnz = array_ops.shape(w_st.values)[0]\n          ratio = math_ops.cast(nnz, dtypes.float32) / np.prod(w_np.shape)\n\n          w_sm = sparse_csr_matrix_ops.sparse_tensor_to_csr_sparse_matrix(\n              w_st.indices, w_st.values, w_st.dense_shape)\n          xw_sparse_matrix = sparse_csr_matrix_ops.sparse_matrix_mat_mul(\n              w_sm,\n              x,\n              transpose_a=transpose,\n              transpose_b=False,\n              transpose_output=False)\n          xw_sparse_tensor = sparse_ops.sparse_tensor_dense_matmul(\n              w_st, x, adjoint_a=transpose, adjoint_b=False)\n\n          with session.Session(\n              config=config_pb2.ConfigProto(\n                  intra_op_parallelism_threads=num_threads)) as sess:\n            nnz_value, ratio_value = sess.run((nnz, ratio))\n            name_template = (\"mat_vec_mul_%s_%s_W_%d_transpose_%s_threads_%d\")\n            self.run_op_benchmark(\n                sess,\n                xw_sparse_matrix.op,\n                name=name_template %\n                (device_str, \"sparse_matrix\", num_rows, transpose, num_threads),\n                extras={\n                    \"percentage_nonzero\": ratio_value,\n                    \"num_nonzero\": nnz_value,\n                },\n                min_iters=10)\n            self.run_op_benchmark(\n                sess,\n                xw_sparse_tensor.op,\n                name=name_template %\n                (device_str, \"sparse_tensor\", num_rows, transpose, num_threads),\n                extras={\n                    \"percentage_nonzero\": ratio_value,\n                    \"num_nonzero\": nnz_value,\n                },\n                min_iters=10)\n\n  def benchmark_sparse_matrix_sparse_matmul(self):\n    density = 0.05\n    # pylint: disable=g-long-lambda\n    sparsify = lambda m: array_ops.where(m > 1. - density, m,\n                                         array_ops.zeros_like(m))\n    # pylint: enable=g-long-lambda\n\n    for batch_size in [1, 16]:\n      for num_threads in [1, 4, 12]:\n        dense_shape = [batch_size, 250, 250]\n\n        for device in [CPU, GPU]:\n          if device == GPU and not test_util.is_gpu_available():\n            continue\n\n          with ops.Graph().as_default(), ops.device(device):\n            x_mats = sparsify(\n                random_ops.random_uniform(dense_shape, dtype=dtypes.float32))\n            y_mats = sparsify(\n                random_ops.random_uniform(dense_shape, dtype=dtypes.float32))\n\n            nnz = array_ops.shape(array_ops.where(x_mats))[0] + array_ops.shape(\n                array_ops.where(y_mats))[0]\n            ratio = math_ops.cast(nnz,\n                                  dtypes.float32) / (2 * np.prod(dense_shape))\n\n            x_sm = dense_to_csr_sparse_matrix(x_mats)\n            y_sm = dense_to_csr_sparse_matrix(y_mats)\n\n            xy_sparse = sparse_csr_matrix_ops.sparse_matrix_sparse_mat_mul(\n                x_sm, y_sm, type=dtypes.float32)\n\n            with session.Session(\n                config=config_pb2.ConfigProto(\n                    intra_op_parallelism_threads=num_threads)) as sess:\n              nnz_value, ratio_value = self.evaluate((nnz, ratio))\n              name_template = (\n                  \"sparse_matrix_sparse_matmul_%s_N_%d_batch_size_%d_threads_%d\"\n              )\n              device_str = \"cpu\" if device == CPU else \"gpu\"\n              self.run_op_benchmark(\n                  sess,\n                  xy_sparse.op,\n                  name=name_template %\n                  (device_str, dense_shape[-1], batch_size, num_threads),\n                  extras={\n                      \"percentage_nonzero\": ratio_value,\n                      \"num_nonzero\": nnz_value\n                  },\n                  min_iters=50)\n\n  def benchmark_sparse_dense_conversion(self):\n    sparsity = 0.05\n\n    for batch_size in [1, 16]:\n      for num_threads in [1, 4, 12]:\n        dense_shape = [batch_size, 750, 750]\n\n        for device in [CPU, GPU]:\n          if device == GPU and not test_util.is_gpu_available():\n            continue\n\n          with ops.Graph().as_default(), ops.device(device):\n            mats = random_ops.random_uniform(dense_shape, dtype=dtypes.float32)\n            mats_locs = array_ops.where(mats > 1.0 - sparsity)\n\n            sparse_matrices = sparse_csr_matrix_ops.dense_to_csr_sparse_matrix(\n                mats, mats_locs)\n            dense_matrices = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\n                sparse_matrices, type=dtypes.float32)\n            nnz = math_ops.reduce_sum(\n                sparse_csr_matrix_ops.sparse_matrix_nnz(sparse_matrices))\n            ratio = math_ops.cast(nnz, dtypes.float32) / np.prod(dense_shape)\n\n            with session.Session(\n                config=config_pb2.ConfigProto(\n                    intra_op_parallelism_threads=num_threads)) as sess:\n              nnz_value, ratio_value = self.evaluate((nnz, ratio))\n              device_str = \"cpu\" if device == CPU else \"gpu\"\n              name_template = (\n                  \"dense_to_sparse_matrix_%s_N_%d_batch_size_%d_num_threads_%d\")\n              self.run_op_benchmark(\n                  sess,\n                  sparse_matrices.op,\n                  name=name_template %\n                  (device_str, dense_shape[-1], batch_size, num_threads),\n                  extras={\n                      \"percentage_nonzero\": ratio_value,\n                      \"num_nonzero\": nnz_value,\n                  },\n                  min_iters=50)\n              name_template = (\n                  \"sparse_matrix_to_dense_%s_N_%d_batch_size_%d_num_threads_%d\")\n              self.run_op_benchmark(\n                  sess,\n                  dense_matrices.op,\n                  name=name_template %\n                  (device_str, dense_shape[-1], batch_size, num_threads),\n                  extras={\n                      \"percentage_nonzero\": ratio_value,\n                      \"num_nonzero\": nnz_value,\n                  },\n                  min_iters=50)\n\n  def benchmark_sparse_cholesky(self):\n    # TODO(anudhyan): Use conversions from SparseTensor instead of to get this\n    # benchmark working for larger matrices. For this to work without GPU, we\n    # need to write CPU kernels for SparseTensor conversions.\n    num_rows = 500\n    density = 0.01\n    # pylint: disable=g-long-lambda\n    sparsify = lambda m: array_ops.where(m > 1. - density, m,\n                                         array_ops.zeros_like(m))\n    # pylint: enable=g-long-lambda\n\n    for batch_size in [1, 16]:\n      for num_threads in [1, 4, 12]:\n        dense_shape = [batch_size, num_rows, num_rows]\n\n        with ops.Graph().as_default(), ops.device(CPU):\n          # Create a \"random\" SPD matrix, by choosing each entry of A between\n          # 0 and 1 at the specified density, and computing 0.5(A + At) + n*I.\n          # This ensures diagonal dominance which implies positive-definiteness.\n          dense_matrix = sparsify(\n              random_ops.random_uniform(dense_shape, dtype=dtypes.float32))\n          spd_dense_matrix = (\n              0.5 *\n              (dense_matrix + array_ops.transpose(dense_matrix, perm=[0, 2, 1]))\n              + num_rows *\n              linalg_ops.eye(dense_shape[-1], batch_shape=[batch_size]))\n\n          # Convert to SparseMatrix and invoke Sparse Cholesky factorization\n          # with AMD Ordering.\n          sparse_matrix = dense_to_csr_sparse_matrix(spd_dense_matrix)\n          ordering_amd = sparse_csr_matrix_ops.sparse_matrix_ordering_amd(\n              sparse_matrix)\n          cholesky_sparse_matrix = (\n              sparse_csr_matrix_ops.sparse_matrix_sparse_cholesky(\n                  sparse_matrix, ordering_amd, type=dtypes.float32))\n\n          nnz = math_ops.reduce_sum(\n              sparse_csr_matrix_ops.sparse_matrix_nnz(sparse_matrix))\n          ratio = math_ops.cast(nnz, dtypes.float32) / np.prod(dense_shape)\n          ordering_amd_name_template = (\n              \"sparse_matrix_ordering_amd_cpu_N_%d_batch_size_%d_threads_%d\")\n          sparse_cholesky_name_template = (\n              \"sparse_matrix_sparse_cholesky_cpu_N_%d_batch_size_%d_threads_%d\")\n          with session.Session(\n              config=config_pb2.ConfigProto(\n                  intra_op_parallelism_threads=num_threads)) as sess:\n            nnz_value, ratio_value = self.evaluate((nnz, ratio))\n            self.run_op_benchmark(\n                sess,\n                ordering_amd.op,\n                name=ordering_amd_name_template %\n                (dense_shape[-1], batch_size, num_threads),\n                extras={\n                    \"percentage_nonzero\": ratio_value,\n                    \"num_nonzero\": nnz_value\n                },\n                min_iters=25)\n            self.run_op_benchmark(\n                sess,\n                cholesky_sparse_matrix.op,\n                name=sparse_cholesky_name_template %\n                (dense_shape[-1], batch_size, num_threads),\n                extras={\n                    \"percentage_nonzero\": ratio_value,\n                    \"num_nonzero\": nnz_value\n                },\n                min_iters=25)\n\n\nif __name__ == \"__main__\":\n  test.main()\n"], "filenames": ["tensorflow/core/kernels/sparse/sparse_matrix.h", "tensorflow/python/kernel_tests/linalg/sparse/csr_sparse_matrix_ops_test.py"], "buggy_code_start_loc": [27, 1315], "buggy_code_end_loc": [635, 1315], "fixing_code_start_loc": [28, 1316], "fixing_code_end_loc": [643, 1326], "type": "CWE-20", "message": "TensorFlow is an open source platform for machine learning. An input `sparse_matrix` that is not a matrix with a shape with rank 0 will trigger a `CHECK` fail in `tf.raw_ops.SparseMatrixNNZ`. We have patched the issue in GitHub commit f856d02e5322821aad155dad9b3acab1e9f5d693. The fix will be included in TensorFlow 2.11. We will also cherrypick this commit on TensorFlow 2.10.1, 2.9.3, and TensorFlow 2.8.4, as these are also affected and still in supported range.", "other": {"cve": {"id": "CVE-2022-41901", "sourceIdentifier": "security-advisories@github.com", "published": "2022-11-18T22:15:20.907", "lastModified": "2022-11-23T13:41:02.837", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "TensorFlow is an open source platform for machine learning. An input `sparse_matrix` that is not a matrix with a shape with rank 0 will trigger a `CHECK` fail in `tf.raw_ops.SparseMatrixNNZ`. We have patched the issue in GitHub commit f856d02e5322821aad155dad9b3acab1e9f5d693. The fix will be included in TensorFlow 2.11. We will also cherrypick this commit on TensorFlow 2.10.1, 2.9.3, and TensorFlow 2.8.4, as these are also affected and still in supported range."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 7.5, "baseSeverity": "HIGH"}, "exploitabilityScore": 3.9, "impactScore": 3.6}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:H/PR:L/UI:R/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "HIGH", "privilegesRequired": "LOW", "userInteraction": "REQUIRED", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 4.8, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.2, "impactScore": 3.6}]}, "weaknesses": [{"source": "security-advisories@github.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-20"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionEndExcluding": "2.8.4", "matchCriteriaId": "A694EEE1-BFB9-4E6C-B275-02DC2731961C"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.9.0", "versionEndExcluding": "2.9.3", "matchCriteriaId": "9057B403-719C-4F10-BAB6-67F84786A89E"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.10.0", "versionEndExcluding": "2.10.1", "matchCriteriaId": "793BC396-7686-47FA-A107-DA6FC90704A2"}]}]}], "references": [{"url": "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/sparse/sparse_matrix.h", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/commit/f856d02e5322821aad155dad9b3acab1e9f5d693", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/security/advisories/GHSA-g9fm-r5mm-rf9f", "source": "security-advisories@github.com", "tags": ["Exploit", "Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/tensorflow/tensorflow/commit/f856d02e5322821aad155dad9b3acab1e9f5d693"}}