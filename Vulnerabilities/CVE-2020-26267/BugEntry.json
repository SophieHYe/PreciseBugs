{"buggy_code": ["/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n// See docs in ../ops/nn_ops.cc.\n\n#define EIGEN_USE_THREADS\n\n#include \"tensorflow/core/kernels/data_format_ops.h\"\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/register_types.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n\nnamespace tensorflow {\n\ntypedef Eigen::ThreadPoolDevice CPUDevice;\ntypedef Eigen::GpuDevice GPUDevice;\n\ntemplate <typename Device, typename T>\nclass DataFormatDimMapOp : public OpKernel {\n public:\n  explicit DataFormatDimMapOp(OpKernelConstruction* context)\n      : OpKernel(context) {\n    string src_format;\n    OP_REQUIRES_OK(context, context->GetAttr(\"src_format\", &src_format));\n    string dst_format;\n    OP_REQUIRES_OK(context, context->GetAttr(\"dst_format\", &dst_format));\n    OP_REQUIRES(context, src_format.size() == 4 || src_format.size() == 5,\n                errors::InvalidArgument(strings::StrCat(\n                    \"Source format must of length 4 or 5, received \"\n                    \"src_format = \",\n                    src_format)));\n    OP_REQUIRES(\n        context, dst_format.size() == 4 || dst_format.size() == 5,\n        errors::InvalidArgument(strings::StrCat(\n            \"Destination format must of length 4 or 5, received dst_format = \",\n            dst_format)));\n    dst_idx_ = Tensor(DT_INT32, {static_cast<int64>(src_format.size())});\n    for (int i = 0; i < src_format.size(); ++i) {\n      for (int j = 0; j < dst_format.size(); ++j) {\n        if (dst_format[j] == src_format[i]) {\n          dst_idx_.vec<int>()(i) = j;\n          break;\n        }\n      }\n    }\n  }\n\n  void Compute(OpKernelContext* context) override {\n    const Tensor& input = context->input(0);\n    Tensor* output;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(0, input.shape(), &output));\n    functor::DataFormatDimMap<Device, T>()(context->eigen_device<Device>(),\n                                           input.flat<T>(), output->flat<T>(),\n                                           dst_idx_.vec<int>());\n  }\n\n  Tensor dst_idx_;\n};\n\ntemplate <typename Device, typename T>\nclass DataFormatVecPermuteOp : public OpKernel {\n public:\n  explicit DataFormatVecPermuteOp(OpKernelConstruction* context)\n      : OpKernel(context) {\n    string src_format;\n    OP_REQUIRES_OK(context, context->GetAttr(\"src_format\", &src_format));\n    string dst_format;\n    OP_REQUIRES_OK(context, context->GetAttr(\"dst_format\", &dst_format));\n    src_format_ = src_format;\n    dst_format_ = dst_format;\n  }\n\n  void Compute(OpKernelContext* context) override {\n    const Tensor& input = context->input(0);\n    OP_REQUIRES(context, input.dims() == 1 || input.dims() == 2,\n                errors::InvalidArgument(\n                    \"input must be a vector or 2D tensor, but got shape \",\n                    input.shape().DebugString()));\n    if (input.dims() == 1) {\n      OP_REQUIRES(context,\n                  input.NumElements() == 2 || input.NumElements() == 4 ||\n                      input.NumElements() == 5,\n                  errors::InvalidArgument(\n                      \"1D input must be of size 2, 4 or 5, but got shape \",\n                      input.shape().DebugString()));\n    } else if (input.dims() == 2) {\n      OP_REQUIRES(context, input.dim_size(0) == 2 || input.dim_size(0) == 4,\n                  errors::InvalidArgument(\"First dimension of 2D input must be \"\n                                          \"of size 2 or 4, but got shape \",\n                                          input.shape().DebugString()));\n      OP_REQUIRES(\n          context, input.dim_size(1) == 2,\n          errors::InvalidArgument(\n              \"Second dimension of 2D input must be of size 2, but got shape \",\n              input.shape().DebugString()));\n    }\n\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(0, input.shape(), &output));\n    // Support 1D and 2D cases.\n    Eigen::DSizes<Eigen::DenseIndex, 8> dst_idx;\n    string src_format_str = src_format_;\n    string dst_format_str = dst_format_;\n    if (input.dim_size(0) == 2) {\n      // If the input is a vector of size 2, treat the two elements as spatial\n      // dimensions.\n      auto keep_only_spatial_dimensions = [](string* format_str) -> void {\n        auto new_end = std::remove_if(\n            format_str->begin(), format_str->end(),\n            [](const char dim) { return dim != 'H' && dim != 'W'; });\n        format_str->erase(new_end, format_str->end());\n      };\n      keep_only_spatial_dimensions(&src_format_str);\n      keep_only_spatial_dimensions(&dst_format_str);\n    }\n    ComputeDstIndex(src_format_str, dst_format_str, input.dims(), &dst_idx);\n\n    functor::DataFormatVecPermute<Device, T>()(context->eigen_device<Device>(),\n                                               input.flat<T>(),\n                                               output->flat<T>(), dst_idx);\n  }\n\n private:\n  // Finds out the destination index. Support 1D and 2D cases.\n  // Example: HWNC --> NHWC\n  // 1D: dst = [1, 2, 0, 3],\n  // 2D: dst = [2, 3, 4, 5, 0, 1, 6, 7]\n  static void ComputeDstIndex(const string& src_format_str,\n                              const string& dst_format_str, int num_dim,\n                              Eigen::DSizes<Eigen::DenseIndex, 8>* dst) {\n    for (int i = 0; i < src_format_str.size(); ++i) {\n      for (int j = 0; j < dst_format_str.size(); ++j) {\n        if (dst_format_str[j] != src_format_str[i]) continue;\n        // Found the dst index. Set output based on the number of dims.\n        for (int k = 0; k < num_dim; ++k) {\n          (*dst)[i * num_dim + k] = j * num_dim + k;\n        }\n      }\n    }\n  }\n\n  string src_format_;\n  string dst_format_;\n};\n\n#define REGISTER_KERNEL(T)                                                \\\n  REGISTER_KERNEL_BUILDER(                                                \\\n      Name(\"DataFormatDimMap\").Device(DEVICE_CPU).TypeConstraint<T>(\"T\"), \\\n      DataFormatDimMapOp<CPUDevice, T>);\nTF_CALL_int32(REGISTER_KERNEL);\nTF_CALL_int64(REGISTER_KERNEL);\n#undef REGISTER_KERNEL\n\n#define REGISTER_KERNEL(T)                                                    \\\n  REGISTER_KERNEL_BUILDER(                                                    \\\n      Name(\"DataFormatVecPermute\").Device(DEVICE_CPU).TypeConstraint<T>(\"T\"), \\\n      DataFormatVecPermuteOp<CPUDevice, T>);\nTF_CALL_int32(REGISTER_KERNEL);\nTF_CALL_int64(REGISTER_KERNEL);\n#undef REGISTER_KERNEL\n\n#define REGISTER_KERNEL(T)                             \\\n  REGISTER_KERNEL_BUILDER(Name(\"DataFormatDimMap\")     \\\n                              .Device(DEVICE_CPU)      \\\n                              .Label(\"host\")           \\\n                              .TypeConstraint<T>(\"T\"), \\\n                          DataFormatDimMapOp<CPUDevice, T>);\nTF_CALL_int32(REGISTER_KERNEL);\nTF_CALL_int64(REGISTER_KERNEL);\n#undef REGISTER_KERNEL\n\n#define REGISTER_KERNEL(T)                             \\\n  REGISTER_KERNEL_BUILDER(Name(\"DataFormatVecPermute\") \\\n                              .Device(DEVICE_CPU)      \\\n                              .Label(\"host\")           \\\n                              .TypeConstraint<T>(\"T\"), \\\n                          DataFormatVecPermuteOp<CPUDevice, T>);\nTF_CALL_int32(REGISTER_KERNEL);\nTF_CALL_int64(REGISTER_KERNEL);\n#undef REGISTER_KERNEL\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n// Forward declarations of the functor specializations for GPU.\nnamespace functor {\n#define DECLARE_GPU_SPEC(T)                                    \\\n  template <>                                                  \\\n  void DataFormatDimMap<GPUDevice, T>::operator()(             \\\n      const GPUDevice& d, typename TTypes<T>::ConstFlat x,     \\\n      typename TTypes<T>::Flat y, const TTypes<int>::Vec dst); \\\n  extern template struct DataFormatDimMap<GPUDevice, T>;\n#define DECLARE_GPU_SPECS(T) DECLARE_GPU_SPEC(T);\nTF_CALL_int32(DECLARE_GPU_SPECS);\nTF_CALL_int64(DECLARE_GPU_SPECS);\n#undef DECLARE_GPU_SPEC\n\n#define DECLARE_GPU_SPEC(T)                                \\\n  template <>                                              \\\n  void DataFormatVecPermute<GPUDevice, T>::operator()(     \\\n      const GPUDevice& d, typename TTypes<T>::ConstFlat x, \\\n      typename TTypes<T>::Vec y,                           \\\n      const Eigen::DSizes<Eigen::DenseIndex, 8>& dst_idx); \\\n  extern template struct DataFormatVecPermute<GPUDevice, T>;\n#define DECLARE_GPU_SPECS(T) DECLARE_GPU_SPEC(T);\nTF_CALL_int32(DECLARE_GPU_SPECS);\nTF_CALL_int64(DECLARE_GPU_SPECS);\n#undef DECLARE_GPU_SPEC\n}  // namespace functor\n\n// Registration of the GPU implementations.\n#define REGISTER_GPU_KERNEL(T)                                            \\\n  REGISTER_KERNEL_BUILDER(                                                \\\n      Name(\"DataFormatDimMap\").Device(DEVICE_GPU).TypeConstraint<T>(\"T\"), \\\n      DataFormatDimMapOp<GPUDevice, T>);                                  \\\n  REGISTER_KERNEL_BUILDER(Name(\"DataFormatDimMap\")                        \\\n                              .Device(DEVICE_GPU)                         \\\n                              .HostMemory(\"x\")                            \\\n                              .HostMemory(\"y\")                            \\\n                              .Label(\"host\")                              \\\n                              .TypeConstraint<T>(\"T\"),                    \\\n                          DataFormatDimMapOp<CPUDevice, T>);\nTF_CALL_int32(REGISTER_GPU_KERNEL);\nTF_CALL_int64(REGISTER_GPU_KERNEL);\n#undef REGISTER_GPU_KERNEL\n\n#define REGISTER_GPU_KERNEL(T)                                                \\\n  REGISTER_KERNEL_BUILDER(                                                    \\\n      Name(\"DataFormatVecPermute\").Device(DEVICE_GPU).TypeConstraint<T>(\"T\"), \\\n      DataFormatVecPermuteOp<GPUDevice, T>);                                  \\\n  REGISTER_KERNEL_BUILDER(Name(\"DataFormatVecPermute\")                        \\\n                              .Device(DEVICE_GPU)                             \\\n                              .HostMemory(\"x\")                                \\\n                              .HostMemory(\"y\")                                \\\n                              .Label(\"host\")                                  \\\n                              .TypeConstraint<T>(\"T\"),                        \\\n                          DataFormatVecPermuteOp<CPUDevice, T>);\nTF_CALL_int32(REGISTER_GPU_KERNEL);\nTF_CALL_int64(REGISTER_GPU_KERNEL);\n#undef REGISTER_GPU_KERNEL\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n}  // namespace tensorflow\n", "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for miscellaneous functionality in tensorflow.ops.nn.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\n\nfrom absl.testing import parameterized\nimport numpy as np\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\n\nfrom tensorflow.python.eager import def_function\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import tensor_spec\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import gradient_checker\nfrom tensorflow.python.ops import gradient_checker_v2\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import nn\nfrom tensorflow.python.ops import nn_impl\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.ops import partitioned_variables\nfrom tensorflow.python.ops import variable_scope\nfrom tensorflow.python.ops import variables\nimport tensorflow.python.ops.nn_grad  # pylint: disable=unused-import\nfrom tensorflow.python.ops.nn_impl import _compute_sampled_logits\nfrom tensorflow.python.ops.ragged import ragged_factory_ops\nfrom tensorflow.python.platform import test as test_lib\n\n\nclass ZeroFractionTest(test_lib.TestCase):\n\n  def _ZeroFraction(self, x):\n    assert x.shape\n    total_elements = np.prod(x.shape)\n    nonzeros = np.count_nonzero(x.flatten())\n    return 1.0 - nonzeros / total_elements\n\n  @test_util.run_deprecated_v1\n  def testZeroFraction(self):\n    x_shape = [5, 17]\n    x_np = np.random.randint(0, 2, size=x_shape).astype(np.float32)\n    y_np = self._ZeroFraction(x_np)\n\n    x_tf = constant_op.constant(x_np)\n    x_tf.set_shape(x_shape)\n    y_tf = nn_impl.zero_fraction(x_tf)\n    y_tf_np = self.evaluate(y_tf)\n\n    eps = 1e-8\n    self.assertAllClose(y_tf_np, y_np, eps)\n\n  @test_util.run_deprecated_v1\n  def testZeroFractionEmpty(self):\n    x = np.zeros(0)\n    y = self.evaluate(nn_impl.zero_fraction(x))\n    self.assertTrue(np.isnan(y))\n\n  @test_util.run_deprecated_v1\n  def testZeroFraction2_27Zeros(self):\n    sparsity = nn_impl.zero_fraction(\n        array_ops.zeros([int(2**27 * 1.01)], dtype=dtypes.int8))\n    self.assertAllClose(1.0, self.evaluate(sparsity))\n\n  @test_util.run_deprecated_v1\n  def testZeroFraction2_27Ones(self):\n    sparsity = nn_impl.zero_fraction(\n        array_ops.ones([int(2**27 * 1.01)], dtype=dtypes.int8))\n    self.assertAllClose(0.0, self.evaluate(sparsity))\n\n  @test_util.run_deprecated_v1\n  def testUnknownSize(self):\n    value = array_ops.placeholder(dtype=dtypes.float32)\n    sparsity = nn_impl.zero_fraction(value)\n    with self.cached_session() as sess:\n      self.assertAllClose(\n          0.25,\n          sess.run(sparsity, {value: [[0., 1.], [0.3, 2.]]}))\n\n\nclass SoftmaxTest(test_lib.TestCase, parameterized.TestCase):\n\n  def _softmax(self, x):\n    assert len(x.shape) == 2\n    m = x.max(1)[:, np.newaxis]\n    u = np.exp(x - m)\n    z = u.sum(1)[:, np.newaxis]\n    return u / z\n\n  @test_util.run_in_graph_and_eager_modes\n  def testSoftmax(self):\n    x_shape = [5, 10]\n    x_np = np.random.randn(*x_shape).astype(np.float32)\n    y_np = self._softmax(x_np)\n    x_tf = constant_op.constant(x_np)\n    y_tf = nn_ops.softmax_v2(x_tf)\n    y_tf_last_dim = nn_ops.softmax_v2(x_tf, 1)\n    y_tf_np = self.evaluate(y_tf)\n    y_tf_last_dim_np = self.evaluate(y_tf_last_dim)\n    eps = 1e-3\n    self.assertAllClose(y_tf_np, y_np, eps)\n    self.assertAllClose(y_tf_last_dim_np, y_np, eps)\n\n  def testSoftmaxAxes(self):\n    arr = np.linspace(0., 1, 12).reshape(3, 4)\n    x_neg_axis = nn_ops.softmax_v2(arr, axis=-2)\n    y_pos_axis = nn_ops.softmax_v2(arr, axis=0)\n    z_gt_axis = nn_ops.softmax_v2(arr, axis=0)\n    x_neg_axis_tf = self.evaluate(x_neg_axis)\n    y_pos_axis_tf = self.evaluate(y_pos_axis)\n    z_gt_axis_tf = self.evaluate(z_gt_axis)\n    eps = 1e-3\n    self.assertAllClose(x_neg_axis_tf, y_pos_axis_tf, eps)\n    self.assertAllClose(y_pos_axis_tf, z_gt_axis_tf, eps)\n\n  def testSoftmaxExtendType(self):\n    x_shape = [5, 10]\n    x_np = np.random.randn(*x_shape).astype(np.float32)\n\n    x_f32_tf = constant_op.constant(x_np)\n    x_bf16_tf = math_ops.cast(x_f32_tf, dtypes.bfloat16)\n    y_f32_tf = self.evaluate(nn_ops.softmax(x_f32_tf))\n    y_bf16_tf = self.evaluate(nn_ops.softmax(x_bf16_tf))\n    expected = math_ops.cast(y_f32_tf, dtypes.bfloat16)\n    tol = x_shape[1] * 1e-3\n    self.assertAllClose(y_bf16_tf, expected, rtol=tol, atol=tol)\n\n  @parameterized.parameters(((5, 10),), ((2, 3, 4),))\n  @test_util.run_deprecated_v1\n  def testGradient(self, x_shape):\n    x_np = np.random.randn(*x_shape).astype(np.float64)\n    with self.cached_session():\n      x_tf = constant_op.constant(x_np)\n      y_tf = nn_ops.softmax_v2(x_tf)\n      err = gradient_checker.compute_gradient_error(x_tf, x_shape, y_tf,\n                                                    x_shape)\n    eps = 2e-8\n    self.assertLess(err, eps)\n\n\nclass LogPoissonLossTest(test_lib.TestCase):\n\n  def _log_poisson_loss(self, x, z, compute_full_loss=False):\n    lpl = np.exp(x) - z * x\n    if compute_full_loss:\n      stirling_approx = z * np.log(z) - z + 0.5 * np.log(2. * np.pi * z)\n      lpl += np.ma.masked_array(stirling_approx, mask=(z <= 1)).filled(0.)\n    return lpl\n\n  @test_util.run_in_graph_and_eager_modes\n  def testLogPoissonLoss(self):\n    x_shape = [5, 10]\n    x_np = np.random.randn(*x_shape).astype(np.float32)\n    z_np = np.random.randint(0, 5, size=x_shape).astype(np.float32)\n    y_np = self._log_poisson_loss(x_np, z_np, compute_full_loss=False)\n    y_np_stirling = self._log_poisson_loss(x_np, z_np, compute_full_loss=True)\n    y_tf = nn_impl.log_poisson_loss(z_np, x_np, compute_full_loss=False)\n    y_tf_stirling = nn_impl.log_poisson_loss(z_np, x_np, compute_full_loss=True)\n    y_tf_np = self.evaluate(y_tf)\n    y_tf_np_stirling = self.evaluate(y_tf_stirling)\n    eps = 1e-3\n    self.assertAllClose(y_tf_np, y_np, eps)\n    self.assertAllClose(y_tf_np_stirling, y_np_stirling, eps)\n\n  @test_util.run_deprecated_v1\n  def testGradient(self):\n    x_shape = [5, 10]\n    x_np = np.random.randn(*x_shape).astype(np.float64)\n    z_np = np.random.randint(0, 5, size=x_shape).astype(np.float64)\n    with self.cached_session():\n      x_tf = constant_op.constant(x_np)\n      y_tf = nn_impl.log_poisson_loss(z_np, x_tf, compute_full_loss=False)\n      y_tf_stirling = nn_impl.log_poisson_loss(\n          z_np, x_tf, compute_full_loss=True)\n      err = gradient_checker.compute_gradient_error(x_tf, x_shape, y_tf,\n                                                    x_shape)\n      err_stirling = gradient_checker.compute_gradient_error(\n          x_tf, x_shape, y_tf_stirling, x_shape)\n    eps = 1e-6\n    self.assertLess(err, eps)\n    self.assertLess(err_stirling, eps)\n\n\nclass LogSoftmaxTest(test_lib.TestCase, parameterized.TestCase):\n\n  def _log_softmax(self, x):\n    assert len(x.shape) == 2\n    m = x.max(1)[:, np.newaxis]\n    u = x - m\n    return u - np.log(np.sum(np.exp(u), 1, keepdims=True))\n\n  @test_util.run_in_graph_and_eager_modes\n  def testLogSoftmax(self):\n    x_shape = [5, 10]\n    x_np = np.random.randn(*x_shape).astype(np.float32)\n    y_np = self._log_softmax(x_np)\n    x_tf = constant_op.constant(x_np)\n    y_tf = nn_ops.log_softmax_v2(x_tf)\n    y_tf_np = self.evaluate(y_tf)\n    eps = 1e-3\n    self.assertAllClose(y_tf_np, y_np, eps)\n\n  def testLogSoftmaxAxes(self):\n    arr = np.linspace(0., 1, 12).reshape(3, 4)\n    x_neg_axis = nn_ops.log_softmax_v2(arr, axis=-2)\n    y_pos_axis = nn_ops.log_softmax_v2(arr, axis=0)\n    z_gt_axis = nn_ops.log_softmax_v2(arr, axis=0)\n    x_neg_axis_tf = self.evaluate(x_neg_axis)\n    y_pos_axis_tf = self.evaluate(y_pos_axis)\n    z_gt_axis_tf = self.evaluate(z_gt_axis)\n    eps = 1e-3\n    self.assertAllClose(x_neg_axis_tf, y_pos_axis_tf, eps)\n    self.assertAllClose(y_pos_axis_tf, z_gt_axis_tf, eps)\n\n  @parameterized.parameters(((5, 10),), ((2, 3, 4),))\n  @test_util.run_deprecated_v1\n  def testGradient(self, x_shape):\n    x_np = np.random.randn(*x_shape).astype(np.float64)\n    with self.cached_session():\n      x_tf = constant_op.constant(x_np)\n      y_tf = nn_ops.log_softmax_v2(x_tf)\n      err = gradient_checker.compute_gradient_error(x_tf, x_shape, y_tf,\n                                                    x_shape)\n    eps = 1e-7\n    self.assertLess(err, eps)\n\n\nclass L2LossTest(test_lib.TestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def testL2Loss(self):\n    for dtype in [dtypes.float32, dtypes.float64]:\n      x = constant_op.constant(\n          [1.0, 0.0, 3.0, 2.0], shape=[2, 2], name=\"x\", dtype=dtype)\n      l2loss = nn_ops.l2_loss(x)\n      value = self.evaluate(l2loss)\n      self.assertAllClose(7.0, value)\n\n  @test_util.run_deprecated_v1\n  def testGradient(self):\n    x_shape = [20, 7, 3]\n    np.random.seed(1)  # Make it reproducible.\n    x_val = np.random.random_sample(x_shape).astype(np.float64)\n    with self.cached_session():\n      x = constant_op.constant(x_val, name=\"x\")\n      output = nn_ops.l2_loss(x)\n      err = gradient_checker.compute_gradient_error(x, x_shape, output, [1])\n    print(\"L2Loss gradient err = %g \" % err)\n    err_tolerance = 1e-10\n    self.assertLess(err, err_tolerance)\n\n\nclass L2NormalizeTest(test_lib.TestCase):\n\n  def _l2Normalize(self, x, dim):\n    if isinstance(dim, list):\n      norm = np.linalg.norm(x, axis=tuple(dim))\n      for d in dim:\n        norm = np.expand_dims(norm, d)\n      return x / norm\n    else:\n      norm = np.apply_along_axis(np.linalg.norm, dim, x)\n      return x / np.expand_dims(norm, dim)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testL2Normalize(self):\n    x_shape = [20, 7, 3]\n    np.random.seed(1)\n    x_np = np.random.random_sample(x_shape).astype(np.float32)\n    for dim in range(len(x_shape)):\n      y_np = self._l2Normalize(x_np, dim)\n      x_tf = constant_op.constant(x_np, name=\"x\")\n      y_tf = nn_impl.l2_normalize_v2(x_tf, dim)\n      self.assertAllClose(y_np, self.evaluate(y_tf))\n\n  @test_util.run_in_graph_and_eager_modes\n  def testL2NormalizeDimArray(self):\n    x_shape = [20, 7, 3]\n    np.random.seed(1)\n    x_np = np.random.random_sample(x_shape).astype(np.float32)\n    dim = [1, 2]\n    y_np = self._l2Normalize(x_np, dim)\n    x_tf = constant_op.constant(x_np, name=\"x\")\n    y_tf = nn_impl.l2_normalize_v2(x_tf, dim)\n    self.assertAllClose(y_np, self.evaluate(y_tf))\n\n  @test_util.run_deprecated_v1\n  def testL2NormalizeGradient(self):\n    x_shape = [20, 7, 3]\n    np.random.seed(1)\n    x_np = np.random.random_sample(x_shape).astype(np.float64)\n    for dim in range(len(x_shape)):\n      with self.cached_session():\n        x_tf = constant_op.constant(x_np, name=\"x\")\n        y_tf = nn_impl.l2_normalize_v2(x_tf, dim)\n        err = gradient_checker.compute_gradient_error(x_tf, x_shape, y_tf,\n                                                      x_shape)\n      print(\"L2Normalize gradient err = %g \" % err)\n      self.assertLess(err, 1e-4)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testL2NormalizeComplex(self):\n    x_shape = [20, 7, 3]\n    for dtype in [np.complex64, np.complex128]:\n      np.random.seed(1)\n      x_np = (\n          np.random.random_sample(x_shape).astype(dtype) +\n          np.random.random_sample(x_shape).astype(dtype) * 1j)\n      for dim in range(len(x_shape)):\n        y_np = self._l2Normalize(x_np, dim)\n        x_tf = constant_op.constant(x_np, name=\"x\")\n        y_tf = nn_impl.l2_normalize_v2(x_tf, dim)\n        self.assertAllClose(y_np, self.evaluate(y_tf))\n\n\nclass DropoutTest(test_lib.TestCase):\n\n  def testDropout(self):\n    # Runs dropout with 0-1 tensor 10 times, sum the number of ones and validate\n    # that it is producing approximately the right number of ones over a large\n    # number of samples, based on the keep probability.\n    x_dim = 40\n    y_dim = 30\n    num_iter = 10\n    for keep_prob in [0.1, 0.5, 0.8]:\n      t = constant_op.constant(1.0, shape=[x_dim, y_dim], dtype=dtypes.float32)\n      dropout = nn_ops.dropout(t, rate=(1 - keep_prob))\n      final_count = 0\n      self.assertEqual([x_dim, y_dim], dropout.get_shape())\n      for _ in xrange(0, num_iter):\n        value = self.evaluate(dropout)\n        final_count += np.count_nonzero(value)\n        # Verifies that there are only two values: 0 and 1/keep_prob.\n        sorted_value = np.unique(np.sort(value))\n        self.assertEqual(0, sorted_value[0])\n        self.assertAllClose(1 / keep_prob, sorted_value[1])\n\n      # Check that we are in the 15% error range\n      expected_count = x_dim * y_dim * keep_prob * num_iter\n      rel_error = math.fabs(final_count - expected_count) / expected_count\n      print(rel_error)\n      self.assertTrue(rel_error < 0.15)\n\n  def testShapedDropout(self):\n    # Runs dropout with 0-1 tensor 10 times, sum the number of ones and validate\n    # that it is producing approximately the right number of ones over a large\n    # number of samples, based on the keep probability. This time with shaped\n    # noise.\n    x_dim = 40 * 30\n    y_dim = 3\n    num_iter = 10\n    for keep_prob in [0.1, 0.5, 0.8]:\n      t = constant_op.constant(1.0, shape=[x_dim, y_dim], dtype=dtypes.float32)\n      dropout = nn_ops.dropout(t, rate=(1 - keep_prob), noise_shape=[x_dim, 1])\n      self.assertEqual([x_dim, y_dim], dropout.get_shape())\n      final_count = 0\n      for _ in xrange(0, num_iter):\n        value = self.evaluate(dropout)\n        final_count += np.count_nonzero(value)\n        # Verifies that there are only two values: 0 and 1/keep_prob.\n        sorted_value = np.unique(np.sort(value))\n        self.assertEqual(0, sorted_value[0])\n        self.assertAllClose(1 / keep_prob, sorted_value[1])\n\n      # Check that we are in the 15% error range\n      expected_count = x_dim * y_dim * keep_prob * num_iter\n      rel_error = math.fabs(final_count - expected_count) / expected_count\n      print(rel_error)\n      self.assertTrue(rel_error < 0.15)\n\n  def testShapedDropoutCorrelation(self):\n    # Runs a shaped dropout and tests that the correlations are correct.\n    x_dim = 40\n    y_dim = 30\n    num_iter = 10\n    for keep_prob in [0.1, 0.5, 0.8]:\n      t = constant_op.constant(1.0, shape=[x_dim, y_dim], dtype=dtypes.float32)\n      dropout = nn_ops.dropout(t, rate=(1 - keep_prob), noise_shape=[x_dim, 1])\n      self.assertEqual([x_dim, y_dim], dropout.get_shape())\n      for _ in xrange(0, num_iter):\n        value = self.evaluate(dropout)\n        # Verifies that each y column as only one type of activation.\n        for i in xrange(x_dim):\n          sorted_value = np.unique(np.sort(value[i, :]))\n          self.assertEqual(sorted_value.size, 1)\n\n  @test_util.run_deprecated_v1\n  def testDropoutPlaceholderKeepProb(self):\n    # Runs dropout with 0-1 tensor 10 times, sum the number of ones and validate\n    # that it is producing approximately the right number of ones over a large\n    # number of samples, based on the keep probability.\n    x_dim = 40\n    y_dim = 30\n    num_iter = 10\n    for keep_prob in [0.1, 0.5, 0.8]:\n      with self.cached_session():\n        t = constant_op.constant(\n            1.0, shape=[x_dim, y_dim], dtype=dtypes.float32)\n        keep_prob_placeholder = array_ops.placeholder(dtypes.float32)\n        dropout = nn_ops.dropout(t, keep_prob_placeholder)\n        final_count = 0\n        self.assertEqual([x_dim, y_dim], dropout.get_shape())\n        for _ in xrange(0, num_iter):\n          value = dropout.eval(feed_dict={keep_prob_placeholder: keep_prob})\n          final_count += np.count_nonzero(value)\n          # Verifies that there are only two values: 0 and 1/keep_prob.\n          sorted_value = np.unique(np.sort(value))\n          self.assertEqual(0, sorted_value[0])\n          self.assertAllClose(1 / keep_prob, sorted_value[1])\n      # Check that we are in the 15% error range\n      expected_count = x_dim * y_dim * keep_prob * num_iter\n      rel_error = math.fabs(final_count - expected_count) / expected_count\n      print(rel_error)\n      self.assertTrue(rel_error < 0.15)\n\n  @test_util.run_deprecated_v1\n  def testShapedDropoutUnknownShape(self):\n    x_dim = 40\n    y_dim = 30\n    keep_prob = 0.5\n    x = constant_op.constant(1.0, shape=[x_dim, y_dim], dtype=dtypes.float32)\n    dropout_x = nn_ops.dropout(\n        x,\n        rate=(1 - keep_prob),\n        noise_shape=array_ops.placeholder(dtypes.int32))\n    self.assertEqual(x.get_shape(), dropout_x.get_shape())\n\n  def testPartialShapedDropout(self):\n    x_dim = 40 * 30\n    y_dim = 3\n    num_iter = 10\n    for keep_prob in [0.1, 0.5, 0.8]:\n      t = constant_op.constant(1.0, shape=[x_dim, y_dim], dtype=dtypes.float32)\n      # Set noise_shape=[None, 1] which means [x_dim, 1].\n      dropout = nn_ops.dropout(t, rate=(1 - keep_prob), noise_shape=[None, 1])\n      self.assertEqual([x_dim, y_dim], dropout.get_shape())\n      final_count = 0\n      for _ in xrange(0, num_iter):\n        value = self.evaluate(dropout)\n        final_count += np.count_nonzero(value)\n        # Verifies that there are only two values: 0 and 1/keep_prob.\n        sorted_value = np.unique(np.sort(value))\n        self.assertEqual(0, sorted_value[0])\n        self.assertAllClose(1 / keep_prob, sorted_value[1])\n\n      # Check that we are in the 15% error range\n      expected_count = x_dim * y_dim * keep_prob * num_iter\n      rel_error = math.fabs(final_count - expected_count) / expected_count\n      print(rel_error)\n      self.assertTrue(rel_error < 0.15)\n\n  @test_util.run_deprecated_v1\n  def testInvalidKeepProb(self):\n    x_dim = 40\n    y_dim = 30\n    t = constant_op.constant(1.0, shape=[x_dim, y_dim], dtype=dtypes.float32)\n    with self.assertRaises(ValueError):\n      nn_ops.dropout(t, -1.0)\n    with self.assertRaises(ValueError):\n      nn_ops.dropout(t, 1.1)\n    with self.assertRaises(ValueError):\n      nn_ops.dropout(t, [0.0, 1.0])\n    with self.assertRaises(ValueError):\n      nn_ops.dropout(t, array_ops.placeholder(dtypes.float64))\n    with self.assertRaises(ValueError):\n      nn_ops.dropout(t, array_ops.placeholder(dtypes.float32, shape=[2]))\n\n  @test_util.run_deprecated_v1\n  def testInvalidRate(self):\n    x_dim = 40\n    y_dim = 30\n    t = constant_op.constant(1.0, shape=[x_dim, y_dim], dtype=dtypes.float32)\n    with self.assertRaises(ValueError):\n      nn_ops.dropout_v2(t, -1.0)\n    with self.assertRaises(ValueError):\n      nn_ops.dropout_v2(t, 1.1)\n    with self.assertRaises(ValueError):\n      nn_ops.dropout_v2(t, [0.0, 1.0])\n\n  def testLargeRate(self):\n    x_dim = 40\n    y_dim = 30\n    t = constant_op.constant(1.0, shape=[x_dim, y_dim], dtype=dtypes.float32)\n    _ = nn_ops.dropout_v2(t, 0.9)\n\n  def testVariableRef(self):\n    x = variable_scope.get_variable(\"x\", shape=[10, 10], dtype=dtypes.float32)\n    _ = nn_ops.dropout(x, keep_prob=0.1)\n\n  @test_util.run_deprecated_v1\n  def testShapedDropoutShapeError(self):\n    # Runs shaped dropout and verifies an error is thrown on misshapen noise.\n    x_dim = 40\n    y_dim = 30\n    keep_prob = 0.5\n    t = constant_op.constant(1.0, shape=[x_dim, y_dim], dtype=dtypes.float32)\n    with self.assertRaises(ValueError):\n      _ = nn_ops.dropout(\n          t, rate=(1 - keep_prob), noise_shape=[x_dim, y_dim + 10])\n    with self.assertRaises(ValueError):\n      _ = nn_ops.dropout(t, rate=(1 - keep_prob), noise_shape=[x_dim, y_dim, 5])\n    with self.assertRaises(ValueError):\n      _ = nn_ops.dropout(t, rate=(1 - keep_prob), noise_shape=[x_dim + 3])\n    with self.assertRaises(ValueError):\n      _ = nn_ops.dropout(t, rate=(1 - keep_prob), noise_shape=[x_dim])\n    # test that broadcasting proceeds\n    _ = nn_ops.dropout(t, rate=(1 - keep_prob), noise_shape=[y_dim])\n    _ = nn_ops.dropout(t, rate=(1 - keep_prob), noise_shape=[1, y_dim])\n    _ = nn_ops.dropout(t, rate=(1 - keep_prob), noise_shape=[x_dim, 1])\n    _ = nn_ops.dropout(t, rate=(1 - keep_prob), noise_shape=[1, 1])\n\n  def testNoDropout(self):\n    x = array_ops.zeros((5,))\n    y = nn_ops.dropout(x, rate=0)\n    self.assertAllEqual(x, y)\n\n    y = nn_ops.dropout_v2(x, rate=0)\n    self.assertAllEqual(x, y)\n\n  def testDropoutWithIntegerInputs(self):\n    x = constant_op.constant([1, 1, 1, 1, 1])\n    with self.assertRaises(ValueError):\n      _ = nn_ops.dropout(x, 0.5)\n\n\n@test_util.run_all_without_tensor_float_32(\n    \"Tests _compute_sampled_logits and related functions, which call matmul\")\nclass ComputeSampledLogitsTest(test_lib.TestCase):\n\n  def setUp(self):\n    self._eps = 1e-3\n\n  def _GenerateTestData(self, num_classes, dim, batch_size, num_true, labels,\n                        sampled, subtract_log_q):\n    \"\"\"Randomly generates input/output data for a single test case.\n\n    This function returns numpy constants for use in a test case.\n\n    Args:\n      num_classes: An int. The number of embedding classes in the test case.\n      dim: An int. The dimension of the embedding.\n      batch_size: An int. The batch size.\n      num_true: An int. The number of target classes per training example.\n      labels: A list of batch_size * num_true ints. The target classes.\n      sampled: A list of indices in [0, num_classes).\n      subtract_log_q: A bool corresponding to the parameter in\n          _compute_sampled_logits().\n\n    Returns:\n      weights: Embedding weights to use as test input. It is a numpy array\n          of shape [num_classes, dim]\n      biases: Embedding biases to use as test input. It is a numpy array\n          of shape [num_classes].\n      hidden_acts: Forward activations of the network to use as test input.\n          It is a numpy array of shape [batch_size, dim].\n      sampled_vals: A tuple based on `sampled` to use as test input in the\n          format returned by a *_candidate_sampler function.\n      exp_logits: The output logits expected from _compute_sampled_logits().\n          It is a numpy array of shape [batch_size, num_true + len(sampled)].\n      exp_labels: The output labels expected from _compute_sampled_logits().\n          It is a numpy array of shape [batch_size, num_true + len(sampled)].\n    \"\"\"\n    weights = np.random.randn(num_classes, dim).astype(np.float32)\n    biases = np.random.randn(num_classes).astype(np.float32)\n    hidden_acts = np.random.randn(batch_size, dim).astype(np.float32)\n\n    true_exp = np.full([batch_size, 1], fill_value=0.5, dtype=np.float32)\n    sampled_exp = np.full([len(sampled)], fill_value=0.5, dtype=np.float32)\n    sampled_vals = (sampled, true_exp, sampled_exp)\n\n    sampled_w, sampled_b = weights[sampled], biases[sampled]\n    true_w, true_b = weights[labels], biases[labels]\n\n    true_logits = np.sum(\n        hidden_acts.reshape((batch_size, 1, dim)) * true_w.reshape(\n            (batch_size, num_true, dim)),\n        axis=2)\n    true_b = true_b.reshape((batch_size, num_true))\n    true_logits += true_b\n    sampled_logits = np.dot(hidden_acts, sampled_w.T) + sampled_b\n\n    if subtract_log_q:\n      true_logits -= np.log(true_exp)\n      sampled_logits -= np.log(sampled_exp[np.newaxis, :])\n\n    exp_logits = np.concatenate([true_logits, sampled_logits], axis=1)\n    exp_labels = np.hstack((np.ones_like(true_logits) / num_true,\n                            np.zeros_like(sampled_logits)))\n\n    return weights, biases, hidden_acts, sampled_vals, exp_logits, exp_labels\n\n  def _ShardTestEmbeddings(self, weights, biases, num_shards):\n    \"\"\"Shards the weights and biases returned by _GenerateTestData.\n\n    Args:\n      weights: The weights returned by _GenerateTestData.\n      biases: The biases returned by _GenerateTestData.\n      num_shards: The number of shards to create.\n\n    Returns:\n      sharded_weights: A list of size `num_shards` containing all the weights.\n      sharded_biases: A list of size `num_shards` containing all the biases.\n    \"\"\"\n    with ops.Graph().as_default() as g:\n      sharded_weights = variable_scope.get_variable(\n          \"w\",\n          partitioner=partitioned_variables.fixed_size_partitioner(num_shards),\n          initializer=constant_op.constant(weights))\n      sharded_biases = variable_scope.get_variable(\n          \"b\",\n          partitioner=partitioned_variables.fixed_size_partitioner(num_shards),\n          initializer=constant_op.constant(biases))\n      with self.session(graph=g) as sess:\n        self.evaluate(variables.global_variables_initializer())\n        return self.evaluate([list(sharded_weights), list(sharded_biases)])\n\n  def testShapes(self):\n    np.random.seed(0)\n    num_classes = 5\n    batch_size = 3\n\n    for num_true in range(1, 5):\n      labels = np.random.randint(\n          low=0, high=num_classes, size=batch_size * num_true)\n      (weights, biases, hidden_acts, sampled_vals, exp_logits,\n       exp_labels) = self._GenerateTestData(\n           num_classes=num_classes,\n           dim=10,\n           batch_size=batch_size,\n           num_true=num_true,\n           labels=labels,\n           sampled=[1, 0, 2, 3],\n           subtract_log_q=False)\n      logits_tensor, labels_tensor = _compute_sampled_logits(\n          weights=constant_op.constant(weights),\n          biases=constant_op.constant(biases),\n          labels=constant_op.constant(\n              labels, dtype=dtypes.int64, shape=(batch_size, num_true)),\n          inputs=constant_op.constant(hidden_acts),\n          num_sampled=4,\n          num_classes=num_classes,\n          num_true=num_true,\n          sampled_values=sampled_vals,\n          subtract_log_q=False,\n          remove_accidental_hits=False,\n          partition_strategy=\"div\",\n          name=\"sampled_logits_basic_num_true_%d\" % num_true)\n      got_logits, got_labels = self.evaluate([logits_tensor, labels_tensor])\n      self.assertEqual(exp_logits.shape, got_logits.shape, self._eps)\n      self.assertEqual(exp_labels.shape, got_labels.shape, self._eps)\n\n  def testBasic(self):\n    \"\"\"Without accidental hit removal or subtract_log_q.\"\"\"\n    np.random.seed(0)\n    num_classes = 5\n    batch_size = 3\n\n    for num_true in range(1, 5):\n      labels = np.random.randint(\n          low=0, high=num_classes, size=batch_size * num_true)\n      (weights, biases, hidden_acts, sampled_vals, exp_logits,\n       exp_labels) = self._GenerateTestData(\n           num_classes=num_classes,\n           dim=10,\n           batch_size=batch_size,\n           num_true=num_true,\n           labels=labels,\n           sampled=[1, 0, 2, 3],\n           subtract_log_q=False)\n      logits_tensor, labels_tensor = _compute_sampled_logits(\n          weights=constant_op.constant(weights),\n          biases=constant_op.constant(biases),\n          labels=constant_op.constant(\n              labels, dtype=dtypes.int64, shape=(batch_size, num_true)),\n          inputs=constant_op.constant(hidden_acts),\n          num_sampled=4,\n          num_classes=num_classes,\n          num_true=num_true,\n          sampled_values=sampled_vals,\n          subtract_log_q=False,\n          remove_accidental_hits=False,\n          partition_strategy=\"div\",\n          name=\"sampled_logits_basic_num_true_%d\" % num_true)\n      got_logits, got_labels = self.evaluate([logits_tensor, labels_tensor])\n      self.assertAllClose(exp_logits, got_logits, self._eps)\n      self.assertAllClose(exp_labels, got_labels, self._eps)\n\n  def testAccidentalHitRemoval(self):\n    \"\"\"With accidental hit removal, no subtract_log_q.\"\"\"\n    np.random.seed(0)\n    num_classes = 5\n    batch_size = 3\n    sampled = [1, 0, 2, 3]\n\n    for num_true in range(1, 5):\n      labels = np.random.randint(\n          low=0, high=num_classes, size=batch_size * num_true)\n      (weights, biases, hidden_acts, sampled_vals, _,\n       _) = self._GenerateTestData(\n           num_classes=num_classes,\n           dim=10,\n           batch_size=batch_size,\n           num_true=num_true,\n           labels=labels,\n           sampled=sampled,\n           subtract_log_q=False)\n      logits_tensor, _ = _compute_sampled_logits(\n          weights=constant_op.constant(weights),\n          biases=constant_op.constant(biases),\n          labels=constant_op.constant(\n              labels, dtype=dtypes.int64, shape=(batch_size, num_true)),\n          inputs=constant_op.constant(hidden_acts),\n          num_sampled=len(sampled),\n          num_classes=num_classes,\n          num_true=num_true,\n          sampled_values=sampled_vals,\n          subtract_log_q=False,\n          remove_accidental_hits=True,\n          partition_strategy=\"div\",\n          name=\"sampled_logits_accidental_hit_removal_num_true_%d\" % num_true)\n      # Test that the exponentiated logits of accidental hits are near 0.\n      # First we need to find the hits in this random test run:\n      labels_reshape = labels.reshape((batch_size, num_true))\n      got_logits = self.evaluate(logits_tensor)\n      for row in xrange(batch_size):\n        row_labels = labels_reshape[row, :]\n        for col in xrange(len(sampled)):\n          if sampled[col] in row_labels:\n            # We need to add the num_true_test offset into logits_*\n            self.assertNear(\n                np.exp(got_logits[row, col + num_true]), 0., self._eps)\n\n  def testSubtractLogQ(self):\n    \"\"\"With subtract_log_q, no accidental hit removal.\"\"\"\n    np.random.seed(0)\n    num_classes = 5\n    batch_size = 3\n\n    for num_true in range(1, 5):\n      labels = np.random.randint(\n          low=0, high=num_classes, size=batch_size * num_true)\n      (weights, biases, hidden_acts, sampled_vals, exp_logits,\n       exp_labels) = self._GenerateTestData(\n           num_classes=num_classes,\n           dim=10,\n           batch_size=batch_size,\n           num_true=num_true,\n           labels=labels,\n           sampled=[1, 0, 2, 3],\n           subtract_log_q=True)\n      logits_tensor, labels_tensor = _compute_sampled_logits(\n          weights=constant_op.constant(weights),\n          biases=constant_op.constant(biases),\n          labels=constant_op.constant(\n              labels, dtype=dtypes.int64, shape=(batch_size, num_true)),\n          inputs=constant_op.constant(hidden_acts),\n          num_sampled=4,\n          num_classes=num_classes,\n          num_true=num_true,\n          sampled_values=sampled_vals,\n          subtract_log_q=True,\n          remove_accidental_hits=False,\n          partition_strategy=\"div\",\n          name=\"sampled_logits_subtract_log_q_num_true_%d\" % num_true)\n      got_logits, got_labels = self.evaluate([logits_tensor, labels_tensor])\n      self.assertAllClose(exp_logits, got_logits, self._eps)\n      self.assertAllClose(exp_labels, got_labels, self._eps)\n\n  def testSharded(self):\n    \"\"\"With sharded weights and sharded biases.\"\"\"\n    np.random.seed(0)\n    num_classes = 5\n    batch_size = 3\n\n    for num_true in range(1, 5):\n      labels = np.random.randint(\n          low=0, high=num_classes, size=batch_size * num_true)\n      (weights, biases, hidden_acts, sampled_vals, exp_logits,\n       exp_labels) = self._GenerateTestData(\n           num_classes=num_classes,\n           dim=10,\n           batch_size=batch_size,\n           num_true=num_true,\n           labels=labels,\n           sampled=[1, 0, 2, 3],\n           subtract_log_q=False)\n      weight_shards, bias_shards = self._ShardTestEmbeddings(\n          weights, biases, num_shards=3)\n      logits_tensor, labels_tensor = _compute_sampled_logits(\n          weights=[constant_op.constant(shard) for shard in weight_shards],\n          biases=[constant_op.constant(shard) for shard in bias_shards],\n          labels=constant_op.constant(\n              labels, dtype=dtypes.int64, shape=(batch_size, num_true)),\n          inputs=constant_op.constant(hidden_acts),\n          num_sampled=4,\n          num_classes=num_classes,\n          num_true=num_true,\n          sampled_values=sampled_vals,\n          subtract_log_q=False,\n          remove_accidental_hits=False,\n          partition_strategy=\"div\",\n          name=\"sampled_logits_sharded_num_true_%d\" % num_true)\n      got_logits, got_labels = self.evaluate([logits_tensor, labels_tensor])\n      self.assertAllClose(exp_logits, got_logits, self._eps)\n      self.assertAllClose(exp_labels, got_labels, self._eps)\n\n  def testNCELoss(self):\n    # A simple test to verify the numerics.\n\n    def _SigmoidCrossEntropyWithLogits(logits, targets):\n      # logits, targets: float arrays of the same shape.\n      assert logits.shape == targets.shape\n      pred = 1. / (1. + np.exp(-logits))\n      eps = 0.0001\n      pred = np.minimum(np.maximum(pred, eps), 1 - eps)\n      return -targets * np.log(pred) - (1. - targets) * np.log(1. - pred)\n\n    np.random.seed(0)\n    num_classes = 5\n    batch_size = 3\n    labels = [0, 1, 2]\n    (weights, biases, hidden_acts, sampled_vals, exp_logits,\n     exp_labels) = self._GenerateTestData(\n         num_classes=num_classes,\n         dim=10,\n         batch_size=batch_size,\n         num_true=1,\n         labels=labels,\n         sampled=[1, 0, 2, 3],\n         subtract_log_q=True)\n    exp_nce_loss = np.sum(\n        _SigmoidCrossEntropyWithLogits(exp_logits, exp_labels), 1)\n\n    got_nce_loss = nn_impl.nce_loss_v2(\n        weights=constant_op.constant(weights),\n        biases=constant_op.constant(biases),\n        labels=constant_op.constant(labels, shape=(batch_size, 1)),\n        inputs=constant_op.constant(hidden_acts),\n        num_sampled=4,\n        num_classes=num_classes,\n        num_true=1,\n        sampled_values=sampled_vals)\n\n    self.assertAllClose(exp_nce_loss, self.evaluate(got_nce_loss), 1e-4)\n\n    # Test with sharded weights and sharded biases.\n    weight_shards, bias_shards = self._ShardTestEmbeddings(\n        weights, biases, num_shards=3)\n    got_nce_loss = nn_impl.nce_loss_v2(\n        weights=[constant_op.constant(shard) for shard in weight_shards],\n        biases=[constant_op.constant(shard) for shard in bias_shards],\n        labels=constant_op.constant(labels, shape=(batch_size, 1)),\n        inputs=constant_op.constant(hidden_acts),\n        num_sampled=4,\n        num_classes=num_classes,\n        num_true=1,\n        sampled_values=sampled_vals)\n\n    self.assertAllClose(exp_nce_loss, self.evaluate(got_nce_loss), 1e-4)\n\n  def testSampledSoftmaxLoss(self):\n    # A simple test to verify the numerics.\n\n    def _SoftmaxCrossEntropyWithLogits(logits, targets):\n      # logits, targets: float arrays of the same shape.\n      assert logits.shape == targets.shape\n      stable_exp_logits = np.exp(\n          logits - np.amax(logits, axis=1, keepdims=True))\n      pred = stable_exp_logits / np.sum(stable_exp_logits, 1, keepdims=True)\n      return -np.sum(targets * np.log(pred + 1.0e-20), axis=1)\n\n    np.random.seed(0)\n    num_classes = 5\n    batch_size = 3\n    labels = [0, 1, 2]\n    (weights, biases, hidden_acts, sampled_vals, exp_logits,\n     exp_labels) = self._GenerateTestData(\n         num_classes=num_classes,\n         dim=10,\n         batch_size=batch_size,\n         num_true=1,\n         labels=labels,\n         sampled=[1, 0, 2, 3],\n         subtract_log_q=True)\n    exp_sampled_softmax_loss = _SoftmaxCrossEntropyWithLogits(\n        exp_logits, exp_labels)\n\n    got_sampled_softmax_loss = nn_impl.sampled_softmax_loss_v2(\n        weights=constant_op.constant(weights),\n        biases=constant_op.constant(biases),\n        labels=constant_op.constant(labels, shape=(batch_size, 1)),\n        inputs=constant_op.constant(hidden_acts),\n        num_sampled=4,\n        num_classes=num_classes,\n        num_true=1,\n        sampled_values=sampled_vals,\n        remove_accidental_hits=False)\n\n    self.assertAllClose(exp_sampled_softmax_loss,\n                        self.evaluate(got_sampled_softmax_loss), 1e-4)\n\n    # Test with sharded weights and sharded biases.\n    weight_shards, bias_shards = self._ShardTestEmbeddings(\n        weights, biases, num_shards=3)\n    got_sampled_softmax_loss = nn_impl.sampled_softmax_loss_v2(\n        weights=[constant_op.constant(shard) for shard in weight_shards],\n        biases=[constant_op.constant(shard) for shard in bias_shards],\n        labels=constant_op.constant(labels, shape=(batch_size, 1)),\n        inputs=constant_op.constant(hidden_acts),\n        num_sampled=4,\n        num_classes=num_classes,\n        num_true=1,\n        sampled_values=sampled_vals,\n        remove_accidental_hits=False)\n\n    self.assertAllClose(exp_sampled_softmax_loss,\n                        self.evaluate(got_sampled_softmax_loss), 1e-4)\n\n  def testSampledSoftmaxLossBf16(self):\n    # A simple test to verify the numerics for bfloat16.\n    def _SoftmaxCrossEntropyWithLogits(logits, targets):\n      # logits, targets: float arrays of the same shape.\n      assert logits.shape == targets.shape\n      stable_exp_logits = np.exp(\n          logits - np.amax(logits, axis=1, keepdims=True))\n      pred = stable_exp_logits / np.sum(stable_exp_logits, 1, keepdims=True)\n      return -np.sum(targets * np.log(pred + 1.0e-20), axis=1)\n\n    np.random.seed(0)\n    num_classes = 5\n    batch_size = 3\n    labels = [0, 1, 2]\n    sampled = [1, 0, 2, 3]\n    (weights, biases, hidden_acts, _, exp_logits,\n     exp_labels) = self._GenerateTestData(\n         num_classes=num_classes,\n         dim=10,\n         batch_size=batch_size,\n         num_true=1,\n         labels=labels,\n         sampled=sampled,\n         subtract_log_q=True)\n    exp_sampled_softmax_loss = _SoftmaxCrossEntropyWithLogits(\n        exp_logits, exp_labels)\n\n    true_exp_bf16 = np.full([batch_size, 1],\n                            fill_value=0.5,\n                            dtype=dtypes.bfloat16.as_numpy_dtype)\n    sampled_exp_bf16 = np.full([len(sampled)],\n                               fill_value=0.5,\n                               dtype=dtypes.bfloat16.as_numpy_dtype)\n    sampled_vals_bf16 = (sampled, true_exp_bf16, sampled_exp_bf16)\n\n    got_sampled_softmax_loss = math_ops.cast(\n        nn_impl.sampled_softmax_loss_v2(\n            weights=constant_op.constant(weights, dtype=dtypes.bfloat16),\n            biases=constant_op.constant(biases, dtype=dtypes.bfloat16),\n            labels=constant_op.constant(\n                labels, shape=(batch_size, 1), dtype=dtypes.bfloat16),\n            inputs=constant_op.constant(hidden_acts, dtype=dtypes.bfloat16),\n            num_sampled=4,\n            num_classes=num_classes,\n            num_true=1,\n            sampled_values=sampled_vals_bf16,\n            remove_accidental_hits=False), dtypes.float32)\n\n    self.assertAllClose(exp_sampled_softmax_loss,\n                        self.evaluate(got_sampled_softmax_loss), 1e-1)\n\n\nclass CReluTest(test_lib.TestCase):\n\n  def test(self):\n    np.random.seed(1)  # Make it reproducible.\n    x = np.random.randn(3, 4).astype(np.float32)\n    y = np.concatenate([x * (x > 0), -x * (x < 0)], axis=1)\n\n    z = self.evaluate(nn_ops.crelu(constant_op.constant(x)))\n    self.assertAllClose(y, z, 1e-4)\n\n\nclass ReluTest(test_lib.TestCase):\n\n  def test(self):\n    np.random.seed(1)  # Make it reproducible.\n    x = np.random.randn(3, 4).astype(np.float32)\n    y = np.maximum(x, 0.0)\n\n    z = self.evaluate(nn_ops.relu(constant_op.constant(x)))\n    self.assertAllEqual(y, z)\n\n  @test_util.disable_xla(\n      \"This test relies on undefined behavior that XLA does not replicate\")\n  @test_util.run_deprecated_v1\n  def testNaNs(self):\n    # Test that relu(nan) = nan for various sizes.\n    for i in range(18):\n      x = np.zeros(i) + np.nan\n      with self.cached_session():\n        z = nn_ops.relu(constant_op.constant(x)).eval()\n        self.assertTrue(np.isnan(z).all())\n\n\nclass LeakyReluTest(test_lib.TestCase):\n\n  def testRange(self):\n    batch_size = 3\n    height, width = 4, 4\n    np.random.seed(1)  # Make it reproducible.\n    inputs = np.random.uniform(size=(batch_size, height, width, 3)).astype(\n        np.float32)\n    inputs = constant_op.constant(inputs)\n\n    outputs = nn_ops.leaky_relu(inputs)\n    self.assertEqual(inputs.shape, outputs.shape)\n\n    inputs, outputs = self.evaluate([inputs, outputs])\n\n    self.assertGreaterEqual(outputs.min(), 0.0)\n    self.assertLessEqual(outputs.max(), 1.0)\n    self.assertAllClose(inputs, outputs)\n\n  @test_util.run_deprecated_v1\n  def testValues(self):\n    for dtype in [np.int32, np.int64, np.float16, np.float32, np.float64]:\n      np_values = np.array([-2, -1, 0, 1, 2], dtype=dtype)\n      outputs = nn_ops.leaky_relu(constant_op.constant(np_values))\n\n      outputs = self.evaluate(outputs)\n\n      tol = 2e-3 if dtype == np.float16 else 1e-6\n      self.assertAllClose(\n          outputs, [-0.4, -0.2, 0.0, 1.0, 2.0], rtol=tol, atol=tol)\n\n  @test_util.run_deprecated_v1\n  def testName(self):\n    np_values = np.array([-2, -1, 0, 1, 2], dtype=np.float64)\n    outputs_with_name_set = nn_ops.leaky_relu(\n        constant_op.constant(np_values),\n        name='test_relu_op')\n    self.assertEqual(outputs_with_name_set.name, 'test_relu_op:0')\n    outputs_without_name_set = nn_ops.leaky_relu(\n        constant_op.constant(np_values))\n    self.assertEqual(outputs_without_name_set.name, 'LeakyRelu:0')\n\n\nclass GeluTest(test_lib.TestCase):\n\n  def test(self):\n\n    def gelu(x, approximate=False):\n      if approximate:\n        return 0.5 * x * (1.0 + np.tanh(\n            np.sqrt(2.0 / np.pi) * (x + 0.044715 * np.power(x, 3))))\n      else:\n        from scipy.stats import norm  # pylint: disable=g-import-not-at-top\n        return x * norm.cdf(x)\n\n    np.random.seed(1)  # Make it reproducible.\n    x = np.random.randn(3, 4).astype(np.float32)\n    y = gelu(x)\n    z = self.evaluate(nn_ops.gelu(x))\n    self.assertAllClose(y, z)\n\n    y = gelu(x, True)\n    z = self.evaluate(nn_ops.gelu(x, True))\n    self.assertAllClose(y, z)\n\n\nclass SwishTest(test_lib.TestCase):\n\n  @test_util.run_deprecated_v1\n  def testValues(self):\n    np_values = np.array(\n        [np.linspace(-7.0, 0.0, 100),\n         np.linspace(0.0, 7.0, 100)],\n        dtype=np.float32)\n    tf_values = constant_op.constant(np_values)\n    actual_tf_outputs = nn_impl.swish(tf_values)\n    expected_tf_outputs = tf_values * math_ops.sigmoid(tf_values)\n\n    actual_outputs, expected_outputs = self.evaluate(\n        [actual_tf_outputs, expected_tf_outputs])\n\n    self.assertAllClose(actual_outputs, expected_outputs)\n\n  @test_util.run_deprecated_v1\n  def testGradients(self):\n    shape = [5, 3, 4]\n    sigma = 5\n    input_values = np.random.randn(*shape) * sigma\n    x_tf = constant_op.constant(input_values)\n    y_tf = nn_impl.swish(x_tf)\n    with self.cached_session():\n      err = gradient_checker.compute_gradient_error(x_tf, shape, y_tf, shape)\n    self.assertLess(err, 1e-4)\n\n\nclass MomentsTest(test_lib.TestCase):\n\n  def doOutputTest(self,\n                   input_shape,\n                   moments_axes,\n                   tol=1e-4,\n                   check_gradients=False):\n    for mu in [0.0, 1.0, 1e3]:\n      for sigma in [1.0, 0.1]:\n        for keep_dims in [True, False]:\n          input_values = np.random.rand(*input_shape) * sigma + mu\n          expected_mean = np.mean(\n              input_values, axis=moments_axes, keepdims=keep_dims)\n          expected_var = np.var(\n              input_values, axis=moments_axes, keepdims=keep_dims)\n          with ops.Graph().as_default() as g:\n            with self.session(graph=g) as sess:\n              inputs = constant_op.constant(\n                  input_values, shape=input_shape, dtype=dtypes.float32)\n              mean, variance = nn_impl.moments_v2(\n                  inputs, moments_axes, keepdims=keep_dims)\n\n              if check_gradients:\n                err = gradient_checker.compute_gradient_error(\n                    inputs, input_shape, mean, mean.shape.as_list())\n                self.assertLess(err, 1e-3)\n                err = gradient_checker.compute_gradient_error(\n                    inputs, input_shape, variance, variance.shape.as_list())\n                self.assertLess(err, 1e-3)\n\n              # Evaluate.\n              [mean, variance] = self.evaluate([mean, variance])\n              # Make sure that there are no NaNs\n              self.assertFalse(np.isnan(mean).any())\n              self.assertFalse(np.isnan(variance).any())\n              self.assertAllClose(mean, expected_mean, rtol=tol, atol=tol)\n              self.assertAllClose(variance, expected_var, rtol=tol, atol=tol)\n\n  def testOutputAndGradient2DInput0(self):\n    self.doOutputTest((10, 10), (0,), check_gradients=True)\n\n  def testOutputAndGradient2DInput01(self):\n    self.doOutputTest((10, 10), (0, 1), check_gradients=True)\n\n  def testOutput2DInput0(self):\n    self.doOutputTest((10, 300), (0,))\n\n  def testOutput2DInput1(self):\n    self.doOutputTest((10, 300), (1,))\n\n  def testOutput2DInput01(self):\n    self.doOutputTest((10, 300), (0, 1))\n\n  def testOutput4DInput0(self):\n    self.doOutputTest((10, 10, 10, 30), (0,))\n\n  def testOutput4DInput1(self):\n    self.doOutputTest((10, 10, 10, 30), (1,))\n\n  def testOutput4DInput3(self):\n    self.doOutputTest((10, 10, 10, 30), (3,))\n\n  def testOutput4DInput012(self):\n    self.doOutputTest((10, 10, 10, 30), (0, 1, 2))\n\n  def testOutput4DInput123(self):\n    self.doOutputTest((10, 10, 10, 30), (1, 2, 3))\n\n\nclass DataFormatDimMapTest(test_lib.TestCase):\n\n  def _test(self, x_val, y_val_expected):\n    x = constant_op.constant(x_val)\n    y = nn_ops.data_format_dim_map(x)\n\n    y_val = self.evaluate(y)\n    self.assertAllEqual(y_val, y_val_expected)\n\n  def test(self):\n    self._test(0, 0)\n    self._test(1, 2)\n    self._test(2, 3)\n    self._test(3, 1)\n    self._test(-1, 1)\n    self._test(-2, 3)\n    self._test(-3, 2)\n    self._test(-4, 0)\n    self._test([1, 3], [2, 1])\n    self._test([1, 3, -2], [2, 1, 3])\n    self._test([1, -3, -2], [2, 2, 3])\n    self._test([[1, -3], [1, -1]], [[2, 2], [2, 1]])\n\n  def testNHWCtoNCHW(self):\n    x_val = [1, -3, -2]\n    y_val_expected = [2, 2, 3]\n    x = constant_op.constant(x_val)\n    y = nn_ops.data_format_dim_map(x, src_format=\"NHWC\", dst_format=\"NCHW\")\n    with test_util.use_gpu():\n      y_val = self.evaluate(y)\n      self.assertAllEqual(y_val, y_val_expected)\n\n  def testNHWCtoHWNC(self):\n    x_val = [-4, -3, -2, -1, 0, 1, 2, 3]\n    y_val_expected = [2, 0, 1, 3, 2, 0, 1, 3]\n    x = constant_op.constant(x_val)\n    y = nn_ops.data_format_dim_map(x, src_format=\"NHWC\", dst_format=\"HWNC\")\n    with test_util.use_gpu():\n      y_val = self.evaluate(y)\n      self.assertAllEqual(y_val, y_val_expected)\n\n  def testNHWCtoWHCN(self):\n    x_val = [-4, -3, -2, -1, 0, 1, 2, 3]\n    y_val_expected = [3, 1, 0, 2, 3, 1, 0, 2]\n    x = constant_op.constant(x_val)\n    y = nn_ops.data_format_dim_map(x, src_format=\"NHWC\", dst_format=\"WHCN\")\n    with test_util.use_gpu():\n      y_val = self.evaluate(y)\n      self.assertAllEqual(y_val, y_val_expected)\n\n  def testNDHWCtoNCDHW(self):\n    x_val = [1, -4, -3, -2]\n    y_val_expected = [2, 2, 3, 4]\n    x = constant_op.constant(x_val)\n    y = nn_ops.data_format_dim_map(x, src_format=\"NDHWC\", dst_format=\"NCDHW\")\n    with test_util.use_gpu():\n      y_val = self.evaluate(y)\n      self.assertAllEqual(y_val, y_val_expected)\n\n  def testNDHWCtoDHWNC(self):\n    x_val = [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4]\n    y_val_expected = [3, 0, 1, 2, 4, 3, 0, 1, 2, 4]\n    x = constant_op.constant(x_val)\n    y = nn_ops.data_format_dim_map(x, src_format=\"NDHWC\", dst_format=\"DHWNC\")\n    with test_util.use_gpu():\n      y_val = self.evaluate(y)\n      self.assertAllEqual(y_val, y_val_expected)\n\n  def testDNHWCtoWHDCN(self):\n    x_val = [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4]\n    y_val_expected = [4, 2, 1, 0, 3, 4, 2, 1, 0, 3]\n    x = constant_op.constant(x_val)\n    y = nn_ops.data_format_dim_map(x, src_format=\"NDHWC\", dst_format=\"WHDCN\")\n    with test_util.use_gpu():\n      y_val = self.evaluate(y)\n      self.assertAllEqual(y_val, y_val_expected)\n\n  def testArbitraryASCII(self):\n    x_val = [-4, -3, -2, -1, 0, 1, 2, 3]\n    y_val_expected = [3, 2, 1, 0, 3, 2, 1, 0]\n    x = constant_op.constant(x_val)\n    y = nn_ops.data_format_dim_map(x, src_format=\"qwer\", dst_format=\"rewq\")\n    with test_util.use_gpu():\n      y_val = self.evaluate(y)\n      self.assertAllEqual(y_val, y_val_expected)\n\n\nclass DataFormatVectorPermuteTest(test_lib.TestCase):\n\n  def testNHWCToNCHW(self):\n    x_val = [7, 4, 9, 3]\n    x = constant_op.constant(x_val)\n    y = nn_ops.data_format_vec_permute(x)\n    with test_util.use_gpu():\n      y_val = self.evaluate(y)\n      self.assertAllEqual(y_val, [7, 3, 4, 9])\n\n  def testNHWCToNCHW_Size2(self):\n    x_val = [4, 9]\n    x = constant_op.constant(x_val)\n    y = nn_ops.data_format_vec_permute(x)\n    with test_util.use_gpu():\n      y_val = self.evaluate(y)\n      self.assertAllEqual(y_val, [4, 9])\n\n  @test_util.disable_xla(\"unsupported data format\")\n  def testNHWCToWHCN(self):\n    x_val = [7, 4, 9, 3]\n    x = constant_op.constant(x_val)\n    y = nn_ops.data_format_vec_permute(x, src_format=\"NHWC\", dst_format=\"WHCN\")\n    with test_util.use_gpu():\n      y_val = self.evaluate(y)\n      self.assertAllEqual(y_val, [9, 4, 3, 7])\n\n  @test_util.disable_xla(\"unsupported data format\")\n  def testNHWCToWHCN_Size2(self):\n    x_val = [4, 9]\n    x = constant_op.constant(x_val)\n    y = nn_ops.data_format_vec_permute(x, src_format=\"NHWC\", dst_format=\"WHCN\")\n    with test_util.use_gpu():\n      y_val = self.evaluate(y)\n      self.assertAllEqual(y_val, [9, 4])\n\n  def testNCHWToNHWC(self):\n    x_val = [7, 4, 9, 3]\n    x = constant_op.constant(x_val)\n    y = nn_ops.data_format_vec_permute(x, src_format=\"NCHW\", dst_format=\"NHWC\")\n    with test_util.use_gpu():\n      y_val = self.evaluate(y)\n      self.assertAllEqual(y_val, [7, 9, 3, 4])\n\n  def testNCHWToNHWC_Size2(self):\n    x_val = [9, 3]\n    x = constant_op.constant(x_val)\n    y = nn_ops.data_format_vec_permute(x)\n    with test_util.use_gpu():\n      y_val = self.evaluate(y)\n      self.assertAllEqual(y_val, [9, 3])\n\n  def testNHWCToHWNC(self):\n    x_val = [7, 4, 9, 3]\n    x = constant_op.constant(x_val)\n    y = nn_ops.data_format_vec_permute(x, src_format=\"NHWC\", dst_format=\"HWNC\")\n    with test_util.use_gpu():\n      y_val = self.evaluate(y)\n      self.assertAllEqual(y_val, [4, 9, 7, 3])\n\n  def testHWNCToNHWC(self):\n    x_val = [7, 4, 9, 3]\n    x = constant_op.constant(x_val)\n    y = nn_ops.data_format_vec_permute(x, src_format=\"HWNC\", dst_format=\"NHWC\")\n    with test_util.use_gpu():\n      y_val = self.evaluate(y)\n      self.assertAllEqual(y_val, [9, 7, 4, 3])\n\n  def testNHWCToNCHW2D(self):\n    x_val = [[7, 4], [9, 3], [4, 5], [5, 1]]\n    x = constant_op.constant(x_val)\n    y = nn_ops.data_format_vec_permute(x)\n    with test_util.use_gpu():\n      y_val = self.evaluate(y)\n      self.assertAllEqual(y_val, [[7, 4], [5, 1], [9, 3], [4, 5]])\n\n  def testNHWCToHWNC2D(self):\n    x_val = [[7, 4], [9, 3], [4, 5], [5, 1]]\n    x = constant_op.constant(x_val)\n    y = nn_ops.data_format_vec_permute(x, src_format=\"NHWC\", dst_format=\"HWNC\")\n    with test_util.use_gpu():\n      y_val = self.evaluate(y)\n      self.assertAllEqual(y_val, [[9, 3], [4, 5], [7, 4], [5, 1]])\n\n  def testHWNCToNHWC2D(self):\n    x_val = [[7, 4], [9, 3], [4, 5], [5, 1]]\n    x = constant_op.constant(x_val)\n    y = nn_ops.data_format_vec_permute(x, src_format=\"HWNC\", dst_format=\"NHWC\")\n    with test_util.use_gpu():\n      y_val = self.evaluate(y)\n      self.assertAllEqual(y_val, [[4, 5], [7, 4], [9, 3], [5, 1]])\n\n  def testNCHWToNHWC2D(self):\n    x_val = [[7, 4], [9, 3], [4, 5], [5, 1]]\n    x = constant_op.constant(x_val)\n    y = nn_ops.data_format_vec_permute(x, src_format=\"NCHW\", dst_format=\"NHWC\")\n    with test_util.use_gpu():\n      y_val = self.evaluate(y)\n      self.assertAllEqual(y_val, [[7, 4], [4, 5], [5, 1], [9, 3]])\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass AvgPoolTest(test_lib.TestCase):\n\n  def test1DTensor(self):\n    x = array_ops.ones([3, 6, 5])\n    ksize = 2\n    strides = 2\n\n    y1 = nn_ops.avg_pool_v2(x, ksize, strides, \"SAME\")\n    y2 = nn_ops.avg_pool1d(x, ksize, strides, \"SAME\")\n\n    self.assertAllEqual(self.evaluate(y1), self.evaluate(y2))\n\n  def test1DNumpy(self):\n    # explicitly use float32 for ROCm, as MIOpen does not yet support float64\n    # np.ones defaults to using float64 when dtype is not explicitly specified\n    dtype = np.float32 if test_lib.is_built_with_rocm() else np.float64\n    x = np.ones([3, 6, 5], dtype=dtype)\n    ksize = 2\n    strides = 2\n\n    y1 = nn_ops.avg_pool_v2(x, ksize, strides, \"SAME\")\n    y2 = nn_ops.avg_pool1d(x, ksize, strides, \"SAME\")\n\n    self.assertAllEqual(self.evaluate(y1), self.evaluate(y2))\n\n  def test1DNumpyWithGolden(self):\n    dtype = np.float32 if test_lib.is_built_with_rocm() else np.float64\n    x = np.array([[[3], [6], [5]],\n                  [[1], [0], [1]]], dtype=dtype)\n    ksize = 2\n    strides = 1\n    y = nn_ops.avg_pool1d(x, ksize, strides, \"SAME\")\n    expected_y = np.array([[[4.5], [5.5], [5.0]],\n                           [[0.5], [0.5], [1.0]]], dtype=dtype)\n    self.assertAllEqual(self.evaluate(y), expected_y)\n\n  def test2DTensor(self):\n    x = array_ops.ones([3, 6, 6, 5])\n    ksize = 2\n    strides = 2\n\n    y1 = nn_ops.avg_pool_v2(x, ksize, strides, \"SAME\")\n    y2 = nn_ops.avg_pool(x, ksize, strides, \"SAME\")\n\n    self.assertAllEqual(self.evaluate(y1), self.evaluate(y2))\n\n  def test2DNumpy(self):\n    # explicitly use float32 for ROCm, as MIOpen does not yet support float64\n    # np.ones defaults to using float64 when dtype is not explicitly specified\n    dtype = np.float32 if test_lib.is_built_with_rocm() else np.float64\n    x = np.ones([3, 6, 6, 5], dtype=dtype)\n    ksize = 2\n    strides = 2\n\n    y1 = nn_ops.avg_pool_v2(x, ksize, strides, \"SAME\")\n    y2 = nn_ops.avg_pool(x, ksize, strides, \"SAME\")\n\n    self.assertAllEqual(self.evaluate(y1), self.evaluate(y2))\n\n  def test3DTensor(self):\n    if test_lib.is_built_with_rocm():\n      self.skipTest(\"Pooling with 3D tensors is not supported in ROCm\")\n    x = array_ops.ones([3, 7, 6, 6, 5])\n    ksize = 2\n    strides = 2\n\n    y1 = nn_ops.avg_pool_v2(x, ksize, strides, \"SAME\")\n    y2 = nn_ops.avg_pool3d(x, ksize, strides, \"SAME\")\n\n    self.assertAllEqual(self.evaluate(y1), self.evaluate(y2))\n\n  def test3DNumpy(self):\n    if test_lib.is_built_with_rocm():\n      self.skipTest(\"Pooling with 3D tensors is not supported in ROCm\")\n    x = np.ones([3, 7, 6, 6, 5], dtype=np.float32)\n    ksize = 2\n    strides = 2\n\n    y1 = nn_ops.avg_pool_v2(x, ksize, strides, \"SAME\")\n    y2 = nn_ops.avg_pool3d(x, ksize, strides, \"SAME\")\n\n    self.assertAllEqual(self.evaluate(y1), self.evaluate(y2))\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass MaxPoolTest(test_lib.TestCase):\n\n  def test1DTensor(self):\n    x = array_ops.ones([3, 6, 5])\n    ksize = 2\n    strides = 2\n\n    y1 = nn_ops.max_pool_v2(x, ksize, strides, \"SAME\")\n    y2 = nn_ops.max_pool1d(x, ksize, strides, \"SAME\")\n\n    self.assertAllEqual(self.evaluate(y1), self.evaluate(y2))\n\n  def test1DNumpy(self):\n    # explicitly use float32 for ROCm, as MIOpen does not yet support float64\n    # np.ones defaults to using float64 when dtype is not explicitly specified\n    dtype = np.float32 if test_lib.is_built_with_rocm() else np.float64\n    x = np.ones([3, 6, 5], dtype=dtype)\n    ksize = 2\n    strides = 2\n\n    y1 = nn_ops.max_pool_v2(x, ksize, strides, \"SAME\")\n    y2 = nn_ops.max_pool1d(x, ksize, strides, \"SAME\")\n\n    self.assertAllEqual(self.evaluate(y1), self.evaluate(y2))\n\n  def test1DNumpyWithGolden(self):\n    dtype = np.float32 if test_lib.is_built_with_rocm() else np.float64\n    x = np.array([[[3], [6], [5]],\n                  [[1], [0], [1]]], dtype=dtype)\n    ksize = 2\n    strides = 1\n    y = nn_ops.max_pool1d(x, ksize, strides, \"SAME\")\n    expected_y = np.array([[[6], [6], [5]],\n                           [[1], [1], [1]]], dtype=dtype)\n    self.assertAllEqual(self.evaluate(y), expected_y)\n\n  def test2DTensor(self):\n    x = array_ops.ones([3, 6, 6, 5])\n    ksize = 2\n    strides = 2\n\n    y1 = nn_ops.max_pool_v2(x, ksize, strides, \"SAME\")\n    y2 = nn_ops.max_pool(x, ksize, strides, \"SAME\")\n\n    self.assertAllEqual(self.evaluate(y1), self.evaluate(y2))\n\n  def test2DNumpy(self):\n    # explicitly use float32 for ROCm, as MIOpen does not yet support float64\n    # np.ones defaults to using float64 when dtype is not explicitly specified\n    dtype = np.float32 if test_lib.is_built_with_rocm() else np.float64\n    x = np.ones([3, 6, 6, 5], dtype=dtype)\n    ksize = 2\n    strides = 2\n\n    y1 = nn_ops.max_pool_v2(x, ksize, strides, \"SAME\")\n    y2 = nn_ops.max_pool(x, ksize, strides, \"SAME\")\n\n    self.assertAllEqual(self.evaluate(y1), self.evaluate(y2))\n\n  def test3DTensor(self):\n    if test_lib.is_built_with_rocm():\n      self.skipTest(\"Pooling with 3D tensors is not supported in ROCm\")\n    x = array_ops.ones([3, 7, 6, 6, 5])\n    ksize = 2\n    strides = 2\n\n    y1 = nn_ops.max_pool_v2(x, ksize, strides, \"SAME\")\n    y2 = nn_ops.max_pool3d(x, ksize, strides, \"SAME\")\n\n    self.assertAllEqual(self.evaluate(y1), self.evaluate(y2))\n\n  def test3DNumpy(self):\n    if test_lib.is_built_with_rocm():\n      self.skipTest(\"Pooling with 3D tensors is not supported in ROCm\")\n    x = np.ones([3, 7, 6, 6, 5], dtype=np.float32)\n    ksize = 2\n    strides = 2\n\n    y1 = nn_ops.max_pool_v2(x, ksize, strides, \"SAME\")\n    y2 = nn_ops.max_pool3d(x, ksize, strides, \"SAME\")\n\n    self.assertAllEqual(self.evaluate(y1), self.evaluate(y2))\n\n  def testIncorrectSizeInputSmall(self):\n    x = array_ops.ones([3, 4])\n    with self.assertRaisesRegex(\n        ValueError, \"Input tensor must be of rank 3, 4 or 5 but was 2.\"):\n      nn_ops.max_pool_v2(x, 2, 2, \"SAME\")\n\n  def testIncorrectSizeInput(self):\n    x = array_ops.ones([3, 4, 1, 2, 1, 2])\n    with self.assertRaisesRegex(\n        ValueError, \"Input tensor must be of rank 3, 4 or 5 but was 6.\"):\n      nn_ops.max_pool_v2(x, 2, 2, \"SAME\")\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass ConvolutionTest(test_lib.TestCase):\n\n  def testUnknownSize(self):\n    x = tensor_spec.TensorSpec(None, dtypes.float32, name=\"x\")\n    k = np.ones([3, 6, 6, 5], dtype=np.float32)\n\n    @def_function.function\n    def F(value):\n      return nn_ops.convolution(value, k, \"SAME\")\n\n    F.get_concrete_function(x)\n\n\nclass ConvTransposeTest(test_lib.TestCase):\n\n  def test1D(self):\n    t = array_ops.ones([2, 4, 3])\n    v = array_ops.ones([2, 5, 3])\n    strides = 2\n\n    y1 = nn_ops.conv1d_transpose(t, v, [2, 8, 5], strides)\n    y2 = nn_ops.conv_transpose(t, v, [2, 8, 5], strides)\n\n    self.assertAllEqual(self.evaluate(y1), self.evaluate(y2))\n\n  def test1DTensor(self):\n    t = array_ops.ones([2, 4, 3])\n    v = array_ops.ones([2, 5, 3])\n    strides = 2\n\n    y1 = nn_ops.conv1d_transpose(t, v, [2, 8, 5], strides)\n    y2 = nn_ops.conv_transpose(t, v, constant_op.constant([2, 8, 5]), strides)\n\n    self.assertAllEqual(self.evaluate(y1), self.evaluate(y2))\n\n  def test2D(self):\n    t = array_ops.ones([2, 4, 4, 3])\n    v = array_ops.ones([2, 2, 5, 3])\n    strides = 2\n\n    y1 = nn_ops.conv2d_transpose_v2(t, v, [2, 8, 8, 5], strides)\n    y2 = nn_ops.conv_transpose(t, v, [2, 8, 8, 5], strides)\n\n    self.assertAllEqual(self.evaluate(y1), self.evaluate(y2))\n\n  def test2DTensor(self):\n    t = array_ops.ones([2, 4, 4, 3])\n    v = array_ops.ones([2, 2, 5, 3])\n    strides = 2\n\n    y1 = nn_ops.conv2d_transpose_v2(t, v, [2, 8, 8, 5], strides)\n    y2 = nn_ops.conv_transpose(t, v, constant_op.constant([2, 8, 8, 5]),\n                               strides)\n\n    self.assertAllEqual(self.evaluate(y1), self.evaluate(y2))\n\n  def test3D(self):\n    t = array_ops.ones([2, 4, 4, 4, 3])\n    v = array_ops.ones([2, 2, 2, 5, 3])\n    strides = 2\n\n    y1 = nn_ops.conv3d_transpose_v2(t, v, [2, 8, 8, 8, 5], strides)\n    y2 = nn_ops.conv_transpose(t, v, [2, 8, 8, 8, 5], strides)\n\n    self.assertAllEqual(self.evaluate(y1), self.evaluate(y2))\n\n  def test3DTensor(self):\n    t = array_ops.ones([2, 4, 4, 4, 3])\n    v = array_ops.ones([2, 2, 2, 5, 3])\n    strides = 2\n\n    y1 = nn_ops.conv3d_transpose_v2(t, v, [2, 8, 8, 8, 5], strides)\n    y2 = nn_ops.conv_transpose(t, v, constant_op.constant([2, 8, 8, 8, 5]),\n                               strides)\n\n    self.assertAllEqual(self.evaluate(y1), self.evaluate(y2))\n\n  def testIncorrectSizeInputSmall(self):\n    with self.assertRaisesRegex(\n        ValueError, \"output_shape must be of length 3, 4 or 5 but was 2.\"):\n      nn_ops.conv_transpose(None, 2, [2, 3], \"SAME\")\n\n  def testIncorrectSizeInput(self):\n    with self.assertRaisesRegex(\n        ValueError, \"output_shape must be of length 3, 4 or 5 but was 6.\"):\n      nn_ops.conv_transpose(None, 2, [2, 3, 4, 2, 5, 1], \"SAME\")\n\n  def testTensorsNoShape(self):\n    with self.assertRaisesRegex(\n        ValueError,\n        \"output_shape must be a tensor or sized collection.\"):\n      nn_ops.conv_transpose(None, None, None, None)\n\n\nclass RaggedEmbeddingTest(test_lib.TestCase):\n\n  def testRaggedTensor(self):\n    weights = constant_op.constant([[0, 0, 0], [1, 1, 1], [2, 2, 2], [3, 3, 3]])\n    ragged_ids = ragged_factory_ops.constant([[1, 2, 3], [0], [1, 2]],\n                                             ragged_rank=1)\n\n    embedded_ragged = nn.embedding_lookup_ragged(weights, ragged_ids)\n    expected_output = ragged_factory_ops.constant(\n        [[[1, 1, 1], [2, 2, 2], [3, 3, 3]], [[0, 0, 0]], [[1, 1, 1], [2, 2, 2]]\n        ],\n        ragged_rank=1)\n\n    self.assertAllEqual(expected_output, embedded_ragged)\n\n  def testMultipleRaggedDimTensor(self):\n    weights = constant_op.constant([[0, 0], [1, 1], [2, 2], [3, 3], [4, 4],\n                                    [5, 5], [6, 6]])\n    ragged_ids = ragged_factory_ops.constant(\n        [[[[3, 4], [0, 6]], []], [[[2, 1], [1, 0]], [[2, 5], [2, 3]]], [[[1, 0]]\n                                                                       ]],\n        ragged_rank=2)\n\n    embedded_ragged = nn.embedding_lookup_ragged(weights, ragged_ids)\n    expected_output = ragged_factory_ops.constant(\n        [[[[[3, 3], [4, 4]], [[0, 0], [6, 6]]], []],\n         [[[[2, 2], [1, 1]], [[1, 1], [0, 0]]],\n          [[[2, 2], [5, 5]], [[2, 2], [3, 3]]]], [[[[1, 1], [0, 0]]]]],\n        ragged_rank=2)\n\n    self.assertAllEqual(expected_output, embedded_ragged)\n\n  def testMissingWeights(self):\n    ragged_ids = ragged_factory_ops.constant([[1, 2, 3], [0], [1, 2]])\n\n    with self.assertRaisesRegex(ValueError,\n                                \"The embedding weights must be specified.*\"):\n      nn.embedding_lookup_ragged(None, ragged_ids)\n\n  def testEmptyWeights(self):\n    ragged_ids = ragged_factory_ops.constant([[1, 2, 3], [0], [1, 2]])\n\n    with self.assertRaisesRegex(ValueError,\n                                \"The embedding weights should not be empty.*\"):\n      nn.embedding_lookup_ragged([], ragged_ids)\n\n  def testInvalidIndicesType(self):\n    weights = constant_op.constant([[0, 0, 0], [1, 1, 1], [2, 2, 2]])\n    ragged_ids = ragged_factory_ops.constant([[1., 2., 3.], [1., 2.]])\n\n    with self.assertRaisesRegex(\n        ValueError, \"The values contained by the inputs have type*\"):\n      nn.embedding_lookup_ragged(weights, ragged_ids)\n\n  def testMaxNormForEmbeddings(self):\n    weights = constant_op.constant([[0, 0, 0, 0], [1, 1, 1, 1],\n                                    [2, 2, 2, 2], [3, 3, 3, 3]],\n                                   dtype=dtypes.float32)\n    ragged_ids = ragged_factory_ops.constant([[1, 2, 3], [0], [1, 2]],\n                                             ragged_rank=1)\n\n    actual_embeddings = [\n        nn.embedding_lookup(weights, ragged_ids, max_norm=max_norm)\n        for max_norm in [1, 2, 5]]\n\n    expected_embeddings = (\n        # max_norm = 1\n        [[[.5, .5, .5, .5], [.5, .5, .5, .5], [.5, .5, .5, .5]],\n         [[0, 0, 0, 0]], [[.5, .5, .5, .5], [.5, .5, .5, .5]]],\n        # max_norm = 2\n        [[[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]],\n         [[0, 0, 0, 0]], [[1, 1, 1, 1], [1, 1, 1, 1]]],\n        # max_norm = 5\n        [[[1, 1, 1, 1], [2, 2, 2, 2], [2.5, 2.5, 2.5, 2.5]],\n         [[0, 0, 0, 0]], [[1, 1, 1, 1], [2, 2, 2, 2]]],\n        )\n\n    for expected, actual in zip(expected_embeddings, actual_embeddings):\n      self.assertAllClose(\n          ragged_factory_ops.constant(expected, dtype=float, ragged_rank=1),\n          actual)\n\n\nclass IsotonicTest(parameterized.TestCase, test_lib.TestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_increasing_and_decreasing(self):\n    x = constant_op.constant([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]],\n                             dtype=dtypes.float64)\n    y, segments = nn_ops.isotonic_regression(x, decreasing=False)\n    self.assertAllClose(y, x)\n    self.assertAllClose(segments, [[0, 1, 2, 3, 4], [0, 1, 2, 3, 4]])\n\n    y, segments = nn_ops.isotonic_regression(x, decreasing=True)\n    self.assertAllClose(\n        y,\n        [\n            [2, 2, 2, 2, 2],  # Average of the inputs.\n            [7, 7, 7, 7, 7]\n        ])\n    self.assertAllClose(segments, array_ops.zeros((2, 5)))\n\n    y, segments = nn_ops.isotonic_regression(-x, decreasing=True)\n    self.assertAllClose(segments, [[0, 1, 2, 3, 4], [0, 1, 2, 3, 4]])\n\n    self.assertAllClose(y, -x)\n    y, segments = nn_ops.isotonic_regression(-x, decreasing=False)\n    self.assertAllClose(\n        -y,\n        [\n            [2, 2, 2, 2, 2],  # Average of the inputs.\n            [7, 7, 7, 7, 7]\n        ])\n    self.assertAllClose(segments, array_ops.zeros((2, 5)))\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_different_axis(self):\n    x = constant_op.constant([[0, 6, 2, 8, 4], [5, 1, 7, 3, 9]],\n                             dtype=dtypes.float64)\n    y, segments = nn_ops.isotonic_regression(x, decreasing=True, axis=0)\n    self.assertAllClose(\n        y,\n        [\n            [2.5, 6, 4.5, 8, 6.5],  # Either identity or average.\n            [2.5, 1, 4.5, 3, 6.5]\n        ])\n    self.assertAllClose(segments, [[0, 0, 0, 0, 0], [0, 1, 0, 1, 0]])\n\n  @test_util.run_v2_only\n  def testGradientV2(self, dtype=np.float64, batch_size=30, dimensions=50):\n\n    @def_function.function\n    def ComputeIsotonicFn(x):\n      y, _ = nn_ops.isotonic_regression(x)  # No gradient wrt segments.\n      return y\n\n    np.random.seed(0)\n    x_init = np.random.randn(batch_size, dimensions).astype(dtype)\n    grad_theoretical, grad_numerical = gradient_checker_v2.compute_gradient(\n        ComputeIsotonicFn, [x_init], delta=1e-5)\n    self.assertAllClose(grad_theoretical, grad_numerical)\n\n  @test_util.run_v1_only(\"compute_gradient_error is v1 only\")\n  def testGradientV1(self, dtype=np.float64, batch_size=30, dimensions=50):\n    np.random.seed(0)\n    x_init = np.random.randn(batch_size, dimensions).astype(dtype)\n    with self.cached_session():\n      x = array_ops.placeholder(dtype, (batch_size, dimensions))\n      y, _ = nn_ops.isotonic_regression(x)  # Segments have no gradient.\n      max_error = gradient_checker.compute_gradient_error(\n          x, (batch_size, dimensions), y, (batch_size, dimensions), x_init)\n    self.assertAllClose(max_error, 0.)\n\n  @parameterized.parameters([[dtypes.half, dtypes.half],\n                             [dtypes.bfloat16, dtypes.bfloat16],\n                             [dtypes.float32, dtypes.float32],\n                             [dtypes.float64, dtypes.float64],\n                             [dtypes.int32, dtypes.float64],\n                             [dtypes.int16, dtypes.float32]])\n  def testTypePromotion(self, dtype_in, expected_dtype_out):\n    x = constant_op.constant([[0, 6, 2, 8, 4], [5, 1, 7, 3, 9]], dtype=dtype_in)\n    y, segments = nn_ops.isotonic_regression(x)\n    self.assertEqual(y.dtype, expected_dtype_out)\n    self.assertEqual(segments.dtype, dtypes.int32)\n\n\nif __name__ == \"__main__\":\n  test_lib.main()\n"], "fixing_code": ["/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n// See docs in ../ops/nn_ops.cc.\n\n#define EIGEN_USE_THREADS\n\n#include \"tensorflow/core/kernels/data_format_ops.h\"\n\n#include <map>\n\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/register_types.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/platform/errors.h\"\n\nnamespace tensorflow {\n\ntypedef Eigen::ThreadPoolDevice CPUDevice;\ntypedef Eigen::GpuDevice GPUDevice;\n\n// Ensure that `src` and `dst` define a valid permutation.\n// Ops defined in this file assume that user specifies a permutation via two\n// string attributes. This check validates that these attributes properly define\n// it to prevent security vulnerabilities.\nstatic bool IsValidPermutation(const std::string& src, const std::string& dst) {\n  if (src.size() != dst.size()) {\n    return false;\n  }\n\n  std::map<char, bool> characters;\n\n  // Every character in `src` must be present only once\n  for (const auto c : src) {\n    if (characters[c]) {\n      return false;\n    }\n    characters[c] = true;\n  }\n\n  // Every character in `dst` must show up in `src` exactly once\n  for (const auto c : dst) {\n    if (!characters[c]) {\n      return false;\n    }\n    characters[c] = false;\n  }\n\n  // At this point, characters[] has been switched to true and false exactly\n  // once for all character in `src` (and `dst`) so we have a valid permutation\n  return true;\n}\n\ntemplate <typename Device, typename T>\nclass DataFormatDimMapOp : public OpKernel {\n public:\n  explicit DataFormatDimMapOp(OpKernelConstruction* context)\n      : OpKernel(context) {\n    string src_format;\n    OP_REQUIRES_OK(context, context->GetAttr(\"src_format\", &src_format));\n    string dst_format;\n    OP_REQUIRES_OK(context, context->GetAttr(\"dst_format\", &dst_format));\n    OP_REQUIRES(context, src_format.size() == 4 || src_format.size() == 5,\n                errors::InvalidArgument(\n                    \"Source format must be of length 4 or 5, received \"\n                    \"src_format = \",\n                    src_format));\n    OP_REQUIRES(context, dst_format.size() == 4 || dst_format.size() == 5,\n                errors::InvalidArgument(\"Destination format must be of length \"\n                                        \"4 or 5, received dst_format = \",\n                                        dst_format));\n    OP_REQUIRES(\n        context, IsValidPermutation(src_format, dst_format),\n        errors::InvalidArgument(\n            \"Destination and source format must determine a permutation, got \",\n            src_format, \" and \", dst_format));\n    dst_idx_ = Tensor(DT_INT32, {static_cast<int64>(src_format.size())});\n    for (int i = 0; i < src_format.size(); ++i) {\n      for (int j = 0; j < dst_format.size(); ++j) {\n        if (dst_format[j] == src_format[i]) {\n          dst_idx_.vec<int>()(i) = j;\n          break;\n        }\n      }\n    }\n  }\n\n  void Compute(OpKernelContext* context) override {\n    const Tensor& input = context->input(0);\n    Tensor* output;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(0, input.shape(), &output));\n    functor::DataFormatDimMap<Device, T>()(context->eigen_device<Device>(),\n                                           input.flat<T>(), output->flat<T>(),\n                                           dst_idx_.vec<int>());\n  }\n\n  Tensor dst_idx_;\n};\n\ntemplate <typename Device, typename T>\nclass DataFormatVecPermuteOp : public OpKernel {\n public:\n  explicit DataFormatVecPermuteOp(OpKernelConstruction* context)\n      : OpKernel(context) {\n    string src_format;\n    OP_REQUIRES_OK(context, context->GetAttr(\"src_format\", &src_format));\n    OP_REQUIRES(context, src_format.size() == 4 || src_format.size() == 5,\n                errors::InvalidArgument(\n                    \"Source format must be of length 4 or 5, received \"\n                    \"src_format = \",\n                    src_format));\n    string dst_format;\n    OP_REQUIRES_OK(context, context->GetAttr(\"dst_format\", &dst_format));\n    OP_REQUIRES(context, dst_format.size() == 4 || dst_format.size() == 5,\n                errors::InvalidArgument(\"Destination format must be of length \"\n                                        \"4 or 5, received dst_format = \",\n                                        dst_format));\n    OP_REQUIRES(\n        context, IsValidPermutation(src_format, dst_format),\n        errors::InvalidArgument(\n            \"Destination and source format must determine a permutation, got \",\n            src_format, \" and \", dst_format));\n    src_format_ = src_format;\n    dst_format_ = dst_format;\n  }\n\n  void Compute(OpKernelContext* context) override {\n    const Tensor& input = context->input(0);\n    OP_REQUIRES(context, input.dims() == 1 || input.dims() == 2,\n                errors::InvalidArgument(\n                    \"input must be a vector or 2D tensor, but got shape \",\n                    input.shape().DebugString()));\n    if (input.dims() == 1) {\n      OP_REQUIRES(context,\n                  input.NumElements() == 2 || input.NumElements() == 4 ||\n                      input.NumElements() == 5,\n                  errors::InvalidArgument(\n                      \"1D input must be of size 2, 4 or 5, but got shape \",\n                      input.shape().DebugString()));\n    } else if (input.dims() == 2) {\n      OP_REQUIRES(context, input.dim_size(0) == 2 || input.dim_size(0) == 4,\n                  errors::InvalidArgument(\"First dimension of 2D input must be \"\n                                          \"of size 2 or 4, but got shape \",\n                                          input.shape().DebugString()));\n      OP_REQUIRES(\n          context, input.dim_size(1) == 2,\n          errors::InvalidArgument(\n              \"Second dimension of 2D input must be of size 2, but got shape \",\n              input.shape().DebugString()));\n    }\n\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(0, input.shape(), &output));\n    // Support 1D and 2D cases.\n    Eigen::DSizes<Eigen::DenseIndex, 8> dst_idx;\n    string src_format_str = src_format_;\n    string dst_format_str = dst_format_;\n    if (input.dim_size(0) == 2) {\n      // If the input is a vector of size 2, treat the two elements as spatial\n      // dimensions.\n      auto keep_only_spatial_dimensions = [](string* format_str) -> void {\n        auto new_end = std::remove_if(\n            format_str->begin(), format_str->end(),\n            [](const char dim) { return dim != 'H' && dim != 'W'; });\n        format_str->erase(new_end, format_str->end());\n      };\n      keep_only_spatial_dimensions(&src_format_str);\n      keep_only_spatial_dimensions(&dst_format_str);\n      OP_REQUIRES(context,\n                  src_format_str.size() == 2 && dst_format_str.size() == 2,\n                  errors::InvalidArgument(\n                      \"Format specifier must contain H and W for 2D case\"));\n    }\n    ComputeDstIndex(src_format_str, dst_format_str, input.dims(), &dst_idx);\n\n    functor::DataFormatVecPermute<Device, T>()(context->eigen_device<Device>(),\n                                               input.flat<T>(),\n                                               output->flat<T>(), dst_idx);\n  }\n\n private:\n  // Finds out the destination index. Support 1D and 2D cases.\n  // Example: HWNC --> NHWC\n  // 1D: dst = [1, 2, 0, 3],\n  // 2D: dst = [2, 3, 4, 5, 0, 1, 6, 7]\n  static void ComputeDstIndex(const string& src_format_str,\n                              const string& dst_format_str, int num_dim,\n                              Eigen::DSizes<Eigen::DenseIndex, 8>* dst) {\n    for (int i = 0; i < src_format_str.size(); ++i) {\n      for (int j = 0; j < dst_format_str.size(); ++j) {\n        if (dst_format_str[j] != src_format_str[i]) continue;\n        // Found the dst index. Set output based on the number of dims.\n        for (int k = 0; k < num_dim; ++k) {\n          (*dst)[i * num_dim + k] = j * num_dim + k;\n        }\n      }\n    }\n  }\n\n  string src_format_;\n  string dst_format_;\n};\n\n#define REGISTER_KERNEL(T)                                                \\\n  REGISTER_KERNEL_BUILDER(                                                \\\n      Name(\"DataFormatDimMap\").Device(DEVICE_CPU).TypeConstraint<T>(\"T\"), \\\n      DataFormatDimMapOp<CPUDevice, T>);\nTF_CALL_int32(REGISTER_KERNEL);\nTF_CALL_int64(REGISTER_KERNEL);\n#undef REGISTER_KERNEL\n\n#define REGISTER_KERNEL(T)                                                    \\\n  REGISTER_KERNEL_BUILDER(                                                    \\\n      Name(\"DataFormatVecPermute\").Device(DEVICE_CPU).TypeConstraint<T>(\"T\"), \\\n      DataFormatVecPermuteOp<CPUDevice, T>);\nTF_CALL_int32(REGISTER_KERNEL);\nTF_CALL_int64(REGISTER_KERNEL);\n#undef REGISTER_KERNEL\n\n#define REGISTER_KERNEL(T)                             \\\n  REGISTER_KERNEL_BUILDER(Name(\"DataFormatDimMap\")     \\\n                              .Device(DEVICE_CPU)      \\\n                              .Label(\"host\")           \\\n                              .TypeConstraint<T>(\"T\"), \\\n                          DataFormatDimMapOp<CPUDevice, T>);\nTF_CALL_int32(REGISTER_KERNEL);\nTF_CALL_int64(REGISTER_KERNEL);\n#undef REGISTER_KERNEL\n\n#define REGISTER_KERNEL(T)                             \\\n  REGISTER_KERNEL_BUILDER(Name(\"DataFormatVecPermute\") \\\n                              .Device(DEVICE_CPU)      \\\n                              .Label(\"host\")           \\\n                              .TypeConstraint<T>(\"T\"), \\\n                          DataFormatVecPermuteOp<CPUDevice, T>);\nTF_CALL_int32(REGISTER_KERNEL);\nTF_CALL_int64(REGISTER_KERNEL);\n#undef REGISTER_KERNEL\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n// Forward declarations of the functor specializations for GPU.\nnamespace functor {\n#define DECLARE_GPU_SPEC(T)                                    \\\n  template <>                                                  \\\n  void DataFormatDimMap<GPUDevice, T>::operator()(             \\\n      const GPUDevice& d, typename TTypes<T>::ConstFlat x,     \\\n      typename TTypes<T>::Flat y, const TTypes<int>::Vec dst); \\\n  extern template struct DataFormatDimMap<GPUDevice, T>;\n#define DECLARE_GPU_SPECS(T) DECLARE_GPU_SPEC(T);\nTF_CALL_int32(DECLARE_GPU_SPECS);\nTF_CALL_int64(DECLARE_GPU_SPECS);\n#undef DECLARE_GPU_SPEC\n\n#define DECLARE_GPU_SPEC(T)                                \\\n  template <>                                              \\\n  void DataFormatVecPermute<GPUDevice, T>::operator()(     \\\n      const GPUDevice& d, typename TTypes<T>::ConstFlat x, \\\n      typename TTypes<T>::Vec y,                           \\\n      const Eigen::DSizes<Eigen::DenseIndex, 8>& dst_idx); \\\n  extern template struct DataFormatVecPermute<GPUDevice, T>;\n#define DECLARE_GPU_SPECS(T) DECLARE_GPU_SPEC(T);\nTF_CALL_int32(DECLARE_GPU_SPECS);\nTF_CALL_int64(DECLARE_GPU_SPECS);\n#undef DECLARE_GPU_SPEC\n}  // namespace functor\n\n// Registration of the GPU implementations.\n#define REGISTER_GPU_KERNEL(T)                                            \\\n  REGISTER_KERNEL_BUILDER(                                                \\\n      Name(\"DataFormatDimMap\").Device(DEVICE_GPU).TypeConstraint<T>(\"T\"), \\\n      DataFormatDimMapOp<GPUDevice, T>);                                  \\\n  REGISTER_KERNEL_BUILDER(Name(\"DataFormatDimMap\")                        \\\n                              .Device(DEVICE_GPU)                         \\\n                              .HostMemory(\"x\")                            \\\n                              .HostMemory(\"y\")                            \\\n                              .Label(\"host\")                              \\\n                              .TypeConstraint<T>(\"T\"),                    \\\n                          DataFormatDimMapOp<CPUDevice, T>);\nTF_CALL_int32(REGISTER_GPU_KERNEL);\nTF_CALL_int64(REGISTER_GPU_KERNEL);\n#undef REGISTER_GPU_KERNEL\n\n#define REGISTER_GPU_KERNEL(T)                                                \\\n  REGISTER_KERNEL_BUILDER(                                                    \\\n      Name(\"DataFormatVecPermute\").Device(DEVICE_GPU).TypeConstraint<T>(\"T\"), \\\n      DataFormatVecPermuteOp<GPUDevice, T>);                                  \\\n  REGISTER_KERNEL_BUILDER(Name(\"DataFormatVecPermute\")                        \\\n                              .Device(DEVICE_GPU)                             \\\n                              .HostMemory(\"x\")                                \\\n                              .HostMemory(\"y\")                                \\\n                              .Label(\"host\")                                  \\\n                              .TypeConstraint<T>(\"T\"),                        \\\n                          DataFormatVecPermuteOp<CPUDevice, T>);\nTF_CALL_int32(REGISTER_GPU_KERNEL);\nTF_CALL_int64(REGISTER_GPU_KERNEL);\n#undef REGISTER_GPU_KERNEL\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n}  // namespace tensorflow\n", "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for miscellaneous functionality in tensorflow.ops.nn.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\n\nfrom absl.testing import parameterized\nimport numpy as np\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\n\nfrom tensorflow.python.eager import def_function\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import tensor_spec\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import gradient_checker\nfrom tensorflow.python.ops import gradient_checker_v2\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import nn\nfrom tensorflow.python.ops import nn_impl\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.ops import partitioned_variables\nfrom tensorflow.python.ops import variable_scope\nfrom tensorflow.python.ops import variables\nimport tensorflow.python.ops.nn_grad  # pylint: disable=unused-import\nfrom tensorflow.python.ops.nn_impl import _compute_sampled_logits\nfrom tensorflow.python.ops.ragged import ragged_factory_ops\nfrom tensorflow.python.platform import test as test_lib\n\n\nclass ZeroFractionTest(test_lib.TestCase):\n\n  def _ZeroFraction(self, x):\n    assert x.shape\n    total_elements = np.prod(x.shape)\n    nonzeros = np.count_nonzero(x.flatten())\n    return 1.0 - nonzeros / total_elements\n\n  @test_util.run_deprecated_v1\n  def testZeroFraction(self):\n    x_shape = [5, 17]\n    x_np = np.random.randint(0, 2, size=x_shape).astype(np.float32)\n    y_np = self._ZeroFraction(x_np)\n\n    x_tf = constant_op.constant(x_np)\n    x_tf.set_shape(x_shape)\n    y_tf = nn_impl.zero_fraction(x_tf)\n    y_tf_np = self.evaluate(y_tf)\n\n    eps = 1e-8\n    self.assertAllClose(y_tf_np, y_np, eps)\n\n  @test_util.run_deprecated_v1\n  def testZeroFractionEmpty(self):\n    x = np.zeros(0)\n    y = self.evaluate(nn_impl.zero_fraction(x))\n    self.assertTrue(np.isnan(y))\n\n  @test_util.run_deprecated_v1\n  def testZeroFraction2_27Zeros(self):\n    sparsity = nn_impl.zero_fraction(\n        array_ops.zeros([int(2**27 * 1.01)], dtype=dtypes.int8))\n    self.assertAllClose(1.0, self.evaluate(sparsity))\n\n  @test_util.run_deprecated_v1\n  def testZeroFraction2_27Ones(self):\n    sparsity = nn_impl.zero_fraction(\n        array_ops.ones([int(2**27 * 1.01)], dtype=dtypes.int8))\n    self.assertAllClose(0.0, self.evaluate(sparsity))\n\n  @test_util.run_deprecated_v1\n  def testUnknownSize(self):\n    value = array_ops.placeholder(dtype=dtypes.float32)\n    sparsity = nn_impl.zero_fraction(value)\n    with self.cached_session() as sess:\n      self.assertAllClose(\n          0.25,\n          sess.run(sparsity, {value: [[0., 1.], [0.3, 2.]]}))\n\n\nclass SoftmaxTest(test_lib.TestCase, parameterized.TestCase):\n\n  def _softmax(self, x):\n    assert len(x.shape) == 2\n    m = x.max(1)[:, np.newaxis]\n    u = np.exp(x - m)\n    z = u.sum(1)[:, np.newaxis]\n    return u / z\n\n  @test_util.run_in_graph_and_eager_modes\n  def testSoftmax(self):\n    x_shape = [5, 10]\n    x_np = np.random.randn(*x_shape).astype(np.float32)\n    y_np = self._softmax(x_np)\n    x_tf = constant_op.constant(x_np)\n    y_tf = nn_ops.softmax_v2(x_tf)\n    y_tf_last_dim = nn_ops.softmax_v2(x_tf, 1)\n    y_tf_np = self.evaluate(y_tf)\n    y_tf_last_dim_np = self.evaluate(y_tf_last_dim)\n    eps = 1e-3\n    self.assertAllClose(y_tf_np, y_np, eps)\n    self.assertAllClose(y_tf_last_dim_np, y_np, eps)\n\n  def testSoftmaxAxes(self):\n    arr = np.linspace(0., 1, 12).reshape(3, 4)\n    x_neg_axis = nn_ops.softmax_v2(arr, axis=-2)\n    y_pos_axis = nn_ops.softmax_v2(arr, axis=0)\n    z_gt_axis = nn_ops.softmax_v2(arr, axis=0)\n    x_neg_axis_tf = self.evaluate(x_neg_axis)\n    y_pos_axis_tf = self.evaluate(y_pos_axis)\n    z_gt_axis_tf = self.evaluate(z_gt_axis)\n    eps = 1e-3\n    self.assertAllClose(x_neg_axis_tf, y_pos_axis_tf, eps)\n    self.assertAllClose(y_pos_axis_tf, z_gt_axis_tf, eps)\n\n  def testSoftmaxExtendType(self):\n    x_shape = [5, 10]\n    x_np = np.random.randn(*x_shape).astype(np.float32)\n\n    x_f32_tf = constant_op.constant(x_np)\n    x_bf16_tf = math_ops.cast(x_f32_tf, dtypes.bfloat16)\n    y_f32_tf = self.evaluate(nn_ops.softmax(x_f32_tf))\n    y_bf16_tf = self.evaluate(nn_ops.softmax(x_bf16_tf))\n    expected = math_ops.cast(y_f32_tf, dtypes.bfloat16)\n    tol = x_shape[1] * 1e-3\n    self.assertAllClose(y_bf16_tf, expected, rtol=tol, atol=tol)\n\n  @parameterized.parameters(((5, 10),), ((2, 3, 4),))\n  @test_util.run_deprecated_v1\n  def testGradient(self, x_shape):\n    x_np = np.random.randn(*x_shape).astype(np.float64)\n    with self.cached_session():\n      x_tf = constant_op.constant(x_np)\n      y_tf = nn_ops.softmax_v2(x_tf)\n      err = gradient_checker.compute_gradient_error(x_tf, x_shape, y_tf,\n                                                    x_shape)\n    eps = 2e-8\n    self.assertLess(err, eps)\n\n\nclass LogPoissonLossTest(test_lib.TestCase):\n\n  def _log_poisson_loss(self, x, z, compute_full_loss=False):\n    lpl = np.exp(x) - z * x\n    if compute_full_loss:\n      stirling_approx = z * np.log(z) - z + 0.5 * np.log(2. * np.pi * z)\n      lpl += np.ma.masked_array(stirling_approx, mask=(z <= 1)).filled(0.)\n    return lpl\n\n  @test_util.run_in_graph_and_eager_modes\n  def testLogPoissonLoss(self):\n    x_shape = [5, 10]\n    x_np = np.random.randn(*x_shape).astype(np.float32)\n    z_np = np.random.randint(0, 5, size=x_shape).astype(np.float32)\n    y_np = self._log_poisson_loss(x_np, z_np, compute_full_loss=False)\n    y_np_stirling = self._log_poisson_loss(x_np, z_np, compute_full_loss=True)\n    y_tf = nn_impl.log_poisson_loss(z_np, x_np, compute_full_loss=False)\n    y_tf_stirling = nn_impl.log_poisson_loss(z_np, x_np, compute_full_loss=True)\n    y_tf_np = self.evaluate(y_tf)\n    y_tf_np_stirling = self.evaluate(y_tf_stirling)\n    eps = 1e-3\n    self.assertAllClose(y_tf_np, y_np, eps)\n    self.assertAllClose(y_tf_np_stirling, y_np_stirling, eps)\n\n  @test_util.run_deprecated_v1\n  def testGradient(self):\n    x_shape = [5, 10]\n    x_np = np.random.randn(*x_shape).astype(np.float64)\n    z_np = np.random.randint(0, 5, size=x_shape).astype(np.float64)\n    with self.cached_session():\n      x_tf = constant_op.constant(x_np)\n      y_tf = nn_impl.log_poisson_loss(z_np, x_tf, compute_full_loss=False)\n      y_tf_stirling = nn_impl.log_poisson_loss(\n          z_np, x_tf, compute_full_loss=True)\n      err = gradient_checker.compute_gradient_error(x_tf, x_shape, y_tf,\n                                                    x_shape)\n      err_stirling = gradient_checker.compute_gradient_error(\n          x_tf, x_shape, y_tf_stirling, x_shape)\n    eps = 1e-6\n    self.assertLess(err, eps)\n    self.assertLess(err_stirling, eps)\n\n\nclass LogSoftmaxTest(test_lib.TestCase, parameterized.TestCase):\n\n  def _log_softmax(self, x):\n    assert len(x.shape) == 2\n    m = x.max(1)[:, np.newaxis]\n    u = x - m\n    return u - np.log(np.sum(np.exp(u), 1, keepdims=True))\n\n  @test_util.run_in_graph_and_eager_modes\n  def testLogSoftmax(self):\n    x_shape = [5, 10]\n    x_np = np.random.randn(*x_shape).astype(np.float32)\n    y_np = self._log_softmax(x_np)\n    x_tf = constant_op.constant(x_np)\n    y_tf = nn_ops.log_softmax_v2(x_tf)\n    y_tf_np = self.evaluate(y_tf)\n    eps = 1e-3\n    self.assertAllClose(y_tf_np, y_np, eps)\n\n  def testLogSoftmaxAxes(self):\n    arr = np.linspace(0., 1, 12).reshape(3, 4)\n    x_neg_axis = nn_ops.log_softmax_v2(arr, axis=-2)\n    y_pos_axis = nn_ops.log_softmax_v2(arr, axis=0)\n    z_gt_axis = nn_ops.log_softmax_v2(arr, axis=0)\n    x_neg_axis_tf = self.evaluate(x_neg_axis)\n    y_pos_axis_tf = self.evaluate(y_pos_axis)\n    z_gt_axis_tf = self.evaluate(z_gt_axis)\n    eps = 1e-3\n    self.assertAllClose(x_neg_axis_tf, y_pos_axis_tf, eps)\n    self.assertAllClose(y_pos_axis_tf, z_gt_axis_tf, eps)\n\n  @parameterized.parameters(((5, 10),), ((2, 3, 4),))\n  @test_util.run_deprecated_v1\n  def testGradient(self, x_shape):\n    x_np = np.random.randn(*x_shape).astype(np.float64)\n    with self.cached_session():\n      x_tf = constant_op.constant(x_np)\n      y_tf = nn_ops.log_softmax_v2(x_tf)\n      err = gradient_checker.compute_gradient_error(x_tf, x_shape, y_tf,\n                                                    x_shape)\n    eps = 1e-7\n    self.assertLess(err, eps)\n\n\nclass L2LossTest(test_lib.TestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def testL2Loss(self):\n    for dtype in [dtypes.float32, dtypes.float64]:\n      x = constant_op.constant(\n          [1.0, 0.0, 3.0, 2.0], shape=[2, 2], name=\"x\", dtype=dtype)\n      l2loss = nn_ops.l2_loss(x)\n      value = self.evaluate(l2loss)\n      self.assertAllClose(7.0, value)\n\n  @test_util.run_deprecated_v1\n  def testGradient(self):\n    x_shape = [20, 7, 3]\n    np.random.seed(1)  # Make it reproducible.\n    x_val = np.random.random_sample(x_shape).astype(np.float64)\n    with self.cached_session():\n      x = constant_op.constant(x_val, name=\"x\")\n      output = nn_ops.l2_loss(x)\n      err = gradient_checker.compute_gradient_error(x, x_shape, output, [1])\n    print(\"L2Loss gradient err = %g \" % err)\n    err_tolerance = 1e-10\n    self.assertLess(err, err_tolerance)\n\n\nclass L2NormalizeTest(test_lib.TestCase):\n\n  def _l2Normalize(self, x, dim):\n    if isinstance(dim, list):\n      norm = np.linalg.norm(x, axis=tuple(dim))\n      for d in dim:\n        norm = np.expand_dims(norm, d)\n      return x / norm\n    else:\n      norm = np.apply_along_axis(np.linalg.norm, dim, x)\n      return x / np.expand_dims(norm, dim)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testL2Normalize(self):\n    x_shape = [20, 7, 3]\n    np.random.seed(1)\n    x_np = np.random.random_sample(x_shape).astype(np.float32)\n    for dim in range(len(x_shape)):\n      y_np = self._l2Normalize(x_np, dim)\n      x_tf = constant_op.constant(x_np, name=\"x\")\n      y_tf = nn_impl.l2_normalize_v2(x_tf, dim)\n      self.assertAllClose(y_np, self.evaluate(y_tf))\n\n  @test_util.run_in_graph_and_eager_modes\n  def testL2NormalizeDimArray(self):\n    x_shape = [20, 7, 3]\n    np.random.seed(1)\n    x_np = np.random.random_sample(x_shape).astype(np.float32)\n    dim = [1, 2]\n    y_np = self._l2Normalize(x_np, dim)\n    x_tf = constant_op.constant(x_np, name=\"x\")\n    y_tf = nn_impl.l2_normalize_v2(x_tf, dim)\n    self.assertAllClose(y_np, self.evaluate(y_tf))\n\n  @test_util.run_deprecated_v1\n  def testL2NormalizeGradient(self):\n    x_shape = [20, 7, 3]\n    np.random.seed(1)\n    x_np = np.random.random_sample(x_shape).astype(np.float64)\n    for dim in range(len(x_shape)):\n      with self.cached_session():\n        x_tf = constant_op.constant(x_np, name=\"x\")\n        y_tf = nn_impl.l2_normalize_v2(x_tf, dim)\n        err = gradient_checker.compute_gradient_error(x_tf, x_shape, y_tf,\n                                                      x_shape)\n      print(\"L2Normalize gradient err = %g \" % err)\n      self.assertLess(err, 1e-4)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testL2NormalizeComplex(self):\n    x_shape = [20, 7, 3]\n    for dtype in [np.complex64, np.complex128]:\n      np.random.seed(1)\n      x_np = (\n          np.random.random_sample(x_shape).astype(dtype) +\n          np.random.random_sample(x_shape).astype(dtype) * 1j)\n      for dim in range(len(x_shape)):\n        y_np = self._l2Normalize(x_np, dim)\n        x_tf = constant_op.constant(x_np, name=\"x\")\n        y_tf = nn_impl.l2_normalize_v2(x_tf, dim)\n        self.assertAllClose(y_np, self.evaluate(y_tf))\n\n\nclass DropoutTest(test_lib.TestCase):\n\n  def testDropout(self):\n    # Runs dropout with 0-1 tensor 10 times, sum the number of ones and validate\n    # that it is producing approximately the right number of ones over a large\n    # number of samples, based on the keep probability.\n    x_dim = 40\n    y_dim = 30\n    num_iter = 10\n    for keep_prob in [0.1, 0.5, 0.8]:\n      t = constant_op.constant(1.0, shape=[x_dim, y_dim], dtype=dtypes.float32)\n      dropout = nn_ops.dropout(t, rate=(1 - keep_prob))\n      final_count = 0\n      self.assertEqual([x_dim, y_dim], dropout.get_shape())\n      for _ in xrange(0, num_iter):\n        value = self.evaluate(dropout)\n        final_count += np.count_nonzero(value)\n        # Verifies that there are only two values: 0 and 1/keep_prob.\n        sorted_value = np.unique(np.sort(value))\n        self.assertEqual(0, sorted_value[0])\n        self.assertAllClose(1 / keep_prob, sorted_value[1])\n\n      # Check that we are in the 15% error range\n      expected_count = x_dim * y_dim * keep_prob * num_iter\n      rel_error = math.fabs(final_count - expected_count) / expected_count\n      print(rel_error)\n      self.assertTrue(rel_error < 0.15)\n\n  def testShapedDropout(self):\n    # Runs dropout with 0-1 tensor 10 times, sum the number of ones and validate\n    # that it is producing approximately the right number of ones over a large\n    # number of samples, based on the keep probability. This time with shaped\n    # noise.\n    x_dim = 40 * 30\n    y_dim = 3\n    num_iter = 10\n    for keep_prob in [0.1, 0.5, 0.8]:\n      t = constant_op.constant(1.0, shape=[x_dim, y_dim], dtype=dtypes.float32)\n      dropout = nn_ops.dropout(t, rate=(1 - keep_prob), noise_shape=[x_dim, 1])\n      self.assertEqual([x_dim, y_dim], dropout.get_shape())\n      final_count = 0\n      for _ in xrange(0, num_iter):\n        value = self.evaluate(dropout)\n        final_count += np.count_nonzero(value)\n        # Verifies that there are only two values: 0 and 1/keep_prob.\n        sorted_value = np.unique(np.sort(value))\n        self.assertEqual(0, sorted_value[0])\n        self.assertAllClose(1 / keep_prob, sorted_value[1])\n\n      # Check that we are in the 15% error range\n      expected_count = x_dim * y_dim * keep_prob * num_iter\n      rel_error = math.fabs(final_count - expected_count) / expected_count\n      print(rel_error)\n      self.assertTrue(rel_error < 0.15)\n\n  def testShapedDropoutCorrelation(self):\n    # Runs a shaped dropout and tests that the correlations are correct.\n    x_dim = 40\n    y_dim = 30\n    num_iter = 10\n    for keep_prob in [0.1, 0.5, 0.8]:\n      t = constant_op.constant(1.0, shape=[x_dim, y_dim], dtype=dtypes.float32)\n      dropout = nn_ops.dropout(t, rate=(1 - keep_prob), noise_shape=[x_dim, 1])\n      self.assertEqual([x_dim, y_dim], dropout.get_shape())\n      for _ in xrange(0, num_iter):\n        value = self.evaluate(dropout)\n        # Verifies that each y column as only one type of activation.\n        for i in xrange(x_dim):\n          sorted_value = np.unique(np.sort(value[i, :]))\n          self.assertEqual(sorted_value.size, 1)\n\n  @test_util.run_deprecated_v1\n  def testDropoutPlaceholderKeepProb(self):\n    # Runs dropout with 0-1 tensor 10 times, sum the number of ones and validate\n    # that it is producing approximately the right number of ones over a large\n    # number of samples, based on the keep probability.\n    x_dim = 40\n    y_dim = 30\n    num_iter = 10\n    for keep_prob in [0.1, 0.5, 0.8]:\n      with self.cached_session():\n        t = constant_op.constant(\n            1.0, shape=[x_dim, y_dim], dtype=dtypes.float32)\n        keep_prob_placeholder = array_ops.placeholder(dtypes.float32)\n        dropout = nn_ops.dropout(t, keep_prob_placeholder)\n        final_count = 0\n        self.assertEqual([x_dim, y_dim], dropout.get_shape())\n        for _ in xrange(0, num_iter):\n          value = dropout.eval(feed_dict={keep_prob_placeholder: keep_prob})\n          final_count += np.count_nonzero(value)\n          # Verifies that there are only two values: 0 and 1/keep_prob.\n          sorted_value = np.unique(np.sort(value))\n          self.assertEqual(0, sorted_value[0])\n          self.assertAllClose(1 / keep_prob, sorted_value[1])\n      # Check that we are in the 15% error range\n      expected_count = x_dim * y_dim * keep_prob * num_iter\n      rel_error = math.fabs(final_count - expected_count) / expected_count\n      print(rel_error)\n      self.assertTrue(rel_error < 0.15)\n\n  @test_util.run_deprecated_v1\n  def testShapedDropoutUnknownShape(self):\n    x_dim = 40\n    y_dim = 30\n    keep_prob = 0.5\n    x = constant_op.constant(1.0, shape=[x_dim, y_dim], dtype=dtypes.float32)\n    dropout_x = nn_ops.dropout(\n        x,\n        rate=(1 - keep_prob),\n        noise_shape=array_ops.placeholder(dtypes.int32))\n    self.assertEqual(x.get_shape(), dropout_x.get_shape())\n\n  def testPartialShapedDropout(self):\n    x_dim = 40 * 30\n    y_dim = 3\n    num_iter = 10\n    for keep_prob in [0.1, 0.5, 0.8]:\n      t = constant_op.constant(1.0, shape=[x_dim, y_dim], dtype=dtypes.float32)\n      # Set noise_shape=[None, 1] which means [x_dim, 1].\n      dropout = nn_ops.dropout(t, rate=(1 - keep_prob), noise_shape=[None, 1])\n      self.assertEqual([x_dim, y_dim], dropout.get_shape())\n      final_count = 0\n      for _ in xrange(0, num_iter):\n        value = self.evaluate(dropout)\n        final_count += np.count_nonzero(value)\n        # Verifies that there are only two values: 0 and 1/keep_prob.\n        sorted_value = np.unique(np.sort(value))\n        self.assertEqual(0, sorted_value[0])\n        self.assertAllClose(1 / keep_prob, sorted_value[1])\n\n      # Check that we are in the 15% error range\n      expected_count = x_dim * y_dim * keep_prob * num_iter\n      rel_error = math.fabs(final_count - expected_count) / expected_count\n      print(rel_error)\n      self.assertTrue(rel_error < 0.15)\n\n  @test_util.run_deprecated_v1\n  def testInvalidKeepProb(self):\n    x_dim = 40\n    y_dim = 30\n    t = constant_op.constant(1.0, shape=[x_dim, y_dim], dtype=dtypes.float32)\n    with self.assertRaises(ValueError):\n      nn_ops.dropout(t, -1.0)\n    with self.assertRaises(ValueError):\n      nn_ops.dropout(t, 1.1)\n    with self.assertRaises(ValueError):\n      nn_ops.dropout(t, [0.0, 1.0])\n    with self.assertRaises(ValueError):\n      nn_ops.dropout(t, array_ops.placeholder(dtypes.float64))\n    with self.assertRaises(ValueError):\n      nn_ops.dropout(t, array_ops.placeholder(dtypes.float32, shape=[2]))\n\n  @test_util.run_deprecated_v1\n  def testInvalidRate(self):\n    x_dim = 40\n    y_dim = 30\n    t = constant_op.constant(1.0, shape=[x_dim, y_dim], dtype=dtypes.float32)\n    with self.assertRaises(ValueError):\n      nn_ops.dropout_v2(t, -1.0)\n    with self.assertRaises(ValueError):\n      nn_ops.dropout_v2(t, 1.1)\n    with self.assertRaises(ValueError):\n      nn_ops.dropout_v2(t, [0.0, 1.0])\n\n  def testLargeRate(self):\n    x_dim = 40\n    y_dim = 30\n    t = constant_op.constant(1.0, shape=[x_dim, y_dim], dtype=dtypes.float32)\n    _ = nn_ops.dropout_v2(t, 0.9)\n\n  def testVariableRef(self):\n    x = variable_scope.get_variable(\"x\", shape=[10, 10], dtype=dtypes.float32)\n    _ = nn_ops.dropout(x, keep_prob=0.1)\n\n  @test_util.run_deprecated_v1\n  def testShapedDropoutShapeError(self):\n    # Runs shaped dropout and verifies an error is thrown on misshapen noise.\n    x_dim = 40\n    y_dim = 30\n    keep_prob = 0.5\n    t = constant_op.constant(1.0, shape=[x_dim, y_dim], dtype=dtypes.float32)\n    with self.assertRaises(ValueError):\n      _ = nn_ops.dropout(\n          t, rate=(1 - keep_prob), noise_shape=[x_dim, y_dim + 10])\n    with self.assertRaises(ValueError):\n      _ = nn_ops.dropout(t, rate=(1 - keep_prob), noise_shape=[x_dim, y_dim, 5])\n    with self.assertRaises(ValueError):\n      _ = nn_ops.dropout(t, rate=(1 - keep_prob), noise_shape=[x_dim + 3])\n    with self.assertRaises(ValueError):\n      _ = nn_ops.dropout(t, rate=(1 - keep_prob), noise_shape=[x_dim])\n    # test that broadcasting proceeds\n    _ = nn_ops.dropout(t, rate=(1 - keep_prob), noise_shape=[y_dim])\n    _ = nn_ops.dropout(t, rate=(1 - keep_prob), noise_shape=[1, y_dim])\n    _ = nn_ops.dropout(t, rate=(1 - keep_prob), noise_shape=[x_dim, 1])\n    _ = nn_ops.dropout(t, rate=(1 - keep_prob), noise_shape=[1, 1])\n\n  def testNoDropout(self):\n    x = array_ops.zeros((5,))\n    y = nn_ops.dropout(x, rate=0)\n    self.assertAllEqual(x, y)\n\n    y = nn_ops.dropout_v2(x, rate=0)\n    self.assertAllEqual(x, y)\n\n  def testDropoutWithIntegerInputs(self):\n    x = constant_op.constant([1, 1, 1, 1, 1])\n    with self.assertRaises(ValueError):\n      _ = nn_ops.dropout(x, 0.5)\n\n\n@test_util.run_all_without_tensor_float_32(\n    \"Tests _compute_sampled_logits and related functions, which call matmul\")\nclass ComputeSampledLogitsTest(test_lib.TestCase):\n\n  def setUp(self):\n    self._eps = 1e-3\n\n  def _GenerateTestData(self, num_classes, dim, batch_size, num_true, labels,\n                        sampled, subtract_log_q):\n    \"\"\"Randomly generates input/output data for a single test case.\n\n    This function returns numpy constants for use in a test case.\n\n    Args:\n      num_classes: An int. The number of embedding classes in the test case.\n      dim: An int. The dimension of the embedding.\n      batch_size: An int. The batch size.\n      num_true: An int. The number of target classes per training example.\n      labels: A list of batch_size * num_true ints. The target classes.\n      sampled: A list of indices in [0, num_classes).\n      subtract_log_q: A bool corresponding to the parameter in\n          _compute_sampled_logits().\n\n    Returns:\n      weights: Embedding weights to use as test input. It is a numpy array\n          of shape [num_classes, dim]\n      biases: Embedding biases to use as test input. It is a numpy array\n          of shape [num_classes].\n      hidden_acts: Forward activations of the network to use as test input.\n          It is a numpy array of shape [batch_size, dim].\n      sampled_vals: A tuple based on `sampled` to use as test input in the\n          format returned by a *_candidate_sampler function.\n      exp_logits: The output logits expected from _compute_sampled_logits().\n          It is a numpy array of shape [batch_size, num_true + len(sampled)].\n      exp_labels: The output labels expected from _compute_sampled_logits().\n          It is a numpy array of shape [batch_size, num_true + len(sampled)].\n    \"\"\"\n    weights = np.random.randn(num_classes, dim).astype(np.float32)\n    biases = np.random.randn(num_classes).astype(np.float32)\n    hidden_acts = np.random.randn(batch_size, dim).astype(np.float32)\n\n    true_exp = np.full([batch_size, 1], fill_value=0.5, dtype=np.float32)\n    sampled_exp = np.full([len(sampled)], fill_value=0.5, dtype=np.float32)\n    sampled_vals = (sampled, true_exp, sampled_exp)\n\n    sampled_w, sampled_b = weights[sampled], biases[sampled]\n    true_w, true_b = weights[labels], biases[labels]\n\n    true_logits = np.sum(\n        hidden_acts.reshape((batch_size, 1, dim)) * true_w.reshape(\n            (batch_size, num_true, dim)),\n        axis=2)\n    true_b = true_b.reshape((batch_size, num_true))\n    true_logits += true_b\n    sampled_logits = np.dot(hidden_acts, sampled_w.T) + sampled_b\n\n    if subtract_log_q:\n      true_logits -= np.log(true_exp)\n      sampled_logits -= np.log(sampled_exp[np.newaxis, :])\n\n    exp_logits = np.concatenate([true_logits, sampled_logits], axis=1)\n    exp_labels = np.hstack((np.ones_like(true_logits) / num_true,\n                            np.zeros_like(sampled_logits)))\n\n    return weights, biases, hidden_acts, sampled_vals, exp_logits, exp_labels\n\n  def _ShardTestEmbeddings(self, weights, biases, num_shards):\n    \"\"\"Shards the weights and biases returned by _GenerateTestData.\n\n    Args:\n      weights: The weights returned by _GenerateTestData.\n      biases: The biases returned by _GenerateTestData.\n      num_shards: The number of shards to create.\n\n    Returns:\n      sharded_weights: A list of size `num_shards` containing all the weights.\n      sharded_biases: A list of size `num_shards` containing all the biases.\n    \"\"\"\n    with ops.Graph().as_default() as g:\n      sharded_weights = variable_scope.get_variable(\n          \"w\",\n          partitioner=partitioned_variables.fixed_size_partitioner(num_shards),\n          initializer=constant_op.constant(weights))\n      sharded_biases = variable_scope.get_variable(\n          \"b\",\n          partitioner=partitioned_variables.fixed_size_partitioner(num_shards),\n          initializer=constant_op.constant(biases))\n      with self.session(graph=g) as sess:\n        self.evaluate(variables.global_variables_initializer())\n        return self.evaluate([list(sharded_weights), list(sharded_biases)])\n\n  def testShapes(self):\n    np.random.seed(0)\n    num_classes = 5\n    batch_size = 3\n\n    for num_true in range(1, 5):\n      labels = np.random.randint(\n          low=0, high=num_classes, size=batch_size * num_true)\n      (weights, biases, hidden_acts, sampled_vals, exp_logits,\n       exp_labels) = self._GenerateTestData(\n           num_classes=num_classes,\n           dim=10,\n           batch_size=batch_size,\n           num_true=num_true,\n           labels=labels,\n           sampled=[1, 0, 2, 3],\n           subtract_log_q=False)\n      logits_tensor, labels_tensor = _compute_sampled_logits(\n          weights=constant_op.constant(weights),\n          biases=constant_op.constant(biases),\n          labels=constant_op.constant(\n              labels, dtype=dtypes.int64, shape=(batch_size, num_true)),\n          inputs=constant_op.constant(hidden_acts),\n          num_sampled=4,\n          num_classes=num_classes,\n          num_true=num_true,\n          sampled_values=sampled_vals,\n          subtract_log_q=False,\n          remove_accidental_hits=False,\n          partition_strategy=\"div\",\n          name=\"sampled_logits_basic_num_true_%d\" % num_true)\n      got_logits, got_labels = self.evaluate([logits_tensor, labels_tensor])\n      self.assertEqual(exp_logits.shape, got_logits.shape, self._eps)\n      self.assertEqual(exp_labels.shape, got_labels.shape, self._eps)\n\n  def testBasic(self):\n    \"\"\"Without accidental hit removal or subtract_log_q.\"\"\"\n    np.random.seed(0)\n    num_classes = 5\n    batch_size = 3\n\n    for num_true in range(1, 5):\n      labels = np.random.randint(\n          low=0, high=num_classes, size=batch_size * num_true)\n      (weights, biases, hidden_acts, sampled_vals, exp_logits,\n       exp_labels) = self._GenerateTestData(\n           num_classes=num_classes,\n           dim=10,\n           batch_size=batch_size,\n           num_true=num_true,\n           labels=labels,\n           sampled=[1, 0, 2, 3],\n           subtract_log_q=False)\n      logits_tensor, labels_tensor = _compute_sampled_logits(\n          weights=constant_op.constant(weights),\n          biases=constant_op.constant(biases),\n          labels=constant_op.constant(\n              labels, dtype=dtypes.int64, shape=(batch_size, num_true)),\n          inputs=constant_op.constant(hidden_acts),\n          num_sampled=4,\n          num_classes=num_classes,\n          num_true=num_true,\n          sampled_values=sampled_vals,\n          subtract_log_q=False,\n          remove_accidental_hits=False,\n          partition_strategy=\"div\",\n          name=\"sampled_logits_basic_num_true_%d\" % num_true)\n      got_logits, got_labels = self.evaluate([logits_tensor, labels_tensor])\n      self.assertAllClose(exp_logits, got_logits, self._eps)\n      self.assertAllClose(exp_labels, got_labels, self._eps)\n\n  def testAccidentalHitRemoval(self):\n    \"\"\"With accidental hit removal, no subtract_log_q.\"\"\"\n    np.random.seed(0)\n    num_classes = 5\n    batch_size = 3\n    sampled = [1, 0, 2, 3]\n\n    for num_true in range(1, 5):\n      labels = np.random.randint(\n          low=0, high=num_classes, size=batch_size * num_true)\n      (weights, biases, hidden_acts, sampled_vals, _,\n       _) = self._GenerateTestData(\n           num_classes=num_classes,\n           dim=10,\n           batch_size=batch_size,\n           num_true=num_true,\n           labels=labels,\n           sampled=sampled,\n           subtract_log_q=False)\n      logits_tensor, _ = _compute_sampled_logits(\n          weights=constant_op.constant(weights),\n          biases=constant_op.constant(biases),\n          labels=constant_op.constant(\n              labels, dtype=dtypes.int64, shape=(batch_size, num_true)),\n          inputs=constant_op.constant(hidden_acts),\n          num_sampled=len(sampled),\n          num_classes=num_classes,\n          num_true=num_true,\n          sampled_values=sampled_vals,\n          subtract_log_q=False,\n          remove_accidental_hits=True,\n          partition_strategy=\"div\",\n          name=\"sampled_logits_accidental_hit_removal_num_true_%d\" % num_true)\n      # Test that the exponentiated logits of accidental hits are near 0.\n      # First we need to find the hits in this random test run:\n      labels_reshape = labels.reshape((batch_size, num_true))\n      got_logits = self.evaluate(logits_tensor)\n      for row in xrange(batch_size):\n        row_labels = labels_reshape[row, :]\n        for col in xrange(len(sampled)):\n          if sampled[col] in row_labels:\n            # We need to add the num_true_test offset into logits_*\n            self.assertNear(\n                np.exp(got_logits[row, col + num_true]), 0., self._eps)\n\n  def testSubtractLogQ(self):\n    \"\"\"With subtract_log_q, no accidental hit removal.\"\"\"\n    np.random.seed(0)\n    num_classes = 5\n    batch_size = 3\n\n    for num_true in range(1, 5):\n      labels = np.random.randint(\n          low=0, high=num_classes, size=batch_size * num_true)\n      (weights, biases, hidden_acts, sampled_vals, exp_logits,\n       exp_labels) = self._GenerateTestData(\n           num_classes=num_classes,\n           dim=10,\n           batch_size=batch_size,\n           num_true=num_true,\n           labels=labels,\n           sampled=[1, 0, 2, 3],\n           subtract_log_q=True)\n      logits_tensor, labels_tensor = _compute_sampled_logits(\n          weights=constant_op.constant(weights),\n          biases=constant_op.constant(biases),\n          labels=constant_op.constant(\n              labels, dtype=dtypes.int64, shape=(batch_size, num_true)),\n          inputs=constant_op.constant(hidden_acts),\n          num_sampled=4,\n          num_classes=num_classes,\n          num_true=num_true,\n          sampled_values=sampled_vals,\n          subtract_log_q=True,\n          remove_accidental_hits=False,\n          partition_strategy=\"div\",\n          name=\"sampled_logits_subtract_log_q_num_true_%d\" % num_true)\n      got_logits, got_labels = self.evaluate([logits_tensor, labels_tensor])\n      self.assertAllClose(exp_logits, got_logits, self._eps)\n      self.assertAllClose(exp_labels, got_labels, self._eps)\n\n  def testSharded(self):\n    \"\"\"With sharded weights and sharded biases.\"\"\"\n    np.random.seed(0)\n    num_classes = 5\n    batch_size = 3\n\n    for num_true in range(1, 5):\n      labels = np.random.randint(\n          low=0, high=num_classes, size=batch_size * num_true)\n      (weights, biases, hidden_acts, sampled_vals, exp_logits,\n       exp_labels) = self._GenerateTestData(\n           num_classes=num_classes,\n           dim=10,\n           batch_size=batch_size,\n           num_true=num_true,\n           labels=labels,\n           sampled=[1, 0, 2, 3],\n           subtract_log_q=False)\n      weight_shards, bias_shards = self._ShardTestEmbeddings(\n          weights, biases, num_shards=3)\n      logits_tensor, labels_tensor = _compute_sampled_logits(\n          weights=[constant_op.constant(shard) for shard in weight_shards],\n          biases=[constant_op.constant(shard) for shard in bias_shards],\n          labels=constant_op.constant(\n              labels, dtype=dtypes.int64, shape=(batch_size, num_true)),\n          inputs=constant_op.constant(hidden_acts),\n          num_sampled=4,\n          num_classes=num_classes,\n          num_true=num_true,\n          sampled_values=sampled_vals,\n          subtract_log_q=False,\n          remove_accidental_hits=False,\n          partition_strategy=\"div\",\n          name=\"sampled_logits_sharded_num_true_%d\" % num_true)\n      got_logits, got_labels = self.evaluate([logits_tensor, labels_tensor])\n      self.assertAllClose(exp_logits, got_logits, self._eps)\n      self.assertAllClose(exp_labels, got_labels, self._eps)\n\n  def testNCELoss(self):\n    # A simple test to verify the numerics.\n\n    def _SigmoidCrossEntropyWithLogits(logits, targets):\n      # logits, targets: float arrays of the same shape.\n      assert logits.shape == targets.shape\n      pred = 1. / (1. + np.exp(-logits))\n      eps = 0.0001\n      pred = np.minimum(np.maximum(pred, eps), 1 - eps)\n      return -targets * np.log(pred) - (1. - targets) * np.log(1. - pred)\n\n    np.random.seed(0)\n    num_classes = 5\n    batch_size = 3\n    labels = [0, 1, 2]\n    (weights, biases, hidden_acts, sampled_vals, exp_logits,\n     exp_labels) = self._GenerateTestData(\n         num_classes=num_classes,\n         dim=10,\n         batch_size=batch_size,\n         num_true=1,\n         labels=labels,\n         sampled=[1, 0, 2, 3],\n         subtract_log_q=True)\n    exp_nce_loss = np.sum(\n        _SigmoidCrossEntropyWithLogits(exp_logits, exp_labels), 1)\n\n    got_nce_loss = nn_impl.nce_loss_v2(\n        weights=constant_op.constant(weights),\n        biases=constant_op.constant(biases),\n        labels=constant_op.constant(labels, shape=(batch_size, 1)),\n        inputs=constant_op.constant(hidden_acts),\n        num_sampled=4,\n        num_classes=num_classes,\n        num_true=1,\n        sampled_values=sampled_vals)\n\n    self.assertAllClose(exp_nce_loss, self.evaluate(got_nce_loss), 1e-4)\n\n    # Test with sharded weights and sharded biases.\n    weight_shards, bias_shards = self._ShardTestEmbeddings(\n        weights, biases, num_shards=3)\n    got_nce_loss = nn_impl.nce_loss_v2(\n        weights=[constant_op.constant(shard) for shard in weight_shards],\n        biases=[constant_op.constant(shard) for shard in bias_shards],\n        labels=constant_op.constant(labels, shape=(batch_size, 1)),\n        inputs=constant_op.constant(hidden_acts),\n        num_sampled=4,\n        num_classes=num_classes,\n        num_true=1,\n        sampled_values=sampled_vals)\n\n    self.assertAllClose(exp_nce_loss, self.evaluate(got_nce_loss), 1e-4)\n\n  def testSampledSoftmaxLoss(self):\n    # A simple test to verify the numerics.\n\n    def _SoftmaxCrossEntropyWithLogits(logits, targets):\n      # logits, targets: float arrays of the same shape.\n      assert logits.shape == targets.shape\n      stable_exp_logits = np.exp(\n          logits - np.amax(logits, axis=1, keepdims=True))\n      pred = stable_exp_logits / np.sum(stable_exp_logits, 1, keepdims=True)\n      return -np.sum(targets * np.log(pred + 1.0e-20), axis=1)\n\n    np.random.seed(0)\n    num_classes = 5\n    batch_size = 3\n    labels = [0, 1, 2]\n    (weights, biases, hidden_acts, sampled_vals, exp_logits,\n     exp_labels) = self._GenerateTestData(\n         num_classes=num_classes,\n         dim=10,\n         batch_size=batch_size,\n         num_true=1,\n         labels=labels,\n         sampled=[1, 0, 2, 3],\n         subtract_log_q=True)\n    exp_sampled_softmax_loss = _SoftmaxCrossEntropyWithLogits(\n        exp_logits, exp_labels)\n\n    got_sampled_softmax_loss = nn_impl.sampled_softmax_loss_v2(\n        weights=constant_op.constant(weights),\n        biases=constant_op.constant(biases),\n        labels=constant_op.constant(labels, shape=(batch_size, 1)),\n        inputs=constant_op.constant(hidden_acts),\n        num_sampled=4,\n        num_classes=num_classes,\n        num_true=1,\n        sampled_values=sampled_vals,\n        remove_accidental_hits=False)\n\n    self.assertAllClose(exp_sampled_softmax_loss,\n                        self.evaluate(got_sampled_softmax_loss), 1e-4)\n\n    # Test with sharded weights and sharded biases.\n    weight_shards, bias_shards = self._ShardTestEmbeddings(\n        weights, biases, num_shards=3)\n    got_sampled_softmax_loss = nn_impl.sampled_softmax_loss_v2(\n        weights=[constant_op.constant(shard) for shard in weight_shards],\n        biases=[constant_op.constant(shard) for shard in bias_shards],\n        labels=constant_op.constant(labels, shape=(batch_size, 1)),\n        inputs=constant_op.constant(hidden_acts),\n        num_sampled=4,\n        num_classes=num_classes,\n        num_true=1,\n        sampled_values=sampled_vals,\n        remove_accidental_hits=False)\n\n    self.assertAllClose(exp_sampled_softmax_loss,\n                        self.evaluate(got_sampled_softmax_loss), 1e-4)\n\n  def testSampledSoftmaxLossBf16(self):\n    # A simple test to verify the numerics for bfloat16.\n    def _SoftmaxCrossEntropyWithLogits(logits, targets):\n      # logits, targets: float arrays of the same shape.\n      assert logits.shape == targets.shape\n      stable_exp_logits = np.exp(\n          logits - np.amax(logits, axis=1, keepdims=True))\n      pred = stable_exp_logits / np.sum(stable_exp_logits, 1, keepdims=True)\n      return -np.sum(targets * np.log(pred + 1.0e-20), axis=1)\n\n    np.random.seed(0)\n    num_classes = 5\n    batch_size = 3\n    labels = [0, 1, 2]\n    sampled = [1, 0, 2, 3]\n    (weights, biases, hidden_acts, _, exp_logits,\n     exp_labels) = self._GenerateTestData(\n         num_classes=num_classes,\n         dim=10,\n         batch_size=batch_size,\n         num_true=1,\n         labels=labels,\n         sampled=sampled,\n         subtract_log_q=True)\n    exp_sampled_softmax_loss = _SoftmaxCrossEntropyWithLogits(\n        exp_logits, exp_labels)\n\n    true_exp_bf16 = np.full([batch_size, 1],\n                            fill_value=0.5,\n                            dtype=dtypes.bfloat16.as_numpy_dtype)\n    sampled_exp_bf16 = np.full([len(sampled)],\n                               fill_value=0.5,\n                               dtype=dtypes.bfloat16.as_numpy_dtype)\n    sampled_vals_bf16 = (sampled, true_exp_bf16, sampled_exp_bf16)\n\n    got_sampled_softmax_loss = math_ops.cast(\n        nn_impl.sampled_softmax_loss_v2(\n            weights=constant_op.constant(weights, dtype=dtypes.bfloat16),\n            biases=constant_op.constant(biases, dtype=dtypes.bfloat16),\n            labels=constant_op.constant(\n                labels, shape=(batch_size, 1), dtype=dtypes.bfloat16),\n            inputs=constant_op.constant(hidden_acts, dtype=dtypes.bfloat16),\n            num_sampled=4,\n            num_classes=num_classes,\n            num_true=1,\n            sampled_values=sampled_vals_bf16,\n            remove_accidental_hits=False), dtypes.float32)\n\n    self.assertAllClose(exp_sampled_softmax_loss,\n                        self.evaluate(got_sampled_softmax_loss), 1e-1)\n\n\nclass CReluTest(test_lib.TestCase):\n\n  def test(self):\n    np.random.seed(1)  # Make it reproducible.\n    x = np.random.randn(3, 4).astype(np.float32)\n    y = np.concatenate([x * (x > 0), -x * (x < 0)], axis=1)\n\n    z = self.evaluate(nn_ops.crelu(constant_op.constant(x)))\n    self.assertAllClose(y, z, 1e-4)\n\n\nclass ReluTest(test_lib.TestCase):\n\n  def test(self):\n    np.random.seed(1)  # Make it reproducible.\n    x = np.random.randn(3, 4).astype(np.float32)\n    y = np.maximum(x, 0.0)\n\n    z = self.evaluate(nn_ops.relu(constant_op.constant(x)))\n    self.assertAllEqual(y, z)\n\n  @test_util.disable_xla(\n      \"This test relies on undefined behavior that XLA does not replicate\")\n  @test_util.run_deprecated_v1\n  def testNaNs(self):\n    # Test that relu(nan) = nan for various sizes.\n    for i in range(18):\n      x = np.zeros(i) + np.nan\n      with self.cached_session():\n        z = nn_ops.relu(constant_op.constant(x)).eval()\n        self.assertTrue(np.isnan(z).all())\n\n\nclass LeakyReluTest(test_lib.TestCase):\n\n  def testRange(self):\n    batch_size = 3\n    height, width = 4, 4\n    np.random.seed(1)  # Make it reproducible.\n    inputs = np.random.uniform(size=(batch_size, height, width, 3)).astype(\n        np.float32)\n    inputs = constant_op.constant(inputs)\n\n    outputs = nn_ops.leaky_relu(inputs)\n    self.assertEqual(inputs.shape, outputs.shape)\n\n    inputs, outputs = self.evaluate([inputs, outputs])\n\n    self.assertGreaterEqual(outputs.min(), 0.0)\n    self.assertLessEqual(outputs.max(), 1.0)\n    self.assertAllClose(inputs, outputs)\n\n  @test_util.run_deprecated_v1\n  def testValues(self):\n    for dtype in [np.int32, np.int64, np.float16, np.float32, np.float64]:\n      np_values = np.array([-2, -1, 0, 1, 2], dtype=dtype)\n      outputs = nn_ops.leaky_relu(constant_op.constant(np_values))\n\n      outputs = self.evaluate(outputs)\n\n      tol = 2e-3 if dtype == np.float16 else 1e-6\n      self.assertAllClose(\n          outputs, [-0.4, -0.2, 0.0, 1.0, 2.0], rtol=tol, atol=tol)\n\n  @test_util.run_deprecated_v1\n  def testName(self):\n    np_values = np.array([-2, -1, 0, 1, 2], dtype=np.float64)\n    outputs_with_name_set = nn_ops.leaky_relu(\n        constant_op.constant(np_values),\n        name='test_relu_op')\n    self.assertEqual(outputs_with_name_set.name, 'test_relu_op:0')\n    outputs_without_name_set = nn_ops.leaky_relu(\n        constant_op.constant(np_values))\n    self.assertEqual(outputs_without_name_set.name, 'LeakyRelu:0')\n\n\nclass GeluTest(test_lib.TestCase):\n\n  def test(self):\n\n    def gelu(x, approximate=False):\n      if approximate:\n        return 0.5 * x * (1.0 + np.tanh(\n            np.sqrt(2.0 / np.pi) * (x + 0.044715 * np.power(x, 3))))\n      else:\n        from scipy.stats import norm  # pylint: disable=g-import-not-at-top\n        return x * norm.cdf(x)\n\n    np.random.seed(1)  # Make it reproducible.\n    x = np.random.randn(3, 4).astype(np.float32)\n    y = gelu(x)\n    z = self.evaluate(nn_ops.gelu(x))\n    self.assertAllClose(y, z)\n\n    y = gelu(x, True)\n    z = self.evaluate(nn_ops.gelu(x, True))\n    self.assertAllClose(y, z)\n\n\nclass SwishTest(test_lib.TestCase):\n\n  @test_util.run_deprecated_v1\n  def testValues(self):\n    np_values = np.array(\n        [np.linspace(-7.0, 0.0, 100),\n         np.linspace(0.0, 7.0, 100)],\n        dtype=np.float32)\n    tf_values = constant_op.constant(np_values)\n    actual_tf_outputs = nn_impl.swish(tf_values)\n    expected_tf_outputs = tf_values * math_ops.sigmoid(tf_values)\n\n    actual_outputs, expected_outputs = self.evaluate(\n        [actual_tf_outputs, expected_tf_outputs])\n\n    self.assertAllClose(actual_outputs, expected_outputs)\n\n  @test_util.run_deprecated_v1\n  def testGradients(self):\n    shape = [5, 3, 4]\n    sigma = 5\n    input_values = np.random.randn(*shape) * sigma\n    x_tf = constant_op.constant(input_values)\n    y_tf = nn_impl.swish(x_tf)\n    with self.cached_session():\n      err = gradient_checker.compute_gradient_error(x_tf, shape, y_tf, shape)\n    self.assertLess(err, 1e-4)\n\n\nclass MomentsTest(test_lib.TestCase):\n\n  def doOutputTest(self,\n                   input_shape,\n                   moments_axes,\n                   tol=1e-4,\n                   check_gradients=False):\n    for mu in [0.0, 1.0, 1e3]:\n      for sigma in [1.0, 0.1]:\n        for keep_dims in [True, False]:\n          input_values = np.random.rand(*input_shape) * sigma + mu\n          expected_mean = np.mean(\n              input_values, axis=moments_axes, keepdims=keep_dims)\n          expected_var = np.var(\n              input_values, axis=moments_axes, keepdims=keep_dims)\n          with ops.Graph().as_default() as g:\n            with self.session(graph=g) as sess:\n              inputs = constant_op.constant(\n                  input_values, shape=input_shape, dtype=dtypes.float32)\n              mean, variance = nn_impl.moments_v2(\n                  inputs, moments_axes, keepdims=keep_dims)\n\n              if check_gradients:\n                err = gradient_checker.compute_gradient_error(\n                    inputs, input_shape, mean, mean.shape.as_list())\n                self.assertLess(err, 1e-3)\n                err = gradient_checker.compute_gradient_error(\n                    inputs, input_shape, variance, variance.shape.as_list())\n                self.assertLess(err, 1e-3)\n\n              # Evaluate.\n              [mean, variance] = self.evaluate([mean, variance])\n              # Make sure that there are no NaNs\n              self.assertFalse(np.isnan(mean).any())\n              self.assertFalse(np.isnan(variance).any())\n              self.assertAllClose(mean, expected_mean, rtol=tol, atol=tol)\n              self.assertAllClose(variance, expected_var, rtol=tol, atol=tol)\n\n  def testOutputAndGradient2DInput0(self):\n    self.doOutputTest((10, 10), (0,), check_gradients=True)\n\n  def testOutputAndGradient2DInput01(self):\n    self.doOutputTest((10, 10), (0, 1), check_gradients=True)\n\n  def testOutput2DInput0(self):\n    self.doOutputTest((10, 300), (0,))\n\n  def testOutput2DInput1(self):\n    self.doOutputTest((10, 300), (1,))\n\n  def testOutput2DInput01(self):\n    self.doOutputTest((10, 300), (0, 1))\n\n  def testOutput4DInput0(self):\n    self.doOutputTest((10, 10, 10, 30), (0,))\n\n  def testOutput4DInput1(self):\n    self.doOutputTest((10, 10, 10, 30), (1,))\n\n  def testOutput4DInput3(self):\n    self.doOutputTest((10, 10, 10, 30), (3,))\n\n  def testOutput4DInput012(self):\n    self.doOutputTest((10, 10, 10, 30), (0, 1, 2))\n\n  def testOutput4DInput123(self):\n    self.doOutputTest((10, 10, 10, 30), (1, 2, 3))\n\n\nclass DataFormatDimMapTest(test_lib.TestCase):\n\n  def _test(self, x_val, y_val_expected):\n    x = constant_op.constant(x_val)\n    y = nn_ops.data_format_dim_map(x)\n\n    y_val = self.evaluate(y)\n    self.assertAllEqual(y_val, y_val_expected)\n\n  def test(self):\n    self._test(0, 0)\n    self._test(1, 2)\n    self._test(2, 3)\n    self._test(3, 1)\n    self._test(-1, 1)\n    self._test(-2, 3)\n    self._test(-3, 2)\n    self._test(-4, 0)\n    self._test([1, 3], [2, 1])\n    self._test([1, 3, -2], [2, 1, 3])\n    self._test([1, -3, -2], [2, 2, 3])\n    self._test([[1, -3], [1, -1]], [[2, 2], [2, 1]])\n\n  def testNHWCtoNCHW(self):\n    x_val = [1, -3, -2]\n    y_val_expected = [2, 2, 3]\n    x = constant_op.constant(x_val)\n    y = nn_ops.data_format_dim_map(x, src_format=\"NHWC\", dst_format=\"NCHW\")\n    with test_util.use_gpu():\n      y_val = self.evaluate(y)\n      self.assertAllEqual(y_val, y_val_expected)\n\n  def testNHWCtoHWNC(self):\n    x_val = [-4, -3, -2, -1, 0, 1, 2, 3]\n    y_val_expected = [2, 0, 1, 3, 2, 0, 1, 3]\n    x = constant_op.constant(x_val)\n    y = nn_ops.data_format_dim_map(x, src_format=\"NHWC\", dst_format=\"HWNC\")\n    with test_util.use_gpu():\n      y_val = self.evaluate(y)\n      self.assertAllEqual(y_val, y_val_expected)\n\n  def testNHWCtoWHCN(self):\n    x_val = [-4, -3, -2, -1, 0, 1, 2, 3]\n    y_val_expected = [3, 1, 0, 2, 3, 1, 0, 2]\n    x = constant_op.constant(x_val)\n    y = nn_ops.data_format_dim_map(x, src_format=\"NHWC\", dst_format=\"WHCN\")\n    with test_util.use_gpu():\n      y_val = self.evaluate(y)\n      self.assertAllEqual(y_val, y_val_expected)\n\n  def testNDHWCtoNCDHW(self):\n    x_val = [1, -4, -3, -2]\n    y_val_expected = [2, 2, 3, 4]\n    x = constant_op.constant(x_val)\n    y = nn_ops.data_format_dim_map(x, src_format=\"NDHWC\", dst_format=\"NCDHW\")\n    with test_util.use_gpu():\n      y_val = self.evaluate(y)\n      self.assertAllEqual(y_val, y_val_expected)\n\n  def testNDHWCtoDHWNC(self):\n    x_val = [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4]\n    y_val_expected = [3, 0, 1, 2, 4, 3, 0, 1, 2, 4]\n    x = constant_op.constant(x_val)\n    y = nn_ops.data_format_dim_map(x, src_format=\"NDHWC\", dst_format=\"DHWNC\")\n    with test_util.use_gpu():\n      y_val = self.evaluate(y)\n      self.assertAllEqual(y_val, y_val_expected)\n\n  def testDNHWCtoWHDCN(self):\n    x_val = [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4]\n    y_val_expected = [4, 2, 1, 0, 3, 4, 2, 1, 0, 3]\n    x = constant_op.constant(x_val)\n    y = nn_ops.data_format_dim_map(x, src_format=\"NDHWC\", dst_format=\"WHDCN\")\n    with test_util.use_gpu():\n      y_val = self.evaluate(y)\n      self.assertAllEqual(y_val, y_val_expected)\n\n  @test_util.disable_xla(\"XLA catches the error and rethrows as different one\")\n  def testArbitraryASCII(self):\n    x_val = [-4, -3, -2, -1, 0, 1, 2, 3]\n    y_val_expected = [3, 2, 1, 0, 3, 2, 1, 0]\n    x = constant_op.constant(x_val)\n    y = nn_ops.data_format_dim_map(x, src_format=\"qwer\", dst_format=\"rewq\")\n    with test_util.use_gpu():\n      y_val = self.evaluate(y)\n      self.assertAllEqual(y_val, y_val_expected)\n\n  @test_util.disable_xla(\"XLA catches the error and rethrows as different one\")\n  def testInvalidLength(self):\n    x = [-4, -3, -2, -1, 0, 1, 2, 3]\n    with self.assertRaisesRegex(errors.InvalidArgumentError,\n                                \"Source format must be of length 4 or 5\"):\n      op = nn_ops.data_format_dim_map(\n          x, src_format=\"12345678\", dst_format=\"87654321\")\n      with test_util.use_gpu():\n        self.evaluate(op)\n\n  @test_util.disable_xla(\"XLA catches the error and rethrows as different one\")\n  def testDuplicateSrc(self):\n    x = [-4, -3, -2, -1, 0, 1, 2, 3]\n    with self.assertRaisesRegex(\n        errors.InvalidArgumentError,\n        \"Destination and source format must determine a permutation\"):\n      op = nn_ops.data_format_dim_map(x, src_format=\"1233\", dst_format=\"4321\")\n      with test_util.use_gpu():\n        self.evaluate(op)\n\n  @test_util.disable_xla(\"XLA catches the error and rethrows as different one\")\n  def testDuplicateDst(self):\n    x = [-4, -3, -2, -1, 0, 1, 2, 3]\n    with self.assertRaisesRegex(\n        errors.InvalidArgumentError,\n        \"Destination and source format must determine a permutation\"):\n      op = nn_ops.data_format_dim_map(x, src_format=\"1234\", dst_format=\"3321\")\n      with test_util.use_gpu():\n        self.evaluate(op)\n\n  @test_util.disable_xla(\"XLA catches the error and rethrows as different one\")\n  def testExtraSpecifiers(self):\n    x = [-4, -3, -2, -1, 0, 1, 2, 3]\n    with self.assertRaisesRegex(\n        errors.InvalidArgumentError,\n        \"Destination and source format must determine a permutation\"):\n      op = nn_ops.data_format_dim_map(x, src_format=\"1234\", dst_format=\"5321\")\n      with test_util.use_gpu():\n        self.evaluate(op)\n\n\nclass DataFormatVectorPermuteTest(test_lib.TestCase):\n\n  def testNHWCToNCHW(self):\n    x_val = [7, 4, 9, 3]\n    x = constant_op.constant(x_val)\n    y = nn_ops.data_format_vec_permute(x)\n    with test_util.use_gpu():\n      y_val = self.evaluate(y)\n      self.assertAllEqual(y_val, [7, 3, 4, 9])\n\n  def testNHWCToNCHW_Size2(self):\n    x_val = [4, 9]\n    x = constant_op.constant(x_val)\n    y = nn_ops.data_format_vec_permute(x)\n    with test_util.use_gpu():\n      y_val = self.evaluate(y)\n      self.assertAllEqual(y_val, [4, 9])\n\n  @test_util.disable_xla(\"unsupported data format\")\n  def testNHWCToWHCN(self):\n    x_val = [7, 4, 9, 3]\n    x = constant_op.constant(x_val)\n    y = nn_ops.data_format_vec_permute(x, src_format=\"NHWC\", dst_format=\"WHCN\")\n    with test_util.use_gpu():\n      y_val = self.evaluate(y)\n      self.assertAllEqual(y_val, [9, 4, 3, 7])\n\n  @test_util.disable_xla(\"unsupported data format\")\n  def testNHWCToWHCN_Size2(self):\n    x_val = [4, 9]\n    x = constant_op.constant(x_val)\n    y = nn_ops.data_format_vec_permute(x, src_format=\"NHWC\", dst_format=\"WHCN\")\n    with test_util.use_gpu():\n      y_val = self.evaluate(y)\n      self.assertAllEqual(y_val, [9, 4])\n\n  def testNCHWToNHWC(self):\n    x_val = [7, 4, 9, 3]\n    x = constant_op.constant(x_val)\n    y = nn_ops.data_format_vec_permute(x, src_format=\"NCHW\", dst_format=\"NHWC\")\n    with test_util.use_gpu():\n      y_val = self.evaluate(y)\n      self.assertAllEqual(y_val, [7, 9, 3, 4])\n\n  def testNCHWToNHWC_Size2(self):\n    x_val = [9, 3]\n    x = constant_op.constant(x_val)\n    y = nn_ops.data_format_vec_permute(x)\n    with test_util.use_gpu():\n      y_val = self.evaluate(y)\n      self.assertAllEqual(y_val, [9, 3])\n\n  def testNHWCToHWNC(self):\n    x_val = [7, 4, 9, 3]\n    x = constant_op.constant(x_val)\n    y = nn_ops.data_format_vec_permute(x, src_format=\"NHWC\", dst_format=\"HWNC\")\n    with test_util.use_gpu():\n      y_val = self.evaluate(y)\n      self.assertAllEqual(y_val, [4, 9, 7, 3])\n\n  def testHWNCToNHWC(self):\n    x_val = [7, 4, 9, 3]\n    x = constant_op.constant(x_val)\n    y = nn_ops.data_format_vec_permute(x, src_format=\"HWNC\", dst_format=\"NHWC\")\n    with test_util.use_gpu():\n      y_val = self.evaluate(y)\n      self.assertAllEqual(y_val, [9, 7, 4, 3])\n\n  def testNHWCToNCHW2D(self):\n    x_val = [[7, 4], [9, 3], [4, 5], [5, 1]]\n    x = constant_op.constant(x_val)\n    y = nn_ops.data_format_vec_permute(x)\n    with test_util.use_gpu():\n      y_val = self.evaluate(y)\n      self.assertAllEqual(y_val, [[7, 4], [5, 1], [9, 3], [4, 5]])\n\n  def testNHWCToHWNC2D(self):\n    x_val = [[7, 4], [9, 3], [4, 5], [5, 1]]\n    x = constant_op.constant(x_val)\n    y = nn_ops.data_format_vec_permute(x, src_format=\"NHWC\", dst_format=\"HWNC\")\n    with test_util.use_gpu():\n      y_val = self.evaluate(y)\n      self.assertAllEqual(y_val, [[9, 3], [4, 5], [7, 4], [5, 1]])\n\n  def testHWNCToNHWC2D(self):\n    x_val = [[7, 4], [9, 3], [4, 5], [5, 1]]\n    x = constant_op.constant(x_val)\n    y = nn_ops.data_format_vec_permute(x, src_format=\"HWNC\", dst_format=\"NHWC\")\n    with test_util.use_gpu():\n      y_val = self.evaluate(y)\n      self.assertAllEqual(y_val, [[4, 5], [7, 4], [9, 3], [5, 1]])\n\n  def testNCHWToNHWC2D(self):\n    x_val = [[7, 4], [9, 3], [4, 5], [5, 1]]\n    x = constant_op.constant(x_val)\n    y = nn_ops.data_format_vec_permute(x, src_format=\"NCHW\", dst_format=\"NHWC\")\n    with test_util.use_gpu():\n      y_val = self.evaluate(y)\n      self.assertAllEqual(y_val, [[7, 4], [4, 5], [5, 1], [9, 3]])\n\n  @test_util.disable_xla(\"XLA catches the error and rethrows as different one\")\n  def testInvalidLength(self):\n    x = [0, 1, 2, 3]\n    with self.assertRaisesRegex(errors.InvalidArgumentError,\n                                \"Source format must be of length 4 or 5\"):\n      op = nn_ops.data_format_vec_permute(\n          x, src_format=\"12345678\", dst_format=\"87654321\")\n      with test_util.use_gpu():\n        self.evaluate(op)\n\n  @test_util.disable_xla(\"XLA catches the error and rethrows as different one\")\n  def testDuplicateSrc(self):\n    x = [0, 1, 2, 3]\n    with self.assertRaisesRegex(\n        errors.InvalidArgumentError,\n        \"Destination and source format must determine a permutation\"):\n      op = nn_ops.data_format_vec_permute(\n          x, src_format=\"1233\", dst_format=\"4321\")\n      with test_util.use_gpu():\n        self.evaluate(op)\n\n  @test_util.disable_xla(\"XLA catches the error and rethrows as different one\")\n  def testDuplicateDst(self):\n    x = [0, 1, 2, 3]\n    with self.assertRaisesRegex(\n        errors.InvalidArgumentError,\n        \"Destination and source format must determine a permutation\"):\n      op = nn_ops.data_format_vec_permute(\n          x, src_format=\"1234\", dst_format=\"3321\")\n      with test_util.use_gpu():\n        self.evaluate(op)\n\n  @test_util.disable_xla(\"XLA catches the error and rethrows as different one\")\n  def testExtraSpecifiers(self):\n    x = [0, 1, 2, 3]\n    with self.assertRaisesRegex(\n        errors.InvalidArgumentError,\n        \"Destination and source format must determine a permutation\"):\n      op = nn_ops.data_format_vec_permute(\n          x, src_format=\"1234\", dst_format=\"5321\")\n      with test_util.use_gpu():\n        self.evaluate(op)\n\n  @test_util.disable_xla(\"XLA catches the error and rethrows as different one\")\n  def test2DNoWH(self):\n    x = [[0, 1], [2, 3]]\n    with self.assertRaisesRegex(\n        errors.InvalidArgumentError,\n        \"Format specifier must contain H and W for 2D case\"):\n      op = nn_ops.data_format_vec_permute(\n          x, src_format=\"1234\", dst_format=\"4321\")\n      with test_util.use_gpu():\n        self.evaluate(op)\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass AvgPoolTest(test_lib.TestCase):\n\n  def test1DTensor(self):\n    x = array_ops.ones([3, 6, 5])\n    ksize = 2\n    strides = 2\n\n    y1 = nn_ops.avg_pool_v2(x, ksize, strides, \"SAME\")\n    y2 = nn_ops.avg_pool1d(x, ksize, strides, \"SAME\")\n\n    self.assertAllEqual(self.evaluate(y1), self.evaluate(y2))\n\n  def test1DNumpy(self):\n    # explicitly use float32 for ROCm, as MIOpen does not yet support float64\n    # np.ones defaults to using float64 when dtype is not explicitly specified\n    dtype = np.float32 if test_lib.is_built_with_rocm() else np.float64\n    x = np.ones([3, 6, 5], dtype=dtype)\n    ksize = 2\n    strides = 2\n\n    y1 = nn_ops.avg_pool_v2(x, ksize, strides, \"SAME\")\n    y2 = nn_ops.avg_pool1d(x, ksize, strides, \"SAME\")\n\n    self.assertAllEqual(self.evaluate(y1), self.evaluate(y2))\n\n  def test1DNumpyWithGolden(self):\n    dtype = np.float32 if test_lib.is_built_with_rocm() else np.float64\n    x = np.array([[[3], [6], [5]],\n                  [[1], [0], [1]]], dtype=dtype)\n    ksize = 2\n    strides = 1\n    y = nn_ops.avg_pool1d(x, ksize, strides, \"SAME\")\n    expected_y = np.array([[[4.5], [5.5], [5.0]],\n                           [[0.5], [0.5], [1.0]]], dtype=dtype)\n    self.assertAllEqual(self.evaluate(y), expected_y)\n\n  def test2DTensor(self):\n    x = array_ops.ones([3, 6, 6, 5])\n    ksize = 2\n    strides = 2\n\n    y1 = nn_ops.avg_pool_v2(x, ksize, strides, \"SAME\")\n    y2 = nn_ops.avg_pool(x, ksize, strides, \"SAME\")\n\n    self.assertAllEqual(self.evaluate(y1), self.evaluate(y2))\n\n  def test2DNumpy(self):\n    # explicitly use float32 for ROCm, as MIOpen does not yet support float64\n    # np.ones defaults to using float64 when dtype is not explicitly specified\n    dtype = np.float32 if test_lib.is_built_with_rocm() else np.float64\n    x = np.ones([3, 6, 6, 5], dtype=dtype)\n    ksize = 2\n    strides = 2\n\n    y1 = nn_ops.avg_pool_v2(x, ksize, strides, \"SAME\")\n    y2 = nn_ops.avg_pool(x, ksize, strides, \"SAME\")\n\n    self.assertAllEqual(self.evaluate(y1), self.evaluate(y2))\n\n  def test3DTensor(self):\n    if test_lib.is_built_with_rocm():\n      self.skipTest(\"Pooling with 3D tensors is not supported in ROCm\")\n    x = array_ops.ones([3, 7, 6, 6, 5])\n    ksize = 2\n    strides = 2\n\n    y1 = nn_ops.avg_pool_v2(x, ksize, strides, \"SAME\")\n    y2 = nn_ops.avg_pool3d(x, ksize, strides, \"SAME\")\n\n    self.assertAllEqual(self.evaluate(y1), self.evaluate(y2))\n\n  def test3DNumpy(self):\n    if test_lib.is_built_with_rocm():\n      self.skipTest(\"Pooling with 3D tensors is not supported in ROCm\")\n    x = np.ones([3, 7, 6, 6, 5], dtype=np.float32)\n    ksize = 2\n    strides = 2\n\n    y1 = nn_ops.avg_pool_v2(x, ksize, strides, \"SAME\")\n    y2 = nn_ops.avg_pool3d(x, ksize, strides, \"SAME\")\n\n    self.assertAllEqual(self.evaluate(y1), self.evaluate(y2))\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass MaxPoolTest(test_lib.TestCase):\n\n  def test1DTensor(self):\n    x = array_ops.ones([3, 6, 5])\n    ksize = 2\n    strides = 2\n\n    y1 = nn_ops.max_pool_v2(x, ksize, strides, \"SAME\")\n    y2 = nn_ops.max_pool1d(x, ksize, strides, \"SAME\")\n\n    self.assertAllEqual(self.evaluate(y1), self.evaluate(y2))\n\n  def test1DNumpy(self):\n    # explicitly use float32 for ROCm, as MIOpen does not yet support float64\n    # np.ones defaults to using float64 when dtype is not explicitly specified\n    dtype = np.float32 if test_lib.is_built_with_rocm() else np.float64\n    x = np.ones([3, 6, 5], dtype=dtype)\n    ksize = 2\n    strides = 2\n\n    y1 = nn_ops.max_pool_v2(x, ksize, strides, \"SAME\")\n    y2 = nn_ops.max_pool1d(x, ksize, strides, \"SAME\")\n\n    self.assertAllEqual(self.evaluate(y1), self.evaluate(y2))\n\n  def test1DNumpyWithGolden(self):\n    dtype = np.float32 if test_lib.is_built_with_rocm() else np.float64\n    x = np.array([[[3], [6], [5]],\n                  [[1], [0], [1]]], dtype=dtype)\n    ksize = 2\n    strides = 1\n    y = nn_ops.max_pool1d(x, ksize, strides, \"SAME\")\n    expected_y = np.array([[[6], [6], [5]],\n                           [[1], [1], [1]]], dtype=dtype)\n    self.assertAllEqual(self.evaluate(y), expected_y)\n\n  def test2DTensor(self):\n    x = array_ops.ones([3, 6, 6, 5])\n    ksize = 2\n    strides = 2\n\n    y1 = nn_ops.max_pool_v2(x, ksize, strides, \"SAME\")\n    y2 = nn_ops.max_pool(x, ksize, strides, \"SAME\")\n\n    self.assertAllEqual(self.evaluate(y1), self.evaluate(y2))\n\n  def test2DNumpy(self):\n    # explicitly use float32 for ROCm, as MIOpen does not yet support float64\n    # np.ones defaults to using float64 when dtype is not explicitly specified\n    dtype = np.float32 if test_lib.is_built_with_rocm() else np.float64\n    x = np.ones([3, 6, 6, 5], dtype=dtype)\n    ksize = 2\n    strides = 2\n\n    y1 = nn_ops.max_pool_v2(x, ksize, strides, \"SAME\")\n    y2 = nn_ops.max_pool(x, ksize, strides, \"SAME\")\n\n    self.assertAllEqual(self.evaluate(y1), self.evaluate(y2))\n\n  def test3DTensor(self):\n    if test_lib.is_built_with_rocm():\n      self.skipTest(\"Pooling with 3D tensors is not supported in ROCm\")\n    x = array_ops.ones([3, 7, 6, 6, 5])\n    ksize = 2\n    strides = 2\n\n    y1 = nn_ops.max_pool_v2(x, ksize, strides, \"SAME\")\n    y2 = nn_ops.max_pool3d(x, ksize, strides, \"SAME\")\n\n    self.assertAllEqual(self.evaluate(y1), self.evaluate(y2))\n\n  def test3DNumpy(self):\n    if test_lib.is_built_with_rocm():\n      self.skipTest(\"Pooling with 3D tensors is not supported in ROCm\")\n    x = np.ones([3, 7, 6, 6, 5], dtype=np.float32)\n    ksize = 2\n    strides = 2\n\n    y1 = nn_ops.max_pool_v2(x, ksize, strides, \"SAME\")\n    y2 = nn_ops.max_pool3d(x, ksize, strides, \"SAME\")\n\n    self.assertAllEqual(self.evaluate(y1), self.evaluate(y2))\n\n  def testIncorrectSizeInputSmall(self):\n    x = array_ops.ones([3, 4])\n    with self.assertRaisesRegex(\n        ValueError, \"Input tensor must be of rank 3, 4 or 5 but was 2.\"):\n      nn_ops.max_pool_v2(x, 2, 2, \"SAME\")\n\n  def testIncorrectSizeInput(self):\n    x = array_ops.ones([3, 4, 1, 2, 1, 2])\n    with self.assertRaisesRegex(\n        ValueError, \"Input tensor must be of rank 3, 4 or 5 but was 6.\"):\n      nn_ops.max_pool_v2(x, 2, 2, \"SAME\")\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass ConvolutionTest(test_lib.TestCase):\n\n  def testUnknownSize(self):\n    x = tensor_spec.TensorSpec(None, dtypes.float32, name=\"x\")\n    k = np.ones([3, 6, 6, 5], dtype=np.float32)\n\n    @def_function.function\n    def F(value):\n      return nn_ops.convolution(value, k, \"SAME\")\n\n    F.get_concrete_function(x)\n\n\nclass ConvTransposeTest(test_lib.TestCase):\n\n  def test1D(self):\n    t = array_ops.ones([2, 4, 3])\n    v = array_ops.ones([2, 5, 3])\n    strides = 2\n\n    y1 = nn_ops.conv1d_transpose(t, v, [2, 8, 5], strides)\n    y2 = nn_ops.conv_transpose(t, v, [2, 8, 5], strides)\n\n    self.assertAllEqual(self.evaluate(y1), self.evaluate(y2))\n\n  def test1DTensor(self):\n    t = array_ops.ones([2, 4, 3])\n    v = array_ops.ones([2, 5, 3])\n    strides = 2\n\n    y1 = nn_ops.conv1d_transpose(t, v, [2, 8, 5], strides)\n    y2 = nn_ops.conv_transpose(t, v, constant_op.constant([2, 8, 5]), strides)\n\n    self.assertAllEqual(self.evaluate(y1), self.evaluate(y2))\n\n  def test2D(self):\n    t = array_ops.ones([2, 4, 4, 3])\n    v = array_ops.ones([2, 2, 5, 3])\n    strides = 2\n\n    y1 = nn_ops.conv2d_transpose_v2(t, v, [2, 8, 8, 5], strides)\n    y2 = nn_ops.conv_transpose(t, v, [2, 8, 8, 5], strides)\n\n    self.assertAllEqual(self.evaluate(y1), self.evaluate(y2))\n\n  def test2DTensor(self):\n    t = array_ops.ones([2, 4, 4, 3])\n    v = array_ops.ones([2, 2, 5, 3])\n    strides = 2\n\n    y1 = nn_ops.conv2d_transpose_v2(t, v, [2, 8, 8, 5], strides)\n    y2 = nn_ops.conv_transpose(t, v, constant_op.constant([2, 8, 8, 5]),\n                               strides)\n\n    self.assertAllEqual(self.evaluate(y1), self.evaluate(y2))\n\n  def test3D(self):\n    t = array_ops.ones([2, 4, 4, 4, 3])\n    v = array_ops.ones([2, 2, 2, 5, 3])\n    strides = 2\n\n    y1 = nn_ops.conv3d_transpose_v2(t, v, [2, 8, 8, 8, 5], strides)\n    y2 = nn_ops.conv_transpose(t, v, [2, 8, 8, 8, 5], strides)\n\n    self.assertAllEqual(self.evaluate(y1), self.evaluate(y2))\n\n  def test3DTensor(self):\n    t = array_ops.ones([2, 4, 4, 4, 3])\n    v = array_ops.ones([2, 2, 2, 5, 3])\n    strides = 2\n\n    y1 = nn_ops.conv3d_transpose_v2(t, v, [2, 8, 8, 8, 5], strides)\n    y2 = nn_ops.conv_transpose(t, v, constant_op.constant([2, 8, 8, 8, 5]),\n                               strides)\n\n    self.assertAllEqual(self.evaluate(y1), self.evaluate(y2))\n\n  def testIncorrectSizeInputSmall(self):\n    with self.assertRaisesRegex(\n        ValueError, \"output_shape must be of length 3, 4 or 5 but was 2.\"):\n      nn_ops.conv_transpose(None, 2, [2, 3], \"SAME\")\n\n  def testIncorrectSizeInput(self):\n    with self.assertRaisesRegex(\n        ValueError, \"output_shape must be of length 3, 4 or 5 but was 6.\"):\n      nn_ops.conv_transpose(None, 2, [2, 3, 4, 2, 5, 1], \"SAME\")\n\n  def testTensorsNoShape(self):\n    with self.assertRaisesRegex(\n        ValueError,\n        \"output_shape must be a tensor or sized collection.\"):\n      nn_ops.conv_transpose(None, None, None, None)\n\n\nclass RaggedEmbeddingTest(test_lib.TestCase):\n\n  def testRaggedTensor(self):\n    weights = constant_op.constant([[0, 0, 0], [1, 1, 1], [2, 2, 2], [3, 3, 3]])\n    ragged_ids = ragged_factory_ops.constant([[1, 2, 3], [0], [1, 2]],\n                                             ragged_rank=1)\n\n    embedded_ragged = nn.embedding_lookup_ragged(weights, ragged_ids)\n    expected_output = ragged_factory_ops.constant(\n        [[[1, 1, 1], [2, 2, 2], [3, 3, 3]], [[0, 0, 0]], [[1, 1, 1], [2, 2, 2]]\n        ],\n        ragged_rank=1)\n\n    self.assertAllEqual(expected_output, embedded_ragged)\n\n  def testMultipleRaggedDimTensor(self):\n    weights = constant_op.constant([[0, 0], [1, 1], [2, 2], [3, 3], [4, 4],\n                                    [5, 5], [6, 6]])\n    ragged_ids = ragged_factory_ops.constant(\n        [[[[3, 4], [0, 6]], []], [[[2, 1], [1, 0]], [[2, 5], [2, 3]]], [[[1, 0]]\n                                                                       ]],\n        ragged_rank=2)\n\n    embedded_ragged = nn.embedding_lookup_ragged(weights, ragged_ids)\n    expected_output = ragged_factory_ops.constant(\n        [[[[[3, 3], [4, 4]], [[0, 0], [6, 6]]], []],\n         [[[[2, 2], [1, 1]], [[1, 1], [0, 0]]],\n          [[[2, 2], [5, 5]], [[2, 2], [3, 3]]]], [[[[1, 1], [0, 0]]]]],\n        ragged_rank=2)\n\n    self.assertAllEqual(expected_output, embedded_ragged)\n\n  def testMissingWeights(self):\n    ragged_ids = ragged_factory_ops.constant([[1, 2, 3], [0], [1, 2]])\n\n    with self.assertRaisesRegex(ValueError,\n                                \"The embedding weights must be specified.*\"):\n      nn.embedding_lookup_ragged(None, ragged_ids)\n\n  def testEmptyWeights(self):\n    ragged_ids = ragged_factory_ops.constant([[1, 2, 3], [0], [1, 2]])\n\n    with self.assertRaisesRegex(ValueError,\n                                \"The embedding weights should not be empty.*\"):\n      nn.embedding_lookup_ragged([], ragged_ids)\n\n  def testInvalidIndicesType(self):\n    weights = constant_op.constant([[0, 0, 0], [1, 1, 1], [2, 2, 2]])\n    ragged_ids = ragged_factory_ops.constant([[1., 2., 3.], [1., 2.]])\n\n    with self.assertRaisesRegex(\n        ValueError, \"The values contained by the inputs have type*\"):\n      nn.embedding_lookup_ragged(weights, ragged_ids)\n\n  def testMaxNormForEmbeddings(self):\n    weights = constant_op.constant([[0, 0, 0, 0], [1, 1, 1, 1],\n                                    [2, 2, 2, 2], [3, 3, 3, 3]],\n                                   dtype=dtypes.float32)\n    ragged_ids = ragged_factory_ops.constant([[1, 2, 3], [0], [1, 2]],\n                                             ragged_rank=1)\n\n    actual_embeddings = [\n        nn.embedding_lookup(weights, ragged_ids, max_norm=max_norm)\n        for max_norm in [1, 2, 5]]\n\n    expected_embeddings = (\n        # max_norm = 1\n        [[[.5, .5, .5, .5], [.5, .5, .5, .5], [.5, .5, .5, .5]],\n         [[0, 0, 0, 0]], [[.5, .5, .5, .5], [.5, .5, .5, .5]]],\n        # max_norm = 2\n        [[[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]],\n         [[0, 0, 0, 0]], [[1, 1, 1, 1], [1, 1, 1, 1]]],\n        # max_norm = 5\n        [[[1, 1, 1, 1], [2, 2, 2, 2], [2.5, 2.5, 2.5, 2.5]],\n         [[0, 0, 0, 0]], [[1, 1, 1, 1], [2, 2, 2, 2]]],\n        )\n\n    for expected, actual in zip(expected_embeddings, actual_embeddings):\n      self.assertAllClose(\n          ragged_factory_ops.constant(expected, dtype=float, ragged_rank=1),\n          actual)\n\n\nclass IsotonicTest(parameterized.TestCase, test_lib.TestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_increasing_and_decreasing(self):\n    x = constant_op.constant([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]],\n                             dtype=dtypes.float64)\n    y, segments = nn_ops.isotonic_regression(x, decreasing=False)\n    self.assertAllClose(y, x)\n    self.assertAllClose(segments, [[0, 1, 2, 3, 4], [0, 1, 2, 3, 4]])\n\n    y, segments = nn_ops.isotonic_regression(x, decreasing=True)\n    self.assertAllClose(\n        y,\n        [\n            [2, 2, 2, 2, 2],  # Average of the inputs.\n            [7, 7, 7, 7, 7]\n        ])\n    self.assertAllClose(segments, array_ops.zeros((2, 5)))\n\n    y, segments = nn_ops.isotonic_regression(-x, decreasing=True)\n    self.assertAllClose(segments, [[0, 1, 2, 3, 4], [0, 1, 2, 3, 4]])\n\n    self.assertAllClose(y, -x)\n    y, segments = nn_ops.isotonic_regression(-x, decreasing=False)\n    self.assertAllClose(\n        -y,\n        [\n            [2, 2, 2, 2, 2],  # Average of the inputs.\n            [7, 7, 7, 7, 7]\n        ])\n    self.assertAllClose(segments, array_ops.zeros((2, 5)))\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_different_axis(self):\n    x = constant_op.constant([[0, 6, 2, 8, 4], [5, 1, 7, 3, 9]],\n                             dtype=dtypes.float64)\n    y, segments = nn_ops.isotonic_regression(x, decreasing=True, axis=0)\n    self.assertAllClose(\n        y,\n        [\n            [2.5, 6, 4.5, 8, 6.5],  # Either identity or average.\n            [2.5, 1, 4.5, 3, 6.5]\n        ])\n    self.assertAllClose(segments, [[0, 0, 0, 0, 0], [0, 1, 0, 1, 0]])\n\n  @test_util.run_v2_only\n  def testGradientV2(self, dtype=np.float64, batch_size=30, dimensions=50):\n\n    @def_function.function\n    def ComputeIsotonicFn(x):\n      y, _ = nn_ops.isotonic_regression(x)  # No gradient wrt segments.\n      return y\n\n    np.random.seed(0)\n    x_init = np.random.randn(batch_size, dimensions).astype(dtype)\n    grad_theoretical, grad_numerical = gradient_checker_v2.compute_gradient(\n        ComputeIsotonicFn, [x_init], delta=1e-5)\n    self.assertAllClose(grad_theoretical, grad_numerical)\n\n  @test_util.run_v1_only(\"compute_gradient_error is v1 only\")\n  def testGradientV1(self, dtype=np.float64, batch_size=30, dimensions=50):\n    np.random.seed(0)\n    x_init = np.random.randn(batch_size, dimensions).astype(dtype)\n    with self.cached_session():\n      x = array_ops.placeholder(dtype, (batch_size, dimensions))\n      y, _ = nn_ops.isotonic_regression(x)  # Segments have no gradient.\n      max_error = gradient_checker.compute_gradient_error(\n          x, (batch_size, dimensions), y, (batch_size, dimensions), x_init)\n    self.assertAllClose(max_error, 0.)\n\n  @parameterized.parameters([[dtypes.half, dtypes.half],\n                             [dtypes.bfloat16, dtypes.bfloat16],\n                             [dtypes.float32, dtypes.float32],\n                             [dtypes.float64, dtypes.float64],\n                             [dtypes.int32, dtypes.float64],\n                             [dtypes.int16, dtypes.float32]])\n  def testTypePromotion(self, dtype_in, expected_dtype_out):\n    x = constant_op.constant([[0, 6, 2, 8, 4], [5, 1, 7, 3, 9]], dtype=dtype_in)\n    y, segments = nn_ops.isotonic_regression(x)\n    self.assertEqual(y.dtype, expected_dtype_out)\n    self.assertEqual(segments.dtype, dtypes.int32)\n\n\nif __name__ == \"__main__\":\n  test_lib.main()\n"], "filenames": ["tensorflow/core/kernels/data_format_ops.cc", "tensorflow/python/ops/nn_test.py"], "buggy_code_start_loc": [20, 29], "buggy_code_end_loc": [129, 1371], "fixing_code_start_loc": [21, 30], "fixing_code_end_loc": [188, 1468], "type": "CWE-125", "message": "In affected versions of TensorFlow the tf.raw_ops.DataFormatVecPermute API does not validate the src_format and dst_format attributes. The code assumes that these two arguments define a permutation of NHWC. This can result in uninitialized memory accesses, read outside of bounds and even crashes. This is fixed in versions 1.15.5, 2.0.4, 2.1.3, 2.2.2, 2.3.2, and 2.4.0.", "other": {"cve": {"id": "CVE-2020-26267", "sourceIdentifier": "security-advisories@github.com", "published": "2020-12-10T23:15:12.723", "lastModified": "2021-08-17T13:23:07.763", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "In affected versions of TensorFlow the tf.raw_ops.DataFormatVecPermute API does not validate the src_format and dst_format attributes. The code assumes that these two arguments define a permutation of NHWC. This can result in uninitialized memory accesses, read outside of bounds and even crashes. This is fixed in versions 1.15.5, 2.0.4, 2.1.3, 2.2.2, 2.3.2, and 2.4.0."}, {"lang": "es", "value": "En las versiones afectadas de TensorFlow, la API tf.raw_ops.DataFormatVecPermute no comprueba los atributos src_format y dst_format.&#xa0;El c\u00f3digo asume que estos dos argumentos definen una permutaci\u00f3n de NHWC.&#xa0;Esto puede resultar en accesos de memoria no inicializados, lectura fuera de l\u00edmites e incluso fallos.&#xa0;Esto es corregido en las versiones 1.15.5, 2.0.4, 2.1.3, 2.2.2, 2.3.2 y 2.4.0."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.9}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:L/A:L", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "LOW", "availabilityImpact": "LOW", "baseScore": 4.4, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 2.5}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:S/C:P/I:P/A:P", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "SINGLE", "confidentialityImpact": "PARTIAL", "integrityImpact": "PARTIAL", "availabilityImpact": "PARTIAL", "baseScore": 4.3}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.1, "impactScore": 6.4, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-125"}]}, {"source": "security-advisories@github.com", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-125"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionEndExcluding": "1.15.5", "matchCriteriaId": "CA3A54AC-E0F8-4741-8A80-04EEF746B14B"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.0.0", "versionEndExcluding": "2.0.4", "matchCriteriaId": "989E4548-7823-436F-A9FE-04158ED41C48"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.1.0", "versionEndExcluding": "2.1.3", "matchCriteriaId": "46417CA8-E666-4E12-B2A8-BB0E97D49BF4"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.2.0", "versionEndExcluding": "2.2.2", "matchCriteriaId": "57B24744-0D81-41E9-9ED0-7296368DEF00"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.3.0", "versionEndExcluding": "2.3.2", "matchCriteriaId": "DBEA56AF-3495-4883-9721-0FA9F08E7F6D"}]}]}], "references": [{"url": "https://github.com/tensorflow/tensorflow/commit/ebc70b7a592420d3d2f359e4b1694c236b82c7ae", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/security/advisories/GHSA-c9f3-9wfr-wgh7", "source": "security-advisories@github.com", "tags": ["Exploit", "Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/tensorflow/tensorflow/commit/ebc70b7a592420d3d2f359e4b1694c236b82c7ae"}}