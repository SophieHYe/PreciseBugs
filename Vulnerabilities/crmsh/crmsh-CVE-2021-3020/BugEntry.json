{"buggy_code": ["# Copyright (C) 2016 Kristoffer Gronlund <kgronlund@suse.com>\n# See COPYING for license information.\n#\n# Bootstrap:\n#\n# Supersedes and replaces both the init/add/remove cluster scripts,\n# and the ha-cluster-bootstrap scripts.\n#\n# Implemented as a straight-forward set of python functions for\n# simplicity and flexibility.\n#\n# TODO: Make csync2 usage optional\n# TODO: Configuration file for bootstrap?\n\nimport os\nimport sys\nimport random\nimport re\nimport time\nimport readline\nimport shutil\nfrom string import Template\nfrom lxml import etree\nfrom pathlib import Path\nfrom . import config\nfrom . import utils\nfrom . import xmlutil\nfrom .cibconfig import mkset_obj, cib_factory\nfrom . import corosync\nfrom . import tmpfiles\nfrom . import clidisplay\nfrom . import term\nfrom . import lock\n\n\nLOG_FILE = \"/var/log/crmsh/ha-cluster-bootstrap.log\"\nCSYNC2_KEY = \"/etc/csync2/key_hagroup\"\nCSYNC2_CFG = \"/etc/csync2/csync2.cfg\"\nCOROSYNC_AUTH = \"/etc/corosync/authkey\"\nSYSCONFIG_SBD = \"/etc/sysconfig/sbd\"\nSYSCONFIG_FW = \"/etc/sysconfig/SuSEfirewall2\"\nSYSCONFIG_FW_CLUSTER = \"/etc/sysconfig/SuSEfirewall2.d/services/cluster\"\nPCMK_REMOTE_AUTH = \"/etc/pacemaker/authkey\"\nCOROSYNC_CONF_ORIG = tmpfiles.create()[1]\nRSA_PRIVATE_KEY = \"/root/.ssh/id_rsa\"\nRSA_PUBLIC_KEY = \"/root/.ssh/id_rsa.pub\"\nAUTHORIZED_KEYS_FILE = \"/root/.ssh/authorized_keys\"\nSERVICES_STOP_LIST = [\"corosync-qdevice.service\", \"corosync.service\", \"hawk.service\"]\n\nINIT_STAGES = (\"ssh\", \"ssh_remote\", \"csync2\", \"csync2_remote\", \"corosync\", \"storage\", \"sbd\", \"cluster\", \"vgfs\", \"admin\", \"qdevice\")\n\n\nclass Context(object):\n    \"\"\"\n    Context object used to avoid having to pass these variables\n    to every bootstrap method.\n    \"\"\"\n    def __init__(self):\n        '''\n        Initialize attributes\n        '''\n        self.type = None # init or join\n        self.quiet = None\n        self.yes_to_all = None\n        self.template = None\n        self.cluster_name = None\n        self.watchdog = None\n        self.no_overwrite_sshkey = None\n        self.nic_list = None\n        self.unicast = None\n        self.admin_ip = None\n        self.second_heartbeat = None\n        self.ipv6 = None\n        self.qdevice_inst = None\n        self.qnetd_addr = None\n        self.qdevice_port = None\n        self.qdevice_algo = None\n        self.qdevice_tie_breaker = None\n        self.qdevice_tls = None\n        self.qdevice_heuristics = None\n        self.qdevice_heuristics_mode = None\n        self.qdevice_rm_flag = None\n        self.shared_device = None\n        self.ocfs2_device = None\n        self.cluster_node = None\n        self.cluster_node_ip = None\n        self.force = None\n        self.arbitrator = None\n        self.clusters = None\n        self.tickets = None\n        self.sbd_manager = None\n        self.sbd_devices = None\n        self.diskless_sbd = None\n        self.stage = None\n        self.args = None\n        self.ui_context = None\n        self.interfaces_inst = None\n        self.default_nic_list = []\n        self.default_ip_list = []\n        self.local_ip_list = []\n        self.local_network_list = []\n        self.rm_list = [SYSCONFIG_SBD, CSYNC2_CFG, corosync.conf(), CSYNC2_KEY,\n                COROSYNC_AUTH, \"/var/lib/heartbeat/crm/*\", \"/var/lib/pacemaker/cib/*\"]\n\n    @classmethod\n    def set_context(cls, options):\n        ctx = cls()\n        for opt in vars(options):\n            setattr(ctx, opt, getattr(options, opt))\n        return ctx\n\n    def initialize_qdevice(self):\n        \"\"\"\n        Initialize qdevice instance\n        \"\"\"\n        if not self.qnetd_addr:\n            return\n        self.qdevice_inst = corosync.QDevice(\n                self.qnetd_addr,\n                port=self.qdevice_port,\n                algo=self.qdevice_algo,\n                tie_breaker=self.qdevice_tie_breaker,\n                tls=self.qdevice_tls,\n                cmds=self.qdevice_heuristics,\n                mode=self.qdevice_heuristics_mode)\n\n    def validate_option(self):\n        \"\"\"\n        Validate options\n        \"\"\"\n        if self.admin_ip:\n            try:\n                Validation.valid_admin_ip(self.admin_ip)\n            except ValueError as err:\n                error(err)\n        if self.qdevice_inst:\n            try:\n                self.qdevice_inst.valid_attr()\n            except ValueError as err:\n                error(err)\n        if self.nic_list:\n            if len(self.nic_list) > 2:\n                error(\"Maximum number of interface is 2\")\n            if len(self.nic_list) != len(set(self.nic_list)):\n                error(\"Duplicated input\")\n        if self.no_overwrite_sshkey:\n            warn(\"--no-overwrite-sshkey option is deprecated since crmsh does not overwrite ssh keys by default anymore and will be removed in future versions\")\n        if self.type == \"join\" and self.watchdog:\n            warn(\"-w option is deprecated and will be removed in future versions\")\n\n    def init_sbd_manager(self):\n        self.sbd_manager = SBDManager(self.sbd_devices, self.diskless_sbd)\n\n\nclass Watchdog(object):\n    \"\"\"\n    Class to find valid watchdog device name\n    \"\"\"\n    QUERY_CMD = \"sbd query-watchdog\"\n    DEVICE_FIND_REGREX = \"\\[[0-9]+\\] (/dev/.*)\\n.*\\nDriver: (.*)\"\n\n    def __init__(self, _input=None, peer_host=None):\n        \"\"\"\n        Init function\n        \"\"\"\n        self._input = _input\n        self._peer_host = peer_host\n        self._watchdog_info_dict = {}\n        self._watchdog_device_name = None\n    \n    @property\n    def watchdog_device_name(self):\n        return self._watchdog_device_name\n\n    @staticmethod\n    def _verify_watchdog_device(dev, ignore_error=False):\n        \"\"\"\n        Use wdctl to verify watchdog device\n        \"\"\"\n        rc, _, err = utils.get_stdout_stderr(\"wdctl {}\".format(dev))\n        if rc != 0:\n            if ignore_error:\n                return False\n            else:\n                error(\"Invalid watchdog device {}: {}\".format(dev, err))\n        return True\n\n    @staticmethod\n    def _load_watchdog_driver(driver):\n        \"\"\"\n        Load specific watchdog driver\n        \"\"\"\n        invoke(\"echo {} > /etc/modules-load.d/watchdog.conf\".format(driver))\n        invoke(\"systemctl restart systemd-modules-load\")\n\n    @staticmethod\n    def _get_watchdog_device_from_sbd_config():\n        \"\"\"\n        Try to get watchdog device name from sbd config file\n        \"\"\"\n        conf = utils.parse_sysconfig(SYSCONFIG_SBD)\n        return conf.get(\"SBD_WATCHDOG_DEV\")\n\n    @staticmethod\n    def _driver_is_loaded(driver):\n        \"\"\"\n        Check if driver was already loaded\n        \"\"\"\n        _, out, _ = utils.get_stdout_stderr(\"lsmod\")\n        return re.search(\"\\n{}\\s+\".format(driver), out)\n\n    def _set_watchdog_info(self):\n        \"\"\"\n        Set watchdog info through sbd query-watchdog command\n        Content in self._watchdog_info_dict: {device_name: driver_name}\n        \"\"\"\n        rc, out, err = utils.get_stdout_stderr(self.QUERY_CMD)\n        if rc == 0 and out:\n            # output format might like:\n            #   [1] /dev/watchdog\\nIdentity: Software Watchdog\\nDriver: softdog\\n\n            self._watchdog_info_dict = dict(re.findall(self.DEVICE_FIND_REGREX, out))\n        else:\n            error(\"Failed to run {}: {}\".format(self.QUERY_CMD, err))\n\n    def _get_device_through_driver(self, driver_name):\n        \"\"\"\n        Get watchdog device name which has driver_name\n        \"\"\"\n        for device, driver in self._watchdog_info_dict.items():\n            if driver == driver_name and self._verify_watchdog_device(device):\n                return device\n        return None\n\n    def _get_driver_through_device_remotely(self, dev_name):\n        \"\"\"\n        Given watchdog device name, get driver name on remote node\n        \"\"\"\n        cmd = \"ssh -o StrictHostKeyChecking=no root@{} {}\".format(self._peer_host, self.QUERY_CMD)\n        rc, out, err = utils.get_stdout_stderr(cmd)\n        if rc == 0 and out:\n            # output format might like:\n            #   [1] /dev/watchdog\\nIdentity: Software Watchdog\\nDriver: softdog\\n\n            device_driver_dict = dict(re.findall(self.DEVICE_FIND_REGREX, out))\n            if device_driver_dict and dev_name in device_driver_dict:\n                return device_driver_dict[dev_name]\n            else:\n                return None\n        else:\n            error(\"Failed to run {} remotely: {}\".format(self.QUERY_CMD, err))\n\n    def _get_first_unused_device(self):\n        \"\"\"\n        Get first unused watchdog device name\n        \"\"\"\n        for dev in self._watchdog_info_dict:\n            if self._verify_watchdog_device(dev, ignore_error=True):\n                return dev\n        return None\n\n    def _set_input(self):\n        \"\"\"\n        If self._input was not provided by option:\n          1. Try to get it from sbd config file\n          2. Try to get the first valid device from result of sbd query-watchdog\n          3. Set the self._input as softdog\n        \"\"\"\n        if not self._input:\n            dev = self._get_watchdog_device_from_sbd_config()\n            if dev and self._verify_watchdog_device(dev, ignore_error=True):\n                self._input = dev\n                return\n            first_unused = self._get_first_unused_device()\n            self._input = first_unused if first_unused else \"softdog\"\n\n    def _valid_device(self, dev):\n        \"\"\"\n        Is an unused watchdog device\n        \"\"\"\n        if dev in self._watchdog_info_dict and self._verify_watchdog_device(dev):\n            return True\n        return False\n\n    def join_watchdog(self):\n        \"\"\"\n        In join proces, get watchdog device from config\n        If that device not exist, get driver name from init node, and load that driver\n        \"\"\"\n        self._set_watchdog_info()\n\n        res = self._get_watchdog_device_from_sbd_config()\n        if not res:\n            error(\"Failed to get watchdog device from {}\".format(SYSCONFIG_SBD))\n        self._input = res\n\n        if not self._valid_device(self._input):\n            driver = self._get_driver_through_device_remotely(self._input)\n            self._load_watchdog_driver(driver)\n\n    def init_watchdog(self):\n        \"\"\"\n        In init process, find valid watchdog device\n        \"\"\"\n        self._set_watchdog_info()\n        self._set_input()\n\n        # self._input is a device name\n        if self._valid_device(self._input):\n            self._watchdog_device_name = self._input\n            return\n\n        # self._input is invalid, exit\n        if not invokerc(\"modinfo {}\".format(self._input)):\n            error(\"Should provide valid watchdog device or driver name by -w option\")\n\n        # self._input is a driver name, load it if it was unloaded\n        if not self._driver_is_loaded(self._input):\n            self._load_watchdog_driver(self._input)\n            self._set_watchdog_info()\n\n        # self._input is a loaded driver name, find corresponding device name\n        res = self._get_device_through_driver(self._input)\n        if res:\n            self._watchdog_device_name = res\n            return\n\n\nclass SBDManager(object):\n    \"\"\"\n    Class to manage sbd configuration and services\n    \"\"\"\n    SYSCONFIG_SBD_TEMPLATE = \"/usr/share/fillup-templates/sysconfig.sbd\"\n    SBD_STATUS_DESCRIPTION = \"\"\"\nConfigure SBD:\n  If you have shared storage, for example a SAN or iSCSI target,\n  you can use it avoid split-brain scenarios by configuring SBD.\n  This requires a 1 MB partition, accessible to all nodes in the\n  cluster.  The device path must be persistent and consistent\n  across all nodes in the cluster, so /dev/disk/by-id/* devices\n  are a good choice.  Note that all data on the partition you\n  specify here will be destroyed.\n\"\"\"\n\n    def __init__(self, sbd_devices=None, diskless_sbd=False):\n        \"\"\"\n        Init function\n\n        sbd_devices is provided by '-s' option on init process\n        diskless_sbd is provided by '-S' option on init process\n        \"\"\"\n        self.sbd_devices_input = sbd_devices\n        self.diskless_sbd = diskless_sbd\n        self._sbd_devices = None\n        self._watchdog_inst = None\n\n    def _parse_sbd_device(self):\n        \"\"\"\n        Parse sbd devices, possible command line is like:\n          -s \"/dev/sdb1;/dev/sdb2\"\n          -s /dev/sdb1 -s /dev/sbd2\n        \"\"\"\n        result_list = []\n        for dev in self.sbd_devices_input:\n            if ';' in dev:\n                result_list.extend(dev.strip(';').split(';'))\n            else:\n                result_list.append(dev)\n        return result_list\n\n    @staticmethod\n    def _get_device_uuid(dev, node=None):\n        \"\"\"\n        Get UUID for specific device and node\n        \"\"\"\n        cmd = \"sbd -d {} dump\".format(dev)\n        if node:\n            cmd = \"ssh -o StrictHostKeyChecking=no root@{} '{}'\".format(node, cmd)\n\n        rc, out, err = utils.get_stdout_stderr(cmd)\n        if rc != 0 and err:\n            raise ValueError(\"Cannot dump sbd meta-data: {}\".format(err))\n        if rc == 0 and out:\n            res = re.search(\"UUID\\s*:\\s*(.*)\\n\", out)\n            if not res:\n                raise ValueError(\"Cannot find sbd device UUID for {}\".format(dev))\n            return res.group(1)\n\n    def _compare_device_uuid(self, dev, node_list):\n        \"\"\"\n        Compare local sbd device UUID with other node's sbd device UUID\n        \"\"\"\n        if not node_list:\n            return\n        local_uuid = self._get_device_uuid(dev)\n        for node in node_list:\n            remote_uuid = self._get_device_uuid(dev, node)\n            if local_uuid != remote_uuid:\n                raise ValueError(\"Device {} doesn't have the same UUID with {}\".format(dev, node))\n\n    def _verify_sbd_device(self, dev_list, compare_node_list=[]):\n        \"\"\"\n        Verify sbd device\n        \"\"\"\n        if len(dev_list) > 3:\n            raise ValueError(\"Maximum number of SBD device is 3\")\n        for dev in dev_list:\n            if not is_block_device(dev):\n                raise ValueError(\"{} doesn't look like a block device\".format(dev))\n            self._compare_device_uuid(dev, compare_node_list)\n\n    def _get_sbd_device_interactive(self):\n        \"\"\"\n        Get sbd device on interactive mode\n        \"\"\"\n        if _context.yes_to_all:\n            warn(\"Not configuring SBD ({} left untouched).\".format(SYSCONFIG_SBD))\n            return\n\n        status(self.SBD_STATUS_DESCRIPTION)\n\n        if not confirm(\"Do you wish to use SBD?\"):\n            warn(\"Not configuring SBD - STONITH will be disabled.\")\n            return\n\n        configured_dev_list = self._get_sbd_device_from_config()\n        if configured_dev_list and not confirm(\"SBD is already configured to use {} - overwrite?\".format(';'.join(configured_dev_list))):\n            return configured_dev_list\n\n        dev_list = []\n        dev_looks_sane = False\n        while not dev_looks_sane:\n            dev = prompt_for_string('Path to storage device (e.g. /dev/disk/by-id/...), or \"none\" for diskless sbd, use \";\" as separator for multi path', r'none|\\/.*')\n            if not dev:\n                continue\n            if dev == \"none\":\n                self.diskless_sbd = True\n                return\n            dev_list = dev.strip(';').split(';')\n            try:\n                self._verify_sbd_device(dev_list)\n            except ValueError as err_msg:\n                print_error_msg(str(err_msg))\n                continue\n            for dev_item in dev_list:\n                warn(\"All data on {} will be destroyed!\".format(dev_item))\n                if confirm('Are you sure you wish to use this device?'):\n                    dev_looks_sane = True\n                else:\n                    dev_looks_sane = False\n                    break\n\n        return dev_list\n\n    def _get_sbd_device(self):\n        \"\"\"\n        Get sbd device from options or interactive mode\n        \"\"\"\n        dev_list = []\n        if self.sbd_devices_input:\n            dev_list = self._parse_sbd_device()\n            self._verify_sbd_device(dev_list)\n        elif not self.diskless_sbd:\n            dev_list = self._get_sbd_device_interactive()\n        self._sbd_devices = dev_list\n\n    def _initialize_sbd(self):\n        \"\"\"\n        Initialize SBD device\n        \"\"\"\n        if self.diskless_sbd:\n            return\n        for dev in self._sbd_devices:\n            rc, _, err = invoke(\"sbd -d {} create\".format(dev))\n            if not rc:\n                error(\"Failed to initialize SBD device {}: {}\".format(dev, err))\n\n    def _update_configuration(self):\n        \"\"\"\n        Update /etc/sysconfig/sbd\n        \"\"\"\n        shutil.copyfile(self.SYSCONFIG_SBD_TEMPLATE, SYSCONFIG_SBD)\n        sbd_config_dict = {\n                \"SBD_PACEMAKER\": \"yes\",\n                \"SBD_STARTMODE\": \"always\",\n                \"SBD_DELAY_START\": \"no\",\n                \"SBD_WATCHDOG_DEV\": self._watchdog_inst.watchdog_device_name\n                }\n        if self._sbd_devices:\n            sbd_config_dict[\"SBD_DEVICE\"] = ';'.join(self._sbd_devices)\n        utils.sysconfig_set(SYSCONFIG_SBD, **sbd_config_dict)\n        csync2_update(SYSCONFIG_SBD)\n\n    @staticmethod\n    def _get_sbd_device_from_config():\n        \"\"\"\n        Gets currently configured SBD device, i.e. what's in /etc/sysconfig/sbd\n        \"\"\"\n        conf = utils.parse_sysconfig(SYSCONFIG_SBD)\n        res = conf.get(\"SBD_DEVICE\")\n        if res:\n            return res.strip(';').split(';')\n        else:\n            return None\n\n    def sbd_init(self):\n        \"\"\"\n        Function sbd_init includes these steps:\n        1. Get sbd device from options or interactive mode\n        2. Initialize sbd device\n        3. Write config file /etc/sysconfig/sbd\n        \"\"\"\n        if not utils.package_is_installed(\"sbd\"):\n            return\n        self._watchdog_inst = Watchdog(_input=_context.watchdog)\n        self._watchdog_inst.init_watchdog()\n        self._get_sbd_device()\n        if not self._sbd_devices and not self.diskless_sbd:\n            invoke(\"systemctl disable sbd.service\")\n            return\n        status_long(\"Initializing {}SBD...\".format(\"diskless \" if self.diskless_sbd else \"\"))\n        self._initialize_sbd()\n        self._update_configuration()\n        invoke(\"systemctl enable sbd.service\")\n        status_done()\n\n    def configure_sbd_resource(self):\n        \"\"\"\n        Configure stonith-sbd resource and stonith-enabled property\n        \"\"\"\n        if not utils.package_is_installed(\"sbd\"):\n            return\n        if utils.service_is_enabled(\"sbd.service\"):\n            if self._get_sbd_device_from_config():\n                if not invokerc(\"crm configure primitive stonith-sbd stonith:external/sbd pcmk_delay_max=30s\"):\n                    error(\"Can't create stonith-sbd primitive\")\n                if not invokerc(\"crm configure property stonith-enabled=true\"):\n                    error(\"Can't enable STONITH for SBD\")\n            else:\n                if not invokerc(\"crm configure property stonith-enabled=true stonith-watchdog-timeout=5s\"):\n                    error(\"Can't enable STONITH for diskless SBD\")\n\n    def join_sbd(self, peer_host):\n        \"\"\"\n        Function join_sbd running on join process only\n        On joining process, check whether peer node has enabled sbd.service\n        If so, check prerequisites of SBD and verify sbd device on join node\n        \"\"\"\n        if not utils.package_is_installed(\"sbd\"):\n            return\n        if not os.path.exists(SYSCONFIG_SBD) or not utils.service_is_enabled(\"sbd.service\", peer_host):\n            invoke(\"systemctl disable sbd.service\")\n            return\n        self._watchdog_inst = Watchdog(peer_host=peer_host)\n        self._watchdog_inst.join_watchdog()\n        dev_list = self._get_sbd_device_from_config()\n        if dev_list:\n            self._verify_sbd_device(dev_list, [peer_host])\n        status(\"Got {}SBD configuration\".format(\"\" if dev_list else \"diskless \"))\n        invoke(\"systemctl enable sbd.service\")\n\n    @classmethod\n    def verify_sbd_device(cls):\n        \"\"\"\n        This classmethod is for verifying sbd device on a running cluster\n        Raise ValueError for exceptions\n        \"\"\"\n        inst = cls()\n        dev_list = inst._get_sbd_device_from_config()\n        if not dev_list:\n            raise ValueError(\"No sbd device configured\")\n        inst._verify_sbd_device(dev_list, utils.list_cluster_nodes_except_me())\n\n\n_context = None\n\n\ndef die(*args):\n    \"\"\"\n    Broken out as special case for log() failure.  Ordinarily you\n    should just use error() to terminate.\n    \"\"\"\n    raise ValueError(\" \".join([str(arg) for arg in args]))\n\n\ndef error(*args):\n    \"\"\"\n    Log an error message and raise ValueError to bail out of\n    bootstrap process.\n    \"\"\"\n    log(\"ERROR: {}\".format(\" \".join([str(arg) for arg in args])))\n    die(*args)\n\n\ndef print_error_msg(msg):\n    \"\"\"\n    Just print error message\n    \"\"\"\n    print(term.render(clidisplay.error(\"ERROR:\")) + \" {}\".format(msg))\n\n\ndef warn(*args):\n    \"\"\"\n    Log and display a warning message.\n    \"\"\"\n    log(\"WARNING: {}\".format(\" \".join(str(arg) for arg in args)))\n    print(term.render(clidisplay.warn(\"WARNING: {}\".format(\" \".join(str(arg) for arg in args)))))\n\n\n@utils.memoize\ndef log_file_fallback():\n    \"\"\"\n    If the standard log location isn't writable,\n    just log to the nearest temp dir.\n    \"\"\"\n    return os.path.join(utils.get_tempdir(), \"ha-cluster-bootstrap.log\")\n\n\ndef log(*args):\n    global LOG_FILE\n    try:\n        Path(os.path.dirname(LOG_FILE)).mkdir(parents=True, exist_ok=True)\n        with open(LOG_FILE, \"ab\") as logfile:\n            text = \" \".join([utils.to_ascii(arg) for arg in args]) + \"\\n\"\n            logfile.write(text.encode('ascii', 'backslashreplace'))\n    except IOError:\n        if LOG_FILE != log_file_fallback():\n            LOG_FILE = log_file_fallback()\n            log(*args)\n        else:\n            die(\"Can't append to {} - aborting\".format(LOG_FILE))\n\n\ndef drop_last_history():\n    hlen = readline.get_current_history_length()\n    if hlen > 0:\n        readline.remove_history_item(hlen - 1)\n\n\ndef prompt_for_string(msg, match=None, default='', valid_func=None, prev_value=[]):\n    if _context.yes_to_all:\n        return default\n\n    while True:\n        disable_completion()\n        val = utils.multi_input('  %s [%s]' % (msg, default))\n        enable_completion()\n        if not val:\n            val = default\n        else:\n            drop_last_history()\n\n        if not val:\n            return None\n        if not match and not valid_func:\n            return val\n        if match and not re.match(match, val):\n            print_error_msg(\"Invalid value entered\")\n            continue\n        if valid_func:\n            try:\n                valid_func(val, prev_value)\n            except ValueError as err:\n                print_error_msg(err)\n                continue\n\n        return val\n\n\ndef confirm(msg):\n    if _context.yes_to_all:\n        return True\n    disable_completion()\n    rc = utils.ask(msg)\n    enable_completion()\n    drop_last_history()\n    return rc\n\n\ndef disable_completion():\n    if _context.ui_context:\n        _context.ui_context.disable_completion()\n\n\ndef enable_completion():\n    if _context.ui_context:\n        _context.ui_context.setup_readline()\n\n\ndef invoke(*args):\n    \"\"\"\n    Log command execution to log file.\n    Log output from command to log file.\n    Return (boolean, stdout, stderr)\n    \"\"\"\n    log(\"+ \" + \" \".join(args))\n    rc, stdout, stderr = utils.get_stdout_stderr(\" \".join(args))\n    if stdout:\n        log(stdout)\n    if stderr:\n        log(stderr)\n    return rc == 0, stdout, stderr\n\n\ndef invokerc(*args):\n    \"\"\"\n    Calling invoke, return True/False\n    \"\"\"\n    rc, _, _ = invoke(*args)\n    return rc\n\n\ndef crm_configure_load(action, configuration):\n    log(\": loading crm config (%s), content is:\" % (action))\n    log(configuration)\n    if not cib_factory.initialize():\n        error(\"Failed to load cluster configuration\")\n    set_obj = mkset_obj()\n    if action == 'replace':\n        cib_factory.erase()\n    if not set_obj.save(configuration, remove=False, method=action):\n        error(\"Failed to load cluster configuration\")\n    if not cib_factory.commit():\n        error(\"Failed to commit cluster configuration\")\n\n\ndef wait_for_resource(message, resource, needle=\"running on\"):\n    status_long(message)\n    while True:\n        _rc, out, err = utils.get_stdout_stderr(\"crm_resource --locate --resource \" + resource)\n        if needle in out:\n            break\n        if needle in err:\n            break\n        status_progress()\n        sleep(1)\n    status_done()\n\n\ndef wait_for_stop(message, resource):\n    return wait_for_resource(message, resource, needle=\"NOT running\")\n\n\ndef wait_for_cluster():\n    status_long(\"Waiting for cluster\")\n    while True:\n        _rc, out, _err = utils.get_stdout_stderr(\"crm_mon -1\")\n        if is_online(out):\n            break\n        status_progress()\n        sleep(2)\n    status_done()\n\n\ndef get_cluster_node_hostname():\n    \"\"\"\n    Get the hostname of the cluster node used during the join process if an IP address is used.\n    \"\"\"\n    peer_node = None\n    if _context.cluster_node:\n        if utils.IP.is_valid_ip(_context.cluster_node):\n            rc, out, err = utils.get_stdout_stderr(\"ssh {} crm_node --name\".format(_context.cluster_node))\n            if rc != 0:\n                error(err)\n            peer_node = out\n        else:\n            peer_node = _context.cluster_node\n    return peer_node\n\n\ndef is_online(crm_mon_txt):\n    \"\"\"\n    Check whether local node is online\n    Besides that, in join process, check whether init node is online\n    \"\"\"\n    if not re.search(\"Online: .* {} \".format(utils.this_node()), crm_mon_txt):\n        return False\n\n    # if peer_node is None, this is in the init process\n    peer_node = get_cluster_node_hostname()\n    if peer_node is None:\n        return True\n    # In join process\n    # If the joining node is already online but can't find the init node\n    # The communication IP maybe mis-configured\n    if not re.search(\"Online: .* {} \".format(peer_node), crm_mon_txt):\n        shutil.copy(COROSYNC_CONF_ORIG, corosync.conf())\n        csync2_update(corosync.conf())\n        utils.stop_service(\"corosync\")\n        print()\n        error(\"Cannot see peer node \\\"{}\\\", please check the communication IP\".format(peer_node))\n    return True\n\n\ndef pick_default_value(default_list, prev_list):\n    \"\"\"\n    Provide default value for function 'prompt_for_string'.\n    Make sure give different default value in multi-ring mode.\n\n    Parameters:\n    * default_list - default value list for config item\n    * prev_list    - previous value for config item in multi-ring mode\n    \"\"\"\n    for value in default_list:\n        if value not in prev_list:\n            return value\n    return \"\"\n\n\ndef sleep(t):\n    \"\"\"\n    Sleep for t seconds.\n    \"\"\"\n    t = float(t)\n    time.sleep(t)\n\n\ndef status(msg):\n    log(\"# \" + msg)\n    if not _context.quiet:\n        print(\"  {}\".format(msg))\n\n\ndef status_long(msg):\n    log(\"# {}...\".format(msg))\n    if not _context.quiet:\n        sys.stdout.write(\"  {}...\".format(msg))\n        sys.stdout.flush()\n\n\ndef status_progress():\n    if not _context.quiet:\n        sys.stdout.write(\".\")\n        sys.stdout.flush()\n\n\ndef status_done():\n    log(\"# done\")\n    if not _context.quiet:\n        print(\"done\")\n\n\ndef partprobe():\n    # This function uses fdisk to create a list of valid devices for probing\n    # with partprobe.  This prevents partprobe from failing on read-only mounted\n    # devices such as /dev/sr0 (etc) that might cause it to return an error when\n    # it exits.  This allows partprobe to run without forcing _die to bail out.\n    # -Brandon Heaton\n    #  ATT Training Engineer\n    #  Data Center Engineer\n    #  bheaton@suse.com\n    _rc, out, _err = utils.get_stdout_stderr(\"sfdisk -l\")\n    disks = re.findall(r'^Disk\\s*(/.+):', out, re.M)\n    invoke(\"partprobe\", *disks)\n\n\ndef probe_partitions():\n    status_long(\"Probing for new partitions\")\n    partprobe()\n    sleep(5)\n    status_done()\n\n\ndef check_tty():\n    \"\"\"\n    Check for pseudo-tty: Cannot display read prompts without a TTY (bnc#892702)\n    \"\"\"\n    if _context.yes_to_all:\n        return\n    if not sys.stdin.isatty():\n        error(\"No pseudo-tty detected! Use -t option to ssh if calling remotely.\")\n\n\ndef my_hostname_resolves():\n    import socket\n    hostname = utils.this_node()\n    try:\n        socket.gethostbyname(hostname)\n        return True\n    except socket.error:\n        return False\n\n\ndef check_prereqs(stage):\n    warned = False\n\n    if not my_hostname_resolves():\n        warn(\"Hostname '{}' is unresolvable. {}\".format(\n            utils.this_node(),\n            \"Please add an entry to /etc/hosts or configure DNS.\"))\n        warned = True\n\n    timekeepers = ('chronyd.service', 'ntp.service', 'ntpd.service')\n    timekeeper = None\n    for tk in timekeepers:\n        if utils.service_is_available(tk):\n            timekeeper = tk\n            break\n\n    if timekeeper is None:\n        warn(\"No NTP service found.\")\n        warned = True\n    elif not utils.service_is_enabled(timekeeper):\n        warn(\"{} is not configured to start at system boot.\".format(timekeeper))\n        warned = True\n\n    if warned:\n        if not confirm(\"Do you want to continue anyway?\"):\n            return False\n\n    firewall_open_basic_ports()\n    return True\n\n\ndef log_start():\n    \"\"\"\n    Convenient side-effect: this will die immediately if the log file\n    is not writable (e.g. if not running as root)\n    \"\"\"\n    # Reload rsyslog to make sure it logs with the correct hostname\n    if utils.service_is_active(\"rsyslog.service\"):\n        invoke(\"systemctl reload rsyslog.service\")\n    datestr = utils.get_stdout(\"date --rfc-3339=seconds\")[1]\n    log('================================================================')\n    log(\"%s %s\" % (datestr, \" \".join(sys.argv)))\n    log('----------------------------------------------------------------')\n\n\ndef init_network():\n    \"\"\"\n    Get all needed network information through utils.InterfacesInfo\n    \"\"\"\n    interfaces_inst = utils.InterfacesInfo(_context.ipv6, _context.second_heartbeat, _context.nic_list)\n    interfaces_inst.get_interfaces_info()\n    _context.default_nic_list = interfaces_inst.get_default_nic_list_from_route()\n    _context.default_ip_list = interfaces_inst.get_default_ip_list()\n\n    # local_ip_list and local_network_list are for validation\n    _context.local_ip_list = interfaces_inst.ip_list\n    _context.local_network_list = interfaces_inst.network_list\n    _context.interfaces_inst = interfaces_inst\n    # use two \"-i\" options equal to use \"-M\" option\n    if len(_context.default_nic_list) == 2 and not _context.second_heartbeat:\n        _context.second_heartbeat = True\n\n\ndef configure_firewall(tcp=None, udp=None):\n    if tcp is None:\n        tcp = []\n    if udp is None:\n        udp = []\n\n    def init_firewall_suse(tcp, udp):\n        if os.path.exists(SYSCONFIG_FW_CLUSTER):\n            cluster = utils.parse_sysconfig(SYSCONFIG_FW_CLUSTER)\n            tcpcurr = set(cluster.get(\"TCP\", \"\").split())\n            tcpcurr.update(tcp)\n            tcp = list(tcpcurr)\n            udpcurr = set(cluster.get(\"UDP\", \"\").split())\n            udpcurr.update(udp)\n            udp = list(udpcurr)\n\n        utils.sysconfig_set(SYSCONFIG_FW_CLUSTER, TCP=\" \".join(tcp), UDP=\" \".join(udp))\n\n        ext = \"\"\n        if os.path.exists(SYSCONFIG_FW):\n            fw = utils.parse_sysconfig(SYSCONFIG_FW)\n            ext = fw.get(\"FW_CONFIGURATIONS_EXT\", \"\")\n            if \"cluster\" not in ext.split():\n                ext = ext + \" cluster\"\n        utils.sysconfig_set(SYSCONFIG_FW, FW_CONFIGURATIONS_EXT=ext)\n\n        # No need to do anything else if the firewall is inactive\n        if not utils.service_is_active(\"SuSEfirewall2\"):\n            return\n\n        # Firewall is active, either restart or complain if we couldn't tweak it\n        status(\"Restarting firewall (tcp={}, udp={})\".format(\" \".join(tcp), \" \".join(udp)))\n        if not invokerc(\"rcSuSEfirewall2 restart\"):\n            error(\"Failed to restart firewall (SuSEfirewall2)\")\n\n    def init_firewall_firewalld(tcp, udp):\n        has_firewalld = utils.service_is_active(\"firewalld\")\n        cmdbase = 'firewall-cmd --zone=public --permanent ' if has_firewalld else 'firewall-offline-cmd --zone=public '\n\n        def cmd(args):\n            if not invokerc(cmdbase + args):\n                error(\"Failed to configure firewall.\")\n\n        for p in tcp:\n            cmd(\"--add-port={}/tcp\".format(p))\n\n        for p in udp:\n            cmd(\"--add-port={}/udp\".format(p))\n\n        if has_firewalld:\n            if not invokerc(\"firewall-cmd --reload\"):\n                error(\"Failed to reload firewall configuration.\")\n\n    def init_firewall_ufw(tcp, udp):\n        \"\"\"\n        try configuring firewall with ufw\n        \"\"\"\n        for p in tcp:\n            if not invokerc(\"ufw allow {}/tcp\".format(p)):\n                error(\"Failed to configure firewall (ufw)\")\n        for p in udp:\n            if not invokerc(\"ufw allow {}/udp\".format(p)):\n                error(\"Failed to configure firewall (ufw)\")\n\n    if utils.package_is_installed(\"firewalld\"):\n        init_firewall_firewalld(tcp, udp)\n    elif utils.package_is_installed(\"SuSEfirewall2\"):\n        init_firewall_suse(tcp, udp)\n    elif utils.package_is_installed(\"ufw\"):\n        init_firewall_ufw(tcp, udp)\n    else:\n        warn(\"Failed to detect firewall: Could not open ports tcp={}, udp={}\".format(\"|\".join(tcp), \"|\".join(udp)))\n\n\ndef firewall_open_basic_ports():\n    \"\"\"\n    Open ports for csync2, mgmtd, hawk & dlm respectively\n    \"\"\"\n    configure_firewall(tcp=[\"30865\", \"5560\", \"7630\", \"21064\"])\n\n\ndef firewall_open_corosync_ports():\n    \"\"\"\n    Have to do this separately, as we need general firewall config early\n    so csync2 works, but need corosync config *after* corosync.conf has\n    been created/updated.\n\n    Please note corosync uses two UDP ports mcastport (for mcast\n    receives) and mcastport - 1 (for mcast sends).\n\n    Also open QNetd/QDevice port if configured.\n    \"\"\"\n    # all mcastports defined in corosync config\n    udp = corosync.get_values(\"totem.interface.mcastport\")\n    udp.extend([str(int(p) - 1) for p in udp])\n\n    tcp = corosync.get_values(\"totem.quorum.device.net.port\")\n\n    configure_firewall(tcp=tcp, udp=udp)\n\n\ndef init_cluster_local():\n    # Caller should check this, but I'm paranoid...\n    if utils.service_is_active(\"corosync.service\"):\n        error(\"corosync service is running!\")\n\n    firewall_open_corosync_ports()\n\n    # reset password, but only if it's not already set\n    _rc, outp = utils.get_stdout(\"passwd -S hacluster\")\n    ps = outp.strip().split()[1]\n    pass_msg = \"\"\n    if ps not in (\"P\", \"PS\"):\n        log(': Resetting password of hacluster user')\n        rc, outp, errp = utils.get_stdout_stderr(\"passwd hacluster\", input_s=b\"linux\\nlinux\\n\")\n        if rc != 0:\n            warn(\"Failed to reset password of hacluster user: %s\" % (outp + errp))\n        else:\n            pass_msg = \", password 'linux'\"\n\n    # evil, but necessary\n    invoke(\"rm -f /var/lib/heartbeat/crm/* /var/lib/pacemaker/cib/*\")\n\n    # only try to start hawk if hawk is installed\n    if utils.service_is_available(\"hawk.service\"):\n        utils.start_service(\"hawk.service\", enable=True)\n        status(\"Hawk cluster interface is now running. To see cluster status, open:\")\n        status(\"  https://{}:7630/\".format(_context.default_ip_list[0]))\n        status(\"Log in with username 'hacluster'{}\".format(pass_msg))\n    else:\n        warn(\"Hawk not installed - not configuring web management interface.\")\n\n    if pass_msg:\n        warn(\"You should change the hacluster password to something more secure!\")\n\n    utils.start_service(\"pacemaker.service\", enable=True)\n    wait_for_cluster()\n\n\ndef install_tmp(tmpfile, to):\n    with open(tmpfile, \"r\") as src:\n        with utils.open_atomic(to, \"w\") as dst:\n            for line in src:\n                dst.write(line)\n\n\ndef append(fromfile, tofile):\n    log(\"+ cat %s >> %s\" % (fromfile, tofile))\n    with open(tofile, \"a\") as tf:\n        with open(fromfile, \"r\") as ff:\n            tf.write(ff.read())\n\n\ndef append_unique(fromfile, tofile):\n    \"\"\"\n    Append unique content from fromfile to tofile\n    \"\"\"\n    if not utils.check_file_content_included(fromfile, tofile):\n        append(fromfile, tofile)\n\n\ndef rmfile(path, ignore_errors=False):\n    \"\"\"\n    Try to remove the given file, and\n    report an error on failure\n    \"\"\"\n    try:\n        os.remove(path)\n    except os.error as err:\n        if not ignore_errors:\n            error(\"Failed to remove {}: {}\".format(path, err))\n\n\ndef mkdirs_owned(dirs, mode=0o777, uid=-1, gid=-1):\n    \"\"\"\n    Create directory path, setting the mode and\n    ownership of the leaf directory to mode/uid/gid.\n    \"\"\"\n    if not os.path.exists(dirs):\n        try:\n            os.makedirs(dirs, mode)\n        except OSError as err:\n            error(\"Failed to create {}: {}\".format(dirs, err))\n        if uid != -1 or gid != -1:\n            utils.chown(dirs, uid, gid)\n\n\ndef init_ssh():\n    \"\"\"\n    Configure passwordless SSH.\n    \"\"\"\n    utils.start_service(\"sshd.service\", enable=True)\n    configure_local_ssh_key()\n\n\ndef configure_local_ssh_key():\n    \"\"\"\n    Configure ssh rsa key locally\n\n    If /root/.ssh/id_rsa not exist, generate a new one\n    Add /root/.ssh/id_rsa.pub to /root/.ssh/authorized_keys anyway, make sure itself authorized\n    \"\"\"\n    if not os.path.exists(RSA_PRIVATE_KEY):\n        status(\"Generating SSH key\")\n        invoke(\"ssh-keygen -q -f {} -C 'Cluster Internal on {}' -N ''\".format(RSA_PRIVATE_KEY, utils.this_node()))\n    if not os.path.exists(AUTHORIZED_KEYS_FILE):\n        open(AUTHORIZED_KEYS_FILE, 'w').close()\n    append_unique(RSA_PUBLIC_KEY, AUTHORIZED_KEYS_FILE)\n\n\ndef init_ssh_remote():\n    \"\"\"\n    Called by ha-cluster-join\n    \"\"\"\n    authorized_keys_file = \"/root/.ssh/authorized_keys\"\n    if not os.path.exists(authorized_keys_file):\n        open(authorized_keys_file, 'w').close()\n    authkeys = open(authorized_keys_file, \"r+\")\n    authkeys_data = authkeys.read()\n    for key in (\"id_rsa\", \"id_dsa\", \"id_ecdsa\", \"id_ed25519\"):\n        fn = os.path.join(\"/root/.ssh\", key)\n        if not os.path.exists(fn):\n            continue\n        keydata = open(fn + \".pub\").read()\n        if keydata not in authkeys_data:\n            append(fn + \".pub\", authorized_keys_file)\n\n\ndef append_to_remote_file(fromfile, remote_node, tofile):\n    \"\"\"\n    Append content of fromfile to tofile on remote_node\n    \"\"\"\n    err_details_string = \"\"\"\n    crmsh has no way to help you to setup up passwordless ssh among nodes at this time. \n    As the hint, likely, `PasswordAuthentication` is 'no' in /etc/ssh/sshd_config. \n    Given in this case, users must setup passwordless ssh beforehand, or change it to 'yes' and manage passwords properly\n    \"\"\"\n    cmd = \"cat {} | ssh -oStrictHostKeyChecking=no root@{} 'cat >> {}'\".format(fromfile, remote_node, tofile)\n    rc, _, err = invoke(cmd)\n    if not rc:\n        error(\"Failed to append contents of {} to {}:\\n\\\"{}\\\"\\n{}\".format(fromfile, remote_node, err, err_details_string))\n\n\ndef init_csync2():\n    status(\"Configuring csync2\")\n    if os.path.exists(CSYNC2_KEY):\n        if not confirm(\"csync2 is already configured - overwrite?\"):\n            return\n\n    invoke(\"rm\", \"-f\", CSYNC2_KEY)\n    status_long(\"Generating csync2 shared key (this may take a while)\")\n    if not invokerc(\"csync2\", \"-k\", CSYNC2_KEY):\n        error(\"Can't create csync2 key {}\".format(CSYNC2_KEY))\n    status_done()\n\n    utils.str2file(\"\"\"group ha_group\n{\nkey /etc/csync2/key_hagroup;\nhost %s;\ninclude /etc/booth;\ninclude /etc/corosync/corosync.conf;\ninclude /etc/corosync/authkey;\ninclude /etc/csync2/csync2.cfg;\ninclude /etc/csync2/key_hagroup;\ninclude /etc/ctdb/nodes;\ninclude /etc/drbd.conf;\ninclude /etc/drbd.d;\ninclude /etc/ha.d/ldirectord.cf;\ninclude /etc/lvm/lvm.conf;\ninclude /etc/multipath.conf;\ninclude /etc/samba/smb.conf;\ninclude /etc/sysconfig/nfs;\ninclude /etc/sysconfig/pacemaker;\ninclude /etc/sysconfig/sbd;\ninclude /etc/pacemaker/authkey;\ninclude /etc/modules-load.d/watchdog.conf;\n}\n    \"\"\" % (utils.this_node()), CSYNC2_CFG)\n\n    utils.start_service(\"csync2.socket\", enable=True)\n    status_long(\"csync2 checking files\")\n    invoke(\"csync2\", \"-cr\", \"/\")\n    status_done()\n\n\ndef csync2_update(path):\n    '''\n    Sync path to all peers\n\n    If there was a conflict, use '-f' to force this side to win\n    '''\n    invoke(\"csync2 -rm {}\".format(path))\n    if invokerc(\"csync2 -rxv {}\".format(path)):\n        return\n    invoke(\"csync2 -rf {}\".format(path))\n    if not invokerc(\"csync2 -rxv {}\".format(path)):\n        warn(\"{} was not synced\".format(path))\n\n\ndef init_csync2_remote():\n    \"\"\"\n    It would be nice if we could just have csync2.cfg include a directory,\n    which in turn included one file per node which would be referenced via\n    something like \"group ha_group { ... config: /etc/csync2/hosts/*; }\"\n    That way, adding a new node would just mean adding a single new file\n    to that directory.  Unfortunately, the 'config' statement only allows\n    inclusion of specific individual files, not multiple files via wildcard.\n    So we have this function which is called by ha-cluster-join to add the new\n    remote node to csync2 config on some existing node.  It is intentionally\n    not documented in ha-cluster-init's user-visible usage information.\n    \"\"\"\n    newhost = _context.cluster_node\n    if not newhost:\n        error(\"Hostname not specified\")\n\n    curr_cfg = open(CSYNC2_CFG).read()\n\n    was_quiet = _context.quiet\n    try:\n        _context.quiet = True\n        # if host doesn't already exist in csync2 config, add it\n        if not re.search(r\"^\\s*host.*\\s+%s\\s*;\" % (newhost), curr_cfg, flags=re.M):\n            curr_cfg = re.sub(r\"\\bhost.*\\s+\\S+\\s*;\", r\"\\g<0>\\n\\thost %s;\" % (utils.doublequote(newhost)), curr_cfg, count=1)\n            utils.str2file(curr_cfg, CSYNC2_CFG)\n            csync2_update(\"/\")\n        else:\n            log(\": Not updating %s - remote host %s already exists\" % (CSYNC2_CFG, newhost))\n    finally:\n        _context.quiet = was_quiet\n\n\ndef init_corosync_auth():\n    \"\"\"\n    Generate the corosync authkey\n    \"\"\"\n    if os.path.exists(COROSYNC_AUTH):\n        if not confirm(\"%s already exists - overwrite?\" % (COROSYNC_AUTH)):\n            return\n        rmfile(COROSYNC_AUTH)\n    invoke(\"corosync-keygen -l\")\n\n\ndef init_remote_auth():\n    \"\"\"\n    Generate the pacemaker-remote authkey\n    \"\"\"\n    if os.path.exists(PCMK_REMOTE_AUTH):\n        if not confirm(\"%s already exists - overwrite?\" % (PCMK_REMOTE_AUTH)):\n            return\n        rmfile(PCMK_REMOTE_AUTH)\n\n    pcmk_remote_dir = os.path.dirname(PCMK_REMOTE_AUTH)\n    mkdirs_owned(pcmk_remote_dir, mode=0o750, gid=\"haclient\")\n    if not invokerc(\"dd if=/dev/urandom of={} bs=4096 count=1\".format(PCMK_REMOTE_AUTH)):\n        warn(\"Failed to create pacemaker authkey: {}\".format(PCMK_REMOTE_AUTH))\n    utils.chown(PCMK_REMOTE_AUTH, \"hacluster\", \"haclient\")\n    os.chmod(PCMK_REMOTE_AUTH, 0o640)\n\n\nclass Validation(object):\n    \"\"\"\n    Class to validate values from interactive inputs\n    \"\"\"\n\n    def __init__(self, value, prev_value_list=[]):\n        \"\"\"\n        Init function\n        \"\"\"\n        self.value = value\n        self.prev_value_list = prev_value_list\n        if self.value in self.prev_value_list:\n            raise ValueError(\"Already in use: {}\".format(self.value))\n\n    def _is_mcast_addr(self):\n        \"\"\"\n        Check whether the address is multicast address\n        \"\"\"\n        if not utils.IP.is_mcast(self.value):\n            raise ValueError(\"{} is not multicast address\".format(self.value))\n\n    def _is_local_addr(self, local_addr_list):\n        \"\"\"\n        Check whether the address is in local\n        \"\"\"\n        if self.value not in local_addr_list:\n            raise ValueError(\"Address must be a local address (one of {})\".format(local_addr_list))\n\n    def _is_valid_port(self):\n        \"\"\"\n        Check whether the port is valid\n        \"\"\"\n        if self.prev_value_list and abs(int(self.value) - int(self.prev_value_list[0])) <= 1:\n            raise ValueError(\"Port {} is already in use by corosync. Leave a gap between multiple rings.\".format(self.value))\n        if int(self.value) <= 1024 or int(self.value) > 65535:\n            raise ValueError(\"Valid port range should be 1025-65535\")\n\n    @classmethod\n    def valid_mcast_address(cls, addr, prev_value_list=[]):\n        \"\"\"\n        Check whether the address is already in use and whether the address is for multicast\n        \"\"\"\n        cls_inst = cls(addr, prev_value_list)\n        cls_inst._is_mcast_addr()\n\n    @classmethod\n    def valid_ucast_ip(cls, addr, prev_value_list=[]):\n        \"\"\"\n        Check whether the address is already in use and whether the address exists on local\n        \"\"\"\n        cls_inst = cls(addr, prev_value_list)\n        cls_inst._is_local_addr(_context.local_ip_list)\n\n    @classmethod\n    def valid_mcast_ip(cls, addr, prev_value_list=[]):\n        \"\"\"\n        Check whether the address is already in use and whether the address exists on local address and network\n        \"\"\"\n        cls_inst = cls(addr, prev_value_list)\n        cls_inst._is_local_addr(_context.local_ip_list + _context.local_network_list)\n\n    @classmethod\n    def valid_port(cls, port, prev_value_list=[]):\n        \"\"\"\n        Check whether the port is valid\n        \"\"\"\n        cls_inst = cls(port, prev_value_list)\n        cls_inst._is_valid_port()\n\n    @staticmethod\n    def valid_admin_ip(addr, prev_value_list=[]):\n        \"\"\"\n        Validate admin IP address\n        \"\"\"\n        ipv6 = utils.IP.is_ipv6(addr)\n\n        # Check whether this IP already configured in cluster\n        ping_cmd = \"ping6\" if ipv6 else \"ping\"\n        if invokerc(\"{} -c 1 {}\".format(ping_cmd, addr)):\n            raise ValueError(\"Address already in use: {}\".format(addr))\n\n\ndef init_corosync_unicast():\n\n    if _context.yes_to_all:\n        status(\"Configuring corosync (unicast)\")\n    else:\n        status(\"\"\"\nConfigure Corosync (unicast):\n  This will configure the cluster messaging layer.  You will need\n  to specify a network address over which to communicate (default\n  is {}'s network, but you can use the network address of any\n  active interface).\n\"\"\".format(_context.default_nic_list[0]))\n\n    ringXaddr_res = []\n    mcastport_res = []\n    default_ports = [\"5405\", \"5407\"]\n    two_rings = False\n\n    for i in range(2):\n        ringXaddr = prompt_for_string(\n                'Address for ring{}'.format(i),\n                default=pick_default_value(_context.default_ip_list, ringXaddr_res),\n                valid_func=Validation.valid_ucast_ip,\n                prev_value=ringXaddr_res)\n        if not ringXaddr:\n            error(\"No value for ring{}\".format(i))\n        ringXaddr_res.append(ringXaddr)\n\n        mcastport = prompt_for_string(\n                'Port for ring{}'.format(i),\n                match='[0-9]+',\n                default=pick_default_value(default_ports, mcastport_res),\n                valid_func=Validation.valid_port,\n                prev_value=mcastport_res)\n        if not mcastport:\n            error(\"Expected a multicast port for ring{}\".format(i))\n        mcastport_res.append(mcastport)\n\n        if i == 1 or \\\n           not _context.second_heartbeat or \\\n           not confirm(\"\\nAdd another heartbeat line?\"):\n            break\n        two_rings = True\n\n    corosync.create_configuration(\n            clustername=_context.cluster_name,\n            ringXaddr=ringXaddr_res,\n            mcastport=mcastport_res,\n            transport=\"udpu\",\n            ipv6=_context.ipv6,\n            two_rings=two_rings)\n    csync2_update(corosync.conf())\n\n\ndef init_corosync_multicast():\n    def gen_mcastaddr():\n        if _context.ipv6:\n            return \"ff3e::%s:%d\" % (\n                ''.join([random.choice('0123456789abcdef') for _ in range(4)]),\n                random.randint(0, 9))\n        return \"239.%d.%d.%d\" % (\n            random.randint(0, 255),\n            random.randint(0, 255),\n            random.randint(1, 255))\n\n    if _context.yes_to_all:\n        status(\"Configuring corosync\")\n    else:\n        status(\"\"\"\nConfigure Corosync:\n  This will configure the cluster messaging layer.  You will need\n  to specify a network address over which to communicate (default\n  is {}'s network, but you can use the network address of any\n  active interface).\n\"\"\".format(_context.default_nic_list[0]))\n\n    bindnetaddr_res = []\n    mcastaddr_res = []\n    mcastport_res = []\n    default_ports = [\"5405\", \"5407\"]\n    two_rings = False\n\n    for i in range(2):\n        bindnetaddr = prompt_for_string(\n                'IP or network address to bind to',\n                default=pick_default_value(_context.default_ip_list, bindnetaddr_res),\n                valid_func=Validation.valid_mcast_ip,\n                prev_value=bindnetaddr_res)\n        if not bindnetaddr:\n            error(\"No value for bindnetaddr\")\n        bindnetaddr_res.append(bindnetaddr)\n\n        mcastaddr = prompt_for_string(\n                'Multicast address',\n                default=gen_mcastaddr(),\n                valid_func=Validation.valid_mcast_address,\n                prev_value=mcastaddr_res)\n        if not mcastaddr:\n            error(\"No value for mcastaddr\")\n        mcastaddr_res.append(mcastaddr)\n\n        mcastport = prompt_for_string(\n                'Multicast port',\n                match='[0-9]+',\n                default=pick_default_value(default_ports, mcastport_res),\n                valid_func=Validation.valid_port,\n                prev_value=mcastport_res)\n        if not mcastport:\n            error(\"No value for mcastport\")\n        mcastport_res.append(mcastport)\n\n        if i == 1 or \\\n           not _context.second_heartbeat or \\\n           not confirm(\"\\nConfigure a second multicast ring?\"):\n            break\n        two_rings = True\n\n    nodeid = None\n    if _context.ipv6:\n        nodeid = utils.gen_nodeid_from_ipv6(_context.default_ip_list[0])\n\n    corosync.create_configuration(\n        clustername=_context.cluster_name,\n        bindnetaddr=bindnetaddr_res,\n        mcastaddr=mcastaddr_res,\n        mcastport=mcastport_res,\n        ipv6=_context.ipv6,\n        nodeid=nodeid,\n        two_rings=two_rings)\n    csync2_update(corosync.conf())\n\n\ndef init_corosync():\n    \"\"\"\n    Configure corosync (unicast or multicast, encrypted?)\n    \"\"\"\n    def requires_unicast():\n        host = utils.detect_cloud()\n        if host is not None:\n            status(\"Detected cloud platform: {}\".format(host))\n        return host is not None\n\n    init_corosync_auth()\n\n    if os.path.exists(corosync.conf()):\n        if not confirm(\"%s already exists - overwrite?\" % (corosync.conf())):\n            return\n\n    if _context.unicast or requires_unicast():\n        init_corosync_unicast()\n    else:\n        init_corosync_multicast()\n\n\ndef is_block_device(dev):\n    from stat import S_ISBLK\n    try:\n        rc = S_ISBLK(os.stat(dev).st_mode)\n    except OSError:\n        return False\n    return rc\n\n\ndef list_partitions(dev):\n    rc, outp, errp = utils.get_stdout_stderr(\"parted -s %s print\" % (dev))\n    partitions = []\n    for line in outp.splitlines():\n        m = re.match(r\"^\\s*([0-9]+)\\s*\", line)\n        if m:\n            partitions.append(m.group(1))\n    if rc != 0:\n        # ignore \"Error: /dev/vdb: unrecognised disk label\"\n        if errp.count('\\n') > 1 or \"unrecognised disk label\" not in errp.strip():\n            error(\"Failed to list partitions in {}: {}\".format(dev, errp))\n    return partitions\n\n\ndef list_devices(dev):\n    \"TODO: THIS IS *WRONG* FOR MULTIPATH! (but possibly nothing we can do about it)\"\n    _rc, outp = utils.get_stdout(\"fdisk -l %s\" % (dev))\n    partitions = []\n    for line in outp.splitlines():\n        m = re.match(r\"^(\\/dev\\S+)\", line)\n        if m:\n            partitions.append(m.group(1))\n    return partitions\n\n\ndef init_storage():\n    \"\"\"\n    Configure SBD and OCFS2 both on the same storage device.\n    \"\"\"\n    dev = _context.shared_device\n    partitions = []\n    dev_looks_sane = False\n\n    if _context.yes_to_all or not dev:\n        status(\"Configuring shared storage\")\n    else:\n        status(\"\"\"\nConfigure Shared Storage:\n  You will need to provide the path to a shared storage device,\n  for example a SAN volume or iSCSI target.  The device path must\n  be persistent and consistent across all nodes in the cluster,\n  so /dev/disk/by-id/* devices are a good choice.  This device\n  will be automatically paritioned into two pieces, 1MB for SBD\n  fencing, and the remainder for an OCFS2 filesystem.\n\"\"\")\n\n    while not dev_looks_sane:\n        dev = prompt_for_string('Path to storage device (e.g. /dev/disk/by-id/...)', r'\\/.*', dev)\n        if not dev:\n            error(\"No value for shared storage device\")\n\n        if not is_block_device(dev):\n            if _context.yes_to_all:\n                error(dev + \" is not a block device\")\n            else:\n                print(\"    That doesn't look like a block device\", file=sys.stderr)\n        else:\n            #\n            # Got something that looks like a block device, there\n            # are four possibilities now:\n            #\n            #  1) It's completely broken/inaccessible\n            #  2) No recognizable partition table\n            #  3) Empty partition table\n            #  4) Non-empty parition table\n            #\n            partitions = list_partitions(dev)\n            if partitions:\n                status(\"WARNING: Partitions exist on %s!\" % (dev))\n                if confirm(\"Are you ABSOLUTELY SURE you want to overwrite?\"):\n                    dev_looks_sane = True\n                else:\n                    dev = \"\"\n            else:\n                # It's either broken, no partition table, or empty partition table\n                status(\"%s appears to be empty\" % (dev))\n                if confirm(\"Device appears empty (no partition table). Do you want to use {}?\".format(dev)):\n                    dev_looks_sane = True\n                else:\n                    dev = \"\"\n\n    if partitions:\n        if not confirm(\"Really?\"):\n            return\n        status_long(\"Erasing existing partitions...\")\n        for part in partitions:\n            if not invokerc(\"parted -s %s rm %s\" % (dev, part)):\n                error(\"Failed to remove partition %s from %s\" % (part, dev))\n        status_done()\n\n    status_long(\"Creating partitions...\")\n    if not invokerc(\"parted\", \"-s\", dev, \"mklabel\", \"msdos\"):\n        error(\"Failed to create partition table\")\n\n    # This is a bit rough, and probably won't result in great performance,\n    # but it's fine for test/demo purposes to carve off 1MB for SBD.  Note\n    # we have to specify the size of the first partition in this in bytes\n    # rather than MB, or parted's rounding gives us a ~30Kb partition\n    # (see rhbz#623268).\n    if not invokerc(\"parted -s %s mkpart primary 0 1048576B\" % (dev)):\n        error(\"Failed to create first partition on %s\" % (dev))\n    if not invokerc(\"parted -s %s mkpart primary 1M 100%%\" % (dev)):\n        error(\"Failed to create second partition\")\n\n    status_done()\n\n    # TODO: May not be strictly necessary, but...\n    probe_partitions()\n\n    # TODO: THIS IS *WRONG* FOR MULTIPATH! (but possibly nothing we can do about it)\n    devices = list_devices(dev)\n\n    _context.sbd_device = devices[0]\n    if not _context.sbd_device:\n        error(\"Unable to determine device path for SBD partition\")\n\n    _context.ocfs2_device = devices[1]\n    if not _context.ocfs2_device:\n        error(\"Unable to determine device path for OCFS2 partition\")\n\n    status(\"Created %s for SBD partition\" % (_context.sbd_device))\n    status(\"Created %s for OCFS2 partition\" % (_context.ocfs2_device))\n\n\ndef init_sbd():\n    \"\"\"\n    Configure SBD (Storage-based fencing).\n\n    SBD can also run in diskless mode if no device\n    is configured.\n    \"\"\"\n    _context.sbd_manager.sbd_init()\n\n\ndef init_cluster():\n    \"\"\"\n    Initial cluster configuration.\n    \"\"\"\n    init_cluster_local()\n\n    _rc, nnodes = utils.get_stdout(\"crm_node -l\")\n    nnodes = len(nnodes.splitlines())\n    if nnodes < 1:\n        error(\"No nodes found in cluster\")\n    if nnodes > 1:\n        error(\"Joined existing cluster - will not reconfigure.\")\n\n    status(\"Loading initial cluster configuration\")\n\n    crm_configure_load(\"update\", \"\"\"\nproperty cib-bootstrap-options: stonith-enabled=false\nop_defaults op-options: timeout=600 record-pending=true\nrsc_defaults rsc-options: resource-stickiness=1 migration-threshold=3\n\"\"\")\n\n    _context.sbd_manager.configure_sbd_resource()\n\n\ndef init_vgfs():\n    \"\"\"\n    Configure cluster OCFS2 device.\n    \"\"\"\n    dev = _context.ocfs2_device\n    if not dev:\n        error(\"vgfs stage requires -o <dev>\")\n    mntpoint = \"/srv/clusterfs\"\n\n    if not is_block_device(dev):\n        error(\"OCFS2 device \\\"{}\\\" does not exist\".format(dev))\n\n    # TODO: configurable mountpoint and vg name\n    crm_configure_load(\"update\", \"\"\"\nprimitive dlm ocf:pacemaker:controld op start timeout=90 op stop timeout=100 op monitor interval=60 timeout=60\nprimitive clusterfs Filesystem directory=%(mntpoint)s fstype=ocfs2 device=%(dev)s \\\n    op monitor interval=20 timeout=40 op start timeout=60 op stop timeout=60 \\\n    meta target-role=Stopped\nclone base-clone dlm meta interleave=true\nclone c-clusterfs clusterfs meta interleave=true clone-max=8\norder base-then-clusterfs inf: base-clone c-clusterfs\ncolocation clusterfs-with-base inf: c-clusterfs base-clone\n    \"\"\" % {\"mntpoint\": utils.doublequote(mntpoint), \"dev\": utils.doublequote(dev)})\n\n    wait_for_resource(\"Waiting for DLM\", \"dlm:0\")\n    wait_for_stop(\"Making sure filesystem is not active\", \"clusterfs:0\")\n\n    _rc, blkid, _err = utils.get_stdout_stderr(\"blkid %s\" % (dev))\n    if \"TYPE\" in blkid:\n        if not confirm(\"Exiting filesystem found on \\\"{}\\\" - destroy?\".format(dev)):\n            for res in (\"base-clone\", \"c-clusterfs\"):\n                invoke(\"crm resource stop %s\" % (res))\n                wait_for_stop(\"Waiting for resource %s to stop\" % (res), res)\n            invoke(\"crm configure delete dlm clusterfs base-group base-clone c-clusterfs base-then-clusterfs clusterfs-with-base\")\n\n    status_long(\"Creating OCFS2 filesystem\")\n    # TODO: want \"-T vmstore\", but this'll only fly on >2GB partition\n    # Note: using undocumented '-x' switch to avoid prompting if overwriting\n    # existing partition.  For the commit that introduced this, see:\n    # http://oss.oracle.com/git/?p=ocfs2-tools.git;a=commit;h=8345a068479196172190f4fa287052800fa2b66f\n    # TODO: if make the cluster name configurable, we need to update it here too\n    if not invokerc(\"mkfs.ocfs2 --cluster-stack pcmk --cluster-name %s -N 8 -x %s\" % (_context.cluster_name, dev)):\n        error(\"Failed to create OCFS2 filesystem on %s\" % (dev))\n    status_done()\n\n    # TODO: refactor, maybe\n    if not invokerc(\"mkdir -p %s\" % (mntpoint)):\n        error(\"Can't create mountpoint %s\" % (mntpoint))\n    if not invokerc(\"crm resource meta clusterfs delete target-role\"):\n        error(\"Can't start cluster filesystem clone\")\n    wait_for_resource(\"Waiting for %s to be mounted\" % (mntpoint), \"clusterfs:0\")\n\n\ndef init_admin():\n    # Skip this section when -y is passed\n    # unless $ADMIN_IP is set\n    adminaddr = _context.admin_ip\n    if _context.yes_to_all and not adminaddr:\n        return\n\n    if not adminaddr:\n        status(\"\"\"\nConfigure Administration IP Address:\n  Optionally configure an administration virtual IP\n  address. The purpose of this IP address is to\n  provide a single IP that can be used to interact\n  with the cluster, rather than using the IP address\n  of any specific cluster node.\n\"\"\")\n        if not confirm(\"Do you wish to configure a virtual IP address?\"):\n            return\n\n        adminaddr = prompt_for_string('Virtual IP', valid_func=Validation.valid_admin_ip)\n        if not adminaddr:\n            error(\"Expected an IP address\")\n\n    crm_configure_load(\"update\", 'primitive admin-ip IPaddr2 ip=%s op monitor interval=10 timeout=20' % (utils.doublequote(adminaddr)))\n    wait_for_resource(\"Configuring virtual IP ({})\".format(adminaddr), \"admin-ip\")\n\n\ndef init_qdevice():\n    \"\"\"\n    Setup qdevice and qnetd service\n    \"\"\"\n    # If don't want to config qdevice, return\n    if not _context.qdevice_inst:\n        utils.disable_service(\"corosync-qdevice.service\")\n        return\n\n    status(\"\"\"\nConfigure Qdevice/Qnetd:\"\"\")\n    qdevice_inst = _context.qdevice_inst\n    qnetd_addr = qdevice_inst.qnetd_addr\n    # Configure ssh passwordless to qnetd if detect password is needed\n    if utils.check_ssh_passwd_need(qnetd_addr):\n        status(\"Copy ssh key to qnetd node({})\".format(qnetd_addr))\n        rc, _, err = invoke(\"ssh-copy-id -i /root/.ssh/id_rsa.pub root@{}\".format(qnetd_addr))\n        if not rc:\n            error(\"Failed to copy ssh key: {}\".format(err))\n    # Start qdevice service if qdevice already configured\n    if utils.is_qdevice_configured() and not confirm(\"Qdevice is already configured - overwrite?\"):\n        start_qdevice_service()\n        return\n\n    # Validate qnetd node\n    qdevice_inst.valid_qnetd()\n    # Config qdevice\n    config_qdevice()\n    # Execute certificate process when tls flag is on\n    if utils.is_qdevice_tls_on():\n        status_long(\"Qdevice certification process\")\n        qdevice_inst.certificate_process_on_init()\n        status_done()\n\n    start_qdevice_service()\n\n\ndef start_qdevice_service():\n    \"\"\"\n    Start qdevice and qnetd service\n    \"\"\"\n    qdevice_inst = _context.qdevice_inst\n    qnetd_addr = qdevice_inst.qnetd_addr\n\n    status(\"Enable corosync-qdevice.service in cluster\")\n    utils.cluster_run_cmd(\"systemctl enable corosync-qdevice\")\n    status(\"Starting corosync-qdevice.service in cluster\")\n    utils.cluster_run_cmd(\"systemctl start corosync-qdevice\")\n\n    status(\"Enable corosync-qnetd.service on {}\".format(qnetd_addr))\n    qdevice_inst.enable_qnetd()\n    status(\"Starting corosync-qnetd.service on {}\".format(qnetd_addr))\n    qdevice_inst.start_qnetd()\n\n\ndef config_qdevice():\n    \"\"\"\n    Process of config qdevice\n    \"\"\"\n    qdevice_inst = _context.qdevice_inst\n\n    qdevice_inst.remove_qdevice_db()\n    qdevice_inst.write_qdevice_config()\n    if not corosync.is_unicast():\n        corosync.add_nodelist_from_cmaptool()\n    status_long(\"Update configuration\")\n    update_expected_votes()\n    utils.cluster_run_cmd(\"crm corosync reload\")\n    status_done()\n\n\ndef init():\n    \"\"\"\n    Basic init\n    \"\"\"\n    log_start()\n    init_network()\n\n\ndef join_ssh(seed_host):\n    \"\"\"\n    SSH configuration for joining node.\n    \"\"\"\n    if not seed_host:\n        error(\"No existing IP/hostname specified (use -c option)\")\n\n    utils.start_service(\"sshd.service\", enable=True)\n    configure_local_ssh_key()\n    swap_public_ssh_key(seed_host)\n\n    # This makes sure the seed host has its own SSH keys in its own\n    # authorized_keys file (again, to help with the case where the\n    # user has done manual initial setup without the assistance of\n    # ha-cluster-init).\n    rc, _, err = invoke(\"ssh root@{} crm cluster init -i {} ssh_remote\".format(seed_host, _context.default_nic_list[0]))\n    if not rc:\n        error(\"Can't invoke crm cluster init -i {} ssh_remote on {}: {}\".format(_context.default_nic_list[0], seed_host, err))\n\n\ndef swap_public_ssh_key(remote_node):\n    \"\"\"\n    Swap public ssh key between remote_node and local\n    \"\"\"\n    # Detect whether need password to login to remote_node\n    if utils.check_ssh_passwd_need(remote_node):\n        # If no passwordless configured, paste /root/.ssh/id_rsa.pub to remote_node's /root/.ssh/authorized_keys\n        status(\"Configuring SSH passwordless with root@{}\".format(remote_node))\n        # After this, login to remote_node is passwordless\n        append_to_remote_file(RSA_PUBLIC_KEY, remote_node, AUTHORIZED_KEYS_FILE)\n\n    try:\n        # Fetch public key file from remote_node\n        public_key_file_remote = fetch_public_key_from_remote_node(remote_node)\n    except ValueError as err:\n        warn(err)\n        return\n    # Append public key file from remote_node to local's /root/.ssh/authorized_keys\n    # After this, login from remote_node is passwordless\n    # Should do this step even passwordless is True, to make sure we got two-way passwordless\n    append_unique(public_key_file_remote, AUTHORIZED_KEYS_FILE)\n\n\ndef fetch_public_key_from_remote_node(node):\n    \"\"\"\n    Fetch public key file from remote node\n    Return a temp file contains public key\n    Return None if no key exist\n    \"\"\"\n\n    # For dsa, might need to add PubkeyAcceptedKeyTypes=+ssh-dss to config file, see\n    # https://superuser.com/questions/1016989/ssh-dsa-keys-no-longer-work-for-password-less-authentication\n    for key in (\"id_rsa\", \"id_ecdsa\", \"id_ed25519\", \"id_dsa\"):\n        public_key_file = \"/root/.ssh/{}.pub\".format(key)\n        cmd = \"ssh -oStrictHostKeyChecking=no root@{} 'test -f {}'\".format(node, public_key_file)\n        if not invokerc(cmd):\n            continue\n        _, temp_public_key_file = tmpfiles.create()\n        cmd = \"scp -oStrictHostKeyChecking=no root@{}:{} {}\".format(node, public_key_file, temp_public_key_file)\n        rc, _, err = invoke(cmd)\n        if not rc:\n            error(\"Failed to run \\\"{}\\\": {}\".format(cmd, err))\n        return temp_public_key_file\n    raise ValueError(\"No ssh key exist on {}\".format(node))\n\n\ndef join_csync2(seed_host):\n    \"\"\"\n    Csync2 configuration for joining node.\n    \"\"\"\n    if not seed_host:\n        error(\"No existing IP/hostname specified (use -c option)\")\n    status_long(\"Configuring csync2\")\n\n    # Necessary if re-running join on a node that's been configured before.\n    rmfile(\"/var/lib/csync2/{}.db3\".format(utils.this_node()), ignore_errors=True)\n\n    # Not automatically updating /etc/hosts - risky in the general case.\n    # etc_hosts_add_me\n    # local hosts_line=$(etc_hosts_get_me)\n    # [ -n \"$hosts_line\" ] || error \"No valid entry for $(hostname) in /etc/hosts - csync2 can't work\"\n\n    # If we *were* updating /etc/hosts, the next line would have \"\\\"$hosts_line\\\"\" as\n    # the last arg (but this requires re-enabling this functionality in ha-cluster-init)\n    cmd = \"crm cluster init -i {} csync2_remote {}\".format(_context.default_nic_list[0], utils.this_node())\n    rc, _, err = invoke(\"ssh -o StrictHostKeyChecking=no root@{} {}\".format(seed_host, cmd))\n    if not rc:\n        error(\"Can't invoke \\\"{}\\\" on {}: {}\".format(cmd, seed_host, err))\n\n    # This is necessary if syncing /etc/hosts (to ensure everyone's got the\n    # same list of hosts)\n    # local tmp_conf=/etc/hosts.$$\n    # invoke scp root@seed_host:/etc/hosts $tmp_conf \\\n    #   || error \"Can't retrieve /etc/hosts from seed_host\"\n    # install_tmp $tmp_conf /etc/hosts\n    rc, _, err = invoke(\"scp root@%s:'/etc/csync2/{csync2.cfg,key_hagroup}' /etc/csync2\" % (seed_host))\n    if not rc:\n        error(\"Can't retrieve csync2 config from {}: {}\".format(seed_host, err))\n\n    utils.start_service(\"csync2.socket\", enable=True)\n\n    # Sync new config out.  This goes to all hosts; csync2.cfg definitely\n    # needs to go to all hosts (else hosts other than the seed and the\n    # joining host won't have the joining host in their config yet).\n    # Strictly, the rest of the files need only go to the new host which\n    # could theoretically be effected using `csync2 -xv -P $(hostname)`,\n    # but this still leaves all the other files in dirty state (becuase\n    # they haven't gone to all nodes in the cluster, which means a\n    # subseqent join of another node can fail its sync of corosync.conf\n    # when it updates expected_votes.  Grrr...\n    if not invokerc('ssh -o StrictHostKeyChecking=no root@{} \"csync2 -rm /; csync2 -rxv || csync2 -rf / && csync2 -rxv\"'.format(seed_host)):\n        print(\"\")\n        warn(\"csync2 run failed - some files may not be sync'd\")\n\n    status_done()\n\n\ndef join_ssh_merge(_cluster_node):\n    status(\"Merging known_hosts\")\n\n    me = utils.this_node()\n    hosts = [m.group(1)\n             for m in re.finditer(r\"^\\s*host\\s*([^ ;]+)\\s*;\", open(CSYNC2_CFG).read(), re.M)\n             if m.group(1) != me]\n    if not hosts:\n        hosts = [_cluster_node]\n        warn(\"Unable to extract host list from %s\" % (CSYNC2_CFG))\n\n    try:\n        import parallax\n    except ImportError:\n        error(\"parallax python library is missing\")\n\n    opts = parallax.Options()\n    opts.ssh_options = ['StrictHostKeyChecking=no']\n\n    # The act of using pssh to connect to every host (without strict host key\n    # checking) ensures that at least *this* host has every other host in its\n    # known_hosts\n    known_hosts_new = set()\n    cat_cmd = \"[ -e /root/.ssh/known_hosts ] && cat /root/.ssh/known_hosts || true\"\n    log(\"parallax.call {} : {}\".format(hosts, cat_cmd))\n    results = parallax.call(hosts, cat_cmd, opts)\n    for host, result in results.items():\n        if isinstance(result, parallax.Error):\n            warn(\"Failed to get known_hosts from {}: {}\".format(host, str(result)))\n        else:\n            if result[1]:\n                known_hosts_new.update((utils.to_ascii(result[1]) or \"\").splitlines())\n    if known_hosts_new:\n        hoststxt = \"\\n\".join(sorted(known_hosts_new))\n        tmpf = utils.str2tmp(hoststxt)\n        log(\"parallax.copy {} : {}\".format(hosts, hoststxt))\n        results = parallax.copy(hosts, tmpf, \"/root/.ssh/known_hosts\")\n        for host, result in results.items():\n            if isinstance(result, parallax.Error):\n                warn(\"scp to {} failed ({}), known_hosts update may be incomplete\".format(host, str(result)))\n\n\ndef update_expected_votes():\n    # get a list of nodes, excluding remote nodes\n    nodelist = None\n    loop_count = 0\n    device_votes = 0\n    nodecount = 0\n    expected_votes = 0\n    while True:\n        rc, nodelist_text = utils.get_stdout(\"cibadmin -Ql --xpath '/cib/status/node_state'\")\n        if rc == 0:\n            try:\n                nodelist_xml = etree.fromstring(nodelist_text)\n                nodelist = [n.get('uname') for n in nodelist_xml.xpath('//node_state') if n.get('remote_node') != 'true']\n                if len(nodelist) >= 2:\n                    break\n            except Exception:\n                break\n        # timeout: 10 seconds\n        if loop_count == 10:\n            break\n        loop_count += 1\n        sleep(1)\n\n    # Increase expected_votes\n    # TODO: wait to adjust expected_votes until after cluster join,\n    # so that we can ask the cluster for the current membership list\n    # Have to check if a qnetd device is configured and increase\n    # expected_votes in that case\n    is_qdevice_configured = utils.is_qdevice_configured()\n    if nodelist is None:\n        for v in corosync.get_values(\"quorum.expected_votes\"):\n            expected_votes = v\n\n            # For node >= 2, expected_votes = nodecount + device_votes\n            # Assume nodecount is N, for ffsplit, qdevice only has one vote\n            # which means that device_votes is 1, ie:expected_votes = N + 1;\n            # while for lms, qdevice has N - 1 votes, ie: expected_votes = N + (N - 1)\n            # and update quorum.device.net.algorithm based on device_votes\n\n            if corosync.get_value(\"quorum.device.net.algorithm\") == \"lms\":\n                device_votes = int((expected_votes - 1) / 2)\n                nodecount = expected_votes - device_votes\n                # as nodecount will increase 1, and device_votes is nodecount - 1\n                # device_votes also increase 1\n                device_votes += 1\n            elif corosync.get_value(\"quorum.device.net.algorithm\") == \"ffsplit\":\n                device_votes = 1\n                nodecount = expected_votes - device_votes\n            elif is_qdevice_configured:\n                device_votes = 0\n                nodecount = v\n\n            nodecount += 1\n            expected_votes = nodecount + device_votes\n            corosync.set_value(\"quorum.expected_votes\", str(expected_votes))\n    else:\n        nodecount = len(nodelist)\n        expected_votes = 0\n        # For node >= 2, expected_votes = nodecount + device_votes\n        # Assume nodecount is N, for ffsplit, qdevice only has one vote\n        # which means that device_votes is 1, ie:expected_votes = N + 1;\n        # while for lms, qdevice has N - 1 votes, ie: expected_votes = N + (N - 1)\n        if corosync.get_value(\"quorum.device.net.algorithm\") == \"ffsplit\":\n            device_votes = 1\n        if corosync.get_value(\"quorum.device.net.algorithm\") == \"lms\":\n            device_votes = nodecount - 1\n\n        if nodecount > 1:\n            expected_votes = nodecount + device_votes\n\n        if corosync.get_value(\"quorum.expected_votes\"):\n            corosync.set_value(\"quorum.expected_votes\", str(expected_votes))\n    if is_qdevice_configured:\n        corosync.set_value(\"quorum.device.votes\", device_votes)\n    corosync.set_value(\"quorum.two_node\", 1 if expected_votes == 2 else 0)\n\n    csync2_update(corosync.conf())\n\n\ndef setup_passwordless_with_other_nodes(init_node):\n    \"\"\"\n    Setup passwordless with other cluster nodes\n\n    Should fetch the node list from init node, then swap the key\n    \"\"\"\n    # Fetch cluster nodes list\n    cmd = \"ssh -o StrictHostKeyChecking=no root@{} crm_node -l\".format(init_node)\n    rc, out, err = utils.get_stdout_stderr(cmd)\n    if rc != 0:\n        error(\"Can't fetch cluster nodes list from {}: {}\".format(init_node, err))\n    cluster_nodes_list = []\n    for line in out.splitlines():\n        _, node, stat = line.split()\n        if stat == \"member\":\n            cluster_nodes_list.append(node)\n\n    # Filter out init node from cluster_nodes_list\n    cmd = \"ssh -o StrictHostKeyChecking=no root@{} hostname\".format(init_node)\n    rc, out, err = utils.get_stdout_stderr(cmd)\n    if rc != 0:\n        error(\"Can't fetch hostname of {}: {}\".format(init_node, err))\n    if out in cluster_nodes_list:\n        cluster_nodes_list.remove(out)\n\n    # Swap ssh public key between join node and other cluster nodes\n    for node in cluster_nodes_list:\n        swap_public_ssh_key(node)\n\n\ndef join_cluster(seed_host):\n    \"\"\"\n    Cluster configuration for joining node.\n    \"\"\"\n    def get_local_nodeid():\n        # for IPv6\n        return utils.gen_nodeid_from_ipv6(_context.local_ip_list[0])\n\n    def update_nodeid(nodeid, node=None):\n        # for IPv6\n        if node and node != utils.this_node():\n            cmd = \"crm corosync set totem.nodeid %d\" % nodeid\n            invoke(\"crm cluster run '{}' {}\".format(cmd, node))\n        else:\n            corosync.set_value(\"totem.nodeid\", nodeid)\n\n    shutil.copy(corosync.conf(), COROSYNC_CONF_ORIG)\n\n    # check if use IPv6\n    ipv6_flag = False\n    ipv6 = corosync.get_value(\"totem.ip_version\")\n    if ipv6 and ipv6 == \"ipv6\":\n        ipv6_flag = True\n    _context.ipv6 = ipv6_flag\n\n    init_network()\n\n    # check whether have two rings\n    rrp_flag = False\n    rrp = corosync.get_value(\"totem.rrp_mode\")\n    if rrp in ('active', 'passive'):\n        rrp_flag = True\n\n    # Need to do this if second (or subsequent) node happens to be up and\n    # connected to storage while it's being repartitioned on the first node.\n    probe_partitions()\n\n    # It would be massively useful at this point if new nodes could come\n    # up in standby mode, so we could query the CIB locally to see if\n    # there was any further local setup that needed doing, e.g.: creating\n    # mountpoints for clustered filesystems.  Unfortunately we don't have\n    # that yet, so the following crawling horror takes a punt on the seed\n    # node being up, then asks it for a list of mountpoints...\n    if _context.cluster_node:\n        _rc, outp, _ = utils.get_stdout_stderr(\"ssh -o StrictHostKeyChecking=no root@{} 'cibadmin -Q --xpath \\\"//primitive\\\"'\".format(seed_host))\n        if outp:\n            xml = etree.fromstring(outp)\n            mountpoints = xml.xpath(' and '.join(['//primitive[@class=\"ocf\"',\n                                                  '@provider=\"heartbeat\"',\n                                                  '@type=\"Filesystem\"]']) +\n                                    '/instance_attributes/nvpair[@name=\"directory\"]/@value')\n            for m in mountpoints:\n                invoke(\"mkdir -p {}\".format(m))\n    else:\n        status(\"No existing IP/hostname specified - skipping mountpoint detection/creation\")\n\n    # Bump expected_votes in corosync.conf\n    # TODO(must): this is rather fragile (see related code in ha-cluster-remove)\n\n    # If corosync.conf() doesn't exist or is empty, we will fail here. (bsc#943227)\n    if not os.path.exists(corosync.conf()):\n        error(\"{} is not readable. Please ensure that hostnames are resolvable.\".format(corosync.conf()))\n\n    # if unicast, we need to add our node to $corosync.conf()\n    is_unicast = corosync.is_unicast()\n    if is_unicast:\n        ringXaddr_res = []\n        for i in 0, 1:\n            while True:\n                ringXaddr = prompt_for_string(\n                        'Address for ring{}'.format(i),\n                        default=pick_default_value(_context.default_ip_list, ringXaddr_res),\n                        valid_func=Validation.valid_ucast_ip,\n                        prev_value=ringXaddr_res)\n                if not ringXaddr:\n                    error(\"No value for ring{}\".format(i))\n                ringXaddr_res.append(ringXaddr)\n                break\n            if not rrp_flag:\n                break\n        print(\"\")\n        invoke(\"rm -f /var/lib/heartbeat/crm/* /var/lib/pacemaker/cib/*\")\n        try:\n            corosync.add_node_ucast(ringXaddr_res)\n        except corosync.IPAlreadyConfiguredError as e:\n            warn(e)\n        csync2_update(corosync.conf())\n        invoke(\"ssh -o StrictHostKeyChecking=no root@{} corosync-cfgtool -R\".format(seed_host))\n\n    _context.sbd_manager.join_sbd(seed_host)\n\n    if ipv6_flag and not is_unicast:\n        # for ipv6 mcast\n        # using ipv6 need nodeid configured\n        local_nodeid = get_local_nodeid()\n        update_nodeid(local_nodeid)\n\n    is_qdevice_configured = utils.is_qdevice_configured()\n    if is_qdevice_configured and not is_unicast:\n        # expected_votes here maybe is \"0\", set to \"3\" to make sure cluster can start\n        corosync.set_value(\"quorum.expected_votes\", \"3\")\n\n    # Initialize the cluster before adjusting quorum. This is so\n    # that we can query the cluster to find out how many nodes\n    # there are (so as not to adjust multiple times if a previous\n    # attempt to join the cluster failed)\n    init_cluster_local()\n\n    status_long(\"Reloading cluster configuration\")\n\n    if ipv6_flag and not is_unicast:\n        # for ipv6 mcast\n        nodeid_dict = {}\n        _rc, outp, _ = utils.get_stdout_stderr(\"crm_node -l\")\n        if _rc == 0:\n            for line in outp.split('\\n'):\n                tmp = line.split()\n                nodeid_dict[tmp[1]] = tmp[0]\n\n    # apply nodelist in cluster\n    if is_unicast or is_qdevice_configured:\n        invoke(\"crm cluster run 'crm corosync reload'\")\n\n    update_expected_votes()\n    # Trigger corosync config reload to ensure expected_votes is propagated\n    invoke(\"corosync-cfgtool -R\")\n\n    # Ditch no-quorum-policy=ignore\n    _rc, outp = utils.get_stdout(\"crm configure show\")\n    if re.search('no-quorum-policy=.*ignore', outp):\n        invoke(\"crm_attribute --attr-name no-quorum-policy --delete-attr\")\n\n    # if unicast, we need to reload the corosync configuration\n    # on the other nodes\n    if is_unicast:\n        invoke(\"crm cluster run 'crm corosync reload'\")\n\n    if ipv6_flag and not is_unicast:\n        # for ipv6 mcast\n        # after csync2_update, all config files are same\n        # but nodeid must be uniqe\n        for node in list(nodeid_dict.keys()):\n            if node == utils.this_node():\n                continue\n            update_nodeid(int(nodeid_dict[node]), node)\n        update_nodeid(local_nodeid)\n    status_done()\n\n    if is_qdevice_configured:\n        start_qdevice_on_join_node(seed_host)\n    else:\n        utils.disable_service(\"corosync-qdevice.service\")\n\n\ndef start_qdevice_on_join_node(seed_host):\n    \"\"\"\n    Doing qdevice certificate process and start qdevice service on join node\n    \"\"\"\n    status_long(\"Starting corosync-qdevice.service\")\n    if not corosync.is_unicast():\n        corosync.add_nodelist_from_cmaptool()\n        csync2_update(corosync.conf())\n        invoke(\"crm corosync reload\")\n    if utils.is_qdevice_tls_on():\n        qnetd_addr = corosync.get_value(\"quorum.device.net.host\")\n        qdevice_inst = corosync.QDevice(qnetd_addr, cluster_node=seed_host)\n        qdevice_inst.certificate_process_on_join()\n    utils.start_service(\"corosync-qdevice.service\", enable=True)\n    status_done()\n\n\ndef set_cluster_node_ip():\n    \"\"\"\n    ringx_addr might be hostname or IP\n    _context.cluster_node by now is always hostname\n\n    If ring0_addr is IP, we should get the configured iplist which belong _context.cluster_node\n    Then filter out which one is configured as ring0_addr\n    At last assign that ip to _context.cluster_node_ip which will be removed later\n    \"\"\"\n    node = _context.cluster_node\n    addr_list = corosync.get_values('nodelist.node.ring0_addr')\n    if node in addr_list:\n        return\n\n    ip_list = utils.get_iplist_from_name(node)\n    for ip in ip_list:\n        if ip in addr_list:\n            _context.cluster_node_ip = ip\n            break\n\n\ndef stop_services(stop_list, remote_addr=None):\n    \"\"\"\n    Stop cluster related service\n    \"\"\"\n    for service in stop_list:\n        if utils.service_is_active(service, remote_addr=remote_addr):\n            status(\"Stopping the {}\".format(service))\n            utils.stop_service(service, disable=True, remote_addr=remote_addr)\n\n\ndef remove_node_from_cluster():\n    \"\"\"\n    Remove node from running cluster and the corosync / pacemaker configuration.\n    \"\"\"\n    node = _context.cluster_node\n    set_cluster_node_ip()\n\n    stop_services(SERVICES_STOP_LIST, remote_addr=node)\n\n    # delete configuration files from the node to be removed\n    rc, _, err = invoke('ssh -o StrictHostKeyChecking=no root@{} \"bash -c \\\\\\\"rm -f {}\\\\\\\"\"'.format(node, \" \".join(_context.rm_list)))\n    if not rc:\n        error(\"Deleting the configuration files failed: {}\".format(err))\n\n    # execute the command : crm node delete $HOSTNAME\n    status(\"Removing the node {}\".format(node))\n    if not invokerc(\"crm node delete {}\".format(node)):\n        error(\"Failed to remove {}\".format(node))\n\n    if not invokerc(\"sed -i /{}/d {}\".format(node, CSYNC2_CFG)):\n        error(\"Removing the node {} from {} failed\".format(node, CSYNC2_CFG))\n\n    # Remove node from nodelist\n    if corosync.get_values(\"nodelist.node.ring0_addr\"):\n        del_target = _context.cluster_node_ip or node\n        corosync.del_node(del_target)\n\n    decrease_expected_votes()\n\n    status(\"Propagating configuration changes across the remaining nodes\")\n    csync2_update(CSYNC2_CFG)\n    csync2_update(corosync.conf())\n\n    # Trigger corosync config reload to ensure expected_votes is propagated\n    invoke(\"corosync-cfgtool -R\")\n\n\ndef decrease_expected_votes():\n    '''\n    Decrement expected_votes in corosync.conf\n    '''\n    vote = corosync.get_value(\"quorum.expected_votes\")\n    if not vote:\n        return\n    quorum = int(vote)\n    new_quorum = quorum - 1\n    if utils.is_qdevice_configured():\n        new_nodecount = 0\n        device_votes = 0\n        nodecount = 0\n\n        if corosync.get_value(\"quorum.device.net.algorithm\") == \"lms\":\n            nodecount = int((quorum + 1)/2)\n            new_nodecount = nodecount - 1\n            device_votes = new_nodecount - 1\n\n        elif corosync.get_value(\"quorum.device.net.algorithm\") == \"ffsplit\":\n            device_votes = 1\n            nodecount = quorum - device_votes\n            new_nodecount = nodecount - 1\n\n        if new_nodecount > 1:\n            new_quorum = new_nodecount + device_votes\n        else:\n            new_quorum = 0\n\n        corosync.set_value(\"quorum.device.votes\", device_votes)\n    else:\n        corosync.set_value(\"quorum.two_node\", 1 if new_quorum == 2 else 0)\n    corosync.set_value(\"quorum.expected_votes\", str(new_quorum))\n\n\ndef bootstrap_init(context):\n    \"\"\"\n    Init cluster process\n    \"\"\"\n    global _context\n    _context = context\n\n    init()\n    _context.initialize_qdevice()\n    _context.validate_option()\n    _context.init_sbd_manager()\n\n    stage = _context.stage\n    if stage is None:\n        stage = \"\"\n\n    # vgfs stage requires running cluster, everything else requires inactive cluster,\n    # except ssh and csync2 (which don't care) and csync2_remote (which mustn't care,\n    # just in case this breaks ha-cluster-join on another node).\n    corosync_active = utils.service_is_active(\"corosync.service\")\n    if stage in (\"vgfs\", \"admin\", \"qdevice\"):\n        if not corosync_active:\n            error(\"Cluster is inactive - can't run %s stage\" % (stage))\n    elif stage == \"\":\n        if corosync_active:\n            error(\"Cluster is currently active - can't run\")\n    elif stage not in (\"ssh\", \"ssh_remote\", \"csync2\", \"csync2_remote\"):\n        if corosync_active:\n            error(\"Cluster is currently active - can't run %s stage\" % (stage))\n\n    # Need hostname resolution to work, want NTP (but don't block ssh_remote or csync2_remote)\n    if stage not in ('ssh_remote', 'csync2_remote'):\n        check_tty()\n        if not check_prereqs(stage):\n            return\n    elif stage == 'csync2_remote':\n        args = _context.args\n        log(\"args: {}\".format(args))\n        if len(args) != 2:\n            error(\"Expected NODE argument to csync2_remote\")\n        _context.cluster_node = args[1]\n\n    if stage != \"\":\n        globals()[\"init_\" + stage]()\n    else:\n        init_ssh()\n        init_csync2()\n        init_corosync()\n        init_remote_auth()\n        if _context.template == 'ocfs2':\n            if _context.sbd_device is None or _context.ocfs2_device is None:\n                init_storage()\n        init_sbd()\n\n        lock_inst = lock.Lock()\n        try:\n            with lock_inst.lock():\n                init_cluster()\n                if _context.template == 'ocfs2':\n                    init_vgfs()\n                init_admin()\n                init_qdevice()\n        except lock.ClaimLockError as err:\n            error(err)\n\n    status(\"Done (log saved to %s)\" % (LOG_FILE))\n\n\ndef bootstrap_join(context):\n    \"\"\"\n    Join cluster process\n    \"\"\"\n    global _context\n    _context = context\n\n    init()\n    _context.init_sbd_manager()\n    _context.validate_option()\n\n    check_tty()\n\n    corosync_active = utils.service_is_active(\"corosync.service\")\n    if corosync_active:\n        error(\"Abort: Cluster is currently active. Run this command on a node joining the cluster.\")\n\n    if not check_prereqs(\"join\"):\n        return\n\n    cluster_node = _context.cluster_node\n    if _context.stage != \"\":\n        globals()[\"join_\" + _context.stage](cluster_node)\n    else:\n        if not _context.yes_to_all and cluster_node is None:\n            status(\"\"\"Join This Node to Cluster:\n  You will be asked for the IP address of an existing node, from which\n  configuration will be copied.  If you have not already configured\n  passwordless ssh between nodes, you will be prompted for the root\n  password of the existing node.\n\"\"\")\n            cluster_node = prompt_for_string(\"IP address or hostname of existing node (e.g.: 192.168.1.1)\", \".+\")\n            _context.cluster_node = cluster_node\n\n        utils.ping_node(cluster_node)\n\n        join_ssh(cluster_node)\n\n        if not utils.service_is_active(\"pacemaker.service\", cluster_node):\n            error(\"Cluster is inactive on {}\".format(cluster_node))\n\n        lock_inst = lock.RemoteLock(cluster_node)\n        try:\n            with lock_inst.lock():\n                setup_passwordless_with_other_nodes(cluster_node)\n                join_remote_auth(cluster_node)\n                join_csync2(cluster_node)\n                join_ssh_merge(cluster_node)\n                join_cluster(cluster_node)\n        except (lock.SSHError, lock.ClaimLockError) as err:\n            error(err)\n\n    status(\"Done (log saved to %s)\" % (LOG_FILE))\n\n\ndef join_remote_auth(node):\n    if os.path.exists(PCMK_REMOTE_AUTH):\n        rmfile(PCMK_REMOTE_AUTH)\n    pcmk_remote_dir = os.path.dirname(PCMK_REMOTE_AUTH)\n    mkdirs_owned(pcmk_remote_dir, mode=0o750, gid=\"haclient\")\n    invoke(\"touch {}\".format(PCMK_REMOTE_AUTH))\n\n\ndef remove_qdevice():\n    \"\"\"\n    Remove qdevice service and configuration from cluster\n    \"\"\"\n    if not utils.is_qdevice_configured():\n        error(\"No QDevice configuration in this cluster\")\n    if not confirm(\"Removing QDevice service and configuration from cluster: Are you sure?\"):\n        return\n\n    status(\"Disable corosync-qdevice.service\")\n    invoke(\"crm cluster run 'systemctl disable corosync-qdevice'\")\n    status(\"Stopping corosync-qdevice.service\")\n    invoke(\"crm cluster run 'systemctl stop corosync-qdevice'\")\n\n    status_long(\"Removing QDevice configuration from cluster\")\n    qnetd_host = corosync.get_value('quorum.device.net.host')\n    qdevice_inst = corosync.QDevice(qnetd_host)\n    qdevice_inst.remove_qdevice_config()\n    qdevice_inst.remove_qdevice_db()\n    update_expected_votes()\n    invoke(\"crm cluster run 'crm corosync reload'\")\n    status_done()\n\n\ndef bootstrap_remove(context):\n    \"\"\"\n    Remove node from cluster, or remove qdevice configuration\n    \"\"\"\n    global _context\n    _context = context\n    force_flag = config.core.force or _context.force\n\n    init()\n\n    if not utils.service_is_active(\"corosync.service\"):\n        error(\"Cluster is not active - can't execute removing action\")\n\n    if _context.qdevice_rm_flag and _context.cluster_node:\n        error(\"Either remove node or qdevice\")\n\n    if _context.qdevice_rm_flag:\n        remove_qdevice()\n        return\n\n    if not _context.yes_to_all and _context.cluster_node is None:\n        status(\"\"\"Remove This Node from Cluster:\n  You will be asked for the IP address or name of an existing node,\n  which will be removed from the cluster. This command must be\n  executed from a different node in the cluster.\n\"\"\")\n        _context.cluster_node = prompt_for_string(\"IP address or hostname of cluster node (e.g.: 192.168.1.1)\", \".+\")\n\n    if not _context.cluster_node:\n        error(\"No existing IP/hostname specified (use -c option)\")\n\n    _context.cluster_node = get_cluster_node_hostname()\n\n    if not force_flag and not confirm(\"Removing node \\\"{}\\\" from the cluster: Are you sure?\".format(_context.cluster_node)):\n        return\n\n    if _context.cluster_node == utils.this_node():\n        if not force_flag:\n            error(\"Removing self requires --force\")\n        remove_self()\n        return\n\n    if _context.cluster_node in xmlutil.listnodes():\n        remove_node_from_cluster()\n    else:\n        error(\"Specified node {} is not configured in cluster! Unable to remove.\".format(_context.cluster_node))\n\n\ndef remove_self():\n    me = _context.cluster_node\n    yes_to_all = _context.yes_to_all\n    nodes = xmlutil.listnodes(include_remote_nodes=False)\n    othernode = next((x for x in nodes if x != me), None)\n    if othernode is not None:\n        # remove from other node\n        cmd = \"crm cluster remove{} -c {}\".format(\" -y\" if yes_to_all else \"\", me)\n        rc = utils.ext_cmd_nosudo(\"ssh{} -o StrictHostKeyChecking=no {} '{}'\".format(\"\" if yes_to_all else \" -t\", othernode, cmd))\n        if rc != 0:\n            error(\"Failed to remove this node from {}\".format(othernode))\n    else:\n        # disable and stop cluster\n        stop_services(SERVICES_STOP_LIST)\n        # remove all trace of cluster from this node\n        # delete configuration files from the node to be removed\n        if not invokerc('bash -c \"rm -f {}\"'.format(\" \".join(_context.rm_list))):\n            error(\"Deleting the configuration files failed\")\n\n\ndef init_common_geo():\n    \"\"\"\n    Tasks to do both on first and other geo nodes.\n    \"\"\"\n    if not utils.package_is_installed(\"booth\"):\n        error(\"Booth not installed - Not configurable as a geo cluster node.\")\n\n\nBOOTH_CFG = \"/etc/booth/booth.conf\"\nBOOTH_AUTH = \"/etc/booth/authkey\"\n\n\ndef init_csync2_geo():\n    \"\"\"\n    TODO: Configure csync2 for geo cluster\n    That is, create a second sync group which\n    syncs the geo configuration across the whole\n    geo cluster.\n    \"\"\"\n\n\ndef create_booth_authkey():\n    status(\"Create authentication key for booth\")\n    if os.path.exists(BOOTH_AUTH):\n        rmfile(BOOTH_AUTH)\n    rc, _, err = invoke(\"booth-keygen {}\".format(BOOTH_AUTH))\n    if not rc:\n        error(\"Failed to generate booth authkey: {}\".format(err))\n\n\ndef create_booth_config(arbitrator, clusters, tickets):\n    status(\"Configure booth\")\n\n    config_template = \"\"\"# The booth configuration file is \"/etc/booth/booth.conf\". You need to\n# prepare the same booth configuration file on each arbitrator and\n# each node in the cluster sites where the booth daemon can be launched.\n\n# \"transport\" means which transport layer booth daemon will use.\n# Currently only \"UDP\" is supported.\ntransport=\"UDP\"\nport=\"9929\"\n\"\"\"\n    cfg = [config_template]\n    if arbitrator is not None:\n        cfg.append(\"arbitrator=\\\"{}\\\"\".format(arbitrator))\n    for s in clusters.values():\n        cfg.append(\"site=\\\"{}\\\"\".format(s))\n    cfg.append(\"authfile=\\\"{}\\\"\".format(BOOTH_AUTH))\n    for t in tickets:\n        cfg.append(\"ticket=\\\"{}\\\"\\nexpire=\\\"600\\\"\".format(t))\n    cfg = \"\\n\".join(cfg) + \"\\n\"\n\n    if os.path.exists(BOOTH_CFG):\n        rmfile(BOOTH_CFG)\n    utils.str2file(cfg, BOOTH_CFG)\n    utils.chown(BOOTH_CFG, \"hacluster\", \"haclient\")\n    os.chmod(BOOTH_CFG, 0o644)\n\n\ndef bootstrap_init_geo(context):\n    \"\"\"\n    Configure as a geo cluster member.\n    \"\"\"\n    global _context\n    _context = context\n\n    if os.path.exists(BOOTH_CFG) and not confirm(\"This will overwrite {} - continue?\".format(BOOTH_CFG)):\n        return\n    if os.path.exists(BOOTH_AUTH) and not confirm(\"This will overwrite {} - continue?\".format(BOOTH_AUTH)):\n        return\n\n    init_common_geo()\n\n    # TODO:\n    # in /etc/drbd.conf or /etc/drbd.d/global_common.conf\n    # set common.startup.wfc-timeout 100\n    # set common.startup.degr-wfc-timeout 120\n\n    create_booth_authkey()\n    create_booth_config(_context.arbitrator, _context.clusters, _context.tickets)\n    status(\"Sync booth configuration across cluster\")\n    csync2_update(\"/etc/booth\")\n    init_csync2_geo()\n    geo_cib_config(_context.clusters)\n\n\ndef geo_fetch_config(node):\n    # TODO: clean this up\n    status(\"Retrieving configuration - This may prompt for root@%s:\" % (node))\n    tmpdir = tmpfiles.create_dir()\n    rc, _, err = invoke(\"scp -oStrictHostKeyChecking=no root@%s:'/etc/booth/*' %s/\" % (node, tmpdir))\n    if not rc:\n        error(\"Failed to retrieve configuration: {}\".format(err))\n    try:\n        if os.path.isfile(\"%s/authkey\" % (tmpdir)):\n            invoke(\"mv %s/authkey %s\" % (tmpdir, BOOTH_AUTH))\n            os.chmod(BOOTH_AUTH, 0o600)\n        if os.path.isfile(\"%s/booth.conf\" % (tmpdir)):\n            invoke(\"mv %s/booth.conf %s\" % (tmpdir, BOOTH_CFG))\n            os.chmod(BOOTH_CFG, 0o644)\n    except OSError as err:\n        raise ValueError(\"Problem encountered with booth configuration from {}: {}\".format(node, err))\n\n\ndef geo_cib_config(clusters):\n    cluster_name = corosync.get_values('totem.cluster_name')[0]\n    if cluster_name not in list(clusters.keys()):\n        error(\"Local cluster name is {}, expected {}\".format(cluster_name, \"|\".join(list(clusters.keys()))))\n\n    status(\"Configure cluster resources for booth\")\n    crm_template = Template(\"\"\"\nprimitive booth-ip ocf:heartbeat:IPaddr2 $iprules\nprimitive booth-site ocf:pacemaker:booth-site \\\n  meta resource-stickiness=\"INFINITY\" \\\n  params config=booth op monitor interval=\"10s\"\ngroup g-booth booth-ip booth-site meta target-role=Stopped\n\"\"\")\n    iprule = 'params rule #cluster-name eq {} ip=\"{}\"'\n\n    crm_configure_load(\"update\", crm_template.substitute(iprules=\" \".join(iprule.format(k, v) for k, v in clusters.items())))\n\n\ndef bootstrap_join_geo(context):\n    \"\"\"\n    Run on second cluster to add to a geo configuration.\n    It fetches its booth configuration from the other node (cluster node or arbitrator).\n    \"\"\"\n    global _context\n    _context = context\n    init_common_geo()\n    check_tty()\n    geo_fetch_config(_context.cluster_node)\n    status(\"Sync booth configuration across cluster\")\n    csync2_update(\"/etc/booth\")\n    geo_cib_config(_context.clusters)\n\n\ndef bootstrap_arbitrator(context):\n    \"\"\"\n    Configure this machine as an arbitrator.\n    It fetches its booth configuration from a cluster node already in the cluster.\n    \"\"\"\n    global _context\n    _context = context\n    node = _context.cluster_node\n\n    init_common_geo()\n    check_tty()\n    geo_fetch_config(node)\n    if not os.path.isfile(BOOTH_CFG):\n        error(\"Failed to copy {} from {}\".format(BOOTH_CFG, node))\n    # TODO: verify that the arbitrator IP in the configuration is us?\n    status(\"Enabling and starting the booth arbitrator service\")\n    utils.start_service(\"booth@booth\", enable=True)\n\n# EOF\n", "# Copyright (C) 2008-2011 Dejan Muhamedagic <dmuhamedagic@suse.de>\n# See COPYING for license information.\n\nimport os\nimport sys\nfrom tempfile import mkstemp\nimport subprocess\nimport re\nimport glob\nimport time\nimport datetime\nimport shutil\nimport shlex\nimport bz2\nimport fnmatch\nimport gc\nimport ipaddress\nimport argparse\nfrom contextlib import contextmanager, closing\nfrom . import config\nfrom . import userdir\nfrom . import constants\nfrom . import options\nfrom . import term\nfrom . import parallax\nfrom .msg import common_warn, common_info, common_debug, common_err, err_buf\n\n\ndef to_ascii(input_str):\n    \"\"\"Convert the bytes string to a ASCII string\n    Usefull to remove accent (diacritics)\"\"\"\n    if input_str is None:\n        return input_str\n    if isinstance(input_str, str):\n        return input_str\n    try:\n        return str(input_str, 'utf-8')\n    except UnicodeDecodeError:\n        if config.core.debug or options.regression_tests:\n            import traceback\n            traceback.print_exc()\n        return input_str.decode('utf-8', errors='ignore')\n\n\ndef filter_keys(key_list, args, sign=\"=\"):\n    \"\"\"Return list item which not be completed yet\"\"\"\n    return [s+sign for s in key_list if any_startswith(args, s+sign) is None]\n\n\ndef any_startswith(iterable, prefix):\n    \"\"\"Return first element in iterable which startswith prefix, or None.\"\"\"\n    for element in iterable:\n        if element.startswith(prefix):\n            return element\n    return None\n\n\ndef rindex(iterable, value):\n    return len(iterable) - iterable[::-1].index(value) - 1\n\n\ndef memoize(function):\n    \"Decorator to invoke a function once only for any argument\"\n    memoized = {}\n\n    def inner(*args):\n        if args in memoized:\n            return memoized[args]\n        r = function(*args)\n        memoized[args] = r\n        return r\n    return inner\n\n\n@contextmanager\ndef nogc():\n    gc.disable()\n    try:\n        yield\n    finally:\n        gc.enable()\n\n\ngetuser = userdir.getuser\ngethomedir = userdir.gethomedir\n\n\n@memoize\ndef this_node():\n    'returns name of this node (hostname)'\n    return os.uname()[1]\n\n\n_cib_shadow = 'CIB_shadow'\n_cib_in_use = ''\n\n\ndef set_cib_in_use(name):\n    os.putenv(_cib_shadow, name)\n    global _cib_in_use\n    _cib_in_use = name\n\n\ndef clear_cib_in_use():\n    os.unsetenv(_cib_shadow)\n    global _cib_in_use\n    _cib_in_use = ''\n\n\ndef get_cib_in_use():\n    return _cib_in_use\n\n\ndef get_tempdir():\n    return os.getenv(\"TMPDIR\") or \"/tmp\"\n\n\ndef is_program(prog):\n    \"\"\"Is this program available?\"\"\"\n    def isexec(filename):\n        return os.path.isfile(filename) and os.access(filename, os.X_OK)\n    for p in os.getenv(\"PATH\").split(os.pathsep):\n        f = os.path.join(p, prog)\n        if isexec(f):\n            return f\n    return None\n\n\ndef pacemaker_20_daemon(new, old):\n    \"helper to discover renamed pacemaker daemons\"\n    if is_program(new):\n        return new\n    return old\n\n\n@memoize\ndef pacemaker_attrd():\n    return pacemaker_20_daemon(\"pacemaker-attrd\", \"attrd\")\n\n\n@memoize\ndef pacemaker_based():\n    return pacemaker_20_daemon(\"pacemaker-based\", \"cib\")\n\n\n@memoize\ndef pacemaker_controld():\n    return pacemaker_20_daemon(\"pacemaker-controld\", \"crmd\")\n\n\n@memoize\ndef pacemaker_execd():\n    return pacemaker_20_daemon(\"pacemaker-execd\", \"lrmd\")\n\n\n@memoize\ndef pacemaker_fenced():\n    return pacemaker_20_daemon(\"pacemaker-fenced\", \"stonithd\")\n\n\n@memoize\ndef pacemaker_remoted():\n    return pacemaker_20_daemon(\"pacemaker-remoted\", \"pacemaker_remoted\")\n\n\n@memoize\ndef pacemaker_schedulerd():\n    return pacemaker_20_daemon(\"pacemaker-schedulerd\", \"pengine\")\n\n\ndef pacemaker_daemon(name):\n    if name == \"attrd\" or name == \"pacemaker-attrd\":\n        return pacemaker_attrd()\n    if name == \"cib\" or name == \"pacemaker-based\":\n        return pacemaker_based()\n    if name == \"crmd\" or name == \"pacemaker-controld\":\n        return pacemaker_controld()\n    if name == \"lrmd\" or name == \"pacemaker-execd\":\n        return pacemaker_execd()\n    if name == \"stonithd\" or name == \"pacemaker-fenced\":\n        return pacemaker_fenced()\n    if name == \"pacemaker_remoted\" or name == \"pacemeaker-remoted\":\n        return pacemaker_remoted()\n    if name == \"pengine\" or name == \"pacemaker-schedulerd\":\n        return pacemaker_schedulerd()\n    raise ValueError(\"Not a Pacemaker daemon name: {}\".format(name))\n\n\ndef can_ask():\n    \"\"\"\n    Is user-interactivity possible?\n    Checks if connected to a TTY.\n    \"\"\"\n    return (not options.ask_no) and sys.stdin.isatty()\n\n\ndef ask(msg):\n    \"\"\"\n    Ask for user confirmation.\n    If core.force is true, always return true.\n    If not interactive and core.force is false, always return false.\n    \"\"\"\n    if config.core.force:\n        common_info(\"%s [YES]\" % (msg))\n        return True\n    if not can_ask():\n        return False\n\n    msg += ' '\n    if msg.endswith('? '):\n        msg = msg[:-2] + ' (y/n)? '\n\n    while True:\n        try:\n            ans = input(msg)\n        except EOFError:\n            ans = 'n'\n        if ans:\n            ans = ans[0].lower()\n            if ans in 'yn':\n                return ans == 'y'\n\n\n# holds part of line before \\ split\n# for a multi-line input\n_LINE_BUFFER = ''\n\n\ndef get_line_buffer():\n    return _LINE_BUFFER\n\n\ndef multi_input(prompt=''):\n    \"\"\"\n    Get input from user\n    Allow multiple lines using a continuation character\n    \"\"\"\n    global _LINE_BUFFER\n    line = []\n    _LINE_BUFFER = ''\n    while True:\n        try:\n            text = input(prompt)\n        except EOFError:\n            return None\n        err_buf.incr_lineno()\n        if options.regression_tests:\n            print(\".INP:\", text)\n            sys.stdout.flush()\n            sys.stderr.flush()\n        stripped = text.strip()\n        if stripped.endswith('\\\\'):\n            stripped = stripped.rstrip('\\\\')\n            line.append(stripped)\n            _LINE_BUFFER += stripped\n            if prompt:\n                prompt = '   > '\n        else:\n            line.append(stripped)\n            break\n    return ''.join(line)\n\n\ndef verify_boolean(opt):\n    return opt.lower() in (\"yes\", \"true\", \"on\", \"1\") or \\\n        opt.lower() in (\"no\", \"false\", \"off\", \"0\")\n\n\ndef is_boolean_true(opt):\n    if opt in (None, False):\n        return False\n    if opt is True:\n        return True\n    return opt.lower() in (\"yes\", \"true\", \"on\", \"1\")\n\n\ndef is_boolean_false(opt):\n    if opt in (None, False):\n        return True\n    if opt is True:\n        return False\n    return opt.lower() in (\"no\", \"false\", \"off\", \"0\")\n\n\ndef get_boolean(opt, dflt=False):\n    if not opt:\n        return dflt\n    return is_boolean_true(opt)\n\n\ndef canonical_boolean(opt):\n    return 'true' if is_boolean_true(opt) else 'false'\n\n\ndef keyword_cmp(string1, string2):\n    return string1.lower() == string2.lower()\n\n\nclass olist(list):\n    \"\"\"\n    Implements the 'in' operator\n    in a case-insensitive manner,\n    allowing \"if x in olist(...)\"\n    \"\"\"\n    def __init__(self, keys):\n        super(olist, self).__init__([k.lower() for k in keys])\n\n    def __contains__(self, key):\n        return super(olist, self).__contains__(key.lower())\n\n    def append(self, key):\n        super(olist, self).append(key.lower())\n\n\ndef os_types_list(path):\n    l = []\n    for f in glob.glob(path):\n        if os.access(f, os.X_OK) and os.path.isfile(f):\n            a = f.split(\"/\")\n            l.append(a[-1])\n    return l\n\n\ndef listtemplates():\n    l = []\n    templates_dir = os.path.join(config.path.sharedir, 'templates')\n    for f in os.listdir(templates_dir):\n        if os.path.isfile(\"%s/%s\" % (templates_dir, f)):\n            l.append(f)\n    return l\n\n\ndef listconfigs():\n    l = []\n    for f in os.listdir(userdir.CRMCONF_DIR):\n        if os.path.isfile(\"%s/%s\" % (userdir.CRMCONF_DIR, f)):\n            l.append(f)\n    return l\n\n\ndef add_sudo(cmd):\n    if config.core.user:\n        return \"sudo -E -u %s %s\" % (config.core.user, cmd)\n    return cmd\n\n\ndef chown(path, user, group):\n    if isinstance(user, int):\n        uid = user\n    else:\n        import pwd\n        uid = pwd.getpwnam(user).pw_uid\n    if isinstance(group, int):\n        gid = group\n    else:\n        import grp\n        gid = grp.getgrnam(group).gr_gid\n    os.chown(path, uid, gid)\n\n\ndef ensure_sudo_readable(f):\n    # make sure the tempfile is readable to crm_diff (bsc#999683)\n    if config.core.user:\n        from pwd import getpwnam\n        uid = getpwnam(config.core.user).pw_uid\n        try:\n            os.chown(f, uid, -1)\n        except os.error as err:\n            common_err('Failed setting temporary file permissions: %s' % (err))\n            return False\n    return True\n\n\ndef pipe_string(cmd, s):\n    rc = -1  # command failed\n    cmd = add_sudo(cmd)\n    common_debug(\"piping string to %s\" % cmd)\n    if options.regression_tests:\n        print(\".EXT\", cmd)\n    p = subprocess.Popen(cmd, shell=True, stdin=subprocess.PIPE)\n    try:\n        # communicate() expects encoded bytes\n        if isinstance(s, str):\n            s = s.encode('utf-8')\n        p.communicate(s)\n        p.wait()\n        rc = p.returncode\n    except IOError as msg:\n        if \"Broken pipe\" not in str(msg):\n            common_err(msg)\n    return rc\n\n\ndef filter_string(cmd, s, stderr_on=True, shell=True):\n    rc = -1  # command failed\n    outp = ''\n    if stderr_on is True:\n        stderr = None\n    else:\n        stderr = subprocess.PIPE\n    cmd = add_sudo(cmd)\n    common_debug(\"pipe through %s\" % cmd)\n    if options.regression_tests:\n        print(\".EXT\", cmd)\n    p = subprocess.Popen(cmd,\n                         shell=shell,\n                         stdin=subprocess.PIPE,\n                         stdout=subprocess.PIPE,\n                         stderr=stderr)\n    try:\n        # bytes expected here\n        if isinstance(s, str):\n            s = s.encode('utf-8')\n        ret = p.communicate(s)\n        if stderr_on == 'stdout':\n            outp = b\"\\n\".join(ret)\n        else:\n            outp = ret[0]\n        p.wait()\n        rc = p.returncode\n    except OSError as err:\n        if err.errno != os.errno.EPIPE:\n            common_err(err.strerror)\n        common_info(\"from: %s\" % cmd)\n    except Exception as msg:\n        common_err(msg)\n        common_info(\"from: %s\" % cmd)\n    return rc, to_ascii(outp)\n\n\ndef str2tmp(_str, suffix=\".pcmk\"):\n    '''\n    Write the given string to a temporary file. Return the name\n    of the file.\n    '''\n    s = to_ascii(_str)\n    fd, tmp = mkstemp(suffix=suffix)\n    try:\n        f = os.fdopen(fd, \"w\")\n    except IOError as msg:\n        common_err(msg)\n        return\n    f.write(s)\n    if not s.endswith('\\n'):\n        f.write(\"\\n\")\n    f.close()\n    return tmp\n\n\n@contextmanager\ndef create_tempfile(suffix='', dir=None):\n    \"\"\" Context for temporary file.\n\n    Will find a free temporary filename upon entering\n    and will try to delete the file on leaving, even in case of an exception.\n\n    Parameters\n    ----------\n    suffix : string\n        optional file suffix\n    dir : string\n        optional directory to save temporary file in\n\n    (from http://stackoverflow.com/a/29491523)\n    \"\"\"\n    import tempfile\n    tf = tempfile.NamedTemporaryFile(delete=False, suffix=suffix, dir=dir)\n    tf.file.close()\n    try:\n        yield tf.name\n    finally:\n        try:\n            os.remove(tf.name)\n        except OSError as e:\n            if e.errno == 2:\n                pass\n            else:\n                raise\n\n\n@contextmanager\ndef open_atomic(filepath, mode=\"r\", buffering=-1, fsync=False, encoding=None):\n    \"\"\" Open temporary file object that atomically moves to destination upon\n    exiting.\n\n    Allows reading and writing to and from the same filename.\n\n    The file will not be moved to destination in case of an exception.\n\n    Parameters\n    ----------\n    filepath : string\n        the file path to be opened\n    fsync : bool\n        whether to force write the file to disk\n\n    (from http://stackoverflow.com/a/29491523)\n    \"\"\"\n\n    with create_tempfile(dir=os.path.dirname(os.path.abspath(filepath))) as tmppath:\n        with open(tmppath, mode, buffering, encoding=encoding) as file:\n            try:\n                yield file\n            finally:\n                if fsync:\n                    file.flush()\n                    os.fsync(file.fileno())\n        os.rename(tmppath, filepath)\n\n\ndef str2file(s, fname):\n    '''\n    Write a string to a file.\n    '''\n    try:\n        with open_atomic(fname, 'w', encoding='utf-8') as dst:\n            dst.write(to_ascii(s))\n    except IOError as msg:\n        common_err(msg)\n        return False\n    return True\n\n\ndef file2str(fname, noerr=True):\n    '''\n    Read a one line file into a string, strip whitespace around.\n    '''\n    try:\n        f = open(fname, \"r\")\n    except IOError as msg:\n        if not noerr:\n            common_err(msg)\n        return None\n    s = f.readline()\n    f.close()\n    return s.strip()\n\n\ndef file2list(fname):\n    '''\n    Read a file into a list (newlines dropped).\n    '''\n    try:\n        return open(fname).read().split('\\n')\n    except IOError as msg:\n        common_err(msg)\n        return None\n\n\ndef safe_open_w(fname):\n    if fname == \"-\":\n        f = sys.stdout\n    else:\n        if not options.batch and os.access(fname, os.F_OK):\n            if not ask(\"File %s exists. Do you want to overwrite it?\" % fname):\n                return None\n        try:\n            f = open(fname, \"w\")\n        except IOError as msg:\n            common_err(msg)\n            return None\n    return f\n\n\ndef safe_close_w(f):\n    if f and f != sys.stdout:\n        f.close()\n\n\ndef is_path_sane(name):\n    if re.search(r\"['`#*?$\\[\\]]\", name):\n        common_err(\"%s: bad path\" % name)\n        return False\n    return True\n\n\ndef is_filename_sane(name):\n    if re.search(r\"['`/#*?$\\[\\]]\", name):\n        common_err(\"%s: bad filename\" % name)\n        return False\n    return True\n\n\ndef is_name_sane(name):\n    if re.search(\"[']\", name):\n        common_err(\"%s: bad name\" % name)\n        return False\n    return True\n\n\ndef show_dot_graph(dotfile, keep_file=False, desc=\"transition graph\"):\n    cmd = \"%s %s\" % (config.core.dotty, dotfile)\n    if not keep_file:\n        cmd = \"(%s; rm -f %s)\" % (cmd, dotfile)\n    if options.regression_tests:\n        print(\".EXT\", cmd)\n    subprocess.Popen(cmd, shell=True, bufsize=0,\n                     stdin=None, stdout=None, stderr=None, close_fds=True)\n    common_info(\"starting %s to show %s\" % (config.core.dotty, desc))\n\n\ndef ext_cmd(cmd, shell=True):\n    cmd = add_sudo(cmd)\n    if options.regression_tests:\n        print(\".EXT\", cmd)\n    common_debug(\"invoke: %s\" % cmd)\n    return subprocess.call(cmd, shell=shell)\n\n\ndef ext_cmd_nosudo(cmd, shell=True):\n    if options.regression_tests:\n        print(\".EXT\", cmd)\n    return subprocess.call(cmd, shell=shell)\n\n\ndef rmdir_r(d):\n    # TODO: Make sure we're not deleting something we shouldn't!\n    if d and os.path.isdir(d):\n        shutil.rmtree(d)\n\n\ndef nvpairs2dict(pairs):\n    '''\n    takes a list of string of form ['a=b', 'c=d']\n    and returns {'a':'b', 'c':'d'}\n    '''\n    data = []\n    for var in pairs:\n        if '=' in var:\n            data.append(var.split('=', 1))\n        else:\n            data.append([var, None])\n    return dict(data)\n\n\ndef is_check_always():\n    '''\n    Even though the frequency may be set to always, it doesn't\n    make sense to do that with non-interactive sessions.\n    '''\n    return options.interactive and config.core.check_frequency == \"always\"\n\n\ndef get_check_rc():\n    '''\n    If the check mode is set to strict, then on errors we\n    return 2 which is the code for error. Otherwise, we\n    pretend that errors are warnings.\n    '''\n    return 2 if config.core.check_mode == \"strict\" else 1\n\n\n_LOCKDIR = \".lockdir\"\n_PIDF = \"pid\"\n\n\ndef check_locker(lockdir):\n    if not os.path.isdir(os.path.join(lockdir, _LOCKDIR)):\n        return\n    s = file2str(os.path.join(lockdir, _LOCKDIR, _PIDF))\n    pid = convert2ints(s)\n    if not isinstance(pid, int):\n        common_warn(\"history: removing malformed lock\")\n        rmdir_r(os.path.join(lockdir, _LOCKDIR))\n        return\n    try:\n        os.kill(pid, 0)\n    except OSError as err:\n        if err.errno == os.errno.ESRCH:\n            common_info(\"history: removing stale lock\")\n            rmdir_r(os.path.join(lockdir, _LOCKDIR))\n        else:\n            common_err(\"%s: %s\" % (_LOCKDIR, err.strerror))\n\n\n@contextmanager\ndef lock(lockdir):\n    \"\"\"\n    Ensure that the lock is released properly\n    even in the face of an exception between\n    acquire and release.\n    \"\"\"\n    def acquire_lock():\n        check_locker(lockdir)\n        while True:\n            try:\n                os.makedirs(os.path.join(lockdir, _LOCKDIR))\n                str2file(\"%d\" % os.getpid(), os.path.join(lockdir, _LOCKDIR, _PIDF))\n                return True\n            except OSError as err:\n                if err.errno != os.errno.EEXIST:\n                    common_err(\"Failed to acquire lock to %s: %s\" % (lockdir, err.strerror))\n                    return False\n                time.sleep(0.1)\n                continue\n            else:\n                return False\n\n    has_lock = acquire_lock()\n    try:\n        yield\n    finally:\n        if has_lock:\n            rmdir_r(os.path.join(lockdir, _LOCKDIR))\n\n\ndef mkdirp(d, mode=0o777):\n    if os.path.isdir(d):\n        return True\n    os.makedirs(d, mode=mode)\n\n\ndef pipe_cmd_nosudo(cmd):\n    if options.regression_tests:\n        print(\".EXT\", cmd)\n    proc = subprocess.Popen(cmd,\n                            shell=True,\n                            stdout=subprocess.PIPE,\n                            stderr=subprocess.PIPE)\n    (outp, err_outp) = proc.communicate()\n    proc.wait()\n    rc = proc.returncode\n    if rc != 0:\n        print(outp)\n        print(err_outp)\n    return rc\n\n\ndef run_cmd_on_remote(cmd, remote_addr, prompt_msg=None):\n    \"\"\"\n    Run a cmd on remote node\n    return (rc, stdout, err_msg)\n    \"\"\"\n    rc = 1\n    out_data = None\n    err_data = None\n\n    need_pw = check_ssh_passwd_need(remote_addr)\n    if need_pw and prompt_msg:\n        print(prompt_msg)\n    try:\n        result = parallax.parallax_call([remote_addr], cmd, need_pw)\n        rc, out_data, _ = result[0][1]\n    except ValueError as err:\n        err_match = re.search(\"Exited with error code ([0-9]+), Error output: (.*)\", str(err))\n        if err_match:\n            rc, err_data = err_match.groups()\n    finally:\n        return int(rc), to_ascii(out_data), err_data\n\n\ndef get_stdout(cmd, input_s=None, stderr_on=True, shell=True, raw=False):\n    '''\n    Run a cmd, return stdout output.\n    Optional input string \"input_s\".\n    stderr_on controls whether to show output which comes on stderr.\n    '''\n    if stderr_on:\n        stderr = None\n    else:\n        stderr = subprocess.PIPE\n    if options.regression_tests:\n        print(\".EXT\", cmd)\n    proc = subprocess.Popen(cmd,\n                            shell=shell,\n                            stdin=subprocess.PIPE,\n                            stdout=subprocess.PIPE,\n                            stderr=stderr)\n    stdout_data, stderr_data = proc.communicate(input_s)\n    if raw:\n        return proc.returncode, stdout_data\n    return proc.returncode, to_ascii(stdout_data).strip()\n\n\ndef get_stdout_stderr(cmd, input_s=None, shell=True, raw=False):\n    '''\n    Run a cmd, return (rc, stdout, stderr)\n    '''\n    if options.regression_tests:\n        print(\".EXT\", cmd)\n    proc = subprocess.Popen(cmd,\n                            shell=shell,\n                            stdin=input_s and subprocess.PIPE or None,\n                            stdout=subprocess.PIPE,\n                            stderr=subprocess.PIPE)\n    stdout_data, stderr_data = proc.communicate(input_s)\n    if raw:\n        return proc.returncode, stdout_data, stderr_data\n    return proc.returncode, to_ascii(stdout_data).strip(), to_ascii(stderr_data).strip()\n\n\ndef stdout2list(cmd, stderr_on=True, shell=True):\n    '''\n    Run a cmd, fetch output, return it as a list of lines.\n    stderr_on controls whether to show output which comes on stderr.\n    '''\n    rc, s = get_stdout(add_sudo(cmd), stderr_on=stderr_on, shell=shell)\n    if not s:\n        return rc, []\n    return rc, s.split('\\n')\n\n\ndef append_file(dest, src):\n    'Append src to dest'\n    try:\n        open(dest, \"a\").write(open(src).read())\n        return True\n    except IOError as msg:\n        common_err(\"append %s to %s: %s\" % (src, dest, msg))\n        return False\n\n\ndef get_dc():\n    cmd = \"crmadmin -D\"\n    rc, s = get_stdout(add_sudo(cmd))\n    if rc != 0:\n        return None\n    if not s.startswith(\"Designated\"):\n        return None\n    return s.split()[-1]\n\n\ndef wait4dc(what=\"\", show_progress=True):\n    '''\n    Wait for the DC to get into the S_IDLE state. This should be\n    invoked only after a CIB modification which would exercise\n    the PE. Parameter \"what\" is whatever the caller wants to be\n    printed if showing progress.\n\n    It is assumed that the DC is already in a different state,\n    usually it should be either PENGINE or TRANSITION. This\n    assumption may not be true, but there's a high chance that it\n    is since crmd should be faster to move through states than\n    this shell.\n\n    Further, it may also be that crmd already calculated the new\n    graph, did transition, and went back to the idle state. This\n    may in particular be the case if the transition turned out to\n    be empty.\n\n    Tricky. Though in practice it shouldn't be an issue.\n\n    There's no timeout, as we expect the DC to eventually becomes\n    idle.\n    '''\n    dc = get_dc()\n    if not dc:\n        common_warn(\"can't find DC\")\n        return False\n    cmd = \"crm_attribute -Gq -t crm_config -n crmd-transition-delay 2> /dev/null\"\n    delay = get_stdout(add_sudo(cmd))[1]\n    if delay:\n        delaymsec = crm_msec(delay)\n        if delaymsec > 0:\n            common_info(\"The crmd-transition-delay is configured. Waiting %d msec before check DC status.\" % delaymsec)\n            time.sleep(delaymsec // 1000)\n    cnt = 0\n    output_started = 0\n    init_sleep = 0.25\n    max_sleep = 1.00\n    sleep_time = init_sleep\n    while True:\n        dc = get_dc()\n        if not dc:\n            common_warn(\"DC lost during wait\")\n            return False\n        cmd = \"crmadmin -S %s\" % dc\n        rc, s = get_stdout(add_sudo(cmd))\n        if not s.startswith(\"Status\"):\n            common_warn(\"%s unexpected output: %s (exit code: %d)\" %\n                        (cmd, s, rc))\n            return False\n        try:\n            dc_status = s.split()[-2]\n        except:\n            common_warn(\"%s unexpected output: %s\" % (cmd, s))\n            return False\n        if dc_status == \"S_IDLE\":\n            if output_started:\n                sys.stderr.write(\" done\\n\")\n            return True\n        time.sleep(sleep_time)\n        if sleep_time < max_sleep:\n            sleep_time *= 2\n        if show_progress:\n            if not output_started:\n                output_started = 1\n                sys.stderr.write(\"waiting for %s to finish .\" % what)\n            cnt += 1\n            if cnt % 5 == 0:\n                sys.stderr.write(\".\")\n\n\ndef run_ptest(graph_s, nograph, scores, utilization, actions, verbosity):\n    '''\n    Pipe graph_s thru ptest(8). Show graph using dotty if requested.\n    '''\n    actions_filter = \"grep LogActions: | grep -vw Leave\"\n    ptest = \"2>&1 %s -x -\" % config.core.ptest\n    if re.search(\"simulate\", ptest) and \\\n            not re.search(\"-[RS]\", ptest):\n        ptest = \"%s -S\" % ptest\n    if verbosity:\n        if actions:\n            verbosity = 'v' * max(3, len(verbosity))\n        ptest = \"%s -%s\" % (ptest, verbosity.upper())\n    if scores:\n        ptest = \"%s -s\" % ptest\n    if utilization:\n        ptest = \"%s -U\" % ptest\n    if config.core.dotty and not nograph:\n        fd, dotfile = mkstemp()\n        ptest = \"%s -D %s\" % (ptest, dotfile)\n    else:\n        dotfile = None\n    # ptest prints to stderr\n    if actions:\n        ptest = \"%s | %s\" % (ptest, actions_filter)\n    if options.regression_tests:\n        ptest = \">/dev/null %s\" % ptest\n    common_debug(\"invoke: %s\" % ptest)\n    rc, s = get_stdout(ptest, input_s=graph_s)\n    if rc != 0:\n        common_debug(\"'%s' exited with (rc=%d)\" % (ptest, rc))\n        if actions and rc == 1:\n            common_warn(\"No actions found.\")\n        else:\n            common_warn(\"Simulation was unsuccessful (RC=%d).\" % (rc))\n    if dotfile:\n        if os.path.getsize(dotfile) > 0:\n            show_dot_graph(dotfile)\n        else:\n            common_warn(\"ptest produced empty dot file\")\n    else:\n        if not nograph:\n            common_info(\"install graphviz to see a transition graph\")\n    if s:\n        page_string(s)\n    return True\n\n\ndef is_id_valid(ident):\n    \"\"\"\n    Verify that the id follows the definition:\n    http://www.w3.org/TR/1999/REC-xml-names-19990114/#ns-qualnames\n    \"\"\"\n    if not ident:\n        return False\n    id_re = r\"^[A-Za-z_][\\w._-]*$\"\n    return re.match(id_re, ident)\n\n\ndef check_range(a):\n    \"\"\"\n    Verify that the integer range in list a is valid.\n    \"\"\"\n    if len(a) != 2:\n        return False\n    if not isinstance(a[0], int) or not isinstance(a[1], int):\n        return False\n    return int(a[0]) <= int(a[1])\n\n\ndef crm_msec(t):\n    '''\n    See lib/common/utils.c:crm_get_msec().\n    '''\n    convtab = {\n        'ms': (1, 1),\n        'msec': (1, 1),\n        'us': (1, 1000),\n        'usec': (1, 1000),\n        '': (1000, 1),\n        's': (1000, 1),\n        'sec': (1000, 1),\n        'm': (60*1000, 1),\n        'min': (60*1000, 1),\n        'h': (60*60*1000, 1),\n        'hr': (60*60*1000, 1),\n    }\n    if not t:\n        return -1\n    r = re.match(r\"\\s*(\\d+)\\s*([a-zA-Z]+)?\", t)\n    if not r:\n        return -1\n    if not r.group(2):\n        q = ''\n    else:\n        q = r.group(2).lower()\n    try:\n        mult, div = convtab[q]\n    except KeyError:\n        return -1\n    return (int(r.group(1))*mult) // div\n\n\ndef crm_time_cmp(a, b):\n    return crm_msec(a) - crm_msec(b)\n\n\ndef shorttime(ts):\n    if isinstance(ts, datetime.datetime):\n        return ts.strftime(\"%X\")\n    if ts is not None:\n        return time.strftime(\"%X\", time.localtime(ts))\n    return time.strftime(\"%X\", time.localtime(0))\n\n\ndef shortdate(ts):\n    if isinstance(ts, datetime.datetime):\n        return ts.strftime(\"%F\")\n    if ts is not None:\n        return time.strftime(\"%F\", time.localtime(ts))\n    return time.strftime(\"%F\", time.localtime(0))\n\n\ndef sort_by_mtime(l):\n    'Sort a (small) list of files by time mod.'\n    l2 = [(os.stat(x).st_mtime, x) for x in l]\n    l2.sort()\n    return [x[1] for x in l2]\n\n\ndef file_find_by_name(root, filename):\n    'Find a file within a tree matching fname'\n    assert root\n    assert filename\n    for root, dirnames, filenames in os.walk(root):\n        for filename in fnmatch.filter(filenames, filename):\n            return os.path.join(root, filename)\n    return None\n\n\ndef convert2ints(l):\n    \"\"\"\n    Convert a list of strings (or a string) to a list of ints.\n    All strings must be ints, otherwise conversion fails and None\n    is returned!\n    \"\"\"\n    try:\n        if isinstance(l, (tuple, list)):\n            return [int(x) for x in l]\n        # it's a string then\n        return int(l)\n    except ValueError:\n        return None\n\n\ndef is_int(s):\n    'Check if the string can be converted to an integer.'\n    try:\n        int(s)\n        return True\n    except ValueError:\n        return False\n\n\ndef is_process(s):\n    \"\"\"\n    Returns true if argument is the name of a running process.\n\n    s: process name\n    returns Boolean\n    \"\"\"\n    from os.path import join, basename\n    # find pids of running processes\n    pids = [pid for pid in os.listdir('/proc') if pid.isdigit()]\n    for pid in pids:\n        try:\n            cmdline = open(join('/proc', pid, 'cmdline'), 'rb').read()\n            procname = basename(to_ascii(cmdline).replace('\\x00', ' ').split(' ')[0])\n            if procname == s:\n                return True\n        except EnvironmentError:\n            # a process may have died since we got the list of pids\n            pass\n    return False\n\n\ndef print_stacktrace():\n    \"\"\"\n    Print the stack at the site of call\n    \"\"\"\n    import traceback\n    import inspect\n    sf = inspect.currentframe().f_back.f_back\n    traceback.print_stack(sf)\n\n\n@memoize\ndef cluster_stack():\n    if is_process(\"heartbeat:.[m]aster\"):\n        return \"heartbeat\"\n    elif is_process(\"[a]isexec\"):\n        return \"openais\"\n    elif os.path.exists(\"/etc/corosync/corosync.conf\") or is_program('corosync-cfgtool'):\n        return \"corosync\"\n    return \"\"\n\n\ndef edit_file(fname):\n    'Edit a file.'\n    if not fname:\n        return\n    if not config.core.editor:\n        return\n    return ext_cmd_nosudo(\"%s %s\" % (config.core.editor, fname))\n\n\ndef edit_file_ext(fname, template=''):\n    '''\n    Edit a file via a temporary file.\n    Raises IOError on any error.\n    '''\n    if not os.path.isfile(fname):\n        s = template\n    else:\n        s = open(fname).read()\n    filehash = hash(s)\n    tmpfile = str2tmp(s)\n    try:\n        try:\n            if edit_file(tmpfile) != 0:\n                return\n            s = open(tmpfile, 'r').read()\n            if hash(s) == filehash:  # file unchanged\n                return\n            f2 = open(fname, 'w')\n            f2.write(s)\n            f2.close()\n        finally:\n            os.unlink(tmpfile)\n    except OSError as e:\n        raise IOError(e)\n\n\ndef need_pager(s, w, h):\n    from math import ceil\n    cnt = 0\n    for l in s.split('\\n'):\n        # need to remove color codes\n        l = re.sub(r'\\${\\w+}', '', l)\n        cnt += int(ceil((len(l) + 0.5) / w))\n        if cnt >= h:\n            return True\n    return False\n\n\ndef term_render(s):\n    'Render for TERM.'\n    try:\n        return term.render(s)\n    except:\n        return s\n\n\ndef get_pager_cmd(*extra_opts):\n    'returns a commandline which calls the configured pager'\n    cmdline = [config.core.pager]\n    if os.path.basename(config.core.pager) == \"less\":\n        cmdline.append('-R')\n    cmdline.extend(extra_opts)\n    return ' '.join(cmdline)\n\n\ndef page_string(s):\n    'Page string rendered for TERM.'\n    if not s:\n        return\n    constants.need_reset = True\n    w, h = get_winsize()\n    if not need_pager(s, w, h):\n        print(term_render(s))\n    elif not config.core.pager or not can_ask() or options.batch:\n        print(term_render(s))\n    else:\n        pipe_string(get_pager_cmd(), term_render(s).encode('utf-8'))\n    constants.need_reset = False\n\n\ndef page_gen(g):\n    'Page lines generated by generator g'\n    w, h = get_winsize()\n    if not config.core.pager or not can_ask() or options.batch:\n        for line in g:\n            sys.stdout.write(term_render(line))\n    else:\n        pipe_string(get_pager_cmd(), term_render(\"\".join(g)))\n\n\ndef page_file(filename):\n    'Open file in pager'\n    if not os.path.isfile(filename):\n        return\n    return ext_cmd_nosudo(get_pager_cmd(filename), shell=True)\n\n\ndef get_winsize():\n    try:\n        import curses\n        curses.setupterm()\n        w = curses.tigetnum('cols')\n        h = curses.tigetnum('lines')\n    except:\n        try:\n            w = os.environ['COLS']\n            h = os.environ['LINES']\n        except KeyError:\n            w = 80\n            h = 25\n    return w, h\n\n\ndef multicolumn(l):\n    '''\n    A ls-like representation of a list of strings.\n    A naive approach.\n    '''\n    min_gap = 2\n    w, _ = get_winsize()\n    max_len = 8\n    for s in l:\n        if len(s) > max_len:\n            max_len = len(s)\n    cols = w // (max_len + min_gap)  # approx.\n    if not cols:\n        cols = 1\n    col_len = w // cols\n    for i in range(len(l) // cols + 1):\n        s = ''\n        for j in range(i * cols, (i + 1) * cols):\n            if not j < len(l):\n                break\n            if not s:\n                s = \"%-*s\" % (col_len, l[j])\n            elif (j + 1) % cols == 0:\n                s = \"%s%s\" % (s, l[j])\n            else:\n                s = \"%s%-*s\" % (s, col_len, l[j])\n        if s:\n            print(s)\n\n\ndef find_value(pl, name):\n    for n, v in pl:\n        if n == name:\n            return v\n    return None\n\n\ndef cli_replace_attr(pl, name, new_val):\n    for i, attr in enumerate(pl):\n        if attr[0] == name:\n            attr[1] = new_val\n            return\n\n\ndef cli_append_attr(pl, name, val):\n    pl.append([name, val])\n\n\ndef lines2cli(s):\n    '''\n    Convert a string into a list of lines. Replace continuation\n    characters. Strip white space, left and right. Drop empty lines.\n    '''\n    cl = []\n    l = s.split('\\n')\n    cum = []\n    for p in l:\n        p = p.strip()\n        if p.endswith('\\\\'):\n            p = p.rstrip('\\\\')\n            cum.append(p)\n        else:\n            cum.append(p)\n            cl.append(''.join(cum).strip())\n            cum = []\n    if cum:  # in case s ends with backslash\n        cl.append(''.join(cum))\n    return [x for x in cl if x]\n\n\ndef datetime_is_aware(dt):\n    \"\"\"\n    Determines if a given datetime.datetime is aware.\n\n    The logic is described in Python's docs:\n    http://docs.python.org/library/datetime.html#datetime.tzinfo\n    \"\"\"\n    return dt and dt.tzinfo is not None and dt.tzinfo.utcoffset(dt) is not None\n\n\ndef make_datetime_naive(dt):\n    \"\"\"\n    Ensures that the datetime is not time zone-aware:\n\n    The returned datetime object is a naive time in UTC.\n    \"\"\"\n    if dt and datetime_is_aware(dt):\n        return dt.replace(tzinfo=None) - dt.utcoffset()\n    return dt\n\n\ndef total_seconds(td):\n    \"\"\"\n    Backwards compatible implementation of timedelta.total_seconds()\n    \"\"\"\n    if hasattr(datetime.timedelta, 'total_seconds'):\n        return td.total_seconds()\n    return (td.microseconds + (td.seconds + td.days * 24 * 3600) * 10**6) // 10**6\n\n\ndef datetime_to_timestamp(dt):\n    \"\"\"\n    Convert a datetime object into a floating-point second value\n    \"\"\"\n    try:\n        return total_seconds(make_datetime_naive(dt) - datetime.datetime(1970, 1, 1))\n    except Exception as e:\n        common_err(\"datetime_to_timestamp error: %s\" % (e))\n        return None\n\n\ndef timestamp_to_datetime(ts):\n    \"\"\"\n    Convert a timestamp into a naive datetime object\n    \"\"\"\n    import dateutil\n    import dateutil.tz\n    return make_datetime_naive(datetime.datetime.fromtimestamp(ts).replace(tzinfo=dateutil.tz.tzlocal()))\n\n\ndef parse_time(t):\n    '''\n    Try to make sense of the user provided time spec.\n    Use dateutil if available, otherwise strptime.\n    Return the datetime value.\n\n    Also does time zone elimination by passing the datetime\n    through a timestamp conversion if necessary\n\n    TODO: dateutil is very slow, avoid it if possible\n    '''\n    try:\n        from dateutil import parser, tz\n        dt = parser.parse(t)\n\n        if datetime_is_aware(dt):\n            ts = datetime_to_timestamp(dt)\n            if ts is None:\n                return None\n            dt = datetime.datetime.fromtimestamp(ts)\n        else:\n            # convert to UTC from local time\n            dt = dt - tz.tzlocal().utcoffset(dt)\n    except ValueError as msg:\n        common_err(\"parse_time %s: %s\" % (t, msg))\n        return None\n    except ImportError as msg:\n        try:\n            tm = time.strptime(t)\n            dt = datetime.datetime(*tm[0:7])\n        except ValueError as msg:\n            common_err(\"no dateutil, please provide times as printed by date(1)\")\n            return None\n    return dt\n\n\ndef parse_to_timestamp(t):\n    '''\n    Read a string and convert it into a UNIX timestamp.\n    Added as an optimization of parse_time to avoid\n    extra conversion steps when result would be converted\n    into a timestamp anyway\n    '''\n    try:\n        from dateutil import parser, tz\n        dt = parser.parse(t)\n\n        if datetime_is_aware(dt):\n            return datetime_to_timestamp(dt)\n        # convert to UTC from local time\n        return total_seconds(dt - tz.tzlocal().utcoffset(dt) - datetime.datetime(1970, 1, 1))\n    except ValueError as msg:\n        common_err(\"parse_time %s: %s\" % (t, msg))\n        return None\n    except ImportError as msg:\n        try:\n            tm = time.strptime(t)\n            dt = datetime.datetime(*tm[0:7])\n            return datetime_to_timestamp(dt)\n        except ValueError as msg:\n            common_err(\"no dateutil, please provide times as printed by date(1)\")\n            return None\n\n\ndef save_graphviz_file(ini_f, attr_d):\n    '''\n    Save graphviz settings to an ini file, if it does not exist.\n    '''\n    if os.path.isfile(ini_f):\n        common_err(\"%s exists, please remove it first\" % ini_f)\n        return False\n    try:\n        f = open(ini_f, \"wb\")\n    except IOError as msg:\n        common_err(msg)\n        return False\n    import configparser\n    p = configparser.ConfigParser()\n    for section, sect_d in attr_d.items():\n        p.add_section(section)\n        for n, v in sect_d.items():\n            p.set(section, n, v)\n    try:\n        p.write(f)\n    except IOError as msg:\n        common_err(msg)\n        return False\n    f.close()\n    common_info(\"graphviz attributes saved to %s\" % ini_f)\n    return True\n\n\ndef load_graphviz_file(ini_f):\n    '''\n    Load graphviz ini file, if it exists.\n    '''\n    if not os.path.isfile(ini_f):\n        return True, None\n    import configparser\n    p = configparser.ConfigParser()\n    try:\n        p.read(ini_f)\n    except Exception as msg:\n        common_err(msg)\n        return False, None\n    _graph_d = {}\n    for section in p.sections():\n        d = {}\n        for n, v in p.items(section):\n            d[n] = v\n        _graph_d[section] = d\n    return True, _graph_d\n\n\ndef get_pcmk_version(dflt):\n    version = dflt\n\n    crmd = pacemaker_controld()\n    if crmd:\n        cmd = crmd\n    else:\n        return version\n\n    try:\n        rc, s, err = get_stdout_stderr(\"%s version\" % (cmd))\n        if rc != 0:\n            common_err(\"%s exited with %d [err: %s][out: %s]\" % (cmd, rc, err, s))\n        else:\n            common_debug(\"pacemaker version: [err: %s][out: %s]\" % (err, s))\n            if err.startswith(\"CRM Version:\"):\n                version = s.split()[0]\n            else:\n                version = s.split()[2]\n            common_debug(\"found pacemaker version: %s\" % version)\n    except Exception as msg:\n        common_warn(\"could not get the pacemaker version, bad installation?\")\n        common_warn(msg)\n    return version\n\n\ndef get_cib_property(cib_f, attr, dflt):\n    \"\"\"A poor man's get attribute procedure.\n    We don't want heavy parsing, this needs to be relatively\n    fast.\n    \"\"\"\n    open_t = \"<cluster_property_set\"\n    close_t = \"</cluster_property_set\"\n    attr_s = 'name=\"%s\"' % attr\n    ver_patt = re.compile('value=\"([^\"]+)\"')\n    ver = dflt  # return some version in any case\n    try:\n        f = open(cib_f, \"r\")\n    except IOError as msg:\n        common_err(msg)\n        return ver\n    state = 0\n    for s in f:\n        if state == 0:\n            if open_t in s:\n                state += 1\n        elif state == 1:\n            if close_t in s:\n                break\n            if attr_s in s:\n                r = ver_patt.search(s)\n                if r:\n                    ver = r.group(1)\n                break\n    f.close()\n    return ver\n\n\ndef get_cib_attributes(cib_f, tag, attr_l, dflt_l):\n    \"\"\"A poor man's get attribute procedure.\n    We don't want heavy parsing, this needs to be relatively\n    fast.\n    \"\"\"\n    open_t = \"<%s \" % tag\n    val_patt_l = [re.compile('%s=\"([^\"]+)\"' % x) for x in attr_l]\n    val_l = []\n    try:\n        f = open(cib_f, \"rb\").read()\n    except IOError as msg:\n        common_err(msg)\n        return dflt_l\n    if os.path.splitext(cib_f)[-1] == '.bz2':\n        cib_bits = bz2.decompress(f)\n    else:\n        cib_bits = f\n    cib_s = to_ascii(cib_bits)\n    for s in cib_s.split('\\n'):\n        if s.startswith(open_t):\n            i = 0\n            for patt in val_patt_l:\n                r = patt.search(s)\n                val_l.append(r and r.group(1) or dflt_l[i])\n                i += 1\n            break\n    return val_l\n\n\ndef is_min_pcmk_ver(min_ver, cib_f=None):\n    if not constants.pcmk_version:\n        if cib_f:\n            constants.pcmk_version = get_cib_property(cib_f, \"dc-version\", \"1.1.11\")\n            common_debug(\"found pacemaker version: %s in cib: %s\" %\n                         (constants.pcmk_version, cib_f))\n        else:\n            constants.pcmk_version = get_pcmk_version(\"1.1.11\")\n    from distutils.version import LooseVersion\n    return LooseVersion(constants.pcmk_version) >= LooseVersion(min_ver)\n\n\ndef is_pcmk_118(cib_f=None):\n    return is_min_pcmk_ver(\"1.1.8\", cib_f=cib_f)\n\n\n@memoize\ndef cibadmin_features():\n    '''\n    # usage example:\n    if 'corosync-plugin' in cibadmin_features()\n    '''\n    rc, outp = get_stdout(['cibadmin', '-!'], shell=False)\n    if rc == 0:\n        m = re.match(r'Pacemaker\\s(\\S+)\\s\\(Build: ([^\\)]+)\\):\\s(.*)', outp.strip())\n        if m and len(m.groups()) > 2:\n            return m.group(3).split()\n    return []\n\n\n@memoize\ndef cibadmin_can_patch():\n    # cibadmin -P doesn't handle comments in <1.1.11 (unless patched)\n    return is_min_pcmk_ver(\"1.1.11\")\n\n\n# quote function from python module shlex.py in python 3.3\n\n_find_unsafe = re.compile(r'[^\\w@%+=:,./-]').search\n\n\ndef quote(s):\n    \"\"\"Return a shell-escaped version of the string *s*.\"\"\"\n    if not s:\n        return \"''\"\n    if _find_unsafe(s) is None:\n        return s\n\n    # use single quotes, and put single quotes into double quotes\n    # the string $'b is then quoted as '$'\"'\"'b'\n    return \"'\" + s.replace(\"'\", \"'\\\"'\\\"'\") + \"'\"\n\n\ndef doublequote(s):\n    \"\"\"Return a shell-escaped version of the string *s*.\"\"\"\n    if not s:\n        return '\"\"'\n    if _find_unsafe(s) is None:\n        return s\n\n    # use double quotes\n    return '\"' + s.replace('\"', \"\\\\\\\"\") + '\"'\n\n\ndef fetch_opts(args, opt_l):\n    '''\n    Get and remove option keywords from args.\n    They are always listed last, at the end of the line.\n    Return a list of options found. The caller can do\n    if keyw in optlist: ...\n    '''\n    re_opt = None\n    if opt_l[0].startswith(\"@\"):\n        re_opt = re.compile(\"^%s$\" % opt_l[0][1:])\n        del opt_l[0]\n    l = []\n    for i in reversed(list(range(len(args)))):\n        if (args[i] in opt_l) or (re_opt and re_opt.search(args[i])):\n            l.append(args.pop())\n        else:\n            break\n    return l\n\n\n_LIFETIME = [\"reboot\", \"forever\"]\n_ISO8601_RE = re.compile(\"(PT?[0-9]|[0-9]+.*[:-])\")\n\n\ndef fetch_lifetime_opt(args, iso8601=True):\n    '''\n    Get and remove a lifetime option from args. It can be one of\n    lifetime_options or an ISO 8601 formatted period/time. There\n    is apparently no good support in python for this format, so\n    we cheat a bit.\n    '''\n    if args:\n        opt = args[-1]\n        if opt in _LIFETIME or (iso8601 and _ISO8601_RE.match(opt)):\n            return args.pop()\n    return None\n\n\ndef resolve_hostnames(hostnames):\n    '''\n    Tries to resolve the given list of hostnames.\n    returns (ok, failed-hostname)\n    ok: True if all hostnames resolved\n    failed-hostname: First failed hostname resolution\n    '''\n    import socket\n    for node in hostnames:\n        try:\n            socket.gethostbyname(node)\n        except socket.error:\n            return False, node\n    return True, None\n\n\ndef list_corosync_node_names():\n    '''\n    Returns list of nodes configured\n    in corosync.conf\n    '''\n    try:\n        cfg = os.getenv('COROSYNC_MAIN_CONFIG_FILE', '/etc/corosync/corosync.conf')\n        lines = open(cfg).read().split('\\n')\n        name_re = re.compile(r'\\s*name:\\s+(.*)')\n        names = []\n        for line in lines:\n            name = name_re.match(line)\n            if name:\n                names.append(name.group(1))\n        return names\n    except Exception:\n        return []\n\n\ndef list_corosync_nodes():\n    '''\n    Returns list of nodes configured\n    in corosync.conf\n    '''\n    try:\n        cfg = os.getenv('COROSYNC_MAIN_CONFIG_FILE', '/etc/corosync/corosync.conf')\n        lines = open(cfg).read().split('\\n')\n        addr_re = re.compile(r'\\s*ring0_addr:\\s+(.*)')\n        nodes = []\n        for line in lines:\n            addr = addr_re.match(line)\n            if addr:\n                nodes.append(addr.group(1))\n        return nodes\n    except Exception:\n        return []\n\n\ndef print_cluster_nodes():\n    \"\"\"\n    Print the output of crm_node -l\n    \"\"\"\n    rc, out, _ = get_stdout_stderr(\"crm_node -l\")\n    if rc == 0 and out:\n        print(\"{}\\n\".format(out))\n\n\ndef list_cluster_nodes():\n    '''\n    Returns a list of nodes in the cluster.\n    '''\n    def getname(toks):\n        if toks and len(toks) >= 2:\n            return toks[1]\n        return None\n\n    try:\n        # when pacemaker running\n        rc, outp = stdout2list(['crm_node', '-l'], stderr_on=False, shell=False)\n        if rc == 0:\n            return [x for x in [getname(line.split()) for line in outp] if x and x != '(null)']\n\n        # when corosync running\n        ip_list = get_member_iplist()\n        if ip_list:\n            return ip_list\n\n        # static situation\n        cib_path = os.getenv('CIB_file', '/var/lib/pacemaker/cib/cib.xml')\n        if not os.path.isfile(cib_path):\n            return None\n        from . import xmlutil\n        node_list = []\n        cib = xmlutil.file2cib_elem(cib_path)\n        if cib is None:\n            return None\n        for node in cib.xpath('/cib/configuration/nodes/node'):\n            name = node.get('uname') or node.get('id')\n            if node.get('type') == 'remote':\n                srv = cib.xpath(\"//primitive[@id='%s']/instance_attributes/nvpair[@name='server']\" % (name))\n                if srv:\n                    continue\n            node_list.append(name)\n        return node_list\n    except OSError as msg:\n        raise ValueError(\"Error listing cluster nodes: %s\" % (msg))\n\n\ndef cluster_run_cmd(cmd):\n    \"\"\"\n    Run cmd in cluster nodes\n    \"\"\"\n    node_list = list_cluster_nodes()\n    if not node_list:\n        raise ValueError(\"Failed to get node list from cluster\")\n    parallax.parallax_call(node_list, cmd)\n\n\ndef list_cluster_nodes_except_me():\n    \"\"\"\n    Get cluster node list and filter out self\n    \"\"\"\n    node_list = list_cluster_nodes()\n    if not node_list:\n        raise ValueError(\"Failed to get node list from cluster\")\n    me = this_node()\n    if me in node_list:\n        node_list.remove(me)\n    return node_list\n\n\ndef service_info(name):\n    p = is_program('systemctl')\n    if p:\n        rc, outp = get_stdout([p, 'show',\n                               '-p', 'UnitFileState',\n                               '-p', 'ActiveState',\n                               '-p', 'SubState',\n                               name + '.service'], shell=False)\n        if rc == 0:\n            info = []\n            for line in outp.split('\\n'):\n                data = line.split('=', 1)\n                if len(data) == 2:\n                    info.append(data[1].strip())\n            return '/'.join(info)\n    return None\n\n\ndef running_on(resource):\n    \"returns list of node names where the given resource is running\"\n    rsc_locate = \"crm_resource --resource '%s' --locate\"\n    rc, out, err = get_stdout_stderr(rsc_locate % (resource))\n    if rc != 0:\n        return []\n    nodes = []\n    head = \"resource %s is running on: \" % (resource)\n    for line in out.split('\\n'):\n        if line.strip().startswith(head):\n            w = line[len(head):].split()\n            if w:\n                nodes.append(w[0])\n    common_debug(\"%s running on: %s\" % (resource, nodes))\n    return nodes\n\n\n# This RE matches nvpair values that can\n# be left unquoted\n_NOQUOTES_RE = re.compile(r'^[\\w\\.-]+$')\n\n\ndef noquotes(v):\n    return _NOQUOTES_RE.match(v) is not None\n\n\ndef unquote(s):\n    \"\"\"\n    Reverse shell-quoting a string, so the string '\"a b c\"'\n    becomes 'a b c'\n    \"\"\"\n    sp = shlex.split(s)\n    if sp:\n        return sp[0]\n    return \"\"\n\n\ndef parse_sysconfig(sysconfig_file):\n    \"\"\"\n    Reads a sysconfig file into a dict\n    \"\"\"\n    ret = {}\n    if os.path.isfile(sysconfig_file):\n        for line in open(sysconfig_file).readlines():\n            if line.lstrip().startswith('#'):\n                continue\n            try:\n                key, val = line.split(\"=\", 1)\n                ret[key] = unquote(val)\n            except ValueError:\n                pass\n    return ret\n\n\ndef sysconfig_set(sysconfig_file, **values):\n    \"\"\"\n    Set the values in the sysconfig file, updating the variables\n    if they exist already, appending them if not.\n    \"\"\"\n    outp = \"\"\n    if os.path.isfile(sysconfig_file):\n        for line in open(sysconfig_file).readlines():\n            if line.lstrip().startswith('#'):\n                outp += line\n            else:\n                matched = False\n                try:\n                    key, _ = line.split(\"=\", 1)\n                    for k, v in values.items():\n                        if k == key:\n                            matched = True\n                            outp += '%s=%s\\n' % (k, doublequote(v))\n                            del values[k]\n                            break\n                    if not matched:\n                        outp += line\n                except ValueError:\n                    outp += line\n\n    for k, v in values.items():\n        outp += '%s=%s\\n' % (k, doublequote(v))\n    str2file(outp, sysconfig_file)\n\n\ndef remote_diff_slurp(nodes, filename):\n    try:\n        import parallax\n    except ImportError:\n        raise ValueError(\"Parallax is required to diff\")\n    from . import tmpfiles\n\n    tmpdir = tmpfiles.create_dir()\n    opts = parallax.Options()\n    opts.localdir = tmpdir\n    dst = os.path.basename(filename)\n    return list(parallax.slurp(nodes, filename, dst, opts).items())\n\n\ndef remote_diff_this(local_path, nodes, this_node):\n    try:\n        import parallax\n    except ImportError:\n        raise ValueError(\"Parallax is required to diff\")\n\n    by_host = remote_diff_slurp(nodes, local_path)\n    for host, result in by_host:\n        if isinstance(result, parallax.Error):\n            raise ValueError(\"Failed on %s: %s\" % (host, str(result)))\n        _, _, _, path = result\n        _, s = get_stdout(\"diff -U 0 -d -b --label %s --label %s %s %s\" %\n                          (host, this_node, path, local_path))\n        page_string(s)\n\n\ndef remote_diff(local_path, nodes):\n    try:\n        import parallax\n    except ImportError:\n        raise ValueError(\"parallax is required to diff\")\n\n    by_host = remote_diff_slurp(nodes, local_path)\n    for host, result in by_host:\n        if isinstance(result, parallax.Error):\n            raise ValueError(\"Failed on %s: %s\" % (host, str(result)))\n    h1, r1 = by_host[0]\n    h2, r2 = by_host[1]\n    _, s = get_stdout(\"diff -U 0 -d -b --label %s --label %s %s %s\" %\n                      (h1, h2, r1[3], r2[3]))\n    page_string(s)\n\n\ndef remote_checksum(local_path, nodes, this_node):\n    try:\n        import parallax\n    except ImportError:\n        raise ValueError(\"Parallax is required to diff\")\n    import hashlib\n\n    by_host = remote_diff_slurp(nodes, local_path)\n    for host, result in by_host:\n        if isinstance(result, parallax.Error):\n            raise ValueError(str(result))\n\n    print(\"%-16s  SHA1 checksum of %s\" % ('Host', local_path))\n    if this_node not in nodes:\n        print(\"%-16s: %s\" % (this_node, hashlib.sha1(open(local_path).read()).hexdigest()))\n    for host, result in by_host:\n        _, _, _, path = result\n        print(\"%-16s: %s\" % (host, hashlib.sha1(open(path).read()).hexdigest()))\n\n\ndef cluster_copy_file(local_path, nodes=None):\n    \"\"\"\n    Copies given file to all other cluster nodes.\n    \"\"\"\n    try:\n        import parallax\n    except ImportError:\n        raise ValueError(\"parallax is required to copy cluster files\")\n    if not nodes:\n        nodes = list_cluster_nodes()\n        nodes.remove(this_node())\n    opts = parallax.Options()\n    opts.timeout = 60\n    opts.ssh_options += ['ControlPersist=no']\n    ok = True\n    for host, result in parallax.copy(nodes,\n                                      local_path,\n                                      local_path, opts).items():\n        if isinstance(result, parallax.Error):\n            err_buf.error(\"Failed to push %s to %s: %s\" % (local_path, host, result))\n            ok = False\n        else:\n            err_buf.ok(host)\n    return ok\n\n\n# a set of fnmatch patterns to match attributes whose values\n# should be obscured as a sequence of **** when printed\n_obscured_nvpairs = []\n\n\ndef obscured(key, value):\n    if key is not None and value is not None:\n        for o in _obscured_nvpairs:\n            if fnmatch.fnmatch(key, o):\n                return '*' * 6\n    return value\n\n\n@contextmanager\ndef obscure(obscure_list):\n    global _obscured_nvpairs\n    prev = _obscured_nvpairs\n    _obscured_nvpairs = obscure_list\n    try:\n        yield\n    finally:\n        _obscured_nvpairs = prev\n\n\ndef gen_nodeid_from_ipv6(addr):\n    return int(ipaddress.ip_address(addr)) % 1000000000\n\n\n# Set by detect_cloud()\n# to avoid multiple requests\n_ip_for_cloud = None\n\n\ndef _cloud_metadata_request(uri, headers={}):\n    try:\n        import urllib2 as urllib\n    except ImportError:\n        import urllib.request as urllib\n    req = urllib.Request(uri)\n    for header, value in headers.items():\n        req.add_header(header, value)\n    try:\n        resp = urllib.urlopen(req, timeout=5)\n        content = resp.read()\n        if type(content) != str:\n            return content.decode('utf-8').strip()\n        return content.strip()\n    except urllib.URLError:\n        return None\n\n\n@memoize\ndef detect_cloud():\n    \"\"\"\n    Tries to determine which (if any) cloud environment\n    the cluster is running on.\n\n    This is mainly done using dmidecode.\n\n    If the host cannot be determined, this function\n    returns None. Otherwise, it returns a string\n    identifying the platform.\n\n    These are the currently known platforms:\n\n    * amazon-web-services\n    * microsoft-azure\n    * google-cloud-platform\n\n    \"\"\"\n    global _ip_for_cloud\n\n    if not is_program(\"dmidecode\"):\n        return None\n    rc, system_version = get_stdout(\"dmidecode -s system-version\")\n    if re.search(r\".*amazon.*\", system_version) is not None:\n        return \"amazon-web-services\"\n    if rc != 0:\n        return None\n    rc, system_manufacturer = get_stdout(\"dmidecode -s system-manufacturer\")\n    if rc == 0 and \"microsoft corporation\" in system_manufacturer.lower():\n        # To detect azure we also need to make an API request\n        result = _cloud_metadata_request(\n            \"http://169.254.169.254/metadata/instance/network/interface/0/ipv4/ipAddress/0/privateIpAddress?api-version=2017-08-01&format=text\",\n            headers={\"Metadata\": \"true\"})\n        if result:\n            _ip_for_cloud = result\n            return \"microsoft-azure\"\n    rc, bios_vendor = get_stdout(\"dmidecode -s bios-vendor\")\n    if rc == 0 and \"Google\" in bios_vendor:\n        # To detect GCP we also need to make an API request\n        result = _cloud_metadata_request(\n            \"http://metadata.google.internal/computeMetadata/v1/instance/network-interfaces/0/ip\",\n            headers={\"Metadata-Flavor\": \"Google\"})\n        if result:\n            _ip_for_cloud = result\n            return \"google-cloud-platform\"\n    return None\n\n\ndef debug_timestamp():\n    return datetime.datetime.now().strftime('%Y/%m/%d %H:%M:%S')\n\n\ndef get_member_iplist():\n    rc, out, err= get_stdout_stderr(\"corosync-cmapctl -b runtime.totem.pg.mrp.srp.members\")\n    if rc != 0:\n        common_debug(err)\n        return None\n\n    ip_list = []\n    for line in out.split('\\n'):\n        match = re.search(r'ip\\((.*?)\\)', line)\n        if match:\n            ip_list.append(match.group(1))\n    return ip_list\n\n\ndef get_iplist_corosync_using():\n    \"\"\"\n    Get ip list used by corosync\n    \"\"\"\n    rc, out, err = get_stdout_stderr(\"corosync-cfgtool -s\")\n    if rc != 0:\n        raise ValueError(err)\n    return re.findall(r'id\\s*=\\s*(.*)', out)\n\n\ndef check_ssh_passwd_need(host):\n    \"\"\"\n    Check whether access to host need password\n    \"\"\"\n    ssh_options = \"-o StrictHostKeyChecking=no -o EscapeChar=none -o ConnectTimeout=15\"\n    ssh_cmd = \"ssh {} -T -o Batchmode=yes {} true\".format(ssh_options, host)\n    rc, _, _ = get_stdout_stderr(ssh_cmd)\n    return rc != 0\n\n\ndef check_port_open(ip, port):\n    import socket\n\n    family = socket.AF_INET6 if IP.is_ipv6(ip) else socket.AF_INET\n    with closing(socket.socket(family, socket.SOCK_STREAM)) as sock:\n        if sock.connect_ex((ip, port)) == 0:\n            return True\n        else:\n            return False\n\n\ndef valid_port(port):\n    return int(port) >= 1024 and int(port) <= 65535\n\n\ndef is_qdevice_configured():\n    from . import corosync\n    return corosync.get_value(\"quorum.device.model\") == \"net\"\n\n\ndef is_qdevice_tls_on():\n    from . import corosync\n    return corosync.get_value(\"quorum.device.net.tls\") == \"on\"\n\n\ndef get_nodeinfo_from_cmaptool():\n    nodeid_ip_dict = {}\n    rc, out = get_stdout(\"corosync-cmapctl -b runtime.totem.pg.mrp.srp.members\")\n    if rc != 0:\n        return nodeid_ip_dict\n\n    for line in out.split('\\n'):\n        match = re.search(r'members\\.(.*)\\.ip', line)\n        if match:\n            node_id = match.group(1)\n            iplist = re.findall(r'[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}', line)\n            nodeid_ip_dict[node_id] = iplist\n    return nodeid_ip_dict\n\n\ndef get_iplist_from_name(name):\n    \"\"\"\n    Given node host name, return this host's ip list in corosync cmap\n    \"\"\"\n    ip_list = []\n    nodeid = get_nodeid_from_name(name)\n    if not nodeid:\n        return ip_list\n    nodeinfo = {}\n    nodeinfo = get_nodeinfo_from_cmaptool()\n    if not nodeinfo:\n        return ip_list\n    return nodeinfo[nodeid]\n\n\ndef valid_nodeid(nodeid):\n    from . import bootstrap\n    if not service_is_active('corosync.service'):\n        return False\n\n    for _id, _ in get_nodeinfo_from_cmaptool().items():\n        if _id == nodeid:\n            return True\n    return False\n\n\ndef get_nodeid_from_name(name):\n    rc, out = get_stdout('crm_node -l')\n    if rc != 0:\n        return None\n    res = re.search(r'^([0-9]+) {} '.format(name), out, re.M)\n    if res:\n        return res.group(1)\n    else:\n        return None\n\n\ndef check_space_option_value(options):\n    if not isinstance(options, argparse.Namespace):\n        raise ValueError(\"Expected type of \\\"options\\\" is \\\"argparse.Namespace\\\", not \\\"{}\\\"\".format(type(options)))\n\n    for opt in vars(options):\n        value = getattr(options, opt)\n        if isinstance(value, str) and len(value.strip()) == 0:\n            raise ValueError(\"Space value not allowed for dest \\\"{}\\\"\".format(opt))\n\n\ndef interface_choice():\n    _, out = get_stdout(\"ip a\")\n    # should consider interface format like \"ethx@xxx\"\n    interface_list = re.findall(r'(?:[0-9]+:) (.*?)(?=: |@.*?: )', out)\n    return [nic for nic in interface_list if nic != \"lo\"]\n\n\nclass IP(object):\n    \"\"\"\n    Class to get some properties of IP address\n    \"\"\"\n\n    def __init__(self, addr):\n        \"\"\"\n        Init function\n        \"\"\"\n        self.addr = addr\n\n    @property\n    def ip_address(self):\n        \"\"\"\n        Create ipaddress instance\n        \"\"\"\n        return ipaddress.ip_address(self.addr)\n\n    @property\n    def version(self):\n        \"\"\"\n        Get IP address version\n        \"\"\"\n        return self.ip_address.version\n\n    @classmethod\n    def is_mcast(cls, addr):\n        \"\"\"\n        Check whether the address is multicast address\n        \"\"\"\n        cls_inst = cls(addr)\n        return cls_inst.ip_address.is_multicast\n\n    @classmethod\n    def is_ipv6(cls, addr):\n        \"\"\"\n        Check whether the address is IPV6 address\n        \"\"\"\n        return cls(addr).version == 6\n\n    @classmethod\n    def is_valid_ip(cls, addr):\n        \"\"\"\n        Check whether the address is valid IP address\n        \"\"\"\n        cls_inst = cls(addr)\n        try:\n            cls_inst.ip_address\n        except ValueError:\n            return False\n        else:\n            return True\n\n    @property\n    def is_loopback(self):\n        \"\"\"\n        Check whether the address is loopback address\n        \"\"\"\n        return self.ip_address.is_loopback\n\n    @property\n    def is_link_local(self):\n        \"\"\"\n        Check whether the address is link-local address\n        \"\"\"\n        return self.ip_address.is_link_local\n\n\nclass Interface(IP):\n    \"\"\"\n    Class to get information from one interface\n    \"\"\"\n\n    def __init__(self, ip_with_mask):\n        \"\"\"\n        Init function\n        \"\"\"\n        self.ip, self.mask = ip_with_mask.split('/')\n        super(__class__, self).__init__(self.ip)\n\n    @property\n    def ip_with_mask(self):\n        \"\"\"\n        Get ip with netmask\n        \"\"\"\n        return '{}/{}'.format(self.ip, self.mask)\n\n    @property\n    def ip_interface(self):\n        \"\"\"\n        Create ip_interface instance\n        \"\"\"\n        return ipaddress.ip_interface(self.ip_with_mask)\n\n    @property\n    def network(self):\n        \"\"\"\n        Get network address\n        \"\"\"\n        return str(self.ip_interface.network.network_address)\n\n    def ip_in_network(self, addr):\n        \"\"\"\n        Check whether the addr in the network\n        \"\"\"\n        return IP(addr).ip_address in self.ip_interface.network\n\n\nclass InterfacesInfo(object):\n    \"\"\"\n    Class to collect interfaces information on local node\n    \"\"\"\n\n    def __init__(self, ipv6=False, second_heartbeat=False, custom_nic_list=[]):\n        \"\"\"\n        Init function\n\n        On init process,\n        \"ipv6\" is provided by -I option\n        \"second_heartbeat\" is provided by -M option\n        \"custom_nic_list\" is provided by -i option\n        \"\"\"\n        self.ip_version = 6 if ipv6 else 4\n        self.second_heartbeat = second_heartbeat\n        self._default_nic_list = custom_nic_list\n        self._nic_info_dict = {}\n\n    def get_interfaces_info(self):\n        \"\"\"\n        Try to get interfaces info dictionary via \"ip\" command\n\n        IMPORTANT: This is the method that populates the data, should always be called after initialize\n        \"\"\"\n        cmd = \"ip -{} -o addr show\".format(self.ip_version)\n        rc, out, err = get_stdout_stderr(cmd)\n        if rc != 0:\n            raise ValueError(err)\n\n        # format on each line will like:\n        # 2: enp1s0    inet 192.168.122.241/24 brd 192.168.122.255 scope global enp1s0\\       valid_lft forever preferred_lft forever\n        for line in out.splitlines():\n            _, nic, _, ip_with_mask, *_ = line.split()\n            # maybe from tun interface\n            if not '/' in ip_with_mask:\n                continue\n            #TODO change this condition when corosync support link-local address\n            interface_inst = Interface(ip_with_mask)\n            if interface_inst.is_loopback or interface_inst.is_link_local:\n                continue\n            # one nic might configured multi IP addresses\n            if nic not in self._nic_info_dict:\n                self._nic_info_dict[nic] = []\n            self._nic_info_dict[nic].append(interface_inst)\n\n        if not self._nic_info_dict:\n            raise ValueError(\"No address configured\")\n        if self.second_heartbeat and len(self._nic_info_dict) == 1:\n            raise ValueError(\"Cannot configure second heartbeat, since only one address is available\")\n\n    @property\n    def nic_list(self):\n        \"\"\"\n        Get interfaces name list\n        \"\"\"\n        return list(self._nic_info_dict.keys())\n\n    @property\n    def interface_list(self):\n        \"\"\"\n        Get instance list of class Interface\n        \"\"\"\n        _interface_list = []\n        for interface in self._nic_info_dict.values():\n            _interface_list.extend(interface)\n        return _interface_list\n\n    @property\n    def ip_list(self):\n        \"\"\"\n        Get IP address list\n        \"\"\"\n        return [interface.ip for interface in self.interface_list]\n\n    @classmethod\n    def get_local_ip_list(cls, is_ipv6):\n        \"\"\"\n        Get IP address list\n        \"\"\"\n        cls_inst = cls(is_ipv6)\n        cls_inst.get_interfaces_info()\n        return cls_inst.ip_list\n\n    @classmethod\n    def ip_in_local(cls, addr):\n        \"\"\"\n        Check whether given address was in one of local address\n        \"\"\"\n        cls_inst = cls(IP.is_ipv6(addr))\n        cls_inst.get_interfaces_info()\n        return addr in cls_inst.ip_list\n\n    @property\n    def network_list(self):\n        \"\"\"\n        Get network list\n        \"\"\"\n        return list(set([interface.network for interface in self.interface_list]))\n\n    def _nic_first_ip(self, nic):\n        \"\"\"\n        Get the first IP of specific nic\n        \"\"\"\n        return self._nic_info_dict[nic][0].ip\n\n    def get_default_nic_list_from_route(self):\n        \"\"\"\n        Get default nic list from route\n        \"\"\"\n        if self._default_nic_list:\n            return self._default_nic_list\n\n        #TODO what if user only has ipv6 route?\n        cmd = \"ip -o route show\"\n        rc, out, err = get_stdout_stderr(cmd)\n        if rc != 0:\n            raise ValueError(err)\n        res = re.search(r'^default via .* dev (.*?) ', out)\n        if res:\n            self._default_nic_list = [res.group(1)]\n        else:\n            if not self.nic_list:\n                self.get_interfaces_info()\n            common_warn(\"No default route configured. Using the first found nic\")\n            self._default_nic_list = [self.nic_list[0]]\n        return self._default_nic_list\n\n    def get_default_ip_list(self):\n        \"\"\"\n        Get default IP list will be used by corosync\n        \"\"\"\n        if not self._default_nic_list:\n            self.get_default_nic_list_from_route()\n        if not self.nic_list:\n            self.get_interfaces_info()\n\n        _ip_list = []\n        for nic in self._default_nic_list:\n            # in case given interface not exist\n            if nic not in self.nic_list:\n                raise ValueError(\"Failed to detect IP address for {}\".format(nic))\n            _ip_list.append(self._nic_first_ip(nic))\n        # in case -M specified but given one interface via -i\n        if self.second_heartbeat and len(self._default_nic_list) == 1:\n            for nic in self.nic_list:\n                if nic not in self._default_nic_list:\n                    _ip_list.append(self._nic_first_ip(nic))\n                    break\n        return _ip_list\n\n    @classmethod\n    def ip_in_network(cls, addr):\n        \"\"\"\n        Check whether given address was in one of local networks\n        \"\"\"\n        cls_inst = cls(IP.is_ipv6(addr))\n        cls_inst.get_interfaces_info()\n        for interface_inst in cls_inst.interface_list:\n            if interface_inst.ip_in_network(addr):\n                return True\n        return False\n\n\ndef check_file_content_included(source_file, target_file):\n    \"\"\"\n    Check whether target_file includes contents of source_file\n    \"\"\"\n    if not os.path.exists(source_file):\n        raise ValueError(\"File {} not exist\".format(source_file))\n    if not os.path.exists(target_file):\n        return False\n\n    with open(target_file, 'r') as target_fd:\n        target_data = target_fd.read()\n    with open(source_file, 'r') as source_fd:\n        source_data = source_fd.read()\n    return source_data in target_data\n\n\nclass ServiceManager(object):\n    \"\"\"\n    Class to manage systemctl services\n    \"\"\"\n    ACTION_MAP = {\n            \"enable\": \"enable\",\n            \"disable\": \"disable\",\n            \"start\": \"start\",\n            \"stop\": \"stop\",\n            \"is_enabled\": \"is-enabled\",\n            \"is_active\": \"is-active\",\n            \"is_available\": \"list-unit-files\"\n            }\n\n    def __init__(self, service_name, remote_addr=None):\n        \"\"\"\n        Init function\n        \"\"\"\n        self.service_name = service_name\n        self.remote_addr = remote_addr\n\n    def _do_action(self, action_type):\n        \"\"\"\n        Actual do actions to manage service\n        \"\"\"\n        if action_type not in self.ACTION_MAP.values():\n            raise ValueError(\"status_type should be {}\".format('/'.join(list(self.ACTION_MAP.values()))))\n\n        cmd = \"systemctl {} {}\".format(action_type, self.service_name)\n        if self.remote_addr:\n            prompt_msg = \"Run \\\"{}\\\" on {}\".format(cmd, self.remote_addr)\n            rc, output, err = run_cmd_on_remote(cmd, self.remote_addr, prompt_msg)\n        else:\n            rc, output, err = get_stdout_stderr(cmd)\n        if rc != 0 and err:\n            raise ValueError(\"Run \\\"{}\\\" error: {}\".format(cmd, err))\n        return rc == 0, output\n\n    @property\n    def is_available(self):\n        return self.service_name in self._do_action(self.ACTION_MAP[\"is_available\"])[1]\n\n    @property\n    def is_enabled(self):\n        return self._do_action(self.ACTION_MAP[\"is_enabled\"])[0]\n\n    @property\n    def is_active(self):\n        return self._do_action(self.ACTION_MAP[\"is_active\"])[0]\n\n    def start(self):\n        self._do_action(self.ACTION_MAP[\"start\"])\n\n    def stop(self):\n        self._do_action(self.ACTION_MAP[\"stop\"])\n\n    def enable(self):\n        self._do_action(self.ACTION_MAP[\"enable\"])\n\n    def disable(self):\n        self._do_action(self.ACTION_MAP[\"disable\"])\n\n    @classmethod\n    def service_is_available(cls, name, remote_addr=None):\n        \"\"\"\n        Check whether service is available\n        \"\"\"\n        inst = cls(name, remote_addr)\n        return inst.is_available\n\n    @classmethod\n    def service_is_enabled(cls, name, remote_addr=None):\n        \"\"\"\n        Check whether service is enabled\n        \"\"\"\n        inst = cls(name, remote_addr)\n        return inst.is_enabled\n\n    @classmethod\n    def service_is_active(cls, name, remote_addr=None):\n        \"\"\"\n        Check whether service is active\n        \"\"\"\n        inst = cls(name, remote_addr)\n        return inst.is_active\n\n    @classmethod\n    def start_service(cls, name, enable=False, remote_addr=None):\n        \"\"\"\n        Start service\n        \"\"\"\n        inst = cls(name, remote_addr)\n        if enable:\n            inst.enable()\n        inst.start()\n\n    @classmethod\n    def stop_service(cls, name, disable=False, remote_addr=None):\n        \"\"\"\n        Stop service\n        \"\"\"\n        inst = cls(name, remote_addr)\n        if disable:\n            inst.disable()\n        inst.stop()\n\n    @classmethod\n    def enable_service(cls, name, remote_addr=None):\n        \"\"\"\n        Enable service\n        \"\"\"\n        inst = cls(name, remote_addr)\n        if inst.is_available and not inst.is_enabled:\n            inst.enable()\n\n    @classmethod\n    def disable_service(cls, name, remote_addr=None):\n        \"\"\"\n        Disable service\n        \"\"\"\n        inst = cls(name, remote_addr)\n        if inst.is_available and inst.is_enabled:\n            inst.disable()\n\n\nservice_is_available = ServiceManager.service_is_available\nservice_is_enabled = ServiceManager.service_is_enabled\nservice_is_active = ServiceManager.service_is_active\nstart_service = ServiceManager.start_service\nstop_service = ServiceManager.stop_service\nenable_service = ServiceManager.enable_service\ndisable_service = ServiceManager.disable_service\n\n\ndef package_is_installed(pkg, remote_addr=None):\n    \"\"\"\n    Check if package is installed\n    \"\"\"\n    cmd = \"rpm -q --quiet {}\".format(pkg)\n    if remote_addr:\n        # check on remote\n        prompt_msg = \"Check whether {} is installed on {}\".format(pkg, remote_addr)\n        rc, _, _ = run_cmd_on_remote(cmd, remote_addr, prompt_msg)\n    else:\n        # check on local\n        rc, _ = get_stdout(cmd)\n    return rc == 0\n\n\ndef ping_node(node):\n    \"\"\"\n    Check if the remote node is reachable\n    \"\"\"\n    rc, _, err = get_stdout_stderr(\"ping -c 1 {}\".format(node))\n    if rc != 0:\n        raise ValueError(\"host \\\"{}\\\" is unreachable: {}\".format(node, err))\n# vim:ts=4:sw=4:et:\n"], "fixing_code": ["# Copyright (C) 2016 Kristoffer Gronlund <kgronlund@suse.com>\n# See COPYING for license information.\n#\n# Bootstrap:\n#\n# Supersedes and replaces both the init/add/remove cluster scripts,\n# and the ha-cluster-bootstrap scripts.\n#\n# Implemented as a straight-forward set of python functions for\n# simplicity and flexibility.\n#\n# TODO: Make csync2 usage optional\n# TODO: Configuration file for bootstrap?\n\nimport os\nimport sys\nimport random\nimport re\nimport time\nimport readline\nimport shutil\nfrom string import Template\nfrom lxml import etree\nfrom pathlib import Path\nfrom . import config\nfrom . import utils\nfrom . import xmlutil\nfrom .cibconfig import mkset_obj, cib_factory\nfrom . import corosync\nfrom . import tmpfiles\nfrom . import clidisplay\nfrom . import term\nfrom . import lock\nfrom . import userdir\n\n\nLOG_FILE = \"/var/log/crmsh/ha-cluster-bootstrap.log\"\nCSYNC2_KEY = \"/etc/csync2/key_hagroup\"\nCSYNC2_CFG = \"/etc/csync2/csync2.cfg\"\nCOROSYNC_AUTH = \"/etc/corosync/authkey\"\nSYSCONFIG_SBD = \"/etc/sysconfig/sbd\"\nSYSCONFIG_FW = \"/etc/sysconfig/SuSEfirewall2\"\nSYSCONFIG_FW_CLUSTER = \"/etc/sysconfig/SuSEfirewall2.d/services/cluster\"\nPCMK_REMOTE_AUTH = \"/etc/pacemaker/authkey\"\nCOROSYNC_CONF_ORIG = tmpfiles.create()[1]\nSERVICES_STOP_LIST = [\"corosync-qdevice.service\", \"corosync.service\", \"hawk.service\"]\nUSER_LIST = [\"root\", \"hacluster\"]\n\nINIT_STAGES = (\"ssh\", \"ssh_remote\", \"csync2\", \"csync2_remote\", \"corosync\", \"storage\", \"sbd\", \"cluster\", \"vgfs\", \"admin\", \"qdevice\")\n\n\nclass Context(object):\n    \"\"\"\n    Context object used to avoid having to pass these variables\n    to every bootstrap method.\n    \"\"\"\n    def __init__(self):\n        '''\n        Initialize attributes\n        '''\n        self.type = None # init or join\n        self.quiet = None\n        self.yes_to_all = None\n        self.template = None\n        self.cluster_name = None\n        self.watchdog = None\n        self.no_overwrite_sshkey = None\n        self.nic_list = None\n        self.unicast = None\n        self.admin_ip = None\n        self.second_heartbeat = None\n        self.ipv6 = None\n        self.qdevice_inst = None\n        self.qnetd_addr = None\n        self.qdevice_port = None\n        self.qdevice_algo = None\n        self.qdevice_tie_breaker = None\n        self.qdevice_tls = None\n        self.qdevice_heuristics = None\n        self.qdevice_heuristics_mode = None\n        self.qdevice_rm_flag = None\n        self.shared_device = None\n        self.ocfs2_device = None\n        self.cluster_node = None\n        self.cluster_node_ip = None\n        self.force = None\n        self.arbitrator = None\n        self.clusters = None\n        self.tickets = None\n        self.sbd_manager = None\n        self.sbd_devices = None\n        self.diskless_sbd = None\n        self.stage = None\n        self.args = None\n        self.ui_context = None\n        self.interfaces_inst = None\n        self.with_other_user = True\n        self.default_nic_list = []\n        self.default_ip_list = []\n        self.local_ip_list = []\n        self.local_network_list = []\n        self.rm_list = [SYSCONFIG_SBD, CSYNC2_CFG, corosync.conf(), CSYNC2_KEY,\n                COROSYNC_AUTH, \"/var/lib/heartbeat/crm/*\", \"/var/lib/pacemaker/cib/*\"]\n\n    @classmethod\n    def set_context(cls, options):\n        ctx = cls()\n        for opt in vars(options):\n            setattr(ctx, opt, getattr(options, opt))\n        return ctx\n\n    def initialize_qdevice(self):\n        \"\"\"\n        Initialize qdevice instance\n        \"\"\"\n        if not self.qnetd_addr:\n            return\n        self.qdevice_inst = corosync.QDevice(\n                self.qnetd_addr,\n                port=self.qdevice_port,\n                algo=self.qdevice_algo,\n                tie_breaker=self.qdevice_tie_breaker,\n                tls=self.qdevice_tls,\n                cmds=self.qdevice_heuristics,\n                mode=self.qdevice_heuristics_mode)\n\n    def validate_option(self):\n        \"\"\"\n        Validate options\n        \"\"\"\n        if self.admin_ip:\n            try:\n                Validation.valid_admin_ip(self.admin_ip)\n            except ValueError as err:\n                error(err)\n        if self.qdevice_inst:\n            try:\n                self.qdevice_inst.valid_attr()\n            except ValueError as err:\n                error(err)\n        if self.nic_list:\n            if len(self.nic_list) > 2:\n                error(\"Maximum number of interface is 2\")\n            if len(self.nic_list) != len(set(self.nic_list)):\n                error(\"Duplicated input\")\n        if self.no_overwrite_sshkey:\n            warn(\"--no-overwrite-sshkey option is deprecated since crmsh does not overwrite ssh keys by default anymore and will be removed in future versions\")\n        if self.type == \"join\" and self.watchdog:\n            warn(\"-w option is deprecated and will be removed in future versions\")\n\n    def init_sbd_manager(self):\n        self.sbd_manager = SBDManager(self.sbd_devices, self.diskless_sbd)\n\n\nclass Watchdog(object):\n    \"\"\"\n    Class to find valid watchdog device name\n    \"\"\"\n    QUERY_CMD = \"sbd query-watchdog\"\n    DEVICE_FIND_REGREX = \"\\[[0-9]+\\] (/dev/.*)\\n.*\\nDriver: (.*)\"\n\n    def __init__(self, _input=None, peer_host=None):\n        \"\"\"\n        Init function\n        \"\"\"\n        self._input = _input\n        self._peer_host = peer_host\n        self._watchdog_info_dict = {}\n        self._watchdog_device_name = None\n    \n    @property\n    def watchdog_device_name(self):\n        return self._watchdog_device_name\n\n    @staticmethod\n    def _verify_watchdog_device(dev, ignore_error=False):\n        \"\"\"\n        Use wdctl to verify watchdog device\n        \"\"\"\n        rc, _, err = utils.get_stdout_stderr(\"wdctl {}\".format(dev))\n        if rc != 0:\n            if ignore_error:\n                return False\n            else:\n                error(\"Invalid watchdog device {}: {}\".format(dev, err))\n        return True\n\n    @staticmethod\n    def _load_watchdog_driver(driver):\n        \"\"\"\n        Load specific watchdog driver\n        \"\"\"\n        invoke(\"echo {} > /etc/modules-load.d/watchdog.conf\".format(driver))\n        invoke(\"systemctl restart systemd-modules-load\")\n\n    @staticmethod\n    def _get_watchdog_device_from_sbd_config():\n        \"\"\"\n        Try to get watchdog device name from sbd config file\n        \"\"\"\n        conf = utils.parse_sysconfig(SYSCONFIG_SBD)\n        return conf.get(\"SBD_WATCHDOG_DEV\")\n\n    @staticmethod\n    def _driver_is_loaded(driver):\n        \"\"\"\n        Check if driver was already loaded\n        \"\"\"\n        _, out, _ = utils.get_stdout_stderr(\"lsmod\")\n        return re.search(\"\\n{}\\s+\".format(driver), out)\n\n    def _set_watchdog_info(self):\n        \"\"\"\n        Set watchdog info through sbd query-watchdog command\n        Content in self._watchdog_info_dict: {device_name: driver_name}\n        \"\"\"\n        rc, out, err = utils.get_stdout_stderr(self.QUERY_CMD)\n        if rc == 0 and out:\n            # output format might like:\n            #   [1] /dev/watchdog\\nIdentity: Software Watchdog\\nDriver: softdog\\n\n            self._watchdog_info_dict = dict(re.findall(self.DEVICE_FIND_REGREX, out))\n        else:\n            error(\"Failed to run {}: {}\".format(self.QUERY_CMD, err))\n\n    def _get_device_through_driver(self, driver_name):\n        \"\"\"\n        Get watchdog device name which has driver_name\n        \"\"\"\n        for device, driver in self._watchdog_info_dict.items():\n            if driver == driver_name and self._verify_watchdog_device(device):\n                return device\n        return None\n\n    def _get_driver_through_device_remotely(self, dev_name):\n        \"\"\"\n        Given watchdog device name, get driver name on remote node\n        \"\"\"\n        cmd = \"ssh -o StrictHostKeyChecking=no root@{} {}\".format(self._peer_host, self.QUERY_CMD)\n        rc, out, err = utils.get_stdout_stderr(cmd)\n        if rc == 0 and out:\n            # output format might like:\n            #   [1] /dev/watchdog\\nIdentity: Software Watchdog\\nDriver: softdog\\n\n            device_driver_dict = dict(re.findall(self.DEVICE_FIND_REGREX, out))\n            if device_driver_dict and dev_name in device_driver_dict:\n                return device_driver_dict[dev_name]\n            else:\n                return None\n        else:\n            error(\"Failed to run {} remotely: {}\".format(self.QUERY_CMD, err))\n\n    def _get_first_unused_device(self):\n        \"\"\"\n        Get first unused watchdog device name\n        \"\"\"\n        for dev in self._watchdog_info_dict:\n            if self._verify_watchdog_device(dev, ignore_error=True):\n                return dev\n        return None\n\n    def _set_input(self):\n        \"\"\"\n        If self._input was not provided by option:\n          1. Try to get it from sbd config file\n          2. Try to get the first valid device from result of sbd query-watchdog\n          3. Set the self._input as softdog\n        \"\"\"\n        if not self._input:\n            dev = self._get_watchdog_device_from_sbd_config()\n            if dev and self._verify_watchdog_device(dev, ignore_error=True):\n                self._input = dev\n                return\n            first_unused = self._get_first_unused_device()\n            self._input = first_unused if first_unused else \"softdog\"\n\n    def _valid_device(self, dev):\n        \"\"\"\n        Is an unused watchdog device\n        \"\"\"\n        if dev in self._watchdog_info_dict and self._verify_watchdog_device(dev):\n            return True\n        return False\n\n    def join_watchdog(self):\n        \"\"\"\n        In join proces, get watchdog device from config\n        If that device not exist, get driver name from init node, and load that driver\n        \"\"\"\n        self._set_watchdog_info()\n\n        res = self._get_watchdog_device_from_sbd_config()\n        if not res:\n            error(\"Failed to get watchdog device from {}\".format(SYSCONFIG_SBD))\n        self._input = res\n\n        if not self._valid_device(self._input):\n            driver = self._get_driver_through_device_remotely(self._input)\n            self._load_watchdog_driver(driver)\n\n    def init_watchdog(self):\n        \"\"\"\n        In init process, find valid watchdog device\n        \"\"\"\n        self._set_watchdog_info()\n        self._set_input()\n\n        # self._input is a device name\n        if self._valid_device(self._input):\n            self._watchdog_device_name = self._input\n            return\n\n        # self._input is invalid, exit\n        if not invokerc(\"modinfo {}\".format(self._input)):\n            error(\"Should provide valid watchdog device or driver name by -w option\")\n\n        # self._input is a driver name, load it if it was unloaded\n        if not self._driver_is_loaded(self._input):\n            self._load_watchdog_driver(self._input)\n            self._set_watchdog_info()\n\n        # self._input is a loaded driver name, find corresponding device name\n        res = self._get_device_through_driver(self._input)\n        if res:\n            self._watchdog_device_name = res\n            return\n\n\nclass SBDManager(object):\n    \"\"\"\n    Class to manage sbd configuration and services\n    \"\"\"\n    SYSCONFIG_SBD_TEMPLATE = \"/usr/share/fillup-templates/sysconfig.sbd\"\n    SBD_STATUS_DESCRIPTION = \"\"\"\nConfigure SBD:\n  If you have shared storage, for example a SAN or iSCSI target,\n  you can use it avoid split-brain scenarios by configuring SBD.\n  This requires a 1 MB partition, accessible to all nodes in the\n  cluster.  The device path must be persistent and consistent\n  across all nodes in the cluster, so /dev/disk/by-id/* devices\n  are a good choice.  Note that all data on the partition you\n  specify here will be destroyed.\n\"\"\"\n\n    def __init__(self, sbd_devices=None, diskless_sbd=False):\n        \"\"\"\n        Init function\n\n        sbd_devices is provided by '-s' option on init process\n        diskless_sbd is provided by '-S' option on init process\n        \"\"\"\n        self.sbd_devices_input = sbd_devices\n        self.diskless_sbd = diskless_sbd\n        self._sbd_devices = None\n        self._watchdog_inst = None\n\n    def _parse_sbd_device(self):\n        \"\"\"\n        Parse sbd devices, possible command line is like:\n          -s \"/dev/sdb1;/dev/sdb2\"\n          -s /dev/sdb1 -s /dev/sbd2\n        \"\"\"\n        result_list = []\n        for dev in self.sbd_devices_input:\n            if ';' in dev:\n                result_list.extend(dev.strip(';').split(';'))\n            else:\n                result_list.append(dev)\n        return result_list\n\n    @staticmethod\n    def _get_device_uuid(dev, node=None):\n        \"\"\"\n        Get UUID for specific device and node\n        \"\"\"\n        cmd = \"sbd -d {} dump\".format(dev)\n        if node:\n            cmd = \"ssh -o StrictHostKeyChecking=no root@{} '{}'\".format(node, cmd)\n\n        rc, out, err = utils.get_stdout_stderr(cmd)\n        if rc != 0 and err:\n            raise ValueError(\"Cannot dump sbd meta-data: {}\".format(err))\n        if rc == 0 and out:\n            res = re.search(\"UUID\\s*:\\s*(.*)\\n\", out)\n            if not res:\n                raise ValueError(\"Cannot find sbd device UUID for {}\".format(dev))\n            return res.group(1)\n\n    def _compare_device_uuid(self, dev, node_list):\n        \"\"\"\n        Compare local sbd device UUID with other node's sbd device UUID\n        \"\"\"\n        if not node_list:\n            return\n        local_uuid = self._get_device_uuid(dev)\n        for node in node_list:\n            remote_uuid = self._get_device_uuid(dev, node)\n            if local_uuid != remote_uuid:\n                raise ValueError(\"Device {} doesn't have the same UUID with {}\".format(dev, node))\n\n    def _verify_sbd_device(self, dev_list, compare_node_list=[]):\n        \"\"\"\n        Verify sbd device\n        \"\"\"\n        if len(dev_list) > 3:\n            raise ValueError(\"Maximum number of SBD device is 3\")\n        for dev in dev_list:\n            if not is_block_device(dev):\n                raise ValueError(\"{} doesn't look like a block device\".format(dev))\n            self._compare_device_uuid(dev, compare_node_list)\n\n    def _get_sbd_device_interactive(self):\n        \"\"\"\n        Get sbd device on interactive mode\n        \"\"\"\n        if _context.yes_to_all:\n            warn(\"Not configuring SBD ({} left untouched).\".format(SYSCONFIG_SBD))\n            return\n\n        status(self.SBD_STATUS_DESCRIPTION)\n\n        if not confirm(\"Do you wish to use SBD?\"):\n            warn(\"Not configuring SBD - STONITH will be disabled.\")\n            return\n\n        configured_dev_list = self._get_sbd_device_from_config()\n        if configured_dev_list and not confirm(\"SBD is already configured to use {} - overwrite?\".format(';'.join(configured_dev_list))):\n            return configured_dev_list\n\n        dev_list = []\n        dev_looks_sane = False\n        while not dev_looks_sane:\n            dev = prompt_for_string('Path to storage device (e.g. /dev/disk/by-id/...), or \"none\" for diskless sbd, use \";\" as separator for multi path', r'none|\\/.*')\n            if not dev:\n                continue\n            if dev == \"none\":\n                self.diskless_sbd = True\n                return\n            dev_list = dev.strip(';').split(';')\n            try:\n                self._verify_sbd_device(dev_list)\n            except ValueError as err_msg:\n                print_error_msg(str(err_msg))\n                continue\n            for dev_item in dev_list:\n                warn(\"All data on {} will be destroyed!\".format(dev_item))\n                if confirm('Are you sure you wish to use this device?'):\n                    dev_looks_sane = True\n                else:\n                    dev_looks_sane = False\n                    break\n\n        return dev_list\n\n    def _get_sbd_device(self):\n        \"\"\"\n        Get sbd device from options or interactive mode\n        \"\"\"\n        dev_list = []\n        if self.sbd_devices_input:\n            dev_list = self._parse_sbd_device()\n            self._verify_sbd_device(dev_list)\n        elif not self.diskless_sbd:\n            dev_list = self._get_sbd_device_interactive()\n        self._sbd_devices = dev_list\n\n    def _initialize_sbd(self):\n        \"\"\"\n        Initialize SBD device\n        \"\"\"\n        if self.diskless_sbd:\n            return\n        for dev in self._sbd_devices:\n            rc, _, err = invoke(\"sbd -d {} create\".format(dev))\n            if not rc:\n                error(\"Failed to initialize SBD device {}: {}\".format(dev, err))\n\n    def _update_configuration(self):\n        \"\"\"\n        Update /etc/sysconfig/sbd\n        \"\"\"\n        shutil.copyfile(self.SYSCONFIG_SBD_TEMPLATE, SYSCONFIG_SBD)\n        sbd_config_dict = {\n                \"SBD_PACEMAKER\": \"yes\",\n                \"SBD_STARTMODE\": \"always\",\n                \"SBD_DELAY_START\": \"no\",\n                \"SBD_WATCHDOG_DEV\": self._watchdog_inst.watchdog_device_name\n                }\n        if self._sbd_devices:\n            sbd_config_dict[\"SBD_DEVICE\"] = ';'.join(self._sbd_devices)\n        utils.sysconfig_set(SYSCONFIG_SBD, **sbd_config_dict)\n        csync2_update(SYSCONFIG_SBD)\n\n    @staticmethod\n    def _get_sbd_device_from_config():\n        \"\"\"\n        Gets currently configured SBD device, i.e. what's in /etc/sysconfig/sbd\n        \"\"\"\n        conf = utils.parse_sysconfig(SYSCONFIG_SBD)\n        res = conf.get(\"SBD_DEVICE\")\n        if res:\n            return res.strip(';').split(';')\n        else:\n            return None\n\n    def sbd_init(self):\n        \"\"\"\n        Function sbd_init includes these steps:\n        1. Get sbd device from options or interactive mode\n        2. Initialize sbd device\n        3. Write config file /etc/sysconfig/sbd\n        \"\"\"\n        if not utils.package_is_installed(\"sbd\"):\n            return\n        self._watchdog_inst = Watchdog(_input=_context.watchdog)\n        self._watchdog_inst.init_watchdog()\n        self._get_sbd_device()\n        if not self._sbd_devices and not self.diskless_sbd:\n            invoke(\"systemctl disable sbd.service\")\n            return\n        status_long(\"Initializing {}SBD...\".format(\"diskless \" if self.diskless_sbd else \"\"))\n        self._initialize_sbd()\n        self._update_configuration()\n        invoke(\"systemctl enable sbd.service\")\n        status_done()\n\n    def configure_sbd_resource(self):\n        \"\"\"\n        Configure stonith-sbd resource and stonith-enabled property\n        \"\"\"\n        if not utils.package_is_installed(\"sbd\"):\n            return\n        if utils.service_is_enabled(\"sbd.service\"):\n            if self._get_sbd_device_from_config():\n                if not invokerc(\"crm configure primitive stonith-sbd stonith:external/sbd pcmk_delay_max=30s\"):\n                    error(\"Can't create stonith-sbd primitive\")\n                if not invokerc(\"crm configure property stonith-enabled=true\"):\n                    error(\"Can't enable STONITH for SBD\")\n            else:\n                if not invokerc(\"crm configure property stonith-enabled=true stonith-watchdog-timeout=5s\"):\n                    error(\"Can't enable STONITH for diskless SBD\")\n\n    def join_sbd(self, peer_host):\n        \"\"\"\n        Function join_sbd running on join process only\n        On joining process, check whether peer node has enabled sbd.service\n        If so, check prerequisites of SBD and verify sbd device on join node\n        \"\"\"\n        if not utils.package_is_installed(\"sbd\"):\n            return\n        if not os.path.exists(SYSCONFIG_SBD) or not utils.service_is_enabled(\"sbd.service\", peer_host):\n            invoke(\"systemctl disable sbd.service\")\n            return\n        self._watchdog_inst = Watchdog(peer_host=peer_host)\n        self._watchdog_inst.join_watchdog()\n        dev_list = self._get_sbd_device_from_config()\n        if dev_list:\n            self._verify_sbd_device(dev_list, [peer_host])\n        status(\"Got {}SBD configuration\".format(\"\" if dev_list else \"diskless \"))\n        invoke(\"systemctl enable sbd.service\")\n\n    @classmethod\n    def verify_sbd_device(cls):\n        \"\"\"\n        This classmethod is for verifying sbd device on a running cluster\n        Raise ValueError for exceptions\n        \"\"\"\n        inst = cls()\n        dev_list = inst._get_sbd_device_from_config()\n        if not dev_list:\n            raise ValueError(\"No sbd device configured\")\n        inst._verify_sbd_device(dev_list, utils.list_cluster_nodes_except_me())\n\n\n_context = None\n\n\ndef die(*args):\n    \"\"\"\n    Broken out as special case for log() failure.  Ordinarily you\n    should just use error() to terminate.\n    \"\"\"\n    raise ValueError(\" \".join([str(arg) for arg in args]))\n\n\ndef error(*args):\n    \"\"\"\n    Log an error message and raise ValueError to bail out of\n    bootstrap process.\n    \"\"\"\n    log(\"ERROR: {}\".format(\" \".join([str(arg) for arg in args])))\n    die(*args)\n\n\ndef print_error_msg(msg):\n    \"\"\"\n    Just print error message\n    \"\"\"\n    print(term.render(clidisplay.error(\"ERROR:\")) + \" {}\".format(msg))\n\n\ndef warn(*args):\n    \"\"\"\n    Log and display a warning message.\n    \"\"\"\n    log(\"WARNING: {}\".format(\" \".join(str(arg) for arg in args)))\n    print(term.render(clidisplay.warn(\"WARNING: {}\".format(\" \".join(str(arg) for arg in args)))))\n\n\n@utils.memoize\ndef log_file_fallback():\n    \"\"\"\n    If the standard log location isn't writable,\n    just log to the nearest temp dir.\n    \"\"\"\n    return os.path.join(utils.get_tempdir(), \"ha-cluster-bootstrap.log\")\n\n\ndef log(*args):\n    global LOG_FILE\n    try:\n        Path(os.path.dirname(LOG_FILE)).mkdir(parents=True, exist_ok=True)\n        with open(LOG_FILE, \"ab\") as logfile:\n            text = \" \".join([utils.to_ascii(arg) for arg in args]) + \"\\n\"\n            logfile.write(text.encode('ascii', 'backslashreplace'))\n    except IOError:\n        if LOG_FILE != log_file_fallback():\n            LOG_FILE = log_file_fallback()\n            log(*args)\n        else:\n            die(\"Can't append to {} - aborting\".format(LOG_FILE))\n\n\ndef drop_last_history():\n    hlen = readline.get_current_history_length()\n    if hlen > 0:\n        readline.remove_history_item(hlen - 1)\n\n\ndef prompt_for_string(msg, match=None, default='', valid_func=None, prev_value=[]):\n    if _context.yes_to_all:\n        return default\n\n    while True:\n        disable_completion()\n        val = utils.multi_input('  %s [%s]' % (msg, default))\n        enable_completion()\n        if not val:\n            val = default\n        else:\n            drop_last_history()\n\n        if not val:\n            return None\n        if not match and not valid_func:\n            return val\n        if match and not re.match(match, val):\n            print_error_msg(\"Invalid value entered\")\n            continue\n        if valid_func:\n            try:\n                valid_func(val, prev_value)\n            except ValueError as err:\n                print_error_msg(err)\n                continue\n\n        return val\n\n\ndef confirm(msg):\n    if _context.yes_to_all:\n        return True\n    disable_completion()\n    rc = utils.ask(msg)\n    enable_completion()\n    drop_last_history()\n    return rc\n\n\ndef disable_completion():\n    if _context.ui_context:\n        _context.ui_context.disable_completion()\n\n\ndef enable_completion():\n    if _context.ui_context:\n        _context.ui_context.setup_readline()\n\n\ndef invoke(*args):\n    \"\"\"\n    Log command execution to log file.\n    Log output from command to log file.\n    Return (boolean, stdout, stderr)\n    \"\"\"\n    log(\"+ \" + \" \".join(args))\n    rc, stdout, stderr = utils.get_stdout_stderr(\" \".join(args))\n    if stdout:\n        log(stdout)\n    if stderr:\n        log(stderr)\n    return rc == 0, stdout, stderr\n\n\ndef invokerc(*args):\n    \"\"\"\n    Calling invoke, return True/False\n    \"\"\"\n    rc, _, _ = invoke(*args)\n    return rc\n\n\ndef crm_configure_load(action, configuration):\n    log(\": loading crm config (%s), content is:\" % (action))\n    log(configuration)\n    if not cib_factory.initialize():\n        error(\"Failed to load cluster configuration\")\n    set_obj = mkset_obj()\n    if action == 'replace':\n        cib_factory.erase()\n    if not set_obj.save(configuration, remove=False, method=action):\n        error(\"Failed to load cluster configuration\")\n    if not cib_factory.commit():\n        error(\"Failed to commit cluster configuration\")\n\n\ndef wait_for_resource(message, resource, needle=\"running on\"):\n    status_long(message)\n    while True:\n        _rc, out, err = utils.get_stdout_stderr(\"crm_resource --locate --resource \" + resource)\n        if needle in out:\n            break\n        if needle in err:\n            break\n        status_progress()\n        sleep(1)\n    status_done()\n\n\ndef wait_for_stop(message, resource):\n    return wait_for_resource(message, resource, needle=\"NOT running\")\n\n\ndef wait_for_cluster():\n    status_long(\"Waiting for cluster\")\n    while True:\n        _rc, out, _err = utils.get_stdout_stderr(\"crm_mon -1\")\n        if is_online(out):\n            break\n        status_progress()\n        sleep(2)\n    status_done()\n\n\ndef get_cluster_node_hostname():\n    \"\"\"\n    Get the hostname of the cluster node used during the join process if an IP address is used.\n    \"\"\"\n    peer_node = None\n    if _context.cluster_node:\n        if utils.IP.is_valid_ip(_context.cluster_node):\n            rc, out, err = utils.get_stdout_stderr(\"ssh {} crm_node --name\".format(_context.cluster_node))\n            if rc != 0:\n                error(err)\n            peer_node = out\n        else:\n            peer_node = _context.cluster_node\n    return peer_node\n\n\ndef is_online(crm_mon_txt):\n    \"\"\"\n    Check whether local node is online\n    Besides that, in join process, check whether init node is online\n    \"\"\"\n    if not re.search(\"Online: .* {} \".format(utils.this_node()), crm_mon_txt):\n        return False\n\n    # if peer_node is None, this is in the init process\n    peer_node = get_cluster_node_hostname()\n    if peer_node is None:\n        return True\n    # In join process\n    # If the joining node is already online but can't find the init node\n    # The communication IP maybe mis-configured\n    if not re.search(\"Online: .* {} \".format(peer_node), crm_mon_txt):\n        shutil.copy(COROSYNC_CONF_ORIG, corosync.conf())\n        csync2_update(corosync.conf())\n        utils.stop_service(\"corosync\")\n        print()\n        error(\"Cannot see peer node \\\"{}\\\", please check the communication IP\".format(peer_node))\n    return True\n\n\ndef pick_default_value(default_list, prev_list):\n    \"\"\"\n    Provide default value for function 'prompt_for_string'.\n    Make sure give different default value in multi-ring mode.\n\n    Parameters:\n    * default_list - default value list for config item\n    * prev_list    - previous value for config item in multi-ring mode\n    \"\"\"\n    for value in default_list:\n        if value not in prev_list:\n            return value\n    return \"\"\n\n\ndef sleep(t):\n    \"\"\"\n    Sleep for t seconds.\n    \"\"\"\n    t = float(t)\n    time.sleep(t)\n\n\ndef status(msg):\n    log(\"# \" + msg)\n    if not _context.quiet:\n        print(\"  {}\".format(msg))\n\n\ndef status_long(msg):\n    log(\"# {}...\".format(msg))\n    if not _context.quiet:\n        sys.stdout.write(\"  {}...\".format(msg))\n        sys.stdout.flush()\n\n\ndef status_progress():\n    if not _context.quiet:\n        sys.stdout.write(\".\")\n        sys.stdout.flush()\n\n\ndef status_done():\n    log(\"# done\")\n    if not _context.quiet:\n        print(\"done\")\n\n\ndef partprobe():\n    # This function uses fdisk to create a list of valid devices for probing\n    # with partprobe.  This prevents partprobe from failing on read-only mounted\n    # devices such as /dev/sr0 (etc) that might cause it to return an error when\n    # it exits.  This allows partprobe to run without forcing _die to bail out.\n    # -Brandon Heaton\n    #  ATT Training Engineer\n    #  Data Center Engineer\n    #  bheaton@suse.com\n    _rc, out, _err = utils.get_stdout_stderr(\"sfdisk -l\")\n    disks = re.findall(r'^Disk\\s*(/.+):', out, re.M)\n    invoke(\"partprobe\", *disks)\n\n\ndef probe_partitions():\n    status_long(\"Probing for new partitions\")\n    partprobe()\n    sleep(5)\n    status_done()\n\n\ndef check_tty():\n    \"\"\"\n    Check for pseudo-tty: Cannot display read prompts without a TTY (bnc#892702)\n    \"\"\"\n    if _context.yes_to_all:\n        return\n    if not sys.stdin.isatty():\n        error(\"No pseudo-tty detected! Use -t option to ssh if calling remotely.\")\n\n\ndef my_hostname_resolves():\n    import socket\n    hostname = utils.this_node()\n    try:\n        socket.gethostbyname(hostname)\n        return True\n    except socket.error:\n        return False\n\n\ndef check_prereqs(stage):\n    warned = False\n\n    if not my_hostname_resolves():\n        warn(\"Hostname '{}' is unresolvable. {}\".format(\n            utils.this_node(),\n            \"Please add an entry to /etc/hosts or configure DNS.\"))\n        warned = True\n\n    timekeepers = ('chronyd.service', 'ntp.service', 'ntpd.service')\n    timekeeper = None\n    for tk in timekeepers:\n        if utils.service_is_available(tk):\n            timekeeper = tk\n            break\n\n    if timekeeper is None:\n        warn(\"No NTP service found.\")\n        warned = True\n    elif not utils.service_is_enabled(timekeeper):\n        warn(\"{} is not configured to start at system boot.\".format(timekeeper))\n        warned = True\n\n    if warned:\n        if not confirm(\"Do you want to continue anyway?\"):\n            return False\n\n    firewall_open_basic_ports()\n    return True\n\n\ndef log_start():\n    \"\"\"\n    Convenient side-effect: this will die immediately if the log file\n    is not writable (e.g. if not running as root)\n    \"\"\"\n    # Reload rsyslog to make sure it logs with the correct hostname\n    if utils.service_is_active(\"rsyslog.service\"):\n        invoke(\"systemctl reload rsyslog.service\")\n    datestr = utils.get_stdout(\"date --rfc-3339=seconds\")[1]\n    log('================================================================')\n    log(\"%s %s\" % (datestr, \" \".join(sys.argv)))\n    log('----------------------------------------------------------------')\n\n\ndef init_network():\n    \"\"\"\n    Get all needed network information through utils.InterfacesInfo\n    \"\"\"\n    interfaces_inst = utils.InterfacesInfo(_context.ipv6, _context.second_heartbeat, _context.nic_list)\n    interfaces_inst.get_interfaces_info()\n    _context.default_nic_list = interfaces_inst.get_default_nic_list_from_route()\n    _context.default_ip_list = interfaces_inst.get_default_ip_list()\n\n    # local_ip_list and local_network_list are for validation\n    _context.local_ip_list = interfaces_inst.ip_list\n    _context.local_network_list = interfaces_inst.network_list\n    _context.interfaces_inst = interfaces_inst\n    # use two \"-i\" options equal to use \"-M\" option\n    if len(_context.default_nic_list) == 2 and not _context.second_heartbeat:\n        _context.second_heartbeat = True\n\n\ndef configure_firewall(tcp=None, udp=None):\n    if tcp is None:\n        tcp = []\n    if udp is None:\n        udp = []\n\n    def init_firewall_suse(tcp, udp):\n        if os.path.exists(SYSCONFIG_FW_CLUSTER):\n            cluster = utils.parse_sysconfig(SYSCONFIG_FW_CLUSTER)\n            tcpcurr = set(cluster.get(\"TCP\", \"\").split())\n            tcpcurr.update(tcp)\n            tcp = list(tcpcurr)\n            udpcurr = set(cluster.get(\"UDP\", \"\").split())\n            udpcurr.update(udp)\n            udp = list(udpcurr)\n\n        utils.sysconfig_set(SYSCONFIG_FW_CLUSTER, TCP=\" \".join(tcp), UDP=\" \".join(udp))\n\n        ext = \"\"\n        if os.path.exists(SYSCONFIG_FW):\n            fw = utils.parse_sysconfig(SYSCONFIG_FW)\n            ext = fw.get(\"FW_CONFIGURATIONS_EXT\", \"\")\n            if \"cluster\" not in ext.split():\n                ext = ext + \" cluster\"\n        utils.sysconfig_set(SYSCONFIG_FW, FW_CONFIGURATIONS_EXT=ext)\n\n        # No need to do anything else if the firewall is inactive\n        if not utils.service_is_active(\"SuSEfirewall2\"):\n            return\n\n        # Firewall is active, either restart or complain if we couldn't tweak it\n        status(\"Restarting firewall (tcp={}, udp={})\".format(\" \".join(tcp), \" \".join(udp)))\n        if not invokerc(\"rcSuSEfirewall2 restart\"):\n            error(\"Failed to restart firewall (SuSEfirewall2)\")\n\n    def init_firewall_firewalld(tcp, udp):\n        has_firewalld = utils.service_is_active(\"firewalld\")\n        cmdbase = 'firewall-cmd --zone=public --permanent ' if has_firewalld else 'firewall-offline-cmd --zone=public '\n\n        def cmd(args):\n            if not invokerc(cmdbase + args):\n                error(\"Failed to configure firewall.\")\n\n        for p in tcp:\n            cmd(\"--add-port={}/tcp\".format(p))\n\n        for p in udp:\n            cmd(\"--add-port={}/udp\".format(p))\n\n        if has_firewalld:\n            if not invokerc(\"firewall-cmd --reload\"):\n                error(\"Failed to reload firewall configuration.\")\n\n    def init_firewall_ufw(tcp, udp):\n        \"\"\"\n        try configuring firewall with ufw\n        \"\"\"\n        for p in tcp:\n            if not invokerc(\"ufw allow {}/tcp\".format(p)):\n                error(\"Failed to configure firewall (ufw)\")\n        for p in udp:\n            if not invokerc(\"ufw allow {}/udp\".format(p)):\n                error(\"Failed to configure firewall (ufw)\")\n\n    if utils.package_is_installed(\"firewalld\"):\n        init_firewall_firewalld(tcp, udp)\n    elif utils.package_is_installed(\"SuSEfirewall2\"):\n        init_firewall_suse(tcp, udp)\n    elif utils.package_is_installed(\"ufw\"):\n        init_firewall_ufw(tcp, udp)\n    else:\n        warn(\"Failed to detect firewall: Could not open ports tcp={}, udp={}\".format(\"|\".join(tcp), \"|\".join(udp)))\n\n\ndef firewall_open_basic_ports():\n    \"\"\"\n    Open ports for csync2, mgmtd, hawk & dlm respectively\n    \"\"\"\n    configure_firewall(tcp=[\"30865\", \"5560\", \"7630\", \"21064\"])\n\n\ndef firewall_open_corosync_ports():\n    \"\"\"\n    Have to do this separately, as we need general firewall config early\n    so csync2 works, but need corosync config *after* corosync.conf has\n    been created/updated.\n\n    Please note corosync uses two UDP ports mcastport (for mcast\n    receives) and mcastport - 1 (for mcast sends).\n\n    Also open QNetd/QDevice port if configured.\n    \"\"\"\n    # all mcastports defined in corosync config\n    udp = corosync.get_values(\"totem.interface.mcastport\")\n    udp.extend([str(int(p) - 1) for p in udp])\n\n    tcp = corosync.get_values(\"totem.quorum.device.net.port\")\n\n    configure_firewall(tcp=tcp, udp=udp)\n\n\ndef init_cluster_local():\n    # Caller should check this, but I'm paranoid...\n    if utils.service_is_active(\"corosync.service\"):\n        error(\"corosync service is running!\")\n\n    firewall_open_corosync_ports()\n\n    # reset password, but only if it's not already set\n    _rc, outp = utils.get_stdout(\"passwd -S hacluster\")\n    ps = outp.strip().split()[1]\n    pass_msg = \"\"\n    if ps not in (\"P\", \"PS\"):\n        log(': Resetting password of hacluster user')\n        rc, outp, errp = utils.get_stdout_stderr(\"passwd hacluster\", input_s=b\"linux\\nlinux\\n\")\n        if rc != 0:\n            warn(\"Failed to reset password of hacluster user: %s\" % (outp + errp))\n        else:\n            pass_msg = \", password 'linux'\"\n\n    # evil, but necessary\n    invoke(\"rm -f /var/lib/heartbeat/crm/* /var/lib/pacemaker/cib/*\")\n\n    # only try to start hawk if hawk is installed\n    if utils.service_is_available(\"hawk.service\"):\n        utils.start_service(\"hawk.service\", enable=True)\n        status(\"Hawk cluster interface is now running. To see cluster status, open:\")\n        status(\"  https://{}:7630/\".format(_context.default_ip_list[0]))\n        status(\"Log in with username 'hacluster'{}\".format(pass_msg))\n    else:\n        warn(\"Hawk not installed - not configuring web management interface.\")\n\n    if pass_msg:\n        warn(\"You should change the hacluster password to something more secure!\")\n\n    utils.start_service(\"pacemaker.service\", enable=True)\n    wait_for_cluster()\n\n\ndef install_tmp(tmpfile, to):\n    with open(tmpfile, \"r\") as src:\n        with utils.open_atomic(to, \"w\") as dst:\n            for line in src:\n                dst.write(line)\n\n\ndef append(fromfile, tofile):\n    log(\"+ cat %s >> %s\" % (fromfile, tofile))\n    with open(tofile, \"a\") as tf:\n        with open(fromfile, \"r\") as ff:\n            tf.write(ff.read())\n\n\ndef append_unique(fromfile, tofile):\n    \"\"\"\n    Append unique content from fromfile to tofile\n    \"\"\"\n    if not utils.check_file_content_included(fromfile, tofile):\n        append(fromfile, tofile)\n\n\ndef rmfile(path, ignore_errors=False):\n    \"\"\"\n    Try to remove the given file, and\n    report an error on failure\n    \"\"\"\n    try:\n        os.remove(path)\n    except os.error as err:\n        if not ignore_errors:\n            error(\"Failed to remove {}: {}\".format(path, err))\n\n\ndef mkdirs_owned(dirs, mode=0o777, uid=-1, gid=-1):\n    \"\"\"\n    Create directory path, setting the mode and\n    ownership of the leaf directory to mode/uid/gid.\n    \"\"\"\n    if not os.path.exists(dirs):\n        try:\n            os.makedirs(dirs, mode)\n        except OSError as err:\n            error(\"Failed to create {}: {}\".format(dirs, err))\n        if uid != -1 or gid != -1:\n            utils.chown(dirs, uid, gid)\n\n\ndef init_ssh():\n    \"\"\"\n    Configure passwordless SSH.\n    \"\"\"\n    utils.start_service(\"sshd.service\", enable=True)\n    for user in USER_LIST:\n        configure_local_ssh_key(user)\n\n\ndef key_files(user):\n    \"\"\"\n    Find home directory for user and return key files with abspath\n    \"\"\"\n    keyfile_dict = {}\n    home_dir = userdir.gethomedir(user)\n    keyfile_dict['private'] = \"{}/.ssh/id_rsa\".format(home_dir)\n    keyfile_dict['public'] = \"{}/.ssh/id_rsa.pub\".format(home_dir)\n    keyfile_dict['authorized'] = \"{}/.ssh/authorized_keys\".format(home_dir)\n    return keyfile_dict\n\n\ndef is_nologin(user):\n    \"\"\"\n    Check if user's shell is /sbin/nologin\n    \"\"\"\n    with open(\"/etc/passwd\") as f:\n        return re.search(\"{}:.*:/sbin/nologin\".format(user), f.read())\n\n\ndef change_user_shell(user):\n    \"\"\"\n    To change user's login shell\n    \"\"\"\n    if user != \"root\" and is_nologin(user):\n        if not _context.yes_to_all:\n            status(\"\"\"\nUser {} will be changed the login shell as /bin/bash, and\nbe setted up authorized ssh access among cluster nodes\"\"\".format(user))\n            if not confirm(\"Continue?\"):\n                _context.with_other_user = False\n                return\n        invoke(\"usermod -s /bin/bash {}\".format(user))\n\n\ndef configure_local_ssh_key(user=\"root\"):\n    \"\"\"\n    Configure ssh rsa key locally\n\n    If <home_dir>/.ssh/id_rsa not exist, generate a new one\n    Add <home_dir>/.ssh/id_rsa.pub to <home_dir>/.ssh/authorized_keys anyway, make sure itself authorized\n    \"\"\"\n    change_user_shell(user)\n\n    private_key, public_key, authorized_file = key_files(user).values()\n    if not os.path.exists(private_key):\n        status(\"Generating SSH key for {}\".format(user))\n        cmd = \"ssh-keygen -q -f {} -C 'Cluster Internal on {}' -N ''\".format(private_key, utils.this_node())\n        cmd = utils.add_su(cmd, user)\n        rc, _, err = invoke(cmd)\n        if not rc:\n            error(\"Failed to generate ssh key for {}: {}\".format(user, err))\n\n    if not os.path.exists(authorized_file):\n        open(authorized_file, 'w').close()\n    append_unique(public_key, authorized_file)\n\n\ndef init_ssh_remote():\n    \"\"\"\n    Called by ha-cluster-join\n    \"\"\"\n    authorized_keys_file = \"/root/.ssh/authorized_keys\"\n    if not os.path.exists(authorized_keys_file):\n        open(authorized_keys_file, 'w').close()\n    authkeys = open(authorized_keys_file, \"r+\")\n    authkeys_data = authkeys.read()\n    for key in (\"id_rsa\", \"id_dsa\", \"id_ecdsa\", \"id_ed25519\"):\n        fn = os.path.join(\"/root/.ssh\", key)\n        if not os.path.exists(fn):\n            continue\n        keydata = open(fn + \".pub\").read()\n        if keydata not in authkeys_data:\n            append(fn + \".pub\", authorized_keys_file)\n\n\ndef append_to_remote_file(fromfile, remote_node, tofile):\n    \"\"\"\n    Append content of fromfile to tofile on remote_node\n    \"\"\"\n    err_details_string = \"\"\"\n    crmsh has no way to help you to setup up passwordless ssh among nodes at this time. \n    As the hint, likely, `PasswordAuthentication` is 'no' in /etc/ssh/sshd_config. \n    Given in this case, users must setup passwordless ssh beforehand, or change it to 'yes' and manage passwords properly\n    \"\"\"\n    cmd = \"cat {} | ssh -oStrictHostKeyChecking=no root@{} 'cat >> {}'\".format(fromfile, remote_node, tofile)\n    rc, _, err = invoke(cmd)\n    if not rc:\n        error(\"Failed to append contents of {} to {}:\\n\\\"{}\\\"\\n{}\".format(fromfile, remote_node, err, err_details_string))\n\n\ndef init_csync2():\n    status(\"Configuring csync2\")\n    if os.path.exists(CSYNC2_KEY):\n        if not confirm(\"csync2 is already configured - overwrite?\"):\n            return\n\n    invoke(\"rm\", \"-f\", CSYNC2_KEY)\n    status_long(\"Generating csync2 shared key (this may take a while)\")\n    if not invokerc(\"csync2\", \"-k\", CSYNC2_KEY):\n        error(\"Can't create csync2 key {}\".format(CSYNC2_KEY))\n    status_done()\n\n    utils.str2file(\"\"\"group ha_group\n{\nkey /etc/csync2/key_hagroup;\nhost %s;\ninclude /etc/booth;\ninclude /etc/corosync/corosync.conf;\ninclude /etc/corosync/authkey;\ninclude /etc/csync2/csync2.cfg;\ninclude /etc/csync2/key_hagroup;\ninclude /etc/ctdb/nodes;\ninclude /etc/drbd.conf;\ninclude /etc/drbd.d;\ninclude /etc/ha.d/ldirectord.cf;\ninclude /etc/lvm/lvm.conf;\ninclude /etc/multipath.conf;\ninclude /etc/samba/smb.conf;\ninclude /etc/sysconfig/nfs;\ninclude /etc/sysconfig/pacemaker;\ninclude /etc/sysconfig/sbd;\ninclude /etc/pacemaker/authkey;\ninclude /etc/modules-load.d/watchdog.conf;\n}\n    \"\"\" % (utils.this_node()), CSYNC2_CFG)\n\n    utils.start_service(\"csync2.socket\", enable=True)\n    status_long(\"csync2 checking files\")\n    invoke(\"csync2\", \"-cr\", \"/\")\n    status_done()\n\n\ndef csync2_update(path):\n    '''\n    Sync path to all peers\n\n    If there was a conflict, use '-f' to force this side to win\n    '''\n    invoke(\"csync2 -rm {}\".format(path))\n    if invokerc(\"csync2 -rxv {}\".format(path)):\n        return\n    invoke(\"csync2 -rf {}\".format(path))\n    if not invokerc(\"csync2 -rxv {}\".format(path)):\n        warn(\"{} was not synced\".format(path))\n\n\ndef init_csync2_remote():\n    \"\"\"\n    It would be nice if we could just have csync2.cfg include a directory,\n    which in turn included one file per node which would be referenced via\n    something like \"group ha_group { ... config: /etc/csync2/hosts/*; }\"\n    That way, adding a new node would just mean adding a single new file\n    to that directory.  Unfortunately, the 'config' statement only allows\n    inclusion of specific individual files, not multiple files via wildcard.\n    So we have this function which is called by ha-cluster-join to add the new\n    remote node to csync2 config on some existing node.  It is intentionally\n    not documented in ha-cluster-init's user-visible usage information.\n    \"\"\"\n    newhost = _context.cluster_node\n    if not newhost:\n        error(\"Hostname not specified\")\n\n    curr_cfg = open(CSYNC2_CFG).read()\n\n    was_quiet = _context.quiet\n    try:\n        _context.quiet = True\n        # if host doesn't already exist in csync2 config, add it\n        if not re.search(r\"^\\s*host.*\\s+%s\\s*;\" % (newhost), curr_cfg, flags=re.M):\n            curr_cfg = re.sub(r\"\\bhost.*\\s+\\S+\\s*;\", r\"\\g<0>\\n\\thost %s;\" % (utils.doublequote(newhost)), curr_cfg, count=1)\n            utils.str2file(curr_cfg, CSYNC2_CFG)\n            csync2_update(\"/\")\n        else:\n            log(\": Not updating %s - remote host %s already exists\" % (CSYNC2_CFG, newhost))\n    finally:\n        _context.quiet = was_quiet\n\n\ndef init_corosync_auth():\n    \"\"\"\n    Generate the corosync authkey\n    \"\"\"\n    if os.path.exists(COROSYNC_AUTH):\n        if not confirm(\"%s already exists - overwrite?\" % (COROSYNC_AUTH)):\n            return\n        rmfile(COROSYNC_AUTH)\n    invoke(\"corosync-keygen -l\")\n\n\ndef init_remote_auth():\n    \"\"\"\n    Generate the pacemaker-remote authkey\n    \"\"\"\n    if os.path.exists(PCMK_REMOTE_AUTH):\n        if not confirm(\"%s already exists - overwrite?\" % (PCMK_REMOTE_AUTH)):\n            return\n        rmfile(PCMK_REMOTE_AUTH)\n\n    pcmk_remote_dir = os.path.dirname(PCMK_REMOTE_AUTH)\n    mkdirs_owned(pcmk_remote_dir, mode=0o750, gid=\"haclient\")\n    if not invokerc(\"dd if=/dev/urandom of={} bs=4096 count=1\".format(PCMK_REMOTE_AUTH)):\n        warn(\"Failed to create pacemaker authkey: {}\".format(PCMK_REMOTE_AUTH))\n    utils.chown(PCMK_REMOTE_AUTH, \"hacluster\", \"haclient\")\n    os.chmod(PCMK_REMOTE_AUTH, 0o640)\n\n\nclass Validation(object):\n    \"\"\"\n    Class to validate values from interactive inputs\n    \"\"\"\n\n    def __init__(self, value, prev_value_list=[]):\n        \"\"\"\n        Init function\n        \"\"\"\n        self.value = value\n        self.prev_value_list = prev_value_list\n        if self.value in self.prev_value_list:\n            raise ValueError(\"Already in use: {}\".format(self.value))\n\n    def _is_mcast_addr(self):\n        \"\"\"\n        Check whether the address is multicast address\n        \"\"\"\n        if not utils.IP.is_mcast(self.value):\n            raise ValueError(\"{} is not multicast address\".format(self.value))\n\n    def _is_local_addr(self, local_addr_list):\n        \"\"\"\n        Check whether the address is in local\n        \"\"\"\n        if self.value not in local_addr_list:\n            raise ValueError(\"Address must be a local address (one of {})\".format(local_addr_list))\n\n    def _is_valid_port(self):\n        \"\"\"\n        Check whether the port is valid\n        \"\"\"\n        if self.prev_value_list and abs(int(self.value) - int(self.prev_value_list[0])) <= 1:\n            raise ValueError(\"Port {} is already in use by corosync. Leave a gap between multiple rings.\".format(self.value))\n        if int(self.value) <= 1024 or int(self.value) > 65535:\n            raise ValueError(\"Valid port range should be 1025-65535\")\n\n    @classmethod\n    def valid_mcast_address(cls, addr, prev_value_list=[]):\n        \"\"\"\n        Check whether the address is already in use and whether the address is for multicast\n        \"\"\"\n        cls_inst = cls(addr, prev_value_list)\n        cls_inst._is_mcast_addr()\n\n    @classmethod\n    def valid_ucast_ip(cls, addr, prev_value_list=[]):\n        \"\"\"\n        Check whether the address is already in use and whether the address exists on local\n        \"\"\"\n        cls_inst = cls(addr, prev_value_list)\n        cls_inst._is_local_addr(_context.local_ip_list)\n\n    @classmethod\n    def valid_mcast_ip(cls, addr, prev_value_list=[]):\n        \"\"\"\n        Check whether the address is already in use and whether the address exists on local address and network\n        \"\"\"\n        cls_inst = cls(addr, prev_value_list)\n        cls_inst._is_local_addr(_context.local_ip_list + _context.local_network_list)\n\n    @classmethod\n    def valid_port(cls, port, prev_value_list=[]):\n        \"\"\"\n        Check whether the port is valid\n        \"\"\"\n        cls_inst = cls(port, prev_value_list)\n        cls_inst._is_valid_port()\n\n    @staticmethod\n    def valid_admin_ip(addr, prev_value_list=[]):\n        \"\"\"\n        Validate admin IP address\n        \"\"\"\n        ipv6 = utils.IP.is_ipv6(addr)\n\n        # Check whether this IP already configured in cluster\n        ping_cmd = \"ping6\" if ipv6 else \"ping\"\n        if invokerc(\"{} -c 1 {}\".format(ping_cmd, addr)):\n            raise ValueError(\"Address already in use: {}\".format(addr))\n\n\ndef init_corosync_unicast():\n\n    if _context.yes_to_all:\n        status(\"Configuring corosync (unicast)\")\n    else:\n        status(\"\"\"\nConfigure Corosync (unicast):\n  This will configure the cluster messaging layer.  You will need\n  to specify a network address over which to communicate (default\n  is {}'s network, but you can use the network address of any\n  active interface).\n\"\"\".format(_context.default_nic_list[0]))\n\n    ringXaddr_res = []\n    mcastport_res = []\n    default_ports = [\"5405\", \"5407\"]\n    two_rings = False\n\n    for i in range(2):\n        ringXaddr = prompt_for_string(\n                'Address for ring{}'.format(i),\n                default=pick_default_value(_context.default_ip_list, ringXaddr_res),\n                valid_func=Validation.valid_ucast_ip,\n                prev_value=ringXaddr_res)\n        if not ringXaddr:\n            error(\"No value for ring{}\".format(i))\n        ringXaddr_res.append(ringXaddr)\n\n        mcastport = prompt_for_string(\n                'Port for ring{}'.format(i),\n                match='[0-9]+',\n                default=pick_default_value(default_ports, mcastport_res),\n                valid_func=Validation.valid_port,\n                prev_value=mcastport_res)\n        if not mcastport:\n            error(\"Expected a multicast port for ring{}\".format(i))\n        mcastport_res.append(mcastport)\n\n        if i == 1 or \\\n           not _context.second_heartbeat or \\\n           not confirm(\"\\nAdd another heartbeat line?\"):\n            break\n        two_rings = True\n\n    corosync.create_configuration(\n            clustername=_context.cluster_name,\n            ringXaddr=ringXaddr_res,\n            mcastport=mcastport_res,\n            transport=\"udpu\",\n            ipv6=_context.ipv6,\n            two_rings=two_rings)\n    csync2_update(corosync.conf())\n\n\ndef init_corosync_multicast():\n    def gen_mcastaddr():\n        if _context.ipv6:\n            return \"ff3e::%s:%d\" % (\n                ''.join([random.choice('0123456789abcdef') for _ in range(4)]),\n                random.randint(0, 9))\n        return \"239.%d.%d.%d\" % (\n            random.randint(0, 255),\n            random.randint(0, 255),\n            random.randint(1, 255))\n\n    if _context.yes_to_all:\n        status(\"Configuring corosync\")\n    else:\n        status(\"\"\"\nConfigure Corosync:\n  This will configure the cluster messaging layer.  You will need\n  to specify a network address over which to communicate (default\n  is {}'s network, but you can use the network address of any\n  active interface).\n\"\"\".format(_context.default_nic_list[0]))\n\n    bindnetaddr_res = []\n    mcastaddr_res = []\n    mcastport_res = []\n    default_ports = [\"5405\", \"5407\"]\n    two_rings = False\n\n    for i in range(2):\n        bindnetaddr = prompt_for_string(\n                'IP or network address to bind to',\n                default=pick_default_value(_context.default_ip_list, bindnetaddr_res),\n                valid_func=Validation.valid_mcast_ip,\n                prev_value=bindnetaddr_res)\n        if not bindnetaddr:\n            error(\"No value for bindnetaddr\")\n        bindnetaddr_res.append(bindnetaddr)\n\n        mcastaddr = prompt_for_string(\n                'Multicast address',\n                default=gen_mcastaddr(),\n                valid_func=Validation.valid_mcast_address,\n                prev_value=mcastaddr_res)\n        if not mcastaddr:\n            error(\"No value for mcastaddr\")\n        mcastaddr_res.append(mcastaddr)\n\n        mcastport = prompt_for_string(\n                'Multicast port',\n                match='[0-9]+',\n                default=pick_default_value(default_ports, mcastport_res),\n                valid_func=Validation.valid_port,\n                prev_value=mcastport_res)\n        if not mcastport:\n            error(\"No value for mcastport\")\n        mcastport_res.append(mcastport)\n\n        if i == 1 or \\\n           not _context.second_heartbeat or \\\n           not confirm(\"\\nConfigure a second multicast ring?\"):\n            break\n        two_rings = True\n\n    nodeid = None\n    if _context.ipv6:\n        nodeid = utils.gen_nodeid_from_ipv6(_context.default_ip_list[0])\n\n    corosync.create_configuration(\n        clustername=_context.cluster_name,\n        bindnetaddr=bindnetaddr_res,\n        mcastaddr=mcastaddr_res,\n        mcastport=mcastport_res,\n        ipv6=_context.ipv6,\n        nodeid=nodeid,\n        two_rings=two_rings)\n    csync2_update(corosync.conf())\n\n\ndef init_corosync():\n    \"\"\"\n    Configure corosync (unicast or multicast, encrypted?)\n    \"\"\"\n    def requires_unicast():\n        host = utils.detect_cloud()\n        if host is not None:\n            status(\"Detected cloud platform: {}\".format(host))\n        return host is not None\n\n    init_corosync_auth()\n\n    if os.path.exists(corosync.conf()):\n        if not confirm(\"%s already exists - overwrite?\" % (corosync.conf())):\n            return\n\n    if _context.unicast or requires_unicast():\n        init_corosync_unicast()\n    else:\n        init_corosync_multicast()\n\n\ndef is_block_device(dev):\n    from stat import S_ISBLK\n    try:\n        rc = S_ISBLK(os.stat(dev).st_mode)\n    except OSError:\n        return False\n    return rc\n\n\ndef list_partitions(dev):\n    rc, outp, errp = utils.get_stdout_stderr(\"parted -s %s print\" % (dev))\n    partitions = []\n    for line in outp.splitlines():\n        m = re.match(r\"^\\s*([0-9]+)\\s*\", line)\n        if m:\n            partitions.append(m.group(1))\n    if rc != 0:\n        # ignore \"Error: /dev/vdb: unrecognised disk label\"\n        if errp.count('\\n') > 1 or \"unrecognised disk label\" not in errp.strip():\n            error(\"Failed to list partitions in {}: {}\".format(dev, errp))\n    return partitions\n\n\ndef list_devices(dev):\n    \"TODO: THIS IS *WRONG* FOR MULTIPATH! (but possibly nothing we can do about it)\"\n    _rc, outp = utils.get_stdout(\"fdisk -l %s\" % (dev))\n    partitions = []\n    for line in outp.splitlines():\n        m = re.match(r\"^(\\/dev\\S+)\", line)\n        if m:\n            partitions.append(m.group(1))\n    return partitions\n\n\ndef init_storage():\n    \"\"\"\n    Configure SBD and OCFS2 both on the same storage device.\n    \"\"\"\n    dev = _context.shared_device\n    partitions = []\n    dev_looks_sane = False\n\n    if _context.yes_to_all or not dev:\n        status(\"Configuring shared storage\")\n    else:\n        status(\"\"\"\nConfigure Shared Storage:\n  You will need to provide the path to a shared storage device,\n  for example a SAN volume or iSCSI target.  The device path must\n  be persistent and consistent across all nodes in the cluster,\n  so /dev/disk/by-id/* devices are a good choice.  This device\n  will be automatically paritioned into two pieces, 1MB for SBD\n  fencing, and the remainder for an OCFS2 filesystem.\n\"\"\")\n\n    while not dev_looks_sane:\n        dev = prompt_for_string('Path to storage device (e.g. /dev/disk/by-id/...)', r'\\/.*', dev)\n        if not dev:\n            error(\"No value for shared storage device\")\n\n        if not is_block_device(dev):\n            if _context.yes_to_all:\n                error(dev + \" is not a block device\")\n            else:\n                print(\"    That doesn't look like a block device\", file=sys.stderr)\n        else:\n            #\n            # Got something that looks like a block device, there\n            # are four possibilities now:\n            #\n            #  1) It's completely broken/inaccessible\n            #  2) No recognizable partition table\n            #  3) Empty partition table\n            #  4) Non-empty parition table\n            #\n            partitions = list_partitions(dev)\n            if partitions:\n                status(\"WARNING: Partitions exist on %s!\" % (dev))\n                if confirm(\"Are you ABSOLUTELY SURE you want to overwrite?\"):\n                    dev_looks_sane = True\n                else:\n                    dev = \"\"\n            else:\n                # It's either broken, no partition table, or empty partition table\n                status(\"%s appears to be empty\" % (dev))\n                if confirm(\"Device appears empty (no partition table). Do you want to use {}?\".format(dev)):\n                    dev_looks_sane = True\n                else:\n                    dev = \"\"\n\n    if partitions:\n        if not confirm(\"Really?\"):\n            return\n        status_long(\"Erasing existing partitions...\")\n        for part in partitions:\n            if not invokerc(\"parted -s %s rm %s\" % (dev, part)):\n                error(\"Failed to remove partition %s from %s\" % (part, dev))\n        status_done()\n\n    status_long(\"Creating partitions...\")\n    if not invokerc(\"parted\", \"-s\", dev, \"mklabel\", \"msdos\"):\n        error(\"Failed to create partition table\")\n\n    # This is a bit rough, and probably won't result in great performance,\n    # but it's fine for test/demo purposes to carve off 1MB for SBD.  Note\n    # we have to specify the size of the first partition in this in bytes\n    # rather than MB, or parted's rounding gives us a ~30Kb partition\n    # (see rhbz#623268).\n    if not invokerc(\"parted -s %s mkpart primary 0 1048576B\" % (dev)):\n        error(\"Failed to create first partition on %s\" % (dev))\n    if not invokerc(\"parted -s %s mkpart primary 1M 100%%\" % (dev)):\n        error(\"Failed to create second partition\")\n\n    status_done()\n\n    # TODO: May not be strictly necessary, but...\n    probe_partitions()\n\n    # TODO: THIS IS *WRONG* FOR MULTIPATH! (but possibly nothing we can do about it)\n    devices = list_devices(dev)\n\n    _context.sbd_device = devices[0]\n    if not _context.sbd_device:\n        error(\"Unable to determine device path for SBD partition\")\n\n    _context.ocfs2_device = devices[1]\n    if not _context.ocfs2_device:\n        error(\"Unable to determine device path for OCFS2 partition\")\n\n    status(\"Created %s for SBD partition\" % (_context.sbd_device))\n    status(\"Created %s for OCFS2 partition\" % (_context.ocfs2_device))\n\n\ndef init_sbd():\n    \"\"\"\n    Configure SBD (Storage-based fencing).\n\n    SBD can also run in diskless mode if no device\n    is configured.\n    \"\"\"\n    _context.sbd_manager.sbd_init()\n\n\ndef init_cluster():\n    \"\"\"\n    Initial cluster configuration.\n    \"\"\"\n    init_cluster_local()\n\n    _rc, nnodes = utils.get_stdout(\"crm_node -l\")\n    nnodes = len(nnodes.splitlines())\n    if nnodes < 1:\n        error(\"No nodes found in cluster\")\n    if nnodes > 1:\n        error(\"Joined existing cluster - will not reconfigure.\")\n\n    status(\"Loading initial cluster configuration\")\n\n    crm_configure_load(\"update\", \"\"\"\nproperty cib-bootstrap-options: stonith-enabled=false\nop_defaults op-options: timeout=600 record-pending=true\nrsc_defaults rsc-options: resource-stickiness=1 migration-threshold=3\n\"\"\")\n\n    _context.sbd_manager.configure_sbd_resource()\n\n\ndef init_vgfs():\n    \"\"\"\n    Configure cluster OCFS2 device.\n    \"\"\"\n    dev = _context.ocfs2_device\n    if not dev:\n        error(\"vgfs stage requires -o <dev>\")\n    mntpoint = \"/srv/clusterfs\"\n\n    if not is_block_device(dev):\n        error(\"OCFS2 device \\\"{}\\\" does not exist\".format(dev))\n\n    # TODO: configurable mountpoint and vg name\n    crm_configure_load(\"update\", \"\"\"\nprimitive dlm ocf:pacemaker:controld op start timeout=90 op stop timeout=100 op monitor interval=60 timeout=60\nprimitive clusterfs Filesystem directory=%(mntpoint)s fstype=ocfs2 device=%(dev)s \\\n    op monitor interval=20 timeout=40 op start timeout=60 op stop timeout=60 \\\n    meta target-role=Stopped\nclone base-clone dlm meta interleave=true\nclone c-clusterfs clusterfs meta interleave=true clone-max=8\norder base-then-clusterfs inf: base-clone c-clusterfs\ncolocation clusterfs-with-base inf: c-clusterfs base-clone\n    \"\"\" % {\"mntpoint\": utils.doublequote(mntpoint), \"dev\": utils.doublequote(dev)})\n\n    wait_for_resource(\"Waiting for DLM\", \"dlm:0\")\n    wait_for_stop(\"Making sure filesystem is not active\", \"clusterfs:0\")\n\n    _rc, blkid, _err = utils.get_stdout_stderr(\"blkid %s\" % (dev))\n    if \"TYPE\" in blkid:\n        if not confirm(\"Exiting filesystem found on \\\"{}\\\" - destroy?\".format(dev)):\n            for res in (\"base-clone\", \"c-clusterfs\"):\n                invoke(\"crm resource stop %s\" % (res))\n                wait_for_stop(\"Waiting for resource %s to stop\" % (res), res)\n            invoke(\"crm configure delete dlm clusterfs base-group base-clone c-clusterfs base-then-clusterfs clusterfs-with-base\")\n\n    status_long(\"Creating OCFS2 filesystem\")\n    # TODO: want \"-T vmstore\", but this'll only fly on >2GB partition\n    # Note: using undocumented '-x' switch to avoid prompting if overwriting\n    # existing partition.  For the commit that introduced this, see:\n    # http://oss.oracle.com/git/?p=ocfs2-tools.git;a=commit;h=8345a068479196172190f4fa287052800fa2b66f\n    # TODO: if make the cluster name configurable, we need to update it here too\n    if not invokerc(\"mkfs.ocfs2 --cluster-stack pcmk --cluster-name %s -N 8 -x %s\" % (_context.cluster_name, dev)):\n        error(\"Failed to create OCFS2 filesystem on %s\" % (dev))\n    status_done()\n\n    # TODO: refactor, maybe\n    if not invokerc(\"mkdir -p %s\" % (mntpoint)):\n        error(\"Can't create mountpoint %s\" % (mntpoint))\n    if not invokerc(\"crm resource meta clusterfs delete target-role\"):\n        error(\"Can't start cluster filesystem clone\")\n    wait_for_resource(\"Waiting for %s to be mounted\" % (mntpoint), \"clusterfs:0\")\n\n\ndef init_admin():\n    # Skip this section when -y is passed\n    # unless $ADMIN_IP is set\n    adminaddr = _context.admin_ip\n    if _context.yes_to_all and not adminaddr:\n        return\n\n    if not adminaddr:\n        status(\"\"\"\nConfigure Administration IP Address:\n  Optionally configure an administration virtual IP\n  address. The purpose of this IP address is to\n  provide a single IP that can be used to interact\n  with the cluster, rather than using the IP address\n  of any specific cluster node.\n\"\"\")\n        if not confirm(\"Do you wish to configure a virtual IP address?\"):\n            return\n\n        adminaddr = prompt_for_string('Virtual IP', valid_func=Validation.valid_admin_ip)\n        if not adminaddr:\n            error(\"Expected an IP address\")\n\n    crm_configure_load(\"update\", 'primitive admin-ip IPaddr2 ip=%s op monitor interval=10 timeout=20' % (utils.doublequote(adminaddr)))\n    wait_for_resource(\"Configuring virtual IP ({})\".format(adminaddr), \"admin-ip\")\n\n\ndef init_qdevice():\n    \"\"\"\n    Setup qdevice and qnetd service\n    \"\"\"\n    # If don't want to config qdevice, return\n    if not _context.qdevice_inst:\n        utils.disable_service(\"corosync-qdevice.service\")\n        return\n\n    status(\"\"\"\nConfigure Qdevice/Qnetd:\"\"\")\n    qdevice_inst = _context.qdevice_inst\n    qnetd_addr = qdevice_inst.qnetd_addr\n    # Configure ssh passwordless to qnetd if detect password is needed\n    if utils.check_ssh_passwd_need(qnetd_addr):\n        status(\"Copy ssh key to qnetd node({})\".format(qnetd_addr))\n        rc, _, err = invoke(\"ssh-copy-id -i /root/.ssh/id_rsa.pub root@{}\".format(qnetd_addr))\n        if not rc:\n            error(\"Failed to copy ssh key: {}\".format(err))\n    # Start qdevice service if qdevice already configured\n    if utils.is_qdevice_configured() and not confirm(\"Qdevice is already configured - overwrite?\"):\n        start_qdevice_service()\n        return\n\n    # Validate qnetd node\n    qdevice_inst.valid_qnetd()\n    # Config qdevice\n    config_qdevice()\n    # Execute certificate process when tls flag is on\n    if utils.is_qdevice_tls_on():\n        status_long(\"Qdevice certification process\")\n        qdevice_inst.certificate_process_on_init()\n        status_done()\n\n    start_qdevice_service()\n\n\ndef start_qdevice_service():\n    \"\"\"\n    Start qdevice and qnetd service\n    \"\"\"\n    qdevice_inst = _context.qdevice_inst\n    qnetd_addr = qdevice_inst.qnetd_addr\n\n    status(\"Enable corosync-qdevice.service in cluster\")\n    utils.cluster_run_cmd(\"systemctl enable corosync-qdevice\")\n    status(\"Starting corosync-qdevice.service in cluster\")\n    utils.cluster_run_cmd(\"systemctl start corosync-qdevice\")\n\n    status(\"Enable corosync-qnetd.service on {}\".format(qnetd_addr))\n    qdevice_inst.enable_qnetd()\n    status(\"Starting corosync-qnetd.service on {}\".format(qnetd_addr))\n    qdevice_inst.start_qnetd()\n\n\ndef config_qdevice():\n    \"\"\"\n    Process of config qdevice\n    \"\"\"\n    qdevice_inst = _context.qdevice_inst\n\n    qdevice_inst.remove_qdevice_db()\n    qdevice_inst.write_qdevice_config()\n    if not corosync.is_unicast():\n        corosync.add_nodelist_from_cmaptool()\n    status_long(\"Update configuration\")\n    update_expected_votes()\n    utils.cluster_run_cmd(\"crm corosync reload\")\n    status_done()\n\n\ndef init():\n    \"\"\"\n    Basic init\n    \"\"\"\n    log_start()\n    init_network()\n\n\ndef join_ssh(seed_host):\n    \"\"\"\n    SSH configuration for joining node.\n    \"\"\"\n    if not seed_host:\n        error(\"No existing IP/hostname specified (use -c option)\")\n\n    utils.start_service(\"sshd.service\", enable=True)\n    for user in USER_LIST:\n        configure_local_ssh_key(user)\n        swap_public_ssh_key(seed_host, user)\n\n    # This makes sure the seed host has its own SSH keys in its own\n    # authorized_keys file (again, to help with the case where the\n    # user has done manual initial setup without the assistance of\n    # ha-cluster-init).\n    rc, _, err = invoke(\"ssh root@{} crm cluster init -i {} ssh_remote\".format(seed_host, _context.default_nic_list[0]))\n    if not rc:\n        error(\"Can't invoke crm cluster init -i {} ssh_remote on {}: {}\".format(_context.default_nic_list[0], seed_host, err))\n\n\ndef swap_public_ssh_key(remote_node, user=\"root\"):\n    \"\"\"\n    Swap public ssh key between remote_node and local\n    \"\"\"\n    if user != \"root\" and not _context.with_other_user:\n        return\n\n    _, public_key, authorized_file = key_files(user).values()\n    # Detect whether need password to login to remote_node\n    if utils.check_ssh_passwd_need(remote_node, user):\n        # If no passwordless configured, paste /root/.ssh/id_rsa.pub to remote_node's /root/.ssh/authorized_keys\n        status(\"Configuring SSH passwordless with {}@{}\".format(user, remote_node))\n        # After this, login to remote_node is passwordless\n        append_to_remote_file(public_key, remote_node, authorized_file)\n\n    try:\n        # Fetch public key file from remote_node\n        public_key_file_remote = fetch_public_key_from_remote_node(remote_node, user)\n    except ValueError as err:\n        warn(err)\n        return\n    # Append public key file from remote_node to local's /root/.ssh/authorized_keys\n    # After this, login from remote_node is passwordless\n    # Should do this step even passwordless is True, to make sure we got two-way passwordless\n    append_unique(public_key_file_remote, authorized_file)\n\n\ndef fetch_public_key_from_remote_node(node, user=\"root\"):\n    \"\"\"\n    Fetch public key file from remote node\n    Return a temp file contains public key\n    Return None if no key exist\n    \"\"\"\n\n    # For dsa, might need to add PubkeyAcceptedKeyTypes=+ssh-dss to config file, see\n    # https://superuser.com/questions/1016989/ssh-dsa-keys-no-longer-work-for-password-less-authentication\n    home_dir = userdir.gethomedir(user)\n    for key in (\"id_rsa\", \"id_ecdsa\", \"id_ed25519\", \"id_dsa\"):\n        public_key_file = \"{}/.ssh/{}.pub\".format(home_dir, key)\n        cmd = \"ssh -oStrictHostKeyChecking=no root@{} 'test -f {}'\".format(node, public_key_file)\n        if not invokerc(cmd):\n            continue\n        _, temp_public_key_file = tmpfiles.create()\n        cmd = \"scp -oStrictHostKeyChecking=no root@{}:{} {}\".format(node, public_key_file, temp_public_key_file)\n        rc, _, err = invoke(cmd)\n        if not rc:\n            error(\"Failed to run \\\"{}\\\": {}\".format(cmd, err))\n        return temp_public_key_file\n    raise ValueError(\"No ssh key exist on {}\".format(node))\n\n\ndef join_csync2(seed_host):\n    \"\"\"\n    Csync2 configuration for joining node.\n    \"\"\"\n    if not seed_host:\n        error(\"No existing IP/hostname specified (use -c option)\")\n    status_long(\"Configuring csync2\")\n\n    # Necessary if re-running join on a node that's been configured before.\n    rmfile(\"/var/lib/csync2/{}.db3\".format(utils.this_node()), ignore_errors=True)\n\n    # Not automatically updating /etc/hosts - risky in the general case.\n    # etc_hosts_add_me\n    # local hosts_line=$(etc_hosts_get_me)\n    # [ -n \"$hosts_line\" ] || error \"No valid entry for $(hostname) in /etc/hosts - csync2 can't work\"\n\n    # If we *were* updating /etc/hosts, the next line would have \"\\\"$hosts_line\\\"\" as\n    # the last arg (but this requires re-enabling this functionality in ha-cluster-init)\n    cmd = \"crm cluster init -i {} csync2_remote {}\".format(_context.default_nic_list[0], utils.this_node())\n    rc, _, err = invoke(\"ssh -o StrictHostKeyChecking=no root@{} {}\".format(seed_host, cmd))\n    if not rc:\n        error(\"Can't invoke \\\"{}\\\" on {}: {}\".format(cmd, seed_host, err))\n\n    # This is necessary if syncing /etc/hosts (to ensure everyone's got the\n    # same list of hosts)\n    # local tmp_conf=/etc/hosts.$$\n    # invoke scp root@seed_host:/etc/hosts $tmp_conf \\\n    #   || error \"Can't retrieve /etc/hosts from seed_host\"\n    # install_tmp $tmp_conf /etc/hosts\n    rc, _, err = invoke(\"scp root@%s:'/etc/csync2/{csync2.cfg,key_hagroup}' /etc/csync2\" % (seed_host))\n    if not rc:\n        error(\"Can't retrieve csync2 config from {}: {}\".format(seed_host, err))\n\n    utils.start_service(\"csync2.socket\", enable=True)\n\n    # Sync new config out.  This goes to all hosts; csync2.cfg definitely\n    # needs to go to all hosts (else hosts other than the seed and the\n    # joining host won't have the joining host in their config yet).\n    # Strictly, the rest of the files need only go to the new host which\n    # could theoretically be effected using `csync2 -xv -P $(hostname)`,\n    # but this still leaves all the other files in dirty state (becuase\n    # they haven't gone to all nodes in the cluster, which means a\n    # subseqent join of another node can fail its sync of corosync.conf\n    # when it updates expected_votes.  Grrr...\n    if not invokerc('ssh -o StrictHostKeyChecking=no root@{} \"csync2 -rm /; csync2 -rxv || csync2 -rf / && csync2 -rxv\"'.format(seed_host)):\n        print(\"\")\n        warn(\"csync2 run failed - some files may not be sync'd\")\n\n    status_done()\n\n\ndef join_ssh_merge(_cluster_node):\n    status(\"Merging known_hosts\")\n\n    me = utils.this_node()\n    hosts = [m.group(1)\n             for m in re.finditer(r\"^\\s*host\\s*([^ ;]+)\\s*;\", open(CSYNC2_CFG).read(), re.M)\n             if m.group(1) != me]\n    if not hosts:\n        hosts = [_cluster_node]\n        warn(\"Unable to extract host list from %s\" % (CSYNC2_CFG))\n\n    try:\n        import parallax\n    except ImportError:\n        error(\"parallax python library is missing\")\n\n    opts = parallax.Options()\n    opts.ssh_options = ['StrictHostKeyChecking=no']\n\n    # The act of using pssh to connect to every host (without strict host key\n    # checking) ensures that at least *this* host has every other host in its\n    # known_hosts\n    known_hosts_new = set()\n    cat_cmd = \"[ -e /root/.ssh/known_hosts ] && cat /root/.ssh/known_hosts || true\"\n    log(\"parallax.call {} : {}\".format(hosts, cat_cmd))\n    results = parallax.call(hosts, cat_cmd, opts)\n    for host, result in results.items():\n        if isinstance(result, parallax.Error):\n            warn(\"Failed to get known_hosts from {}: {}\".format(host, str(result)))\n        else:\n            if result[1]:\n                known_hosts_new.update((utils.to_ascii(result[1]) or \"\").splitlines())\n    if known_hosts_new:\n        hoststxt = \"\\n\".join(sorted(known_hosts_new))\n        tmpf = utils.str2tmp(hoststxt)\n        log(\"parallax.copy {} : {}\".format(hosts, hoststxt))\n        results = parallax.copy(hosts, tmpf, \"/root/.ssh/known_hosts\")\n        for host, result in results.items():\n            if isinstance(result, parallax.Error):\n                warn(\"scp to {} failed ({}), known_hosts update may be incomplete\".format(host, str(result)))\n\n\ndef update_expected_votes():\n    # get a list of nodes, excluding remote nodes\n    nodelist = None\n    loop_count = 0\n    device_votes = 0\n    nodecount = 0\n    expected_votes = 0\n    while True:\n        rc, nodelist_text = utils.get_stdout(\"cibadmin -Ql --xpath '/cib/status/node_state'\")\n        if rc == 0:\n            try:\n                nodelist_xml = etree.fromstring(nodelist_text)\n                nodelist = [n.get('uname') for n in nodelist_xml.xpath('//node_state') if n.get('remote_node') != 'true']\n                if len(nodelist) >= 2:\n                    break\n            except Exception:\n                break\n        # timeout: 10 seconds\n        if loop_count == 10:\n            break\n        loop_count += 1\n        sleep(1)\n\n    # Increase expected_votes\n    # TODO: wait to adjust expected_votes until after cluster join,\n    # so that we can ask the cluster for the current membership list\n    # Have to check if a qnetd device is configured and increase\n    # expected_votes in that case\n    is_qdevice_configured = utils.is_qdevice_configured()\n    if nodelist is None:\n        for v in corosync.get_values(\"quorum.expected_votes\"):\n            expected_votes = v\n\n            # For node >= 2, expected_votes = nodecount + device_votes\n            # Assume nodecount is N, for ffsplit, qdevice only has one vote\n            # which means that device_votes is 1, ie:expected_votes = N + 1;\n            # while for lms, qdevice has N - 1 votes, ie: expected_votes = N + (N - 1)\n            # and update quorum.device.net.algorithm based on device_votes\n\n            if corosync.get_value(\"quorum.device.net.algorithm\") == \"lms\":\n                device_votes = int((expected_votes - 1) / 2)\n                nodecount = expected_votes - device_votes\n                # as nodecount will increase 1, and device_votes is nodecount - 1\n                # device_votes also increase 1\n                device_votes += 1\n            elif corosync.get_value(\"quorum.device.net.algorithm\") == \"ffsplit\":\n                device_votes = 1\n                nodecount = expected_votes - device_votes\n            elif is_qdevice_configured:\n                device_votes = 0\n                nodecount = v\n\n            nodecount += 1\n            expected_votes = nodecount + device_votes\n            corosync.set_value(\"quorum.expected_votes\", str(expected_votes))\n    else:\n        nodecount = len(nodelist)\n        expected_votes = 0\n        # For node >= 2, expected_votes = nodecount + device_votes\n        # Assume nodecount is N, for ffsplit, qdevice only has one vote\n        # which means that device_votes is 1, ie:expected_votes = N + 1;\n        # while for lms, qdevice has N - 1 votes, ie: expected_votes = N + (N - 1)\n        if corosync.get_value(\"quorum.device.net.algorithm\") == \"ffsplit\":\n            device_votes = 1\n        if corosync.get_value(\"quorum.device.net.algorithm\") == \"lms\":\n            device_votes = nodecount - 1\n\n        if nodecount > 1:\n            expected_votes = nodecount + device_votes\n\n        if corosync.get_value(\"quorum.expected_votes\"):\n            corosync.set_value(\"quorum.expected_votes\", str(expected_votes))\n    if is_qdevice_configured:\n        corosync.set_value(\"quorum.device.votes\", device_votes)\n    corosync.set_value(\"quorum.two_node\", 1 if expected_votes == 2 else 0)\n\n    csync2_update(corosync.conf())\n\n\ndef setup_passwordless_with_other_nodes(init_node):\n    \"\"\"\n    Setup passwordless with other cluster nodes\n\n    Should fetch the node list from init node, then swap the key\n    \"\"\"\n    # Fetch cluster nodes list\n    cmd = \"ssh -o StrictHostKeyChecking=no root@{} crm_node -l\".format(init_node)\n    rc, out, err = utils.get_stdout_stderr(cmd)\n    if rc != 0:\n        error(\"Can't fetch cluster nodes list from {}: {}\".format(init_node, err))\n    cluster_nodes_list = []\n    for line in out.splitlines():\n        _, node, stat = line.split()\n        if stat == \"member\":\n            cluster_nodes_list.append(node)\n\n    # Filter out init node from cluster_nodes_list\n    cmd = \"ssh -o StrictHostKeyChecking=no root@{} hostname\".format(init_node)\n    rc, out, err = utils.get_stdout_stderr(cmd)\n    if rc != 0:\n        error(\"Can't fetch hostname of {}: {}\".format(init_node, err))\n    if out in cluster_nodes_list:\n        cluster_nodes_list.remove(out)\n\n    # Swap ssh public key between join node and other cluster nodes\n    for node in cluster_nodes_list:\n        for user in USER_LIST:\n            swap_public_ssh_key(node, user)\n\n\ndef join_cluster(seed_host):\n    \"\"\"\n    Cluster configuration for joining node.\n    \"\"\"\n    def get_local_nodeid():\n        # for IPv6\n        return utils.gen_nodeid_from_ipv6(_context.local_ip_list[0])\n\n    def update_nodeid(nodeid, node=None):\n        # for IPv6\n        if node and node != utils.this_node():\n            cmd = \"crm corosync set totem.nodeid %d\" % nodeid\n            invoke(\"crm cluster run '{}' {}\".format(cmd, node))\n        else:\n            corosync.set_value(\"totem.nodeid\", nodeid)\n\n    shutil.copy(corosync.conf(), COROSYNC_CONF_ORIG)\n\n    # check if use IPv6\n    ipv6_flag = False\n    ipv6 = corosync.get_value(\"totem.ip_version\")\n    if ipv6 and ipv6 == \"ipv6\":\n        ipv6_flag = True\n    _context.ipv6 = ipv6_flag\n\n    init_network()\n\n    # check whether have two rings\n    rrp_flag = False\n    rrp = corosync.get_value(\"totem.rrp_mode\")\n    if rrp in ('active', 'passive'):\n        rrp_flag = True\n\n    # Need to do this if second (or subsequent) node happens to be up and\n    # connected to storage while it's being repartitioned on the first node.\n    probe_partitions()\n\n    # It would be massively useful at this point if new nodes could come\n    # up in standby mode, so we could query the CIB locally to see if\n    # there was any further local setup that needed doing, e.g.: creating\n    # mountpoints for clustered filesystems.  Unfortunately we don't have\n    # that yet, so the following crawling horror takes a punt on the seed\n    # node being up, then asks it for a list of mountpoints...\n    if _context.cluster_node:\n        _rc, outp, _ = utils.get_stdout_stderr(\"ssh -o StrictHostKeyChecking=no root@{} 'cibadmin -Q --xpath \\\"//primitive\\\"'\".format(seed_host))\n        if outp:\n            xml = etree.fromstring(outp)\n            mountpoints = xml.xpath(' and '.join(['//primitive[@class=\"ocf\"',\n                                                  '@provider=\"heartbeat\"',\n                                                  '@type=\"Filesystem\"]']) +\n                                    '/instance_attributes/nvpair[@name=\"directory\"]/@value')\n            for m in mountpoints:\n                invoke(\"mkdir -p {}\".format(m))\n    else:\n        status(\"No existing IP/hostname specified - skipping mountpoint detection/creation\")\n\n    # Bump expected_votes in corosync.conf\n    # TODO(must): this is rather fragile (see related code in ha-cluster-remove)\n\n    # If corosync.conf() doesn't exist or is empty, we will fail here. (bsc#943227)\n    if not os.path.exists(corosync.conf()):\n        error(\"{} is not readable. Please ensure that hostnames are resolvable.\".format(corosync.conf()))\n\n    # if unicast, we need to add our node to $corosync.conf()\n    is_unicast = corosync.is_unicast()\n    if is_unicast:\n        ringXaddr_res = []\n        for i in 0, 1:\n            while True:\n                ringXaddr = prompt_for_string(\n                        'Address for ring{}'.format(i),\n                        default=pick_default_value(_context.default_ip_list, ringXaddr_res),\n                        valid_func=Validation.valid_ucast_ip,\n                        prev_value=ringXaddr_res)\n                if not ringXaddr:\n                    error(\"No value for ring{}\".format(i))\n                ringXaddr_res.append(ringXaddr)\n                break\n            if not rrp_flag:\n                break\n        print(\"\")\n        invoke(\"rm -f /var/lib/heartbeat/crm/* /var/lib/pacemaker/cib/*\")\n        try:\n            corosync.add_node_ucast(ringXaddr_res)\n        except corosync.IPAlreadyConfiguredError as e:\n            warn(e)\n        csync2_update(corosync.conf())\n        invoke(\"ssh -o StrictHostKeyChecking=no root@{} corosync-cfgtool -R\".format(seed_host))\n\n    _context.sbd_manager.join_sbd(seed_host)\n\n    if ipv6_flag and not is_unicast:\n        # for ipv6 mcast\n        # using ipv6 need nodeid configured\n        local_nodeid = get_local_nodeid()\n        update_nodeid(local_nodeid)\n\n    is_qdevice_configured = utils.is_qdevice_configured()\n    if is_qdevice_configured and not is_unicast:\n        # expected_votes here maybe is \"0\", set to \"3\" to make sure cluster can start\n        corosync.set_value(\"quorum.expected_votes\", \"3\")\n\n    # Initialize the cluster before adjusting quorum. This is so\n    # that we can query the cluster to find out how many nodes\n    # there are (so as not to adjust multiple times if a previous\n    # attempt to join the cluster failed)\n    init_cluster_local()\n\n    status_long(\"Reloading cluster configuration\")\n\n    if ipv6_flag and not is_unicast:\n        # for ipv6 mcast\n        nodeid_dict = {}\n        _rc, outp, _ = utils.get_stdout_stderr(\"crm_node -l\")\n        if _rc == 0:\n            for line in outp.split('\\n'):\n                tmp = line.split()\n                nodeid_dict[tmp[1]] = tmp[0]\n\n    # apply nodelist in cluster\n    if is_unicast or is_qdevice_configured:\n        invoke(\"crm cluster run 'crm corosync reload'\")\n\n    update_expected_votes()\n    # Trigger corosync config reload to ensure expected_votes is propagated\n    invoke(\"corosync-cfgtool -R\")\n\n    # Ditch no-quorum-policy=ignore\n    _rc, outp = utils.get_stdout(\"crm configure show\")\n    if re.search('no-quorum-policy=.*ignore', outp):\n        invoke(\"crm_attribute --attr-name no-quorum-policy --delete-attr\")\n\n    # if unicast, we need to reload the corosync configuration\n    # on the other nodes\n    if is_unicast:\n        invoke(\"crm cluster run 'crm corosync reload'\")\n\n    if ipv6_flag and not is_unicast:\n        # for ipv6 mcast\n        # after csync2_update, all config files are same\n        # but nodeid must be uniqe\n        for node in list(nodeid_dict.keys()):\n            if node == utils.this_node():\n                continue\n            update_nodeid(int(nodeid_dict[node]), node)\n        update_nodeid(local_nodeid)\n    status_done()\n\n    if is_qdevice_configured:\n        start_qdevice_on_join_node(seed_host)\n    else:\n        utils.disable_service(\"corosync-qdevice.service\")\n\n\ndef start_qdevice_on_join_node(seed_host):\n    \"\"\"\n    Doing qdevice certificate process and start qdevice service on join node\n    \"\"\"\n    status_long(\"Starting corosync-qdevice.service\")\n    if not corosync.is_unicast():\n        corosync.add_nodelist_from_cmaptool()\n        csync2_update(corosync.conf())\n        invoke(\"crm corosync reload\")\n    if utils.is_qdevice_tls_on():\n        qnetd_addr = corosync.get_value(\"quorum.device.net.host\")\n        qdevice_inst = corosync.QDevice(qnetd_addr, cluster_node=seed_host)\n        qdevice_inst.certificate_process_on_join()\n    utils.start_service(\"corosync-qdevice.service\", enable=True)\n    status_done()\n\n\ndef set_cluster_node_ip():\n    \"\"\"\n    ringx_addr might be hostname or IP\n    _context.cluster_node by now is always hostname\n\n    If ring0_addr is IP, we should get the configured iplist which belong _context.cluster_node\n    Then filter out which one is configured as ring0_addr\n    At last assign that ip to _context.cluster_node_ip which will be removed later\n    \"\"\"\n    node = _context.cluster_node\n    addr_list = corosync.get_values('nodelist.node.ring0_addr')\n    if node in addr_list:\n        return\n\n    ip_list = utils.get_iplist_from_name(node)\n    for ip in ip_list:\n        if ip in addr_list:\n            _context.cluster_node_ip = ip\n            break\n\n\ndef stop_services(stop_list, remote_addr=None):\n    \"\"\"\n    Stop cluster related service\n    \"\"\"\n    for service in stop_list:\n        if utils.service_is_active(service, remote_addr=remote_addr):\n            status(\"Stopping the {}\".format(service))\n            utils.stop_service(service, disable=True, remote_addr=remote_addr)\n\n\ndef remove_node_from_cluster():\n    \"\"\"\n    Remove node from running cluster and the corosync / pacemaker configuration.\n    \"\"\"\n    node = _context.cluster_node\n    set_cluster_node_ip()\n\n    stop_services(SERVICES_STOP_LIST, remote_addr=node)\n\n    # delete configuration files from the node to be removed\n    rc, _, err = invoke('ssh -o StrictHostKeyChecking=no root@{} \"bash -c \\\\\\\"rm -f {}\\\\\\\"\"'.format(node, \" \".join(_context.rm_list)))\n    if not rc:\n        error(\"Deleting the configuration files failed: {}\".format(err))\n\n    # execute the command : crm node delete $HOSTNAME\n    status(\"Removing the node {}\".format(node))\n    if not invokerc(\"crm node delete {}\".format(node)):\n        error(\"Failed to remove {}\".format(node))\n\n    if not invokerc(\"sed -i /{}/d {}\".format(node, CSYNC2_CFG)):\n        error(\"Removing the node {} from {} failed\".format(node, CSYNC2_CFG))\n\n    # Remove node from nodelist\n    if corosync.get_values(\"nodelist.node.ring0_addr\"):\n        del_target = _context.cluster_node_ip or node\n        corosync.del_node(del_target)\n\n    decrease_expected_votes()\n\n    status(\"Propagating configuration changes across the remaining nodes\")\n    csync2_update(CSYNC2_CFG)\n    csync2_update(corosync.conf())\n\n    # Trigger corosync config reload to ensure expected_votes is propagated\n    invoke(\"corosync-cfgtool -R\")\n\n\ndef decrease_expected_votes():\n    '''\n    Decrement expected_votes in corosync.conf\n    '''\n    vote = corosync.get_value(\"quorum.expected_votes\")\n    if not vote:\n        return\n    quorum = int(vote)\n    new_quorum = quorum - 1\n    if utils.is_qdevice_configured():\n        new_nodecount = 0\n        device_votes = 0\n        nodecount = 0\n\n        if corosync.get_value(\"quorum.device.net.algorithm\") == \"lms\":\n            nodecount = int((quorum + 1)/2)\n            new_nodecount = nodecount - 1\n            device_votes = new_nodecount - 1\n\n        elif corosync.get_value(\"quorum.device.net.algorithm\") == \"ffsplit\":\n            device_votes = 1\n            nodecount = quorum - device_votes\n            new_nodecount = nodecount - 1\n\n        if new_nodecount > 1:\n            new_quorum = new_nodecount + device_votes\n        else:\n            new_quorum = 0\n\n        corosync.set_value(\"quorum.device.votes\", device_votes)\n    else:\n        corosync.set_value(\"quorum.two_node\", 1 if new_quorum == 2 else 0)\n    corosync.set_value(\"quorum.expected_votes\", str(new_quorum))\n\n\ndef bootstrap_init(context):\n    \"\"\"\n    Init cluster process\n    \"\"\"\n    global _context\n    _context = context\n\n    init()\n    _context.initialize_qdevice()\n    _context.validate_option()\n    _context.init_sbd_manager()\n\n    stage = _context.stage\n    if stage is None:\n        stage = \"\"\n\n    # vgfs stage requires running cluster, everything else requires inactive cluster,\n    # except ssh and csync2 (which don't care) and csync2_remote (which mustn't care,\n    # just in case this breaks ha-cluster-join on another node).\n    corosync_active = utils.service_is_active(\"corosync.service\")\n    if stage in (\"vgfs\", \"admin\", \"qdevice\"):\n        if not corosync_active:\n            error(\"Cluster is inactive - can't run %s stage\" % (stage))\n    elif stage == \"\":\n        if corosync_active:\n            error(\"Cluster is currently active - can't run\")\n    elif stage not in (\"ssh\", \"ssh_remote\", \"csync2\", \"csync2_remote\"):\n        if corosync_active:\n            error(\"Cluster is currently active - can't run %s stage\" % (stage))\n\n    # Need hostname resolution to work, want NTP (but don't block ssh_remote or csync2_remote)\n    if stage not in ('ssh_remote', 'csync2_remote'):\n        check_tty()\n        if not check_prereqs(stage):\n            return\n    elif stage == 'csync2_remote':\n        args = _context.args\n        log(\"args: {}\".format(args))\n        if len(args) != 2:\n            error(\"Expected NODE argument to csync2_remote\")\n        _context.cluster_node = args[1]\n\n    if stage != \"\":\n        globals()[\"init_\" + stage]()\n    else:\n        init_ssh()\n        init_csync2()\n        init_corosync()\n        init_remote_auth()\n        if _context.template == 'ocfs2':\n            if _context.sbd_device is None or _context.ocfs2_device is None:\n                init_storage()\n        init_sbd()\n\n        lock_inst = lock.Lock()\n        try:\n            with lock_inst.lock():\n                init_cluster()\n                if _context.template == 'ocfs2':\n                    init_vgfs()\n                init_admin()\n                init_qdevice()\n        except lock.ClaimLockError as err:\n            error(err)\n\n    status(\"Done (log saved to %s)\" % (LOG_FILE))\n\n\ndef bootstrap_join(context):\n    \"\"\"\n    Join cluster process\n    \"\"\"\n    global _context\n    _context = context\n\n    init()\n    _context.init_sbd_manager()\n    _context.validate_option()\n\n    check_tty()\n\n    corosync_active = utils.service_is_active(\"corosync.service\")\n    if corosync_active and _context.stage != \"ssh\":\n        error(\"Abort: Cluster is currently active. Run this command on a node joining the cluster.\")\n\n    if not check_prereqs(\"join\"):\n        return\n\n    cluster_node = _context.cluster_node\n    if _context.stage != \"\":\n        globals()[\"join_\" + _context.stage](cluster_node)\n    else:\n        if not _context.yes_to_all and cluster_node is None:\n            status(\"\"\"Join This Node to Cluster:\n  You will be asked for the IP address of an existing node, from which\n  configuration will be copied.  If you have not already configured\n  passwordless ssh between nodes, you will be prompted for the root\n  password of the existing node.\n\"\"\")\n            cluster_node = prompt_for_string(\"IP address or hostname of existing node (e.g.: 192.168.1.1)\", \".+\")\n            _context.cluster_node = cluster_node\n\n        utils.ping_node(cluster_node)\n\n        join_ssh(cluster_node)\n\n        if not utils.service_is_active(\"pacemaker.service\", cluster_node):\n            error(\"Cluster is inactive on {}\".format(cluster_node))\n\n        lock_inst = lock.RemoteLock(cluster_node)\n        try:\n            with lock_inst.lock():\n                setup_passwordless_with_other_nodes(cluster_node)\n                join_remote_auth(cluster_node)\n                join_csync2(cluster_node)\n                join_ssh_merge(cluster_node)\n                join_cluster(cluster_node)\n        except (lock.SSHError, lock.ClaimLockError) as err:\n            error(err)\n\n    status(\"Done (log saved to %s)\" % (LOG_FILE))\n\n\ndef join_remote_auth(node):\n    if os.path.exists(PCMK_REMOTE_AUTH):\n        rmfile(PCMK_REMOTE_AUTH)\n    pcmk_remote_dir = os.path.dirname(PCMK_REMOTE_AUTH)\n    mkdirs_owned(pcmk_remote_dir, mode=0o750, gid=\"haclient\")\n    invoke(\"touch {}\".format(PCMK_REMOTE_AUTH))\n\n\ndef remove_qdevice():\n    \"\"\"\n    Remove qdevice service and configuration from cluster\n    \"\"\"\n    if not utils.is_qdevice_configured():\n        error(\"No QDevice configuration in this cluster\")\n    if not confirm(\"Removing QDevice service and configuration from cluster: Are you sure?\"):\n        return\n\n    status(\"Disable corosync-qdevice.service\")\n    invoke(\"crm cluster run 'systemctl disable corosync-qdevice'\")\n    status(\"Stopping corosync-qdevice.service\")\n    invoke(\"crm cluster run 'systemctl stop corosync-qdevice'\")\n\n    status_long(\"Removing QDevice configuration from cluster\")\n    qnetd_host = corosync.get_value('quorum.device.net.host')\n    qdevice_inst = corosync.QDevice(qnetd_host)\n    qdevice_inst.remove_qdevice_config()\n    qdevice_inst.remove_qdevice_db()\n    update_expected_votes()\n    invoke(\"crm cluster run 'crm corosync reload'\")\n    status_done()\n\n\ndef bootstrap_remove(context):\n    \"\"\"\n    Remove node from cluster, or remove qdevice configuration\n    \"\"\"\n    global _context\n    _context = context\n    force_flag = config.core.force or _context.force\n\n    init()\n\n    if not utils.service_is_active(\"corosync.service\"):\n        error(\"Cluster is not active - can't execute removing action\")\n\n    if _context.qdevice_rm_flag and _context.cluster_node:\n        error(\"Either remove node or qdevice\")\n\n    if _context.qdevice_rm_flag:\n        remove_qdevice()\n        return\n\n    if not _context.yes_to_all and _context.cluster_node is None:\n        status(\"\"\"Remove This Node from Cluster:\n  You will be asked for the IP address or name of an existing node,\n  which will be removed from the cluster. This command must be\n  executed from a different node in the cluster.\n\"\"\")\n        _context.cluster_node = prompt_for_string(\"IP address or hostname of cluster node (e.g.: 192.168.1.1)\", \".+\")\n\n    if not _context.cluster_node:\n        error(\"No existing IP/hostname specified (use -c option)\")\n\n    _context.cluster_node = get_cluster_node_hostname()\n\n    if not force_flag and not confirm(\"Removing node \\\"{}\\\" from the cluster: Are you sure?\".format(_context.cluster_node)):\n        return\n\n    if _context.cluster_node == utils.this_node():\n        if not force_flag:\n            error(\"Removing self requires --force\")\n        remove_self()\n        return\n\n    if _context.cluster_node in xmlutil.listnodes():\n        remove_node_from_cluster()\n    else:\n        error(\"Specified node {} is not configured in cluster! Unable to remove.\".format(_context.cluster_node))\n\n\ndef remove_self():\n    me = _context.cluster_node\n    yes_to_all = _context.yes_to_all\n    nodes = xmlutil.listnodes(include_remote_nodes=False)\n    othernode = next((x for x in nodes if x != me), None)\n    if othernode is not None:\n        # remove from other node\n        cmd = \"crm cluster remove{} -c {}\".format(\" -y\" if yes_to_all else \"\", me)\n        rc = utils.ext_cmd_nosudo(\"ssh{} -o StrictHostKeyChecking=no {} '{}'\".format(\"\" if yes_to_all else \" -t\", othernode, cmd))\n        if rc != 0:\n            error(\"Failed to remove this node from {}\".format(othernode))\n    else:\n        # disable and stop cluster\n        stop_services(SERVICES_STOP_LIST)\n        # remove all trace of cluster from this node\n        # delete configuration files from the node to be removed\n        if not invokerc('bash -c \"rm -f {}\"'.format(\" \".join(_context.rm_list))):\n            error(\"Deleting the configuration files failed\")\n\n\ndef init_common_geo():\n    \"\"\"\n    Tasks to do both on first and other geo nodes.\n    \"\"\"\n    if not utils.package_is_installed(\"booth\"):\n        error(\"Booth not installed - Not configurable as a geo cluster node.\")\n\n\nBOOTH_CFG = \"/etc/booth/booth.conf\"\nBOOTH_AUTH = \"/etc/booth/authkey\"\n\n\ndef init_csync2_geo():\n    \"\"\"\n    TODO: Configure csync2 for geo cluster\n    That is, create a second sync group which\n    syncs the geo configuration across the whole\n    geo cluster.\n    \"\"\"\n\n\ndef create_booth_authkey():\n    status(\"Create authentication key for booth\")\n    if os.path.exists(BOOTH_AUTH):\n        rmfile(BOOTH_AUTH)\n    rc, _, err = invoke(\"booth-keygen {}\".format(BOOTH_AUTH))\n    if not rc:\n        error(\"Failed to generate booth authkey: {}\".format(err))\n\n\ndef create_booth_config(arbitrator, clusters, tickets):\n    status(\"Configure booth\")\n\n    config_template = \"\"\"# The booth configuration file is \"/etc/booth/booth.conf\". You need to\n# prepare the same booth configuration file on each arbitrator and\n# each node in the cluster sites where the booth daemon can be launched.\n\n# \"transport\" means which transport layer booth daemon will use.\n# Currently only \"UDP\" is supported.\ntransport=\"UDP\"\nport=\"9929\"\n\"\"\"\n    cfg = [config_template]\n    if arbitrator is not None:\n        cfg.append(\"arbitrator=\\\"{}\\\"\".format(arbitrator))\n    for s in clusters.values():\n        cfg.append(\"site=\\\"{}\\\"\".format(s))\n    cfg.append(\"authfile=\\\"{}\\\"\".format(BOOTH_AUTH))\n    for t in tickets:\n        cfg.append(\"ticket=\\\"{}\\\"\\nexpire=\\\"600\\\"\".format(t))\n    cfg = \"\\n\".join(cfg) + \"\\n\"\n\n    if os.path.exists(BOOTH_CFG):\n        rmfile(BOOTH_CFG)\n    utils.str2file(cfg, BOOTH_CFG)\n    utils.chown(BOOTH_CFG, \"hacluster\", \"haclient\")\n    os.chmod(BOOTH_CFG, 0o644)\n\n\ndef bootstrap_init_geo(context):\n    \"\"\"\n    Configure as a geo cluster member.\n    \"\"\"\n    global _context\n    _context = context\n\n    if os.path.exists(BOOTH_CFG) and not confirm(\"This will overwrite {} - continue?\".format(BOOTH_CFG)):\n        return\n    if os.path.exists(BOOTH_AUTH) and not confirm(\"This will overwrite {} - continue?\".format(BOOTH_AUTH)):\n        return\n\n    init_common_geo()\n\n    # TODO:\n    # in /etc/drbd.conf or /etc/drbd.d/global_common.conf\n    # set common.startup.wfc-timeout 100\n    # set common.startup.degr-wfc-timeout 120\n\n    create_booth_authkey()\n    create_booth_config(_context.arbitrator, _context.clusters, _context.tickets)\n    status(\"Sync booth configuration across cluster\")\n    csync2_update(\"/etc/booth\")\n    init_csync2_geo()\n    geo_cib_config(_context.clusters)\n\n\ndef geo_fetch_config(node):\n    # TODO: clean this up\n    status(\"Retrieving configuration - This may prompt for root@%s:\" % (node))\n    tmpdir = tmpfiles.create_dir()\n    rc, _, err = invoke(\"scp -oStrictHostKeyChecking=no root@%s:'/etc/booth/*' %s/\" % (node, tmpdir))\n    if not rc:\n        error(\"Failed to retrieve configuration: {}\".format(err))\n    try:\n        if os.path.isfile(\"%s/authkey\" % (tmpdir)):\n            invoke(\"mv %s/authkey %s\" % (tmpdir, BOOTH_AUTH))\n            os.chmod(BOOTH_AUTH, 0o600)\n        if os.path.isfile(\"%s/booth.conf\" % (tmpdir)):\n            invoke(\"mv %s/booth.conf %s\" % (tmpdir, BOOTH_CFG))\n            os.chmod(BOOTH_CFG, 0o644)\n    except OSError as err:\n        raise ValueError(\"Problem encountered with booth configuration from {}: {}\".format(node, err))\n\n\ndef geo_cib_config(clusters):\n    cluster_name = corosync.get_values('totem.cluster_name')[0]\n    if cluster_name not in list(clusters.keys()):\n        error(\"Local cluster name is {}, expected {}\".format(cluster_name, \"|\".join(list(clusters.keys()))))\n\n    status(\"Configure cluster resources for booth\")\n    crm_template = Template(\"\"\"\nprimitive booth-ip ocf:heartbeat:IPaddr2 $iprules\nprimitive booth-site ocf:pacemaker:booth-site \\\n  meta resource-stickiness=\"INFINITY\" \\\n  params config=booth op monitor interval=\"10s\"\ngroup g-booth booth-ip booth-site meta target-role=Stopped\n\"\"\")\n    iprule = 'params rule #cluster-name eq {} ip=\"{}\"'\n\n    crm_configure_load(\"update\", crm_template.substitute(iprules=\" \".join(iprule.format(k, v) for k, v in clusters.items())))\n\n\ndef bootstrap_join_geo(context):\n    \"\"\"\n    Run on second cluster to add to a geo configuration.\n    It fetches its booth configuration from the other node (cluster node or arbitrator).\n    \"\"\"\n    global _context\n    _context = context\n    init_common_geo()\n    check_tty()\n    geo_fetch_config(_context.cluster_node)\n    status(\"Sync booth configuration across cluster\")\n    csync2_update(\"/etc/booth\")\n    geo_cib_config(_context.clusters)\n\n\ndef bootstrap_arbitrator(context):\n    \"\"\"\n    Configure this machine as an arbitrator.\n    It fetches its booth configuration from a cluster node already in the cluster.\n    \"\"\"\n    global _context\n    _context = context\n    node = _context.cluster_node\n\n    init_common_geo()\n    check_tty()\n    geo_fetch_config(node)\n    if not os.path.isfile(BOOTH_CFG):\n        error(\"Failed to copy {} from {}\".format(BOOTH_CFG, node))\n    # TODO: verify that the arbitrator IP in the configuration is us?\n    status(\"Enabling and starting the booth arbitrator service\")\n    utils.start_service(\"booth@booth\", enable=True)\n\n# EOF\n", "# Copyright (C) 2008-2011 Dejan Muhamedagic <dmuhamedagic@suse.de>\n# See COPYING for license information.\n\nimport os\nimport sys\nfrom tempfile import mkstemp\nimport subprocess\nimport re\nimport glob\nimport time\nimport datetime\nimport shutil\nimport shlex\nimport bz2\nimport fnmatch\nimport gc\nimport ipaddress\nimport argparse\nfrom contextlib import contextmanager, closing\nfrom . import config\nfrom . import userdir\nfrom . import constants\nfrom . import options\nfrom . import term\nfrom . import parallax\nfrom .msg import common_warn, common_info, common_debug, common_err, err_buf\n\n\ndef to_ascii(input_str):\n    \"\"\"Convert the bytes string to a ASCII string\n    Usefull to remove accent (diacritics)\"\"\"\n    if input_str is None:\n        return input_str\n    if isinstance(input_str, str):\n        return input_str\n    try:\n        return str(input_str, 'utf-8')\n    except UnicodeDecodeError:\n        if config.core.debug or options.regression_tests:\n            import traceback\n            traceback.print_exc()\n        return input_str.decode('utf-8', errors='ignore')\n\n\ndef filter_keys(key_list, args, sign=\"=\"):\n    \"\"\"Return list item which not be completed yet\"\"\"\n    return [s+sign for s in key_list if any_startswith(args, s+sign) is None]\n\n\ndef any_startswith(iterable, prefix):\n    \"\"\"Return first element in iterable which startswith prefix, or None.\"\"\"\n    for element in iterable:\n        if element.startswith(prefix):\n            return element\n    return None\n\n\ndef rindex(iterable, value):\n    return len(iterable) - iterable[::-1].index(value) - 1\n\n\ndef memoize(function):\n    \"Decorator to invoke a function once only for any argument\"\n    memoized = {}\n\n    def inner(*args):\n        if args in memoized:\n            return memoized[args]\n        r = function(*args)\n        memoized[args] = r\n        return r\n    return inner\n\n\n@contextmanager\ndef nogc():\n    gc.disable()\n    try:\n        yield\n    finally:\n        gc.enable()\n\n\ngetuser = userdir.getuser\ngethomedir = userdir.gethomedir\n\n\n@memoize\ndef this_node():\n    'returns name of this node (hostname)'\n    return os.uname()[1]\n\n\n_cib_shadow = 'CIB_shadow'\n_cib_in_use = ''\n\n\ndef set_cib_in_use(name):\n    os.putenv(_cib_shadow, name)\n    global _cib_in_use\n    _cib_in_use = name\n\n\ndef clear_cib_in_use():\n    os.unsetenv(_cib_shadow)\n    global _cib_in_use\n    _cib_in_use = ''\n\n\ndef get_cib_in_use():\n    return _cib_in_use\n\n\ndef get_tempdir():\n    return os.getenv(\"TMPDIR\") or \"/tmp\"\n\n\ndef is_program(prog):\n    \"\"\"Is this program available?\"\"\"\n    def isexec(filename):\n        return os.path.isfile(filename) and os.access(filename, os.X_OK)\n    for p in os.getenv(\"PATH\").split(os.pathsep):\n        f = os.path.join(p, prog)\n        if isexec(f):\n            return f\n    return None\n\n\ndef pacemaker_20_daemon(new, old):\n    \"helper to discover renamed pacemaker daemons\"\n    if is_program(new):\n        return new\n    return old\n\n\n@memoize\ndef pacemaker_attrd():\n    return pacemaker_20_daemon(\"pacemaker-attrd\", \"attrd\")\n\n\n@memoize\ndef pacemaker_based():\n    return pacemaker_20_daemon(\"pacemaker-based\", \"cib\")\n\n\n@memoize\ndef pacemaker_controld():\n    return pacemaker_20_daemon(\"pacemaker-controld\", \"crmd\")\n\n\n@memoize\ndef pacemaker_execd():\n    return pacemaker_20_daemon(\"pacemaker-execd\", \"lrmd\")\n\n\n@memoize\ndef pacemaker_fenced():\n    return pacemaker_20_daemon(\"pacemaker-fenced\", \"stonithd\")\n\n\n@memoize\ndef pacemaker_remoted():\n    return pacemaker_20_daemon(\"pacemaker-remoted\", \"pacemaker_remoted\")\n\n\n@memoize\ndef pacemaker_schedulerd():\n    return pacemaker_20_daemon(\"pacemaker-schedulerd\", \"pengine\")\n\n\ndef pacemaker_daemon(name):\n    if name == \"attrd\" or name == \"pacemaker-attrd\":\n        return pacemaker_attrd()\n    if name == \"cib\" or name == \"pacemaker-based\":\n        return pacemaker_based()\n    if name == \"crmd\" or name == \"pacemaker-controld\":\n        return pacemaker_controld()\n    if name == \"lrmd\" or name == \"pacemaker-execd\":\n        return pacemaker_execd()\n    if name == \"stonithd\" or name == \"pacemaker-fenced\":\n        return pacemaker_fenced()\n    if name == \"pacemaker_remoted\" or name == \"pacemeaker-remoted\":\n        return pacemaker_remoted()\n    if name == \"pengine\" or name == \"pacemaker-schedulerd\":\n        return pacemaker_schedulerd()\n    raise ValueError(\"Not a Pacemaker daemon name: {}\".format(name))\n\n\ndef can_ask():\n    \"\"\"\n    Is user-interactivity possible?\n    Checks if connected to a TTY.\n    \"\"\"\n    return (not options.ask_no) and sys.stdin.isatty()\n\n\ndef ask(msg):\n    \"\"\"\n    Ask for user confirmation.\n    If core.force is true, always return true.\n    If not interactive and core.force is false, always return false.\n    \"\"\"\n    if config.core.force:\n        common_info(\"%s [YES]\" % (msg))\n        return True\n    if not can_ask():\n        return False\n\n    msg += ' '\n    if msg.endswith('? '):\n        msg = msg[:-2] + ' (y/n)? '\n\n    while True:\n        try:\n            ans = input(msg)\n        except EOFError:\n            ans = 'n'\n        if ans:\n            ans = ans[0].lower()\n            if ans in 'yn':\n                return ans == 'y'\n\n\n# holds part of line before \\ split\n# for a multi-line input\n_LINE_BUFFER = ''\n\n\ndef get_line_buffer():\n    return _LINE_BUFFER\n\n\ndef multi_input(prompt=''):\n    \"\"\"\n    Get input from user\n    Allow multiple lines using a continuation character\n    \"\"\"\n    global _LINE_BUFFER\n    line = []\n    _LINE_BUFFER = ''\n    while True:\n        try:\n            text = input(prompt)\n        except EOFError:\n            return None\n        err_buf.incr_lineno()\n        if options.regression_tests:\n            print(\".INP:\", text)\n            sys.stdout.flush()\n            sys.stderr.flush()\n        stripped = text.strip()\n        if stripped.endswith('\\\\'):\n            stripped = stripped.rstrip('\\\\')\n            line.append(stripped)\n            _LINE_BUFFER += stripped\n            if prompt:\n                prompt = '   > '\n        else:\n            line.append(stripped)\n            break\n    return ''.join(line)\n\n\ndef verify_boolean(opt):\n    return opt.lower() in (\"yes\", \"true\", \"on\", \"1\") or \\\n        opt.lower() in (\"no\", \"false\", \"off\", \"0\")\n\n\ndef is_boolean_true(opt):\n    if opt in (None, False):\n        return False\n    if opt is True:\n        return True\n    return opt.lower() in (\"yes\", \"true\", \"on\", \"1\")\n\n\ndef is_boolean_false(opt):\n    if opt in (None, False):\n        return True\n    if opt is True:\n        return False\n    return opt.lower() in (\"no\", \"false\", \"off\", \"0\")\n\n\ndef get_boolean(opt, dflt=False):\n    if not opt:\n        return dflt\n    return is_boolean_true(opt)\n\n\ndef canonical_boolean(opt):\n    return 'true' if is_boolean_true(opt) else 'false'\n\n\ndef keyword_cmp(string1, string2):\n    return string1.lower() == string2.lower()\n\n\nclass olist(list):\n    \"\"\"\n    Implements the 'in' operator\n    in a case-insensitive manner,\n    allowing \"if x in olist(...)\"\n    \"\"\"\n    def __init__(self, keys):\n        super(olist, self).__init__([k.lower() for k in keys])\n\n    def __contains__(self, key):\n        return super(olist, self).__contains__(key.lower())\n\n    def append(self, key):\n        super(olist, self).append(key.lower())\n\n\ndef os_types_list(path):\n    l = []\n    for f in glob.glob(path):\n        if os.access(f, os.X_OK) and os.path.isfile(f):\n            a = f.split(\"/\")\n            l.append(a[-1])\n    return l\n\n\ndef listtemplates():\n    l = []\n    templates_dir = os.path.join(config.path.sharedir, 'templates')\n    for f in os.listdir(templates_dir):\n        if os.path.isfile(\"%s/%s\" % (templates_dir, f)):\n            l.append(f)\n    return l\n\n\ndef listconfigs():\n    l = []\n    for f in os.listdir(userdir.CRMCONF_DIR):\n        if os.path.isfile(\"%s/%s\" % (userdir.CRMCONF_DIR, f)):\n            l.append(f)\n    return l\n\n\ndef add_sudo(cmd):\n    if config.core.user:\n        return \"sudo -E -u %s %s\" % (config.core.user, cmd)\n    return cmd\n\n\ndef add_su(cmd, user):\n    \"\"\"\n    Wrapped cmd with su -c \"<cmd>\" <user>\n    \"\"\"\n    if user == \"root\":\n        return cmd\n    return \"su -c \\\"{}\\\" {}\".format(cmd, user)\n\n\ndef chown(path, user, group):\n    if isinstance(user, int):\n        uid = user\n    else:\n        import pwd\n        uid = pwd.getpwnam(user).pw_uid\n    if isinstance(group, int):\n        gid = group\n    else:\n        import grp\n        gid = grp.getgrnam(group).gr_gid\n    os.chown(path, uid, gid)\n\n\ndef ensure_sudo_readable(f):\n    # make sure the tempfile is readable to crm_diff (bsc#999683)\n    if config.core.user:\n        from pwd import getpwnam\n        uid = getpwnam(config.core.user).pw_uid\n        try:\n            os.chown(f, uid, -1)\n        except os.error as err:\n            common_err('Failed setting temporary file permissions: %s' % (err))\n            return False\n    return True\n\n\ndef pipe_string(cmd, s):\n    rc = -1  # command failed\n    cmd = add_sudo(cmd)\n    common_debug(\"piping string to %s\" % cmd)\n    if options.regression_tests:\n        print(\".EXT\", cmd)\n    p = subprocess.Popen(cmd, shell=True, stdin=subprocess.PIPE)\n    try:\n        # communicate() expects encoded bytes\n        if isinstance(s, str):\n            s = s.encode('utf-8')\n        p.communicate(s)\n        p.wait()\n        rc = p.returncode\n    except IOError as msg:\n        if \"Broken pipe\" not in str(msg):\n            common_err(msg)\n    return rc\n\n\ndef filter_string(cmd, s, stderr_on=True, shell=True):\n    rc = -1  # command failed\n    outp = ''\n    if stderr_on is True:\n        stderr = None\n    else:\n        stderr = subprocess.PIPE\n    cmd = add_sudo(cmd)\n    common_debug(\"pipe through %s\" % cmd)\n    if options.regression_tests:\n        print(\".EXT\", cmd)\n    p = subprocess.Popen(cmd,\n                         shell=shell,\n                         stdin=subprocess.PIPE,\n                         stdout=subprocess.PIPE,\n                         stderr=stderr)\n    try:\n        # bytes expected here\n        if isinstance(s, str):\n            s = s.encode('utf-8')\n        ret = p.communicate(s)\n        if stderr_on == 'stdout':\n            outp = b\"\\n\".join(ret)\n        else:\n            outp = ret[0]\n        p.wait()\n        rc = p.returncode\n    except OSError as err:\n        if err.errno != os.errno.EPIPE:\n            common_err(err.strerror)\n        common_info(\"from: %s\" % cmd)\n    except Exception as msg:\n        common_err(msg)\n        common_info(\"from: %s\" % cmd)\n    return rc, to_ascii(outp)\n\n\ndef str2tmp(_str, suffix=\".pcmk\"):\n    '''\n    Write the given string to a temporary file. Return the name\n    of the file.\n    '''\n    s = to_ascii(_str)\n    fd, tmp = mkstemp(suffix=suffix)\n    try:\n        f = os.fdopen(fd, \"w\")\n    except IOError as msg:\n        common_err(msg)\n        return\n    f.write(s)\n    if not s.endswith('\\n'):\n        f.write(\"\\n\")\n    f.close()\n    return tmp\n\n\n@contextmanager\ndef create_tempfile(suffix='', dir=None):\n    \"\"\" Context for temporary file.\n\n    Will find a free temporary filename upon entering\n    and will try to delete the file on leaving, even in case of an exception.\n\n    Parameters\n    ----------\n    suffix : string\n        optional file suffix\n    dir : string\n        optional directory to save temporary file in\n\n    (from http://stackoverflow.com/a/29491523)\n    \"\"\"\n    import tempfile\n    tf = tempfile.NamedTemporaryFile(delete=False, suffix=suffix, dir=dir)\n    tf.file.close()\n    try:\n        yield tf.name\n    finally:\n        try:\n            os.remove(tf.name)\n        except OSError as e:\n            if e.errno == 2:\n                pass\n            else:\n                raise\n\n\n@contextmanager\ndef open_atomic(filepath, mode=\"r\", buffering=-1, fsync=False, encoding=None):\n    \"\"\" Open temporary file object that atomically moves to destination upon\n    exiting.\n\n    Allows reading and writing to and from the same filename.\n\n    The file will not be moved to destination in case of an exception.\n\n    Parameters\n    ----------\n    filepath : string\n        the file path to be opened\n    fsync : bool\n        whether to force write the file to disk\n\n    (from http://stackoverflow.com/a/29491523)\n    \"\"\"\n\n    with create_tempfile(dir=os.path.dirname(os.path.abspath(filepath))) as tmppath:\n        with open(tmppath, mode, buffering, encoding=encoding) as file:\n            try:\n                yield file\n            finally:\n                if fsync:\n                    file.flush()\n                    os.fsync(file.fileno())\n        os.rename(tmppath, filepath)\n\n\ndef str2file(s, fname):\n    '''\n    Write a string to a file.\n    '''\n    try:\n        with open_atomic(fname, 'w', encoding='utf-8') as dst:\n            dst.write(to_ascii(s))\n    except IOError as msg:\n        common_err(msg)\n        return False\n    return True\n\n\ndef file2str(fname, noerr=True):\n    '''\n    Read a one line file into a string, strip whitespace around.\n    '''\n    try:\n        f = open(fname, \"r\")\n    except IOError as msg:\n        if not noerr:\n            common_err(msg)\n        return None\n    s = f.readline()\n    f.close()\n    return s.strip()\n\n\ndef file2list(fname):\n    '''\n    Read a file into a list (newlines dropped).\n    '''\n    try:\n        return open(fname).read().split('\\n')\n    except IOError as msg:\n        common_err(msg)\n        return None\n\n\ndef safe_open_w(fname):\n    if fname == \"-\":\n        f = sys.stdout\n    else:\n        if not options.batch and os.access(fname, os.F_OK):\n            if not ask(\"File %s exists. Do you want to overwrite it?\" % fname):\n                return None\n        try:\n            f = open(fname, \"w\")\n        except IOError as msg:\n            common_err(msg)\n            return None\n    return f\n\n\ndef safe_close_w(f):\n    if f and f != sys.stdout:\n        f.close()\n\n\ndef is_path_sane(name):\n    if re.search(r\"['`#*?$\\[\\]]\", name):\n        common_err(\"%s: bad path\" % name)\n        return False\n    return True\n\n\ndef is_filename_sane(name):\n    if re.search(r\"['`/#*?$\\[\\]]\", name):\n        common_err(\"%s: bad filename\" % name)\n        return False\n    return True\n\n\ndef is_name_sane(name):\n    if re.search(\"[']\", name):\n        common_err(\"%s: bad name\" % name)\n        return False\n    return True\n\n\ndef show_dot_graph(dotfile, keep_file=False, desc=\"transition graph\"):\n    cmd = \"%s %s\" % (config.core.dotty, dotfile)\n    if not keep_file:\n        cmd = \"(%s; rm -f %s)\" % (cmd, dotfile)\n    if options.regression_tests:\n        print(\".EXT\", cmd)\n    subprocess.Popen(cmd, shell=True, bufsize=0,\n                     stdin=None, stdout=None, stderr=None, close_fds=True)\n    common_info(\"starting %s to show %s\" % (config.core.dotty, desc))\n\n\ndef ext_cmd(cmd, shell=True):\n    cmd = add_sudo(cmd)\n    if options.regression_tests:\n        print(\".EXT\", cmd)\n    common_debug(\"invoke: %s\" % cmd)\n    return subprocess.call(cmd, shell=shell)\n\n\ndef ext_cmd_nosudo(cmd, shell=True):\n    if options.regression_tests:\n        print(\".EXT\", cmd)\n    return subprocess.call(cmd, shell=shell)\n\n\ndef rmdir_r(d):\n    # TODO: Make sure we're not deleting something we shouldn't!\n    if d and os.path.isdir(d):\n        shutil.rmtree(d)\n\n\ndef nvpairs2dict(pairs):\n    '''\n    takes a list of string of form ['a=b', 'c=d']\n    and returns {'a':'b', 'c':'d'}\n    '''\n    data = []\n    for var in pairs:\n        if '=' in var:\n            data.append(var.split('=', 1))\n        else:\n            data.append([var, None])\n    return dict(data)\n\n\ndef is_check_always():\n    '''\n    Even though the frequency may be set to always, it doesn't\n    make sense to do that with non-interactive sessions.\n    '''\n    return options.interactive and config.core.check_frequency == \"always\"\n\n\ndef get_check_rc():\n    '''\n    If the check mode is set to strict, then on errors we\n    return 2 which is the code for error. Otherwise, we\n    pretend that errors are warnings.\n    '''\n    return 2 if config.core.check_mode == \"strict\" else 1\n\n\n_LOCKDIR = \".lockdir\"\n_PIDF = \"pid\"\n\n\ndef check_locker(lockdir):\n    if not os.path.isdir(os.path.join(lockdir, _LOCKDIR)):\n        return\n    s = file2str(os.path.join(lockdir, _LOCKDIR, _PIDF))\n    pid = convert2ints(s)\n    if not isinstance(pid, int):\n        common_warn(\"history: removing malformed lock\")\n        rmdir_r(os.path.join(lockdir, _LOCKDIR))\n        return\n    try:\n        os.kill(pid, 0)\n    except OSError as err:\n        if err.errno == os.errno.ESRCH:\n            common_info(\"history: removing stale lock\")\n            rmdir_r(os.path.join(lockdir, _LOCKDIR))\n        else:\n            common_err(\"%s: %s\" % (_LOCKDIR, err.strerror))\n\n\n@contextmanager\ndef lock(lockdir):\n    \"\"\"\n    Ensure that the lock is released properly\n    even in the face of an exception between\n    acquire and release.\n    \"\"\"\n    def acquire_lock():\n        check_locker(lockdir)\n        while True:\n            try:\n                os.makedirs(os.path.join(lockdir, _LOCKDIR))\n                str2file(\"%d\" % os.getpid(), os.path.join(lockdir, _LOCKDIR, _PIDF))\n                return True\n            except OSError as err:\n                if err.errno != os.errno.EEXIST:\n                    common_err(\"Failed to acquire lock to %s: %s\" % (lockdir, err.strerror))\n                    return False\n                time.sleep(0.1)\n                continue\n            else:\n                return False\n\n    has_lock = acquire_lock()\n    try:\n        yield\n    finally:\n        if has_lock:\n            rmdir_r(os.path.join(lockdir, _LOCKDIR))\n\n\ndef mkdirp(d, mode=0o777):\n    if os.path.isdir(d):\n        return True\n    os.makedirs(d, mode=mode)\n\n\ndef pipe_cmd_nosudo(cmd):\n    if options.regression_tests:\n        print(\".EXT\", cmd)\n    proc = subprocess.Popen(cmd,\n                            shell=True,\n                            stdout=subprocess.PIPE,\n                            stderr=subprocess.PIPE)\n    (outp, err_outp) = proc.communicate()\n    proc.wait()\n    rc = proc.returncode\n    if rc != 0:\n        print(outp)\n        print(err_outp)\n    return rc\n\n\ndef run_cmd_on_remote(cmd, remote_addr, prompt_msg=None):\n    \"\"\"\n    Run a cmd on remote node\n    return (rc, stdout, err_msg)\n    \"\"\"\n    rc = 1\n    out_data = None\n    err_data = None\n\n    need_pw = check_ssh_passwd_need(remote_addr)\n    if need_pw and prompt_msg:\n        print(prompt_msg)\n    try:\n        result = parallax.parallax_call([remote_addr], cmd, need_pw)\n        rc, out_data, _ = result[0][1]\n    except ValueError as err:\n        err_match = re.search(\"Exited with error code ([0-9]+), Error output: (.*)\", str(err))\n        if err_match:\n            rc, err_data = err_match.groups()\n    finally:\n        return int(rc), to_ascii(out_data), err_data\n\n\ndef get_stdout(cmd, input_s=None, stderr_on=True, shell=True, raw=False):\n    '''\n    Run a cmd, return stdout output.\n    Optional input string \"input_s\".\n    stderr_on controls whether to show output which comes on stderr.\n    '''\n    if stderr_on:\n        stderr = None\n    else:\n        stderr = subprocess.PIPE\n    if options.regression_tests:\n        print(\".EXT\", cmd)\n    proc = subprocess.Popen(cmd,\n                            shell=shell,\n                            stdin=subprocess.PIPE,\n                            stdout=subprocess.PIPE,\n                            stderr=stderr)\n    stdout_data, stderr_data = proc.communicate(input_s)\n    if raw:\n        return proc.returncode, stdout_data\n    return proc.returncode, to_ascii(stdout_data).strip()\n\n\ndef get_stdout_stderr(cmd, input_s=None, shell=True, raw=False):\n    '''\n    Run a cmd, return (rc, stdout, stderr)\n    '''\n    if options.regression_tests:\n        print(\".EXT\", cmd)\n    proc = subprocess.Popen(cmd,\n                            shell=shell,\n                            stdin=input_s and subprocess.PIPE or None,\n                            stdout=subprocess.PIPE,\n                            stderr=subprocess.PIPE)\n    stdout_data, stderr_data = proc.communicate(input_s)\n    if raw:\n        return proc.returncode, stdout_data, stderr_data\n    return proc.returncode, to_ascii(stdout_data).strip(), to_ascii(stderr_data).strip()\n\n\ndef stdout2list(cmd, stderr_on=True, shell=True):\n    '''\n    Run a cmd, fetch output, return it as a list of lines.\n    stderr_on controls whether to show output which comes on stderr.\n    '''\n    rc, s = get_stdout(add_sudo(cmd), stderr_on=stderr_on, shell=shell)\n    if not s:\n        return rc, []\n    return rc, s.split('\\n')\n\n\ndef append_file(dest, src):\n    'Append src to dest'\n    try:\n        open(dest, \"a\").write(open(src).read())\n        return True\n    except IOError as msg:\n        common_err(\"append %s to %s: %s\" % (src, dest, msg))\n        return False\n\n\ndef get_dc():\n    cmd = \"crmadmin -D\"\n    rc, s = get_stdout(add_sudo(cmd))\n    if rc != 0:\n        return None\n    if not s.startswith(\"Designated\"):\n        return None\n    return s.split()[-1]\n\n\ndef wait4dc(what=\"\", show_progress=True):\n    '''\n    Wait for the DC to get into the S_IDLE state. This should be\n    invoked only after a CIB modification which would exercise\n    the PE. Parameter \"what\" is whatever the caller wants to be\n    printed if showing progress.\n\n    It is assumed that the DC is already in a different state,\n    usually it should be either PENGINE or TRANSITION. This\n    assumption may not be true, but there's a high chance that it\n    is since crmd should be faster to move through states than\n    this shell.\n\n    Further, it may also be that crmd already calculated the new\n    graph, did transition, and went back to the idle state. This\n    may in particular be the case if the transition turned out to\n    be empty.\n\n    Tricky. Though in practice it shouldn't be an issue.\n\n    There's no timeout, as we expect the DC to eventually becomes\n    idle.\n    '''\n    dc = get_dc()\n    if not dc:\n        common_warn(\"can't find DC\")\n        return False\n    cmd = \"crm_attribute -Gq -t crm_config -n crmd-transition-delay 2> /dev/null\"\n    delay = get_stdout(add_sudo(cmd))[1]\n    if delay:\n        delaymsec = crm_msec(delay)\n        if delaymsec > 0:\n            common_info(\"The crmd-transition-delay is configured. Waiting %d msec before check DC status.\" % delaymsec)\n            time.sleep(delaymsec // 1000)\n    cnt = 0\n    output_started = 0\n    init_sleep = 0.25\n    max_sleep = 1.00\n    sleep_time = init_sleep\n    while True:\n        dc = get_dc()\n        if not dc:\n            common_warn(\"DC lost during wait\")\n            return False\n        cmd = \"crmadmin -S %s\" % dc\n        rc, s = get_stdout(add_sudo(cmd))\n        if not s.startswith(\"Status\"):\n            common_warn(\"%s unexpected output: %s (exit code: %d)\" %\n                        (cmd, s, rc))\n            return False\n        try:\n            dc_status = s.split()[-2]\n        except:\n            common_warn(\"%s unexpected output: %s\" % (cmd, s))\n            return False\n        if dc_status == \"S_IDLE\":\n            if output_started:\n                sys.stderr.write(\" done\\n\")\n            return True\n        time.sleep(sleep_time)\n        if sleep_time < max_sleep:\n            sleep_time *= 2\n        if show_progress:\n            if not output_started:\n                output_started = 1\n                sys.stderr.write(\"waiting for %s to finish .\" % what)\n            cnt += 1\n            if cnt % 5 == 0:\n                sys.stderr.write(\".\")\n\n\ndef run_ptest(graph_s, nograph, scores, utilization, actions, verbosity):\n    '''\n    Pipe graph_s thru ptest(8). Show graph using dotty if requested.\n    '''\n    actions_filter = \"grep LogActions: | grep -vw Leave\"\n    ptest = \"2>&1 %s -x -\" % config.core.ptest\n    if re.search(\"simulate\", ptest) and \\\n            not re.search(\"-[RS]\", ptest):\n        ptest = \"%s -S\" % ptest\n    if verbosity:\n        if actions:\n            verbosity = 'v' * max(3, len(verbosity))\n        ptest = \"%s -%s\" % (ptest, verbosity.upper())\n    if scores:\n        ptest = \"%s -s\" % ptest\n    if utilization:\n        ptest = \"%s -U\" % ptest\n    if config.core.dotty and not nograph:\n        fd, dotfile = mkstemp()\n        ptest = \"%s -D %s\" % (ptest, dotfile)\n    else:\n        dotfile = None\n    # ptest prints to stderr\n    if actions:\n        ptest = \"%s | %s\" % (ptest, actions_filter)\n    if options.regression_tests:\n        ptest = \">/dev/null %s\" % ptest\n    common_debug(\"invoke: %s\" % ptest)\n    rc, s = get_stdout(ptest, input_s=graph_s)\n    if rc != 0:\n        common_debug(\"'%s' exited with (rc=%d)\" % (ptest, rc))\n        if actions and rc == 1:\n            common_warn(\"No actions found.\")\n        else:\n            common_warn(\"Simulation was unsuccessful (RC=%d).\" % (rc))\n    if dotfile:\n        if os.path.getsize(dotfile) > 0:\n            show_dot_graph(dotfile)\n        else:\n            common_warn(\"ptest produced empty dot file\")\n    else:\n        if not nograph:\n            common_info(\"install graphviz to see a transition graph\")\n    if s:\n        page_string(s)\n    return True\n\n\ndef is_id_valid(ident):\n    \"\"\"\n    Verify that the id follows the definition:\n    http://www.w3.org/TR/1999/REC-xml-names-19990114/#ns-qualnames\n    \"\"\"\n    if not ident:\n        return False\n    id_re = r\"^[A-Za-z_][\\w._-]*$\"\n    return re.match(id_re, ident)\n\n\ndef check_range(a):\n    \"\"\"\n    Verify that the integer range in list a is valid.\n    \"\"\"\n    if len(a) != 2:\n        return False\n    if not isinstance(a[0], int) or not isinstance(a[1], int):\n        return False\n    return int(a[0]) <= int(a[1])\n\n\ndef crm_msec(t):\n    '''\n    See lib/common/utils.c:crm_get_msec().\n    '''\n    convtab = {\n        'ms': (1, 1),\n        'msec': (1, 1),\n        'us': (1, 1000),\n        'usec': (1, 1000),\n        '': (1000, 1),\n        's': (1000, 1),\n        'sec': (1000, 1),\n        'm': (60*1000, 1),\n        'min': (60*1000, 1),\n        'h': (60*60*1000, 1),\n        'hr': (60*60*1000, 1),\n    }\n    if not t:\n        return -1\n    r = re.match(r\"\\s*(\\d+)\\s*([a-zA-Z]+)?\", t)\n    if not r:\n        return -1\n    if not r.group(2):\n        q = ''\n    else:\n        q = r.group(2).lower()\n    try:\n        mult, div = convtab[q]\n    except KeyError:\n        return -1\n    return (int(r.group(1))*mult) // div\n\n\ndef crm_time_cmp(a, b):\n    return crm_msec(a) - crm_msec(b)\n\n\ndef shorttime(ts):\n    if isinstance(ts, datetime.datetime):\n        return ts.strftime(\"%X\")\n    if ts is not None:\n        return time.strftime(\"%X\", time.localtime(ts))\n    return time.strftime(\"%X\", time.localtime(0))\n\n\ndef shortdate(ts):\n    if isinstance(ts, datetime.datetime):\n        return ts.strftime(\"%F\")\n    if ts is not None:\n        return time.strftime(\"%F\", time.localtime(ts))\n    return time.strftime(\"%F\", time.localtime(0))\n\n\ndef sort_by_mtime(l):\n    'Sort a (small) list of files by time mod.'\n    l2 = [(os.stat(x).st_mtime, x) for x in l]\n    l2.sort()\n    return [x[1] for x in l2]\n\n\ndef file_find_by_name(root, filename):\n    'Find a file within a tree matching fname'\n    assert root\n    assert filename\n    for root, dirnames, filenames in os.walk(root):\n        for filename in fnmatch.filter(filenames, filename):\n            return os.path.join(root, filename)\n    return None\n\n\ndef convert2ints(l):\n    \"\"\"\n    Convert a list of strings (or a string) to a list of ints.\n    All strings must be ints, otherwise conversion fails and None\n    is returned!\n    \"\"\"\n    try:\n        if isinstance(l, (tuple, list)):\n            return [int(x) for x in l]\n        # it's a string then\n        return int(l)\n    except ValueError:\n        return None\n\n\ndef is_int(s):\n    'Check if the string can be converted to an integer.'\n    try:\n        int(s)\n        return True\n    except ValueError:\n        return False\n\n\ndef is_process(s):\n    \"\"\"\n    Returns true if argument is the name of a running process.\n\n    s: process name\n    returns Boolean\n    \"\"\"\n    from os.path import join, basename\n    # find pids of running processes\n    pids = [pid for pid in os.listdir('/proc') if pid.isdigit()]\n    for pid in pids:\n        try:\n            cmdline = open(join('/proc', pid, 'cmdline'), 'rb').read()\n            procname = basename(to_ascii(cmdline).replace('\\x00', ' ').split(' ')[0])\n            if procname == s:\n                return True\n        except EnvironmentError:\n            # a process may have died since we got the list of pids\n            pass\n    return False\n\n\ndef print_stacktrace():\n    \"\"\"\n    Print the stack at the site of call\n    \"\"\"\n    import traceback\n    import inspect\n    sf = inspect.currentframe().f_back.f_back\n    traceback.print_stack(sf)\n\n\n@memoize\ndef cluster_stack():\n    if is_process(\"heartbeat:.[m]aster\"):\n        return \"heartbeat\"\n    elif is_process(\"[a]isexec\"):\n        return \"openais\"\n    elif os.path.exists(\"/etc/corosync/corosync.conf\") or is_program('corosync-cfgtool'):\n        return \"corosync\"\n    return \"\"\n\n\ndef edit_file(fname):\n    'Edit a file.'\n    if not fname:\n        return\n    if not config.core.editor:\n        return\n    return ext_cmd_nosudo(\"%s %s\" % (config.core.editor, fname))\n\n\ndef edit_file_ext(fname, template=''):\n    '''\n    Edit a file via a temporary file.\n    Raises IOError on any error.\n    '''\n    if not os.path.isfile(fname):\n        s = template\n    else:\n        s = open(fname).read()\n    filehash = hash(s)\n    tmpfile = str2tmp(s)\n    try:\n        try:\n            if edit_file(tmpfile) != 0:\n                return\n            s = open(tmpfile, 'r').read()\n            if hash(s) == filehash:  # file unchanged\n                return\n            f2 = open(fname, 'w')\n            f2.write(s)\n            f2.close()\n        finally:\n            os.unlink(tmpfile)\n    except OSError as e:\n        raise IOError(e)\n\n\ndef need_pager(s, w, h):\n    from math import ceil\n    cnt = 0\n    for l in s.split('\\n'):\n        # need to remove color codes\n        l = re.sub(r'\\${\\w+}', '', l)\n        cnt += int(ceil((len(l) + 0.5) / w))\n        if cnt >= h:\n            return True\n    return False\n\n\ndef term_render(s):\n    'Render for TERM.'\n    try:\n        return term.render(s)\n    except:\n        return s\n\n\ndef get_pager_cmd(*extra_opts):\n    'returns a commandline which calls the configured pager'\n    cmdline = [config.core.pager]\n    if os.path.basename(config.core.pager) == \"less\":\n        cmdline.append('-R')\n    cmdline.extend(extra_opts)\n    return ' '.join(cmdline)\n\n\ndef page_string(s):\n    'Page string rendered for TERM.'\n    if not s:\n        return\n    constants.need_reset = True\n    w, h = get_winsize()\n    if not need_pager(s, w, h):\n        print(term_render(s))\n    elif not config.core.pager or not can_ask() or options.batch:\n        print(term_render(s))\n    else:\n        pipe_string(get_pager_cmd(), term_render(s).encode('utf-8'))\n    constants.need_reset = False\n\n\ndef page_gen(g):\n    'Page lines generated by generator g'\n    w, h = get_winsize()\n    if not config.core.pager or not can_ask() or options.batch:\n        for line in g:\n            sys.stdout.write(term_render(line))\n    else:\n        pipe_string(get_pager_cmd(), term_render(\"\".join(g)))\n\n\ndef page_file(filename):\n    'Open file in pager'\n    if not os.path.isfile(filename):\n        return\n    return ext_cmd_nosudo(get_pager_cmd(filename), shell=True)\n\n\ndef get_winsize():\n    try:\n        import curses\n        curses.setupterm()\n        w = curses.tigetnum('cols')\n        h = curses.tigetnum('lines')\n    except:\n        try:\n            w = os.environ['COLS']\n            h = os.environ['LINES']\n        except KeyError:\n            w = 80\n            h = 25\n    return w, h\n\n\ndef multicolumn(l):\n    '''\n    A ls-like representation of a list of strings.\n    A naive approach.\n    '''\n    min_gap = 2\n    w, _ = get_winsize()\n    max_len = 8\n    for s in l:\n        if len(s) > max_len:\n            max_len = len(s)\n    cols = w // (max_len + min_gap)  # approx.\n    if not cols:\n        cols = 1\n    col_len = w // cols\n    for i in range(len(l) // cols + 1):\n        s = ''\n        for j in range(i * cols, (i + 1) * cols):\n            if not j < len(l):\n                break\n            if not s:\n                s = \"%-*s\" % (col_len, l[j])\n            elif (j + 1) % cols == 0:\n                s = \"%s%s\" % (s, l[j])\n            else:\n                s = \"%s%-*s\" % (s, col_len, l[j])\n        if s:\n            print(s)\n\n\ndef find_value(pl, name):\n    for n, v in pl:\n        if n == name:\n            return v\n    return None\n\n\ndef cli_replace_attr(pl, name, new_val):\n    for i, attr in enumerate(pl):\n        if attr[0] == name:\n            attr[1] = new_val\n            return\n\n\ndef cli_append_attr(pl, name, val):\n    pl.append([name, val])\n\n\ndef lines2cli(s):\n    '''\n    Convert a string into a list of lines. Replace continuation\n    characters. Strip white space, left and right. Drop empty lines.\n    '''\n    cl = []\n    l = s.split('\\n')\n    cum = []\n    for p in l:\n        p = p.strip()\n        if p.endswith('\\\\'):\n            p = p.rstrip('\\\\')\n            cum.append(p)\n        else:\n            cum.append(p)\n            cl.append(''.join(cum).strip())\n            cum = []\n    if cum:  # in case s ends with backslash\n        cl.append(''.join(cum))\n    return [x for x in cl if x]\n\n\ndef datetime_is_aware(dt):\n    \"\"\"\n    Determines if a given datetime.datetime is aware.\n\n    The logic is described in Python's docs:\n    http://docs.python.org/library/datetime.html#datetime.tzinfo\n    \"\"\"\n    return dt and dt.tzinfo is not None and dt.tzinfo.utcoffset(dt) is not None\n\n\ndef make_datetime_naive(dt):\n    \"\"\"\n    Ensures that the datetime is not time zone-aware:\n\n    The returned datetime object is a naive time in UTC.\n    \"\"\"\n    if dt and datetime_is_aware(dt):\n        return dt.replace(tzinfo=None) - dt.utcoffset()\n    return dt\n\n\ndef total_seconds(td):\n    \"\"\"\n    Backwards compatible implementation of timedelta.total_seconds()\n    \"\"\"\n    if hasattr(datetime.timedelta, 'total_seconds'):\n        return td.total_seconds()\n    return (td.microseconds + (td.seconds + td.days * 24 * 3600) * 10**6) // 10**6\n\n\ndef datetime_to_timestamp(dt):\n    \"\"\"\n    Convert a datetime object into a floating-point second value\n    \"\"\"\n    try:\n        return total_seconds(make_datetime_naive(dt) - datetime.datetime(1970, 1, 1))\n    except Exception as e:\n        common_err(\"datetime_to_timestamp error: %s\" % (e))\n        return None\n\n\ndef timestamp_to_datetime(ts):\n    \"\"\"\n    Convert a timestamp into a naive datetime object\n    \"\"\"\n    import dateutil\n    import dateutil.tz\n    return make_datetime_naive(datetime.datetime.fromtimestamp(ts).replace(tzinfo=dateutil.tz.tzlocal()))\n\n\ndef parse_time(t):\n    '''\n    Try to make sense of the user provided time spec.\n    Use dateutil if available, otherwise strptime.\n    Return the datetime value.\n\n    Also does time zone elimination by passing the datetime\n    through a timestamp conversion if necessary\n\n    TODO: dateutil is very slow, avoid it if possible\n    '''\n    try:\n        from dateutil import parser, tz\n        dt = parser.parse(t)\n\n        if datetime_is_aware(dt):\n            ts = datetime_to_timestamp(dt)\n            if ts is None:\n                return None\n            dt = datetime.datetime.fromtimestamp(ts)\n        else:\n            # convert to UTC from local time\n            dt = dt - tz.tzlocal().utcoffset(dt)\n    except ValueError as msg:\n        common_err(\"parse_time %s: %s\" % (t, msg))\n        return None\n    except ImportError as msg:\n        try:\n            tm = time.strptime(t)\n            dt = datetime.datetime(*tm[0:7])\n        except ValueError as msg:\n            common_err(\"no dateutil, please provide times as printed by date(1)\")\n            return None\n    return dt\n\n\ndef parse_to_timestamp(t):\n    '''\n    Read a string and convert it into a UNIX timestamp.\n    Added as an optimization of parse_time to avoid\n    extra conversion steps when result would be converted\n    into a timestamp anyway\n    '''\n    try:\n        from dateutil import parser, tz\n        dt = parser.parse(t)\n\n        if datetime_is_aware(dt):\n            return datetime_to_timestamp(dt)\n        # convert to UTC from local time\n        return total_seconds(dt - tz.tzlocal().utcoffset(dt) - datetime.datetime(1970, 1, 1))\n    except ValueError as msg:\n        common_err(\"parse_time %s: %s\" % (t, msg))\n        return None\n    except ImportError as msg:\n        try:\n            tm = time.strptime(t)\n            dt = datetime.datetime(*tm[0:7])\n            return datetime_to_timestamp(dt)\n        except ValueError as msg:\n            common_err(\"no dateutil, please provide times as printed by date(1)\")\n            return None\n\n\ndef save_graphviz_file(ini_f, attr_d):\n    '''\n    Save graphviz settings to an ini file, if it does not exist.\n    '''\n    if os.path.isfile(ini_f):\n        common_err(\"%s exists, please remove it first\" % ini_f)\n        return False\n    try:\n        f = open(ini_f, \"wb\")\n    except IOError as msg:\n        common_err(msg)\n        return False\n    import configparser\n    p = configparser.ConfigParser()\n    for section, sect_d in attr_d.items():\n        p.add_section(section)\n        for n, v in sect_d.items():\n            p.set(section, n, v)\n    try:\n        p.write(f)\n    except IOError as msg:\n        common_err(msg)\n        return False\n    f.close()\n    common_info(\"graphviz attributes saved to %s\" % ini_f)\n    return True\n\n\ndef load_graphviz_file(ini_f):\n    '''\n    Load graphviz ini file, if it exists.\n    '''\n    if not os.path.isfile(ini_f):\n        return True, None\n    import configparser\n    p = configparser.ConfigParser()\n    try:\n        p.read(ini_f)\n    except Exception as msg:\n        common_err(msg)\n        return False, None\n    _graph_d = {}\n    for section in p.sections():\n        d = {}\n        for n, v in p.items(section):\n            d[n] = v\n        _graph_d[section] = d\n    return True, _graph_d\n\n\ndef get_pcmk_version(dflt):\n    version = dflt\n\n    crmd = pacemaker_controld()\n    if crmd:\n        cmd = crmd\n    else:\n        return version\n\n    try:\n        rc, s, err = get_stdout_stderr(\"%s version\" % (cmd))\n        if rc != 0:\n            common_err(\"%s exited with %d [err: %s][out: %s]\" % (cmd, rc, err, s))\n        else:\n            common_debug(\"pacemaker version: [err: %s][out: %s]\" % (err, s))\n            if err.startswith(\"CRM Version:\"):\n                version = s.split()[0]\n            else:\n                version = s.split()[2]\n            common_debug(\"found pacemaker version: %s\" % version)\n    except Exception as msg:\n        common_warn(\"could not get the pacemaker version, bad installation?\")\n        common_warn(msg)\n    return version\n\n\ndef get_cib_property(cib_f, attr, dflt):\n    \"\"\"A poor man's get attribute procedure.\n    We don't want heavy parsing, this needs to be relatively\n    fast.\n    \"\"\"\n    open_t = \"<cluster_property_set\"\n    close_t = \"</cluster_property_set\"\n    attr_s = 'name=\"%s\"' % attr\n    ver_patt = re.compile('value=\"([^\"]+)\"')\n    ver = dflt  # return some version in any case\n    try:\n        f = open(cib_f, \"r\")\n    except IOError as msg:\n        common_err(msg)\n        return ver\n    state = 0\n    for s in f:\n        if state == 0:\n            if open_t in s:\n                state += 1\n        elif state == 1:\n            if close_t in s:\n                break\n            if attr_s in s:\n                r = ver_patt.search(s)\n                if r:\n                    ver = r.group(1)\n                break\n    f.close()\n    return ver\n\n\ndef get_cib_attributes(cib_f, tag, attr_l, dflt_l):\n    \"\"\"A poor man's get attribute procedure.\n    We don't want heavy parsing, this needs to be relatively\n    fast.\n    \"\"\"\n    open_t = \"<%s \" % tag\n    val_patt_l = [re.compile('%s=\"([^\"]+)\"' % x) for x in attr_l]\n    val_l = []\n    try:\n        f = open(cib_f, \"rb\").read()\n    except IOError as msg:\n        common_err(msg)\n        return dflt_l\n    if os.path.splitext(cib_f)[-1] == '.bz2':\n        cib_bits = bz2.decompress(f)\n    else:\n        cib_bits = f\n    cib_s = to_ascii(cib_bits)\n    for s in cib_s.split('\\n'):\n        if s.startswith(open_t):\n            i = 0\n            for patt in val_patt_l:\n                r = patt.search(s)\n                val_l.append(r and r.group(1) or dflt_l[i])\n                i += 1\n            break\n    return val_l\n\n\ndef is_min_pcmk_ver(min_ver, cib_f=None):\n    if not constants.pcmk_version:\n        if cib_f:\n            constants.pcmk_version = get_cib_property(cib_f, \"dc-version\", \"1.1.11\")\n            common_debug(\"found pacemaker version: %s in cib: %s\" %\n                         (constants.pcmk_version, cib_f))\n        else:\n            constants.pcmk_version = get_pcmk_version(\"1.1.11\")\n    from distutils.version import LooseVersion\n    return LooseVersion(constants.pcmk_version) >= LooseVersion(min_ver)\n\n\ndef is_pcmk_118(cib_f=None):\n    return is_min_pcmk_ver(\"1.1.8\", cib_f=cib_f)\n\n\n@memoize\ndef cibadmin_features():\n    '''\n    # usage example:\n    if 'corosync-plugin' in cibadmin_features()\n    '''\n    rc, outp = get_stdout(['cibadmin', '-!'], shell=False)\n    if rc == 0:\n        m = re.match(r'Pacemaker\\s(\\S+)\\s\\(Build: ([^\\)]+)\\):\\s(.*)', outp.strip())\n        if m and len(m.groups()) > 2:\n            return m.group(3).split()\n    return []\n\n\n@memoize\ndef cibadmin_can_patch():\n    # cibadmin -P doesn't handle comments in <1.1.11 (unless patched)\n    return is_min_pcmk_ver(\"1.1.11\")\n\n\n# quote function from python module shlex.py in python 3.3\n\n_find_unsafe = re.compile(r'[^\\w@%+=:,./-]').search\n\n\ndef quote(s):\n    \"\"\"Return a shell-escaped version of the string *s*.\"\"\"\n    if not s:\n        return \"''\"\n    if _find_unsafe(s) is None:\n        return s\n\n    # use single quotes, and put single quotes into double quotes\n    # the string $'b is then quoted as '$'\"'\"'b'\n    return \"'\" + s.replace(\"'\", \"'\\\"'\\\"'\") + \"'\"\n\n\ndef doublequote(s):\n    \"\"\"Return a shell-escaped version of the string *s*.\"\"\"\n    if not s:\n        return '\"\"'\n    if _find_unsafe(s) is None:\n        return s\n\n    # use double quotes\n    return '\"' + s.replace('\"', \"\\\\\\\"\") + '\"'\n\n\ndef fetch_opts(args, opt_l):\n    '''\n    Get and remove option keywords from args.\n    They are always listed last, at the end of the line.\n    Return a list of options found. The caller can do\n    if keyw in optlist: ...\n    '''\n    re_opt = None\n    if opt_l[0].startswith(\"@\"):\n        re_opt = re.compile(\"^%s$\" % opt_l[0][1:])\n        del opt_l[0]\n    l = []\n    for i in reversed(list(range(len(args)))):\n        if (args[i] in opt_l) or (re_opt and re_opt.search(args[i])):\n            l.append(args.pop())\n        else:\n            break\n    return l\n\n\n_LIFETIME = [\"reboot\", \"forever\"]\n_ISO8601_RE = re.compile(\"(PT?[0-9]|[0-9]+.*[:-])\")\n\n\ndef fetch_lifetime_opt(args, iso8601=True):\n    '''\n    Get and remove a lifetime option from args. It can be one of\n    lifetime_options or an ISO 8601 formatted period/time. There\n    is apparently no good support in python for this format, so\n    we cheat a bit.\n    '''\n    if args:\n        opt = args[-1]\n        if opt in _LIFETIME or (iso8601 and _ISO8601_RE.match(opt)):\n            return args.pop()\n    return None\n\n\ndef resolve_hostnames(hostnames):\n    '''\n    Tries to resolve the given list of hostnames.\n    returns (ok, failed-hostname)\n    ok: True if all hostnames resolved\n    failed-hostname: First failed hostname resolution\n    '''\n    import socket\n    for node in hostnames:\n        try:\n            socket.gethostbyname(node)\n        except socket.error:\n            return False, node\n    return True, None\n\n\ndef list_corosync_node_names():\n    '''\n    Returns list of nodes configured\n    in corosync.conf\n    '''\n    try:\n        cfg = os.getenv('COROSYNC_MAIN_CONFIG_FILE', '/etc/corosync/corosync.conf')\n        lines = open(cfg).read().split('\\n')\n        name_re = re.compile(r'\\s*name:\\s+(.*)')\n        names = []\n        for line in lines:\n            name = name_re.match(line)\n            if name:\n                names.append(name.group(1))\n        return names\n    except Exception:\n        return []\n\n\ndef list_corosync_nodes():\n    '''\n    Returns list of nodes configured\n    in corosync.conf\n    '''\n    try:\n        cfg = os.getenv('COROSYNC_MAIN_CONFIG_FILE', '/etc/corosync/corosync.conf')\n        lines = open(cfg).read().split('\\n')\n        addr_re = re.compile(r'\\s*ring0_addr:\\s+(.*)')\n        nodes = []\n        for line in lines:\n            addr = addr_re.match(line)\n            if addr:\n                nodes.append(addr.group(1))\n        return nodes\n    except Exception:\n        return []\n\n\ndef print_cluster_nodes():\n    \"\"\"\n    Print the output of crm_node -l\n    \"\"\"\n    rc, out, _ = get_stdout_stderr(\"crm_node -l\")\n    if rc == 0 and out:\n        print(\"{}\\n\".format(out))\n\n\ndef list_cluster_nodes():\n    '''\n    Returns a list of nodes in the cluster.\n    '''\n    def getname(toks):\n        if toks and len(toks) >= 2:\n            return toks[1]\n        return None\n\n    try:\n        # when pacemaker running\n        rc, outp = stdout2list(['crm_node', '-l'], stderr_on=False, shell=False)\n        if rc == 0:\n            return [x for x in [getname(line.split()) for line in outp] if x and x != '(null)']\n\n        # when corosync running\n        ip_list = get_member_iplist()\n        if ip_list:\n            return ip_list\n\n        # static situation\n        cib_path = os.getenv('CIB_file', '/var/lib/pacemaker/cib/cib.xml')\n        if not os.path.isfile(cib_path):\n            return None\n        from . import xmlutil\n        node_list = []\n        cib = xmlutil.file2cib_elem(cib_path)\n        if cib is None:\n            return None\n        for node in cib.xpath('/cib/configuration/nodes/node'):\n            name = node.get('uname') or node.get('id')\n            if node.get('type') == 'remote':\n                srv = cib.xpath(\"//primitive[@id='%s']/instance_attributes/nvpair[@name='server']\" % (name))\n                if srv:\n                    continue\n            node_list.append(name)\n        return node_list\n    except OSError as msg:\n        raise ValueError(\"Error listing cluster nodes: %s\" % (msg))\n\n\ndef cluster_run_cmd(cmd):\n    \"\"\"\n    Run cmd in cluster nodes\n    \"\"\"\n    node_list = list_cluster_nodes()\n    if not node_list:\n        raise ValueError(\"Failed to get node list from cluster\")\n    parallax.parallax_call(node_list, cmd)\n\n\ndef list_cluster_nodes_except_me():\n    \"\"\"\n    Get cluster node list and filter out self\n    \"\"\"\n    node_list = list_cluster_nodes()\n    if not node_list:\n        raise ValueError(\"Failed to get node list from cluster\")\n    me = this_node()\n    if me in node_list:\n        node_list.remove(me)\n    return node_list\n\n\ndef service_info(name):\n    p = is_program('systemctl')\n    if p:\n        rc, outp = get_stdout([p, 'show',\n                               '-p', 'UnitFileState',\n                               '-p', 'ActiveState',\n                               '-p', 'SubState',\n                               name + '.service'], shell=False)\n        if rc == 0:\n            info = []\n            for line in outp.split('\\n'):\n                data = line.split('=', 1)\n                if len(data) == 2:\n                    info.append(data[1].strip())\n            return '/'.join(info)\n    return None\n\n\ndef running_on(resource):\n    \"returns list of node names where the given resource is running\"\n    rsc_locate = \"crm_resource --resource '%s' --locate\"\n    rc, out, err = get_stdout_stderr(rsc_locate % (resource))\n    if rc != 0:\n        return []\n    nodes = []\n    head = \"resource %s is running on: \" % (resource)\n    for line in out.split('\\n'):\n        if line.strip().startswith(head):\n            w = line[len(head):].split()\n            if w:\n                nodes.append(w[0])\n    common_debug(\"%s running on: %s\" % (resource, nodes))\n    return nodes\n\n\n# This RE matches nvpair values that can\n# be left unquoted\n_NOQUOTES_RE = re.compile(r'^[\\w\\.-]+$')\n\n\ndef noquotes(v):\n    return _NOQUOTES_RE.match(v) is not None\n\n\ndef unquote(s):\n    \"\"\"\n    Reverse shell-quoting a string, so the string '\"a b c\"'\n    becomes 'a b c'\n    \"\"\"\n    sp = shlex.split(s)\n    if sp:\n        return sp[0]\n    return \"\"\n\n\ndef parse_sysconfig(sysconfig_file):\n    \"\"\"\n    Reads a sysconfig file into a dict\n    \"\"\"\n    ret = {}\n    if os.path.isfile(sysconfig_file):\n        for line in open(sysconfig_file).readlines():\n            if line.lstrip().startswith('#'):\n                continue\n            try:\n                key, val = line.split(\"=\", 1)\n                ret[key] = unquote(val)\n            except ValueError:\n                pass\n    return ret\n\n\ndef sysconfig_set(sysconfig_file, **values):\n    \"\"\"\n    Set the values in the sysconfig file, updating the variables\n    if they exist already, appending them if not.\n    \"\"\"\n    outp = \"\"\n    if os.path.isfile(sysconfig_file):\n        for line in open(sysconfig_file).readlines():\n            if line.lstrip().startswith('#'):\n                outp += line\n            else:\n                matched = False\n                try:\n                    key, _ = line.split(\"=\", 1)\n                    for k, v in values.items():\n                        if k == key:\n                            matched = True\n                            outp += '%s=%s\\n' % (k, doublequote(v))\n                            del values[k]\n                            break\n                    if not matched:\n                        outp += line\n                except ValueError:\n                    outp += line\n\n    for k, v in values.items():\n        outp += '%s=%s\\n' % (k, doublequote(v))\n    str2file(outp, sysconfig_file)\n\n\ndef remote_diff_slurp(nodes, filename):\n    try:\n        import parallax\n    except ImportError:\n        raise ValueError(\"Parallax is required to diff\")\n    from . import tmpfiles\n\n    tmpdir = tmpfiles.create_dir()\n    opts = parallax.Options()\n    opts.localdir = tmpdir\n    dst = os.path.basename(filename)\n    return list(parallax.slurp(nodes, filename, dst, opts).items())\n\n\ndef remote_diff_this(local_path, nodes, this_node):\n    try:\n        import parallax\n    except ImportError:\n        raise ValueError(\"Parallax is required to diff\")\n\n    by_host = remote_diff_slurp(nodes, local_path)\n    for host, result in by_host:\n        if isinstance(result, parallax.Error):\n            raise ValueError(\"Failed on %s: %s\" % (host, str(result)))\n        _, _, _, path = result\n        _, s = get_stdout(\"diff -U 0 -d -b --label %s --label %s %s %s\" %\n                          (host, this_node, path, local_path))\n        page_string(s)\n\n\ndef remote_diff(local_path, nodes):\n    try:\n        import parallax\n    except ImportError:\n        raise ValueError(\"parallax is required to diff\")\n\n    by_host = remote_diff_slurp(nodes, local_path)\n    for host, result in by_host:\n        if isinstance(result, parallax.Error):\n            raise ValueError(\"Failed on %s: %s\" % (host, str(result)))\n    h1, r1 = by_host[0]\n    h2, r2 = by_host[1]\n    _, s = get_stdout(\"diff -U 0 -d -b --label %s --label %s %s %s\" %\n                      (h1, h2, r1[3], r2[3]))\n    page_string(s)\n\n\ndef remote_checksum(local_path, nodes, this_node):\n    try:\n        import parallax\n    except ImportError:\n        raise ValueError(\"Parallax is required to diff\")\n    import hashlib\n\n    by_host = remote_diff_slurp(nodes, local_path)\n    for host, result in by_host:\n        if isinstance(result, parallax.Error):\n            raise ValueError(str(result))\n\n    print(\"%-16s  SHA1 checksum of %s\" % ('Host', local_path))\n    if this_node not in nodes:\n        print(\"%-16s: %s\" % (this_node, hashlib.sha1(open(local_path).read()).hexdigest()))\n    for host, result in by_host:\n        _, _, _, path = result\n        print(\"%-16s: %s\" % (host, hashlib.sha1(open(path).read()).hexdigest()))\n\n\ndef cluster_copy_file(local_path, nodes=None):\n    \"\"\"\n    Copies given file to all other cluster nodes.\n    \"\"\"\n    try:\n        import parallax\n    except ImportError:\n        raise ValueError(\"parallax is required to copy cluster files\")\n    if not nodes:\n        nodes = list_cluster_nodes()\n        nodes.remove(this_node())\n    opts = parallax.Options()\n    opts.timeout = 60\n    opts.ssh_options += ['ControlPersist=no']\n    ok = True\n    for host, result in parallax.copy(nodes,\n                                      local_path,\n                                      local_path, opts).items():\n        if isinstance(result, parallax.Error):\n            err_buf.error(\"Failed to push %s to %s: %s\" % (local_path, host, result))\n            ok = False\n        else:\n            err_buf.ok(host)\n    return ok\n\n\n# a set of fnmatch patterns to match attributes whose values\n# should be obscured as a sequence of **** when printed\n_obscured_nvpairs = []\n\n\ndef obscured(key, value):\n    if key is not None and value is not None:\n        for o in _obscured_nvpairs:\n            if fnmatch.fnmatch(key, o):\n                return '*' * 6\n    return value\n\n\n@contextmanager\ndef obscure(obscure_list):\n    global _obscured_nvpairs\n    prev = _obscured_nvpairs\n    _obscured_nvpairs = obscure_list\n    try:\n        yield\n    finally:\n        _obscured_nvpairs = prev\n\n\ndef gen_nodeid_from_ipv6(addr):\n    return int(ipaddress.ip_address(addr)) % 1000000000\n\n\n# Set by detect_cloud()\n# to avoid multiple requests\n_ip_for_cloud = None\n\n\ndef _cloud_metadata_request(uri, headers={}):\n    try:\n        import urllib2 as urllib\n    except ImportError:\n        import urllib.request as urllib\n    req = urllib.Request(uri)\n    for header, value in headers.items():\n        req.add_header(header, value)\n    try:\n        resp = urllib.urlopen(req, timeout=5)\n        content = resp.read()\n        if type(content) != str:\n            return content.decode('utf-8').strip()\n        return content.strip()\n    except urllib.URLError:\n        return None\n\n\n@memoize\ndef detect_cloud():\n    \"\"\"\n    Tries to determine which (if any) cloud environment\n    the cluster is running on.\n\n    This is mainly done using dmidecode.\n\n    If the host cannot be determined, this function\n    returns None. Otherwise, it returns a string\n    identifying the platform.\n\n    These are the currently known platforms:\n\n    * amazon-web-services\n    * microsoft-azure\n    * google-cloud-platform\n\n    \"\"\"\n    global _ip_for_cloud\n\n    if not is_program(\"dmidecode\"):\n        return None\n    rc, system_version = get_stdout(\"dmidecode -s system-version\")\n    if re.search(r\".*amazon.*\", system_version) is not None:\n        return \"amazon-web-services\"\n    if rc != 0:\n        return None\n    rc, system_manufacturer = get_stdout(\"dmidecode -s system-manufacturer\")\n    if rc == 0 and \"microsoft corporation\" in system_manufacturer.lower():\n        # To detect azure we also need to make an API request\n        result = _cloud_metadata_request(\n            \"http://169.254.169.254/metadata/instance/network/interface/0/ipv4/ipAddress/0/privateIpAddress?api-version=2017-08-01&format=text\",\n            headers={\"Metadata\": \"true\"})\n        if result:\n            _ip_for_cloud = result\n            return \"microsoft-azure\"\n    rc, bios_vendor = get_stdout(\"dmidecode -s bios-vendor\")\n    if rc == 0 and \"Google\" in bios_vendor:\n        # To detect GCP we also need to make an API request\n        result = _cloud_metadata_request(\n            \"http://metadata.google.internal/computeMetadata/v1/instance/network-interfaces/0/ip\",\n            headers={\"Metadata-Flavor\": \"Google\"})\n        if result:\n            _ip_for_cloud = result\n            return \"google-cloud-platform\"\n    return None\n\n\ndef debug_timestamp():\n    return datetime.datetime.now().strftime('%Y/%m/%d %H:%M:%S')\n\n\ndef get_member_iplist():\n    rc, out, err= get_stdout_stderr(\"corosync-cmapctl -b runtime.totem.pg.mrp.srp.members\")\n    if rc != 0:\n        common_debug(err)\n        return None\n\n    ip_list = []\n    for line in out.split('\\n'):\n        match = re.search(r'ip\\((.*?)\\)', line)\n        if match:\n            ip_list.append(match.group(1))\n    return ip_list\n\n\ndef get_iplist_corosync_using():\n    \"\"\"\n    Get ip list used by corosync\n    \"\"\"\n    rc, out, err = get_stdout_stderr(\"corosync-cfgtool -s\")\n    if rc != 0:\n        raise ValueError(err)\n    return re.findall(r'id\\s*=\\s*(.*)', out)\n\n\ndef check_ssh_passwd_need(host, user=\"root\"):\n    \"\"\"\n    Check whether access to host need password\n    \"\"\"\n    ssh_options = \"-o StrictHostKeyChecking=no -o EscapeChar=none -o ConnectTimeout=15\"\n    ssh_cmd = \"ssh {} -T -o Batchmode=yes {} true\".format(ssh_options, host)\n    ssh_cmd = add_su(ssh_cmd, user)\n    rc, _, _ = get_stdout_stderr(ssh_cmd)\n    return rc != 0\n\n\ndef check_port_open(ip, port):\n    import socket\n\n    family = socket.AF_INET6 if IP.is_ipv6(ip) else socket.AF_INET\n    with closing(socket.socket(family, socket.SOCK_STREAM)) as sock:\n        if sock.connect_ex((ip, port)) == 0:\n            return True\n        else:\n            return False\n\n\ndef valid_port(port):\n    return int(port) >= 1024 and int(port) <= 65535\n\n\ndef is_qdevice_configured():\n    from . import corosync\n    return corosync.get_value(\"quorum.device.model\") == \"net\"\n\n\ndef is_qdevice_tls_on():\n    from . import corosync\n    return corosync.get_value(\"quorum.device.net.tls\") == \"on\"\n\n\ndef get_nodeinfo_from_cmaptool():\n    nodeid_ip_dict = {}\n    rc, out = get_stdout(\"corosync-cmapctl -b runtime.totem.pg.mrp.srp.members\")\n    if rc != 0:\n        return nodeid_ip_dict\n\n    for line in out.split('\\n'):\n        match = re.search(r'members\\.(.*)\\.ip', line)\n        if match:\n            node_id = match.group(1)\n            iplist = re.findall(r'[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}', line)\n            nodeid_ip_dict[node_id] = iplist\n    return nodeid_ip_dict\n\n\ndef get_iplist_from_name(name):\n    \"\"\"\n    Given node host name, return this host's ip list in corosync cmap\n    \"\"\"\n    ip_list = []\n    nodeid = get_nodeid_from_name(name)\n    if not nodeid:\n        return ip_list\n    nodeinfo = {}\n    nodeinfo = get_nodeinfo_from_cmaptool()\n    if not nodeinfo:\n        return ip_list\n    return nodeinfo[nodeid]\n\n\ndef valid_nodeid(nodeid):\n    from . import bootstrap\n    if not service_is_active('corosync.service'):\n        return False\n\n    for _id, _ in get_nodeinfo_from_cmaptool().items():\n        if _id == nodeid:\n            return True\n    return False\n\n\ndef get_nodeid_from_name(name):\n    rc, out = get_stdout('crm_node -l')\n    if rc != 0:\n        return None\n    res = re.search(r'^([0-9]+) {} '.format(name), out, re.M)\n    if res:\n        return res.group(1)\n    else:\n        return None\n\n\ndef check_space_option_value(options):\n    if not isinstance(options, argparse.Namespace):\n        raise ValueError(\"Expected type of \\\"options\\\" is \\\"argparse.Namespace\\\", not \\\"{}\\\"\".format(type(options)))\n\n    for opt in vars(options):\n        value = getattr(options, opt)\n        if isinstance(value, str) and len(value.strip()) == 0:\n            raise ValueError(\"Space value not allowed for dest \\\"{}\\\"\".format(opt))\n\n\ndef interface_choice():\n    _, out = get_stdout(\"ip a\")\n    # should consider interface format like \"ethx@xxx\"\n    interface_list = re.findall(r'(?:[0-9]+:) (.*?)(?=: |@.*?: )', out)\n    return [nic for nic in interface_list if nic != \"lo\"]\n\n\nclass IP(object):\n    \"\"\"\n    Class to get some properties of IP address\n    \"\"\"\n\n    def __init__(self, addr):\n        \"\"\"\n        Init function\n        \"\"\"\n        self.addr = addr\n\n    @property\n    def ip_address(self):\n        \"\"\"\n        Create ipaddress instance\n        \"\"\"\n        return ipaddress.ip_address(self.addr)\n\n    @property\n    def version(self):\n        \"\"\"\n        Get IP address version\n        \"\"\"\n        return self.ip_address.version\n\n    @classmethod\n    def is_mcast(cls, addr):\n        \"\"\"\n        Check whether the address is multicast address\n        \"\"\"\n        cls_inst = cls(addr)\n        return cls_inst.ip_address.is_multicast\n\n    @classmethod\n    def is_ipv6(cls, addr):\n        \"\"\"\n        Check whether the address is IPV6 address\n        \"\"\"\n        return cls(addr).version == 6\n\n    @classmethod\n    def is_valid_ip(cls, addr):\n        \"\"\"\n        Check whether the address is valid IP address\n        \"\"\"\n        cls_inst = cls(addr)\n        try:\n            cls_inst.ip_address\n        except ValueError:\n            return False\n        else:\n            return True\n\n    @property\n    def is_loopback(self):\n        \"\"\"\n        Check whether the address is loopback address\n        \"\"\"\n        return self.ip_address.is_loopback\n\n    @property\n    def is_link_local(self):\n        \"\"\"\n        Check whether the address is link-local address\n        \"\"\"\n        return self.ip_address.is_link_local\n\n\nclass Interface(IP):\n    \"\"\"\n    Class to get information from one interface\n    \"\"\"\n\n    def __init__(self, ip_with_mask):\n        \"\"\"\n        Init function\n        \"\"\"\n        self.ip, self.mask = ip_with_mask.split('/')\n        super(__class__, self).__init__(self.ip)\n\n    @property\n    def ip_with_mask(self):\n        \"\"\"\n        Get ip with netmask\n        \"\"\"\n        return '{}/{}'.format(self.ip, self.mask)\n\n    @property\n    def ip_interface(self):\n        \"\"\"\n        Create ip_interface instance\n        \"\"\"\n        return ipaddress.ip_interface(self.ip_with_mask)\n\n    @property\n    def network(self):\n        \"\"\"\n        Get network address\n        \"\"\"\n        return str(self.ip_interface.network.network_address)\n\n    def ip_in_network(self, addr):\n        \"\"\"\n        Check whether the addr in the network\n        \"\"\"\n        return IP(addr).ip_address in self.ip_interface.network\n\n\nclass InterfacesInfo(object):\n    \"\"\"\n    Class to collect interfaces information on local node\n    \"\"\"\n\n    def __init__(self, ipv6=False, second_heartbeat=False, custom_nic_list=[]):\n        \"\"\"\n        Init function\n\n        On init process,\n        \"ipv6\" is provided by -I option\n        \"second_heartbeat\" is provided by -M option\n        \"custom_nic_list\" is provided by -i option\n        \"\"\"\n        self.ip_version = 6 if ipv6 else 4\n        self.second_heartbeat = second_heartbeat\n        self._default_nic_list = custom_nic_list\n        self._nic_info_dict = {}\n\n    def get_interfaces_info(self):\n        \"\"\"\n        Try to get interfaces info dictionary via \"ip\" command\n\n        IMPORTANT: This is the method that populates the data, should always be called after initialize\n        \"\"\"\n        cmd = \"ip -{} -o addr show\".format(self.ip_version)\n        rc, out, err = get_stdout_stderr(cmd)\n        if rc != 0:\n            raise ValueError(err)\n\n        # format on each line will like:\n        # 2: enp1s0    inet 192.168.122.241/24 brd 192.168.122.255 scope global enp1s0\\       valid_lft forever preferred_lft forever\n        for line in out.splitlines():\n            _, nic, _, ip_with_mask, *_ = line.split()\n            # maybe from tun interface\n            if not '/' in ip_with_mask:\n                continue\n            #TODO change this condition when corosync support link-local address\n            interface_inst = Interface(ip_with_mask)\n            if interface_inst.is_loopback or interface_inst.is_link_local:\n                continue\n            # one nic might configured multi IP addresses\n            if nic not in self._nic_info_dict:\n                self._nic_info_dict[nic] = []\n            self._nic_info_dict[nic].append(interface_inst)\n\n        if not self._nic_info_dict:\n            raise ValueError(\"No address configured\")\n        if self.second_heartbeat and len(self._nic_info_dict) == 1:\n            raise ValueError(\"Cannot configure second heartbeat, since only one address is available\")\n\n    @property\n    def nic_list(self):\n        \"\"\"\n        Get interfaces name list\n        \"\"\"\n        return list(self._nic_info_dict.keys())\n\n    @property\n    def interface_list(self):\n        \"\"\"\n        Get instance list of class Interface\n        \"\"\"\n        _interface_list = []\n        for interface in self._nic_info_dict.values():\n            _interface_list.extend(interface)\n        return _interface_list\n\n    @property\n    def ip_list(self):\n        \"\"\"\n        Get IP address list\n        \"\"\"\n        return [interface.ip for interface in self.interface_list]\n\n    @classmethod\n    def get_local_ip_list(cls, is_ipv6):\n        \"\"\"\n        Get IP address list\n        \"\"\"\n        cls_inst = cls(is_ipv6)\n        cls_inst.get_interfaces_info()\n        return cls_inst.ip_list\n\n    @classmethod\n    def ip_in_local(cls, addr):\n        \"\"\"\n        Check whether given address was in one of local address\n        \"\"\"\n        cls_inst = cls(IP.is_ipv6(addr))\n        cls_inst.get_interfaces_info()\n        return addr in cls_inst.ip_list\n\n    @property\n    def network_list(self):\n        \"\"\"\n        Get network list\n        \"\"\"\n        return list(set([interface.network for interface in self.interface_list]))\n\n    def _nic_first_ip(self, nic):\n        \"\"\"\n        Get the first IP of specific nic\n        \"\"\"\n        return self._nic_info_dict[nic][0].ip\n\n    def get_default_nic_list_from_route(self):\n        \"\"\"\n        Get default nic list from route\n        \"\"\"\n        if self._default_nic_list:\n            return self._default_nic_list\n\n        #TODO what if user only has ipv6 route?\n        cmd = \"ip -o route show\"\n        rc, out, err = get_stdout_stderr(cmd)\n        if rc != 0:\n            raise ValueError(err)\n        res = re.search(r'^default via .* dev (.*?) ', out)\n        if res:\n            self._default_nic_list = [res.group(1)]\n        else:\n            if not self.nic_list:\n                self.get_interfaces_info()\n            common_warn(\"No default route configured. Using the first found nic\")\n            self._default_nic_list = [self.nic_list[0]]\n        return self._default_nic_list\n\n    def get_default_ip_list(self):\n        \"\"\"\n        Get default IP list will be used by corosync\n        \"\"\"\n        if not self._default_nic_list:\n            self.get_default_nic_list_from_route()\n        if not self.nic_list:\n            self.get_interfaces_info()\n\n        _ip_list = []\n        for nic in self._default_nic_list:\n            # in case given interface not exist\n            if nic not in self.nic_list:\n                raise ValueError(\"Failed to detect IP address for {}\".format(nic))\n            _ip_list.append(self._nic_first_ip(nic))\n        # in case -M specified but given one interface via -i\n        if self.second_heartbeat and len(self._default_nic_list) == 1:\n            for nic in self.nic_list:\n                if nic not in self._default_nic_list:\n                    _ip_list.append(self._nic_first_ip(nic))\n                    break\n        return _ip_list\n\n    @classmethod\n    def ip_in_network(cls, addr):\n        \"\"\"\n        Check whether given address was in one of local networks\n        \"\"\"\n        cls_inst = cls(IP.is_ipv6(addr))\n        cls_inst.get_interfaces_info()\n        for interface_inst in cls_inst.interface_list:\n            if interface_inst.ip_in_network(addr):\n                return True\n        return False\n\n\ndef check_file_content_included(source_file, target_file):\n    \"\"\"\n    Check whether target_file includes contents of source_file\n    \"\"\"\n    if not os.path.exists(source_file):\n        raise ValueError(\"File {} not exist\".format(source_file))\n    if not os.path.exists(target_file):\n        return False\n\n    with open(target_file, 'r') as target_fd:\n        target_data = target_fd.read()\n    with open(source_file, 'r') as source_fd:\n        source_data = source_fd.read()\n    return source_data in target_data\n\n\nclass ServiceManager(object):\n    \"\"\"\n    Class to manage systemctl services\n    \"\"\"\n    ACTION_MAP = {\n            \"enable\": \"enable\",\n            \"disable\": \"disable\",\n            \"start\": \"start\",\n            \"stop\": \"stop\",\n            \"is_enabled\": \"is-enabled\",\n            \"is_active\": \"is-active\",\n            \"is_available\": \"list-unit-files\"\n            }\n\n    def __init__(self, service_name, remote_addr=None):\n        \"\"\"\n        Init function\n        \"\"\"\n        self.service_name = service_name\n        self.remote_addr = remote_addr\n\n    def _do_action(self, action_type):\n        \"\"\"\n        Actual do actions to manage service\n        \"\"\"\n        if action_type not in self.ACTION_MAP.values():\n            raise ValueError(\"status_type should be {}\".format('/'.join(list(self.ACTION_MAP.values()))))\n\n        cmd = \"systemctl {} {}\".format(action_type, self.service_name)\n        if self.remote_addr:\n            prompt_msg = \"Run \\\"{}\\\" on {}\".format(cmd, self.remote_addr)\n            rc, output, err = run_cmd_on_remote(cmd, self.remote_addr, prompt_msg)\n        else:\n            rc, output, err = get_stdout_stderr(cmd)\n        if rc != 0 and err:\n            raise ValueError(\"Run \\\"{}\\\" error: {}\".format(cmd, err))\n        return rc == 0, output\n\n    @property\n    def is_available(self):\n        return self.service_name in self._do_action(self.ACTION_MAP[\"is_available\"])[1]\n\n    @property\n    def is_enabled(self):\n        return self._do_action(self.ACTION_MAP[\"is_enabled\"])[0]\n\n    @property\n    def is_active(self):\n        return self._do_action(self.ACTION_MAP[\"is_active\"])[0]\n\n    def start(self):\n        self._do_action(self.ACTION_MAP[\"start\"])\n\n    def stop(self):\n        self._do_action(self.ACTION_MAP[\"stop\"])\n\n    def enable(self):\n        self._do_action(self.ACTION_MAP[\"enable\"])\n\n    def disable(self):\n        self._do_action(self.ACTION_MAP[\"disable\"])\n\n    @classmethod\n    def service_is_available(cls, name, remote_addr=None):\n        \"\"\"\n        Check whether service is available\n        \"\"\"\n        inst = cls(name, remote_addr)\n        return inst.is_available\n\n    @classmethod\n    def service_is_enabled(cls, name, remote_addr=None):\n        \"\"\"\n        Check whether service is enabled\n        \"\"\"\n        inst = cls(name, remote_addr)\n        return inst.is_enabled\n\n    @classmethod\n    def service_is_active(cls, name, remote_addr=None):\n        \"\"\"\n        Check whether service is active\n        \"\"\"\n        inst = cls(name, remote_addr)\n        return inst.is_active\n\n    @classmethod\n    def start_service(cls, name, enable=False, remote_addr=None):\n        \"\"\"\n        Start service\n        \"\"\"\n        inst = cls(name, remote_addr)\n        if enable:\n            inst.enable()\n        inst.start()\n\n    @classmethod\n    def stop_service(cls, name, disable=False, remote_addr=None):\n        \"\"\"\n        Stop service\n        \"\"\"\n        inst = cls(name, remote_addr)\n        if disable:\n            inst.disable()\n        inst.stop()\n\n    @classmethod\n    def enable_service(cls, name, remote_addr=None):\n        \"\"\"\n        Enable service\n        \"\"\"\n        inst = cls(name, remote_addr)\n        if inst.is_available and not inst.is_enabled:\n            inst.enable()\n\n    @classmethod\n    def disable_service(cls, name, remote_addr=None):\n        \"\"\"\n        Disable service\n        \"\"\"\n        inst = cls(name, remote_addr)\n        if inst.is_available and inst.is_enabled:\n            inst.disable()\n\n\nservice_is_available = ServiceManager.service_is_available\nservice_is_enabled = ServiceManager.service_is_enabled\nservice_is_active = ServiceManager.service_is_active\nstart_service = ServiceManager.start_service\nstop_service = ServiceManager.stop_service\nenable_service = ServiceManager.enable_service\ndisable_service = ServiceManager.disable_service\n\n\ndef package_is_installed(pkg, remote_addr=None):\n    \"\"\"\n    Check if package is installed\n    \"\"\"\n    cmd = \"rpm -q --quiet {}\".format(pkg)\n    if remote_addr:\n        # check on remote\n        prompt_msg = \"Check whether {} is installed on {}\".format(pkg, remote_addr)\n        rc, _, _ = run_cmd_on_remote(cmd, remote_addr, prompt_msg)\n    else:\n        # check on local\n        rc, _ = get_stdout(cmd)\n    return rc == 0\n\n\ndef ping_node(node):\n    \"\"\"\n    Check if the remote node is reachable\n    \"\"\"\n    rc, _, err = get_stdout_stderr(\"ping -c 1 {}\".format(node))\n    if rc != 0:\n        raise ValueError(\"host \\\"{}\\\" is unreachable: {}\".format(node, err))\n# vim:ts=4:sw=4:et:\n"], "buggy_code_start_loc": [33, 344], "buggy_code_end_loc": [2491, 2092], "fixing_code_start_loc": [34, 345], "fixing_code_end_loc": [2542, 2103], "type": "CWE-77", "message": "An issue was discovered in ClusterLabs Hawk (aka HA Web Konsole) through 2.3.0-15. It ships the binary hawk_invoke (built from tools/hawk_invoke.c), intended to be used as a setuid program. This allows the hacluster user to invoke certain commands as root (with an attempt to limit this to safe combinations). This user is able to execute an interactive \"shell\" that isn't limited to the commands specified in hawk_invoke, allowing escalation to root.", "other": {"cve": {"id": "CVE-2021-3020", "sourceIdentifier": "cve@mitre.org", "published": "2022-08-26T00:15:08.773", "lastModified": "2022-09-02T13:29:03.253", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "An issue was discovered in ClusterLabs Hawk (aka HA Web Konsole) through 2.3.0-15. It ships the binary hawk_invoke (built from tools/hawk_invoke.c), intended to be used as a setuid program. This allows the hacluster user to invoke certain commands as root (with an attempt to limit this to safe combinations). This user is able to execute an interactive \"shell\" that isn't limited to the commands specified in hawk_invoke, allowing escalation to root."}, {"lang": "es", "value": "Se ha detectado un problema en ClusterLabs Hawk (tambi\u00e9n se conoce como HA Web Konsole) hasta la versi\u00f3n 2.3.0-15. Es enviado el binario hawk_invoke (construido a partir de tools/hawk_invoke.c), destinado a ser usado como un programa setuid. Esto permite al usuario hacluster invocar determinados comandos como root (con un intento de limitar esto a combinaciones seguras). Este usuario es capaz de ejecutar un \"shell\" interactivo que no est\u00e1 limitado a los comandos especificados en hawk_invoke, permitiendo una escalada a root."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 8.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 2.8, "impactScore": 5.9}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-77"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:clusterlabs:hawk:*:*:*:*:*:*:*:*", "versionEndIncluding": "2.3.0-15", "matchCriteriaId": "BB239D06-C0CE-4BAB-9974-D872E9D3E167"}]}]}], "references": [{"url": "https://bugzilla.suse.com/show_bug.cgi?id=1180571", "source": "cve@mitre.org", "tags": ["Permissions Required"]}, {"url": "https://github.com/ClusterLabs/crmsh/commit/c538024b8ebd138dc373b005189471d9b77e9c82", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/ClusterLabs/hawk/releases", "source": "cve@mitre.org", "tags": ["Release Notes", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/ClusterLabs/crmsh/commit/c538024b8ebd138dc373b005189471d9b77e9c82"}}