{"buggy_code": ["/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include \"tensorflow/c/eager/dlpack.h\"\n\n#include \"include/dlpack/dlpack.h\"  // from @dlpack\n#include \"tensorflow/c/eager/c_api.h\"\n#include \"tensorflow/c/eager/c_api_experimental.h\"\n#include \"tensorflow/c/eager/tfe_tensorhandle_internal.h\"\n#include \"tensorflow/c/tf_status_internal.h\"\n#include \"tensorflow/core/common_runtime/eager/tensor_handle.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/tensor_reference.h\"\n#include \"tensorflow/core/platform/logging.h\"\n\nnamespace tensorflow {\n\nnamespace {\n\n// Managing context for the DLManagedTensor, will manage the lifetime of\n// DLManagedTensor. When calling DLManagedTensor::deleter, it will notify the\n// original framework of destruction, and this context will be deleted also.\nstruct TfDlManagedTensorCtx {\n  TensorReference reference;\n  std::vector<int64_t> shape;\n  std::vector<int64_t> strides;\n  DLManagedTensor tensor;\n\n  explicit TfDlManagedTensorCtx(const TensorReference& ref) : reference(ref) {}\n};\n\n// Gets tensor from eager tensor handle.\nconst Tensor* GetTensorFromHandle(TFE_TensorHandle* h, TF_Status* status) {\n  if (h == nullptr) {\n    status->status = tensorflow::errors::InvalidArgument(\"Invalid handle\");\n    return nullptr;\n  }\n  tensorflow::TensorHandle* handle =\n      tensorflow::TensorHandleFromInterface(tensorflow::unwrap(h));\n  if (handle->Type() != TensorHandle::LOCAL) {\n    status->status = tensorflow::errors::InvalidArgument(\n        \"DLPack doesn't support \", handle->TypeString(), \" tensor\");\n    return nullptr;\n  }\n  const tensorflow::Tensor* tensor;\n  status->status = handle->Tensor(&tensor);\n  if (!status->status.ok()) {\n    return nullptr;\n  }\n  return tensor;\n}\n\n// Deleter for DLManagedTensor\nvoid DLManagedTensorDeleter(DLManagedTensor* arg) {\n  TfDlManagedTensorCtx* owner =\n      static_cast<TfDlManagedTensorCtx*>(arg->manager_ctx);\n  owner->reference.Unref();\n  delete owner;\n}\n\n// Converts TF_DATAType to DLPack data type.\nDLDataType GetDlDataType(TF_DataType data_type, TF_Status* status) {\n  DLDataType dtype;\n  dtype.lanes = 1;\n  dtype.bits = TF_DataTypeSize(data_type) * 8;\n  switch (data_type) {\n    case TF_DataType::TF_HALF:\n    case TF_DataType::TF_FLOAT:\n    case TF_DataType::TF_DOUBLE:\n      dtype.code = DLDataTypeCode::kDLFloat;\n      break;\n    case TF_DataType::TF_INT8:\n    case TF_DataType::TF_INT16:\n    case TF_DataType::TF_INT32:\n    case TF_DataType::TF_INT64:\n      dtype.code = DLDataTypeCode::kDLInt;\n      break;\n    case TF_DataType::TF_BOOL:\n    case TF_DataType::TF_UINT8:\n    case TF_DataType::TF_UINT16:\n    case TF_DataType::TF_UINT32:\n    case TF_DataType::TF_UINT64:\n      dtype.code = DLDataTypeCode::kDLUInt;\n      break;\n    case TF_DataType::TF_BFLOAT16:\n      dtype.code = DLDataTypeCode::kDLBfloat;\n      break;\n    default:\n      status->status = tensorflow::errors::InvalidArgument(\n          DataType_Name(static_cast<DataType>(data_type)),\n          \" is not supported by dlpack\");\n      break;\n  }\n  return dtype;\n}\n\n// Gets DLPack's DLContext from eager tensor handle.\nDLContext GetDlContext(TFE_TensorHandle* h, TF_Status* status) {\n  DLContext ctx;\n  const char* device_name =\n      tensorflow::unwrap(h)->BackingDeviceName(&status->status);\n  DeviceNameUtils::ParsedName parsed_name;\n  tensorflow::DeviceNameUtils::ParseFullName(device_name, &parsed_name);\n  std::string device_type = parsed_name.type;\n  int device_id = 0;\n  if (parsed_name.has_id) {\n    device_id = parsed_name.id;\n  }\n\n  ctx.device_id = device_id;\n  if (device_type == \"CPU\") {\n    ctx.device_type = DLDeviceType::kDLCPU;\n  } else if (device_type == \"GPU\") {\n    ctx.device_type = DLDeviceType::kDLGPU;\n  } else {\n    status->status = tensorflow::errors::InvalidArgument(\n        \"Unsupported Device Type for dlpack\");\n  }\n\n  return ctx;\n}\n\n// Converts DLContext to TF device name.\nabsl::optional<std::string> DeviceNameFromDlContext(const DLContext& ctx,\n                                                    TF_Status* status) {\n  switch (ctx.device_type) {\n    case DLDeviceType::kDLCPU:\n      return \"CPU:0\";\n    case DLDeviceType::kDLGPU:\n      return absl::StrCat(\"GPU:\", ctx.device_id);\n    default:\n      return absl::nullopt;\n  }\n}\n\n// Converts DLPack data type to TF_DATATYPE.\nStatus TfDataTypeFormDlDataType(const DLDataType& dtype,\n                                TF_DataType* tf_dtype) {\n  switch (dtype.code) {\n    case DLDataTypeCode::kDLUInt:\n      switch (dtype.bits) {\n        case 8:\n          *tf_dtype = TF_DataType::TF_UINT8;\n          return Status::OK();\n        case 16:\n          *tf_dtype = TF_DataType::TF_UINT16;\n          return Status::OK();\n        case 32:\n          *tf_dtype = TF_DataType::TF_UINT32;\n          return Status::OK();\n        case 64:\n          *tf_dtype = TF_DataType::TF_UINT64;\n          return Status::OK();\n        default:\n          return tensorflow::errors::InvalidArgument(\"Unsupported UInt bits: \",\n                                                     dtype.bits);\n      }\n      return Status::OK();\n    case DLDataTypeCode::kDLInt:\n      switch (dtype.bits) {\n        case 8:\n          *tf_dtype = TF_DataType::TF_INT8;\n          return Status::OK();\n        case 16:\n          *tf_dtype = TF_DataType::TF_INT16;\n          return Status::OK();\n        case 32:\n          *tf_dtype = TF_DataType::TF_INT32;\n          return Status::OK();\n        case 64:\n          *tf_dtype = TF_DataType::TF_INT64;\n          return Status::OK();\n        default:\n          return tensorflow::errors::InvalidArgument(\"Unsupported Int bits: \",\n                                                     dtype.bits);\n      }\n      return Status::OK();\n    case DLDataTypeCode::kDLFloat:\n      switch (dtype.bits) {\n        case 16:\n          *tf_dtype = TF_DataType::TF_HALF;\n          return Status::OK();\n        case 32:\n          *tf_dtype = TF_DataType::TF_FLOAT;\n          return Status::OK();\n        case 64:\n          *tf_dtype = TF_DataType::TF_DOUBLE;\n          return Status::OK();\n        default:\n          return tensorflow::errors::InvalidArgument(\"Unsupported Float bits: \",\n                                                     dtype.bits);\n      }\n      break;\n    case DLDataTypeCode::kDLBfloat:\n      switch (dtype.bits) {\n        case 16:\n          *tf_dtype = TF_DataType::TF_BFLOAT16;\n          return Status::OK();\n        default:\n          return tensorflow::errors::InvalidArgument(\n              \"Unsupported BFloat bits: \", dtype.bits);\n      }\n      break;\n    default:\n      return tensorflow::errors::InvalidArgument(\"Unsupported Type Codes: \",\n                                                 dtype.code);\n  }\n}\n\n// Wraps the deleter function of DLManagedTensor to match the function signature\n// TFE_NewTensorHandleFromDeviceMemory.\nvoid DeallocatorWrapperFunc(void* data, size_t len, void* dlmt_vptr) {\n  TFE_CallDLManagedTensorDeleter(dlmt_vptr);\n}\n\n// Checks whether the stride array matches the layout of compact, row-majored\n// data.\nbool IsValidStrideCompactRowMajorData(int64_t* shape_arr, int64_t* stride_arr,\n                                      int ndim) {\n  if (ndim >= 1 && stride_arr[ndim - 1] != 1) {\n    return false;\n  }\n  for (int i = ndim - 2; i >= 0; --i) {\n    if (stride_arr[i] != shape_arr[i + 1] * stride_arr[i + 1]) {\n      return false;\n    }\n  }\n  return true;\n}\n}  // namespace\n\nvoid TFE_CallDLManagedTensorDeleter(void* dlm_ptr) {\n  DLManagedTensor* dlMTensor = static_cast<DLManagedTensor*>(dlm_ptr);\n  if (dlMTensor->deleter != nullptr) {\n    dlMTensor->deleter(dlMTensor);\n  }\n}\n\nvoid* TFE_HandleToDLPack(TFE_TensorHandle* h, TF_Status* status) {\n  const Tensor* tensor = GetTensorFromHandle(h, status);\n  TF_DataType data_type = static_cast<TF_DataType>(tensor->dtype());\n  TensorReference tensor_ref(*tensor);  // This will call buf_->Ref()\n\n  auto* tf_dlm_tensor_ctx = new TfDlManagedTensorCtx(tensor_ref);\n  tf_dlm_tensor_ctx->reference = tensor_ref;\n\n  DLManagedTensor* dlm_tensor = &tf_dlm_tensor_ctx->tensor;\n  dlm_tensor->manager_ctx = tf_dlm_tensor_ctx;\n  dlm_tensor->deleter = &DLManagedTensorDeleter;\n  dlm_tensor->dl_tensor.ctx = GetDlContext(h, status);\n  int ndim = tensor->dims();\n  dlm_tensor->dl_tensor.ndim = ndim;\n  dlm_tensor->dl_tensor.data = TFE_TensorHandleDevicePointer(h, status);\n  dlm_tensor->dl_tensor.dtype = GetDlDataType(data_type, status);\n\n  std::vector<int64_t>* shape_arr = &tf_dlm_tensor_ctx->shape;\n  std::vector<int64_t>* stride_arr = &tf_dlm_tensor_ctx->strides;\n  shape_arr->resize(ndim);\n  stride_arr->resize(ndim, 1);\n  for (int i = 0; i < ndim; i++) {\n    (*shape_arr)[i] = tensor->dim_size(i);\n  }\n  for (int i = ndim - 2; i >= 0; --i) {\n    (*stride_arr)[i] = (*shape_arr)[i + 1] * (*stride_arr)[i + 1];\n  }\n\n  dlm_tensor->dl_tensor.shape = &(*shape_arr)[0];\n  // There are two ways to represent compact row-major data\n  // 1) nullptr indicates tensor is compact and row-majored.\n  // 2) fill in the strides array as the real case for compact row-major data.\n  // Here we choose option 2, since some frameworks didn't handle the strides\n  // argument properly.\n  dlm_tensor->dl_tensor.strides = &(*stride_arr)[0];\n  dlm_tensor->dl_tensor.byte_offset =\n      0;  // TF doesn't handle the strides and byte_offsets here\n  return static_cast<void*>(dlm_tensor);\n}\n\nTFE_TensorHandle* TFE_HandleFromDLPack(void* dlm, TF_Status* status,\n                                       TFE_Context* ctx) {\n  DLManagedTensor* dlmt = static_cast<DLManagedTensor*>(dlm);\n  DLTensor* dl_tensor = &dlmt->dl_tensor;\n  absl::optional<std::string> device_name =\n      DeviceNameFromDlContext(dl_tensor->ctx, status);\n  if (!device_name.has_value()) {\n    status->status =\n        tensorflow::errors::InvalidArgument(\"Unsupported Device Type\");\n    return nullptr;\n  }\n  TF_DataType dtype;\n  Status s = TfDataTypeFormDlDataType(dl_tensor->dtype, &dtype);\n  if (!s.ok()) {\n    status->status = std::move(s);\n    return nullptr;\n  }\n  int num_dims = dl_tensor->ndim;\n  const int64_t* dims = dl_tensor->shape;\n  void* data = dl_tensor->data;\n\n  size_t total_bytes = dl_tensor->dtype.bits / 8;\n  for (int i = 0; i < num_dims; i++) {\n    total_bytes *= dims[i];\n  }\n\n  if (dl_tensor->strides != nullptr &&\n      !IsValidStrideCompactRowMajorData(dl_tensor->shape, dl_tensor->strides,\n                                        num_dims)) {\n    status->status = tensorflow::errors::InvalidArgument(\n        \"Invalid strides array from DLPack\");\n    return nullptr;\n  }\n\n  TFE_TensorHandle* handle = TFE_NewTensorHandleFromDeviceMemory(\n      ctx, device_name.value().c_str(), dtype, dims, num_dims, data,\n      total_bytes, &DeallocatorWrapperFunc, dlmt, status);\n\n  return handle;\n}\n\n}  // namespace tensorflow\n", "load(\"//tensorflow:tensorflow.bzl\", \"cuda_py_test\")\n\npackage(\n    default_visibility = [\"//visibility:private\"],\n    licenses = [\"notice\"],  # Apache 2.0\n)\n\npy_library(\n    name = \"dlpack\",\n    srcs = [\"dlpack.py\"],\n    srcs_version = \"PY2AND3\",\n    visibility = [\"//tensorflow:__subpackages__\"],\n    deps = [\n        \"//tensorflow/python:pywrap_tensorflow\",\n    ],\n)\n\ncuda_py_test(\n    name = \"dlpack_test\",\n    srcs = [\"dlpack_test.py\"],\n    srcs_version = \"PY2AND3\",\n    tags = [\"noasan\"],  # TODO(b/159774807)\n    deps = [\n        \":dlpack\",\n        \"//tensorflow/python/eager:test\",\n        \"@absl_py//absl/testing:absltest\",\n        \"@absl_py//absl/testing:parameterized\",\n    ],\n)\n", "# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for DLPack functions.\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nimport numpy as np\n\nfrom tensorflow.python.dlpack import dlpack\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.platform import test\nfrom tensorflow.python.ops import array_ops\n\nint_dtypes = [\n    np.int8, np.int16, np.int32, np.int64, np.uint8, np.uint16, np.uint32,\n    np.uint64\n]\nfloat_dtypes = [np.float16, np.float32, np.float64]\ncomplex_dtypes = [np.complex64, np.complex128]\ndlpack_dtypes = int_dtypes + float_dtypes + [dtypes.bfloat16]\n\ntestcase_shapes = [(), (1,), (2, 3), (2, 0), (0, 7), (4, 1, 2)]\n\n\ndef FormatShapeAndDtype(shape, dtype):\n  return \"_{}[{}]\".format(str(dtype), \",\".join(map(str, shape)))\n\n\ndef GetNamedTestParameters():\n  result = []\n  for dtype in dlpack_dtypes:\n    for shape in testcase_shapes:\n      result.append({\n          \"testcase_name\": FormatShapeAndDtype(shape, dtype),\n          \"dtype\": dtype,\n          \"shape\": shape\n      })\n  return result\n\n\nclass DLPackTest(parameterized.TestCase, test.TestCase):\n\n  @parameterized.named_parameters(GetNamedTestParameters())\n  def testRoundTrip(self, dtype, shape):\n    np.random.seed(42)\n    np_array = np.random.randint(0, 10, shape)\n    # copy to gpu if available\n    tf_tensor = array_ops.identity(constant_op.constant(np_array, dtype=dtype))\n    tf_tensor_device = tf_tensor.device\n    tf_tensor_dtype = tf_tensor.dtype\n    dlcapsule = dlpack.to_dlpack(tf_tensor)\n    del tf_tensor  # should still work\n    tf_tensor2 = dlpack.from_dlpack(dlcapsule)\n    self.assertAllClose(np_array, tf_tensor2)\n    if tf_tensor_dtype == dtypes.int32:\n      # int32 tensor is always on cpu for now\n      self.assertEqual(tf_tensor2.device,\n                       \"/job:localhost/replica:0/task:0/device:CPU:0\")\n    else:\n      self.assertEqual(tf_tensor_device, tf_tensor2.device)\n\n  def testTensorsCanBeConsumedOnceOnly(self):\n    np.random.seed(42)\n    np_array = np.random.randint(0, 10, (2, 3, 4))\n    tf_tensor = constant_op.constant(np_array, dtype=np.float32)\n    dlcapsule = dlpack.to_dlpack(tf_tensor)\n    del tf_tensor  # should still work\n    _ = dlpack.from_dlpack(dlcapsule)\n\n    def ConsumeDLPackTensor():\n      dlpack.from_dlpack(dlcapsule)  # Should can be consumed only once\n\n    self.assertRaisesRegex(Exception,\n                           \".*a DLPack tensor may be consumed at most once.*\",\n                           ConsumeDLPackTensor)\n\n  def testUnsupportedTypeToDLPack(self):\n\n    def UnsupportedQint16():\n      tf_tensor = constant_op.constant([[1, 4], [5, 2]], dtype=dtypes.qint16)\n      _ = dlpack.to_dlpack(tf_tensor)\n\n    def UnsupportedComplex64():\n      tf_tensor = constant_op.constant([[1, 4], [5, 2]], dtype=dtypes.complex64)\n      _ = dlpack.to_dlpack(tf_tensor)\n\n    self.assertRaisesRegex(Exception, \".* is not supported by dlpack\",\n                           UnsupportedQint16)\n    self.assertRaisesRegex(Exception, \".* is not supported by dlpack\",\n                           UnsupportedComplex64)\n\n\nif __name__ == \"__main__\":\n  ops.enable_eager_execution()\n  test.main()\n", "/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");;\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include <memory>\n\n#include \"Python.h\"\n#include \"absl/strings/str_format.h\"\n#include \"pybind11/chrono.h\"\n#include \"pybind11/complex.h\"\n#include \"pybind11/functional.h\"\n#include \"pybind11/pybind11.h\"\n#include \"pybind11/stl.h\"\n#include \"tensorflow/c/c_api.h\"\n#include \"tensorflow/c/c_api_experimental.h\"\n#include \"tensorflow/c/eager/c_api.h\"\n#include \"tensorflow/c/eager/c_api_experimental.h\"\n#include \"tensorflow/c/eager/c_api_internal.h\"\n#include \"tensorflow/c/eager/dlpack.h\"\n#include \"tensorflow/c/eager/tfe_tensorhandle_internal.h\"\n#include \"tensorflow/c/tf_status.h\"\n#include \"tensorflow/c/tf_status_helper.h\"\n#include \"tensorflow/compiler/jit/flags.h\"\n#include \"tensorflow/compiler/jit/get_compiler_ir.h\"\n#include \"tensorflow/python/eager/pywrap_tensor_conversion.h\"\n#include \"tensorflow/python/eager/pywrap_tfe.h\"\n#include \"tensorflow/python/lib/core/py_exception_registry.h\"\n#include \"tensorflow/python/lib/core/pybind11_lib.h\"\n#include \"tensorflow/python/lib/core/pybind11_status.h\"\n#include \"tensorflow/python/lib/core/safe_ptr.h\"\n#include \"tensorflow/python/lib/core/safe_pyobject_ptr.h\"\n#include \"tensorflow/python/util/util.h\"\n\nnamespace py = pybind11;\n\nPYBIND11_MAKE_OPAQUE(TFE_Executor);\nPYBIND11_MAKE_OPAQUE(TFE_ContextOptions);\nPYBIND11_MAKE_OPAQUE(TFE_CancellationManager);\n\nPYBIND11_MAKE_OPAQUE(TFE_MonitoringCounter0);\nPYBIND11_MAKE_OPAQUE(TFE_MonitoringCounter1);\nPYBIND11_MAKE_OPAQUE(TFE_MonitoringCounter2);\nPYBIND11_MAKE_OPAQUE(TFE_MonitoringStringGauge0);\nPYBIND11_MAKE_OPAQUE(TFE_MonitoringStringGauge1);\nPYBIND11_MAKE_OPAQUE(TFE_MonitoringStringGauge2);\nPYBIND11_MAKE_OPAQUE(TFE_MonitoringIntGauge0);\nPYBIND11_MAKE_OPAQUE(TFE_MonitoringIntGauge1);\nPYBIND11_MAKE_OPAQUE(TFE_MonitoringIntGauge2);\nPYBIND11_MAKE_OPAQUE(TFE_MonitoringBoolGauge0);\nPYBIND11_MAKE_OPAQUE(TFE_MonitoringBoolGauge1);\nPYBIND11_MAKE_OPAQUE(TFE_MonitoringBoolGauge2);\nPYBIND11_MAKE_OPAQUE(TFE_MonitoringSampler0);\nPYBIND11_MAKE_OPAQUE(TFE_MonitoringSampler1);\nPYBIND11_MAKE_OPAQUE(TFE_MonitoringSampler2);\nPYBIND11_MAKE_OPAQUE(TFE_MonitoringCounterCell);\nPYBIND11_MAKE_OPAQUE(TFE_MonitoringIntGaugeCell);\nPYBIND11_MAKE_OPAQUE(TFE_MonitoringStringGaugeCell);\nPYBIND11_MAKE_OPAQUE(TFE_MonitoringBoolGaugeCell);\nPYBIND11_MAKE_OPAQUE(TFE_MonitoringSamplerCell);\n\nPYBIND11_MAKE_OPAQUE(TF_DeviceList);\nPYBIND11_MAKE_OPAQUE(TF_Function);\nPYBIND11_MAKE_OPAQUE(TF_Buffer);\n\n// Eager helper functions migrated from pywrap_tfe.i.\n\nnamespace tensorflow {\n\n// We cannot use Context as an opaque type. SWIG also had\n// difficult directly passing the pointer around. These\n// typemaps are migrated over from pywrap_tfe.i. I tried\n// using a custom type caster, but we get segfaults periodically.\n\n// TODO(amitpatankar): Move input and output logic of Context into a\n// pybind11 custom type caster.\n\nTFE_Context* InputTFE_Context(const py::handle& ctx) {\n  return static_cast<TFE_Context*>(PyCapsule_GetPointer(ctx.ptr(), nullptr));\n}\n\nPyObject* OutputTFE_Context(TFE_Context* context) {\n  return PyCapsule_New(context, nullptr, TFE_DeleteContextCapsule);\n}\n\nTF_Buffer* ProtoStringToTFBuffer(PyObject* input) {\n  // Convert a Python string object to TF_Buffer.\n  char* c_string;\n  Py_ssize_t py_size;\n  // PyBytes_AsStringAndSize() does not copy but simply interprets the input\n  if (PyBytes_AsStringAndSize(input, &c_string, &py_size) == -1) {\n    // Python has raised an error (likely TypeError or UnicodeEncodeError).\n    throw py::error_already_set();\n  }\n  return TF_NewBufferFromString(static_cast<void*>(c_string),\n                                static_cast<size_t>(py_size));\n}\n\n// These functions are typemaps from the Python side. I did not use\n// a custom type caster since the logic is slightly harder to follow. This\n// converter is also only used once in `TFE_Py_ExecuteCancelable_wrapper`.\nTFE_InputTensorHandles InputTFE_InputTensorHandles(\n    const py::handle& input_tensors) {\n  TFE_InputTensorHandles input_tensor_handles;\n  if (input_tensors.ptr() != Py_None) {\n    if (!PyList_Check(input_tensors.ptr())) {\n      tensorflow::ThrowTypeError(\"must provide a list of Tensors as inputs\");\n    }\n    Py_ssize_t len = PyList_Size(input_tensors.ptr());\n    input_tensor_handles.resize(len);\n    for (Py_ssize_t i = 0; i < len; ++i) {\n      PyObject* elem = PyList_GetItem(input_tensors.ptr(), i);\n      if (!elem) {\n        tensorflow::ThrowTypeError(\"Input Tensor does not exist.\");\n      }\n      if (EagerTensor_CheckExact(elem)) {\n        (input_tensor_handles)[i] = EagerTensor_Handle(elem);\n      } else if (tensorflow::swig::IsEagerTensorSlow(elem)) {\n        // Use equivalent of object.__getattribute__ to get the underlying\n        // tf wrapped EagerTensor (if there is one).\n        tensorflow::Safe_PyObjectPtr tf_should_use_attr(\n#if PY_MAJOR_VERSION < 3\n            PyString_InternFromString(\"_tf_should_use_wrapped_value\")\n#else\n            PyUnicode_InternFromString(\"_tf_should_use_wrapped_value\")\n#endif\n        );\n        tensorflow::Safe_PyObjectPtr value_attr(\n            PyObject_GenericGetAttr(elem, tf_should_use_attr.get()));\n        if (value_attr) {\n          // This is an EagerTensor wrapped inside a TFShouldUse wrapped object.\n          (input_tensor_handles)[i] = EagerTensor_Handle(value_attr.get());\n        } else {\n          // This is a subclass of EagerTensor that we don't support.\n          PyErr_Clear();\n          tensorflow::ThrowTypeError(\n              tensorflow::strings::StrCat(\n                  \"Saw an object that is an instance of a strict subclass of \"\n                  \"EagerTensor, which is not supported.  Item \",\n                  i, \" is type: \", elem->ob_type->tp_name)\n                  .c_str());\n        }\n      } else if (tensorflow::swig::IsTensor(elem)) {\n        // If it isnt an EagerTensor, but is still a Tensor, it must be a graph\n        // tensor.\n        tensorflow::Safe_PyObjectPtr name_attr(\n            PyObject_GetAttrString(elem, \"name\"));\n        tensorflow::ThrowTypeError(\n            tensorflow::strings::StrCat(\n                \"An op outside of the function building code is being passed\\n\"\n                \"a \\\"Graph\\\" tensor. It is possible to have Graph tensors\\n\"\n                \"leak out of the function building context by including a\\n\"\n                \"tf.init_scope in your function building code.\\n\"\n                \"For example, the following function will fail:\\n\",\n                \"  @tf.function\\n\", \"  def has_init_scope():\\n\",\n                \"    my_constant = tf.constant(1.)\\n\",\n                \"    with tf.init_scope():\\n\",\n                \"      added = my_constant * 2\\n\",\n                \"The graph tensor has name: \",\n                name_attr ? TFE_GetPythonString(name_attr.get()) : \"<unknown>\")\n                .c_str());\n      } else {\n        tensorflow::ThrowTypeError(\n            tensorflow::strings::StrCat(\n                \"provided list of inputs contains objects other \"\n                \"than 'EagerTensor'. Item \",\n                i, \" is type: \", elem->ob_type->tp_name)\n                .c_str());\n      }\n    }\n  }\n  return input_tensor_handles;\n}\n\n// These functions are typemaps from the Python side. I did not use\n// a custom type caster since the logic is slightly harder to follow. This\n// converter is also only used once in `TFE_Py_ExecuteCancelable_wrapper`.\n// This function actually takes a number rather than an output Tensor holder.\nTFE_OutputTensorHandles InputTFE_OutputTensorHandles(\n    const py::handle& num_outputs) {\n  TFE_OutputTensorHandles output_tensor_handles;\n#if PY_MAJOR_VERSION < 3\n  if (!PyInt_Check(num_outputs.ptr())) {\n#else\n  if (!PyLong_Check(num_outputs.ptr())) {\n#endif\n    PyErr_SetString(PyExc_TypeError,\n                    \"expected an integer value (size of the number of \"\n                    \"outputs of the operation)\");\n    throw py::error_already_set();\n  }\n#if PY_MAJOR_VERSION < 3\n  long sz = PyInt_AsLong(num_outputs.ptr());  // NOLINT\n#else\n  long sz = PyLong_AsLong(num_outputs.ptr());  // NOLINT\n#endif\n  if (sz > 0) {\n#if PY_MAJOR_VERSION < 3\n    output_tensor_handles.resize(PyInt_AsLong(num_outputs.ptr()), nullptr);\n#else\n    output_tensor_handles.resize(PyLong_AsLong(num_outputs.ptr()), nullptr);\n#endif\n  }\n  return output_tensor_handles;\n}\n\n// Packs multiple `EagerTensor`s of the same dtype and shape into one\n// `EagerTensor`.\npy::object TFE_Py_PackEagerTensors_wrapper(const py::handle& context,\n                                           const py::handle& tensors) {\n  TFE_Context* ctx = tensorflow::InputTFE_Context(context);\n  TFE_InputTensorHandles handles = InputTFE_InputTensorHandles(tensors);\n  tensorflow::Safe_TF_StatusPtr status = tensorflow::make_safe(TF_NewStatus());\n  int size = handles.size();\n  TFE_TensorHandle* packed_handle =\n      TFE_CreatePackedTensorHandle(ctx, handles.data(), &size, status.get());\n  tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n  PyObject* packed_tensor =\n      EagerTensorFromHandle(packed_handle, /*is_packed=*/true);\n  return tensorflow::PyoOrThrow(packed_tensor);\n}\n\n// This function was created from fusing the typemap logic in platform/base.i.\npy::object TFE_Py_ExecuteCancelable_wrapper(\n    const py::handle& context, const char* device_name, const char* op_name,\n    const py::handle& inputs, const py::handle& attrs,\n    TFE_CancellationManager* cancellation_manager,\n    const py::handle& num_outputs) {\n  TFE_Context* ctx = tensorflow::InputTFE_Context(context);\n  TFE_InputTensorHandles input_tensor_handles =\n      InputTFE_InputTensorHandles(inputs);\n  TFE_OutputTensorHandles output_tensor_handles =\n      InputTFE_OutputTensorHandles(num_outputs);\n  tensorflow::Safe_TF_StatusPtr status = tensorflow::make_safe(TF_NewStatus());\n  TFE_Py_ExecuteCancelable(ctx, device_name, op_name, &input_tensor_handles,\n                           attrs.ptr(), cancellation_manager,\n                           &output_tensor_handles, status.get());\n\n  int output_len = output_tensor_handles.size();\n  PyObject* output_list = PyList_New(output_len);\n  for (int i = 0; i < output_len; ++i) {\n    PyObject* output;\n    output = EagerTensorFromHandle(output_tensor_handles.at(i));\n    PyList_SetItem(output_list, i, output);\n  }\n  tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n  return tensorflow::PyoOrThrow(output_list);\n}\n\nstatic py::object TF_ListPhysicalDevices() {\n  std::vector<string> devices;\n  tensorflow::Status s =\n      tensorflow::DeviceFactory::ListAllPhysicalDevices(&devices);\n  MaybeRaiseRegisteredFromStatus(s);\n  PyObject* result = PyList_New(devices.size());\n  int i = 0;\n  for (auto& dev : devices) {\n    PyObject* dev_obj = PyBytes_FromStringAndSize(dev.data(), dev.size());\n    PyList_SetItem(result, i, dev_obj);\n    ++i;\n  }\n  return tensorflow::PyoOrThrow(result);\n}\n\nstatic std::unordered_map<string, string> TF_GetDeviceDetails(int index) {\n  tensorflow::Safe_TF_StatusPtr status = tensorflow::make_safe(TF_NewStatus());\n  std::unordered_map<string, string> device_details;\n  tensorflow::Status s =\n      tensorflow::DeviceFactory::GetAnyDeviceDetails(index, &device_details);\n  tensorflow::Set_TF_Status_from_Status(status.get(), s);\n  MaybeRaiseRegisteredFromTFStatus(status.get());\n  return device_details;\n}\n\nstatic py::object TFE_ClearScalarCache() {\n  tensorflow::TFE_TensorHandleCache::Get()->Clear();\n  return py::none();\n}\n\n// Returns compiler IR for a given function.\nstatic std::string TFE_GetCompilerIr(py::handle& ctx,\n                                     const char* concrete_function_name,\n                                     const char* stage, const char* device_name,\n                                     py::handle& inputs) {\n  EagerContext* context = ContextFromInterface(\n      reinterpret_cast<ImmediateExecutionContext*>(InputTFE_Context(ctx)));\n\n  std::string s_stage(stage);\n  IrExportStage selected_stage = [&] {\n    if (s_stage == \"hlo\") {\n      return IrExportStage::HLO;\n    } else if (s_stage == \"optimized_hlo\") {\n      return IrExportStage::OPTIMIZED_HLO;\n    } else {\n      ThrowValueError(\n          absl::StrFormat(\"Invalid stage selected: '%s'. Valid values are: \"\n                          \"'hlo', 'optimized_hlo'\",\n                          s_stage)\n              .c_str());\n    }\n  }();\n\n  TFE_InputTensorHandles handles = InputTFE_InputTensorHandles(inputs);\n\n  std::vector<const Tensor*> input_tensors;\n  for (TFE_TensorHandle* tensor_handle : handles) {\n    AbstractTensorHandle* abstract_tensor_handle = unwrap(tensor_handle);\n    TensorHandle* th = TensorHandleFromInterface(abstract_tensor_handle);\n\n    const Tensor* t;\n    Status st = th->Tensor(&t);\n    if (!st.ok()) {\n      ThrowValueError(\n          absl::StrFormat(\"Could not resolve tensor: '%s'\", st.error_message())\n              .c_str());\n    }\n    input_tensors.push_back(t);\n  }\n\n  DeviceNameUtils::ParsedName input_device_name;\n  if (!DeviceNameUtils::ParseFullOrLocalName(device_name, &input_device_name)) {\n    ThrowValueError(\n        absl::StrFormat(\"Failed parsing device name: '%s'\", device_name)\n            .c_str());\n  }\n\n  std::vector<Device*> devices = context->local_device_mgr()->ListDevices();\n  auto selected_device = absl::c_find_if(devices, [&](const Device* d) {\n    return DeviceNameUtils::AreCompatibleDevNames(input_device_name,\n                                                  d->parsed_name());\n  });\n  if (selected_device == devices.end()) {\n    ThrowValueError(\"No matching device found\");\n  }\n\n  xla::StatusOr<std::string> hlo_text =\n      GetCompilerIr(selected_stage, context->pflr(), concrete_function_name,\n                    *selected_device, input_tensors);\n\n  if (!hlo_text.ok()) {\n    ThrowValueError(absl::StrFormat(\"Failed getting HLO text: '%s'\",\n                                    hlo_text.status().error_message())\n                        .c_str());\n  }\n  return *hlo_text;\n}\n\n}  // namespace tensorflow\n\nnamespace {\n\n// Wrapper around the EagerContextThreadLocalData struct (defined in\n// pywrap_tfe.h), so it can be accessed from Python.\n//\n// For PyObject* fields, the get_*() methods return a new reference; and the\n// set_*() methods create a new reference (i.e., they do not steal a reference).\nclass EagerContextThreadLocalDataWrapper {\n public:\n  explicit EagerContextThreadLocalDataWrapper(py::handle py_eager_context,\n                                              py::handle is_eager,\n                                              py::handle device_spec)\n      : py_eager_context_(py_eager_context.ptr()) {\n    tensorflow::MakeEagerContextThreadLocalData(\n        py_eager_context.ptr(), is_eager.ptr(), device_spec.ptr());\n  }\n\n  ~EagerContextThreadLocalDataWrapper() {\n    tensorflow::DestroyEagerContextThreadLocalData(py_eager_context_);\n  }\n\n  bool get_is_eager() const { return GetData()->is_eager; }\n  void set_is_eager(bool v) { GetData()->is_eager = v; }\n\n  bool get_invoking_op_callbacks() const {\n    return GetData()->invoking_op_callbacks;\n  }\n  void set_invoking_op_callbacks(bool v) {\n    GetData()->invoking_op_callbacks = v;\n  }\n\n  py::handle get_device_name() const {\n    return GetPyObject(&GetData()->device_name);\n  }\n  void set_device_name(py::handle v) {\n    SetPyObject(v, &GetData()->device_name);\n  }\n\n  py::handle get_scope_name() const {\n    return GetPyObject(&GetData()->scope_name);\n  }\n  void set_scope_name(py::handle v) { SetPyObject(v, &GetData()->scope_name); }\n\n  py::handle get_device_spec() const {\n    return GetPyObject(&GetData()->device_spec);\n  }\n  void set_device_spec(py::handle v) {\n    SetPyObject(v, &GetData()->device_spec);\n  }\n\n  py::handle get_function_call_options() const {\n    return GetPyObject(&GetData()->function_call_options);\n  }\n  void set_function_call_options(py::handle v) {\n    SetPyObject(v, &GetData()->function_call_options);\n  }\n\n  py::handle get_executor() const { return GetPyObject(&GetData()->executor); }\n  void set_executor(py::handle v) { SetPyObject(v, &GetData()->executor); }\n\n  py::handle get_op_callbacks() const {\n    return GetPyObject(&GetData()->op_callbacks);\n  }\n  void set_op_callbacks(py::handle v) {\n    SetPyObject(v, &GetData()->op_callbacks);\n  }\n\n private:\n  tensorflow::EagerContextThreadLocalData* GetData() const {\n    auto* result =\n        tensorflow::GetEagerContextThreadLocalData(py_eager_context_);\n    if (!result) {\n      throw py::error_already_set();\n    }\n    return result;\n  }\n\n  py::handle GetPyObject(tensorflow::Safe_PyObjectPtr* obj) const {\n    Py_INCREF(obj->get());\n    return obj->get();\n  }\n\n  void SetPyObject(py::handle value, tensorflow::Safe_PyObjectPtr* ptr) {\n    Py_INCREF(value.ptr());\n    ptr->reset(value.ptr());\n  }\n\n  PyObject* py_eager_context_;  // not owned (borrowed reference).\n};\n\n}  // namespace\n\n// py::return_value_policy::reference is defined as specified by the\n// pybind11 documents listed here.\n// https://pybind11.readthedocs.io/en/stable/advanced/functions.html#return-value-policies\n// This means that C++ maintains ownership of the object. We\n// are only assigning this to functions that return opaque types.\n\nPYBIND11_MODULE(_pywrap_tfe, m) {\n  py::class_<TFE_Executor> TFE_Executor_class(m, \"TFE_Executor\");\n  py::class_<TFE_ContextOptions> TFE_ContextOptions_class(m,\n                                                          \"TFE_ContextOptions\");\n  py::class_<TFE_MonitoringCounter0> TFE_MonitoringCounter0_class(\n      m, \"TFE_MonitoringCounter0\");\n  py::class_<TFE_MonitoringCounter1> TFE_MonitoringCounter1_class(\n      m, \"TFE_MonitoringCounter1\");\n  py::class_<TFE_MonitoringCounter2> TFE_MonitoringCounter2_class(\n      m, \"TFE_MonitoringCounter2\");\n  py::class_<TFE_MonitoringStringGauge0> TFE_MonitoringStringGauge0_class(\n      m, \"TFE_MonitoringStringGauge0\");\n  py::class_<TFE_MonitoringStringGauge1> TFE_MonitoringStringGauge1_class(\n      m, \"TFE_MonitoringStringGauge1\");\n  py::class_<TFE_MonitoringStringGauge2> TFE_MonitoringStringGauge2_class(\n      m, \"TFE_MonitoringStringGauge2\");\n  py::class_<TFE_MonitoringIntGauge0> TFE_MonitoringIntGauge0_class(\n      m, \"TFE_MonitoringIntGauge0\");\n  py::class_<TFE_MonitoringIntGauge1> TFE_MonitoringIntGauge1_class(\n      m, \"TFE_MonitoringIntGauge1\");\n  py::class_<TFE_MonitoringIntGauge2> TFE_MonitoringIntGauge2_class(\n      m, \"TFE_MonitoringIntGauge2\");\n  py::class_<TFE_MonitoringBoolGauge0> TFE_MonitoringBoolGauge0_class(\n      m, \"TFE_MonitoringBoolGauge0\");\n  py::class_<TFE_MonitoringBoolGauge1> TFE_MonitoringBoolGauge1_class(\n      m, \"TFE_MonitoringBoolGauge1\");\n  py::class_<TFE_MonitoringBoolGauge2> TFE_MonitoringBoolGauge2_class(\n      m, \"TFE_MonitoringBoolGauge2\");\n  py::class_<TFE_MonitoringCounterCell> TFE_MonitoringCounterCell_class(\n      m, \"TFE_MonitoringCounterCell\");\n  py::class_<TFE_MonitoringIntGaugeCell> TFE_MonitoringIntGaugeCell_class(\n      m, \"TFE_MonitoringIntGaugeCell\");\n  py::class_<TFE_MonitoringStringGaugeCell> TFE_MonitoringStringGaugeCell_class(\n      m, \"TFE_MonitoringStringGaugeCell\");\n  py::class_<TFE_MonitoringBoolGaugeCell> TFE_MonitoringBoolGaugeCell_class(\n      m, \"TFE_MonitoringBoolGaugeCell\");\n  py::class_<TFE_MonitoringSamplerCell> TFE_MonitoringSamplerCell_class(\n      m, \"TFE_MonitoringSamplerCell\");\n  py::class_<TFE_MonitoringBuckets> TFE_MonitoringBuckets_class(\n      m, \"TFE_MonitoringBuckets\");\n  py::class_<TFE_MonitoringSampler0> TFE_MonitoringSampler0_class(\n      m, \"TFE_MonitoringSampler0\");\n  py::class_<TFE_MonitoringSampler1> TFE_MonitoringSampler1_class(\n      m, \"TFE_MonitoringSampler1\");\n  py::class_<TFE_MonitoringSampler2> TFE_MonitoringSampler2_class(\n      m, \"TFE_MonitoringSampler2\");\n  py::class_<TFE_CancellationManager> TFE_CancellationManager_class(\n      m, \"TFE_CancellationManager\");\n\n  py::class_<TF_DeviceList> TF_DeviceList_class(m, \"TF_DeviceList\");\n  py::class_<TF_Function> TF_Function_class(m, \"TF_Function\");\n\n  m.def(\"TFE_Py_RegisterExceptionClass\", [](const py::handle& e) {\n    return tensorflow::PyoOrThrow(TFE_Py_RegisterExceptionClass(e.ptr()));\n  });\n  m.def(\"TFE_Py_RegisterFallbackExceptionClass\", [](const py::handle& e) {\n    return tensorflow::PyoOrThrow(\n        TFE_Py_RegisterFallbackExceptionClass(e.ptr()));\n  });\n\n  m.def(\n      \"TFE_GetTotalMemoryUsage\", [](py::handle& ctx, const char* device_name) {\n        tensorflow::EagerContext* context = tensorflow::ContextFromInterface(\n            reinterpret_cast<tensorflow::ImmediateExecutionContext*>(\n                tensorflow::InputTFE_Context(ctx)));\n\n        tensorflow::DeviceNameUtils::ParsedName input_device_name;\n        if (!tensorflow::DeviceNameUtils::ParseFullOrLocalName(\n                device_name, &input_device_name)) {\n          tensorflow::ThrowValueError(\n              absl::StrFormat(\"Failed parsing device name: '%s'\", device_name)\n                  .c_str());\n        }\n\n        std::vector<tensorflow::Device*> devices =\n            context->local_device_mgr()->ListDevices();\n\n        tensorflow::Device* matched_device = nullptr;\n        for (int device_idx = 0; device_idx < devices.size(); device_idx++) {\n          tensorflow::Device* device = devices[device_idx];\n\n          if (tensorflow::DeviceNameUtils::AreCompatibleDevNames(\n                  input_device_name, device->parsed_name())) {\n            if (device->device_type() == tensorflow::DEVICE_CPU) {\n              tensorflow::ThrowValueError(\n                  \"CPU does not support getting allocator information\");\n            }\n\n            if (matched_device != nullptr) {\n              tensorflow::ThrowValueError(\n                  absl::StrFormat(\n                      \"Multiple devices matching the provided string \"\n                      \"'%s': '%s' and \"\n                      \"'%s' \",\n                      device_name, matched_device->name(), device->name())\n                      .c_str());\n            }\n            matched_device = device;\n          }\n        }\n\n        if (matched_device == nullptr) {\n          tensorflow::ThrowValueError(\n              absl::StrFormat(\"No matching devices found for '%s'\", device_name)\n                  .c_str());\n        }\n\n        tensorflow::AllocatorAttributes attrs;\n        tensorflow::Allocator* allocator = matched_device->GetAllocator(attrs);\n\n        if (absl::optional<tensorflow::AllocatorStats> stats =\n                allocator->GetStats()) {\n          return stats->bytes_in_use;\n        }\n\n        tensorflow::ThrowTypeError(\n            absl::StrFormat(\"Allocator stats not available for device '%s'\",\n                            matched_device->name())\n                .c_str());\n      });\n\n  // XLA Eager Logic\n  m.def(\"TF_SetXlaEnableLazyCompilation\", &TF_SetXlaEnableLazyCompilation);\n  m.def(\"TF_SetTfXlaCpuGlobalJit\", &TF_SetTfXlaCpuGlobalJit);\n  m.def(\"TF_SetXlaAutoJitMode\", &TF_SetXlaAutoJitMode);\n  m.def(\"TF_SetXlaConstantFoldingDisabled\", &TF_SetXlaConstantFoldingDisabled);\n  m.def(\"TF_GetXlaConstantFoldingDisabled\", &TF_GetXlaConstantFoldingDisabled);\n  m.def(\"TF_SetXlaMinClusterSize\", &TF_SetXlaMinClusterSize);\n  m.def(\"TF_GetCompilerIr\", &tensorflow::TFE_GetCompilerIr);\n\n  // MLIR Logic\n  m.def(\"TF_IsMlirBridgeEnabled\", [] {\n    return tensorflow::GetMlirCommonFlags()->tf_mlir_enable_mlir_bridge;\n  });\n  m.def(\"TF_EnableMlirBridge\", [](bool enabled) {\n    tensorflow::GetMlirCommonFlags()->tf_mlir_enable_mlir_bridge = enabled;\n  });\n  m.def(\"TF_EnableXlaDevices\", [] {\n    tensorflow::GetXlaDeviceFlags()->tf_xla_enable_xla_devices = true;\n  });\n\n  // // TFE_Context Logic\n  m.def(\n      \"TFE_NewContext\",\n      [](const TFE_ContextOptions* opts) {\n        tensorflow::Safe_TF_StatusPtr status =\n            tensorflow::make_safe(TF_NewStatus());\n        TFE_Context* context = TFE_NewContext(opts, status.get());\n        tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n        return tensorflow::PyoOrThrow(tensorflow::OutputTFE_Context(context));\n      },\n      py::return_value_policy::reference);\n  m.def(\"TFE_DeleteContext\", [](py::handle& o) {\n    TFE_DeleteContext(tensorflow::InputTFE_Context(o));\n  });\n  m.def(\n      \"TFE_ContextListDevices\",\n      [](py::handle& o) {\n        tensorflow::Safe_TF_StatusPtr status =\n            tensorflow::make_safe(TF_NewStatus());\n        auto output = TFE_ContextListDevices(tensorflow::InputTFE_Context(o),\n                                             status.get());\n        tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n        return output;\n      },\n      py::return_value_policy::reference);\n  m.def(\"TFE_HostAddressSpace\", [](py::handle& o, TF_Buffer& buf) {\n    TFE_HostAddressSpace(tensorflow::InputTFE_Context(o), &buf);\n  });\n  m.def(\"TFE_ContextAddFunction\", [](py::handle& ctx, TF_Function* func) {\n    tensorflow::Safe_TF_StatusPtr status =\n        tensorflow::make_safe(TF_NewStatus());\n    TFE_ContextAddFunction(tensorflow::InputTFE_Context(ctx), func,\n                           status.get());\n    tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n  });\n  m.def(\"TFE_ContextAddFunctionDef\",\n        [](py::handle& ctx, const char* serialized_function_def, size_t size) {\n          tensorflow::Safe_TF_StatusPtr status =\n              tensorflow::make_safe(TF_NewStatus());\n          TFE_ContextAddFunctionDef(tensorflow::InputTFE_Context(ctx),\n                                    serialized_function_def, size,\n                                    status.get());\n          tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n        });\n  m.def(\"TFE_ContextGetFunctionDef\",\n        [](py::handle& ctx, const char* function_name, TF_Buffer& buf) {\n          tensorflow::Safe_TF_StatusPtr status =\n              tensorflow::make_safe(TF_NewStatus());\n          TFE_ContextGetFunctionDef(tensorflow::InputTFE_Context(ctx),\n                                    function_name, &buf, status.get());\n          tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n        });\n  m.def(\"TFE_ContextRemoveFunction\", [](py::handle& ctx, const char* name) {\n    tensorflow::Safe_TF_StatusPtr status =\n        tensorflow::make_safe(TF_NewStatus());\n    TFE_ContextRemoveFunction(tensorflow::InputTFE_Context(ctx), name,\n                              status.get());\n    tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n  });\n  m.def(\"TFE_ContextHasFunction\", [](py::handle& ctx, const char* name) {\n    tensorflow::Safe_TF_StatusPtr status =\n        tensorflow::make_safe(TF_NewStatus());\n    auto output =\n        TFE_ContextHasFunction(tensorflow::InputTFE_Context(ctx), name);\n    tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n    return output;\n  });\n  m.def(\"TFE_ContextEnableRunMetadata\", [](py::handle& ctx) {\n    TFE_ContextEnableRunMetadata(tensorflow::InputTFE_Context(ctx));\n  });\n  m.def(\"TFE_ContextDisableRunMetadata\", [](py::handle& ctx) {\n    TFE_ContextEnableRunMetadata(tensorflow::InputTFE_Context(ctx));\n  });\n  m.def(\"TFE_ContextEnableGraphCollection\", [](py::handle& ctx) {\n    TFE_ContextEnableGraphCollection(tensorflow::InputTFE_Context(ctx));\n  });\n  m.def(\"TFE_ContextDisableGraphCollection\", [](py::handle& ctx) {\n    TFE_ContextDisableGraphCollection(tensorflow::InputTFE_Context(ctx));\n  });\n  m.def(\"TFE_ContextExportRunMetadata\", [](py::handle& ctx, TF_Buffer& buf) {\n    tensorflow::Safe_TF_StatusPtr status =\n        tensorflow::make_safe(TF_NewStatus());\n    TFE_ContextExportRunMetadata(tensorflow::InputTFE_Context(ctx), &buf,\n                                 status.get());\n    tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n  });\n  m.def(\"TFE_ContextClearCaches\", [](py::handle& o) {\n    TFE_ContextClearCaches(tensorflow::InputTFE_Context(o));\n  });\n  m.def(\"TFE_GetContextId\", [](py::handle& ctx) {\n    return TFE_GetContextId(tensorflow::InputTFE_Context(ctx));\n  });\n  m.def(\"TFE_ContextGetDevicePlacementPolicy\", [](py::handle& ctx) {\n    return TFE_ContextGetDevicePlacementPolicy(\n        tensorflow::InputTFE_Context(ctx));\n  });\n  m.def(\"TFE_ContextSetThreadLocalDevicePlacementPolicy\",\n        [](py::handle& ctx, TFE_ContextDevicePlacementPolicy policy) {\n          TFE_ContextSetThreadLocalDevicePlacementPolicy(\n              tensorflow::InputTFE_Context(ctx), policy);\n        });\n  m.def(\"TFE_ContextSetServerDef\", [](py::handle& ctx, int keep_alive_secs,\n                                      py::bytes proto) {\n    tensorflow::Safe_TF_StatusPtr status =\n        tensorflow::make_safe(TF_NewStatus());\n    tensorflow::Safe_TF_BufferPtr buf =\n        tensorflow::make_safe(tensorflow::ProtoStringToTFBuffer(proto.ptr()));\n    TFE_ContextSetServerDef(tensorflow::InputTFE_Context(ctx), keep_alive_secs,\n                            buf.get()->data, buf.get()->length, status.get());\n    tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n  });\n  m.def(\"TFE_ContextUpdateServerDef\", [](py::handle& ctx, int keep_alive_secs,\n                                         py::bytes proto) {\n    tensorflow::Safe_TF_StatusPtr status =\n        tensorflow::make_safe(TF_NewStatus());\n    tensorflow::Safe_TF_BufferPtr buf =\n        tensorflow::make_safe(tensorflow::ProtoStringToTFBuffer(proto.ptr()));\n    Py_BEGIN_ALLOW_THREADS;\n    TFE_ContextUpdateServerDef(tensorflow::InputTFE_Context(ctx),\n                               keep_alive_secs, buf.get()->data,\n                               buf.get()->length, status.get());\n    Py_END_ALLOW_THREADS;\n    tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n  });\n  m.def(\"TFE_ContextCheckAlive\", [](py::handle& ctx, const char* worker_name) {\n    tensorflow::Safe_TF_StatusPtr status =\n        tensorflow::make_safe(TF_NewStatus());\n    bool output = TFE_ContextCheckAlive(tensorflow::InputTFE_Context(ctx),\n                                        worker_name, status.get());\n    tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n    return output;\n  });\n  m.def(\"TFE_ContextSyncExecutors\", [](py::handle& ctx) {\n    tensorflow::Safe_TF_StatusPtr status =\n        tensorflow::make_safe(TF_NewStatus());\n    TFE_ContextAsyncWait(tensorflow::InputTFE_Context(ctx), status.get());\n    tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n  });\n  m.def(\"TFE_ContextClearExecutors\", [](py::handle& ctx) {\n    tensorflow::Safe_TF_StatusPtr status =\n        tensorflow::make_safe(TF_NewStatus());\n    TFE_ContextAsyncWait(tensorflow::InputTFE_Context(ctx), status.get());\n    // NOTE: different from TFE_ContextSyncExecutors that raises potential\n    // errors, deliberately ignore executor statuses in cleanup.\n  });\n  m.def(\"TFE_ContextSetSoftDevicePlacement\", [](py::handle& ctx, bool enable) {\n    tensorflow::Safe_TF_StatusPtr status =\n        tensorflow::make_safe(TF_NewStatus());\n    TFE_ContextSetSoftDevicePlacement(tensorflow::InputTFE_Context(ctx), enable,\n                                      status.get());\n  });\n  m.def(\"TFE_ContextSetLogDevicePlacement\", [](py::handle& ctx, bool enable) {\n    tensorflow::Safe_TF_StatusPtr status =\n        tensorflow::make_safe(TF_NewStatus());\n    TFE_ContextSetSoftDevicePlacement(tensorflow::InputTFE_Context(ctx), enable,\n                                      status.get());\n  });\n\n  // TFE_Executor logic\n  m.def(\n      \"TFE_NewExecutor\",\n      [](const bool is_async) {\n        TFE_Executor* exc = TFE_NewExecutor(is_async);\n        return exc;\n      },\n      py::return_value_policy::reference);\n  m.def(\"TFE_DeleteExecutor\", &TFE_DeleteExecutor);\n  m.def(\"TFE_ExecutorIsAsync\", &TFE_ExecutorIsAsync);\n  m.def(\"TFE_ExecutorWaitForAllPendingNodes\", [](TFE_Executor& exc) {\n    tensorflow::Safe_TF_StatusPtr status =\n        tensorflow::make_safe(TF_NewStatus());\n    // NOTE: release Python GIL for pending PyFunc ops to be executed properly.\n    Py_BEGIN_ALLOW_THREADS;\n    TFE_ExecutorWaitForAllPendingNodes(&exc, status.get());\n    Py_END_ALLOW_THREADS;\n    tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n  });\n  m.def(\"TFE_ExecutorClearError\", &TFE_ExecutorClearError);\n  m.def(\"TFE_ContextSetExecutorForThread\", [](py::handle& ctx,\n                                              TFE_Executor& exc) {\n    TFE_ContextSetExecutorForThread(tensorflow::InputTFE_Context(ctx), &exc);\n  });\n  m.def(\n      \"TFE_ContextGetExecutorForThread\",\n      [](py::handle& o) {\n        return TFE_ContextGetExecutorForThread(tensorflow::InputTFE_Context(o));\n      },\n      py::return_value_policy::reference);\n\n  m.def(\"TFE_OpNameGetAttrType\",\n        [](py::handle& ctx, const char* op_or_function_name,\n           const char* attr_name) {\n          int temp = 0;\n          unsigned char* is_list = reinterpret_cast<unsigned char*>(&temp);\n          tensorflow::Safe_TF_StatusPtr status =\n              tensorflow::make_safe(TF_NewStatus());\n          auto output = TFE_OpNameGetAttrType(tensorflow::InputTFE_Context(ctx),\n                                              op_or_function_name, attr_name,\n                                              is_list, status.get());\n          tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n#if PY_MAJOR_VERSION < 3\n          PyObject* output_pyo = PyInt_FromLong(output);\n#else\n          PyObject* output_pyo = PyLong_FromLong(output);\n#endif\n          if (*is_list == 1) {\n            PyObject* list = PyList_New(1);\n            PyList_SetItem(list, 0, output_pyo);\n            return tensorflow::PyoOrThrow(list);\n          }\n          return tensorflow::PyoOrThrow(output_pyo);\n        });\n  m.def(\"TFE_Py_InitEagerTensor\", [](const py::handle& o) {\n    return tensorflow::PyoOrThrow(TFE_Py_InitEagerTensor(o.ptr()));\n  });\n  m.def(\"TFE_Py_PackEagerTensors\",\n        [](const py::handle& context, const py::handle& handles) {\n          return tensorflow::TFE_Py_PackEagerTensors_wrapper(context, handles);\n        });\n  m.def(\"TFE_Py_SetEagerTensorProfiler\", &TFE_Py_SetEagerTensorProfiler);\n  m.def(\"TFE_Py_RegisterJVPFunction\", [](const py::handle& o) {\n    return tensorflow::PyoOrThrow(TFE_Py_RegisterJVPFunction(o.ptr()));\n  });\n  m.def(\"TFE_Py_RegisterGradientFunction\", [](const py::handle& o) {\n    return tensorflow::PyoOrThrow(TFE_Py_RegisterGradientFunction(o.ptr()));\n  });\n  m.def(\"TFE_Py_Execute\",\n        [](const py::handle& context, const char* device_name,\n           const char* op_name, const py::handle& inputs,\n           const py::handle& attrs, const py::handle& num_outputs) {\n          return tensorflow::TFE_Py_ExecuteCancelable_wrapper(\n              context, device_name, op_name, inputs, attrs.ptr(), nullptr,\n              num_outputs);\n        });\n  m.def(\n      \"TFE_Py_ExecuteCancelable\",\n      [](const py::handle& context, const char* device_name,\n         const char* op_name, const py::handle& inputs, const py::handle& attrs,\n         TFE_CancellationManager& cancellation_manager,\n         const py::handle& num_outputs) {\n        return tensorflow::TFE_Py_ExecuteCancelable_wrapper(\n            context, device_name, op_name, inputs, attrs.ptr(),\n            &cancellation_manager, num_outputs);\n      });\n  m.def(\"TFE_Py_FastPathExecute\", [](const py::args args) {\n    // TFE_Py_FastPathExecute requires error checking prior to returning.\n    return tensorflow::PyoOrThrow(TFE_Py_FastPathExecute_C(args.ptr()));\n  });\n  m.def(\"TFE_Py_RecordGradient\",\n        [](const py::handle& op_name, const py::handle& inputs,\n           const py::handle& attrs, const py::handle& results,\n           const py::handle& forward_pass_name_scope) {\n          return tensorflow::PyoOrThrow(TFE_Py_RecordGradient(\n              op_name.ptr(), inputs.ptr(), attrs.ptr(), results.ptr(),\n              forward_pass_name_scope.ptr()));\n        });\n  m.def(\"TFE_Py_UID\", []() { return tensorflow::PyoOrThrow(TFE_Py_UID()); });\n\n  // TFE_Py_Tape Logic\n  m.def(\"TFE_Py_TapeSetNew\", [](const py::handle& persistent,\n                                const py::handle& watch_accessed_variables) {\n    return tensorflow::PyoOrThrow(\n        TFE_Py_TapeSetNew(persistent.ptr(), watch_accessed_variables.ptr()));\n  });\n  m.def(\"TFE_Py_TapeSetAdd\",\n        [](const py::handle& tape) { TFE_Py_TapeSetAdd(tape.ptr()); });\n  m.def(\"TFE_Py_TapeSetRemove\",\n        [](const py::handle& tape) { TFE_Py_TapeSetRemove(tape.ptr()); });\n  m.def(\"TFE_Py_TapeSetStopOnThread\", &TFE_Py_TapeSetStopOnThread);\n  m.def(\"TFE_Py_TapeSetRestartOnThread\", &TFE_Py_TapeSetRestartOnThread);\n  m.def(\"TFE_Py_TapeSetIsStopped\",\n        []() { return tensorflow::PyoOrThrow(TFE_Py_TapeSetIsStopped()); });\n  m.def(\"TFE_Py_TapeSetIsEmpty\",\n        []() { return tensorflow::PyoOrThrow(TFE_Py_TapeSetIsEmpty()); });\n  m.def(\"TFE_Py_TapeSetShouldRecordBackprop\", [](const py::handle& tensors) {\n    return tensorflow::PyoOrThrow(\n        TFE_Py_TapeSetShouldRecordBackprop(tensors.ptr()));\n  });\n  m.def(\"TFE_Py_TapeSetPossibleGradientTypes\", [](const py::handle& tensors) {\n    return tensorflow::PyoOrThrow(\n        TFE_Py_TapeSetPossibleGradientTypes(tensors.ptr()));\n  });\n  m.def(\"TFE_Py_TapeSetDeleteTrace\", &TFE_Py_TapeSetDeleteTrace);\n  m.def(\"TFE_Py_TapeSetRecordOperation\",\n        [](const py::handle& op_type, const py::handle& output_tensors,\n           const py::handle& input_tensors, const py::handle& backward_function,\n           const py::handle& forward_function) {\n          return tensorflow::PyoOrThrow(TFE_Py_TapeSetRecordOperation(\n              op_type.ptr(), output_tensors.ptr(), input_tensors.ptr(),\n              backward_function.ptr(), forward_function.ptr()));\n        });\n  m.def(\n      \"TFE_Py_TapeSetRecordOperationBackprop\",\n      [](const py::handle& op_type, const py::handle& output_tensors,\n         const py::handle& input_tensors, const py::handle& backward_function) {\n        return tensorflow::PyoOrThrow(TFE_Py_TapeSetRecordOperationBackprop(\n            op_type.ptr(), output_tensors.ptr(), input_tensors.ptr(),\n            backward_function.ptr()));\n      });\n  m.def(\n      \"TFE_Py_TapeSetRecordOperationForwardprop\",\n      [](const py::handle& op_type, const py::handle& output_tensors,\n         const py::handle& input_tensors, const py::handle& backward_function,\n         const py::handle& forwardprop_output_indices) {\n        return tensorflow::PyoOrThrow(TFE_Py_TapeSetRecordOperationForwardprop(\n            op_type.ptr(), output_tensors.ptr(), input_tensors.ptr(),\n            backward_function.ptr(), forwardprop_output_indices.ptr()));\n      });\n  m.def(\"TFE_Py_TapeGradient\",\n        [](const py::handle& tape, const py::handle& target,\n           const py::handle& sources, const py::handle& output_gradients,\n           const py::handle& sources_raw,\n           const py::handle& unconnected_gradients) {\n          tensorflow::Safe_TF_StatusPtr status =\n              tensorflow::make_safe(TF_NewStatus());\n          PyObject* output = TFE_Py_TapeGradient(\n              tape.ptr(), target.ptr(), sources.ptr(), output_gradients.ptr(),\n              sources_raw.ptr(), unconnected_gradients.ptr(), status.get());\n          tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n          return tensorflow::PyoOrThrow(output);\n        });\n\n  m.def(\"TFE_Py_TapeVariableAccessed\", [](const py::handle& variable) {\n    TFE_Py_TapeVariableAccessed(variable.ptr());\n  });\n  m.def(\"TFE_Py_TapeWatch\",\n        [](const py::handle& tape, const py::handle& tensor) {\n          TFE_Py_TapeWatch(tape.ptr(), tensor.ptr());\n        });\n  m.def(\"TFE_Py_TapeWatchVariable\",\n        [](const py::handle& tape, const py::handle& variable) {\n          TFE_Py_TapeWatchVariable(tape.ptr(), variable.ptr());\n        });\n  m.def(\"TFE_Py_TapeWatchedVariables\", [](const py::handle& tape) {\n    return tensorflow::PyoOrThrow(TFE_Py_TapeWatchedVariables(tape.ptr()));\n  });\n\n  // TFE_Py_VariableWatcher logic.\n  m.def(\"TFE_Py_VariableWatcherNew\",\n        []() { return tensorflow::PyoOrThrow(TFE_Py_VariableWatcherNew()); });\n  m.def(\"TFE_Py_VariableWatcherRemove\", [](const py::handle& variable_watcher) {\n    TFE_Py_VariableWatcherRemove(variable_watcher.ptr());\n  });\n  m.def(\"TFE_Py_VariableWatcherVariableAccessed\",\n        [](const py::handle& variable) {\n          TFE_Py_VariableWatcherVariableAccessed(variable.ptr());\n        });\n  m.def(\"TFE_Py_VariableWatcherWatchedVariables\",\n        [](const py::handle& variable_watcher) {\n          return tensorflow::PyoOrThrow(\n              TFE_Py_VariableWatcherWatchedVariables(variable_watcher.ptr()));\n        });\n\n  // TFE_Py_ForwardAccumulator logic.\n  m.def(\"TFE_Py_ForwardAccumulatorNew\", [](bool use_batch) {\n    return tensorflow::PyoOrThrow(TFE_Py_ForwardAccumulatorNew(use_batch));\n  });\n\n  m.def(\"TFE_Py_ForwardAccumulatorSetAdd\", [](const py::handle& accumulator) {\n    return tensorflow::PyoOrThrow(\n        TFE_Py_ForwardAccumulatorSetAdd(accumulator.ptr()));\n  });\n  m.def(\"TFE_Py_ForwardAccumulatorSetRemove\",\n        [](const py::handle& accumulator) {\n          TFE_Py_ForwardAccumulatorSetRemove(accumulator.ptr());\n        });\n\n  m.def(\"TFE_Py_ForwardAccumulatorWatch\",\n        [](const py::handle& accumulator, const py::handle& tensor,\n           const py::handle& tangent) {\n          TFE_Py_ForwardAccumulatorWatch(accumulator.ptr(), tensor.ptr(),\n                                         tangent.ptr());\n        });\n  m.def(\"TFE_Py_ForwardAccumulatorJVP\",\n        [](const py::handle& accumulator, const py::handle& tensor) {\n          return tensorflow::PyoOrThrow(\n              TFE_Py_ForwardAccumulatorJVP(accumulator.ptr(), tensor.ptr()));\n        });\n  m.def(\"TFE_Py_ForwardAccumulatorPushState\", []() {\n    return tensorflow::PyoOrThrow(TFE_Py_ForwardAccumulatorPushState());\n  });\n  m.def(\"TFE_Py_ForwardAccumulatorPopState\", []() {\n    return tensorflow::PyoOrThrow(TFE_Py_ForwardAccumulatorPopState());\n  });\n  m.def(\"TFE_Py_PackJVPs\", [](const py::handle& tensors) {\n    return tensorflow::PyoOrThrow(TFE_Py_PackJVPs(tensors.ptr()));\n  });\n\n  // TFE_ContextOptions Logic\n  m.def(\"TFE_NewContextOptions\", &TFE_NewContextOptions,\n        py::return_value_policy::reference);\n  m.def(\"TFE_ContextOptionsSetConfig\", [](TFE_ContextOptions* options,\n                                          py::bytes proto) {\n    tensorflow::Safe_TF_StatusPtr status =\n        tensorflow::make_safe(TF_NewStatus());\n    tensorflow::Safe_TF_BufferPtr buf =\n        tensorflow::make_safe(tensorflow::ProtoStringToTFBuffer(proto.ptr()));\n    TFE_ContextOptionsSetConfig(options, buf.get()->data, buf.get()->length,\n                                status.get());\n    tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n  });\n  m.def(\"TFE_ContextOptionsSetDevicePlacementPolicy\",\n        &TFE_ContextOptionsSetDevicePlacementPolicy);\n  m.def(\"TFE_ContextOptionsSetLazyRemoteInputsCopy\",\n        &TFE_ContextOptionsSetLazyRemoteInputsCopy);\n  m.def(\"TFE_ContextOptionsSetTfrt\", &TFE_ContextOptionsSetTfrt);\n  m.def(\"TFE_ContextOptionsSetAsync\", &TFE_ContextOptionsSetAsync);\n  m.def(\"TFE_DeleteContextOptions\", &TFE_DeleteContextOptions,\n        py::return_value_policy::reference);\n\n  // TFE_Py_TensorShape Logic\n  m.def(\"TFE_Py_TensorShapeSlice\",\n        [](const py::handle& tensors, int slice_dim) {\n          return tensorflow::PyoOrThrow(\n              TFE_Py_TensorShapeSlice(tensors.ptr(), slice_dim));\n        });\n  m.def(\"TFE_Py_TensorShapeOnDevice\", [](const py::handle& tensors,\n                                         int slice_dim) {\n    return tensorflow::PyoOrThrow(TFE_Py_TensorShapeOnDevice(tensors.ptr()));\n  });\n  m.def(\"TFE_Py_EnableInteractivePythonLogging\",\n        &TFE_Py_EnableInteractivePythonLogging);\n\n  // Additional Context Logic\n  m.def(\"TFE_Py_SetEagerContext\", [](const py::handle& o) {\n    return tensorflow::PyoOrThrow(TFE_Py_SetEagerContext(o.ptr()));\n  });\n  m.def(\"TFE_ContextStartStep\", [](py::handle& o) {\n    TFE_ContextStartStep(tensorflow::InputTFE_Context(o.ptr()));\n  });\n  m.def(\"TFE_ContextEndStep\", [](py::handle& o) {\n    TFE_ContextEndStep(tensorflow::InputTFE_Context(o.ptr()));\n  });\n  m.def(\"TFE_Py_RegisterVSpace\", [](const py::handle& o) {\n    return tensorflow::PyoOrThrow(TFE_Py_RegisterVSpace(o.ptr()));\n  });\n  m.def(\"TFE_Py_EncodeArg\",\n        [](const py::handle& o, bool include_tensor_ranks_only) {\n          return tensorflow::PyoOrThrow(\n              TFE_Py_EncodeArg(o.ptr(), include_tensor_ranks_only));\n        });\n  m.def(\"TFE_EnableCollectiveOps\", [](const py::handle& ctx, py::bytes proto) {\n    tensorflow::Safe_TF_StatusPtr status =\n        tensorflow::make_safe(TF_NewStatus());\n    tensorflow::Safe_TF_BufferPtr buf =\n        tensorflow::make_safe(tensorflow::ProtoStringToTFBuffer(proto.ptr()));\n    TFE_EnableCollectiveOps(tensorflow::InputTFE_Context(ctx), buf.get()->data,\n                            buf.get()->length, status.get());\n    tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n  });\n  m.def(\"TFE_AbortCollectiveOps\", [](const py::handle& ctx, int code,\n                                     const char* message) {\n    tensorflow::Safe_TF_StatusPtr status =\n        tensorflow::make_safe(TF_NewStatus());\n    TF_SetStatus(status.get(), static_cast<TF_Code>(code), message);\n    TFE_AbortCollectiveOps(tensorflow::InputTFE_Context(ctx), status.get());\n  });\n  m.def(\"TFE_CollectiveOpsCheckPeerHealth\",\n        [](const py::handle& ctx, const char* task) {\n          tensorflow::Safe_TF_StatusPtr status =\n              tensorflow::make_safe(TF_NewStatus());\n          TFE_CollectiveOpsCheckPeerHealth(tensorflow::InputTFE_Context(ctx),\n                                           task, status.get());\n          tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n        });\n  m.def(\"TF_ListPhysicalDevices\", &tensorflow::TF_ListPhysicalDevices);\n  m.def(\"TF_GetDeviceDetails\", &tensorflow::TF_GetDeviceDetails);\n  m.def(\"TF_DeleteDeviceList\", &TF_DeleteDeviceList,\n        py::return_value_policy::reference);\n  m.def(\"TF_DeviceListCount\", &TF_DeviceListCount);\n  m.def(\"TF_DeviceListName\", [](const TF_DeviceList* list, int index) {\n    tensorflow::Safe_TF_StatusPtr status =\n        tensorflow::make_safe(TF_NewStatus());\n    auto output = TF_DeviceListName(list, index, status.get());\n    tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n    return output;\n  });\n  m.def(\"TF_DeviceListType\", [](const TF_DeviceList* list, int index) {\n    tensorflow::Safe_TF_StatusPtr status =\n        tensorflow::make_safe(TF_NewStatus());\n    auto output = TF_DeviceListType(list, index, status.get());\n    tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n    return output;\n  });\n\n  m.def(\"TF_PickUnusedPortOrDie\", &TF_PickUnusedPortOrDie);\n\n  // TFE_MonitoringCounter Logic\n  m.def(\"TFE_MonitoringCounterCellIncrementBy\",\n        &TFE_MonitoringCounterCellIncrementBy);\n  m.def(\"TFE_MonitoringCounterCellValue\", &TFE_MonitoringCounterCellValue);\n  m.def(\n      \"TFE_MonitoringNewCounter0\",\n      [](const char* name, const char* description) {\n        tensorflow::Safe_TF_StatusPtr status =\n            tensorflow::make_safe(TF_NewStatus());\n        auto output =\n            TFE_MonitoringNewCounter0(name, status.get(), description);\n        tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n        return output;\n      },\n      py::return_value_policy::reference);\n  m.def(\"TFE_MonitoringDeleteCounter0\", &TFE_MonitoringDeleteCounter0,\n        py::return_value_policy::reference);\n  m.def(\"TFE_MonitoringGetCellCounter0\", &TFE_MonitoringGetCellCounter0,\n        py::return_value_policy::reference);\n  m.def(\n      \"TFE_MonitoringNewCounter1\",\n      [](const char* name, const char* description, const char* label1) {\n        tensorflow::Safe_TF_StatusPtr status =\n            tensorflow::make_safe(TF_NewStatus());\n        auto output =\n            TFE_MonitoringNewCounter1(name, status.get(), description, label1);\n        tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n        return output;\n      },\n      py::return_value_policy::reference);\n  m.def(\"TFE_MonitoringDeleteCounter1\", &TFE_MonitoringDeleteCounter1,\n        py::return_value_policy::reference);\n  m.def(\"TFE_MonitoringGetCellCounter1\", &TFE_MonitoringGetCellCounter1,\n        py::return_value_policy::reference);\n  m.def(\n      \"TFE_MonitoringNewCounter2\",\n      [](const char* name, const char* description, const char* label1,\n         const char* label2) {\n        tensorflow::Safe_TF_StatusPtr status =\n            tensorflow::make_safe(TF_NewStatus());\n        auto output = TFE_MonitoringNewCounter2(name, status.get(), description,\n                                                label1, label2);\n        tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n        return output;\n      },\n      py::return_value_policy::reference);\n  m.def(\"TFE_MonitoringDeleteCounter2\", &TFE_MonitoringDeleteCounter2,\n        py::return_value_policy::reference);\n  m.def(\"TFE_MonitoringGetCellCounter2\", &TFE_MonitoringGetCellCounter2,\n        py::return_value_policy::reference);\n\n  // TFE_MonitoringIntGauge Logic\n  m.def(\"TFE_MonitoringIntGaugeCellSet\", &TFE_MonitoringIntGaugeCellSet);\n  m.def(\"TFE_MonitoringIntGaugeCellValue\", &TFE_MonitoringIntGaugeCellValue);\n  m.def(\n      \"TFE_MonitoringNewIntGauge0\",\n      [](const char* name, const char* description) {\n        tensorflow::Safe_TF_StatusPtr status =\n            tensorflow::make_safe(TF_NewStatus());\n        auto output =\n            TFE_MonitoringNewIntGauge0(name, status.get(), description);\n        tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n        return output;\n      },\n      py::return_value_policy::reference);\n  m.def(\"TFE_MonitoringDeleteIntGauge0\", &TFE_MonitoringDeleteIntGauge0,\n        py::return_value_policy::reference);\n  m.def(\"TFE_MonitoringGetCellIntGauge0\", &TFE_MonitoringGetCellIntGauge0,\n        py::return_value_policy::reference);\n  m.def(\n      \"TFE_MonitoringNewIntGauge1\",\n      [](const char* name, const char* description, const char* label1) {\n        tensorflow::Safe_TF_StatusPtr status =\n            tensorflow::make_safe(TF_NewStatus());\n        auto output =\n            TFE_MonitoringNewIntGauge1(name, status.get(), description, label1);\n        tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n        return output;\n      },\n      py::return_value_policy::reference);\n  m.def(\"TFE_MonitoringDeleteIntGauge1\", &TFE_MonitoringDeleteIntGauge1,\n        py::return_value_policy::reference);\n  m.def(\"TFE_MonitoringGetCellIntGauge1\", &TFE_MonitoringGetCellIntGauge1,\n        py::return_value_policy::reference);\n  m.def(\n      \"TFE_MonitoringNewIntGauge2\",\n      [](const char* name, const char* description, const char* label1,\n         const char* label2) {\n        tensorflow::Safe_TF_StatusPtr status =\n            tensorflow::make_safe(TF_NewStatus());\n        auto output = TFE_MonitoringNewIntGauge2(name, status.get(),\n                                                 description, label1, label2);\n        tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n        return output;\n      },\n      py::return_value_policy::reference);\n  m.def(\"TFE_MonitoringDeleteIntGauge2\", &TFE_MonitoringDeleteIntGauge2,\n        py::return_value_policy::reference);\n  m.def(\"TFE_MonitoringGetCellIntGauge2\", &TFE_MonitoringGetCellIntGauge2,\n        py::return_value_policy::reference);\n  m.def(\"TFE_MonitoringStringGaugeCellSet\", &TFE_MonitoringStringGaugeCellSet);\n  m.def(\"TFE_MonitoringStringGaugeCellValue\",\n        &TFE_MonitoringStringGaugeCellValue);\n  m.def(\n      \"TFE_MonitoringNewStringGauge0\",\n      [](const char* name, const char* description) {\n        tensorflow::Safe_TF_StatusPtr status =\n            tensorflow::make_safe(TF_NewStatus());\n        auto output =\n            TFE_MonitoringNewStringGauge0(name, status.get(), description);\n        tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n        return output;\n      },\n      py::return_value_policy::reference);\n\n  // TFE_MonitoringStringGauge Logic\n  m.def(\"TFE_MonitoringDeleteStringGauge0\", &TFE_MonitoringDeleteStringGauge0);\n  m.def(\"TFE_MonitoringGetCellStringGauge0\", &TFE_MonitoringGetCellStringGauge0,\n        py::return_value_policy::reference);\n  m.def(\n      \"TFE_MonitoringNewStringGauge1\",\n      [](const char* name, const char* description, const char* label1) {\n        tensorflow::Safe_TF_StatusPtr status =\n            tensorflow::make_safe(TF_NewStatus());\n        auto output = TFE_MonitoringNewStringGauge1(name, status.get(),\n                                                    description, label1);\n        tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n        return output;\n      },\n      py::return_value_policy::reference);\n  m.def(\"TFE_MonitoringDeleteStringGauge1\", &TFE_MonitoringDeleteStringGauge1);\n  m.def(\"TFE_MonitoringGetCellStringGauge1\", &TFE_MonitoringGetCellStringGauge1,\n        py::return_value_policy::reference);\n  m.def(\n      \"TFE_MonitoringNewStringGauge2\",\n      [](const char* name, const char* description, const char* label1,\n         const char* label2) {\n        tensorflow::Safe_TF_StatusPtr status =\n            tensorflow::make_safe(TF_NewStatus());\n        auto output = TFE_MonitoringNewStringGauge2(\n            name, status.get(), description, label1, label2);\n        tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n        return output;\n      },\n      py::return_value_policy::reference);\n  m.def(\"TFE_MonitoringDeleteStringGauge2\", &TFE_MonitoringDeleteStringGauge2);\n  m.def(\"TFE_MonitoringGetCellStringGauge2\", &TFE_MonitoringGetCellStringGauge2,\n        py::return_value_policy::reference);\n\n  // TFE_MonitoringBoolGauge Logic\n  m.def(\"TFE_MonitoringBoolGaugeCellSet\", &TFE_MonitoringBoolGaugeCellSet);\n  m.def(\"TFE_MonitoringBoolGaugeCellValue\", &TFE_MonitoringBoolGaugeCellValue);\n  m.def(\n      \"TFE_MonitoringNewBoolGauge0\",\n      [](const char* name, const char* description) {\n        tensorflow::Safe_TF_StatusPtr status =\n            tensorflow::make_safe(TF_NewStatus());\n        auto output =\n            TFE_MonitoringNewBoolGauge0(name, status.get(), description);\n        tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n        return output;\n      },\n      py::return_value_policy::reference);\n  m.def(\"TFE_MonitoringDeleteBoolGauge0\", &TFE_MonitoringDeleteBoolGauge0,\n        py::return_value_policy::reference);\n  m.def(\"TFE_MonitoringGetCellBoolGauge0\", &TFE_MonitoringGetCellBoolGauge0,\n        py::return_value_policy::reference);\n  m.def(\n      \"TFE_MonitoringNewBoolGauge1\",\n      [](const char* name, const char* description, const char* label1) {\n        tensorflow::Safe_TF_StatusPtr status =\n            tensorflow::make_safe(TF_NewStatus());\n        auto output = TFE_MonitoringNewBoolGauge1(name, status.get(),\n                                                  description, label1);\n        tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n        return output;\n      },\n      py::return_value_policy::reference);\n  m.def(\"TFE_MonitoringDeleteBoolGauge1\", &TFE_MonitoringDeleteBoolGauge1,\n        py::return_value_policy::reference);\n  m.def(\"TFE_MonitoringGetCellBoolGauge1\", &TFE_MonitoringGetCellBoolGauge1,\n        py::return_value_policy::reference);\n  m.def(\n      \"TFE_MonitoringNewBoolGauge2\",\n      [](const char* name, const char* description, const char* label1,\n         const char* label2) {\n        tensorflow::Safe_TF_StatusPtr status =\n            tensorflow::make_safe(TF_NewStatus());\n        auto output = TFE_MonitoringNewBoolGauge2(name, status.get(),\n                                                  description, label1, label2);\n        tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n        return output;\n      },\n      py::return_value_policy::reference);\n  m.def(\"TFE_MonitoringDeleteBoolGauge2\", &TFE_MonitoringDeleteBoolGauge2,\n        py::return_value_policy::reference);\n  m.def(\"TFE_MonitoringGetCellBoolGauge2\", &TFE_MonitoringGetCellBoolGauge2,\n        py::return_value_policy::reference);\n\n  // TFE_MonitoringSampler Logic\n  m.def(\"TFE_MonitoringSamplerCellAdd\", &TFE_MonitoringSamplerCellAdd);\n  m.def(\"TFE_MonitoringSamplerCellValue\", &TFE_MonitoringSamplerCellValue);\n  m.def(\"TFE_MonitoringNewExponentialBuckets\",\n        &TFE_MonitoringNewExponentialBuckets,\n        py::return_value_policy::reference);\n  m.def(\"TFE_MonitoringDeleteBuckets\", &TFE_MonitoringDeleteBuckets,\n        py::return_value_policy::reference);\n  m.def(\n      \"TFE_MonitoringNewSampler0\",\n      [](const char* name, TFE_MonitoringBuckets* buckets,\n         const char* description) {\n        tensorflow::Safe_TF_StatusPtr status =\n            tensorflow::make_safe(TF_NewStatus());\n        auto output =\n            TFE_MonitoringNewSampler0(name, buckets, status.get(), description);\n        tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n        return output;\n      },\n      py::return_value_policy::reference);\n  m.def(\"TFE_MonitoringDeleteSampler0\", &TFE_MonitoringDeleteSampler0,\n        py::return_value_policy::reference);\n  m.def(\"TFE_MonitoringGetCellSampler0\", &TFE_MonitoringGetCellSampler0,\n        py::return_value_policy::reference);\n  m.def(\n      \"TFE_MonitoringNewSampler1\",\n      [](const char* name, TFE_MonitoringBuckets* buckets,\n         const char* description, const char* label1) {\n        tensorflow::Safe_TF_StatusPtr status =\n            tensorflow::make_safe(TF_NewStatus());\n        auto output = TFE_MonitoringNewSampler1(name, buckets, status.get(),\n                                                description, label1);\n        tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n        return output;\n      },\n      py::return_value_policy::reference);\n  m.def(\"TFE_MonitoringDeleteSampler1\", &TFE_MonitoringDeleteSampler1,\n        py::return_value_policy::reference);\n  m.def(\"TFE_MonitoringGetCellSampler1\", &TFE_MonitoringGetCellSampler1,\n        py::return_value_policy::reference);\n  m.def(\n      \"TFE_MonitoringNewSampler2\",\n      [](const char* name, TFE_MonitoringBuckets* buckets,\n         const char* description, const char* label1, const char* label2) {\n        tensorflow::Safe_TF_StatusPtr status =\n            tensorflow::make_safe(TF_NewStatus());\n        auto output = TFE_MonitoringNewSampler2(name, buckets, status.get(),\n                                                description, label1, label2);\n        tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n        return output;\n      },\n      py::return_value_policy::reference);\n  m.def(\"TFE_MonitoringDeleteSampler2\", &TFE_MonitoringDeleteSampler2,\n        py::return_value_policy::reference);\n  m.def(\"TFE_MonitoringGetCellSampler2\", &TFE_MonitoringGetCellSampler2,\n        py::return_value_policy::reference);\n\n  // TFE_CancellationManager Logic\n  m.def(\"TFE_NewCancellationManager\", &TFE_NewCancellationManager,\n        py::return_value_policy::reference);\n  m.def(\"TFE_CancellationManagerIsCancelled\",\n        &TFE_CancellationManagerIsCancelled);\n  m.def(\"TFE_CancellationManagerStartCancel\",\n        &TFE_CancellationManagerStartCancel);\n  m.def(\"TFE_DeleteCancellationManager\", &TFE_DeleteCancellationManager,\n        py::return_value_policy::reference);\n\n  m.def(\"TFE_ClearScalarCache\", &tensorflow::TFE_ClearScalarCache);\n\n  // Util buffer helper functions\n  m.def(\"TF_NewBufferFromString\", &TF_NewBufferFromString,\n        py::return_value_policy::reference);\n\n  // DLPack functions\n  m.def(\"TFE_ToDlpackCapsule\", [](py::handle& o) {\n    PyObject* eager_tensor_pyobject_ptr = o.ptr();\n    TFE_TensorHandle* thandle = EagerTensor_Handle(eager_tensor_pyobject_ptr);\n    tensorflow::Safe_TF_StatusPtr status =\n        tensorflow::make_safe(TF_NewStatus());\n    void* dlm_ptr = tensorflow::TFE_HandleToDLPack(thandle, status.get());\n    tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n\n    py::capsule capsule(\n        dlm_ptr, tensorflow::kDlTensorCapsuleName, [](PyObject* capsule) {\n          if (PyCapsule_IsValid(capsule, tensorflow::kDlTensorCapsuleName)) {\n            void* dlm_rptr =\n                PyCapsule_GetPointer(capsule, tensorflow::kDlTensorCapsuleName);\n            if (dlm_rptr) {\n              tensorflow::TFE_CallDLManagedTensorDeleter(dlm_rptr);\n              PyCapsule_SetDestructor(capsule, nullptr);\n            }\n          }\n        });\n    return capsule;\n  });\n\n  m.def(\"TFE_FromDlpackCapsule\", [](const py::capsule& pycapsule,\n                                    const py::handle& context) {\n    tensorflow::Safe_TF_StatusPtr status =\n        tensorflow::make_safe(TF_NewStatus());\n    if (absl::string_view(pycapsule.name()) !=\n        tensorflow::kDlTensorCapsuleName) {\n      status->status = tensorflow::errors::InvalidArgument(\n          \"DLPack tensor must be a capsule with name \\\"dltensor\\\", got \\\"%s\\\". \"\n          \"Note that a DLPack tensor may be consumed at most once.\",\n          absl::string_view(pycapsule.name()));\n      tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n    }\n\n    TFE_TensorHandle* thandle = tensorflow::TFE_HandleFromDLPack(\n        pycapsule, status.get(), tensorflow::InputTFE_Context(context));\n\n    tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n\n    PyCapsule_SetName(pycapsule.ptr(), \"used_dltensor\");\n    PyCapsule_SetDestructor(pycapsule.ptr(), nullptr);\n\n    PyObject* pyhandle = EagerTensorFromHandle(thandle);\n    return tensorflow::PyoOrThrow(pyhandle);\n  });\n\n  m.def(\"TFE_Py_RegisterCustomDevice\", [](const py::handle& context,\n                                          const py::capsule& device,\n                                          const char* device_name,\n                                          const py::capsule& device_info) {\n    tensorflow::Safe_TF_StatusPtr status =\n        tensorflow::make_safe(TF_NewStatus());\n    if (absl::string_view(device.name()) != \"TFE_CustomDevice\") {\n      status->status = tensorflow::errors::InvalidArgument(\n          \"Expected a capsule named 'TFE_CustomDevice' for the `device` \"\n          \"argument, got \",\n          absl::string_view(device.name()));\n      tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n    }\n    if (absl::string_view(device_info.name()) !=\n        \"TFE_CustomDevice_DeviceInfo\") {\n      status->status = tensorflow::errors::InvalidArgument(\n          \"Expected a capsule named 'TFE_CustomDevice_DeviceInfo' for \"\n          \"the `device_info` argument, got \",\n          absl::string_view(device_info.name()));\n      tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n    }\n    // TFE_RegisterCustomDevice takes ownership\n    PyCapsule_SetDestructor(device_info.ptr(), nullptr);\n    TFE_RegisterCustomDevice(\n        tensorflow::InputTFE_Context(context),\n        *reinterpret_cast<TFE_CustomDevice*>(\n            PyCapsule_GetPointer(device.ptr(), \"TFE_CustomDevice\")),\n        device_name,\n        PyCapsule_GetPointer(device_info.ptr(), \"TFE_CustomDevice_DeviceInfo\"),\n        status.get());\n    tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n  });\n\n  py::class_<EagerContextThreadLocalDataWrapper>(m,\n                                                 \"EagerContextThreadLocalData\")\n      .def(py::init<py::handle, py::handle, py::handle>(),\n           py::arg(\"py_eager_context\"), py::arg(\"is_eager\"),\n           py::arg(\"device_spec\"))\n      .def_property(\"is_eager\",\n                    &EagerContextThreadLocalDataWrapper::get_is_eager,\n                    &EagerContextThreadLocalDataWrapper::set_is_eager)\n      .def_property(\n          \"invoking_op_callbacks\",\n          &EagerContextThreadLocalDataWrapper::get_invoking_op_callbacks,\n          &EagerContextThreadLocalDataWrapper::set_invoking_op_callbacks)\n      .def_property(\"device_name\",\n                    &EagerContextThreadLocalDataWrapper::get_device_name,\n                    &EagerContextThreadLocalDataWrapper::set_device_name)\n      .def_property(\"scope_name\",\n                    &EagerContextThreadLocalDataWrapper::get_scope_name,\n                    &EagerContextThreadLocalDataWrapper::set_scope_name)\n      .def_property(\"device_spec\",\n                    &EagerContextThreadLocalDataWrapper::get_device_spec,\n                    &EagerContextThreadLocalDataWrapper::set_device_spec)\n      .def_property(\n          \"function_call_options\",\n          &EagerContextThreadLocalDataWrapper::get_function_call_options,\n          &EagerContextThreadLocalDataWrapper::set_function_call_options)\n      .def_property(\"executor\",\n                    &EagerContextThreadLocalDataWrapper::get_executor,\n                    &EagerContextThreadLocalDataWrapper::set_executor)\n      .def_property(\"op_callbacks\",\n                    &EagerContextThreadLocalDataWrapper::get_op_callbacks,\n                    &EagerContextThreadLocalDataWrapper::set_op_callbacks);\n\n  // C API Enum\n\n  py::enum_<TFE_ContextDevicePlacementPolicy>(\n      m, \"TFE_ContextDevicePlacementPolicy\")\n      .value(\"TFE_DEVICE_PLACEMENT_EXPLICIT\", TFE_DEVICE_PLACEMENT_EXPLICIT)\n      .value(\"TFE_DEVICE_PLACEMENT_WARN\", TFE_DEVICE_PLACEMENT_WARN)\n      .value(\"TFE_DEVICE_PLACEMENT_SILENT\", TFE_DEVICE_PLACEMENT_SILENT)\n      .value(\"TFE_DEVICE_PLACEMENT_SILENT_FOR_INT32\",\n             TFE_DEVICE_PLACEMENT_SILENT_FOR_INT32)\n      .export_values();\n\n  py::enum_<TF_AttrType>(m, \"TF_AttrType\")\n      .value(\"TF_ATTR_STRING\", TF_ATTR_STRING)\n      .value(\"TF_ATTR_INT\", TF_ATTR_INT)\n      .value(\"TF_ATTR_FLOAT\", TF_ATTR_FLOAT)\n      .value(\"TF_ATTR_BOOL\", TF_ATTR_BOOL)\n      .value(\"TF_ATTR_TYPE\", TF_ATTR_TYPE)\n      .value(\"TF_ATTR_SHAPE\", TF_ATTR_SHAPE)\n      .value(\"TF_ATTR_TENSOR\", TF_ATTR_TENSOR)\n      .value(\"TF_ATTR_PLACEHOLDER\", TF_ATTR_PLACEHOLDER)\n      .value(\"TF_ATTR_FUNC\", TF_ATTR_FUNC)\n      .export_values();\n};\n"], "fixing_code": ["/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include \"tensorflow/c/eager/dlpack.h\"\n\n#include \"include/dlpack/dlpack.h\"  // from @dlpack\n#include \"tensorflow/c/eager/c_api.h\"\n#include \"tensorflow/c/eager/c_api_experimental.h\"\n#include \"tensorflow/c/eager/tfe_tensorhandle_internal.h\"\n#include \"tensorflow/c/tf_status_internal.h\"\n#include \"tensorflow/core/common_runtime/eager/tensor_handle.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/tensor_reference.h\"\n#include \"tensorflow/core/platform/logging.h\"\n\nnamespace tensorflow {\n\nnamespace {\n\n// Managing context for the DLManagedTensor, will manage the lifetime of\n// DLManagedTensor. When calling DLManagedTensor::deleter, it will notify the\n// original framework of destruction, and this context will be deleted also.\nstruct TfDlManagedTensorCtx {\n  TensorReference reference;\n  std::vector<int64_t> shape;\n  std::vector<int64_t> strides;\n  DLManagedTensor tensor;\n\n  explicit TfDlManagedTensorCtx(const TensorReference& ref) : reference(ref) {}\n};\n\n// Gets tensor from eager tensor handle.\nconst Tensor* GetTensorFromHandle(TFE_TensorHandle* h, TF_Status* status) {\n  if (h == nullptr) {\n    status->status = tensorflow::errors::InvalidArgument(\"Invalid handle\");\n    return nullptr;\n  }\n  tensorflow::TensorHandle* handle =\n      tensorflow::TensorHandleFromInterface(tensorflow::unwrap(h));\n  if (handle->Type() != TensorHandle::LOCAL) {\n    status->status = tensorflow::errors::InvalidArgument(\n        \"DLPack doesn't support \", handle->TypeString(), \" tensor\");\n    return nullptr;\n  }\n  const tensorflow::Tensor* tensor;\n  status->status = handle->Tensor(&tensor);\n  if (!status->status.ok()) {\n    return nullptr;\n  }\n  return tensor;\n}\n\n// Deleter for DLManagedTensor\nvoid DLManagedTensorDeleter(DLManagedTensor* arg) {\n  TfDlManagedTensorCtx* owner =\n      static_cast<TfDlManagedTensorCtx*>(arg->manager_ctx);\n  owner->reference.Unref();\n  delete owner;\n}\n\n// Converts TF_DATAType to DLPack data type.\nDLDataType GetDlDataType(TF_DataType data_type, TF_Status* status) {\n  DLDataType dtype;\n  dtype.lanes = 1;\n  dtype.bits = TF_DataTypeSize(data_type) * 8;\n  switch (data_type) {\n    case TF_DataType::TF_HALF:\n    case TF_DataType::TF_FLOAT:\n    case TF_DataType::TF_DOUBLE:\n      dtype.code = DLDataTypeCode::kDLFloat;\n      break;\n    case TF_DataType::TF_INT8:\n    case TF_DataType::TF_INT16:\n    case TF_DataType::TF_INT32:\n    case TF_DataType::TF_INT64:\n      dtype.code = DLDataTypeCode::kDLInt;\n      break;\n    case TF_DataType::TF_BOOL:\n    case TF_DataType::TF_UINT8:\n    case TF_DataType::TF_UINT16:\n    case TF_DataType::TF_UINT32:\n    case TF_DataType::TF_UINT64:\n      dtype.code = DLDataTypeCode::kDLUInt;\n      break;\n    case TF_DataType::TF_BFLOAT16:\n      dtype.code = DLDataTypeCode::kDLBfloat;\n      break;\n    default:\n      status->status = tensorflow::errors::InvalidArgument(\n          DataType_Name(static_cast<DataType>(data_type)),\n          \" is not supported by dlpack\");\n      break;\n  }\n  return dtype;\n}\n\n// Gets DLPack's DLContext from eager tensor handle.\nDLContext GetDlContext(TFE_TensorHandle* h, TF_Status* status) {\n  DLContext ctx;\n  const char* device_name =\n      tensorflow::unwrap(h)->BackingDeviceName(&status->status);\n  DeviceNameUtils::ParsedName parsed_name;\n  tensorflow::DeviceNameUtils::ParseFullName(device_name, &parsed_name);\n  std::string device_type = parsed_name.type;\n  int device_id = 0;\n  if (parsed_name.has_id) {\n    device_id = parsed_name.id;\n  }\n\n  ctx.device_id = device_id;\n  if (device_type == \"CPU\") {\n    ctx.device_type = DLDeviceType::kDLCPU;\n  } else if (device_type == \"GPU\") {\n    ctx.device_type = DLDeviceType::kDLGPU;\n  } else {\n    status->status = tensorflow::errors::InvalidArgument(\n        \"Unsupported Device Type for dlpack\");\n  }\n\n  return ctx;\n}\n\n// Converts DLContext to TF device name.\nabsl::optional<std::string> DeviceNameFromDlContext(const DLContext& ctx,\n                                                    TF_Status* status) {\n  switch (ctx.device_type) {\n    case DLDeviceType::kDLCPU:\n      return \"CPU:0\";\n    case DLDeviceType::kDLGPU:\n      return absl::StrCat(\"GPU:\", ctx.device_id);\n    default:\n      return absl::nullopt;\n  }\n}\n\n// Converts DLPack data type to TF_DATATYPE.\nStatus TfDataTypeFormDlDataType(const DLDataType& dtype,\n                                TF_DataType* tf_dtype) {\n  switch (dtype.code) {\n    case DLDataTypeCode::kDLUInt:\n      switch (dtype.bits) {\n        case 8:\n          *tf_dtype = TF_DataType::TF_UINT8;\n          return Status::OK();\n        case 16:\n          *tf_dtype = TF_DataType::TF_UINT16;\n          return Status::OK();\n        case 32:\n          *tf_dtype = TF_DataType::TF_UINT32;\n          return Status::OK();\n        case 64:\n          *tf_dtype = TF_DataType::TF_UINT64;\n          return Status::OK();\n        default:\n          return tensorflow::errors::InvalidArgument(\"Unsupported UInt bits: \",\n                                                     dtype.bits);\n      }\n      return Status::OK();\n    case DLDataTypeCode::kDLInt:\n      switch (dtype.bits) {\n        case 8:\n          *tf_dtype = TF_DataType::TF_INT8;\n          return Status::OK();\n        case 16:\n          *tf_dtype = TF_DataType::TF_INT16;\n          return Status::OK();\n        case 32:\n          *tf_dtype = TF_DataType::TF_INT32;\n          return Status::OK();\n        case 64:\n          *tf_dtype = TF_DataType::TF_INT64;\n          return Status::OK();\n        default:\n          return tensorflow::errors::InvalidArgument(\"Unsupported Int bits: \",\n                                                     dtype.bits);\n      }\n      return Status::OK();\n    case DLDataTypeCode::kDLFloat:\n      switch (dtype.bits) {\n        case 16:\n          *tf_dtype = TF_DataType::TF_HALF;\n          return Status::OK();\n        case 32:\n          *tf_dtype = TF_DataType::TF_FLOAT;\n          return Status::OK();\n        case 64:\n          *tf_dtype = TF_DataType::TF_DOUBLE;\n          return Status::OK();\n        default:\n          return tensorflow::errors::InvalidArgument(\"Unsupported Float bits: \",\n                                                     dtype.bits);\n      }\n      break;\n    case DLDataTypeCode::kDLBfloat:\n      switch (dtype.bits) {\n        case 16:\n          *tf_dtype = TF_DataType::TF_BFLOAT16;\n          return Status::OK();\n        default:\n          return tensorflow::errors::InvalidArgument(\n              \"Unsupported BFloat bits: \", dtype.bits);\n      }\n      break;\n    default:\n      return tensorflow::errors::InvalidArgument(\"Unsupported Type Codes: \",\n                                                 dtype.code);\n  }\n}\n\n// Wraps the deleter function of DLManagedTensor to match the function signature\n// TFE_NewTensorHandleFromDeviceMemory.\nvoid DeallocatorWrapperFunc(void* data, size_t len, void* dlmt_vptr) {\n  TFE_CallDLManagedTensorDeleter(dlmt_vptr);\n}\n\n// Checks whether the stride array matches the layout of compact, row-majored\n// data.\nbool IsValidStrideCompactRowMajorData(int64_t* shape_arr, int64_t* stride_arr,\n                                      int ndim) {\n  if (ndim >= 1 && stride_arr[ndim - 1] != 1) {\n    return false;\n  }\n  for (int i = ndim - 2; i >= 0; --i) {\n    if (stride_arr[i] != shape_arr[i + 1] * stride_arr[i + 1]) {\n      return false;\n    }\n  }\n  return true;\n}\n}  // namespace\n\nvoid TFE_CallDLManagedTensorDeleter(void* dlm_ptr) {\n  DLManagedTensor* dlMTensor = static_cast<DLManagedTensor*>(dlm_ptr);\n  if (dlMTensor->deleter != nullptr) {\n    dlMTensor->deleter(dlMTensor);\n  }\n}\n\nvoid* TFE_HandleToDLPack(TFE_TensorHandle* h, TF_Status* status) {\n  auto tf_dlm_context = GetDlContext(h, status);\n  if (!status->status.ok()) {\n    return nullptr;\n  }\n\n  auto* tf_dlm_data = TFE_TensorHandleDevicePointer(h, status);\n  if (!status->status.ok()) {\n    return nullptr;\n  }\n\n  const Tensor* tensor = GetTensorFromHandle(h, status);\n  TF_DataType data_type = static_cast<TF_DataType>(tensor->dtype());\n\n  auto tf_dlm_type = GetDlDataType(data_type, status);\n  if (!status->status.ok()) {\n    return nullptr;\n  }\n\n  TensorReference tensor_ref(*tensor);  // This will call buf_->Ref()\n  auto* tf_dlm_tensor_ctx = new TfDlManagedTensorCtx(tensor_ref);\n  tf_dlm_tensor_ctx->reference = tensor_ref;\n\n  DLManagedTensor* dlm_tensor = &tf_dlm_tensor_ctx->tensor;\n  dlm_tensor->manager_ctx = tf_dlm_tensor_ctx;\n  dlm_tensor->deleter = &DLManagedTensorDeleter;\n  dlm_tensor->dl_tensor.ctx = tf_dlm_context;\n  int ndim = tensor->dims();\n  dlm_tensor->dl_tensor.ndim = ndim;\n  dlm_tensor->dl_tensor.data = tf_dlm_data;\n  dlm_tensor->dl_tensor.dtype = tf_dlm_type;\n\n  std::vector<int64_t>* shape_arr = &tf_dlm_tensor_ctx->shape;\n  std::vector<int64_t>* stride_arr = &tf_dlm_tensor_ctx->strides;\n  shape_arr->resize(ndim);\n  stride_arr->resize(ndim, 1);\n  for (int i = 0; i < ndim; i++) {\n    (*shape_arr)[i] = tensor->dim_size(i);\n  }\n  for (int i = ndim - 2; i >= 0; --i) {\n    (*stride_arr)[i] = (*shape_arr)[i + 1] * (*stride_arr)[i + 1];\n  }\n\n  dlm_tensor->dl_tensor.shape = shape_arr->data();\n  // There are two ways to represent compact row-major data\n  // 1) nullptr indicates tensor is compact and row-majored.\n  // 2) fill in the strides array as the real case for compact row-major data.\n  // Here we choose option 2, since some frameworks didn't handle the strides\n  // argument properly.\n  dlm_tensor->dl_tensor.strides = stride_arr->data();\n\n  dlm_tensor->dl_tensor.byte_offset =\n      0;  // TF doesn't handle the strides and byte_offsets here\n  return static_cast<void*>(dlm_tensor);\n}\n\nTFE_TensorHandle* TFE_HandleFromDLPack(void* dlm, TF_Status* status,\n                                       TFE_Context* ctx) {\n  DLManagedTensor* dlmt = static_cast<DLManagedTensor*>(dlm);\n  DLTensor* dl_tensor = &dlmt->dl_tensor;\n  absl::optional<std::string> device_name =\n      DeviceNameFromDlContext(dl_tensor->ctx, status);\n  if (!device_name.has_value()) {\n    status->status =\n        tensorflow::errors::InvalidArgument(\"Unsupported Device Type\");\n    return nullptr;\n  }\n  TF_DataType dtype;\n  Status s = TfDataTypeFormDlDataType(dl_tensor->dtype, &dtype);\n  if (!s.ok()) {\n    status->status = std::move(s);\n    return nullptr;\n  }\n  int num_dims = dl_tensor->ndim;\n  const int64_t* dims = dl_tensor->shape;\n  void* data = dl_tensor->data;\n\n  size_t total_bytes = dl_tensor->dtype.bits / 8;\n  for (int i = 0; i < num_dims; i++) {\n    total_bytes *= dims[i];\n  }\n\n  if (dl_tensor->strides != nullptr &&\n      !IsValidStrideCompactRowMajorData(dl_tensor->shape, dl_tensor->strides,\n                                        num_dims)) {\n    status->status = tensorflow::errors::InvalidArgument(\n        \"Invalid strides array from DLPack\");\n    return nullptr;\n  }\n\n  TFE_TensorHandle* handle = TFE_NewTensorHandleFromDeviceMemory(\n      ctx, device_name.value().c_str(), dtype, dims, num_dims, data,\n      total_bytes, &DeallocatorWrapperFunc, dlmt, status);\n\n  return handle;\n}\n\n}  // namespace tensorflow\n", "load(\"//tensorflow:tensorflow.bzl\", \"cuda_py_test\")\n\npackage(\n    default_visibility = [\"//visibility:private\"],\n    licenses = [\"notice\"],  # Apache 2.0\n)\n\npy_library(\n    name = \"dlpack\",\n    srcs = [\"dlpack.py\"],\n    srcs_version = \"PY2AND3\",\n    visibility = [\"//tensorflow:__subpackages__\"],\n    deps = [\n        \"//tensorflow/python:pywrap_tensorflow\",\n    ],\n)\n\ncuda_py_test(\n    name = \"dlpack_test\",\n    srcs = [\"dlpack_test.py\"],\n    srcs_version = \"PY2AND3\",\n    deps = [\n        \":dlpack\",\n        \"//tensorflow/python/eager:test\",\n        \"@absl_py//absl/testing:absltest\",\n        \"@absl_py//absl/testing:parameterized\",\n    ],\n)\n", "# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for DLPack functions.\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nimport numpy as np\n\n\nfrom tensorflow.python.dlpack import dlpack\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.platform import test\nfrom tensorflow.python.ops import array_ops\n\nint_dtypes = [\n    np.int8, np.int16, np.int32, np.int64, np.uint8, np.uint16, np.uint32,\n    np.uint64\n]\nfloat_dtypes = [np.float16, np.float32, np.float64]\ncomplex_dtypes = [np.complex64, np.complex128]\ndlpack_dtypes = int_dtypes + float_dtypes + [dtypes.bfloat16]\n\ntestcase_shapes = [(), (1,), (2, 3), (2, 0), (0, 7), (4, 1, 2)]\n\n\ndef FormatShapeAndDtype(shape, dtype):\n  return \"_{}[{}]\".format(str(dtype), \",\".join(map(str, shape)))\n\n\ndef GetNamedTestParameters():\n  result = []\n  for dtype in dlpack_dtypes:\n    for shape in testcase_shapes:\n      result.append({\n          \"testcase_name\": FormatShapeAndDtype(shape, dtype),\n          \"dtype\": dtype,\n          \"shape\": shape\n      })\n  return result\n\n\nclass DLPackTest(parameterized.TestCase, test.TestCase):\n\n  @parameterized.named_parameters(GetNamedTestParameters())\n  def testRoundTrip(self, dtype, shape):\n    np.random.seed(42)\n    np_array = np.random.randint(0, 10, shape)\n    # copy to gpu if available\n    tf_tensor = array_ops.identity(constant_op.constant(np_array, dtype=dtype))\n    tf_tensor_device = tf_tensor.device\n    tf_tensor_dtype = tf_tensor.dtype\n    dlcapsule = dlpack.to_dlpack(tf_tensor)\n    del tf_tensor  # should still work\n    tf_tensor2 = dlpack.from_dlpack(dlcapsule)\n    self.assertAllClose(np_array, tf_tensor2)\n    if tf_tensor_dtype == dtypes.int32:\n      # int32 tensor is always on cpu for now\n      self.assertEqual(tf_tensor2.device,\n                       \"/job:localhost/replica:0/task:0/device:CPU:0\")\n    else:\n      self.assertEqual(tf_tensor_device, tf_tensor2.device)\n\n  def testTensorsCanBeConsumedOnceOnly(self):\n    np.random.seed(42)\n    np_array = np.random.randint(0, 10, (2, 3, 4))\n    tf_tensor = constant_op.constant(np_array, dtype=np.float32)\n    dlcapsule = dlpack.to_dlpack(tf_tensor)\n    del tf_tensor  # should still work\n    _ = dlpack.from_dlpack(dlcapsule)\n\n    def ConsumeDLPackTensor():\n      dlpack.from_dlpack(dlcapsule)  # Should can be consumed only once\n\n    self.assertRaisesRegex(Exception,\n                           \".*a DLPack tensor may be consumed at most once.*\",\n                           ConsumeDLPackTensor)\n\n  def testUnsupportedTypeToDLPack(self):\n\n    def UnsupportedQint16():\n      tf_tensor = constant_op.constant([[1, 4], [5, 2]], dtype=dtypes.qint16)\n      _ = dlpack.to_dlpack(tf_tensor)\n\n    def UnsupportedComplex64():\n      tf_tensor = constant_op.constant([[1, 4], [5, 2]], dtype=dtypes.complex64)\n      _ = dlpack.to_dlpack(tf_tensor)\n\n    self.assertRaisesRegex(Exception, \".* is not supported by dlpack\",\n                           UnsupportedQint16)\n    self.assertRaisesRegex(Exception, \".* is not supported by dlpack\",\n                           UnsupportedComplex64)\n\n  def testMustPassTensorArgumentToDLPack(self):\n    with self.assertRaisesRegex(\n        errors.InvalidArgumentError,\n        \"The argument to `to_dlpack` must be a TF tensor, not Python object\"):\n      dlpack.to_dlpack([1])\n\n\nif __name__ == \"__main__\":\n  ops.enable_eager_execution()\n  test.main()\n", "/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");;\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include <memory>\n\n#include \"Python.h\"\n#include \"absl/strings/str_format.h\"\n#include \"pybind11/chrono.h\"\n#include \"pybind11/complex.h\"\n#include \"pybind11/functional.h\"\n#include \"pybind11/pybind11.h\"\n#include \"pybind11/stl.h\"\n#include \"tensorflow/c/c_api.h\"\n#include \"tensorflow/c/c_api_experimental.h\"\n#include \"tensorflow/c/eager/c_api.h\"\n#include \"tensorflow/c/eager/c_api_experimental.h\"\n#include \"tensorflow/c/eager/c_api_internal.h\"\n#include \"tensorflow/c/eager/dlpack.h\"\n#include \"tensorflow/c/eager/tfe_tensorhandle_internal.h\"\n#include \"tensorflow/c/tf_status.h\"\n#include \"tensorflow/c/tf_status_helper.h\"\n#include \"tensorflow/compiler/jit/flags.h\"\n#include \"tensorflow/compiler/jit/get_compiler_ir.h\"\n#include \"tensorflow/python/eager/pywrap_tensor_conversion.h\"\n#include \"tensorflow/python/eager/pywrap_tfe.h\"\n#include \"tensorflow/python/lib/core/py_exception_registry.h\"\n#include \"tensorflow/python/lib/core/pybind11_lib.h\"\n#include \"tensorflow/python/lib/core/pybind11_status.h\"\n#include \"tensorflow/python/lib/core/safe_ptr.h\"\n#include \"tensorflow/python/lib/core/safe_pyobject_ptr.h\"\n#include \"tensorflow/python/util/util.h\"\n\nnamespace py = pybind11;\n\nPYBIND11_MAKE_OPAQUE(TFE_Executor);\nPYBIND11_MAKE_OPAQUE(TFE_ContextOptions);\nPYBIND11_MAKE_OPAQUE(TFE_CancellationManager);\n\nPYBIND11_MAKE_OPAQUE(TFE_MonitoringCounter0);\nPYBIND11_MAKE_OPAQUE(TFE_MonitoringCounter1);\nPYBIND11_MAKE_OPAQUE(TFE_MonitoringCounter2);\nPYBIND11_MAKE_OPAQUE(TFE_MonitoringStringGauge0);\nPYBIND11_MAKE_OPAQUE(TFE_MonitoringStringGauge1);\nPYBIND11_MAKE_OPAQUE(TFE_MonitoringStringGauge2);\nPYBIND11_MAKE_OPAQUE(TFE_MonitoringIntGauge0);\nPYBIND11_MAKE_OPAQUE(TFE_MonitoringIntGauge1);\nPYBIND11_MAKE_OPAQUE(TFE_MonitoringIntGauge2);\nPYBIND11_MAKE_OPAQUE(TFE_MonitoringBoolGauge0);\nPYBIND11_MAKE_OPAQUE(TFE_MonitoringBoolGauge1);\nPYBIND11_MAKE_OPAQUE(TFE_MonitoringBoolGauge2);\nPYBIND11_MAKE_OPAQUE(TFE_MonitoringSampler0);\nPYBIND11_MAKE_OPAQUE(TFE_MonitoringSampler1);\nPYBIND11_MAKE_OPAQUE(TFE_MonitoringSampler2);\nPYBIND11_MAKE_OPAQUE(TFE_MonitoringCounterCell);\nPYBIND11_MAKE_OPAQUE(TFE_MonitoringIntGaugeCell);\nPYBIND11_MAKE_OPAQUE(TFE_MonitoringStringGaugeCell);\nPYBIND11_MAKE_OPAQUE(TFE_MonitoringBoolGaugeCell);\nPYBIND11_MAKE_OPAQUE(TFE_MonitoringSamplerCell);\n\nPYBIND11_MAKE_OPAQUE(TF_DeviceList);\nPYBIND11_MAKE_OPAQUE(TF_Function);\nPYBIND11_MAKE_OPAQUE(TF_Buffer);\n\n// Eager helper functions migrated from pywrap_tfe.i.\n\nnamespace tensorflow {\n\n// We cannot use Context as an opaque type. SWIG also had\n// difficult directly passing the pointer around. These\n// typemaps are migrated over from pywrap_tfe.i. I tried\n// using a custom type caster, but we get segfaults periodically.\n\n// TODO(amitpatankar): Move input and output logic of Context into a\n// pybind11 custom type caster.\n\nTFE_Context* InputTFE_Context(const py::handle& ctx) {\n  return static_cast<TFE_Context*>(PyCapsule_GetPointer(ctx.ptr(), nullptr));\n}\n\nPyObject* OutputTFE_Context(TFE_Context* context) {\n  return PyCapsule_New(context, nullptr, TFE_DeleteContextCapsule);\n}\n\nTF_Buffer* ProtoStringToTFBuffer(PyObject* input) {\n  // Convert a Python string object to TF_Buffer.\n  char* c_string;\n  Py_ssize_t py_size;\n  // PyBytes_AsStringAndSize() does not copy but simply interprets the input\n  if (PyBytes_AsStringAndSize(input, &c_string, &py_size) == -1) {\n    // Python has raised an error (likely TypeError or UnicodeEncodeError).\n    throw py::error_already_set();\n  }\n  return TF_NewBufferFromString(static_cast<void*>(c_string),\n                                static_cast<size_t>(py_size));\n}\n\n// These functions are typemaps from the Python side. I did not use\n// a custom type caster since the logic is slightly harder to follow. This\n// converter is also only used once in `TFE_Py_ExecuteCancelable_wrapper`.\nTFE_InputTensorHandles InputTFE_InputTensorHandles(\n    const py::handle& input_tensors) {\n  TFE_InputTensorHandles input_tensor_handles;\n  if (input_tensors.ptr() != Py_None) {\n    if (!PyList_Check(input_tensors.ptr())) {\n      tensorflow::ThrowTypeError(\"must provide a list of Tensors as inputs\");\n    }\n    Py_ssize_t len = PyList_Size(input_tensors.ptr());\n    input_tensor_handles.resize(len);\n    for (Py_ssize_t i = 0; i < len; ++i) {\n      PyObject* elem = PyList_GetItem(input_tensors.ptr(), i);\n      if (!elem) {\n        tensorflow::ThrowTypeError(\"Input Tensor does not exist.\");\n      }\n      if (EagerTensor_CheckExact(elem)) {\n        (input_tensor_handles)[i] = EagerTensor_Handle(elem);\n      } else if (tensorflow::swig::IsEagerTensorSlow(elem)) {\n        // Use equivalent of object.__getattribute__ to get the underlying\n        // tf wrapped EagerTensor (if there is one).\n        tensorflow::Safe_PyObjectPtr tf_should_use_attr(\n#if PY_MAJOR_VERSION < 3\n            PyString_InternFromString(\"_tf_should_use_wrapped_value\")\n#else\n            PyUnicode_InternFromString(\"_tf_should_use_wrapped_value\")\n#endif\n        );\n        tensorflow::Safe_PyObjectPtr value_attr(\n            PyObject_GenericGetAttr(elem, tf_should_use_attr.get()));\n        if (value_attr) {\n          // This is an EagerTensor wrapped inside a TFShouldUse wrapped object.\n          (input_tensor_handles)[i] = EagerTensor_Handle(value_attr.get());\n        } else {\n          // This is a subclass of EagerTensor that we don't support.\n          PyErr_Clear();\n          tensorflow::ThrowTypeError(\n              tensorflow::strings::StrCat(\n                  \"Saw an object that is an instance of a strict subclass of \"\n                  \"EagerTensor, which is not supported.  Item \",\n                  i, \" is type: \", elem->ob_type->tp_name)\n                  .c_str());\n        }\n      } else if (tensorflow::swig::IsTensor(elem)) {\n        // If it isnt an EagerTensor, but is still a Tensor, it must be a graph\n        // tensor.\n        tensorflow::Safe_PyObjectPtr name_attr(\n            PyObject_GetAttrString(elem, \"name\"));\n        tensorflow::ThrowTypeError(\n            tensorflow::strings::StrCat(\n                \"An op outside of the function building code is being passed\\n\"\n                \"a \\\"Graph\\\" tensor. It is possible to have Graph tensors\\n\"\n                \"leak out of the function building context by including a\\n\"\n                \"tf.init_scope in your function building code.\\n\"\n                \"For example, the following function will fail:\\n\",\n                \"  @tf.function\\n\", \"  def has_init_scope():\\n\",\n                \"    my_constant = tf.constant(1.)\\n\",\n                \"    with tf.init_scope():\\n\",\n                \"      added = my_constant * 2\\n\",\n                \"The graph tensor has name: \",\n                name_attr ? TFE_GetPythonString(name_attr.get()) : \"<unknown>\")\n                .c_str());\n      } else {\n        tensorflow::ThrowTypeError(\n            tensorflow::strings::StrCat(\n                \"provided list of inputs contains objects other \"\n                \"than 'EagerTensor'. Item \",\n                i, \" is type: \", elem->ob_type->tp_name)\n                .c_str());\n      }\n    }\n  }\n  return input_tensor_handles;\n}\n\n// These functions are typemaps from the Python side. I did not use\n// a custom type caster since the logic is slightly harder to follow. This\n// converter is also only used once in `TFE_Py_ExecuteCancelable_wrapper`.\n// This function actually takes a number rather than an output Tensor holder.\nTFE_OutputTensorHandles InputTFE_OutputTensorHandles(\n    const py::handle& num_outputs) {\n  TFE_OutputTensorHandles output_tensor_handles;\n#if PY_MAJOR_VERSION < 3\n  if (!PyInt_Check(num_outputs.ptr())) {\n#else\n  if (!PyLong_Check(num_outputs.ptr())) {\n#endif\n    PyErr_SetString(PyExc_TypeError,\n                    \"expected an integer value (size of the number of \"\n                    \"outputs of the operation)\");\n    throw py::error_already_set();\n  }\n#if PY_MAJOR_VERSION < 3\n  long sz = PyInt_AsLong(num_outputs.ptr());  // NOLINT\n#else\n  long sz = PyLong_AsLong(num_outputs.ptr());  // NOLINT\n#endif\n  if (sz > 0) {\n#if PY_MAJOR_VERSION < 3\n    output_tensor_handles.resize(PyInt_AsLong(num_outputs.ptr()), nullptr);\n#else\n    output_tensor_handles.resize(PyLong_AsLong(num_outputs.ptr()), nullptr);\n#endif\n  }\n  return output_tensor_handles;\n}\n\n// Packs multiple `EagerTensor`s of the same dtype and shape into one\n// `EagerTensor`.\npy::object TFE_Py_PackEagerTensors_wrapper(const py::handle& context,\n                                           const py::handle& tensors) {\n  TFE_Context* ctx = tensorflow::InputTFE_Context(context);\n  TFE_InputTensorHandles handles = InputTFE_InputTensorHandles(tensors);\n  tensorflow::Safe_TF_StatusPtr status = tensorflow::make_safe(TF_NewStatus());\n  int size = handles.size();\n  TFE_TensorHandle* packed_handle =\n      TFE_CreatePackedTensorHandle(ctx, handles.data(), &size, status.get());\n  tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n  PyObject* packed_tensor =\n      EagerTensorFromHandle(packed_handle, /*is_packed=*/true);\n  return tensorflow::PyoOrThrow(packed_tensor);\n}\n\n// This function was created from fusing the typemap logic in platform/base.i.\npy::object TFE_Py_ExecuteCancelable_wrapper(\n    const py::handle& context, const char* device_name, const char* op_name,\n    const py::handle& inputs, const py::handle& attrs,\n    TFE_CancellationManager* cancellation_manager,\n    const py::handle& num_outputs) {\n  TFE_Context* ctx = tensorflow::InputTFE_Context(context);\n  TFE_InputTensorHandles input_tensor_handles =\n      InputTFE_InputTensorHandles(inputs);\n  TFE_OutputTensorHandles output_tensor_handles =\n      InputTFE_OutputTensorHandles(num_outputs);\n  tensorflow::Safe_TF_StatusPtr status = tensorflow::make_safe(TF_NewStatus());\n  TFE_Py_ExecuteCancelable(ctx, device_name, op_name, &input_tensor_handles,\n                           attrs.ptr(), cancellation_manager,\n                           &output_tensor_handles, status.get());\n\n  int output_len = output_tensor_handles.size();\n  PyObject* output_list = PyList_New(output_len);\n  for (int i = 0; i < output_len; ++i) {\n    PyObject* output;\n    output = EagerTensorFromHandle(output_tensor_handles.at(i));\n    PyList_SetItem(output_list, i, output);\n  }\n  tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n  return tensorflow::PyoOrThrow(output_list);\n}\n\nstatic py::object TF_ListPhysicalDevices() {\n  std::vector<string> devices;\n  tensorflow::Status s =\n      tensorflow::DeviceFactory::ListAllPhysicalDevices(&devices);\n  MaybeRaiseRegisteredFromStatus(s);\n  PyObject* result = PyList_New(devices.size());\n  int i = 0;\n  for (auto& dev : devices) {\n    PyObject* dev_obj = PyBytes_FromStringAndSize(dev.data(), dev.size());\n    PyList_SetItem(result, i, dev_obj);\n    ++i;\n  }\n  return tensorflow::PyoOrThrow(result);\n}\n\nstatic std::unordered_map<string, string> TF_GetDeviceDetails(int index) {\n  tensorflow::Safe_TF_StatusPtr status = tensorflow::make_safe(TF_NewStatus());\n  std::unordered_map<string, string> device_details;\n  tensorflow::Status s =\n      tensorflow::DeviceFactory::GetAnyDeviceDetails(index, &device_details);\n  tensorflow::Set_TF_Status_from_Status(status.get(), s);\n  MaybeRaiseRegisteredFromTFStatus(status.get());\n  return device_details;\n}\n\nstatic py::object TFE_ClearScalarCache() {\n  tensorflow::TFE_TensorHandleCache::Get()->Clear();\n  return py::none();\n}\n\n// Returns compiler IR for a given function.\nstatic std::string TFE_GetCompilerIr(py::handle& ctx,\n                                     const char* concrete_function_name,\n                                     const char* stage, const char* device_name,\n                                     py::handle& inputs) {\n  EagerContext* context = ContextFromInterface(\n      reinterpret_cast<ImmediateExecutionContext*>(InputTFE_Context(ctx)));\n\n  std::string s_stage(stage);\n  IrExportStage selected_stage = [&] {\n    if (s_stage == \"hlo\") {\n      return IrExportStage::HLO;\n    } else if (s_stage == \"optimized_hlo\") {\n      return IrExportStage::OPTIMIZED_HLO;\n    } else {\n      ThrowValueError(\n          absl::StrFormat(\"Invalid stage selected: '%s'. Valid values are: \"\n                          \"'hlo', 'optimized_hlo'\",\n                          s_stage)\n              .c_str());\n    }\n  }();\n\n  TFE_InputTensorHandles handles = InputTFE_InputTensorHandles(inputs);\n\n  std::vector<const Tensor*> input_tensors;\n  for (TFE_TensorHandle* tensor_handle : handles) {\n    AbstractTensorHandle* abstract_tensor_handle = unwrap(tensor_handle);\n    TensorHandle* th = TensorHandleFromInterface(abstract_tensor_handle);\n\n    const Tensor* t;\n    Status st = th->Tensor(&t);\n    if (!st.ok()) {\n      ThrowValueError(\n          absl::StrFormat(\"Could not resolve tensor: '%s'\", st.error_message())\n              .c_str());\n    }\n    input_tensors.push_back(t);\n  }\n\n  DeviceNameUtils::ParsedName input_device_name;\n  if (!DeviceNameUtils::ParseFullOrLocalName(device_name, &input_device_name)) {\n    ThrowValueError(\n        absl::StrFormat(\"Failed parsing device name: '%s'\", device_name)\n            .c_str());\n  }\n\n  std::vector<Device*> devices = context->local_device_mgr()->ListDevices();\n  auto selected_device = absl::c_find_if(devices, [&](const Device* d) {\n    return DeviceNameUtils::AreCompatibleDevNames(input_device_name,\n                                                  d->parsed_name());\n  });\n  if (selected_device == devices.end()) {\n    ThrowValueError(\"No matching device found\");\n  }\n\n  xla::StatusOr<std::string> hlo_text =\n      GetCompilerIr(selected_stage, context->pflr(), concrete_function_name,\n                    *selected_device, input_tensors);\n\n  if (!hlo_text.ok()) {\n    ThrowValueError(absl::StrFormat(\"Failed getting HLO text: '%s'\",\n                                    hlo_text.status().error_message())\n                        .c_str());\n  }\n  return *hlo_text;\n}\n\n}  // namespace tensorflow\n\nnamespace {\n\n// Wrapper around the EagerContextThreadLocalData struct (defined in\n// pywrap_tfe.h), so it can be accessed from Python.\n//\n// For PyObject* fields, the get_*() methods return a new reference; and the\n// set_*() methods create a new reference (i.e., they do not steal a reference).\nclass EagerContextThreadLocalDataWrapper {\n public:\n  explicit EagerContextThreadLocalDataWrapper(py::handle py_eager_context,\n                                              py::handle is_eager,\n                                              py::handle device_spec)\n      : py_eager_context_(py_eager_context.ptr()) {\n    tensorflow::MakeEagerContextThreadLocalData(\n        py_eager_context.ptr(), is_eager.ptr(), device_spec.ptr());\n  }\n\n  ~EagerContextThreadLocalDataWrapper() {\n    tensorflow::DestroyEagerContextThreadLocalData(py_eager_context_);\n  }\n\n  bool get_is_eager() const { return GetData()->is_eager; }\n  void set_is_eager(bool v) { GetData()->is_eager = v; }\n\n  bool get_invoking_op_callbacks() const {\n    return GetData()->invoking_op_callbacks;\n  }\n  void set_invoking_op_callbacks(bool v) {\n    GetData()->invoking_op_callbacks = v;\n  }\n\n  py::handle get_device_name() const {\n    return GetPyObject(&GetData()->device_name);\n  }\n  void set_device_name(py::handle v) {\n    SetPyObject(v, &GetData()->device_name);\n  }\n\n  py::handle get_scope_name() const {\n    return GetPyObject(&GetData()->scope_name);\n  }\n  void set_scope_name(py::handle v) { SetPyObject(v, &GetData()->scope_name); }\n\n  py::handle get_device_spec() const {\n    return GetPyObject(&GetData()->device_spec);\n  }\n  void set_device_spec(py::handle v) {\n    SetPyObject(v, &GetData()->device_spec);\n  }\n\n  py::handle get_function_call_options() const {\n    return GetPyObject(&GetData()->function_call_options);\n  }\n  void set_function_call_options(py::handle v) {\n    SetPyObject(v, &GetData()->function_call_options);\n  }\n\n  py::handle get_executor() const { return GetPyObject(&GetData()->executor); }\n  void set_executor(py::handle v) { SetPyObject(v, &GetData()->executor); }\n\n  py::handle get_op_callbacks() const {\n    return GetPyObject(&GetData()->op_callbacks);\n  }\n  void set_op_callbacks(py::handle v) {\n    SetPyObject(v, &GetData()->op_callbacks);\n  }\n\n private:\n  tensorflow::EagerContextThreadLocalData* GetData() const {\n    auto* result =\n        tensorflow::GetEagerContextThreadLocalData(py_eager_context_);\n    if (!result) {\n      throw py::error_already_set();\n    }\n    return result;\n  }\n\n  py::handle GetPyObject(tensorflow::Safe_PyObjectPtr* obj) const {\n    Py_INCREF(obj->get());\n    return obj->get();\n  }\n\n  void SetPyObject(py::handle value, tensorflow::Safe_PyObjectPtr* ptr) {\n    Py_INCREF(value.ptr());\n    ptr->reset(value.ptr());\n  }\n\n  PyObject* py_eager_context_;  // not owned (borrowed reference).\n};\n\n}  // namespace\n\n// py::return_value_policy::reference is defined as specified by the\n// pybind11 documents listed here.\n// https://pybind11.readthedocs.io/en/stable/advanced/functions.html#return-value-policies\n// This means that C++ maintains ownership of the object. We\n// are only assigning this to functions that return opaque types.\n\nPYBIND11_MODULE(_pywrap_tfe, m) {\n  py::class_<TFE_Executor> TFE_Executor_class(m, \"TFE_Executor\");\n  py::class_<TFE_ContextOptions> TFE_ContextOptions_class(m,\n                                                          \"TFE_ContextOptions\");\n  py::class_<TFE_MonitoringCounter0> TFE_MonitoringCounter0_class(\n      m, \"TFE_MonitoringCounter0\");\n  py::class_<TFE_MonitoringCounter1> TFE_MonitoringCounter1_class(\n      m, \"TFE_MonitoringCounter1\");\n  py::class_<TFE_MonitoringCounter2> TFE_MonitoringCounter2_class(\n      m, \"TFE_MonitoringCounter2\");\n  py::class_<TFE_MonitoringStringGauge0> TFE_MonitoringStringGauge0_class(\n      m, \"TFE_MonitoringStringGauge0\");\n  py::class_<TFE_MonitoringStringGauge1> TFE_MonitoringStringGauge1_class(\n      m, \"TFE_MonitoringStringGauge1\");\n  py::class_<TFE_MonitoringStringGauge2> TFE_MonitoringStringGauge2_class(\n      m, \"TFE_MonitoringStringGauge2\");\n  py::class_<TFE_MonitoringIntGauge0> TFE_MonitoringIntGauge0_class(\n      m, \"TFE_MonitoringIntGauge0\");\n  py::class_<TFE_MonitoringIntGauge1> TFE_MonitoringIntGauge1_class(\n      m, \"TFE_MonitoringIntGauge1\");\n  py::class_<TFE_MonitoringIntGauge2> TFE_MonitoringIntGauge2_class(\n      m, \"TFE_MonitoringIntGauge2\");\n  py::class_<TFE_MonitoringBoolGauge0> TFE_MonitoringBoolGauge0_class(\n      m, \"TFE_MonitoringBoolGauge0\");\n  py::class_<TFE_MonitoringBoolGauge1> TFE_MonitoringBoolGauge1_class(\n      m, \"TFE_MonitoringBoolGauge1\");\n  py::class_<TFE_MonitoringBoolGauge2> TFE_MonitoringBoolGauge2_class(\n      m, \"TFE_MonitoringBoolGauge2\");\n  py::class_<TFE_MonitoringCounterCell> TFE_MonitoringCounterCell_class(\n      m, \"TFE_MonitoringCounterCell\");\n  py::class_<TFE_MonitoringIntGaugeCell> TFE_MonitoringIntGaugeCell_class(\n      m, \"TFE_MonitoringIntGaugeCell\");\n  py::class_<TFE_MonitoringStringGaugeCell> TFE_MonitoringStringGaugeCell_class(\n      m, \"TFE_MonitoringStringGaugeCell\");\n  py::class_<TFE_MonitoringBoolGaugeCell> TFE_MonitoringBoolGaugeCell_class(\n      m, \"TFE_MonitoringBoolGaugeCell\");\n  py::class_<TFE_MonitoringSamplerCell> TFE_MonitoringSamplerCell_class(\n      m, \"TFE_MonitoringSamplerCell\");\n  py::class_<TFE_MonitoringBuckets> TFE_MonitoringBuckets_class(\n      m, \"TFE_MonitoringBuckets\");\n  py::class_<TFE_MonitoringSampler0> TFE_MonitoringSampler0_class(\n      m, \"TFE_MonitoringSampler0\");\n  py::class_<TFE_MonitoringSampler1> TFE_MonitoringSampler1_class(\n      m, \"TFE_MonitoringSampler1\");\n  py::class_<TFE_MonitoringSampler2> TFE_MonitoringSampler2_class(\n      m, \"TFE_MonitoringSampler2\");\n  py::class_<TFE_CancellationManager> TFE_CancellationManager_class(\n      m, \"TFE_CancellationManager\");\n\n  py::class_<TF_DeviceList> TF_DeviceList_class(m, \"TF_DeviceList\");\n  py::class_<TF_Function> TF_Function_class(m, \"TF_Function\");\n\n  m.def(\"TFE_Py_RegisterExceptionClass\", [](const py::handle& e) {\n    return tensorflow::PyoOrThrow(TFE_Py_RegisterExceptionClass(e.ptr()));\n  });\n  m.def(\"TFE_Py_RegisterFallbackExceptionClass\", [](const py::handle& e) {\n    return tensorflow::PyoOrThrow(\n        TFE_Py_RegisterFallbackExceptionClass(e.ptr()));\n  });\n\n  m.def(\n      \"TFE_GetTotalMemoryUsage\", [](py::handle& ctx, const char* device_name) {\n        tensorflow::EagerContext* context = tensorflow::ContextFromInterface(\n            reinterpret_cast<tensorflow::ImmediateExecutionContext*>(\n                tensorflow::InputTFE_Context(ctx)));\n\n        tensorflow::DeviceNameUtils::ParsedName input_device_name;\n        if (!tensorflow::DeviceNameUtils::ParseFullOrLocalName(\n                device_name, &input_device_name)) {\n          tensorflow::ThrowValueError(\n              absl::StrFormat(\"Failed parsing device name: '%s'\", device_name)\n                  .c_str());\n        }\n\n        std::vector<tensorflow::Device*> devices =\n            context->local_device_mgr()->ListDevices();\n\n        tensorflow::Device* matched_device = nullptr;\n        for (int device_idx = 0; device_idx < devices.size(); device_idx++) {\n          tensorflow::Device* device = devices[device_idx];\n\n          if (tensorflow::DeviceNameUtils::AreCompatibleDevNames(\n                  input_device_name, device->parsed_name())) {\n            if (device->device_type() == tensorflow::DEVICE_CPU) {\n              tensorflow::ThrowValueError(\n                  \"CPU does not support getting allocator information\");\n            }\n\n            if (matched_device != nullptr) {\n              tensorflow::ThrowValueError(\n                  absl::StrFormat(\n                      \"Multiple devices matching the provided string \"\n                      \"'%s': '%s' and \"\n                      \"'%s' \",\n                      device_name, matched_device->name(), device->name())\n                      .c_str());\n            }\n            matched_device = device;\n          }\n        }\n\n        if (matched_device == nullptr) {\n          tensorflow::ThrowValueError(\n              absl::StrFormat(\"No matching devices found for '%s'\", device_name)\n                  .c_str());\n        }\n\n        tensorflow::AllocatorAttributes attrs;\n        tensorflow::Allocator* allocator = matched_device->GetAllocator(attrs);\n\n        if (absl::optional<tensorflow::AllocatorStats> stats =\n                allocator->GetStats()) {\n          return stats->bytes_in_use;\n        }\n\n        tensorflow::ThrowTypeError(\n            absl::StrFormat(\"Allocator stats not available for device '%s'\",\n                            matched_device->name())\n                .c_str());\n      });\n\n  // XLA Eager Logic\n  m.def(\"TF_SetXlaEnableLazyCompilation\", &TF_SetXlaEnableLazyCompilation);\n  m.def(\"TF_SetTfXlaCpuGlobalJit\", &TF_SetTfXlaCpuGlobalJit);\n  m.def(\"TF_SetXlaAutoJitMode\", &TF_SetXlaAutoJitMode);\n  m.def(\"TF_SetXlaConstantFoldingDisabled\", &TF_SetXlaConstantFoldingDisabled);\n  m.def(\"TF_GetXlaConstantFoldingDisabled\", &TF_GetXlaConstantFoldingDisabled);\n  m.def(\"TF_SetXlaMinClusterSize\", &TF_SetXlaMinClusterSize);\n  m.def(\"TF_GetCompilerIr\", &tensorflow::TFE_GetCompilerIr);\n\n  // MLIR Logic\n  m.def(\"TF_IsMlirBridgeEnabled\", [] {\n    return tensorflow::GetMlirCommonFlags()->tf_mlir_enable_mlir_bridge;\n  });\n  m.def(\"TF_EnableMlirBridge\", [](bool enabled) {\n    tensorflow::GetMlirCommonFlags()->tf_mlir_enable_mlir_bridge = enabled;\n  });\n  m.def(\"TF_EnableXlaDevices\", [] {\n    tensorflow::GetXlaDeviceFlags()->tf_xla_enable_xla_devices = true;\n  });\n\n  // // TFE_Context Logic\n  m.def(\n      \"TFE_NewContext\",\n      [](const TFE_ContextOptions* opts) {\n        tensorflow::Safe_TF_StatusPtr status =\n            tensorflow::make_safe(TF_NewStatus());\n        TFE_Context* context = TFE_NewContext(opts, status.get());\n        tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n        return tensorflow::PyoOrThrow(tensorflow::OutputTFE_Context(context));\n      },\n      py::return_value_policy::reference);\n  m.def(\"TFE_DeleteContext\", [](py::handle& o) {\n    TFE_DeleteContext(tensorflow::InputTFE_Context(o));\n  });\n  m.def(\n      \"TFE_ContextListDevices\",\n      [](py::handle& o) {\n        tensorflow::Safe_TF_StatusPtr status =\n            tensorflow::make_safe(TF_NewStatus());\n        auto output = TFE_ContextListDevices(tensorflow::InputTFE_Context(o),\n                                             status.get());\n        tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n        return output;\n      },\n      py::return_value_policy::reference);\n  m.def(\"TFE_HostAddressSpace\", [](py::handle& o, TF_Buffer& buf) {\n    TFE_HostAddressSpace(tensorflow::InputTFE_Context(o), &buf);\n  });\n  m.def(\"TFE_ContextAddFunction\", [](py::handle& ctx, TF_Function* func) {\n    tensorflow::Safe_TF_StatusPtr status =\n        tensorflow::make_safe(TF_NewStatus());\n    TFE_ContextAddFunction(tensorflow::InputTFE_Context(ctx), func,\n                           status.get());\n    tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n  });\n  m.def(\"TFE_ContextAddFunctionDef\",\n        [](py::handle& ctx, const char* serialized_function_def, size_t size) {\n          tensorflow::Safe_TF_StatusPtr status =\n              tensorflow::make_safe(TF_NewStatus());\n          TFE_ContextAddFunctionDef(tensorflow::InputTFE_Context(ctx),\n                                    serialized_function_def, size,\n                                    status.get());\n          tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n        });\n  m.def(\"TFE_ContextGetFunctionDef\",\n        [](py::handle& ctx, const char* function_name, TF_Buffer& buf) {\n          tensorflow::Safe_TF_StatusPtr status =\n              tensorflow::make_safe(TF_NewStatus());\n          TFE_ContextGetFunctionDef(tensorflow::InputTFE_Context(ctx),\n                                    function_name, &buf, status.get());\n          tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n        });\n  m.def(\"TFE_ContextRemoveFunction\", [](py::handle& ctx, const char* name) {\n    tensorflow::Safe_TF_StatusPtr status =\n        tensorflow::make_safe(TF_NewStatus());\n    TFE_ContextRemoveFunction(tensorflow::InputTFE_Context(ctx), name,\n                              status.get());\n    tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n  });\n  m.def(\"TFE_ContextHasFunction\", [](py::handle& ctx, const char* name) {\n    tensorflow::Safe_TF_StatusPtr status =\n        tensorflow::make_safe(TF_NewStatus());\n    auto output =\n        TFE_ContextHasFunction(tensorflow::InputTFE_Context(ctx), name);\n    tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n    return output;\n  });\n  m.def(\"TFE_ContextEnableRunMetadata\", [](py::handle& ctx) {\n    TFE_ContextEnableRunMetadata(tensorflow::InputTFE_Context(ctx));\n  });\n  m.def(\"TFE_ContextDisableRunMetadata\", [](py::handle& ctx) {\n    TFE_ContextEnableRunMetadata(tensorflow::InputTFE_Context(ctx));\n  });\n  m.def(\"TFE_ContextEnableGraphCollection\", [](py::handle& ctx) {\n    TFE_ContextEnableGraphCollection(tensorflow::InputTFE_Context(ctx));\n  });\n  m.def(\"TFE_ContextDisableGraphCollection\", [](py::handle& ctx) {\n    TFE_ContextDisableGraphCollection(tensorflow::InputTFE_Context(ctx));\n  });\n  m.def(\"TFE_ContextExportRunMetadata\", [](py::handle& ctx, TF_Buffer& buf) {\n    tensorflow::Safe_TF_StatusPtr status =\n        tensorflow::make_safe(TF_NewStatus());\n    TFE_ContextExportRunMetadata(tensorflow::InputTFE_Context(ctx), &buf,\n                                 status.get());\n    tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n  });\n  m.def(\"TFE_ContextClearCaches\", [](py::handle& o) {\n    TFE_ContextClearCaches(tensorflow::InputTFE_Context(o));\n  });\n  m.def(\"TFE_GetContextId\", [](py::handle& ctx) {\n    return TFE_GetContextId(tensorflow::InputTFE_Context(ctx));\n  });\n  m.def(\"TFE_ContextGetDevicePlacementPolicy\", [](py::handle& ctx) {\n    return TFE_ContextGetDevicePlacementPolicy(\n        tensorflow::InputTFE_Context(ctx));\n  });\n  m.def(\"TFE_ContextSetThreadLocalDevicePlacementPolicy\",\n        [](py::handle& ctx, TFE_ContextDevicePlacementPolicy policy) {\n          TFE_ContextSetThreadLocalDevicePlacementPolicy(\n              tensorflow::InputTFE_Context(ctx), policy);\n        });\n  m.def(\"TFE_ContextSetServerDef\", [](py::handle& ctx, int keep_alive_secs,\n                                      py::bytes proto) {\n    tensorflow::Safe_TF_StatusPtr status =\n        tensorflow::make_safe(TF_NewStatus());\n    tensorflow::Safe_TF_BufferPtr buf =\n        tensorflow::make_safe(tensorflow::ProtoStringToTFBuffer(proto.ptr()));\n    TFE_ContextSetServerDef(tensorflow::InputTFE_Context(ctx), keep_alive_secs,\n                            buf.get()->data, buf.get()->length, status.get());\n    tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n  });\n  m.def(\"TFE_ContextUpdateServerDef\", [](py::handle& ctx, int keep_alive_secs,\n                                         py::bytes proto) {\n    tensorflow::Safe_TF_StatusPtr status =\n        tensorflow::make_safe(TF_NewStatus());\n    tensorflow::Safe_TF_BufferPtr buf =\n        tensorflow::make_safe(tensorflow::ProtoStringToTFBuffer(proto.ptr()));\n    Py_BEGIN_ALLOW_THREADS;\n    TFE_ContextUpdateServerDef(tensorflow::InputTFE_Context(ctx),\n                               keep_alive_secs, buf.get()->data,\n                               buf.get()->length, status.get());\n    Py_END_ALLOW_THREADS;\n    tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n  });\n  m.def(\"TFE_ContextCheckAlive\", [](py::handle& ctx, const char* worker_name) {\n    tensorflow::Safe_TF_StatusPtr status =\n        tensorflow::make_safe(TF_NewStatus());\n    bool output = TFE_ContextCheckAlive(tensorflow::InputTFE_Context(ctx),\n                                        worker_name, status.get());\n    tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n    return output;\n  });\n  m.def(\"TFE_ContextSyncExecutors\", [](py::handle& ctx) {\n    tensorflow::Safe_TF_StatusPtr status =\n        tensorflow::make_safe(TF_NewStatus());\n    TFE_ContextAsyncWait(tensorflow::InputTFE_Context(ctx), status.get());\n    tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n  });\n  m.def(\"TFE_ContextClearExecutors\", [](py::handle& ctx) {\n    tensorflow::Safe_TF_StatusPtr status =\n        tensorflow::make_safe(TF_NewStatus());\n    TFE_ContextAsyncWait(tensorflow::InputTFE_Context(ctx), status.get());\n    // NOTE: different from TFE_ContextSyncExecutors that raises potential\n    // errors, deliberately ignore executor statuses in cleanup.\n  });\n  m.def(\"TFE_ContextSetSoftDevicePlacement\", [](py::handle& ctx, bool enable) {\n    tensorflow::Safe_TF_StatusPtr status =\n        tensorflow::make_safe(TF_NewStatus());\n    TFE_ContextSetSoftDevicePlacement(tensorflow::InputTFE_Context(ctx), enable,\n                                      status.get());\n  });\n  m.def(\"TFE_ContextSetLogDevicePlacement\", [](py::handle& ctx, bool enable) {\n    tensorflow::Safe_TF_StatusPtr status =\n        tensorflow::make_safe(TF_NewStatus());\n    TFE_ContextSetSoftDevicePlacement(tensorflow::InputTFE_Context(ctx), enable,\n                                      status.get());\n  });\n\n  // TFE_Executor logic\n  m.def(\n      \"TFE_NewExecutor\",\n      [](const bool is_async) {\n        TFE_Executor* exc = TFE_NewExecutor(is_async);\n        return exc;\n      },\n      py::return_value_policy::reference);\n  m.def(\"TFE_DeleteExecutor\", &TFE_DeleteExecutor);\n  m.def(\"TFE_ExecutorIsAsync\", &TFE_ExecutorIsAsync);\n  m.def(\"TFE_ExecutorWaitForAllPendingNodes\", [](TFE_Executor& exc) {\n    tensorflow::Safe_TF_StatusPtr status =\n        tensorflow::make_safe(TF_NewStatus());\n    // NOTE: release Python GIL for pending PyFunc ops to be executed properly.\n    Py_BEGIN_ALLOW_THREADS;\n    TFE_ExecutorWaitForAllPendingNodes(&exc, status.get());\n    Py_END_ALLOW_THREADS;\n    tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n  });\n  m.def(\"TFE_ExecutorClearError\", &TFE_ExecutorClearError);\n  m.def(\"TFE_ContextSetExecutorForThread\", [](py::handle& ctx,\n                                              TFE_Executor& exc) {\n    TFE_ContextSetExecutorForThread(tensorflow::InputTFE_Context(ctx), &exc);\n  });\n  m.def(\n      \"TFE_ContextGetExecutorForThread\",\n      [](py::handle& o) {\n        return TFE_ContextGetExecutorForThread(tensorflow::InputTFE_Context(o));\n      },\n      py::return_value_policy::reference);\n\n  m.def(\"TFE_OpNameGetAttrType\",\n        [](py::handle& ctx, const char* op_or_function_name,\n           const char* attr_name) {\n          int temp = 0;\n          unsigned char* is_list = reinterpret_cast<unsigned char*>(&temp);\n          tensorflow::Safe_TF_StatusPtr status =\n              tensorflow::make_safe(TF_NewStatus());\n          auto output = TFE_OpNameGetAttrType(tensorflow::InputTFE_Context(ctx),\n                                              op_or_function_name, attr_name,\n                                              is_list, status.get());\n          tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n#if PY_MAJOR_VERSION < 3\n          PyObject* output_pyo = PyInt_FromLong(output);\n#else\n          PyObject* output_pyo = PyLong_FromLong(output);\n#endif\n          if (*is_list == 1) {\n            PyObject* list = PyList_New(1);\n            PyList_SetItem(list, 0, output_pyo);\n            return tensorflow::PyoOrThrow(list);\n          }\n          return tensorflow::PyoOrThrow(output_pyo);\n        });\n  m.def(\"TFE_Py_InitEagerTensor\", [](const py::handle& o) {\n    return tensorflow::PyoOrThrow(TFE_Py_InitEagerTensor(o.ptr()));\n  });\n  m.def(\"TFE_Py_PackEagerTensors\",\n        [](const py::handle& context, const py::handle& handles) {\n          return tensorflow::TFE_Py_PackEagerTensors_wrapper(context, handles);\n        });\n  m.def(\"TFE_Py_SetEagerTensorProfiler\", &TFE_Py_SetEagerTensorProfiler);\n  m.def(\"TFE_Py_RegisterJVPFunction\", [](const py::handle& o) {\n    return tensorflow::PyoOrThrow(TFE_Py_RegisterJVPFunction(o.ptr()));\n  });\n  m.def(\"TFE_Py_RegisterGradientFunction\", [](const py::handle& o) {\n    return tensorflow::PyoOrThrow(TFE_Py_RegisterGradientFunction(o.ptr()));\n  });\n  m.def(\"TFE_Py_Execute\",\n        [](const py::handle& context, const char* device_name,\n           const char* op_name, const py::handle& inputs,\n           const py::handle& attrs, const py::handle& num_outputs) {\n          return tensorflow::TFE_Py_ExecuteCancelable_wrapper(\n              context, device_name, op_name, inputs, attrs.ptr(), nullptr,\n              num_outputs);\n        });\n  m.def(\n      \"TFE_Py_ExecuteCancelable\",\n      [](const py::handle& context, const char* device_name,\n         const char* op_name, const py::handle& inputs, const py::handle& attrs,\n         TFE_CancellationManager& cancellation_manager,\n         const py::handle& num_outputs) {\n        return tensorflow::TFE_Py_ExecuteCancelable_wrapper(\n            context, device_name, op_name, inputs, attrs.ptr(),\n            &cancellation_manager, num_outputs);\n      });\n  m.def(\"TFE_Py_FastPathExecute\", [](const py::args args) {\n    // TFE_Py_FastPathExecute requires error checking prior to returning.\n    return tensorflow::PyoOrThrow(TFE_Py_FastPathExecute_C(args.ptr()));\n  });\n  m.def(\"TFE_Py_RecordGradient\",\n        [](const py::handle& op_name, const py::handle& inputs,\n           const py::handle& attrs, const py::handle& results,\n           const py::handle& forward_pass_name_scope) {\n          return tensorflow::PyoOrThrow(TFE_Py_RecordGradient(\n              op_name.ptr(), inputs.ptr(), attrs.ptr(), results.ptr(),\n              forward_pass_name_scope.ptr()));\n        });\n  m.def(\"TFE_Py_UID\", []() { return tensorflow::PyoOrThrow(TFE_Py_UID()); });\n\n  // TFE_Py_Tape Logic\n  m.def(\"TFE_Py_TapeSetNew\", [](const py::handle& persistent,\n                                const py::handle& watch_accessed_variables) {\n    return tensorflow::PyoOrThrow(\n        TFE_Py_TapeSetNew(persistent.ptr(), watch_accessed_variables.ptr()));\n  });\n  m.def(\"TFE_Py_TapeSetAdd\",\n        [](const py::handle& tape) { TFE_Py_TapeSetAdd(tape.ptr()); });\n  m.def(\"TFE_Py_TapeSetRemove\",\n        [](const py::handle& tape) { TFE_Py_TapeSetRemove(tape.ptr()); });\n  m.def(\"TFE_Py_TapeSetStopOnThread\", &TFE_Py_TapeSetStopOnThread);\n  m.def(\"TFE_Py_TapeSetRestartOnThread\", &TFE_Py_TapeSetRestartOnThread);\n  m.def(\"TFE_Py_TapeSetIsStopped\",\n        []() { return tensorflow::PyoOrThrow(TFE_Py_TapeSetIsStopped()); });\n  m.def(\"TFE_Py_TapeSetIsEmpty\",\n        []() { return tensorflow::PyoOrThrow(TFE_Py_TapeSetIsEmpty()); });\n  m.def(\"TFE_Py_TapeSetShouldRecordBackprop\", [](const py::handle& tensors) {\n    return tensorflow::PyoOrThrow(\n        TFE_Py_TapeSetShouldRecordBackprop(tensors.ptr()));\n  });\n  m.def(\"TFE_Py_TapeSetPossibleGradientTypes\", [](const py::handle& tensors) {\n    return tensorflow::PyoOrThrow(\n        TFE_Py_TapeSetPossibleGradientTypes(tensors.ptr()));\n  });\n  m.def(\"TFE_Py_TapeSetDeleteTrace\", &TFE_Py_TapeSetDeleteTrace);\n  m.def(\"TFE_Py_TapeSetRecordOperation\",\n        [](const py::handle& op_type, const py::handle& output_tensors,\n           const py::handle& input_tensors, const py::handle& backward_function,\n           const py::handle& forward_function) {\n          return tensorflow::PyoOrThrow(TFE_Py_TapeSetRecordOperation(\n              op_type.ptr(), output_tensors.ptr(), input_tensors.ptr(),\n              backward_function.ptr(), forward_function.ptr()));\n        });\n  m.def(\n      \"TFE_Py_TapeSetRecordOperationBackprop\",\n      [](const py::handle& op_type, const py::handle& output_tensors,\n         const py::handle& input_tensors, const py::handle& backward_function) {\n        return tensorflow::PyoOrThrow(TFE_Py_TapeSetRecordOperationBackprop(\n            op_type.ptr(), output_tensors.ptr(), input_tensors.ptr(),\n            backward_function.ptr()));\n      });\n  m.def(\n      \"TFE_Py_TapeSetRecordOperationForwardprop\",\n      [](const py::handle& op_type, const py::handle& output_tensors,\n         const py::handle& input_tensors, const py::handle& backward_function,\n         const py::handle& forwardprop_output_indices) {\n        return tensorflow::PyoOrThrow(TFE_Py_TapeSetRecordOperationForwardprop(\n            op_type.ptr(), output_tensors.ptr(), input_tensors.ptr(),\n            backward_function.ptr(), forwardprop_output_indices.ptr()));\n      });\n  m.def(\"TFE_Py_TapeGradient\",\n        [](const py::handle& tape, const py::handle& target,\n           const py::handle& sources, const py::handle& output_gradients,\n           const py::handle& sources_raw,\n           const py::handle& unconnected_gradients) {\n          tensorflow::Safe_TF_StatusPtr status =\n              tensorflow::make_safe(TF_NewStatus());\n          PyObject* output = TFE_Py_TapeGradient(\n              tape.ptr(), target.ptr(), sources.ptr(), output_gradients.ptr(),\n              sources_raw.ptr(), unconnected_gradients.ptr(), status.get());\n          tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n          return tensorflow::PyoOrThrow(output);\n        });\n\n  m.def(\"TFE_Py_TapeVariableAccessed\", [](const py::handle& variable) {\n    TFE_Py_TapeVariableAccessed(variable.ptr());\n  });\n  m.def(\"TFE_Py_TapeWatch\",\n        [](const py::handle& tape, const py::handle& tensor) {\n          TFE_Py_TapeWatch(tape.ptr(), tensor.ptr());\n        });\n  m.def(\"TFE_Py_TapeWatchVariable\",\n        [](const py::handle& tape, const py::handle& variable) {\n          TFE_Py_TapeWatchVariable(tape.ptr(), variable.ptr());\n        });\n  m.def(\"TFE_Py_TapeWatchedVariables\", [](const py::handle& tape) {\n    return tensorflow::PyoOrThrow(TFE_Py_TapeWatchedVariables(tape.ptr()));\n  });\n\n  // TFE_Py_VariableWatcher logic.\n  m.def(\"TFE_Py_VariableWatcherNew\",\n        []() { return tensorflow::PyoOrThrow(TFE_Py_VariableWatcherNew()); });\n  m.def(\"TFE_Py_VariableWatcherRemove\", [](const py::handle& variable_watcher) {\n    TFE_Py_VariableWatcherRemove(variable_watcher.ptr());\n  });\n  m.def(\"TFE_Py_VariableWatcherVariableAccessed\",\n        [](const py::handle& variable) {\n          TFE_Py_VariableWatcherVariableAccessed(variable.ptr());\n        });\n  m.def(\"TFE_Py_VariableWatcherWatchedVariables\",\n        [](const py::handle& variable_watcher) {\n          return tensorflow::PyoOrThrow(\n              TFE_Py_VariableWatcherWatchedVariables(variable_watcher.ptr()));\n        });\n\n  // TFE_Py_ForwardAccumulator logic.\n  m.def(\"TFE_Py_ForwardAccumulatorNew\", [](bool use_batch) {\n    return tensorflow::PyoOrThrow(TFE_Py_ForwardAccumulatorNew(use_batch));\n  });\n\n  m.def(\"TFE_Py_ForwardAccumulatorSetAdd\", [](const py::handle& accumulator) {\n    return tensorflow::PyoOrThrow(\n        TFE_Py_ForwardAccumulatorSetAdd(accumulator.ptr()));\n  });\n  m.def(\"TFE_Py_ForwardAccumulatorSetRemove\",\n        [](const py::handle& accumulator) {\n          TFE_Py_ForwardAccumulatorSetRemove(accumulator.ptr());\n        });\n\n  m.def(\"TFE_Py_ForwardAccumulatorWatch\",\n        [](const py::handle& accumulator, const py::handle& tensor,\n           const py::handle& tangent) {\n          TFE_Py_ForwardAccumulatorWatch(accumulator.ptr(), tensor.ptr(),\n                                         tangent.ptr());\n        });\n  m.def(\"TFE_Py_ForwardAccumulatorJVP\",\n        [](const py::handle& accumulator, const py::handle& tensor) {\n          return tensorflow::PyoOrThrow(\n              TFE_Py_ForwardAccumulatorJVP(accumulator.ptr(), tensor.ptr()));\n        });\n  m.def(\"TFE_Py_ForwardAccumulatorPushState\", []() {\n    return tensorflow::PyoOrThrow(TFE_Py_ForwardAccumulatorPushState());\n  });\n  m.def(\"TFE_Py_ForwardAccumulatorPopState\", []() {\n    return tensorflow::PyoOrThrow(TFE_Py_ForwardAccumulatorPopState());\n  });\n  m.def(\"TFE_Py_PackJVPs\", [](const py::handle& tensors) {\n    return tensorflow::PyoOrThrow(TFE_Py_PackJVPs(tensors.ptr()));\n  });\n\n  // TFE_ContextOptions Logic\n  m.def(\"TFE_NewContextOptions\", &TFE_NewContextOptions,\n        py::return_value_policy::reference);\n  m.def(\"TFE_ContextOptionsSetConfig\", [](TFE_ContextOptions* options,\n                                          py::bytes proto) {\n    tensorflow::Safe_TF_StatusPtr status =\n        tensorflow::make_safe(TF_NewStatus());\n    tensorflow::Safe_TF_BufferPtr buf =\n        tensorflow::make_safe(tensorflow::ProtoStringToTFBuffer(proto.ptr()));\n    TFE_ContextOptionsSetConfig(options, buf.get()->data, buf.get()->length,\n                                status.get());\n    tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n  });\n  m.def(\"TFE_ContextOptionsSetDevicePlacementPolicy\",\n        &TFE_ContextOptionsSetDevicePlacementPolicy);\n  m.def(\"TFE_ContextOptionsSetLazyRemoteInputsCopy\",\n        &TFE_ContextOptionsSetLazyRemoteInputsCopy);\n  m.def(\"TFE_ContextOptionsSetTfrt\", &TFE_ContextOptionsSetTfrt);\n  m.def(\"TFE_ContextOptionsSetAsync\", &TFE_ContextOptionsSetAsync);\n  m.def(\"TFE_DeleteContextOptions\", &TFE_DeleteContextOptions,\n        py::return_value_policy::reference);\n\n  // TFE_Py_TensorShape Logic\n  m.def(\"TFE_Py_TensorShapeSlice\",\n        [](const py::handle& tensors, int slice_dim) {\n          return tensorflow::PyoOrThrow(\n              TFE_Py_TensorShapeSlice(tensors.ptr(), slice_dim));\n        });\n  m.def(\"TFE_Py_TensorShapeOnDevice\", [](const py::handle& tensors,\n                                         int slice_dim) {\n    return tensorflow::PyoOrThrow(TFE_Py_TensorShapeOnDevice(tensors.ptr()));\n  });\n  m.def(\"TFE_Py_EnableInteractivePythonLogging\",\n        &TFE_Py_EnableInteractivePythonLogging);\n\n  // Additional Context Logic\n  m.def(\"TFE_Py_SetEagerContext\", [](const py::handle& o) {\n    return tensorflow::PyoOrThrow(TFE_Py_SetEagerContext(o.ptr()));\n  });\n  m.def(\"TFE_ContextStartStep\", [](py::handle& o) {\n    TFE_ContextStartStep(tensorflow::InputTFE_Context(o.ptr()));\n  });\n  m.def(\"TFE_ContextEndStep\", [](py::handle& o) {\n    TFE_ContextEndStep(tensorflow::InputTFE_Context(o.ptr()));\n  });\n  m.def(\"TFE_Py_RegisterVSpace\", [](const py::handle& o) {\n    return tensorflow::PyoOrThrow(TFE_Py_RegisterVSpace(o.ptr()));\n  });\n  m.def(\"TFE_Py_EncodeArg\",\n        [](const py::handle& o, bool include_tensor_ranks_only) {\n          return tensorflow::PyoOrThrow(\n              TFE_Py_EncodeArg(o.ptr(), include_tensor_ranks_only));\n        });\n  m.def(\"TFE_EnableCollectiveOps\", [](const py::handle& ctx, py::bytes proto) {\n    tensorflow::Safe_TF_StatusPtr status =\n        tensorflow::make_safe(TF_NewStatus());\n    tensorflow::Safe_TF_BufferPtr buf =\n        tensorflow::make_safe(tensorflow::ProtoStringToTFBuffer(proto.ptr()));\n    TFE_EnableCollectiveOps(tensorflow::InputTFE_Context(ctx), buf.get()->data,\n                            buf.get()->length, status.get());\n    tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n  });\n  m.def(\"TFE_AbortCollectiveOps\", [](const py::handle& ctx, int code,\n                                     const char* message) {\n    tensorflow::Safe_TF_StatusPtr status =\n        tensorflow::make_safe(TF_NewStatus());\n    TF_SetStatus(status.get(), static_cast<TF_Code>(code), message);\n    TFE_AbortCollectiveOps(tensorflow::InputTFE_Context(ctx), status.get());\n  });\n  m.def(\"TFE_CollectiveOpsCheckPeerHealth\",\n        [](const py::handle& ctx, const char* task) {\n          tensorflow::Safe_TF_StatusPtr status =\n              tensorflow::make_safe(TF_NewStatus());\n          TFE_CollectiveOpsCheckPeerHealth(tensorflow::InputTFE_Context(ctx),\n                                           task, status.get());\n          tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n        });\n  m.def(\"TF_ListPhysicalDevices\", &tensorflow::TF_ListPhysicalDevices);\n  m.def(\"TF_GetDeviceDetails\", &tensorflow::TF_GetDeviceDetails);\n  m.def(\"TF_DeleteDeviceList\", &TF_DeleteDeviceList,\n        py::return_value_policy::reference);\n  m.def(\"TF_DeviceListCount\", &TF_DeviceListCount);\n  m.def(\"TF_DeviceListName\", [](const TF_DeviceList* list, int index) {\n    tensorflow::Safe_TF_StatusPtr status =\n        tensorflow::make_safe(TF_NewStatus());\n    auto output = TF_DeviceListName(list, index, status.get());\n    tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n    return output;\n  });\n  m.def(\"TF_DeviceListType\", [](const TF_DeviceList* list, int index) {\n    tensorflow::Safe_TF_StatusPtr status =\n        tensorflow::make_safe(TF_NewStatus());\n    auto output = TF_DeviceListType(list, index, status.get());\n    tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n    return output;\n  });\n\n  m.def(\"TF_PickUnusedPortOrDie\", &TF_PickUnusedPortOrDie);\n\n  // TFE_MonitoringCounter Logic\n  m.def(\"TFE_MonitoringCounterCellIncrementBy\",\n        &TFE_MonitoringCounterCellIncrementBy);\n  m.def(\"TFE_MonitoringCounterCellValue\", &TFE_MonitoringCounterCellValue);\n  m.def(\n      \"TFE_MonitoringNewCounter0\",\n      [](const char* name, const char* description) {\n        tensorflow::Safe_TF_StatusPtr status =\n            tensorflow::make_safe(TF_NewStatus());\n        auto output =\n            TFE_MonitoringNewCounter0(name, status.get(), description);\n        tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n        return output;\n      },\n      py::return_value_policy::reference);\n  m.def(\"TFE_MonitoringDeleteCounter0\", &TFE_MonitoringDeleteCounter0,\n        py::return_value_policy::reference);\n  m.def(\"TFE_MonitoringGetCellCounter0\", &TFE_MonitoringGetCellCounter0,\n        py::return_value_policy::reference);\n  m.def(\n      \"TFE_MonitoringNewCounter1\",\n      [](const char* name, const char* description, const char* label1) {\n        tensorflow::Safe_TF_StatusPtr status =\n            tensorflow::make_safe(TF_NewStatus());\n        auto output =\n            TFE_MonitoringNewCounter1(name, status.get(), description, label1);\n        tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n        return output;\n      },\n      py::return_value_policy::reference);\n  m.def(\"TFE_MonitoringDeleteCounter1\", &TFE_MonitoringDeleteCounter1,\n        py::return_value_policy::reference);\n  m.def(\"TFE_MonitoringGetCellCounter1\", &TFE_MonitoringGetCellCounter1,\n        py::return_value_policy::reference);\n  m.def(\n      \"TFE_MonitoringNewCounter2\",\n      [](const char* name, const char* description, const char* label1,\n         const char* label2) {\n        tensorflow::Safe_TF_StatusPtr status =\n            tensorflow::make_safe(TF_NewStatus());\n        auto output = TFE_MonitoringNewCounter2(name, status.get(), description,\n                                                label1, label2);\n        tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n        return output;\n      },\n      py::return_value_policy::reference);\n  m.def(\"TFE_MonitoringDeleteCounter2\", &TFE_MonitoringDeleteCounter2,\n        py::return_value_policy::reference);\n  m.def(\"TFE_MonitoringGetCellCounter2\", &TFE_MonitoringGetCellCounter2,\n        py::return_value_policy::reference);\n\n  // TFE_MonitoringIntGauge Logic\n  m.def(\"TFE_MonitoringIntGaugeCellSet\", &TFE_MonitoringIntGaugeCellSet);\n  m.def(\"TFE_MonitoringIntGaugeCellValue\", &TFE_MonitoringIntGaugeCellValue);\n  m.def(\n      \"TFE_MonitoringNewIntGauge0\",\n      [](const char* name, const char* description) {\n        tensorflow::Safe_TF_StatusPtr status =\n            tensorflow::make_safe(TF_NewStatus());\n        auto output =\n            TFE_MonitoringNewIntGauge0(name, status.get(), description);\n        tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n        return output;\n      },\n      py::return_value_policy::reference);\n  m.def(\"TFE_MonitoringDeleteIntGauge0\", &TFE_MonitoringDeleteIntGauge0,\n        py::return_value_policy::reference);\n  m.def(\"TFE_MonitoringGetCellIntGauge0\", &TFE_MonitoringGetCellIntGauge0,\n        py::return_value_policy::reference);\n  m.def(\n      \"TFE_MonitoringNewIntGauge1\",\n      [](const char* name, const char* description, const char* label1) {\n        tensorflow::Safe_TF_StatusPtr status =\n            tensorflow::make_safe(TF_NewStatus());\n        auto output =\n            TFE_MonitoringNewIntGauge1(name, status.get(), description, label1);\n        tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n        return output;\n      },\n      py::return_value_policy::reference);\n  m.def(\"TFE_MonitoringDeleteIntGauge1\", &TFE_MonitoringDeleteIntGauge1,\n        py::return_value_policy::reference);\n  m.def(\"TFE_MonitoringGetCellIntGauge1\", &TFE_MonitoringGetCellIntGauge1,\n        py::return_value_policy::reference);\n  m.def(\n      \"TFE_MonitoringNewIntGauge2\",\n      [](const char* name, const char* description, const char* label1,\n         const char* label2) {\n        tensorflow::Safe_TF_StatusPtr status =\n            tensorflow::make_safe(TF_NewStatus());\n        auto output = TFE_MonitoringNewIntGauge2(name, status.get(),\n                                                 description, label1, label2);\n        tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n        return output;\n      },\n      py::return_value_policy::reference);\n  m.def(\"TFE_MonitoringDeleteIntGauge2\", &TFE_MonitoringDeleteIntGauge2,\n        py::return_value_policy::reference);\n  m.def(\"TFE_MonitoringGetCellIntGauge2\", &TFE_MonitoringGetCellIntGauge2,\n        py::return_value_policy::reference);\n  m.def(\"TFE_MonitoringStringGaugeCellSet\", &TFE_MonitoringStringGaugeCellSet);\n  m.def(\"TFE_MonitoringStringGaugeCellValue\",\n        &TFE_MonitoringStringGaugeCellValue);\n  m.def(\n      \"TFE_MonitoringNewStringGauge0\",\n      [](const char* name, const char* description) {\n        tensorflow::Safe_TF_StatusPtr status =\n            tensorflow::make_safe(TF_NewStatus());\n        auto output =\n            TFE_MonitoringNewStringGauge0(name, status.get(), description);\n        tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n        return output;\n      },\n      py::return_value_policy::reference);\n\n  // TFE_MonitoringStringGauge Logic\n  m.def(\"TFE_MonitoringDeleteStringGauge0\", &TFE_MonitoringDeleteStringGauge0);\n  m.def(\"TFE_MonitoringGetCellStringGauge0\", &TFE_MonitoringGetCellStringGauge0,\n        py::return_value_policy::reference);\n  m.def(\n      \"TFE_MonitoringNewStringGauge1\",\n      [](const char* name, const char* description, const char* label1) {\n        tensorflow::Safe_TF_StatusPtr status =\n            tensorflow::make_safe(TF_NewStatus());\n        auto output = TFE_MonitoringNewStringGauge1(name, status.get(),\n                                                    description, label1);\n        tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n        return output;\n      },\n      py::return_value_policy::reference);\n  m.def(\"TFE_MonitoringDeleteStringGauge1\", &TFE_MonitoringDeleteStringGauge1);\n  m.def(\"TFE_MonitoringGetCellStringGauge1\", &TFE_MonitoringGetCellStringGauge1,\n        py::return_value_policy::reference);\n  m.def(\n      \"TFE_MonitoringNewStringGauge2\",\n      [](const char* name, const char* description, const char* label1,\n         const char* label2) {\n        tensorflow::Safe_TF_StatusPtr status =\n            tensorflow::make_safe(TF_NewStatus());\n        auto output = TFE_MonitoringNewStringGauge2(\n            name, status.get(), description, label1, label2);\n        tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n        return output;\n      },\n      py::return_value_policy::reference);\n  m.def(\"TFE_MonitoringDeleteStringGauge2\", &TFE_MonitoringDeleteStringGauge2);\n  m.def(\"TFE_MonitoringGetCellStringGauge2\", &TFE_MonitoringGetCellStringGauge2,\n        py::return_value_policy::reference);\n\n  // TFE_MonitoringBoolGauge Logic\n  m.def(\"TFE_MonitoringBoolGaugeCellSet\", &TFE_MonitoringBoolGaugeCellSet);\n  m.def(\"TFE_MonitoringBoolGaugeCellValue\", &TFE_MonitoringBoolGaugeCellValue);\n  m.def(\n      \"TFE_MonitoringNewBoolGauge0\",\n      [](const char* name, const char* description) {\n        tensorflow::Safe_TF_StatusPtr status =\n            tensorflow::make_safe(TF_NewStatus());\n        auto output =\n            TFE_MonitoringNewBoolGauge0(name, status.get(), description);\n        tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n        return output;\n      },\n      py::return_value_policy::reference);\n  m.def(\"TFE_MonitoringDeleteBoolGauge0\", &TFE_MonitoringDeleteBoolGauge0,\n        py::return_value_policy::reference);\n  m.def(\"TFE_MonitoringGetCellBoolGauge0\", &TFE_MonitoringGetCellBoolGauge0,\n        py::return_value_policy::reference);\n  m.def(\n      \"TFE_MonitoringNewBoolGauge1\",\n      [](const char* name, const char* description, const char* label1) {\n        tensorflow::Safe_TF_StatusPtr status =\n            tensorflow::make_safe(TF_NewStatus());\n        auto output = TFE_MonitoringNewBoolGauge1(name, status.get(),\n                                                  description, label1);\n        tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n        return output;\n      },\n      py::return_value_policy::reference);\n  m.def(\"TFE_MonitoringDeleteBoolGauge1\", &TFE_MonitoringDeleteBoolGauge1,\n        py::return_value_policy::reference);\n  m.def(\"TFE_MonitoringGetCellBoolGauge1\", &TFE_MonitoringGetCellBoolGauge1,\n        py::return_value_policy::reference);\n  m.def(\n      \"TFE_MonitoringNewBoolGauge2\",\n      [](const char* name, const char* description, const char* label1,\n         const char* label2) {\n        tensorflow::Safe_TF_StatusPtr status =\n            tensorflow::make_safe(TF_NewStatus());\n        auto output = TFE_MonitoringNewBoolGauge2(name, status.get(),\n                                                  description, label1, label2);\n        tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n        return output;\n      },\n      py::return_value_policy::reference);\n  m.def(\"TFE_MonitoringDeleteBoolGauge2\", &TFE_MonitoringDeleteBoolGauge2,\n        py::return_value_policy::reference);\n  m.def(\"TFE_MonitoringGetCellBoolGauge2\", &TFE_MonitoringGetCellBoolGauge2,\n        py::return_value_policy::reference);\n\n  // TFE_MonitoringSampler Logic\n  m.def(\"TFE_MonitoringSamplerCellAdd\", &TFE_MonitoringSamplerCellAdd);\n  m.def(\"TFE_MonitoringSamplerCellValue\", &TFE_MonitoringSamplerCellValue);\n  m.def(\"TFE_MonitoringNewExponentialBuckets\",\n        &TFE_MonitoringNewExponentialBuckets,\n        py::return_value_policy::reference);\n  m.def(\"TFE_MonitoringDeleteBuckets\", &TFE_MonitoringDeleteBuckets,\n        py::return_value_policy::reference);\n  m.def(\n      \"TFE_MonitoringNewSampler0\",\n      [](const char* name, TFE_MonitoringBuckets* buckets,\n         const char* description) {\n        tensorflow::Safe_TF_StatusPtr status =\n            tensorflow::make_safe(TF_NewStatus());\n        auto output =\n            TFE_MonitoringNewSampler0(name, buckets, status.get(), description);\n        tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n        return output;\n      },\n      py::return_value_policy::reference);\n  m.def(\"TFE_MonitoringDeleteSampler0\", &TFE_MonitoringDeleteSampler0,\n        py::return_value_policy::reference);\n  m.def(\"TFE_MonitoringGetCellSampler0\", &TFE_MonitoringGetCellSampler0,\n        py::return_value_policy::reference);\n  m.def(\n      \"TFE_MonitoringNewSampler1\",\n      [](const char* name, TFE_MonitoringBuckets* buckets,\n         const char* description, const char* label1) {\n        tensorflow::Safe_TF_StatusPtr status =\n            tensorflow::make_safe(TF_NewStatus());\n        auto output = TFE_MonitoringNewSampler1(name, buckets, status.get(),\n                                                description, label1);\n        tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n        return output;\n      },\n      py::return_value_policy::reference);\n  m.def(\"TFE_MonitoringDeleteSampler1\", &TFE_MonitoringDeleteSampler1,\n        py::return_value_policy::reference);\n  m.def(\"TFE_MonitoringGetCellSampler1\", &TFE_MonitoringGetCellSampler1,\n        py::return_value_policy::reference);\n  m.def(\n      \"TFE_MonitoringNewSampler2\",\n      [](const char* name, TFE_MonitoringBuckets* buckets,\n         const char* description, const char* label1, const char* label2) {\n        tensorflow::Safe_TF_StatusPtr status =\n            tensorflow::make_safe(TF_NewStatus());\n        auto output = TFE_MonitoringNewSampler2(name, buckets, status.get(),\n                                                description, label1, label2);\n        tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n        return output;\n      },\n      py::return_value_policy::reference);\n  m.def(\"TFE_MonitoringDeleteSampler2\", &TFE_MonitoringDeleteSampler2,\n        py::return_value_policy::reference);\n  m.def(\"TFE_MonitoringGetCellSampler2\", &TFE_MonitoringGetCellSampler2,\n        py::return_value_policy::reference);\n\n  // TFE_CancellationManager Logic\n  m.def(\"TFE_NewCancellationManager\", &TFE_NewCancellationManager,\n        py::return_value_policy::reference);\n  m.def(\"TFE_CancellationManagerIsCancelled\",\n        &TFE_CancellationManagerIsCancelled);\n  m.def(\"TFE_CancellationManagerStartCancel\",\n        &TFE_CancellationManagerStartCancel);\n  m.def(\"TFE_DeleteCancellationManager\", &TFE_DeleteCancellationManager,\n        py::return_value_policy::reference);\n\n  m.def(\"TFE_ClearScalarCache\", &tensorflow::TFE_ClearScalarCache);\n\n  // Util buffer helper functions\n  m.def(\"TF_NewBufferFromString\", &TF_NewBufferFromString,\n        py::return_value_policy::reference);\n\n  // DLPack functions\n  m.def(\"TFE_ToDlpackCapsule\", [](py::handle& o) {\n    PyObject* eager_tensor_pyobject_ptr = o.ptr();\n    tensorflow::Safe_TF_StatusPtr status =\n        tensorflow::make_safe(TF_NewStatus());\n\n    if (!EagerTensor_CheckExact(eager_tensor_pyobject_ptr)) {\n      status->status = tensorflow::errors::InvalidArgument(\n          \"The argument to `to_dlpack` must be a TF tensor, not Python object\");\n      tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n    }\n\n    TFE_TensorHandle* thandle = EagerTensor_Handle(eager_tensor_pyobject_ptr);\n    void* dlm_ptr = tensorflow::TFE_HandleToDLPack(thandle, status.get());\n    tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n\n    py::capsule capsule(\n        dlm_ptr, tensorflow::kDlTensorCapsuleName, [](PyObject* capsule) {\n          if (PyCapsule_IsValid(capsule, tensorflow::kDlTensorCapsuleName)) {\n            void* dlm_rptr =\n                PyCapsule_GetPointer(capsule, tensorflow::kDlTensorCapsuleName);\n            if (dlm_rptr) {\n              tensorflow::TFE_CallDLManagedTensorDeleter(dlm_rptr);\n              PyCapsule_SetDestructor(capsule, nullptr);\n            }\n          }\n        });\n    return capsule;\n  });\n\n  m.def(\"TFE_FromDlpackCapsule\", [](const py::capsule& pycapsule,\n                                    const py::handle& context) {\n    tensorflow::Safe_TF_StatusPtr status =\n        tensorflow::make_safe(TF_NewStatus());\n    if (absl::string_view(pycapsule.name()) !=\n        tensorflow::kDlTensorCapsuleName) {\n      status->status = tensorflow::errors::InvalidArgument(\n          \"DLPack tensor must be a capsule with name \\\"dltensor\\\", got \\\"%s\\\". \"\n          \"Note that a DLPack tensor may be consumed at most once.\",\n          absl::string_view(pycapsule.name()));\n      tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n    }\n\n    TFE_TensorHandle* thandle = tensorflow::TFE_HandleFromDLPack(\n        pycapsule, status.get(), tensorflow::InputTFE_Context(context));\n\n    tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n\n    PyCapsule_SetName(pycapsule.ptr(), \"used_dltensor\");\n    PyCapsule_SetDestructor(pycapsule.ptr(), nullptr);\n\n    PyObject* pyhandle = EagerTensorFromHandle(thandle);\n    return tensorflow::PyoOrThrow(pyhandle);\n  });\n\n  m.def(\"TFE_Py_RegisterCustomDevice\", [](const py::handle& context,\n                                          const py::capsule& device,\n                                          const char* device_name,\n                                          const py::capsule& device_info) {\n    tensorflow::Safe_TF_StatusPtr status =\n        tensorflow::make_safe(TF_NewStatus());\n    if (absl::string_view(device.name()) != \"TFE_CustomDevice\") {\n      status->status = tensorflow::errors::InvalidArgument(\n          \"Expected a capsule named 'TFE_CustomDevice' for the `device` \"\n          \"argument, got \",\n          absl::string_view(device.name()));\n      tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n    }\n    if (absl::string_view(device_info.name()) !=\n        \"TFE_CustomDevice_DeviceInfo\") {\n      status->status = tensorflow::errors::InvalidArgument(\n          \"Expected a capsule named 'TFE_CustomDevice_DeviceInfo' for \"\n          \"the `device_info` argument, got \",\n          absl::string_view(device_info.name()));\n      tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n    }\n    // TFE_RegisterCustomDevice takes ownership\n    PyCapsule_SetDestructor(device_info.ptr(), nullptr);\n    TFE_RegisterCustomDevice(\n        tensorflow::InputTFE_Context(context),\n        *reinterpret_cast<TFE_CustomDevice*>(\n            PyCapsule_GetPointer(device.ptr(), \"TFE_CustomDevice\")),\n        device_name,\n        PyCapsule_GetPointer(device_info.ptr(), \"TFE_CustomDevice_DeviceInfo\"),\n        status.get());\n    tensorflow::MaybeRaiseRegisteredFromTFStatus(status.get());\n  });\n\n  py::class_<EagerContextThreadLocalDataWrapper>(m,\n                                                 \"EagerContextThreadLocalData\")\n      .def(py::init<py::handle, py::handle, py::handle>(),\n           py::arg(\"py_eager_context\"), py::arg(\"is_eager\"),\n           py::arg(\"device_spec\"))\n      .def_property(\"is_eager\",\n                    &EagerContextThreadLocalDataWrapper::get_is_eager,\n                    &EagerContextThreadLocalDataWrapper::set_is_eager)\n      .def_property(\n          \"invoking_op_callbacks\",\n          &EagerContextThreadLocalDataWrapper::get_invoking_op_callbacks,\n          &EagerContextThreadLocalDataWrapper::set_invoking_op_callbacks)\n      .def_property(\"device_name\",\n                    &EagerContextThreadLocalDataWrapper::get_device_name,\n                    &EagerContextThreadLocalDataWrapper::set_device_name)\n      .def_property(\"scope_name\",\n                    &EagerContextThreadLocalDataWrapper::get_scope_name,\n                    &EagerContextThreadLocalDataWrapper::set_scope_name)\n      .def_property(\"device_spec\",\n                    &EagerContextThreadLocalDataWrapper::get_device_spec,\n                    &EagerContextThreadLocalDataWrapper::set_device_spec)\n      .def_property(\n          \"function_call_options\",\n          &EagerContextThreadLocalDataWrapper::get_function_call_options,\n          &EagerContextThreadLocalDataWrapper::set_function_call_options)\n      .def_property(\"executor\",\n                    &EagerContextThreadLocalDataWrapper::get_executor,\n                    &EagerContextThreadLocalDataWrapper::set_executor)\n      .def_property(\"op_callbacks\",\n                    &EagerContextThreadLocalDataWrapper::get_op_callbacks,\n                    &EagerContextThreadLocalDataWrapper::set_op_callbacks);\n\n  // C API Enum\n\n  py::enum_<TFE_ContextDevicePlacementPolicy>(\n      m, \"TFE_ContextDevicePlacementPolicy\")\n      .value(\"TFE_DEVICE_PLACEMENT_EXPLICIT\", TFE_DEVICE_PLACEMENT_EXPLICIT)\n      .value(\"TFE_DEVICE_PLACEMENT_WARN\", TFE_DEVICE_PLACEMENT_WARN)\n      .value(\"TFE_DEVICE_PLACEMENT_SILENT\", TFE_DEVICE_PLACEMENT_SILENT)\n      .value(\"TFE_DEVICE_PLACEMENT_SILENT_FOR_INT32\",\n             TFE_DEVICE_PLACEMENT_SILENT_FOR_INT32)\n      .export_values();\n\n  py::enum_<TF_AttrType>(m, \"TF_AttrType\")\n      .value(\"TF_ATTR_STRING\", TF_ATTR_STRING)\n      .value(\"TF_ATTR_INT\", TF_ATTR_INT)\n      .value(\"TF_ATTR_FLOAT\", TF_ATTR_FLOAT)\n      .value(\"TF_ATTR_BOOL\", TF_ATTR_BOOL)\n      .value(\"TF_ATTR_TYPE\", TF_ATTR_TYPE)\n      .value(\"TF_ATTR_SHAPE\", TF_ATTR_SHAPE)\n      .value(\"TF_ATTR_TENSOR\", TF_ATTR_TENSOR)\n      .value(\"TF_ATTR_PLACEHOLDER\", TF_ATTR_PLACEHOLDER)\n      .value(\"TF_ATTR_FUNC\", TF_ATTR_FUNC)\n      .export_values();\n};\n"], "filenames": ["tensorflow/c/eager/dlpack.cc", "tensorflow/python/dlpack/BUILD", "tensorflow/python/dlpack/dlpack_test.py", "tensorflow/python/tfe_wrapper.cc"], "buggy_code_start_loc": [251, 22, 22, 1360], "buggy_code_end_loc": [286, 23, 107, 1364], "fixing_code_start_loc": [252, 21, 23, 1361], "fixing_code_end_loc": [302, 21, 116, 1370], "type": "CWE-252", "message": "In Tensorflow before versions 2.2.1 and 2.3.1, if a user passes an invalid argument to `dlpack.to_dlpack` the expected validations will cause variables to bind to `nullptr` while setting a `status` variable to the error condition. However, this `status` argument is not properly checked. Hence, code following these methods will bind references to null pointers. This is undefined behavior and reported as an error if compiling with `-fsanitize=null`. The issue is patched in commit 22e07fb204386768e5bcbea563641ea11f96ceb8 and is released in TensorFlow versions 2.2.1, or 2.3.1.", "other": {"cve": {"id": "CVE-2020-15191", "sourceIdentifier": "security-advisories@github.com", "published": "2020-09-25T19:15:14.417", "lastModified": "2021-11-18T17:18:41.497", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "In Tensorflow before versions 2.2.1 and 2.3.1, if a user passes an invalid argument to `dlpack.to_dlpack` the expected validations will cause variables to bind to `nullptr` while setting a `status` variable to the error condition. However, this `status` argument is not properly checked. Hence, code following these methods will bind references to null pointers. This is undefined behavior and reported as an error if compiling with `-fsanitize=null`. The issue is patched in commit 22e07fb204386768e5bcbea563641ea11f96ceb8 and is released in TensorFlow versions 2.2.1, or 2.3.1."}, {"lang": "es", "value": "En Tensorflow versiones anteriores a 2.2.1 y 2.3.1, si un usuario pasa un argumento no v\u00e1lido hacia \"dlpack.to_dlpack\", las comprobaciones previstas har\u00e1n que las variables se unan a \"nullptr\" mientras se establece una variable \"status\" para la condici\u00f3n de error.&#xa0;Sin embargo, este argumento \"status\" no se comprueba correctamente.&#xa0;Por lo tanto, el c\u00f3digo que sigue estos m\u00e9todos vincular\u00e1 referencias a punteros null.&#xa0;Este es un comportamiento indefinido y se reporta como un error si se compila con \"-fsanitize=null\".&#xa0;El problema es parcheado en el commit 22e07fb204386768e5bcbea563641ea11f96ceb8 y es publicado en TensorFlow versiones 2.2.1 o 2.3.1"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:L", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "LOW", "baseScore": 5.3, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 3.9, "impactScore": 1.4}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:L", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "LOW", "baseScore": 5.3, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 3.9, "impactScore": 1.4}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:L/Au:N/C:N/I:N/A:P", "accessVector": "NETWORK", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "PARTIAL", "baseScore": 5.0}, "baseSeverity": "MEDIUM", "exploitabilityScore": 10.0, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-252"}]}, {"source": "security-advisories@github.com", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-20"}, {"lang": "en", "value": "CWE-476"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.2.0:*:*:*:-:*:*:*", "matchCriteriaId": "FB9BCD7D-1626-429F-B479-7D2F1E46B9C4"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.3.0:*:*:*:-:*:*:*", "matchCriteriaId": "D0A7B69E-9388-48F0-B744-49453EBAF5D5"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:opensuse:leap:15.2:*:*:*:*:*:*:*", "matchCriteriaId": "B009C22E-30A4-4288-BCF6-C3E81DEAF45A"}]}]}], "references": [{"url": "http://lists.opensuse.org/opensuse-security-announce/2020-10/msg00065.html", "source": "security-advisories@github.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/commit/22e07fb204386768e5bcbea563641ea11f96ceb8", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/releases/tag/v2.3.1", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/security/advisories/GHSA-q8qj-fc9q-cphr", "source": "security-advisories@github.com", "tags": ["Exploit", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/tensorflow/tensorflow/commit/22e07fb204386768e5bcbea563641ea11f96ceb8"}}