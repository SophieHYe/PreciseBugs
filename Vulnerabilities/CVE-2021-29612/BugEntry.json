{"buggy_code": ["/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n// See docs in ../ops/linalg_ops.cc.\n\n#include \"third_party/eigen3/Eigen/Core\"\n#include \"tensorflow/core/framework/kernel_def_builder.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/tensor_shape.h\"\n#include \"tensorflow/core/kernels/fill_functor.h\"\n#include \"tensorflow/core/kernels/linalg/linalg_ops_common.h\"\n#include \"tensorflow/core/lib/core/errors.h\"\n#include \"tensorflow/core/platform/logging.h\"\n#include \"tensorflow/core/platform/macros.h\"\n#include \"tensorflow/core/platform/types.h\"\n#include \"tensorflow/core/util/matmul_bcast.h\"\n\nnamespace tensorflow {\n\ntypedef Eigen::ThreadPoolDevice CPUDevice;\n\ntemplate <typename Scalar>\nScalar eigen_conj(const Scalar& scalar) {\n  return Eigen::numext::conj<Scalar>(scalar);\n}\n\n// Sequential batch matrix triangular solve kernel that calls Eigen's\n// matrix triangular solve.\ntemplate <typename Scalar>\nstruct SequentialBandedTriangularSolveKernel {\n  using Matrix =\n      Eigen::Matrix<Scalar, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor>;\n  using ConstMatrixMap = Eigen::Map<const Matrix>;\n  using MatrixMap = Eigen::Map<Matrix>;\n  using RealScalar = typename Eigen::NumTraits<Scalar>::Real;\n\n  static ConstMatrixMap ConstTensorSliceToEigenMatrix(const Tensor& t,\n                                                      int slice) {\n    return ConstMatrixMap(\n        t.flat<Scalar>().data() + slice * t.dim_size(1) * t.dim_size(2),\n        t.dim_size(1), t.dim_size(2));\n  }\n\n  static MatrixMap TensorSliceToEigenMatrix(Tensor* t, int slice) {\n    return MatrixMap(\n        t->flat<Scalar>().data() + slice * t->dim_size(1) * t->dim_size(2),\n        t->dim_size(1), t->dim_size(2));\n  }\n\n  static void Run(const Tensor& in_x, const Tensor& in_y, bool lower,\n                  bool adjoint, const MatMulBCast& bcast, Tensor* out,\n                  int start, int limit) {\n    const bool should_bcast = bcast.IsBroadcastingRequired();\n    const auto& x_batch_indices = bcast.x_batch_indices();\n    const auto& y_batch_indices = bcast.y_batch_indices();\n    int num_bands = in_x.dim_size(1);\n    int matrix_size = in_x.dim_size(2);\n\n    for (int64 i = start; i < limit; ++i) {\n      const int64 x_batch_index = should_bcast ? x_batch_indices[i] : i;\n      const int64 y_batch_index = should_bcast ? y_batch_indices[i] : i;\n      auto matrix = ConstTensorSliceToEigenMatrix(in_x, x_batch_index);\n      auto rhs = ConstTensorSliceToEigenMatrix(in_y, y_batch_index);\n      auto output = TensorSliceToEigenMatrix(out, i);\n      // Below, we use the standard algorithm for computing a triangular solve,\n      // except we band limit it.\n      // Given A x = b, where A is lower triangular,\n      // x_i = (b_i - sum a_ij * x_j) / a_ii, where the sum is from\n      // j = 0 to i - 1.\n      //\n      // Now, in a banded triangular matrix, when i exceeds the band size,\n      // then the sum goes from j = i - band_size to i - 1, since the other\n      // elements are zero.\n      //\n      // Finally, given the band storage format, we'll need to change the\n      // indexing.\n      if (lower) {\n        if (!adjoint) {\n          output.row(0) = rhs.row(0) / matrix(0, 0);\n          for (int i = 1; i < matrix_size; ++i) {\n            if (i < num_bands) {\n              output.row(i).noalias() =\n                  (rhs.row(i) - matrix.block(1, i, i, 1).reverse().transpose() *\n                                    output.topRows(i)) /\n                  matrix(0, i);\n            } else {\n              output.row(i).noalias() =\n                  (rhs.row(i) -\n                   matrix.block(1, i, num_bands - 1, 1).reverse().transpose() *\n                       output.middleRows(i - (num_bands - 1), num_bands - 1)) /\n                  matrix(0, i);\n            }\n          }\n        } else {\n          // In the adjoint case, here and below, we now have an upper (lower)\n          // triangular matrix, and thus need to work through with the other\n          // case. We can't simply conjugate `matrix` and use the upper (lower)\n          // algorithm because the band storage format for upper and lower\n          // triangular matrices are different (in the lower case, we pad\n          // entries on the left, and in the upper case we pad entries on the\n          // right.\n          output.row(matrix_size - 1) =\n              rhs.row(matrix_size - 1) / eigen_conj(matrix(0, matrix_size - 1));\n          for (int i = matrix_size - 1; i >= 0; --i) {\n            output.row(i).noalias() = rhs.row(i);\n            for (int j = i + 1; j < std::min(matrix_size, i + num_bands); ++j) {\n              output.row(i).noalias() -=\n                  eigen_conj(matrix(j - i, j)) * output.row(j);\n            }\n            output.row(i) /= eigen_conj(matrix(0, i));\n          }\n        }\n      } else {\n        if (!adjoint) {\n          output.row(matrix_size - 1) =\n              rhs.row(matrix_size - 1) / matrix(num_bands - 1, matrix_size - 1);\n          for (int i = 1; i < matrix_size; ++i) {\n            int k = matrix_size - 1 - i;\n            if (i < num_bands) {\n              output.row(k).noalias() =\n                  (rhs.row(k) - matrix.block(num_bands - 1 - i, k, i, 1)\n                                        .reverse()\n                                        .transpose() *\n                                    output.bottomRows(i)) /\n                  matrix(num_bands - 1, k);\n            } else {\n              output.row(k).noalias() =\n                  (rhs.row(k) -\n                   matrix.block(0, k, num_bands - 1, 1).reverse().transpose() *\n                       output.middleRows(k + 1, num_bands - 1)) /\n                  matrix(num_bands - 1, k);\n            }\n          }\n        } else {\n          output.row(0) = rhs.row(0) / eigen_conj(matrix(num_bands - 1, 0));\n          for (int i = 1; i < matrix_size; ++i) {\n            output.row(i).noalias() = rhs.row(i);\n            for (int j = std::max(0, i - (num_bands - 1)); j < i; ++j) {\n              output.row(i).noalias() -=\n                  eigen_conj(matrix(num_bands - 1 - (i - j), j)) *\n                  output.row(j);\n            }\n            output.row(i) /= eigen_conj(matrix(num_bands - 1, i));\n          }\n        }\n      }\n    }\n  }\n};\n\ntemplate <typename Scalar>\nstruct LaunchBatchBandedTriangularSolve;\n\ntemplate <typename Scalar>\nstruct LaunchBatchBandedTriangularSolve {\n  static void Launch(OpKernelContext* context, const Tensor& in_x,\n                     const Tensor& in_y, bool adjoint, bool lower,\n                     const MatMulBCast& bcast, Tensor* out) {\n    // Number of banded matrix triangular solves i.e. size of the batch.\n    const int64 batch_size = bcast.output_batch_size();\n    const int64 cost_per_unit =\n        in_x.dim_size(1) * in_x.dim_size(2) * in_y.dim_size(2);\n    auto worker_threads = *(context->device()->tensorflow_cpu_worker_threads());\n\n    using Matrix =\n        Eigen::Matrix<Scalar, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor>;\n    using ConstMatrixMap = Eigen::Map<const Matrix>;\n    using RealScalar = typename Eigen::NumTraits<Scalar>::Real;\n    // Check diagonal before doing any solves. This is the first row in the\n    // lower case and else is the last row.\n    auto matrix = ConstMatrixMap(in_x.flat<Scalar>().data(), in_x.dim_size(1),\n                                 in_x.dim_size(2));\n    RealScalar min_abs_pivot;\n    if (lower) {\n      min_abs_pivot = matrix.row(0).cwiseAbs().minCoeff();\n    } else {\n      min_abs_pivot = matrix.row(in_x.dim_size(1) - 1).cwiseAbs().minCoeff();\n    }\n    OP_REQUIRES(context, min_abs_pivot > RealScalar(0),\n                errors::InvalidArgument(\"Input matrix is not invertible.\"));\n\n    Shard(worker_threads.num_threads, worker_threads.workers, batch_size,\n          cost_per_unit,\n          [&in_x, &in_y, adjoint, lower, &bcast, out](int64 start,\n                                                      int64 limit) {\n            SequentialBandedTriangularSolveKernel<Scalar>::Run(\n                in_x, in_y, lower, adjoint, bcast, out, start, limit);\n          });\n  }\n};\n\ntemplate <typename Scalar>\nclass BandedTriangularSolveOpCpu : public OpKernel {\n public:\n  explicit BandedTriangularSolveOpCpu(OpKernelConstruction* context)\n      : OpKernel(context) {\n    OP_REQUIRES_OK(context, context->GetAttr(\"lower\", &lower_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"adjoint\", &adjoint_));\n  }\n\n  ~BandedTriangularSolveOpCpu() override {}\n\n  void Compute(OpKernelContext* ctx) override {\n    const Tensor& in0 = ctx->input(0);\n    const Tensor& in1 = ctx->input(1);\n\n    ValidateInputTensors(ctx, in0, in1);\n\n    MatMulBCast bcast(in0.shape().dim_sizes(), in1.shape().dim_sizes());\n    OP_REQUIRES(\n        ctx, bcast.IsValid(),\n        errors::InvalidArgument(\n            \"In[0] and In[1] must have compatible batch dimensions: \",\n            in0.shape().DebugString(), \" vs. \", in1.shape().DebugString()));\n\n    TensorShape out_shape = bcast.output_batch_shape();\n    auto batch_size = bcast.output_batch_size();\n    auto d0 = in0.dim_size(in0.dims() - 2);  // Band size.\n    auto d1 = in0.dim_size(in0.dims() - 1);\n    Tensor in0_reshaped;\n    OP_REQUIRES(\n        ctx,\n        in0_reshaped.CopyFrom(in0, TensorShape({bcast.x_batch_size(), d0, d1})),\n        errors::Internal(\"Failed to reshape In[0] from \",\n                         in0.shape().DebugString()));\n    auto d2 = in1.dim_size(in1.dims() - 2);\n    auto d3 = in1.dim_size(in1.dims() - 1);\n    Tensor in1_reshaped;\n    OP_REQUIRES(\n        ctx,\n        in1_reshaped.CopyFrom(in1, TensorShape({bcast.y_batch_size(), d2, d3})),\n        errors::Internal(\"Failed to reshape In[1] from \",\n                         in1.shape().DebugString()));\n    OP_REQUIRES(ctx, d1 == d2,\n                errors::InvalidArgument(\n                    \"In[0] mismatch In[1] shape: \", d1, \" vs. \", d2, \": \",\n                    in0.shape().DebugString(), \" \", in1.shape().DebugString(),\n                    \" \", lower_, \" \", adjoint_));\n    out_shape.AddDim(d1);\n    out_shape.AddDim(d3);\n    Tensor* out = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, out_shape, &out));\n    if (out->NumElements() == 0) {\n      return;\n    }\n    Tensor out_reshaped;\n    OP_REQUIRES(ctx,\n                out_reshaped.CopyFrom(*out, TensorShape({batch_size, d1, d3})),\n                errors::Internal(\"Failed to reshape output from \",\n                                 out->shape().DebugString()));\n    LaunchBatchBandedTriangularSolve<Scalar>::Launch(\n        ctx, in0_reshaped, in1_reshaped, adjoint_, lower_, bcast,\n        &out_reshaped);\n  }\n\n private:\n  void ValidateInputTensors(OpKernelContext* ctx, const Tensor& in0,\n                            const Tensor& in1) {\n    OP_REQUIRES(\n        ctx, in0.dims() >= 2,\n        errors::InvalidArgument(\"In[0] ndims must be >= 2: \", in0.dims()));\n\n    OP_REQUIRES(\n        ctx, in1.dims() >= 2,\n        errors::InvalidArgument(\"In[1] ndims must be >= 2: \", in1.dims()));\n\n    OP_REQUIRES(ctx, in0.NumElements() > 0,\n                errors::InvalidArgument(\"In[0] must not be an empty tensor: \",\n                                        in0.DebugString()));\n\n    OP_REQUIRES(ctx, in1.NumElements() > 0,\n                errors::InvalidArgument(\"In[1] must not be an empty tensor: \",\n                                        in1.DebugString()));\n  }\n  bool lower_;\n  bool adjoint_;\n};\n\n#define REGISTER_BANDED_TRIANGULAR_SOLVE_CPU(TYPE)        \\\n  REGISTER_KERNEL_BUILDER(Name(\"BandedTriangularSolve\")   \\\n                              .Device(DEVICE_CPU)         \\\n                              .TypeConstraint<TYPE>(\"T\"), \\\n                          BandedTriangularSolveOpCpu<TYPE>);\n\nREGISTER_BANDED_TRIANGULAR_SOLVE_CPU(float);\nREGISTER_BANDED_TRIANGULAR_SOLVE_CPU(double);\nREGISTER_BANDED_TRIANGULAR_SOLVE_CPU(complex64);\nREGISTER_BANDED_TRIANGULAR_SOLVE_CPU(complex128);\n\n}  // namespace tensorflow\n"], "fixing_code": ["/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n// See docs in ../ops/linalg_ops.cc.\n\n#include \"third_party/eigen3/Eigen/Core\"\n#include \"tensorflow/core/framework/kernel_def_builder.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/tensor_shape.h\"\n#include \"tensorflow/core/kernels/fill_functor.h\"\n#include \"tensorflow/core/kernels/linalg/linalg_ops_common.h\"\n#include \"tensorflow/core/lib/core/errors.h\"\n#include \"tensorflow/core/platform/logging.h\"\n#include \"tensorflow/core/platform/macros.h\"\n#include \"tensorflow/core/platform/types.h\"\n#include \"tensorflow/core/util/matmul_bcast.h\"\n\nnamespace tensorflow {\n\ntypedef Eigen::ThreadPoolDevice CPUDevice;\n\ntemplate <typename Scalar>\nScalar eigen_conj(const Scalar& scalar) {\n  return Eigen::numext::conj<Scalar>(scalar);\n}\n\n// Sequential batch matrix triangular solve kernel that calls Eigen's\n// matrix triangular solve.\ntemplate <typename Scalar>\nstruct SequentialBandedTriangularSolveKernel {\n  using Matrix =\n      Eigen::Matrix<Scalar, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor>;\n  using ConstMatrixMap = Eigen::Map<const Matrix>;\n  using MatrixMap = Eigen::Map<Matrix>;\n  using RealScalar = typename Eigen::NumTraits<Scalar>::Real;\n\n  static ConstMatrixMap ConstTensorSliceToEigenMatrix(const Tensor& t,\n                                                      int slice) {\n    return ConstMatrixMap(\n        t.flat<Scalar>().data() + slice * t.dim_size(1) * t.dim_size(2),\n        t.dim_size(1), t.dim_size(2));\n  }\n\n  static MatrixMap TensorSliceToEigenMatrix(Tensor* t, int slice) {\n    return MatrixMap(\n        t->flat<Scalar>().data() + slice * t->dim_size(1) * t->dim_size(2),\n        t->dim_size(1), t->dim_size(2));\n  }\n\n  static void Run(const Tensor& in_x, const Tensor& in_y, bool lower,\n                  bool adjoint, const MatMulBCast& bcast, Tensor* out,\n                  int start, int limit) {\n    const bool should_bcast = bcast.IsBroadcastingRequired();\n    const auto& x_batch_indices = bcast.x_batch_indices();\n    const auto& y_batch_indices = bcast.y_batch_indices();\n    int num_bands = in_x.dim_size(1);\n    int matrix_size = in_x.dim_size(2);\n\n    for (int64 i = start; i < limit; ++i) {\n      const int64 x_batch_index = should_bcast ? x_batch_indices[i] : i;\n      const int64 y_batch_index = should_bcast ? y_batch_indices[i] : i;\n      auto matrix = ConstTensorSliceToEigenMatrix(in_x, x_batch_index);\n      auto rhs = ConstTensorSliceToEigenMatrix(in_y, y_batch_index);\n      auto output = TensorSliceToEigenMatrix(out, i);\n      // Below, we use the standard algorithm for computing a triangular solve,\n      // except we band limit it.\n      // Given A x = b, where A is lower triangular,\n      // x_i = (b_i - sum a_ij * x_j) / a_ii, where the sum is from\n      // j = 0 to i - 1.\n      //\n      // Now, in a banded triangular matrix, when i exceeds the band size,\n      // then the sum goes from j = i - band_size to i - 1, since the other\n      // elements are zero.\n      //\n      // Finally, given the band storage format, we'll need to change the\n      // indexing.\n      if (lower) {\n        if (!adjoint) {\n          output.row(0) = rhs.row(0) / matrix(0, 0);\n          for (int i = 1; i < matrix_size; ++i) {\n            if (i < num_bands) {\n              output.row(i).noalias() =\n                  (rhs.row(i) - matrix.block(1, i, i, 1).reverse().transpose() *\n                                    output.topRows(i)) /\n                  matrix(0, i);\n            } else {\n              output.row(i).noalias() =\n                  (rhs.row(i) -\n                   matrix.block(1, i, num_bands - 1, 1).reverse().transpose() *\n                       output.middleRows(i - (num_bands - 1), num_bands - 1)) /\n                  matrix(0, i);\n            }\n          }\n        } else {\n          // In the adjoint case, here and below, we now have an upper (lower)\n          // triangular matrix, and thus need to work through with the other\n          // case. We can't simply conjugate `matrix` and use the upper (lower)\n          // algorithm because the band storage format for upper and lower\n          // triangular matrices are different (in the lower case, we pad\n          // entries on the left, and in the upper case we pad entries on the\n          // right.\n          output.row(matrix_size - 1) =\n              rhs.row(matrix_size - 1) / eigen_conj(matrix(0, matrix_size - 1));\n          for (int i = matrix_size - 1; i >= 0; --i) {\n            output.row(i).noalias() = rhs.row(i);\n            for (int j = i + 1; j < std::min(matrix_size, i + num_bands); ++j) {\n              output.row(i).noalias() -=\n                  eigen_conj(matrix(j - i, j)) * output.row(j);\n            }\n            output.row(i) /= eigen_conj(matrix(0, i));\n          }\n        }\n      } else {\n        if (!adjoint) {\n          output.row(matrix_size - 1) =\n              rhs.row(matrix_size - 1) / matrix(num_bands - 1, matrix_size - 1);\n          for (int i = 1; i < matrix_size; ++i) {\n            int k = matrix_size - 1 - i;\n            if (i < num_bands) {\n              output.row(k).noalias() =\n                  (rhs.row(k) - matrix.block(num_bands - 1 - i, k, i, 1)\n                                        .reverse()\n                                        .transpose() *\n                                    output.bottomRows(i)) /\n                  matrix(num_bands - 1, k);\n            } else {\n              output.row(k).noalias() =\n                  (rhs.row(k) -\n                   matrix.block(0, k, num_bands - 1, 1).reverse().transpose() *\n                       output.middleRows(k + 1, num_bands - 1)) /\n                  matrix(num_bands - 1, k);\n            }\n          }\n        } else {\n          output.row(0) = rhs.row(0) / eigen_conj(matrix(num_bands - 1, 0));\n          for (int i = 1; i < matrix_size; ++i) {\n            output.row(i).noalias() = rhs.row(i);\n            for (int j = std::max(0, i - (num_bands - 1)); j < i; ++j) {\n              output.row(i).noalias() -=\n                  eigen_conj(matrix(num_bands - 1 - (i - j), j)) *\n                  output.row(j);\n            }\n            output.row(i) /= eigen_conj(matrix(num_bands - 1, i));\n          }\n        }\n      }\n    }\n  }\n};\n\ntemplate <typename Scalar>\nstruct LaunchBatchBandedTriangularSolve;\n\ntemplate <typename Scalar>\nstruct LaunchBatchBandedTriangularSolve {\n  static void Launch(OpKernelContext* context, const Tensor& in_x,\n                     const Tensor& in_y, bool adjoint, bool lower,\n                     const MatMulBCast& bcast, Tensor* out) {\n    // Number of banded matrix triangular solves i.e. size of the batch.\n    const int64 batch_size = bcast.output_batch_size();\n    const int64 cost_per_unit =\n        in_x.dim_size(1) * in_x.dim_size(2) * in_y.dim_size(2);\n    auto worker_threads = *(context->device()->tensorflow_cpu_worker_threads());\n\n    using Matrix =\n        Eigen::Matrix<Scalar, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor>;\n    using ConstMatrixMap = Eigen::Map<const Matrix>;\n    using RealScalar = typename Eigen::NumTraits<Scalar>::Real;\n    // Check diagonal before doing any solves. This is the first row in the\n    // lower case and else is the last row.\n    auto matrix = ConstMatrixMap(in_x.flat<Scalar>().data(), in_x.dim_size(1),\n                                 in_x.dim_size(2));\n    RealScalar min_abs_pivot;\n    if (lower) {\n      min_abs_pivot = matrix.row(0).cwiseAbs().minCoeff();\n    } else {\n      min_abs_pivot = matrix.row(in_x.dim_size(1) - 1).cwiseAbs().minCoeff();\n    }\n    OP_REQUIRES(context, min_abs_pivot > RealScalar(0),\n                errors::InvalidArgument(\"Input matrix is not invertible.\"));\n\n    Shard(worker_threads.num_threads, worker_threads.workers, batch_size,\n          cost_per_unit,\n          [&in_x, &in_y, adjoint, lower, &bcast, out](int64 start,\n                                                      int64 limit) {\n            SequentialBandedTriangularSolveKernel<Scalar>::Run(\n                in_x, in_y, lower, adjoint, bcast, out, start, limit);\n          });\n  }\n};\n\ntemplate <typename Scalar>\nclass BandedTriangularSolveOpCpu : public OpKernel {\n public:\n  explicit BandedTriangularSolveOpCpu(OpKernelConstruction* context)\n      : OpKernel(context) {\n    OP_REQUIRES_OK(context, context->GetAttr(\"lower\", &lower_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"adjoint\", &adjoint_));\n  }\n\n  ~BandedTriangularSolveOpCpu() override {}\n\n  void Compute(OpKernelContext* ctx) override {\n    const Tensor& in0 = ctx->input(0);\n    const Tensor& in1 = ctx->input(1);\n\n    ValidateInputTensors(ctx, in0, in1);\n    if (!ctx->status().ok()) return;\n\n    MatMulBCast bcast(in0.shape().dim_sizes(), in1.shape().dim_sizes());\n    OP_REQUIRES(\n        ctx, bcast.IsValid(),\n        errors::InvalidArgument(\n            \"In[0] and In[1] must have compatible batch dimensions: \",\n            in0.shape().DebugString(), \" vs. \", in1.shape().DebugString()));\n\n    TensorShape out_shape = bcast.output_batch_shape();\n    auto batch_size = bcast.output_batch_size();\n    auto d0 = in0.dim_size(in0.dims() - 2);  // Band size.\n    auto d1 = in0.dim_size(in0.dims() - 1);\n    Tensor in0_reshaped;\n    OP_REQUIRES(\n        ctx,\n        in0_reshaped.CopyFrom(in0, TensorShape({bcast.x_batch_size(), d0, d1})),\n        errors::Internal(\"Failed to reshape In[0] from \",\n                         in0.shape().DebugString()));\n    auto d2 = in1.dim_size(in1.dims() - 2);\n    auto d3 = in1.dim_size(in1.dims() - 1);\n    Tensor in1_reshaped;\n    OP_REQUIRES(\n        ctx,\n        in1_reshaped.CopyFrom(in1, TensorShape({bcast.y_batch_size(), d2, d3})),\n        errors::Internal(\"Failed to reshape In[1] from \",\n                         in1.shape().DebugString()));\n    OP_REQUIRES(ctx, d1 == d2,\n                errors::InvalidArgument(\n                    \"In[0] mismatch In[1] shape: \", d1, \" vs. \", d2, \": \",\n                    in0.shape().DebugString(), \" \", in1.shape().DebugString(),\n                    \" \", lower_, \" \", adjoint_));\n    out_shape.AddDim(d1);\n    out_shape.AddDim(d3);\n    Tensor* out = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, out_shape, &out));\n    if (out->NumElements() == 0) {\n      return;\n    }\n    Tensor out_reshaped;\n    OP_REQUIRES(ctx,\n                out_reshaped.CopyFrom(*out, TensorShape({batch_size, d1, d3})),\n                errors::Internal(\"Failed to reshape output from \",\n                                 out->shape().DebugString()));\n    LaunchBatchBandedTriangularSolve<Scalar>::Launch(\n        ctx, in0_reshaped, in1_reshaped, adjoint_, lower_, bcast,\n        &out_reshaped);\n  }\n\n private:\n  void ValidateInputTensors(OpKernelContext* ctx, const Tensor& in0,\n                            const Tensor& in1) {\n    OP_REQUIRES(\n        ctx, in0.dims() >= 2,\n        errors::InvalidArgument(\"In[0] ndims must be >= 2: \", in0.dims()));\n\n    OP_REQUIRES(\n        ctx, in1.dims() >= 2,\n        errors::InvalidArgument(\"In[1] ndims must be >= 2: \", in1.dims()));\n\n    OP_REQUIRES(ctx, in0.NumElements() > 0,\n                errors::InvalidArgument(\"In[0] must not be an empty tensor: \",\n                                        in0.DebugString()));\n\n    OP_REQUIRES(ctx, in1.NumElements() > 0,\n                errors::InvalidArgument(\"In[1] must not be an empty tensor: \",\n                                        in1.DebugString()));\n  }\n  bool lower_;\n  bool adjoint_;\n};\n\n#define REGISTER_BANDED_TRIANGULAR_SOLVE_CPU(TYPE)        \\\n  REGISTER_KERNEL_BUILDER(Name(\"BandedTriangularSolve\")   \\\n                              .Device(DEVICE_CPU)         \\\n                              .TypeConstraint<TYPE>(\"T\"), \\\n                          BandedTriangularSolveOpCpu<TYPE>);\n\nREGISTER_BANDED_TRIANGULAR_SOLVE_CPU(float);\nREGISTER_BANDED_TRIANGULAR_SOLVE_CPU(double);\nREGISTER_BANDED_TRIANGULAR_SOLVE_CPU(complex64);\nREGISTER_BANDED_TRIANGULAR_SOLVE_CPU(complex128);\n\n}  // namespace tensorflow\n"], "filenames": ["tensorflow/core/kernels/linalg/banded_triangular_solve_op.cc"], "buggy_code_start_loc": [219], "buggy_code_end_loc": [219], "fixing_code_start_loc": [220], "fixing_code_end_loc": [221], "type": "CWE-787", "message": "TensorFlow is an end-to-end open source platform for machine learning. An attacker can trigger a heap buffer overflow in Eigen implementation of `tf.raw_ops.BandedTriangularSolve`. The implementation(https://github.com/tensorflow/tensorflow/blob/eccb7ec454e6617738554a255d77f08e60ee0808/tensorflow/core/kernels/linalg/banded_triangular_solve_op.cc#L269-L278) calls `ValidateInputTensors` for input validation but fails to validate that the two tensors are not empty. Furthermore, since `OP_REQUIRES` macro only stops execution of current function after setting `ctx->status()` to a non-OK value, callers of helper functions that use `OP_REQUIRES` must check value of `ctx->status()` before continuing. This doesn't happen in this op's implementation(https://github.com/tensorflow/tensorflow/blob/eccb7ec454e6617738554a255d77f08e60ee0808/tensorflow/core/kernels/linalg/banded_triangular_solve_op.cc#L219), hence the validation that is present is also not effective. The fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range.", "other": {"cve": {"id": "CVE-2021-29612", "sourceIdentifier": "security-advisories@github.com", "published": "2021-05-14T20:15:15.990", "lastModified": "2022-10-25T20:07:16.133", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "TensorFlow is an end-to-end open source platform for machine learning. An attacker can trigger a heap buffer overflow in Eigen implementation of `tf.raw_ops.BandedTriangularSolve`. The implementation(https://github.com/tensorflow/tensorflow/blob/eccb7ec454e6617738554a255d77f08e60ee0808/tensorflow/core/kernels/linalg/banded_triangular_solve_op.cc#L269-L278) calls `ValidateInputTensors` for input validation but fails to validate that the two tensors are not empty. Furthermore, since `OP_REQUIRES` macro only stops execution of current function after setting `ctx->status()` to a non-OK value, callers of helper functions that use `OP_REQUIRES` must check value of `ctx->status()` before continuing. This doesn't happen in this op's implementation(https://github.com/tensorflow/tensorflow/blob/eccb7ec454e6617738554a255d77f08e60ee0808/tensorflow/core/kernels/linalg/banded_triangular_solve_op.cc#L219), hence the validation that is present is also not effective. The fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range."}, {"lang": "es", "value": "TensorFlow es una plataforma de c\u00f3digo abierto de extremo a extremo para el aprendizaje autom\u00e1tico.&#xa0;Un atacante puede desencadenar un desbordamiento del b\u00fafer en una implementaci\u00f3n de Eigen de \"tf.raw_ops.BandedTriangularSolve\".&#xa0;La implementaci\u00f3n (https://github.com/tensorflow/tensorflow/blob/eccb7ec454e6617738554a255d77f08e60ee0808/tensorflow/core/kernels/linalg/banded_triangular_solve_op.cc#L269-L278) llama a \"ValidateInputTensors\" no est\u00e1n vac\u00edas.&#xa0;Adem\u00e1s, dado que la macro \"OP_REQUIRES\" solo detiene una ejecuci\u00f3n de la funci\u00f3n actual despu\u00e9s de configurar la funci\u00f3n \"ctx-)status()\" a un valor que no es OK, los llamadores de funciones auxiliares que usan \"OP_REQUIRES\" deben comprobar el valor de \"ctx-)status()\" antes de continuar.&#xa0;Esto no sucede en una implementaci\u00f3n de esta operaci\u00f3n (https://github.&#xa0;com/tensorflow/tensorflow/blob/eccb7ec454e6617738554a255d77f08e60ee0808/tensorflow/core/kernels/linalg/banded_triangular_solve_op.cc#L219), por lo tanto, Una comprobaci\u00f3n que est\u00e1 presente tampoco es efectiva.&#xa0;La correcci\u00f3n ser\u00e1 inclu\u00edda en TensorFlow versi\u00f3n 2.5.0.&#xa0;Tambi\u00e9n seleccionaremos este commit en TensorFlow versi\u00f3n 2.4.2, TensorFlow versi\u00f3n 2.3.3, TensorFlow versi\u00f3n 2.2.3 y TensorFlow versi\u00f3n 2.1.4, ya que estos tambi\u00e9n est\u00e1n afectados y a\u00fan est\u00e1n en el rango compatible"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.9}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:H/PR:L/UI:N/S:U/C:N/I:L/A:L", "attackVector": "LOCAL", "attackComplexity": "HIGH", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "LOW", "availabilityImpact": "LOW", "baseScore": 3.6, "baseSeverity": "LOW"}, "exploitabilityScore": 1.0, "impactScore": 2.5}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:P/I:P/A:P", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "PARTIAL", "availabilityImpact": "PARTIAL", "baseScore": 4.6}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.9, "impactScore": 6.4, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-787"}]}, {"source": "security-advisories@github.com", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-120"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionEndExcluding": "2.1.4", "matchCriteriaId": "323ABCCE-24EB-47CC-87F6-48C101477587"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.2.0", "versionEndExcluding": "2.2.3", "matchCriteriaId": "64ABA90C-0649-4BB0-89C9-83C14BBDCC0F"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.3.0", "versionEndExcluding": "2.3.3", "matchCriteriaId": "0F83E0CF-CBF6-4C24-8683-3E7A5DC95BA9"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.4.0", "versionEndExcluding": "2.4.2", "matchCriteriaId": "8259531B-A8AC-4F8B-B60F-B69DE4767C03"}]}]}], "references": [{"url": "https://github.com/tensorflow/tensorflow/commit/0ab290774f91a23bebe30a358fde4e53ab4876a0", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/commit/ba6822bd7b7324ba201a28b2f278c29a98edbef2", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/security/advisories/GHSA-2xgj-xhgf-ggjv", "source": "security-advisories@github.com", "tags": ["Exploit", "Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/tensorflow/tensorflow/commit/0ab290774f91a23bebe30a358fde4e53ab4876a0"}}