{"buggy_code": ["/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#define EIGEN_USE_THREADS\n\n#include <stddef.h>\n\n#include <algorithm>\n#include <string>\n#include <vector>\n\n#include \"tensorflow/core/framework/kernel_def_builder.h\"\n#include \"tensorflow/core/framework/numeric_types.h\"\n#include \"tensorflow/core/framework/op.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/register_types.h\"\n#include \"tensorflow/core/framework/shape_inference.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/tensor_shape.h\"\n#include \"tensorflow/core/framework/tensor_shape.pb.h\"\n#include \"tensorflow/core/framework/tensor_types.h\"\n#include \"tensorflow/core/framework/types.h\"\n#include \"tensorflow/core/kernels/broadcast_to_op.h\"\n#include \"tensorflow/core/kernels/list_kernels.h\"\n#include \"tensorflow/core/lib/core/errors.h\"\n#include \"tensorflow/core/lib/core/status.h\"\n#include \"tensorflow/core/platform/bfloat16.h\"\n#include \"tensorflow/core/platform/types.h\"\n#include \"tensorflow/core/util/bcast.h\"\n#include \"tensorflow/core/util/ragged_to_dense_util.h\"\n\nnamespace tensorflow {\n\nnamespace {\ntypedef Eigen::ThreadPoolDevice CPUDevice;\nusing ::std::vector;\n\nconst int kShapeInputIndex = 0;\nconst int kValueInputIndex = 1;\nconst int kDefaultValueInputIndex = 2;\nconst int kFirstPartitionInputIndex = 3;\n\ntemplate <typename INDEX_TYPE>\nclass RaggedTensorToTensorBaseOp : public OpKernel {\n public:\n  typedef\n      typename ::tensorflow::TTypes<const INDEX_TYPE>::Flat RowPartitionTensor;\n\n  explicit RaggedTensorToTensorBaseOp(OpKernelConstruction* context)\n      : OpKernel(context) {\n    OP_REQUIRES_OK(context, GetRowPartitionTypes<OpKernelConstruction>(\n                                context, &row_partition_types_));\n    ragged_rank_ = GetRaggedRank(row_partition_types_);\n  }\n\n  // Returns the relationship between dimension and dimension + 1.\n  RowPartitionType GetRowPartitionTypeByDimension(int dimension) {\n    if (row_partition_types_[0] == RowPartitionType::FIRST_DIM_SIZE) {\n      return row_partition_types_[dimension + 1];\n    } else {\n      return row_partition_types_[dimension];\n    }\n  }\n\n  // Returns the relationship between dimension and dimension + 1.\n  RowPartitionTensor GetRowPartitionTensor(OpKernelContext* c, int dimension) {\n    if (row_partition_types_[0] == RowPartitionType::FIRST_DIM_SIZE) {\n      return c->input(dimension + 1 + kFirstPartitionInputIndex)\n          .flat<INDEX_TYPE>();\n    } else {\n      return c->input(dimension + kFirstPartitionInputIndex).flat<INDEX_TYPE>();\n    }\n  }\n\n  Status GetMaxWidth(OpKernelContext* c, int dimension, INDEX_TYPE* result) {\n    const RowPartitionTensor row_partition_tensor =\n        GetRowPartitionTensor(c, dimension - 1);\n    switch (GetRowPartitionTypeByDimension(dimension - 1)) {\n      case RowPartitionType::VALUE_ROWIDS:\n        *result = GetMaxWidthValueRowID(row_partition_tensor);\n        return Status::OK();\n      case RowPartitionType::ROW_SPLITS:\n        *result = GetMaxWidthRowSplit(row_partition_tensor);\n        return Status::OK();\n      default:\n        return errors::InvalidArgument(\n            \"Cannot handle partition type \",\n            RowPartitionTypeToString(\n                GetRowPartitionTypeByDimension(dimension - 1)));\n    }\n  }\n\n  static INDEX_TYPE GetMaxWidthRowSplit(const RowPartitionTensor& row_split) {\n    const INDEX_TYPE tensor_length = row_split.size();\n    if (tensor_length == 0 || tensor_length == 1) {\n      return 0;\n    }\n    INDEX_TYPE max_width = 0;\n    for (INDEX_TYPE i = 0; i < tensor_length - 1; ++i) {\n      const INDEX_TYPE current_width = row_split(i + 1) - row_split(i);\n      if (current_width > max_width) {\n        max_width = current_width;\n      }\n    }\n    return max_width;\n  }\n\n  static INDEX_TYPE GetMaxWidthValueRowID(\n      const RowPartitionTensor& value_rowids) {\n    const INDEX_TYPE index_length = value_rowids.size();\n    if (index_length == 0) {\n      return 0;\n    }\n    INDEX_TYPE first_equal_index = 0;\n    INDEX_TYPE first_equal_index_value = value_rowids(0);\n    INDEX_TYPE max_width = 0;\n    for (INDEX_TYPE i = 1; i < index_length; ++i) {\n      const INDEX_TYPE value = value_rowids(i);\n      if (value != first_equal_index_value) {\n        first_equal_index_value = value;\n        max_width = std::max(i - first_equal_index, max_width);\n        first_equal_index = i;\n      }\n    }\n    return std::max(index_length - first_equal_index, max_width);\n  }\n\n  Status CalculateOutputSize(INDEX_TYPE first_dim, OpKernelContext* c,\n                             vector<INDEX_TYPE>* result) {\n    TensorShapeProto value_shape_proto;\n    c->input(kValueInputIndex).shape().AsProto(&value_shape_proto);\n\n    TensorShapeProto default_value_shape_proto;\n    c->input(kDefaultValueInputIndex)\n        .shape()\n        .AsProto(&default_value_shape_proto);\n\n    TensorShapeProto output_shape_proto;\n    TF_RETURN_IF_ERROR(ValidateDefaultValueShape(default_value_shape_proto,\n                                                 value_shape_proto));\n\n    TensorShapeProto shape_proto;\n    {\n      PartialTensorShape partial_tensor_shape;\n      TF_RETURN_IF_ERROR(TensorShapeFromTensor(c->input(kShapeInputIndex),\n                                               &partial_tensor_shape));\n      partial_tensor_shape.AsProto(&shape_proto);\n    }\n\n    TF_RETURN_IF_ERROR(CombineRaggedTensorToTensorShapes(\n        ragged_rank_, shape_proto, value_shape_proto, &output_shape_proto));\n\n    result->reserve(output_shape_proto.dim_size());\n    for (const TensorShapeProto::Dim& dim : output_shape_proto.dim()) {\n      // Note that this may be -1 (if dimension size is unknown).\n      result->push_back(dim.size());\n    }\n\n    if ((*result)[0] < 0) {\n      (*result)[0] = first_dim;\n    }\n    for (int i = 1; i <= ragged_rank_; ++i) {\n      if ((*result)[i] < 0) {\n        TF_RETURN_IF_ERROR(GetMaxWidth(c, i, &(*result)[i]));\n      }\n    }\n    return Status::OK();\n  }\n\n  /**\n   * The output_index represents the index in the output tensor\n   * where the first element of a particular dimension would be written.\n   * If it is -1, it indicates that the index is out of scope.\n   * Example, given first_dimension = 10, first_dimension_output = 6,\n   * and output_index_multiplier = 100:\n   * result = [0 100 200 300 400 500 -1 -1 -1 -1]\n   * If first_dimension_output = 11 instead, then:\n   * result = [0 100 200 300 400 500 600 700 800 900]\n   */\n  void CalculateFirstParentOutputIndex(INDEX_TYPE first_dimension,\n                                       INDEX_TYPE output_index_multiplier,\n                                       INDEX_TYPE first_dimension_output,\n                                       vector<INDEX_TYPE>* result) {\n    const INDEX_TYPE min_dimension =\n        std::min(first_dimension, first_dimension_output);\n    result->reserve(first_dimension);\n    int current_output_index = 0;\n    for (INDEX_TYPE i = 0; i < min_dimension;\n         ++i, current_output_index += output_index_multiplier) {\n      result->push_back(current_output_index);\n    }\n    for (INDEX_TYPE i = min_dimension; i < first_dimension; ++i) {\n      result->push_back(-1);\n    }\n    DCHECK_EQ(result->size(), first_dimension);\n  }\n\n  void CalculateOutputIndexRowSplit(\n      const RowPartitionTensor& row_split,\n      const vector<INDEX_TYPE>& parent_output_index,\n      INDEX_TYPE output_index_multiplier, INDEX_TYPE output_size,\n      vector<INDEX_TYPE>* result) {\n    INDEX_TYPE row_split_size = row_split.size();\n    if (row_split_size > 0) {\n      result->reserve(row_split(row_split_size - 1));\n    }\n    for (INDEX_TYPE i = 0; i < row_split_size - 1; ++i) {\n      INDEX_TYPE row_length = row_split(i + 1) - row_split(i);\n      INDEX_TYPE real_length = std::min(output_size, row_length);\n      INDEX_TYPE parent_output_index_current = parent_output_index[i];\n\n      if (parent_output_index_current == -1) {\n        real_length = 0;\n      }\n      for (INDEX_TYPE j = 0; j < real_length; ++j) {\n        result->push_back(parent_output_index_current);\n        parent_output_index_current += output_index_multiplier;\n      }\n      for (INDEX_TYPE j = 0; j < row_length - real_length; ++j) {\n        result->push_back(-1);\n      }\n    }\n    if (row_split_size > 0) {\n      DCHECK_EQ(result->size(), row_split(row_split_size - 1));\n    }\n  }\n\n  // Calculate the output index of the first element of a list.\n  // The parent_output_index is the same computation for the previous list.\n  // -1 indicates an element or list that is out of range.\n  // The output_index_multiplier is the number of output indices one moves\n  // forward for each column.\n  // E.g., given:\n  // value_rowids:[0 1 2 2 2 3 5 5 6]\n  // parent_output_index:[1000 1100 2000 2100 -1 3000 4000]\n  // output_index_multiplier: 10\n  // output_size: 2\n  // You get:\n  // result = [1000 1100 2000 2010 -1 2100 -1 -1 3000]\n  // result[0] = parent_output_index[value_rowids[0]]\n  // result[1] = parent_output_index[value_rowids[1]]\n  // result[2] = parent_output_index[value_rowids[2]]\n  // result[3] = parent_output_index[value_rowids[2] + 10]\n  // result[4] = -1 because it is the third element the size is 2.\n  // result[5] = parent_output_index[value_rowids[3]]\n  // result[6] = -1 because parent_output_index[value_rowids[6]] == -1\n  // result[7] = -1 because parent_output_index[value_rowids[6]] == -1\n  // result[8] = parent_output_index[value_rowids[7]]\n  void CalculateOutputIndexValueRowID(\n      const RowPartitionTensor& value_rowids,\n      const vector<INDEX_TYPE>& parent_output_index,\n      INDEX_TYPE output_index_multiplier, INDEX_TYPE output_size,\n      vector<INDEX_TYPE>* result) {\n    const INDEX_TYPE index_size = value_rowids.size();\n    result->reserve(index_size);\n    if (index_size == 0) {\n      return;\n    }\n\n    INDEX_TYPE current_output_column = 0;\n    INDEX_TYPE current_value_rowid = value_rowids(0);\n    DCHECK_LT(current_value_rowid, parent_output_index.size());\n    INDEX_TYPE current_output_index = parent_output_index[current_value_rowid];\n    result->push_back(current_output_index);\n    for (INDEX_TYPE i = 1; i < index_size; ++i) {\n      INDEX_TYPE next_value_rowid = value_rowids(i);\n      if (next_value_rowid == current_value_rowid) {\n        if (current_output_index >= 0) {\n          ++current_output_column;\n          if (current_output_column < output_size) {\n            current_output_index += output_index_multiplier;\n          } else {\n            current_output_index = -1;\n          }\n        }\n      } else {\n        current_output_column = 0;\n        current_value_rowid = next_value_rowid;\n        DCHECK_LT(next_value_rowid, parent_output_index.size());\n        current_output_index = parent_output_index[next_value_rowid];\n      }\n      result->push_back(current_output_index);\n    }\n    DCHECK_EQ(result->size(), value_rowids.size());\n  }\n\n  Status CalculateOutputIndex(OpKernelContext* context, int dimension,\n                              const vector<INDEX_TYPE>& parent_output_index,\n                              INDEX_TYPE output_index_multiplier,\n                              INDEX_TYPE output_size,\n                              vector<INDEX_TYPE>* result) {\n    const RowPartitionTensor row_partition_tensor =\n        GetRowPartitionTensor(context, dimension);\n    auto partition_type = GetRowPartitionTypeByDimension(dimension);\n    switch (partition_type) {\n      case RowPartitionType::VALUE_ROWIDS:\n        CalculateOutputIndexValueRowID(\n            row_partition_tensor, parent_output_index, output_index_multiplier,\n            output_size, result);\n        return tensorflow::Status::OK();\n      case RowPartitionType::ROW_SPLITS:\n        CalculateOutputIndexRowSplit(row_partition_tensor, parent_output_index,\n                                     output_index_multiplier, output_size,\n                                     result);\n        return tensorflow::Status::OK();\n      default:\n        return errors::InvalidArgument(\n            \"Unsupported partition type:\",\n            RowPartitionTypeToString(partition_type));\n    }\n  }\n\n  Status GetFirstDimensionSize(OpKernelContext* context, INDEX_TYPE* result) {\n    const Tensor first_partition_tensor =\n        context->input(kFirstPartitionInputIndex);\n    const RowPartitionType first_partition_type = row_partition_types_[0];\n    switch (first_partition_type) {\n      case RowPartitionType::FIRST_DIM_SIZE:\n        *result = first_partition_tensor.scalar<INDEX_TYPE>()();\n        return Status::OK();\n      case RowPartitionType::VALUE_ROWIDS:\n        return errors::InvalidArgument(\n            \"Cannot handle VALUE_ROWIDS in first dimension.\");\n      case RowPartitionType::ROW_SPLITS:\n        *result = first_partition_tensor.shape().dim_size(0) - 1;\n        return Status::OK();\n      default:\n        return errors::InvalidArgument(\n            \"Cannot handle type \",\n            RowPartitionTypeToString(first_partition_type));\n    }\n  }\n\n  void Compute(OpKernelContext* context) override {\n    INDEX_TYPE first_dimension;\n    const Tensor first_partition_tensor =\n        context->input(kFirstPartitionInputIndex);\n    OP_REQUIRES(context, first_partition_tensor.NumElements() > 0,\n                errors::InvalidArgument(\"Invalid first partition input. Tensor \"\n                                        \"requires at least one element.\"));\n    OP_REQUIRES_OK(context, GetFirstDimensionSize(context, &first_dimension));\n    vector<INDEX_TYPE> output_size;\n    OP_REQUIRES_OK(context,\n                   CalculateOutputSize(first_dimension, context, &output_size));\n    vector<INDEX_TYPE> multiplier;\n    multiplier.resize(ragged_rank_ + 1);\n\n    multiplier[multiplier.size() - 1] = 1;\n    for (int i = multiplier.size() - 2; i >= 0; --i) {\n      multiplier[i] = multiplier[i + 1] * output_size[i + 1];\n    }\n    // Full size of the tensor.\n    TensorShape output_shape;\n    OP_REQUIRES_OK(context,\n                   TensorShapeUtils::MakeShape(output_size, &output_shape));\n    Tensor* output_tensor = nullptr;\n\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(0, output_shape, &output_tensor));\n    const INDEX_TYPE full_size = multiplier[0] * output_size[0];\n    if (full_size > 0) {\n      vector<INDEX_TYPE> output_index, new_output_index;\n      int nvals = context->input(kValueInputIndex).shape().dim_size(0);\n      output_index.reserve(nvals);\n      new_output_index.reserve(nvals);\n\n      CalculateFirstParentOutputIndex(first_dimension, multiplier[0],\n                                      output_size[0], &output_index);\n      for (int i = 1; i <= ragged_rank_; ++i) {\n        OP_REQUIRES_OK(context, CalculateOutputIndex(\n                                    context, i - 1, output_index, multiplier[i],\n                                    output_size[i], &new_output_index));\n        output_index.swap(new_output_index);\n        new_output_index.clear();\n      }\n\n      SetOutput(context, ragged_rank_, output_index, output_tensor);\n    }\n  }\n  virtual void SetOutput(OpKernelContext* context, int ragged_rank,\n                         const vector<INDEX_TYPE>& output_index,\n                         Tensor* output_tensor) = 0;\n\n private:\n  vector<RowPartitionType> row_partition_types_;\n  int ragged_rank_;\n};\n\ntemplate <typename VALUE_TYPE, typename INDEX_TYPE>\nvoid slow_copy_array(VALUE_TYPE* dst, const VALUE_TYPE* src, INDEX_TYPE size) {\n  for (INDEX_TYPE index = 0; index < size; ++index) {\n    dst[index] = src[index];\n  }\n}\n\ntemplate <typename VALUE_TYPE, typename INDEX_TYPE>\nvoid copy_array(VALUE_TYPE* dst, const VALUE_TYPE* src, INDEX_TYPE size) {\n  memcpy(dst, src, size * sizeof(VALUE_TYPE));\n}\n\ntemplate <>\nvoid copy_array<tstring, int64>(tstring* dst, const tstring* src, int64 size) {\n  slow_copy_array(dst, src, size);\n}\n\ntemplate <>\nvoid copy_array<tstring, int32>(tstring* dst, const tstring* src, int32 size) {\n  slow_copy_array(dst, src, size);\n}\n\n// If we don't specialize for Eigen::half, we get:\n// undefined behavior, destination object type 'Eigen::half'\n// is not TriviallyCopyable\ntemplate <>\nvoid copy_array<Eigen::half, int64>(Eigen::half* dst, const Eigen::half* src,\n                                    int64 size) {\n  slow_copy_array(dst, src, size);\n}\n\ntemplate <>\nvoid copy_array<Eigen::half, int32>(Eigen::half* dst, const Eigen::half* src,\n                                    int32 size) {\n  slow_copy_array(dst, src, size);\n}\n\ntemplate <typename VALUE_TYPE, typename INDEX_TYPE>\nclass RaggedTensorToTensorOp : public RaggedTensorToTensorBaseOp<INDEX_TYPE> {\n public:\n  explicit RaggedTensorToTensorOp(OpKernelConstruction* context)\n      : RaggedTensorToTensorBaseOp<INDEX_TYPE>(context) {}\n\n  void SetOutput(OpKernelContext* context, int ragged_rank,\n                 const vector<INDEX_TYPE>& output_index,\n                 Tensor* output_tensor) override {\n    // Note: it's ok to use OP_REQUIRES_OK (rather than TF_RETURN_IF_ERROR)\n    // in this function, but only because it's the last thing we do before\n    // returning from Compute().\n\n    if (output_tensor->NumElements() == 0) return;\n\n    const auto& values_tensor = context->input(kValueInputIndex);\n    const VALUE_TYPE* values_base = values_tensor.flat<VALUE_TYPE>().data();\n    const auto& default_value_tensor = context->input(kDefaultValueInputIndex);\n    VALUE_TYPE* output_base = output_tensor->flat<VALUE_TYPE>().data();\n\n    TensorShape element_shape = output_tensor->shape();\n    element_shape.RemoveDimRange(0, ragged_rank + 1);\n    int value_element_size = element_shape.num_elements();\n    size_t output_index_size = output_index.size();\n\n    // Broadcast the default value to value_element_size.  (We can skip this\n    // if default_value_tensor.NumElements() == 1, since we use std::fill\n    // when that's true.)\n    const VALUE_TYPE* default_value =\n        default_value_tensor.flat<VALUE_TYPE>().data();\n    Tensor bcast_default;  // Temporary tensor for result of broadcast\n    if (default_value_tensor.NumElements() != value_element_size &&\n        default_value_tensor.NumElements() != 1) {\n      const auto& src_shape = default_value_tensor.shape();\n      BCast bcast(BCast::FromShape(src_shape), BCast::FromShape(element_shape),\n                  /*fewer_dims_optimization=*/true);\n      // Note: bcast should always be valid, since we rejected any incompatible\n      // shapes when we called ValidateDefaultValueShape().\n      OP_REQUIRES(context, bcast.IsValid(),\n                  errors::InvalidArgument(\"Error broadcasting default_value\"));\n      OP_REQUIRES_OK(context,\n                     context->allocate_temp(default_value_tensor.dtype(),\n                                            element_shape, &bcast_default));\n      const CPUDevice& device = context->eigen_device<CPUDevice>();\n      functor::BroadcastTo<CPUDevice, VALUE_TYPE>()(\n          device, context, bcast_default, element_shape, default_value_tensor,\n          src_shape, bcast);\n      default_value = bcast_default.flat<VALUE_TYPE>().data();\n    }\n\n    // Loop through the output_index vector, finding contiguous regions that\n    // should be copied.  Once we find the end of a contiguous region, copy it\n    // and add any necessary padding (with default_value).\n    INDEX_TYPE src_start = 0;  // Start of contiguous region (in values)\n    INDEX_TYPE dst_start = 0;  // Destination for contiguous region (in output)\n    INDEX_TYPE dst_end = 0;    // Destination for contiguous region (in output)\n    for (int src_i = 0; src_i <= output_index_size; ++src_i) {\n      // dst_i is the destination where the value at src_i should be copied.\n      INDEX_TYPE dst_i = src_i < output_index_size ? output_index[src_i] : -1;\n\n      // If we're still in a contiguous region, then update dst_end go to the\n      // next src_i.\n      if (dst_i == dst_end) {\n        ++dst_end;\n        continue;\n      }\n\n      // We found the end of contiguous region.  This can be because we found\n      // a gap (dst_i > dst_end), or a source value that shouldn't be copied\n      // because it's out-of-bounds (dst_i == -1), or the end of the tensor\n      // (dst_i = -1).\n      if (dst_start < dst_end) {\n        // Copy the contiguous region.\n        const VALUE_TYPE* src = values_base + src_start * value_element_size;\n        VALUE_TYPE* dst = output_base + dst_start * value_element_size;\n        INDEX_TYPE nvals = (dst_end - dst_start) * value_element_size;\n        copy_array<VALUE_TYPE, INDEX_TYPE>(dst, src, nvals);\n      }\n\n      // Add any necessary padding (w/ default_value).\n      if (src_i >= output_index_size) {\n        // We reached the end of values: pad to the end of output.\n        size_t output_size = output_tensor->NumElements();\n        dst_i = output_size / value_element_size;\n      }\n      if (dst_i > dst_end) {\n        if (default_value_tensor.NumElements() == 1) {\n          std::fill(output_base + dst_end * value_element_size,\n                    output_base + dst_i * value_element_size, *default_value);\n          dst_end = dst_i;\n        } else {\n          while (dst_i > dst_end) {\n            VALUE_TYPE* dst = output_base + dst_end * value_element_size;\n            copy_array<VALUE_TYPE, INDEX_TYPE>(dst, default_value,\n                                               value_element_size);\n            ++dst_end;\n          }\n        }\n      }\n\n      // Update indices.\n      if (dst_i < 0) {\n        // src_i should be skipped -- leave it out of the contiguous region.\n        src_start = src_i + 1;\n        dst_start = dst_end;\n      } else {\n        // src_i should be copied -- include it in the contiguous region.\n        src_start = src_i;\n        dst_start = dst_end;\n        dst_end = dst_start + 1;\n      }\n    }\n  }\n};\n\n#define REGISTER_CPU_KERNEL_INDEX_TYPE(value_type, index_type)       \\\n  REGISTER_KERNEL_BUILDER(Name(\"RaggedTensorToTensor\")               \\\n                              .Device(DEVICE_CPU)                    \\\n                              .TypeConstraint<value_type>(\"T\")       \\\n                              .TypeConstraint<index_type>(\"Tindex\"), \\\n                          RaggedTensorToTensorOp<value_type, index_type>);\n\n#define REGISTER_CPU_KERNEL(value_type)                          \\\n  REGISTER_CPU_KERNEL_INDEX_TYPE(value_type, tensorflow::int64); \\\n  REGISTER_CPU_KERNEL_INDEX_TYPE(value_type, tensorflow::int32);\n\nTF_CALL_POD_TYPES(REGISTER_CPU_KERNEL);\nTF_CALL_string(REGISTER_CPU_KERNEL);\nTF_CALL_QUANTIZED_TYPES(REGISTER_CPU_KERNEL);\nTF_CALL_quint16(REGISTER_CPU_KERNEL);\nTF_CALL_qint16(REGISTER_CPU_KERNEL);\n\n#undef REGISTER_CPU_KERNEL\n\n}  // namespace\n}  // namespace tensorflow\n"], "fixing_code": ["/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#define EIGEN_USE_THREADS\n\n#include <stddef.h>\n\n#include <algorithm>\n#include <string>\n#include <vector>\n\n#include \"tensorflow/core/framework/kernel_def_builder.h\"\n#include \"tensorflow/core/framework/numeric_types.h\"\n#include \"tensorflow/core/framework/op.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/register_types.h\"\n#include \"tensorflow/core/framework/shape_inference.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/tensor_shape.h\"\n#include \"tensorflow/core/framework/tensor_shape.pb.h\"\n#include \"tensorflow/core/framework/tensor_types.h\"\n#include \"tensorflow/core/framework/types.h\"\n#include \"tensorflow/core/kernels/broadcast_to_op.h\"\n#include \"tensorflow/core/kernels/list_kernels.h\"\n#include \"tensorflow/core/lib/core/errors.h\"\n#include \"tensorflow/core/lib/core/status.h\"\n#include \"tensorflow/core/platform/bfloat16.h\"\n#include \"tensorflow/core/platform/types.h\"\n#include \"tensorflow/core/util/bcast.h\"\n#include \"tensorflow/core/util/ragged_to_dense_util.h\"\n\nnamespace tensorflow {\n\nnamespace {\ntypedef Eigen::ThreadPoolDevice CPUDevice;\nusing ::std::vector;\n\nconst int kShapeInputIndex = 0;\nconst int kValueInputIndex = 1;\nconst int kDefaultValueInputIndex = 2;\nconst int kFirstPartitionInputIndex = 3;\n\ntemplate <typename INDEX_TYPE>\nclass RaggedTensorToTensorBaseOp : public OpKernel {\n public:\n  typedef\n      typename ::tensorflow::TTypes<const INDEX_TYPE>::Flat RowPartitionTensor;\n\n  explicit RaggedTensorToTensorBaseOp(OpKernelConstruction* context)\n      : OpKernel(context) {\n    OP_REQUIRES_OK(context, GetRowPartitionTypes<OpKernelConstruction>(\n                                context, &row_partition_types_));\n    ragged_rank_ = GetRaggedRank(row_partition_types_);\n  }\n\n  // Returns the relationship between dimension and dimension + 1.\n  RowPartitionType GetRowPartitionTypeByDimension(int dimension) {\n    if (row_partition_types_[0] == RowPartitionType::FIRST_DIM_SIZE) {\n      return row_partition_types_[dimension + 1];\n    } else {\n      return row_partition_types_[dimension];\n    }\n  }\n\n  // Returns the relationship between dimension and dimension + 1.\n  RowPartitionTensor GetRowPartitionTensor(OpKernelContext* c, int dimension) {\n    if (row_partition_types_[0] == RowPartitionType::FIRST_DIM_SIZE) {\n      return c->input(dimension + 1 + kFirstPartitionInputIndex)\n          .flat<INDEX_TYPE>();\n    } else {\n      return c->input(dimension + kFirstPartitionInputIndex).flat<INDEX_TYPE>();\n    }\n  }\n\n  Status GetMaxWidth(OpKernelContext* c, int dimension, INDEX_TYPE* result) {\n    const RowPartitionTensor row_partition_tensor =\n        GetRowPartitionTensor(c, dimension - 1);\n    switch (GetRowPartitionTypeByDimension(dimension - 1)) {\n      case RowPartitionType::VALUE_ROWIDS:\n        *result = GetMaxWidthValueRowID(row_partition_tensor);\n        return Status::OK();\n      case RowPartitionType::ROW_SPLITS:\n        *result = GetMaxWidthRowSplit(row_partition_tensor);\n        return Status::OK();\n      default:\n        return errors::InvalidArgument(\n            \"Cannot handle partition type \",\n            RowPartitionTypeToString(\n                GetRowPartitionTypeByDimension(dimension - 1)));\n    }\n  }\n\n  static INDEX_TYPE GetMaxWidthRowSplit(const RowPartitionTensor& row_split) {\n    const INDEX_TYPE tensor_length = row_split.size();\n    if (tensor_length == 0 || tensor_length == 1) {\n      return 0;\n    }\n    INDEX_TYPE max_width = 0;\n    for (INDEX_TYPE i = 0; i < tensor_length - 1; ++i) {\n      const INDEX_TYPE current_width = row_split(i + 1) - row_split(i);\n      if (current_width > max_width) {\n        max_width = current_width;\n      }\n    }\n    return max_width;\n  }\n\n  static INDEX_TYPE GetMaxWidthValueRowID(\n      const RowPartitionTensor& value_rowids) {\n    const INDEX_TYPE index_length = value_rowids.size();\n    if (index_length == 0) {\n      return 0;\n    }\n    INDEX_TYPE first_equal_index = 0;\n    INDEX_TYPE first_equal_index_value = value_rowids(0);\n    INDEX_TYPE max_width = 0;\n    for (INDEX_TYPE i = 1; i < index_length; ++i) {\n      const INDEX_TYPE value = value_rowids(i);\n      if (value != first_equal_index_value) {\n        first_equal_index_value = value;\n        max_width = std::max(i - first_equal_index, max_width);\n        first_equal_index = i;\n      }\n    }\n    return std::max(index_length - first_equal_index, max_width);\n  }\n\n  Status CalculateOutputSize(INDEX_TYPE first_dim, OpKernelContext* c,\n                             vector<INDEX_TYPE>* result) {\n    TensorShapeProto value_shape_proto;\n    c->input(kValueInputIndex).shape().AsProto(&value_shape_proto);\n\n    TensorShapeProto default_value_shape_proto;\n    c->input(kDefaultValueInputIndex)\n        .shape()\n        .AsProto(&default_value_shape_proto);\n\n    TensorShapeProto output_shape_proto;\n    TF_RETURN_IF_ERROR(ValidateDefaultValueShape(default_value_shape_proto,\n                                                 value_shape_proto));\n\n    TensorShapeProto shape_proto;\n    {\n      PartialTensorShape partial_tensor_shape;\n      TF_RETURN_IF_ERROR(TensorShapeFromTensor(c->input(kShapeInputIndex),\n                                               &partial_tensor_shape));\n      partial_tensor_shape.AsProto(&shape_proto);\n    }\n\n    TF_RETURN_IF_ERROR(CombineRaggedTensorToTensorShapes(\n        ragged_rank_, shape_proto, value_shape_proto, &output_shape_proto));\n\n    result->reserve(output_shape_proto.dim_size());\n    for (const TensorShapeProto::Dim& dim : output_shape_proto.dim()) {\n      // Note that this may be -1 (if dimension size is unknown).\n      result->push_back(dim.size());\n    }\n\n    if ((*result)[0] < 0) {\n      (*result)[0] = first_dim;\n    }\n    for (int i = 1; i <= ragged_rank_; ++i) {\n      if ((*result)[i] < 0) {\n        TF_RETURN_IF_ERROR(GetMaxWidth(c, i, &(*result)[i]));\n      }\n    }\n    return Status::OK();\n  }\n\n  /**\n   * The output_index represents the index in the output tensor\n   * where the first element of a particular dimension would be written.\n   * If it is -1, it indicates that the index is out of scope.\n   * Example, given first_dimension = 10, first_dimension_output = 6,\n   * and output_index_multiplier = 100:\n   * result = [0 100 200 300 400 500 -1 -1 -1 -1]\n   * If first_dimension_output = 11 instead, then:\n   * result = [0 100 200 300 400 500 600 700 800 900]\n   */\n  void CalculateFirstParentOutputIndex(INDEX_TYPE first_dimension,\n                                       INDEX_TYPE output_index_multiplier,\n                                       INDEX_TYPE first_dimension_output,\n                                       vector<INDEX_TYPE>* result) {\n    const INDEX_TYPE min_dimension =\n        std::min(first_dimension, first_dimension_output);\n    result->reserve(first_dimension);\n    int current_output_index = 0;\n    for (INDEX_TYPE i = 0; i < min_dimension;\n         ++i, current_output_index += output_index_multiplier) {\n      result->push_back(current_output_index);\n    }\n    for (INDEX_TYPE i = min_dimension; i < first_dimension; ++i) {\n      result->push_back(-1);\n    }\n    DCHECK_EQ(result->size(), first_dimension);\n  }\n\n  void CalculateOutputIndexRowSplit(\n      OpKernelContext* context, const RowPartitionTensor& row_split,\n      const vector<INDEX_TYPE>& parent_output_index,\n      INDEX_TYPE output_index_multiplier, INDEX_TYPE output_size,\n      vector<INDEX_TYPE>* result) {\n    INDEX_TYPE row_split_size = row_split.size();\n    if (row_split_size > 0) {\n      result->reserve(row_split(row_split_size - 1));\n    }\n    for (INDEX_TYPE i = 0; i < row_split_size - 1; ++i) {\n      INDEX_TYPE row_length = row_split(i + 1) - row_split(i);\n      INDEX_TYPE real_length = std::min(output_size, row_length);\n      INDEX_TYPE parent_output_index_current = parent_output_index[i];\n\n      if (parent_output_index_current == -1) {\n        real_length = 0;\n      }\n      for (INDEX_TYPE j = 0; j < real_length; ++j) {\n        result->push_back(parent_output_index_current);\n        parent_output_index_current += output_index_multiplier;\n      }\n      for (INDEX_TYPE j = 0; j < row_length - real_length; ++j) {\n        result->push_back(-1);\n      }\n    }\n    if (row_split_size > 0) {\n      OP_REQUIRES(context, result->size() == row_split(row_split_size - 1),\n                  errors::InvalidArgument(\"Invalid row split size.\"));\n    }\n  }\n\n  // Calculate the output index of the first element of a list.\n  // The parent_output_index is the same computation for the previous list.\n  // -1 indicates an element or list that is out of range.\n  // The output_index_multiplier is the number of output indices one moves\n  // forward for each column.\n  // E.g., given:\n  // value_rowids:[0 1 2 2 2 3 5 5 6]\n  // parent_output_index:[1000 1100 2000 2100 -1 3000 4000]\n  // output_index_multiplier: 10\n  // output_size: 2\n  // You get:\n  // result = [1000 1100 2000 2010 -1 2100 -1 -1 3000]\n  // result[0] = parent_output_index[value_rowids[0]]\n  // result[1] = parent_output_index[value_rowids[1]]\n  // result[2] = parent_output_index[value_rowids[2]]\n  // result[3] = parent_output_index[value_rowids[2] + 10]\n  // result[4] = -1 because it is the third element the size is 2.\n  // result[5] = parent_output_index[value_rowids[3]]\n  // result[6] = -1 because parent_output_index[value_rowids[6]] == -1\n  // result[7] = -1 because parent_output_index[value_rowids[6]] == -1\n  // result[8] = parent_output_index[value_rowids[7]]\n  void CalculateOutputIndexValueRowID(\n      OpKernelContext* context, const RowPartitionTensor& value_rowids,\n      const vector<INDEX_TYPE>& parent_output_index,\n      INDEX_TYPE output_index_multiplier, INDEX_TYPE output_size,\n      vector<INDEX_TYPE>* result) {\n    const INDEX_TYPE index_size = value_rowids.size();\n    result->reserve(index_size);\n    if (index_size == 0) {\n      return;\n    }\n\n    INDEX_TYPE current_output_column = 0;\n    INDEX_TYPE current_value_rowid = value_rowids(0);\n    DCHECK_LT(current_value_rowid, parent_output_index.size());\n    INDEX_TYPE current_output_index = parent_output_index[current_value_rowid];\n    result->push_back(current_output_index);\n    for (INDEX_TYPE i = 1; i < index_size; ++i) {\n      INDEX_TYPE next_value_rowid = value_rowids(i);\n      if (next_value_rowid == current_value_rowid) {\n        if (current_output_index >= 0) {\n          ++current_output_column;\n          if (current_output_column < output_size) {\n            current_output_index += output_index_multiplier;\n          } else {\n            current_output_index = -1;\n          }\n        }\n      } else {\n        current_output_column = 0;\n        current_value_rowid = next_value_rowid;\n        DCHECK_LT(next_value_rowid, parent_output_index.size());\n        current_output_index = parent_output_index[next_value_rowid];\n      }\n      result->push_back(current_output_index);\n    }\n    OP_REQUIRES(context, result->size() == value_rowids.size(),\n                errors::InvalidArgument(\"Invalid row ids.\"));\n  }\n\n  Status CalculateOutputIndex(OpKernelContext* context, int dimension,\n                              const vector<INDEX_TYPE>& parent_output_index,\n                              INDEX_TYPE output_index_multiplier,\n                              INDEX_TYPE output_size,\n                              vector<INDEX_TYPE>* result) {\n    const RowPartitionTensor row_partition_tensor =\n        GetRowPartitionTensor(context, dimension);\n    auto partition_type = GetRowPartitionTypeByDimension(dimension);\n    switch (partition_type) {\n      case RowPartitionType::VALUE_ROWIDS:\n        CalculateOutputIndexValueRowID(\n            context, row_partition_tensor, parent_output_index,\n            output_index_multiplier, output_size, result);\n        return tensorflow::Status::OK();\n      case RowPartitionType::ROW_SPLITS:\n        CalculateOutputIndexRowSplit(\n            context, row_partition_tensor, parent_output_index,\n            output_index_multiplier, output_size, result);\n        return tensorflow::Status::OK();\n      default:\n        return errors::InvalidArgument(\n            \"Unsupported partition type:\",\n            RowPartitionTypeToString(partition_type));\n    }\n  }\n\n  Status GetFirstDimensionSize(OpKernelContext* context, INDEX_TYPE* result) {\n    const Tensor first_partition_tensor =\n        context->input(kFirstPartitionInputIndex);\n    const RowPartitionType first_partition_type = row_partition_types_[0];\n    switch (first_partition_type) {\n      case RowPartitionType::FIRST_DIM_SIZE:\n        *result = first_partition_tensor.scalar<INDEX_TYPE>()();\n        return Status::OK();\n      case RowPartitionType::VALUE_ROWIDS:\n        return errors::InvalidArgument(\n            \"Cannot handle VALUE_ROWIDS in first dimension.\");\n      case RowPartitionType::ROW_SPLITS:\n        *result = first_partition_tensor.shape().dim_size(0) - 1;\n        return Status::OK();\n      default:\n        return errors::InvalidArgument(\n            \"Cannot handle type \",\n            RowPartitionTypeToString(first_partition_type));\n    }\n  }\n\n  void Compute(OpKernelContext* context) override {\n    INDEX_TYPE first_dimension;\n    const Tensor first_partition_tensor =\n        context->input(kFirstPartitionInputIndex);\n    OP_REQUIRES(context, first_partition_tensor.NumElements() > 0,\n                errors::InvalidArgument(\"Invalid first partition input. Tensor \"\n                                        \"requires at least one element.\"));\n    OP_REQUIRES_OK(context, GetFirstDimensionSize(context, &first_dimension));\n    vector<INDEX_TYPE> output_size;\n    OP_REQUIRES_OK(context,\n                   CalculateOutputSize(first_dimension, context, &output_size));\n    vector<INDEX_TYPE> multiplier;\n    multiplier.resize(ragged_rank_ + 1);\n\n    multiplier[multiplier.size() - 1] = 1;\n    for (int i = multiplier.size() - 2; i >= 0; --i) {\n      multiplier[i] = multiplier[i + 1] * output_size[i + 1];\n    }\n    // Full size of the tensor.\n    TensorShape output_shape;\n    OP_REQUIRES_OK(context,\n                   TensorShapeUtils::MakeShape(output_size, &output_shape));\n    Tensor* output_tensor = nullptr;\n\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(0, output_shape, &output_tensor));\n    const INDEX_TYPE full_size = multiplier[0] * output_size[0];\n    if (full_size > 0) {\n      vector<INDEX_TYPE> output_index, new_output_index;\n      int nvals = context->input(kValueInputIndex).shape().dim_size(0);\n      output_index.reserve(nvals);\n      new_output_index.reserve(nvals);\n\n      CalculateFirstParentOutputIndex(first_dimension, multiplier[0],\n                                      output_size[0], &output_index);\n      for (int i = 1; i <= ragged_rank_; ++i) {\n        OP_REQUIRES_OK(context, CalculateOutputIndex(\n                                    context, i - 1, output_index, multiplier[i],\n                                    output_size[i], &new_output_index));\n        output_index.swap(new_output_index);\n        new_output_index.clear();\n      }\n\n      SetOutput(context, ragged_rank_, output_index, output_tensor);\n    }\n  }\n  virtual void SetOutput(OpKernelContext* context, int ragged_rank,\n                         const vector<INDEX_TYPE>& output_index,\n                         Tensor* output_tensor) = 0;\n\n private:\n  vector<RowPartitionType> row_partition_types_;\n  int ragged_rank_;\n};\n\ntemplate <typename VALUE_TYPE, typename INDEX_TYPE>\nvoid slow_copy_array(VALUE_TYPE* dst, const VALUE_TYPE* src, INDEX_TYPE size) {\n  for (INDEX_TYPE index = 0; index < size; ++index) {\n    dst[index] = src[index];\n  }\n}\n\ntemplate <typename VALUE_TYPE, typename INDEX_TYPE>\nvoid copy_array(VALUE_TYPE* dst, const VALUE_TYPE* src, INDEX_TYPE size) {\n  memcpy(dst, src, size * sizeof(VALUE_TYPE));\n}\n\ntemplate <>\nvoid copy_array<tstring, int64>(tstring* dst, const tstring* src, int64 size) {\n  slow_copy_array(dst, src, size);\n}\n\ntemplate <>\nvoid copy_array<tstring, int32>(tstring* dst, const tstring* src, int32 size) {\n  slow_copy_array(dst, src, size);\n}\n\n// If we don't specialize for Eigen::half, we get:\n// undefined behavior, destination object type 'Eigen::half'\n// is not TriviallyCopyable\ntemplate <>\nvoid copy_array<Eigen::half, int64>(Eigen::half* dst, const Eigen::half* src,\n                                    int64 size) {\n  slow_copy_array(dst, src, size);\n}\n\ntemplate <>\nvoid copy_array<Eigen::half, int32>(Eigen::half* dst, const Eigen::half* src,\n                                    int32 size) {\n  slow_copy_array(dst, src, size);\n}\n\ntemplate <typename VALUE_TYPE, typename INDEX_TYPE>\nclass RaggedTensorToTensorOp : public RaggedTensorToTensorBaseOp<INDEX_TYPE> {\n public:\n  explicit RaggedTensorToTensorOp(OpKernelConstruction* context)\n      : RaggedTensorToTensorBaseOp<INDEX_TYPE>(context) {}\n\n  void SetOutput(OpKernelContext* context, int ragged_rank,\n                 const vector<INDEX_TYPE>& output_index,\n                 Tensor* output_tensor) override {\n    // Note: it's ok to use OP_REQUIRES_OK (rather than TF_RETURN_IF_ERROR)\n    // in this function, but only because it's the last thing we do before\n    // returning from Compute().\n\n    if (output_tensor->NumElements() == 0) return;\n\n    const auto& values_tensor = context->input(kValueInputIndex);\n    const VALUE_TYPE* values_base = values_tensor.flat<VALUE_TYPE>().data();\n    const auto& default_value_tensor = context->input(kDefaultValueInputIndex);\n    VALUE_TYPE* output_base = output_tensor->flat<VALUE_TYPE>().data();\n\n    TensorShape element_shape = output_tensor->shape();\n    element_shape.RemoveDimRange(0, ragged_rank + 1);\n    int value_element_size = element_shape.num_elements();\n    size_t output_index_size = output_index.size();\n\n    // Broadcast the default value to value_element_size.  (We can skip this\n    // if default_value_tensor.NumElements() == 1, since we use std::fill\n    // when that's true.)\n    const VALUE_TYPE* default_value =\n        default_value_tensor.flat<VALUE_TYPE>().data();\n    Tensor bcast_default;  // Temporary tensor for result of broadcast\n    if (default_value_tensor.NumElements() != value_element_size &&\n        default_value_tensor.NumElements() != 1) {\n      const auto& src_shape = default_value_tensor.shape();\n      BCast bcast(BCast::FromShape(src_shape), BCast::FromShape(element_shape),\n                  /*fewer_dims_optimization=*/true);\n      // Note: bcast should always be valid, since we rejected any incompatible\n      // shapes when we called ValidateDefaultValueShape().\n      OP_REQUIRES(context, bcast.IsValid(),\n                  errors::InvalidArgument(\"Error broadcasting default_value\"));\n      OP_REQUIRES_OK(context,\n                     context->allocate_temp(default_value_tensor.dtype(),\n                                            element_shape, &bcast_default));\n      const CPUDevice& device = context->eigen_device<CPUDevice>();\n      functor::BroadcastTo<CPUDevice, VALUE_TYPE>()(\n          device, context, bcast_default, element_shape, default_value_tensor,\n          src_shape, bcast);\n      default_value = bcast_default.flat<VALUE_TYPE>().data();\n    }\n\n    // Loop through the output_index vector, finding contiguous regions that\n    // should be copied.  Once we find the end of a contiguous region, copy it\n    // and add any necessary padding (with default_value).\n    INDEX_TYPE src_start = 0;  // Start of contiguous region (in values)\n    INDEX_TYPE dst_start = 0;  // Destination for contiguous region (in output)\n    INDEX_TYPE dst_end = 0;    // Destination for contiguous region (in output)\n    for (int src_i = 0; src_i <= output_index_size; ++src_i) {\n      // dst_i is the destination where the value at src_i should be copied.\n      INDEX_TYPE dst_i = src_i < output_index_size ? output_index[src_i] : -1;\n\n      // If we're still in a contiguous region, then update dst_end go to the\n      // next src_i.\n      if (dst_i == dst_end) {\n        ++dst_end;\n        continue;\n      }\n\n      // We found the end of contiguous region.  This can be because we found\n      // a gap (dst_i > dst_end), or a source value that shouldn't be copied\n      // because it's out-of-bounds (dst_i == -1), or the end of the tensor\n      // (dst_i = -1).\n      if (dst_start < dst_end) {\n        // Copy the contiguous region.\n        const VALUE_TYPE* src = values_base + src_start * value_element_size;\n        VALUE_TYPE* dst = output_base + dst_start * value_element_size;\n        INDEX_TYPE nvals = (dst_end - dst_start) * value_element_size;\n        copy_array<VALUE_TYPE, INDEX_TYPE>(dst, src, nvals);\n      }\n\n      // Add any necessary padding (w/ default_value).\n      if (src_i >= output_index_size) {\n        // We reached the end of values: pad to the end of output.\n        size_t output_size = output_tensor->NumElements();\n        dst_i = output_size / value_element_size;\n      }\n      if (dst_i > dst_end) {\n        if (default_value_tensor.NumElements() == 1) {\n          std::fill(output_base + dst_end * value_element_size,\n                    output_base + dst_i * value_element_size, *default_value);\n          dst_end = dst_i;\n        } else {\n          while (dst_i > dst_end) {\n            VALUE_TYPE* dst = output_base + dst_end * value_element_size;\n            copy_array<VALUE_TYPE, INDEX_TYPE>(dst, default_value,\n                                               value_element_size);\n            ++dst_end;\n          }\n        }\n      }\n\n      // Update indices.\n      if (dst_i < 0) {\n        // src_i should be skipped -- leave it out of the contiguous region.\n        src_start = src_i + 1;\n        dst_start = dst_end;\n      } else {\n        // src_i should be copied -- include it in the contiguous region.\n        src_start = src_i;\n        dst_start = dst_end;\n        dst_end = dst_start + 1;\n      }\n    }\n  }\n};\n\n#define REGISTER_CPU_KERNEL_INDEX_TYPE(value_type, index_type)       \\\n  REGISTER_KERNEL_BUILDER(Name(\"RaggedTensorToTensor\")               \\\n                              .Device(DEVICE_CPU)                    \\\n                              .TypeConstraint<value_type>(\"T\")       \\\n                              .TypeConstraint<index_type>(\"Tindex\"), \\\n                          RaggedTensorToTensorOp<value_type, index_type>);\n\n#define REGISTER_CPU_KERNEL(value_type)                          \\\n  REGISTER_CPU_KERNEL_INDEX_TYPE(value_type, tensorflow::int64); \\\n  REGISTER_CPU_KERNEL_INDEX_TYPE(value_type, tensorflow::int32);\n\nTF_CALL_POD_TYPES(REGISTER_CPU_KERNEL);\nTF_CALL_string(REGISTER_CPU_KERNEL);\nTF_CALL_QUANTIZED_TYPES(REGISTER_CPU_KERNEL);\nTF_CALL_quint16(REGISTER_CPU_KERNEL);\nTF_CALL_qint16(REGISTER_CPU_KERNEL);\n\n#undef REGISTER_CPU_KERNEL\n\n}  // namespace\n}  // namespace tensorflow\n"], "filenames": ["tensorflow/core/kernels/ragged_tensor_to_tensor_op.cc"], "buggy_code_start_loc": [211], "buggy_code_end_loc": [317], "fixing_code_start_loc": [211], "fixing_code_end_loc": [319], "type": "CWE-131", "message": "TensorFlow is an end-to-end open source platform for machine learning. Due to lack of validation in `tf.raw_ops.RaggedTensorToTensor`, an attacker can exploit an undefined behavior if input arguments are empty. The implementation(https://github.com/tensorflow/tensorflow/blob/656e7673b14acd7835dc778867f84916c6d1cac2/tensorflow/core/kernels/ragged_tensor_to_tensor_op.cc#L356-L360) only checks that one of the tensors is not empty, but does not check for the other ones. There are multiple `DCHECK` validations to prevent heap OOB, but these are no-op in release builds, hence they don't prevent anything. The fix will be included in TensorFlow 2.5.0. We will also cherrypick these commits on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range.", "other": {"cve": {"id": "CVE-2021-29608", "sourceIdentifier": "security-advisories@github.com", "published": "2021-05-14T20:15:15.803", "lastModified": "2021-07-26T16:23:47.727", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "TensorFlow is an end-to-end open source platform for machine learning. Due to lack of validation in `tf.raw_ops.RaggedTensorToTensor`, an attacker can exploit an undefined behavior if input arguments are empty. The implementation(https://github.com/tensorflow/tensorflow/blob/656e7673b14acd7835dc778867f84916c6d1cac2/tensorflow/core/kernels/ragged_tensor_to_tensor_op.cc#L356-L360) only checks that one of the tensors is not empty, but does not check for the other ones. There are multiple `DCHECK` validations to prevent heap OOB, but these are no-op in release builds, hence they don't prevent anything. The fix will be included in TensorFlow 2.5.0. We will also cherrypick these commits on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range."}, {"lang": "es", "value": "TensorFlow es una plataforma de c\u00f3digo abierto de extremo a extremo para el aprendizaje autom\u00e1tico.&#xa0;Debido a una falta de comprobaci\u00f3n en \"tf.raw_ops.RaggedTensorToTensor\", un atacante puede explotar un comportamiento indefinido si los argumentos de entrada est\u00e1n vac\u00edos.&#xa0;La implementaci\u00f3n (https://github.com/tensorflow/tensorflow/blob/656e7673b14acd7835dc778867f84916c6d1cac2/tensorflow/core/kernels/ragged_tensor_to_tensor_op.cc#L356-L360) solo comprueba que uno de los tensores no est\u00e9 vac\u00edo, sino que no comprueba si est\u00e1 vac\u00edo, otros.&#xa0;Se presentan m\u00faltiples comprobaciones de \"DCHECK\" para evitar la pila de OOB, pero estas no son operativas en unas versiones de lanzamiento, por lo que no evitan nada.&#xa0;La correcci\u00f3n ser\u00e1 incluida en TensorFlow versi\u00f3n 2.5.0.&#xa0;Tambi\u00e9n seleccionaremos este commits en TensorFlow versi\u00f3n 2.4.2, TensorFlow versi\u00f3n 2.3.3, TensorFlow versi\u00f3n 2.2.3 y TensorFlow versi\u00f3n 2.1.4, ya que tambi\u00e9n est\u00e1n afectadas y a\u00fan se encuentran en el rango compatible"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.9}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:H/PR:L/UI:N/S:U/C:N/I:L/A:H", "attackVector": "LOCAL", "attackComplexity": "HIGH", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "LOW", "availabilityImpact": "HIGH", "baseScore": 5.3, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.0, "impactScore": 4.2}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:P/I:P/A:P", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "PARTIAL", "availabilityImpact": "PARTIAL", "baseScore": 4.6}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.9, "impactScore": 6.4, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "security-advisories@github.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-131"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionEndExcluding": "2.1.4", "matchCriteriaId": "323ABCCE-24EB-47CC-87F6-48C101477587"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.2.0", "versionEndExcluding": "2.2.3", "matchCriteriaId": "64ABA90C-0649-4BB0-89C9-83C14BBDCC0F"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.3.0", "versionEndExcluding": "2.3.3", "matchCriteriaId": "0F83E0CF-CBF6-4C24-8683-3E7A5DC95BA9"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.4.0", "versionEndExcluding": "2.4.2", "matchCriteriaId": "8259531B-A8AC-4F8B-B60F-B69DE4767C03"}]}]}], "references": [{"url": "https://github.com/tensorflow/tensorflow/commit/b761c9b652af2107cfbc33efd19be0ce41daa33e", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/commit/c4d7afb6a5986b04505aca4466ae1951686c80f6", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/commit/f94ef358bb3e91d517446454edff6535bcfe8e4a", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/security/advisories/GHSA-rgvq-pcvf-hx75", "source": "security-advisories@github.com", "tags": ["Exploit", "Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/tensorflow/tensorflow/commit/b761c9b652af2107cfbc33efd19be0ce41daa33e"}}